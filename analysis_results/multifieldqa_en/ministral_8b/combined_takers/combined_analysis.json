{
  "overall_stats": {
    "avg_difficulty": 0.5391487552465792,
    "avg_discrimination": 1.2517563371849187,
    "total_questions": 403
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.5096486724577847,
      "avg_discrimination": 1.240389247826948,
      "avg_feasibility": 0.26531576720563277,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.55989518865599,
      "avg_discrimination": 1.2346436008132007,
      "avg_feasibility": 0.249895441524787,
      "num_questions": 87
    },
    "3": {
      "avg_difficulty": 0.4916154107185506,
      "avg_discrimination": 1.2421858636199088,
      "avg_feasibility": 0.2540842865092371,
      "num_questions": 38
    },
    "4": {
      "avg_difficulty": 0.550473497417048,
      "avg_discrimination": 1.3079057133739422,
      "avg_feasibility": 0.25832093962895425,
      "num_questions": 51
    },
    "5": {
      "avg_difficulty": 0.5891328845072495,
      "avg_discrimination": 1.26076843170265,
      "avg_feasibility": 0.24659163105343263,
      "num_questions": 77
    }
  },
  "model_abilities": {
    "llama3-8b_DenseV3": 1.5992931029550084,
    "llama3-8b_DenseV4": 1.5845700040053319,
    "llama3-8b_SparseV3": 1.6421161443350192,
    "llama3-8b_HybridV3": 1.5318487638226566,
    "llama3-8b_RerankV3": 1.683465894005995,
    "llama3-8b_DenseV5-5": 1.6086602536517192,
    "llama3-8b_SparseV5-5": 1.7088661408362955,
    "llama3-8b_HybridV5-5": 1.7105857366893185,
    "llama3-8b_RerankV5-5": 1.6350401188609345,
    "llama3-8b_DenseV5-10": 1.6648870815652808,
    "llama3-8b_SparseV5-10": 1.6705355004298705,
    "llama3-8b_HybridV5-10": 1.5754374888778435,
    "llama3-8b_RerankV5-10": 1.5538070187212174,
    "llama3-8b_open_book": 1.511752535974277,
    "mistral-8b_DenseV3": 1.5305012768288253,
    "mistral-8b_SparseV3": 1.5733243182088361,
    "mistral-8b_HybridV3": 1.4630569376964735,
    "mistral-8b_RerankV3": 1.614674067879812,
    "mistral-8b_DenseV5-5": 1.539868427525536,
    "mistral-8b_SparseV5-5": 1.6400743147101124,
    "mistral-8b_HybridV5-5": 1.6417939105631354,
    "mistral-8b_RerankV5-5": 1.5662482927347514,
    "mistral-8b_DenseV5-10": 1.5960952554390977,
    "mistral-8b_SparseV5-10": 1.6017436743036875,
    "mistral-8b_HybridV5-10": 1.5066456627516605,
    "mistral-8b_RerankV5-10": 1.4850151925950343,
    "mistral-8b_open_book": 1.4429607098480939,
    "gemma2_9b": 0.8148755535592778,
    "gemma2_9b_DenseV3": 1.569830703794049,
    "gemma2_9b_SparseV3": 1.6126537451740601,
    "gemma2_9b_HybridV3": 1.5023863646616973,
    "gemma2_9b_RerankV3": 1.6540034948450357,
    "gemma2_9b_DenseV5-5": 1.5791978544907601,
    "gemma2_9b_SparseV5-5": 1.6794037416753365,
    "gemma2_9b_HybridV5-5": 1.6811233375283594,
    "gemma2_9b_RerankV5-5": 1.6055777196999754,
    "gemma2_9b_DenseV5-10": 1.6354246824043215,
    "gemma2_9b_SparseV5-10": 1.6410731012689115,
    "gemma2_9b_HybridV5-10": 1.5459750897168845,
    "gemma2_9b_RerankV5-10": 1.5243446195602581,
    "gemma2_9b_open_book": 1.4822901368133177
  },
  "component_abilities": {
    "llms": {
      "gemma2_9b": 0.8148755535592778,
      "llama3-8b": 0.8443379527202369,
      "mistral-8b": 0.7755461265940539
    },
    "retrievers": {
      "RerankV5-5": 0.7907021661406975,
      "DenseV5-5": 0.7643223009314823,
      "open_book": 0.6674145832540399,
      "HybridV5-10": 0.7310995361576067,
      "RerankV5-10": 0.7094690660009804,
      "HybridV3": 0.6875108111024196,
      "DenseV4": 0.740232051285095,
      "HybridV5-5": 0.8662477839690816,
      "SparseV3": 0.7977781916147824,
      "SparseV5-5": 0.8645281881160587,
      "DenseV5-10": 0.8205491288450438,
      "RerankV3": 0.839127941285758,
      "DenseV3": 0.7549551502347713,
      "SparseV5-10": 0.8261975477096336
    }
  }
}