{
  "overall_stats": {
    "avg_difficulty": 0.5128186891905012,
    "avg_discrimination": 0.7694458134887424,
    "total_questions": 445
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.4549130366507967,
      "avg_discrimination": 1.2216895893118258,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.49195620344536795,
      "avg_discrimination": 0.5751609587465583,
      "num_questions": 155
    },
    "3": {
      "avg_difficulty": 0.5818892055342134,
      "avg_discrimination": 0.5,
      "num_questions": 62
    },
    "4": {
      "avg_difficulty": 0.5570372450342281,
      "avg_discrimination": 0.5,
      "num_questions": 39
    },
    "5": {
      "avg_difficulty": 0.6644247784273171,
      "avg_discrimination": 0.5,
      "num_questions": 39
    }
  },
  "model_abilities": {
    "llama3-8b_Dense": 0.5993026733048212,
    "llama3-8b_Sparse": 0.656986218024221,
    "mistral-8b_Dense": 0.617075465342811,
    "mistral-8b_Sparse": 0.6747590100622107,
    "mistral-8b_Hybrid": 0.6744159456830803,
    "mistral-8b_Rerank": 0.6157497179562508,
    "mistral-8b_open_book": 0.9149728441209573,
    "gemma2-27b_Dense": 1.1029551977080587,
    "gemma2-27b_Sparse": 1.1606387424274585,
    "gemma2-27b_Hybrid": 1.160295678048328,
    "gemma2-27b_Rerank": 1.1016294503214985
  },
  "component_abilities": {
    "llms": {
      "llama3-8b": -0.4931792201783147,
      "mistral-8b": -0.4754064281403249,
      "gemma2-27b": 0.01047330422492274
    },
    "retrievers": {
      "open_book": 1.3903792722612822,
      "Rerank": 1.0911561460965757,
      "Sparse": 1.1501654382025357,
      "Hybrid": 1.1498223738234052,
      "Dense": 1.092481893483136
    }
  }
}