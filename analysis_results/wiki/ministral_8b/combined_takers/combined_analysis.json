{
  "overall_stats": {
    "avg_difficulty": 0.5430847758527616,
    "avg_discrimination": 1.2646570391986065,
    "total_questions": 247
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.5528711931406652,
      "avg_discrimination": 1.2694954707937096,
      "avg_feasibility": 0.24129353263848913,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.5162706306958039,
      "avg_discrimination": 1.289456073260237,
      "avg_feasibility": 0.24826620408519265,
      "num_questions": 51
    },
    "3": {
      "avg_difficulty": 0.606983568538369,
      "avg_discrimination": 1.3192154134098846,
      "avg_feasibility": 0.219033986257538,
      "num_questions": 12
    },
    "4": {
      "avg_difficulty": 0.6374770970621435,
      "avg_discrimination": 1.2111739118222076,
      "avg_feasibility": 0.24064542847683756,
      "num_questions": 17
    },
    "5": {
      "avg_difficulty": 0.39767911920761856,
      "avg_discrimination": 1.1625392273430064,
      "avg_feasibility": 0.2385091627387684,
      "num_questions": 17
    }
  },
  "model_abilities": {
    "llama3-8b_DenseV3": 1.5029263291338768,
    "llama3-8b_DenseV4": 1.5495414136078658,
    "llama3-8b_SparseV3": 1.4982154537378922,
    "llama3-8b_HybridV3": 1.50431231794509,
    "llama3-8b_RerankV3": 1.6057712832021047,
    "llama3-8b_DenseV5-5": 1.5955107603418597,
    "llama3-8b_SparseV5-5": 1.6514066315674976,
    "llama3-8b_HybridV5-5": 1.5340056138300704,
    "llama3-8b_RerankV5-5": 1.5539205900746933,
    "llama3-8b_DenseV5-10": 1.6382272691279396,
    "llama3-8b_SparseV5-10": 1.5917198152391971,
    "llama3-8b_HybridV5-10": 1.5747837276294057,
    "llama3-8b_RerankV5-10": 1.5303830141950483,
    "llama3-8b_open_book": 1.6388485751460489,
    "mistral-8b_DenseV3": 1.4609657097822621,
    "mistral-8b_SparseV3": 1.4562548343862778,
    "mistral-8b_HybridV3": 1.4623516985934755,
    "mistral-8b_RerankV3": 1.56381066385049,
    "mistral-8b_DenseV5-5": 1.5535501409902452,
    "mistral-8b_SparseV5-5": 1.6094460122158831,
    "mistral-8b_HybridV5-5": 1.492044994478456,
    "mistral-8b_RerankV5-5": 1.5119599707230786,
    "mistral-8b_DenseV5-10": 1.5962666497763252,
    "mistral-8b_SparseV5-10": 1.5497591958875825,
    "mistral-8b_HybridV5-10": 1.5328231082777912,
    "mistral-8b_RerankV5-10": 1.4884223948434339,
    "mistral-8b_open_book": 1.5968879557944344,
    "gemma2_9b": 0.8593862546630968,
    "gemma2_9b_DenseV3": 1.6133437347726405,
    "gemma2_9b_SparseV3": 1.6086328593766561,
    "gemma2_9b_HybridV3": 1.6147297235838538,
    "gemma2_9b_RerankV3": 1.7161886888408682,
    "gemma2_9b_DenseV5-5": 1.7059281659806234,
    "gemma2_9b_SparseV5-5": 1.7618240372062612,
    "gemma2_9b_HybridV5-5": 1.644423019468834,
    "gemma2_9b_RerankV5-5": 1.6643379957134568,
    "gemma2_9b_DenseV5-10": 1.7486446747667035,
    "gemma2_9b_SparseV5-10": 1.7021372208779608,
    "gemma2_9b_HybridV5-10": 1.6852011332681696,
    "gemma2_9b_RerankV5-10": 1.6408004198338122,
    "gemma2_9b_open_book": 1.7492659807848125
  },
  "component_abilities": {
    "llms": {
      "gemma2_9b": 0.8593862546630968,
      "llama3-8b": 0.7489688490243331,
      "mistral-8b": 0.7070082296727186
    },
    "retrievers": {
      "RerankV5-5": 0.80495174105036,
      "DenseV5-5": 0.8465419113175265,
      "open_book": 0.8898797261217157,
      "HybridV5-10": 0.8258148786050726,
      "RerankV5-10": 0.7814141651707153,
      "HybridV3": 0.7553434689207569,
      "DenseV4": 0.8005725645835328,
      "HybridV5-5": 0.7850367648057373,
      "SparseV3": 0.7492466047135592,
      "SparseV5-5": 0.9024377825431644,
      "DenseV5-10": 0.8892584201036066,
      "RerankV3": 0.8568024341777715,
      "DenseV3": 0.7539574801095437,
      "SparseV5-10": 0.842750966214864
    }
  }
}