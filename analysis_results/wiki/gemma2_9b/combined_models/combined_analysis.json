{
  "overall_stats": {
    "avg_difficulty": 0.583013978131693,
    "avg_discrimination": 0.7376617120317099,
    "total_questions": 355
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.4880985731141802,
      "avg_discrimination": 0.8297311630213703,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.6745534429775966,
      "avg_discrimination": 0.7328992348157024,
      "num_questions": 146
    },
    "3": {
      "avg_difficulty": 0.6158437109693018,
      "avg_discrimination": 0.5211621603374015,
      "num_questions": 36
    },
    "4": {
      "avg_difficulty": 0.6039999999999999,
      "avg_discrimination": 0.5145107262812416,
      "num_questions": 10
    },
    "5": {
      "avg_difficulty": 0.543076923076923,
      "avg_discrimination": 0.5,
      "num_questions": 13
    }
  },
  "model_abilities": {
    "llama3-8b_Dense": -0.1973933226125515,
    "llama3-8b_Sparse": -0.23453798739027654,
    "mistral-8b_Dense": -0.37853300181842714,
    "mistral-8b_Sparse": -0.4156776665961522,
    "mistral-8b_Hybrid": -0.45457894809119986,
    "mistral-8b_Rerank": -0.5039271235441216,
    "mistral-8b_open_book": 0.0783606545057931,
    "gemma2-9b_Dense": -0.0053995595009205255,
    "gemma2-9b_Sparse": -0.04254422427864557,
    "gemma2-9b_Hybrid": -0.08144550577369325,
    "gemma2-9b_Rerank": -0.13079368122661494
  },
  "component_abilities": {
    "llms": {
      "llama3-8b": 0.5359752856847901,
      "gemma2-9b": 0.7279690487964211,
      "mistral-8b": 0.3548356064789145
    },
    "retrievers": {
      "Hybrid": -0.8094145545701144,
      "open_book": -0.2764749519731214,
      "Sparse": -0.7705132730750667,
      "Rerank": -0.858762730023036,
      "Dense": -0.7333686082973416
    }
  }
}