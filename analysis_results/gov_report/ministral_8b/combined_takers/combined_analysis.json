{
  "overall_stats": {
    "avg_difficulty": 0.5521742185969074,
    "avg_discrimination": 0.6545683028604292,
    "total_questions": 388
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.473718520692838,
      "avg_discrimination": 0.788515163026841,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.5567132049090118,
      "avg_discrimination": 0.6659425746873411,
      "num_questions": 81
    },
    "3": {
      "avg_difficulty": 0.6089952178442124,
      "avg_discrimination": 0.5613939340782215,
      "num_questions": 53
    },
    "4": {
      "avg_difficulty": 0.6383895404486662,
      "avg_discrimination": 0.5,
      "num_questions": 44
    },
    "5": {
      "avg_difficulty": 0.6287693798093308,
      "avg_discrimination": 0.5,
      "num_questions": 60
    }
  },
  "model_abilities": {
    "llama3-8b_DenseV3": -0.030486692926170206,
    "llama3-8b_SparseV3": -0.1223630442647044,
    "llama3-8b_HybridV3": -0.01722701669439483,
    "llama3-8b_RerankV3": -0.014239298754238128,
    "llama3-8b_DenseV5-5": -0.12052701276815052,
    "llama3-8b_SparseV5-5": -0.09241892065578444,
    "llama3-8b_HybridV5-5": -0.13602924361280902,
    "llama3-8b_RerankV5-5": -0.08348989530531681,
    "llama3-8b_DenseV5-10": -0.12895019371271182,
    "llama3-8b_SparseV5-10": -0.1270632407283689,
    "llama3-8b_HybridV5-10": -0.09500105516211668,
    "llama3-8b_RerankV5-10": -0.06629546291712257,
    "llama3-8b_open_book": 0.2369358736206896,
    "mistral-8b_DenseV3": 0.01611893408623148,
    "mistral-8b_SparseV3": -0.07575741725230273,
    "mistral-8b_HybridV3": 0.029378610318006856,
    "mistral-8b_RerankV3": 0.03236632825816356,
    "mistral-8b_DenseV5-5": -0.07392138575574883,
    "mistral-8b_SparseV5-5": -0.045813293643382755,
    "mistral-8b_HybridV5-5": -0.08942361660040732,
    "mistral-8b_RerankV5-5": -0.03688426829291512,
    "mistral-8b_DenseV5-10": -0.08234456670031014,
    "mistral-8b_SparseV5-10": -0.0804576137159672,
    "mistral-8b_HybridV5-10": -0.048395428149714995,
    "mistral-8b_RerankV5-10": -0.019689835904720884,
    "mistral-8b_open_book": 0.2835415006330913,
    "gemma2_9b": -0.006630996304500966,
    "gemma2_9b_DenseV3": 0.09756915519404886,
    "gemma2_9b_SparseV3": 0.005692803855514662,
    "gemma2_9b_HybridV3": 0.11082883142582424,
    "gemma2_9b_RerankV3": 0.11381654936598094,
    "gemma2_9b_DenseV5-5": 0.007528835352068556,
    "gemma2_9b_SparseV5-5": 0.03563692746443463,
    "gemma2_9b_HybridV5-5": -0.007973395492589939,
    "gemma2_9b_RerankV5-5": 0.04456595281490226,
    "gemma2_9b_DenseV5-10": -0.0008943455924927575,
    "gemma2_9b_SparseV5-10": 0.0009926073918501838,
    "gemma2_9b_HybridV5-10": 0.03305479295810239,
    "gemma2_9b_RerankV5-10": 0.0617603852030965,
    "gemma2_9b_open_book": 0.36499172174090866
  },
  "component_abilities": {
    "llms": {
      "mistral-8b": -0.08808121741231835,
      "llama3-8b": -0.13468684442472004,
      "gemma2_9b": -0.006630996304500966
    },
    "retrievers": {
      "open_book": 0.37162271804540964,
      "DenseV3": 0.10420015149854983,
      "HybridV3": 0.1174598277303252,
      "DenseV5-5": 0.014159831656569522,
      "SparseV5-5": 0.042267923768935595,
      "HybridV5-10": 0.039685789262603355,
      "RerankV5-5": 0.05119694911940323,
      "SparseV5-10": 0.00762360369635115,
      "DenseV5-10": 0.005736650712008209,
      "HybridV5-5": -0.0013423991880889725,
      "SparseV3": 0.012323800160015629,
      "RerankV5-10": 0.06839138150759747,
      "RerankV3": 0.12044754567048191
    }
  }
}