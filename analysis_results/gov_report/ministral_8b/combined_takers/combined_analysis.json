{
  "overall_stats": {
    "avg_difficulty": 0.5351946260289566,
    "avg_discrimination": 1.1021782303212528,
    "total_questions": 388
  },
  "hop_analysis": {
    "1": {
      "avg_difficulty": 0.5160566801715555,
      "avg_discrimination": 1.1003342479413238,
      "avg_feasibility": 0.2647633753155448,
      "num_questions": 150
    },
    "2": {
      "avg_difficulty": 0.49357911301306895,
      "avg_discrimination": 1.1211081416260564,
      "avg_feasibility": 0.25030377369798046,
      "num_questions": 81
    },
    "3": {
      "avg_difficulty": 0.5647623246368931,
      "avg_discrimination": 1.109161258222488,
      "avg_feasibility": 0.2590415217167837,
      "num_questions": 53
    },
    "4": {
      "avg_difficulty": 0.5683104176630261,
      "avg_discrimination": 1.0770284334320628,
      "avg_feasibility": 0.24967721204541646,
      "num_questions": 44
    },
    "5": {
      "avg_difficulty": 0.5888173856085798,
      "avg_discrimination": 1.093507649082238,
      "avg_feasibility": 0.24935734800952994,
      "num_questions": 60
    }
  },
  "model_abilities": {
    "llama3-8b_DenseV3": 1.5778315140109225,
    "llama3-8b_DenseV4": 1.5070653169116204,
    "llama3-8b_SparseV3": 1.4769476236216477,
    "llama3-8b_HybridV3": 1.5511139067533954,
    "llama3-8b_RerankV3": 1.5752683959630185,
    "llama3-8b_DenseV5-5": 1.536370445189589,
    "llama3-8b_SparseV5-5": 1.5188591931583089,
    "llama3-8b_HybridV5-5": 1.552715389490721,
    "llama3-8b_RerankV5-5": 1.647045013461179,
    "llama3-8b_DenseV5-10": 1.564389270838963,
    "llama3-8b_SparseV5-10": 1.4909334668659697,
    "llama3-8b_HybridV5-10": 1.4994977162144068,
    "llama3-8b_RerankV5-10": 1.4615513954125583,
    "llama3-8b_open_book": 1.5387787515584817,
    "mistral-8b_DenseV3": 1.7437448025408955,
    "mistral-8b_SparseV3": 1.642860912151621,
    "mistral-8b_HybridV3": 1.7170271952833684,
    "mistral-8b_RerankV3": 1.7411816844929917,
    "mistral-8b_DenseV5-5": 1.7022837337195622,
    "mistral-8b_SparseV5-5": 1.6847724816882819,
    "mistral-8b_HybridV5-5": 1.718628678020694,
    "mistral-8b_RerankV5-5": 1.8129583019911522,
    "mistral-8b_DenseV5-10": 1.7303025593689363,
    "mistral-8b_SparseV5-10": 1.656846755395943,
    "mistral-8b_HybridV5-10": 1.66541100474438,
    "mistral-8b_RerankV5-10": 1.6274646839425313,
    "mistral-8b_open_book": 1.7046920400884549,
    "gemma2_9b": 0.7507420983844587,
    "gemma2_9b_DenseV3": 1.5716156963215595,
    "gemma2_9b_SparseV3": 1.470731805932285,
    "gemma2_9b_HybridV3": 1.5448980890640325,
    "gemma2_9b_RerankV3": 1.5690525782736557,
    "gemma2_9b_DenseV5-5": 1.5301546275002262,
    "gemma2_9b_SparseV5-5": 1.512643375468946,
    "gemma2_9b_HybridV5-5": 1.546499571801358,
    "gemma2_9b_RerankV5-5": 1.6408291957718162,
    "gemma2_9b_DenseV5-10": 1.5581734531496003,
    "gemma2_9b_SparseV5-10": 1.484717649176607,
    "gemma2_9b_HybridV5-10": 1.493281898525044,
    "gemma2_9b_RerankV5-10": 1.4553355777231953,
    "gemma2_9b_open_book": 1.532562933869119
  },
  "component_abilities": {
    "llms": {
      "gemma2_9b": 0.7507420983844587,
      "llama3-8b": 0.7569579160738216,
      "mistral-8b": 0.9228712046037947
    },
    "retrievers": {
      "RerankV5-5": 0.8900870973873574,
      "DenseV5-5": 0.7794125291157675,
      "open_book": 0.7818208354846601,
      "HybridV5-10": 0.7425398001405853,
      "RerankV5-10": 0.7045934793387367,
      "HybridV3": 0.7941559906795739,
      "DenseV4": 0.7501074008377988,
      "HybridV5-5": 0.7957574734168995,
      "SparseV3": 0.7199897075478262,
      "SparseV5-5": 0.7619012770844872,
      "DenseV5-10": 0.8074313547651416,
      "RerankV3": 0.8183104798891969,
      "DenseV3": 0.8208735979371008,
      "SparseV5-10": 0.7339755507921482
    }
  }
}