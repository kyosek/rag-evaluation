[
  {
    "question": "In McPherson County, the median income for a household is $41,138, and the median income for a family is $48,243. However, the per capita income for the county is $18,921. What is the likely reason for the discrepancy between the median household and family incomes?",
    "choices": [
      "A) The county has a high percentage of single-parent households, which tend to have lower incomes than two-parent households.",
      "B) The county has a significant number of families with incomes above the median, which are not reflected in the median family income.",
      "C) The county has a large number of elderly individuals who are living alone, which are not accounted for in the median household income.",
      "D) The county has a high percentage of families with incomes above the median, which are not reflected in the median household income, and the median family income is skewed by a few very low-income families."
    ],
    "correct_answer": "D)",
    "documentation": [
      "25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 2.99. In the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 95.90 males. For every 100 females age 18 and over, there were 92.90 males. The median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921. About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over. Government\n\nPresidential elections\nMcPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson. Laws\nFollowing amendment to the Kansas Constitution in 1986, the county remained a prohibition, or \"dry\", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement. Education\n\nColleges\n McPherson College in McPherson\n Bethany College in Lindsborg\n Central Christian College in McPherson\n\nUnified school districts\n Smoky Valley USD 400\n McPherson USD 418\n Canton-Galva USD 419\n Moundridge USD 423\n Inman USD 448\n\nSchool district office in neighboring county\n Goessel USD 411\n Little River-Windom USD 444\n\nMuseums\n Birger Sandz\u00e9n Memorial Gallery in Lindsborg\n McCormick-Deering Days Museum in Inman\n McPherson Museum in McPherson\n Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg\n Kansas Motorcycle Museum in Marquette\n\nCommunities\n\nCities\n\n Canton\n Galva\n Inman\n Lindsborg\n Marquette\n McPherson (county seat) \n Moundridge\n Windom\n\nUnincorporated communities\n\u2020 means a Census-Designated Place (CDP) by the United States Census Bureau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the metabolism of vitamin K in rats found that dietary phylloquinone is converted to tissue menaquinone-4 in the liver, but the exact mechanism of this conversion is still unclear. Which of the following statements best describes the current understanding of this process?",
    "choices": [
      "A) The conversion of phylloquinone to menaquinone-4 is dependent on the presence of gut bacteria, which are necessary for the breakdown of phylloquinone into its active form.",
      "B) The conversion of phylloquinone to menaquinone-4 is a direct result of the action of a specific enzyme, which is responsible for the gamma-carboxylation of glutamic acid residues in proteins.",
      "C) The conversion of phylloquinone to menaquinone-4 is a two-step process, involving the initial conversion of phylloquinone to a intermediate compound, which is then converted to menaquinone-4 through a separate enzymatic reaction.",
      "D) The conversion of phylloquinone to menaquinone-4 is a complex process that involves the coordinated action of multiple enzymes and cofactors, including vitamin K-dependent proteins such as gamma-carboxylase and osteocalcin."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\"Vitamin K distribution in rat tissues: dietary phylloquinone is a source of tissue menaquinone-4\". The British Journal of Nutrition. 72 (3): 415\u2013425. doi:10.1079/BJN19940043. PMID 7947656. ^ Will, B. H.; Usui, Y.; Suttie, J. W. (Dec 1992). \"Comparative metabolism and requirement of vitamin K in chicks and rats\". Journal of Nutrition. 122 (12): 2354\u20132360. PMID 1453219. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). \"Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria\". Journal of Nutrition. 128 (2): 220\u2013223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). \"Intestinal flora is not an intermediate in the phylloquinone-menaquinone-4 conversion in the rat\". Biochimica et Biophysica Acta. 1379 (1): 69\u201375. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Al Rajabi, Ala (2011). The Enzymatic Conversion of Phylloquinone to Menaquinone-4 (PhD thesis). Tufts University, Friedman School of Nutrition Science and Policy. ^ Furie, B.; Bouchard, B. A.; Furie, B. C. (Mar 1999). \"Vitamin K-dependent biosynthesis of gamma-carboxyglutamic acid\". Blood. 93 (6): 1798\u20131808. PMID 10068650. ^ Mann, K. G. (Aug 1999). \"Biochemistry and physiology of blood coagulation\". Thrombosis and Haemostasis. 82 (2): 165\u2013174. PMID 10605701. ^ Price, P. A. (1988). \"Role of vitamin-K-dependent proteins in bone metabolism\". Annual Review of Nutrition. 8: 565\u2013583. doi:10.1146/annurev.nu.08.070188.003025. PMID 3060178. ^ Coutu, D. L.; Wu, J. H.; Monette, A.; Rivard, G. E.; Blostein, M. D.; Galipeau, J (Jun 2008). \"Periostin, a member of a novel family of vitamin K-dependent proteins, is expressed by mesenchymal stromal cells\". Journal of Biological Chemistry. 283 (26): 17991\u201318001. doi:10.1074/jbc. M708029200. PMID 18450759. ^ Viegas, C. S.; Simes, D. C.; Laiz\u00e9, V.; Williamson, M. K.; Price, P. A.; Cancela, M. L. (Dec 2008). \"Gla-rich protein (GRP), a new vitamin K-dependent protein identified from sturgeon cartilage and highly conserved in vertebrates\"."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The ACS Younger Chemists Committee (YCC) has been actively involved in promoting diversity and inclusion in the chemistry community. What is a key aspect of YCC's strategic plan, as outlined in its charter, that aligns with the ACS's mission to make the society relevant to younger chemists?",
    "choices": null,
    "correct_answer": "D)",
    "documentation": [
      "Twelve women presented their research at this meeting with funding by the WCC/Eli Lilly Travel Grant Award. WCC members also spent time educating expo attendees on programs offered by the ACS Office of Diversity Programs at its new booth. In Chicago, the Younger Chemists Committee (YCC) welcomed its new committee members with an information session centered on YCC's charter as well as on its strategic plan: to make ACS relevant to younger chemists, to involve younger chemists in all levels of the society, and to integrate younger chemists into the profession. In January, YCC again hosted a Leadership Development Workshop during the ACS Leaders Conference. There were more than 80 applications for the 15 awards, which covered travel and registration for the conference. YCC plans to again fund the travel awards and provide leadership training for young chemists in 2008. YCC also solicited applications and selected a new graduate student representative on the Graduate Education Advisory Board. During the Chicago meeting, YCC programs included \"Starting a Successful Research Program at a Predominantly Undergraduate Institution,\" \"Career Experiences at the Interface of Chemistry & Biology,\" and \"Chemistry Pedagogy 101.\" In addition to these programs, YCC cosponsored five programs with various committees and divisions. YCC continues to reach out to ACS committees and divisions and has initiated liaisonships with 11 technical divisions to encourage technical programming that highlights the contributions of younger chemists. Looking forward to Boston, YCC is planning symposia including \"The Many Faces of Chemistry: International Opportunities for Chemists\"; \"Being a Responsible Chemist: Ethics, Politics & Policy\"; and \"Changing Landscapes of the Bio-Pharma Industry. \"\nThe Committee on Committees (ConC) conducted its annual training session for new national committee chairs at the ACS Leaders Conference in January 2007. ConC's interactive session for committee chairs in Chicago served as an opportune follow-on and a forum for informative interchange among seasoned and new chairs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new material has been synthesized with a unique crystal structure, exhibiting a high degree of disorder. The material's properties are being studied using a machine learning algorithm that infers couplings between atoms based on the reconstruction error. The algorithm is trained on a dataset of samples with different numbers of samples and temperatures. However, the algorithm's performance is sensitive to the choice of temperature and sample size. Which of the following statements best describes the relationship between the reconstruction error and the number of samples at different temperatures?",
    "choices": [
      "A) The reconstruction error decreases as the number of samples increases, regardless of the temperature.",
      "B) The reconstruction error decreases as the number of samples increases, but only at high temperatures.",
      "C) The reconstruction error decreases as the number of samples increases, but only at low temperatures.",
      "D) The reconstruction error decreases as the number of samples increases, and the relationship between temperature and reconstruction error is not significant."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Even though the jump is smeared, the difference between inferred couplings corresponding to the set of non-zero couplings \n     and to the set of zero couplings can be clearly appreciated. Similarly, the plots in the right column of Fig. \\ref{PL-Jor1} show the results obtained for the case with  bimodal disordered couplings, for the same working temperature and number of samples. In particular, note that the algorithm infers half positive and half negative couplings, as expected.\n     \n     \n\\begin{figure}\n\\centering\n\\includegraphics[width=1\\linewidth]{Jor11_2D_l2_errJ_varT_varM}\n\\caption{Reconstruction error $\\mbox{err}_J$, cf. Eq. (\\ref{eq:errj}), plotted as a function of temperature (left) for three values of the number of samples $M$ and  as a function $M$ (right) for three values of temperature in the ordered system, i.e., $J_{ij}=0,1$. \nThe system size is $N=64$.}\n\\label{PL-err-Jor1}\n\\end{figure} In order to analyze the effects of the number of samples and of the temperature regimes, we plot in Fig. \\ref{PL-err-Jor1} the reconstruction error, Eq. (\\ref{err}), as a function of temperature for three different sample sizes $M=64,128$ and $512$. The error is seen to sharply rise al low temperature, incidentally, in the ordered case, for  $T<T_c \\sim 0.893$, which is the Kosterlitz-Thouless transition temperature of the 2XY model\\cite{Olsson92}. However, we can see that if only $M=64$ samples are considered, $\\mbox{err}_J$ remains high independently on the working temperature. In the right plot of Fig. \\ref{PL-err-Jor1},  $\\mbox{err}_J$ is plotted as a function of $M$ for three different working temperatures $T/J=0.4,0.7$ and $1.3$. As we expect, \n $\\mbox{err}_J$  decreases as $M$ increases. This effect was observed also with mean-field inference techniques on the same model\\cite{Tyagi15}. To better understand the performances of the algorithms, in Fig. \\ref{PL-varTP-Jor1} we show several True Positive (TP) curves obtained for various values of $M$ at three different temperatures $T$. As $M$ is large and/or temperature is not too small,  we are able to reconstruct correctly all the couplings present in the system (see bottom plots)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of deliberative democracy, what is the primary function of the \"public sphere\" in facilitating citizen deliberation?",
    "choices": [
      "A) To provide a platform for citizens to express their opinions and engage in discussions with like-minded individuals, thereby reinforcing their existing views and social networks.",
      "B) To serve as a neutral, impartial space where citizens can engage in deliberation with individuals from diverse backgrounds and perspectives, thereby promoting the exchange of ideas and the development of more informed opinions.",
      "C) To facilitate the aggregation of individual opinions and preferences, allowing for the derivation of a collective decision that reflects the will of the majority.",
      "D) To enable citizens to engage in deliberation with experts and specialists, thereby providing them with a more informed and nuanced understanding of complex policy issues."
    ],
    "correct_answer": "B)",
    "documentation": [
      "This is a graduate seminar on contemporary topics in democratic theory. Topics to be covered include: democratic epistemology; deliberative democracy; the meaning of the people; oracular democracy; agonistic democracy; and possibly new theories of republicanism, representation and partisanship. Texts (tentative) Helene Landemore, Democratic Reason\nJeffrey Edward Green, The Eyes of the People\nAmy Gutmann and Dennis Thompson, Why Deliberative Democracy? Alan Keenan, Democracy in Question\nJason Frank, Constituent Moments\nJason Frank, Publius and Political Imagination\nNadia Urbanati, Democracy Disfigured\nRussell Muirhead, Partisanship in a Polarized Age\nBryan Garsten, manuscript\nActive seminar participation; an annotated bibliography or review essay; a research/analytic paper. GOV 310L \u2022 American Government-Honors 37615 \u2022 Fall 2015 Meets TTH 2:00PM-3:30PM BEN 1.106 show description\nTTH 2-3:30/BEN 1.106\nBruce Ackerman,Before the Next Attack: Preserving Civil Liberties in an Age of Terrorism GOV 370L \u2022 Presidency In Constitutl Order 37845 \u2022 Fall 2015 Meets TTH 5:00PM-6:30PM PAR 310 show description\nGOV 370L (37845) TTH 5-6:30 PAR 310\nThe Presidency in the Constitutional Order\nA study of the place of the presidency in the American political order that stresses tension between power and accountability inherent in the office and the system. Topics include: separation of powers, presidential selection, impeachment, relations with Congress and bureaucracy, emergency powers, presidential character, and leadership. This is a very demanding writing flag class. If you are enrolling in this class just in order to satisfy the writing flag, you are in the wrong class. Interest in political theory and willingness to work very hard are necessary for success in this class. Joseph M. Bessette, The Constitutional Presidency\nAndrew Rudalevige, The New Imperial Presidency\nBruce Ackerman, The Rise and Decline of the American Republic\nMichael Nelson, ed., The Presidency in the Political System\nMichael Nelson, ed., The Evolving Presidency\nLouis Fisher, Constitutional Conflicts Between Congress and the President\nActive and prepared class participation\nRegular quizzes on the reading\nFour analytic essays (approximately 1200 words)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on quantum error correction has shown that a probabilistic error cancellation scheme can significantly mitigate errors in second-order Trotter circuits with up to 64 layers. However, the authors also note that the effectiveness of this scheme decreases for larger system sizes. What is the primary reason for this decrease in effectiveness?",
    "choices": [
      "A) The non-unitarity of the denoiser channels leads to a loss of correlation between qubits, making it more difficult to mitigate errors.",
      "B) The measurement of a single qubit can reduce some highly entangled state to a product state, but this effect is not significant enough to impact the overall effectiveness of the scheme.",
      "C) The non-unitarity of the denoiser channels leads to a non-local effect, where the measurement of a single qubit can reduce some highly entangled state to a product state, but this effect is only significant for small system sizes.",
      "D) The non-unitarity of the denoiser channels leads to a non-local effect, where the measurement of a single qubit can reduce some highly entangled state to a product state, and this effect becomes more significant as the system size increases, leading to a decrease in the effectiveness of the scheme."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In the Supplementary Material we show the spectra for a p = 0.036 denoiser, where we observe a clustering of eigenvalues reminiscent of Refs. . There we also investigate the channel entropy of the various supercircuits . Conclusion. -We have introduced a probabilistic error cancellation scheme, where a classically determined denoiser mitigates the accumulated noise of a (generally non-Clifford) local noise channel. The required number of mitigation gates, i.e. the dimensionality of the corresponding quasiprobability distribution, is tunable and the parameterization of the corresponding channels provides control over the sign problem that is inherent to probabilistic error cancellation. We have shown that a denoiser with one layer can already significantly mitigate errors for second-order Trotter circuits with up to 64 layers. This effectiveness of low-depth compressed circuits for denoising, in contrast with the noiseless time evolution operator compression from , can be understood from the non-unitarity of the denoiser channels. In particu-lar, measurements can have non-local effects, since the measurement of a single qubit can reduce some highly entangled state (e.g. a GHZ state) to a product state, whereas in unitary circuits the spreading of correlations forms a light-cone. To optimize a denoiser with convenience at L > 8, the optimization can be formulated in terms of matrix product operators or channels , which is convenient because the circuit calculations leading to the normalized distance and its gradient are easily formulated in terms of tensor contractions and singular value decompositions . This provides one route to a practical denoiser, which is relevant because the targeted noiseless circuit and the accompanying noisy variant in (4) need to be simulated classically, confining the optimization procedure to limited system sizes with an exact treatment or limited entanglement with tensor networks. Nonetheless, we can use e.g. matrix product operators to calculate (4) for some relatively small t, such that the noiseless and denoised supercircuits in (4) have relatively small entanglement, and then stack the final denoised supercircuit on a quantum processor to generate classically intractable states."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the partition function Z, what is the relationship between the effective input-input coupling matrix U and the transmission matrix J?",
    "choices": [
      "A) The effective input-input coupling matrix U is equal to the inverse of the transmission matrix J.",
      "B) The effective input-input coupling matrix U is equal to the product of the transmission matrix J and the attenuation factor c.",
      "C) The effective input-input coupling matrix U is equal to the sum of the transmission matrix J and the identity matrix I.",
      "D) The effective input-input coupling matrix U is equal to the difference between the transmission matrix J and the attenuation factor c times the identity matrix I."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(\\ref{eq:deltas}). Moreover, we move to consider the ensemble of all possible solutions of Eq. (\\ref{eq:transm}) at given $\\mathbb{T}$, looking at  all configurations of input fields. We, thus, define the function:\n \n   \\begin{eqnarray}\n  Z &\\equiv &\\int_{{\\cal S}_{\\rm in}} \\prod_{j=1}^{N_I}  dE^{\\rm in}_j \\int_{{\\cal S}_{\\rm out}}\\prod_{k=1}^{N_O} dE^{\\rm out}_k \n  \\label{def:Z}\n\\\\\n    \\times\n  &&\\prod_{k=1}^{N_O}\n   \\frac{1}{\\sqrt{2\\pi \\Delta^2}}  \\exp\\left\\{-\\frac{1}{2 \\Delta^2}\\left|\n  E^{\\rm out}_k -\\sum_{j=1}^{N_I}  t_{kj} E^{\\rm in}_j\\right|^2\n\\right\\} \n\\nonumber\n \\end{eqnarray} We stress that the integral of Eq. \\eqref{def:Z} is not exactly a Gaussian integral. Indeed, starting from Eq. \\eqref{eq:deltas}, two constraints on the electromagnetic field intensities must be taken into account. The space of solutions is  delimited by the total power ${\\cal P}$ received by system, i.e., \n  ${\\cal S}_{\\rm in}: \\{E^{\\rm in} |\\sum_k I^{\\rm in}_k = \\mathcal{P}\\}$, also implying  a constraint on the total amount of energy that is transmitted through the medium, i. e., \n  ${\\cal S}_{\\rm out}:\\{E^{\\rm out} |\\sum_k I^{\\rm out}_k=c\\mathcal{P}\\}$, where the attenuation factor  $c<1$ accounts for total losses. As we will see more in details in the following, being interested in inferring the transmission matrix through the PLM, we can omit to explicitly include these terms in Eq. \\eqref{eq:H_J} since they do not depend on $\\mathbb{T}$ not adding any information on the gradients with respect to the elements of $\\mathbb{T}$.\n  \n Taking the same number of incoming and outcoming channels, $N_I=N_O=N/2$, and  ordering the input fields in the first $N/2$ mode indices and the output fields in the last $N/2$ indices, we can drop the ``in'' and ``out'' superscripts and formally write $Z$  as a partition function\n    \\begin{eqnarray}\n        \\label{eq:z}\n && Z =\\int_{\\mathcal S} \\prod_{j=1}^{N} dE_j \\left(   \\frac{1}{\\sqrt{2\\pi \\Delta^2}} \\right)^{N/2} \n \\hspace*{-.4cm} \\exp\\left\\{\n  -\\frac{ {\\cal H} [\\{E\\};\\mathbb{T}] }{2\\Delta^2}\n  \\right\\}\n  \\\\\n&&{\\cal H} [\\{E\\};\\mathbb{T}] =\n-  \\sum_{k=1}^{N/2}\\sum_{j=N/2+1}^{N} \\left[E^*_j t_{jk} E_k + E_j t^*_{kj} E_k^* \n\\right]\n \\nonumber\n\\\\\n&&\\qquad\\qquad \\qquad + \\sum_{j=N/2+1}^{N} |E_j|^2+ \\sum_{k,l}^{1,N/2}E_k\nU_{kl} E_l^*\n \\nonumber\n \\\\\n \\label{eq:H_J}\n &&\\hspace*{1.88cm } = - \\sum_{nm}^{1,N} E_n J_{nm} E_m^*\n \\end{eqnarray}\n where ${\\cal H}$ is a real-valued function by construction, we have introduced the effective input-input coupling matrix\n\\begin{equation}\nU_{kl} \\equiv \\sum_{j=N/2+1}^{N}t^*_{lj} t_{jk} \n \\label{def:U}\n \\end{equation}\n and the whole interaction matrix reads (here $\\mathbb{T} \\equiv \\{ t_{jk} \\}$)\n \\begin{equation}\n \\label{def:J}\n \\mathbb J\\equiv \\left(\\begin{array}{ccc|ccc}\n \\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}&-\\mathbb{U} \\phantom{()}&\\phantom{()}&\\phantom{()}&{\\mathbb{T}}&\\phantom{()}\\\\\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\hline\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}& \\mathbb   T^\\dagger&\\phantom{()}&\\phantom{()}& - \\mathbb{I} &\\phantom{()}\\\\\n\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}\\\\\n \\end{array}\\right)\n \\end{equation}\n \n Determining the electromagnetic complex amplitude configurations that minimize the {\\em cost function} ${\\cal H}$, Eq.  (\\ref{eq:H_J}),  means to maximize the overall distribution peaked around the solutions of the transmission Eqs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A manufacturer of aircraft components is considering a new production process that involves bending and sloping the fuselage sides to form a conical section. This process is similar to one used in the marine trades, where a conical shape is cut with a plane not perpendicular to its axis, resulting in an elliptical shape. However, the manufacturer is concerned that the process may not be suitable for their materials, which have a high degree of in-plane strain. What is the primary reason for the manufacturer's concern?",
    "choices": [
      "A) The process may not be able to accommodate the curvature of the fuselage sides, which would result in a cylindrical section.",
      "B) The process relies on the fuselage sides being able to be bent and sloped without any deformation, which is not possible with the manufacturer's materials.",
      "C) The process may not be able to produce a flat surface, which is required for the assembly of the aircraft components.",
      "D) The process may not be able to accommodate the compound curvature of the fuselage sides, which would result in an elliptical shape."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel \"undevelopable\" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition \"developable\". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a \"compounded\" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home) , the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock. This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new development project is proposed for a land that is currently used for agricultural purposes. The Union Government intends to carry out the development, which will include the construction of a new road and the construction of a new office building. The Director of the relevant department has raised objections to the proposal, citing concerns that the development will not be in conformity with the provisions of the development plan. However, the Director has not specified which provisions are being violated. The officer-in-charge of the development project has been informed of the Director's objections, but has not yet made any modifications to the proposal. What should the officer-in-charge do next?",
    "choices": [
      "A) Submit the proposal for development to the State Government for approval, without making any modifications to the proposal.",
      "B) Make necessary modifications to the proposal to meet the Director's objections, and then submit the revised proposal to the State Government for approval.",
      "C) Submit the proposal for development to the State Government for approval, without making any modifications to the proposal, and then appeal to the State Government if the proposal is rejected.",
      "D) Direct the Director to provide more specific information about the provisions of the development plan that are being violated, before making any further decisions about the proposal."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(f) for use for any purpose incidental to the use of building for human habitation, or any other building or land attached to such buildings;\n(g) for the construction of a road intended to give access to land solely for agricultural purposes\n28. Development undertaken on behalf of Union or State Government. - (1) When the Union Government or the State Government intends to carry out development of any land for the purpose of its departments or offices or authorities, the officer-in-charge thereof shall inform in writing to the Director the intention of the Government to do so, giving full particulars thereof, accompanied by such documents and plans as may be prescribed at least thirty days before undertaking such development. (2) Where the Director raises any objection to the proposed development on the ground that the development is not in conformity with the provisions of the development plan, the officer shall,-\n(i) make necessary modification in the proposals for development to meet the objections raised by the Director, or\n(ii) submit the proposal for development together with the objections raised by the Director to the State Government for decision: Provided that where no modification is proposed by the Director within thirty days of the receipt of the proposed plan by the Government, the plan will be presumed to have been approved.\n(3) The State Government, on receipt of the proposals for development together with the objections of the Director shall, approve the proposals with or without modifications or direct the officer to make such modifications in the proposals as it considers necessary in the circumstances. (4) The decision of the State Government under sub-section (3) shall be final and binding. 29. Development by local authority or by any authority constituted under this Act. - Where a local authority or any authority specially constituted under this Act intends to carry out development on any land for the purpose of that authority, the procedure applicable to the Union or State Government, under section 28 shall, mutatis' mutandis, apply in respect of such authority."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study investigating the role of astrocytes in Downs syndrome neuropathology found that astrocytes from individuals with DS exhibit impaired mitochondrial function, leading to metabolic alterations in protein processing and secretion. However, the same study also found that the number of dendritic spines in DS astrocytes is reduced, and that this reduction is associated with a decrease in the expression of a specific protein involved in actin cytoskeleton regulation. Which of the following conclusions can be drawn from this study?",
    "choices": [
      "A) The impaired mitochondrial function in DS astrocytes is the primary cause of the reduced number of dendritic spines in DS neuropathology.",
      "B) The reduced number of dendritic spines in DS astrocytes is a result of the impaired mitochondrial function, and that this impairment leads to a decrease in the expression of the specific protein involved in actin cytoskeleton regulation.",
      "C) The study suggests that the reduced number of dendritic spines in DS astrocytes is a result of the impaired mitochondrial function, but that this impairment does not affect the expression of the specific protein involved in actin cytoskeleton regulation.",
      "D) The impaired mitochondrial function in DS astrocytes leads to a decrease in the expression of the specific protein involved in actin cytoskeleton regulation, which in turn contributes to the reduced number of dendritic spines in DS neuropathology."
    ],
    "correct_answer": "D)",
    "documentation": [
      "JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols\nA role for thrombospondin-1 deficits in astrocyte-mediated spine and synaptic pathology in Downs syndrome. Octavio Garcia, Maria Torres, Pablo Helguera, Pinar Coskun, Jorge Busciglio. PUBLISHED: 07-02-2010\tDowns syndrome (DS) is the most common genetic cause of mental retardation. Reduced number and aberrant architecture of dendritic spines are common features of DS neuropathology. However, the mechanisms involved in DS spine alterations are not known. In addition to a relevant role in synapse formation and maintenance, astrocytes can regulate spine dynamics by releasing soluble factors or by physical contact with neurons. We have previously shown impaired mitochondrial function in DS astrocytes leading to metabolic alterations in protein processing and secretion. In this study, we investigated whether deficits in astrocyte function contribute to DS spine pathology. Analysis of Dendritic Spine Morphology in Cultured CNS Neurons Authors: Deepak P. Srivastava, Kevin M. Woolfrey, Peter Penzes. Published: 07-13-2011 JoVE Neuroscience\nDendritic spines are the sites of the majority of excitatory connections within the brain, and form the post-synaptic compartment of synapses. These structures are rich in actin and have been shown to be highly dynamic. In response to classical Hebbian plasticity as well as neuromodulatory signals, dendritic spines can change shape and number, which is thought to be critical for the refinement of neural circuits and the processing and storage of information within the brain. Within dendritic spines, a complex network of proteins link extracellular signals with the actin cyctoskeleton allowing for control of dendritic spine morphology and number. Neuropathological studies have demonstrated that a number of disease states, ranging from schizophrenia to autism spectrum disorders, display abnormal dendritic spine morphology or numbers. Moreover, recent genetic studies have identified mutations in numerous genes that encode synaptic proteins, leading to suggestions that these proteins may contribute to aberrant spine plasticity that, in part, underlie the pathophysiology of these disorders."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was a key factor in the National Party's poor performance in the 2002 election, according to English's own statement?",
    "choices": [
      "A) The party's failure to implement his own \"Rogernomics\" policies, which he believed would have led to economic growth and job creation.",
      "B) The fact that he had not been able to capitalize on the public's desire for a more pragmatic approach to politics, as exemplified by his predecessors Roger Douglas and Ruth Richardson.",
      "C) The party's inability to effectively challenge the Labour Party's policies, particularly in the areas of healthcare and education.",
      "D) The fact that he had not been able to build a strong coalition with other parties, which he believed was essential for a successful opposition."
    ],
    "correct_answer": "D)",
    "documentation": [
      "English had been a supporter of Bolger as leader, but Shipley reappointed him Minister of Health in her new cabinet. English was promoted to Minister of Finance in a reshuffle in January 1999, a position which was at the time subordinate to the Treasurer, Bill Birch. After a few months, the pair switched positions as part of Birch's transition to retirement, with English assuming the senior portfolio. In early interviews, he emphasised his wish to be seen as a pragmatist rather than an ideologue, and said that the initiatives of some of his predecessors (Roger Douglas's \"Rogernomics\" and Ruth Richardson's \"Ruthanasia\") had focused on \"fruitless, theoretical debates\" when \"people just want to see problems solved\". Opposition (1999\u20132008)\n\nAfter the National Party lost the 1999 election to Helen Clark's Labour Party, English continued on in the shadow cabinet as National's spokesperson for finance. He was elected deputy leader of the party in February 2001, following the resignation of Wyatt Creech, with Gerry Brownlee being his unsuccessful opponent. Leader of the Opposition\nIn October 2001, after months of speculation, Jenny Shipley resigned as leader of the National Party after being told she no longer had the support of the party caucus. English was elected as her replacement unopposed (with Roger Sowry as his deputy), and consequently became Leader of the Opposition. However, he did not openly organise against Shipley, and according to The Southland Times \"there was almost an element of 'aw, shucks, I'll do it then' about Mr English's ascension\". Aged 39 when he was elected, English became the second-youngest leader in the National Party's history, after Jim McLay (who was 38 when elected in 1984). He also became only the third Southlander to lead a major New Zealand political party, after Joseph Ward and Adam Hamilton. However, English failed to improve the party's performance. In the 2002 election, National suffered its worst electoral defeat ever, gaining barely more than twenty percent of the vote."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary reason for the delay in meeting with Gadhafi, according to Weldon's explanation?",
    "choices": [
      "A) The Libyan delegation is waiting for further instructions from their government before proceeding with the meeting.",
      "B) The security concerns surrounding Gadhafi's whereabouts are causing a significant delay in the meeting.",
      "C) The US delegation is waiting for a response from the Libyan government regarding their proposals for a peaceful resolution.",
      "D) The Libyan delegation is holding up the meeting due to disagreements over the terms of the proposed peace agreement."
    ],
    "correct_answer": "D)",
    "documentation": [
      "And I said it's worth me coming over to support the administration and to try to let the leader know face to face that this is facing -- it's very grave timing in the situation and they have to have some movement fairly quickly or they're not going to be happy with the -- with the alternatives. BLITZER: What's taking so long? Why haven't you been able to meet with Gadhafi yet? WELDON: Well, it -- that's not unusual. I mean all three of the delegation trips that I led here in 2003 and 2004 -- or, actually, 2004 and 2005 -- they always make you wait until 30 minutes before the meeting and then you go. And some of those meetings were at 10:00 at night, some were at 5:00 in the afternoon. As you know from the excellent reporting being done by your folks here, there's a lot of security concerns, and they are very concerned where Gadhafi is at any given moment. That's one of the issues, but we have been making ourselves available. We have been doing a lot of back-channel meetings with friends and associates that I have here, and we have met with the chief of staff and one of the sons, and today with the prime minister, a very lengthy meeting for two hours. So, we're going to give them until tomorrow. We're not going to stay beyond that. And we have given them some suggestions, and we expect a response by midday tomorrow. And if we don't, we will done exit conversation with your people and let you know our feelings.\nBLITZER: What's the major headline that you got out of these meetings with other leaders? I take it you met with Saif Al-Islam Gadhafi, one of the sons of Moammar Gadhafi. What are they saying to you? WELDON: Well, we actually didn't meet with Saif. I have met with Saif probably 10 times over the past seven years, both in America and here in Libya. I have not yet met with Saif. I have offered, if he is available. I have met with Saadi. And the general thrust is obviously that they want peace and they want to find a way out of this. But as I have explained to them, there's certain things that have to be done according to our president and our secretary of state, who I'm here to support."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new update has been released, and the developers have made several changes to the game's balance. One of the changes is to the frequency and duration of Juggernaut's \"Unstoppable\" ability. However, this change has also affected the balance of another Champion's ability. Which of the following statements best describes the net effect of this change on the game's balance?",
    "choices": [
      "A) The change to Juggernaut's \"Unstoppable\" ability has increased the overall power of Special Attacks, making them more visible in fights.",
      "B) The change to Juggernaut's \"Unstoppable\" ability has reduced the overall power of Special Attacks, making them less visible in fights.",
      "C) The change to Juggernaut's \"Unstoppable\" ability has had no effect on the balance of Special Attacks, as they were already being used too infrequently.",
      "D) The change to Juggernaut's \"Unstoppable\" ability has increased the power of Special Attacks, but only against opponents with high Critical Hit rates, making them more visible in fights."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We\u2019ve compared our notes with the feedback you\u2019ve been sending us and are making some balance changes to them. Thanks for your feedback!\nSlightly reduced the frequency and duration of Juggernaut\u2019s \u201cUnstoppable\u201d ability. \u2022 He was indeed a bit too...unstoppable. We\u2019ve toned down the frequency this ability triggers, as well as reduced the duration it\u2019s active for when it does trigger. We feel Juggernaut is still a powerful Champion despite these revisions. Take care! Slightly reduced the starting values of Wolverine's \u201cCellular Regeneration\u201d. \u2022 We found that Cellular Regeneration was too strong at lower levels where fewer counters to Regeneration exist. Re-scaled Gamora's \u201cAssassination\u201d to start higher but scale slower. \u2022 At lower levels, Special Attacks were used too infrequently, giving this powerful ability little visibility. We\u2019ve adjusted the scaling to better match Special Attack usage at all levels. Increased the frequency that Black Bolt\u2019s \u201cProvocation\u201d triggers. \u2022 Due to the varying Critical Hit rates across all Champions, in some cases Provocation would trigger rarely or not at all within a fight. We\u2019ve increased the frequency to ensure you\u2019ll see it every match \u2013 but especially so against opponents with high Critical Hit rates. We\u2019ll continue to follow the effect of these new abilities on gameplay. Please keep your feedback coming! Hey everyone! We have been hard at work on improving the game and have prepared a big update inspired in part by your great community feedback. Please keep letting us know what you think! \u2022 Fixed many Dash, Medium, Heavy and Special Attacks missing or failing to execute. \u2022 Added Alliances and a new Alliance Crystal. \u2022 Rocket Raccoon and Unstoppable Colossus join The Contest. \u2022 Temporary Boosts to Attack, Health, and XP are now available from the Alliance Crystal. \u2022 Rewards for completing and exploring Chapters and Acts. Earn a guaranteed 3-Star hero crystal for each fully explored Act! This is retroactive, just complete any quest to claim them. \u2022 A new Fight Menu combines The Arenas, Story Quests and Event Quest menus."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher studying the evolution of life on Earth proposes a new theory that the origin of self-replication in prebiotic evolution was facilitated by the presence of a catalyst that accelerated chemical reactions, allowing for the regeneration of components and the emergence of complex life forms. However, this theory is challenged by the fact that some organisms, such as paramecia, appear to lack a clear sense of self and are unable to maintain themselves through self-replication. Which of the following statements best summarizes the implications of this challenge for the researcher's theory?",
    "choices": [
      "A) The presence of a catalyst is not necessary for the emergence of self-replication, as some organisms can still replicate and maintain themselves without it.",
      "B) The lack of self-awareness in paramecia suggests that self-replication is not a necessary feature of life, and that other mechanisms can facilitate the emergence of complex life forms.",
      "C) The fact that paramecia can still replicate and maintain themselves without a catalyst suggests that the origin of self-replication in prebiotic evolution was not dependent on the presence of a catalyst.",
      "D) The challenge posed by the lack of self-awareness in paramecia highlights the importance of considering the role of consciousness in the emergence of self-replication, and suggests that a catalyst may not be necessary for the origin of self-replication if it is accompanied by a clear sense of self."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Metabolism first theories of abiogenesis and the study of prebiotic evolution strongly suggest that key features of the life process are located way before the development of self-replicating molecules such as RNA and DNA. On the other hand, perhaps this idea of self-replication can be extended to processes in prebiotic evolution in which there is a catalysis of chemical reactions which replenish the chemical components. After all, self-maintenance is a definitive feature of the life process and would suggest that any life process must include the regeneration of its components. DragonFly \u00bb April 23rd, 2018, 1:51 pm wrote: The epistemic cut, the subject/object cut, the mind/matter cut, all are rooted to that original cut at the origin of life. The gap between subjective feeling and objective neural firings didn\u2019t come about with the appearance of brains. It was already there when the first cell started living. Two complementary modes of behavior, two levels of description are inherent in life itself, were present at the origin of life, have been conserved by evolution, and continue to be necessary for differentiating subjective experience from the event itself. That is a mind-boggling idea.\u201d This would only work if you can make a logical connection with this definitive feature of life in a process of self maintenance. I have already suggested a connection between this and consciousness by pointing out that self maintenance requires some kind of awareness of self, both as it is and as it \"should be.\" Without some sort of \"should be\" in some form there can be no self-maintenance. It should be noted that there are numerous quantitative features to this, such as the clarity with which this goal of self as it \"should be\" is represented, the determination/flexibility with which it is adhered to (or in other words the range of circumstances which can be handled in holding to this goal).\nby TheVat on April 24th, 2018, 1:52 pm\nIt seems likely a paramecium does no representing to a self, and is pretty much a cellular machine lacking sentience."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on multilingual language identification models used a version of XLM-RoBERTa finetuned on a language identification dataset to achieve near-perfect test accuracy of 99.6%. However, the authors noted that the dataset was imbalanced, with the class PT-BR having the most number of samples and the class EN having the least number of samples. Which of the following statements best describes the implications of this imbalance on the performance of the language identification model?",
    "choices": [
      "A) The imbalance ratio of 1:8 between PT-BR and EN classes would lead to overfitting of the model to the majority class.",
      "B) The imbalance ratio would result in the model being biased towards the minority class, EN.",
      "C) The imbalance ratio would have no significant impact on the model's performance, as the majority of samples are from PT-BR.",
      "D) The imbalance ratio would lead to underfitting of the model, as the minority class, EN, would not have enough data to train on."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Lastly, language specific models like Spanish BERT (la Rosa y Eduardo G. Ponferrada y Manu Romero y Paulo Villegas y Pablo Gonz\u00e1lez de Prado Salas y Mar\u00eda Grandury, 2022) and Portuguese BERT are available as well. Our winning solution makes use of these large language models trained on specific languages. Language Identification Models\n\nMany multilingual language identification models have been developed in order to classify the language of the input sentence beforehand. Even though the initial works used n-gram models and generative mixture models or even conditional random fields and other classical machine learning methods like naive bayes , modern methods have shifted to the use of deep learning for language identification . Recent works have mainly focused on deep learning based language identification, where handling codemixed data is a big challenge in the domain. For our experiments, we use a version of XLM-RoBERTa finetuned on a language identification dataset 2 . This model has near-perfect test accuracy of 99.6%. Dialect Classification\n\nDialect classification has been previously solved using statistical methods like Gaussian Mixture Models and Frame Selection Decoding or Support Vector Machines (SVM) . It has been explored relatively sparsely, mostly in the case for local languages . Deep learning approaches have been explored in previous editions of the VarDial workshop shared tasks and otherwise . Dialect classification was also explored previously as a part of other shared tasks . We want to stress that given the multilingual nature of the dataset, using the present methods directly was not an option. In our work, although we take inspiration from the previous works, we propose a novel system that surpasses the performance of the previous systems by a large margin. Data\n\nThe dataset  We observed that the class PT-BR had the most number of samples (2,724) and the class EN had the least number of samples (349), and thus the imbalance ratio was almost 1:8. We have illustrated the data distribution in Figure ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study investigated the effects of vitamin K2 on the coagulation cascade. The researchers found that vitamin K2 inhibited the activity of thrombin, but not fibrinogen. However, they also observed that the inhibition of thrombin was dependent on the presence of a specific cofactor. Which of the following statements best describes the relationship between vitamin K2, thrombin, and the cofactor?",
    "choices": [
      "A) Vitamin K2 inhibits thrombin activity only in the presence of a specific cofactor, which is also required for the activation of prothrombin.",
      "B) Vitamin K2 inhibits thrombin activity regardless of the presence of a specific cofactor, and this inhibition is sufficient to prevent the formation of blood clots.",
      "C) Vitamin K2 inhibits thrombin activity only in the absence of a specific cofactor, which is required for the activation of prothrombin.",
      "D) Vitamin K2 inhibits thrombin activity by binding to a specific site on the enzyme, which is distinct from the site where the cofactor binds."
    ],
    "correct_answer": "D)",
    "documentation": [
      "doi:10.3181/00379727-37-9668P. ^ Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). \"Vitamin K dependent modifications of glutamic acid residues in prothrombin\". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730\u20132733. doi:10.1073/pnas.71.7.2730. PMC 388542. PMID 4528109. ^ Nelsestuen, G. L.; Zytkovicz, T. H.; Howard, J. B. (Oct 1974). \"The mode of action of vitamin K. Identification of gamma-carboxyglutamic acid as a component of prothrombin\" (PDF). Journal of Biological Chemistry. 249 (19): 6347\u20136350. PMID 4214105. ^ Magnusson, S.; Sottrup-Jensen, L.; Petersen, T. E.; Morris, H. R.; Dell, A. (Aug 1974). \"Primary structure of the vitamin K-dependent part of prothrombin\". FEBS Letters. 44 (2): 189\u2013193. doi:10.1016/0014-5793(74)80723-4. PMID 4472513. Bibliography[edit]\nRh\u00e9aume-Bleue, Kate (2012). Vitamin K2 and the Calcium Paradox. John Wiley & Sons, Canada. ISBN 1-118-06572-7. External links[edit]\n\"Vitamin K: Another Reason to Eat Your Greens\". v\nTPP / ThDP (B1)\nFMN, FAD (B2)\nNAD+, NADH, NADP+, NADPH (B3)\nCoenzyme A (B5)\nPLP / P5P (B6)\nTHFA / H4FA, DHFA / H2FA, MTHF (B9)\nAdoCbl, MeCbl (B12)\nPhylloquinone (K1), Menaquinone (K2)\nnon-vitamins\nCoenzyme B\nHeme / Haem (A, B, C, O)\nMolybdopterin/Molybdenum cofactor\nTHMPT / H4MPT\nFe2+, Fe3+\nvitamins: see vitamins\nAntihemorrhagics (B02)\n(coagulation) Phytomenadione (K1)\nMenadione (K3)\nintrinsic: IX/Nonacog alfa\nVIII/Moroctocog alfa/Turoctocog alfa\nextrinsic: VII/Eptacog alfa\ncommon: X\nII/Thrombin\nI/Fibrinogen\nXIII/Catridecacog\ncombinations: Prothrombin complex concentrate (II, VII, IX, X, protein C and S)\nCarbazochrome\nthrombopoietin receptor agonist (Romiplostim\nEltrombopag) Tetragalacturonic acid hydroxymethylester\nEpinephrine/Adrenalone\namino acids (Aminocaproic acid\nAminomethylbenzoic acid)\nserpins (Aprotinin\nAlfa1 antitrypsin\nCamostat)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary consequence of the proposed reorganization of the United States Armed Forces by Secretary of Defense Louis A. Johnson, as revealed by the Revolt of the Admirals?",
    "choices": [
      "A) The Navy's budget was increased to reflect the growing importance of the service, leading to a surge in recruitment and expansion of naval operations.",
      "B) The merger of the Marine Corps into the Army resulted in a significant reduction in the number of military personnel, allowing for a more streamlined and efficient defense force.",
      "C) The concentration of power in the civilian executive branch led to a decrease in the autonomy of the individual services, resulting in a more coordinated and effective military response to external threats.",
      "D) The proposal to merge the Marine Corps into the Army and reduce the Navy to a convoy-escort force was met with widespread criticism from senior commanders, ultimately leading to the resignation of Secretary Johnson and the Secretary of the Navy, Francis P. Matthews, and the forced retirement of Admiral William H. P. Blandy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Goodwin returned with San Jacinto to the United States in mid-September 1945 and he was detached in January 1946. He subsequently served in the office of the Chief of Naval Operations until May that year, when he entered the instruction at National War College. Goodwin graduated in June 1947 and served on Secretary's committee for Research on Reorganization. Upon promotion to Rear admiral on April 1, 1949, Goodwin was appointed Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet under Admiral William H. P. Blandy. Revolt of the Admirals\n\nIn April 1949, the budget's cuts and proposed reorganization of the United States Armed Forces by the Secretary of Defense Louis A. Johnson launched the wave of discontent between senior commanders in the United States Navy. Johnson proposed the merging of the Marine Corps into the Army, and reduce the Navy to a convoy-escort force. Goodwin's superior officer, Admiral Blandy was call to testify before the House Committee on Armed Services and his harsh statements for the defense of the Navy, costed him his career. Goodwin shared his views and openly criticized Secretary Johnson for having power concentrated in a single civilian executive, who is an appointee of the Government and not an elected representative of the people. He also criticized aspects of defense unification which permitted the Joint Chiefs of Staff to vote on arms policies of individual services, and thus \"rob\" the branches of autonomy. The outbreak of the Korean War in summer 1950 proved the proposal of Secretary Johnson as incorrect and he resigned in September that year. Also Secretary of the Navy, Francis P. Matthews resigned one month earlier. Later service\n\nDue to the Revolts of the admirals, Blandy was forced to retire in February 1950 and Goodwin was ordered to Newport, Rhode Island for temporary duty as Chief of Staff and Aide to the President of the Naval War College under Vice admiral Donald B. Beary in April 1950. Goodwin was detached from that assignment two months and appointed member of the General Board of the Navy."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A regional plan is being prepared for a new development project in a rural area. The Director has obtained maps and survey reports from the local authority, but the project's environmental impact assessment is still pending. What is the next step in the preparation of the regional plan?",
    "choices": [
      "A) The Director should allocate land for the development project without considering the environmental impact assessment, as it is not yet complete.",
      "B) The Director should publish the draft regional plan and invite objections and suggestions from the public, as the project's environmental impact assessment is not yet available.",
      "C) The Director should delay the publication of the draft regional plan until the environmental impact assessment is complete, as it is a crucial factor in determining the project's feasibility.",
      "D) The Director should prepare a revised draft regional plan that takes into account the environmental impact assessment, and then publish it for public inspection and comment."
    ],
    "correct_answer": "D)",
    "documentation": [
      "6. Survey. - (1) The Director shall, with a view to prepare the existing land use map, and other maps as are necessary for the purpose of regional plan,-\n(a) carry out such surveys as may be necessary;\n(b) obtain from any department of Government and any local authority such maps, survey reports and land records as may be necessary for the purpose. (2) It shall be the duty of every Government department and local authority to furnish, as soon as may be possible, maps, reports and record, as may be required by the Director. 7. Contents of regional plan. - The regional plan shall indicate the manner in which land in the region should be used, the phasing of development, the net work of communications and transport, the proposals for conservation and development of natural resources, and in particular-\n(a) allocation of land to such purposes as residential, industrial; agricultural or as forests or for mineral exploitation;\n(b) reservation of open spaces for recreational purposes, gardens, tree belts, and animal sanctuaries;\n(c) access or development of transport and communication facilities such as roads, railways, water ways, and the allocation and development of airports;\n(d) requirements and suggestions for development of public utilities such as water supply, drainage and electricity;\n(e) allocation of areas to be developed as \"Special Areas\ufffd wherein new towns, townships, large industrial estates or any other type of large development projects may be established;\n(f) landscaping and the preservation of areas in their natural state,\n(g) measures relating to the prevention of erosion, including rejuvenation of forest areas;\n(h) proposals relating to irrigation, water supply or flood control works. 8. Preparation of regional plan. - (1) After preparation of the existing land use map, the Director shall cause to be prepared a draft regional plan and publish it by making a copy thereof available for inspection and publishing a notice in such form and manner as may be prescribed inviting objections and suggestions from any person with respect to the draft plan before such date as may be specified in the notice, such date not being earlier than sixty days from the publication of the notice."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When considering the balance between Captain America WW2 and his non-WW2 counterpart, what is the primary reason for the recent changes made to their abilities?",
    "choices": [
      "A) To make Captain America WW2 more vulnerable to Armor Breaks, in order to create a more balanced matchup against his non-WW2 counterpart.",
      "B) To increase the range of Captain America WW2's signature ability, making it more comparable to Iron Man's.",
      "C) To address the issue of Daredevil's inability to trigger Armor Breaks from Heavy Attacks, and to make his signature ability more comparable to Iron Man's.",
      "D) To create a more nuanced balance between the two characters, allowing Captain America WW2 to have unique uses and applications in combat, while still maintaining a range of options for his non-WW2 counterpart."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This issue has been fixed. Captain America WW2 has started to become outpaced by his non-WW2 counterpart and while we want the two to feel different and each have their own specific uses, we also want to ensure they are kept within range of each other in terms of balance. To accomplish this, we\u2019ve given WW2 Cap the ability to Stun with his Special 1 and Special 3 attacks, but kept his Bleed on Special 2 the same, giving him options during combat against non-bleeding champions. A bug that prevented Daredevil from triggering Armor Breaks from Heavy Attacks has been fixed and is now working as intended. Against non-bleeding champions: Critical Hits have a chance to Armor Break on Special Attacks. Increase range of Signature to 25% from 20%. Many players found Elektra\u2019s signature ability lacked enough opportunities to use it. To remedy this, we\u2019ve increased the range from 20% to 25%. Additionally, to help make Elektra unique from other skill champions, we\u2019ve given her the ability to deal with naturally Bleed Immune champions. Note: This Armor Break only applies to champions naturally immune to bleed, such as Colossus and Ultron, and not to champions granted Bleed Immunity from Local or Link Nodes. Guillotine\u2019s Bleed effect used to have a chance to activate from any given attack, meaning that it had to be kept quite weak to compensate for the frequency of triggers. We\u2019ve made the switch to have her Bleed behave closer to existing champions, and in doing so have boosted the strength of the Bleed and have allowed it to stack. Norman Osborn overloads the Arc Reactor in his chest if Health drops below 10%, granting a large burst of power, with (18% - 48% ) Armor, Regeneration, and Power Gain. After that, his suit burns out and cannot trigger Armor Up, Armor Break or Stun and loses all base Armor. Many players didn\u2019t like Iron Patriot\u2019s old signature ability, feeling that due to the lack of Regeneration, it was considerably weaker than Iron Man\u2019s. While we agreed, we didn\u2019t want to just copy and paste his signature ability, but rather give him his own unique twist on the ability."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the shutdown of the H7 platform in 2007, and how did this event impact the operator responsibility for the platform?",
    "choices": [
      "A) The shutdown was caused by a technical issue with the metering instruments installed on the platform, and as a result, the operator responsibility was transferred to Statoil, which had taken over the project from Gassco.",
      "B) The shutdown was due to a safety measure taken after a German cargo carrier, Reint, collided with the platform in 1995, and the operator responsibility remained with Gassco, which had already transferred its responsibility to Norway's state-owned company in 2003.",
      "C) The shutdown was caused by a change in the weather, and the operator responsibility remained with Gassco, which had transferred its responsibility to Norway's state-owned company in 2003.",
      "D) The shutdown was caused by the discovery of a new Ekofisk field in 1969, which led to an increase in oil production and subsequently, the shutdown of the H7 platform in 2007, and as a result, the operator responsibility was transferred to Statoil, which had taken over the project from Gassco."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Control panels on turbines and compressors were replaced and metering instruments installed to conduct measurements in this equipment. While the nearest neighbour to B11 was a Danish oil field, H7 stood in the middle of the shipping channel. M/S Hero broke down 15 nautical miles west of the latter platform at around 13.00 on 12 November 1977. By 21.00, the ship was still adrift and heading directly for H7, and all 14 crew on the platform made ready to evacuate by helicopter \u2013 the waves were too high for the lifeboats. The wreck passed at 21.40 with a clearance of 400 metres. German cargo carrier Reint collided with H7 on 30 September 1995, despite efforts by the standby ship to avert the threat. Production was halted as a safety measure, but the platform luckily suffered only minor damage. The collision was caused by inadequate watchkeeping on the ship\u2019s bridge. Operator responsibility for B11 and H7 was transferred at the beginning of 2003 to Norway\u2019s state-owned Gassco company, which runs the Norwegian gas transport network. This change had little significance for operation of the platforms, since the actual work was still carried out by ConocoPhillips as a technical service provider to Gassco. H7 was shut down in 2007, and removal had been completed in 2013. In connection with preparations to remove the structure, operator responsibility was transferred to Statoil as the company in charge of the project on Gassco\u2019s behalf. Published 24. August 2016 \u2022 Updated 22. October 2019\nPhillips inundates Sola with oil revenues\nperson by Kristin \u00d8ye Gjerde\nStavanger and neighbouring Sola were the first Norwegian local authorities to experience fantastic oil-related growth after the award of the first exploration licences in 1965. \u2014 Phillips er i ferd med \u00e5 etablere seg p\u00e5 Norscobasen nederst til h\u00f8yre Ca 1972 Foto: Norsk fly og flyfoto/Norsk Oljemuseum\nThe Shell refinery at Risavika in Sola was completed two years later, while the Norsco base in Tananger became operational as early as 1966. But things really took off once the Ekofisk field had been discovered in the autumn of 1969 and started trial production on 14 July 1971."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A local radio station, which had previously aired a popular conservative talk show, switched to a new format in 2010. This change was made possible because the station's previous affiliation with a major sports network had expired. As a result, the station was able to pick up a new sports network's programming, including a show hosted by a well-known sports personality. However, the station's new format also led to the cancellation of a long-running weekend talk show. Which of the following is a likely reason for the cancellation of the talk show?",
    "choices": [
      "A) The station's new format was too similar to its previous format, causing confusion among listeners.",
      "B) The station's new sports network affiliation required it to air more sports programming, leading to a reduction in talk shows.",
      "C) The station's new format was too focused on local programming, leading to a decrease in national syndicated shows.",
      "D) The station's decision to rebrand as a sports radio station led to a shift in its target audience, resulting in the cancellation of the talk show."
    ],
    "correct_answer": "D)",
    "documentation": [
      "These broadcasters were supported by producers such as Bruce Huff, Rob Pendleton, Alison Brown, Jean Bjorgen, David Elvin (who Vogel dubbed the \"Steven Spielberg of Talk Radio\"), Mitch Berg and others. The station has, for the most part, emphasized local hosts over the years. But in 1988, KSTP was one of Rush Limbaugh's first affiliates when his conservative talk show was rolled out for national syndication. (Clear Channel-owned KTLK-FM took over rights to Limbaugh's show in January 2006). Other syndicated hosts previously heard on KSTP include Sean Hannity, Bruce Williams, Larry King, and Owen Spann. Sports Radio\nKSTP switched to Sports Radio on February 15, 2010. As the station had to wait for ESPN's contract with rival KFAN and its sister station KFXN to expire, it did not become an ESPN Radio affiliate until April 12, the same day that the Minnesota Twins were scheduled to play the first game in their new ball park, Target Field, against the Boston Red Sox. As a result Coast to Coast AM and Live on Sunday Night, it's Bill Cunningham were retained during this period. One ESPN Radio network program, The Herd with Colin Cowherd, was picked up by KSTP immediately following the format change. In 2018, the station was approved for an FM translator on 94.1 FM, broadcasting from a transmitter atop the IDS Center in downtown Minneapolis. The two-watt signal threw most of its power to the west, preventing interference to low powered FM stations on the same channel including WFNU-LP in St. Paul. With only two watts of power, however, the signal was limited to the immediate downtown area surrounding the IDS Center. It later acquired a 250 watt translator, K235BP at 94.9 MHz. The original translator was discontinued. On January 15, 2019, KSTP rebranded as \"SKOR North\" (a reference to the Vikings team song/chant, \"Skol, Vikings\"), with local programming between 12 noon and 7 pm. About a year later, in May of 2020, KSTP suspended most of its local programming and laid off nearly all of its local staff."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robot is tasked with moving an object from one end of a room to the other while navigating around obstacles. The robot's goal is to reach the object, but the human collaborator has provided conflicting instructions. The human wants the robot to take a path around the obstacle on the left, but the robot's navigation system is designed to prioritize reaching the goal. If the robot only infers the goal from the human's inputs, it may incorrectly assume that the goal is on the right, and become over-confident in this belief. However, if the robot only infers the preferred path, it may mistakenly assume that the goal is on the left, leading to a failure in completing the task. To resolve this challenge, the robot needs to consider both the human's intended goal and their preferred path to that goal.",
    "choices": [
      "A) The robot should prioritize reaching the goal, as the human's instructions are ambiguous and may be incorrect.",
      "B) The robot should only consider the human's preferred path, as the goal is not relevant to the task.",
      "C) The robot should ignore the human's instructions and take a path around the obstacle on the left, as the goal is on the right.",
      "D) The robot should use a joint inference approach that considers both the human's intended goal and their preferred path to that goal, taking into account the navigation system's limitations and the conflicting instructions."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Bhattacharya propose an efficient algorithm for solving pathplanning problems under homotopic constraints. However, the number of homotopy classes for a given problem can be infinite, and as the robot changes location and updates its representation of the world, carrying out inference over homotopy classes in a dynamic environment requires recomputing the set of homotopies at every iteration, making the belief update challenging. Prior work has addressed the challenge of shared autonomy by considering how robots can infer a human's intended goal, or how they can infer the preferred path to a goal. However, we argue that inferring the goal and the path as separate problems can lead to over-confidence in incorrect beliefs about the user's preferences. To illustrate this point, consider the following scenario: a robot and a human are collaborating to move an object from one end of a room to Fig. : Using the hyperplanes composing the H-representation of each obstacle, we construct a hyperplane arrangement of the obstacle-free space (a). We define the human's preference for the robot's one step action choices as the posterior distribution (given all human input up to that point) over transitions from the current to the neighboring polytopes, i.e. edges on the graph. Each time the robot transitions to a new polytope, the set of neighbor polytopes and the distribution over human preferences are updated. another, but there is an obstacle in the way. The human would like the robot to take a path around the obstacle on the left, even though the goal is on the right. If the robot only infers the goal from the human's inputs, it may incorrectly assume that the goal is on the right, and become over-confident in this belief. On the other hand, if the robot only infers the preferred path, it may mistakenly assume that the goal is on the left, leading to a failure in completing the task. To overcome these challenges, our work proposes a joint inference approach that considers both the human's intended goal and their preferred path to that goal."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is interested in analyzing the response of a nonlinear system to a specific input. They have access to the measured response in the time domain, as well as the generalized frequency response function (GFRF) of the system. However, the system's Volterra kernel function is unknown. Which of the following methods would be most suitable for estimating the Volterra kernel function?",
    "choices": [
      "A) Using the measured response in the time domain to directly estimate the Volterra kernel coefficients, as proposed by Son and Kim.",
      "B) Transforming the measured response into the frequency domain and solving the resulting algebraic equation to obtain the Volterra kernel function, as described by Billings et al.",
      "C) Using the measured response in the time domain to identify the kernel function, as proposed by Peng et al., and then performing nonlinear structural damage detection.",
      "D) Employing a time-delay neural network to identify the Volterra kernel function, as proposed by De Paula and Marques, and then using the identified kernel function to compute the system's response."
    ],
    "correct_answer": "D)",
    "documentation": [
      "For a long response with small time steps, the time domain methods are very costly in computational time. Volterra series is another widely used method, which is the extension of the Duhamel integral for linear systems . Volterra series can reproduce many nonlinear phenomena, but they are very complex due to higher-dimensional convolution integrals . Since 1980's, significant progress has been made in the general area of the Volterra series. The reader is referred to Ref. for a quite thorough literature review on the relevant topics. After 2017, most papers focus on Volterra series identification. De Paula and Marques proposed a method for the identification of Volterra kernels, which was based on time-delay neural networks. Son and Kim presented a method for a direct estimation of the Volterra kernel coefficients. Dalla Libera et al. introduced two new kernels for Volterra series identification. Peng et al. used the measured response to identify the kernel function and performed the nonlinear structural damage detection. Only a few papers concentrated on simplifying the computation of convolution integrals. Traditional methods for computing convolution integrals involved in the Volterra series have been performed in three distinct domains: time, frequency and Laplace. The time domain method based on Volterra series refers to discrete time convolution methods, which also suffer computational cost problems . Both the frequency domain method and the Laplace domain method based on the Volterra series consist of three steps: (1) Volterra series are transformed into an algebraic equation in the frequency domain or Laplace domain; the algebraic equation is solved by purely algebraic manipulations; and (3) the solution in Step ( ) is transformed back to the time domain. Many researchers have used the frequency domain method to compute the responses of nonlinear systems. Billings et al. developed a new method for identifying the generalized frequency response function (GFRF) of nonlinear systems and then predicted the nonlinear response based on these GFRFs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the most likely reason for Njoroge's brothers to have funded his education, given the context of the Mau Mau movement and the family's involvement with Jacobo?",
    "choices": [
      "A) To provide Njoroge with a sense of security and stability, as his family's situation was becoming increasingly unstable due to Jacobo's accusations and the family's involvement with the Mau Mau.",
      "B) To distance themselves from Jacobo's actions and avoid any potential repercussions, as Njoroge's brothers may have been trying to maintain a neutral stance in the conflict.",
      "C) To prepare Njoroge for a potential career in law, given that he had recently passed an important exam and was advancing to High School.",
      "D) To ensure Njoroge's future success and independence, as his brothers may have been motivated by a desire to secure their son's future in the face of the deteriorating economic and political situation in Kenya."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Jacobo survives and swears revenge. Ngotho loses his job and Njoroge\u2019s family is forced to move. Njoroge\u2019s brothers fund his education and seem to lose respect for their father. Mwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school. For a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta \u2013 a revered leader of the movement. Many blacks think that he is going to bring forth Kenya\u2019s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population. Jacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods. One day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition. Several months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In a system comprising two quantum dots in a T-shaped geometry with respect to normal leads and proximized by a superconductor, the CAR exchange mechanism leads to a two-stage Kondo screening even in the absence of other exchange mechanisms. However, if the superconducting contact is replaced with a normal metal, the CAR exchange mechanism is still present, but the resulting Kondo screening is different. What is the consequence of this change on the residual low-temperature conductance at particle-hole symmetric case?",
    "choices": [
      "A) The residual low-temperature conductance at particle-hole symmetric case is unaffected by the change in the superconducting contact, and the two-stage Kondo screening remains unchanged.",
      "B) The residual low-temperature conductance at particle-hole symmetric case increases due to the increased competition between CAR exchange and RKKY interaction, leading to a more complex Kondo screening scenario.",
      "C) The residual low-temperature conductance at particle-hole symmetric case decreases due to the reduced CAR exchange mechanism, leading to a simpler Kondo screening scenario.",
      "D) The residual low-temperature conductance at particle-hole symmetric case is reduced due to the increased SC coupling necessary to observe the second stage of Kondo screening caused by CAR outside the experimentally achievable range, and the narrowing of the related local spectral density dip."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This does not have to be the case in real setups, yet relaxing this assumption does not \nintroduce qualitative changes. Nevertheless, the model cannot be extended to inter-dot \ndistances much larger than the coherence length, where $\\GS{\\rm X}\\to 0$.\n\nTo quantitatively analyze the consequences of less effective Andreev coupling we define the \nCAR efficiency as $\\mathcal{C} \\equiv \\GS{\\rm X} / \\sqrt{\\GS{1}\\GS{2}}$ and analyze $\\mathcal{C} < 1$\nin the wide range of $\\GS{1}=\\GS{2}=\\GS{}$ and other parameters corresponding to \\fig{3}. The results are presented in \\fig{C}. Clearly, decreasing $\\mathcal{C}$ from $\\mathcal{C}=1$ causes diminishing of $\\GS{\\rm X}$, and consequently of CAR \nexchange. For a change as small as $\\mathcal{C}=0.9$, the consequences reduce to some shift of the \nconventional Kondo regime, compare \\fig{C}(a) with \\fig{3}. Stronger suppression of CAR may, \nhowever, increase the SC coupling necessary to observe the second stage of Kondo screening caused\nby CAR outside the experimentally achievable range, see \\fig{C}(b). Moreover, the reduced $T^*$\nleads to narrowing of the related local spectral density dip, while the\nincreased critical $\\GS{}$ necessary for the observation of the second stage of screening leads to the\nshallowing of the dip. This is visible especially in the inset in \\fig{C}(b). \\section{Conclusions}\n\\label{sec:conclusions}\n\nThe CAR exchange mechanism is present in any system comprising at least\ntwo QDs or magnetic impurities coupled to the same superconducting contact\nin a way allowing for crossed Andreev reflections. In the considered setup, comprised of two quantum dots in a T-shaped geometry \nwith respect to normal leads and proximized by superconductor,\nit leads to the two-stage Kondo\nscreening even in the absence of other exchange mechanisms. This CAR induced exchange screening is characterized by a residual \nlow-temperature conductance at particle-hole symmetric case. We have also shown that the competition between CAR exchange and RKKY\ninteraction may result in completely different Kondo screening scenarios."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for KSTP's shift from a Top 40 format to an adult contemporary format in 1979?",
    "choices": [
      "A) The station's management wanted to appeal to a more mature audience, citing the success of other adult contemporary stations in the area.",
      "B) The station's ownership group was under pressure from advertisers to switch to a format that would attract more listeners in the 25-54 demographic.",
      "C) The station's programming director, Tom Barnard, left the station and was replaced by a new director who preferred the adult contemporary format.",
      "D) The station's shift to adult contemporary was a result of the increasing popularity of FM radio among younger listeners, who were more likely to tune into FM stations for music."
    ],
    "correct_answer": "D)",
    "documentation": [
      "An FM station, KSTP-FM, was founded in 1946 but shut down in 1952. Hubbard reportedly acquired an RCA TV camera in 1939, and started experimenting with television broadcasts. But World War II put a hold on the development of television. In 1948, with the war over, KSTP-TV became the first television station in Minnesota. With KSTP 1500 already associated with NBC Radio, KSTP-TV became an NBC Television Network affiliate. From 1946 to 1952, KSTP also had an FM counterpart. KSTP-FM 102.1 was only on the air four years. There were few radios equipped to receive FM signals in that era, and management decided to discontinue FM broadcasts. MOR and Top 40\nAs network programming moved from radio to television, KSTP programmed a full service Middle of the Road (MOR) radio format, in the shadow of its chief competitor, CBS Radio affiliate 830 WCCO. In 1965, a new FM station, reviving the KSTP-FM call sign, was put on the air, largely simulcasting the AM station. But by the late 1960s, KSTP-FM began a separate format of beautiful music. KSTP was the radio home of the Minnesota Vikings football team from 1970 to 1975. In 1973, KSTP broke away from its longtime adult MOR sound and became one of four area stations at the time to program a Top 40 format. \"15 KSTP, The Music Station\" competed with Top 40 AM rivals WDGY, KDWB and later, WYOO. The competition would eventually shake itself out, with outrageous rocker WYOO dropping out after being sold in 1976, and then the staid WDGY switching to country music the following year. As for uptempo hits station 15 KSTP, it went from a tight Top 40 format to leaning adult rock in 1978, to leaning adult contemporary in 1979, to evolving into adult contemporary/talk by 1980. In 1982, it officially shifted to talk. Most Top 40 rock music, by this time, had moved to the FM band. Past Personalities\n\nNotable hosts who have been on KSTP include John Hines, Jesse Ventura, Larry Carolla, Tom Barnard, Big Al Davis, Don Vogel, John MacDougall, Griff, Mike Edwards, Geoff Charles, Joe Soucheray, James Lileks, Leigh Kamman, Barbara Carlson, Peter Thiele, Tom Mischke, Jason Lewis, Chuck Knapp, Machine Gun Kelly, Charle Bush, Mark O'Connell and Paul Brand."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the effect of electric field strength on the repulsive force between two small spheres charged with the same type of electricity. They find that the repulsive force is inversely proportional to the square of the distance between the centres of the two spheres. However, they also notice that the electric field strength of the spheres is not uniform, but rather varies in space. Which of the following statements best describes the relationship between the electric field strength and the repulsive force?",
    "choices": [
      "A) The repulsive force is directly proportional to the square of the electric field strength.",
      "B) The repulsive force is inversely proportional to the square of the electric field strength, and the electric field strength is uniform in space.",
      "C) The repulsive force is inversely proportional to the square of the electric field strength, but the electric field strength is not uniform in space.",
      "D) The repulsive force is directly proportional to the square of the electric field strength, and the electric field strength is uniform in space."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Annalen der Physik. 267 (8): S. 983\u20131000. Bibcode:1887AnP... 267..983H. doi:10.1002/andp.18872670827.\n^ \"The Nobel Prize in Physics 1921\". Nobel Foundation. Retrieved 2013-03-16. ^ John Sydney Blakemore, Solid state physics, pp. 1\u20133, Cambridge University Press, 1985 ISBN 0-521-31391-0.\n^ Richard C. Jaeger, Travis N. Blalock, Microelectronic circuit design, pp. 46\u201347, McGraw-Hill Professional, 2003 ISBN 0-07-250503-6.\n^ \"The repulsive force between two small spheres charged with the same type of electricity is inversely proportional to the square of the distance between the centres of the two spheres.\" Charles-Augustin de Coulomb, Histoire de l'Academie Royal des Sciences, Paris 1785. ^ Sewell, Tyson (1902), The Elements of Electrical Engineering, Lockwood, p. 18 . The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge'.\n^ a b Berkson, William (1974), Fields of Force: The Development of a World View from Faraday to Einstein, Routledge, p. 370, ISBN 0-7100-7626-6 Accounts differ as to whether this was before, during, or after a lecture.\n^ \"Lab Note #105 EMI Reduction \u2013 Unsuppressed vs. Suppressed\". Arc Suppression Technologies. April 2011. Retrieved March 7, 2012.\n^ Almost all electric fields vary in space. An exception is the electric field surrounding a planar conductor of infinite extent, the field of which is uniform. ^ Paul J. Nahin (9 October 2002). Oliver Heaviside: The Life, Work, and Times of an Electrical Genius of the Victorian Age. JHU Press. ISBN 978-0-8018-6909-9.\n^ \"The Bumpy Road to Energy Deregulation\". EnPowered. 2016-03-28.\n^ a b c d e f g h Van Riper, op.cit., p. 71. Look up electricity in Wiktionary, the free dictionary. Basic Concepts of Electricity chapter from Lessons In Electric Circuits Vol 1 DC book and series."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the introduction of the DTaP vaccine led to a significant decrease in pertussis cases in developed countries. However, the same study also revealed that the vaccine was not as effective in preventing pertussis in infants under the age of 6 months. What is the most likely explanation for this discrepancy?",
    "choices": [
      "A) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months due to the presence of a specific antigen that is not present in the vaccine.",
      "B) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not administered frequently enough to provide adequate protection.",
      "C) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not sufficient to provide long-term immunity against the disease.",
      "D) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not designed to target the specific strain of pertussis that is most common in this age group."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. .. Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. .. (Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.\" As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing. Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of correlated hybrid nanostructures, what is the primary mechanism by which the superconducting proximity effect induces Kondo screening?",
    "choices": [
      "A) The superconducting proximity effect induces Kondo screening through the formation of Cooper-pair condensates, which in turn lead to the suppression of the Kondo effect.",
      "B) The superconducting proximity effect induces Kondo screening through the formation of non-local pairing, which is a consequence of the overlap of Yu-Shiba-Rusinov (YSR) states.",
      "C) The superconducting proximity effect induces Kondo screening through the formation of a non-local pairing, which is a consequence of the coupling to Cooper-pair condensate, and this effect is most effective when the YSR states are close to the middle of the SC gap.",
      "D) The superconducting proximity effect induces Kondo screening through the formation of a non-local pairing, which is a consequence of the overlap of Yu-Shiba-Rusinov (YSR) states, and this effect is most effective when the YSR states are close to the middle of the SC gap, and the presence of YSR bound states close to the Fermi level is not necessary for significant consequences for the Kondo physics."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Such processes give rise to an exchange mechanism \\cite{Yao},\nthat we henceforth refer to as \\emph{the CAR exchange}, which can greatly modify\nthe low-temperature transport behavior of correlated hybrid nanostructures. The CAR exchange may be seen as RKKY-like interaction between\ntwo nearby impurities on SC surface \\cite{Yao}. The effect can be understood as a consequence\nof spin-dependent hybridization of the Yu-Shiba-Rusinov (YSR)\nstates \\cite{Yu,Shiba,Rusinov} in SC contact,\ncaused both by the overlap of their wave functions\nand their coupling to Cooper-pair condensate. This process is the most effective when the YSR states \nare close to the middle of the SC gap, {\\it e.g.} in the YSR-screened phase \\cite{YSRscreening}. The mechanism presented here is essentially the same,\nyet in the considered regime can be understood\nperturbatively without referring to YSR states,\nas a consequence of the non-local pairing induced by SC electrode. In particular, the presence of YSR bound states close to the Fermi level \nis not necessary for significant consequences for the Kondo physics, \nas long as some inter-dot pairing is present. The proximity of SC induces pairing in QDs \\cite{RozhkovArovas,Buitelaar} \nand tends to suppress the Kondo effect if the superconducting energy gap $2\\Delta$ \nbecomes larger than the relevant Kondo temperature $T_K$ \n\\cite{Buitelaar2002Dec,adatomsSC,Kondo_vs_SC1,Kondo_vs_SC2,Zitko_Kondo-Andreev,Zitko_S-QD-N,IW_Sau,YSRscreening}. Moreover, the strength of SC pairing can greatly affect the Kondo physics in the sub-gap transport regime: For QDs attached to SC and normal contacts, it can enhance the Kondo effect\n\\cite{DomanskiIW,KWIW,part1}, while\nfor DQD-based Cooper pair splitters, it tends to suppress both the $\\mathrm{SU}(2)$ and $\\mathrm{SU}(4)$ Kondo effects \\cite{IW_Kacper}. Our main result is that the non-local pairing induced by superconducting \nproximity effect, which gives rise to CAR exchange, can be the sole cause of the Kondo screening. Moreover, relatively small values of coupling to SC, $\\GS{}\\ll U$, are sufficient for the effect to occur."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a scenario where a light source is polarized in the x-direction and transmitted through a transmission matrix with 3 incoming channels and 2 outgoing channels. The transmission matrix elements are random complex coefficients. After transmission, the outgoing fields are measured using a polarizer aligned with the y-axis. What is the probability that the observed intensity of the transmitted light is greater than 50% of the incident intensity?",
    "choices": [
      "A) The probability is 1, since the transmission matrix ensures that the output intensity is always greater than or equal to the input intensity.",
      "B) The probability is 0.5, since the transmission matrix is symmetric and the polarizer is aligned with the y-axis, which is orthogonal to the x-axis.",
      "C) The probability is 1, since the transmission matrix is symmetric and the polarizer is aligned with the y-axis, which is orthogonal to the x-axis, and the light source is polarized in the x-direction.",
      "D) The probability is less than 0.5, since the transmission matrix elements are random complex coefficients and the polarizer is aligned with the y-axis, which is orthogonal to the x-axis, and the light source is polarized in the x-direction. This requires synthesizing information from the transmission matrix elements, the polarizer alignment, and the light source polarization."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Consider the case in which there are $N_I$ incoming channels and $N_O$ outgoing ones; we can indicate with $E^{\\rm in,out}_k$ the input/output electromagnetic field phasors of channel $k$. In the most general case, i.e., without making any particular assumptions on the field polarizations, each light mode and its polarization polarization state can be represented by means of the $4$-dimensional Stokes vector. Each $ t_{ki}$ element of $\\mathbb{T}$, thus, is a $4 \\times 4$ M{\\\"u}ller matrix. If, on the other hand, we know that the source is polarized and the observation is made on the same polarization, one can use a scalar model and adopt Jones calculus \\cite{Goodman85,Popoff10a,Akbulut11}:\n   \\begin{eqnarray}\n E^{\\rm out}_k = \\sum_{i=1}^{N_I}  t_{ki} E^{\\rm in}_i \\qquad \\forall~ k=1,\\ldots,N_O\n \\label{eq:transm}\n \\end{eqnarray}\n  We recall that the elements of the transmission matrix are random complex coefficients\\cite{Popoff10a}. For the case of completely unpolarized modes, we can also use a scalar model similar to Eq. \\eqref{eq:transm}, but whose variables are  the intensities of the outgoing/incoming fields, rather than the fields themselves.\\\\ \nIn the following, for simplicity, we will consider Eq. (\\ref{eq:transm}) as our starting point,\nwhere $E^{\\rm out}_k$, $E^{\\rm in}_i$ and $t_{ki}$ are all complex scalars. If Eq. \\eqref{eq:transm} holds for any $k$, we can write:\n  \\begin{eqnarray}\n  \\int \\prod_{k=1}^{N_O} dE^{\\rm out}_k \\prod_{k=1}^{N_O}\\delta\\left(E^{\\rm out}_k - \\sum_{j=1}^{N_I}  t_{kj} E^{\\rm in}_j \\right) = 1\n  \\nonumber\n  \\\\\n  \\label{eq:deltas}\n  \\end{eqnarray}\n\n Observed data are a noisy representation of the true values of the fields. Therefore, in inference problems it is statistically more meaningful to take that noise into account in a probabilistic way, \n rather than looking  at the precise solutions of the exact equations (whose parameters are unknown). To this aim we can introduce Gaussian distributions whose limit for zero variance are the Dirac deltas in Eq."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study found that the Fukushima Daiichi nuclear accident released a significant amount of radioactive iodine into the atmosphere, which was carried by winds and deposited on the ocean surface. However, the levels of radioactive iodine in the ocean waters off the coast of Japan are not expected to have a significant impact on the health of the Japanese people, according to the head of the United Nations' scientific committee on the effects of atomic radiation. This statement is likely true because the levels of radioactive iodine in the ocean waters off the coast of Japan are much lower than those seen at similar distances following the Chernobyl accident. However, some experts are concerned that the release of radioactive iodine could have long-term effects on the marine ecosystem. What is the most likely reason why the head of the United Nations' scientific committee on the effects of atomic radiation made this statement?",
    "choices": [
      "A) The levels of radioactive iodine in the ocean waters off the coast of Japan are not expected to have a significant impact on the health of the Japanese people because the release of radioactive iodine is not a significant source of radiation exposure for the general population.",
      "B) The head of the United Nations' scientific committee on the effects of atomic radiation made this statement because the release of radioactive iodine is a natural process that occurs after a nuclear accident, and it is not a cause for concern.",
      "C) The head of the United Nations' scientific committee on the effects of atomic radiation made this statement because the levels of radioactive iodine in the ocean waters off the coast of Japan are not expected to have a significant impact on the health of the Japanese people because the Japanese government has implemented effective measures to mitigate the effects of the accident.",
      "D) The head of the United Nations' scientific committee on the effects of atomic radiation made this statement because the levels of radioactive iodine in the ocean waters off the coast of Japan are much lower than those seen at similar distances following the Chernobyl accident, and this is a reliable indicator of the potential health risks associated with the accident."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This week, workers plugged a crack in the plant that had been gushing contaminated water into the ocean for weeks. As a result, TEPCO says now radiation levels in the ocean waters off the coast there have dropped dramatically. Yesterday, the head of the United Nations' scientific committee on the effects of atomic radiation said the Fukushima accident is not expected to have any serious impact on the health of the Japanese people. He said, quote: \"We have seen traces of iodine in the air all over the world, but they are much, much, much lower than traces we have seen at similar distances following Chernobyl,\" unquote. Well, not everybody is convinced. In South Korea, more than 130 primary schools and kindergartens ordered closed today outside of Seoul. People there were worried that windy, rainy weather could be carrying radioactive material from Japan. North Korea aired warnings on television for its people to stay indoors during that rain storm and to take a full shower if they were caught outside in the storm. Even here in the United States, some chefs are now using sensors to test levels of radiation in the fish they plan to serve in restaurants. Here's the question -- do you think you're being told the truth about the nuclear accident in Japan? If your -- if your trout is he glowing, Wolf --\nBLITZER: Yes?\nCAFFERTY: -- you might want to send it back and get a ham sandwich. BLITZER: You want it well done, but not necessarily that well done. CAFFERTY: No.\nBLITZER: All right, Jack. Not a laughing matter. BLITZER: Serious stuff. CAFFERTY: Right. BLITZER: See you in a few moments. New questions this hour about the capabilities of the rebels in Libya and whether they have the power to overthrow Moammar Gadhafi. Should the United States -- should the United States have a hand in helping arm the rebels? Let's bring in our foreign affairs correspondent, Jill Dougherty, with this part of the story. What are you hearing over at the State Department -- Jill. JILL DOUGHERTY, CNN FOREIGN AFFAIRS CORRESPONDENT:"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robot is navigating through a cluttered office space to reach a goal location, and it receives direction indications from a human that indicate which path to take. The robot's method of adapting to the human's preference involves leveraging probabilistic representations of human preference and incorporating real-time feedback. However, this approach can become exponentially complex when the number of obstacles in the space increases. What is a potential limitation of this approach?",
    "choices": [
      "A) The robot's ability to adapt to human preference is limited by the number of possible homotopy classes in the space.",
      "B) The robot's method of inference is only effective when the goal location is known in advance.",
      "C) The complexity of the decision-making problem increases with the number of candidate destinations, but not with the number of obstacles.",
      "D) The robot's ability to adapt to human preference is limited by the need for frequent updates to its probabilistic representation of human preference."
    ],
    "correct_answer": "D)",
    "documentation": [
      "To optimize the use of human input and quickly infer the human's preference, Fig. : An autonomous robot navigates in a simulated classroom towards a goal location (pink circle). At the start of its mission, it receives direction indications (arrows) from a human that indicate which path it should take to get to the goal. In this scenario, the human wants the robot to go around the desks on the right side of the classroom. A robot that does not reason over path preferences (green) will take the shortest path to the goal regardless of the human's input. Our method (blue) infers the human's path preference from these indications and adapts to their recommendations. we propose an approach that leverages probabilistic representations of human preference and incorporates real-time feedback. Previous research by Bajcsy et al. considered an online adaptation problem in a manipulation task, where the person can apply forces to the robot to indicate their preferences. By allowing the robot to continue its task while taking into account a probabilistic representation of human preference, their approach does not require frequent inputs. Building on this idea, we adopt a similar approach to adapt to a human's preference in the context of a robot autonomously navigating through a known environment, such as a cluttered office space. Specifically, we focus on allowing the human to influence the robot's trajectory with respect to obstacles, by providing guidance on preferred routes or paths, while the robot continues to execute its task. Paths can be represented using homotopy classes . However, homotopies can pose computational challenges when used to encode and infer human preferences. When the robot maintains a belief over homotopy classes, the inference problem can become exponentially complex with the number of obstacles in the space. Additionally, when the goal is unknown, the number of variables increases with the number of candidate destinations. This complexity can render the decision-making problem intractable."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In McPherson County, the median income for a household is $41,138, and the median income for a family is $48,243. However, the per capita income for the county is $18,921. What is the likely reason for the discrepancy between the median household and family incomes?",
    "choices": [
      "A) The county has a high percentage of single-parent households, which tend to have lower incomes than two-parent households.",
      "B) The county has a significant number of families with incomes above the median, which are not reflected in the median family income.",
      "C) The county has a large number of elderly individuals who are living alone, which are not accounted for in the median household income.",
      "D) The county has a high percentage of families with incomes above the median, which are not reflected in the median household income, and the median family income is skewed by a few very low-income families."
    ],
    "correct_answer": "D)",
    "documentation": [
      "25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 2.99. In the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 95.90 males. For every 100 females age 18 and over, there were 92.90 males. The median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921. About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over. Government\n\nPresidential elections\nMcPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson. Laws\nFollowing amendment to the Kansas Constitution in 1986, the county remained a prohibition, or \"dry\", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement. Education\n\nColleges\n McPherson College in McPherson\n Bethany College in Lindsborg\n Central Christian College in McPherson\n\nUnified school districts\n Smoky Valley USD 400\n McPherson USD 418\n Canton-Galva USD 419\n Moundridge USD 423\n Inman USD 448\n\nSchool district office in neighboring county\n Goessel USD 411\n Little River-Windom USD 444\n\nMuseums\n Birger Sandz\u00e9n Memorial Gallery in Lindsborg\n McCormick-Deering Days Museum in Inman\n McPherson Museum in McPherson\n Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg\n Kansas Motorcycle Museum in Marquette\n\nCommunities\n\nCities\n\n Canton\n Galva\n Inman\n Lindsborg\n Marquette\n McPherson (county seat) \n Moundridge\n Windom\n\nUnincorporated communities\n\u2020 means a Census-Designated Place (CDP) by the United States Census Bureau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is investigating the use of electrospun nanofibers to promote wound healing in diabetic patients. The nanofibers are immobilized with human epidermal growth factor (EGF) and are aligned in a specific direction to mimic the structure of the skin. Which of the following statements best describes the expected outcome of this treatment?",
    "choices": [
      "A) The use of EGF-immobilized nanofibers will lead to increased cell proliferation and differentiation in the wound area, resulting in faster wound closure.",
      "B) The alignment of the nanofibers is crucial for the delivery of EGF to the wound site, as it allows for more efficient penetration of the growth factor into the tissue.",
      "C) The use of EGF-immobilized nanofibers will stimulate the production of collagen and other extracellular matrix components, leading to improved wound strength and durability.",
      "D) The combination of EGF-immobilized nanofibers and electromechanical stimulation will enhance the maturation of myotubes on aligned electrospun fibers, leading to improved muscle function in diabetic patients."
    ],
    "correct_answer": "D)",
    "documentation": [
      "no. 3 (2008), pp. 252-259 [abs]\nChoi, J. S. and Leong, K. W. and Yoo, H. S., In vivo wound healing of diabetic ulcers using electrospun nanofibers immobilized with human epidermal growth factor (EGF), Biomaterials, vol. 29 no. 5 (2008), pp. 587-96 [abs]\nLiao, I. C. and Liu, J. B. and Bursac, N. and Leong, K. W., Effect of Electromechanical Stimulation on the Maturation of Myotubes on Aligned Electrospun Fibers, Cellular and Molecular Bioengineering, vol. 1 no. 2-3 (2008), pp. 133-145 [abs]\nProw, T. W. and Bhutto, I. and Kim, S. Y. and Grebe, R. and Merges, C. and McLeod, D. S. and Uno, K. and Mennon, M. and Rodriguez, L. and Leong, K. and Lutty, G. A., Ocular nanoparticle toxicity and transfection of the retina and retinal pigment epithelium, Nanomedicine-Nanotechnology Biology and Medicine, vol. 4 no. 4 (2008), pp. 340-349 [abs]\nTan, S. C. W. and Pan, W. X. and Ma, G. and Cai, N. and Leong, K. W. and Liao, K., Viscoelastic behaviour of human mesenchymal stem cells, Bmc Cell Biology, vol. 9 (2008), pp. - [abs]\nChalut, K. J. and Chen, S. and Finan, J. D. and Giacomelli, M. G. and Guilak, F. and Leong, K. W. and Wax, A., Label-free, high-throughput measurements of dynamic changes in cell nuclei using angle-resolved low coherence interferometry, Biophysical Journal, vol. 94 no. 12 (2008), pp. 4948-4956 [abs]\nHaider, M. and Cappello, J. and Ghandehari, H. and Leong, K. W., In vitro chondrogenesis of mesenchymal stem cells in recombinant silk-elastinlike hydrogels, Pharmaceutical Research, vol. 25 no. 3 (2008), pp. 692-699 [abs]\nN. Bursac and Y. H. Loo and K. Leong and L. Tung, Novel anisotropic engineered cardiac tissues: Studies of electrical propagation, Biochemical And Biophysical Research Communications, vol. 361 no. 4 (October, 2007), pp. 847 -- 853 , ISSN 0006-291X [abs]\nChen, Beiyi and Dang, Jiyoung and Tan, Tuan Lin and Fang, Ning and Chen, Wei Ning and Leong, Kam W. and Chan, Vincent, Dynamics of smooth muscle cell deadhesion from thermosensitive hydroxybutyl chitosan, Biomaterials, vol. 28"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A popular online multiplayer game has been experiencing issues with players exploiting the system by creating multiple accounts to refresh their in-game currency. The game's developers have stated that they are investigating the matter and have assured players that their system is secure. However, some players are concerned that the game's design may be flawed, allowing for such exploits to occur. What is a potential reason why the game's developers may not be able to prevent players from creating multiple accounts to refresh their currency?",
    "choices": [
      "A) The game's developers may not be using a robust enough system to detect and prevent account duplication.",
      "B) The game's developers may be relying too heavily on player cooperation to prevent exploits, and are not doing enough to educate players on the risks of account sharing.",
      "C) The game's developers may be using a system that allows players to create multiple accounts without properly verifying their identities, making it easier for players to create multiple accounts.",
      "D) The game's developers may be aware of the issue, but are choosing not to implement a solution that would require significant changes to the game's design, in order to avoid disrupting the game's balance and player experience."
    ],
    "correct_answer": "D)",
    "documentation": [
      "More money from various players might mean they can limit the players who spend a ton and still generate a healthy income. The main issue i see with limiting refreshes is someone multiscoping and spending money. He now has two, three, four accounts to refresh with and gets the advantage. Its tricky. ey dun new ho to yet it uff. Yet you complain almost everyday here, on your website, Twitter, and YouTube channel that the game needs to change because cubing has such an impact. It was fun. Swarm had me scared at first, but it turned into kind of a bullying match between us and legion. Last hour became p obvious which way it was gonna go. Legion rly stepped up their game in the end there, respect. We are investigating this. Here is what we know: Several of the accounts used the same password. Most of the accounts belonged to people who knew each other personally. The accounts were all switched from the same IP Addresses. The person who logged in, got into each account on the first attempt, so they knew the password for each account. What you should know: QONQR never stores passwords, not even in the logs. Passwords are hashed (one way encrypted) and can never be decrypted When you authenticate to our servers, we hash the password you gave us and compare it to the encrypted password in the database to see if they match. Access to our database in the could is restricted tightly and we are confident no one breached the system. What you should do: Don't use the same password as other people you play with. Don't share your password with anyone. I heard all the French players fled to the UK after one German player accidentally shot a single missile into France. Most factions now use GroupMe or Line as their means of communication, the forums are too slow as a means of communication and insecure for specific faction conversations. Think of the forums are more of a gaming information resource rather than a means of communication. Contact the top players of your faction in the leader boards of your state and they will likely point you in the right direction to chatting with your local faction."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In a DBM model, when constructing a DBM that only retains the first-order term of the Knudsen number (i.e., only the first-order TNE effects are retained), what is the total number of kinetic moments that need to be reserved?",
    "choices": [
      "A) 5 kinetic moments, including M 0 , M 1 , M 2,0 , M 2 , and M 3,1 , which are sufficient for the simulation of the fluid's hydrodynamic behavior.",
      "B) 7 kinetic moments, including M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , and M 4,2 , which are necessary to capture the full range of TNE effects.",
      "C) 9 kinetic moments, including M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , and M 5,3 , which are required to accurately model the fluid's behavior under all TNE conditions.",
      "D) 10 kinetic moments, including M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , M 5,3 , and M 6,4 , which are necessary to capture the full range of TNE effects, including the effects of the second-order TNE."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In DBM modeling, the CE multiscale analysis is used to determine quickly the reserved kinetic moments. Specifically, when constructing a DBM which only the first order term of Kn number is retained (i.e., only the first order TNE effects are retained), seven kinetic moments should be reserved, i.e., the M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 . Two more kinetic moments ( M 4 and M 5,3 ) are needed when the second order TNE is considered . However, it should be noted that the function of CE analysis in DBM modeling is only to determine the kinetic moments that need to be preserved. Whether or not to derive the hydrodynamic equations does not affect the DBM simulation. The kinetic moments used in our physical modeling are shown in the Appendix B. Their expressions can be obtained by integrating v and \u03b7 with continuous-form f eq . For better understanding, the Appendix C gives the two-fluid hydrodynamic equations recovered from the Boltzmann equation. The kinetic moments in Appendix B can be written in matrix form, i.e., C \u2022 f \u03c3 ,eq = f\u03c3,eq , (\nwhere C is the matrix of discrete velocity and feq represents the kinetic moments. A proper discrete velocity model is needed to confirm the values of f \u03c3 ,eq i . The f \u03c3 ,eq can be obtained by solving the inverse matrix, i.e., f \u03c3 ,eq = C \u22121 \u2022 f\u03c3,eq , where C \u22121 is the inverse matrix of C. It is very convenient to obtain the inverse matrix of C through some mathematical softwares such as Mathematica, etc. The D2V16 model is chosen in this paper, its sketches can be seen in Fig. . The specific values of D2V16 are given in the following equations: where \"cyc\" indicates cyclic permutation and c is an adjustable parameter of the discrete velocity model. The sketch of \u03b7 in D2V16 is \u03b7 i = \u03b7 0 for i = 1 \u2212 4, and \u03b7 i = 0 for i = 5 \u2212 16. Checking the TNE state and extracting TNE information\n\nMany physical quantities can characterize the degree of TNE in a fluid system, such as relaxation time, Kn number, viscosity, heat conduction, the gradients of macroscopic quantity, etc."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A student with Asperger's syndrome, who has been successfully managing his academic workload and extracurricular activities, suddenly starts to struggle with completing assignments and attending classes. His grades begin to slip, and he becomes increasingly withdrawn. What is the most likely underlying reason for this change in behavior?",
    "choices": [
      "A) The student has developed a new interest in a hobby that requires significant time commitment, causing him to neglect his academic responsibilities.",
      "B) The student has been experiencing stress related to his upcoming graduation and is using his gaming habits as a coping mechanism.",
      "C) The student has developed a learning disability that affects his ability to complete assignments, and his grades are a result of this new condition.",
      "D) The student's gaming habits have become an addiction, causing him to spend excessive amounts of time playing games and neglecting his academic responsibilities, which is a common consequence of untreated addiction."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We worked with him on making eye contact when talking with others. We explained different emotions in people's faces and mannerisms to help him know how to interact with others. We discussed the fact that people would say things that did not mean what they souneded like - such as \"I'm so hungry I could eat a horse\". As we did these things he worked hard to better understand communication with others. During his 4th grade year I had a teacher from the gifted program ask me if I had ever heard of Aspergers. I told her that I had not heard of it. She proceeded to read me some of the charateristics and so many of them described my son. So we had him tested by the school district during the summer between 4th and 5th grade and they did find that he had Aspergers but that he was high functioning. We then set him up with and EIP which stayed with him until his sophomore year. We pulled him from it at that time because we had moved and the new district was requiring him to take one class a day that was a study class. This reduced the number of required classes he could take and he was doing fine with his studies at the time. It was during the 2nd half of his Junior year that we noticed some of his grades going down. Then during his Senior year is when he started skipping classes and not doing assignments. We had not realized it before then but we soon became aware that he was addicted to gaming. He would go to the library or somewhere else on campus and play games on the computer rather than go to class. It was also at this time that he began lying about his actions (so as not to get in trouble). Based on his grades and his ACT score he received offers from colleges for full tuition scholarships. He chose the college where he had taken concurrent classes during his high school years. But he proceeded to skip class and not turn in assignments so he lost his scholarship and quit attending college. During this time he was only able to find employment through an employment agency where he was mostly sent to manuel labor type jobs (which is not something he enjoys but he did it anyway)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company like Intel, which has a strong position in the x86 market, may consider manufacturing SoCs on nodes that are a step behind the state of the art. What are the potential implications of this strategy for Intel's long-term competitiveness in the market?",
    "choices": [
      "A) If Intel focuses on manufacturing SoCs on nodes that are only one step behind the state of the art, it will be able to maintain its current market share without significant investment in new technology. This approach would allow Intel to continue to leverage its existing strengths in process design and yield optimization.",
      "B) By manufacturing SoCs on nodes that are only one step behind the state of the art, Intel can reduce its costs and increase its profit margins, which would give it a significant advantage over its competitors. This approach would allow Intel to focus on other areas of the business, such as software and services.",
      "C) If Intel focuses on manufacturing SoCs on nodes that are only two steps behind the state of the art, it will be able to maintain its current market share without significant investment in new technology. This approach would allow Intel to continue to leverage its existing strengths in process design and yield optimization, and would also allow the company to focus on other areas of the business, such as software and services.",
      "D) If Intel focuses on manufacturing SoCs on nodes that are only one or two steps behind the state of the art, it will be able to maintain its current market share while also reducing its costs and increasing its profit margins. This approach would allow Intel to continue to leverage its existing strengths in process design and yield optimization, while also investing in new technologies to stay ahead of the competition. Additionally, Intel's strong position in the x86 market would give it a significant advantage in terms of market share and revenue, allowing it to maintain its competitiveness in the market even if it is not at the forefront of process technology."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art. I think the processing is a bigger advantage than many realize. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. All else being equal a 14nm chip should be significantly faster and more efficient than the same chip at 22nm. Add in the fact that yields increase geometrically - you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you have a very appealing proposition. And then add in the fact that Intel actually has a pretty good graphics stack and IP. My point was that Intel might have a one or two process advantage over the rest of the industry at the cutting edge but that doesn't mean that they can afford to manufacture on those processes for very low margin parts. If the SoC market becomes increasingly commoditised, there isn't going to be the money to justify making them in a state of the art fab. Remember that one of the big selling points of Itanium was that it would make use of process advantages that were effectively paid for by the mainstream x86 market. That didn't quite work out in practice and Itanium processors were often well behind Xeons in process technology. paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the distribution of power-law exponents for a set of cryptocurrencies. They find that the median exponent for large positive returns is 2.78, while the median exponent for large negative returns is 3.11. However, they notice that the distribution of exponents for the top 2000 cryptocurrencies by market capitalization is shifted towards larger values compared to the top 200 cryptocurrencies. Which of the following statements best describes the relationship between the median exponents for large positive and negative returns?",
    "choices": [
      "A) The median exponent for large positive returns is always greater than the median exponent for large negative returns.",
      "B) The median exponent for large positive returns is equal to the median exponent for large negative returns.",
      "C) The median exponent for large positive returns is always less than the median exponent for large negative returns, regardless of the market capitalization of the cryptocurrencies.",
      "D) The median exponent for large positive returns is greater than the median exponent for large negative returns, but only for cryptocurrencies with market capitalization above the top 1000."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Panels (c) and (d) show the distributions of these median exponents when considering the top 2000 and the top 200 cryptocurrencies by market capitalization, respectively. We observe that the distributions of \u03b1+ and \u03b1\u2212 tend to shift toward larger values when considering the largest cryptoassets. number of return observations (between 100 and 300 days) and filtering out cryptoassets with missing observations (Supplementary Figures ). Still, it is worth noticing the existence of a few cryptoassets (9 of them) with relatively small market capitalization (ranking below the top 1000) for which the power-law hypothesis is always rejected (Supplementary Table ). Having verified that large price movements in the cryptocurrency market are generally well-described by powerlaw distributions, we now focus on the power-law exponents that typically characterize each cryptoasset. To do so, we select all exponent estimates over the entire history of each digital asset for which the power-law hypothesis is not rejected and calculate their median values for both the positive ( \u03b1+ ) and negative ( \u03b1\u2212 ) returns. The dashed lines in Fig. ) show these median values for Bitcoin where \u03b1+ = 4.50 and \u03b1\u2212 = 2.99. It is worth noticing that the variance of large price movements \u03c3 2 is finite only for \u03b1 > 3, as the integral \u03c3 2 \u223c \u221e r min r 2 p(r)dr diverges outside this interval. Thus, while the typical variance of large positive returns is finite for Bitcoin, negative returns are at the limit of not having a typical scale and are thus susceptible to much larger variations. Figure shows the probability distribution for the median power-law exponents of all cryptoassets grouped by large positive and negative returns. We note that the distribution of typical power-law exponents associated with large positive returns is shifted to smaller values when compared with the distribution of exponents related to large negative returns. The medians of these typical exponents are respectively 2.78 and 3.11 for positive and negative returns."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A medical historian studying the 18th-century London anatomical scene might infer that the dual function of a building as both domestic accommodation and a site for lecturing and dissection was a common practice among medical professionals of the time. Which of the following statements best supports this inference?",
    "choices": [
      "A) The fact that John Leake, a man-midwife, occupied a nearby building and advertised lectures on the art of making preparations for childbirth, suggesting a blurring of professional boundaries.",
      "B) The presence of a museum and a complete theatre in the same building, as described in an advertisement for the lease of 27 Craven Street, implies that the building was primarily used for educational purposes.",
      "C) The fact that Benjamin Franklin, a prominent figure in the medical community, lived in a building with a museum and a complete theatre, and was known to have been involved in the study of human dissection, suggests that he was a pioneer in the field of anatomical education.",
      "D) The description of the building at 27 Craven Street as \"a genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses\u2026consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences\" implies that the building was primarily used for domestic purposes, with the museum and theatre serving as secondary facilities."
    ],
    "correct_answer": "D)",
    "documentation": [
      "http://babel.hathitrust.org/cgi/pt?id=wu.89072985302;view=1up;seq=4;size=150 [33] The Journal of the Society of Arts, Vol. LXII, No. 3,183, (Nov. 21, 1913): 18.\nhttp://babel.hathitrust.org/cgi/pt?id=mdp.39015058422968;view=1up;seq=26\n[36] Allen, \u201cDear and Serviceable,\u201d 263-264. [37] Papers of Benjamin Franklin, 19:20. [38] Thomas Joseph Pettigrew, F. L. S., Memoirs of the Life and Writings of the Late John Coakley Lettsom With a Selection From His Correspondence, Vol. I, (London: Nichols, Son, and Bentley, 1817), 144 of Correspondence. [39] Papers of Benjamin Franklin, 19:321b. [40] Ibid., 19:314. [41] Ibid., 19:353a. [43] Simon David John Chaplin, John Hunter and the \u2018museum oeconomy\u2019, 1750-1800, Department of History, King\u2019s College London. Thesis submitted for the degree of Doctor of Philosophy of the University of London., 202. \u201cFollowing Falconar\u2019s death [1778] the lease [27 Craven Street] was advertised, and the buildings were described as:\nA genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses\u2026consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences. (Daily Advertiser, 27 August 1778)\u201d [44] Simon Chaplin, \u201cDissection and Display in Eighteenth-Century London,\u201d in Anatomical Dissection in Enlightenment England and Beyond: Autopsy, Pathology and Display, ed. Dr. Piers Mitchell, (Burlington: Ashgate Publishing Company, 2012), 108. \u201cGiven that a nearby building at 35 [ No. 26 in Franklin\u2019s time] was occupied by the man-midwife John Leake, who advertised lectures \u2013 including lessons in the art of making preparations \u2013 at his \u2018theatre\u2019 between 1764 and 1788, it is possible that some facilities were shared. In both cases, however, the buildings [Leake\u2019s residence at No. 26 and Hewson\u2019s residence next door at 27] served a dual function as domestic accommodation and as sites for lecturing and dissection.\u201d [45] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xviii."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a system of equations where the weights are given by $\\omega_1(x) = (|x|^2 +1)^\\frac{\\alpha}{2}$ and $\\omega_2(x)= g(x) (|x|^2 +1)^\\frac{\\beta}{2}$, where $g(x)$ is positive except at a point, smooth, and $\\lim_{|x| \\rightarrow \\infty} g(x) = C \\in (0,\\infty)$. If the advection equation is used with $\\omega_1$ and $\\omega_2$ as weights, what is the condition on $N$ that guarantees the existence of a stable sub-solution for the equation $(G)$?",
    "choices": [
      "A) If $N + \\alpha - 2 > 0$, then there is no stable sub-solution for $(G)$.",
      "B) If $N + \\alpha - 2 < 4(\\beta-\\alpha+2)$, then there is a stable sub-solution for $(G)$.",
      "C) If $N + \\alpha - 2 < \\frac{2(\\beta-\\alpha+2)}{p-1} \\left( p+\\sqrt{p(p-1)} \\right)$, then there is a stable sub-solution for $(G)$.",
      "D) If $N + \\alpha - 2 > 0$ and $N < 1 + \\frac{2}{p-1} \\left( p+\\sqrt{p(p-1)} \\right)$, then there is a stable sub-solution for $(G)$."
    ],
    "correct_answer": "D)",
    "documentation": [
      "If one takes $ \\omega_1=\\omega_2=1$ in the above corollary, the results obtained for $(G)$ and  $(L)$,  and for some values of $p$ in $(M)$, are optimal, see \\cite{f2,f3,zz}. We now drop all monotonicity conditions on $ \\omega_1$.\n\n\\begin{cor} \\label{po} Suppose  $ \\omega_1 \\le C \\omega_2$ for big $x$, $ \\omega_2 \\in L^\\infty$, $ | \\nabla \\omega_1| \\le C \\omega_2$ for big $x$.\n\\begin{enumerate} \\item  There is no stable sub-solution of $(G)$ if $ N \\le 4$.\n\n\\item  There is no positive stable sub-solution of $(L)$ if $$N<1+\\frac{2}{p-1} \\left( p+\\sqrt{p(p-1)}  \\right).$$\n\n\\item There is no positive super-solution of $(M)$ if $$N<1+\\frac{2}{p+1} \\left( p+\\sqrt{p(p+1)}  \\right).$$\n\n\\end{enumerate}\n\n\\end{cor}\n\nSome of the conditions on $ \\omega_i$ in Corollary \\ref{po} seem somewhat artificial. If we shift over to the advection equation (and we take $ \\omega_1=\\omega_2$  for simplicity)\n\\[ -\\Delta u + \\nabla \\gamma \\cdot \\nabla u = f(u), \\] the conditions on $ \\gamma$ become: $ \\gamma$ is bounded from below and has a bounded gradient. In what follows we examine the case where $ \\omega_1(x) = (|x|^2 +1)^\\frac{\\alpha}{2}$ and $ \\omega_2(x)= g(x) (|x|^2 +1)^\\frac{\\beta}{2}$,  where $ g(x) $ is positive except at say a point, smooth and where $ \\lim_{|x| \\rightarrow \\infty} g(x) = C \\in (0,\\infty)$. For this class of weights we can essentially obtain optimal results. \\begin{thm} \\label{alpha_beta}   Take $ \\omega_1 $ and $ \\omega_2$ as above. \\begin{enumerate}\n\n\\item If $ N+ \\alpha - 2 <0$ then there is no stable sub-solution for $(G)$, $(L)$ (here we require it to be positive) and in the case of $(M)$ there is no positive  stable  super-solution. This case is the trivial case, see Remark \\ref{triv}. \\\\\n\n\n\n\\textbf{Assumption:} For the remaining cases we assume that $ N + \\alpha -2 > 0$.\n\n  \\item If  $N+\\alpha-2<4(\\beta-\\alpha+2)$ then there is no  stable sub-solution for $ (G)$.\n\n\\item If $N+\\alpha-2<\\frac{ 2(\\beta-\\alpha+2)   }{p-1} \\left( p+\\sqrt{p(p-1)}  \\right)$ then there is  no positive stable sub-solution of $(L)$.\n\n\\item If $N+\\alpha-2<\\frac{2(\\beta-\\alpha+2)   }{p+1} \\left( p+\\sqrt{p(p+1)}  \\right)$ then there is no positive stable super-solution of $(M)$.\n\n\\item Further more 2,3,4 are optimal in the sense if $ N + \\alpha -2 > 0$ and the remaining inequality is not satisfied (and in addition we assume we don't have equality in the inequality) then we can find a suitable function $ g(x)$ which satisfies the above properties and a stable sub/super-solution $u$ for the appropriate equation."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When designing a composite fuselage, what is the primary consideration for ensuring the aft bulkhead is properly aligned with the canopy frame?",
    "choices": [
      "A) The aft bulkhead should be positioned at a 45-degree angle to the canopy frame to maximize structural integrity.",
      "B) The aft bulkhead should be aligned with the aft end of the canopy frame, taking into account the curvature of the fuselage.",
      "C) The aft bulkhead should be positioned just forward of the lower-front of the vertical fin, regardless of the canopy frame's shape.",
      "D) The aft bulkhead should be shaped to closely match the aft end of the canopy frame, using a straight edge and the trailing edge of the horizontal stabilizer as a guide."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo. After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade. After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic hand equipped with force sensors on its fingertips is used to manipulate objects in a dynamic environment. The force controller uses a framework that generates grasp postures based on the forces measured on the robot's fingertips. In a recent experiment, the robot successfully picked up a bottle, transported it, and placed it down on a desk. However, during the task, the robot's average normal force applied by all fingers exceeded the threshold of 10 N, causing the object to slip. To address this issue, the force controller needs to adjust the grasp size. What is the most likely reason for the robot's average normal force to exceed the threshold, and how would the force controller adjust the grasp size to prevent the object from slipping?",
    "choices": [
      "A) The robot's fingertips are not calibrated properly, resulting in inaccurate force measurements.",
      "B) The robot's grasp size is too large, causing the object to be lifted too high and the normal force to exceed the threshold.",
      "C) The robot's posture mapping function is not properly configured, resulting in an incorrect finger configuration that causes the object to slip.",
      "D) The robot's force controller is using a fixed grasp size that is not adjusted based on the object's weight and the robot's grip strength, leading to an excessive normal force."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Paper Info\n\nTitle: Force Feedback Control For Dexterous Robotic Hands Using Conditional Postural Synergies\nPublish Date: Unkown\nAuthor List: Dimitrios Dimou, Jos\u00e9 Santos-Victor, Plinio Moreno\n\nFigure\n\nFig. 1.Example of modeling the contacts and friction during manipulation. Fig. 2. Schematic representation of the proposed force controller. The input is the state (GRASP or RELEASE) and the force readings. Based on that the grasp size is adjusted by a value C and is given to the posture mapping function along with the desired grasp type. A finger configuration is then generated and commanded to the robot. Fig. 3. Our control algorithm in Python-like pseudocode.\nFig. 4. Our first experiment. The robot picks up a bottle, transports it, and places down on the desk. In the bottom part of the figure, you can see the control signals during this task. Fig. 5.The household objects used in our experiments. Under the pictures of the execution you can see the signals recorded by the controller: the average normal force applied by all fingers (blue line), the thresholds f threshold high n .(purple dashed line) and f threshold low n.(yellow dashed line), the average tangential force (green), and the grasp size used in each time-step (red).The task is divided four stages: 1) (red part) the initial grasp of the object, in this stage the force controller closes the grasp until the applied normal\nFig.6.In the upper row of images, you can see our second experiment. The robot picks up the chips can, rotates it 90 degrees, and places back down. In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp. abstract\n\nWe present a force feedback controller for a dexterous robotic hand equipped with force sensors on its fingertips. Our controller uses the conditional postural synergies framework to generate the grasp postures, i.e. the finger configuration of the robot, at each time step based on forces measured on the robot's fingertips."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The CDF and D\u00d8 collaborations have reported different results for the $\\Delta m_d$ value in semileptonic $B$ decays. The CDF result is based on fully reconstructed decays, while the D\u00d8 result is based on partially reconstructed decays. However, both collaborations have also reported results for the $\\Delta m_d$ value in semileptonic $B_s$ decays. Which of the following statements is most consistent with the data?",
    "choices": [
      "A) The CDF result for semileptonic $B$ decays is more precise than the D\u00d8 result due to its use of fully reconstructed decays, which reduces the systematic uncertainty.",
      "B) The D\u00d8 result for semileptonic $B$ decays is more precise than the CDF result due to its use of opposite-side muon tagging, which reduces the flavor tagging uncertainty.",
      "C) The CDF result for semileptonic $B_s$ decays is more precise than the D\u00d8 result due to its use of a more efficient reconstruction algorithm, which reduces the statistical uncertainty.",
      "D) The D\u00d8 result for semileptonic $B$ decays is more consistent with the CDF result for semileptonic $B_s$ decays, given that both collaborations have reported similar systematic uncertainties."
    ],
    "correct_answer": "D)",
    "documentation": [
      "As a benchmark for future $B_s$ oscillation measurement, both groups\nstudy  $B_d$ mixing, gaining an understanding of the different components\nof a $B$ mixing analysis (sample composition, flavor tagging, vertexing,\nasymmetry fitting). For a sample of partially reconstructed decays\n$B\\rightarrow D^*(2010)^+\\mu^-X$, D\\O\\ obtains \n$\\Delta m_d = 0.506 \\pm 0.055 (stat) \\pm  0.049 (syst))$ ps$^{-1}$ and\n$\\Delta m_d = 0.488 \\pm 0.066 (stat) \\pm  0.044 (syst))$ ps$^{-1}$\nwhen employing  opposite side muon tagging and the same side tagging,\nrespectively. The CDF result for semileptonic channels is\n$\\Delta m_d = 0.536 \\pm 0.037 (stat) \\pm  0.009 (s.c.) \\pm 0.015 (syst)$ ps$^{-1}$.\nCDF also reports a result on $B$ oscillations using fully reconstructed\ndecays:\n$\\Delta m_d = 0.526 \\pm 0.056 (stat) \\pm  0.005 (syst))$ ps$^{-1}$.\n\nReconstructing $B_s$ decays into different final states is another\nimportant\n step in the ${B_s^0}$ -${\\overline{B}_s^0}$ ~mixing analysis. Thanks to the  large muon and tracking coverage,   D\\O\\ is accumulating\na  high statistics sample of semileptonic $B_s$ decays. D\\O\\ reconstructs the $B_s \\rightarrow D^+_s \\mu^- X$ decays, with\n$D^+_s \\rightarrow \\phi \\pi^+ $ and\n$D^+_s \\rightarrow K^* K^+ $,\nat a rate of $\\approx$ 40(25) events per pb$^{-1}$,  respectively. Figure \\ref{fig:d0_bsdsphipi} shows the mass distribution of the\n$D^+_s \\rightarrow \\phi \\pi$ candidates. \\begin{figure}[htb]\n\\vspace*{-5mm}\n\\includegraphics[height=0.3\\textheight,width=8.0cm]  {blds-250.eps}\n\\vspace*{-1.2cm}\n\\caption{  $D^+_s \\rightarrow \\phi \\pi^+$  signal. (D\\O)}\n\\label{fig:d0_bsdsphipi}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\vspace*{-10mm}\n\\hspace*{-4mm}\n\\includegraphics[height=0.35\\textheight,width=7.9cm]  {cdf_Bs-DsPi-PhiPi.eps}\n\n\\vspace*{-1.0cm}\n\\caption{ $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$  signal. (CDF)}\n\\label{fig:cdf_bsdsphipi}\n\\end{figure}\n\n\nCDF has clean signals for fully hadronic, flavor-specific  $B_s$ decays,\nproviding the best sensitivity to $B_s$ oscillations at high\n$\\Delta m_s$. Figure \\ref{fig:cdf_bsdsphipi} shows the signal for\nthe best channel, $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$.\n\n\\clearpage\n\n\n\\subsection{Rare decays}\n\nThe purely leptonic decays $B_{d,s}^0 \\rightarrow \\mu^+\n\\mu^-$ are flavor-changing neutral current (FCNC) processes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new numerical method for solving the neutron transport equation is being developed, which combines elements of the Flux-Particle Separation Algorithm (FPSA) and the Newton Flux Particle Acceleration (NFPA) method. The new method, dubbed \"FPSA-NFPA\", uses a similar stiffness matrix setup as NFPA, but with a different convergence criterion. In the FPSA-NFPA method, the stiffness matrix is inverted only once, but the algorithm requires multiple sweeps in the transport equation to achieve convergence. Which of the following statements best describes the key advantage of the FPSA-NFPA method?",
    "choices": null,
    "correct_answer": "B)",
    "documentation": [
      "Iteration is done until a convergence criterion is met. The main advantage of setting up the LO equation in this fashion is that the stiffness matrix for LO needs to be setup and inverted \\textit{only once}, just as with FPSA \\cite{JapanFPSA, japanDiss}. This has a large impact on the method's performance. A flowchart of this algorithm is shown in \\cref{Nalgorithm}.\n\n\\begin{figure}[H]\n\\centering\n\\begin{tikzpicture}[node distance = 3cm, auto]\n   \n    \\node [block] (init) {Initial guess of flux moments};\n    \\node [cloud_HO, right of=init, node distance=4cm] (HOm) {HO};\n    \\node [cloud_LO, below of=HOm, node distance=2cm] (LOm) {LO};\n    \\node [HO, below of=init] (transport) { One sweep in transport equation};\n    \\node [decision, below of=transport,node distance=4cm] (decide) {Flux moments converged?}; \\node [LO, left of=decide, node distance=4cm] (dterm) {Solve for consistency term}; \\node [LO, left of=dterm, node distance=3cm] (MFP) {Solve for FP angular flux}; \\node [LO, above of=MFP, node distance=4cm] (moments) {Convert angular flux to moments};\n    \\node [block, right of=decide, node distance=4cm] (stop) {Stop};\n   \n    \\path [line] (init) -- (transport);\n    \\path [line] (transport) -- (decide);\n    \\path [line] (decide) -- node {no} (dterm);\n    \\path [line] (dterm) -- (MFP);\n    \\path [line] (MFP) -- (moments);\n    \\path [line] (moments) -- (transport);\n    \\path [line] (decide) -- node {yes}(stop);\n\\end{tikzpicture}\n\\caption{NFPA algorithm}\n\\label{Nalgorithm}\n\\end{figure}\n\n\\section{Numerical Experiments}\\label{sec3} In \\cref{sec31} we describe the discretization methods used to implement the algorithms. In \\cref{sec32} we provide numerical results for 2 different choices of source $Q$ and boundary conditions. For each choice we solve the problem using 3 different scattering kernels, applying 3 different choices of parameters for each kernel. We provide NFPA numerical results for these 18 cases and compare them against those obtained from FPSA and other standard methods. All numerical experiments were performed using MATLAB."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Carli's behavior of making up a story to get her friend's sister off the computer can be seen as a manifestation of her underlying behavioral issues. Considering her history of behavioral problems and her current situation, what is a possible underlying reason for Carli's actions?",
    "choices": [
      "A) She is trying to assert her dominance over her friend's sister by taking control of the computer.",
      "B) She is experiencing a temporary lapse in impulse control due to her emotional state, which is exacerbated by her recent conflicts with friends.",
      "C) She is trying to protect her friend's sister from an external threat, as she has done in the past when she made up a story about someone being at the door.",
      "D) She is exhibiting a classic case of \"social learning theory\" in action, where she is imitating the behavior of others, including her friend's sister, who may have been using the computer to avoid doing her homework."
    ],
    "correct_answer": "D)",
    "documentation": [
      "He\u2019s highly intelligent and successful, a pattern seeker, has a tendency to focus on the project to hand to the total exclusion of all else for as long sit takes (work or home) socially awkward (has learned coping strategies), sensitive to loud noise, high anxiety with control strategies, black and white thinking etc. He\u2019s currently not working and I\u2019ve seen a slow withdrawal over the last 6 weeks, including the need to \u2018escape\u2019 and leave a situation at least once. He also has a bipolar ex overseas who has primary custody one daughter where there has been ongoing patterns of drama which has recently increased. Over the past couple of months (since stopping work and drama increase) I\u2019ve gone from being \u2018wonderful\u2019 in his eyes to him now being sorry and not having the \u2018urge\u2019 to spend close/intimate time with me and offering friendship. Since he shared that with me in a message he\u2019s stonewalled and has retreated to the safety of minimal messages and talks about not knowing what best to say and not being able to find the right words somehow. He\u2019s a good kind man who I feel is struggling. I\u2019m concerned about his anxiety and possibly the risk of depression. I\u2019m fairly resilient and whilst i\u2019m disappointed he doesn\u2019t want to pursue a relationship with me, i\u2019m concerned for him and his well being. One of his very few close friends is also just leaving the country to live overseas. The strategy I\u2019ve used so far is simply to back off and give him space. I\u2019ve asked to take him up on an original offer he made to talk but haven\u2019t pushed it. I also haven\u2019t been aggressive or accusatory in the few messages i\u2019ve sent. Any advise you could give would be greatly appreciated,\nCarli who is 10 years old and has had behavioral issues her whole life. The other night she came home very upset after having a conflict with a friend. She was at her friend's house and her and her friend wanted to get on the computer and the older sister was using it. Carli made up a story that someone was at the door to get the older sister off the computer. Her friend didn't understand that she was making up a story to get the sister off the computer."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A philosopher argues that the concept of free will is incompatible with determinism because if our choices are determined, then we cannot be held morally responsible for them. However, some libertarians claim that free will is compatible with determinism, as long as our choices are not entirely determined by prior causes. If a person's decision to donate to charity is influenced by their genetic predisposition, but also by their personal values and experiences, can they still be said to have free will?",
    "choices": [
      "A) Free will requires the absence of any external influence, including genetic predisposition.",
      "B) If our choices are determined by prior causes, then we are not morally responsible for them, regardless of the extent to which those causes are internal or external.",
      "C) Free will is compatible with determinism as long as our choices are not entirely determined by prior causes, and can be influenced by a combination of internal and external factors.",
      "D) If a person's decision to donate to charity is influenced by both their genetic predisposition and personal values, then they can only be said to have free will if they are able to override their genetic predisposition through a deliberate choice."
    ],
    "correct_answer": "D)",
    "documentation": [
      "[/quote]\nDragonFly \u00bb April 21st, 2018, 3:57 pm wrote: Yes, as I said, some is indeterminate, so there is no ignoring. Incorrect. You did not say \"some is indeterminate.\" So either you do not write well, cannot understand the logic of your own words, or you make up things as an excuse to attack other people. In fact, this can be identified with a logical fallacy. \"Whatever is indeterminate diminishes our modeling\" means our modeling is diminished IF there is anything indeterminate. If A then B does not allow you affirm A, so by equating these two you have committed a logical fallacy. Furthermore it is amazing how far out on a limb you go to concoct such an attack. You said, \"we cannot know if everything is deterministic,\" which is utterly inconsistent with a clam that \"some is indeterminate,\" because if some is indeterminate then you would know that it is NOT deterministic. DragonFly \u00bb April 21st, 2018, 3:57 pm wrote: Total libertarians do claim that they are first cause, self made people at every instant. The philosophers who claim that we have free actions are called libertarians. The radical opposition that libertarians pose to the determinist position is their acceptance of free actions. Libertarians accept the incompatibility premise that holds agents morally responsible for free actions. Incompatibilism maintains that determinism is incompatible with human freedom. Libertarians accept that there are free actions, and in doing so, believe that we are morally responsible for some of our actions, namely, the free ones. The libertarian ONLY claims that we do have free will actions and affirm the incompatibility of determinism with free will. There is no claim here that free will is absolute, inviolable, and applies to every action and thus that people are \"self made at every instance. \"\nThus in the following it is clear you are burning an absurd strawman. DragonFly \u00bb April 21st, 2018, 3:57 pm wrote: How does this work? A theory of conscious intentions happening without any underlying physical processes ('you') behind them is the toughest sell of all proposals on the will, so it's no wonder that this 'being free of the will' can't be shown."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When building the fuselage of a Rand Robinson KR-2S plane, a common issue arises when the completed fuselage sides are laid into position to form the fuselage box section. The sides are found to be curved, forming a \"banana\" shape, despite hours of building them flat. What is the primary reason for this issue?",
    "choices": [
      "A) The builder failed to properly clamp the sides during construction, causing them to warp.",
      "B) The plans show the finished form of the plane, but the distances given by measuring the profile drawing are not accurate, resulting in a curved shape.",
      "C) The builder used preformed fiberglass parts that were not designed to fit together perfectly, causing the curved shape.",
      "D) The problem arises from the fact that the plans show the \"projected\" form of the plane, which is foreshortened, and the builder needs to \"develop\" the \"true\" distances and shape of the flat panel to ensure the longerons lay flat."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go \"perfectly.\" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane. This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes. While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying \"banana\" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built. First understand that the plans show the finished form of the plane. They show the \"projected\" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are \"foreshortened\" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to \"develop\" the \"true\" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a function $f(x)$ defined on the interval $[0, \\infty)$, where $f(x)$ is non-negative and satisfies the following properties:\n\n(a) For all $x \\geq 1$, $f(x) \\leq f(x-1)$.\n(b) For all $x \\geq 2$, $f(x) \\leq f(x-1)^2$.\n(c) For all $x \\geq 3$, $f(x) \\leq f(x-1)^3$.\n\nUsing these properties, prove that there exists a constant $C$ such that for all $x \\geq 3$, $f(x) \\leq C x^{-2}$.",
    "choices": [
      "A) For all $x \\geq 3$, $f(x) \\leq f(x-1)^2$.",
      "B) For all $x \\geq 3$, $f(x) \\leq f(x-1)^3$.",
      "C) For all $x \\geq 3$, $f(x) \\leq f(x-1)^2 + f(x-1)$.",
      "D) For all $x \\geq 3$, $f(x) \\leq C x^{-2}$."
    ],
    "correct_answer": "D)",
    "documentation": [
      "So if we assume that $ \\nabla \\omega_1 \\cdot x \\le 0$ for big $x$ then we see that this last term is non-positive and hence we can drop the term. The the proof is as before but now we only require that $ \\lim_{R \\rightarrow \\infty} I_G=0$.\n\n (2). Suppose that $ u >0$ is a stable sub-solution of $(L)$  and so (\\ref{shit}) holds for all $  p - \\sqrt{p(p-1)} <t< p + \\sqrt{p(p-1)}$. Now we wish to use monotonicity to drop the term from (\\ref{shit}) involving the term $ \\nabla \\omega_1 \\cdot \\nabla \\phi$.      $ \\phi$ is chosen the same as in (1)  but here one notes that the co-efficient for this term changes sign at $ t=1$ and hence by restriction $t$ to the appropriate side of 1 (along with the above condition on $t$ and $\\omega_1$) we can drop the last term depending on which monotonicity we have and hence to obtain a contraction we only require that $ \\lim_{R \\rightarrow \\infty} I_L =0$. The result for the non-existence of a stable super-solution is similar be here one restricts $ 0 < t < \\frac{1}{2}$.\n\n\n(3). The proof here is similar to (1) and (2) and we omit the details. \\hfill $\\Box$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\textbf{Proof of Corollary \\ref{thing}.} We suppose  that $ \\omega_1 \\le C  \\omega_2$  for big $ x$, $ \\omega_2 \\in L^\\infty$,  $ \\nabla \\omega_1(x) \\cdot  x \\le 0$ for big $ x$.     \\\\\n(1). Since $ \\nabla \\omega_1 \\cdot x \\le 0$ for big $x$ we can apply Theorem \\ref{mono} to show the non-existence of a stable solution to $(G)$.   Note that  with the above assumptions on $ \\omega_i$ we have that\n\\[ I_G \\le \\frac{C R^N}{R^{4t+2}}.\\]  For $ N \\le 9$  we can take $ 0 <t<2$  but close enough to $2$ so the right hand side goes to zero as $ R \\rightarrow \\infty$.\n\nBoth (2) and (3) also follow directly from applying Theorem \\ref{mono}. Note that one can say more about (2) by taking the multiple cases as listed in Theorem \\ref{mono} but we have choice to leave this to the reader. \\hfill $ \\Box$\n\n\n\\textbf{Proof of Corollary \\ref{po}.} Since we have no monotonicity conditions now we will need both $I$ and $J$ to go to zero to show the non-existence of a stable solution."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A paramecium's ability to adapt to its environment is often attributed to its capacity for self-organization. However, this process is not solely driven by internal mechanisms, but also influenced by external factors such as temperature and nutrient availability. Which of the following statements best describes the relationship between the paramecium's behavior and its environment?",
    "choices": [
      "A) The paramecium's behavior is solely determined by its internal state, and external factors play no role in its decision-making process.",
      "B) The paramecium's behavior is a direct result of its internal mechanisms, and external factors are merely a byproduct of its self-organization.",
      "C) The paramecium's behavior is influenced by both internal and external factors, but the internal mechanisms are the primary drivers of its decision-making process.",
      "D) The paramecium's behavior is a result of its environment, and its internal mechanisms are simply a byproduct of its adaptation to its surroundings."
    ],
    "correct_answer": "D)",
    "documentation": [
      "A paramecium is not full of Schnitt. It is not measuring or having goals or anything else. It is an automaton. To think otherwise would be to invite some sort of Bergsonian \"elan vital\" or other dualistic essence. The problem with the term \"observation\" is that it's prejudicial in common parlance. It implies some sort of sentient observer. It implies a subjective aspect. So a more neutral term would be needed when a microbe registers a chance in its environment and contracts or expands or fires up the cilia or dumps the vacuoles or whatever. Or when a Bose Einstein condensate loses its coherence in a wet noisy puddle. Braininvat \u00bb April 24th, 2018 , 12:52 pm wrote: It seems likely a paramecium does no representing to a self, and is pretty much a cellular machine lacking sentience. But it is not a machine for the simple reason that it is not a product of design. The only reasons for which it does things are its own reasons. It is a product of self organization, and the learning process which is evolution. I certainly agree with the term \"biological machinery,\" which is to say that there is no reason to distinguish things simply on the basis that one uses the interactions of organic chemistry. Thus I think the locus of difference between the living organism and the machine has to do with origins whether it is by design or by learning, evolution, and self-organization. Braininvat \u00bb April 24th, 2018, 12:52 pm wrote: The problem with the term \"observation\" is that it's prejudicial in common parlance. It implies some sort of sentient observer. It implies a subjective aspect. So a more neutral term would be needed when a microbe registers a chance in its environment and contracts or expands or fires up the cilia or dumps the vacuoles or whatever. But the problem with this is that the prejudice in language goes both ways with the presumption of an uncrossable divide between the sentient and the non-sentient, when all the evidence points to a continuum going all the way from the non-living to the living to the sentient."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher uses a deep structure prior (DSP) framework to complete fragmented contours in images. The framework is trained using a generator network that is solely trained on the single incomplete input image. However, the researcher also uses a pre-trained hour-glass model as a feature extractor to improve the performance of the DSP framework. What is the primary advantage of using a pre-trained hour-glass model in this context?",
    "choices": [
      "A) It allows the model to learn the optimal proportion coefficient for reconstruction loss, which is essential for contour completion.",
      "B) It enables the model to leverage the knowledge of the pre-trained model to improve the performance of the generator network.",
      "C) It provides a way to transfer the learned features from the pre-trained model to the generator network, which can help to improve the quality of the completed contours.",
      "D) It allows the model to learn the perceivable illusory contours and connect the fragmented pieces using a generator network that is solely trained on the single incomplete input image, without the need for a pre-trained model."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In training, we use the MSE loss between the degraded image and the output of the network, and we optimize the loss using the ADAM optimizer and a learning rate equal to 0.01 . In our experiments, we also used \u03b1 = 0.15 as an optimal proportion coefficient for reconstruction loss. Conclusion\n\nIn this work, we introduced a novel framework for contour completion using deep structure priors (DSP). This work offers a novel notion of a maskless grouping of fragmented contours. In our proposed framework, we introduced a novel loss metric that does not require a strict definition of the mask. Instead, it lets the model learn the perceivable illusory contours and connects those fragmented pieces using a generator network that is solely trained on just the single incomplete input image. Our model does not require any pre-training which demonstrates that the convolutional architecture of the hour-glass model is able to connect disconnected contours. We present an extended set of experiments that show the capability of our algorithm. We investigate the effect of each parameter introduced in our algorithm separately and show how one could possibly achieve the best result for their problem using this model. In future work, we plan to extend this model and try to see how it performs with real images. In particular, we want to determine whether we can inpaint real-world photographs while retaining perceptually aware scene structures. The importance of shape in perception by deep neural networks has been highlighted in many adversarial examples to appearance-based networks . The outcome of this work has strong potential to impact the designing and implementation of models that are robust to such perturbations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the opioid epidemic in Ohio found that the majority of oxycodone prescriptions were written by doctors who had never met their patients. What is the most likely reason for this phenomenon?",
    "choices": [
      "A) Many of the doctors in question were simply overwhelmed with the number of patients they needed to see, leading them to rely on standardized prescriptions.",
      "B) The doctors had a common misconception that oxycodone prescriptions were less likely to be diverted to the black market if they were written in bulk.",
      "C) The doctors were aware of the risks associated with oxycodone addiction, but believed that the benefits of the medication outweighed these risks, leading them to prescribe it liberally.",
      "D) The doctors had a prior agreement with the Ohio distributors to ensure a steady supply of oxycodone, which was then dispensed to patients without proper verification of their medical need."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Superior Pharmacy not only filled oxycodone prescriptions for pain clinics, it shared waiting room space with a pain clinic in a Temple Terrace strip mall outside Tampa. Neither Masters nor Superior had so much as Googled the background of pain clinic doctors writing those prescriptions, the DEA later said. Had they done so, the DEA dryly noted, they \u201cwould likely have come across a press release\u201d announcing one of the doctors had been arrested and charged with trafficking in prescription drugs. Hundreds of thousands of oxycodone pills were sent from Ohio distributors to Florida pharmacies. Unknown thousands of pills headed right back up to Ohio. When Ohio police burst into Christopher Thompson\u2019s home outside Columbus, they found an assault rifle, $80,000 in cash and oxycodone from his Florida deals. A construction worker whose own pill habit started at age 14, Thompson oversaw a ring of 15 Ohio buyers who traveled to Florida to pick up oxycodone to resell in Central Ohio. Two hours to the west in Martin\u2019s Ferry, David L. Kidd orchestrated a ring of buyers traveling to West Palm Beach and Central Florida to pick up oxycodone for resale on the streets of eastern Ohio and West Virginia. Doctors and pharmacies from Florida were complicit with Kidd\u2019s ring in fueling Ohio\u2019s opioid epidemic, wrote the U.S. attorney for West Virginia after Kidd\u2019s 2011 arrest: \u201cThe steady flow of pain pills into the Ohio Valley from Florida must stop.\u201d Driving To Pick Up Death By Rx\nWith more drugs came more deaths, in January 2010, say police, Fort Lauderdale pathologist Dr. Lynn Averill started a seven-month oxycodone shopping spree, buying 437,880 oxycodone pills from drug distributors. The same month, Matthew Koutouzis drove from Toms River, N.J., to see Averill in her Broward County pain clinic. The 26-year-old collected prescriptions for 390 pills and overdosed two days later. Brian Moore traveled 13 hours from his Laurel County, Ky., home to see Averill. He left with prescriptions for 600 pills and also overdosed within 48 hours."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic arm is used to grasp and manipulate objects in a manufacturing environment. The control architecture of the robotic arm is modular, allowing for easy changes to the synergy grasp mapping component to control different precision grasp types. However, experiments have revealed that the controller fails to stabilize the object when rotational slip occurs, and hardware limitations such as slow update rates and noise in the force measurements can cause the object to fall. To address these issues, the manufacturer plans to incorporate additional sensing modalities, such as vision, to improve the overall performance of the robotic arm.",
    "choices": [
      "A) The manufacturer's solution to the problem is to increase the update rate of the force measurements, which will improve the stability of the object.",
      "B) The robotic arm's control architecture is not modular, and the synergy grasp mapping component is fixed to control only one precision grasp type.",
      "C) The manufacturer's plan to incorporate vision as an additional sensing modality will eliminate the need for the controller to stabilize the object when rotational slip occurs.",
      "D) The robotic arm's performance can be improved by incorporating additional sensing modalities, such as vision, to alleviate the problems caused by hardware limitations and rotational slip."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In addition, the control architecture is modular, so the synergy grasp mapping component can be easily changed in order to control several precision grasp types. However, our experiments also revealed various limitations of our controller. For example our method fails to stabilize the object when rotational slip occurs. In addition hardware limitations such as, slow update rates and noise in the force measurements can create problems that result in the object falling. In future work we plan to incorporate additional sensing modalities, such as vision to alleviate some of these issues."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the efficiency of a new algorithm for solving complex optimization problems. The algorithm's performance was verified using the fourth-order Runge-Kutta method, which is known for its high accuracy. However, the algorithm's response is sensitive to the initial conditions, and the researcher wants to investigate how the response changes when the initial conditions are non-zero. Which of the following statements best describes the researcher's next step?",
    "choices": [
      "A) The researcher should re-run the simulation with the same initial conditions but with a different time step to see if the results change.",
      "B) The researcher should assume that the algorithm's response will be the same under both zero and non-zero initial conditions, since the fourth-order Runge-Kutta method is a deterministic algorithm.",
      "C) The researcher should investigate the algorithm's response under non-zero initial conditions using a different method, such as the Euler method, which is less accurate but faster.",
      "D) The researcher should re-run the simulation with the same initial conditions but with a different initial guess to see if the results change."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The efficiency of the proposed method was verified by the fourth-order Runge-Kutta method. This paper only computes the response under zero initial conditions. The response under non-zero initial conditions will be investigated in our future work."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "An Agency Spotter user is notified that their account has been flagged for potential security breaches due to a series of suspicious login attempts. The user is required to verify their account information to ensure its security. However, upon reviewing their account settings, the user discovers that they have been assigned an administrator to their verified Agency Spotter account, which seems to contradict the terms of service that state they must maintain only one Agency Spotter account at any given time. What is the most likely reason for this apparent contradiction?",
    "choices": [
      "A) The administrator was assigned to the user's account as part of a routine verification process, and the user's account was flagged for security breaches due to a third-party software issue.",
      "B) The user's account was compromised by a malicious third-party, and the administrator was assigned to the account to prevent further unauthorized access.",
      "C) The user's account was verified as an agency, and the administrator was assigned to the account as a result of the verification process, which is allowed under the terms of service.",
      "D) The user's account was flagged for security breaches due to a failure to comply with the terms of service, specifically the requirement to maintain only one Agency Spotter account at any given time, and the administrator was assigned to the account to monitor its activity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "You are entirely responsible for maintaining the confidentiality of the information you hold for your account, including your password, and for any and all activity that occurs under your account until you close down your account or prove that your account security was compromised due to no fault of your own. To close your account, please email us at [email protected] You agree to notify Agency Spotter immediately of any unauthorized use of your account or password, or any other breach of security. You may be held liable for losses incurred by Agency Spotter or any other user of or visitor to the Site due to someone else using your Agency Spotter ID, password or account as a result of your failing to keep your account information secure and confidential. You may not use anyone else\u2019s Agency Spotter ID, password or account at any time without the express permission and consent of the holder of that Agency Spotter ID, password or account. Agency Spotter cannot and will not be liable for any loss or damage arising from your failure to comply with these obligations. Agency Spotter may verify Agency Accounts to confirm that such accounts meet Agency Spotter\u2019s minimum requirements to be an agency, as the same may be modified or amended from time to time, and may assign an administrator to such verified Agency Account. (b) To eligible to use the Site and the Services, you must meet the following criteria and represent and warrant that you: (i) are at least 18 years of age; (ii) are not currently restricted from the Site or Services, and are not otherwise prohibited from having an Agency Spotter account, (iii) are not a competitor of Agency Spotter or are not using the Site or Services for reasons that are in competition with Agency Spotter, (iv) will only maintain one Agency Spotter account at any given time, (v) have full power and authority to enter into this Agreement and doing so will not violate any other agreement to which you are bound, (vi) will not violate any rights of Agency Spotter, including intellectual property rights such as copyright and trademark rights, and (vii) agree to provide at your cost all equipment, software and internet access necessary to use the Site or Services."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A 65-year-old patient presents with chest pain and is diagnosed with a myocardial infarction. The patient's EKG shows a normal sinus rhythm, but the nuclear stress test reveals a significant reduction in myocardial perfusion. Which of the following is the most likely explanation for the patient's symptoms?",
    "choices": [
      "A) The patient's myocardium is damaged due to a blocked blood supply, causing a reduction in myocardial perfusion, which is consistent with a STEMI diagnosis.",
      "B) The patient's peripheral artery disease is causing a reduction in blood flow to the heart muscle, resulting in a myocardial infarction.",
      "C) The patient's mitral valve is not closing properly, causing murmurs and a reduction in myocardial perfusion, which is consistent with a diagnosis of mitral regurgitation.",
      "D) The patient's pacemaker is malfunctioning, causing an abnormal heart rhythm and a reduction in myocardial perfusion, which is consistent with a diagnosis of atrial fibrillation."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Murmur \u2013 Noises superimposed on normal heart sounds. They are caused by congenital defects or damaged heart valves that do not close properly and allow blood to leak back into the originating chamber. MV \u2013 Mitral Valve: The structure that controls blood flow between the heart\u2019s left atrium (upper chamber) and left ventricle (lower chamber). Myocardial Infarction (MI, heart attack) \u2013 The damage or death of an area of the heart muscle (myocardium) resulting from a blocked blood supply to the area. The affected tissue dies, injuring the heart. Myocardium \u2013 The muscular tissue of the heart. New Wall-Motion Abnormalities \u2013 Results seen on an echocardiogram test report (see NWMA, below). Nitroglycerin \u2013 A medicine that helps relax and dilate arteries; often used to treat cardiac chest pain (angina). Also called NTG or GTN.\nNSR \u2013 Normal Sinus Rhythm: The characteristic rhythm of the healthy human heart. NSR is considered to be present if the heart rate is in the normal range, the P waves are normal on the EKG/ECG, and the rate does not vary significantly. NSTEMI \u2013 Non-ST-segment-elevation myocardial infarction: The milder form of the two main types of heart attack. An NSTEMI heart attack does not produce an ST-segment elevation seen on an electrocardiogram test (EKG). See also STEMI. Nuclear Stress Test \u2013 A diagnostic test that usually involves two exercise stress tests, one while you\u2019re exercising on a treadmill/stationary bike or with medication that stresses your heart, and another set while you\u2019re at rest. A nuclear stress test is used to gather information about how well your heart works during physical activity and at rest. See also: Exercise stress test, Nuclear perfusion test, MIBI. Open heart surgery \u2013 Any surgery in which the chest is opened and surgery is done on the heart muscle, valves, coronary arteries, or other parts of the heart (such as the aorta). See also CABG. Pacemaker \u2013 A surgically implanted electronic device that helps regulate the heartbeat. PAD \u2013 Peripheral Artery Disease: A common circulatory problem in which narrowed arteries reduce blood flow to the limbs, usually to the legs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the Indonesian government's shark conservation efforts have led to a significant reduction in the number of threatened shark species caught in the country's fisheries. However, the study also noted that the majority of fishers in these communities continue to rely on shark fishing as their primary source of income. What is the most likely reason for this persistence of shark fishing in these communities?",
    "choices": [
      "A) The Indonesian government's conservation efforts have been ineffective in changing fisher behavior, and many fishers are unwilling to adapt to new, more sustainable fishing methods.",
      "B) The lack of economic alternatives to shark fishing in these communities means that fishers have no choice but to continue catching sharks to support their families.",
      "C) The Indonesian government's conservation efforts have been successful in reducing the number of threatened shark species caught, but the fishers in these communities are not aware of the impact of their actions on the shark population.",
      "D) The Indonesian government's conservation efforts have been successful in reducing the number of threatened shark species caught, but the fishers in these communities are not willing to change their fishing behavior because they are not aware of the economic benefits of sustainable fishing practices."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Indonesia is the world\u2019s largest shark fishing nation [9, 14], and a global priority for shark conservation . Until recently Indonesia\u2019s shark fishery has largely functioned as de facto open-access [12, 16]. However, in the past five years the Indonesian government has demonstrated a clear commitment to shark conservation and resource management, with domestic measures put in place to implement international obligations under CITES . Exploitation of all CITES-listed species is now regulated, either through full species protection or export controls (these species are hereafter referred to as \u2018regulated\u2019 species). However, CITES only affords protection to a small number of Indonesia\u2019s 112 known shark species , of which 83 are threatened with extinction according to the IUCN Red List of Threatened Species (i.e. Vulnerable (VU), Endangered (EN) or Critically Endangered (CR) , these species are hereafter referred to as \u2018threatened\u2019 species), many of which continue to be landed throughout the country . Further, these policy measures predominantly regulate trade at the point of export, but do not necessarily influence fisher behaviour or local demand at the point of catch, such that the \u2018trickle-down\u2019 impacts on species mortality are unknown. In addition, effectively implementing species-specific shark mortality controls remains challenging due to the non-selectivity of fishing gears, and practical and cultural barriers to changing fisher preferences for certain gear-types and fishing methods. As such, existing regulations alone (e.g. Indonesian Law on Fisheries 31/2004 and its derivative regulations) will likely be insufficient to curb mortality of threatened and regulated species, as fishers must be both willing and able to change their fishing behaviour . Moreover, most of Indonesia\u2019s shark fisheries are small-scale, and in relatively poor coastal communities where there are often no legal, sustainable marine-based alternatives to shark fishing that offer similar financial returns [22, 23]."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher observes that the temporal evolution of the total entropy production (S NOMF + S NOEF) in a system with a specific-heat ratio \u03b3 = 1.09 is similar to the S NOEF profile. However, the entropy production caused by NOMF increases with reduced specific-heat ratio, whereas the entropy production caused by NOEF first reduces with decreasing specific-heat ratio and then approaches a saturation value. What can be inferred about the relationship between the specific-heat ratio and the entropy production caused by NOMF and NOEF?",
    "choices": [
      "A) The entropy production caused by NOMF is more significant than that caused by NOEF for all values of \u03b3.",
      "B) The entropy production caused by NOEF is more significant than that caused by NOMF when \u03b3 > \u03b3c, where \u03b3c \u2248 1.315.",
      "C) The entropy production caused by NOMF is more significant than that caused by NOEF when \u03b3 < \u03b3c.",
      "D) The entropy production caused by NOMF and NOEF are equal for all values of \u03b3, and the total entropy production (S NOMF + S NOEF) is a direct sum of the two."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In the first stage (t < 0.03), cases with different specific-heat ratios show various trends. At the stage where the bubble deformation is not very large, i.e., 0.03 < t < 0.06, values of \u1e60NOEF fluctuate near the average value. In the third stage (t > 0.06), evolutions of \u1e60NOEF in cases with larger specific-heat ratios show an apparent growing tendency. Differently, the values of \u1e60NOEF in cases with smaller specific-heat ratios remain almost unchanged. The influence of specific heat ratio on the \u1e60NOEF , similar with the effect on NOEF, is also affected by the heat conductivity and the temperature gradient. It can be seen that, except for the case of \u03b3 = 1.09, the larger the specific-heat ratio, the higher entropy production rate \u1e60NOEF . The temporal evolutions of \u1e60NOEF of case \u03b3 = 1.09 and case \u03b3 = 1.12 are very similar. Consequently, the specific-heat ratio increases the \u1e60NOEF by raising the temperature gradient. Further understanding can be seen in Fig. , where the entropy productions over this period are plotted. For convenience, the sum and difference between S NOMF and S NOEF are also plotted in the figure. The variation range of S NOEF is larger than that of S NOMF . It indicates that the influence of specific-heat ratio on S NOEF is more significant than that on S NOMF . Effects of specific-heat ratio on entropy production caused by NOMF and NOEF are contrary. Specifically, it can be seen that the entropy production contributed by NOMF increases with re- duced specific-heat ratio. But the entropy production caused by NOEF first reduces with decreasing specific-heat ratio and then approaches to a saturation value. The S NOEF in case \u03b3 = 1.09 is almost the same with it in case \u03b3 = 1.12. When the specificheat ratio \u03b3 is smaller than a threshold value \u03b3 c (\u03b3 c \u2248 1.315), the entropy production induced by NOEF is more significant than that caused by NOMF. However, in the case of \u03b3 > \u03b3 c , the situation reverses. The temporal evolution of the total entropy production (S NOMF +S NOEF ) is similar to the S NOEF profile."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the effects of climate change on marine ecosystems found that the Fokker-Planck equation can be used to model the population dynamics of certain species. However, the authors noted that the equation's accuracy is sensitive to the choice of diffusion coefficient. In a scenario where the diffusion coefficient is set to a value that is too high, what is likely to happen to the population growth rate of the species?",
    "choices": [
      "A) The population growth rate will increase, leading to an explosion in population size.",
      "B) The population growth rate will decrease, but the species will still be able to adapt to changing environmental conditions.",
      "C) The population growth rate will remain stable, as the species will be able to maintain a balance with its environment.",
      "D) The population growth rate will decrease, but the species will eventually go extinct due to the inability to adapt to changing environmental conditions."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The authors would also like to thank Dr.~Anil Prinja for discussions involving Fokker-Planck acceleration."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When handling situations like the one described, it's essential to consider the emotional regulation strategies that work best for the individual. In this case, the girlfriend is concerned about her boyfriend's approach to managing her daughter's outbursts. She feels that his leniency may be enabling her daughter's behavior and undermining her own emotional well-being. However, her boyfriend believes that his approach is empathetic and understanding. Which of the following statements best captures the potential consequences of the girlfriend's request to have her daughter removed from the household due to excessive crying?",
    "choices": [
      "A) The girlfriend's request may lead to increased stress and anxiety for her daughter, as being removed from the household could be perceived as punishment rather than a solution to the problem.",
      "B) The boyfriend's approach is likely to be effective in the long run, as it acknowledges and validates his daughter's emotions, which can help her develop emotional regulation skills.",
      "C) The girlfriend's request may be seen as an overreaction, as it doesn't take into account the potential impact on her daughter's attachment to her parents and the household environment.",
      "D) The girlfriend's request is reasonable, as it prioritizes the well-being and emotional regulation of her daughter, which is essential for her development and adjustment to the household."
    ],
    "correct_answer": "D)",
    "documentation": [
      "She got excited that someone was at the door and ran downstairs to answer the door. In the process of getting the door, she fell and yelled at Carli. Carli became extremely upset. She was able to control her feelings at her friend's house, but when she came home, she proceeded to cry extremely loudly for over an hour. Her dad spent most of that time with her, talking to her and trying to calm her down. After an hour, I asked him if he could please tell her to be more quiet because the other members of the household were trying to go to sleep. My question is....how do I as the girlfriend, handle this? He did not like that I asked her to be quiet. We have a rule that if she is having bad behavior, and can't calm down in 5 minutes, he takes her out of the house because her yelling doesn't stop for a long time and is very upsetting to everyone in the household. I would like to ask him to do this with this kind of situation as well. Is this a reasonable request? His thought was that she shouldn't be made to calm down, because everyone handles being upset in a different way. But, she was literally sobbing and wailing very loudly. My other question is should she have been told that if she wouldn't have lied, this wouldn't have happened? She has a history of lying and of not accepting responsibility for her actions. My boyfriend became very upset with me when I brought this up. He was being very sympathetic and understanding to her. I feel like he was giving her negative attention, and being an over indulgent parent by not putting his foot gown and saying, \"you can't carry on like this, even though you are upset\". Please let me know how we can handle these situations better. I am contacting you for help with adult AS. I am taking initiative to pre screen potential therapists to help my current boyfriend get therapy and help with Adult AS. He has seen many therapists, but it seems like they aren\u2019t really helping him with his problems. They don\u2019t seem to understand how his (undiagnosed) AS would affect therapy approaches."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the thermalization of a spin glass model using the Metropolis algorithm and parallel tempering found that the performances of the model are independent of the probability distribution of the couplings. However, the results also showed that the model can reconstruct the correct couplings, albeit with a small inferred non-zero value for zero couplings. In the context of this study, what can be inferred about the thermalization process of the spin glass model?",
    "choices": [
      "A) The thermalization process is only complete when the system has reached a temperature of T/J = 0.7, regardless of the number of samples M.",
      "B) The thermalization process is accelerated by the parallel tempering method, allowing the system to reach thermal equilibrium faster.",
      "C) The thermalization process is only complete when the system has reached a number of samples M = 1024, regardless of the temperature T/J.",
      "D) The thermalization process is complete when the system has reached a temperature T/J = 0.7 and has been sampled for a sufficient number of times, M, to accurately reconstruct the correct couplings."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Each spin has then connectivity $4$, i.e., we expect to infer an adjacency matrix with $N c = 256$ couplings different from zero. The dynamics of the simulated model is based on the Metropolis algorithm and parallel tempering\\cite{earl05} is used to speed up the thermalization of the system. The thermalization is tested looking at the average energy over logarithmic time windows and\nthe acquisition of independent configurations\nstarts only after the system is well thermalized. For the values of the couplings we considered two cases: an ordered case, indicated in the figure as  $J$ ordered (e.g., left column of Fig. \\ref{PL-Jor1}) where the couplings can take values $J_{ij}=0,J$, with $J=1$, \n   and a quenched disordered case, indicated in the figures as  $J$ disordered (e.g., right column of Fig. \\ref{PL-Jor1})\n   where the couplings can take also  negative values, i.e., \n    $J_{ij}=0,J,-J$, with a certain probability. The results here presented were obtained with bimodal distributed $J$s: \n   \n    $P(J_{ij}=J)=P(J_{ij}=-J)=1/2$.  The performances of the PLM have shown not to depend on $P(J)$. \n   \n    We recall that in Sec. \\ref{sec:plm} we used the temperature-rescaled notation, i.e., $J_{ij}$ stands for $J_{ij}/T$. \n   \n    To analyze the performances of the PLM, in Fig. \\ref{PL-Jor1} the inferred couplings, $\\mathbb{J}^R_{\\rm inf}$, are shown on top of the original couplings,  $\\mathbb{J}^R_{\\rm true}$.\n     The first figure (from top) in the left column shows  the $\\mathbb{J}^R_{\\rm inf}$ (black) and the $\\mathbb{J}^R_{\\rm tru}$ (green) for a given spin\n     at temperature $T/J=0.7$ and number of samples  $M=1024$. PLM appears to reconstruct the correct couplings, though zero couplings are always given a small inferred non-zero value. In the left column of Fig.  \\ref{PL-Jor1},  both the $\\mathbb{J}^R_{\\rm{inf}}$ and the $\\mathbb{J}^R_{\\rm{tru}}$ are sorted in decreasing order and plotted on top of each other. We can clearly see that $\\mathbb{J}^R_{\\rm inf}$ reproduces the expected step function."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the conduction gap of a strained graphene junction using Green's function calculations. The tight-binding Hamiltonian is expressed as a sum of terms, each representing the coupling of a cell to its nearest neighbor cells. The hopping parameters are defined as $t_{nm} \\left( \\sigma \\right) = t_0 \\exp\\left[-3.37\\left(r_{nm} \\left( \\sigma \\right) /r_0 - 1\\right)\\right]$, where $t_0 = -2.7$ $eV$ and $r_{nm} \\left( 0 \\right) \\equiv r_0 = 0.142$ $nm$ in the unstrained case. The strain tensor is a function of position along the transport direction Ox while it is constant along the Oy-axis. The transport direction, $\\phi$, and strain direction, $\\theta$, are determined as schematized in Fig. 1. Based on this tight binding model, what is the expected effect of increasing the Poisson ratio $\\gamma$ on the hopping parameters $t_{1,2,3}$?",
    "choices": [
      "A) The hopping parameters $t_{1,2,3}$ will increase exponentially with increasing $\\gamma$, leading to a significant increase in the conduction gap.",
      "B) The hopping parameters $t_{1,2,3}$ will decrease linearly with increasing $\\gamma$, resulting in a decrease in the conduction gap.",
      "C) The hopping parameters $t_{1,2,3}$ will remain unchanged with increasing $\\gamma$, as the strain tensor is a function of position along the transport direction Ox.",
      "D) The hopping parameters $t_{1,2,3}$ will increase quadratically with increasing $\\gamma$, leading to a non-linear increase in the conduction gap."
    ],
    "correct_answer": "D)",
    "documentation": [
      "= \\sum\\nolimits_{nm} {{t_{nm}}c_n^\\dag {c_m}}$ where $t_{nm}$ is the hopping energy between nearest neighbor \\emph{n}th and \\emph{m}th atoms. The application of a uniaxial strain of angle $\\theta$ causes the following changes in the $C-C$ bond vectors:\n\\begin{eqnarray}\n {{\\vec r}_{nm}}\\left( \\sigma \\right) &=& \\left\\{ {1 + {M_s}\\left( \\sigma, \\theta \\right)} \\right\\}{{\\vec r}_{nm}}\\left( 0 \\right) \\\\\n {M_s}\\left( \\sigma, \\theta \\right) &=& \\sigma \\left[ {\\begin{array}{*{20}{c}}\n {{{\\cos }^2}\\theta  - \\gamma {{\\sin }^2}\\theta }&{\\left( {1 + \\gamma } \\right)\\sin \\theta \\cos \\theta }\\\\\n {\\left( {1 + \\gamma } \\right)\\sin \\theta \\cos \\theta }&{{{\\sin }^2}\\theta  - \\gamma {{\\cos }^2}\\theta }\n \\end{array}} \\right] \\nonumber\n\\end{eqnarray}\nwhere $\\sigma$ represents the strain and $\\gamma \\simeq 0.165$ is the Poisson ratio \\cite{blak70}. The hopping parameters are defined as $t_{nm} \\left( \\sigma \\right) = t_0 \\exp\\left[-3.37\\left(r_{nm} \\left( \\sigma \\right) /r_0 - 1\\right)\\right]$, where the hopping energy $t_0 = -2.7$ $eV$ and the bond length $r_{nm} \\left( 0 \\right) \\equiv r_0 = 0.142$ $nm$ in the unstrained case. Therefore, there are three different hoping parameters $t_{1,2,3}$ corresponding to three bond vectors ${\\vec r}_{1,2,3}$, respectively, in the strained graphene part of the structure (see Fig. 1). Here, we assume a 1D profile of applied strain, i.e., the strain tensor is a function of position along the transport direction Ox while it is constant along the Oy-axis. The transport direction, $\\phi$, and strain direction, $\\theta$, are determined as schematized in Fig. 1. Based on this tight binding model, two methods described below can be used to investigate the conduction gap of the considered strained junctions. \\textbf{Green's function calculations.} First, we split the graphene sheet into the smallest possible unit cells periodically repeated along the Ox/Oy directions with the indices $p/q$, respectively (similarly, see the details in \\cite{hung12}). The tight-binding Hamiltonian can therefore be expressed in the following form:\n\\begin{eqnarray}\n{H_{tb}} = \\sum\\limits_{p,q} {\\left( {{H_{p,q}} + \\sum\\limits_{{p_1},{q_1}} {{H_{p,q \\to p_1,q_1}}} } \\right)}\n\\end{eqnarray}\nwhere $H_{p,q}$ is the Hamiltonian of cell $\\{p,q\\}$, and $H_{p,q \\to p_1,q_1}$ denotes the coupling of cell $\\{p,q\\}$ to its nearest neighbor cell $\\{p_1,q_1\\}$."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company's revenue has been increasing steadily over the past few years, but its profit margins have been declining. This trend is likely due to the company's decision to invest heavily in research and development, which has allowed it to expand its market share but has also increased its costs. However, the company's management has stated that they are committed to maintaining a high level of profitability, and have announced plans to reduce costs by outsourcing certain functions. Which of the following statements best summarizes the company's current financial situation?",
    "choices": [
      "A) The company's revenue growth is likely to continue, but its profitability will remain stable due to its cost-cutting measures.",
      "B) The company's declining profit margins are a sign that its market share is decreasing, and it will need to invest more in marketing to maintain its position.",
      "C) The company's decision to invest in R&D has led to a significant increase in its costs, but its revenue growth will continue to outpace its costs, resulting in a stable profit margin.",
      "D) The company's revenue growth is likely to slow down in the near future due to increased competition and declining market share, and its profitability will decline accordingly."
    ],
    "correct_answer": "D)",
    "documentation": [
      "No portion of the Site may be reprinted, republished, modified or distributed in any form without Broadjam's express written permission. You agree not to reproduce, reverse engineer, decompile, disassemble or modify any portion of the Site. Certain content may be licensed from third parties and all such third party content and all intellectual property rights related to such content belong to the respective third parties. (e) You acknowledge that Broadjam retains exclusive ownership of the Site and all intellectual property rights associated therewith. Except as expressly provided herein, you are not granted any rights or license to patents, copyrights, trade secrets or trademarks with respect to the Site or any Service, and Broadjam reserves all rights not expressly granted hereunder. You shall promptly notify Broadjam in writing upon your discovery of any unauthorized use or infringement of the Site or any Service or Broadjam's patents, copyrights, trade secrets, trademarks or other intellectual property rights. The Site contains proprietary and confidential information that is protected by copyright laws and international treaty provisions.\n(f) Violations of this Agreement may result in civil or criminal liability. We have the right to investigate occurrences, which may involve such violations and may involve, provide information to and cooperate with, law enforcement authorities in prosecuting users who are involved in such violations. (h) If applicable, You agree to comply with the Acceptable Use Policies (\"AUPs\") of vendors providing bandwidth, merchant or related services to Broadjam. Broadjam will provide links to applicable AUPs upon your written request. (i) \"Broadjam,\" \"Broadjam Top 10,\" \"Metajam\", \"broadjam.com\", \"Musicians of Broadjam,\" Mini MoB, PRIMO MoB and all other trademarks, service marks, logos, labels, product names, service names and trade dress appearing on the Site, registered and unregistered (collectively, the \"Marks\") are owned exclusively or are licensed by Broadjam."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A human-robot collaboration system is designed to navigate a robot towards an unknown goal while accounting for a human's preference for a particular path in the presence of obstacles. The robot's mission time is fixed at 30 time steps, and the human's preference is to go to the goal by passing on the right side of the obstacle. In this scenario, what is the primary advantage of using a method that accounts for the human's path preference over the goal-only baseline?",
    "choices": [
      "A) The method that accounts for the human's path preference is more computationally efficient, resulting in faster computation times.",
      "B) The method that accounts for the human's path preference is more likely to reach the correct goal, as it takes into account the human's preference for a particular path.",
      "C) The method that accounts for the human's path preference is more robust to noisy human inputs, as it can adapt to the human's changing preferences.",
      "D) The method that accounts for the human's path preference is more effective in reducing the entropy of the goal distribution, making it easier for the robot to reach the correct goal."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The green robot selects actions according to the goal-only baseline, and the blue robot uses our proposed method to infer path preferences. The polytopes composing G are drawn in blue. Probability of correct goal. WLPHVWHS +J (c) Entropy of goal distribution g.\nFig. 6: Probability of the correct goal, fig.6b, and entropy of the goal belief distribution P (g), fig.6c, for the same problem setup, fig.6a. In this problem instance, the human's preference is to go to the goal by passing on the right side of the obstacle. Results are averaged over 50 runs and the area filled represents one standard deviation above and below the mean value. The goal-only baseline shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Success rates in the simple environment (Map 1).The results are averaged over 6 randomly sampled problem instances (start location, goal location, and goal possibilities), and over 50 runs per problem instance.\u2206T is the number of time steps separating two consecutive human inputs. The robot's mission time is Tmax = 30 time steps. We selected \u03b3 h = 1.5, corresponding to relatively noisy human inputs and making the problem more difficult to solve for the robot. Computation times for Goal Only and Path Preference methods on Map 1 (fig.5a),Map 2 (fig.5b), and Map 3 (fig.5c),averaged over 100 runs with randomly sampled problem instances. The 95 % confidence interval is provided with the mean. We evaluate computation time at the first iteration of each run (where the search depth takes on its highest value Tmax). abstract\n\nRobots that can effectively understand human intentions from actions are crucial for successful human-robot collaboration. In this work, we address the challenge of a robot navigating towards an unknown goal while also accounting for a human's preference for a particular path in the presence of obstacles."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the probabilistic extension, what is the primary advantage of using the Evidence Lower Bound (ELBO) in maximizing the parameter values \u03b8?",
    "choices": [
      "A) The ELBO provides a direct estimate of the posterior distribution of the state variables z 0:T, allowing for precise predictions without uncertainty quantification.",
      "B) The ELBO is a computationally efficient method for optimizing the model parameters \u03b8, as it leverages the reparametrization trick to reduce the dimensionality of the optimization problem.",
      "C) The ELBO is a sufficient statistic for the model parameters \u03b8, meaning that the maximum ELBO value is equal to the log-likelihood of the training data.",
      "D) The ELBO provides a lower bound on the log-likelihood of the training data, allowing for the estimation of the model parameters \u03b8 while also quantifying the uncertainty in predictions."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In particular we are employing a probabilistic decoder The encoder is used to infer the state variables z based on the given data and thus defined in the inference and learning section. Inference and Learning\n\nGiven the probabilistic relations , our goal is to infer the latent variables z 0:T as well as all model parameters \u03b8. We follow a hybrid Bayesian approach in which the posterior of the state variables is approximated using amortized Variational Inference and Maximum-A-Posteriori (MAP) point-estimates for \u03b8 are computed. The application of Bayes' rule for each data sequence x 0:T leads to the following posterior: where p(\u03b8) denotes the prior on the model parameters. In the context of variational inference, we use the following factorization of the approximate posterior i.e. we infer only the mean \u00b5 and variance \u03c3 for each state variable based on the given data points. This conditional density used for inference is the encoder-counterpart to the probabilistic decoder defined in the section before. It can be readily shown that the optimal parameter values are found by maximizing the Evidence Lower Bound (ELBO) F(q \u03c6 (z 0:T ), \u03b8) which is derived in Appendix B. We compute Monte Carlo estimates of the gradient of the ELBO with respect to \u03c6 and \u03b8 with the help of the reparametrization trick and carry out stochastic optimization with the ADAM algorithm .\n\nResults for the probabilistic extension\n\nWe applied our probabilistic version to the KS-equation. We used the same settings as for the deterministic approach but considered up to 10 complex latent variables. The obtained \u03bb's are in Figure . The probabilistic model allows us to quantify the uncertainty in predictions. In Figure predictions for various time-steps and the respective uncertainty bounds are shown for an unseen initial condition. Due to the chaotic nature of the KS-equation and the small amount of training data, the underlying linear dynamic of our model is only able to capture the full dynamics for a limited time horizon."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new Prestige System has been introduced to the game, which adjusts the difficulty and score setting based on the player's performance and the difficulty of the quests. However, some players have reported that the Prestige System can lead to an uneven distribution of rewards among players. What is a potential consequence of the Prestige System's dynamic difficulty adjustment?",
    "choices": [
      "A) The game's difficulty level will remain constant, but the rewards for completing quests will increase accordingly.",
      "B) The Prestige System will only adjust the difficulty level based on the player's performance, but not the rewards.",
      "C) The Prestige System will only adjust the rewards based on the player's performance, but not the difficulty level.",
      "D) The Prestige System's dynamic difficulty adjustment can lead to an uneven distribution of rewards among players, as the game's difficulty level may not be adjusted accordingly."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Redesigned chat and mail screens. Take on other Summoners\u2019 top Champions for bragging rights and prizes in 1-on-1 Duels! A new series of special Ultron quests are available, starting with the first Chapter. Fight back against Ultron\u2019s infection alongside the Summoner, and team up with some of Marvel\u2019s finest! New quests unlock each week! The Spider-Man Champion gate has been removed from Act 1, Chapter 1, Quest 5. \u2022 Fixed an issue where chat snapped to the most recent message. \u2022 Fixed several issues where Hero Rating would fluctuate. \u2022 Various improvements to the Summoner Mastery screens and descriptions. \u2022 Increased the ISO8 awarded by duplicate 2-Star Champions. Quest through the new single-player campaign, Ant-Man\u2019s Adventure! In addition to Ant-Man and Yellowjacket feuding throughout the Battlerealm, additional new Champions will be joining The Contest! Access more Masteries in the new Utility Mastery tree! Please note, these changes may result in a loss of Hero Rating as incorrect effects are restored back to normal levels. Improved and polished combat mechanics to reduce the amount of stutters and lost input. Fixed and optimized rendering related issues with Metal enabled devices. Team up with Ant-Man, and put a stop to Yellowjacket\u2019s mysterious mission! All Alliance Quests only last for a specified amount of time, defeat the boss with your Alliance before it expires! New Prestige System - A dynamic difficulty and score setting that adjusts as you and your Alliance succeed in harder quests. The better you do and the tougher your Alliance is, the higher the prestige. The higher the prestige, the better the rewards!\nChoose your teams carefully as Champions within Alliance Quests cannot be used in other Story or Event Quests. Act 4 has been released! Play Chapter 1 now! Summoner level maximum has been increased to level 60! 5-Star Champions are co\u03a9ming to The Contest! These are the most powerful Champions yet! Additional improvements have been made to the UI, Versus Arenas, Synergy Bonuses, the Stash & Items Store."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that individuals who consumed more fruits had lower levels of mercury in their blood, but the relationship between fruit consumption and mercury exposure was not linear. Which of the following statements best describes the findings of this study?",
    "choices": [
      "A) The study found that for every additional fruit consumed, mercury levels in the blood decreased by 1.5 \u03bcg/L.",
      "B) The study found that individuals who consumed more fruits had a 20% lower risk of mercury exposure compared to those who consumed fewer fruits.",
      "C) The study found that the protective effect of fruit consumption on mercury exposure was only significant for individuals who consumed more than 10 fruits per week.",
      "D) The study found that the relationship between fruit consumption and mercury exposure was influenced by the type of fish consumed, with fruits providing a protective effect against mercury exposure from predatory fish."
    ],
    "correct_answer": "D)",
    "documentation": [
      "consumption influences mercury: Topics by WorldWideScience.org\nSample records for consumption influences mercury\nEpidemiologic confirmation that fruit consumption influences mercury exposure in riparian communities in the Brazilian Amazon\nSousa Passos, Carlos Jose; Mergler, Donna; Fillion, Myriam; Lemire, Melanie; Mertens, Frederic; Guimaraes, Jean Remy Davee; Philibert, Aline\nSince deforestation has recently been associated with increased mercury load in the Amazon, the problem of mercury exposure is now much more widespread than initially thought. A previous exploratory study suggested that fruit consumption may reduce mercury exposure. The objectives of the study were to determine the effects of fruit consumption on the relation between fish consumption and bioindicators of mercury (Hg) exposure in Amazonian fish-eating communities. A cross-sectional dietary survey based on a 7-day recall of fish and fruit consumption frequency was conducted within 13 riparian communities from the Tapajos River, Brazilian Amazon. Hair samples were collected from 449 persons, and blood samples were collected from a subset of 225, for total and inorganic mercury determination by atomic absorption spectrometry. On average, participants consumed 6.6 fish meals/week and ate 11 fruits/week. The average blood Hg (BHg) was 57.1\u00c2\u00b136.3 \u00ce\u00bcg/L (median: 55.1 \u00ce\u00bcg/L), and the average hair-Hg (HHg) was 16.8\u00c2\u00b110.3 \u00ce\u00bcg/g (median: 15.7 \u00ce\u00bcg/g). There was a positive relation between fish consumption and BHg (r=0.48; P 2 =36.0%) and HHg levels (fish: \u00ce\u00b2=1.2, P 2 =21.0%). ANCOVA models showed that for the same number of fish meals, persons consuming fruits more frequently had significantly lower blood and HHg concentrations. For low fruit consumers, each fish meal contributed 9.8 \u00ce\u00bcg/ L Hg increase in blood compared to only 3.3 \u00ce\u00bcg/ L Hg increase for the high fruit consumers. In conclusion, fruit consumption may provide a protective effect for Hg exposure in Amazonian riparians. Prevention strategies that seek to maintain fish consumption while reducing Hg exposure in fish-eating communities should be pursued\nInfluence of mercury bioaccessibility on exposure assessment associated with consumption of cooked predatory fish in Spain."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is interested in evaluating the performance of the proposed 8-hand gesture model in a real-world setting. However, collecting ground truth labels for the diverse postural environment is a significant challenge. What is a potential limitation of using the rotational normalization method to merge hand-gestures subject to directional differences?",
    "choices": [
      "A) The method may not account for the variability in walker usage among older adults, leading to inaccurate gesture recognition.",
      "B) The method may not be effective in recognizing gestures performed while using a wheelchair, as the 3-axis data from the wrist-worn ACC sensor may not capture the full range of motion.",
      "C) The method may not be able to distinguish between gestures performed with a single walking stick and those performed with a double walking stick, leading to incorrect classification.",
      "D) The method may not be able to recognize gestures performed in environments with high levels of ambient noise, which can interfere with the 0.4Hz low-pass filtered data."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\section{Activity Recognition}\nWe aim to detect single wrist-worn ACC sensor based hand gesture and postural activities and insert these into an HDBN graphical model in conjunction with ambient and object sensor values for complex activity recognition. We consider the recognition problem asan activity tupple of $\\langle gesture,posture,ambient,object \\rangle$. Though, Alam et. al. provides significant performance improvement for single wrist-worn ACC sensor aided 18-hand gesture based postural activity recognition in lab environment \\cite{alam17}, it faces some practical challenges in real-time smart environment with older adults due to the diversity of their postures. For example, some older adults use walker, double walking sticks or wheel chair for walking in which cases collecting 18 hand gestures and corresponding postural activities for training requires endless efforts and carefulness. To reduce the complexity of ground truth labeling and later state space explosion for graphical model (HDBN), we propose to use rotational normalization method that can merge some hand-gestures subject to directional differences and forms an 8-hand gesture model. However, our proposed Feature Weight Naive Bayes (FWNB) classifier adds significant improvement on Alam et. al. proposed sparse-deconvolution method as well as recognition in diverse postural environment. \\begin{figure}[!htb]\n\\begin{center}\n   \\epsfig{file=hand_gestures.pdf,height=0.5in, width=3in}\n   \\vspace{-.2in}\n\\caption{8 hand gesture dictionary with direction}\n   \\label{fig:hand_gestures}\n   \\vspace{-.2in}\n\\end{center}\n\\end{figure}\n\\subsection{Hand Gesture Recognition}\n\\label{sec:hand_gesture}\n\\emph{AutoCogniSys} proposes an 8-gesture dictionary (as shown in Fig. \\ref{fig:hand_gestures}) and a Feature Weighted Naive Bayesian (FWNB) framework for building, modeling and recognizing hand gestures. The method comprises of the following steps: (i) \\emph{Preprocessing:} wrist-worn ACC sensor provided 3-axis data are passed through 0.4Hz low-pass filter to remove the data drift."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the relationship between the number of hours spent watching TV and the likelihood of developing obesity in a population. The data shows a strong correlation between the two variables, with individuals who watch more than 4 hours of TV per day being more likely to develop obesity. However, the researcher also notes that the correlation is strongest among individuals who watch TV in a sedentary manner, such as while sitting on the couch. Which of the following statements best summarizes the implications of this finding?",
    "choices": [
      "A) The researcher's findings suggest that watching TV is a causal factor in the development of obesity, and that the sedentary nature of TV watching is the key mechanism by which this occurs.",
      "B) The researcher's findings support the idea that obesity is caused by a combination of genetic and environmental factors, and that TV watching is just one of many potential contributors.",
      "C) The researcher's findings suggest that the relationship between TV watching and obesity is strongest among individuals who are already at risk of developing obesity, and that TV watching is not a causal factor in the development of obesity.",
      "D) The researcher's findings suggest that the sedentary nature of TV watching is a key factor in the development of obesity, and that this is because TV watching is a proxy for a broader pattern of physical inactivity that is associated with obesity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(It should be noted, in relation to such models, that even if this is how our choices are made, our choice to choose one of these \u201calternative possibilities\u201d will still be caused by prior causes that are ultimately completely beyond our own control. Nothing changes this fact, again because decision-making is the product of complex physical processes; it is not an uncaused event.) It is generally unclear what the purpose of such models is. Are they a hypotheses we should test? They do not seem to be. Generally, these models most of all seem like an attempt to make the world fit our preconceived intuitions, which most of all resembles pseudoscience. Fortunately, there is plenty of relief available to the libertarians and other people who have this fear, and it does not involve any unscientific models \u2013 neither two-stage, three-stage, nor any other number of stages. The source of this relief is the simple earlier-mentioned fact that we can never know whether there is just one or infinitely many possible outcomes from the present state of the universe. This simple fact gives us all the relief we could ask for, because it reveals that there is no reason to be sure that there is just one possible outcome from the present state of the universe. And, to repeat an important point, we are then left with the conclusion that the only reasonable thing to do is to try to make the best impact we can in the world, which is true no matter whether there is just one possible outcome from the present state of the universe or not, since our actions still have consequences and therefore still matter even in a fully deterministic universe. Some, especially libertarians, might want to object to the claim that we can never know whether determinism is true or not, and even claim that we in fact now know, or at least have good reasons to believe, that indeterminism is true. Here is neuroscientist Peter Tse expressing something along those lines: \u201cHenceforth, I will accept the weight of evidence from modern physics, and assume ontological indeterminism to be the case.\u201d"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the role of astrocytes in regulating dendritic spine morphologies in the context of neurodegenerative diseases. They have isolated and cultured mouse cortical astrocytes using a protocol that involves the absence of viable neurons and the separation of astrocytes, oligodendrocytes, and microglia. However, they notice that the cultured astrocytes are not as pure as expected, with some cells showing characteristics of oligodendrocytes. Which of the following statements best describes the implications of this finding for the researcher's study?",
    "choices": [
      "A) The presence of oligodendrocyte-like cells in the cultured astrocytes suggests that the isolation protocol is not effective in separating astrocytes from oligodendrocytes, and therefore, the astrocyte-conditioned medium may contain oligodendrocyte-derived factors that could influence the study's results.",
      "B) The presence of oligodendrocyte-like cells in the cultured astrocytes indicates that astrocytes and oligodendrocytes share a common ancestor and therefore, the two cell types have similar functions and may be interchangeable in the study.",
      "C) The presence of oligodendrocyte-like cells in the cultured astrocytes suggests that the astrocytes are not as pure as expected, but this does not necessarily affect the validity of the study's results, as the researcher can still use the cultured astrocytes to study their functions in isolation.",
      "D) The presence of oligodendrocyte-like cells in the cultured astrocytes indicates that the isolation protocol is not effective in separating astrocytes from oligodendrocytes, and therefore, the researcher should re-run the isolation protocol to obtain pure astrocyte cultures."
    ],
    "correct_answer": "A)",
    "documentation": [
      "In order to study the potential role of these proteins in controlling dendritic spine morphologies/number, the use of cultured cortical neurons offers several advantages. Firstly, this system allows for high-resolution imaging of dendritic spines in fixed cells as well as time-lapse imaging of live cells. Secondly, this in vitro system allows for easy manipulation of protein function by expression of mutant proteins, knockdown by shRNA constructs, or pharmacological treatments. These techniques allow researchers to begin to dissect the role of disease-associated proteins and to predict how mutations of these proteins may function in vivo. Play ButtonIsolation and Culture of Mouse Cortical AstrocytesAuthors: Sebastian Schildge, Christian Bohrer, Kristina Beck, Christian Schachtrup. Institutions: University of Freiburg , University of Freiburg .Astrocytes are an abundant cell type in the mammalian brain, yet much remains to be learned about their molecular and functional characteristics. In vitro astrocyte cell culture systems can be used to study the biological functions of these glial cells in detail. This video protocol shows how to obtain pure astrocytes by isolation and culture of mixed cortical cells of mouse pups. The method is based on the absence of viable neurons and the separation of astrocytes, oligodendrocytes and microglia, the three main glial cell populations of the central nervous system, in culture. Representative images during the first days of culture demonstrate the presence of a mixed cell population and indicate the timepoint, when astrocytes become confluent and should be separated from microglia and oligodendrocytes. Moreover, we demonstrate purity and astrocytic morphology of cultured astrocytes using immunocytochemical stainings for well established and newly described astrocyte markers. This culture system can be easily used to obtain pure mouse astrocytes and astrocyte-conditioned medium for studying various aspects of astrocyte biology. Neuroscience, Issue 71, Neurobiology, Cellular Biology, Medicine, Molecular Biology, Anatomy, Physiology, brain, mouse, astrocyte culture, astrocyte, fibroblast, fibrinogen, chondroitin sulfate proteoglycan, neuronal regeneration, cell culture, animal model50079Play ButtonImaging Dendritic Spines of Rat Primary Hippocampal Neurons using Structured Illumination MicroscopyAuthors: Marijn Schouten, Giulia M. R. De Luca, Diana K. Alatriste Gonz\u00e1lez, Babette E. de Jong, Wendy Timmermans, Hui Xiong, Harm Krugers, Erik M. M. Manders, Carlos P. Fitzsimons."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study of the molecular structure of a star-forming region reveals that the C2H molecule is present in the early stages of star formation, but its abundance decreases as the region evolves. This suggests that C2H is consumed in the chemical network forming CO and more complex molecules. However, the data also show that C2H is not suited to investigate the central gas cores in more evolved sources. What is the most plausible explanation for the observed decrease in C2H abundance?",
    "choices": [
      "A) The decrease in C2H abundance is due to the increased temperature of the region, which causes the C2H molecule to dissociate.",
      "B) The decrease in C2H abundance is due to the increased abundance of CO, which reacts with C2H to form more complex molecules.",
      "C) The decrease in C2H abundance is due to the increased abundance of N2H+, which reacts with C2H to form more complex molecules.",
      "D) The decrease in C2H abundance is due to the chemical effects of the UV photodissociation of CO, which replenishes the C2H molecule at the core edges."
    ],
    "correct_answer": "D)",
    "documentation": [
      "At the same time disks and\noutflows evolve, which should hence have similar time-scales. The\ndiameter of the shell-like C$_2$H structure in IRAS\\,18089-1732 is\n$\\sim 5''$ (Fig.\\,\\ref{18089}), or $\\sim$9000\\,AU in radius at the\ngiven distance of 3.6\\,kpc. This value is well matched by the modeled\nregion with decreased C$_2$H abundance (Fig.\\,\\ref{model}). Although\nin principle optical depths and/or excitation effects could mimic the\nC$_2$H morphology, we consider this as unlikely because the other\nobserved molecules with many different transitions all peak toward the\ncentral submm continuum emission in IRAS\\,18089-1732\n\\citep{beuther2005c}. Since C$_2$H is the only exception in that rich\ndataset, chemical effects appear the more plausible explanation. The fact that we see C$_2$H at the earliest and the later evolutionary\nstages can be explained by the reactive nature of C$_2$H: it is\nproduced quickly early on and gets replenished at the core edges by\nthe UV photodissociation of CO. The inner ``chemical'' hole observed\ntoward IRAS\\,18089-1732 can be explained by C$_2$H being consumed in\nthe chemical network forming CO and more complex molecules like larger\ncarbon-hydrogen complexes and/or depletion. The data show that C$_2$H is not suited to investigate the central gas\ncores in more evolved sources, however, our analysis indicates that\nC$_2$H may be a suitable tracer of the earliest stages of (massive)\nstar formation, like N$_2$H$^+$ or NH$_3$ (e.g.,\n\\citealt{bergin2002,tafalla2004,beuther2005a,pillai2006}). While a\nspatial analysis of the line emission will give insights into the\nkinematics of the gas and also the evolutionary stage from chemical\nmodels, multiple C$_2$H lines will even allow a temperature\ncharacterization. With its lowest $J=1-0$ transitions around 87\\,GHz,\nC$_2$H has easily accessible spectral lines in several bands between\nthe 3\\,mm and 850\\,$\\mu$m. Furthermore, even the 349\\,GHz lines\npresented here have still relatively low upper level excitation\nenergies ($E_u/k\\sim42$\\,K), hence allowing to study cold cores even\nat sub-millimeter wavelengths."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A vehicle-to-Infrastructure (V2I) communication system uses orthogonal subchannels to transmit signals between vehicles and Road Side Units (RSUs). The received signal at the RSU is affected by the channel power gain from the vehicle to the RSU, which is modeled as the product of the large-scale fading and the small-scale fading. The large-scale fading includes path-loss and shadowing, and is given by the formula: \u03b1t,knR = G\u03b2dt,nR-\u03b3. If a vehicle is experiencing a jamming attack, the channel power gain from the vehicle to the RSU will be affected. However, the jamming attack will not affect the GPS signal, which is used to determine the vehicle's position. Which of the following statements is true about the joint GPS spoofing and jamming detection problem?",
    "choices": [
      "A) The joint detection problem can be solved by analyzing the channel power gain from the vehicle to the RSU alone, without considering the GPS signal.",
      "B) The joint detection problem can be solved by analyzing the GPS signal alone, without considering the channel power gain from the vehicle to the RSU.",
      "C) The joint detection problem can be solved by analyzing the channel power gain from the vehicle to the RSU and the GPS signal separately, and then combining the results.",
      "D) The joint detection problem can be solved by analyzing the channel power gain from the vehicle to the RSU and the GPS signal simultaneously, taking into account the effects of both the jamming attack and the GPS spoofing."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The time-varying positions of the $n$-th vehicle is given by $\\mathrm{p}_{n,t}=[{x}_{n,t},{y}_{n,t}]$ where $n \\in N$. Among the $K$ orthogonal subchannels available for the Vehicle-to-Infrastructure (V2I) communications, RSU assigns one V2I link to each vehicle. Each vehicle exchanges messages composed of the vehicle's state (i.e., position and velocity) with RSU through the $k$-th V2I link by transmitting a signal $\\textrm{x}_{t,k}$ carrying those messages at each time instant $t$ where $k \\in K$. We consider a reactive RSJ that aims to attack the V2I link by injecting intentional interference to the communication link between vehicles and RSU to alter the transmitted signals by the vehicles. In contrast, the RSS purposes to mislead the vehicles by spoofing the GPS signal and so registering wrong GPS positions. RSU aims to detect both the spoofer on the satellite link and the jammer on multiple V2I links in order to take effective actions and protect the vehicular network. The joint GPS spoofing and jamming detection problem can be formulated as the following ternary hypothesis test:\n\\begin{equation}\n    \\begin{cases}\n        \\mathcal{H}_{0}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k} + \\mathrm{v}_{t,k}, \\\\\n        \\mathcal{H}_{1}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k} + \\mathrm{g}_{t,k}^{JR} \\mathrm{x}_{t,k}^{j} + \\mathrm{v}_{t,k}, \\\\\n        \\mathcal{H}_{2}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k}^{*} + \\mathrm{v}_{t,k},\n    \\end{cases}\n\\end{equation}\nwhere $\\mathcal{H}_{0}$, $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ denote three hypotheses corresponding to the absence of both jammer and spoofer, the presence of the jammer, and the presence of the spoofer, respectively. $\\textrm{z}_{t,k}$ is the received signal at the RSU at $t$ over the $k$-th V2I link, $\\textrm{g}_{t,k}^{nR}$ is the channel power gain from vehicle $n$ to the RSU formulated as: $\\textrm{g}_{t,k}^{nR} = \\alpha_{t,k}^{nR} \\mathrm{h}_{t,k}^{nR}$, where $\\alpha_{t,k}^{nR}$ is the large-scale fading including path-loss and shadowing modeled as \\cite{8723178}: $\\alpha_{t,k}^{nR}=G\\beta d_{t,nR}^{-\\gamma}$.\n\\begin{figure}[t!]"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the behavior of a rarefied gas in a microchannel. They are interested in understanding the effects of the Knudsen number on the distribution function of the gas molecules. The researcher performs a CE expansion around the equilibrium distribution function and obtains the following equation:\n\nf \u03c3 i = f \u03c3 ,eq (\u03c1 \u03c3 , u, T ) + \u03b5 f \u03c3 ,(1) (\u03c1 \u03c3 , u, T ) + \u03b5 2 f \u03c3 ,(2) (\u03c1 \u03c3 , u, T )\n\nwhere \u03b5 is a coefficient referring to the Knudsen number. The researcher then substitutes this expansion into the Boltzmann equation and obtains:\n\n\u2202 f \u03c3 i /\u2202t + u \u2202 f \u03c3 i /\u2202x = Q ( f \u03c3 i )\n\nwhere Q is a collision operator. The researcher is interested in understanding the effects of the collision operator on the distribution function. However, they are concerned that the complexity of the resulting equations may make it difficult to simulate and solve.",
    "choices": [
      "A) The collision operator Q can be neglected for small Knudsen numbers, and the resulting equation is a simple conservation equation.",
      "B) The collision operator Q is proportional to the Knudsen number, and the resulting equation is a non-conservative equation.",
      "C) The collision operator Q is independent of the Knudsen number, and the resulting equation is a conservative equation.",
      "D) The collision operator Q is a function of the Knudsen number and the distribution function, and the resulting equation is a non-linear equation that requires numerical methods to solve."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The discrete Boltzmann equation for component \u03c3 is (C.1) In Eq. C.1 there are two equilibrium distribution functions, i.e., f \u03c3 ,seq = f \u03c3 ,seq (\u03c1 \u03c3 , u \u03c3 , T \u03c3 ) and f \u03c3 ,eq = f \u03c3 ,eq (\u03c1 \u03c3 , u, T ). For convenience, S \u03c3 i is defined as We perform the CE expansion around the f \u03c3 ,seq . That is, the distribution function f \u03c3 i can be expanded as where \u03b5 is a coefficient referring to Knudsen number. The partial derivatives of time and space can also be expanded to Substituting the above four equations into Eq.\n(C.1), we can obtain (C.6) When retaining to \u03b5 terms, the following equation is obtained When retaining to \u03b5 2 terms, we can obtain where M 2,\u03b1\u03b2 ( f \u03c3 ,(1) ) = \u2211 i v i\u03b1 v i\u03b2 f \u03c3 ,(1) i and M 3,1,\u03b1 ( f \u03c3 ,(1) ) = . Substituting Eq. (C.14) into the above three equations, and replacing the time derivatives with the space derivatives, we obtain It should be noted that the ability to recover the corresponding level of macroscopic fluid mechanics equations is only part of the physical function of DBM. The corresponding to the physical functions of DBM is the EHEs, which, in addition to the conserved moments evolution equations corresponding to the three conservation laws of mass, momentum and energy, also includes some of the most closely related nonconserved moments evolution equations. We refer the EHEs derivation based on kinetic equation to as KMM. The necessity of the expanded part, the evolution equations of the relevant non-conserved moments, increases rapidly as increasing the degree of non-continuity/non-equilibrium. As the degree of non-continuity/non-equilibrium increases, the complexity will rapidly make KMM simulation studies, deriving and solving EHE, impossible."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new traffic management system is being implemented to optimize lane usage on highways. The system uses a dynamic programming approach to allocate lanes to vehicles based on real-time traffic conditions. However, the system's computational complexity is a major concern due to the large number of possible lane allocations. To address this issue, the system's developers have proposed a reformulation of the optimization problem using binary variables.",
    "choices": [
      "A) The proposed reformulation uses binary variables to replace integer decision variables, which reduces the computational complexity of the problem by approximately 50%.",
      "B) The reformulation is based on a linear programming approach, which is more suitable for large-scale optimization problems.",
      "C) The use of binary variables in the reformulation allows for the incorporation of additional constraints, such as the augmented safety constraint, which is not present in the original formulation.",
      "D) The reformulation can accommodate an arbitrary number of lanes at any given time instant k, making it suitable for highways with varying traffic conditions."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\"\nFurthermore, the absolute value constraint can be decomposed into linear constraints by the application of big-M method and the introduction of an auxiliary variable, as shown in the Appendix. Remark 3: The proposed formulation can accommodate arbitrary number of lanes at any given time instant k. This means that if at any given time, the number of available lanes for traveling either increases or decreases, the proposed formulation will still continue to hold. This is an important consideration since many a times on highways, some lanes are blocked due to various unanticipated situations such as road accidents, roadwork, narrowing of road etc. 2) Computational Complexity Reduction: This section details the optimization problem reformulation with binary variables, optimization warm start technique and lazy constraint implementation, all of which combine to improve the computational complexity of our SLAS module. Binary Variables: The proposed formulation in Section III-B has relatively high computation complexity (computation time of \u223c 2s in the worst case scenario -slow moving traffic blocking all the lanes) due to the integer decision variables yielding a mixed-integer optimization problem . To circumvent the computational overload, we reformulate the problem with binary variables that replace the integer variables, as follows:\nwhere the Lk (i, j) represents the modified target lane variable, indexed by the lane (i) as well as the planning step (j) and Lk (a, b) = 1 represents the choice of lane a \u2208 L as the target lane at planning step b \u2208 Z . Then, some of the constraints from the SLAS formulation in Section III-B are modified as follows:\nHere, initializes the target lane, (21) restricts the target lane at any planning step to the set of available lanes, restricts the lane change between consecutive planning steps to the adjacent lanes, and ( ) represents the augmented safety constraint. The implication ( =\u21d2 ) in ( ) can easily be transformed into a linear constraint (see Appendix)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the shark fishing vessels in Tanjung Luar village tend to have a higher number of sets per trip when targeting sharks in the Java Sea compared to those in West Nusa Tenggara. However, the study also revealed that larger vessels with higher horsepower engines spend more time at sea than smaller vessels. Which of the following can be inferred about the fishing behavior of the vessels in the Java Sea?",
    "choices": [
      "A) Smaller vessels with smaller engines tend to fish in the Java Sea, while larger vessels with higher HP engines fish in West Nusa Tenggara.",
      "B) The vessels in the Java Sea have a higher number of sets per trip due to the use of surface longlines, which are more efficient in this region.",
      "C) The vessels in the Java Sea have a lower number of sets per trip due to the use of bottom longlines, which are less efficient in this region.",
      "D) The vessels in the Java Sea have a higher number of sets per trip due to the fact that they spend more time at sea and have a higher number of sets per trip than vessels in West Nusa Tenggara."
    ],
    "correct_answer": "D)",
    "documentation": [
      "These vessels are operated by approximately 150 highly-specialised shark fishers, from Tanjung Luar village and Gili Maringkik, who make up roughly 5% of the local fisher population. The shark industry is more profitable than non-shark fisheries, and shark fishers report high household dependency on shark resources, low occupational diversity, and limited capacity and aspirations to move into other fisheries or industries. Surface and bottom longlines are used as the primary fishing gears to target sharks, with pelagic fish (e.g. Euthynnus spp., Rastrellinger spp.) used as bait. Surface and bottom longlines systematically vary in length, depth deployed, number of sets, number of hooks used, and soak times (Table 2). Gear types are typically associated with certain vessel types, and fishers\u2013captain and crew\u2014tend to exhibit preferences for specific gear types. Shark fishers also use gillnets and troll lines as secondary gears, to catch bait and opportunistically target other species, such as grouper, snapper, skipjack and mackerel tuna. Table 2. Characteristics of surface and bottom longlines. The shark fishing vessels can be divided into two broad categories according to fishing behaviour: larger vessels (\u226514 m) with higher horsepower (HP) engines spend more time at sea than smaller vessels (\u226412m) (p<0.001), and reach fishing grounds outside of West Nusa Tenggara. These vessels primarily fish in southern Sumbawa and Sumba Islands, however, they also reach as far as eastern Flores, Timor Island, and the Java Sea (Fig 1). Larger, higher HP vessels also tend to employ surface longlines (p<0.001), and since they spend more time at sea, have a higher number of sets per trip than smaller vessels (p<0.001). Smaller vessels (\u226412 m) with smaller engines tend to remain in waters around West Nusa Tenggara only, carrying out shorter fishing trips using bottom longlines (Table 3). Table 3. Characterisation of the different fishing vessels used to target sharks in Tanjung Luar. During the study period we recorded shark catch from a total of 595 fishing trips."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The presence of C$_2$H lines in all evolutionary stages of sources suggests that the molecule is not affected by the evolutionary stage of the source. However, the line-widths of the C$_2$H lines vary significantly between the different sub-groups. What can be inferred about the physical conditions that govern the formation of C$_2$H lines in these sources?",
    "choices": [
      "A) The C$_2$H lines are more sensitive to the luminosity of the source, with more luminous sources exhibiting broader line-widths.",
      "B) The C$_2$H lines are more sensitive to the distance of the source from the observer, with sources at greater distances exhibiting broader line-widths.",
      "C) The C$_2$H lines are more sensitive to the density of the source, with denser sources exhibiting broader line-widths.",
      "D) The C$_2$H lines are more sensitive to the temperature of the source, with colder sources exhibiting broader line-widths, and the difference in line-widths between IRDCs and HMPOs can be attributed to the temperature of the source."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Table\n\\ref{sample} lists the observed sources, their coordinates, distances,\nluminosities and a first order classification into the evolutionary\nsub-groups IRDCs, HMPOs and UCH{\\sc ii}s based on the previously\navailable data. Although this classification is only based on a\nlimited set of data, here we are just interested in general\nevolutionary trends. Hence, the division into the three main classes\nis sufficient. Figure \\ref{spectra} presents sample spectra toward one source of each\nevolutionary group. While we see several CH$_3$OH lines as well as\nSO$_2$ and H$_2$CS toward some of the HMPOs and UCH{\\sc ii}s but not\ntoward the IRDCs, the surprising result of this comparison is the\npresence of the C$_2$H lines around 349.4\\,GHz toward all source types\nfrom young IRDCs via the HMPOs to evolved UCH{\\sc ii}s. Table\n\\ref{sample} lists the peak brightness temperatures, the integrated\nintensities and the FWHM line-widths of the C$_2$H line blend at\n349.399\\,GHz. The separation of the two lines of 1.375\\,MHz already\ncorresponds to a line-width of 1.2\\,km\\,s$^{-1}$. We have three C$_2$H\nnon-detections (2 IRDCs and 1 HMPO), however, with no clear trend with\nrespect to the distances or the luminosities (the latter comparison is\nonly possible for the HMPOs). While IRDCs are on average colder than\nmore evolved sources, and have lower brightness temperatures, the\nnon-detections are more probable due to the relatively low sensitivity\nof the short observations (\\S\\ref{obs}). Hence, the data indicate\nthat the C$_2$H lines are detected independent of the evolutionary\nstage of the sources in contrast to the situation with other\nmolecules. When comparing the line-widths between the different\nsub-groups, one finds only a marginal difference between the IRDCs and\nthe HMPOs (the average $\\Delta v$ of the two groups are 2.8 and\n3.1\\,km\\,s$^{-1}$). However, the UCH{\\sc ii}s exhibit significantly\nbroader line-widths with an average value of 5.5\\,km\\,s$^{-1}$.\n\nIntrigued by this finding, we wanted to understand the C$_2$H spatial\nstructure during the different evolutionary stages."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the effectiveness of MPAs in reducing shark catch in Tanjung Luar found that the optimal fishing effort for increasing overall CPUE of the fishery is characterized by low to intermediate trip lengths and gear sets. However, the same study also revealed that the catch per set and overall mortality of threatened and endangered species are significantly lower when fewer hooks are deployed. Which of the following is a likely consequence of implementing regulations that control the number of hooks in combination with incentives for shark fishers to tightly manage the number of hooks they deploy?",
    "choices": [
      "A) The fishery will experience a significant increase in overall CPUE, but the number of CITES-listed species caught will remain unchanged.",
      "B) The fishery will experience a significant decrease in overall CPUE, as well as an increase in the number of CITES-listed species caught.",
      "C) The fishery will experience a significant increase in overall CPUE, as well as a decrease in the number of CITES-listed species caught.",
      "D) The fishery will experience a significant increase in overall CPUE, as well as a decrease in the number of CITES-listed species caught, due to the implementation of regulations that control the number of hooks and incentives for shark fishers to tightly manage the number of hooks they deploy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "[37, 38]. Strengthening Indonesia\u2019s existing MPA network for shark conservation, such as making all MPAs no-take zones for sharks and expanding spatial protection to critical shark habitat, including aggregation sites or pupping and nursery grounds for species of conservation concern, could have considerable conservation benefits. It should be noted, however, that MPAs may only be effective for certain species, such as those with small ranges or site-fidelity . More research is required to identify critical shark habitat and life history stages. For Tanjung Luar these efforts could focus on better understanding scalloped hammerhead (Sphyrna lewini) aggregation sites. Well-targeted spatial closures for this species could significantly reduce catch of threatened species in this fishery. The relationships between gear type, several aspects of fishing effort (i.e. hook number, engine power, number of sets, trip length), standardised CPUE of all shark species and standardised CPUE of threatened and regulated species suggest that there is an optimal effort that could increase overall CPUE of the fishery and significantly reduce fishing mortality of species of conservation concern. For example, our data suggest that CPUE peaks with low to intermediate trip lengths and gear sets, intermediate engine power and hook numbers of less than 75 per set longline. Although standardised CPUE of threatened and regulated species is also higher when fewer hooks are deployed, the catch per set and overall mortality is significantly lower. Regulations that control the number of hooks in combination with incentives for shark fishers to tightly manage the number of hooks they deploy could significantly reduce mortality of threatened and endangered species, maximise the overall CPUE of the fishery, and reduce operational costs for fishers, making shark fishing in Tanjung Luar more sustainable and more cost effective [39\u201341]. Acknowledging that almost half of Tanjung Luar\u2019s shark catch consists of CITES-listed species, developing measures that ensure both the sustainability of the fishery, and full traceability and control of onward trade, will be crucial for implementing CITES ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Supreme Court's decision in Wayfair v. Amex challenged the physical presence rule, a constitutional precedent for stare decisis. This rule has been subject to various justifications, including the idea that Congress's ability to correct an erroneous decision is irrelevant. However, some argue that the Court's new thinking on stare decisis threatens other constitutional default rules. Which of the following best captures the essence of the Court's reasoning in Wayfair?",
    "choices": [
      "A) The Court's decision in Wayfair was influenced by the idea that stare decisis is a \"magic\" word that should be used to justify the continued application of a precedent, regardless of the facts on the ground.",
      "B) The Court's new thinking on stare decisis is based on the notion that the physical presence rule is a constitutional precedent that can be overruled due to changed facts, and that Congress's ability to correct an erroneous decision is irrelevant.",
      "C) The Court's decision in Wayfair was motivated by the desire to limit the power of Congress to correct an erroneous decision, and to instead rely on the formalism of stare decisis to justify the continued application of a precedent.",
      "D) The Court's new thinking on stare decisis is rooted in the idea that the physical presence rule is a constitutional precedent that can be overruled due to changed facts, and that the continued application of stare decisis is necessary to prevent the Court from getting the Constitution wrong."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Or it could have argued that new facts on the ground \u2014 namely, the blast of e-commerce that hit like a comet after Quill \u2014 overpowered stare decisis of any force, special or plain.61\u00d7 61. Two recent studies of stare decisis highlighted the physical presence rule as exemplifying a precedent that may reasonably be overruled due to changed facts. See Bryan A. Garner et al., The Law of Judicial Precedent 364\u201365 (2016); Randy J. Kozel, Settled Versus Right: A Theory of Precedent 112\u201313 (2017). It should be noted that the authors of The Law of Judicial Precedent classify the physical presence rule as a constitutional precedent for stare decisis purposes, thus anticipating the Court\u2019s misstep in Wayfair. Garner et al., supra, at 354\u201365. Because even statutory precedents may sometimes be overruled,62\u00d7 62. See Patterson v. McLean Credit Union, 491 U.S. 164, 173\u201374 (1989) (discussing justifications for overruling statutory precedents). Contra Lawrence C. Marshall, \u201cLet Congress Do It\u201d: The Case for an Absolute Rule of Statutory Stare Decisis, 88 Mich. L. Rev. 177 (1989). the Court could have killed Quill without first planting its constitutional kiss of death.63\u00d7 63. Cf. Thomas R. Lee, Stare Decisis in Historical Perspective: From the Founding Era to the Rehnquist Court, 52 Vand. L. Rev. 647, 704 (1999) (\u201cJustice Brandeis\u2019 . . . memorable prose has since become a mandatory part of the burial rite for any constitutional precedent.\u201d). The Court resisted such arguments. Instead, Wayfair reasoned that Congress\u2019s total ability to correct an erroneous decision counts for nothing when the Court gets the Constitution wrong. That such a theory sprouts from a case like Wayfair, which repudiated a \u201cformalistic distinction,\u201d64\u00d7 64. Wayfair, 138 S. Ct. at 2092. is ironic. Wayfair\u2019s stare decisis analysis resorts to the formalism of making constitutional a \u201cmagic\u201d word65\u00d7 65. See Transcript of Oral Argument, supra note 21, at 12. rather than asking whether Congress can step in. Moreover, the Court\u2019s new thinking on stare decisis threatens other constitutional default rules."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that rats given a high dose of vitamin K3 (menadione) developed blood-clot formation, which is a concern for human health. However, the researchers also noted that the rats were not given warfarin, a common anticoagulant that blocks vitamin K recycling. In fact, warfarin is often used to treat patients with vitamin K deficiency. Which of the following statements best explains why vitamin K3 (menadione) is toxic at high levels?",
    "choices": [
      "A) Vitamin K3 (menadione) is a natural form of vitamin K that is produced by the body, and its toxicity is due to its inability to be converted to its active form.",
      "B) Vitamin K3 (menadione) is a synthetic form of vitamin K that is similar to warfarin, and its toxicity is due to its ability to block vitamin K recycling.",
      "C) Vitamin K3 (menadione) is a form of vitamin K that is commonly used in pet food, and its toxicity is due to its high levels of isoprenoid residues.",
      "D) Vitamin K3 (menadione) is a form of vitamin K that is capable of reversing the anticoagulant activity of warfarin, and its toxicity is due to its ability to cause allergic reactions and cytotoxicity in liver cells."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Even doses in rats as high as 250 mg/kg, body weight did not alter the tendency for blood-clot formation to occur.[14]\nUnlike the safe natural forms of vitamin K1 and vitamin K2 and their various isomers, a synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells.[2]\nPhylloquinone (K1)[15][16] or menaquinone (K2) are capable of reversing the anticoagulant activity of the anticoagulant warfarin (tradename Coumadin). Warfarin works by blocking recycling of vitamin K, so that the body and tissues have lower levels of active vitamin K, and thus a deficiency of vitamin K.\nSupplemental vitamin K (for which oral dosing is often more active than injectable dosing in human adults) reverses the vitamin K deficiency caused by warfarin, and therefore reduces the intended anticoagulant action of warfarin and related drugs.[17] Sometimes small amounts of vitamin K are given orally to patients taking warfarin so that the action of the drug is more predictable.[17] The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient.[citation needed] The action of warfarin and vitamin K both require two to five days after dosing to have maximum effect, and neither warfarin or vitamin K shows much effect in the first 24 hours after they are given.[18]\nThe newer anticoagulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K, and may be taken with supplemental vitamin K.[19][20]\nVitamin K2 (menaquinone). In menaquinone, the side chain is composed of a varying number of isoprenoid residues. The most common number of these residues is four, since animal enzymes normally produce menaquinone-4 from plant phylloquinone. A sample of phytomenadione for injection, also called phylloquinone\nThe three synthetic forms of vitamin K are vitamins K3 (menadione), K4, and K5, which are used in many areas, including the pet food industry (vitamin K3) and to inhibit fungal growth (vitamin K5).[21]\nConversion of vitamin K1 to vitamin K2[edit]\nVitamin K1 (phylloquinone) \u2013 both forms of the vitamin contain a functional naphthoquinone ring and an aliphatic side chain."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A technology company is considering expanding its product line to include a new type of portable device that combines the functionality of a laptop and a tablet. The company's current laptop sales have been stagnant, while tablet sales have been increasing. However, the company's research indicates that the tablet market is expected to saturate in the next few years, and the company's current manufacturing capacity is not sufficient to meet the demand for both laptops and tablets. What is the most likely reason for the company to abandon its laptop business?",
    "choices": [
      "A) The company's laptops are not competitive with tablets in terms of price and features, and the tablet market is expected to continue growing.",
      "B) The company's laptops are too expensive and heavy, making them less appealing to consumers who prefer lighter and more affordable devices.",
      "C) The company's laptops are not designed to be easily upgradable, making them less attractive to consumers who want to be able to customize their devices.",
      "D) The company's laptops are not compatible with the new tablet operating system, making it difficult for consumers to switch to a tablet."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Intel has a large benefit of having a relatively \"good name\" when it comes to CPUs, so they can effectively charge a brand-name premium. I'm sure there are other reasons, and probably better reasons, but these are the main ones that I think of. Mabsark wrote:Actually, that trend will not simply keep increasing going forward. The reason desktop/laptop sales are stagnating/decreasing is due to the fact that most people already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease. The PC market is saturated and in a couple of years, the tablet market will be saturated too. Basically, in order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product. That's true as long as most people are still buying both a tablet and a laptop when each needs to be replaced. I think the assumption is that, as you say, the tablet market will saturate, with people just replacing existing ones, but the desktop/laptop market could decrease much farther than that, if most people stop replacing them at all. I'm not sure of the likelihood of that, but I think that's where this idea comes from. ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward. But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that. The upcoming Haswell chip is showing to consume 1/3 the power of IvyBridge at peak, consumes 1/20th the power at idle, all the while maintaining Identical or better performance. This chip should actually compete with ARM CPUs on both power/performance and idle."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A local church is considering a new community outreach program that aims to provide food and clothing to underprivileged families in the area. The program's budget is limited, and the church must decide how to allocate its resources. The pastor of the church has a strong conviction that the program should focus on providing spiritual support to the families, rather than just material assistance. However, the church's leadership is concerned that this approach may not be effective in addressing the immediate needs of the families. Which of the following options best reflects the church's approach to the outreach program?",
    "choices": [
      "A) The church should allocate its resources to provide spiritual support to the families, as this will have a more lasting impact on their well-being.",
      "B) The church should focus on providing material assistance to the families, as this is the most effective way to address their immediate needs.",
      "C) The church should allocate its resources to provide a combination of spiritual support and material assistance, as this will allow the families to receive both types of support.",
      "D) The church should not allocate its resources to the outreach program, as the focus on spiritual support may not be effective in addressing the immediate needs of the families."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This is a story about fishing. This isn\u2019t only a story about four fisherman, or only a story about fishing. It\u2019s also, and perhaps, most importantly, a story about God. If this is only a story about four fisherman who decide to follow Jesus, the pressures on you and me! After all, aren\u2019t we too called to follow Jesus? Called to be his disciples? Wasn\u2019t that the invitation you first heard when you first heard about Jesus? God has called us and we must decide. Jesus wants us all to follow him, to be like him, to walk in his footsteps, to do what he does. Of course this story is about that! And they do it, don\u2019t they? Simon, Andrew, James, John, they do it! They decide and they do follow Jesus, imperfectly at that. Still, it\u2019s a lot of pressure, a lot of responsibility. If life becomes all about what we do for Jesus, something is missing. If this is only a story about fishing, have some of us failed? Is it too late for us? Some of us might not be the best at fishing, not all the great about casting Jesus\u2019 loving net to our brothers and sisters. His net is sometimes, or maybe more than sometimes, a bit more expansive than we might be comfortable with. He calls us to be like him and fish for people, and yet, sometimes we can barely get the net into the water. Perhaps for others, we aren\u2019t even convinced that Jesus would include us in the net at all, no matter how deep into the water he goes. He can really mean me? Would his net really reach me? There\u2019s still more to the story. This is a story about God, who God is, how God acts, what God does. Before Andrew, Simon, James and John follow Jesus, Jesus finds them. Before they follow Jesus, Jesus comes to them! They don\u2019t have to go searching, they have been found. Jesus saw. Jesus spoke. Jesus called. Jesus said, \u201cCome.\u201c We don\u2019t follow Jesus in order to find him, to prove our worthiness with what we do, or even by showing Jesus how big our nets are. We follow Jesus because he first came to us. He came down to the beach to meet these four fishermen. He came specifically for Simon and for Andrew, for James and for John, for you and me."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the non-equilibrium flow behavior of a fluid system using the DBM model. They are interested in characterizing the TNE strength and describing the TNE behaviors of the system. Which of the following statements best describes the TNE strength from a macroscopic perspective?",
    "choices": [
      "A) The TNE strength is directly proportional to the difference between the central velocity and the macro flow velocity of the mixture.",
      "B) The TNE strength is a measure of the non-organized momentum flux and heat flux in the system, and can be described using the viscous stress tensor and heat flux tensor.",
      "C) The TNE strength is a measure of the non-equilibrium flow behavior of the system, and can be described using the central velocity and the macro flow velocity of the mixture.",
      "D) The TNE strength is a measure of the non-equilibrium flow behavior of the system, and can be described using the non-conservative moments of (f - f_eq) and the TNE quantities \u2206\u03c3*2, \u2206\u03c3*3,1, \u2206\u03c3*3, and \u2206\u03c3*4,2."
    ],
    "correct_answer": "D)",
    "documentation": [
      "They are all helpful to characterize the TNE strength and describe the TNE behaviors of a fluid system from their perspectives. But it is not enough only relying on these quantities. Besides the above physical quantities describing the TNE behaviors, in DBM modeling, we can also use the non-conservative moments of ( f \u2212 f eq ) to characterize the TNE state and extract TNE information from the fluid system. Fundamentally, four TNE quantities can be defined in a firstorder DBM, i.e., \u2206 \u03c3 * 2 , \u2206 \u03c3 * 3,1 , \u2206 \u03c3 * 3 , and \u2206 \u03c3 * 4,2 . Their definitions can be seen in Table , where v * i = v i \u2212 u represents the central velocity and u is the macro flow velocity of the mixture. Physically, \u2206 \u03c3 * 2 = \u2206 \u03c3 * 2,\u03b1\u03b2 e \u03b1 e \u03b2 and \u2206 \u03c3 * 3,1 = \u2206 \u03c3 * 3,1 e \u03b1 represent the viscous stress tensor (or non-organized momentum flux, NOMF) and heat flux tensor (or non-organized energy flux, NOEF), respectively. The e \u03b1 (e \u03b2 ) is the unit vector in the \u03b1 (\u03b2 ) direction. The later two higher-order TNE quantities contain more condensed information. Specifically, and it indicates the flux information of \u2206 \u03c3 * 2 . To describe the TNE strength of the whole fluid system, some TNE quantities contained more condensed information are also defined, i.e.,\nOther TNE quantities can be defined based on specific requirements. All the independent components of TNE characteristic quantities open a highdimensional phase space, and this space and its subspaces provide an intuitive image for characterizing the TNE state and understanding TNE behaviors . It should be emphasized that: (i) The TNE strength/intensity/degree is the most basic parameter of non-equilibrium flow description; And any definition of non-equilibrium strength/intensity/degree depends on the research perspective. (ii) The physical meaning of D * m,n is the TNE strength of this perspective. (iii) From a certain perspective, the TNE strength is increasing; While from a different perspective, the TNE strength, on the other hand, may be decreasing. It is normal, one of the concrete manifestations of the complexity of non-equilibrium flow behavior."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is investigating the reconstruction error of a method for inferring the state of a system from noisy measurements. They find that the reconstruction error decreases as the temperature of the system decreases, but increases as the temperature approaches zero. They also observe that the decimation method improves the reconstruction error by almost an order of magnitude compared to other methods. Which of the following statements best describes the relationship between the reconstruction error and the temperature of the system?",
    "choices": [
      "A) The reconstruction error decreases as the temperature of the system increases, and the decimation method has no effect on the reconstruction error.",
      "B) The reconstruction error decreases as the temperature of the system decreases, and the decimation method improves the reconstruction error by a factor of 10 for every 10% decrease in temperature.",
      "C) The reconstruction error decreases as the temperature of the system decreases, but the decimation method has no effect on the reconstruction error.",
      "D) The reconstruction error decreases as the temperature of the system decreases, and the decimation method improves the reconstruction error by almost an order of magnitude, with the optimal temperature being close to the critical point of the system."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The behavior of the inference quality in temperature and in the size of data samples is also investigated, basically confirming the low $T$ behavior hinted by Nguyen and Berg \\cite{Nguyen12b} for the Ising model. In temperature, in particular, the reconstruction error curve displays a minimum at a low temperature, close to the critical point in those cases in which a critical behavior occurs, and a sharp increase as temperature goes to zero. The decimation method, once again, appears to enhance this minimum of the reconstruction error of almost an order of magnitude with respect to other methods. The techniques displayed and the results obtained in this work can be of use in any of the many systems whose theoretical representation is given by Eq. \\eqref{eq:HXY} or Eq. \\eqref{eq:h_im}, some of which are recalled in Sec. \\ref{sec:model}. In particular, a possible application can be the field of light waves propagation through random media and the corresponding problem of the  reconstruction of an object seen through an opaque medium or a disordered optical fiber \\cite{Vellekoop07,Vellekoop08a,Vellekoop08b, Popoff10a,Akbulut11,Popoff11,Yilmaz13,Riboli14}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the evolution of shock-accelerated heavy bubbles in a numerical simulation. The simulation is based on the Euler model, which assumes a continuous hypothesis. However, the researcher notices that the simulation is not capturing the effects of material defects on the bubble behavior. What is the primary limitation of the Euler model in this context?",
    "choices": [
      "A) The Euler model is not capable of capturing the effects of non-continuity in the system, which is pronounced in cases with small structures.",
      "B) The Euler model assumes a thermodynamic equilibrium state, which is not sufficient to describe the complex behavior of shock-accelerated heavy bubbles.",
      "C) The Euler model is limited to smaller spatiotemporal scales due to its high computational costs, making it unsuitable for studying large-scale phenomena.",
      "D) The Euler model is based on the assumption of a continuous hypothesis, which is challenged by the presence of material defects that introduce non-continuity in the system."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Generally, there are three kinds of physical modeling methods (or models) for SBI numerical research, i.e., the macroscopic, mesoscopic, and microscopic modeling methods. Most of the existing numerical researches on SBI are related to the macroscopic modeling methods (such as the Euler and Navier-Stokes (NS) models) based on the continuous hypothesis (or equilibrium and nearequilibrium hypothesis) . For example, presented the computational results on the evolution of the shock-accelerated heavy bubbles through the multi-fluid Eulerian equation . There also exist a few SBI works based on the mesoscopic modeling method, such as the Direct Simulation Monte Carlo method . The microscopic modeling methods such as the Molecular dynamics (MD) simulation, is capable of capturing much more flow behaviors but restricted to smaller spatiotemporal scales because of its huge computing costs. In the numerical research on SBI, three points need to be concerned. (i) Investigation of kinetic modeling that describes the non-continuity/non-equilibrium flows. Most of the current researches are based on macroscopic models. However, there exist abundant small structure (and fast-changing patterns) behaviors and effects such as the shock wave, boundary layer, material defects, etc. For cases with small structures, the mean free path of molecules cannot be ignored compared to the characteristic length, i.e., the non-continuity (discreteness) of the system is pronounced, which challenge the rationality and physical function of the macroscopic models based on the continuity hypothesis. For cases with fast-changing patterns, the system dose not have enough time to relax to the thermodynamic equilibrium state, i.e., the system may significantly deviate from the thermodynamic equilibrium state. Therefore, the rational-ity and physical function of the macroscopic models based on the hypothesis of thermodynamic equilibrium (or near thermodynamic equilibrium) will be challenged. (ii) Improvement of method that describes the evolution characteristics of bubbles and flows morphology."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study suggests that the increasing popularity of smartphones has led to a significant decrease in the average lifespan of mobile devices. However, this trend is not unique to smartphones, as many other electronic devices have also experienced a similar decline in lifespan. Which of the following statements best explains this phenomenon?",
    "choices": [
      "A) The increasing complexity of modern smartphones has led to a decrease in their overall durability, resulting in a shorter lifespan.",
      "B) The widespread adoption of smartphones has created a culture of disposability, where consumers are more likely to upgrade to newer devices rather than repairing or maintaining their existing ones.",
      "C) The decreasing cost of smartphones has led to a decrease in the average lifespan of mobile devices, as manufacturers are no longer able to recoup their investment in the device through extended use.",
      "D) The increasing use of cloud-based services and streaming has led to a decrease in the average lifespan of mobile devices, as users are no longer reliant on their devices for storage and processing power."
    ],
    "correct_answer": "D)",
    "documentation": [
      "I am expecting a large war. Apple once again is dictating the performance in the mobile industry. Nice to see others being able to keep the pace, as well.\npaul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple evolutionary path by the SoC providers since then. Yeah, and most of the innovation in the automobile industry came about before Henry Ford came into the business. Doesn't change the fact that cars would probably have been an asterisk in the history books under \"toys for rich people\" if it weren't for him. The same applies to to mobile computing for Apple, Samsung, et al.\nSheldonRoss wrote:Lagrange wrote:The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each?The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the levels of menaquinone-7 and menaquinone-8 in the blood of postmenopausal women were significantly lower than in premenopausal women. This decrease was associated with a higher risk of osteoporosis. However, the study also found that the intake of fermented soybeans, which is a rich source of vitamin K2, was inversely correlated with the risk of osteoporosis. Which of the following statements best summarizes the relationship between vitamin K2 intake and osteoporosis risk?",
    "choices": [
      "A) Vitamin K2 intake is directly correlated with osteoporosis risk, with higher intake leading to a higher risk.",
      "B) Vitamin K2 intake is inversely correlated with osteoporosis risk, with higher intake leading to a lower risk.",
      "C) Vitamin K2 intake has no effect on osteoporosis risk, as the relationship between vitamin K2 and osteoporosis is complex and influenced by multiple factors.",
      "D) Vitamin K2 intake is only beneficial for osteoporosis risk in individuals who are deficient in vitamin K2, and has no effect on individuals with adequate vitamin K2 levels."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\"Age-related changes in the circulating levels of congeners of vitamin K2, menaquinone-7 and menaquinone-8\". Clinical Science. 78 (1): 63\u201366. PMID 2153497. ^ \"Vitamin K\". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press. 2001. p. 162\u2013196. ^ Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006 ^ a b Rh\u00e9aume-Bleue, p. 42\n^ \"Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K\" (PDF). National Institutes of Health Clinical Center. ^ \"Nutrition Facts and Information for Parsley, raw\". Nutritiondata.com. Retrieved 21 Apr 2013. ^ \"Nutrition facts, calories in food, labels, nutritional information and analysis\". Nutritiondata.com. 13 Feb 2008. Retrieved 21 Apr 2013. ^ \"Vitamin K\". Vivo.colostate.edu. 2 Jul 1999. Retrieved 21 Apr 2013. ^ \"Vitamin K\". Micronutrient Data Centre. ^ Ikeda, Y.; Iki, M.; Morita, A.; Kajita, E.; Kagamimori, S.; Kagawa, Y.; Yoneshima, H. (May 2006). \"Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women: Japanese Population-Based Osteoporosis (JPOS) Study\". Journal of Nutrition. 136 (5): 1323\u20131328. PMID 16614424. ^ Katsuyama, H.; Ideguchi, S.; Fukunaga, M.; Saijoh, K.; Sunami, S. (Jun 2002). \"Usual dietary intake of fermented soybeans (Natto) is associated with bone mineral density in premenopausal women\". Journal of Nutritional Science and Vitaminology. 48 (3): 207\u2013215. doi:10.3177/jnsv.48.207. PMID 12350079. ^ Sano, M.; Fujita, H.; Morita, I.; Uematsu, H.; Murota, S. (Dec 1999). \"Vitamin K2 (menatetrenone) induces iNOS in bovine vascular smooth muscle cells: no relationship between nitric oxide production and gamma-carboxylation\". Journal of Nutritional Science and Vitaminology. 45 (6): 711\u2013723. doi:10.3177/jnsv.45.711. PMID 10737225. ^ Gast, G. C ; de Roos, N. M.; Sluijs, I.; Bots, M. L.; Beulens, J. W.; Geleijnse, J. M.; Witteman, J. C.; Grobbee, D. E.; Peeters, P. H.; van der Schouw, Y. T. (Sep 2009)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic hand is designed to perform delicate tasks, such as handling small objects with precise movements. The hand's control system uses tactile feedback from the fingertips to provide information about the characteristics of the exerted contact forces. However, the system's performance can be affected by the object's weight and material. What is the primary advantage of using a shared synergy space in the hand's control system?",
    "choices": [
      "A) It allows for more precise control over individual finger movements, which is essential for handling objects with varying weights and materials.",
      "B) It enables the system to adapt to changes in the object's weight and material by adjusting the desired normal forces applied by the fingertips.",
      "C) It simplifies the control system's design by reducing the number of independent force controllers needed for each finger.",
      "D) It enables the system to learn and adapt to new objects and environments through machine learning models, such as neural networks and random forests."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Using this framework we are able to control the hand during different grasp types using only one variable, the grasp size, which we define as the distance between the tip of the thumb and the index finger. Instead of controlling the finger limbs independently, our controller generates control signals for all the hand joints in a (lowdimensional) shared space (i.e.\nsynergy space). In addition, our approach is modular, which allows to execute various types of precision grips, by changing the synergy space according to the type of grasp. We show that our controller is able to lift objects of various weights and materials, adjust the grasp configuration during changes in the object's weight, and perform object placements and object handovers. INTRODUCTION\n\nTo perform complex manipulation tasks in unstructured environments, humans use tactile feedback from their fingers. This feedback is provided by tactile afferents located in the skin of the hand. Particularly, for handling small objects with precise movements, the afferents located in the fingertips are used, which have high density and adapt fast to pressure changes . These afferents provide information about the characteristics of the exerted contact forces, such as the magnitude and the direction. For anthropomorphic robots to be able to perform dexterous tasks similar force feedback signals must be used to alleviate problems arising from uncertainty in measurements, and handle external perturbations. For example, using open-loop position control to lift a heavy object may fail due to slip without any feedback mechanism to provide tactile information. Previous works have used tactile sensors to design force controllers that use slip prediction to update the desired normal forces applied by the fingertips. The slip predictors are based on machine learning models such as neural networks and random forests to classify multi-modal signals from a tactile sensor. In all previous works, each finger was separately controlled by an independent force controller."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new feature has been added to the game that allows players to earn extra points for participating in Alliance Events. However, this feature is only available to players who have reached a certain level of Summoner Mastery. Which of the following statements is true about this new feature?",
    "choices": [
      "A) It is only available to players who have spent a certain amount of Units on stamina recharges and unlocking arenas.",
      "B) It is only available to players who have defeated their competition in classic Arena combat and have a high rank in the Alliance.",
      "C) It is only available to players who have contributed to their Alliance's win in the competition and have a high rank in the Alliance.",
      "D) It is only available to players who have reached a certain level of Summoner Mastery and have a high rank in the Alliance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Fixed a bug with Rocket Raccoon\u2019s Dash attack being slower than intended. Added a confirmation popup when spending Units on stamina recharges and unlocking arenas. Regeneration no longer displays green Health values if you\u2019re at full Health. Several new improvements to how status effects are displayed. AI opponents are no longer able to perform one unavoidable attack in response to a Special Attack 3. A new and improved look for all Health Potions in the Battlerealm. All Revive Potions now revive your Champions with +10% more Health. We\u2019re adding so many new Champions, they could form their own Alliance! Some of your favourite heroes of the Marvel Cinematic Universe join The Contest!\nSummoner Mastery is on the horizon! Masteries provide beneficial effects for your Champions. Access Masteries through your Summoner Profile. Earn Mastery Points when you level up. Choose your Masteries wisely and strategically customize your benefits. Recover your points to try a new specialization as often as you\u2019d like. Keep an eye on in-game messaging for more information. The daily loyalty limit has been set to refresh at 08:00UTC for all players. A timer has been added to show when the daily loyalty limit resets. Loyalty balance is now displayed in the Alliance menus. Ask for Versus help with a single tap on the \u2018Help\u2019 icon in Team Select. New Alliance Events are coming very soon! Work together with your Alliance to complete objectives and receive rewards! Muster your might, Alliance Arenas will soon open their gates! Competing in Alliance Arenas shares your points across your whole Alliance; work together to reach milestones and top ranks! Work together to amass a huge score, and defeat your competition in classic Arena combat! No slackers here either - if you don\u2019t contribute to win the competition, you\u2019re not eligible for the goods! All social features (Chat, Mail, and Friends) can now be accessed through the new Social Hub. Search for and add friends, and send private messages to Summoners on your Friends List."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new quantum processor is being designed to simulate the time evolution of a quantum many-body system with controllable precision. The processor will utilize a supercircuit to achieve this, but due to hardware limitations, the supercircuit needs to be interpreted as an ensemble of circuits. To mitigate the accumulated errors, the denoiser will be constructed by averaging over many samples of the noisy circuit. However, the dimensionality of the quasiprobabilistic mitigating ensemble can be controlled, and the goal is to find the optimal number of samples required to achieve a good approximation of the noiseless expectation values.",
    "choices": [
      "A) To achieve a good approximation of the noiseless expectation values, the dimensionality of the quasiprobabilistic mitigating ensemble should be equal to the gate count.",
      "B) The optimal number of samples required to achieve a good approximation of the noiseless expectation values is independent of the dimensionality of the quasiprobabilistic mitigating ensemble.",
      "C) The dimensionality of the quasiprobabilistic mitigating ensemble should be equal to the number of gates in the supercircuit.",
      "D) The optimal number of samples required to achieve a good approximation of the noiseless expectation values is directly proportional to the dimensionality of the quasiprobabilistic mitigating ensemble."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This circuit architecture is commonly used to simulate the time evolution of a quantum many-body system, until some time t, with controllable precision , and we will use it to benchmark the denoiser. In practice, we cannot directly implement a supercircuit, and so we have to utilize its interpretation as an ensemble of circuits. Essentially, after executing a shot of the noisy circuit we sample the denoiser and apply it. The goal is to construct the denoiser in a way that averaging over many of its samples cancels the accumulated errors and gives us a good approximation of the noiseless expectation values. It should be noted that our approach requires more gate applications on the quantum processor than with the gate-wise scheme, since there each sample from the mitigation quasiprobability distribution can be absorbed into the original circuit, whereas our approach increases the circuit depth. We take this into account by imposing the same noise on the denoiser. Furthermore, within our scheme, the dimensionality of the quasiprobabilistic mitigating ensemble can be controlled, in contrast to the gate-wise approach where it is equal to the gate count. To facilitate the stochastic interpretation we parameterize each two-qubit denoiser channel G i as a sum of CPTP maps, such that we can sample the terms in this sum and execute the sampled gate on the quantum processor. Concretely, we use a trace preserv-ing sum of a unitary and a non-unitary channel. For the unitary part we take a two-qubit unitary channel U( \u03c6 i ) = U ( \u03c6 i ) \u2297 U * ( \u03c6 i ), with U ( \u03c6 i ) a two-qubit unitary gate parameterized by \u03c6 i . For this we take the two-qubit ZZ rotation exp(\u2212i\u03b1(\u03c3 z \u2297 \u03c3 z )) with angle \u03b1, which can be obtained from native gates on current hardware , and dress it with four general one-qubit unitaries, only two of which are independent if we want a circuit that is space inversion symmetric around every bond. The resulting gate has 7 real parameters \u03c6 i . For the non-unitary part, which is essential because D has to cancel the non-unitary accumulated noise to obtain the noiseless unitary circuit, we use a general onequbit measurement followed by conditional preparation channel M( , with V a general one-qubit unitary and each \u03ba i a 3-dimensional vector, resulting in a real 9-dimensional \u03b6 i ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new study suggests that individuals with hemoglobin H disease may be at increased risk of developing severe anemia due to exposure to certain environmental toxins. However, the study's findings are based on a small sample size and have not been replicated in larger studies. What is the primary concern for individuals with hemoglobin H disease regarding environmental toxins?",
    "choices": [
      "A) The risk of developing hemoglobin H disease is increased by exposure to certain toxins, but the overall impact on life expectancy is still unclear.",
      "B) Individuals with hemoglobin H disease are more likely to experience severe anemia due to the theoretical risk of toxin exposure, which can be mitigated through education and avoidance of high-risk substances.",
      "C) The study's findings suggest that individuals with hemoglobin H disease are at increased risk of developing severe anemia due to toxin exposure, but the risk is not significantly higher than that of the general population.",
      "D) The primary concern for individuals with hemoglobin H disease regarding environmental toxins is the potential for toxin exposure to trigger a severe anemia event, which can be treated with blood transfusion therapy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The risk for specific individuals depends on current health status, age, and other factors. Because of the risks involved and the fact that beta thalassemia is a treatable condition, transplant physicians require a brother or sister donor who has an identically matched tissue type, called HLA type. HLA type refers to the unique set of proteins present on each individual's cells, which allows the immune system to recognize \"self\" from \"foreign.\" HLA type is genetically determined, so there is a 25% chance for two siblings to be a match. Transplant physicians and researchers are also investigating ways to improve the safety and effectiveness of bone marrow transplantation. Using newborn sibling umbilical cord blood\u2014the blood from the placenta that is otherwise discarded after birth but contains cells that can go on to make bone marrow\u2014seems to provide a safer and perhaps more effective source of donor cells. Donors and recipients may not have to be perfect HLA matches for a successful transplant using cord blood cells. Trials are also underway to determine the effectiveness of \"partial transplants,\" in which a safer transplant procedure is used to replace only a percentage of the affected individual's bone marrow. Other possible treatments on the horizon may include gene therapy techniques aimed at increasing the amount of normal hemoglobin the body is able to make. Hemoglobin H disease is a relatively mild form of thalassemia that may go unrecognized. It is not generally considered a condition that will reduce one's life expectancy. Education is an important part of managing the health of an individual with hemoglobin H disease. It is important to be able to recognize the signs of severe anemia that require medical attention. It is also important to be aware of the medications, chemicals, and other exposures to avoid due to the theoretical risk they pose of causing a severe anemia event. When severe anemia occurs, it is treated with blood transfusion therapy. For individuals with hemoglobin H disease, this is rarely required."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the performance of a least-mean-square (LMS) filter in a tracking scenario. The filter is designed to adapt to changing channel conditions, and its performance is evaluated using the steady-state mean square deviation (MSD) metric. According to the results shown in Table 1, the LMS filter with a forgetting factor of 0.95 outperforms the standard LMS filter. However, the researcher notices that the proposed algorithm's performance is not as good as expected, especially in the presence of high-frequency components. What is the most likely reason for this discrepancy?",
    "choices": [
      "D) metric. According to the results shown in Table 1, the LMS filter with a forgetting factor of 0.95 outperforms the standard LMS filter. However, the researcher notices that the proposed algorithm's performance is not as good as expected, especially in the presence of high-frequency components. What is the most likely reason for this discrepancy?",
      "A) The proposed algorithm's step size is too small, resulting in slow convergence.",
      "B) The forgetting factor is too high, causing the algorithm to forget too much of the previous estimate.",
      "C) The algorithm is not properly regularized, leading to overfitting to the training data."
    ],
    "correct_answer": "D)",
    "documentation": [
      "More details on the setup can be found in \\cite{gutierrez2011frequency}. Fig. \\ref{fig_2} shows the real part of one of the channels, and the estimate of the proposed algorithm. The shaded area represents the estimated uncertainty for each prediction, i.e. $\\hat{\\mu}_k\\pm2\\hat{\\sigma}_k$. Since the experimental setup does not allow us to obtain the optimal values for the parameters, we fix these parameters to their values that optimize the steady-state mean square deviation (MSD). \\hbox{Table \\ref{tab:table_MSD}} shows this steady-state MSD of the estimate of the MISO channel with different methods. As can be seen, the best tracking performance is obtained by standard LMS and the proposed method. \n\n\n\n\n\n\\section{Conclusions and Opened Extensions}\n\\label{sec:conclusions}\n\n{We have presented a probabilistic interpretation of the least-mean-square filter. The resulting algorithm is an adaptable step-size LMS that performs well both in stationary and tracking scenarios. Moreover, it has fewer free parameters than previous approaches and these parameters have a clear physical meaning. Finally, as stated in the introduction, one of the advantages of having a probabilistic model is that it is easily extensible:}\n\n\\begin{itemize}\n\\item If, instead of using an isotropic Gaussian distribution in the approximation, we used a Gaussian with diagonal covariance matrix, we would obtain a similar algorithm with different step sizes and measures of uncertainty, for each component of ${\\bf w}_k$. Although this model can be more descriptive, it needs more parameters to be tuned, and the parallelism with LMS vanishes. \\item Similarly, if we substitute the transition model of \\eqref{eq:trans_eq} by an Ornstein-Uhlenbeck process, \n\n\\begin{equation}\np({\\bf w}_k|{\\bf w}_{k-1})= \\mathcal{N}({\\bf w}_k;\\lambda {\\bf w}_{k-1}, \\sigma_d^2), \\nonumber\n\\label{eq:trans_eq_lambda}\n\\end{equation}\na similar algorithm is obtained but with a forgetting factor $\\lambda$ multiplying ${\\bf w}_{k-1}^{(LMS)}$ in \\eqref{eq:lms}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If the opposition in Libya is unable to topple Moammar Gadhafi due to a lack of manpower, and the US government is considering recognizing the rebels, what are the potential consequences for the US military's involvement in the conflict?",
    "choices": [
      "A) The US military's involvement in Libya would be severely limited due to the lack of a clear victory condition, and the US would be forced to focus on other global priorities.",
      "B) The US government's recognition of the rebels would be seen as a tacit endorsement of Gadhafi's regime, and would likely lead to increased tensions between the US and Libya.",
      "C) The US military's involvement in Libya would be unaffected by the opposition's lack of manpower, as the US would be able to provide air support and logistical assistance to the rebels.",
      "D) The US government's consideration of recognizing the rebels would be a strategic miscalculation, as it would undermine the US's credibility as a champion of democracy and human rights in the region."
    ],
    "correct_answer": "D)",
    "documentation": [
      "CNN.com - Transcripts\nTensions Boil Over possible government shutdown; New trouble targeting Gadhafi; Libyan Rebels in Panicked Retreat; Should U.S. Recognize the Rebels?; Meeting With Gadhafi; Washington, D.C. to Feel Burden of Shutdown; Religious Leaders Fast to Protests Cuts for Poor\nWOLF BLITZER, HOST: Don, thanks very much. Happening now, the top U.S. general in charge of the military mission in Libya now expressing doubts that the opposition has the manpower to topple Moammar Gadhafi, as deadly new air strikes force rebel fighters into another retreat. This hour, I'll speak with a former Republican Congressman who's in Tripoli right now trying to get Gadhafi to step down. Also, growing outrage across the United States, amidst new signs tomorrow's potential government shutdown may -- repeat may be unavoidable. Why one lawmaker is telling Congress -- and I'm quoting right now -- \"go straight to hell. \"\nAnd possible presidential hopeful, Donald Trump, on a mission to tell President Obama, \"you're fired.\" We're fact checking his controversial investigation into the president's birth. Up first, the political showdown over the budget, as tensions reach a boiling point about 31 hours until impending government shutdown. Just hours from now, President Obama will meet with Republican House speaker, John Boehner, and the Democratic Senate majority leader, Harry Reid, for further negotiations. Those talks scheduled to begin 7:00 p.m. Eastern. Hundreds of thousands of people across the country will be impacted by the shutdown. And we'll be bringing you examples throughout the next two hours. One place it would be felt heavily is right here in Congress' backyard, the city of Washington. Washington, DC -- its spending is tied to the federal budget. And this major metropolitan area could lose millions of dollars while a number of critical services, like trash collection, for example, would be suspended for at least a week. Today, an enraged Eleanor Holmes, the delegate representing Washington, DC, lit into Congress over the stalemate."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If Benjamin Franklin lived on Craven Street in the 1740s, what can be inferred about the numbering system used on the street during that time?",
    "choices": [
      "A) The numbering system was based on the alphabetical order of the residents' names, with \"A\" being the first resident and \"Z\" being the last.",
      "B) The numbering system was based on the distance from the northernmost house on the west side of the street, with each house being numbered consecutively in a clockwise direction.",
      "C) The numbering system was based on the list of residents in the Westminster Rate Books, with the first resident listed being numbered one and subsequent residents being numbered in a counter-clockwise direction down the west side and up the east side.",
      "D) The numbering system was based on the street's original layout, with the first house being numbered one and subsequent houses being numbered in a sequential manner, regardless of the direction of the street."
    ],
    "correct_answer": "C)",
    "documentation": [
      "Few of them were rated at more than a few shillings and many of them were unoccupied.\u201d[1] The landowner, William, 5th Baron Craven, desiring to increase the profitability of his assets, tore down the derelict structures on Spur Alley around 1730 and leased the newly established lots to builders. By 1735, twenty brick houses in the Georgian style had been built on the west side and sixteen on the east side of the way now called Craven Street.[2] Figure 2. Craven Street 1746. (John Rocque London, Westminster and Southwark, First Edition 1746, Motco Enterprises Limited, motco.com)\nLetters to Franklin during his residence with Mrs. Margaret Stevenson, his landlady on Craven Street, were addressed rather vaguely; \u201cCraven Street/Strand\u201d, \u201cMrs. Stevensons in Craven Street\u201d, or \u201cBenjamin Franklin Esqr.\u201d are but a few examples. Letters from Franklin referenced \u201cLondon,\u201d or sometimes \u201cCravenstreet,\u201d but never included a number. Despite the absence of numbered addresses in Franklin\u2019s correspondence, there was a sense of one\u2019s place in the neighborhood based on entries in the Westminster Rate Books (tax assessments). The Rate Books did not list house numbers during Franklin\u2019s time there, but they did list the residents of Craven Street in a particular order that became the default numbering system for the street. Number one was associated with the first resident listed under \u201cCraven Street\u201d in the Rate Books and was the northernmost house on the west side of the street. The numbers increased counter-clockwise down the west side and up the east side in accordance with the list of residents. In 1748, the first year of Margaret Stevenson\u2019s (Stevens in the Rate Books for that year) residence on Craven Street, she is listed as the twenty-seventh resident, the second house north of Court Street (later Craven Court, now Craven Passage) on the east side of the street.[3]\nIn 1766, Parliament passed the London Paving and Lighting Act (6 Geo. 3 c. 26), \u201cAn act for the better paving, cleansing, and enlightening, the city of London, and the liberties thereof; and for preventing obstructions and annoyances within the same; and for other purposes therein mentioned."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The ultracold plasma lifetime is crucial for producing the late-signal excitation spectra shown in Figure \\ref{fig:w2_spectra}. What is the primary mechanism by which the Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - affects the plasma lifetime?",
    "choices": [
      "A) The Penning fraction is directly proportional to the initial principal quantum number $n_0$, resulting in a linear increase in plasma lifetime with increasing $n_0$.",
      "B) The Penning fraction is inversely proportional to the initial principal quantum number $n_0$, leading to a decrease in plasma lifetime as $n_0$ increases.",
      "C) The Penning fraction is independent of the initial principal quantum number $n_0$, and its effect on plasma lifetime is solely determined by the ambipolar expansion.",
      "D) The Penning fraction is a critical factor in achieving the long ultracold plasma lifetime, as it regulates the balance between ion and Rydberg velocities, effectively channeling electron energy into the overall $\\pm x$ motion of gas volumes in the laboratory."
    ],
    "correct_answer": "D)",
    "documentation": [
      "}\n\\end{figure}\n\nFigure \\ref{fig:w2_spectra} shows a series of $\\omega_2$ late-signal excitation spectra for a set of initial densities. Here, we see a clear consequence of the higher-order dependence of Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - on $n_0$, the $\\omega_2$-selected Rydberg gas initial principal quantum number. This Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance appears necessary as a critical factor in achieving the long ultracold plasma lifetime required to produce this signal. We are progressing in theoretical work that explains the stability apparently conferred by this balance.  \n\n\n\\subsection{Bifurcation and arrested relaxation}\n\nAmbipolar expansion quenches electron kinetic energy as the initially formed plasma expands. Core ions follow electrons into the wings of the Rydberg gas. There, recurring charge exchange between NO$^+$ ions and NO$^*$ Rydberg molecules redistributes the ambipolar force of the expanding electron gas, equalizing ion and Rydberg velocities. This momentum matching effectively channels electron energy through ion motion into the overall $\\pm x$ motion of gas volumes in the laboratory. The internal kinetic energy of the plasma, which at this point is defined almost entirely by the ion-Rydberg relative motion, falls. Spatial correlation develops, and over a period of 500 ns, the system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure \\ref{fig:SFI}. \\begin{figure}[h!] \\centering\n\\includegraphics[width= .4 \\textwidth]{Bifurcation.pdf}\n   \\caption{$x,y$ detector images of ultracold plasma volumes produced by 2:1 aspect ratio ellipsoidal Rydberg gases with selected initial state, $40f(2)$ after a flight time of 402 $\\mu$s over a distance of 575 mm. Lower frame displays the distribution in $x$ of the charge integrated in $y$ and $z$.  Both images represent the unadjusted raw signal acquired in each case after 250 shots.  \n   }\n\\label{fig:bifurcation}\n\\end{figure}"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A large, hollow conducting sphere is placed in a region with a uniform electric field. The electric field is strong enough to cause a significant amount of charge to accumulate on the surface of the sphere. As a result, the electric field inside the sphere becomes zero due to the shielding effect of the conducting material. However, the sphere is then brought into contact with a high-voltage power source, causing a large amount of charge to be transferred to the sphere. What happens to the electric field inside the sphere after the power source is turned off?",
    "choices": [
      "A) The electric field inside the sphere remains zero, as the conducting material continues to shield the interior from the external electric field.",
      "B) The electric field inside the sphere becomes stronger, as the accumulated charge on the surface of the sphere induces a polarization field that opposes the external electric field.",
      "C) The electric field inside the sphere becomes zero again, as the charge on the surface of the sphere redistributes itself to cancel out the external electric field.",
      "D) The electric field inside the sphere becomes uniform and equal to the external electric field, as the conducting material allows the charge to redistribute itself to maintain a constant electric potential difference across the sphere."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker. A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects. The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh. A pair of AA cells. The + sign indicates the polarity of the potential difference between the battery terminals. The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity.:494\u201398 This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A nonlinear oscillator is subjected to a time-dependent excitation f(t) = sin(2\u03c0t) with a limited energy. The governing equation of motion is given by:\n\nm\u1e8f'' + c\u1e8f' + ky = f(t)\n\nwhere m, c, and k are the mass, damping, and linear stiffness, respectively. The nonlinear response under zero initial conditions can be represented by the Volterra series:\n\ny(t) = \u2211[h_n(\u03c4_1, ..., \u03c4_N) * sin(2\u03c0(\u03c4_1 + ... + \u03c4_N))]\n\nwhere h_n(\u03c4_1, ..., \u03c4_N) are the Volterra kernel functions. If the Volterra series converges, the nonlinear response can be computed using the pole-residue method.",
    "choices": [
      "A) The Volterra series only converges for systems with linear stiffness k = 1.",
      "B) The nonlinear response can be represented by a single Volterra kernel function h_1(\u03c4).",
      "C) The pole-residue method is only applicable to systems with a convergent Volterra series representation.",
      "D) The nonlinear response under zero initial conditions can be represented by the Volterra series, and the pole-residue method can be used to compute the response if the Volterra series converges."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Compared to Hu et al. , which was regarded as an efficient tool to compute responses of linear systems, the generalized pole-residue method in this paper is introduced to compute responses of nonlinear systems. The proposed method involves two steps: (1) the Volterra kernels are decoupled in terms of Laguerre polynomials, and (2) the partial response related to a single Laguerre polynomial is obtained analytically in terms of the pole-residue method. Compared to the traditional pole-residue method for a linear system, one of the novelties of the generalized pole-residue method is how to deal with the higher-order poles and their corresponding coefficients. Similar to the Taylor series, the Volterra series representation is an infinite series, and convergence conditions are needed to assure that the representation is meaningful. Because the proposed method is based on the Volterra series, only the system with convergent Volterra series representation can be treated by the proposed method. The paper is organized as follows. In Section 2, the nonlinear response is modelled by a Volterra series, and Volterra kernel functions are decoupled by Laguerre polynomials. Then, the pole-residue method for computing explicit responses is developed in Section 3. Numerical studies and discussions are given in Section 4. Finally, the conclusions are drawn in Section 5. Response calculation based on Volterra series\n\nA nonlinear oscillator, whose governing equation of motion is given by where z(t, y, \u1e8f) represents an arbitrary nonlinear term; m, c, and k are the mass, damping and linear stiffness, respectively; y(t), \u1e8f(t) and \u00ff(t) are the displacement, velocity and acceleration, respectively; and f (t) is the time-dependent excitation. If the energy of excitation f (t) is limited, the nonlinear response under zero initial conditions (i.e., zero displacement and zero velocity) can be represented by the Volterra series : where N is the order of Volterra series and In Eq. 3, h 1 (\u03c4 ) is called the first-order Volterra kernel function, which represents the linear behaviour of the system; h n (\u03c4 1 , . ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A local authority is considering the development of a new residential complex on a rural land that falls within a Planning Area. The landowner has applied for permission under the Act, but the application is incomplete as it does not include the required fee. Which of the following is a valid reason for the local authority to grant permission for the development?",
    "choices": [
      "A) The landowner has provided a detailed plan for the development, including the number of residential units and the expected impact on the local infrastructure.",
      "B) The development is exempt from permission under the Act because it is a small-scale project that does not require a Development Plan to be notified.",
      "C) The local authority has already approved a similar development project in the same area, and therefore, the new project should be allowed to proceed without further scrutiny.",
      "D) The landowner has provided a letter from the local panchayat office stating that the development will not have any adverse impact on the local community, and therefore, permission should be granted."
    ],
    "correct_answer": "D)",
    "documentation": [
      "30. Application for permission for development by others. - (1) Any person, not being the Union Government, State Government, a local authority or a special authority constituted under this Act intending to carry out any development on any land, shall make an application in writing to the Director for permission, in such form and containing such particulars and accompanied by such documents as may be prescribed. (2) Such application shall also be accompanied by such fee as may be prescribed. [30A. Exemption from development permission in rural areas falling within Planning or Special Area. - (1) Any person who owns land in rural areas, falling within Planning or Special Areas wherein neither Interim Development Plan nor Development Plan has been notified, shall be exempted from permission under this Act for the following development activities up to the limits as may be prescribed: -\n(i) Residential activities such as farm-houses and residential houses up to three storeys, cattle shed, toilet, septic tank, kitchen, store, parking shed or garage and rain shelter;\n(ii) Commercial activities such as basic commercial activities like shops of general merchandise, cobbler, barber, tailoring, fruit, vegetable, tea or sweet, eating places and dhabas, chemist and farm produce sale depot;\n(iii) Service Industries such as cottage or house-hold, service industries like carpentry, knitting, weaving, blacksmith, goldsmith, atta-chakki with capacity up to five horse-power, water mill, agriculture equipments or machinery repair, electrical, electronic and house-hold appliances;\n(iv) Public amenities such as public amenities like panchayat offices, schools, mahila mandals, yuvak mandals, community halls, post offices, dispensaries and clinics (including health, veterinary and Indian System of Medicines) information technology kiosks, patwar khanas, guard huts, anganwaries, electricity and telephone installations and connections, roads and paths, ropeways, water tanks, rain harvesting tanks, overhead or underground water tan."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user is setting up a Linux system and encounters an issue with the installation of the Go programming language. They have installed the `golang-go` package, but when they run `go version`, they get a version number that is lower than expected. What should they do to resolve this issue?",
    "choices": [
      "A) Run `sudo apt-get update` to update the package list and try installing the `golang-go` package again.",
      "B) Remove the `golang-go` package and install the latest version from the official Go website, using the command `wget https://studygolang.com/dl/golang/go1.11.linux-amd64.tar.gz`.",
      "C) Run `sudo apt-get remove golang-go` to uninstall the package, and then install the latest version from the official Go website, using the command `wget https://studygolang.com/dl/golang/go1.11.linux-amd64.tar.gz`.",
      "D) Run `sudo apt-get install golang-go` to reinstall the package, and then run `go version` to verify the version number."
    ],
    "correct_answer": "B)",
    "documentation": [
      "Gwenview\n\u662f\u8f83\u597d\u7684\u4e00\u9879\u5e94\u7528\uff0c\u652f\u6301\u51e0\u4e4e\u6240\u6709\u56fe\u7247\u683c\u5f0f\uff0c\u53ef\u8fdb\u884c\u57fa\u672c\u7684\u7f16\u8f91\u3001\u6807\u7b7e\u3001\u7f29\u7565\u56fe\u3001\u5168\u5c4f\u3001\u5e7b\u706f\u663e\u793a\u529f\u80fd\u7b49\u7b49\u3002\nsudo apt-get install gwenview\n2. Eye of GNOME\n\u662fGNOME\u73af\u5883\u4e0b\u8f83\u597d\u7684\u56fe\u7247\u67e5\u770b\u5668\uff0c\u652f\u6301JPG, PNG, BMP, GIF, SVG, TGA, TIFF or XPM\u7b49\u56fe\u7247\u683c\u5f0f\uff0c\u4e5f\u53ef\u653e\u5927\u3001\u5e7b\u706f\u663e\u793a\u56fe\u7247\u3001\u5168\u5c4f\u3001\u7f29\u7565\u56fe\u7b49\u529f\u80fd\u3002\nsudo apt-get install eog\n3. gThumb\n\u662f\u53e6\u4e00GTK\u56fe\u7247\u67e5\u770b\u5668\uff0c\u53ef\u5bfc\u5165Picasa\u6216Flickr\u56fe\u7247\uff0c\u4e5f\u53ef\u5bfc\u51fa\u5230 Facebook, Flickr, Photobucker, Picasa \u548c\u672c\u5730\u6587\u4ef6\u5939\u3002\n4. Viewnior\n\u662f\u5c0f\u578b\u5316\u7684\u56fe\u7247\u67e5\u770b\u5668\uff0c\u652f\u6301JPG\u548cPNG\u683c\u5f0f\u3002\nsudo apt-get install viewnior\n5.gPicView\n\u662fLXDE\u4e0b\u7684\u9ed8\u8ba4\u56fe\u7247\u67e5\u770b\u5668\uff0c\u64cd\u4f5c\u6309\u94ae\u4f4d\u4e8e\u7a97\u53e3\u5e95\u90e8\u3002\u53ea\u9700\u53f3\u51fb\u56fe\u7247\uff0c\u5b9e\u73b0\u6240\u6709\u76f8\u5173\u529f\u80fd\u3002\u652f\u6301JPG, TIFF, BMP, PNG \uff0c ICO\u683c\u5f0f\u3002\nsudo apt-get install gpicview\nhttps://www.linuxidc.com/Linux/2011-03/33659.htm\n\u4ee5\u592a\u574a\u591a\u8282\u70b9(\u4e24\u4e2a\u8282\u70b9)\u79c1\u94fe\u642d\u5efa\nhttps://blog.csdn.net/apple9005/article/details/81282735\nubuntu apt-get \u5b89\u88c5 golang \u7248\u672c\u8fc7\u4f4e\u95ee\u9898\napt-get install golang-go\u8fd9\u6837\u5b89\u88c5\u7248\u672c\u53ef\u80fd\u8fc7\u4f4e\u3002\ngo version\u67e5\u770b\u7248\u672c\u4e3a 1.6.2\u3002\napt-get \u5378\u8f7d\u6b64\u7248\u672c\u91cd\u65b0\u5b89\u88c5\n\u91cd\u65b0\u5b89\u88c5\n\u53bb\u5b98\u7f51\u67e5\u770b\u6700\u65b0\u7248\u94fe\u63a5 https://studygolang.com/dl\n\u6bd4\u5982\u6211\u8981\u4e0b\u7684\u662f https://studygolang.com/dl/golang/go1.11.linux-amd64.tar.gz\nwget https://studygolang.com/dl/golang/go1.11.linux-amd64.tar.gz\n\u4e5f\u53ef\u4ee5\u5230go\u8bed\u8a00\u4e2d\u6587\u7f51https://studygolang.com/dl\u4e0b\u8f7d\u6700\u65b0\u7248\ntar -zxvf go1.11.linux-amd64.tar.gz -C /usr/lib\n\u5c06\u89e3\u538b\u540e\u7684\u6587\u4ef6\u5939go\u79fb\u52a8\u5230 /usr/local\n\u8f93\u5165\u547d\u4ee4\uff1a sudo mv go /usr/local\n\u8bbe\u7f6e\u6dfb\u52a0\u73af\u5883\u53d8\u91cf\nsudo gedit ~/.profile \u5728\u6700\u540e\u9762\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\nexport PATH=$PATH:/usr/local/go/bin \u6216\u8005\nexport GOPATH=/opt/gopath export GOROOT=/usr/lib/go export GOARCH=386 export GOOS=linux export GOTOOLS=$GOROOT/pkg/tool export PATH=$PATH:$GOROOT/bin:$GOPATH/bin\n\u5378\u8f7d\u8001\u7684go\nsudo apt-get remove golang-go\n\u7ed3\u679c go version go1.11 linux/amd64\nhttps://blog.csdn.net/Booboochen/article/details/82463162\nhttps://www.jianshu.com/p/85e98e9b003d\n\u81ea\u4ece2015\u5e74\u5f00\u59cb\u4f7f\u7528ubuntu\u4e4b\u540e\uff0c\u5c31\u5f00\u59cb\u4e86\u5404\u79cd\u6298\u817e\u3002\u53ef\u60dc\u7684\u662f\uff0clinux\u4e0b\uff0c\u80fd\u7528\u7684\u97f3\u4e50\u8f6f\u4ef6\u5b9e\u5728\u662f\u5c11\u4e4b\u53c8\u5c11\uff01\u7f51\u6613\u4e91\u97f3\u4e50\u52c9\u5f3a\u53ef\u4ee5\uff0c\u4f46\u662f\u7ecf\u5e38\u6253\u4e0d\u5f00\u3002\u70e6\u6b7b\u3002\u5076\u7136\u53d1\u73b0\u8fd9\u4e2a\u8f6f\u4ef6\uff1aCoCoMusic\uff0c\u624d\u60ca\u89c9\u662fubuntu 18.04.2\u4e0b\u6700\u597d\u7528\u7684\u97f3\u4e50\u8f6f\u4ef6\uff01\u6ca1\u6709\u4e4b\u4e00! \u540c\u65f6\u4e5f\u9002\u7528\u4e8elinux mint19.1\u3002\u5373\u70b9\u5373\u5f00\uff01\u582a\u79f0\u662f\uff0clinux\u4e0b\u7684\u9177\u72d7\u97f3\u4e50\uff01\u4e0b\u8f7d\u5730\u5740\uff1ahttps://github.com/xtuJSer/CoCoMusic/releases\uff0c\u76f4\u63a5\u4e0b\u8f7d\uff1acocomusic_2.0.4_amd64.deb\u5b89\u88c5\u5373\u53ef\u3002\n~$ cocomusic\n\u5373\u53ef\u542f\u52a8\nhttps://www.ubuntukylin.com/ukylin/forum.php?mod=viewthread&tid=188255\nubuntu18.04\u5b89\u88c5\u626b\u63cf\u4eea\nLinux\u4e0b\u4e00\u822c\u4f7f\u7528sane\u505a\u4e3a\u626b\u63cf\u4eea\u540e\u7aef\uff0c\u5b89\u88c5\u5982\u4e0b\uff1a\nsudo apt-get install sane sane-utils xsane\n@node1:~$ sudo sane-find-scanner\nfound USB scanner (vendor=0x04a9 [Canon], product=0x190d [CanoScan]) at libusb:003:006\ndevice `pixma:04A9190D' is a CANON Canoscan 9000F Mark II multi-function peripheral\n\u671f\u95f4\u4e5f\u66fe\u88c5\u8fc7VueScan\uff0c\u53ef\u4ee5\u8bc6\u522b\u626b\u63cf\u4eea\uff0c\u4f46\u662f\u8981\u6536\u8d39\u3002\n$ simple-scan\n\u7ec8\u4e8e\u53ef\u4ee5\u4f7f\u7528\u626b\u63cf\u4eea\u4e86\u3002\nHyperLedger Fabric\u94fe\u7801\u5f00\u53d1\u53ca\u6d4b\u8bd5\nhttps://blog.csdn.net/TripleS_X/article/details/80550401\nfabric-samples\nhttps://github.com/hyperledger/fabric-samples\nLinux\uff08Ubuntu18.04\uff09\u5b89\u88c5Chrome\u6d4f\u89c8\u5668\n\u4e00\u5206\u949f\u5b89\u88c5\u6559\u7a0b\uff01\n1\u3001\u5c06\u4e0b\u8f7d\u6e90\u52a0\u5165\u5230\u7cfb\u7edf\u7684\u6e90\u5217\u8868\uff08\u6dfb\u52a0\u4f9d\u8d56\uff09\nsudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/\n2\u3001\u5bfc\u5165\u8c37\u6b4c\u8f6f\u4ef6\u7684\u516c\u94a5\uff0c\u7528\u4e8e\u5bf9\u4e0b\u8f7d\u8f6f\u4ef6\u8fdb\u884c\u9a8c\u8bc1\u3002\nwget -q -O - https://dl.google.com/linux/linux_signing_key.pub"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study investigated the effects of L-Histidinol on the Integrated Stress Response (ISR) in two cell lines, HeLa and HepG2. The researchers found that L-Histidinol inhibited the reactivation of a transgene in both cell lines, but the extent of inhibition varied between the two cell lines. In HeLa cells, L-Histidinol inhibited transgene reactivation by 30%, whereas in HepG2 cells, the inhibition was 50%. The researchers also found that the inhibition of transgene reactivation was dependent on the presence of the ISRIB, a known ISR inhibitor. However, the study did not investigate the role of other stressors, such as glucose and NaHCO3, on the ISR.",
    "choices": [
      "A) L-Histidinol inhibited transgene reactivation by 40% in both HeLa and HepG2 cells, and the inhibition was independent of the presence of ISRIB.",
      "B) The study found that L-Histidinol inhibited transgene reactivation by 50% in HeLa cells, but not in HepG2 cells, suggesting that the two cell lines have different responses to L-Histidinol.",
      "C) The researchers found that the inhibition of transgene reactivation was dependent on the presence of glucose, and that L-Histidinol inhibited transgene reactivation by 30% in both HeLa and HepG2 cells when glucose was present.",
      "D) L-Histidinol inhibited transgene reactivation by 30% in HeLa cells and 50% in HepG2 cells, and the inhibition was dependent on the presence of ISRIB, which was used to initiate the treatments 1 hour before the addition of L-Histidinol."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Single AAs, Glucose, and NaHCO3 were from Sigma. Further details and amounts utilized are indicated in S1 Table. All media were supplemented with 10% dialyzed FBS (Invitrogen), 100 U/ml penicillin G (Invitrogen), 100 mg/ml streptomycin (Invitrogen), and G418 as required. HBSS was from Invitrogen. Cells were seeded at 10\u201330% of confluency; cells to be starved for 48 h were plated 2\u20133 times more confluent compared to the control. The following day, cells were washed and cultured in the appropriate medium, with or without EAA, for 24\u201348 h.\nL-Histidinol (HisOH), PP242, Integrated Stress Response Inhibitor (ISRIB), SP600125, Cycloheximide (CHX) were from Sigma; Salubrinal was from Tocris Bioscience; U0126 was from Promega. Drugs were used at the following final concentrations: HisOH at 4\u201316 mM; PP242 at 1\u20133 \u03bcM; ISRIB at 100 nM; SP600125 at 20 \u03bcM in HepG2 cells and 50 \u03bcM in HeLa cells; Cycloheximide (CHX) at 50 ug/ml in HepG2 cells and 100 ug/ml in HeLa cells; Salubrinal at 75 \u03bcM; U0126 at 50 \u03bcM. Vehicle was used as mock control. Treatments with drugs to be tested for their ability to inhibit transgene reactivation (ISRIB, SP600125 and U0126) were initiated 1h before the subsequent addition of L-Histidinol (ISRIB) or the subsequent depletion of Met/Cys (SP600125 and U0126). Total RNA was purified using the RNeasy Mini kit (Qiagen), according to manufacturer\u2019s instructions. RNA concentration was determined by Nanodrop 8000 Spectrophotometer (Thermo Scientific). Equal amount (1 \u03bcg) of RNA from HeLa, HepG2 and C2C12 cells was reverse transcribed using the SuperScript First-Strand Synthesis System for RT-PCR (Invitrogen) using oligo-dT as primers, and diluted to 5 ng/\u03bcl. The cDNA (2 \u03bcl) was amplified by real-time PCR using SYBR green Master Mix on a Light Cycler 480 (Roche), according to manufacturer\u2019s instructions. The thermal cycling conditions were: 1 cycle at 95\u00b0C for 5 min, followed by 40\u201345 cycles at 95\u00b0 for 20 sec, 56\u00b0 for 20 sec and 72\u00b0 for 20 sec. The sequences, efficiencies and annealing temperatures of the primers are provided in S2 Table."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robot is navigating through a polytope environment, and its goal is to reach a specific vertex. The robot's preference for transitioning to a neighboring polytope is determined by a set of preferred transitions, which are represented by a discrete-valued random variable p_v. Assuming the conditional independence relationships described by the new problem diagram, what is the number of Bayesian updates required to update the belief at every iteration, given that the robot has already visited m_g = 5 possible goals and has a preference for transitioning to a neighboring polytope?",
    "choices": [
      "A) The number of Bayesian updates required is m_g \u00d7 m_g, since the robot's preference for transitioning to a neighboring polytope is independent of its previous visits to other polytopes.",
      "B) The number of Bayesian updates required is m_g \u00d7 m_g, since the robot's preference for transitioning to a neighboring polytope is determined by the number of possible goals it has already visited.",
      "C) The number of Bayesian updates required is m_g \u00d7 m_g, since the robot's preference for transitioning to a neighboring polytope is independent of the number of essential constraints of the polytope it is currently in.",
      "D) The number of Bayesian updates required is m_g \u00d7 m_\u03b8, since the robot's preference for transitioning to a neighboring polytope is determined by the number of preferred transitions it has already made, and the number of possible goals it has already visited."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In other words, as the robot continually moves in space, the first hyperplane that it will cross upon exiting the polytope will correspond to one of the polytope's nonredundant constraints. Vincent and Schwager outline an iterative method for removing redundant constraints by solving n linear programs. We use this method in practice for computing \u03b1 j e for each polytope. We can now characterize each polytope by a vector \u03b1 j e \u2208 {\u22121, 1} n j e , where n j e \u2264 n is the number of essential constraints of the polytope. The polytopes P j partition the environment into a hyperplane arrangement. Path Preference\n\nIn this section, we provide a definition of preference \u03b8 according to a graphical representation of the environment based on the hyperplane arrangement. Under this representation, a path preference corresponds to a set of preferred transitions. In other words, for each polytope in the space, the human will have a preference to which neighboring polytope they wish to transition. Let G := (V, E) be an undirected graph, where vertices are obstacle-free polytopes, and edges connect two adjacent polytopes. Each polytope is described by a unique vector \u03b1 j as defined in eq. ( ). Two polytopes are adjacent if they share non-redundant constraints (rows in eq. ( )) corresponding to the same hyperplane (i.e. they are on opposite sides of the hyperplane). Let N (v) be the set of neighbors of a vertex v. For each vertex, we denote p v the discrete-valued random variable describing which edge in N (v) the human intends to transition to. Using this formalism, we define a path preference as the set of preferred transitions over all nodes in the graph, Let m \u03b8 = v\u2208V |N (v)| be the cardinality of \u0398, and m g = |\u2126 g | the number of possible goals. A priori, the number of Bayesian updates required to update the belief at every iteration should be m \u03b8 \u00d7 m g . Now, let us assume the conditional independence relationships described by the new problem diagram in fig. . More specifically, we introduce the assumption that conditioned on a robot location s t , the goal g, and the preference for the corresponding vertex p v in the graph, the observation o t and the preference for any other vertex are conditionally independent."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A Broadjam subscriber is found to be sending spam emails from their hosting services. According to Broadjam's terms of service, what is the minimum period of suspension for such hosting services?",
    "choices": [
      "A) 1 day, as long as the subscriber agrees to cease sending spam and pays a non-refundable reactivation fee",
      "B) 2 days, regardless of the subscriber's agreement to stop sending spam",
      "C) 30 days, as stated in Section 4.05 of Broadjam's terms of service",
      "D) 2 days, as stated in Article IV of Broadjam's terms of service, but only if the subscriber responds to Broadjam's email and pays the reactivation fee"
    ],
    "correct_answer": "D)",
    "documentation": [
      "Upon request by Broadjam, conclusive proof of optin may be required for an email address or fax number. (d) If Broadjam determines that Hosting Services are being used in association with spam, Broadjam will re-direct, suspend, or cancel such Hosting Service for a period of no less than 2 days. The Hosting Subscriber will be required to respond by email to Broadjam stating that Hosting Subscriber will cease to send spam and/or have spam sent on their behalf. Broadjam will require a non-refundable reactivation fee to be paid before Hosting Subscriber's Website, email boxes and/or other Hosting Services are reactivated. In the event Broadjam determines the abuse has not stopped after services have been restored the first time, Broadjam may terminate all Services associated with the Hosting Subscriber. This Article IV applies to all Users. Fees and prices appearing on the Site are based on United States dollars. Payments for any Service or purchase made on or through the Site shall be made to Broadjam in United States dollars, except as provided in Section 4.05 herein. You agree to pay for all fees and charges incurred under your Broadjam account or Username. If you have configured the account associated with your Username (your \"Account\") to pay for Services or purchases with a credit or debit card or similar form of payment (a \"Card\" payment method), you authorize any and all charges and fees incurred under your Account to be billed from time to time to your Card account. Regardless of the method of payment, it is your sole responsibility to advise Broadjam of any billing problems or discrepancies within thirty (30) days after such discrepancies or problems become known to you. Your Card issuer agreement governs the use of your designated Card account in connection with any fee, purchase or Service; you must refer exclusively to such issuer agreement, and not this Agreement, to determine your rights and liabilities as a Cardholder. If you submit a payment that results in Broadjam being charged non-sufficient funds, chargeback fees, or other similar fees, you agree to reimburse all such fees."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a sequence of independent and identically distributed (i.i.d.) random variables X1, X2, ..., Xn, where each Xi follows a Gamma(a, b)-distribution with shape parameter a and rate parameter b. The moment generating function (MGF) of | log(Xi)| is given by:\n\nMGF(| log(Xi)|) = exp{a log(1 - t) - b t / (1 - t)}\n\nwhere 0 \u2264 t < 1. Suppose we want to estimate the parameter b using the method of maximum likelihood. If we assume that the sample mean of the log(Xi) values is a consistent estimator of the parameter a, what is the asymptotic variance of the estimator of b?",
    "choices": [
      "A) \u221a(a / (n b^2))",
      "B) \u221a(a / (n b^2)) + a / (n b)",
      "C) \u221a(a / (n b^2)) + a / (n b) + 1 / (n b)",
      "D) \u221a(a / (n b^2)) + a / (n b) + 1 / (n b) + 1 / (n b^2)"
    ],
    "correct_answer": "D)",
    "documentation": [
      "In the next paragraph, we derive the asymptotic order of E[exp{\u03bb H(f ) \u221e }] for n, p \u2192 \u221e, where \u03bb > 0 may depend on n, p or not. By the exponential decay property of the kernel K stated in Lemma 2 holds First, H(f ) \u221e is bounded with the maximum over a finite number of points. Calculating the derivative of s : Since \u03b4 \u03b4x s(x) > 0 almost surely for x = x k , the extrema occur at x k , k = 1, ..., T . Thus, for \u03bb > 0 the moment generating function of H(f ) \u221e is bounded by\nLet M j = ( T h) \u22121 T k=1 \u03b3 h (x j , x k ), which by Lemma 2 is bounded uniformly in j by some global constant M > 0. By the convexity of the exponential function we obtain \u221a 2 and by assumption 0 \u2264 \u03b4 \u2264 f \u2264 M 0 . Using Lemma 3, Q k can be written as a sum of m = np/T independent gamma random variables, i.e.\nThe moment generating function of | log(X)| when X follows a \u0393(a, b)-distribution is given by where \u0393(a) is the gamma function and \u03b3(a, b) is the lower incomplete gamma function. In particular, To derive the asymptotic order of E[exp{\u03bb H(f ) \u221e }] for n, p \u2192 \u221e we first establish the asymptotic order of the ratio \u0393(a + t)/\u0393(a) for a \u2192 \u221e.\nWe distinguish the two cases where t is independent of a and where t linearly depends on a. Thus, for 0 < t < a and t independent of a, equation (A.17) implies for a \u2192 \u221e that \u0393(a + t)/\u0393(a) = O(a t ). Similarly, it can be seen that \u0393(a \u2212 t)/\u0393(a) = O(a \u2212t ). If 0 < t < a and t linearly depends on a, i.e. t = ca for some constant c \u2208 (0, 1), then we get \u0393(a \u00b1 t)/\u0393(a) = O(a \u00b1t exp{a}) for a \u2192 \u221e.\nHence, for a fixed \u03bb not depending on n, p and such that 0 < \u03bb < m/( \u221a 2M j ) we get for sufficiently large n, p If \u03bb = cm such that 0 < \u03bb < m/( \u221a 2M j ), then for sufficiently large n, p b\u2208{c\u03b4/m,cM 0 /m} (bm/2) for some constant L > 1. Set K = min j=1,..., T 1/( \u221a 2M j ) which is a constant independent of n, p. Altogether, we showed that for 0 < \u03bb < Km and n, p \u2192 \u221e\nBounding the right hand side of (A.15) for some constants c 0 , c 1 > 0 and n, p \u2192 \u221e Since g lies between H(f ) and H(f ), and f almost surely pointwise."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A historian is researching the academic career of John Roper, a prominent figure in the 16th century. Roper was a fellow of Magdalen College, Oxford, and later became the principal of Salesbury. However, Roper's academic career was also marked by controversy, as he was involved in a dispute with the university's vice-chancellor. Which of the following statements best describes Roper's relationship with the university's vice-chancellor?",
    "choices": [
      "A) Roper was appointed as the vice-chancellor of the university in 1505, and later became the principal of Salesbury.",
      "B) Roper was a strong supporter of the university's vice-chancellor, and was known for his loyalty to the institution.",
      "C) Roper was involved in a dispute with the university's vice-chancellor, who was also a fellow of Magdalen College, and was later appointed as the principal of Salesbury.",
      "D) Roper was a member of the university's council, which was responsible for appointing the vice-chancellor, and was known for his expertise in academic governance."
    ],
    "correct_answer": "C)",
    "documentation": [
      "See Mayor, 138; Surtees' Durham, i. 107; & Foster's Index Eccl. Roper, John (or Rooper) demy Magdalen Coll., from Berks, M.A. fellow, 1483, D.D. disp. 27 June, 1506, (first) Margaret professor of divinity, 1500, vice-chancellor of the university 1505, and 1511, principal of Salesurry and George Hall, rector of Witney, Oxon, 1493, vicar of St. Mary's church, Oxford, canon of Cardinal Coll. 1532; died May, 1534. See Ath. i. 76; & Landsowne MS. 979, f. 118. Roper, John B.A. disp. 4 July, 1512. Roper, Thomas of Trinity Coll. 1699. See Rooper. Roper, Philip of Kent, arm. Gloucester Hall, matric. 7 Sept., 1588, aged 15 (subscribes Rooper). Roper, William (subscribes Rooper) of co. Hereford, militis fil. St. Alban Hall, matric. entry dated 5 June, 1607, aged 13; probably of Malmains, Kent, 2nd son of Sir Christopher Roper, afterwards 2nd baron Teynham. See Foster's Peerage. Roscarrock, Henry of Cornwall, arm. Hart Hall, matric. entry under date 17 Dec., 1576, aged 21; probably son of Thomas, of Roscarrock, and brother of the next, and of Richard 1581. Roscarrock, John B.A. 11 Feb., 1576-7; perhaps from Exeter Coll. (and 1s. Thomas, of Roscarrock, Cornwall); died 24 Nov., 1608; brother of Henry and Richard. See O.H.S. xii. 65. Roscarrock, Nicolas (Roiscariot) B.A. supd. 3 May, 1568, student Inner Temple 1571, as of Roscarrock, Cornwall. See Foster's Inns of Court Reg. Roscarrock, Richard of Cornwall, arm. Broadgates Hall, matric. entry under date circa 1581, aged 19; student of Middle Temple 1583 (as 3s. Thomas, of Roscarrock, Cornwall, esq.), brother of Henry and John. See Foster's Inns of Court Reg. Rosdell, Christopher of Yorks, pleb. St. Edmund Hall, matric. entry under date 22 Dec., 1576, aged 22, B.A. 4 July, 1576; rector of St. Bennet Sherehog, London, 1579, and vicar of Somerton, Somerset, 1582. See Foster's Index Eccl. Rose, Christopher s. John, of Marlow, Bucks, gent. Christ Church, matric. 13 Feb., 1622-3, aged 21, B.A. same day; rector of Hutton, Essex, 1642. See Foster's Index Ecclesiasticus. Rose, Christopher s. Giles, of Lynn Regis, Norfolk, gent."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Hugh H. Goodwin's early life and education were marked by a unique circumstance. What is the most likely reason he was able to attend the United States Naval Academy at Annapolis, Maryland despite not completing his high school diploma?",
    "choices": [
      "A) He was able to demonstrate exceptional leadership skills during his time in the Navy, which led to his appointment to the academy.",
      "B) He was a member of a prominent naval family and was able to secure an appointment through his family's connections.",
      "C) He was a talented athlete and was able to secure a scholarship to the academy, which allowed him to attend despite not completing his high school diploma.",
      "D) He was able to take advantage of a special program offered by the Navy that allowed him to attend the academy without completing his high school diploma, which was a common practice at the time."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Hugh Hilton Goodwin (December 21, 1900 \u2013 February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War. Following the War, he remained in the Navy and rose to the flag rank and held several important commands including Vice Commander, Military Air Transport Service, Commander, Carrier Division Two and Commander, Naval Air Forces, Continental Air Defense Command. Early life and career\n\nHugh H. Goodwin was born on December 21, 1900, in Monroe, Louisiana and attended Monroe High School there (now Neville High School). Following the United States' entry into World War I in April 1917, Goodwin left the school without receiving the diploma in order to see some combat and enlisted the United States Navy on May 7, 1917. He completed basic training and was assigned to the battleship . Goodwin participated in the training of armed guard crews and engine room personnel as the Atlantic Fleet prepared to go to war and in November 1917, he sailed with the rest of Battleship Division 9, bound for Britain to reinforce the Grand Fleet in the North Sea. Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the United States Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname \"Huge\" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Robert E. Blick Jr., Herbert S. Duckworth, Clayton C. Jerome, James P. Riseley, James A. Stuart, Frank Peak Akers, Sherman Clark, Raymond P. Coffman, Delbert S. Cornwell, Frederick J. Eckhoff, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy, William S. Parsons, Harold R. Stevens, John P. Whitney, Lyman G. Miller and George J. O'Shea."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the effects of shock waves on bubble deformation in a compressible fluid. They observe that the temporal evolution of entropy production rate (\u1e60NOEF) shows a rapid decrease at t = 0.03, followed by a gradual decrease until t = 0.06, and then a rapid increase again. What can be inferred about the specific heat ratio (\u03b3) of the fluid?",
    "choices": [
      "A) The specific heat ratio is constant throughout the process, and the entropy production rate is directly proportional to the velocity gradient.",
      "B) The specific heat ratio is inversely proportional to the velocity gradient, and the entropy production rate is directly proportional to the temperature gradient.",
      "C) The specific heat ratio is constant, and the entropy production rate is directly proportional to the velocity gradient.",
      "D) The specific heat ratio is inversely proportional to the velocity gradient, and the entropy production rate is directly proportional to the temperature gradient, but the effect of the specific heat ratio on the entropy production rate is only significant during the shock compression stage."
    ],
    "correct_answer": "D)",
    "documentation": [
      "They are key factors in compression science field. The former is induced by temperature gradient and the NOEF (\u2206 * 3,1 ). The latter is affected by velocity gradient and the NOMF (\u2206 * 2 ). The entropy production rates are defined by the following formulas : Integrating the \u1e60NOEF and \u1e60NOMF over time t, the entropy generations over this period of time are obtained, i.e., S NOEF = t 0 \u1e60NOEF dt and S NOMF = t 0 \u1e60NOMF dt. Plotted in Fig. ) and 14(b) are the temporal evolution of \u1e60NOMF and \u1e60NOEF , respectively. The evolution of entropy generation rate is related to two aspects: (i) the propagation of the shock wave, and (ii) the deformation of the bubble. The former generates a macroscopic quantity gradient, and the latter makes the contact interface wider, longer, and deformed. Depending on the location of the shock wavefront, there exist two critical moments in this SBI process: (i) at around t = 0.03, the shock wave just sweeps through the bubble, and (ii) at t = 0.06, the shock wave exits the flow field. Therefore, the temporal evolution of the entropy production rate shows three stages, i.e., t < 0.03, 0.03 < t < 0.06, and t > 0.06. At the stage t < 0.03, the shock compression stage, the shock effects compress the bubble. It generates the large macroscopic quantity gradients, resulting in a quick increase of \u1e60NOMF . At around t = 0.03, the shock wave passed through the bubble. So the values of \u1e60NOMF decreases. The values of \u1e60NOMF would continue to decrease due to the gradually wider contact interface caused by the diffusion effect. At around t = 0.06, the shock wave comes out of the flow field so that the values of \u1e60NOMF drops rapidly. In the third stage, i.e., t > 0.06, because of the diffusive effect, the general trend of \u1e60NOMF is downward. However, it shows an oscillatory trend due to the influence of various reflected shock waves. The specific heat ratio indirectly changes the value of \u1e60NOMF by changing the velocity gradient. The smaller the specific-heat ratio, the larger \u1e60NOMF . Different understanding can be seen in Fig. , where the temporal evolution of \u1e60NOEF is plotted."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A CDMA receiver using the PLMS-PPIC method is designed to estimate the phases of all user channels. However, in practice, the receiver only has access to the quarter of each channel phase in (0,2\u03c0). What is the primary advantage of this modified PLMS-PPIC procedure?",
    "choices": [
      "A) It reduces the computational complexity of the algorithm by eliminating the need to estimate the phases of all user channels.",
      "B) It improves the accuracy of the channel phase estimation by using a more robust estimation method.",
      "C) It allows the receiver to estimate the phases of all user channels simultaneously with the cancelation weights, which is essential for optimal performance.",
      "D) It enables the receiver to use a single NLMS algorithm with a fixed step-size, which simplifies the implementation of the algorithm."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\section{Introduction}\\label{S1}\n\nThe multiple access interferences (MAI) is the root of user\nlimitation in CDMA systems \\cite{R1,R3}. The parallel least mean\nsquare-partial parallel interference cancelation (PLMS-PPIC) method\nis a multiuser detector for code division multiple access (CDMA)\nreceivers which reduces the effect of MAI in bit detection. In this\nmethod and similar to its former versions like LMS-PPIC \\cite{R5}\n(see also \\cite{RR5}), a weighted value of the MAI of other users is\nsubtracted before making the decision for a specific user in\ndifferent stages \\cite{cohpaper}. In both of these methods, the\nnormalized least mean square (NLMS) algorithm is engaged\n\\cite{Haykin96}. The $m^{\\rm th}$ element of the weight vector in\neach stage is the true transmitted binary value of the $m^{\\rm th}$\nuser divided by its hard estimate value from the previous stage. The\nmagnitude of all weight elements in all stages are equal to unity. Unlike the LMS-PPIC, the PLMS-PPIC method tries to keep this\nproperty in each iteration by using a set of NLMS algorithms with\ndifferent step-sizes instead of one NLMS algorithm used in LMS-PPIC. In each iteration, the parameter estimate of the NLMS algorithm is\nchosen whose element magnitudes of cancelation weight estimate have\nthe best match with unity. In PLMS-PPIC implementation it is assumed\nthat the receiver knows the phases of all user channels. However in\npractice, these phases are not known and should be estimated. In\nthis paper we improve the PLMS-PPIC procedure \\cite{cohpaper} in\nsuch a way that when there is only a partial information of the\nchannel phases, this modified version simultaneously estimates the\nphases and the cancelation weights. The partial information is the\nquarter of each channel phase in $(0,2\\pi)$.\n\nThe rest of the paper is organized as follows: In section \\ref{S4}\nthe modified version of PLMS-PPIC with capability of channel phase\nestimation is introduced. In section \\ref{S5} some simulation\nexamples illustrate the results of the proposed method."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When designing a composite structure to reinforce a curved fuselage, what is the primary benefit of using a combination of stringers and a skin?",
    "choices": [
      "A) The stringers provide additional structural support, while the skin helps to distribute loads evenly across the curved surface, reducing the risk of delamination.",
      "B) The skin provides a smooth, aerodynamic surface, while the stringers add unnecessary weight and complexity to the design.",
      "C) The stringers help to maintain the structural integrity of the fuselage, while the skin can be designed to be thinner and lighter, reducing overall weight.",
      "D) The combination of stringers and skin allows for a more efficient distribution of loads, enabling the structure to withstand external forces while minimizing material usage and weight."
    ],
    "correct_answer": "D)",
    "documentation": [
      "When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out. At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the \"backbone\" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "\u0627\u0648 \u0641\u064a \u0645\u0622\u0632\u0642 \u0645\u0627\u0644\u064a\u0629 \u062b\u0645 \u062a\u0642\u062f\u0645\u0648\u0627 \u0644\u0627\u0646\u0642\u0627\u0630\u0647 (\u0648\u0642\u062f \u0641\u0639\u0644\u0647\u0627 \u062f\u0632\u0631\u0627\u0626\u064a\u0644\u064a \u0645\u0639 \u0627\u0644\u062e\u062f\u064a\u0648 \u0648\u0627\u0633\u062a\u0648\u0644\u0649 \u0639\u0644\u0649 \u0627\u0644\u0642\u0646\u0627\u0644).. \u0648\u0625\u0630 \u062a\u0639\u0630\u0631 \u0627\u0644\u0627\u0645\u0631 \u0633\u0627\u0631\u0639\u0648\u0627 \u0627\u0644\u0649 \u0627\u063a\u062a\u064a\u0627\u0644\u0647 (\u0648\u0642\u062f \u0641\u0639\u0644\u0648\u0647\u0627 \u0628\u0643\u0646\u064a\u062f\u064a) \u062b\u0645 \u0627\u0642\u062a\u0644\u0648\u0627 \u0642\u0627\u062a\u0644\u0647 \u0644\u062a\u062f\u0641\u0646\u0648\u0627 \u0627\u0633\u0631\u0627\u0631\u0646\u0627 \u0645\u0639\u0647 \u0627\u0644\u0649 \u0627\u0644\u0623\u0628\u062f (\u0648\u0642\u062f \u0641\u0639\u0644\u0648\u0647\u0627 \u0628\u0642\u0627\u062a\u0644 \u0643\u0646\u064a\u062f\u064a). \u2022\u0627\u0642\u062a\u0644\u0648\u0627 \u0627\u0644\u0642\u0648\u0645\u064a\u0627\u062a \u0648\u0627\u0644\u0648\u0637\u0646\u064a\u0627\u062a \u0628\u0627\u0644\u062f\u0639\u0648\u0629 \u0627\u0644\u0649 \u0627\u0644\u0627\u0645\u0645\u064a\u0629 \u0648\u0627\u0644\u0645\u0648\u0627\u0637\u0646\u0629 \u0627\u0644\u0639\u0627\u0644\u0645\u064a\u0629 \u0648\u0642\u062f \u0641\u0639\u0644\u0647\u0627 \u0645\u0627\u0631\u0643\u0633 \u0641\u064a \u0627\u0644\u0634\u064a\u0648\u0639\u064a\u0629. \u2022\u0643\u0644 \u0645\u0627 \u0639\u062f\u0627 \u0627\u0644\u064a\u0647\u0648\u062f \u062d\u064a\u0648\u0627\u0646\u0627\u062a \u0646\u0627\u0637\u0642\u0629 \u0633\u062e\u0631\u0647\u0627 \u0627\u0644\u0644\u0647 \u0641\u064a \u062e\u062f\u0645\u0629 \u0627\u0644\u064a\u0647\u0648\u062f. \u0648\u0627\u0644\u064a\u0647\u0648\u062f\u064a\u0629 \u062a\u0631\u0649 \u0627\u0646 \u0627\u0644\u0644\u0647 \u0648\u0627\u062d\u062f \u0648\u0644\u0643\u0646\u0647\u0627 \u062a\u062d\u062a\u0643\u0631\u0647 \u0644\u0646\u0641\u0633\u0647\u0627 \u0641\u0644\u0627 \u0639\u0645\u0644 \u0644\u0644\u0647 \u0627\u0644\u0627 \u0627\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649 \u0627\u0633\u0631\u0627\u0626\u064a\u0644 \u0648\u062a\u0633\u062e\u064a\u0631 \u062c\u0645\u064a\u0639 \u0627\u0644\u0634\u0639\u0648\u0628 \u0644\u062e\u062f\u0645\u062a\u0647\u0627. \u0648\u0627\u0644\u0644\u0627\u0647\u0648\u062a \u0627\u0644\u064a\u0647\u0648\u062f\u064a \u0644\u0627 \u064a\u0624\u0645\u0646 \u0628\u0622\u062e\u0631\u0629\u060c \u0648\u0642\u062f \u0634\u0637\u0628\u0648\u0627 \u0643\u0644 \u0645\u0627 \u062c\u0627\u0621 \u0639\u0646 \u0627\u0644\u0622\u062e\u0631\u0629 \u0641\u064a \u0627\u0644\u062a\u0648\u0631\u0627\u0629 .. \u0648\u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0639\u0646\u062f\u0647\u0645 \u0647\u064a \u0642\u064a\u0627\u0645\u0629 \u062f\u0648\u0644\u062a\u0647\u0645 \u0641\u064a \u0641\u0644\u0633\u0637\u064a\u0646 \u0648\u0627\u0644\u0628\u0639\u062b \u0628\u0639\u062b\u0647\u0627 \u0648\u0627\u0644\u0646\u0634\u0631 \u0646\u0634\u0631\u0647\u0627 .. \u0648\u064a\u0648\u0645 \u0627\u0644\u062d\u0633\u0627\u0628 \u0647\u0648 \u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0630\u064a \u064a\u062d\u0627\u0633\u0628\u0648\u0646 \u0641\u064a\u0647 \u0643\u0644 \u0627\u0644\u0623\u0645\u0645 \u064a\u0648\u0645 \u064a\u0639\u0648\u062f \u0627\u0644\u0645\u0633\u064a\u062d \u0648\u064a\u0628\u0627\u0631\u0643\u0647\u0645 \u0648\u064a\u062e\u062a\u0627\u0631\u0647\u0645 \u0646\u0648\u0627\u0628\u0627\u064b \u0644\u0647 \u0641\u064a \u062d\u0643\u0645 \u0627\u0644\u0639\u0627\u0644\u0645 \u0648\u0625\u0642\u0627\u0645\u0629 \u0645\u0644\u0643\u0648\u062a \u0627\u0644\u0644\u0647 \u0639\u0644\u0649 \u0627\u0644\u0623\u0631\u0636 .. \u0648\u0627\u0644\u0639\u062c\u064a\u0628 \u0627\u0646\u0647\u0645 \u0643\u0641\u0631\u0648\u0627 \u0628\u0627\u0644\u0645\u0633\u064a\u062d \u062d\u064a\u0646\u0645\u0627 \u062c\u0627\u0621 \u062b\u0645 \u0623\u0639\u0644\u0646\u0648\u0627 \u0625\u064a\u0645\u0627\u0646\u0647\u0645 \u0628\u0639\u0648\u062f\u062a\u0647 \u0648\u0634\u0631\u0637\u0648\u0627 \u0647\u0630\u0647 \u0627\u0644\u0639\u0648\u062f\u0629 \u0628\u0627\u0646\u0647\u0627 \u0631\u062c\u0639\u0629 \u0645\u0646 \u0627\u0644\u0645\u0633\u064a\u062d \u0644\u064a\u062e\u062a\u0627\u0631\u0647\u0645 \u0631\u0624\u0633\u0627\u0621 \u0648\u062d\u0643\u0627\u0645\u0627\u064b \u0644\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0649 \u0627\u0644\u0623\u0628\u062f. \u0648\u0627\u0644\u0641\u0643\u0631 \u0627\u0644\u064a\u0647\u0648\u062f\u064a \u064a\u0644\u0642\u0649 \u063a\u0644\u0627\u0644\u0629 \u0645\u0646 \u0627\u0644\u0623\u0633\u0631\u0627\u0631 \u0648\u0627\u0644\u0637\u0644\u0627\u0633\u0645 \u0648\u0627\u0644\u0643\u062a\u0645\u0627\u0646 \u0648\u0627\u0644\u063a\u0645\u0648\u0636 \u0639\u0644\u0649 \u0643\u0644 \u0634\u0626 .. \u0648\u0627\u0644\u0643\u0628\u0627\u0644\u0627 \u0648\u0627\u0644\u0633\u062d\u0631 \u0648\u0639\u0644\u0645 \u0627\u0644\u0623\u0639\u062f\u0627\u062f \u0648\u0627\u0644\u062d\u0631\u0648\u0641 \u0648\u062a\u0633\u062e\u064a\u0631 \u0627\u0644\u0634\u064a\u0627\u0637\u064a\u0646 \u0645\u0646 \u0639\u0644\u0648\u0645\u0647\u0645 \u0627\u0644\u062a\u0649 \u0634\u063a\u0641\u0648\u0627 \u0628\u0647\u0627 \u0648\u0631\u0648\u062c\u0648\u0647\u0627 \u0648\u0646\u0634\u0631\u0648\u0647\u0627. \u0648\u0643\u0627\u0646\u062a \u0648\u0633\u064a\u0644\u062a\u0647\u0645 \u0627\u0644\u0649 \u0647\u062f\u0645 \u0627\u0644\u0643\u062a\u0628 \u0627\u0644\u0633\u0645\u0627\u0648\u064a\u0629 \u0647\u0649 \u062a\u0641\u0633\u064a\u0631\u0647\u0627 \u0628\u0627\u0644\u062a\u0623\u0648\u064a\u0644 \u0648\u0630\u0644\u0643 \u0628\u0631\u0641\u0636 \u0627\u0644\u0645\u0639\u0627\u0646\u064a \u0627\u0644\u0638\u0627\u0647\u0631\u0629 \u0648\u0627\u062e\u062a\u0631\u0627\u0639 \u0645\u0639\u0627\u0646 \u0628\u0627\u0637\u0646\u0629 \u062a\u0647\u062f\u0645 \u0627\u0644\u063a\u0631\u0636 \u0627\u0644\u062f\u064a\u0646\u064a \u0648\u062a\u0641\u0633\u062f \u0647\u062f\u0641\u0647. \u0648\u0646\u0633\u062a\u0637\u064a\u0639 \u0627\u0646 \u0646\u0631\u0649 \u0627\u062b\u0631 \u0627\u0644\u062a\u0648\u062c\u064a\u0647 \u0627\u0644\u0647\u0648\u062f\u064a \u0641\u064a \u0627\u0644\u0641\u0644\u0633\u0641\u0627\u062a \u0627\u0644\u0639\u0628\u062b\u064a\u0629 \u0648\u0627\u0644\u062f\u0645\u064a\u0629 \u0648\u0627\u0644\u0645\u0627\u062f\u064a\u0629 \u0648\u0627\u0644\u0641\u0648\u0636\u064a\u0629 \u0648\u0627\u0644\u0625\u0628\u0627\u062d\u064a\u0629 .. \u0648\u0627\u062d\u064a\u0627\u0646\u0627 \u0646\u0644\u0645\u062d \u0627\u0633\u0645\u0627\u0621 \u064a\u0647\u0648\u062f\u064a\u0629 \u062e\u0644\u0641\u0647\u0627 \u0645\u062b\u0644 : \u0633\u0627\u0631\u062a\u0631 \u2013 \u0641\u0631\u0648\u064a\u062f \u2013 \u0645\u0627\u0631\u0643\u0633 \u2013 \u0645\u0627\u0631\u0643\u0648\u0632. \u0648\u0627\u0630\u0627 \u0641\u062a\u062d\u0646\u0627 \u0645\u0644\u0641 \u0627\u0644\u062f\u064a\u0627\u0646\u0629 \u0627\u0644\u0628\u0647\u0627\u0626\u064a\u0629 \u0641\u0625\u0646\u0646\u0627 \u0646\u062c\u062f \u0627\u062b\u0631 \u0627\u0644\u062a\u0648\u062c\u064a\u0647 \u0627\u0644\u064a\u0647\u0648\u062f\u064a \u0648\u0627\u0636\u062d\u0627 \u0641\u064a \u0643\u062a\u0628\u0647\u0627. \u0639\u0628\u062f \u0627\u0644\u0647\u0627\u0621 \u062a\u0623\u0644\u064a\u0641 \u0633\u0644\u064a\u0645 \u0642\u0628\u0639\u064a\u0646 \u0627\u0644\u0642\u0627\u0647\u0631\u0629 \u0645\u0637\u0628\u0639\u0629 \u0627\u0644\u0639\u0645\u0631\u0627\u0646 1922. \u0645\u0641\u0648\u0627\u0636\u0627\u062a \u0639\u0628\u062f \u0627\u0644\u0628\u0647\u0627\u0621 \u0627\u0644\u0637\u0628\u0639\u0629 \u0627\u0644\u0627\u0648\u0644\u0649 1928\u0645. \u0645\u0648\u0639\u0648\u062f \u0643\u0644 \u0627\u0644\u0627\u0645\u0645. \u062c\u0648\u0631\u062c \u062a\u0627\u0648\u0632\u0646\u0647 \u0645\u0637\u0628\u0648\u0639 \u0628\u0625\u0630\u0646 \u0645\u0646 \u0627\u0644\u0645\u062d\u0641\u0644 \u0627\u0644\u0631\u0648\u062d\u0627\u0646\u064a \u0644\u0645\u0635\u0631 \u0648\u0627\u0644\u0633\u0648\u062f\u0627\u0646. \u2022\u0627\u0643\u062b\u0631 \u0641\u0644\u0627\u0633\u0641\u0629 \u0627\u0644\u064a\u0648\u0646\u0627\u0646 \u062a\u0639\u0644\u0645\u0648\u0627 \u0627\u0644\u062d\u0643\u0645\u0629 \u0645\u0646 \u0628\u0646\u0649 \u0627\u0633\u0631\u0627\u0626\u064a\u0644. \u2022\u0631\u0633\u0627\u0644\u0629 \u0639\u0628\u062f \u0627\u0644\u0628\u0647\u0627\u0621 \u0647\u064a \u062a\u0648\u062d\u064a\u062f \u0627\u0644\u0645\u0633\u0644\u0645\u064a\u0646 \u0648\u0627\u0644\u0646\u0635\u0627\u0631\u0649 \u0648\u0627\u0644\u064a\u0647\u0648\u062f \u0648\u062c\u0645\u0639\u0647\u0645 \u0639\u0644\u0649 \u0623\u0635\u0644 \u0646\u0648\u0627\u0645\u064a\u0633 \u0645\u0648\u0633\u0649. \u2022\u0639\u0645\u0644 \u0645\u0648\u0633\u0649 \u0644\u0627 \u064a\u0633"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the effectiveness of pain management clinics in Florida found that the majority of patients who visited these clinics were from out of state, with an estimated 60% of patients coming from outside the state. However, the study also revealed that the clinics were often staffed by doctors who had no medical training in pain management. Despite this, the clinics continued to operate, with some patients reporting that they were able to obtain large quantities of prescription medication. What is a likely consequence of the lack of regulation in the pain management clinic industry in Florida?",
    "choices": [
      "A) The majority of patients who visited these clinics were actually from within the state, and the clinics were able to provide effective pain management due to their experienced staff.",
      "B) The clinics were able to provide effective pain management due to the large quantities of prescription medication they were able to obtain, despite the lack of medical training among their staff.",
      "C) The clinics were able to operate with minimal oversight, leading to a significant increase in the number of patients who overdosed on prescription medication.",
      "D) The lack of regulation in the pain management clinic industry in Florida led to a significant increase in the number of patients who were able to obtain large quantities of prescription medication, contributing to the nation's oxycodone habit."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Kenneth Hammond didn\u2019t make it back to his Knoxville, Tenn., home. He had a seizure after picking up prescriptions for 540 pills and died in an Ocala gas station parking lot. Keith Konkol didn\u2019t make it back to Tennessee, either. His body was dumped on the side of a remote South Carolina road after he overdosed in the back seat of a car the same day of his clinic visit. He had collected eight prescriptions totaling 720 doses of oxycodone, methadone, Soma and Xanax. Konkol had every reason to believe he would get those prescriptions: In three previous visits to the Plantation clinic, he had picked up prescriptions for 1,890 pills. An estimated 60 percent of her patients were from out of state, a former medical assistant told the DEA. In 2015, Averill pleaded not guilty to eight manslaughter charges. She is awaiting trial in Broward County. Averill was just one doctor at just one clinic. In 2010, the year Averill\u2019s patients overdosed, Florida received applications to open 1,026 more pain clinics. An online message board advising drug users summed it up: \u201cJust go anywhere in South Florida and look for a \u2018pain management clinic.\u2019 It shouldn\u2019t be too hard; you can\u2019t swing a dead cat without hitting one.\u201d Complain about anything from a back injury to a hangnail, it advised, \u201cand they\u2019ll set you right up.\u201d By this time, Kentucky had reined in its pill mills. It didn\u2019t matter, Ohio, Delaware, North Carolina, Connecticut acted as well, but other state\u2019s efforts didn\u2019t matter either, Florida continued ignoring the pill mills and rogue doctors feeding the nation\u2019s oxycodone habit, the pills flowed. \u201cThere were folks down there, where if I had an opportunity to, get my hands around their throat, I would have wrung their neck,\u201d said Huntington Mayor Steve Williams. On Florida\u2019s inaction he stated, \u201cThere was total evidence as to what was happening. It lays at the foot, in my opinion, of the public officials there that allowed it to continue on.\u201d Governor Jeb Bush Backed A Solution\nOne of the first dinners Florida Gov. Jeb Bush hosted after moving into the governor\u2019s mansion in 1999 was a small one."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient with hemoglobin H/Constant Spring disease is undergoing regular blood transfusions to manage their anemia. However, they have recently experienced a bout of jaundice, which has led to concerns about the potential for excessive bilirubin buildup. What is the primary reason for the patient's need for desferoxamine treatment?",
    "choices": [
      "A) To reduce the risk of bone marrow failure due to iron overload",
      "B) To counteract the life-threatening buildup of bilirubin in the body",
      "C) To prevent the development of a splenectomy due to spleen enlargement",
      "D) To maintain optimal levels of hemoglobin A in the blood"
    ],
    "correct_answer": "D)",
    "documentation": [
      "For those with the hemoglobin H/Constant Spring form of the disease, the need for transfusions may be intermittent or ongoing, perhaps on a monthly basis and requiring desferoxamine treatment. Individuals with this more severe form of the disease may also have an increased chance of requiring removal of an enlarged and/or overactive spleen. Anemia \u2014 A blood condition in which the level of hemoglobin or the number of red blood cells falls below normal values. Common symptoms include paleness, fatigue, and shortness of breath. Bilirubin \u2014 A yellow pigment that is the end result of hemoglobin breakdown. This pigment is metabolized in the liver and excreted from the body through the bile. Bloodstream levels are normally low; however, extensive red cell destruction leads to excessive bilirubin formation and jaundice. Bone marrow \u2014 A spongy tissue located in the hollow centers of certain bones, such as the skull and hip bones. Bone marrow is the site of blood cell generation. Bone marrow transplantation \u2014 A medical procedure used to treat some diseases that arise from defective blood cell formation in the bone marrow. Healthy bone marrow is extracted from a donor to replace the marrow in an ailing individual. Proteins on the surface of bone marrow cells must be identical or very closely matched between a donor and the recipient. Desferoxamine \u2014 The primary drug used in iron chelation therapy. It aids in counteracting the life-threatening buildup of iron in the body associated with long-term blood transfusions. Globin \u2014 One of the component protein molecules found in hemoglobin. Normal adult hemoglobin has a pair each of alpha-globin and beta-globin molecules. Heme \u2014 The iron-containing molecule in hemoglobin that serves as the site for oxygen binding. Hemoglobin \u2014 Protein-iron compound in the blood that carries oxygen to the cells and carries carbon dioxide away from the cells. Hemoglobin A \u2014 Normal adult hemoglobin that contains a heme molecule, two alpha-globin molecules, and two beta-globin molecules."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the behavior of a complex physical system, which is characterized by a high Knudsen number. The system exhibits both hydrodynamic and non-hydrodynamic behaviors, and the researcher wants to understand the underlying physical mechanisms. Which of the following statements best describes the relationship between the Knudsen number and the information lost in traditional hydrodynamic equations?",
    "choices": [
      "A) As the Knudsen number increases, the information lost in traditional hydrodynamic equations decreases, allowing for a more accurate description of the system's behavior.",
      "B) The Knudsen number is a measure of the ratio of the mean free path to the characteristic length scale of the system, and it is directly proportional to the amount of information lost in traditional hydrodynamic equations.",
      "C) The Knudsen number is a measure of the ratio of the mean free path to the characteristic length scale of the system, and it is inversely proportional to the amount of information lost in traditional hydrodynamic equations.",
      "D) The Knudsen number is a measure of the ratio of the mean free path to the characteristic length scale of the system, and it is a critical parameter in determining the balance between hydrodynamic and non-hydrodynamic behaviors in the system."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(1) Physical modelling, (2) Algorithm design, (3) Numerical experiments and analysis of complex physical fields. The research of equation algorithm corresponds to the part (2) of the above three parts. The DBM aims at parts (1) and (3) of the three mentioned above. It belongs to a physical model construction method rather than a numerical solution for the equations. The tasks of DBM are to: (i) Ensure the rationality of the physical model (theoretical model) and balance the simplicity for the problem to be studied; (ii) Try to extract more valuable physical information from massive data and complex physical fields. Based on the coarse-grained modeling method of nonequilibrium statistical physics, the DBM aims to solve the following dilemma: (i) The traditional hydrodynamic modelings are based on the continuous hypothesis (or near-equilibrium hypothesis). They only concern the evolution of three conserved kinetic moments of the distribution function, i.e. the density, momentum and energy, so their physical functions are insufficient. (ii) The situation that the MD can be used is restricted to too small spatial-temporal scales. The physical requirement for the modeling is that except for the Hydrodynamic Non-Equilibriums (HNE), the most related TNE are also needed to be captured. Theoretically, the Boltzmann equation is suitable for all-regime flows, including the continuum regime, slip regime, transition regime, and free molecule flow regime. Based on the Chapman-Enskog (CE) multiscale analysis , through retaining various orders of Kn number (or considering different orders of TNE effects), the Boltzmann equation can be reduced to the various orders of hydrodynamic equations. They can be used to describe the hydrodynamic behaviors, i.e., the conservations of mass, momentum and energy , in corresponding flow regimes. Because what the traditional hydrodynamic equations describe are only the conservation laws of mass, momentum and energy. Consequently, it should be pointed out that, the information lost in the traditional hydrodynamic equations increases sharply with increasing the Kn number."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company's product is subject to a constitutional limit on its ability to engage in interstate commerce. However, the Court has established a doctrine that allows for the expansion of this limit in certain circumstances. Which of the following statements best describes the implications of this doctrine for the relationship between the legislative and judicial branches?",
    "choices": [
      "A) The doctrine ensures that Congress has the final say in determining the scope of interstate commerce, and the Court's role is limited to enforcing this decision.",
      "B) The doctrine allows the Court to unilaterally override Congressional decisions on interstate commerce, effectively giving the judiciary a veto over legislative action.",
      "C) The doctrine provides a mechanism for the Court to review and potentially overturn Congressional decisions on interstate commerce, but only in cases where the Court determines that Congress has acted in a way that is inconsistent with the Constitution.",
      "D) The doctrine represents a shift towards a more collaborative approach between the legislative and judicial branches, in which the Court works with Congress to establish and enforce constitutional limits on interstate commerce."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Not anymore. Why? Because it is improper to \u201cask Congress to address a false constitutional premise of th[e] Court\u2019s own creation. \u201d88\u00d7 88. Wayfair, 138 S. Ct. at 2096. The Latin for Wayfair\u2019s doctrine is not stare decisis, which should reflect a realistic, working relationship between the legislative and judicial branches. It is mea culpa. In its zeal to update the Constitution for \u201cthe Cyber Age,\u201d89\u00d7 89. Id. at 2097. the Court deleted Congress from stare decisis doctrine in constitutional cases. The Court had better options. It could have left Quill on Congress\u2019s doorstep, as the dissent argued. Or it could have justified overruling Quill notwithstanding the special force of stare decisis. Instead, the Court reasoned that it doesn\u2019t matter whether Congress is willing and able to do the job: a constitutional mess calls for a judicial clean-up crew. For constitutional default rules \u2014 a category of decisions embracing the dormant commerce clause and sweeping far beyond \u2014 Wayfair\u2019s new theory of stare decisis makes the Court\u2019s precedents less sticky and Congress less relevant."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the electronic structure of a coinage metal surface-bound molecule, 5, used bilayer NaCl films to experimentally decouple the molecule from the metal surfaces. The researchers performed DFT calculations on the neutral charge state of 5 in the gas phase, which led to the identification of a local minimum, 5OS. This local minimum corresponds to the open-shell resonance structure of 5. However, the researchers also observed that the molecule's electronic structure is influenced by the presence of trilayer NaCl islands on the surface. Which of the following statements best describes the relationship between the trilayer NaCl islands and the electronic structure of 5OS?",
    "choices": [
      "A) The trilayer NaCl islands have no effect on the electronic structure of 5OS, as the molecule is decoupled from the metal surfaces.",
      "B) The trilayer NaCl islands enhance the electronic structure of 5OS by increasing the molecule's spin density.",
      "C) The trilayer NaCl islands have a negligible effect on the electronic structure of 5OS, as the molecule's open-shell resonance structure dominates the electronic properties.",
      "D) The trilayer NaCl islands influence the electronic structure of 5OS by altering the molecule's spin density and bond lengths, which are critical for understanding its electronic properties."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In the main text, we focus on the generation and characterization of 5 on insulating surfaces. Generation and characterization of 5 on coinage metal surfaces is shown in Supplementary Fig. . ). Blue and orange colors represent spin up and spin down densities, respectively. c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -\u00c5 -3 ). d, DFT-calculated bond lengths of 5OS. e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra. Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side). Acquisition position of the spectra is shown in Supplementary Fig. . f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.\nAlso shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible. Scanning parameters: I = 0.3 pA (V = -1.2 V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, \u0394z = -0.3\n\u00c5. The tip-height offset \u0394z for each panel is provided with respect to the STM setpoint, and positive (negative) values of \u0394z denote tip approach (retraction) from the STM setpoint. f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island. The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively. Scale bars: 10 \u00c5 (f) and 5 \u00c5 (g). To experimentally explore the electronic structure of 5, we used bilayer NaCl films on coinage metal surfaces to electronically decouple the molecule from the metal surfaces. Before presenting the experimental findings, we summarize the results of our theoretical calculations performed on 5 in the neutral charge state (denoted as 5 0 ). We start by performing DFT calculations on 5 0 in the gas phase. Geometry optimization performed at the spin-unrestricted UB3LYP/6-31G level of theory leads to one local minimum, 5OS, the geometry of which corresponds to the open-shell resonance structure of 5 (Fig. , the label OS denotes open-shell)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the genetic basis of beta thalassemia found that the disease is more prevalent in populations with a higher frequency of a specific genetic variant. This variant is associated with a reduced ability to produce hemoglobin A, which is the normal adult hemoglobin. However, the study also found that the variant is not the sole cause of the disease, as some individuals with the variant do not develop beta thalassemia. In fact, the study suggested that the variant is more likely to be associated with a milder form of the disease. What is the most likely explanation for the association between the genetic variant and beta thalassemia?",
    "choices": [
      "A) The variant is a necessary condition for the development of beta thalassemia, and its presence is sufficient to cause the disease.",
      "B) The variant is a common genetic mutation that is found in many populations, and its presence is not sufficient to cause beta thalassemia, but it is a necessary condition for the development of a milder form of the disease.",
      "C) The variant is a rare genetic mutation that is only found in individuals with beta thalassemia, and its presence is necessary for the development of the disease.",
      "D) The variant is a genetic mutation that affects the production of hemoglobin A, but it is not directly associated with the development of beta thalassemia. Instead, the disease is caused by a combination of genetic and environmental factors."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Scientists continue to study the causes. For instance, a new mutation for alpha-thalassemia was discovered for the first time among Iranian patients in 2004. BETA-THALASSEMIA. Most individuals have two normal copies of the beta globin gene, which is located on chromosome 11 and makes the beta globin component of normal adult hemoglobin, hemoglobin A. There are approximately 100 genetic mutations that have been described that cause beta thalassemia, designated as either beta0 or beta + mutations. No beta globin is produced with a beta0 mutation, and only a small fraction of the normal amount of beta globin is produced with a beta + mutation. When an individual has one normal beta globin gene and one with a beta thalassemia mutation, he or she is said to carry the beta thalassemia trait. Beta thalassemia trait, like other hemoglobin traits, is protective against malaria infection. Trait status is generally thought not to cause health problems, although some women with beta thalassemia trait may have an increased tendency toward anemia during pregnancy. When two members of a couple carry the beta thalassemia trait, there is a 25% chance that each of their children will inherit beta thalassemia disease by inheriting two beta thalassemia mutations, one from each parent. The clinical severity of the beta thalassemia disease\u2014whether an individual has beta thalassemia intermedia or beta thalassemia major\u2014will depend largely on whether the mutations inherited are beta0 thalassemia or beta + thalassemia mutations. Two beta0 mutations generally lead to beta thalassemia major, and two beta+ thalassemia mutations generally lead to beta thalassemia intermedia. Inheritance of one beta0 and one beta + thalassemia mutation tends to be less predictable. Although relatively uncommon, there are other thalassemia-like mutations that can affect the beta globin gene. Hemoglobin E is the result of a substitution of a single nucleotide. This change results in a structurally altered hemoglobin that is produced in decreased amounts."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher studying the dynamics of molecular ultracold plasmas in a laboratory setting observes that the ion temperature is elevated to around one degree Kelvin, despite the initial electron temperature being above 30 K. This outcome is likely due to:",
    "choices": [
      "A) The fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes, which are thought to be the primary mechanisms for elevating the ion temperature in this regime.",
      "B) The presence of a strong external electromagnetic field, which causes the ions to become magnetically confined and prevents them from escaping, leading to a rapid increase in ion temperature.",
      "C) The fact that the initial electron temperature was above 30 K, which is sufficient to overcome the Coulomb barrier and allow for efficient ionization, resulting in a rapid increase in ion temperature.",
      "D) The combination of fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes, which work together to elevate the ion temperature to around one degree Kelvin, while also allowing for the efficient ionization of the ions due to the initial electron temperature being above 30 K."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\section{Introduction}\n\nUltracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \\cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \\cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \\cite{Rousse,Esarey}. Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field. The relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s. A series of reviews affords a good overview of progress in the last twenty years \\cite{Gallagher,Killian_Science,PhysRept,Lyon}. Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas. While molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \\cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \\cite{Killian}. This work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions. Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \\cite{Kuzmin,Hanson,Laha}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the evolution of plasticity rules in agents performing a foraging task found that environmental variability significantly impacts the development of neuronal plasticity mechanisms. However, the researchers also observed that the interaction between a static motor network and a plastic sensory network can lead to a greater variety of well-functioning learning rules. What is a potential reason for this degeneracy in learning rule variability?",
    "choices": [
      "A) The motor network's ability to extract consistent information from the sensory system limits the development of plasticity rules.",
      "B) The plastic sensory network's increased structure leads to a more predictable output, which the motor network can interpret in a behaviorally useful way.",
      "C) The agents' performance in the task is not affected by the interaction between the motor and sensory networks, and the degeneracy is due to other factors.",
      "D) The motor network's complexity allows it to read out and process the outputs from the plastic network, leading to a greater variety of learning rules."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The agents perform equally well in this variation of the task as before (Fig. ), but now, the evolved plasticity rules seem to be more structured (Fig. ). Moreover, the variance of the learned weights in the bestperforming agents is significantly reduced (Fig. ), which indicates that the bottleneck in the sensory network is in-creasing selection pressure for rules that learn the environment's food distribution accurately. We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity. Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. Our results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents. Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way. Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability. Our findings on the effect of environmental variability concur with the findings of previous studies that have identified the constraints that environmental variability places on the evolutionary viability of learning behaviors."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user deactivates their account on Agency Spotter, but still wants to maintain their profile information in case they reactivate their account in the future. What happens to their profile information after deactivation?",
    "choices": [
      "A) It is permanently deleted from Agency Spotter's database and cannot be restored.",
      "B) It is saved in Agency Spotter's backup and archival copies, but not accessible to other users.",
      "C) It is deleted from Agency Spotter's database, but may still be viewable elsewhere due to shared or distributed information.",
      "D) It is retained by Agency Spotter to prevent identity theft and other misconduct, even after deletion has been requested."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Please be aware that even after your request for a change is processed, Agency Spotter may, for a time, retain residual information about you in its backup and/or archival copies of its database. Deactivating or deleting your account. If you want to stop using your account you may deactivate it or delete it. When you deactivate an account, no user will be able to see it, but it will not be deleted. We save your profile information in case you later decide to reactivate your account. Many users deactivate their accounts for temporary reasons and in doing so are asking us to maintain their information until they return to Agency Spotter. You will still have the ability to reactivate your account and restore your profile in its entirety. When you delete an account, it is permanently deleted from Agency Spotter. You should only delete your account if you are certain you never want to reactivate it. You may deactivate your account or delete your account within your account profile. Limitations on removal. Even after you remove information from your profile or delete your account, copies of that information may remain viewable elsewhere to the extent it has been shared with others, it was otherwise distributed pursuant to your privacy settings, or it was copied or stored by other users. However, your name will no longer be associated with that information on Agency Spotter. (For example, if you post something to another user\u2019s or Agency\u2019s profile or Agency\u2019s portfolio and then you delete your account, that post may remain, but be attributed to an \u201cAnonymous Agency Spotter User.\u201d) Additionally, we may retain certain information to prevent identity theft and other misconduct even if deletion has been requested. If you have given third party applications or websites access to your information, they may retain your information to the extent permitted under their terms of service or privacy policies. But they will no longer be able to access the information through our platform after you disconnect from them."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A system with sinusoidal excitation has poles at \u03bb = \u00b1i\u2126 and residues at \u03b1 = \u2213iA/2. The response coefficients \u03b2p_i,k are calculated using Eqs. 20 and 19, and the first three orders of responses are obtained. In Case 1, the first-order response is in good agreement with the total response obtained by the Runge-Kutta method. However, the higher-order responses only slightly improve the transient parts. What is the primary reason for this phenomenon?",
    "choices": [
      "A) The system's poles are too close to the excitation poles, resulting in a strong resonance effect that dominates the response.",
      "B) The system's residues are too large, causing the response to become non-linear and dominated by higher-order terms.",
      "C) The excitation frequency is too close to the linear natural frequency, resulting in a strong frequency response that dominates the transient parts.",
      "D) The Volterra series expansion is not sufficient to capture the non-linear response, and higher-order terms are necessary to accurately model the system's behavior."
    ],
    "correct_answer": "D)",
    "documentation": [
      "All cases have same amplitudes. The poles of a sinusoidal excitation are \u03bb 1,2 = \u00b1i\u2126, and the residues are \u03b1 1,2 = \u2213iA/2. Numerical values of excitation poles and residues for different cases are listed in Table . Table : Parameter values, poles and residues of the sinusoidal excitation Substituting poles and residues of the excitation, as well as those of the system into Eqs.\n20 and 19, response coefficients \u03b2 p i ,k corresponding to system poles \u2212a i and response coefficients \u03b3 p i ,\u2113 corresponding to excitation poles \u03bb \u2113 are calculated, respectively. According to Eq. 22, the first three orders of responses for each case in Table are calculated. Figures )-15(a) show the comparison of responses obtained by the proposed method and the fourth-order Runge-Kutta method with \u2206t = 10 \u22124 . For Cases 1 and 2, the first-order responses agree well with the total responses obtained by the Runge-Kutta method, and the higher-order responses only slightly improve the transient parts. For Cases 3-5, the sum of the first three orders of responses is in good agreement with the Runge-Kutta solution. When the response nonlinearity increases, higher-order responses need to be considered. In other words, the proposed method can accurately compute the nonlinear responses by choosing a small number N of Volterra series terms. Figures )-15(b) show the contributions of the three response components for the five cases. In each case, the first-order response is the most dominant component, and the contributions of secondand third-order responses are much less than those of the first-order response. Especially for Cases 1 and 2, whose excitation frequencies are far from the linear natural frequency, second-and thirdorder responses are close to zero. This may be because the QFRF and CFRF approach zero when the frequency is larger than 4 rad/s (see Figs. ). Furthermore, the mean values of the first-order responses are approximately zero, and those of the second-order responses are always smaller than zero, which are the difference frequency components in Eq. 27."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A genetic counselor is advising a family of Southeast Asian descent on the risk of beta thalassemia in their newborn. The family's ancestry is primarily Cambodian, with some Vietnamese and Thai ancestry. The counselor notes that the prevalence of beta thalassemia major is highest among Southeast Asians, but also mentions that the prevalence of E/beta thalassemia is relatively high in some specific Southeast Asian populations. Which of the following statements best reflects the genetic counselor's advice?",
    "choices": [
      "A) The family's risk of beta thalassemia major is approximately 1 in 10,000, based on the general prevalence among Southeast Asians.",
      "B) The family's risk of beta thalassemia major is approximately 1 in 4,000, based on the prevalence among Cambodian populations.",
      "C) The family's risk of beta thalassemia major is approximately 1 in 7,000, based on the prevalence among Vietnamese populations.",
      "D) The family's risk of beta thalassemia major is approximately 1 in 2,600, based on the prevalence among Cambodian and Vietnamese populations, with a higher risk of E/beta thalassemia due to their ancestry."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Survivors passed the mutation onto their offspring, and the trait became established throughout areas where malaria is common. As populations migrated, so did the thalassemia traits. Beta thalassemia trait is seen most commonly in people with the following ancestry: Mediterranean (including North African, and particularly Italian and Greek), Middle Eastern, Indian, African, Chinese, and Southeast Asian (including Vietnamese, Laotian, Thai, Singaporean, Filipino, Cambodian, Malaysian, Burmese, and Indonesian). Alpha-thalassemia trait is seen with increased frequency in the same ethnic groups. However, there are different types of alpha thalassemia traits within these populations. The frequency of hemoglobin H disease and alpha thalassemia major depends on the type of alpha thalassemia trait. The populations in which alpha thalassemia diseases are most common include Southeast Asians and Chinese (particularly Southern Chinese). It is difficult to obtain accurate prevalence figures for various types of thalassemia within different populations. This difficulty arises due to testing limitations in determining exact genetic diagnoses, as well as the fact that many studies have focused on small, biased hospital populations. Two studies reflect prevalence figures that can be helpful counseling families and determining who to screen for beta thalassemia. Between the years of 1990 and 1996, the State of California screened more than 3.1 million infants born in the state for beta thalassemia. Approximately 1 in 114,000 infants had beta thalassemia major, with prevalence rates being highest among Asian Indians (about one in 4,000), Southeast Asians (about one in 10,000), and Middle Easterners (about one in 7,000). Another type of beta thalassemia disease, E/beta thalassemia, was represented in approximately one in 110,000 births, all of which occurred in families of Southeast Asian ancestry. Among Southeast Asians, the prevalence of E/beta thalassemia was approximately one in 2,600 births. This is in keeping with the observation that hemoglobin E trait carrier rates are relatively high within the Southeast Asian population: 16% in a study of 768 immigrants to California, and up to 25% in some specific Southeast Asian populations such as Cambodians."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary concern of Brooksley Born, the former Commodity Futures Trading Commission (CFTC) chair, when she warned of an impending financial crisis in 2009?",
    "choices": null,
    "correct_answer": "D)",
    "documentation": [
      "Interview: Brooksley Born for \"PBS Frontline: The Warning\", PBS, (streaming VIDEO 1 hour), October 20, 2009. Articles\nManuel Roig-Franzia. \"Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On\", The Washington Post, May 26, 2009\n Taibbi, Matt. \"The Great American Bubble Machine\", Rolling Stone'', July 9\u201323, 2009\n\n1940 births\nAmerican women lawyers\nArnold & Porter people\nClinton administration personnel\nColumbus School of Law faculty\nCommodity Futures Trading Commission personnel\nHeads of United States federal agencies\nLawyers from San Francisco\nLiving people\nStanford Law School alumni\n21st-century American women\nStanford University alumni"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A bipartisan bill has been passed in the House to delay the sale of Plum Island, but it does not address the issue of gypsy moth infestation in the area. What is a likely consequence of this delay for the ecosystem of Rogers Lake?",
    "choices": [
      "A) The delay will lead to an increase in gypsy moth populations, as the larvae will have more time to mature and spread.",
      "B) The delay will allow the Connecticut delegation to focus on finding a permanent solution to protect Plum Island, which will also benefit the ecosystem of Rogers Lake.",
      "C) The delay will give the Old Lyme Selectman's office more time to implement an aquatic treatment plan for Rogers Lake, which will help mitigate the effects of gypsy moth infestation.",
      "D) The delay will prevent the federal agency in charge of Plum Island from using any of its operational funding to support conservation efforts for Rogers Lake, including the aquatic treatment plan."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Current law states that Plum Island must be sold publicly to help finance the new research facility. Aerial view of Plum Island. The lawmakers joint statement explained, \u201cThe amendment will prevent the federal agency in charge of the island from moving forward with a sale by prohibiting it from using any of its operational funding provided by Congress for that purpose,\u201d concluding, \u201d This will not be the end of the fight to preserve Plum Island, but this will provide us with more time to find a permanent solution for protecting the Island for generations to come.\u201d For several years, members from both sides of Long Island Sound have been working in a bipartisan manner to delay and, ultimately, repeal the mandated sale of this ecological treasure. Earlier this year, the representatives, along with the whole Connecticut delegation, cosponsored legislation that passed the House unanimously to delay the sale of Plum Island. Filed Under: Outdoors July 1 Update: Aquatic Treatment Planned for Rogers Lake, July 5 July 1, 2016 by admin Leave a Comment We received this updated information from the Old Lyme Selectman\u2019s office at 11:05 a.m. this morning:\nFiled Under: Lyme, Old Lyme, Outdoors, Town Hall They\u2019re Everywhere! All About Gypsy Moth Caterpillars \u2014 Advice from CT Agricultural Experiment Station June 2, 2016 by Adina Ripin Leave a Comment Gypsy moth caterpillars \u2013 photo by Peter Trenchard, CAES. The potential for gypsy moth outbreak exists every year in our community. Dr. Kirby Stafford III, head of the Department of Entomology at the Connecticut Agricultural Experiment Station, has written a fact sheet on the gypsy moth available on the CAES website. The following information is from this fact sheet. The gypsy moth, Lymantria dispar, was introduced into the US (Massachusetts) by Etienne Leopold Trouvelot in about 1860. The escaped larvae led to small outbreaks in the area in 1882, increasing rapidly. It was first detected in Connecticut in 1905. By 1952, it had spread to 169 towns. In 1981, 1.5 million acres were defoliated in Connecticut."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A high-performance error mitigation scheme for quantum computing is proposed, which involves a denoiser ensemble with a large number of terms (M) and a sampling overhead (\u03b3) that is exponentially dependent on the number of gates (\u03b3g) in each sample. However, the scheme is known to suffer from a sign problem, which arises due to the probabilistic error cancellation of gate noise. To mitigate this issue, the scheme requires careful tuning of the parameters to minimize the normalized Frobenius distance between the noiseless and denoised supercircuits. What is the primary challenge in implementing this scheme, and how can it be addressed?",
    "choices": [
      "A) The scheme is limited by the number of available qubits, which restricts the number of gates that can be applied in each sample.",
      "B) The scheme is sensitive to the choice of basis states, which can lead to inaccurate denoising if not properly accounted for.",
      "C) The scheme is prone to errors due to the exponential dependence of the sampling overhead on the number of gates, which can result in a large variance of the denoised observables.",
      "D) The scheme requires a large number of samples to resolve the sign problem, which can be addressed by carefully tuning the parameters to minimize the normalized Frobenius distance between the noiseless and denoised supercircuits."
    ],
    "correct_answer": "D)",
    "documentation": [
      "All gates are affected by two-qubit depolarizing noise with p = 0.01. The non-denoised results are labelled with M = 0, and the noiseless values with p = 0. where sgn(\u03b7 g ) is the sign of the sampled coefficient of the gth channel. \u03b3 = 1 means that all signs are positive. Observables \u00d4 p=0 for the noiseless circuit are then approximated by resampling the observables from the denoiser ensemble\nwhere \u03b3 = N G g=1 \u03b3 g is the overall sampling overhead, with \u03b3 g the overhead of the gth gate. Clearly, a large \u03b3 implies a large variance of \u00d4 p=0 for a given number of samples, with accurate estimation requiring the cancellation of large signed terms. The number of samples required to resolve this cancellation of signs is bounded by Hoeffding's inequality, which states that a sufficient number of samples to estimate \u00d4 p=0 with error \u03b4 at probability 1 \u2212 \u03c9 is bounded by (2\u03b3 2 /\u03b4 2 ) ln(2/\u03c9) . Since \u03b3 scales exponentially in \u03b3 g , it is clear that a denoiser with large M and \u03b3 1 will require many samples. We observed that decompositions with \u03b3 > 1 are crucial for an accurate denoiser. Restricting to \u03b3 = 1 leads to large infidelity and no improvement upon increasing the number of terms in or the depth M of the denoiser. Simply put, probabilistic error cancellation of gate noise introduces a sign problem and it is crucial to find optimal parameterizations (1) which minimize \u03b3 to make the approach scalable. This issue arises in all high performance error mitigation schemes , because the inverse of a physical noise channel is unphysical and cannot be represented as a positive sum over CPTP maps. This is clearly visible in the spectra of the denoiser, which lies outside the unit circle (cf. Fig. ). This makes the tunability of the number of gates in each denoiser sample a crucial ingredient, which allows control over the sign problem, because we can freely choose the \u03b7 i in . For the parametrization (1) of denoiser channels, we try to find a set of parameters for error mitigation by minimizing the normalized Frobenius distance between the noiseless and denoised supercircuits\nwhich bounds the distance of output density matrices and becomes zero for perfect denoising."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the decline of Mennonite settlements in McPherson County, Kansas, by the mid-20th century?",
    "choices": [
      "A) The influx of non-Mennonite settlers led to increased competition for land and resources, causing tensions between the two groups.",
      "B) The Mennonites' refusal to adopt modern agricultural practices and technology led to decreased crop yields and economic instability.",
      "C) The construction of the Kansas Highway System in the 1920s and 1930s bypassed many Mennonite settlements, reducing their access to markets and trade.",
      "D) The Mennonites' decision to establish a more centralized church leadership structure led to a decline in community cohesion and social support networks."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(Download 6.8MB PDF eBook)\n\nMennonite Settlements\n Impact of Mennonite settlement on the cultural landscape of Kansas; Brenda Martin; Kansas State University; 1985/1988. Mennonite settlement : the relationship between the physical and cultural environment; Susan Movle; University of Utah; 1975/1886. Status of Mennonite women in Kansas in their church and home relationships; Eva Harshbarger; Bluffton College; 1925/1945. External links\n\nCounty\n \n McPherson County - Directory of Public Officials\nHistorical\n , from Hatteberg's People'' on KAKE TV news\nMaps\n McPherson County Maps: Current, Historic, KDOT\n Kansas Highway Maps: Current, Historic, KDOT\n Kansas Railroad Maps: Current, 1996, 1915, KDOT and Kansas Historical Society\n\n \nKansas counties\n1867 establishments in Kansas"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic hand is designed to perform dexterous manipulation tasks, such as lifting and placing objects. The hand is equipped with force sensors on the fingertips, which provide tactile feedback to the control algorithm. The algorithm uses this feedback to compute the forces needed to apply to the object in each step of the task. However, the hand is also subject to friction and slip, which can cause the object to move or fall. To mitigate this, the algorithm must ensure that the normal contact force is greater than the tangential force divided by the friction coefficient of the materials involved.",
    "choices": [
      "A) The control algorithm should prioritize the tangential force over the normal contact force to prevent slip.",
      "B) The algorithm can use a fixed friction coefficient for all materials, regardless of the object's surface texture or the fingertip's material.",
      "C) The algorithm should only consider the friction coefficient of the object's surface when computing the normal contact force.",
      "D) The control algorithm should use the friction coefficient of the fingertip material and the object's surface texture to compute the normal contact force, taking into account the tangential force and the desired grasp type."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Our control algorithm could also be applied to different hands as it does not depend on the hands configuration. Finally, in previous approaches only lifting tasks had been considered. In our work we demonstrate that our approach can be used to perform more complex tasks, such as placing objects on surfaces and performing handovers, which was not done in previous works. Our goal in this work is to design a control algorithm for an anthropomorphic robotic hand to perform dexterous manipulation skills such as lifting and placing down objects. Our control algorithm will use tactile feedback from the force sensors on the fingertips of the hand to decide the forces that need to be applied to the object in each step of the task. Given the desired forces to be applied, the size of the grasp will be computed. Given the grasp size and a desired grasp type, the posture generator will generate a grasp posture, i.e. the hand configuration, such that the force constraints are satisfied. To model the contacts and friction we use Coulombs' law, which states that in order to avoid slip, the normal contact force f n to the contact surface of an object, times the fiction coefficient \u00b5, has to be larger than the tangential force f t :\n\u00b5f n \u2265 f t You can see an example in Figure , where an object is pressed against a wall by an applied normal force f n , and we have the tangential force f t = mg due to gravity. In order for the object to remain stable we need to apply a normal force: where \u00b5 is the friction coefficient between the object and the wall. In the case of a dexterous hand manipulating an object, we want the normal forces applied by all fingers to be greater than the tangential force divided by the friction coefficient of the materials of the object and the fingertip. Since it is hard to accurately compute the friction coefficient between all possible object materials previous works have used multi-modal tactile sensors like the BioTac sensor, which provides information about the pressure, skin deformation, and temperature, to predict slip and based on that signal to increase the applied normal force."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new object detection model, dubbed \"Efficient R-CNN\", has been proposed, which combines the strengths of Faster R-CNN and RetinaNet. The model uses a novel feature pyramid network (FPN) that is more efficient than the traditional FPN used in Faster R-CNN. However, the model's performance is slightly lower than that of Cascade R-CNN on the COCO dataset. Which of the following statements about the Efficient R-CNN model is true?",
    "choices": [
      "A) It uses a more efficient FPN than Faster R-CNN, but its performance is lower than Cascade R-CNN due to the loss of spatial pyramid pooling (SPP) layers.",
      "B) It uses a more efficient FPN than Cascade R-CNN, but its performance is lower than Faster R-CNN due to the loss of the RetinaNet's focal loss.",
      "C) It uses a more efficient FPN than Cascade R-CNN, but its performance is lower than Faster R-CNN due to the loss of the Faster R-CNN's multi-stage architecture.",
      "D) It uses a more efficient FPN than Faster R-CNN, and its performance is comparable to Cascade R-CNN due to the use of a more efficient loss function."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\label{ben}\n\\begin{tabular}{|l|l|c|c|c|ccc|ccc|cccc|}\n\\hline\nMethod&Backbone&Param.&FLOPs&FPS&AP&AP$_{50}$&AP$_{75}$&AP$_{S}$&AP$_{M}$&AP$_{L}$&AP$_{Ho}$&AP$_{Ec}$&AP$_{Sc}$&AP$_{St}$ \\\\ \n\\hline \n\\emph{multi-stage:} &&&&&&&&&&&&&& \\\\\n\n\\multirow{3}{*}{Faster R-CNN \\cite{Ren2015Faster}}\n&R-18&28.14M&49.75G&5.7&50.1&72.6&57.8&42.9&51.9&48.7&49.1&60.1&31.6&59.7\\\\\n&R-50&41.14M&63.26G&4.7&54.8&75.9&63.1&53.0&56.2&53.8&55.5&62.4&38.7&62.5\\\\\n&R-101&60.13M&82.74G&3.7&53.8&75.4&61.6&39.0&55.2&52.8&54.3&62.0&38.5&60.4\\\\\n\\hline\n\n\\multirow{3}{*}{Cascade R-CNN \\cite{Cai_2019}}\n&R-18&55.93M&77.54G&3.4&52.7&73.4&60.3&\\bf 49.0&54.7&50.9&51.4&62.3&34.9&62.3\\\\\n&R-50&68.94M&91.06G&3.0&55.6&75.5&63.8&44.9&57.4&54.4&56.8&63.6&38.7&63.5\\\\\n&R-101&87.93M&110.53G&2.6&56.0&76.1&63.6&51.2&57.5&54.7&56.2&63.9&41.3&62.6\\\\\n\\hline\n\n\\multirow{3}{*}{Grid R-CNN \\cite{lu2019grid}}\n&R-18&51.24M&163.15G&3.9&51.9&72.1&59.2&40.4&54.2&50.1&50.7&61.8&33.3&61.9\\\\\n&R-50&64.24M&176.67G&3.4&55.9&75.8&64.3&40.9&57.5&54.8&56.7&62.9&39.5&64.4\\\\\n&R-101&83.24M&196.14G&2.8&55.6&75.6&62.9&45.6&57.1&54.5&55.5&62.9&41.0&62.9\\\\\n\\hline\n\n\\multirow{3}{*}{RepPoints \\cite{yang2019reppoints}}\n&R-18&20.11M&\\bf 35.60G&5.6&51.7&76.9&57.8&43.8&54.0&49.7&50.8&63.3&33.6&59.2\\\\\n&R-50&36.60M&48.54G&4.8&56.0&80.2&63.1&40.8&58.5&53.7&56.7&65.7&39.3&62.3\\\\\n&R-101&55.60M&68.02G&3.8&55.4&79.0&62.6&42.2&57.3&53.9&56.0&65.8&39.0&60.9\\\\\n\\hline \n\\hline \n\\emph{one-stage:} &&&&&&&&&&&&&& \\\\\n\\multirow{3}{*}{RetinaNet \\cite{Lin2017Focal}}\n&R-18&19.68M&39.68G&7.1&44.7&66.3&50.7&29.3&47.6&42.5&46.9&54.2&23.9&53.8\\\\\n&R-50&36.17M&52.62G&5.9&49.3&70.3&55.4&36.5&51.9&47.6&54.4&56.6&27.8&58.3\\\\\n&R-101&55.16M&72.10G&4.5&50.4&71.7&57.3&34.6&52.8&49.0&54.6&57.0&33.7&56.3\\\\\n\\hline \n\n\\multirow{3}{*}{FreeAnchor \\cite{2019arXiv190902466Z}}\n&R-18&19.68M&39.68G&6.8&49.0&71.9&55.3&38.6&51.7&46.7&47.2&62.8&28.6&57.6\\\\\n&R-50&36.17M&52.62G&5.8&54.4&76.6&62.5&38.1&55.7&53.4&55.3&65.2&35.3&61.8\\\\\n&R-101&55.16M&72.10G&4.4&54.6&76.9&62.9&36.5&56.5&52.9&54.0&65.1&38.4&60.7\\\\\n\\hline \n\n\\multirow{3}{*}{FoveaBox \\cite{DBLP:journals/corr/abs-1904-03797}}\n&R-18&21.20M&44.75G&6.7&51.6&74.9&57.4&40.0&53.6&49.8&51.0&61.9&34.6&59.1\\\\\n&R-50&37.69M&57.69G&5.5&55.3&77.8&62.3&44.7&57.4&53.4&57.9&64.2&36.4&62.8\\\\\n&R-101&56.68M&77.16G&4.2&54.7&77.3&62.3&37.7&57.1&52.4&55.3&63.6&38.9&60.8\\\\\n\\hline \n\n\\multirow{3}{*}{PAA \\cite{2020arXiv200708103K}}\n&R-18&\\bf 18.94M&38.84G&3.0&52.6&75.3&58.8&41.3&55.1&50.2&49.9&64.6&35.6&60.5\\\\\n&R-50&31.89M&51.55G&2.9&56.8&79.0&63.8&38.9&58.9&54.9&56.5&66.9&39.9&64.0\\\\\n&R-101&50.89M&71.03G&2.4&56.5&78.5&63.7&40.9&58.7&54.5&55.8&66.5&42.0&61.6\\\\\n\\hline \n\n\\multirow{3}{*}{FSAF \\cite{zhu2019feature}}\n&R-18&19.53M&38.88G&\\bf 7.4&49.6&74.3&55.1&43.4&51.8&47.5&45.5&63.5&30.3&58.9\\\\\n&R-50&36.02M&51.82G&6.0&54.9&79.3&62.1&46.2&56.7&53.3&53.7&66.4&36.8&62.5\\\\\n&R-101&55.01M&55.01G&4.5&54.6&78.7&61.9&46.0&57.1&52.2&53.0&66.3&38.2&61.1\\\\\n\\hline \n\n\\multirow{3}{*}{FCOS \\cite{DBLP:journals/corr/abs-1904-01355}}\n&R-18&\\bf 18.94M&38.84G&6.5&48.4&72.8&53.7&30.7&50.9&46.3&46.5&61.5&29.1&56.6\\\\\n&R-50&31.84M&50.34G&5.4&53.0&77.1&59.9&39.7&55.6&50.5&52.3&64.5&35.2&60.0\\\\\n&R-101&50.78M&69.81G&4.2&53.2&77.3&60.1&43.4&55.4&51.2&51.7&64.1&38.5&58.5\\\\\n\\hline \n\n\\multirow{3}{*}{ATSS \\cite{zhang2019bridging}}\n&R-18&\\bf 18.94M&38.84G&6.0&54.0&76.5&60.9&44.1&56.6&51.4&52.6&65.5&35.8&61.9\\\\\n&R-50&31.89M&51.55G&5.2&58.2&\\bf 80.1&66.5&43.9&60.6&55.9&\\bf 58.6&67.6&41.8&64.6\\\\\n&R-101&50.89M&71.03G&3.8&57.6&79.4&65.3&46.5&60.3&55.0&57.7&67.2&42.6&62.9\\\\\n\\hline \n\n\\multirow{3}{*}{GFL \\cite{li2020generalized}}\n&R-18&19.09M&39.63G&6.3&54.4&75.5&61.9&35.0&57.1&51.8&51.8&66.9&36.5&62.5\\\\\n&R-50&32.04M&52.35G&5.5&\\bf 58.6&79.3&\\bf 66.7&46.5&\\bf 61.6&55.6&\\bf 58.6&\\bf 69.1&41.3&\\bf 65.3\\\\\n&R-101&51.03M&71.82G&4.1&58.3&79.3&65.5&45.1&60.5&\\bf 56.3&57.0&\\bf"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a likely consequence of the US's long-term military partnership with the next Iraqi government, as requested by the Obama administration?",
    "choices": [
      "A) The US will be forced to withdraw its troops from Iraq immediately, as the Iraqi government will no longer require their military presence.",
      "B) The US will be able to maintain its strategic air bases in Iraq, including Balad and Tallil, without facing significant opposition from the Iraqi government.",
      "C) The US will be required to provide significant economic aid to the Iraqi government to compensate for the loss of American troops, as part of a new Status of Forces Agreement.",
      "D) The US will be able to maintain its military presence in Iraq, including its strategic air bases, while also ensuring that the Iraqi government takes steps to address the concerns of its female politicians, such as increasing representation in the Cabinet."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In reality, constant American pressure was applied to Maliki, Allawi, Kurdish leaders and other prominent Iraqi politicians throughout the entire nine-month process to form a cabinet. The US intervention included numerous personal phone calls and visits to Baghdad by both President Barack Obama and Vice President Joe Biden. The key objective of the Obama administration has been to ensure that the next Iraqi government will \"request\" a long-term military partnership with the US when the current Status of Forces Agreement (SOFA) expires at the end of 2011. The SOFA is the legal basis upon which some 50,000 American troops remain in Iraq, operating from large strategic air bases such as Balad and Tallil and Al Asad. US imperialism spent billions of dollars establishing these advanced bases as part of its wider strategic plans and has no intention of abandoning them. Cogan's only the second person to include the SOFA in his report. Some are impressed with the 'feat' of taking nearly ten months to form a government, stringing the country along for ten months while no decisions could go through. The editorial board of the Washington Post, for example, was full of praise yesterday. Today they're joined by Iran's Ambassador to Iraq, Hassan Danaiifar. The Tehran Times reports that Danaiifar was full of praise today hailing the \"positive and final step which ended the 10-month political limbo in Iraq.\" However, Danaiifar was less pie-in-the-sky than the Post editorial board because he can foresee future problems as evidenced by his statement, \"We may witness the emergence of some problems after one and half of a year -- for example, some ministers may be impeached.\" Of course, there are already many clouds on the horizon, even if Iranian diplomats and Post editorial boards can't suss them out. For example, Ben Bendig (Epoch Times) noted the objection of Iraq's female politicians to Nouri al-Maliki's decision to nominate only one woman (so far) to his Cabinet: \"Some 50 female lawmakers went to the country's top leadership, the United Nations and the Arab League to voice their concern and desire for increased representation.\""
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study used a Periodic Moving Average Filter (PMAF) to remove motion artifacts from PPG signals, resulting in a filtered signal with a mean period length of 10 minutes. The researchers then estimated the maximum and minimum values of each period using the zero crossing method and found that the mean period length was inversely proportional to the total peak values (R-peak or beat) in each minute. However, the study did not account for the smoothing of the PPG signal using a 1-dimensional Gaussian Filter and Convolution. Which of the following statements is most likely true about the relationship between the mean period length and the total peak values?",
    "choices": [
      "A) The mean period length is directly proportional to the total peak values, indicating a high level of mental arousal.",
      "B) The mean period length is inversely proportional to the total peak values, indicating a low level of mental arousal.",
      "C) The mean period length is independent of the total peak values, and the relationship between the two is not significant.",
      "D) The mean period length is directly proportional to the total peak values, but only after applying the smoothing filter."
    ],
    "correct_answer": "B)",
    "documentation": [
      "We use Periodic Moving Average Filter (PMAF) to remove motion artifacts and noises \\cite{lee07}. We first segment the PPG signal on periodic boundaries and then average the $m^{th}$ samples of each period. After filtering the input PPG signal with a 5-Hz $8^{th}$-order Butterworth low-pass filter, we estimate the maximum and minimum value of each period. The mean of each period are obtained from the maximum and minimum values applying the zero crossing method. These points of the means help determine the boundaries of each period. Then, interpolation or decimation is performed to ensure that each period had the same number of samples \\cite{lee07}. \\subsubsection{Heart Rate and Heart Rate Variability Estimation} We first apply PMAF on PPG signal to remove noises and motion artifacts, refine PPG by smoothing the signal using 1-dimensional Gaussian Filter and Convolution, calculate first derivative of the convoluted signal and finally find the differences between two consecutive peak values which is called HRV \\cite{sel08}. The occurrences of total peak values (R-peak or beat) in each minute is called Heart Rate (HR) with an unit of Beat Per Minute. The signal value property of HRV and HR are inversely proportional which means the mental arousal that increases HR should decrease HRV in the time segment window. Fig~\\ref{fig:ppg_artifact_removal} shows a sample of the noisy and filtered PPG signal and their corresponding Instant Heart Rate. \\begin{figure}[!htb]\n\\vspace{-.1in}\n\\begin{center}\n   \\epsfig{file=ppg_artifact_removal.pdf,height=1.4in, width=3.5in}\n   \\vspace{-.15in}\n\\caption{Top figure illustrates the noisy signal (dotted line) and filtered signal from PPG sensor based on our filtering method. Bottom figure illustrates instant heart rate calculated from noisy signal (dotted line) and filtered signal}\n   \\label{fig:ppg_artifact_removal}\n\\end{center}\n\\vspace{-.15in}\n\\end{figure}\n\\subsection{Physiological Sensor Signal Feature Extraction}\nUsing the above mentioned methods, we removed the noises and motion artifacts from EDA and PPG signals and generated two time series signal from EDA (tonic and phasic components) and one time series signal from PPG (HRV)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Veterans for Peace event was notable for its unique approach to protesting the wars in Iraq and Afghanistan. What was the significance of the American flag that was folded and played taps during the event?",
    "choices": [
      "A) The flag was a symbol of the protesters' respect for the fallen veterans, and its presence was meant to honor the sacrifices of those who had lost their lives in the wars.",
      "B) The flag was a deliberate provocation, meant to stir up controversy and outrage among the public, and its folding was a way to show disrespect to the fallen veterans.",
      "C) The flag was a gesture of solidarity with the families of the fallen veterans, and its presence was meant to provide comfort and support to those who had lost loved ones in the wars.",
      "D) The flag was a historical artifact that had been left behind at a recent funeral, and its presence at the event was meant to serve as a reminder of the human cost of war and the importance of remembering those who had fallen."
    ],
    "correct_answer": "D)",
    "documentation": [
      "A group of veterans under the leadership of Veterans for Peace members Tarak Kauff, Will Covert and Elaine Brower, mother of a Marine who has served three tours of duty in Iraq, sponsored the event with the explicit purpose of putting their bodies on the line. Many participants were Vietnam War veterans; others ranged from Iraq and Afghanistan war veterans in their 20s and 30s to World War II vets in their 80s and older. They were predominately white; men outnumbered women by at least three to one. After a short rally in Lafayette Park, they formed a single-file procession, walking across Pennsylvania Avenue to the solemn beat of a drum. As they reached the police barricade (erected to prevent them from chaining themselves to the gate, a plan they announced on their web site), the activists stood shoulder to shoulder, their bodies forming a human link across the 'picture postcard' tableau in front of the White House.\" Maria Chutchian (Arlington Advocate) quotes, participant Nate Goldshlag (Vietnam veteran) stating, \"\"There was a silent, single file march around Lafayette Park to a drum beat. Then we went in front of the White House,. There were barricades set up in front of white house fence. So when we got there, we jumped over barricades and were able to get right next to the White House fence.\" Participant Linda LeTendre (Daily Gazette) reports: At the end of the rally, before the silent, solemn procession to the White House fence, in honor of those killed in Iraq and Afghan wars of lies and deceptions, the VFP played taps and folded an American flag that had been left behind at a recent funeral for the veteran of one of those wars. Two attendees in full dress uniform held and folded the flag. I had the image of all of the people who stood along the roads and bridges when the bodies of the two local men, Benjamin Osborn and David Miller, were returned to the Capital District. I thought if all of those people were here now or spoke out against war these two fine young men might still be with us."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A city council member is accused of violating the city's code of ethics by accepting a large donation from a developer who has a vested interest in a new project. The council member claims that the donation was made in the spirit of public service and that they have always acted in the best interests of the city. However, an investigation reveals that the council member has been in close communication with the developer and has been seen attending events sponsored by the developer. What is the most likely reason for the council member's actions?",
    "choices": [
      "A) The council member is trying to curry favor with the developer in order to secure funding for the city's infrastructure projects.",
      "B) The council member is simply a good friend of the developer and is not aware of the potential conflict of interest.",
      "C) The council member is trying to use their position to influence the outcome of the project in order to benefit the developer's business interests.",
      "D) The council member is being transparent about their relationship with the developer and is using their position to advocate for the city's interests, despite the potential conflict of interest."
    ],
    "correct_answer": "D)",
    "documentation": [
      "That is a good thing. So we can move the discussion ahead. Once, again this was handled incorrectly by our city custodians from the beginning. And now here we are. The public is not supporting this very expensive, very limited benefit project. As you said, until a plan is developed that the public can support, things don\u2019t look good. All this discussion about the water issue has only reinforced my opinion the issue hasn\u2019t been about water, only how the plan should be paid for. Or more specifically, to what extent do we allow our elected custodians and our un-elected GOD tzar decide which laws they will follow and which laws they will ignore. When the City GOD tzar tell citizens at a council meeting if we don\u2019t agree with the City\u2019s plan, then we should just sue him, and when the City Attorney explains to a citizen at a City Council meeting that she does have to respond to their questions because she does NOT work for them. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job providing direction to their employees and listening to and representing the CITIZENS. The subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the introduction of the DTaP vaccine led to a significant decrease in pertussis cases in developed countries. However, the same study also revealed that the vaccine was not as effective in preventing pertussis in infants under the age of 6 months. What is the most likely explanation for this discrepancy?",
    "choices": [
      "A) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months due to the presence of a specific antigen that is not present in the vaccine.",
      "B) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not administered frequently enough to provide adequate protection.",
      "C) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not sufficient to provide long-term immunity against the disease.",
      "D) The DTaP vaccine is not effective in preventing pertussis in infants under 6 months because the vaccine is not designed to target the specific strain of pertussis that is most common in this age group."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. .. Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. .. (Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.\" As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing. Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the NFPA method, what is the role of the consistency term $\\hat{D}_F$ in the HOLO-plus-consistency system?",
    "choices": [
      "A) It is used to solve the LO equation by providing an additional source term.",
      "B) It is used to force the transport and modified FP equations to be consistent, accounting for large-angle scattering.",
      "C) It is used to modify the HO equation to make it consistent with the LO equation, but does not account for large-angle scattering.",
      "D) It is used to solve the HO equation by providing an additional source term, but is not necessary for the consistency of the system."
    ],
    "correct_answer": "B)",
    "documentation": [
      "\\end{equation}\nThe role of $\\hat{D}_F$ is to force the transport and modified FP equations to be consistent. Subtracting \\cref{mfp1} from \\cref{transport1} and rearranging, we obtain the consistency term\n\\begin{equation}\n\\label{dfp}\n\\hat{D}_F = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_l - \\frac{\\sigma_{tr}}{2}\\frac{\\partial}{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi}{\\partial \\mu} - \\sigma_{s,0} \\psi\\,. \\end{equation}\n\nThe NFPA method is given by the following equations:\n\\begin{subequations}\\label[pluraleq]{holocons}\n\\begin{align}\n\\label{HO1}\n\\text{HO}&: \\mu\\frac{\\partial \\psi_{HO}}{\\partial x} + \\sigma_t \\psi_{HO} = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_{l, LO} + Q\\,,\\\\\n\\label{LO11}\n\\text{LO}&: \\mu\\frac{\\partial \\psi_{LO}}{\\partial x} + \\sigma_a \\psi_{LO} = \\frac{\\sigma_{tr}}{2}\\frac{\\partial }{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi_{LO}}{\\partial \\mu} + \\hat{D}_F + Q\\,,\\\\\n\\label{con1}\n\\text{Consistency term}&: \\hat{D}_F = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_{l, HO}^m - \\frac{\\sigma_{tr}}{2}\\frac{\\partial }{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi_{HO}}{\\partial \\mu} - \\sigma_{s,0} \\psi_{HO}\\,,\n\\end{align}\n\\end{subequations}\nwhere $\\psi_{HO}$ is the angular flux obtained from the HO equation and $\\psi_{LO}$ is the angular flux obtained from the LO equation. The nonlinear HOLO-plus-consistency system given by \\cref{holocons} can be solved using any nonlinear solution technique \\cite{kelley}. Note that the NFPA scheme returns a FP equation that is consistent with HO transport. Moreover, this modified FP equation accounts for large-angle scattering which the standard FP equation does not. The LO equation (\\ref{fp1}) can then be integrated into multiphysics models in a similar fashion to standard HOLO schemes \\cite{patelFBR}. To solve the HOLO-plus-consistency system above, we use Picard iteration \\cite{kelley}:\n\\begin{subequations}\n\\begin{align}\n\\label{H1}\n\\text{Transport Sweep for HO}&:\n\\mathcal{L} \\psi_{HO}^{k+1} = \\mathcal{S} \\psi_{LO}^{k} + Q, \\\\\n\\label{L1}\n\\text{Evaluate Consistency Term}&: \\hat{D}_F^{k+1} = \\left(\\mathcal{S} - \\mathcal{F} - \\sigma_{s,0}\\mathcal{I}\\right) \\psi_{HO}^{k+1}, \\\\\n\\label{c1}\n\\text{Solve LO Equation}&: \\psi_{LO}^{k+1} = \\mathcal{P}^{-1} \\left(\\hat{D}_F^{k+1} + Q\\right), \n\\end{align}\n\\end{subequations}\nwhere $\\mathcal{L}$ and $\\mathcal{S}$ are given in \\cref{trans1}, $\\mathcal{P}$ and $\\mathcal{F}$ are given in \\cref{FPSAsi1}, $\\mathcal{I}$ is the identity operator, and $k$ is the iteration index."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher analyzes the time series of power-law exponents for a cryptocurrency and finds that the exponents seem to increase with time for both positive and negative returns. However, the results also suggest that market capitalization affects these power-law exponents. What can be inferred about the relationship between the power-law exponents and the cryptocurrency's age, given that the researcher assumes a linear association between the power-law exponents and the logarithm of market capitalization?",
    "choices": [
      "A) The power-law exponents are likely to increase with time for cryptocurrencies with higher market capitalization.",
      "B) The researcher's assumption of a linear association between the power-law exponents and the logarithm of market capitalization is consistent with the finding that market capitalization affects these power-law exponents.",
      "C) The power-law exponents are likely to increase with time for cryptocurrencies with lower market capitalization.",
      "D) The researcher's assumption of a linear association between the power-law exponents and the logarithm of market capitalization is consistent with the finding that the power-law exponents increase with time for both positive and negative returns, and that market capitalization affects these power-law exponents."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Indeed, only four ('stablecoin\", \"scrypt\", \"fantom-ecosystem\" and \"alameda-research-portfolio\") out of the fifty groupings have both distributions indistinguishable under a two-sample Kolmogorov-Smirnov test (p-value > 0.05). Focusing now on the evolution of the power-law exponents quantified by the time series \u03b1 t for positive and negative returns, we ask whether these exponents present particular time trends. For Bitcoin [Fig. )], \u03b1 t seems to increase with time for both positive and negative returns. At the same time, the results of Fig. also suggest that market capitalization affects these power-law exponents. To verify these possibilities, we assume the power-law exponents (\u03b1 t ) to be linearly associated with the cryptocurrency's age (y t , measured in years) and the logarithm of market capitalization (log c t ). As detailed in the Methods section, we frame this problem using a hierarchical Bayesian model. This approach assumes that the linear coefficients associated with the effects of age (A) and market capitalization (C) of each digital currency are drawn from distributions with means \u00b5 A and \u00b5 C and standard deviations \u03c3 A and \u03c3 C , which are in turn distributed according to global distributions representing the overall impact of these quantities on the cryptocurrency market. The Bayesian inference process consists of estimating the posterior probability distributions of the linear coefficients for each cryptocurrency as well as the posterior distributions of \u00b5 A , \u00b5 C , \u03c3 A , and \u03c3 C , allowing us to simultaneously probe asset-specific tendencies and overall market characteristics. Moreover, we restrict this analysis to the 2140 digital currencies having more than 50 observations of market capitalization concomitantly to the time series of the power-law exponents in order to have enough data points for detecting possible trends. When considering the overall market characteristics, we find that the 94% highest density intervals for \u00b5 A ([-0.01, 0.06] for positive and [-0.02, 0.03] for negative returns) and \u00b5 C ([-0.02, 0.03] for positive and [-0.001, 0.04] for negative returns) include the zero (see Supplementary Figure for their distributions)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Margaret Way's writing career began when her friend brought her a pile of Mills & Boon books, which she read and decided to write her own novels. However, her background as a pianist and vocal coach may have influenced her writing style. Which of the following statements is most consistent with Margaret Way's background and writing career?",
    "choices": [
      "A) Margaret Way's writing style was heavily influenced by her experience as a pianist, which she used to compose music for her novels.",
      "B) Margaret Way's novels often featured strong female protagonists who were also accomplished pianists and vocal coaches.",
      "C) Margaret Way's writing career was delayed until she had completed her education as a pianist and vocal coach.",
      "D) Margaret Way's novels were often set in exotic locations, reflecting her experience as a pianist and vocal coach who had traveled extensively."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Margaret Way (b. Brisbane d. Cleveland, Queensland, Australia ) was an Australian writer of romance novels and women's fiction. A prolific author, Way wrote more than 120 novels since 1970, many through Mills & Boon, a romance imprint of British publisher Harlequin UK Ltd., owned by Harlequin Enterprises. Biography\nBefore her marriage, she was a well-known pianist, teacher, vocal coach and accompanist. She began writing when her son, Laurence Way, was born, a friend took a pile of Mills & Boon books to her, she read all and decided that she also could write these types of novels. She began to write and promote her country with her stories set in Australia. She sold her first novels in 1970. Margaret Way lives with her family in her native Brisbane. Beginning in 2013, Margaret began to self-publish, releasing her first \"e-book\" mid-July. Margaret died on the 10th of August 2022 in Cleveland, Queensland. Bibliography\n\nSingle Novels\nKing Country (1970)\nBlaze of Silk (1970) The Time of the Jacaranda (1970)\nBauhinia Junction (1971)\nMan from Bahl Bahla (1971)\nSummer Magic (1971)\nReturn to Belle Amber (1971)\nRing of Jade (1972)\nCopper Moon (1972) Rainbow Bird (1972) Man Like Daintree (1972)\nNoonfire (1972)\nStorm Over Mandargi (1973)\nWind River (1973)\nLove Theme (1974)\nMcCabe's Kingdom (1974)\nSweet Sundown (1974) Reeds of Honey (1975)\nStorm Flower (1975)\nLesson in Loving (1975)\nFlight into Yesterday (1976)\nRed Cliffs of Malpara (1976)\nMan on Half-moon (1976) Swan's Reach (1976)\nMutiny in Paradise (1977) One Way Ticket (1977) Portrait of Jaime (1977)\nBlack Ingo (1977)\nAwakening Flame (1978)\nWild Swan (1978) Ring of Fire (1978)\nWake the Sleeping Tiger (1978)\nValley of the Moon (1979)\nWhite Magnolia (1979)\nWinds of Heaven (1979)\nBlue Lotus (1979) Butterfly and the Baron (1979)\nGolden Puma (1980)\nTemple of Fire (1980) Lord of the High Valley (1980)\nFlamingo Park (1980)\nNorth of Capricorn (1981)\nSeason for Change (1981)\nShadow Dance (1981)\nMcIvor Affair (1981)\nHome to Morning Star (1981)\nBroken Rhapsody (1982)"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A fast charger using the FS312A chip can deliver up to 20V of voltage to the device, but it also supports a maximum current of 35W. What is the maximum power that can be delivered to a device connected to the FS312A chip?",
    "choices": [
      "A) 20V x 35W = 700W",
      "B) 20V x 35W = 700W, but the charger can also deliver up to 48V of voltage",
      "C) 20V x 35W = 700W, but the charger can only deliver up to 20V of voltage",
      "D) 20V x 35W = 700W, but the charger can only deliver up to 20V of voltage and 18W of current"
    ],
    "correct_answer": "D)",
    "documentation": [
      "'\u65e0\u9521\u901f\u82af\u5fae\u7535\u5b50\u6709\u9650\u516c\u53f8\u662f\u4e00\u5bb6\u96c6\u82af\u7247 \u7814\u53d1\uff0c\u9500\u552e\u548c\u670d\u52a1\u4e8e\u4e00\u4f53\u7684\u56fd\u5bb6\u9ad8\u65b0\u6280 \u672f\u4f01\u4e1a\uff0c\u4e3a\u5ba2\u6237\u63d0\u4f9b\u9ad8\u6027\u80fd\uff0c\u9ad8\u96c6\u6210 \u5ea6\uff0c\u6781\u81f4\u4f53\u9a8c\u7684\u5168\u534f\u8bae\u5feb\u5145\u82af\u7247\u3002 \u65e0\u9521\u901f\u82af\u5fae\u7535\u5b50\u6709\u9650\u516c\u53f8 FastSOC Microelectronics Co.,Ltd. \u9500\u552e\u8054\u7cfb\u65b9\u5f0f\uff1a \u8054\u7cfb\u4eba\uff1a\u987e\u5148\u751f \u624b\u673a\uff1a1800 185 3071 \u90ae\u7bb1\uff1agpp@fastsoc.com \u7f51\u5740\uff1awww.fastsoc.com \u5730\u5740\uff1a\u65e0\u9521\u5e02\u65b0\u5434\u533a\u83f1\u6e56\u5927\u9053200\u53f7\u4e2d\u56fd\u7269\u8054\u7f51\u56fd\u9645\u521b\u65b0\u56edE-503\u5ba4 \u987e\u5de5\u5fae\u4fe1\u53f7 \u901f\u82af\u5fae\u516c\u4f17\u53f7 \u514d\u8d23\u58f0\u660e\uff1a\u672c\u6587\u6240\u8ff0\u65b9\u6cd5\u3001\u65b9\u6848\u5747\u4f9b\u5ba2\u6237\u53c2\u8003\uff0c\u7528\u4e8e\u63d0\u793a\u6216\u8005\u5c55\u793a\u82af\u7247\u5e94\u7528\u7684\u4e00\u79cd\u6216\u8005\u591a\u79cd\u65b9\u5f0f\uff0c\u4e0d\u4f5c\u4e3a\u6700\u7ec8\u4ea7\u54c1\u7684\u5b9e\u9645\u65b9\u6848\u3002\u6587\u4e2d\u6240\u63cf\u8ff0\u7684\u529f\u80fd\u548c\u6027\u80fd\u6307\u6807\u5728\u5b9e \u9a8c\u5ba4\u73af\u5883\u4e0b\u6d4b\u8bd5\u5f97\u5230\uff0c\u90e8\u5206\u53ef\u4ee5\u63d0\u4f9b\u7b2c\u4e09\u65b9\u6d4b\u8bd5\u62a5\u544a\uff0c\u4f46\u662f\u4e0d\u4fdd\u8bc1\u5ba2\u6237\u4ea7\u54c1\u4e0a\u80fd\u83b7\u5f97\u76f8\u540c\u7684\u6570\u636e\u3002\u672c\u6587\u4fe1\u606f\u53ea\u4f5c\u4e3a\u82af\u7247\u4f7f\u7528\u7684\u6307\u5bfc\uff0c\u4e0d\u6388\u6743\u7528\u6237\u4f7f\u7528\u672c\u516c\u53f8\u6216\u8005\u5176 \u4ed6\u516c\u53f8\u7684\u77e5\u8bc6\u4ea7\u6743\u3002\u672c\u6587\u4fe1\u606f\u53ea\u4f5c\u4e3a\u82af\u7247\u4f7f\u7528\u7684\u6307\u5bfc\uff0c\u4e0d\u627f\u62c5\u56e0\u4e3a\u5ba2\u6237\u81ea\u8eab\u5e94\u7528\u4e0d\u5f53\u800c\u9020\u6210\u7684\u4efb\u4f55\u635f\u5931\u3002 **\u6587\u4e2d\u4fe1\u606f\u4ec5\u4f9b\u53c2\u8003\uff0c\u8be6\u60c5\u8bf7\u8054\u7cfb\u6211\u53f8\u83b7\u53d6\u6700\u65b0\u8d44\u6599\u201d \u65e0\u9521\u901f\u82af\u5fae\u7535\u5b50\u6709\u9650\u516c\u53f8 FastSOC Microelectronics Co.,Ltd. \u4ea7\u54c1\u624b\u518c 2023\u5e74 \n\u65b0\u54c1\u5feb\u89c8 FS312A\uff1aPD3.0 \u8bf1\u9a97- FS312A\u652f\u6301PD2.0/PD3.0\u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a20V - FS312AE\u652f\u6301PD2.0/PD3.0 \u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a20V\u652f\u6301Emarker\u6a21\u62df\u529f\u80fd - \u5c01\u88c5\uff1aSOT23-5 VBUS CC1 CC2 DM DP \u7528\u7535\u7535\u8def 4.7K 0.47uF R C C 1 V D D F U N C C C 2F S 3 1 2 B D M D P EP GND \u5e94\u7528\u56fe FS8628\uff1aA+C\u5feb\u5145\u534f\u8baeCC2 CC1 VBUS CC2 CC1 FS312A FUNC GND VDD 4.7K GND R \u7528\u7535\u7535\u8def 1uF GND \u5e94\u7528\u56fe \u591a\u53e3\u6781\u7b80\u65b9\u6848 FS8611SP*2+CCM-8611SP-A+7533B-T \u53ccC\u667a\u80fd\u964d\u529f\u7387\u65b9\u6848 FS8611S USB-C AC-DC \u53cc\u53d8\u538b\u5668 7533B-T CCM-8611SP-A FS8611S USB-C \u91c7\u75282\u9897FS8611SP\u642d\u914dCCM-8611SP-A \uff08MCU\uff09\uff0c7533B-T\u914d\u5408\u5de5\u4f5c - \u652f\u6301\u591a\u79cd\u534f\u8bae - \u652f\u6301I2C\u63a7\u5236 - \u4efb\u610f\u5355 C \u7684\u4e3a 35W - \u53cc \u63d2 \u964d \u529f \u7387 \uff0c \u4e09 \u6863 \u529f \u7387 \u667a \u80fd \u914d \u7f6e\uff1a27.4W+7.4W\uff1b17.4W+17.4W\uff1b 27.4W - BOM\u6781\u7b80\uff0c\u6210\u672c\u4f4e FS312B\uff1aPD3.1 \u8bf1\u9a97FS8611K*2+CCM-8611K-A+7550B-T \u53ccC\u65b9\u6848 - FS312BL\u652f\u6301PD2.0/PD3.0/PD3.1/\u7b2c\u4e09\u65b9\u534f\u8bae\u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a20V - FS312BLE\u652f\u6301PD2.0/PD3.0/PD3.1/\u7b2c\u4e09\u65b9\u534f\u8bae\u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a20V\u652f\u6301Emarker\u6a21\u62df\u529f\u80fd - FS312BH\u652f\u6301PD2.0/PD3.0/PD3.1/\u7b2c\u4e09\u65b9\u534f\u8bae\u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a48V - FS312BHE\u652f\u6301PD2.0/PD3.0/PD3.1/\u7b2c\u4e09\u65b9\u534f\u8bae\u6700\u9ad8\u8bf1\u9a97\u7535\u538b\uff1a48V \u652f\u6301Emarker\u6a21\u62df\u529f\u80fd - \u5c01\u88c5\uff1aDFN2x2-6L - \u517c\u5bb9\u517c\u5bb9BC1.2\u3001Apple2.4A\u3001 QC2.0 Class A\u3001QC3.0 Class A/B\u3001 FCP\u3001SCP\u3001AFC\u3001\u4f4e\u538b\u76f4\u5145\u7b49 - \u517c\u5bb9Type-C PD2.0\u3001Type-C PD3.0\u3001 Type-C PD3.0 PPS\u3001QC4.0\u534f\u8bae - \u652f\u6301\u4e24\u8defDP/DM - \u652f\u6301CV/CC\uff08\u5206\u6bb5CC\uff09\u529f\u80fd - \u652f\u6301\u5b9a\u5236PDO - \u652f\u6301A+C\u53cc\u53e3\u5de5\u4f5c\uff0c\u7535\u538b\u81ea\u52a8\u56de5V - \u652f\u6301FB/OPTO\u53cd\u9988 - \u5c01\u88c5\uff1aQFN3x3-20L VPWR FB PowerSystem 100K GND R1 GND 19 VIN 17 FB FUNC1 FUNC2 20 15 18 13 PLUGIND VFB FS8628 QFN3x3-20L AGATE 47K 7.5K 47K 7.5K 1 16 8 7 3 4 5 6 10 9 11 CGATE CVBUS CC2 CC1 CDP CDM AVBUS DM DP ISP ISN 12 \u5e94\u7528\u56fe 2 V3P3 100\u03a9 1u EP GND GND CVBUS TYPE- C CC2 CC1 CDP CDM CGND TYPE-A AVBUS DM DP 10n 200 AGND 5m\u03a9 GND FS8611K USB-C AC-DC DC-DC 7550B-T CCM-8611K-A FS8611K USB-C \u91c7\u75282\u9897FS8611K\u642d\u914dCCM-8611K-A \uff08MCU\uff09\u5de5\u4f5c\uff0c7550B-T\u914d\u5408\u5de5\u4f5c - \u652f\u6301PD2.0/PD3.0/QC2.0/AFC/FCP - \u652f\u6301PDO\u5b9a\u5236 - \u4efb\u610f\u5355 C \u7684\u4e3a 35W(\u53ef\u5b9a\u5236) - \u53cc\u63d218W\uff08\u53ef\u5b9a\u523615W/20W\uff09 -"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the condition under which the conduction gap in graphene is zero, considering the strain-induced shifts of Dirac points and the effects of tensile and compressive strains?",
    "choices": [
      "A) The conduction gap is zero when the strain-induced shift of Dirac points in the \u03bay-axis is equal to the shift in the \u03bax-axis, and the strain is either tensile or compressive.",
      "B) The conduction gap is zero when the strain-induced shift of Dirac points in the \u03bay-axis is equal to the shift in the \u03bax-axis, and the strain is always tensile.",
      "C) The conduction gap is zero when the strain-induced shift of Dirac points in the \u03bay-axis is equal to the shift in the \u03bax-axis, and the strain is always compressive.",
      "D) The conduction gap is zero when the strain-induced shift of Dirac points in the \u03bay-axis is equal to the shift in the \u03bax-axis, and the strain is either tensile or compressive, with the condition that the shift in the \u03bax-axis is equal to the shift in the \u03bay-axis."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Hence, the data obtained for $\\phi$ ranging from $-30^\\circ$ to $30^\\circ$ and $\\theta  \\in \\left[ {0^\\circ ,180^\\circ } \\right]$ covers the properties of conduction gap in all possible cases. In Fig. 5, we present the maps of conduction gap with respect to the strain and its applied direction in two particular cases: the transport is either along the armchair ($\\phi = 0$) or the zigzag ($\\phi = 30^\\circ$) directions. Both tensile and compressive strains are considered. Let us first discuss the results obtained in the armchair case. Figs. 5(a,b) show that (i) a large conduction gap up to about 500 meV can open with a strain of 6 $\\%$ and (ii) again the conduction gap is strongly $\\theta$-dependent, in particular, its peaks occur at $\\theta = 0$ or $90^\\circ$ while the gap is zero at $\\theta \\approx 47^\\circ$ and $133^\\circ$ for tensile strain and at $\\theta \\approx 43^\\circ$ and $137^\\circ$ for compressive strain. In principle, the conduction gap is larger if the shift of Dirac points in the $\\kappa_y$-axis is larger, as discussed above about Figs. 3-4. We notice that the strain-induced shifts can be different for the six Dirac points of graphene \\cite{kitt12} and the gap is zero when there is any Dirac point observed at the same $\\kappa_y$ in the two graphene sections. From Eq. (9), we find that the Dirac points are determined by the following equations:\n\\begin{eqnarray*}\n  {\\cos}\\frac{\\kappa_y}{2} & =& \\pm \\frac{1}{2}\\sqrt{\\frac{{t_3^2 - {{\\left( {{t_1} - {t_2}} \\right)}^2}}}{{{t_1}{t_2}}}}, \\\\\n  \\cos \\frac{{\\kappa_x}}{2} &=& \\frac{{{t_1} + {t_2}}}{{\\left| {{t_3}} \\right|}}\\cos \\frac{{\\kappa_y}}{2},\\,\\,\\,\\sin \\frac{{\\kappa_x}}{2} = \\frac{{{t_2} - {t_1}}}{{\\left| {{t_3}} \\right|}}\\sin \\frac{{\\kappa_y}}{2},\n\\end{eqnarray*}\nwhich simplify into ${\\cos}\\frac{\\kappa_y}{2} = \\pm \\frac{1}{2}$ and, respectively, $\\cos \\left( {\\frac{{{\\kappa _x}}}{2}} \\right) = \\mp 1$ in the unstrained case. Hence, the zero conduction gap is obtained if\n\\begin{equation*}\n  \\frac{{t_3^2 - {{\\left( {{t_1} - {t_2}} \\right)}^2}}}{{4{t_1}{t_2}}} = \\frac{1}{4}\n\\end{equation*}\nAdditionally, it is observed that the effects of a strain $\\{\\sigma,\\theta\\}$ are qualitatively similar to those of a strain $\\{-\\sigma,\\theta+90^\\circ\\}$, i.e., the peaks and zero values of conduction gap are obtained at the same $\\theta$ in these two situations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is investigating the effects of specific-heat ratio on the dynamic process and TNE behaviors in a two-fluid DBM simulation. The simulation involves a planar shock wave interacting with a 2-D heavy-cylindrical bubble filled with a mixture of Air and SF6. The initial conditions of the ambient gas are \u03c10 = 1.29 kg/m\u00b3, T0 = 293 K, and p0 = 101.3 kPa. The initial parameters of the bubble are \u03c1bubble = 4.859 kg/m\u00b3, pbubble = 101.3 kPa, and Tbubble = 293 K. The dimensionless conditions of macroscopic quantities of the fluid field in the initial time are (\u03c1, T, u_x, u_y) bubble = (4.0347, 1.0, 0.0, 0.0), (\u03c1, T, u_x, u_y) 1 = (1.3416, 1.128, 0.3616, 0.0), and (\u03c1, T, u_x, u_y) 0 = (1.0, 1.0, 0.0, 0.0). The grid size is N_x \u00d7 N_y = 800 \u00d7 400, and the simulation has passed the mesh convergence test.",
    "choices": [
      "A) The specific-heat ratio has a negligible effect on the dynamic process and TNE behaviors in this simulation.",
      "B) The bubble is assumed to be a perfect sphere, and the specific-heat ratio is not relevant to the simulation.",
      "C) The initial conditions of the ambient gas are the same as the initial conditions of the bubble, and the specific-heat ratio is not necessary to consider.",
      "D) The specific-heat ratio affects the dynamic process and TNE behaviors in the simulation, and the correct value of the specific-heat ratio is 1.2."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Strictly speaking, those TNE intensity and effect descriptions that do not account for the research perspective are not correct. Do not explain the research perspective, the corresponding is not dependent on the research perspective. Numerical simulations and results\n\nIn this section, we first validate the DBM code by comparing the DBM results with experimental results. Then, the effects of specific-heat ratio on the dynamic process and TNE behaviors on SBI are investigated. Comparison with experimental results\n\nIn the following part, we use a first-order two-fluid DBM to simulate the interaction between a planar shock wave with a 2-D heavy-cylindrical bubbles, and compare the DBM results with the experimental results from Ref. . The computational configuration can be seen in Fig. . In a flow field which is filled with Air, there is a static bubble composed of 26% Air and 74% SF 6 . A shock with Ma = 1.2 would pass through the bubble from left to right. The initial conditions of ambient gas are \u03c1 0 = 1.29kg/m 3 , T 0 = 293K, p 0 = 101.3kPa. Ignoring the pressure difference between interior gas and ambient gas, the initial parameters of the bubble are \u03c1 bubble = 4.859kg/m 3 , p bubble = 101.3kPa,\nand T 0 = 293K. For simulating, these actual physical quantities should be transferred to dimensionless parameters. This process can refer to the Appendix A. The dimensionless conditions of macroscopic quantities of the fluid field in initial time are (\u03c1, T, u x , u y ) bubble = (4.0347, 1.0, 0.0, 0.0), (\u03c1, T, u x , u y ) 1 = (1.3416,\n1.128, 0.3616, 0.0), (\u03c1, T, u x , u y ) 0 = (1.0, 1.0, 0.0, 0.0), where the subscript \"0\" (\"1\") represents downstream (upstream) region. In two-fluid DBM code, the distribution function f Air is used to describe the ambient gas, i.e., Air. The f bubble characters the bubble which is a mixture that composed of Air and SF 6 . The grid number is N x \u00d7 N y = 800 \u00d7 400, where the N x and N y are grid number in x and y direction, respectively. This grid size has passed the mesh convergence test."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Njoroge's aspiration to attend university is ultimately thwarted by the violence of the Mau Mau rebels and the violent response of the colonial government. What is the likely consequence of this disappointment on Njoroge's relationship with his family?",
    "choices": [
      "A) Mwihaki will become more involved in Njoroge's life, as she sees his struggles as a way to cope with her own loss.",
      "B) Njoroge's family will become more supportive of his education, recognizing the importance of his aspirations.",
      "C) Njoroge's family will distance themselves from him, as they are unable to cope with the stress of his struggles.",
      "D) Njoroge's family will become more involved in the Mau Mau movement, seeing his struggles as an opportunity to fight against colonial rule."
    ],
    "correct_answer": "C)",
    "documentation": [
      "It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as \"entering politics\") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo. Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother. Jacobo: Mwihaki's father and an important landowner. Chief of the village. Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school. Themes and motifs\nWeep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt. The novel also ponders the role of saviours and salvation. The author notes in his The River Between: \"Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people.\" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, \"Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel.\" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the fishing effort of Tanjung Luar's targeted shark fishery found that the number of sets and engine power were significant factors influencing the likelihood of catching regulated species. However, the study also revealed that the relationships between catch per trip and catch per 100 hooks per set varied spatially and temporally, and with several aspects of fishing effort. Which of the following conclusions can be drawn from this study?",
    "choices": [
      "A) The use of larger engines in fishing vessels is the primary factor contributing to the higher catch rates of regulated species in West Nusa Tenggara Province (WNTP) compared to East Nusa Tenggara Province (ENTP).",
      "B) The study suggests that the abundance of sharks in West Nusa Tenggara Province (WNTP) is lower due to the adverse conditions at sea during the west monsoon season, which is characterized by high rainfall.",
      "C) The study found that the number of sets and engine power were significant factors influencing the likelihood of catching regulated species, but the relationships between catch per trip and catch per 100 hooks per set were not affected by the fishing effort.",
      "D) The study highlights the importance of using appropriate standardization for meaningful comparisons of catch per 100 hooks per set across different gears and vessel types, and suggests that this is crucial for fisheries management."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The most significant factors influencing the likelihood of catching regulated species were month (January was significantly lower: p<0.001), number of hooks (p<0.001) and engine power (<0.01). Significant factors associated with standardised CPUE of regulated species were number of hooks (p<0.001), fishing gear (<0.001), number of sets (p<0.001), engine power (p<0.01) and month (November and January: p<0.05) (Table 5 and Fig 4). Plots of most significant factors affecting standardised CPUE (number of individuals per 100 hooks per set) of regulated species: a) hook number, b) gear type, c) number of sets. Although Tanjung Luar\u2019s targeted shark fishery is small in scale, considerable numbers of shark are landed, including a large proportion of threatened and regulated species. A key finding is that measures of CPUE, for all sharks and for threatened and regulated species, vary spatially and temporally, and with several aspects of fishing effort including gear type, hook number, engine power and number of sets. Moreover, the relationships between CPUE and fishing behaviour variables are different for different measures of CPUE (CPUE per trip, CPUE per set, CPUE per 100 hooks per set). This highlights the importance of using appropriate standardisation for meaningful comparisons of CPUE across different gears and vessel types, and has important implications for fisheries management. Unstandardised CPUE (individuals per set) was significantly lower in January. This is during the west monsoon season, which is characterised by high rainfall and adverse conditions at sea for fishing. Unstandardised CPUE was also significantly lower in West Nusa Tenggara Province (WNTP) than East Nusa Tenggara Province (ENTP) and other provinces, suggesting a lower abundance of sharks in this area. Engine power had a significant positive influence on unstandardised CPUE, and was also associated with longer trips and more sets, which was likely due to the ability of vessels with larger engines to travel longer distances, over longer time periods, and with higher numbers of sets, to favoured fishing grounds."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A pastor is preparing to deliver a sermon and has spent the past week refining the content. After reading the sermon out loud, they notice that their tone is too formal in some parts and too informal in others. To address this, they decide to memorize the entire sermon, which they believe will help them find the best words for the job and polish the sermon. However, they also recognize that memorization can sometimes lead to a weak connection between ideas. As they prepare to deliver the sermon, they consider using a preaching outline to cue themselves in case they get stuck. Despite their best efforts, they still feel a bit nervous about their performance. What is the most likely reason for the pastor's nervousness?",
    "choices": [
      "A) They are worried that their gestures will be distracting and send mixed signals to the congregation.",
      "B) They are concerned that their tone is too formal and will put the congregation to sleep.",
      "C) They are anxious about forgetting key phrases and losing the congregation's attention.",
      "D) They are worried that their movements will be too repetitive and stilted, undermining the natural flow of the sermon."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In order to make it presentable, I have a few more steps I go through, and these typically take me a week all by themselves. My goal is to make the sermon sound as natural and engaging as possible. First, I read the sermon out loud and mark anything that doesn\u2019t sound like me. Maybe I was copying someone\u2019s tone, or more likely my tone was too formal or too informal for the moment. I also italicize the words I want to emphasize. It\u2019s all about the sound. Second, I memorize the sermon. (Yes, the whole thing.) This is what they trained us to do in seminary, and I thought it was overkill. Yes, you can get better eye contact, step away from the podium, I get that. But what I\u2019ve discovered is that when I memorize my work it polishes the sermon like nothing else. If I can\u2019t remember what I\u2019m about to say, how can I expect the congregation to remember? Memorizing forces me to find the best words for the job. It also helps me on a structural level, because if I can\u2019t remember what I was about to say next, it shows that there\u2019s a weak connection between the two points. In a compelling script, the next thing has to follow the last. Once you know why the two are married, you can go back and make it more obvious to the congregation. As I memorize, I boil down the transcript into a preaching outline, which has just enough structure and content to cue me if my mind goes blank in the pulpit. It will have the necessary structural elements, markers for key phrases, and all condensed so that it fits on just a few pages on the platform. (One danger is if I don\u2019t use it in practice, it\u2019s less helpful on Sunday.) Third\u2014and frankly this is the step I\u2019m most likely to skip\u2014I try to choreograph my movements. I believe good preaching is theater, but not in the sense that you\u2019re dramatizing the text. Your whole body is communicating whether you want it to or not, so your gestures should be purposeful. Use the space to organize thoughts, repeat certain motions when you repeat the same thought, make sure you\u2019re not sending mixed signals."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on twisted bilayers has revealed a nonlinear dynamical Hall effect that is characteristic of the material's unique electronic structure. The authors propose that this effect is a result of the interplay between the moir\u00e9 pattern and the layer composition. However, the exact mechanism behind this effect is still not fully understood. Which of the following statements best describes the relationship between the twist angle and the intrinsic Hall conductivity of the material?",
    "choices": [
      "A) The intrinsic Hall conductivity is directly proportional to the twist angle, with a slope of 1/2.",
      "B) The intrinsic Hall conductivity is inversely proportional to the twist angle, with a slope of 1/4.",
      "C) The intrinsic Hall conductivity is independent of the twist angle, as the moir\u00e9 pattern and layer composition are in a state of equilibrium.",
      "D) The intrinsic Hall conductivity is proportional to the square of the twist angle, with a slope of 2/3."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Paper Info\n\nTitle: Crossed Nonlinear Dynamical Hall Effect in Twisted Bilayers\nPublish Date: 17 Mar 2023\nAuthor List: Figure\n\nFIG. 1.(a) Schematics of experimental setup.(b, c) Valence band structure and intrinsic Hall conductivity with respect to in-plane input for tMoTe2 at twist angles (b) \u03b8 = 1.2 \u2022 and (c) \u03b8 = 2 \u2022 in +K valley. Color coding in (b) and (c) denotes the layer composition \u03c3 z n (k).\nFIG. 2. (a) The interlayer BCP G, and (b) its vorticity [\u2202 k \u00d7 G]z on the first valence band from +K valley of 1.2 \u2022 tMoTe2.Background color and arrows in (a) denote the magnitude and vector flow, respectively. Grey curves in (b) show energy contours at 1/2 and 3/4 of the band width. The black dashed arrow denotes direction of increasing hole doping level. Black dashed hexagons in (a, b) denote the boundary of moir\u00e9 Brillouin zone (mBZ). FIG. 3. (a-c) Three high-symmetry stacking registries for tBG with a commensurate twist angle \u03b8 = 21.8 \u2022 .Lattice geometries with rotation center on an overlapping atomic site (a, b) and hexagonal center (c).(d) Schematic of the moir\u00e9 pattern when the twist angle slightly deviates from 21.8 \u2022 , here \u03b8 = 21 \u2022 .Red squares marked by A, B and C are the local regions that resemble commensurate 21.8 \u2022 patterns in (a), (b) and (c), respectively.(e, f) Low-energy band structures and intrinsic Hall conductivity of the two geometries [(a) and (b) are equivalent].The shaded areas highlight energy windows \u223c \u03c9 around band degeneracies where interband transitions, not considered here, may quantitatively affect the conductivity measured.\nFIG. S4.Band structure and layer composition \u03c3 z n in +K valley of tBG (left panel) and the intrinsic Hall conductivity (right panel) at three different twist angle \u03b8. The shaded areas highlight energy windows \u223c \u03c9 around band degeneracies in which the conductivity results should not be considered. Here \u03c3H should be multiplied by a factor of 2 accounting for spin degeneracy. abstract\n\nWe propose an unconventional nonlinear dynamical Hall effect characteristic of twisted bilayers."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that patients with chronic kidney disease (CKD) who received vitamin K2 supplementation had a significant reduction in the risk of cardiovascular events. However, another study suggested that vitamin K2 may actually increase the risk of cardiovascular events in patients with CKD who have a history of antibiotic use. Which of the following statements best summarizes the relationship between vitamin K2 supplementation and cardiovascular risk in patients with CKD?",
    "choices": [
      "D) who received vitamin K2 supplementation had a significant reduction in the risk of cardiovascular events. However, another study suggested that vitamin K2 may actually increase the risk of cardiovascular events in patients with CKD who have a history of antibiotic use. Which of the following statements best summarizes the relationship between vitamin K2 supplementation and cardiovascular risk in patients with CKD?",
      "A) Vitamin K2 supplementation is effective in reducing cardiovascular risk in patients with CKD, regardless of their antibiotic use history.",
      "B) Vitamin K2 supplementation increases the risk of cardiovascular events in patients with CKD who have a history of antibiotic use.",
      "C) Vitamin K2 supplementation is effective in reducing cardiovascular risk in patients with CKD, but only in those who have not received antibiotics in the past year."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Journal of Biological Chemistry. 283 (52): 36655\u201336664. doi:10.1074/jbc. M802761200. PMC 2605998. PMID 18836183. ^ Viegas, C. S.; Cavaco, S.; Neves, P. L.; Ferreira, A.; Jo\u00e3o, A.; Williamson, M. K.; Price, P. A.; Cancela, M. L.; Simes, D. C. (Dec 2009). \"Gla-rich protein is a novel vitamin K-dependent protein present in serum that accumulates at sites of pathological calcifications\". American Journal of Pathology. 175 (6): 2288\u20132298. doi:10.2353/ajpath.2009.090474. PMC 2789615. PMID 19893032. ^ Hafizi, S.; Dahlb\u00e4ck, B. (Dec 2006). \"Gas6 and protein S. Vitamin K-dependent ligands for the Axl receptor tyrosine kinase subfamily\". The FEBS Journal. 273 (23): 5231\u20135244. doi:10.1111/j.1742-4658.2006.05529.x. PMID 17064312. ^ Kulman, J. D.; Harris, J. E.; Xie, L.; Davie, E. W. (May 2007). \"Proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein\". Proceedings of the National Academy of Sciences of the United States of America. 104 (21): 8767\u20138772. doi:10.1073/pnas.0703195104. PMC 1885577. PMID 17502622. ^ \"Vitamin K\". MedlinePlus. US National Library of Medicine, National Institutes of Health. Sep 2016. Retrieved 26 May 2009. ^ Conly, J; Stein, K. (Dec 1994). \"Reduction of vitamin K2 concentrations in human liver associated with the use of broad spectrum antimicrobials\". Clinical and Investigative Medicine. 17 (6): 531\u2013539. PMID 7895417. ^ Ferland, G.; Sadowski, J. A.; O'Brien, M. E. (Apr 1993). \"Dietary induced subclinical vitamin K deficiency in normal human subjects\". Journal of Clinical Investigation. 91 (4): 1761\u20131768. doi:10.1172/JCI116386. PMC 288156. PMID 8473516. ^ Holden, R. M.; Morton, A. R.; Garland, J. S.; Pavlov, A.; Day, A. G.; Booth, S. L. (Apr 2010). \"Vitamins K and D status in stages 3-5 chronic kidney disease\". Clinical Journal of the American Society of Nephrology. 5 (4): 590\u2013597. doi:10.2215/CJN.06420909. PMC 2849681. PMID 20167683. ^ Hodges, S. J.; Pilkington, M. J.; Shearer, M. J.; Bitensky, L.; Chayen, J (Jan 1990)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "An advertiser on Agency Spotter wants to measure the effectiveness of their ads and personalize advertising content. What is the primary reason why the advertiser may use technological methods to measure the effectiveness of their ads?",
    "choices": [
      "A) To collect personally identifiable information about users and share it with advertisers, as stated in the Site's privacy policy.",
      "B) To comply with the U.S. law that requires advertisers to disclose their methods for measuring ad effectiveness.",
      "C) To prevent a crime or protect national security, as stated in the Site's terms of service.",
      "D) To limit or prevent the placement of cookies by advertising networks, as allowed by the user's browser cookie settings."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In addition, when you use the Site, our servers automatically record certain information that your web browser sends whenever you visit any website. These server logs may include information such as your web request, Internet Protocol address, browser type, browser language, referring/exit pages and URLs, platform type, number of clicks, domain names, landing pages, pages viewed and the order of those pages, the amount of time spent on particular pages, the date and time of your request, and one or more cookies that may uniquely identify your browser. Information from third party services and other websites. Do not upload or insert any information to or into the Site or Services that you do not want to be shared or used in the manner described in this section. Advertisements. Advertisers who present ads on the Site may use technological methods to measure the effectiveness of their ads and to personalize advertising content. You may use your browser cookie settings to limit or prevent the placement of cookies by advertising networks. Agency Spotter does not share personally identifiable information with advertisers unless we get your permission.\nLinks. When you click on links on Agency Spotter you may leave our site. We are not responsible for the privacy practices of other sites, and we encourage you to read their privacy statements. If we are requested to disclose your information to a government agency or official, we will do so if we believe in good faith, after considering your privacy interests and other relevant factors, that such disclosure is necessary to: (i) conform to legal requirements or comply with a legal process with which we are involved; (ii) protect our rights or property or the rights or property of our affiliated companies; (iii) prevent a crime or protect national security; or (iv) protect the personal safety of Site users or the public. Because Agency Spotter is a United States limited liability company and information collected on our Site is stored in whole or in part in the United States, your information may be subject to U.S. law."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A plasma blob is initialized with a seeded density profile given by $n(\\vec x, 0) = n_0 + \\triangle n \\exp\\left( -\\frac{\\vec x^2}{2\\ell^2} \\right)$. The blob's center of mass (COM) is defined as $X(t):= \\int\\mathrm{dA}\\, x(n-n_0)/M$. Assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$, estimate the acceleration of the plasma blob.",
    "choices": [
      "A) The acceleration of the plasma blob can be estimated by assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$, resulting in an acceleration of $A_0$.",
      "B) The acceleration of the plasma blob is directly proportional to the total radial particle flux, which is a characteristic of incompressible flows. Therefore, the acceleration of the plasma blob is also $A_0$.",
      "C) The acceleration of the plasma blob can be estimated by assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$, resulting in an acceleration of $A_0$ for both incompressible and compressible flows.",
      "D) The acceleration of the plasma blob can be estimated by considering the relationship between the kinetic energy and the center of mass velocity. Since the kinetic energy is bounded by $E(t) \\leq T_eS(t) + E(t) = T_e S(0)$, the acceleration of the plasma blob is proportional to the total radial particle flux, which is a characteristic of compressible flows. Therefore, the acceleration of the plasma blob is $A_0$."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Since both $S(t)\\geq 0$ and $E(t)\\geq 0$ we further derive from Eq.~\\eqref{eq:energya} and Eq.~\\eqref{eq:energyb} that the kinetic energy\nis bounded by $E(t) \\leq T_eS(t) + E(t) = T_e S(0)$; a feature absent from the gravitational system with \nincompressible flows, where $S(t) = S(0)$. \n\nWe now show that the invariants Eqs.~\\eqref{eq:energya} and \\eqref{eq:energyb} present restrictions on the velocity and\nacceleration of plasma blobs. First, we define the blobs' center of mass (COM) via $X(t):= \\int\\mathrm{dA}\\, x(n-n_0)/M$ and \nits COM velocity as $V(t):=\\d X(t)/\\d t$. The latter is proportional to the total radial particle flux~\\cite{Garcia_Bian_Fundamensky_POP_2006, Held2016a}. We assume\nthat $n>n_0$ and $(n-n_0)^2/2 \\leq [ n\\ln (n/n_0) - (n-n_0)]n $ to show for both systems \n\\begin{align}\n  (MV)^2 &= \\left( \\int \\mathrm{dA}\\, n{\\phi_y}/{B} \\right)^2\n  = \\left( \\int \\mathrm{dA}\\, (n-n_0){\\phi_y}/{B} \\right)^2\\nonumber\\\\\n \n&\\leq 2 \\left( \\int \\mathrm{dA}\\, \\left[n\\ln (n/n_0) -(n-n_0)\\right]^{1/2}\\sqrt{n}{\\phi_y}/{B}\\right)^2\\nonumber\\\\\n \n  &\\leq 4 S(0) E(t)/m_i \n \n  \\label{eq:inequality}\n\\end{align} Here we use the Cauchy-Schwartz inequality and \n$\\phi_y:=\\partial\\phi/\\partial y$. \nNote that although we derive the inequality Eq.~\\eqref{eq:inequality} only for amplitudes $\\triangle n >0$  we assume that the results also hold for depletions. This is justified by our numerical results later in this letter. If we initialize our density field with a seeded blob of radius $\\ell$ and amplitude $\\triangle n$ as \n\\begin{align}\n  n(\\vec x, 0) &= n_0 + \\triangle n \\exp\\left( -\\frac{\\vec x^2}{2\\ell^2} \\right), \\label{eq:inita}\n \n \n\\end{align}\nand  \n$\\phi(\\vec x, 0 ) = 0$,\nwe immediately have $M := M(0) = 2\\pi \\ell^2 \\triangle n$, $E(0) = G(0) = 0$ and \n$S(0) = 2\\pi \\ell^2 f(\\triangle n)$, where $f(\\triangle n)$ captures the amplitude dependence of \nthe integral for $S(0)$. \n\nThe acceleration for both incompressible and compressible flows can be estimated\nby assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$~\\cite{Held2016a} and using \n$E(t) = G(t)"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A mother's day gift for her daughter, who is a fan of romance novels, is a book by a well-known Australian romantic fiction writer. The writer's first name is Margaret, and she has written several books in the Mills and Boon Collection. However, the daughter is not a fan of the more explicit titles, such as \"Outback Desire\" or \"One Night Before Marriage\". Which of the following authors is most likely to write a romance novel that would be suitable for her?",
    "choices": [
      "A) Margaret Way, who wrote \"A Very Special Mother's Day\" and is known for her more sentimental titles, such as \"To Mum, with Love\".",
      "B) Robyn Donald, who has written several books in the Australian Billionaires series and is known for her more dramatic and intense titles, such as \"Island Heat\".",
      "C) Penny Jordan, who has written several books in the Mills and Boon Collection and is known for her more straightforward and uncomplicated titles, such as \"The Mills and Boon Collection\".",
      "D) Caroline Anderson, who has written several books in the Australian Heroes series and is known for her more complex and nuanced titles, such as \"Tall, Dark and Sexy\"."
    ],
    "correct_answer": "D)",
    "documentation": [
      "A Mother's Day Gift (2004) (with Anne Ashley and Lucy Monroe)\nWhite Wedding (2004) (with Judy Christenberry and Jessica Steele)\nA Christmas Engagement (2004) (with Sara Craven and Jessica Matthews) A Very Special Mother's Day (2005) (with Anne Herries)\nAll I Want for Christmas... (2005) (with Betty Neels and Jessica Steele) The Mills and Boon Collection (2006) (with Caroline Anderson and Penny Jordan) Outback Desire (2006) (with Emma Darcy and Carol Marinelli) To Mum, with Love (2006) (with Rebecca Winters)\nAustralian Heroes (2007) (with Marion Lennox and Fiona McArthur) Tall, Dark and Sexy (2008) (with Caroline Anderson and Helen Bianchin)\nThe Boss's Proposal (2008) (with Jessica Steele and Patricia Thayer)\nIsland Heat / Outback Man Seeks Wife / Prince's Forbidden Virgin / One Night Before Marriage / Their Lost-and-found Family / Single Dad's Marriage Wish (2008) (with Robyn Donald, Marion Lennox, Carol Marinelli, Sarah Mayberry and Anne Oliver) Australian Billionaires (2009) (with Jennie Adams and Amy Andrews)\nCattle Baron : Nanny Needed / Bachelor Dad on Her Doorstep (2009) (with Michelle Douglas)\n\nExternal links\nMargaret Way at Harlequin Enterprises Ltd\n\nAustralian romantic fiction writers\nAustralian women novelists\nLiving people\nYear of birth missing (living people)\nWomen romantic fiction writers"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Graham Stewart's comments on BC's recruiting strategy suggest that the program prioritizes developing players for the long-term, rather than rushing them into the NFL. However, this approach may not be effective for players who are already highly touted and have a strong desire to play at the highest level. What is a potential consequence of BC's recruiting strategy on players like Kevin Pierre-Louis, who was a highly touted recruit but played behind Luke Kuechly?",
    "choices": [
      "A) BC's focus on development will lead to a higher number of players being drafted into the NFL, but may result in some players feeling underutilized and leaving the program early.",
      "B) The program's emphasis on development will lead to a decrease in the number of players being drafted into the NFL, as players will be more likely to stay with the program for their entire college career.",
      "C) BC's recruiting strategy will lead to a higher number of players being drafted into the NFL, as the program's focus on development will prepare players for the physical demands of the league.",
      "D) The program's emphasis on development will lead to a more balanced approach to recruiting, with a mix of highly touted players and those who are more likely to develop into NFL-caliber players."
    ],
    "correct_answer": "D)",
    "documentation": [
      "But like Aboushi, his case was less about being seduced by a bigger, flashier program and more about not meeting BC's minimum standards for admissions at one point in time. Where these guys all over-hyped by the recruiting services? Where they wrong fits at their post-BC choices? I don't know. I do think a program like BC is probably more patient with players than some of the bigger programs. We won't rush a guy out of the program to free up a scholarship. We prefer to redshirt. And I think the nature of a program -- with good academic support and less of a big school mentality -- keeps kids from falling through the cracks. Every recruit thinks they are going to be a star, so selling them on development and a safety net doesn't sway many, but it should. If anything BC should use a guy like Marcus Grant -- a local kid who left a Big Ten program to come \"home\" as an example to Massachusetts recruits. Massachusetts kids keep leaving to play at the \"highest level.\" Our counter should be that we will develop them for life and the NFL (the real highest level) and not chew them up and spit them out like a football factory. I am sure that there will be a guy in the near future who decommits from BC and becomes a star. Or a guy we should have had who leads another team to glory. Right now I am just glad that we have very few regrets when it comes to old recruits. Our recruiting still has major challenges, but that's one area where things have broken our way. [Note to commentors: let me know if you think I missed any recruits who \"got away. \"]\nLabels: Graham Stewart, Joe Boisture, mike siravo, Recruiting, Spaz recruiting\nKey Players for 2012: Kevin Pierre-Louis\nJunior Linebacker, Kevin Pierre-Louis\nWhat he's been: BC's second-leading tackler. On most teams KPL would already be a star. But he played next to college football's tackling machine. There wasn't much room for headlines or an extra tackle with Luke Kuechly doing so much. Pierre-Louis also missed three games last year and played through pain in others."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a time series with a spectral density function f that is H\u00f6lder continuous with exponent \u03b1 = 2. The autocovariances \u03c3 k of the time series decay at a rate that is at least O(k^(-3)). What can be concluded about the convergence rate of a Toeplitz covariance estimator for this time series?",
    "choices": [
      "A) The convergence rate is O(k^(-2)), since the spectral density function f is H\u00f6lder continuous with exponent \u03b1 = 2.",
      "B) The convergence rate is O(k^(-3)), since the autocovariances \u03c3 k decay at a rate that is at least O(k^(-3)).",
      "C) The convergence rate is O(k^(-2)), since the spectral density function f is H\u00f6lder continuous with exponent \u03b1 = 2, and the autocovariances \u03c3 k are the Fourier coefficients of f.",
      "D) The convergence rate is O(k^(-4)), since the spectral density function f is H\u00f6lder continuous with exponent \u03b1 = 2, and the autocovariances \u03c3 k are the Fourier coefficients of f, which decay at a rate that is at least O(k^(-3))."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The sample size n may tend to infinity or to be a constant. The case n = 1 corresponds to a single observation of a stationary time series, and in this case the data are simply denoted by Y \u223cN p (0 p , \u03a3). The dimension p is assumed to grow. The spectral density function f , corresponding to a Toeplitz covariance matrix \u03a3, is given by so that for f \u2208 L 2 (\u2212\u03c0, \u03c0) the inverse Fourier transform implies Hence, \u03a3 is completely characterized by f , and the non-negativity of the spectral density function implies the positive definiteness of the covariance matrix. Moreover, the decay of the autocovariance \u03c3 k is directly connected to the smoothness of f . Finally, the convergence rate of a Toeplitz covariance estimator and that of the corresponding spectral density estimator are directly related via \u03a3 \u2264 f \u221e := sup x\u2208 |f (x)|, where \u2022 denotes the spectral norm (see . As in , we introduce a class of positive definite Toeplitz covariance matrices with H\u00f6lder continuous spectral densities. For \u03b2 = \u03b3 + \u03b1 > 0, where The optimal convergence rate for estimating Toeplitz covariance matrices over P \u03b2 (M 0 , M 1 ) depends crucially on \u03b2. It is well known that the k-th Fourier coefficient of a function whose \u03b3-th derivative is \u03b1-H\u00f6lder continuous decays at least with order O(k \u2212\u03b2 ) (see . Hence, \u03b2 determines the decay rate of the autocovariances \u03c3 k , which are the Fourier coefficients of the spectral density f , as k \u2192 \u221e. In particular, this implies that for \u03b2 \u2208 (0, 1], the class P \u03b2 (M 0 , M 1 ) includes Toeplitz covariance matrices corresponding to long-memory processes with bounded spectral densities, since the sequence of corresponding autocovariances is not summable. A connection between Toeplitz covariance matrices and their spectral densities is further exploited in the following lemma. Lemma 1. Let \u03a3 \u2208 P \u03b2 (M 0 , M 1 ) and let x j = (j \u2212 1)/(p \u2212 1), j = 1, ..., p, then where \u03b4 i,j is the Kroneker delta, O(\u2022) terms are uniform over i, j = 1, . . . , p and divided by \u221a 2 when i, j \u2208 {1, p} is the Discrete Cosine Transform I (DCT-I) matrix."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the impact of the \"War on Terror\" on the American constitutional order suggests that the executive branch's assertion of executive privilege has led to an increase in the power of the presidency. However, this increase in power has also led to a decrease in the effectiveness of Congress in passing legislation. On the other hand, some argue that the Supreme Court's role in interpreting the Constitution has become more prominent, leading to a shift in the balance of power between the branches. Which of the following statements best summarizes the relationship between the War on Terror, executive privilege, and the Supreme Court's role in the American constitutional order?",
    "choices": [
      "A) The War on Terror has led to a decrease in the power of the presidency, allowing Congress to pass more legislation.",
      "B) The War on Terror has led to an increase in the power of the presidency, which has resulted in a decrease in the effectiveness of Congress.",
      "C) The War on Terror has led to an increase in the power of the Supreme Court, allowing it to interpret the Constitution in a more prominent role.",
      "D) The War on Terror has led to a shift in the balance of power between the branches, with the executive branch asserting executive privilege and the Supreme Court playing a more prominent role in interpreting the Constitution."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Four take home writing assignments. Analytic essays, each 1000-1500 words. (Grades weighted: 10%, 25%, 25%, and 25%) Late essays will not be accepted, except with a doctor\u2019s excuse or a Dean\u2019s excuse for family emergency. Regular preparation and class participation: 15%. OR as an option: By prior arrangement with me by the due date of the second analytic essay, students may substitute one longer research paper (15 \u2013 20 pages) for two of the last three analytic papers This paper will be on a topic of the students choosing , if I approve, and the due date will be the same as the last assigned analytic essay. This project would count 50% of the students course grade. Selected writings by Frederick Douglass, W.E.B. Dubois, Ralph Ellison, James Baldwin\nSolzhenitsyn, \u201cA World Split Apart\u201d\nTocqueville, Democracy in America GOV 382M \u2022 Tocqueville 39150 \u2022 Spring 2011 Meets T 6:30PM-9:30PM BAT 5.102 show description\nSee syllabus GOV 370L \u2022 President, Congress, And Court 38695 \u2022 Fall 2010 Meets TTH 8:00AM-9:30AM UTC 3.112 show description\nCourse Description: A Study of the political relationship of the President, Congress and Court in the American constitutional order. Has this relationship changed over the course of American history? Is American national politics prone to stalemate or deadlock between the branches regarding major issues of public policy? Do we have a new \u201cimperial presidency?\u201d Should the Court arbitrate disputes between the President and Congress over custody of their respective powers? Has Congress abdicated its constitutional responsibilities? We will examine questions like these in light of practical problems such as executive privilege and secrecy, the war on terror, budget politics and controversies regarding appointments to the Supreme Court. Grading: Three in class essay tests, for which study questions will be distributed in advance. The exam questions will be chosen from the list of study questions. (25% each) One short take home essay (10% each). Class participation and attendance (15%)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A driver is navigating a multi-lane highway with a speed limit of 70 km/h. The ego vehicle is currently in lane 1, which has a fixed lateral displacement of 3.5m from the centerline. The driver's objective is to minimize travel time while maintaining a safe headway of 50m. However, a slow-moving vehicle is ahead, and the driver needs to decide whether to change lanes to the fast-moving lane (lane 2) or adjust its speed without changing lanes. Which of the following is the most appropriate decision for the driver?",
    "choices": [
      "A) Change lanes to lane 2 to minimize travel time, as the fast-moving lane has a higher speed limit and will allow the driver to maintain a safe headway.",
      "B) Adjust the speed without changing lanes, as the slow-moving vehicle is ahead and changing lanes would not provide any benefits in terms of travel time or safety.",
      "C) Change lanes to lane 2, as the driver can maintain a safe headway by adjusting its speed without changing lanes, and the fast-moving lane will allow the driver to take advantage of the higher speed limit.",
      "D) Adjust the speed without changing lanes, as the driver can maintain a safe headway by adjusting its speed, and changing lanes would not provide any benefits in terms of travel time or safety, especially since the slow-moving vehicle is ahead."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Paper Info\n\nTitle: SLAS: Speed and Lane Advisory System for Highway Navigation\nPublish Date: Unkown\nAuthor List: Faizan Tariq, David Isele, John Baras, Sangjae Bae\n\nFigure\n\nFig. 1.Motivational Example. With a slow moving vehicle ahead, the ego vehicle (in blue) may decide to either change lane to the fast moving lane (left) to minimize travel time or adjust its speed without changing lanes to preserve safety but it would be unwise for it to switch to the slow moving lane (right) as that would not benefit travel time or safety.\nFig. 3. Simulation Setup.Scenario Runner sets up the scenario for the CARLA Simulator, which then communicates with the SLAS and the Planning and Control ROS (Robot Operating System) nodes through the ROS bridge node. Fig. 4. Testing scenario with three lanes: lane 0 (left), lane 1 (center) and lane 2 (right).The expected motion of the ego vehicle, over the course of the simulation, is shown with numbered frames. The right most lane (lane 3) is reserved for merging traffic so it is not utilized in our simulation. Fig. 5. Left: Travel time comparison. Center: Lane choice (lateral position) comparison. The center lines of lanes 0 (left), 1 (center) and 2 (right) have fixed lateral displacements of 0m, 3.5m and 7m respectively. Right: Headway comparison. With no leading vehicle, the headway is restricted by the visibility range of 50m.\n\nabstract\n\nThis paper proposes a hierarchical autonomous vehicle navigation architecture, composed of a high-level speed and lane advisory system (SLAS) coupled with low-level trajectory generation and trajectory following modules. Specifically, we target a multi-lane highway driving scenario where an autonomous ego vehicle navigates in traffic. We propose a novel receding horizon mixed-integer optimization based method for SLAS with the objective to minimize travel time while accounting for passenger comfort. We further incorporate various modifications in the proposed approach to improve the overall computational efficiency and achieve real-time performance."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A theoretical model predicts that the energy splitting between the singlet and triplet states in a Double Quantum Dot (DQD) system is influenced by the proximity of the Superconductor (SC). Which of the following statements accurately describes the effect of the Coulomb interactions between the DQD and the SC on the singlet-triplet energy splitting?",
    "choices": [
      "D) system is influenced by the proximity of the Superconductor (SC). Which of the following statements accurately describes the effect of the Coulomb interactions between the DQD and the SC on the singlet-triplet energy splitting?",
      "A) The Coulomb interactions between the DQD and the SC increase the energy of the intermediate states contributing to the direct exchange, leading to a decrease in the singlet-triplet energy splitting.",
      "B) The Coulomb interactions between the DQD and the SC increase the energy of the intermediate states causing the Cooper pair absorption and re-emission process, leading to an increase in the singlet-triplet energy splitting.",
      "C) The Coulomb interactions between the DQD and the SC decrease the energy of the intermediate states contributing to the direct exchange, leading to a decrease in the singlet-triplet energy splitting."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The proximity of SC gives rise to two further exchange mechanisms that\ndetermine the system's behavior. First of all, the (conventional)\n\\emph{RKKY interaction} appears, $J \\sim \\GS{}^2$ \\cite{RK,K,Y}. Moreover, the \\emph{CAR exchange} emerges as a consequence of finite $\\GS{}$ \\cite{Yao}. It can be understood on the basis \nof perturbation theory as follows. DQD in the inter-dot singlet state may absorb\nand re-emit a Cooper pair approaching from SC; see \\fig{system}(e)-(g). As a second-order\nprocess, it reduces the energy of the singlet, which is the ground state of isolated DQD. A similar process is not possible in the triplet state due to spin conservation. Therefore, the singlet-triplet energy splitting $J^{\\mathrm{eff}}$ is increased (or generated for $t=J=0$). More precisely, the leading ($2$nd-order in $t$ and $\\GS{}$) terms\nin the total exchange are \n\\begin{equation}\nJ^{\\mathrm{eff}} \t\\approx \tJ + \\frac{4t^2}{U-U'+\\frac{3}{4}J} + \\frac{4\\GS{}^2}{U+U'+\\frac{3}{4}J}. \\label{Jeff}\n\\end{equation}\nUsing this estimation, one can predict $T^*$ for finite $\\GS{}$, $t$ and $J$ with \\eq{Tstar}. Apparently, from three contributions corresponding to:\n(i) RKKY interaction, (ii) direct exchange and (iii) CAR exchange, only the first may bear a negative (ferromagnetic) sign. The two other contributions always have an anti-ferromagnetic nature. More accurate expression for $J^{\\mathrm{eff}}$ is derived in Appendix~\\ref{sec:downfolding} [see \\eq{A_J}] by the Hamiltonian down-folding procedure. The relevant terms differ \nby factors important only for large $\\GS{}/U$. Finally, it seems worth stressing that normal leads are not necessary for CAR exchange to occur. At least one of them is inevitable for the Kondo screening though, and two symmetrically coupled \nnormal leads allow for measurement of the normal conductance. It is also noteworthy that inter-dot Coulomb interactions\ndecrease the energy of intermediate states contributing to direct exchange [\\fig{system}(c)], while increasing the energy of intermediate\nstates causing the CAR exchange [\\fig{system}(f)]."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on the behavior of deep denoisers in the context of the sign problem in lattice gauge theory found that the sampling overhead \u03b3 is not always uniformly distributed across different time steps t. In fact, the study suggests that the optimization procedure may only find a local minimum in certain cases, leading to large fluctuations in \u03b3. However, the study also notes that the distribution of \u03b3 is not directly related to the \u03b1 distributions, which are shown in Fig. . Instead, the study focuses on the channel count N G versus \u03b1 histograms, which are stacked and show a widening distribution upon increasing M trot . Which of the following statements best summarizes the relationship between the sampling overhead \u03b3 and the channel count N G versus \u03b1 histograms?",
    "choices": [
      "A) The sampling overhead \u03b3 is directly proportional to the channel count N G versus \u03b1 histograms, and the widening distribution upon increasing M trot indicates a decrease in \u03b3.",
      "B) The sampling overhead \u03b3 is inversely proportional to the channel count N G versus \u03b1 histograms, and the widening distribution upon increasing M trot indicates an increase in \u03b3.",
      "C) The sampling overhead \u03b3 is not related to the channel count N G versus \u03b1 histograms, and the widening distribution upon increasing M trot is due to the difficulty in finding optimal deep denoisers.",
      "D) The sampling overhead \u03b3 is related to the channel count N G versus \u03b1 histograms, and the widening distribution upon increasing M trot indicates a decrease in \u03b3 due to the optimization procedure finding a local minimum in certain cases."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The results are shown in Fig. , which contains histograms for the channel count N G versus \u03b1. The histograms are stacked, with the lightest color corresponding to the angles of the denoiser at t = 0.5 and the darkest at t = 5. The top four panels are for a denoiser with M = 2 and the bottom four with M = 8. We consider M trot = 8, 16, 32, 64. We see that in both cases the distribution widens upon increasing M trot , indicating that the unitary channels start deviating more from the identity. Moreover, while the M = 2 denoisers in all cases except M trot = 64 have ZZ contributions close to the identity, this is clearly not the case for M = 8. For simplicity, we did not focus on obtaining denoisers with the smallest sampling overhead \u03b3, which is required to minimize the sign problem and hence ease the sampling of mitigated quantities. Instead, we let the optimization freely choose the \u03b7 i in the denoiser parameterization, as defined in the main text. In Fig. we show the sampling overhead of the denoisers from Fig. of the main text. We see that for M = 1 and M = 2 the sampling overhead is relatively small and uniform across the different t, whereas for M > 2 the optimization sometimes yields a denoiser with large \u03b3 and other times with small \u03b3. This could be related to the difference in \u03b1 distributions from Fig. . The large fluctuations of \u03b3 appears to stem from the difficulty in finding optimal deep denoisers, and our optimization procedure likely only finds a local minimum in these cases. Here C(t) is the Trotter supercircuit for time t. In Fig. we show Z dw for the circuits from Fig."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the relationship between the speed of a moving object and the accuracy of human perception. They find that when an object is moving at a speed of 10 meters per second, humans are able to accurately perceive its distance from them. However, when the object is moving at a speed of 20 meters per second, humans are less accurate in their perception. What is the most likely explanation for this phenomenon?",
    "choices": [
      "A) The researcher's equipment is not calibrated correctly, causing the accuracy of human perception to vary with speed.",
      "B) The human brain is able to process visual information more quickly when an object is moving at a speed of 10 meters per second, but becomes overwhelmed by the increased speed of 20 meters per second.",
      "C) The accuracy of human perception is directly proportional to the speed of the moving object, meaning that as the speed increases, so does the accuracy of perception.",
      "D) The relationship between speed and accuracy of human perception is due to the way in which the brain processes visual information, taking into account the object's motion parallax and the time it takes for the object to move into the field of view."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This question veritably answers itself. Only a madman would deny the evidence of his own senses. It is essential to understand that the correspondence of which I speak depends on the reality of motion [from which we derive the ideas of time and space]. To keep ourselves safe, it is necessary that we have the ability to know when a material object is moving closer or further from us and to be able to recognize an object as a danger. This, the senses give us, for perceptions like all other experiences are memories [are preserved over time]. An object is recognized as a danger through prior sensory experiences preserved as long-term memories. In order to be recognized and remembered as a danger, a material object must have the power to produce a particular human experience of it. That power is part of the nature of the object and is thus truly reflected in the perception of it\u2014even though there may be more to the object than its power to yield a human perception. To the reasonable mind, the above comments may properly be seen as statements of the obvious. The curious fact, however, is that a whole school of western philosophy has labored mightily to deny the obvious. I agree; I'm only delving into the inner experience to see how it works and what may become of that.\nby TheVat on April 22nd, 2018, 11:57 am\nRJG, this tablet ate the quoted part of your post and somehow hid the submit button, so sorry about the missing comment.... No, I was not assuming the in-the-moment knowledge, but rather that facts about buses are physically verifiable when science is applied. It is not difficult to verify that I was neither dreaming nor hallucinating. We are saved from solipsism by the multiplicity of observers and their reports. We can open a book and read complex prose (something that can't be done in dreams) that reveals areas of knowledge utterly unknown to us and beyond our previous experiences. We have senses enhanced by instruments that can show us photons leaving the photosphere of the sun and bouncing off solid objects like buses in particular and regular patterns, etc."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation), what is the primary benefit of reducing the H-representation of a polytope to include only non-redundant constraints?",
    "choices": [
      "A) It allows for more efficient computation of the polytope's H-representation, but may lead to loss of information about the polytope's edges.",
      "B) It enables the use of more advanced algorithms for path planning, but may result in a loss of precision in the polytope's H-representation.",
      "C) It reduces the computational complexity of the polytope's H-representation, but may not capture the polytope's edges accurately.",
      "D) It enables the construction of a more accurate and efficient intent inference model, by focusing on the non-redundant constraints that define the polytope's edges and preferred neighboring polytopes."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Hyperplane arrangements have been used by Vincent and Schwager in the context of Neural Network verification. In our setting, we leverage this representation to define path preferences as preferred transitions between adjacent regions of the space. Hyperplane Arrangement\n\nWe assume a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation) where A i \u2208 R di\u00d72 and b i \u2208 R di , and where d i is the number of edges (hyperplanes) composing polytope i. Let n = i d i be the total number of hyperplanes. We leverage each obstacle's H-representation to construct a hyperplane arrangement of the environment as shown in fig.\n.e. a partitioning of the space into polytopes. More specifically, each location in space belongs to a polytope j for which we can write an H-representation of the form where \u03b1 j i \u2208 {\u22121, 1} di is a vector specific to polytope j and obstacle i corresponding to the relative position of any point in the set with respect to each hyperplane in O i . Fig. : Intent inference model in a hyperplane arrangement of the obstacle free space. We spatially decompose the preference \u03b8 into a set of preferred neighboring polytopes per region of the space. Within each polytope j, the human preference pj is a discrete distribution over the preferred neighbor in N (j). We assume that for a location st belonging to polytope j, and given goal g and preference pj, the observation ot and any other preference p i,i =j are conditionally independent. Concatenating elements from each obstacle's Hrepresentation, we can write polytope j's H-representation as where Some of the constraints in eq. ( ) (corresponding to rows of A, b and \u03b1 j ) are redundant, i.e. the set P j does not change upon their removal. We can further reduce the Hrepresentation of a polytope to include only non-redundant constraints. By removing the rows corresponding to redundant constraints, we obtain new matrices A j e , b j e and \u03b1 j e such that we can write the polytope's reduced H-representation as The non-redundant constraints correspond to edges of the polytope."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on the performance of various solvers for solving problems in anisotropic media has yielded intriguing results. The study found that for problems with a high degree of anisotropy, the solvers GMRES and DSA tend to converge more slowly than their counterparts, NFPA and FPSA. However, the study also revealed that NFPA and FPSA exhibit a non-linear relationship between runtime and iteration count, with NFPA consistently outperforming FPSA in terms of runtime. Furthermore, the study showed that the performance of the solvers is highly dependent on the value of the parameter g, with GMRES and DSA performing poorly for high values of g.",
    "choices": [
      "A) NFPA and FPSA exhibit a non-linear relationship between runtime and iteration count, with NFPA consistently outperforming FPSA in terms of runtime, but GMRES and DSA perform poorly for high values of g.",
      "B) NFPA and FPSA exhibit a non-linear relationship between runtime and iteration count, with NFPA consistently outperforming FPSA in terms of runtime, but GMRES and DSA perform poorly for low values of g.",
      "C) NFPA and FPSA exhibit a non-linear relationship between runtime and iteration count, with NFPA consistently outperforming FPSA in terms of runtime, but GMRES and DSA perform poorly for high values of g, and NFPA outperforms FPSA in iteration count for problems with a low degree of anisotropy.",
      "D) NFPA and FPSA exhibit a non-linear relationship between runtime and iteration count, with NFPA consistently outperforming FPSA in terms of runtime, GMRES and DSA perform poorly for high values of g, and NFPA outperforms FPSA in iteration count for problems with a high degree of anisotropy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The three kernels tested are shown in \\cref{HGK}. GMRES, DSA, FPSA, and NFPA all converged to the same solution for problems 1 and 2.\n\\Cref{HGK_plots} shows the solutions for HGK with $g = 0.99$.\nThe results of each solver are shown in \\cref{HGKresults1,HGKresults2}. \\begin{table}[h]\n\\begin{center}\n\\scalebox{0.8}{\n\\begin{tabular}{c || c || c || c} \\hline \nParameter & Solver & Runtime (s) & Iterations \\\\ \\hline \\hline\n\\multirow{4}{*}{$g=0.9$} & GMRES & 9.88 & 76 \\\\\n& DSA & 24.5 & 554 \\\\\n& FPSA & 1.50 & 32 \\\\ \n& NFPA & 1.39 & 27 \\\\ \\hline \n\\multirow{4}{*}{$g=0.95$} & GMRES & 12.2 & 131 \\\\\n& DSA & 47.7 & 1083 \\\\\n& FPSA & 1.75 & 38 \\\\ \n& NFPA & 1.83 & 35 \\\\ \\hline \n\\multirow{4}{*}{$g=0.99$} & GMRES & 40.0 & 27 \\\\\n& DSA & 243 & 5530  \\\\\n& FPSA & 3.38 & 74 \\\\ \n& NFPA & 3.93 & 73 \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{Runtime and Iteration Counts for Problem 1 with HGK}\n\\label{HGKresults1} \n\\end{table}\n\\begin{table}[h]\n\\begin{center}\n\\scalebox{0.8}{\n\\begin{tabular}{c || c || c || c} \\hline \nParameter & Solver & Runtime (s) & Iterations \\\\ \\hline \\hline\n\\multirow{4}{*}{$g=0.9$} & GMRES & 24.3 & 135 \\\\\n& DSA & 14.8 & 336  \\\\\n& FPSA & 1.15 & 23 \\\\ \n& NFPA & 1.35 & 24 \\\\ \\hline \n\\multirow{4}{*}{$g=0.95$} & GMRES & 31.3 & 107 \\\\\n& DSA & 29.7 & 675 \\\\\n& FPSA & 1.56 & 32 \\\\ \n& NFPA & 1.90 & 33 \\\\ \\hline \n\\multirow{4}{*}{$g=0.99$} & GMRES & 41.4 & 126 \\\\\n& DSA & 146 & 3345 \\\\\n& FPSA & 3.31 & 67 \\\\ \n& NFPA & 3.99 & 67 \\\\ \\hline  \n\\end{tabular}}\n\\end{center}\n\\caption{Runtime and Iteration Counts for Problem 2 with HGK}\n\\label{HGKresults2} \n\\end{table} Here we see that NFPA and FPSA do not perform as well compared to their results for the SRK and EK. Contrary to what happened in those cases, both solvers require more time and iterations as the problem becomes more anisotropic. This is somewhat expected, due to HGK not having a valid Fokker-Planck limit. However, both NFPA and FPSA continue to greatly outperform GMRES and DSA. Moreover, NFPA outperforms FPSA in iteration count for problem 1."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new study on hand gesture recognition using Feature Weighted Naive Bayes (FWNB) classification achieved an accuracy of 92% in the RCC dataset, outperforming the baseline methods by 5%. However, the same study also reported that the accuracy of postural activity recognition using a 6-class diverse postural activity recognition framework was 91%, which is 8% higher than the baseline approach. What is the most likely reason for the significant improvement in postural activity recognition accuracy in the new study?",
    "choices": null,
    "correct_answer": "D)",
    "documentation": [
      "Then, the start/end duration error is 9 minutes ($|$5 minutes delayed start$|$ + $|$4 minutes hastened end$|$), in an overall error of e.g., 30\\% (9/30=0.3). We measure cross-participant accuracy using leave-two-participants-out method for performance metrics, i.e., we take out two of the participants' data points from the entire dataset, train our proposed classification models, test the model accuracy on the two left-out participants relevant data points, and continue the process for entire dataset. \\begin{figure*}[!htb]\n\\begin{minipage}{0.45\\textwidth}\n\\begin{center}\n   \\epsfig{file=hand_gesture_accuracy.pdf,height=1.6in, width=3in}\n\\caption{Feature Weighted Naive Bayes (FWNB) classification accuracy comparisons with baseline approaches (graphical signatures of all hand gestures are shown).}\n   \\label{fig:hand_gesture_accuracy}\n\\end{center}\n\\end{minipage}\n\\begin{minipage}{0.29\\textwidth}\n\\begin{center}\n\\vspace{-.12in}\n   \\epsfig{file=posture_accuracy_normal.pdf,height=1.6in, width=2.1in}\n\\caption{4-class postural level activity recognition performance and comparisons with baseline method}\n   \\label{fig:posture_accuracy_normal}\n\\end{center}\n\\end{minipage}\n\\begin{minipage}{0.25\\textwidth}\n \\begin{center}\n \\vspace{-.12in}\n   \\epsfig{file=posture_accuracy_extended.pdf,height=1.6in, width=2.1in}\n\\caption{6-class diverse postural activity recognition framework accuracy comparisons with the baseline approach.}\n   \\label{fig:posture_accuracy_extended}\n\\end{center}\n \\end{minipage}\n\\end{figure*}\n\nFig~\\ref{fig:hand_gesture_accuracy} displays Feature Weighted Naive Bayes (FWNB) based the 8-hand gestural activity recognition accuracies comparisons with the baseline methods which clearly depicts the outperformance of our method (5\\% improvement) with an overall accuracy of 92\\% (FP rate 6.7\\%) in RCC dataset. For postural activity recognition, dataset achieving 91\\% postural activity recognition accuracy (FP rate 9.5\\%) which outperforms the baseline approach significantly (8\\% improvement)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A libertarian philosopher argues that the concept of free will is not incompatible with indeterminism, as it is possible that our choices are influenced by factors beyond our control, but we can still be held morally responsible for our actions. However, critics of libertarianism point out that this view is flawed because it fails to account for the fact that our actions are ultimately caused by prior causes beyond our own control. Which of the following statements best captures the main motivation behind the libertarian notion of free will?",
    "choices": [
      "A) The desire to avoid determinism and ensure that our choices are truly our own.",
      "B) The need to justify the existence of moral responsibility in a deterministic universe.",
      "C) The fear that if our choices are truly indeterminate, then we cannot be held accountable for our actions.",
      "D) The recognition that free will is an illusion, and that our choices are ultimately determined by factors beyond our control."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Libertarians do not necessarily accept that this argument shows that we do not have free will, and the reason, or at least a big part of it, should not surprise anyone at this point: they simply define free will differently. According to libertarians, such as Robert Nozick and Robert Kane, one has free will if one could have acted otherwise than one did, and if indeterminism is true, then it may be true that we could have \u201cacted\u201d differently than we did under the exact same circumstances, and that we thereby might have free will in this sense. It should be pointed out, though, that critics of libertarianism are\u201crightly skeptical about the relevance of this kind of free will. First of all, the free will that libertarians endorse is, unlike what many libertarians seem to think, not an ethically relevant kind of freedom, and it does not have anything to do with the freedom of action that we by definition want. Second, the hard incompatibilist is right that no matter what is true about the degree to which the universe is deterministic, our actions are still caused by prior causes ultimately beyond our own control, which few of those who identify themselves as libertarians seem to want to acknowledge. And lastly, the fact that our actions are caused by causes ultimately beyond our own control does, if we truly appreciated, undermine our intuition of retributive justice, an intuition that libertarians generally seem to want to defend intellectually. So, as many have pointed out already, libertarians are simply on a failed mission. Together with the want to defend retributive blame and punishment, what seems to be the main motivation for people who defend a libertarian notion of free will seems to be a fear of predeterminism, a fear of there being just one possible outcome from the present state of the universe, which would imply that we ultimately cannot do anything to cause a different outcome than the one possible. Libertarians and others with the same fear have artfully tried to make various models to help them overcome this fear, for instance so-called two-stage models that propose that our choices consist of an indeterministic stage of generation of possible actions, and then our non-random choice of one of them."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Which of the following statements about the performance of the SLAS approach in the Monte Carlo simulations is most likely to be true, given the results presented in the table?",
    "choices": [
      "A) SLAS consistently outperforms EA in terms of completion time, but is outperformed by MOBIL in terms of throttle jerk.",
      "B) The strong performance of SLAS in terms of headway and distance can be attributed to its ability to handle complex scenarios with multiple lanes and traffic participants.",
      "C) The results in the table suggest that SLAS is the most efficient approach in terms of angular acceleration, but its performance in terms of brake jerk is comparable to that of MOBIL.",
      "D) The fact that SLAS maintains a greater headway throughout the simulation and achieves the maximum headway prior to EA is a strong indication that it is the most effective approach in terms of passenger comfort."
    ],
    "correct_answer": "D)",
    "documentation": [
      "3) Headway: The right plot in Fig. shows the headway maintained by the ego vehicle over the course of the simulation. In accordance with our prior discussion, MOBIL cruises behind the front vehicle, maintaining a relatively low headway until a sufficient space in the adjacent lane is found to perform the lane-change maneuver. On the other hand, EA and SLAS show a comparable headway trajectory, however, SLAS maintains a greater headway throughout and achieves the maximum headway prior to EA . Quantitatively, SLAS maintains on average 9.43%, 36.57% and 113.17% more headway than the EA , MOBIL and No-change approaches respectively. This strong performance by SLAS can be attributed to its incorporation of safety guarantees coupled with its consideration for passenger comfort. 4) Distance to closest vehicle: Finally, we compare the distance that ego vehicle maintains from the closest vehicle throughout the simulation. On average, SLAS maintains 9.28%, 32.01%, and 22.84% more distance in comparison to EA , MOBIL and No-change approaches respectively. These numbers are a testament to the strength of our approach resulting from consideration of long planning horizon coupled with speed control. Monte Carlo Simulations\n\nTo demonstrate the long-term performance of the three approaches (SLAS, EA and MOBIL), we run a series of Monte Carlo simulations on scenarios with randomized initial positions (within a range of 8m) and velocities (within    ranges of 8, 5 and 2 m/s assigned to each of the three lanes randomly) of traffic participants. The result from 50 simulations is presented in Table . In this table, the columns represent the different evaluation metrics, the rows identify the three algorithms, and the values highlighted in green represent the best result with respect to each evaluation metric. The evaluation metrics, going from left to right in the table, are completion time (s), brake (R [\u22121,0] ), brake jerk (R [\u22121,0] ), throttle (R [0,1] ), throttle jerk (R [0,1] ), angular acceleration ( \u2022 /s 2 ) and angular jerk ( \u2022 /s 3 )."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A politician who was previously known for his conservative views on social issues has recently stated that he would have voted in favor of same-sex marriage if he had been in a different position. However, he has also expressed concerns about the potential impact of same-sex marriage on traditional family structures. Which of the following best describes his current stance on the issue?",
    "choices": [
      "A) He has always been opposed to same-sex marriage and believes it would be a threat to traditional family structures.",
      "B) He has changed his stance on same-sex marriage and now believes it would be a positive development for society.",
      "C) He has not made a public statement on the issue, but his past voting record suggests that he would likely oppose same-sex marriage.",
      "D) He believes that same-sex marriage would be a threat to traditional family structures, but also acknowledges the importance of individual freedom and choice in matters of marriage."
    ],
    "correct_answer": "D)",
    "documentation": [
      "At the time of his re-election, English announced his intention to stay on as leader until the next general election. On 13 February 2018, however, he stood down as National Party leader due to personal reasons, and instructed the party to put into motion the processes to elect a new leader. He also retired from Parliament. English's resignation followed weeks of speculation that he would step aside for a new leader. On 27 February, he was succeeded as party leader by Simon Bridges as the result of the leadership election held that day. Post-premiership \nIn 2018, English joined the board of Australian conglomerate, Wesfarmers. English serves in Chairmanships of Mount Cook Alpine Salmon, Impact Lab Ltd and Manawanui Support Ltd. He is also a director of The Instillery, Centre for Independent Studies and The Todd Corporation Limited, and is a member of the Impact Advisory Group of Macquarie Infrastructure and Real Assets. Political and social views\n\nEnglish is regarded as more socially conservative than his predecessor, John Key. He has stated his opposition to voluntary euthanasia and physician-assisted suicide, same-sex civil unions, and the decriminalisation of prostitution. As Prime Minister he opposed any \"liberalisation\" of abortion law. In 2004, English voted against a bill to establish civil unions for both same-sex and opposite-sex couples. In 2005, he voted for the Marriage (Gender Clarification) Amendment Bill, which would have amended the Marriage Act to define marriage as only between a man and a woman. English voted against the Marriage (Definition of Marriage) Amendment Bill, a bill that legalised same-sex marriage in New Zealand. However, in December 2016 he stated, \"I'd probably vote differently now on the gay marriage issue. I don't think that gay marriage is a threat to anyone else's marriage\". In 2009, English voted against the Misuse of Drugs (Medicinal Cannabis) Amendment Bill, a bill aimed at amending the Misuse of Drugs Act so that cannabis could be used for medical purposes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new robot learning to navigate a complex environment with multiple goals requires a model that accounts for both the robot's policy and the human's input. In a study comparing different approaches, the results show that the \"Path Preference + Blend\" method outperforms the \"Goal-Only\" baseline in terms of computation time. However, the study also notes that the \"Path Preference + Blend\" method can be computationally expensive, especially in larger environments. Which of the following statements best describes the relationship between the \"Path Preference + Blend\" method and the computation time in larger environments?",
    "choices": [
      "A) The \"Path Preference + Blend\" method is significantly faster than the \"Goal-Only\" baseline in larger environments.",
      "B) The \"Path Preference + Blend\" method is only slightly slower than the \"Goal-Only\" baseline in larger environments.",
      "C) The \"Path Preference + Blend\" method is only faster than the \"Goal-Only\" baseline in smaller environments, but the difference in computation time is negligible in larger environments.",
      "D) The \"Path Preference + Blend\" method is significantly slower than the \"Goal-Only\" baseline in larger environments, due to the increased computational cost of blending the human's input with the robot's policy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We find that in these runs, accounting for path preference consistently improves performance compared with the goal-only baseline. Results also show that blending the user's input with the robot's policy (Path Preference + Blend) when the human provides information leads to improved performance. Belief entropy. Figure shows a challenging problem instance where the directions the human provides do not align directly with the shortest path to the goal. By ignoring the effects of preferences in the problem model (goal only), the robot quickly infers from observations that the upper left goal is less likely than others (P (g) drops). The strong decrease in entropy shows that the robot becomes overconfident in this prediction. Overconfidence in an incorrect goal will prevent the agent from finding the correct goal once the human's indications directly align with it, as it needs to correct for the wrong predictions, as shown in the path realization (fig.\n). In this realization, the goal-only method (green robot) fails to search the upper left area within the allotted time. By accounting for path preferences in its model, the blue robot's entropy over the goal distribution decreases more steadily, allowing for it to leverage the human's latest observations and reach the goal successfully. shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Computation time. In table II we provide the time required to solve the POMDP, and the time required to update the robot's belief as it receives new observations. We compute solutions on three maps: a simple 10 \u00d7 10 grid world with 8 polytopes (fig. ), a 10 \u00d7 10 grid world with 56 polytopes (fig. ), and a 20\u00d720 grid world with 73 polytopes (fig. ). The latter environment being larger, we increase the mission time and the depth of the search tree in POMCP from T max = 30 (Map 1 and Map 2) to T max = 60 (Map 3)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the distance to XMMSL1~J060636.2-694933 based on the estimated peak magnitude of the lightcurve?",
    "choices": [
      "A) The distance to XMMSL1~J060636.2-694933 is likely to be within the Milky Way Galaxy, given the estimated peak magnitude of 9.5.",
      "B) The distance to XMMSL1~J060636.2-694933 is likely to be greater than 100 kpc, based on the estimated peak magnitude of 12.0.",
      "C) The distance to XMMSL1~J060636.2-694933 is likely to be between 29 and 115 kpc, given the estimated peak magnitude of 10.5.",
      "D) The distance to XMMSL1~J060636.2-694933 is likely to be within the LMC itself, given the estimated peak magnitude of 12.0 and the presence of sizable X-ray hydrogen column densities."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We have no\ninformation over the 12 days between the data point of maximum\nbrightness and the lower limit prior to this (Fig.\\,\\ref{optlc}), and\ntherefore we have no exact outburst date, nor exact apparent\nmagnitude at outburst. Assuming for the moment though that we have\ncaught the outburst exactly in the Sep.~30, 2005 observation, then we\ncan estimate (Sect.~5.3) $t_{2}$ to be 8$\\pm$2\\,days, and using this,\nwe can estimate (Della Valle \\& Livio 1995) the absolute magnitude at\nmaximum brightness $M_{V}$ to be --8.7$\\pm$0.6. An absolute magnitude\nof $M_{V}$=--8.7 implies a peak luminosity $\\sim$7 times the Eddington\nluminosity for a 1\\,$M_{\\odot}$ white dwarf. This is quite typical of\nnovae. With $A_{V}$=0.39$^{+0.05}_{-0.09}$ (90\\% error), as derived (Predehl\n\\& Schmitt 1995) from $N_{\\rm\n  H}$=6.9$^{+1.0}_{-1.6}\\times10^{20}$\\,cm$^{-2}$ (from the highest\nstatistic spectral fit; the XMM-Newton ToO observation), and with\n$M_{V}$=--8.7$\\pm$0.6, and a peak $m_{V}$ of 12.0, we can derive a\ndistance to XMMSL1~J060636.2-694933 of 115$^{+43}_{-30}$\\,kpc. As\ndiscussed above however, we are unsure as to the exact outburst date\nand the maximum brightness at outburst. Our assumed peak $m_{V}$ of\n12.0 is almost certainly an underestimation. Although we have no\ninformation in the 12 days prior to Sep.~30, 2005, a simple linear\nextrapolation of the early October lightcurve back prior to Sep.~30,\n2005 suggests that the actual peak $m_{V}$ was somewhere between 9 and\n12. The corresponding distance estimates are then between 29 and\n115\\,kpc (with a mid-point $m_{V}$=10.5 value yielding a distance\nestimate of 58\\,kpc). Many methods have been used to estimate the\ndistance to the LMC (e.g.  Kovacs 2000, Nelson et al.\\ 2000), but a\nvalue of around 50\\,kpc appears to be quite robust. Our distance\nestimate is certainly consistent with that of the LMC, though the\nerrors are quite large. It does appear to be the case however, that\nour distance estimate places the source far outside of our own Galaxy. This, together with the source's position on the sky (at the eastern\nedge of the LMC) and the sizable ($\\sim$Galactic) X-ray hydrogen\ncolumn densities obtained from the spectral fits, suggest strongly\nthat XMMSL1~J060636.2-694933 lies within the LMC itself."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Minnesota Twins' decision to switch their flagship radio station from KSTP-AM to KTWN-FM in 2012 was likely motivated by a desire to improve the team's broadcast reach, particularly in rural areas. However, this move also led to a reduction in the station's signal strength, which may have negatively impacted the team's revenue. Which of the following statements best summarizes the Twins' long-term strategy regarding their radio broadcast partnerships?",
    "choices": [
      "A) The Twins have consistently prioritized maintaining a strong signal strength over increasing their broadcast reach, recognizing that a weaker signal can be a deterrent to advertisers.",
      "B) The Twins' decision to switch to KTWN-FM was a strategic move to expand their reach into the Twin Cities' suburbs, where the new station's FM signal is more effective.",
      "C) The Twins' partnership with KFXN-FM, which began in 2011, has been a success, and the team has since focused on maintaining a strong relationship with KFXN rather than exploring other broadcast options.",
      "D) The Twins' decision to switch to KTWN-FM was a response to the growing popularity of sports radio stations in the Twin Cities, and the team has since sought to maintain a strong presence in the market by partnering with other sports radio stations."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Station management cited the economic toll of the coronavirus for the changes. Sports broadcasting continues, primarily composed of ESPN radio network broadcasts. Sports Teams\n\nKSTP-AM served as the radio flagship for the Minnesota Vikings football team from 1970 to 1975. On August 1, 2006, the station announced that it would be the new flagship station for the Minnesota Twins baseball team, effective with the start of the 2007 season. The Twins had been on rival WCCO since arriving in Minnesota in 1961. KSTP served as the flagship for the Twins until the end of the 2012 season, when games moved to 96.3 KTWN-FM (now KMWA). The Twins have since returned to WCCO 830. The switch to a fairly weak FM station caused dissent among some listeners, particularly in communities that had trouble picking up KSTP 1500. Although KSTP is the state's second most powerful AM station, it must operate directionally at night, delivering a reduced signal to parts of the market. WCCO, by comparison, offers a signal with a wider coverage area during the day than KSTP does, with WCCO's non-directional 50,000 watt signal. In response, the Twins have expanded the number of affiliates. On March 9, 2011, KSTP announced it would be the new flagship for the University of Minnesota Golden Gophers men's and women's basketball and men's ice hockey, ending a 68-year run on WCCO. The rights have since moved to KFXN-FM, which already aired Gopher football. On March 2, 2017, KSTP announced it would be the first radio broadcaster for Minnesota United FC. The move brings live soccer action to 1500 AM. Previous logos\n\nReferences\n\nExternal links\nKSTP website\n\nFCC History Cards for KSTP (covering 1928-1980)\nRadiotapes.com Historic Minneapolis/St. Paul airchecks dating back to 1924 including KSTP and other Twin Cities radio stations. Rick Burnett's TwinCitiesRadioAirchecks.com has additional airchecks of KSTP and other Twin Cities radio stations from the '60s and '70s, including Chuck Knapp's 2nd show on KSTP. Hubbard Broadcasting\nESPN Radio stations\nPeabody Award winners\nRadio stations in Minneapolis\u2013Saint Paul\nRadio stations established in 1925\n1925 establishments in Minnesota\nMinnesota Kicks\nSports radio stations in the United States\nClear-channel radio stations"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A team of researchers is studying the effects of climate change on the spread of yellow fever in Africa. They analyze data from various regions and find that the disease is more prevalent in areas with high temperatures and humidity. However, they also notice that the disease seems to be more common in areas with high population density. Which of the following conclusions can be drawn from this analysis?",
    "choices": [
      "A) The researchers should focus on developing a vaccine that targets the mosquito population, as the disease is more prevalent in areas with high mosquito populations.",
      "B) The researchers should prioritize areas with high population density for vaccination efforts, as the disease is more common in these areas.",
      "C) The researchers should focus on developing a vaccine that targets the human population, as the disease is more prevalent in areas with high population density.",
      "D) The researchers should consider the role of climate change in exacerbating the spread of yellow fever, as the disease is more prevalent in areas with high temperatures and humidity, and also in areas with high population density."
    ],
    "correct_answer": "D)",
    "documentation": [
      "But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them, considering all the factors involved. And no one can have 100% certainty that he has made the right decision, but that's life. But if you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito and getting, often dying, of yellow fever. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice. At the end of this stage there is a remission of two or three days. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the toxic stage dies, even now, even with the best of hospital care. The Faget's sign can also occur at the end of the first stage. You asked specifically about the symptoms of the Americans on Dr. Reed's team who got yellow fever in Cuba in 1900. I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. \"In his logbook, Lazear wrote an unusual entry on September 13. In all cases before those, page after page of records, Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. A one-line entry with a name and a date. On that day, however, in his elegant hand, Lazear did not write the soldier's name, but instead wrote 'Guinea Pig No. 1.' He went on to write that this guinea pig had been bitten by a mosquito that developed from an egg laid by a mosquito that developed from an egg laid by a mosquito that fed on a number of yellow fever cases: Suarez, Hern\u00e1ndez, De Long, Fer\u00e1ndez."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be \"independent\" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so. Please help give the ICAN letter the widest possible distribution, particularly to politicians. \"The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system.\"\nNope. This makes no sense. Lots of people who seemed vibrant will get a very severe case of the same illness that a vulnerable baby overcomes in a day. And under the germ theory it doesn't matter how strong your immune system *was*. Once it's been overcome by the pathogen it is every bit as weak as anybody else's with that pathogen. What you say makes no sense. There's no reason for me to reply to you again. \"Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared?\"\nWhy do you keep asking this question when I've already provided the answer hundreds of times? Why are you so desperate to believe the people who you already recognize are harming our children? Why would Walter Reed be any more trustworthy than Paul Offit or Senator Pan? Why would Jenner or Pasteur? And you went no way to explaining my arguments against germ theory. If we are attacked by billions of viruses every day then if even a tiny fraction of them are pathogenic then we couldn't possibly survive."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new policy is being implemented to increase the number of HDTV broadcasts available to the public. This policy will have the effect of reducing the number of broadcast television stations that are currently licensed to operate. However, the policy will also create new opportunities for individuals to produce and distribute their own content. Which of the following statements best describes the likely outcome of this policy?",
    "choices": [
      "A) The number of people participating in the media will increase, as more individuals will have the opportunity to produce and distribute their own content.",
      "B) The government will be able to exert greater control over the content of HDTV broadcasts, as the new policy will allow for more centralized control over the distribution of content.",
      "C) The number of people who are currently engaged in creative activities will decrease, as the new policy will make it more difficult for individuals to produce and distribute their own content.",
      "D) The new policy will lead to a more diverse range of viewpoints being represented in HDTV broadcasts, as more individuals will have the opportunity to produce and distribute their own content."
    ],
    "correct_answer": "D)",
    "documentation": [
      "I just want things to be able to sort themselves out in a much more equitable fashion. We have this enormous, artificial scarcity today over the means of communication, because the government awards licenses which self-perpetuate. They are about to do the same thing, and give every broadcast television station another license for HDTV. So if you've got a license today, you get a second one; if you don't have one, you get nothing. That is going to be our policy about HDTV. I think it would be a lot better if we had more markets, more choices, and better values. I don't know how to do better values, but we know how to do more choices. So the point is, we'll wind up with some new regime which I don't think that we can particularly predict. I don't think that it is going to be chaotic or anarchic. I think there is something about people as social animals or creatures -- we will create some new forms of social organization. There will be information middlemen; there will be the equivalent of editors and packagers. There will be trusted intermediaries who help organize these new media. If you open it up and equalize things so that everybody can participate, you will get more diversity of points of view, you will get less homogenization. One of the reasons that tons of people have just dropped out, or are in terminal couch-potato-dom is that the sets of choices and the values that come across the tube are not ones that stir the human heart. And people know that. They can't figure out what to do about that, so they sort of fuzz out on drugs and alcohol. I say let's edit TV, which is the electronic drug. Let's do something about that. DAVIES: I like your idea, Mitch. I think it's sweet. (laughter) The problem is that I really worry that the ultimate test of the future is going to be the outcome of the quest, the battle between those who are looking for the sort of vision you've got of the right of the individual, the individual being the producer. And that, probably, is the way we solve our problems on this planet."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company is considering deploying a fleet of secure devices, such as smartphones and wearables, to its employees. These devices are equipped with built-in security features that control access to potentially harmful networks, websites, and content. However, the company's IT department is concerned about the management and configuration of these devices, as they will be used by hundreds of employees. Which of the following is a potential solution to this problem?",
    "choices": [
      "A) The company can use a traditional IoT-MDM system provider to manage and configure the devices, as they have experience in managing large fleets of devices.",
      "B) The company can use a cloud-based security platform to manage the devices, as it provides a centralized solution for device management and security.",
      "C) The company can use a hybrid approach, where they use a traditional IoT-MDM system provider for some devices and a cloud-based security platform for others.",
      "D) The company can use a device manufacturer's proprietary management and configuration tool, as it is specifically designed for their secure devices."
    ],
    "correct_answer": "D)",
    "documentation": [
      "While these service provisioning scenarios are relevant to overall 5G, they are also significantly related to the case of IoT-MDM system/services. Figure 3 Scenarios for 5G cybersecurity provisioning [Adapted from 35]. As a result, we find four major drivers of security, which potentially will come with new business opportunities in the 5G era. Device driven security comprises distributed and D2D security techniques. Platform driven security will focus on centralized and D2D security techniques. Whereas, network infrastructure driven security should focus on centralized and infrastructure security methods. Lastly, location driven security should harness distributed and infrastructure security techniques. In a quest to identify potential business entities operating in each of the quadrants relevant to device management, we recognize a \u2018secure device manufacturer/provider\u2019 focuses on device-driven security. Currently, multiple device manufacturers are developing devices where security features are built-in, regardless of which network or websites the users are accessing. This built-in security can be offered to multiple kinds of devices including smartphones, tablets, pcs, wearables, and even to IoT devices with communication and computation capability. These secure devices are built in a way that it will control access to potentially harmful networks, websites, and content; even without any commercially available security applications installed. However, when a customer enterprise (e.g. the future digital hospital) is buying a fleet of hundreds of such secure devices, the question raises how to manage and configure all these devices from time-to-time. In such a case, even the secure devices will need IoT-MDM services for proper management and seamless upgrading when needed. For the platform-driven security, we observe a \u2018traditional IoT-MDM system provider\u2019 can be a good example. In many cases, IoT-MDM system providers are selling there device management systems to enterprises directly."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the partition function Z, what is the relationship between the effective input-input coupling matrix U and the transmission matrix J?",
    "choices": [
      "A) The effective input-input coupling matrix U is equal to the inverse of the transmission matrix J.",
      "B) The effective input-input coupling matrix U is equal to the product of the transmission matrix J and the attenuation factor c.",
      "C) The effective input-input coupling matrix U is equal to the sum of the transmission matrix J and the identity matrix I.",
      "D) The effective input-input coupling matrix U is equal to the difference between the transmission matrix J and the attenuation factor c times the identity matrix I."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(\\ref{eq:deltas}). Moreover, we move to consider the ensemble of all possible solutions of Eq. (\\ref{eq:transm}) at given $\\mathbb{T}$, looking at  all configurations of input fields. We, thus, define the function:\n \n   \\begin{eqnarray}\n  Z &\\equiv &\\int_{{\\cal S}_{\\rm in}} \\prod_{j=1}^{N_I}  dE^{\\rm in}_j \\int_{{\\cal S}_{\\rm out}}\\prod_{k=1}^{N_O} dE^{\\rm out}_k \n  \\label{def:Z}\n\\\\\n    \\times\n  &&\\prod_{k=1}^{N_O}\n   \\frac{1}{\\sqrt{2\\pi \\Delta^2}}  \\exp\\left\\{-\\frac{1}{2 \\Delta^2}\\left|\n  E^{\\rm out}_k -\\sum_{j=1}^{N_I}  t_{kj} E^{\\rm in}_j\\right|^2\n\\right\\} \n\\nonumber\n \\end{eqnarray} We stress that the integral of Eq. \\eqref{def:Z} is not exactly a Gaussian integral. Indeed, starting from Eq. \\eqref{eq:deltas}, two constraints on the electromagnetic field intensities must be taken into account. The space of solutions is  delimited by the total power ${\\cal P}$ received by system, i.e., \n  ${\\cal S}_{\\rm in}: \\{E^{\\rm in} |\\sum_k I^{\\rm in}_k = \\mathcal{P}\\}$, also implying  a constraint on the total amount of energy that is transmitted through the medium, i. e., \n  ${\\cal S}_{\\rm out}:\\{E^{\\rm out} |\\sum_k I^{\\rm out}_k=c\\mathcal{P}\\}$, where the attenuation factor  $c<1$ accounts for total losses. As we will see more in details in the following, being interested in inferring the transmission matrix through the PLM, we can omit to explicitly include these terms in Eq. \\eqref{eq:H_J} since they do not depend on $\\mathbb{T}$ not adding any information on the gradients with respect to the elements of $\\mathbb{T}$.\n  \n Taking the same number of incoming and outcoming channels, $N_I=N_O=N/2$, and  ordering the input fields in the first $N/2$ mode indices and the output fields in the last $N/2$ indices, we can drop the ``in'' and ``out'' superscripts and formally write $Z$  as a partition function\n    \\begin{eqnarray}\n        \\label{eq:z}\n && Z =\\int_{\\mathcal S} \\prod_{j=1}^{N} dE_j \\left(   \\frac{1}{\\sqrt{2\\pi \\Delta^2}} \\right)^{N/2} \n \\hspace*{-.4cm} \\exp\\left\\{\n  -\\frac{ {\\cal H} [\\{E\\};\\mathbb{T}] }{2\\Delta^2}\n  \\right\\}\n  \\\\\n&&{\\cal H} [\\{E\\};\\mathbb{T}] =\n-  \\sum_{k=1}^{N/2}\\sum_{j=N/2+1}^{N} \\left[E^*_j t_{jk} E_k + E_j t^*_{kj} E_k^* \n\\right]\n \\nonumber\n\\\\\n&&\\qquad\\qquad \\qquad + \\sum_{j=N/2+1}^{N} |E_j|^2+ \\sum_{k,l}^{1,N/2}E_k\nU_{kl} E_l^*\n \\nonumber\n \\\\\n \\label{eq:H_J}\n &&\\hspace*{1.88cm } = - \\sum_{nm}^{1,N} E_n J_{nm} E_m^*\n \\end{eqnarray}\n where ${\\cal H}$ is a real-valued function by construction, we have introduced the effective input-input coupling matrix\n\\begin{equation}\nU_{kl} \\equiv \\sum_{j=N/2+1}^{N}t^*_{lj} t_{jk} \n \\label{def:U}\n \\end{equation}\n and the whole interaction matrix reads (here $\\mathbb{T} \\equiv \\{ t_{jk} \\}$)\n \\begin{equation}\n \\label{def:J}\n \\mathbb J\\equiv \\left(\\begin{array}{ccc|ccc}\n \\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}&-\\mathbb{U} \\phantom{()}&\\phantom{()}&\\phantom{()}&{\\mathbb{T}}&\\phantom{()}\\\\\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\hline\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}& \\mathbb   T^\\dagger&\\phantom{()}&\\phantom{()}& - \\mathbb{I} &\\phantom{()}\\\\\n\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}\\\\\n \\end{array}\\right)\n \\end{equation}\n \n Determining the electromagnetic complex amplitude configurations that minimize the {\\em cost function} ${\\cal H}$, Eq.  (\\ref{eq:H_J}),  means to maximize the overall distribution peaked around the solutions of the transmission Eqs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A secure device manufacturer/provider is considering offering a mixed-source business model for their IoT-MDM services. In this scenario, the manufacturer has developed a proprietary IoT-MDM platform that is integrated with their own secure devices. However, they also want to offer an open extensions model, where customers can purchase their devices but use a third-party IoT-MDM service. Which of the following statements best describes the manufacturer's approach to IoT-MDM services?",
    "choices": [
      "A) By offering a proprietary model, the manufacturer is creating a closed core, where customers are locked into their own IoT-MDM platform, and closed extensions, where they can only use their own IoT-MDM service.",
      "B) The manufacturer is using an open core approach, where they are offering their own IoT-MDM platform as an open-source solution, and open extensions, where customers can use a third-party IoT-MDM service.",
      "C) The manufacturer is using a proprietary model, where they are offering their own IoT-MDM platform, and open extensions, where customers can use a third-party IoT-MDM service, but only for a limited time period.",
      "D) By offering a mixed-source business model, the manufacturer is creating a hybrid approach, where customers can choose between a proprietary model, where they use the manufacturer's own IoT-MDM platform, and an open extensions model, where they use a third-party IoT-MDM service."
    ],
    "correct_answer": "D)",
    "documentation": [
      "And, in other cases, they are selling the service through MNO\u2019s bundled with connectivity and/or infrastructure. While as network infrastructure-driven security, a \u2018mobile network operator/carrier\u2019 or a \u2018network infrastructure vendor\u2019 can build own IoT-MDM system to offer their clients as well. And, finally, a location-specific micro operator can offer location-driven security. Micro operators offer mobile connectivity combined with specific, local services. The operation of a micro operator is spatially confined to either its premises or to a defined area of operation. As a part of the location-specific services, these micro operators can also offer IoT-MDM services for the users through outsourcing. Further, we attempt to connect the aforementioned classification and examples of different players offering IoT-MDM services with the mixed source business model approach. Table 2 summarizes our understanding on how each kind of cybersecurity provider can open and mix the core value creation logic for end users. As mentioned previously, the mixed-source business model options are: open source (open core, open extensions), open core (open core, closed extensions), open extensions (closed core, open extensions), and proprietary (closed core, closed extensions). In relation to these mixed-source options, we analyze the plausible options for each of the four distinct cybersecurity providers in the context of this study. Secure device manufacturer/provider Secure devices Proprietary (own device, own IoT-MDM platform), Open extensions (own device, outsourced IoT-MDM service). From a secure device manufacturer perspective, device business can be considered as the core operation whereas IoT-MDM services would be extended solution. A secure device manufacturer/provider can have either a proprietary model or an open extensions model. In the proprietary model, the secure device manufacturer will offer their own devices alongwith their own IoT-MDM system/service. This is a viable case in a sense that customers who are purchasing the fleet of secure devices might prefer the IoT-MDM service from the same vendor, which is ideally less risk prone."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A mother is concerned about her 10-year-old son's increasing social isolation and difficulty with changes in routine. She has noticed that he has always been a bit of a loner, preferring to spend time reading and playing alone, but has never been diagnosed with any condition that would explain his behavior. Recently, she has started to worry that he may be struggling with anxiety or depression, given his frequent tantrums and crying spells. She has tried to talk to his teachers and friends about her concerns, but they seem to think that he is just going through a normal phase of childhood. How might the mother's concerns be best addressed, and what potential underlying issues might be contributing to her son's behavior?",
    "choices": [
      "A) The mother should try to increase her son's social interactions by enrolling him in extracurricular activities, as this is a common way to help children with social anxiety.",
      "B) The mother's son is likely experiencing a normal phase of childhood, and his behavior is not a cause for concern. He is simply going through a period of self-discovery and will outgrow his difficulties with changes in routine.",
      "C) The mother's son is likely experiencing anxiety or depression, and the best course of action would be to refer him to a therapist or counselor for individual therapy sessions.",
      "D) The mother's son's behavior is likely indicative of a neurodevelopmental disorder, such as autism spectrum disorder (ASD), which is characterized by difficulties with social interactions, changes in routine, and repetitive behaviors. A comprehensive diagnostic evaluation by a qualified professional is necessary to determine the underlying cause of her son's behavior."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We changed her diet and tried getting her involved with activities but she is anti-social and prefers reading than being social. She is terrified of change even in daily routine (even that will trigger prolonged crying). It frustrates me because I don't know what else to do with her behavior. I've tried acupuncture (she refused at the first session); she refuses massage too. She is an honor-roll student at school and has very minimal issues at school but if she has had a bad day it does result in a tantrum or crying and defiance. How can I get her tested for Asperger's Syndrome? Last night our 24 year old son with Aspergers told his dad and I that he is pulling out of the 4 college classes that he recetnly enrolled in because he has not been attending class or turning in his assignments. He paid $2800 (his own money) for tuition and I reminded him of this when he told us but it did not seem to bother him. This is the 3rd time he has started college courses and has not completed them. (He also took some concurrent college classes while he was in high school that he failed). This is a son who basically had a 4.0 grade point average through 10th grade and got a 34 on the ACT the first time he took it. With the news that he was once again not sticking with college courses I did not sleep well. When I got up this mornning I began looking online for help in how to deal with his situation. I found your \"Launching Adult Children With Aspergers\" and purchased it. Most of what is included are things we have done or did with our son throughout his life. I was hoping for more help so I am emailing you now in hopes of more specific ideas. We noticed some things with our son, Taylor, as a yound child but as we had not heard of Aspergers at that time we just did what we thought would help him. As a toddler and a child at pre-school he generally went off on his own to play. When I talked to his pre-school teacher about my concerns (that I was worried he would end up a hermit) she said she did not see him being a loner and that he seemed to interact fine with others in many situations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new algorithm, dubbed \"GraphSweep\", has been proposed to solve the graph coloring problem on large-scale graphs. According to the authors, GraphSweep outperforms the current state-of-the-art algorithm, HEAD, on graphs with over 1 million edges. However, when tested on smaller graphs, GraphSweep's performance is comparable to that of the conflict optimizer, CWLS, which is known to be fast but has poor performance on random graphs. The authors also mention that GraphSweep's running time is significantly longer than that of the conflict optimizer, PWLS, which is optimized for geometric graphs. Which of the following statements best summarizes the performance of GraphSweep on different types of graphs?",
    "choices": [
      "A) GraphSweep outperforms HEAD on all graph sizes, including small ones, and is comparable to CWLS on random graphs.",
      "B) GraphSweep is comparable to HEAD on large graphs, but outperforms CWLS on small graphs, which are typically geometric in nature.",
      "C) GraphSweep's performance on random graphs is comparable to that of the conflict optimizer, CWLS, but it is slower than PWLS on geometric graphs.",
      "D) GraphSweep outperforms CWLS on all graph sizes, but its performance on random graphs is comparable to that of the conflict optimizer, PWLS."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Notice also that HEAD algorithm provides 283 colors after one hour compared to less than 240 colors for the conflict optimizers. We ran the three implementations on three different servers and compared the results shown in Figure . For each implementation, the x coordinate is the running time in hours, while the y coordinate is the smallest number of colors found at that time. Results on DIMACS Graphs\n\nWe tested the implementation of each team on the DIMACS instances to gauge the performance of the conflict optimizer on other classes of graphs. We compared our results to the best known bounds and to the state of the art coloring algorithms HEAD and QACOL . The time limit for Lasa's algorithms is 1 hour. CWLS is Lasa's conflict optimizer with the neighbourhood presented in TABUCOL , while PWLS is the optimizer with the neighbourhood presented in PARTIALCOL . Gitastrophe algorithm ran 10 minutes after which the number of colors no longer decreases. Shadoks algorithm ran for 1 hour without the BDFS option (results with BDFS are worse). Results are presented in Table . We only kept the difficult DIMACS instances. For the other instances, all the results match the best known bounds. The DIMACS instances had comparatively few edges (on the order of thousands or millions); the largest intersection graphs considered in the CG:SHOP challenge had over 1.5 billion edges. We notice that the conflict optimizer works extremely poorly on random graphs, but it is fast and appears to perform well on geometric graphs (r250.5, r1000.1c, r1000.5, dsjr500.1c and dsjr500.5), matching the best-known results . Interestingly, these geometric graphs are not intersection graphs as in the CG:SHOP challenge, but are generated based on a distance threshold. On the DIMACS graphs, Lasa implementation shows better performance than the other implementations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the Libyan government's stance on human rights, based on the statements made by the Libyan delegate, former congressman Curt Weldon, and the international reporting on Saif Al-Islam's actions?",
    "choices": [
      "A) The Libyan government is committed to upholding human rights and is actively working to improve relations with the West.",
      "B) The Libyan government is willing to turn a blind eye to human rights violations in order to maintain good relations with Western countries.",
      "C) The Libyan government is divided on the issue of human rights, with some members advocating for accountability and others prioritizing economic interests.",
      "D) The Libyan government's stance on human rights is complex and multifaceted, influenced by a range of factors including cultural, historical, and political considerations."
    ],
    "correct_answer": "D)",
    "documentation": [
      "And despite what Sarkozy said about resolving the issues of the Bulgarian nurses when they were sentenced to death twice, it was Saif who played a very critical role against some very powerful forces in this country that wanted to kill those people. You know, I don't know of any incidences where I, first hand, have seen evidence of him committing human rights violations, and if he did, he has to be held accountable like everyone else. And I have said that publicly and I will say that privately. So my judgment is just based upon my experience with him, the fact that he is a knowledgeable person, he understands the need to interact and interface with the West. I think he could be a viable candidate. But ultimately, my opinion is hopefully going to be the opinion of the Libyan people. BLITZER: Because you probably have seen all of the articles, the reports over the past month, month and a half, of mass murder, of killings, not only by Saif Al-Islam, but some of his brothers that have gone on, the atrocities that have been so widely reported. I hear what you're saying about his role over the recent years when the Bush administration, and later the Obama administration, was trying to improve relations with Libya, but over the past several weeks, based on all of the international reporting we have seen, it's been a brutal record that he has accomplished. WELDON: Well, again, I don't have firsthand evidence of that. I just got here two days ago. And I fully support an international tribunal to look at human rights violations on everyone in this country. That's necessary. And if they find evidence that he has been involved in that, then he should suffer the consequences of his actions. (END VIDEOTAPE) BLITZER: In our next hour, part two of the interview with former congressman Curt Weldon. There have been some questions raised about his motive. Is he in all of this for the money? You're going to find out his answer to that and more. Stand by. Also, Washington, D.C.'s congressional delegate is telling colleagues -- and I'm quoting her now -- \"Go to hell.\""
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In McPherson County, Kansas, the population center for the township of Groveland is the city of Elyria\u2020, which is also the largest city in the county. However, the county's governmentally independent cities, Lindsborg and McPherson, are excluded from the census figures for the townships. According to the 1921 Standard Atlas of McPherson County, Kansas, the township of Groveland's population total is comprised of the following towns: Alta Mills, Battle Hill, and Christian. Which of the following statements is true about the population center for the township of Groveland?",
    "choices": [
      "A) The population center for Groveland is the city of Johnstown, which is also the largest city in the county.",
      "B) The population center for Groveland is the city of Elivon, which is the largest city in the county.",
      "C) The population center for Groveland is the city of Doles Park, which is the largest city in the county.",
      "D) The population center for Groveland is the city of Elyria\u2020, which is the largest city in the county, and is excluded from the census figures for the townships."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Conway\n Elyria\u2020\n Groveland\n Johnstown\n New Gottland\n Roxbury\u2020\n\nGhost towns\n Alta Mills\n Battle Hill\n Christian\n Doles Park\n Elivon\n King City\n Sweadal\n\nTownships\nMcPherson County is divided into twenty-five townships. The cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships. In the following table, the population center is the largest city (or cities) included in that township's population total, if it is of a significant size. See also\n List of people from McPherson County, Kansas\n National Register of Historic Places listings in McPherson County, Kansas\n McPherson Valley Wetlands\n Maxwell Wildlife Refuge\n\nReferences\n\nNotes\n\nFurther reading\n\n Wheeler, Wayne Leland. \"An Analysis of Social Change in a Swedish-Immigrant Community: The Case of Lindsborg, Kansas.\" (PhD dissertation, University of Missouri-Columbia; ProQuest Dissertations Publishing, 1959. 5905657). County\n Through the Years: A Pictorial History of McPherson County; McPherson Sentinel' Heritage House Publishing Co; 1992. McPherson County First Courthouse Built About 1869 or 1870; Lindsborg News-Record; March 30, 1959. Pioneer Life and Lore of McPherson County, Kansas; Edna Nyquist; Democratic-Opinion Press; 1932. A History of the Church of the Brethren in Kansas (includes McPherson College history); Elmer LeRoy Craik; McPherson Daily; Republican Press; 397 pages; 1922. Portrait and Biographical Record of Dickinson, Saline, McPherson, and Marion Counties, Kansas; Chapman Bros; 614 pages; 1893. Standard Atlas of McPherson County, Kansas; Geo. A. Ogle & Co; 82 pages; 1921. Plat Book of McPherson County, Kansas; North West Publishing Co; 50 pages; 1903. Edwards' Atlas of McPherson County, Kansas; John P. Edwards; 51 pages; 1884. Trails\n The Story of the Marking of the Santa Fe Trail by the Daughters of the American Revolution in Kansas and the State of Kansas; Almira Cordry; Crane Co; 164 pages; 1915. (Download 4MB PDF eBook)\n The National Old Trails Road To Southern California, Part 1 (LA to KC); Automobile Club Of Southern California; 64 pages; 1916."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A manufacturer of aircraft components is considering a new production process that involves bending and sloping the fuselage sides to form a conical section. This process is similar to one used in the marine trades, where a conical shape is cut with a plane not perpendicular to its axis, resulting in an elliptical shape. However, the manufacturer is concerned that the process may not be suitable for their materials, which have a high degree of in-plane strain. What is the primary reason for the manufacturer's concern?",
    "choices": [
      "A) The process may not be able to accommodate the curvature of the fuselage sides, which would result in a cylindrical section.",
      "B) The process relies on the fuselage sides being able to be bent and sloped without any deformation, which is not possible with the manufacturer's materials.",
      "C) The process may not be able to produce a flat surface, which is required for the assembly of the aircraft components.",
      "D) The process may not be able to accommodate the compound curvature of the fuselage sides, which would result in an elliptical shape."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel \"undevelopable\" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition \"developable\". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a \"compounded\" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home) , the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock. This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient with Familial Hypercholesterolemia (FH) undergoes a coronary catheterization to evaluate for Ischemic Heart Disease (IHD). During the procedure, a FFR test is performed to assess the pressure differences across a coronary artery stenosis. The patient's Holter Monitor data shows Hypokinesia, indicating decreased heart wall motion. Which of the following statements is most consistent with the patient's clinical presentation?",
    "choices": [
      "A) The patient's high cholesterol levels are likely to cause a significant narrowing of the coronary artery, leading to a high FFR value, which would indicate a blockage.",
      "B) The patient's Hypokinesia is likely due to a cardiac arrhythmia, which is being treated by the ICD implanted in the patient.",
      "C) The patient's IHD is likely caused by a blockage in the Radial Artery, which is a common alternative to Femoral Artery access in the US.",
      "D) The patient's high cholesterol levels, combined with the FFR test results and Holter Monitor data, suggest that the patient has a significant coronary artery stenosis, which is likely to cause a decrease in blood flow to the heart muscle, leading to IHD."
    ],
    "correct_answer": "D)",
    "documentation": [
      "See also: MIBI, Echocardiogram, Nuclear Stress Test. Familial hypercholesterolemia (FH) \u2013 A genetic predisposition to dangerously high cholesterol levels. FH is an inherited disorder that can lead to aggressive and premature cardiovascular disease, including problems like heart attacks, strokes, or narrowing of the heart valves. Femoral Artery: a major artery in your groin/upper thigh area, through which a thin catheter is inserted, eventually making its way into the heart during angioplasty to implant a stent; currently the most widely used angioplasty approach in the United States, but many other countries now prefer the Radial Artery access in the wrist. FFR \u2013 Fractional Flow Reserve: A test used during coronary catheterization (angiogram) to measure pressure differences across a coronary artery stenosis (narrowing or blockage) defined as as the pressure behind a blockage relative to the pressure before the blockage. HC \u2013 High Cholesterol: When fatty deposits build up in your coronary arteries. HCTZ \u2013 Hydrochlorothiazide: A drug used to lower blood pressure; it acts by inhibiting the kidneys\u2019 ability to retain water. Used to be called \u201cwater pills\u201d. Heart Failure \u2013 a chronic progressive condition that affects the pumping power of your heart muscle. Sometimes called Congestive Heart Failure (CHF). Holter Monitor \u2013 A portable monitoring device that patients wear for recording heartbeats over a period of 24 hours or more. HTN \u2013 Hypertension: High blood pressure, the force of blood pushing against the walls of arteries as it flows through them. Hypokinesia \u2013 Decreased heart wall motion during each heartbeat, associated with cardiomyopathy, heart failure, or heart attack. Hypokinesia can involve small areas of the heart (segmental) or entire sections of heart muscle (global). Also called hypokinesis. ICD \u2013 Implantable Cardioverter Defibrillator: A surgically implanted electronic device to treat life-threatening heartbeat irregularities. IHD \u2013 Ischemic Heart Disease: heart problems caused by narrowing of the coronary arteries, causing a decreased blood supply to the heart muscle."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In Ng\u0169g\u0129 wa Thiong'o's novel \"Things Fall Apart\", the character of Kamba is portrayed as a strong advocate for the traditional Igbo way of life. However, his views on the role of women in society are often at odds with those of his father, Unoka. What is the primary reason for Kamba's ambivalence towards women's roles in Igbo society?",
    "choices": [
      "A) He believes that women should be allowed to own property and participate in trade, but only if they are married to men who are willing to support them financially.",
      "B) He thinks that women should be restricted to domestic roles, as this is the only way to maintain social order and prevent chaos.",
      "C) He is concerned that women's participation in traditional Igbo rituals and ceremonies will lead to a loss of cultural heritage and identity.",
      "D) He believes that women should be educated and empowered to take on leadership roles in Igbo society, but only if they are willing to adopt Western values and customs."
    ],
    "correct_answer": "D)",
    "documentation": [
      "See also\n\nThings Fall Apart\nDeath and the King's Horseman\n\nReferences\n\nExternal links\nOfficial homepage of Ng\u0169g\u0129 wa Thiong'o\nBBC profile of Ng\u0169g\u0129 wa Thiong'o\nWeep Not, Child at Google Books\n\nBritish Empire in fiction\nNovels set in colonial Africa\nHistorical novels\nKenyan English-language novels\nNovels by Ng\u0169g\u0129 wa Thiong'o\nNovels set in Kenya\n1964 novels\nHeinemann (publisher) books\nPostcolonial novels\nAfrican Writers Series\n1964 debut novels"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the Federal Radio Commission's (FRC) assignment of KSTP to 1460 kHz in November 1928?",
    "choices": null,
    "correct_answer": "D)",
    "documentation": [
      "Beginning on November 24, 1927 the WAMD broadcasts, still on 1330\u00a0kHz, were shifted to KFOY's facility in St. Paul. (At this time KFOY was assigned to 1050\u00a0kHz). The next day it was announced that National Battery had purchased KFOY, and as of December 1, 1927 both KFOY and WAMD were reassigned to 1350\u00a0kHz. WAMD continued making regular broadcasts until the end of March 1928, while KFOY, although it continued to be licensed for a few more months on a time-sharing basis with WAMD, ceased operations at this point. National Battery Company\nIn mid-December 1927, the National Battery Company announced it had received permission from the Federal Radio Commission (FRC) to build a new station, with the call letters KSTP, operating from a transmitter site to be constructed three miles south of Wescott. The next month it was reported that the new station, still under construction, had been assigned to 1360\u00a0kHz. KSTP made its debut broadcast on March 29, 1928. Although technically it was a separate station from WAMD and KFOY, both of which were formally deleted on April 30, 1928, overall KSTP was treated as the direct successor to a consolidated WAMD and KFOY. Hubbard became the merged station's general manager, acquiring controlling interest in 1941. A month after the merger, KSTP became an affiliate for the NBC Red Network. It remained with NBC for 46 years. On November 11, 1928, under the provisions of the FRC's General Order 40, KSTP was assigned to a \"high-powered regional\" frequency of 1460\u00a0kHz. The only other station assigned to this frequency was WTFF in Mount Vernon Hills, Virginia (later WJSV, now WFED, Washington, D.C.). On February 7, 1933, the FRC authorized KSTP to increase its daytime power to 25 KW. In 1938 and 1939 KSTP also operated a high-fidelity AM \"experimental audio broadcasting station\" Apex station, W9XUP, originally on 25,950\u00a0kHz and later on 26,150\u00a0kHz. In 1941, as part of the implementation of the North American Regional Broadcasting Agreement, KSTP was assigned to its current \"clear channel\" frequency of 1500\u00a0kHz, with the provision that it and WJSV, as \"Class I-B\" stations, had to maintain directional antennas at night in order to mutually protect each other from interference."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user, John, has created a subscription on Broadjam to access premium features. According to the terms of service, the subscription will automatically renew for successive one-year periods unless terminated by John. However, John has recently experienced technical difficulties with his internet connection and is concerned that he may miss some of the premium features during the initial period of his subscription. What is the most likely outcome for John's subscription after the initial period?",
    "choices": [
      "A) The subscription will automatically renew for another year, and John will have access to all premium features.",
      "B) The subscription will not renew automatically, and John will need to manually renew it to access premium features.",
      "C) The subscription will renew automatically, but John will not have access to some premium features due to technical difficulties with his internet connection.",
      "D) The subscription will not renew automatically, and John will need to manually renew it to access premium features, but he will also be responsible for any charges incurred during the initial period."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Broadjam is not liable for any harm caused by or related to the theft of your Username, your disclosure of your Username, or your authorization to allow another person to access and use the Site or any Service using your Username. Furthermore, you are solely and entirely responsible for any and all activities that occur under your account, including, but not limited to, any charges incurred relating to the Site or any Service. You agree to immediately notify us of any unauthorized use of your account or any other breach of security known to you. You acknowledge that the complete privacy of your data transmitted while using the Site or any Service cannot be guaranteed. The term of any Subscription Service shall commence when the Subscriber initiates payment for such Subscription Service or, if the Subscription Service is complimentary, when the Subscriber registers for such Subscription Service. All Subscription Services will extend for an initial period of oneyear (the \"Term\") and, unless terminated as provided herein, shall renew automatically for successive one-year periods. During the Term, the Subscriber shall be afforded the full use and benefit of the applicable Subscription Service as described on the Site (the \"Service Benefits\"), which Service Benefits may be revised by Broadjam from time to time without notice to the Subscriber. Due to technical considerations, certain Service Benefits may not be available to the Subscriber immediately upon commencement of the Term, but shall be provided to the Subscriber as soon as commercially reasonable. Please direct any questions about Subscription Services or Service Benefits to Broadjam by email at: customerservice@broadjam.com or by US mail at: Broadjam Inc., 100 S. Baldwin St. Ste. #204, Madison, WI 53703, Attn: Customer Service.\n(b) maintain and update such information as needed to keep it current, complete and accurate. Subscriber acknowledges that Broadjam relies and will rely upon the accuracy of such information as supplied by Subscriber."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the number of steps taken by older adults is a significant predictor of cognitive decline. However, the study also revealed that the type of flooring in the home can affect the accuracy of step count estimates. Which of the following conclusions can be drawn from this information?",
    "choices": [
      "A) Older adults who live in homes with hardwood flooring are more likely to experience cognitive decline than those who live in homes with carpeted flooring.",
      "B) The relationship between step count and cognitive decline is stronger in older adults who live in homes with hardwood flooring.",
      "C) The study suggests that the type of flooring in the home is a more important predictor of cognitive decline than the number of steps taken.",
      "D) The findings of the study support the idea that early detection of cognitive decline can lead to improved outcomes, regardless of the type of flooring in the home."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\section{Introduction}\nCognitive deficit of older adults is one of the biggest global public health challenges in elderly care. Approximately 5.2 million people of 65 and older are suffered with any form of cognitive impairments in United States in 2012 \\cite{stat12}. Dementia is one of the major causes of the cognitive impairments which is more acute among 85 and older population (50\\%) \\cite{stat12}. However, the costs (financial and time) of health care and long-term care for individuals with Alzheimer's (special form of dementia) or other dementias are substantial. For example, during 2016, about 15.9 million family and friends in United States provided 18.2 billion hours of unpaid assistance to those with cognitive impairments which is a contribution to the nation valued at \\$230.1 billion. One the other hand, total payments for all individuals with all form of cognitive impairments are estimated at \\$259 billion. Total annual payments for health care, long-term care and hospice care for people with Alzheimer's or other dementias are projected to increase from \\$259 billion in 2017 to more than \\$1.1 trillion in 2050. Among the above costs, a significant amount are relevant to clinical and diagnostic tests \\cite{stat17}. Although clinical and diagnostic tests have become more precise in identifying dementia, studies have shown that there is a high degree of underrecognition especially in early detection. However, there are many advantages to obtaining an early and accurate diagnosis when cognitive symptoms are first noticed as the root cause findings of impairment always lessen the progress of impairment status and sometimes symptoms can be reversible and cured. With the proliferation of emerging ubiquitous computing technologies, many mobile and wearable devices have been available to capture continuous functional and physiological behavior of older adults. Wearable sensors are now capable of estimating number of steps being taken, physical activity levels, sleep patterns and physiological outcomes (heart rate, skin conductance) of older adults \\cite{sano15}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is interested in using a deep generative network to complete fragmented contours in an image. They have read about the work of Deep Image Prior (DIP) and the hourglass architecture of UNet, which has shown good performance in inverse problems such as image denoising and super-resolution. However, they are concerned that their model may overfit to the incomplete image. What is the primary advantage of removing the dependency of the algorithm on the guiding mask in their proposed novel algorithm?",
    "choices": [
      "A) The model can learn to recognize patterns in the completed image, even if the original image is incomplete.",
      "B) The model can generalize better to new, unseen images, without relying on the guiding mask.",
      "C) The model can produce more realistic results, by allowing it to explore the entire image space.",
      "D) The model can be trained on a single query image, without the need for additional training data or guiding masks."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Contributions of our work are summarized as follows: 1. In our pipeline, we propose a novel algorithm that enables us to complete contours that appear to be connected to each other in an illusory form. 2. Our model is trained on just one single query image and does not need any training data. 3. Our model does not need to know which regions of the image are masked or occluded, i.e., we remove the dependency of the algorithm on the guiding mask (a guiding mask is a mask that informs the model on where the missing regions are located at). We also introduce two metrics to produce a stopping criterion to know when to stop training before the model fully overfits to the incomplete image, i.e., we guide the model to stop when the completed image is produced. Methods\n\nOur eyes are trained to predict a missing region of an occluded object within a scene. We can easily perceive or make guesses about parts of objects or shapes that we do not necessarily see. Even when we are looking at an image, we might guess about the shape, property, or other attributes of an unknown object within a scene. Such capability extends beyond just known objects or shapes. We can look at a disconnected set of contours and guess what the connected form may look like. This capability is rooted in our prior knowledge about the world. (see Figure ). In this work, we aim to achieve a similar capability using deep generative networks. Most neural networks that we work with these days are trained with a massive amount of data and one might think that this is the only way that a neural network can obtain prior information. Authors of Deep Image Prior (DIP) suggest that the convolutional architecture can capture a fair amount of information about image distribution. They show that the hourglass architectures like UNet can show some good performances in some inverse problems such as image denoising, super-resolution, and inpainting. In this work, we focus on completing fragmented contours end-to-end just by using a single image."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on postural activity detection in older adults found that the removal of fundamental physiological sensor artifacts significantly improves the accuracy of automated cross-sectional cognitive health assessments. However, the researchers also noted that the efficiency of their evaluation method depends on the modality used (physical, physiological, or ambient) and the activity mode (single activity or multiple activities). Which of the following statements best summarizes the implications of these findings?",
    "choices": [
      "A) The use of a single modality (e.g., physical activity) is sufficient for accurate cognitive health assessments in older adults.",
      "B) The removal of sensor artifacts is crucial for improving cognitive health assessments, but it does not affect the accuracy of the assessment.",
      "C) The efficiency of the evaluation method is independent of the modality used and the activity mode.",
      "D) The use of a single activity mode (e.g., single activity) and a single modality (e.g., physiological activity) can provide a significant improvement in cognitive health assessments for older adults."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Additionally, our postural activity detection approach in diverse population cum improved activity performance measurement and fundamental physiological sensor artifacts removal from physiological sensors help facilitate the automated cross-sectional cognitive health assessment of the older adults. Our efficient evaluation on each modality (physical, physiological and ambient) and each activity mode proves that any of the mode (say single activity and single sensor) also can provide significant improved cognitive health assessment measure."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is evaluating the performance of a V2I link using the proposed method to jointly detect jammer and GPS spoofer. The jammer detection probability (Pd^j) and the spoofer detection probability (Pd^s) are calculated as follows:\n\nPd^j = Pr(\u03a5_{\\tilde{X}_t^{(1)}} \u2265 \u03be_1, \u03a5_{\\tilde{X}_t^{(2)}} \u2265 \u03be_2 | H_1)\nPd^s = Pr(\u03a5_{\\tilde{X}_t^{(1)}} < \u03be_1, \u03a5_{\\tilde{X}_t^{(2)}} \u2265 \u03be_2 | H_2)\n\nGiven that \u03be_1 = E[\u03a5_{\\tilde{X}_t^{(1)}}] + 3\u221aV[\u03a5_{\\tilde{X}_t^{(1)}}] and \u03be_2 = E[\u03a5_{\\tilde{X}_t^{(2)}}] + 3\u221aV[\u03a5_{\\tilde{X}_t^{(2)}}], and assuming that the abnormality signals during training (i.e., normal situation when jammer and spoofer are absent) are normally distributed, what is the correct interpretation of the joint detection probability (Pd^j) and the spoofer detection probability (Pd^s)?",
    "choices": [
      "A) The joint detection probability (Pd^j) is equal to the probability that both \u03a5_{\\tilde{X}_t^{(1)}} and \u03a5_{\\tilde{X}_t^{(2)}} are greater than or equal to \u03be_1 and \u03be_2, respectively, given that the jammer and spoofer are absent.",
      "B) The spoofer detection probability (Pd^s) is equal to the probability that \u03a5_{\\tilde{X}_t^{(1)}} is less than \u03be_1 and \u03a5_{\\tilde{X}_t^{(2)}} is greater than or equal to \u03be_2, given that the jammer is present.",
      "C) The joint detection probability (Pd^j) is equal to the probability that both \u03a5_{\\tilde{X}_t^{(1)}} and \u03a5_{\\tilde{X}_t^{(2)}} are greater than or equal to \u03be_1 and \u03be_2, respectively, given that the jammer and spoofer are both present.",
      "D) The spoofer detection probability (Pd^s) is equal to the probability that \u03a5_{\\tilde{X}_t^{(1)}} is less than \u03be_1 and \u03a5_{\\tilde{X}_t^{(2)}} is greater than or equal to \u03be_2, given that the jammer is absent."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The second indicator calculates the similarity between the predicted GPS signal (from the RF signal) and the observed one after decoding the RF signal which is defined as:\n\\begin{equation}\\label{eq_CLA2}\n    \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} = -ln \\bigg( \\mathcal{BC} \\big(\\pi(\\mathrm{\\tilde{X}}_{t}^{(2)}),\\lambda(\\mathrm{\\tilde{X}}_{t}^{(2)}) \\big) \\bigg),\n\\end{equation}\nwhere $\\mathcal{BC}(.){=}\\int \\sqrt{\\pi(\\mathrm{\\tilde{X}}_{t}^{(2)}),\\lambda(\\mathrm{\\tilde{X}}_{t}^{(2)}})d\\mathrm{\\tilde{X}}_{t}^{(2)}$.\nDifferent hypotheses can be identified by the RSU to understand the current situation whether there is: a jammer attacking the V2I link, or a spoofer attacking the link between the satellite and the vehicle or both jammer and spoofer are absent according to:\n\\begin{equation}\n    \\begin{cases}\n        \\mathcal{H}_{0}: \\text{if} \\ \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(1)}} < \\xi_{1} \\ \\text{and} \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} < \\xi_{2}, \\\\\n        \\mathcal{H}_{1}: \\text{if} \\ \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(1)}} \\geq \\xi_{1} \\ \\text{and} \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} \\geq \\xi_{2}, \\\\\n        \\mathcal{H}_{2}: \\text{if} \\ \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(1)}} < \\xi_{1} \\ \\text{and} \\ \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} \\geq \\xi_{2},\n    \\end{cases}\n\\end{equation}\nwhere $\\xi_{1} = \\mathbb{E}[\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(1)}}] + 3\\sqrt{\\mathbb{V}[\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(1)}}]}$, and $\\xi_{2} = \\mathbb{E}[\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(2)}}] + 3\\sqrt{\\mathbb{V}[\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(2)}}]}$. In $\\xi_{1}$ and $\\xi_{2}$, $\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(1)}}$ and $\\Bar{\\Upsilon}_{\\mathrm{\\tilde{X}}_{t}^{(2)}}$ stand for the abnormality signals during training (i.e., normal situation when jammer and spoofer are absent).\n\n\\subsection{Evaluation metrics}\nIn order to evaluate the performance of the proposed method to jointly detect jammer and GPS spoofer, we adopt the jammer detection probability ($\\mathrm{P}_{d}^{j}$) and the spoofer detection probability ($\\mathrm{P}_{d}^{s}$), respectively, which are defined as:\n\\begin{equation}\n    \\mathrm{P}_{d}^{j} = \\mathrm{Pr}(\\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(1)}}\\geq \\xi_{1}, \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} \\geq \\xi_{2}|\\mathcal{H}_{1}),\n\\end{equation}\n\\begin{equation}\n    \\mathrm{P}_{d}^{s} = \\mathrm{Pr}(\\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(1)}}< \\xi_{1}, \\Upsilon_{\\mathrm{\\tilde{X}}_{t}^{(2)}} \\geq \\xi_{2}|\\mathcal{H}_{2})."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the significance of the two-stage Kondo effect in realistic setups, given that the exact particle-hole symmetry (PHS) conditions are hardly possible in real systems?",
    "choices": [
      "A) The two-stage Kondo effect is only relevant in the presence of a superconductor, as the direct exchange mechanism is insufficient to achieve a significant increase in conductance at the PHS point.",
      "B) The two-stage Kondo effect is a hallmark of the direct exchange mechanism, and its importance is diminished when the QD energy levels are fine-tuned to the PHS point.",
      "C) The two-stage Kondo effect is only significant in the presence of a superconductor, as the direct exchange mechanism is unable to achieve a significant increase in conductance at the PHS point.",
      "D) The two-stage Kondo effect remains a crucial phenomenon in realistic setups, even when the exact PHS conditions are not met, as the fine-tuning of the QD energy levels to the PHS point can still lead to a significant increase in conductance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Consequently, the underscreened Kondo effect occurs \n\\cite{Mattis,NozieresBlandin} for weak $\\GS{}$ and, {\\it e.g.}, $J=-0.1U$; \nsee the point indicated by square in \\fig{3}. This leads to $G=G_{\\mathrm{max}}$ and a peak in $\\mathcal{A}(\\omega)$, whose shape is significantly different from the\nKondo peak, cf. the curve denoted by square in the inset in \\fig{3}. \n\n\n\n\\section{Effects of detuning from the particle-hole symmetry point}\n\\label{sec:asym}\n\n\\begin{figure}\n\\includegraphics[width=0.98\\linewidth]{Fig4.pdf}\n\\caption{\n         (a) Linear conductance between the normal leads $G$ as a function of temperature $T$\n         for parameters corresponding to \\fig{G-T}(a) with $\\xi=U/10$, and additional curves for finite \n         detuning from particle-hole symmetry point, $\\delta_1=-\\delta_2$, \n         and two values of $\\xi=\\sqrt{t^2+\\GS{}^2}$, as indicated in the figure. (b) $G_{\\mathrm{min}} \\equiv G(T \\!=\\! 0)$ as a function of QD1 detuning $\\delta_1$ for different\n         exchange mechanisms, $\\xi=U/10$ and $\\delta_2=\\pm\\delta_1$ (as indicated).\n\t\t}\n\\label{fig:asym}\n\\end{figure}\n\nAt PHS $G_{\\mathrm{min}}=G(T \\!=\\! 0)=0$ in the absence of superconducting lead, making $G_{\\mathrm{min}} > 0$ a hallmark\nof SC-induced two-stage Kondo effect. However, outside of PHS point $G_{\\mathrm{min}} > 0$ even in the case of \nthe two-stage Kondo effect caused by the direct exchange. Exact PHS conditions are hardly possible in real systems, and the fine-tuning of the QD energy\nlevels to PHS point is limited to some finite accuracy. Therefore, there may appear a question, if the results obtained at PHS are of any importance for the\nrealistic setups. As we show below --- they are,\nin a reasonable range of detunings $\\delta_i=\\varepsilon_i +U/2$.\n\nIn \\fig{asym}(a) we present the $G(T)$ dependence in and outside the PHS, corresponding to \nparameters of \\fig{G-T}(a). Clearly, for considered small values of $\\delta_1=\\delta_2=\\delta$, \n$G_{\\mathrm{min}}<10^{-3}e^2/h$ for direct exchange only, while $G_{\\mathrm{min}}$ in the presence of a superconductor is \nsignificantly increased and close to the PHS value."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In a shell-model simulation of NO+ dissociation, the ambipolar expansion of the plasma is driven by the electron temperature. However, the simulation also accounts for the effect of predissociation on the yield of NO+ molecules. Which of the following statements best describes the relationship between the electron temperature, predissociation, and the resulting NO+ density profiles?",
    "choices": [
      "A) The electron temperature has a direct impact on the predissociation rate, which in turn affects the NO+ density profiles.",
      "B) The predissociation rate is independent of the electron temperature, and the resulting NO+ density profiles are solely determined by the initial density of NO+ molecules.",
      "C) The electron temperature has a negligible effect on the predissociation rate, and the resulting NO+ density profiles are primarily influenced by the initial density of NO+ molecules.",
      "D) The electron temperature and predissociation rate are both important factors in determining the resulting NO+ density profiles, with the electron temperature influencing the predissociation rate and the predissociation rate affecting the yield of NO+ molecules."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This numerical approximation contains no provision for predissociation. Coupled rate-equation simulations for uniform volumes show that predissociation depresses yield to some degree, but has less effect on the avalanche kinetics \\cite{Saquet2012}. Therefore, we can expect sets of numerically estimated shell densities, scaled to agree with the simulated ion density at the elapsed time of 100 ns to provide a reasonable account of the earlier NO$^+$ density profiles as a function of time. \\begin{figure}[h!] \\centering\n\\includegraphics[width= .4 \\textwidth]{Shell_pop}\n   \\caption{Global population fractions of particles as they evolve in the avalanche of a shell-model ellipsoidal Rydberg gas with the initial principal quantum number and density distribution of Figure \\ref{fig:shell}\n   }\n\\label{fig:shell_yields}\n\\end{figure} For each time step, the difference, $\\rho_0 - \\rho_e$ defines the neutral population of each shell. We assign a fraction of this population to surviving Rydberg molecules, such that the total population of NO$^*$ as a function of time agrees with the prediction of the shell-model simulation, as shown in Figure \\ref{fig:shell_yields}. We consider the balance of this neutral population to reflect NO$^*$ molecules that have dissociated to form N($^4$S) $+$ O($^3$P). Figure \\ref{fig:shell} plots these surviving Rydberg densities as a function of radial distance for each evolution time. At the initial density of this simulation, note at each time step that a higher density of Rydberg molecules encloses the tail of the ion density distribution in $x$.   \n\n\\subsection{Plasma expansion and NO$^+$ - NO$^*$ charge exchange as an avenue of quench} We regard the ions as initially stationary. The release of electrons creates a radial electric potential gradient, which gives rise to a force, $-e\\nabla \\phi_{k,j}(t)$, that accelerates the ions in shell $j$ in direction $k$ according to \\cite{Sadeghi.2012}:\n\n\\begin{align}\n\\frac{-e}{m'}\\nabla \\phi_{k,j}(t) = & \\frac{\\partial u_{k,j}(t)}{\\partial t} \\notag  \\\\\n= & \\frac{k_BT_e(t)}{m'\\rho_j(t)} \\frac{\\rho_{j+1}(t) - \\rho_j(t)}{r_{k,j+1}(t) - r_{k,j}(t)},\n  \\label{dr_dt}\n\\end{align}\n\n\\noindent where $\\rho_j(t)$ represents the density of ions in shell $j$.  \n\nThe instantaneous velocity, $u_{k,j}(t)$ determines the change in the radial coordinates of each shell, $r_{k,j}(t)$, \n\\begin{equation}\n\\frac{\\partial r_{k,j}(t)}{\\partial t}=u_{k,j}(t) = \\gamma_{k,j}(t) r_{k,j}(t),\n  \\label{dr_dt}\n\\end{equation}\n\\noindent which in turn determines shell volume and thus its density, $ \\rho_j(t)$.   \nThe electron temperature supplies the thermal energy that drives this ambipolar expansion."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a signal x(t) with a spectral density f(t) that is estimated using the variance stabilizing transform (VST) with a smoothing parameter h. The estimated spectral density is then used to compute the autocovariance matrix \u03a3. However, the estimation procedure for \u03a3 is sensitive to the choice of the penalty order q. Which of the following statements is true about the estimation of \u03a3?",
    "choices": [
      "A) The estimation of \u03a3 is independent of the choice of q, and the resulting matrix is always positive definite.",
      "B) The estimation of \u03a3 is sensitive to the choice of q, and the resulting matrix is only positive definite when q is equal to the number of lags.",
      "C) The estimation of \u03a3 is sensitive to the choice of q, and the resulting matrix is only positive definite when q is greater than or equal to the number of lags.",
      "D) The estimation of \u03a3 is independent of the choice of q, and the resulting matrix is always positive definite when the penalty order q is chosen using generalized cross-validation (GCV)."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Next, applying the variance stabilizing transform (VST) \u223c where H(y) = {\u03c6(m/2) + log (2y/m)} / \u221a 2 and \u03c6 is the digamma function (see . Now, the scaled and shifted log-spectral density H(f ) can be estimated with a periodic smoothing spline\nwhere h > 0 denotes a smoothing parameter, q \u2208 N is the penalty order and S per (2q \u2212 1) a space of periodic splines of degree 2q \u2212 1. The smoothing parameter h can be chosen either with generalized cross-validation (GCV) as derived in or with the restricted maximum likelihood, see . Once an estimator H(f ) is obtained, application of the inverse transform function H \u22121 (y) = m exp \u221a 2y \u2212 \u03c6 (m/2) /2 yields the spectral density estimator f = H \u22121 H(f ) . Finally, using the inverse Fourier transform leads to the fol- The precision matrix \u2126 is estimated by the inverse Fourier transform of the reciprocal of the spectral density estimator, i.e., \u03a9 = (\u03c9 |i\u2212j| ) p i,j=1 with \u03c9k = The estimation procedure for \u03a3 and \u03a9 can be summarised as follows. 1. Data Transformation:\nwhere D is the (p \u00d7 p)-dimensional DCT-I matrix as given in Lemma 1 and D j is its j-th column. 2. Binning: Set T = p \u03c5 for any \u03c5 \u2208 ((4 \u2212 2 min{\u03b2, 1})/3, 1) and calculate W i,j , k = 1, . . . , T.\n\nVST:\n\nwhere k are asymptotically i.i.d. Gaussian variables. Inverse VST: Estimate the spectral density f with f = H \u22121 H(f ) , where Note that \u03a3 and \u03a9 are positive definite matrices by construction, since their spectral density functions f and f \u22121 are non-negative, respectively. Unlike the banding and tapering estimators, the autocovariance estimators \u03c3k are controlled by a single smoothing parameter h, which can be estimated fully data-driven with several available automatic methods, which are numerically efficient and well-studied. In addition, one can also use methods for adaptive mean estimation, see e.g., , which in turn leads to adaptive Toeplitz covariance matrix estimation. All inferential procedures developed in the Gaussian regression context can also be adopted accordingly. Theoretical Properties\n\nIn this section, we study the asymptotic properties of the estimators f , \u03a3 and \u03a9."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The D\\O\\ experiment reports the first observation of the excited $B$ mesons $B_1$ and $B^*_2$ as two separate states in fully reconstructed decays to $B^{(*)}\\pi$. The mass of $B_1$ is measured to be $5724\\pm4\\pm7$ MeV/c$^2$, and the mass difference $\\Delta M$ between $B^*_2$ and $B_1$ is $23.6\\pm7.7\\pm3.9$ MeV/c$^2$. What can be inferred about the relative masses of $B_1$ and $B^*_2$?",
    "choices": [
      "A) The mass of $B_1$ is approximately 23.6 MeV/c$^2$ less than the mass of $B^*_2$.",
      "B) The mass of $B^*_2$ is approximately 5724 MeV/c$^2$.",
      "C) The mass of $B_1$ is approximately 23.6 MeV/c$^2$ more than the mass of $B^*_2$.",
      "D) The mass of $B^*_2$ is approximately 5724 MeV/c$^2$."
    ],
    "correct_answer": "C)",
    "documentation": [
      "The measurements of the $B_s$ and $\\Lambda_b$ (Fig. \\ref{fig:masslb})\nmasses are the current world's best.\\\\\n\n$m(B^+)$ = 5279.10$\\pm$0.41$(stat)\\pm$0.36$(syst)$,\n\n$m(B^0)$ = 5279.63$\\pm$0.53$(stat)\\pm$0.33$(syst)$,\n\n$m(B_s)$ = 5366.01$\\pm$0.73$(stat)\\pm$0.33$(syst)$,\n\n$m(\\Lambda_b)$ = 5619.7$\\pm$1.2$(stat)\\pm$1.2$(syst)$ MeV/$c^2$.\\\\\n\n\n\\begin{figure}[htb]\n\\vspace*{-1mm}\n\\includegraphics[height=0.30\\textheight,width=7.5cm]  {lambdav1c.eps}\n\\vspace*{-1cm}\n\n\\caption{The mass spectrum of $\\Lambda_b$ candidates (CDF).}\n\\label{fig:masslb}\n\\end{figure}\n\n\nD\\O\\ reports the first observation of the excited $B$ mesons \n$B_1$ and $B^*_2$ as two separate states in fully reconstructed\ndecays to $B^{(*)}\\pi$. The mass of $B_1$ is measured to be\n5724$\\pm$4$\\pm$7 MeV/c$^2$, and the mass difference $\\Delta M$ between\n$B^*_2$ and $B_1$ is 23.6$\\pm$7.7$\\pm$3.9 MeV/c$^2$\n(Fig. \\ref{fig:d0_bexc}). D\\O\\ observes semileptonic $B$ decays to narrow $D^{**}$ states,\nthe orbitally excited states  of the $D$ meson\nseen as resonances in the $D^{*+}\\pi^-$ invariant mass spectrum. The $D^*$ mesons are reconstructed through the decay sequence \n$D^{*+} \\rightarrow D^0\\pi^+$, $D^0\\rightarrow K^-\\pi^+$.\nThe invariant mass  of oppositely charged $(D^*,\\pi)$ pairs\nis shown in Fig. \\ref{fig:d0_dstst}. The mass peak between 2.4 and 2.5 GeV/$c^2$ can be interpreted as two merged \nnarrow $D^{**}$ states, $D^0_1(2420)$ and $D^0_2(2460)$.\nThe combined branching fraction is \n$ {\\cal B}(B\\rightarrow D^0_1,D^0_2)\\cdot {\\cal B}(D^0_1,D^0_2\\rightarrow D^{*+}\\pi^-)=(0.280\\pm0.021(stat)\\pm0.088(syst)$\\%. The systematic error includes the unknown phase between the\ntwo resonances. Work is in progress on extracting the two Breit-Wigner\namplitudes. \\begin{figure}[htb]\n\\vspace*{-2mm}\n\\hspace*{-3mm}\n\\includegraphics[height=0.28\\textheight,width=8.3cm]  {B08F02.eps}\n\n\\vspace*{-1cm}\n\\caption{Mass difference $\\Delta M = M(B\\pi)-M(B)$ for exclusive $B$ decays. The background-subtracted signal is a sum of \n$B^*_1 \\rightarrow B^* \\pi$, $B^* \\rightarrow B \\gamma $ (open area)\nand $B^*_2 \\rightarrow B^*\\pi$ $B^*\\rightarrow B \\gamma$ (lower peak in the shaded area)\nand $B^*_2 \\rightarrow B \\pi$ (upper peak in the shaded area)  \n(D\\O).}\n\\label{fig:d0_bexc}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\includegraphics[height=0.25\\textheight,width=7.5cm]  {B05F03.eps}\n\n\\vspace*{-1cm}"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the shutdown of the Norpipe H-7 compressor platform in October 2007?",
    "choices": [
      "A) The pipeline's capacity was exceeded, resulting in a surge in gas pressure that damaged the compressor.",
      "B) The platform's maintenance schedule had expired, and repairs were not feasible within the given timeframe.",
      "C) The rig's production from the wellheads was shut down due to a sudden increase in oil prices, causing a temporary imbalance in the gas supply.",
      "D) The platform's shutdown was a result of the cumulative effect of 20% of the first year's production being suspended due to weather conditions, leading to a gradual decline in gas pressure that exceeded the platform's capacity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The problem with this approach arose when weather conditions meant the tankers had to cast off from the buoys because of strong winds or high waves. The rig then had to shut down production from the wellheads immediately. Given the weather conditions found on Ekofisk, output regularly had to cease. Production was suspended for 20 per cent of the first year for this reason. Output began cautiously on 8 July 1971 from a single well. The second producer came on stream that September, the third was ready the following month and all four were producing by February 1972. They each flowed 10 000 barrels of oil per day. Source: Kvendseth, Stig, Giant discovery, 1988. Published 9. April 2019 \u2022 Updated 25. October 2019\nNorpipe H-7 This platform served as a pumping/compressor station to maintain pressure in the 443-kilometre Norpipe gas pipeline from Ekofisk to Emden in Germany, which became operational in September 1977. Kjappe fakta:: Compressor platform on Ekofisk-Emden gas pipeline\nInstalled 1976\nOperational 1977\nShut down 29 October 2007 Removed 2013\n\u2014 Norpipe GNSC-H7. Photo: Husmo Foto/Norwegian Petroleum Museum\nGas received initial compression to 132 bar at the Ekofisk Complex. The pipeline was divided into three equal lengths, with Norpipe GNSC B11 positioned at the end of the first third to maintain pressure as and when required. From there, the gas then travelled the next third of the distance to the second and virtually identical compressor platform, H7. This was also responsible for maintaining pressure, but additional compression was seldom required on this final leg of the journey to Emden. Both platforms stood on the German continental shelf, but 48 kilometres of the pipeline also ran across the Danish North Sea sector. The pipeline is trenched or covered with sand. On its final approach to the coast of East Friesland, it passes beneath the island of Juist before making landfall north of Emden. Capacity in Norpipe is about 60 million standard cubic metres (scm) or 2.1 billion cubic feet per day."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the performance of denoised second-order Trotter supercircuits for a fixed evolution time t found that the correlation function for the middle site exhibits a significant improvement when the denoiser depth M is increased from 1 to 2. However, the same study also showed that the OTOC, defined in the main text, converges to the exact result as a function of M for long evolution times. Which of the following conclusions can be drawn from these results?",
    "choices": [
      "A) Increasing the denoiser depth M from 1 to 2 leads to a significant improvement in the correlation function for the middle site, but the OTOC does not converge to the exact result for long evolution times.",
      "B) Increasing the denoiser depth M from 1 to 2 leads to a significant improvement in the OTOC, but the correlation function for the middle site does not exhibit a significant improvement.",
      "C) Increasing the denoiser depth M from 1 to 2 leads to a significant improvement in the correlation function for the middle site, but the OTOC converges to the exact result for long evolution times only when the Trotter depth M_trot is increased from 8 to 16.",
      "D) Increasing the denoiser depth M from 1 to 2 leads to a significant improvement in both the correlation function for the middle site and the OTOC, which converges to the exact result as a function of M for long evolution times."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We optimize the denoiser for a Trotter supercircuit for a fixed evolution time t. Then, to reach later times, we stack the denoised supercircuit n times to approximate the evolution up to time nt: In Fig. we stack a denoised t = 1 supercircuit up to n = 20 times and calculate the correlation function, defined in the main text, for the middle site. We consider Trotter depths M trot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8, for a L = 14 Heisenberg chain with p = 0.01 depolarizing two-qubit noise. The noisy results correspond to M = 0 and the noiseless results to p = 0. In Fig. we calculate the OTOC, defined in the main text, with stacked time evolution for a denoised t = 2 supercircuit with M trot = 32 and M = 2, stacked up to ten times. We see that the stacked supercircuit performs very well, and the additional precision obtained by using deep denoisers (M = 8) pays off for long evolution times, where we see convergence to the exact result (black dashed lines in Fig. ) as a function of M . FIG. . The two-point z-spin correlator C zz i=L/2,j=L/2 (t) of a spin on the middle site at times 0 and t, for the infinite temperature initial state, for denoised second-order Trotter supercircuits that are optimized at evolution time t = 1 and then stacked up to twenty times. We use Trotter depths Mtrot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8. The calculations were performed for a periodic Heisenberg model with L = 14 and PBC, affected by two-qubit depolarizing noise with strength p = 0.01, which also affects the denoiser. The non-denoised results are labelled with M = 0, and the noiseless results with p = 0. The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively. The costliest and most noise-susceptible operation is the two-qubit ZZ rotation with angle \u03b1, which is the foundation of the unitary piece in our channel parameterization, defined in the main text. For completeness, we here present the \u03b1 angles of the optimized denoisers."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A social media platform's algorithm is designed to learn a user's interests by analyzing their past interactions with content. This process is based on the idea that users are more likely to engage with content that is relevant to their interests. However, the algorithm also takes into account the user's friends' interactions with content, as well as the content that their friends have liked or shared. This information is used to infer that the user is interested in a particular topic, even if they have not explicitly interacted with it. The algorithm also generates a model that contains global meta-information about the user's consumption patterns, including how frequently they consume content from a particular channel and how likely they are to reshare items.",
    "choices": [
      "A) The algorithm only considers the user's past interactions with content when making predictions about their interests.",
      "B) The algorithm assumes that if a user's friends like or share content, the user is also interested in it, regardless of the user's own interactions with the content.",
      "C) The algorithm only updates the user's model every week, based on the user's past interactions with content.",
      "D) The algorithm uses a combination of the user's past interactions with content, their friends' interactions with content, and global meta-information about the user's consumption patterns to make predictions about their interests."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Inferred information takes into account a user's activities. The model generation engine 207 will infer that a user is interested in a particular subject, for example, if the subject matter appears in search terms. For example, the model generation engine 207 infers that a user who searches for information about different types of butterflies is interested in butterflies. The model generation engine 207 can even infer information based on the user's friends' activities. For example, content items that interest the user's friends might also interest the user. As a result, in one embodiment, the model includes the user's friends' interests. In one embodiment, the model generation engine 207 also generates a model that contains several pieces of global meta-information about the user's consumption patterns including how frequently the user consumes the stream of content of a channel and global statistics on how likely the user is to reshare various types of items. Lastly, the model includes a sequence of weights and multipliers that are used to make predictions about the user's likelihood of clicking on, sharing or otherwise engaging with stream items. The model generation engine 207 generates the model from the user information across the heterogeneous data sources. In one embodiment, the model generation engine 207 builds extensions to the model that employ the patterns of behavior of other users. For example, the model predicts the user's behavior based on the reaction of similar users. All the data that is derived from other users is anonymized before it is incorporated into the model. In one embodiment, the model generation engine 207 generates a model based on user information, for example, based on the user's search history or third-party accounts. Alternatively, the model generation engine 207 receives periodic updates (one hour, one day, one week, etc.) from the heterogeneous data sources and in turn updates the model. In yet another embodiment, the model generation engine 207 generates a model each time it receives a request for generating a stream of content for a channel."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company is considering a new marketing strategy that involves co-branding with a popular social media platform. The company's agreement with the platform includes a clause that prohibits the platform from using the company's logo or trademark in any way that could be considered co-branding. However, the platform's terms of service also state that users are allowed to share content from the platform on their own social media profiles, including images with the company's logo. Which of the following statements is true?",
    "choices": [
      "A) The company's agreement with the platform prohibits the platform from sharing any content featuring the company's logo on its own social media profiles.",
      "B) The company's agreement with the platform allows the platform to share content featuring the company's logo on its own social media profiles, as long as the content is not used in a way that could be considered co-branding.",
      "C) The company's agreement with the platform prohibits the platform from sharing any content featuring the company's logo on its own social media profiles, but allows the company to share its own content featuring the platform's logo on its own social media profiles.",
      "D) The company's agreement with the platform prohibits the platform from sharing any content featuring the company's logo on its own social media profiles, and also prohibits the company from sharing any content featuring the platform's logo on its own social media profiles."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Marks not owned by Broadjam or its subsidiaries are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Broadjam. Other trademarks, service marks, logos, labels, product names and service names appearing in Material posted on the Site and not owned by Broadjam or its organizational affiliates, are the property of their respective owners. You agree not to copy, display or otherwise use any Marks without Broadjam's prior written permission. The Marks may never be used in any manner likely to cause confusion, disparage or dilute the Marks and/or in connection with any product or service that is not authorized or sponsored by Broadjam. (j) You may not remove or alter, or cause to be removed or altered, any copyright, trademark, trade name, service mark, or any other proprietary notice or legend appearing on the Site. Co-Branding. You may not co-brand the Site. For purposes of this Agreement, \"co-branding\" means to display a name, logo, trademark, or other means of attribution or identification of Broadjam in such a manner as is reasonably likely to give the impression that you have the right to display, publish,or distribute the Site or content accessible within the Site, including but not limited to Materials. You agree to cooperate with Broadjam in causing any unauthorized co-branding immediately to cease. You may not frame or use framing techniques to enclose any Broadjam trademark, logo, or other proprietary information (including but not limited to images, text, page layout, and form) without Broadjam's express written consent. You may not use any metatags or any other \"hidden text\" using Broadjam's name or trademarks without Broadjam's express written consent. Any such unauthorized use shall result in the immediate and automatic termination of all permission, rights and/or licenses granted to you by Broadjam and may also result in such additional action as Broadjam deems necessary to protect and enforce its legal rights."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the relationship between protein structure and function found that the partial least squares (PLS) algorithm can effectively identify atomic dynamics relevant to protein function. However, the same study noted that the performance of the PLS algorithm can be improved by decorrelating the data. What is the primary advantage of decorrelating the data in the context of the PLS algorithm?",
    "choices": [
      "A) Decorrelating the data reduces the dimensionality of the data, making it easier to identify patterns and relationships.",
      "B) Decorrelating the data increases the correlation between the true protein structure and its predicted function, leading to more accurate predictions.",
      "C) Decorrelating the data allows for the estimation of the covariance matrix of the data, which is essential for the PLS algorithm to work effectively.",
      "D) Decorrelating the data enables the use of the ensemble-weighted maximally correlated mode (ewMCM) estimator, which is a key component of the PLS algorithm."
    ],
    "correct_answer": "D)",
    "documentation": [
      "have shown that the partial least squares (PLS) algorithm performs exceptionally well on this type of data, leading to a small-dimensional and robust representation of proteins, which is able to identify the atomic dynamics relevant for Y . Singer et al. ( ) studied the convergence rates of the PLS algorithm for dependent observations and showed that decorrelating the data before running the PLS algorithm improves its performance. Since Y is a linear combination of columns of X, it can be assumed that Y and all columns of X have the same correlation structure. Hence, it is sufficient to estimate \u03a3 = cov(Y ) to decorrelate the data for the PLS algorithm, i.e., \u03a3 \u22121/2 Y = \u03a3 \u22121/2 X\u03b2 + \u03a3 \u22121/2 results in a standard linear regression with independent errors. Our goal now is to estimate \u03a3 and compare the performance of the PLS algorithm on original and decorrelated data. For this purpose, we divided the data set into a training and a test set (each with p = 10 000 observations). First, we tested whether the data are stationary. The augmented Dickey-Fuller test confirmed stationarity for Y with a p-value< 0.01. The Hurst exponent of Y is 0.85, indicating moderate long-range dependence supported by a rather slow decay of the sample autocovariances (see grey line in the left plot of Figure ). Therefore, we set q = 1 for the VST-DCT estimator to match the low smoothness of the corresponding spectral density. Moreover, the smoothing parameter is selected with the restricted maximum likelihood method and T = 550 bins are used. Obviously, the performance of the PLS algorithm on the decorrelated data is significantly better for the small number of components. In particular, with just one PLS component, the correlation between the true opening diameter on the test set and its prediction that takes into account the dependence in the data is already 0.54, while it is close to zero for PLS that ignores the dependence in the data. showed that the estimator of \u03b2 based on one PLS component is exactly the ensemble-weighted maximally correlated mode (ewMCM), which is defined as the collective mode of atoms that has the highest probability to achieve a specific alteration of the response Y ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The experimental results on the generation of 5 from 6 on insulating surfaces suggest that the open-shell ground state is a result of the cleavage of one C-H bond at each of the pentagonal apices of 6. However, the STM data on this compound also indicate that the results do not imply the ground state of unsubstituted 5. What is the most likely reason for the existence of two ground states (open-shell and closed-shell) of 5 on insulating surfaces?",
    "choices": [
      "A) The cleavage of one C-H bond at each of the pentagonal apices of 6 leads to a change in the molecular orbital densities, resulting in the open-shell ground state.",
      "B) The STM data on this compound indicate that the open-shell ground state is a result of the cleavage of one C-H bond at each of the pentagonal apices of 6, but the results do not imply the ground state of unsubstituted 5 because the cleavage of C-H bonds is not a necessary condition for the open-shell state.",
      "C) The high-resolution AFM imaging with bond-order discrimination and STM imaging of molecular orbital densities reveal that the two geometries of 6 correspond to the open-or and closed-shell states, but the STM images at ionic resonances show molecular orbital densities corresponding to SOMOs for the open-shell geometry, but orbital densities of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) for the closed-shell geometry.",
      "D) The experimental results are in good agreement with density functional theory (DFT) and multireference perturbation theory calculations, which suggest that the existence of two ground states (open-shell and closed-shell) of 5 on insulating surfaces is a result of the interplay between the cleavage of C-H bonds and the molecular orbital densities, and the switching between open-and closed-shell states of a single molecule by changing its adsorption site on the surface."
    ],
    "correct_answer": "D)",
    "documentation": [
      "On-surface generation of a derivative of 5, starting from truxene as a precursor, was recently reported . STM data on this compound, containing the indeno[1,2-a]fluorene moiety as part of a larger PCH, was interpreted to indicate its open-shell ground state. However, the results did not imply the ground state of unsubstituted 5. Here, we show that on insulating surfaces 5 can exhibit either of two ground states: an open-shell or a closed-shell. We infer the existence of these two ground states based on high-resolution AFM imaging with bond-order discrimination and STM imaging of molecular orbital densities . AFM imaging reveals molecules with two different geometries. Characteristic bond-order differences in the two geometries concur with the geometry of either an open-or a closed-shell state. Concurrently, STM images at ionic resonances show molecular orbital densities corresponding to SOMOs for the open-shell geometry, but orbital densities of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) for the closed-shell geometry. Our experimental results are in good agreement with density functional theory (DFT) and multireference perturbation theory calculations. Finally, we observe switching between open-and closed-shell states of a single molecule by changing its adsorption site on the surface. Synthetic strategy toward indeno[1,2-a]fluorene. The generation of 5 relies on the solution-phase synthesis of the precursor 7,12-dihydro indeno[1,2-a]fluorene (6). Details on synthesis and characterization of 6 are reported in Supplementary Figs.\n. Single molecules of 6 are deposited on coinage metal (Au(111), Ag(111) and Cu(111)) or insulator surfaces. In our work, insulating surfaces correspond to two monolayer-thick (denoted as bilayer) NaCl on coinage metal surfaces. Voltage pulses ranging between 4-6 V are applied by the tip of a combined STM/AFM system, which result in cleavage of one C-H bond at each of the pentagonal apices of 6, thereby leading to the generation of 5 (Fig. )."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent report by the US State Department has sparked controversy over the role of external influence in Iraq's new government. According to Philip Crowley, a US State Department official, the US has not \"dictated the terms of the government\". However, some critics argue that the US has been exerting significant pressure on Iraq through its military presence and economic aid. Others point out that the US has been providing significant support to the Iraqi military, which could be seen as an attempt to exert influence. What is the most plausible explanation for the US's involvement in Iraq's new government?",
    "choices": [
      "A) The US is providing significant economic aid to Iraq, which is being used to influence the government's policies.",
      "B) The US has been exerting significant pressure on Iraq through its military presence, which is being used to shape the government's decisions.",
      "C) The US has been providing significant support to the Iraqi military, which is being used to stabilize the country and prevent sectarian violence.",
      "D) The US has been working closely with Iraq's new government to ensure that it aligns with US foreign policy goals, including promoting democracy and stability in the region."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\"There is no holiday spirit. All we have is fear,\" she said. This holiday will instead mark another year without news from her 46-year-old son, who was kidnapped outside Baghdad in late 2006.From Turkey, Sebnem Arsu (New York Times -- link has text and video) notes the increase in Iraq refugees to the country since October 31st and quotes Father Emlek stating, \"I've never seen as many people coming here as I have in the last few weeks. They also go to Lebanon, Jordan and Syria but it seems that Turkey is the most popular despite the fact that they do not speak the language.\" Jeff Karoub (AP) reports on the small number of Iraqi refugees who have made it to the US and how some of them \"struggle with insomnia, depression and anxiety. \"One group in Iraq who can openly celebrate Christmas are US service members who elect to. Barbara Surk (AP) reports that tomorrow Chief Warrant Officer Archie Morgan will celebrate his fourth Christmas in Iraq and Captain Diana Crane is celebrating her second Christmas in Iraq: \"Crane was among several dozen troops attending a Christmas Eve mass in a chapel in Camp Victory, an American military base just outside Baghdad.\" Marc Hansen (Des Moines Reigster) speaks with six service members from Iowa who are stationed in Iraq. Sgt 1st Class Dennis Crosser tells Hansen, \"I certainly understand from reading the paper what's going on in Afghanistan and the attention definitely needs to be on the troops there. But everyone serving here in Operation New Dawn appreciates a little bit of attention as we finish this up. \"Today Jiang Yu, China's Foreign Minister, issued the following statement, \"We welcome and congratulate Iraq on forming a new government. We hope that the Iraqi Government unite all its people, stabilize the security situation, accelerate economic reconstruction and make new progress in building its country.\" James Cogan (WSWS) reports:US State Department official Philip Crowley declared on Wednesday that Washington had not \"dictated the terms of the government\"."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the ability of convolutional neural networks (CNNs) to perceive shapes and forms in images. They find that CNNs can learn to group regions with similar shapes and forms, even in the absence of color and texture cues. However, they also notice that the CNNs tend to favor \"good continuation\" - the tendency to extend a shape or form in a consistent direction - over \"similarity\" - the tendency to group regions with similar patterns. Which of the following statements best describes the researcher's findings?",
    "choices": [
      "A) The CNNs are able to learn both \"good continuation\" and \"similarity\" cues, but the latter is more important in certain scenarios.",
      "B) The CNNs are only able to learn \"good continuation\" cues, and are unable to learn \"similarity\" cues.",
      "C) The CNNs are able to learn both \"good continuation\" and \"similarity\" cues, but the former is more important in scenarios where color and texture are present.",
      "D) The CNNs are able to learn both \"good continuation\" and \"similarity\" cues, and are able to adapt to different scenarios based on the presence or absence of color and texture cues."
    ],
    "correct_answer": "D)",
    "documentation": [
      "contour completion problem . Different types of lines and curves have been studied to maximize the connectivity of two broken ends in the planer contour completion problem. Geometry-based constraints can be utilized to address some challenges of contour completion problems, such as smoothness and curvature consistency . However, such approaches only work for simple, smooth contours and usually fail in more complex settings. On the other hand, we currently have deep models that could easily take an incomplete image and complete the missing regions using enough training data . The amazing capability of such models especially those that are trained on different modalities with millions or billions of training data raises the question of whether we need such a large amount of training to perceive all the visual cues that are present in an image, which underlies visual perception by humans. In human vision, Gestalt psychology suggests that our brain is designed to perceive structures and patterns that are grouped by some known rules. In this work, we show that some perceptual structures can also be learned from the image itself directly using architectures that enable such learning. Earlier work has shown  This is an extraordinary capability of our human brain and in this paper, we tried to see whether convolutional neural networks can show such capabilities. that some forms of perceptual grouping can be achieved using computational models, such as stochastic completion fields . This type of learning resonates with some of the Gestalt perceptual grouping principles including \"proximity\", \"good continuation\" and \"similarity\". In scenarios where color and/or texture are present, the cue of \"similarity\" helps us group regions with consistent patterns . When color and texture are present, they provide a collection of rich information for such cues. In the present article, we probe convolutional neural networks in a scenario where both are absent, and the neural network is dealing with just forms and shapes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A medical historian studying the 18th-century London anatomical scene might infer that the dual function of a building as both domestic accommodation and a site for lecturing and dissection was a common practice among medical professionals of the time. Which of the following statements best supports this inference?",
    "choices": [
      "A) The fact that John Leake, a man-midwife, occupied a nearby building and advertised lectures on the art of making preparations for childbirth, suggesting a blurring of professional boundaries.",
      "B) The presence of a museum and a complete theatre in the same building, as described in an advertisement for the lease of 27 Craven Street, implies that the building was primarily used for educational purposes.",
      "C) The fact that Benjamin Franklin, a prominent figure in the medical community, lived in a building with a museum and a complete theatre, and was known to have been involved in the study of human dissection, suggests that he was a pioneer in the field of anatomical education.",
      "D) The description of the building at 27 Craven Street as \"a genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses\u2026consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences\" implies that the building was primarily used for domestic purposes, with the museum and theatre serving as secondary facilities."
    ],
    "correct_answer": "D)",
    "documentation": [
      "http://babel.hathitrust.org/cgi/pt?id=wu.89072985302;view=1up;seq=4;size=150 [33] The Journal of the Society of Arts, Vol. LXII, No. 3,183, (Nov. 21, 1913): 18.\nhttp://babel.hathitrust.org/cgi/pt?id=mdp.39015058422968;view=1up;seq=26\n[36] Allen, \u201cDear and Serviceable,\u201d 263-264. [37] Papers of Benjamin Franklin, 19:20. [38] Thomas Joseph Pettigrew, F. L. S., Memoirs of the Life and Writings of the Late John Coakley Lettsom With a Selection From His Correspondence, Vol. I, (London: Nichols, Son, and Bentley, 1817), 144 of Correspondence. [39] Papers of Benjamin Franklin, 19:321b. [40] Ibid., 19:314. [41] Ibid., 19:353a. [43] Simon David John Chaplin, John Hunter and the \u2018museum oeconomy\u2019, 1750-1800, Department of History, King\u2019s College London. Thesis submitted for the degree of Doctor of Philosophy of the University of London., 202. \u201cFollowing Falconar\u2019s death [1778] the lease [27 Craven Street] was advertised, and the buildings were described as:\nA genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses\u2026consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences. (Daily Advertiser, 27 August 1778)\u201d [44] Simon Chaplin, \u201cDissection and Display in Eighteenth-Century London,\u201d in Anatomical Dissection in Enlightenment England and Beyond: Autopsy, Pathology and Display, ed. Dr. Piers Mitchell, (Burlington: Ashgate Publishing Company, 2012), 108. \u201cGiven that a nearby building at 35 [ No. 26 in Franklin\u2019s time] was occupied by the man-midwife John Leake, who advertised lectures \u2013 including lessons in the art of making preparations \u2013 at his \u2018theatre\u2019 between 1764 and 1788, it is possible that some facilities were shared. In both cases, however, the buildings [Leake\u2019s residence at No. 26 and Hewson\u2019s residence next door at 27] served a dual function as domestic accommodation and as sites for lecturing and dissection.\u201d [45] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xviii."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study investigating the relationship between vitamin K supplementation in newborns and childhood cancer found that the risk of cancer was significantly lower in infants who received vitamin K supplementation compared to those who did not. However, the study also found that the risk of cancer was not significantly lower in breastfed infants who received vitamin K supplementation compared to those who did not. Which of the following statements best summarizes the findings of this study?",
    "choices": [
      "A) Vitamin K supplementation in newborns significantly reduces the risk of childhood cancer, regardless of breastfeeding status.",
      "B) Vitamin K supplementation in newborns has no significant impact on the risk of childhood cancer, and breastfeeding status does not affect this risk.",
      "C) Vitamin K supplementation in newborns significantly reduces the risk of childhood cancer, but only in breastfed infants.",
      "D) Vitamin K supplementation in newborns significantly reduces the risk of childhood cancer, but only in infants who are not breastfed."
    ],
    "correct_answer": "B)",
    "documentation": [
      "Vitamin K2 concentrations in human milk appear to be much lower than those of vitamin K1. Occurrence of vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25\u20131.7%, with a prevalence of 2\u201310 cases per 100,000 births.[72] Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency. Bleeding in infants due to vitamin K deficiency can be severe, leading to hospitalization, blood transfusions, brain damage, and death. Supplementation can prevent most cases of vitamin K deficiency bleeding in the newborn. Intramuscular administration is more effective in preventing late vitamin K deficiency bleeding than oral administration.[73][74]\nAs a result of the occurrences of vitamin K deficiency bleeding, the Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5\u20131 mg of vitamin K1 be administered to all newborns shortly after birth.[74]\nIn the UK vitamin K supplementation is recommended for all newborns within the first 24 hours.[75] This is usually given as a single intramuscular injection of 1 mg shortly after birth but as a second-line option can be given by three oral doses over the first month.[76]\nControversy arose in the early 1990s regarding this practice, when two studies suggested a relationship between parenteral administration of vitamin K and childhood cancer,[77] however, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two.[78] Doctors reported emerging concerns in 2013,[79] after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose. In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet.[80] He initially replicated experiments reported by scientists at the Ontario Agricultural College (OAC).[81] McFarlane, Graham and Richardson, working on the chick feed program at OAC, had used chloroform to remove all fat from chick chow."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If Curt Weldon's meeting with Moammar Gadhafi were to be successful, it could potentially lead to a significant shift in the US-Libya relations, but it also raises concerns about the potential consequences of engaging with a leader who has been accused of human rights abuses. What are the potential risks and benefits of such a meeting?",
    "choices": [
      "A) The meeting could lead to a significant increase in US investment in Libya's education sector, which could have a positive impact on the country's economic development.",
      "B) The meeting could lead to a significant increase in US military presence in Libya, which could have a negative impact on the country's stability and security.",
      "C) The meeting could lead to a significant increase in US diplomatic efforts to address human rights abuses in Libya, which could have a positive impact on the country's human rights record.",
      "D) The meeting could lead to a significant shift in the US-Libya relations, potentially leading to increased cooperation on issues such as counter-terrorism and economic development, but also raises concerns about the potential consequences of engaging with a leader who has been accused of human rights abuses."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Her approval rating has plunged to 17 percent. Black chaired \"First\" magazine before overseeing the nation's largest school system. Deputy Mayor Dennis Walcott will replace her. And a war of words is erupting between an emerging Republican star, New Jersey Governor Chris Christie, and his state's largest teachers' union. In a network TV interview, Christie called the union leaders, quote, \"political thugs.\" He blames them for teacher lay-offs that he says could have been avoided if they had not opposed salary freezes. The New Jersey Education Association is firing back, accusing Christie of name-calling -- Wolf.\nBLITZER: Sticks and stones will break many bones.\nSYLVESTER: Sticks and stones may break my bones --\nSYLVESTER: But words never hurt me. A former U.S. Congressman is in Tripoli, Libya right now. His goal -- to talk to Moammar Gadhafi. His message -- we'll talk about that. My interview with Curt Weldon coming up next. Plus, we showed it to you earlier -- a member of Congress telling colleagues to, quote, \"go to hell. \"\nNow she's is joining us live here in THE SITUATION ROOM to explain. HOLMES NORTON: -- of Columbia. It's another thing to drop a bomb on a city. And that's what this --\nBLITZER: Former Congressman Curt Weldon is in a -- Weldon is on a mission to Libya right now to try to meet with the embattled leader, Moammar Gadhafi. But that may be easier said than done. Joining us now from Tripoli, former Republican Congressman Curt Weldon of Pennsylvania. Congressman, thanks very much for coming in. And joining us now from Tripoli, former Republican Congressman Curt Weldon of Pennsylvania. CURT WELDON, FORMER U.S. CONGRESSMAN: My pleasure, Wolf.\nBLITZER: Let's talk about your meeting with Moammar Gadhafi. I take it it has not yet happened. Do you expect to meet with the Libyan leader? WELDON: Absolutely. The invitation that was sent to me was from his chief of staff, Bashir Salah, who I've met on all three of my official visits here in 2004 and 2005. And the letter specifically says we want you to come over and meet with the leader and our senior leadership."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The analysis of the UVOT and XRT data suggests that the UVW2 flux is not consistent with a single black-body model. What is the most likely explanation for this discrepancy?",
    "choices": [
      "A) The UVW2 filter was not properly calibrated, resulting in an overestimation of the flux.",
      "B) The XRT-derived black body model is not applicable to the UVW2 data due to differences in spectral shape.",
      "C) The UVW2 data is contaminated by instrumental noise, which is not accounted for in the XRT model.",
      "D) The UVW2 flux is dominated by a high-energy component that is not described by the XRT model, which assumes a single black-body spectrum."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\begin{figure}\n\\centering\n\\includegraphics[bb=80 70 535 380,clip,width=8.7cm]{12082f6.ps}\n\\caption{Variation of the UVW2 magnitude of the bright UV source\n  during the Swift observations. The same time axis as\n  Fig.\\,\\ref{lightcurve} has been used to aid comparison, and a zoom\n  is also shown. The UVW2 filter was only employed during observations\n  00030895002, 00030895004, 00030895005, 00030895006 \\& 00030895007\n  (hence the points span the dates 07/03/07 to 22/03/07). The errors here are 1-$\\sigma$. }\n\\label{uvotlc}\n\\end{figure}\n\nIt is possible to include the UVOT-detected flux with the XRT spectrum\ndescribed in Section~3. UVOT files, created using {\\em uvot2pha} for\nthe five observations (00030895002, 00030895004, 00030895005,\n00030895006 \\& 00030895007) where the UVW2 filter was employed, were\nincorporated into {\\em xspec}, along with the appropriate response\nfile (swuw2\\_20041120v104.rsp) from the Swift-XRT Calibration\nDatabase. We attempted to fit a single black-body spectrum to the\nSwift-XRT+UV data (again using C-statistics, the {\\em wabs} absorption\nmodel and the {\\em wilm} cosmic abundance table, plus the inclusion of\nthe {\\em xspec-redden} component to model the absorption in the UV\nband). The best fit however, with a much lower temperature of\n$kT$=$36^{+3}_{-4}$\\,eV, is a very poor fit to the data; we obtain a\n{\\em goodness} P-statistic value of 0.00, based on 5000 random\nsimulations. This notwithstanding, a flux in the UVW2\n(1.57$-$7.77\\,eV) band of 3.5$\\pm{0.2}\\times10^{-13}$\\,ergs cm$^{-2}$\ns$^{-1}$ can be obtained, corresponding to a UVW2 luminosity, for the\nassumed distance of 50\\,kpc, of 1.0$\\pm{0.1}\\times10^{35}$\\,ergs\ns$^{-1}$.\n\nThe very poor single black-body fit above, plus the large change in\nfitted temperature is strongly suggestive that a model other than, or\nin addition to the XRT-derived kT=59\\,eV black body model (Section~3)\nshould be used to describe the UVW2 data. As we have no UV data other\nthan in the UVW2 filter, all that can be done is to apply the\nXRT-derived black body model to the UVW2+XRT data, and in doing this,\na large flux excess with respect to the XRT-derived black body model\nis seen in the UVW2 band."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A 25-year-old individual with a history of Asperger's syndrome is experiencing increased anxiety and stress due to their current living situation. They have been renting a room from a lady who has a son with ADHD, and the individual has been working hard to balance their financial responsibilities. However, they have recently expressed a desire to find their own place due to concerns about the lady's smoking habits. What is the most likely reason for the individual's increased anxiety and stress?",
    "choices": [
      "A) The individual is experiencing a typical stress response to a new living situation, and their Asperger's syndrome is exacerbating their anxiety.",
      "B) The individual's Asperger's syndrome is causing them to be overly sensitive to the lady's smoking habits, leading to increased anxiety and stress.",
      "C) The individual's desire to find their own place is a result of their Asperger's syndrome, which is causing them to have difficulty with social interactions and relationships.",
      "D) The individual's anxiety and stress are being triggered by the lady's smoking habits, which are affecting their ability to focus and concentrate, and their Asperger's syndrome is making it difficult for them to cope with the stress."
    ],
    "correct_answer": "D)",
    "documentation": [
      "My Aspergers Child: COMMENTS & QUESTIONS [for Feb., 2017]\nI emailed you a while back and you mentioned that I could email when I needed to. Thank you. I last wrote you in December that my son became involved in a dispute involving the local police. We have had 3 court dates. It keeps delaying due to not being able to come to an agreement. But the attorney, even though he was just vaguely familiar with Aspergers, has been very good with Craig. He has the compassion and excellence that is needed here. What started out very bad is turning into a good thing. It will probably take another 90 days or more. But Craig is working hard. Too hard sometimes. He goes to therapy 3 times a week. Doing excellent. He's more focused and can calm down easier. He's got a lot on his plate but has support from his family. From his attorney. From therapy. And from his work. He has been renting a room from a lady who has a son with ADHD. It is good for him. I'm a little worried though because since she smokes he wants to find his own place. With all the costs he has to balance it out financially. That is good. I can't help him more than I am which is good. He is stepping up and taking responsibility. He is listening much better. He is going to have an evaluation today to get an accurate diagnosis. I understand that is a little difficult since he is an adult. Also the PTSD may cover it over. The attorney stated it would help to have the diagnosis. Aware this is a long update, but thanks for reading. I am fighting much guilt still but I have a lot of peace now. My daughter and her 4 year old son also have Aspergers symptoms. So my life chapters may not close for a while. :-) My name is Mac. I'm sure you're quite busy, so I'll get right to it I just wanted to pass on compliments on My Aspergers Child and your post, How to Implement the GFCF Diet: Tips for Parents of Autistic Children. Me and my wife absolutely loved it! I got a facebook message from him today begging to be able to come home saying he misses home and he will change. He says he will follow rules now."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A 27-year-old individual, Kyle, has been struggling with anxiety, social isolation, and substance abuse issues. He has been seeing a psychiatrist and a psychologist, but his symptoms persist. What is the most likely outcome if Kyle's parents decide to tell him about his Asperger's diagnosis, given that he has a history of being highly intelligent, but also experiencing anxiety and social difficulties?",
    "choices": [
      "A) Kyle will immediately start making new friends and become more confident in social situations, leading to improved mental health outcomes.",
      "B) Kyle's parents should wait until Kyle is in a more stable emotional state before sharing the diagnosis, as this may help him better cope with the news.",
      "C) Kyle's Asperger's diagnosis will not have a significant impact on his substance abuse issues, and he will continue to struggle with addiction.",
      "D) Kyle's parents should consider the potential benefits of Kyle knowing his Asperger's diagnosis, including increased self-awareness and access to targeted support services, but also be prepared for potential challenges in his mental health and relationships."
    ],
    "correct_answer": "D)",
    "documentation": [
      "It's really too bad that Asperger's got a diagnostic code back in the 90's, yet all the so called doctors,\nphysiologist's, etc, didn't know how to diagnose it. Too bad. There seems to be no one answer to \"should I tell my adult son he has Asperger's\" from a few specialists I asked. He is typical Asperger,\ncomplicated, highly intelligent (high IQ), anxiety at times, socially isolated, hard to make friends. Not knowing how he will react is the hard part. How will he be better off knowing he has it? Do I wait to tell him in person, or ease into it with him over Skype? He likes direct, honest, concrete communication. Why is this so hard for me? Maybe because no one know's if he is going to be better off knowing or not. Do you know if people are better off\nknowing? I try to get up the courage to just let him know, then I back down. I have been searching the web looking for advice and came upon your site. I am trying to read blogs, websites, books, and articles to help guide me. I was so happy when you said that I could ask you a question. My husband and I are struggling with my 27 year old son who lives with us. Kyle is the youngest of 4 sons. He is a college graduate but never could find the \"right\" job. He has always been quiet and never had a lot of friends. Two years ago, his girlfriend broke up with him. Kyle had an online gambling addiction and was using pot all the time. After the breakup, Kyle was very depressed and started using heroin and finally told my husband he was using. He is now seeing a psychiatrist who has him on suboxone and antidepressants. He is also seeing a psychologist weekly for counseling but it does not seem to be helping. Last October,, Kyle lost his job, got drunk, and was agitated and came home , fighting with us, damaging our home and being verbally abusive. My other son , age 32, who also lives with us called the police and Kyle got arrested. He is currently in the family court system. He went through an anger management course and now is in substance abuse classes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for Lazear's decision to continue working in the lab despite showing symptoms of yellow fever?",
    "choices": [
      "A) He was concerned about the potential impact on his research and the mosquitoes' health.",
      "B) He believed that his symptoms were not severe enough to warrant a break from work.",
      "C) He was trying to prove a point about the mosquito's role in transmitting the virus, even if it meant risking his own health.",
      "D) He was aware of the importance of documenting his case record and vital signs, and wanted to ensure that his data was accurate, even if it meant working through his symptoms."
    ],
    "correct_answer": "D)",
    "documentation": [
      "It was a precise, detailed history that proved beyond doubt that the mosquito was loaded with the virus when it bit a healthy soldier... (If he had entered his name, then his death would have been considered medical suicide by the insurance company, and his wife and two children would not have gotten any payment.) For the next few days, Lazear's life continued much as it had over the last few months in Cuba. He fed and cared for the mosquitoes in the lab. ..Then he began to lose his appetite. He skipped a few meals in the mess hall. He didn't mention it to anyone, nor did he ask to see one of the yellow fever doctors; instead, he worked hard in the lab trying to ignore the oncoming headache. \"On September 18, he complained of feeling 'out of sorts,' and stayed in his officer's quarters. His head pounded and L. decided to write a letter. .. (he wrote to his mother, and referred to his one-year old son Houston and the baby his wife Mabel was about to have: they were staying with his mother in the US). ..That night, L. started to feel chilled as the fever came on. He never went to sleep but worked at his desk all through the night, trying to get all the information about the mosquitoes organized. By morning, he showed all the signs of a severe attack of yellow fever. The camp doctors made the diagnosis, and L. agreed to go to the yellow fever ward. ..L. was carried by litter out of the two-room, white pine board house in which he had lived since he and Mabel first arrived in Cuba. .. (In the yellow fever ward, in a separate one-room building), Lena Warner (the immune nurse who had survived the yellow fever in 1878, when she was nine, and was found in her boarded-up house by a former slave who first thought she was dead, and carried her to safety) nursed J.L., recording his vitals. (I put up a link to his case record and vital signs last week. The surgeon general required that this record be made for every yellow fever patient.)... (On September 25,) Lena Warner braced L's arms with all of her weight, shouting for help."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of spacetime coordinates, what is the implication of the absence of a mixed term in the metric equation for a static spacetime with spherical symmetry?",
    "choices": [
      "A) The spacetime is not static, and the absence of a mixed term indicates that the coordinate system is not adapted to the symmetry.",
      "B) The spacetime is static, and the absence of a mixed term implies that the radial coordinate slices space into spherical shells that are invariant under rotations around the origin.",
      "C) The spacetime is not static, and the absence of a mixed term indicates that the coordinate system is adapted to the symmetry, but the radial coordinate slices space into spherical shells that are not invariant under rotations around the origin.",
      "D) The spacetime is static, and the absence of a mixed term implies that the radial coordinate slices space into spherical shells that are invariant under rotations around the origin, and the coordinate system is adapted to the symmetry."
    ],
    "correct_answer": "D)",
    "documentation": [
      "If we use ${t}$ to slice our spacetime into three-dimensional hyperplanes, each corresponding to ``space at time ${t}$,'' then each of those 3-spaces has the same spatial geometry. A mixed term would indicate that those slices of space would need to be shifted relative to another in order to identify corresponding points. The mixed term's absence indicates that in adapted coordinates, there is no need for such an extra shift. In those coordinates, we can talk about the 3-spaces as just ``space,'' without the need for specifying which of the slices we are referring to. In the case of spherical symmetry, we can introduce spherical coordinates that are adapted to the symmetry: a radial coordinate $r$ and the usual angular coordinates $\\vartheta,\\varphi$, so that the spherical shell at constant $r$ has the total area $4\\pi r^2$. In consequence, the part of our metric involving $\\mathrm{d}\\vartheta$ and $\\mathrm{d}\\varphi$ will have the standard form\n\\begin{equation}\nr^2(\\mathrm{d}\\vartheta^2+\\sin^2\\theta\\mathrm{d}\\varphi^2) \\equiv r^2\\mathrm{d}\\Omega^2,\n\\end{equation}\nwhere the right-hand side defines $\\mathrm{d}\\Omega^2$, the infinitesimal solid angle corresponding to each particular combination of $\\mathrm{d}\\vartheta$ and $\\mathrm{d}\\varphi$.\n\nThe radial coordinate slices space into spherical shells, each corresponding to a particular value $r=const.$ The rotations around the origin, which are the symmetry transformations of spherical symmetry, map each of those spherical shells onto itself, and they leave all physical quantities that do not explicitly depend on $\\vartheta$ or $\\varphi$ invariant. In what follows, we will use the basic structures introduced in this way --- the slices of simultaneous ${t}$, the radial directions within each slice, the angular coordinates spanning the symmetry--adapted spherical shells of area $4\\pi r^2$ --- as auxiliary structures for introducing spacetime coordinates. For now, let us write down the shape that our metric has by simple virtue of the spherical symmetry, the requirement that the spacetime be static, and the adapted coordinates, namely\n\\begin{equation}\n\\mathrm{d} s^2 = -c^2F(r) \\mathrm{d} {t}^2 + G(r) \\mathrm{d} r^2 + r^2\\:\\mathrm{d}\\Omega^2."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the conservation of marine fishes subject to international trade found that the use of fish parasites as biological indicators can be effective in monitoring environmental impact and climate change. However, the same study also highlighted the need for more effective regulations to prevent the over-exploitation of elasmobranch species. Which of the following statements best summarizes the implications of the study's findings?",
    "choices": [
      "A) The use of fish parasites as biological indicators is sufficient to regulate the trade of all marine fishes, regardless of their elasmobranch hosts.",
      "B) The study's findings suggest that the conservation of marine fishes subject to international trade can be achieved through the implementation of stricter regulations on the trade of elasmobranch species.",
      "C) The use of fish parasites as biological indicators is a useful tool for monitoring the impact of climate change on marine fishes, but it does not provide any information on the conservation of elasmobranch species.",
      "D) The study's findings indicate that the conservation of marine fishes subject to international trade requires a multi-faceted approach that includes the regulation of elasmobranch species, as well as the use of fish parasites as biological indicators to monitor environmental impact and climate change."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The role of CITES in the conservation of marine fishes subject to international trade. Fish and Fisheries. 2014 Dec 1;15(4):563\u201392. 43. Palm HW. Fish parasites as biological indicators in a changing world: can we monitor environmental impact and climate change?. InProgress in Parasitology 2011 (pp. 223\u2013250). Springer Berlin Heidelberg.\n44. Palm HW, Yulianto I, Piatkowski U. Trypanorhynch Assemblages Indicate Ecological and Phylogenetical Attributes of Their Elasmobranch Final Hosts. Fishes. 2017 Jun 17;2(2):8. 46. Booth H. Using the case of illegal manta ray trade in Indonesia to evaluate the impact of wildlife trade policy (Master Thesis, Imperial College London).\n47. White WT, Last PR, Stevens JD, Yearsley GK. Economically important sharks & rays of Indonesia. Austr-alian Centre for International Agricultural Research (ACIAR); 2006."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A road side unit (RSU) is equipped with a cognitive system that can predict and estimate the positions of vehicles based on real-time RF signals. This allows the RSU to evaluate whether the RF signals and vehicles' trajectories are evolving according to the dynamic rules encoded in the cognitive system. If the RSU detects an abnormal behavior in the V2X environment, it can identify the cause as either a jammer attacking the V2I or a spoofer attacking the satellite link. However, the RSU's cognitive system is not perfect and can make mistakes. In a simulation study, the RSU's cognitive system was found to have a detection probability of 90% for jamming attacks and 95% for spoofing attacks. What is the most likely cause of the RSU's detection error?",
    "choices": [
      "A) The RSU's cognitive system is not able to accurately estimate the positions of vehicles, leading to incorrect predictions of RF signals.",
      "B) The RSJ is located at a position that is too far away from the RSU, making it difficult for the RSU to detect the jamming signal.",
      "C) The RSS is located at a position that is too close to the RSU, causing interference with the RSU's cognitive system and leading to incorrect predictions of RF signals.",
      "D) The RSU's cognitive system is able to accurately detect both jamming and spoofing attacks, but the detection probability is lower for spoofing attacks due to the complexity of the spoofing signal."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In this work, we propose a method to jointly detect GPS spoofing and jamming attacks in the V2X network. A coupled generalized dynamic Bayesian network (C-GDBN) is employed to learn the interaction between RF signals received by the RSU from multiple vehicles and their corresponding trajectories. This integration of vehicles' positional information with vehicle-to-infrastructure (V2I) communications allows semantic learning while mapping RF signals with vehicles' trajectories and enables the RSU to jointly predict the RF signals it expects to receive from the vehicles from which it can anticipate the expected trajectories. The main contributions of this paper can be summarized as follows: \\textit{i)} A joint GPS spoofing and jamming detection method is proposed for the V2X scenario, which is based on learning a generative interactive model as the C-GDBN. Such a model encodes the cross-correlation between the RF signals transmitted by multiple vehicles and their trajectories, where their semantic meaning is coupled stochastically at a high abstraction level. \\textit{ii)} A cognitive RSU equipped with the acquired C-GDBN can predict and estimate vehicle positions based on real-time RF signals. This allows RSU to evaluate whether both RF signals and vehicles' trajectories are evolving according to the dynamic rules encoded in the C-GDBN and, consequently, to identify the cause (i.e., a jammer attacking the V2I or a spoofer attacking the satellite link) of the abnormal behaviour that occurred in the V2X environment. \\textit{iii)} Extensive simulation results demonstrate that the proposed method accurately estimates the vehicles' trajectories from the predicted RF signals, effectively detect any abnormal behaviour and identify the type of abnormality occurring with high detection probabilities. To our best knowledge, this is the first work that studies the joint detection of jamming and spoofing in V2X systems. \\section{System model and problem formulation}\nThe system model depicted in Fig.~\\ref{fig_SystemModel}, includes a single cell vehicular network consisting of a road side unit (RSU) located at $\\mathrm{p}_{R}=[{x}_{R},{y}_{R}]$, a road side jammer (RSJ) located at $\\mathrm{p}_{J}=[{x}_{J},{y}_{J}]$, a road side spoofer (RSS) located at $\\mathrm{p}_{s}=[{x}_{s},{y}_{s}]$ and $N$ vehicles moving along multi-lane road in an urban area."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is evaluating the performance of the AutoCogniSys system for activity recognition. The system is compared to a baseline method that uses a three-level Dynamic Bayesian Network framework for complex activity recognition. The researcher also considers the performance of the system for hand gesture and postural activity recognition, as well as activity performance estimation using EDA and PPG signals. However, the system's performance is affected by the variability in the duration of activities, such as cooking. What is the correct definition of accuracy for the activity recognition evaluation?",
    "choices": [
      "A) The ratio of true positives to the sum of true positives, true negatives, false positives, and false negatives, where the denominator includes only the number of true positives and true negatives.",
      "B) The ratio of true positives to the sum of true positives, true negatives, false positives, and false negatives, where the denominator includes only the number of true positives and false positives.",
      "C) The ratio of true positives to the sum of true positives, true negatives, false positives, and false negatives, where the denominator includes only the number of true positives and false negatives.",
      "D) The ratio of true positives to the sum of true positives, true negatives, false positives, and false negatives, where the denominator includes only the number of true positives, true negatives, and the start/end duration error."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This sketching can help us significantly to identify which particular hand gesture is being performed in the time segment. \\subsubsection{EES Datasets: EDA and PPG Sensor Datasets} We used Eight-Emotion Sentics (EES) dataset to validate \\emph{AutoCogniSys} proposed physiological signal processing approaches \\cite{picard01}. The dataset consists of measurements of four physiological signals (PPG/Blood Volume Pulse, electromyogram, respiration and Skin Conductance/EDA) and eight affective states (neutral, anger, hate, grief, love, romantic love, joy, and reverence). The study was taken once a day in a session lasting around 25 minutes for 20 days of recordings from an individual participant. We consider only PPG and EDA for all of the affective states in our study. \\subsubsection{Baseline Methods}\nThough no frameworks ever combined all modalities together into real-time automated cognitive health assessment, we evaluate \\emph{AutoCogniSys} performance by comparing the performances of its components individually with upto date relevant works. For hand gesture and postural activity recognition, we consider \\cite{alam17} proposed method as baseline. For complex activity recognition, we compare our hand gesture and postural activity classifiers aided HDBN model with three-level Dynamic Bayesian Network \\cite{zhu12} framework. For activity performance estimation, activity performance based cognitive health assessment; and EDA and PPG based cognitive health assessment, we have considered \\cite{alam16} proposed method as baseline. \\subsection{Activity Recognition Evaluation}\nThe standard definition for \\emph{accuracy} in any classification problem is $\\frac{TP+TN}{TP+TN+FP+FN}$ where $TP,TN,FP$ and $FN$ are defined as true positive, true negative, false positive and false negative. For complex activity recognition evaluation, we additionally consider \\emph{start/end duration error} as performance metric that can be explained as follows: consider that the true duration of ``cooking'' is 30 minutes (10:05 AM - 10:35 AM) and our algorithm predicts 29 minutes (10.10 - to 10.39 AM)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When considering the properties of graphene under strain, what can be inferred about the relationship between strain strength and bandgap opening?",
    "choices": [
      "A) A bandgap opens only when the strain is greater than 10%, regardless of the direction of the strain.",
      "B) The preferred direction for bandgap opening under compressive strain is the zigzag direction, similar to the case of tensile strain.",
      "C) A small strain of a few percent can change the gapless character of graphene, resulting in a significant conduction-gap opening in the unstrained/strained graphene junctions.",
      "D) The properties of graphene bandstructure at low energy are qualitatively the same when applying strains of {\u03c3, \u03b8} and of {-\u03c3, \u03b8 + 90\u00b0}, and a large strain is necessary to open a bandgap in graphene."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We remind as displayed in Fig. 2(a) that a finite bandgap opens only for strain larger than $\\sim 23 \\%$ and the zigzag (not armchair) is the preferred direction for bandgap opening under a tensile strain \\cite{per209}. We extend our investigation to the case of compressive strain and find  (see in Fig. 2(b) ) that (i) the same gap threshold of $\\sigma \\simeq 23 \\%$ is observed but (ii) the preferred direction to open the gap under a compressive strain is the armchair, not the zigzag as the case of tensile strain. This implies that the properties of graphene bandstructure at low energy should be qualitatively the same when applying strains of $\\left\\{ {\\sigma ,\\theta } \\right\\}$ and of $\\left\\{ {-\\sigma ,\\theta + 90^\\circ} \\right\\}$. This feature can be understood by considering, for example, strains of $\\left\\{ {\\sigma , \\theta = 0} \\right\\}$ and of $\\left\\{ {-\\sigma , \\theta = 90^\\circ} \\right\\}$. Indeed, these strains result in the same qualitative changes on the bond-lengths, i.e., an increased bond-length $r_3$ and reduced bond-lengths $r_{1,2}$. However, for the same strain strength, because of the exponential dependence of hoping energies on the bond-lengths, the compressive strain generally induces a larger bandgap than the tensile one, as can be seen when comparing the data displayed in Figs. 2(a) and 2(b). To conclude, we would like to emphasize that a large strain is necessary to open a bandgap in graphene. This could be an issue for practical applications, compared to the use of graphene strained junctions explored in \\cite{hung14}. We now go to explore the properties of conduction gap in the graphene strained junctions. In Fig. 3, we display the conductance as a function of energy computed from Eq. (5) using the Green's function technique. As discussed above, a small strain of a few percent (e.g., 4 $\\%$ here) can not change the gapless character of graphene, i.e., there is no gap of conductance in the case of uniformly strained graphene. However, similar to that reported in \\cite{hung14}, a significant conduction-gap of a few hundreds meV can open in the unstrained/strained graphene junctions."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the article, what is the significance of the fact that the present-day #36 Craven Street is the correct house for Ben Franklin tourists to visit, given that #1 Craven Street was incorrectly identified as the correct house for the last three years?",
    "choices": [
      "A) The perpetuation of a wrong house number in street map printings is a common occurrence in London's history.",
      "B) The correct house number for Ben Franklin tourists to visit is actually #1 Craven Street, which was mistakenly identified as the correct house for the last three years.",
      "C) The fact that the present-day #36 Craven Street is the correct house for Ben Franklin tourists to visit is irrelevant to the history of street renumbering in London.",
      "D) The correct house number for Ben Franklin tourists to visit is #36 Craven Street, which was incorrectly identified as #1 Craven Street for the last three years due to a mistake in the street numbering system that was perpetuated over time."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Excellent job of deciphering street renumbering material spanning sixty years, including that of a wrong house number (# 7) being erroneously identified and then perpetuated in subsequent street map printings. It\u2019s gratifying at least to know that the present day #36 Craven Street is the correct house for Ben Franklin tourists to visit. Except for #1 Craven Street for the last three years Franklin was in London, but we won\u2019t get into that. Again, excellent article, David!"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A pregnant woman with alpha thalassemia major is at increased risk of developing complications during pregnancy and delivery. Which of the following is a potential consequence of her condition?",
    "choices": [
      "A) She is more likely to experience a normal, uncomplicated pregnancy with no complications.",
      "B) She is at increased risk of developing toxemia, a disturbance of metabolism that can potentially lead to convulsions and coma, due to the enlarged placenta.",
      "C) She is more likely to experience premature delivery and increased rates of delivery by cesarean section due to the enlarged spleen.",
      "D) She is at increased risk of developing severe anemia events, which may require blood transfusion, due to the damage to the red blood cell membrane and the increased fragility of the cells."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In addition, hemoglobin H tends to precipitate out in the cells, causing damage to the red blood cell membrane. When affected individuals are exposed to certain drugs and chemicals known to make the membrane more fragile, the cells are thought to become vulnerable to breakdown in large numbers, a complication called hemolytic anemia. Fever and infection are also considered to be triggers of hemolytic anemia in hemoglobin H disease. This can result in fatigue, paleness, and a yellow discoloration of the skin and whites of eyes called jaundice. Usually, the anemia is mild enough not to require treatment. Severe anemia events may require blood transfusion, however, and are usually accompanied by such other symptoms as dark feces or urine and abdominal or back pain. These events are uncommon in hemoglobin H disease, although they occur more frequently in a more serious type of hemoglobin H disease called hemoglobin H/Constant Spring disease. Individuals effected with this type of hemoglobin H disease are also more likely to have enlargement of and other problems with the spleen. Alpha thalassemia major\nBecause alpha globin is a necessary component of all major hemoglobins and some minor hemoglobins, absence of all functioning alpha globin genes leads to serious medical consequences that begin even before birth. Affected fetuses develop severe anemia as early as the first trimester of pregnancy. The placenta, heart, liver, spleen, and adrenal glands may all become enlarged. Fluid can begin collecting throughout the body as early as the start of the second trimester, causing damage to developing tissues and organs. Growth retardation is also common. Affected fetuses usually miscarry or die shortly after birth. In addition, women carrying affected fetuses are at increased risk of developing complications of pregnancy and delivery. Up to 80% of such women develop toxemia, a disturbance of metabolism that can potentially lead to convulsions and coma. Other maternal complications include premature delivery and increased rates of delivery by cesarean section, as well as hemorrhage after delivery."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the security of AES-128 against large-scale fault-tolerant quantum adversaries. They compare the security of AES-128 using the more conservative $p_g=10^{-3}$ and the more optimistic $p_g=10^{-5}$. However, they neglect to consider the impact of improved methods for generating generic input states on the total memory cost. Which of the following statements is most accurate?",
    "choices": [
      "A) The security of AES-128 is significantly improved when using the more optimistic $p_g=10^{-5}$, as this assumption is more realistic from a quantum computing perspective.",
      "B) The memory required for generating generic input states accounts for a substantial fraction of the total memory cost, but this does not affect the security of AES-128 against large-scale fault-tolerant quantum adversaries.",
      "C) The security of AES-128 is not affected by the memory required for generating generic input states, as this is a separate consideration from the security against quantum adversaries.",
      "D) The security of AES-128 is improved when using the more conservative $p_g=10^{-3}$, as this assumption is more realistic from a cybersecurity perspective and takes into account the impact of improved methods for generating generic input states."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We assume a surface code cycle time of 200ns, in conformance with~\\cite{PhysRevA.86.032324}. For each scheme we analyze, we compare its security using the more conservative (and realistic in the short term) $p_g=10^{-3}$ and also the more optimistic  $p_g=10^{-5}$. Note that assuming the more optimistic assumption from a quantum computing perspective is the more conservative assumption from a cybersecurity perspective. Furthermore, in this analysis, we are reporting the full physical footprint, including the memory required for magic state distillation. Using present-day techniques, the memory required for generating these generic input states accounts for a substantial fraction of the total memory cost and thus we are including these in the total cost estimate and will track the impact of improved methods.\n\n\\section{Symmetric ciphers\\label{sct::ciphers}} Below we analyze the security of AES family of symmetric ciphers against large-scale fault-tolerant quantum adversaries. We used the highly optimized logical circuits produced in\n\\cite{10.1007/978-3-319-29360-8_3}. \\subsection{AES-128}\n\n        \\includegraphics[width=0.429\\textwidth]{figures/AES-128_cycles.pdf}\n      \t\\captionof{figure}{AES-128 block cipher. Required surface clock cycles per processor, as a function of the  number of processors ($\\log_2$ scale). The bottom brown line (theoretical lower bound, black box) represents the minimal number of queries required\n\tby Grover's algorithm, the cost function being the total number of queries to a black-box oracle, each query assumed to have unit cost, and a completely error-free circuit. The purple line (ideal grover, non-black-box) takes into consideration the structure of the oracle, the cost function being the total number of gates in the circuit, each gate having unit cost; the quantum circuit is assumed error-free as well. Both brown and magenta lines are displayed only for comparisons; for both of them, the $y$ axis should be interpreted as number of logical queries (operations, respectively)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher simulates the behavior of a bubble in a fluid using a numerical method. The simulation shows that the bubble undergoes a complex evolution, including the formation of high-pressure regions and the generation of a jet structure. However, the simulation also reveals that the jet structure is not stable and eventually breaks down due to the deposited vorticity. What is the most likely outcome of this breakdown?",
    "choices": [
      "A) The high-pressure regions will merge to form a stronger shock wave, leading to further deformation of the bubble.",
      "B) The jet structure will persist and continue to propagate downstream, causing damage to surrounding structures.",
      "C) The breakdown of the jet structure will lead to a decrease in the pressure inside the bubble, causing it to expand.",
      "D) The deposited vorticity will cause the bubble to break apart into smaller fragments, leading to a loss of its structural integrity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The below results also show that it is sufficient to meet the requirements of the following research problem. Other parameters used for the simulation are: c = 1.0, \u03b7 Air = \u03b7 bubble = 10.0,\nI Air = 3, I bubble = 15, \u2206x = \u2206y = 1.2 \u00d7 10 \u22124 and \u2206t = 1 \u00d7 10 \u22126 . The viscosity effect is feeble compared to the shock compression effect, so it does not significantly affect the deformation of the bubble. Therefore, in this part, the relaxation time \u03c4 is set sufficiently small. The inflow (outflow) boundary condition is used in the left (right) boundary, and the periodic boundary is adopted in the y direction. The first-order forward difference scheme is used to calculate the temporal derivative, and the second-order nonoscillatory nonfree dissipative scheme is adopted to solve the spatial derivative in Eq. ( ) . Two quantitative comparisons between experimental results and DBM simulations are shown in the following part, including snapshots of schlieren images and evolutions of characteristic scales for the bubble. The first is shown in Fig. non-organized momentum flux (NOMF) uously to form a diffracted shock (DS). As TS propagates, it will split into three branches due to the considerable pressure perturbations caused by the gradual decay of the DS strength . Afterward, as shown in the subfigure at about t = 128\u00b5s, two high pressure regions (ROH) generate because of the interaction of these branches. Subsequently, at about t = 148\u00b5s, the two ROHs meet, causing the shock focusing. On the one hand, at about t = 168\u00b5s, the shock focusing causes the generation of downstream-propagating second transmitted shock (STS) and upward-moving rarefaction wave. On the other hand, it will produce high pressure region inside the bubble, which later leads to a jet structure, as shown at about t = 288\u00b5s. At about t = 428\u00b5s, due to the deposited vorticity, there will produce a pair of counter-rotating vortexes at the pole region of the bubble. The further development of the vortex pair and the effect of viscosity decrease the amplitude of the jet."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that in Florida, the number of opioid prescriptions written by doctors increased significantly between 1999 and 2001, with Medicaid paying for a substantial portion of these prescriptions. However, the study also revealed that many of these prescriptions were not being filled by the intended patients. What is a likely explanation for this discrepancy?",
    "choices": [
      "A) The majority of Medicaid patients were actually using their prescriptions for non-medical purposes, such as selling them on the black market.",
      "B) The Florida Medicaid program was simply more efficient at processing prescriptions than other states, resulting in a higher volume of prescriptions being filled.",
      "C) The increase in opioid prescriptions was largely driven by the growing number of patients with chronic pain, who were using their prescriptions to manage their symptoms.",
      "D) The Florida Medicaid program was knowingly and intentionally overprescribing opioids to patients, resulting in a significant number of prescriptions being filled by individuals who were not the intended recipients."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In Florida, \u201cthere was none of that \u2026 stuff that they check and find out what doctor you\u2019ve been to,\u201d said Frazier. \u201cAnd one person does it, and then they tell a friend, and then they go do it, and that\u2019s how it all really got started here.\u201d\nMEDICAID-MEDICAIRE PAID MILLIONS FOR OXY\nTallahassee wasn\u2019t just ignoring the epidemic, It was financing it. Before her office was raided by law enforcement in December 2001, Asuncion M. Luyao\u2019s patients would wait in a line in the rain to get prescriptions from the Port St. Lucie internist and acupuncturist. She was one of the most prolific prescribers of OxyContin in the state. And hundreds of thousands of those pills were being paid for by Medicaid, Florida\u2019s taxpayer-financed health program for the state\u2019s poorest and sickest citizens. Between 1999 and 2001, Medicaid shelled out $935,634 for OxyContin prescriptions written by Luyao. That was just OxyContin. Luyao was prescribing an array of addictive drugs. In the 12 months leading up to the clinic raid, Medicaid paid roughly $1 million for 7,000 prescriptions, only about 17 percent of them for OxyContin. Nor did the raid slow her down. Between the raid and her arrest on trafficking charges four months later, Luyao wrote another 282 OxyContin prescriptions billed to Medicaid. She was not an outlier. In 24 months, taxpayers footed the bill for more than 49 million doses of pills containing oxycodone, even though there were only 1.36 million Medicaid patients. Half were children. The sheer volume of pills might have been a tipoff that the drugs were not all intended for legitimate use. So were arrest reports dating to 2001. One man had used his 7-year-old son\u2019s Medicaid number to doctor-shop for OxyContin. A Miramar pharmacist who billed Medicaid $3.7 million for OxyContin pills was charged with paying Medicaid patients $150 each to use their IDs. Medicaid paid for more than $300,000 to fill Dr. James Graves\u2019 OxyContin prescriptions. The Florida Panhandle physician was the first doctor in the nation convicted of killing patients by overprescribing OxyContin."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on ancient agriculture in the Mediterranean region found that the introduction of wheat cultivation led to a significant increase in crop yields, but this increase was largely due to the adoption of new irrigation techniques rather than the introduction of new crop varieties. Which of the following factors is most likely to have contributed to the increased crop yields in this region?",
    "choices": [
      "A) The widespread adoption of new crop varieties, such as emmer wheat, which were more resistant to disease and pests.",
      "B) The increased use of fertilizers and pesticides, which allowed for more intensive farming practices.",
      "C) The introduction of new irrigation techniques, such as canals and dams, which allowed for more efficient water management.",
      "D) The shift from a subsistence-based economy to a market-based economy, which led to increased investment in agriculture and more efficient farming practices."
    ],
    "correct_answer": "C)",
    "documentation": [
      "2. Data Publication and Re-use Practices in Archaeobotany\n2.1. History of data production and publication\nArchaeobotanical data falls within the category of observational data in archaeology (Marwick & Pilaar Birch 2018). Archaeobotanical data is considered as the quantitative assessment of plant macrofossils present within a sample from a discrete archaeological context, which can include species identification, plant part, levels of identification (cf. \u2013 confer or \u201ccompares to\u201d), and a range of quantification methods including count, minimum number of individuals, levels of abundance and weight (Popper 1988). Archaeobotanical data is usually entered into a two-way data table organised by sample number. Alongside the counts of individual taxa, other information is also necessary to interpret archaeobotanical data, including sample volume, flot volume, charcoal volume, flot weight, level of preservation, sample number, context number, feature number, feature type and period. Beyond taxonomic identifications, a range of other types of data are increasingly gathered on individual plant macrofossils (morphometric measurements, isotopic values, aDNA). Archaeobotanical training places a strong emphasis on recording data on a sample-by-sample basis (Jacomet & Kreuz 1999: 138\u2013139; Jones & Charles 2009; Pearsall 2016: 97\u2013107). Time-consuming methodologies utilised in the pursuit of accurate sample-level data recording include sub-sampling and splitting samples into size fractions and counting a statistically useful number of items per sample (Van der Veen & Fieller 1982). The creation of sample-level data means analysis is often undertaken on the basis of individual samples, for instance the assessment of crop-processing stages and weed ecological evidence for crop husbandry practices. The analysis of sample level data also enables archaeobotanical finds to be integrated alongside contextual evidence from archaeological sites. Requirements for the publication of this data are in place in some archaeological guidelines, for instance current Historic England guidelines for archaeological practice in England (Campbell, Moffett & Straker 2011: 8)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Njoroge's decision to leave town and attempt suicide can be understood as a result of his feelings of hopelessness and shame. However, it is also clear that his situation is further complicated by the fact that he is the sole provider for his two mothers. What is the most likely reason for Njoroge's feelings of hopelessness, given his role as the sole provider for his two mothers?",
    "choices": [
      "A) He is struggling to cope with the loss of his father, Ngotho, and the family's forced relocation.",
      "B) He is overwhelmed by the responsibility of supporting his two mothers, and feels that he is failing them.",
      "C) He is angry with his brothers, Boro and Kamau, for not helping him support his mothers.",
      "D) He is struggling to reconcile his desire for education with the harsh realities of his family's situation, and feels that he is unable to make ends meet."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God. Njoroge asks Mwihaki's for support, but she is angry because of her father\u2019s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice. Characters in Weep Not, Child\n Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible. Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression. Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi. Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II). Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "Under the germ theory there are no negative feedbacks. This makes a stable biological system by definition impossible. The immune system is *not* a negative feedback it is the opposite. It actually reinforces our math problem because the immune system will weaken as the number of pathogens increase. There is no way of resolving this problem without a discontinuity. A Deus ex Machina as The Almighty Pill so beautifully put it. So the germ theory is quite literally, mathematically impossible. There is as much chance of it being true as 2+2 = 5. There are plenty of other massive problems with germ theory such as why did things like SARS and bird flu magically disappear? Why do we have the symptoms that we do? Is our body controlling the symptoms to help fight the germs and if so, why would suppressing the symptoms with antibiotics or Tamiflu be considered a good idea? If the virus is causing the symptoms then why would it cause these kinds of things?"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher studying the electrogenic properties of sharks notes that the electric fields detected by these animals are similar to those produced by the electric eel. However, the shark's electroreception system is not limited to detecting prey, but also plays a role in navigation and social behavior. Which of the following statements best describes the relationship between the electric fields detected by sharks and their electroreception system?",
    "choices": [
      "A) The electric fields detected by sharks are only used for navigation and social behavior, and have no role in detecting prey.",
      "B) The electric fields detected by sharks are similar to those produced by the electric eel, and are used for both navigation and social behavior, as well as detecting prey.",
      "C) The electric fields detected by sharks are similar to those produced by the electric eel, but are only used for detecting prey, and have no role in navigation and social behavior.",
      "D) The electric fields detected by sharks are not similar to those produced by the electric eel, and are instead used for navigation and social behavior, but not for detecting prey."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\u00a7Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon. Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants. In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. \"Revitalization\" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored Frankenstein (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films. As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who \"finger death at their gloves' end as they piece and repiece the living wires\" in Rudyard Kipling's 1907 poem Sons of Martha."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A graph coloring algorithm is designed to color a graph with a target number of colors k. The algorithm uses a local search approach to recolor vertices at a bounded distance from a conflict vertex. In the initial phase, the algorithm searches for a recoloring of some adjacent vertices that allows for direct recoloring of the conflict vertex. If no solution is found, the algorithm increases the depth of the search. The algorithm has two parameters: adjacency bound a max and depth d. Given that the algorithm used BDFS with parameters a max = 3 and d = 3, and the depth was increased to 5 when the number of vertices in the queue was 2, what is the likely reason for the increase in depth?",
    "choices": [
      "A) The algorithm is trying to recolor vertices at a larger distance from the conflict vertex to reduce the number of colors used.",
      "B) The algorithm is trying to avoid degenerate cases where a single vertex has all the same color as its neighbors.",
      "C) The algorithm is trying to recolor vertices at a larger distance from the conflict vertex to increase the number of colors used.",
      "D) The algorithm is trying to recolor vertices at a larger distance from the conflict vertex to reduce the number of colors used, but only if the number of vertices in the queue is greater than 1."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The bounded depth-first search (BDFS) algorithm tries to improve the dequeuing process. The goal is to prevent a vertex in conflict with some adjacent colored vertices from entering in the conflict set. At the first level, the algorithm searches for a recoloring of some adjacent vertices which allows us to directly recolor the conflict vertex. If no solution is found, the algorithm In both figures the algorithm uses p = 1.2, easy vertices, q max = 59022, but does not use the BDFS nor any clique. For \u03c3 \u2265 0.25, no solution better than 248 colors is found. could recolor some vertices at larger distances from the conflict vertex. To do so, a local search is performed by trying to recolor vertices at a bounded distance from the conflict vertex in the current partial solution. The BDFS algorithm has two parameters: adjacency bound a max and depth d. In order to recolor a vertex v, BDFS gets the set C of color classes with at most a max neighbors of v. If a class in C has no neighbor of v, v is assigned to C. Otherwise, for each class C \u2208 C, BDFS tries to recolor the vertices in C which are adjacent to v by recursively calling itself with depth d \u2212 1. At depth d = 0 the algorithm stops trying to color the vertices. During the challenge the Shadoks used BDFS with parameters a max = 3 and d = 3. The depth was increased to 5 (resp. 7) when the number of vertices in the queue was 2 (resp. 1). Degeneracy order Given a target number of colors k, we call easy vertices a set of vertices Y such that, if the remainder of the vertices of G are colored using k colors, then we are guaranteed to be able to color all vertices of G with k colors. This is obtained using the degeneracy order Y . To obtain Y we iteratively remove from the graph a vertex v that has at most k \u2212 1 neighbors, appending v to the end of Y . We repeat until no other vertex can be added to Y . Notice that, once we color the remainder of the graph with at least k colors, we can use a greedy coloring for Y in order from last to first without increasing the number of colors used."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the effect of environmental distance on the convergence of a genetic algorithm used to optimize the learning rate and amplitudes of a neural network. The algorithm converges to a weight vector that predicts the correct food values. Which of the following factors is most likely to influence the convergence time of the algorithm?",
    "choices": [
      "A) The rate of environmental transition, as it affects the frequency of updates to the weight vector.",
      "B) The noisiness of the reward, as it affects the accuracy of the weight vector updates.",
      "C) The distance between the environments, as it affects the initial conditions of the algorithm.",
      "D) The population size of the genetic algorithm, as it affects the diversity of the population."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We use a genetic algorithm to optimize the learning rate \u03b7 p and amplitudes of different terms \u03b8 = (\u03b8 1 , . . . , \u03b8 8 ). The successful plasticity rule after many food presentations must converge to a weight vector that predicts the correct food values (or allows the agent to correctly decide whether to eat a food or avoid it). To have comparable results, we divide \u03b8 = (\u03b8 1 , . . . , \u03b8 8 ) by We then multiply the learning rate \u03b7 p with \u03b8 max to maintain the rule's evolved form unchanged, \u03b7 norm p = \u03b7 p \u2022 \u03b8 max . In the following, we always use normalized \u03b7 p and \u03b8, omitting norm . To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism . The agents' parameters are initialized at random (drawn from a Gaussian distribution), then the sensory network is trained by the plasticity rule and finally, the agents are evaluated. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation. The remaining 90 % of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (\u03c3 = 0.1) to its parameters. To start with, we consider a static agent whose goal is to identify the value of presented food correctly. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task. We first look at the evolved learning rate \u03b7 p , which determines how fast (if at all) the network's weight vector is updated during the lifetime of the agents. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition. The first natural factor is the distance d e between the two environments, with a larger distance requiring a higher learning rate, Fig. . This is an expected result since the convergence time to the \"correct\" weights is highly dependent on the initial conditions."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new Champion has been added to the game, and the Arena Crystal has been adjusted to reflect the changes in the Champion's stats. As a result, the payout for defeating this new Champion in the Arena Crystal has increased. However, the payout is still capped at a certain amount, and the amount of Gold awarded is also limited. Which of the following statements is true about the new payout structure?",
    "choices": [
      "A) The payout for defeating the new Champion in the Arena Crystal is now capped at 1000 Gold, and the amount of Gold awarded is directly proportional to the Champion's Star rating.",
      "B) The payout for defeating the new Champion in the Arena Crystal is now capped at 500 Gold, and the amount of Gold awarded is directly proportional to the Champion's Star rating.",
      "C) The payout for defeating the new Champion in the Arena Crystal is now capped at 500 Gold, and the amount of Gold awarded is directly proportional to the Champion's Star rating, but only if the player has a 3-Star Champion at Rank 1.",
      "D) The payout for defeating the new Champion in the Arena Crystal is now capped at 1000 Gold, and the amount of Gold awarded is directly proportional to the Champion's Star rating, but only if the player has a duplicate 2-Star, 3-Star, or 4-Star Champion."
    ],
    "correct_answer": "D)",
    "documentation": [
      "*NOTE: Special Attacks only generate Power for the target struck, not for the user; this prevents infinite loops and helps serve as a comeback mechanic. Versus Crystal prizes have been adjusted due to the Champion Stamina changes. Arena Crystal prizes have been increased to help balance the adjustments to the Versus Crystal. Payouts have significantly increased when receiving a duplicate Champion with a Star rating of two or more. The boosted amount increases based on Star rating. We apologize for any inconvenience caused by delivering each reward individually, and are working to get a fix to you as soon as possible. In the meantime, using the \u201cSkip\u201d button avoids the inconvenience. \u2022 We fixed a bug where finding a new match could cost a player Units. \u2022 Spending Units to find a new opponent will now return opponents with lower ratings. \u2022 Chapters 3 and 4 of Act 2 Story Quests are now available. A mysterious opponent awaits you at the end of Act 2! *NOTE: This caused some players' progress to reset for a brief time, but that issue should now be corrected. \u2022 Event Quest difficulty has been adjusted to match Catalyst availability. \u2022 Rank-Up Recipes have been adjusted to be more accessible across all ranks. \u2022 Bosses for the Monday through Saturday Daily Events now have a small chance to drop a Class Catalyst. This is in addition to the drop chance from Chests. \u2022 Ambush Rates have been adjusted on all Event Quests. \u2022 Increased Catalyst drops for the Collector Free-For-All Event Quest. \u2022 Alpha Catalysts now have a chance to drop from Chests in Medium and Hard difficulties of The Collector Free-For-All event. \u2022 The unobtainable chest in Act 1, Chapter 1, Quest 6 has been removed from the Battlerealm. Increased the amount of Gold awarded by the Arena Crystal. Slightly reduced the cost to level-up a 3-Star Champion at Rank 1 to cleanly align with ISO-8 chunk values. Fixed a bug with Billion-Dollar Punch not triggering Armor Break. \u2022 Duplicate 2-Star, 3-Star, and 4-Star Champions now awaken a brand new ability unique to that Champion in addition to the rare ISO8 they currently give."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the effectiveness of AutoCogniSys in improving cognitive function in older adults found that participants who wore the wristband and performed complex activities had a significant reduction in fall risk. However, the study also noted that the annotation of hand gestures was extremely difficult due to the limitations of the video-based hand tracker. Which of the following is a potential explanation for the difficulty in annotating hand gestures?",
    "choices": [
      "A) The video-based hand tracker was unable to accurately track the participants' wrist movements due to the presence of background noise.",
      "B) The participants were not wearing the wristband correctly, causing the tracker to misinterpret their hand gestures.",
      "C) The annotation of hand gestures was not a critical component of the study's outcome measures, and therefore, it was not a priority for the researchers.",
      "D) The video-based hand tracker was able to accurately track the participants' wrist movements, but the annotators were not trained to recognize the specific hand gestures that were relevant to the study's outcome measures."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We validate and compare \\emph{AutoCogniSys} with baseline methods on both publicly available and our collected datasets. \\subsubsection{RCC Dataset: Collection and Ground Truth Annotation} For collecting Retirement Community Center Dataset (RCC Dataset), we recruited 22 participants (19 females and 3 males) with age range from 77-93 (mean 85.5, std 3.92) in a continuing care retirement community with the appropriate institutional IRB approval and signed consent. The gender diversity in the recruited participants reflects the gender distribution (85\\% female and 15\\% male) in the retirement community facility. A trained gerontology graduate student evaluator completes surveys with participants to fill out the surveys. Participants are given a wrist band to wear on their dominant hand, and concurrently another trained IT graduate student have the IoT system setup in participants' own living environment (setup time 15-30 minutes). The participants are instructed to perform 13 \\emph{complex ADLs}. Another project member remotely monitors the sensor readings, videos and system failure status. The entire session lasts from 2-4 hours of time depending on participants' physical and cognitive ability. We follow the standard protocol to annotate demographics and activities mentioned in the IRB. Two graduate students are engaged to annotate activities (postural, gestural and complex activity) whereas the observed activity performances are computed by the evaluator. Two more graduate students are engaged to validate the annotations on the videos. In overall, we are able to annotate 13 complex activities (total 291 samples) labeling for each participant; 8 hand gestures (total 43561 samples) and 4 postural activities (total 43561 samples) labeling. Annotation of postural and complex activities outcomes no difficulties from recorded videos. However, annotation of hand-gestures is extremely difficult in our scenario. We used video based hand tracker that can track and sketch wrist movements from a video episode \\cite{hugo14}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new language model is being developed to classify dialects of languages spoken in different regions. The model is trained on a dataset that consists of three languages, each with three dialects. The training data is divided into two stages: the first stage classifies the input into one of the three languages, and the second stage further classifies the input into one of the three dialects within the selected language. The model achieves a score of 58.54% on the Track-1 validation dataset. However, the authors of the paper also report that the model's performance on the Track-2 validation dataset is significantly higher, at 85.61%. What is the primary reason for the difference in performance between the two stages of the model?",
    "choices": [
      "A) The language models used in the second stage are more accurate than those used in the first stage, but the dialect classification task is more complex and requires more data.",
      "B) The dataset used for the Track-1 validation dataset is more diverse and representative of the languages spoken in different regions, while the dataset used for the Track-2 validation dataset is more limited and biased towards one particular region.",
      "C) The model's performance on the Track-2 validation dataset is higher because the dialect classification task is easier to solve when the input is already classified into a language, and the model can focus on the specific dialects within that language.",
      "D) The primary reason for the difference in performance between the two stages is that the language models used in the second stage are more fine-tuned to the specific dialects within each language, allowing the model to better capture the nuances of each dialect."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Paper Info\n\nTitle: Two-stage Pipeline for Multilingual Dialect Detection\nPublish Date: Unkown\nAuthor List: Ankit Vaidya (from Pune Institute of Computer Technology), Aditya Kane (from Pune Institute of Computer Technology) Figure\n\nFigure 1: Class distribution of dialects\nFigure 2: System diagram for dialect classification. The LID classifies the input into one of 3 languages. The sample is then further classified into dialects by language specific models. Figure 3: Confusion matrix of 9-way classification. Note that rows are normalized according to the number of samples is that class. Our complete results for Track-1 using the two-stage dialect detection pipeline.Model- * denotes the language of the models used for the experiments. Performance on Track-1 validation dataset of individual models used in the two-stage pipeline. \"Lg\" stands for language of the model used. Comparative results of two-way classification using the finetuned (F.T.) predictions and predictions adapted from three-way classification models. abstract\n\nDialect Identification is a crucial task for localizing various Large Language Models. This paper outlines our approach to the VarDial 2023 DSL-TL shared task. Here we have to identify three or two dialects from three languages each which results in a 9-way classification for Track-1 and 6-way classification for Track-2 respectively. Our proposed approach consists of a two-stage system and outperforms other participants' systems and previous works in this domain. We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase is available publicly 1 . Introduction\n\nLanguage has been the primary mode of communication for humans since the pre-historic ages. Studies have explored the evolution of language and outlined mathematical models that govern the intricacies of natural language . Inevitably, as humans established civilization in various parts of the world, this language was modified by, and for the group of people occupied by that particular geographical region."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary advantage of using high-quality twisted bilayer graphene (tBG) under low temperatures and sub-terahertz input for isolating the intrinsic Hall effect?",
    "choices": [
      "A) The clean limit condition allows for the suppression of extrinsic contributions, making it easier to distinguish the intrinsic effect from other phenomena. However, this also means that the intrinsic effect is only observable at very low temperatures, which may not be feasible for all experiments.",
      "B) The high-quality tBG samples under low temperatures and sub-terahertz input are ideal for isolating the intrinsic Hall effect because the extrinsic contributions are negligible, and the intrinsic effect is directly proportional to the relaxation time \u03c4.",
      "C) The intrinsic Hall effect in tBG can be isolated by using high-quality samples under low temperatures and sub-terahertz input, as the extrinsic contributions are suppressed, and the intrinsic effect is directly observable. However, this approach may not be suitable for all materials, as the relaxation time \u03c4 can vary significantly depending on the material properties.",
      "D) The primary advantage of using high-quality tBG under low temperatures and sub-terahertz input for isolating the intrinsic Hall effect lies in the fact that the clean limit condition allows for the suppression of extrinsic contributions, making it possible to distinguish the intrinsic effect from other phenomena. Additionally, the intrinsic effect is directly proportional to the relaxation time \u03c4, which can be evaluated quantitatively for each material, providing a benchmark for experiments."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Discussion. We have uncovered the crossed nonlinear dynamical intrinsic Hall effect characteristic of layer hybridized electronic states in twisted bilayers, and elucidated its geometric origin in the k -space curl of interlayer BCP. It offers a new tool for rectification and frequency doubling in chiral vdW bilayers, and is sizable in tTMD and tBG. Here our focus is on the intrinsic effect, which can be evaluated quantitatively for each material and provides a benchmark for experiments. There may also be extrinsic contributions, similar to the side jump and skew scattering ones in anomalous Hall effect. They typically have distinct scaling behavior with the relaxation time \u03c4 from the intrinsic effect, hence can be distinguished from the latter in experiments . Moreover, they are suppressed in the clean limit \u03c9\u03c4 1 [(\u03c9\u03c4 ) 2 1, more precisely] . In high-quality tBG samples, \u03c4 \u223c ps at room temperature . Much longer \u03c4 can be obtained at lower temperatures. In fact, a recent theory explaining well the resistivity of tBG predicted \u03c4 \u223c 10 \u22128 s at 10 K . As such, high-quality tBG under low temperatures and sub-terahertz input (\u03c9/2\u03c0 = 0.1 THz) is located in the clean limit, rendering an ideal platform for isolating the intrinsic effect. This work paves a new route to driving in-plane response by out-of-plane dynamical control of layered vdW structures . The study can be generalized to other observables such as spin current and spin polarization, and the in-plane driving can be statistical forces, like temperature gradient. Such orthogonal controls rely critically on the nonconservation of layer pseudospin degree of freedom endowed by interlayer coupling, and constitute an emerging research field at the crossing of 2D vdW materials, layertronics, twistronics and nonlinear electronics. This work is supported by the Research Grant Council of Hong Kong (AoE/P-701/20, HKU SRFS2122-7S05), and the Croucher Foundation. W.Y. also acknowledges support by Tencent Foundation. Cong Chen, 1, 2, * Dawei Zhai, 1, 2, * Cong Xiao, 1, 2, \u2020 and Wang Yao 1, 2, \u2021 1 Department of Physics, The University of Hong Kong, Hong Kong, China 2 HKU-UCAS Joint Institute of Theoretical and Computational Physics at Hong Kong, China Extra figures for tBG at small twist angles Figure (a) shows the band structure of tBG with \u03b8 = 1.47 \u2022 obtained from the continuum model ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the physiological signals from a subject who is experiencing cognitive, affective, and physical arousal. The subject's skin conductance and heart rate are being measured using EDA and PPG sensors, respectively. The researcher notices that the subject's skin conductance is increasing during cognitive arousal, but the heart rate is actually decreasing. What can be inferred about the relationship between skin conductance and heart rate during cognitive arousal?",
    "choices": [
      "A) During cognitive arousal, the subject's skin conductance and heart rate are both increasing, indicating a strong sympathetic nervous system response.",
      "B) The increase in skin conductance during cognitive arousal is due to the subject's physical arousal, which is causing the sweat glands to secrete more sweat.",
      "C) The decrease in heart rate during cognitive arousal is due to the subject's parasympathetic nervous system response, which is helping the subject to relax.",
      "D) The relationship between skin conductance and heart rate during cognitive arousal is complex, and both signals are influenced by the subject's sympathetic and parasympathetic nervous systems."
    ],
    "correct_answer": "D)",
    "documentation": [
      "A min-max normalization is applied that provides us a uniform range of the variables using $\nz_i=\\frac{x_i-min(x)}{max(x)-min(x)}$ equation where $x=\\{x1,\\ldots,x_n\\}$ and $z_i$ is $i^{th}$ normalized data. The final single dimensional score represents machine learning based TS score.\n\\section{Physiological Sensor Signals Processing} The autonomic nervous system (ANS) restrains the body's physiological activities including the heart rate, skin gland secretion, blood pressure, and respiration. The ANS is divided into sympathetic (SNS) and parasympathetic (PNS) branches. While SNS actuates the body's resources for action under arousal conditions, PNS attenuates the body to help regain the steady state. Mental arousal (say stress, anxiety etc.) activates the sweat gland causing the increment and reduction of  Skin Conductance on SNS and PNS physiological conditions respectively. However, Instant Heart Rate also has similar effect on SNS and PNS physiological condtions i.e., a higher value of heart rate is the effect of SNS and lower value is the outcome of PNS. EDA and PPG sensors are widely used to estimate the instant value of skin conductance and heart rate respectively \\cite{alam16}. \\subsection{EDA Sensor Signal Processing}\nEDA is the property of the human body that causes continuous variation in the electrical characteristics of the skin which varies with the state of sweat glands in the skin. There are three types of arousal: \\emph{cognitive, affective and physical}. \\emph{Cognitive} arousal occurs when a person tries to solve any problem using her cognitive ability. \\emph{Affective} arousal occurs when a person is worried, frightened or angry either doing daily activities or in resting position. On the other hand, \\emph{physical} arousal is related to the brain command to move bodily parts which is imposed on the total arousal as an artifact, called \\emph{motion artifact}. However, there are always some noises due to the weather conditions (temperature, humidity etc.) and device motion."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the development of the London docks during the 18th century, based on the accounts of various authors?",
    "choices": [
      "A) The London docks underwent significant expansion during the 18th century, driven primarily by the growth of international trade and the increasing demand for cargo space.",
      "B) The London docks were largely unaffected by the growth of international trade during the 18th century, as the city's ports were already well-established and efficient.",
      "C) The London docks were a major hub for the transportation of goods during the 18th century, but their development was hindered by the lack of adequate infrastructure and the limited availability of labor.",
      "D) The London docks were a key factor in the growth of the British Empire during the 18th century, as they provided a vital link between the city's ports and the colonies, facilitating the exchange of goods and ideas."
    ],
    "correct_answer": "D)",
    "documentation": [
      "[11] Survey of London, Early History of the Site. [12] Survey of London, Footnotes/n 10. [13] Survey of London, Historical Notes/No. 31. [14] David Hughson, LL.D., London; Being An Accurate History And Description Of The British Metropolis And Its Neighbourhood, To Thirty Miles Extent, From An Actual Perambulation, Vol. IV, (London: W. Stratford, 1807), 227. [15] The Reverend Joseph Nightingale, The Beauties of England and Wales: Or, Original Delineations, Topographical, Historical, and Descriptive, of Each County, Vol. X, Part III, Vol. II (London: J. Harris; Longman and Co.; J. Walker; R. Baldwin; Sherwood and Co.; J. and J. Cundee; B. and R. Crosby and Co.; J Cuthell; J. and J. Richardson; Cadell and Davies; C. and J. Rivington; and G. Cowie and Co., 1815), 245. [16] John Britton, F.S.A. & Co., ed., The Original Picture of London, Enlarged and Improved: Being A Correct Guide For The Stranger, As Well As For the Inhabitant, To The Metropolis Of The British Empire Together With A Description Of The Environs, The Twenty-Fourth Edition (London: Longman, Rees, Orme, Brown, and Green, 1826), 479. [17] Jared Sparks, The Works of Benjamin Franklin, Vol. VII, (Philadelphia: Childs & Peterson, 1840), 151. [18] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xx. [19] Peter Cunningham, Handbook for London; Past and Present, Vol. I, (London: John Murray, 1849), 245. [20] F. Saunders, Memories of the Great Metropolis: or, London, from the Tower to the Crystal Palace, (New York: G.P. Putnam, MDCCCLII), 138. [21] Leigh Hunt, The Town; Its Memorable Characters and Events, (London: Smith, Elder and Co., 1859), 185. [22] K. Baedeker, London and Its Environs, Including Excursions To Brighton, The Isle of Wight, Etc.: Handbook For Travelers, Second Edition, (London: Dulau and Co., 1879), 133. [23] Herbert Fry, London In 1880 Illustrated With Bird\u2019s-Eye Views of the Principal Streets, Sixth Edition, (New York: Scribner, Welford, & Co., 1880), 50."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the structural properties of Ge1-xMnx films grown at 130\u00b0C, based on the transmission electron micrographs and RHEED patterns?",
    "choices": [
      "A) The presence of nanocolumns in the film is indicative of a high degree of crystallinity, which is consistent with the diamond structure observed in the film.",
      "B) The streaky RHEED pattern at low growth temperatures suggests that the film is amorphous, and the presence of nanocolumns is a result of the amorphous structure.",
      "C) The high resolution image of the interface between the Ge1-xMnx film and the Ge buffer layer shows no defects that could be caused by the presence of nanocolumns, suggesting that the nanocolumns are not a result of defects in the film.",
      "D) The presence of nanocolumns in the film, as observed in the plane view micrograph, is consistent with the growth of Ge1-xMnx films at 130\u00b0C, and the diamond structure observed in the film is a result of the Mn doping, which leads to the formation of Ge3Mn5 clusters at the surface of the film."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Sample preparation was carried out by standard mechanical polishing and argon ion milling for cross-section investigations and plane views were prepared by wet etching with H$_3$PO$_4$-H$_2$O$_2$ solution \\cite{Kaga82}. \\begin{figure}[htb]\n    \\center\n    \\includegraphics[width=.29\\linewidth]{./fig1a.eps}\n    \\includegraphics[width=.29\\linewidth]{./fig1b.eps}\n    \\includegraphics[width=.29\\linewidth]{./fig1c.eps}\n    \\caption{RHEED patterns recorded during the growth of Ge$_{1-x}$Mn$_{x}$ films: (a) 2 $\\times$ 1 surface reconstruction of the germanium buffer layer. (b) 1 $\\times$ 1 streaky RHEED pattern obtained at low growth temperatures ($T_g<$180$^{\\circ}$C). (c) RHEED pattern of a sample grown at $T_g=$180$^{\\circ}$C. The additional spots reveal the presence of Ge$_3$Mn$_5$ clusters at the surface of the film.}\n\\label{fig1}\n\\end{figure}\n\n\\section{Structural properties \\label{structural}}\n\n\\begin{figure}[htb]\n    \\center\n\t\\includegraphics[width=.49\\linewidth]{./fig2a.eps}\n\t\\includegraphics[width=.49\\linewidth]{./fig2b.eps}\n\t \\includegraphics[width=.49\\linewidth]{./fig2c.eps}\n\t \\includegraphics[width=.49\\linewidth]{./fig2d.eps}\n    \\caption{Transmission electron micrographs of a Ge$_{1-x}$Mn$_{x}$ film grown at 130$^{\\circ}$C and containing 6 \\% of manganese. (a) cross-section along the [110] axis : we clearly see the presence of nanocolumns elongated along the growth axis. (b) High resolution image of the interface between the Ge$_{1-x}$Mn$_{x}$ film and the Ge buffer layer. The Ge$_{1-x}$Mn$_{x}$ film exhibits the same diamond structure as pure germanium. No defect can be seen which could be caused by the presence of nanocolumns. (c) Plane view micrograph performed on the same sample confirms the columnar structure and gives the density and size distribution of nanocolumns. (d) Mn chemical map obtained by energy filtered transmission electron microcopy (EFTEM). The background was carefully substracted from pre-edge images. Bright areas correspond to Mn-rich regions.}\n\\label{fig2}\n\\end{figure}\n\nIn samples grown at 130$^{\\circ}$C and containing 6 \\% Mn, we can observe vertical elongated nanostructures \\textit{i.e.} nanocolumns as shown in Fig."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The development of the MTA project has been marked by several challenges, including concerns over the role of Indian engineers and scientists in the design and development process. However, the project has made significant progress in recent years, with the first flight expected to take place in 2017-18. What is a likely outcome of the upcoming development of the \"BrahMos mini missile\" by the Indo-Russian joint venture BrahMos Aerospace?",
    "choices": [
      "A) The mini missile will have a significantly reduced range compared to the present BrahMos missile, due to the smaller size and lower payload capacity.",
      "B) The development of the mini missile will be hindered by the lack of experience and expertise of Indian engineers and scientists in the design and development process.",
      "C) The mini missile will have a similar range and payload capacity to the present BrahMos missile, due to the adoption of similar technology and design principles.",
      "D) The development of the mini missile will be accelerated by the collaboration between DRDO, NPOM lab, and BrahMos Aerospace, allowing for a faster and more efficient design and development process."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The cost of the $600 million project is being equally shared by the two countries. The MTA, when developed, will have ready market for 205 aircraft - 45 for the Indian Air Force, 100 for the Russian Air Force, and 60 more for exporting to friendly countries. The international market for MTA is estimated at 390 planes. Under the agreement, thirty percent of the annual production of planes could be exported to third countries. The MTA was expected to go in service with the Russian and Indian Air Forces in 2015. But the project faced a number of problems, delaying the development of the MTA. The project got into rough weather after India felt there was nothing much for Indian engineers and scientists to do in the design and development of the MTA. However, all the issues related to the project were resolved with the Russians when the HAL undertook to carry out design and development of its work-share of MTA at Aircraft R&D Centre at Bangalore. Russian Ilyushin Design Bureau and the Irkut Corporation and HAL are participating in the project. The first flight is expected to take place in 2017-18. The MTA would replace the AN- 32 aircraft being used by the IAF. It will be used for both cargo and troop transportation, para-drop and air drop of supplies, including low-altitude parachute extraction system. BrahMos missile exports a challenging proposition\nAnother key deal expected to be signed during the summit, is for the development of \u201cBrahMos mini missile\u201d by the Indo-Russian joint venture BrahMos Aerospace which manufactures supersonic cruise missile. BrahMos\u2019 new CEO Sudhir Mishra recently said he was hopeful that a deal to develop the mini version of the missile will be signed during Putin\u2019s summit with Modi. \u201cWe are hoping to sign a tripartite agreement between DRDO, NPOM lab and BrahMos Aerospace during the planned visit of Russian President in December,\u201d Mishra said. He said that the new missile will have a speed of 3.5 mach and carry a payload of 300 km up to a range of 290 km. In size, it will be about half of the present missile, which is around 10 metres long."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "Is there any mention of the stories of prophets outside the holy books? Are there any archeological evidences of each prophet\u2019s reign (and I stress prophet\u2019s)? Are there any mention of each prophet in history books outside its holy book, or the books that were based on it\u2019s holy books? Ok, that was too general, let\u2019s narrow it down a bit since your knowledge of your religion, mashallah, especially in reciting Quran is superb. Is there any mention of the prophet Mohammad reign in non-Moslem history books other than those referenced to Moslem books? And if there were; did those stories mach? Did archeology of Mohammad\u2019s time match those stories? Did ancient books of other nations confirm those stories? After all Mecca was an open, mid trade center for traders of the South and the North, and supposedly was exposed and open to other nations, and an emergence of a new religion would not have gone unnoticed in the ancient books of those nations. That's why faith is called \"FAITH\" to believe in the unseen, you see. keep in mind that every major religion has gone through various phases from emergence to growth to stagnation to depression and reformation. these are cycles that play out across centuries and i suspect islam is not immune to them. islam is unique in that it is truly the last 'great' religion and it is here to stay. the future should be one of consilience and consolidation between religions and peoples. at the end of the day - tolerance of thought will be the salvation of mankind. I've been asking your first question for as long as I can remember and nobody has given me a convincing answer yet. I also asked the second question verbatim when I was in my high school's Tarbiya Islamiya class, and the teacher had no answer. I kept hounding him until the students yelled at me to shut up and then he kicked me out! We're all adults here, so why can't you answer Anon's query? I still find it amazing that you named yourself after Saint Joan. A Christian (gasp!) who will burn in hell according to some of the comments here."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is interested in understanding the relationship between cognitive ability and daily activity performance in older adults. They collect data on the completeness of task (TC), sequential task ability (SEQ), and interruption avoidance capabilities (INT) of participants. However, they notice that the data on TC and SEQ are inconsistent, with some participants reporting higher or lower values than expected. Which of the following conclusions can be drawn from this inconsistency?",
    "choices": [
      "A) The researcher's activity features estimation model is not accurately capturing the relationship between cognitive ability and daily activity performance.",
      "B) The inconsistency in TC and SEQ data is due to the participants' inability to follow the gerontologist-defined standard sequences of sub-tasks.",
      "C) The researcher's data collection method is flawed, and the inconsistency in TC and SEQ data is a result of the participants' inability to accurately report their task completion and sequence abilities.",
      "D) The inconsistency in TC and SEQ data suggests that the relationship between cognitive ability and daily activity performance is more complex than initially thought, and may require a more nuanced understanding of the underlying factors."
    ],
    "correct_answer": "D)",
    "documentation": [
      "At first, we obtain instant hand gestural and postural activities from our above proposed models, and additionally motion sensor and object sensor readings from our IoT-system for every time instant generating a 4-hierarchy of HDBN model. Considering the context set $\\langle gestural, postural, ambient,object\\rangle$ as a hierarchical activity structure (extending two 2-hierarchical HDBN \\cite{alam16b}), we build complex activity recognition model for single inhabitant scenario. Finally, we infer the most-likely sequence of complex activities (and their time boundaries), utilizing the well-known Expectation Maximization (EM) algorithm \\cite{dempster77} for training and the Viterbi algorithm \\cite{forney73} for run-time inference. \\section{Automatic Activity Features Estimation}\nThe effects of cognitive ability on daily activity performance have been studied before \\cite{dawadi14,akl15}. They experimentally and clinically validated that cognitive impairment highly reduces the daily activity performances and this activity performance can be computed as an indicator of cognitive ability status of older adults. The standard activity features refer to completeness of task (TC), sequential task ability (SEQ), interruption avoidance capabilities (INT) etc. In current behavioral science literature, the above activity features carry specific definition based on the sub-tasks involved with a complex activity \\cite{dawadi14,akl15}. Completeness of task refers to how many sub-tasks are missed by the participants. Sequential task ability refers to how many sequences of sub-tasks are missed referring the gerontologist defined standard sequences of the sub-task for the particular complex activity. Interruption avoidance capability refers to how many times the participants stop or interleave while doing any sub-task. The final goal of activity features estimation is to provide overall task score. The task score is proportional to the functional ability of participants in performance daily activities."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A genetic counselor is advising a couple who are both carriers of alpha-thalassemia trait. The woman has a 'cis' type of alpha-thalassemia trait, meaning she has deletions of two alpha globin genes on the same chromosome 16. The man has a silent alpha-thalassemia trait, meaning he has an absence of one alpha globin gene. What is the probability that their child will inherit the 'cis' type of alpha-thalassemia trait?",
    "choices": [
      "A) The child will definitely inherit the 'cis' type of alpha-thalassemia trait, as both parents are carriers.",
      "B) The child will have a 50% chance of inheriting the 'cis' type of alpha-thalassemia trait, as each parent has a 50% chance of passing on the deleted gene.",
      "C) The child will have a 25% chance of inheriting the 'cis' type of alpha-thalassemia trait, as the man's silent alpha-thalassemia trait reduces the likelihood of passing on the deleted gene.",
      "D) The child will have a 100% chance of inheriting the 'cis' type of alpha-thalassemia trait, as the woman's 'cis' type trait guarantees the deletion of two alpha globin genes."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Therefore, hemoglobin E is unique in that it is both a quantitative (i.e. thalassemia-like) and qualitative trait. When co-inherited with a beta thalassemia trait, it causes a disease that is almost indistinguishable from beta thalassemia disease. Large deletions around and including the beta globin gene can lead to delta/beta thalassemia or hereditary persistence of fetal hemoglobin (HPFH). Interestingly, delta/beta thalassemia trait behaves very similarly to beta thalassemia trait in its clinical manifestations. However, HPFH trait does not tend to cause hemoglobin disease when co-inherited with a second thalassemia or other beta globin mutation. ALPHA-THALASSEMIA. Most individuals have four normal copies of the alpha globin gene, two copies on each chromosome 16. These genes make the alpha globin component of normal adult hemoglobin, which is called hemoglobin A. Alpha globin is also a component of fetal hemoglobin and the other major adult hemoglobin called hemoglobin A2. Mutations of the alpha globin genes are usually deletions of the gene, resulting in absent production of alpha globin. Since there are four genes (instead of the usual two) to consider when looking at alpha globin gene inheritance, there are several alpha globin types that are possible. Absence of one alpha globin gene leads to a condition known as silent alpha thalassemia trait. This condition causes no health problems and can be detected only by special genetic testing. Alpha thalassemia trait occurs when two alpha globin genes are missing. This can occur in two ways. The genes may be deleted from the same chromosome, causing the 'cis' type of alpha thalassemia trait. Alternately, they may be deleted from different chromosomes, causing the 'trans' type of alpha thalassemia trait. In both instances, there are no associated health problems, although the trait status may be detected by more routine blood screening. Hemoglobin H disease results from the deletion of three alpha globin genes, such that there is only one functioning gene."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of stability analysis for a given differential equation, consider the following two cases: (G) and (L). For case (G), the function g(x) is defined as g(x) := (\u221a(\u03b2-\u03b1+2) * (N + (\u03b1-2) * |x|^2 / (1 + |x|^2))) and the function u(x) is defined as u(x) := -((\u03b2-\u03b1+2) / 2) * ln(1 + |x|^2). Using the Hardy inequality, one can show that u(x) is a stable sub-solution of (G) if the following inequality holds for all \u03c8 \u2208 C_c^\u221e:\n\n\u222b[g(x) * \u03c8^2] / ((1 + |x|^2)^(-\u03b1/2 + 1)) \u2264 \u222b[|\u2207\u03c8|^2] / ((1 + |x|^2)^(-\u03b1/2))\n\nFor case (L), the function g(x) is defined as g(x) := (\u221a(\u03b2-\u03b1+2) / (p-1)) * (N + ((\u03b1-2) - (\u221a(\u03b2-\u03b1+2) / (p-1))) * |x|^2 / (1 + |x|^2)) and the function u(x) is defined as u(x) := (1 + |x|^2)^(-\u221a(\u03b2-\u03b1+2) / (2 * (p-1))). Using the same approach as in case (G), one can show that u(x) is a stable sub-solution of (L) if the following inequality holds for all \u03c8 \u2208 C_c^\u221e:\n\n\u222b[g(x) * \u03c8^2] / ((1 + |x|^2)^(-\u03b1/2 + 1)) \u2264 \u222b[|\u2207\u03c8|^2] / ((1 + |x|^2)^(-\u03b1/2))\n\nWhich of the following statements is true?",
    "choices": [
      "A) The stability of u(x) in case (G) is guaranteed if the inequality holds for all values of \u03b1, \u03b2, and N.",
      "B) The stability of u(x) in case (L) is guaranteed if the inequality holds for all values of p, \u03b1, and \u03b2.",
      "C) The stability of u(x) in case (G) is guaranteed if the inequality holds for all values of \u03b1 and \u03b2, regardless of the value of N.",
      "D) The stability of u(x) in case (L) is guaranteed if the inequality holds for all values of p, \u03b1, and \u03b2, regardless of the value of N."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\text {for all} \\ \\ x\\in \\mathbb{R}^N\n\\end{equation*}\n we get stability. \\item ($\\beta-\\alpha+2>0$) In the case of $(G)$ we take   $u(x)=-\\frac{\\beta-\\alpha+2}{2} \\ln(1+|x|^2)$ and $g(x):= (\\beta-\\alpha+2)(N+(\\alpha-2)\\frac{|x|^2}{1+|x|^2})$. By a computation one sees that $u$ is a sub-solution of $(G)$ and hence we need now to only show the stability, which amounts to showing that\n\\begin{equation*}\n\\int \\frac{g(x)\\psi^2}{(1+|x|^{2   }) ^{-\\frac{\\alpha}{2}+1}}\\le \\int\\frac{|\\nabla\\psi|^2}{    (1+|x|^2)^{-\\frac{\\alpha}{2}}     },\n\\end{equation*} for all $ \\psi \\in C_c^\\infty$.  To show this we use  Corollary \\ref{Hardy}. So  we  need to choose an appropriate $t$ in   $-\\frac{\\alpha}{2}\\le t\\le\\frac{N-2}{2}$  such that for all $x\\in {\\mathbb{R}}^N$ we have\n \\begin{eqnarray*}\n (\\beta-\\alpha+2)\\left(    N+  (\\alpha-2)\\frac{|x|^2}{1+|x|^2}\\right)         &\\le& (t+\\frac{\\alpha}{2})^2 \\frac{ |x|^2 }{(1+|x|^2}\\\\\n&&+(t+\\frac{\\alpha}{2}) \\left(N-2(t+1)   \\frac{|x|^2}{1+|x|^2}\\right). \\end{eqnarray*}\nWith a  simple calculation one sees we need just to have\n   \\begin{eqnarray*}\n (\\beta-\\alpha+2)&\\le& (t+\\frac{\\alpha}{2}) \\\\\n  (\\beta-\\alpha+2) \\left(    N+  \\alpha-2\\right)      &   \\le&  (t+\\frac{\\alpha}{2}) \\left(N-t-2+\\frac{\\alpha}{2}) \\right). \\end{eqnarray*}     If one takes $ t= \\frac{N-2}{2}$ in the case where $ N \\neq 2$ and $ t $ close to zero in the case for $ N=2$ one easily sees the above inequalities both hold, after considering all the constraints on $ \\alpha,\\beta$ and $N$.\n\n We now consider the case of $(L)$. Here one takes $g(x):=\\frac {\\beta-\\alpha+2}{p-1}(    N+  (\\alpha-2-\\frac{\\beta-\\alpha+2}{p-1})\n\\frac{|x|^2}{1+|x|^2})$ and $ u(x)=(1+|x|^2)^{ -\\frac  {\\beta-\\alpha+2}{2(p-1)} }$. Using essentially the same approach as in $(G)$ one shows that $u$ is a stable sub-solution of $(L)$ with this choice of $g$.   \\\\\nFor the case of $(M)$ we take   $u(x)=(1+|x|^2)^{ \\frac  {\\beta-\\alpha+2}{2(p+1)}   }$ and $g(x):=\\frac {\\beta-\\alpha+2}{p+1}(    N+  (\\alpha-2+\\frac{\\beta-\\alpha+2}{p+1})"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company uses a channel generator to create a public channel for its customers to share content. The channel generator receives a stream of content from the scoring engine, which ranks the content items based on their global popularity and quality specific to the source stream. The global scorer normalizes the score across streams to ensure comparability. However, the company notices that the top-ranked items in their channel are not always the most relevant to their customers. What is a possible reason for this discrepancy?",
    "choices": [
      "A) The global scorer is biased towards channels with more followers, causing it to prioritize items from those channels.",
      "B) The channel generator is not properly configured to account for the varying quality of content across different streams.",
      "C) The company's customers are not providing accurate feedback on the relevance of the content items, leading to an inaccurate ranking.",
      "D) The global scorer is using a flawed algorithm that fails to consider the nuances of human judgment in evaluating content relevance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The channel generator 378 then resubmits the request based on the changes made by the user. In response to the request, the channel generator 378 receives a stream of content from the scoring engine 211 and generates the channel for the user. The generated channel is either public or private depending upon the user's preferences. In one embodiment, the user shares the channel to a community, a group of people or any internet user. The channel is then displayed to the user with an interface generated by the user interface engine 260. Referring now to FIG. 3B, one embodiment of a scoring engine 211 is shown in more detail. The scoring engine 211 includes a query generator 301, a global scorer 302 and a content stream generator 304 that are each coupled to signal line 228. The global scorer 302 is used to rank new content items that are stored in the data storage server 265 or memory 237 (depending upon the embodiment). The global scorer 302 uses signals from the different verticals to compute a global user-independent score for each item to approximate its popularity or importance within the stream that produced it. The global scorer 302 normalizes the score across streams so that items from various streams are comparable to aid in generating a quick yet reasonable ranking of items. The global score is a combination of its quality specific to the source stream (depending on the rank of the source, number of known followers of a source, etc.) and its global popularity (trigger rate on universal search, relevance to trending queries, number of clicks, long clicks received, etc.). The global scorer 302 transmits the global score to storage where it is associated with the item. The global score helps rank the items for faster retrieval. For example, if the query generated by the query generator 301 includes a request for the top ten items about skiing, those items are already organized in the data storage server 265 or memory 237 according to the global score. The query generator 301 receives a request for a stream of content for a channel from the channel engine 240. The query generator 301 generates a query based on the channel attributes that are included in the request."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new numerical method, called Nonlinear Fokker-Planck Acceleration (NFPA), has been developed to accelerate the convergence of radiative transfer in clouds. This method is based on the quasi-diffusion method and has shown significant improvement in the convergence rate for problems in electron transport. NFPA returns a modified Fokker-Planck (FP) equation that preserves the angular moments of the flux given by the transport equation. This preservation of moments is particularly appealing for applications to multiphysics problems. However, the method's accuracy is sensitive to the choice of discretization scheme. Which of the following statements best describes the relationship between NFPA and the quasi-diffusion method?",
    "choices": [
      "A) NFPA is a direct extension of the quasi-diffusion method, with the same discretization scheme used for both methods.",
      "B) NFPA is a variant of the quasi-diffusion method, but with a different discretization scheme that is more suitable for problems in electron transport.",
      "C) NFPA is a generalization of the quasi-diffusion method, but only for problems in slab geometry, and the discretization scheme used is the same as that for the quasi-diffusion method.",
      "D) NFPA is a new method that combines the quasi-diffusion method with a different discretization scheme, specifically designed to preserve the angular moments of the flux given by the transport equation."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In order to speed up the convergence of radiative transfer in clouds, a quasi-diffusion method has been developed \\cite{aristova}. In addition, the DSA-multigrid method was developed to solve problems in electron transport more efficiently \\cite{trucksin}. One of the most recent convergence methods developed is Fokker-Planck Synthetic Acceleration (FPSA) \\cite{JapanFPSA,japanDiss}. FPSA accelerates up to $N$ moments of the angular flux and has shown significant improvement in the convergence rate for the types of problems described above. The method returns a speed-up of several orders of magnitude with respect to wall-clock time when compared to DSA  \\cite{JapanFPSA}. In this paper, we introduce a new acceleration technique, called \\textit{Nonlinear Fokker-Planck Acceleration} (NFPA). This  method  returns  a  modified  Fokker-Planck (FP) equation  that  preserves  the  angular moments of the flux given by the transport  equation. This preservation of moments is particularly appealing for applications to multiphysics problems \\cite{multiphysics}, in which the coupling between the transport physics and the other physics can be done through the (lower-order) FP equation. To our knowledge, this is the first implementation of a numerical method that returns a Fokker-Planck-like equation that is discretely consistent with the linear Boltzmann equation. This paper is organized as follows. \\Cref{sec2} starts with a brief description of FPSA. Then, we derive the NFPA scheme. In \\cref{sec3}, we discuss the discretization schemes used in this work and present numerical results. These are compared against standard acceleration techniques. We conclude with a discussion in \\cref{sec4}. \\section{Fokker-Planck Acceleration}\\label{sec2}\n\\setcounter{equation}{0} In this section we briefly outline the theory behind FPSA, describe NFPA for monoenergetic, steady-state transport problems in slab geometry, and present the numerical methodology behind NFPA. The theory given here can be easily extended to higher-dimensional problems."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A vehicle is traveling at a speed of 60 km/h on a straight road. The driver has a forward safety margin of 10 meters and a rear safety margin of 5 meters. The vehicle's length is 15 meters. If the driver needs to stop within a distance of 20 meters to avoid a collision, what is the minimum distance the vehicle can travel before the driver needs to apply the brakes?",
    "choices": [
      "A) The driver needs to stop within 20 meters, so the vehicle can travel up to 20 meters before applying the brakes.",
      "B) Since the vehicle's length is 15 meters, the driver needs to stop within 20 meters, which means the vehicle can travel up to 35 meters before applying the brakes.",
      "C) The driver needs to stop within 20 meters, but the vehicle's length is 15 meters, so the driver needs to stop within 5 meters before applying the brakes.",
      "D) The vehicle's forward and rear safety margins are 10 meters and 5 meters, respectively, and the vehicle's length is 15 meters. To determine the minimum distance the vehicle can travel before the driver needs to apply the brakes, we need to consider the sum of the vehicle's length and the sum of the forward and rear safety margins. Therefore, the minimum distance the vehicle can travel before the driver needs to apply the brakes is 15 + 10 + 5 = 30 meters."
    ],
    "correct_answer": "D)",
    "documentation": [
      "L f s \u2228 \u2206s \u2264 \u2212L r s where L f s and L r s are forward and rear safety margins respectively. This can be represented with the following linear constraints:\ns \u2265 0 where M 0 (big-M) and c \u2208 {0, 1} is responsible for making a choice between the two constraints."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the emitting region of the source from the XMM-Newton EPIC data, considering the model normalization and the assumption of a distance of 50 kpc?",
    "choices": [
      "A) The emitting region is likely to be smaller than 4.5 \u00d7 10^8 cm, as the model normalization is highly constrained at the lower bound.",
      "B) The emitting region is consistent with that of moderately massive white dwarfs, as the model normalization is essentially unconstrained at the lower bound, and the source is emitting at 59 eV.",
      "C) The emitting region is likely to be larger than 4.5 \u00d7 10^8 cm, as the XMM-Newton EPIC data show a high-energy lightcurve peaking above 0.75 ct s^-1, indicating a high-background period.",
      "D) The emitting region is consistent with that of moderately massive white dwarfs, as the model normalization yields a maximum radius of 4.5 \u00d7 10^8 cm, which is consistent with the expected size of moderately massive white dwarfs, and the source is emitting at 59 eV."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The source has faded by a\n  factor of $>100$ since the XMM-Newton revolution 1210 slew\n  discovery. The solid line show the best fit to the spectra. The\n  ratio of the data to the best fit model is shown in the lower panel.\n}\n\\label{xrtspec}\n\\end{figure}\n\nA cautious estimate of the size of the emitting region can be obtained\nfrom the model normalization; the assumed distance of 50\\,kpc yields a\nmaximum radius of 4.5$\\times$10$^{8}$\\,cm (the fit normalization is\nessentially unconstrained at the lower bound). Though great care\nshould be taken in interpreting this result, as the black body model\nis possibly overestimating the luminosity, this obtained radius is\nstill consistent with that of moderately massive ($>$1.1$M_{\\odot}$)\nwhite dwarfs (Hamada \\& Salpeter 1961), i.e.\\,the whole white dwarf\nsurface may still be emitting at 59\\,eV.\n\n\\section{Dedicated XMM-Newton observations}\n\nWe were granted an XMM-Newton Target of Opportunity (ToO) observation,\nonce the source became again visible to XMM-Newton, and a 10\\,ks\nXMM-Newton EPIC observation was made on 19th June 2007 (see\nTable~\\ref{slewtable}). All the XMM-Newton EPIC data, i.e.  the data\nfrom the two MOS cameras and the single pn camera, were taken in\nfull-frame mode with the thin filter in place. These data from the\nthree EPIC instruments have been reprocessed using the standard\nprocedures in XMM-Newton SAS (Science Analysis System) $-$ v.7.1.0. Periods of high-background, of which there were very few, were\nfiltered out of each dataset by creating a high-energy 10$-$15\\,keV\nlightcurve of single events over the entire field of view, and\nselecting times when this lightcurve peaked above 0.75\\,ct s$^{-1}$\n(for pn) or 0.25\\,ct s$^{-1}$ (for MOS). This resulted in\n$\\approx$9.4(8.0)\\,ks of low-background MOS(pn) data. Details of this dedicated\nXMM-Newton observation, together with source position, and\n(0.2$-$2\\,keV) all-EPIC combined (pn, MOS1, MOS2) detected source\ncounts, count rate and detection likelihood are given in\nTable~\\ref{slewtable}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new experiment is designed to test the effectiveness of different learning configurations for predicting RF signals in a dynamic environment. The results show that the average RMSE for trajectories is lower when using a configuration with high interaction matrix values (M1 = 25, M2 = 25) compared to a configuration with low interaction matrix values (M1 = 5, M2 = 5). However, the average RMSE for RF signals is higher when using the high interaction matrix configuration. Which of the following statements best explains this apparent contradiction?",
    "choices": [
      "A) The high interaction matrix configuration is better suited for predicting RF signals, but the lower RMSE for trajectories indicates that the model is better at tracking the vehicles' positions.",
      "B) The high interaction matrix configuration is actually worse at predicting RF signals, but the lower RMSE for trajectories indicates that the model is better at tracking the vehicles' positions.",
      "C) The high interaction matrix configuration is better suited for predicting RF signals, but the lower RMSE for trajectories indicates that the model is better at tracking the vehicles' positions when the interaction matrix values are low.",
      "D) The high interaction matrix configuration is better suited for predicting RF signals, but the lower RMSE for trajectories indicates that the model is better at tracking the vehicles' positions when the interaction matrix values are high."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Predicted RF signal from: (a) vehicle 1 using $\\mathrm{M_{1}}{=}5$, $\\mathrm{M_{2}}{=}5$, (b) vehicle 1 using $\\mathrm{M_{1}}{=}25$, $\\mathrm{M_{2}}{=}25$, (c) vehicle 2 using $\\mathrm{M_{1}}{=}5$, $\\mathrm{M_{2}}{=}5$, (d) vehicle 2 using $\\mathrm{M_{1}}{=}25$, $\\mathrm{M_{2}}{=}25$.}\n            \\label{fig_situation1_PredictedRF}\n    \\end{center}\n\\end{figure}\n\n\\begin{figure}[t!]\n    \\begin{center}\n        \\begin{minipage}[b]{.49\\linewidth} \\centering\n            \\includegraphics[width=4.8cm]{Results/GPSfromRF_situation1_best}\n        \\\\[-1.0mm]\n        {\\scriptsize (a)}\n        \\end{minipage}\n        \\begin{minipage}[b]{0.49\\linewidth}\n            \\centering\n            \\includegraphics[width=4.8cm]{Results/GPSfromRF_situation1_worst}\n            \\\\[-1.0mm]\n            {\\scriptsize (b)}\n        \\end{minipage}\n        %\n        \\caption{An example visualizing the predicted and observed trajectories of two vehicles interacting in the environment. (a) $\\mathrm{M_{1}}{=}5$, $\\mathrm{M_{2}}{=}5$, (b) $\\mathrm{M_{1}}{=}25$, $\\mathrm{M_{2}}{=}25$.}\n            \\label{fig_situation1_VehiclesTrajectories}\n    \\end{center}\n\\end{figure}\n\n\\begin{figure}[ht!]\n    \\begin{center}\n        \\begin{minipage}[b]{.49\\linewidth} \\centering\n            \\includegraphics[width=4.8cm]{Results/rmse_on_trajectory}\n        \\\\[-1.0mm]\n        {\\scriptsize (a)}\n        \\end{minipage}\n        \\begin{minipage}[b]{0.49\\linewidth}\n            \\centering\n            \\includegraphics[width=4.8cm]{Results/rmse_on_RFSignal}\n            \\\\[-1.0mm]\n            {\\scriptsize (b)}\n        \\end{minipage}\n        \\caption{The average RMSE after testing different experiences and examples of: (a) trajectories and (b) RF signals.}\n            \\label{fig_rmse_onTraj_onSig}\n    \\end{center}\n\\end{figure}\n\nFig.~\\ref{fig_situation1_PredictedRF} illustrates an example comparing between predicted RF signals and observed ones based on two different configurations in learning the interactive matrix (as shown in Fig.~\\ref{fig_interactiveMatrices})."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a potential consequence of BC's decision not to have a dominant, long-tenured coach like TOB, and how does this relate to the school's overall culture and values?",
    "choices": [
      "A) The lack of a dominant coach may lead to a more collaborative team environment, where players and coaches work together to achieve common goals. This could result in improved team chemistry and a more cohesive unit.",
      "B) Without a dominant coach, BC may struggle to establish a strong identity and sense of tradition, leading to a lack of direction and purpose on the field.",
      "C) The absence of a long-tenured coach may actually lead to more stability and continuity, as new coaches are not burdened by the weight of previous decisions and can focus on building a new foundation.",
      "D) The decision not to have a dominant coach may actually be a positive development for BC, as it allows for a more meritocratic approach to leadership and rewards players and coaches who demonstrate exceptional talent and work ethic, rather than simply being loyal to a particular coach or system."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Labels: Kevin Pierre-Louis, Links, Phil Steele, Steve Hailey\nNo King at BC may be a good thing\nNearly everything that needs to be said about Penn State and Joe Paterno has been said. But one point that Bruce Feldman made regarding College Football's King culture reminded me of how different BC is from most FBS schools. We don't and never have had a \"King.\" I've often longed for that iconic winner that could serve as the face of the program, but maybe it is better that we don't. For whatever we miss by not having a \"Woody\" or a \"JoePa\" or \"Bear\" we also avoid the power struggles and corruption that often comes with an all-powerful Head Coach. What does a King really provide anyway? Branding...a little nostalgia...someone to embrace. But it doesn't really win you football games or make you a better school. Florida State's losing in the final Bowden years proved that and Penn State has a permanent stain on their whole community because of their Kingdom. BC's had some dynamic leaders, but none have ever stayed long enough to reach statue status. Our winningest coach and longest tenured even left and no one put up a fight to keep him. TOB could have been our icon. A little more fire and one or two more critical wins and we would have celebrated him like other schools have done with their biggest winners. But it wasn't to be and that's probably a good thing. When I've profiled coaching candidates in the past, I've hoped that one would stay for a long time. That's changed. I don't want a guy who want to be bigger than the University. Things are better that way. Labels: Bruce Feldman, college football, penn state, TOB\nKey Players for 2012: Emmett Cleary\nThis is a series on the key players for the 2012 season. Big things are expected for some, while others will need to improve over their previous performances. If 2012 is a good year, it will be in part to the key players overachieving. Senior Offensive Tackle, Emmett Cleary\nWhat he's been: A long-time starter and one of the leaders on offense. Cleary has played on both sides of the line and at both guard and tackle."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new machine learning algorithm is being proposed to reconstruct a network of interacting particles from noisy data. The algorithm uses a combination of techniques to improve its performance. In particular, it employs a decimation procedure to reduce the dimensionality of the data, and then uses a regularization technique to prevent overfitting. The algorithm is compared to two other methods: a mean-field technique and a technique that uses $l_2$ regularization. Which of the following statements best describes the key advantage of the proposed algorithm over the other two methods?",
    "choices": [
      "A) The proposed algorithm is able to reconstruct the network with higher accuracy than the other two methods when the sample size is small.",
      "B) The proposed algorithm is able to reconstruct the network with higher accuracy than the other two methods when the sample size is large, but the mean-field technique is able to reconstruct the network with higher accuracy when the sample size is small.",
      "C) The proposed algorithm is able to reconstruct the network with higher accuracy than the other two methods when the sample size is large, but the mean-field technique is able to reconstruct the network with higher accuracy when the sample size is small.",
      "D) The proposed algorithm is able to reconstruct the network with higher accuracy than the other two methods when the sample size is large, and it is able to reconstruct the network with higher accuracy than the mean-field technique even when the sample size is small."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Once the most likely network has been identified through the decimation procedure, we perform the same analysis displayed in Fig. \\ref{Jor1_dec}  for ordered and then quenched disordered real-valued couplings\nand in Fig. \\ref{Jor3_dec} for  complex-valued ordered couplings. In comparison to the results shown in Sec. \\ref{sec:res_reg},\n  the PLM with decimation leads to rather cleaner results. In Figs. \\ref{MF_PL_err} and \\ref{MF_PL_TP} we compare the performances of the PLM with decimation in respect to ones of the PLM with $l_2$-regularization. These two techniques are also analysed in respect to a mean-field technique previously implemented on the same XY systems\\cite{Tyagi15}. For what concerns the network of connecting links, in Fig. \\ref{MF_PL_TP} we compare the TP curves obtained with the three techniques. The results refer to the case of ordered and real valued couplings, but similar behaviours were obtained for the other cases analysed. The four graphs are related to different sample sizes, with $M$ increasing clockwise. When $M$ is high enough, all techniques reproduce the true network. However, for lower values of $M$ the performances of the PLM with $l_2$ regularization and with decimation drastically overcome those ones of the previous mean field technique. In particular, for $M=256$ the PLM techniques still reproduce the original network while the mean-field method fails to find more than half of the couplings. When $M=128$, the network is clearly reconstructed only through the PLM with decimation while the PLM with $l_2$ regularization underestimates the couplings. Furthermore, we notice that the PLM method with decimation is able to clearly infer the network of interaction even when $M=N$ signalling that it could be considered also in the under-sampling regime $M<N$.  \n \n \nIn Fig. \\ref{MF_PL_err} we compare the temperature behaviour of the reconstruction error. In can be observed that for all temperatures and for all sample sizes  the reconstruction error, ${\\rm err_J}$, (plotted here in log-scale) obtained with the PLM+decimation is always smaller than \nthat one obtained with the other techniques."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the evolution of a neural network in a 2D environment with moving embodied agents. The network's fitness is measured as the total food consumed over its lifetime, and the Pearson correlation coefficient of the evolved agent's weights with the ingredient value vector of the current environment is 0.8. However, the researcher notices that the network's performance is not improving over generations, despite the increasing fitness. What could be the reason for this stagnation?",
    "choices": [
      "A) The network's motor network is too small, limiting the number of interactions with the environment, and the sensory network's plasticity parameters are not evolving fast enough to adapt to the changing environment.",
      "B) The network's weights are becoming too specialized to the current environment, making it difficult for the network to generalize to new situations, and the reward-prediction learning rule is not effective in this case.",
      "C) The network's motor network is too large, causing the output to be too noisy, and the sensory network's plasticity parameters are not evolving fast enough to compensate for the noise.",
      "D) The network's learning rule is not effective in this case because the reward threshold (\u03b3) is too high, requiring the network to receive a high reward to update its weights, and the Pearson correlation coefficient is not a reliable measure of the network's performance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The qualitative relation between \u03b7 p and parameters of environment d e , \u03c3 and p tr is preserved in the changed experiment. However, the resulting learning rule is significantly different (Fig. ). The evolution converges to the following learning rule: In both cases, the rule has the form \u2206W t = \u03b7 p X t [\u03b1 y R t + \u03b2 y ]. Thus, the \u2206W t is positive or negative depending on whether the reward R t is above or below a threshold (\u03b3 = \u2212\u03b2 y /\u03b1 y ) that depends on the output decision of the network (y t = 0 or 1). Both learning rules (for the reward-prediction and decision tasks) have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold. These similarities indicate some common organizing principles of reward-modulated learning rules, but their significant differences highlight the sensitivity of the optimization process to task details. We now turn to the moving embodied agents in the 2D environment. To optimize these agents, both the motor network's connections and the sensory network's plasticity parameters evolve simultaneously. Since the motor network is initially random and the agent has to move to find food, the number of interactions an agent experiences in its lifetime can be small, slowing down the learning. However, having the larger motor network also has benefits for evolution because it allows the output of the plastic network to be read out and transformed in different ways, resulting in a broad set of solutions. The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. e. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red). In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study found that the City of Paso Robles' water rate increase was not only being delayed in court, but also that the City Attorney's office had been receiving large sums of money from the law firm hired to represent the city. What is a likely explanation for the City Attorney's actions?",
    "choices": [
      "A) The City Attorney is simply trying to ensure that the city's interests are represented in court, and is therefore receiving compensation for their services.",
      "B) The City Attorney is being blackmailed by the law firm, and is using the money to fund their own campaign for re-election.",
      "C) The City Attorney is being bribed by the law firm to delay the rate increase, allowing the city to continue to spend money on unnecessary projects.",
      "D) The City Attorney is being forced to receive the money by the law firm, as a condition of their contract to represent the city, and is therefore obligated to use the funds for the firm's benefit."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Time to clean house in Paso Robles Home\nFront Page \u00bb Time to clean house in Paso Robles\nSeptember 5, 2010 Opinion By JIM REED\nI\u2019d like to give you an update on the issue of our civil servants cramming hundreds of millions of dollars in spending down our throats after the people of Paso Robles voted down the water rate increase last November. The rate increase is being hung up in the courts by the City Attorney. What was supposed to be a quick issue to get in front of a judge, has been drug out as long as possible by the City Attorney. Even if the courts throw out the current rate increase, I expect that our civil servants will just change a couple of words in the rate increase notice and force the same old plan on us again. There is a real problem with the people we have hired to work for us in Paso Robles. It seems that decisions are made based on some agenda, even if it is contrary to citizens\u2019 wishes. City Councilmen Ed Steinbeck, Nick Gilman and Mayor Duane Picanco, on August 19th, voted unanimously to hire the same law firm employed by the City of Bell. You may have heard the recent news story about the City of Bell\u2019s corrupt city representatives. This law firm allowed the elected officials and City employees to pillage the General Fund for their own benefit, contrary to the rights and interests of the citizens. We are already paying several City employees $12,000 per month with equally ridiculous benefits and pensions. What does this say about our elected representatives? I believe most residents are like me. We elect people we believe have our best interest in mind. Over the last few years I have seen that nothing is farther from the truth. The people we have elected have lost track of the fact that \u201cthe City\u201d exists to protect and deliver services to the citizens. To them it is some all-important ideal they strive to cultivate and improve according to their agenda. They have forgotten that they are elected to represent the citizens. We have an election coming up in November. We have the opportunity to elect some responsible, principled people to represent us."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Director of Town and Country Planning is responsible for preparing regional plans for the State. However, the Director's powers and duties are subject to certain limitations. What is the primary limitation on the Director's ability to prepare regional plans?",
    "choices": [
      "A) The Director can only prepare regional plans for areas that have been declared regions by the State Government, and only after a survey of the region has been completed.",
      "B) The Director can only prepare regional plans for areas that are not part of a region that has been amalgamated or divided.",
      "C) The Director can only prepare regional plans for areas that are not part of a region that has been declared to cease to be a region or part thereof.",
      "D) The Director can only prepare regional plans for areas that have been designated as regions by the State Government, and only after the Director has received approval from the State Government for the plan."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Director of Town and Country Planning\n3. Director and other officers. - (1) After the commencement of this Act the State Government shall, by notification in the Official Gazette, appoint an officer for the purpose of carrying out functions assigned to him under this Act, as the Director of Town and Country Planning for the State and may appoint such other categories of officers as it may deem fit. (2) The Director shall exercise such powers and perform such duties as are conferred or imposed upon him by or under this Act and the officers appointed to assist the Director shall, within such area as the State Government may specify, exercise such powers and perform such duties conferred and imposed on the Director by or under this Act as the State Government may, by special or general order, direct. (3) The officers appointed to assist the Director shall be subordinate to him and shall work under his guidance, supervision and control. 4. Establishment of regions. - (1) The State Government may, by notification,-\n(a) declare any area in the State to be a region for the purposes of this Act;\n(b) define the limits of such area; and\n(c) specify the name by which such region shall be known. (2) The State Government may, by notification, alter the name of any such region and on such alteration, any reference in any law or instrument or other document to the region shall be deemed to be a reference to the region as re-named unless expressly otherwise provided or the context so requires.\n(3) The State Government may, by notification,-\n(a) alter the limits of a region so as to include therein or exclude therefrom Such area as may be specified in the notification;\n(b) amalgamate two or more regions so as to form one region;\n(c) divide any region into two or more region; or\n(d) declare that the whole or part of the area comprising a region shall cease to be a region or part thereof. 5. Director to prepare regional plan. - Subject to the provisions of this Act and the rules made thereunder, it shall be the duty of the Director-\n(i) to carry out a survey of the regions;\n(ii) to prepare an existing land use map; and\n(iii) to prepare a regional plan."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the distribution of C$_2$H emission in the observed star-forming regions, based on the fact that the original SMA data showed good images for all spectral lines except C$_2$H, and the re-worked compact configuration data revealed a larger-scale distribution of C$_2$H emission?",
    "choices": [
      "A) The C$_2$H emission is concentrated in the compact configuration data, indicating a smaller-scale distribution.",
      "B) The C$_2$H emission is distributed on smaller scales than the other spectral lines, suggesting that the compact configuration is more sensitive to smaller-scale features.",
      "C) The C$_2$H emission is distributed on larger scales than the other spectral lines, but the compact configuration is more sensitive to smaller-scale features, leading to a larger-scale distribution of C$_2$H emission.",
      "D) The C$_2$H emission is distributed on larger scales than the other spectral lines, and the re-worked compact configuration data revealed a larger-scale distribution of C$_2$H emission, indicating that the compact configuration is more sensitive to larger-scale features."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Although\nC$_2$H was previously observed in low-mass cores and Photon Dominated\nRegions (e.g., \\citealt{millar1984,jansen1995}), so far it was not\nsystematically investigated in the framework of high-mass star\nformation.\n\n\\section{Observations}\n\\label{obs}\n\nThe 21 massive star-forming regions were observed with the Atacama\nPathfinder Experiment (APEX) in the 875\\,$\\mu$m window in fall 2006. We observed 1\\,GHz from 338 to 339\\,GHz and 1\\,GHz in the image\nsideband from 349 to 350\\,GHz. The spectral resolution was\n0.1\\,km\\,s$^{-1}$, but we smoothed the data to\n$\\sim$0.9\\,km\\,s$^{-1}$. The average system temperatures were around\n200\\,K, each source had on-source integration times between 5 and 16\nmin. The data were converted to main-beam temperatures with forward\nand beam efficiencies of 0.97 and 0.73, respectively\n\\citep{belloche2006}. The average $1\\sigma$ rms was 0.4\\,K.  The main\nspectral features of interest are the C$_2$H lines around 349.4\\,GHz\nwith upper level excitation energies $E_u/k$ of 42\\,K (line blends of\nC$_2$H$(4_{5,5}-3_{4,4})$ \\& C$_2$H$(4_{5,4}-3_{4,3})$ at\n349.338\\,GHz, and C$_2$H$(4_{4,4}-3_{3,3})$ \\&\nC$_2$H$(4_{4,3}-3_{3,2})$ at 349.399\\,GHz). The beam size was $\\sim\n18''$.\n\nThe original Submillimeter Array (SMA) C$_2$H data toward the\nHMPO\\,18089-1732 were first presented in \\citet{beuther2005c}. There\nwe used the compact and extended configurations resulting in good\nimages for all spectral lines except of C$_2$H. For this project, we\nre-worked on these data only using the compact configuration. Because\nthe C$_2$H emission is distributed on larger scales (see\n\\S\\ref{results}), we were now able to derive a C$_2$H image. The\nintegration range was from 32 to 35\\,km\\,s$^{-1}$, and the achieved\n$1\\sigma$ rms of the C$_2$H image was 450\\,mJy\\,beam$^{-1}$.  For more\ndetails on these observations see \\citet{beuther2005c}. \\section{Results}\n\\label{results}\n\nThe sources were selected to cover all evolutionary stages from IRDCs\nvia HMPOs to UCH{\\sc ii}s. We derived our target list from the samples\nof \\citet{klein2005,fontani2005,hill2005,beltran2006}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If the Federal Reserve's intervention in the LTCM crisis was successful in averting a systemic economic collapse, what can be inferred about the potential consequences of inadequate regulation of over-the-counter derivatives?",
    "choices": [
      "A) The lack of regulation would lead to a significant increase in financial innovation, as firms would be incentivized to take on more risk to maximize returns. This, in turn, would lead to a more stable financial system, as firms would be better equipped to manage their risk exposure.",
      "B) The absence of regulation would result in a decrease in financial innovation, as firms would be less likely to take on risk due to the uncertainty surrounding the regulatory environment. This, in turn, would lead to a less stable financial system, as firms would be less able to manage their risk exposure.",
      "C) The lack of regulation would lead to a significant decrease in financial innovation, as firms would be less likely to take on risk due to the uncertainty surrounding the regulatory environment. However, this would not necessarily lead to a more stable financial system, as firms would still be able to manage their risk exposure through other means.",
      "D) The lack of regulation would lead to a significant increase in financial instability, as firms would be more likely to take on excessive risk due to the uncertainty surrounding the regulatory environment. This, in turn, would lead to a less stable financial system, as firms would be less able to manage their risk exposure."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Their response dismissed Born's analysis and focused on the hypothetical possibility that CFTC regulation of swaps and other OTC derivative instruments could create a \"legal uncertainty\" regarding such financial instruments,  hypothetically reducing the value of the instruments. They argued that the imposition of regulatory costs would \"stifle financial innovation\" and encourage financial capital to transfer its  transactions offshore. The disagreement between Born and the Executive Office's top economic policy advisors has been described not only as a classic Washington turf war, but also a war of ideologies,  insofar as it is possible to argue that Born's actions were consistent with Keynesian and neoclassical economics while Greenspan, Rubin, Levitt, and Summers consistently espoused neoliberal, and neoconservative policies. In 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse. Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions. The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures. Born stated, \"I thought that LTCM was exactly what I had been worried about\". In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance. After intervention by the Federal Reserve, the crisis was averted. In congressional hearings into the crisis, Greenspan acknowledged that language had been introduced into an agriculture bill that would prevent CFTC from regulating the derivatives which were at the center of the crisis that threatened the US economy. U.S. Representative Maurice Hinchey (D-NY) asked \"How many more failures do you think we'd have to have before some regulation in this area might be appropriate?\" In response, Greenspan brushed aside the substance of Born's warnings with the simple assertion that \"the degree of supervision of regulation of the over-the-counter derivatives market is quite adequate to maintain a degree of stability in the system\"."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "God comes and gives them a spanking and sends them out of the garden. Ok, so the details might be a little off\u2026 maybe even a little forgettable. But the story is familiar and the consequences tragic. We live in those consequences. So what does this story have to tell us about the world today and our lives in it? You can learn a lot about sin and humanity by really chewing on the details here. Consider:\nThe serpent (we\u2019re later told it\u2019s also Satan) begins with questioning what God has said. The question overgeneralizes and invites a conversation. The woman adds to God\u2019s commandment. The serpent challenges God and offers a desirable half-truth. The woman (although perfect) is tempted. That temptation draws her to inspect the fruit. Looking at the fruit, the woman focused on the positive side of the equation. She risked her life trusting the serpent over God, because eating the fruit should have meant certain death. The man ate without any signs of a struggle. Their eyes were open. Before they knew only the good; now they knew good and evil. Their first response to sin is to cover up, which indicates fear and probably shame. Their response to God (their creator whom they knew personally!) was to hide. Apparently they either didn\u2019t know or forgot that God is everywhere and knows everything. God asks the man a question for effect. The man blames his wife and even seems to accuse God. The woman blames the serpent and even seems to deflect by saying she was tricked. At this point God punishes the serpent, the man, and the woman by cursing all creation. Work will be hard, childbearing will be painful. But there is hope in the promise of One to come who will crush the serpent. I love how the Good News glimmers even in that first dark moment. It\u2019s so tempting to read more into the story because there are so many more details we wish we had. The gaps in the story invite our imaginations to jump in, but we need to be careful not to put words in God\u2019s mouth. One theme we see in the Fall is one broken relationship after another."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary driver of the observed decrease in the abundance of simple diatomic molecules (e.g., H$_2$) in high-mass star-forming regions, as inferred from spectral line surveys?",
    "choices": [
      "A) The rapid formation of complex molecules through gas-phase reactions, which deplete the reservoir of simple diatomic species.",
      "B) The increased ionization of the gas by the intense radiation fields from the forming stars, leading to the destruction of neutral molecules.",
      "C) The efficient sputtering of molecules from the surface of dust grains, which are then destroyed by the high-energy particles in the protostellar environment.",
      "D) The chemical evolution of the gas, driven by the interplay between gas-phase reactions, dust-grain surface chemistry, and the formation of complex molecules through associative reactions, which ultimately lead to the depletion of simple diatomic species."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\section{Introduction}\n\nSpectral line surveys have revealed that high-mass star-forming\nregions are rich reservoirs of molecules from simple diatomic species\nto complex and larger molecules (e.g.,\n\\citealt{schilke1997b,hatchell1998b,comito2005,bisschop2007}). However, there have been rarely studies undertaken to investigate the\nchemical evolution during massive star formation from the earliest\nevolutionary stages, i.e., from High-Mass Starless Cores (HMSCs) and\nHigh-Mass Cores with embedded low- to intermediate-mass protostars\ndestined to become massive stars, via High-Mass Protostellar Objects\n(HMPOs) to the final stars that are able to produce Ultracompact H{\\sc\n  ii} regions (UCH{\\sc ii}s, see \\citealt{beuther2006b} for a recent\ndescription of the evolutionary sequence). The first two evolutionary\nstages are found within so-called Infrared Dark Clouds (IRDCs). While\nfor low-mass stars the chemical evolution from early molecular\nfreeze-out to more evolved protostellar cores is well studied (e.g.,\n\\citealt{bergin1997,dutrey1997,pavlyuchenkov2006,joergensen2007}),\nit is far from clear whether similar evolutionary patterns are present\nduring massive star formation. To better understand the chemical evolution of high-mass star-forming\nregions we initiated a program to investigate the chemical properties\nfrom IRDCs to UCH{\\sc ii}s from an observational and theoretical\nperspective. We start with single-dish line surveys toward a large\nsample obtaining their basic characteristics, and then perform\ndetailed studies of selected sources using interferometers on smaller\nscales. These observations are accompanied by theoretical modeling of\nthe chemical processes. Long-term goals are the chemical\ncharacterization of the evolutionary sequence in massive star\nformation, the development of chemical clocks, and the identification\nof molecules as astrophysical tools to study the physical processes\nduring different evolutionary stages. Here, we present an initial\nstudy of the reactive radical ethynyl (C$_2$H) combining single-dish\nand interferometer observations with chemical modeling."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "Ann's Mega Dub: 12/19/10 - 12/26/10\nGot o have a penis to be an expert\nThursday on NPR's Fresh Air, Terry Gross wanted to talk film and music. Since women don't know a thing about either and aren't interested in either, Terry had to find men who were 'experts. 'This is C.I.'s \" Iraq snapshot Friday, December 24, 2010. Chaos and violence continue, Nouri's incomplete Cabinet continues to receive criticism, a father offers an 'excuse' for killing his own daughter, and more. Marci Stone (US Headlines Examiner) reports, \"Friday afternoon, Santa is currently in Baghdad, Iraq and on his next stop is Moscow, Russia, according to the 2010 NORAD Santa Tracker. The North American Aerospace Defense Command (NORAD) has been tracking Santa as he makes his annual journey throughout the world.\" Gerald Skoning (Palm Beach Post) quotes Santa saying, \"We send our special wishes for peace and goodwill to all. That includes the people of Iraq, Afghanistan, Iran and North Korea.\" Please note that this is Santa's seventh trip to Iraq since the start of the Iraq War and, as usual, his journey was known in advance. No waiting until he hit the ground to announce he was going to Iraq -- the way George The Bully Boy Bush had to and the way US President Barack Obama still has to. In the lead up to Santa's yearly visit, many 'authorities' in Iraq began insisting that Christmas couldn't be celebrated publicly, that even Santa was banned. Gabriel Gatehouse (BBC News) quotes Shemmi Hanna stating, \"I wasn't hurt but I wish that I had been killed. I wish I had become a martyr for this church, but God kept me alive for my daughters.\" Shemmi Hanna was in Our Lady of Salvation Church in Baghdad when it was assaulted October 31st and she lost her husband, her son, her daughter-in-law and her infant grandson in the attack. The October 31st attack marks the latest wave of violence targeting Iraqi Christians. The violence has led many to flee to northern Iraq (KRG) or to other countries. Zvi Bar'el (Haaretz) notes, \"This week the Iraqi legislature discussed the Christians' situation and passed a resolution in principle to help families who fled."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher simulates the deformation of a bubble in a fluid using a computational model, and the results show that the bubble's length and width exhibit oscillations due to complex wave patterns. However, the researcher notices that the simulation parameters used to characterize the bubble's morphology are not sufficient to capture the effects of specific-heat ratio on the bubble's behavior. To address this issue, the researcher adjusts the simulation parameters to include the effects of specific-heat ratio. Which of the following statements is true about the researcher's approach?",
    "choices": [
      "A) The researcher assumes that the effects of specific-heat ratio on the bubble's behavior are negligible, and therefore, the simulation parameters are sufficient to capture the bubble's morphology.",
      "B) The researcher uses a simplified model that neglects the effects of specific-heat ratio on the bubble's behavior, and therefore, the simulation parameters are sufficient to capture the bubble's morphology.",
      "C) The researcher uses a model that assumes a constant specific-heat ratio for the entire simulation, and therefore, the simulation parameters are sufficient to capture the bubble's morphology.",
      "D) The researcher uses a model that accounts for the effects of specific-heat ratio on the bubble's behavior, and therefore, the simulation parameters are sufficient to capture the bubble's morphology, including the oscillations due to complex wave patterns."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Finally, the jet structure disappears. The second quantitative comparison is the interface structure described by the length and width of the bubble, as shown in Fig. . The experimental data are extracted from Fig. , in Ref. . Quantitative agreements between DBM simulation and experimental results are seen. For the profile of bubble width, there are mainly two stages. At an early time (t < 150\u00b5s), it decreases to a minimum value because of the shock compression effect. After the shock wave passes through the bubble (t > 150\u00b5s), the developed vortex pair caused by the deposited vorticity gradually dominates the growth of bubble width. Different from width evolution, the temporal variation of length experiences three stages. In the early stages (t < 150\u00b5s), it decreases quickly due to the shock compression effect. Then, the jet structure emerges, which results in a growth in length (150\u00b5s < t < 250\u00b5s). Because the upstream interface moves faster than the downstream interface, the bubble length would decrease at 250\u00b5s < t < 500\u00b5s. In the third stage (t > 500\u00b5s), the vortex pair forms and then leads to a continuous development of bubble length. Both the length and width experience oscillations in the later stages due to complex wave patterns. The quantitative agreements between DBM simulation and experimental results indicate the following two facts: (i) the order of TNE considered in the current DBM is sufficient, (ii) the choosing of discrete velocities and spatial-temporal steps and simulation parameters like the relaxation times is suitable for characterizing the deformation of bubble, wave patterns, main characteristics of flow morphology. Effects of specific-heat ratio on SBI\n\nThe major of current works on SBI research have not focused on specific-heat ratio effects. In this part, the simulation parameters are fine-adjusted based on the parameters in Section 3.1 to highlight the influence of specific-heat ratio. Through adjusting the extra degree of freedom I, five cases with various specific-heat ratios of the bubble are simulated, i.e., \u03b3 = 1.4,\n1.28, 1.18, 1.12, and 1.09."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the conversion of vitamin K1 to vitamin K2 in the testes is not dependent on gut bacteria, but rather occurs through a process that involves the removal of the phytyl tail of K1 to produce menadione as an intermediate. However, the same study also found that tissues with high amounts of MK-4 have a remarkable capacity to convert up to 90% of the available K1 into MK-4. Which of the following statements best describes the relationship between the conversion of K1 to MK-4 and the function of phylloquinone in plants?",
    "choices": [
      "A) The conversion of K1 to MK-4 is a direct result of the function of phylloquinone in plants, which is to facilitate the production of MK-4.",
      "B) The conversion of K1 to MK-4 is a separate process from the function of phylloquinone in plants, which is to facilitate the production of MK-4.",
      "C) The conversion of K1 to MK-4 is a result of the removal of the phytyl tail of K1, which is a byproduct of the function of phylloquinone in plants.",
      "D) The conversion of K1 to MK-4 is a result of the removal of the phytyl tail of K1, which is a necessary step in the production of MK-4, and is facilitated by the function of phylloquinone in plants."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Phylloquinone has a phytyl side chain. The MK-4 form of vitamin K2 is produced by conversion of vitamin K1 in the testes, pancreas, and arterial walls.[22] While major questions still surround the biochemical pathway for this transformation, the conversion is not dependent on gut bacteria, as it occurs in germ-free rats[23][24] and in parenterally-administered K1 in rats.[25][26] In fact, tissues that accumulate high amounts of MK-4 have a remarkable capacity to convert up to 90% of the available K1 into MK-4.[27][28] There is evidence that the conversion proceeds by removal of the phytyl tail of K1 to produce menadione as an intermediate, which is then condensed with an activated geranylgeranyl moiety (see also prenylation) to produce vitamin K2 in the MK-4 (menatetrione) form.[29]\nVitamin K2[edit]\nMain article: Vitamin K2\nVitamin K2 (menaquinone) includes several subtypes. The two subtypes most studied are menaquinone-4 (menatetrenone, MK-4) and menaquinone-7 (MK-7). Vitamin K1, the precursor of most vitamin K in nature, is a stereoisomer of phylloquinone, an important chemical in green plants, where it functions as an electron acceptor in photosystem I during photosynthesis. For this reason, vitamin K1 is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach), but it occurs in far smaller quantities in other plant tissues (roots, fruits, etc.). Iceberg lettuce contains relatively little. The function of phylloquinone in plants appears to have no resemblance to its later metabolic and biochemical function (as \"vitamin K\") in animals, where it performs a completely different biochemical reaction. Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are often (but not always) situated within specific protein domains called Gla domains. Gla residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins.[30]\nAt this time[update], 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes:\nBlood coagulation: prothrombin (factor II), factors VII, IX, and X, and proteins C, S, and Z[31]\nBone metabolism: osteocalcin, also called bone Gla protein (BGP), matrix Gla protein (MGP),[32] periostin,[33] and the recently discovered Gla-rich protein (GRP).[34][35]\nVascular biology: growth arrest-specific protein 6 (Gas6)[36]\nUnknown function: proline-rich \u03b3-carboxyglutamyl proteins (PRGPs) 1 and 2, and transmembrane \u03b3-carboxy glutamyl proteins (TMGs) 3 and 4.[37]\nLike other lipid-soluble vitamins (A, D and E), vitamin K is stored in the fatty tissue of the human body."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company is considering implementing a new data analytics platform to improve its customer service. The platform would allow the company to analyze customer behavior and preferences, and provide personalized recommendations to customers. However, the company is concerned about the potential impact on its employees, who may feel that the platform would automate their jobs. The company's CEO has stated that the platform is necessary to stay competitive in the market and to improve customer satisfaction. The company's HR department has also expressed concerns about the potential impact on employee morale and job security.",
    "choices": [
      "A) Implementing the platform would likely lead to increased employee morale, as it would provide them with more opportunities for professional development and growth.",
      "B) The platform would not have a significant impact on employee morale, as it would primarily be used for data analysis and would not require employees to perform tasks that are currently automated.",
      "C) The platform would not have a significant impact on employee morale, as it would be a necessary step to improve customer satisfaction and stay competitive in the market.",
      "D) The platform would likely lead to decreased employee morale, as it would automate jobs and reduce the need for human interaction."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The first two of those values -- it will not surprise you to know -- were freedom of discourse and individual privacy. But there were more: freedom of economic choice is one; the general welfare another; popular sovereignty, worth pausing on, I described as avoiding concentrations of economic and political power in any sector of industry or government that impinge unduly on the freedoms or welfare of the citizenry. And then there is progress, social progress, the fostering, I said, of market incentives and opportunities for technological and service innovations and for widened consumer choice among technologies and services. Now obviously if you give just a moment's thought to it, you will recognize, as I think we have in this conference, that these values can collide with each other at key points, and therefore accommodations must be made. For that we need processes of accommodation. I also suggested some of those. After you identify the relevant values and goals, you then should ask yourself about the necessity and the appropriateness of having government make any decision on the matter. And this has to do with such things like the adequacy of decision-making standards, the availability of adequate information, and the adequacy of personnel resources to deal with it. Then you get into dividing up the possible roles of the various elements of government -- the regulatory agencies, the Executive Branch, the Judiciary, and the Congress. It doesn't stop there, because you need to ask about international implications, which we have done some of here. And federal/state implications -- very often allowing the state to make a stab at social ordering in the first instance is, as Justice Brandeis often said, the best way, through the social laboratory technique, to try out what is the right answer, without endangering the whole society. And as we have heard today, we need also to think about the availability of non-coercive instruments of accommodation, like a federal data protection board."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher studying the history of electrical engineering notes that the discovery of the photoelectric effect by Albert Einstein in 1905 led to the development of the first solid-state devices. However, they also find that the first solid-state device, the \"cat's-whisker detector\", was actually invented in the 1900s by \u00c1nyos Jedlik, a Hungarian inventor. Which of the following statements best describes the relationship between the discovery of the photoelectric effect and the development of solid-state devices?",
    "choices": [
      "A) The discovery of the photoelectric effect directly led to the development of the first solid-state devices, which were used to amplify radio signals.",
      "B) The development of solid-state devices was a consequence of the discovery of the photoelectric effect, but only if the researchers who developed the devices also understood the principles of electromagnetism.",
      "C) The discovery of the photoelectric effect was a necessary condition for the development of solid-state devices, but it was not a sufficient condition, as the devices required a solid crystal to function.",
      "D) The development of solid-state devices was a result of the convergence of several factors, including the discovery of the photoelectric effect, the work of inventors like \u00c1nyos Jedlik, and advances in electrical engineering."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian \u00d8rsted and Andr\u00e9-Marie Amp\u00e8re in 1819\u20131820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his \"On Physical Lines of Force\" in 1861 and 1862. While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ott\u00f3 Bl\u00e1thy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, \u00c1nyos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life. In 1887, Heinrich Hertz:843\u201344 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for \"his discovery of the law of the photoelectric effect\". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially. The first solid-state device was the \"cat's-whisker detector\" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new company, specializing in cloud infrastructure, has acquired a leading provider of server hardware. This acquisition is expected to significantly reduce the company's reliance on third-party foundries for its server chips. However, the company's CEO has stated that the acquisition will not lead to a reduction in the number of different server architectures available to customers. Instead, the company plans to continue offering a range of server options, each with its own unique features and performance characteristics. What is the most likely reason for this decision?",
    "choices": [
      "A) The company wants to maintain a strong relationship with its existing foundry partners and ensure a steady supply of high-quality server chips.",
      "B) The company is trying to differentiate its products from those of its competitors by offering a wider range of server architectures.",
      "C) The company is concerned that a reduction in server architecture options would lead to a decrease in innovation and competitiveness in the market.",
      "D) The company is planning to use the acquired foundry to produce a single, standardized server architecture that will be used across all of its products."
    ],
    "correct_answer": "D)",
    "documentation": [
      "I know this article is narrowly focused on the hardware, but MS and Intel getting into hardware, Amazon getting into hardware, Google buying Moto, this is all vertical integration. How can you support the idea that this trend will be reversed with no real justification? I'm sure mobile chips will continue to specialize, but I don't think this means what you think it means. Automobile companies started making their own engines and with rare exceptions, never went back to being more horizontal. Same with retail and their store brands. Same with cloud companies and their servers. Same with mobile companies and their OSs. The horizontal market of PCs created by long-lasting standards and loose hegemony is the exception, not the norm. Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale? I'm not so sure about several things:1- Moore's law's relevance. Moore's Law is about ICs. ICs are not as big a part of mobile computers as they are of desktops, even of laptops: screens, batteries, radios are a huge part of tablets' and phones' costs, as opposed to the bare SoC + RAM.2- The tablet vs phone dichotomy. For some reason (probably price insensitivity due to subsidies), Phones have a tendency to be more powerful than Tablets, ie phone SoCs are more than good enough for tablets. Since the OS and peripherals are the same, it makes more sense to design and build just one type of SoC, and just disable the phone-modem part of it (even the other radios are still required: BT, Wifi, GPS...), same as Intel disable cache and cores for their entry-level CPUs. Once you're fabbing a SoC, it makes more sense to make more of the same than to setup a separate run of a cut-down SoC on an older process, unless volumes are huge. We might still be getting previous-generation, well amortized SoCs in cheaper tablets, though.3- On the contrary, I see a tablet and phone convergence (the ugly phablet). I'm patiently waiting for the new 6\"+ phones to replace my Nook Color and Galaxy Note 1 with a single device.4- The advantage of diversity ?"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the relocation of the county seat from Sweadal to McPherson in 1870?",
    "choices": [
      "A) The McPherson Town Company offered a 10-year lease on rooms and a donation of land, which was deemed more attractive than Sweadal's location.",
      "B) The County Commissioners were influenced by the town's proximity to the Kansas River, which made it a more convenient location for trade and commerce.",
      "C) The town's population had grown significantly since the previous year's election, making it the most populous location in the county.",
      "D) The County Commissioners had previously visited McPherson and were impressed by its accessibility and natural resources, making it an ideal location for the county seat."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In 1868, Solomon Stephens and L. N. Holmberg were appointed Justices of the Peace\u2014the first officers in what is now McPherson County. The next year (1869) occurred the first election for the township, now the county of McPherson. McPherson was regularly organized as a county in the spring of 1870, a mass meeting being held at Sweadal. Sweadal, the county seat thus selected, was located about one mile and a half southwest of the present site of Lindsborg. In September, however, the County Commissioners resolved to meet at the latter place, McPherson which had already been located some two years. In April, 1873, a petition was filed for the county seat re-location. It was signed by 483 voters, and a special election was accordingly ordered for June 10. Upon that day, McPherson received 605 votes, New Gottland 325, King City 3 and Lindsborg 1; McPherson's majority over all, 276. In May the McPherson Town Company had offered, as an inducement for the location of the county seat at this point, the free use of rooms for ten years, and the donation of two squares of land on the town site. The offer was accepted the next month, the County Commissioners selecting blocks 56 and 65. Thus the county seat was established at McPherson and has remained since. As early as 1875, city leaders of Marion held a meeting to consider a branch railroad from Florence. In 1878, Atchison, Topeka and Santa Fe Railway and parties from Marion County and McPherson County chartered the Marion and McPherson Railway Company. In 1879, a branch line was built from Florence to McPherson, in 1880 it was extended to Lyons, in 1881 it was extended to Ellinwood. The line was leased and operated by the Atchison, Topeka and Santa Fe Railway. The line from Florence to Marion, was abandoned in 1968. In 1992, the line from Marion to McPherson was sold to Central Kansas Railway. In 1993, after heavy flood damage, the line from Marion to McPherson was abandoned. The original branch line connected Florence, Marion, Canada, Hillsboro, Lehigh, Canton, Galva, McPherson, Conway, Windom, Little River, Mitchell, Lyons, Chase, then connected with the original AT&SF main line at Ellinwood."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A woman, who had been living in a remote area for several years, decided to leave her husband and move to the city to pursue a career in writing. She had been struggling to make ends meet and felt that the city would offer her more opportunities. However, she had a young child who was still in school and would need to be cared for. She also had a significant amount of debt that she would need to pay off. What was the most likely reason for her decision to leave her husband?",
    "choices": [
      "A) She wanted to escape the harsh realities of rural life and experience the luxuries of city living.",
      "B) She was seeking a more stable and secure financial future for herself and her child.",
      "C) She was trying to get away from her husband's controlling behavior and needed a fresh start.",
      "D) She was motivated by a desire to escape the constraints of traditional rural life and forge a new path for herself."
    ],
    "correct_answer": "B)",
    "documentation": [
      "The Silver Veil (1982)\nSpellbound (1982) Hunter's Moon (1982)\nGirl at Cobalt Creek (1983)\nNo Alternative (1983)\nHouse of Memories (1983)\nAlmost a Stranger (1984) A place called Rambulara (1984)\nFallen Idol (1984) Hunt the Sun (1985)\nEagle's Ridge (1985) The Tiger's Cage (1986)\nInnocent in Eden (1986)\nDiamond Valley (1986)\nMorning Glory (1988) Devil Moon (1988)\nMowana Magic (1988)\nHungry Heart (1988)\nRise of an Eagle (1988) One Fateful Summer (1993) The Carradine Brand (1994)\nHolding on to Alex (1997)\nThe Australian Heiress (1997) Claiming His Child (1999) The Cattleman's Bride (2000)\nThe Cattle Baron (2001)\nThe Husbands of the Outback (2001)\nSecrets of the Outback (2002) With This Ring (2003)\nInnocent Mistress (2004)\nCattle Rancher, Convenient Wife (2007)\nOutback Marriages (2007) Promoted: Nanny to Wife (2007)\nCattle Rancher, Secret Son (2007) Genni's Dilemma (2008)\nBride At Briar Ridge (2009) Outback Heiress, Surprise Proposal (2009)\nCattle Baron, Nanny Needed (2009)\n\nLegends of the Outback Series\nMail Order Marriage (1999) The Bridesmaid's Wedding (2000)\nThe English Bride (2000)\nA Wife at Kimbara (2000)\n\nKoomera Crossing Series\nSarah's Baby (2003)\nRunaway Wife (2003)\nOutback Bridegroom (2003)\nOutback Surrender (2003)\nHome to Eden (2004)\n\nMcIvor Sisters Series\nThe Outback Engagement (2005)\nMarriage at Murraree (2005)\n\nMen Of The Outback Series\nThe Cattleman (2006)\nThe Cattle Baron's Bride (2006)\nHer Outback Protector (2006)\nThe Horseman (2006)\n\nOutback Marriages Series\nOutback Man Seeks Wife (2007) Cattle Rancher, Convenient Wife (2007)\n\nBarons of the Outback Series Multi-Author\nWedding At Wangaree Valley (2008) Bride At Briar's Ridge (2008) Family Ties Multi-Author\nOnce Burned (1995) Hitched! Multi-Author\nA Faulkner Possession (1996) Simply the Best Multi-Author\nGeorgia and the Tycoon (1997) The Big Event Multi-Author\nBeresford's Bride (1998)\n\nGuardian Angels Multi-Author\nGabriel's Mission (1998)\n\nAustralians Series Multi-Author\n7. Her Outback Man (1998)\n17. Master of Maramba (2001)\n19. Outback Fire (2001)\n22."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary consequence of Samsung and TSMC's involvement in the ARM project, considering the historical context of the mobile industry's shift towards horizontal integration?",
    "choices": [
      "A) The increased competition between Samsung and TSMC will lead to a decrease in the overall market share of Intel and other manufacturers, as the new ARM-based SoCs will be more power-efficient and cost-effective.",
      "B) The partnership between Samsung and TSMC will result in a significant increase in the production capacity of ARM, allowing the company to dominate the market and reduce its reliance on Intel.",
      "C) The involvement of Samsung and TSMC in the ARM project will lead to a decrease in the number of phone manufacturers, as the new ARM-based SoCs will be more expensive and less compatible with existing designs.",
      "D) The partnership between Samsung and TSMC will put significant pressure on Intel to adapt to the changing market landscape, potentially leading to a shift in Intel's focus towards more power-efficient and cost-effective SoCs."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This will increase yields dramatically. Of course Samsung and TSMC will build ARM out, but it definitely puts quite a bit of pressure on all other manufacturers. As the article mentions Intel and Samsung are the only ones who control production top to bottom, and Samsung must share some of the benefits with ARM. As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP. Last edited by paul5ra on Wed Feb 13, 2013 11:06 am\nintroiboad wrote:I am happy to see Kanter here at Ars, I like his writing and he maintains Real World Tech, where Linus Torvalds often shows up to comment on CPU arch and other interesting topics. Indeed. Most tech writing in this area is atrocious. This piece is one of the few well informed articles I've read in a long time. ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new dataset for contour completion tasks, similar to FlatShapeNet, has been released, which includes shapes with varying levels of complexity. This new dataset is expected to provide a more realistic challenge for contour completion models. What is a key difference between the \"Simple\" and \"Complex\" datasets used in the original experiments?",
    "choices": [
      "A) The \"Complex\" dataset contains only shapes with a single gap, while the \"Simple\" dataset contains shapes with multiple gaps.",
      "B) The \"Complex\" dataset was generated using a different algorithm than the \"Simple\" dataset, resulting in more varied and realistic shapes.",
      "C) The \"Complex\" dataset contains only shapes with overlapping regions, while the \"Simple\" dataset contains shapes with non-overlapping regions.",
      "D) The \"Complex\" dataset was created by combining the \"Simple\" dataset with additional shapes that have varying levels of complexity, including fragmented lines and more gaps."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In our experiments, we tried two datasets of images with different gap sizes. We observed that the best the \u03b3 for one set of samples is \u223c 5 (the set with shorter gaps) while it is \u223c 23 for samples from the other set, i. e, the set with longer gaps (see Figure for some completed examples). Experiments and Results\n\nPerforming unsupervised contour completion is a difficult task to benchmark as one can never know what fragments exactly are connected to each other in a real-world scenario. This makes the problem of contour completion a hard problem to solve. In this paper, we tried to create artificial shapes that are occluded by some masks and then tried to see if our model can regenerate the missing pieces and glue those divided contours together. To demonstrate our model's behavior, we will conduct experiments on datasets created for this task and will report on them in this section. To compare network results in different settings, we will use pixel-wise Mean Squared Error (MSE) and Intersection over Union (IoU) between the produced result of the network and unmasked ground truth data and the reconstructed image on black pixels (where contours live). Data\n\nWe prepared two datasets, one labeled \"Simple\" and one \"Complex\", in accordance with the number of gaps in each shape. Both datasets contain nine different categories of shapes. In order to generate the Complex dataset, we used FlatShapeNet which is a dataset for the educational game Ariga. The dataset includes the following categories: Circle, Kite, Parallelogram, Rectangle, Rhombus, Square, Trapezoid, Triangle and Overlap. The \"overlap\" category contains images that are made as a mixture of two shapes that are overlapping from the previous categories. These are some standard shapes with a few gaps in simple dataset, while the complex dataset has some hand-drawn shapes with fragmented lines and more gaps that produce more variety in general. For each instance, a ground truth image is available for comparison. Most of our experiments have been conducted using the complex dataset in order to evaluate the generalization of our approach."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In McPherson County, Kansas, the population center for the township of Groveland is the city of Elyria\u2020, which is also the largest city in the county. However, the county's governmentally independent cities, Lindsborg and McPherson, are excluded from the census figures for the townships. According to the 1921 Standard Atlas of McPherson County, Kansas, the township of Groveland's population total is comprised of the following towns: Alta Mills, Battle Hill, and Christian. Which of the following statements is true about the population center for the township of Groveland?",
    "choices": [
      "A) The population center for Groveland is the city of Johnstown, which is also the largest city in the county.",
      "B) The population center for Groveland is the city of Elivon, which is the largest city in the county.",
      "C) The population center for Groveland is the city of Doles Park, which is the largest city in the county.",
      "D) The population center for Groveland is the city of Elyria\u2020, which is the largest city in the county, and is excluded from the census figures for the townships."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Conway\n Elyria\u2020\n Groveland\n Johnstown\n New Gottland\n Roxbury\u2020\n\nGhost towns\n Alta Mills\n Battle Hill\n Christian\n Doles Park\n Elivon\n King City\n Sweadal\n\nTownships\nMcPherson County is divided into twenty-five townships. The cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships. In the following table, the population center is the largest city (or cities) included in that township's population total, if it is of a significant size. See also\n List of people from McPherson County, Kansas\n National Register of Historic Places listings in McPherson County, Kansas\n McPherson Valley Wetlands\n Maxwell Wildlife Refuge\n\nReferences\n\nNotes\n\nFurther reading\n\n Wheeler, Wayne Leland. \"An Analysis of Social Change in a Swedish-Immigrant Community: The Case of Lindsborg, Kansas.\" (PhD dissertation, University of Missouri-Columbia; ProQuest Dissertations Publishing, 1959. 5905657). County\n Through the Years: A Pictorial History of McPherson County; McPherson Sentinel' Heritage House Publishing Co; 1992. McPherson County First Courthouse Built About 1869 or 1870; Lindsborg News-Record; March 30, 1959. Pioneer Life and Lore of McPherson County, Kansas; Edna Nyquist; Democratic-Opinion Press; 1932. A History of the Church of the Brethren in Kansas (includes McPherson College history); Elmer LeRoy Craik; McPherson Daily; Republican Press; 397 pages; 1922. Portrait and Biographical Record of Dickinson, Saline, McPherson, and Marion Counties, Kansas; Chapman Bros; 614 pages; 1893. Standard Atlas of McPherson County, Kansas; Geo. A. Ogle & Co; 82 pages; 1921. Plat Book of McPherson County, Kansas; North West Publishing Co; 50 pages; 1903. Edwards' Atlas of McPherson County, Kansas; John P. Edwards; 51 pages; 1884. Trails\n The Story of the Marking of the Santa Fe Trail by the Daughters of the American Revolution in Kansas and the State of Kansas; Almira Cordry; Crane Co; 164 pages; 1915. (Download 4MB PDF eBook)\n The National Old Trails Road To Southern California, Part 1 (LA to KC); Automobile Club Of Southern California; 64 pages; 1916."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the conservation of sharks in Indonesia found that the majority of shark species are targeted by fisheries, with some species being overfished at an alarming rate. However, the same study also revealed that the decline in shark catches has led to a decrease in the demand for shark fins, which in turn has resulted in a reduction in the number of sharks being finned. Which of the following statements best summarizes the conservation implications of this finding?",
    "choices": [
      "A) The decline in shark catches has led to an increase in the number of sharks being finned, as the reduced demand for shark fins has resulted in more sharks being targeted by fisheries.",
      "B) The decline in shark catches has led to a decrease in the number of sharks being finned, as the reduced demand for shark fins has resulted in fewer sharks being targeted by fisheries.",
      "C) The decline in shark catches has led to an increase in the number of sharks being finned, as the reduced demand for shark fins has resulted in more sharks being targeted by fisheries, but the reduced demand has also led to a decrease in the number of sharks being caught.",
      "D) The decline in shark catches has led to a decrease in the number of sharks being finned, as the reduced demand for shark fins has resulted in fewer sharks being targeted by fisheries, and the reduced demand has also led to a decrease in the number of sharks being caught."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Simpfendorfer CA, Heupel MR, White WT, Dulvy NK. The importance of research and public opinion to conservation management of sharks and rays: a synthesis. Marine and Freshwater Research. 2011 Jul 21;62(6):518\u201327.\n14. Lack M, Sant G. The future of sharks: a review of action and inaction. TRAFFIC International and the Pew Environment Group. 2011 Jan:44. 15. Br\u00e4utigam A, Callow M, Campbell IR, Camhi MD, Cornish AS, Dulvy NK, et al. Global priorities for conserving sharks and rays: A 2015\u20132025 strategy. The Global Sharks and Rays Initiative; 2015. 27p. 16. Satria A, Matsuda Y. Decentralization of fisheries management in Indonesia. Marine Policy. 2004 Sep 30;28(5):437\u201350.\n17. Dharmadi , Fahmi , Satria F. Fisheries management and conservation of sharks in Indonesia. African journal of marine science. 2015 Apr 3;37(2):249\u201358. 20. Sembiring A, Pertiwi NP, Mahardini A, Wulandari R, Kurniasih EM, Kuncoro AW, Cahyani ND, Anggoro AW, Ulfa M, Madduppa H, Carpenter KE. DNA barcoding reveals targeted fisheries for endangered sharks in Indonesia. Fisheries Research. 2015 Apr 30;164:130\u20134.\n21. Clarke S. Re-examining the shark trade as a tool for conservation. SPC Fisheries Newsletter. 2014:49\u201356. 22. Jaiteh VF, Loneragan NR, Warren C. The end of shark finning? Impacts of declining catches and fin demand on coastal community livelihoods. Marine Policy. 2017 Mar 24.\n24. Cohen D, Crabtree B. Qualitative research guidelines project. Robert Wood Johnson Foundation, Princeton. 2006 Available from: http://www.qualres.org/index.html Cited in August 2016.\n25. Skud BE. Manipulation of fixed gear and the effect on catch-per-unit effort. FAO Fisheries Report (FAO). 1984. 26. Damalas D, Megalofonou P, Apostolopoulou M. Environmental, spatial, temporal and operational effects on swordfish (Xiphias gladius) catch rates of eastern Mediterranean Sea longline fisheries. Fisheries Research. 2007 Apr 30;84(2):233\u201346.\n27. Burnham KP, Anderson DR. Model selection and multimodel inference: a practical information-theoretic approach. Springer Science & Business Media; 2003 Dec 4.\n28."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is evaluating the performance of a language model on a multilingual dataset. The model is trained on a dataset that consists of 6 classes, which are a proper subset of the 9 classes from a previous task. The researcher wants to adapt the model to a new task that only includes 3 of the original classes. Which of the following statements best describes the approach the researcher should take?",
    "choices": [
      "A) The researcher should use the model that was finetuned on the original 9 classes and only include the 3 relevant classes in the evaluation metric.",
      "B) The researcher should use the model that was finetuned on the original 6 classes and only include the 3 relevant classes in the evaluation metric.",
      "C) The researcher should use the model that was finetuned on the original 9 classes and only exclude the 3 irrelevant classes from the evaluation metric.",
      "D) The researcher should use the model that was finetuned on the original 6 classes and only exclude the 3 irrelevant classes from the evaluation metric, and then fine-tune the model on the new 3 classes."
    ],
    "correct_answer": "D)",
    "documentation": [
      "These 6 classes are a proper subset of the 9 classes from Track-1. Thus, an intuitive baseline for Track-2 is to use the model finetuned for Track-1, whilst considering only the relevant classes for the latter task. The classes EN , ES and P T , i.e. the classes without any national dialect associated with them are not included in Track-2 as compared to Track-1. Thus, we calculate the predictions for the Track-2 validation dataset using the models for Track-1 and exclude the metrics for Track-1 specific classes to get the metrics for this \"adapted\" 2-way classification. We show the results of this experiment in Table and observe that, as expected, the adapted 2-way classification performs worse compared to the explicitly finetuned variant. Results for Track-1 and Track-2\n\nWe now present our experiments and their performance for both tracks. Our experiments for Track-1 are described in Table and our experiments for Track-2 are described in Table . The participants were allowed three submissions for evaluation on the test set, so we submitted predictions using the three systems which performed the best on the validation set. As mentioned in Section 5.2, we performed 2 3 , i.e. a total of 8 experiments using the two best models for each language. We observed that RoBERTa base on English, Spanish BERT base on Spanish and Portuguese BERT base performed the best on the testing set for Track-1. The same combination, with RoBERTa base for English, worked best for Track-2. All of our submissions were the top submissions for each track, which surpassed the next best competitors by a margin of 4.5% and 5.6% for Track-1 and Track-2 respectively. Ablation of best submissions\n\nWe hereby make some observations of our submissions and other experiments. To assist this, we plot the confusion matrices of our best submissions for Track-1 and Track-2 in Figures respectively. Note that these confusion matrices have their rows (i.e. true labels axes) normalized according to the number of samples in the class."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new policy initiative aims to reduce the influence of special interest groups in the decision-making process. However, some critics argue that this approach may lead to a lack of representation for marginalized communities. Which of the following statements best captures the underlying concern of the critics?",
    "choices": [
      "A) The critics are worried that the policy initiative will lead to a more efficient allocation of resources, as special interest groups are often inefficient in their decision-making processes.",
      "B) The critics believe that the policy initiative will create a new class of powerful elites who will dominate the decision-making process, further marginalizing already vulnerable groups.",
      "C) The critics are concerned that the policy initiative will lead to a more decentralized decision-making process, allowing for greater representation of marginalized communities.",
      "D) The critics are worried that the policy initiative will undermine the ability of special interest groups to advocate for their interests, potentially leading to a loss of representation for marginalized communities."
    ],
    "correct_answer": "D)",
    "documentation": [
      "But there is the other side, and that's the planetary managers. Planetary management is the path of the least resistance. You know all the powermongers go for the planetary management model, because they all think they can clamber over the bodies to get to the top. Ultimately the test is going to be who comes out on the top, the individual rightist or the planetary managers. Unfortunately, I'm not a betting man, but at the moment I'd like to bet on the planetary managers. DYSON: Part of this issue is reducing the value of incumbency, whether it's incumbency in prime time live, or incumbency in the government. There is much more fluidity of movement; you can't accumulate power because the unorganized forces have more power than you do. P. DENNING: I feel a little strange being on the left end of the stage, because most people think of me as being on the far right sometimes, but right now I'd like to comment on something that is halfway between what Mitch is saying, and what Simon is saying. The way I hear what Simon is saying, is that there is a disease of today which I will call inward- centeredness. We are very worried about ourselves and our organizations. We find in that orientation a lot of instability of things and technologies that change rapidly. In order to achieve the world that Mitch is talking about, we need to cure the disease, and instead come from an orientation that we could call outward-centeredness, instead of inward-centeredness. The question is the shift from, How do we accumulate power? to, How do we help others accumulate power? How do we go from looking for stability in things to looking for stability in relationships? In watching my own children grow up, I am convinced that they know more about this than I do. In listening to some of the younger people here, I'm more convinced that they know more about this than I do. They know something about the outward-centeredness that I have yet to learn. Observing this among children and among students gives me a lot of optimism, as a matter of fact, against the apocalypse that Simon talks about, because Simon is talking about the world that would be created if we continued \"us,\" and I think that the world that is being created by our children with their outward-centeredness is going to be the kind of world that Mitch is pointing towards."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new streaming service is launching a feature that allows users to access their favorite TV shows and movies on their personal computers and laptops, in addition to their traditional TVs. This feature is made possible by a new type of media equipment device that can connect to both the internet and a user's TV. However, the streaming service is concerned that some users may not be able to access the content due to issues with their internet connectivity. To address this issue, the streaming service is considering offering a free trial period for users who experience connectivity issues. Which of the following is a potential benefit of this approach?",
    "choices": [
      "A) The streaming service can increase its revenue by offering a premium subscription service to users who experience connectivity issues.",
      "B) The streaming service can reduce its costs by not having to provide technical support for users who experience connectivity issues.",
      "C) The streaming service can increase its user base by offering a free trial period to users who experience connectivity issues.",
      "D) The streaming service can improve its customer satisfaction ratings by providing a convenient solution for users who experience connectivity issues."
    ],
    "correct_answer": "D)",
    "documentation": [
      "As referred to herein, the phrase \u201cuser equipment device,\u201d \u201cuser equipment,\u201d \u201cuser device,\u201d \u201celectronic device,\u201d \u201celectronic equipment,\u201d \u201cmedia equipment device,\u201d or \u201cmedia device\u201d should be understood to mean any device for accessing the content described above, such as a television, a Smart TV, a set-top box, an integrated receiver decoder (IRD) for handling satellite television, a digital storage device, a digital media receiver (DMR), a digital media adapter (DMA), a streaming media device, a DVD player, a DVD recorder, a connected DVD, a local media server, a BLU-RAY player, a BLU-RAY recorder, a personal computer (PC), a laptop computer, a tablet computer, a WebTV box, a personal computer television (PC/TV), a PC media server, a PC media center, a hand-held computer, a stationary telephone, a personal digital assistant (PDA), a mobile telephone, a portable video player, a portable music player, a portable gaming machine, a smart phone, or any other television equipment, computing equipment, or wireless device, and/or combination of the same. In some embodiments, the user equipment device may have a front facing screen and a rear facing screen, multiple front screens, or multiple angled screens. In some embodiments, the user equipment device may have a front facing camera and/or a rear facing camera. On these user equipment devices, users may be able to navigate among and locate the same content available through a television. Consequently, media may be available on these devices, as well. The media provided may be for content available only through a television, for content available only through one or more of other types of user equipment devices, or for content available both through a television and one or more of the other types of user equipment devices. The media applications may be provided as on-line applications (i.e., provided on a website), or as stand-alone applications or clients on user equipment devices. Various devices and platforms that may implement media applications are described in more detail below."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A transport problem is solved using the source iteration method, which involves solving the equation \\(\\mathcal{L} \\psi^{m+1} = \\mathcal{S} \\psi^{m} + Q\\). The macroscopic absorption cross section \\(\\sigma_a\\) is given by \\(\\sigma_a = \\sigma_t - \\sigma_{s,0}\\), where \\(\\sigma_t\\) is the total scattering cross section and \\(\\sigma_{s,0}\\) is the scattering cross section at zero energy. However, the momentum transfer cross section \\(\\sigma_{tr}\\) is not explicitly defined in the problem. Which of the following statements is true?",
    "choices": [
      "A) The momentum transfer cross section \\(\\sigma_{tr}\\) can be calculated using the Legendre moment expansion of the angular flux.",
      "B) The macroscopic absorption cross section \\(\\sigma_a\\) is equal to the total scattering cross section \\(\\sigma_t\\).",
      "C) The source iteration method can be used to solve the Fokker-Planck equation in the forward-peaked limit.",
      "D) The Legendre moment expansion of the angular flux is a necessary condition for the convergence of the source iteration method."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Moreover, extending the method to energy-dependence shall not lead to significant additional theoretical difficulties. To solve the transport problem given by \\cref{eq1} we approximate the in-scattering term in \\cref{t1} with a Legendre moment expansion:\n\\begin{equation}\n\\label{transport1}\n\\mu\\frac{\\partial}{\\partial x} \\psi(x,\\mu) + \\sigma_t \\psi(x,\\mu) = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l(\\mu) \\sigma_{s,l} \\phi_l(x) + Q(x, \\mu),\n\\end{equation}\nwith \n\\begin{equation}\n\\label{transport2}\n\\phi_l(x) =  \\int_{-1}^{1} d\\mu P_l(\\mu) \\psi(x,\\mu). \\end{equation}\nHere, $\\phi_l$ is the $l^{th}$ Legendre moment of the angular flux, $ \\sigma_{s,l}$ is the $l^{th}$ Legendre coefficient of the differential scattering cross section,  and $P_l$ is the $l^{th}$-order Legendre polynomial. For simplicity, we will drop the notation $(x,\\mu)$ in the remainder of this section. The solution to \\cref{transport1} converges asymptotically to the solution of the following Fokker-Planck equation in the forward-peaked limit \\cite{pomraning1}:\n\\begin{equation}\n\\label{fp1}\n\\mu\\frac{\\partial \\psi}{\\partial x} + \\sigma_a \\psi = \\frac{\\sigma_{tr}}{2}\\frac{\\partial }{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi}{\\partial \\mu} + Q\\,,\n\\end{equation}\nwhere $\\sigma_{tr}= \\sigma_{s,0} -\\sigma_{s,1}$ is the momentum transfer cross section and $\\sigma_a = \\sigma_t-\\sigma_{s,0}$ is the macroscopic absorption cross section. Source Iteration \\cite{adamslarsen} is generally used to solve \\cref{transport1}, which can be rewritten in operator notation:\n\\begin{equation} \\label{si1}\n\\mathcal{L} \\psi^{m+1} = \\mathcal{S} \\psi^{m} + Q\\,,\n\\end{equation}\nwhere \n\\begin{equation}\n\\mathcal{L} = \\mu \\frac{\\partial}{\\partial x} + \\sigma_t,\n   \\quad\n\\mathcal{S} = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l(\\mu) \\sigma_{s,l} \\int_{-1}^{1}d\\mu P_l(\\mu) ,\n\\label{trans1}\n\\end{equation}\nand $m$ is the iteration index. This equation is solved iteratively until a tolerance criterion is met. The FP approximation shown in \\cref{fp1} can be used to accelerate the convergence of \\cref{transport1}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the performance of a machine learning algorithm on a dataset with a complex structure. The algorithm is designed to optimize the sum of the $L_i$ values, which represent the likelihood of each data point given the model parameters. The researcher notices that the algorithm's performance improves significantly when it recursively sets to zero couplings that are estimated to be very small. However, the algorithm's performance also decreases abruptly at some point, indicating that relevant couplings are being decimated. What is the most likely reason for this abrupt decrease in performance?",
    "choices": [
      "A) The algorithm is overfitting to the training data, causing it to perform poorly on unseen data.",
      "B) The algorithm is underfitting, meaning it is not complex enough to capture the underlying patterns in the data.",
      "C) The algorithm is experiencing a phenomenon known as \"catastrophic forgetting\", where it is forgetting previously learned patterns in the data.",
      "D) The algorithm's performance is being hindered by the presence of a large number of non-decimated couplings, which are not contributing to the model's accuracy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Moreover, in the technique there is the bias of the $l_2$ regularizer. Trying to overcome these problems, Decelle and Ricci-Tersenghi introduced a new method \\cite{Decelle14}, known as PLM + decimation: the algorithm maximizes the sum of the $L_i$,\n   \\begin{eqnarray}\n    {\\cal L}\\equiv \\frac{1}{N}\\sum_{i=1}^N \\mbox{L}_i\n    \\end{eqnarray}  \n    and, then, it recursively set to zero couplings which are estimated very small. We expect that as long as we are setting to zero couplings that are unnecessary to fit the data, there should be not much changing on ${\\cal L}$. Keeping on with decimation, a point is reached where ${\\cal L}$ decreases abruptly indicating  that relevant couplings are being decimated and under-fitting is taking place. Let us define  by $x$  the fraction of non-decimated couplings. To have a quantitative measure for the halt criterion of the decimation process, a tilted ${\\cal L}$ is defined as,\n   \\begin{eqnarray}\n  \\mathcal{L}_t &\\equiv& \\mathcal{L}  - x \\mathcal{L}_{\\textup{max}} - (1-x) \\mathcal{L}_{\\textup{min}} \\label{$t$PLF} \n   \\end{eqnarray}\n   where \n   \\begin{itemize}\n   \\item $\\mathcal{L}_{\\textup{min}}$ is the pseudolikelyhood of a model with independent variables. In the XY case: $\\mathcal{L}_{\\textup{min}}=-\\ln{2 \\pi}$.\n   \\item\n   $\\mathcal{L}_{\\textup{max}}$ is the pseudolikelyhood in the fully-connected model and it is maximized over all the $N(N-1)/2$ possible couplings. \n   \\end{itemize} At the first step, when $x=1$, $\\mathcal{L}$ takes value $\\mathcal{L}_{\\rm max}$ and  $\\mathcal{L}_t=0$. On the last step, for an empty graph, i.e., $x=0$, $\\mathcal{L}$ takes the value $\\mathcal{L}_{\\rm min}$ and, hence, again $\\mathcal{L}_t =0$. In the intermediate steps, during the decimation procedure, as $x$ is decreasing from $1$ to $0$, one observes firstly that $\\mathcal{L}_t$ increases linearly and, then, it displays an abrupt decrease indicating that from this point on relevant couplings are being decimated\\cite{Decelle14}. In Fig. \\ref{Jor1-$t$PLF} we give an instance of this behavior for the 2D short-range XY model with ordered couplings."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user agrees to the terms of service, but later discovers that the agency's payment processing system is not secure. What is the user's best course of action to avoid being charged for renewal fees?",
    "choices": [
      "A) The user should immediately contact the agency's customer service to request a refund for any past payments, and then cancel their subscription before it renews.",
      "B) The user should assume that the agency's terms of service supersede any security concerns, and continue to use the service as usual.",
      "C) The user should only cancel their subscription if they have already been charged for renewal fees, in order to avoid any potential liability for the agency.",
      "D) The user should terminate their subscription immediately, and notify the agency's customer service of their decision to cancel, in order to avoid being charged for renewal fees."
    ],
    "correct_answer": "D)",
    "documentation": [
      "By purchasing now, you agree to the following terms. You authorize Agency Spotter to store and charge your payment method on file. Your paid account will renew automatically, unless you terminate it, or you notify Customer Service by email ([email protected]) of your decision to terminate your paid account. You must cancel your subscription before it renews in order to avoid billing of subscription fees for the renewal form to your credit card. Should You object to any of the Terms or any subsequent modifications thereto, or become dissatisfied with the Site in any way, Your only recourse is to immediately discontinue use of the Site. Agency Spotter has the right, but is not obligated, to strictly enforce the Terms through self-help, community moderation, active investigation, litigation and prosecution.\n(b) Agency Spotter will use commercially reasonable efforts to make the Services available on a 24 hours a day, 7 days a week, and 365 days a year basis, subject to Section 23 below and to downtime for maintenance purposes.\n(c) Agency Spotter may from time to time modify the Services and add, change, or delete features of the Services in its sole discretion, without notice to you. Your continued use of the Service after any such changes to the Service constitutes your acceptance of these changes. Agency Spotter will use commercially reasonable efforts to post information on the Site regarding material changes to the Services. (d) The contents of the Site, such as text, graphics, images, logos, user interfaces, visual interfaces, photographs, button icons, software, trademarks, sounds, music, artwork and computer code, and other Agency Spotter content (collectively, \u201cAgency Spotter Content\u201d), are protected under both United States and foreign copyright, trademark and other laws. All Agency Spotter Content is the property of Agency Spotter or its content suppliers or clients. The compilation (meaning the collection, arrangement and assembly) of all content on the Site is the exclusive property of Agency Spotter and is protected by United States and foreign copyright, trademark, and other laws."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A recent study on data sharing and reuse in archaeobotany found that the majority of published articles did not provide sufficient information for reuse of datasets. However, the study also noted that some researchers had made their datasets publicly available. Which of the following statements best summarizes the implications of this finding?",
    "choices": [
      "A) The lack of data sharing and reuse in archaeobotany is a significant barrier to the advancement of the field, and researchers must prioritize data publication and citation to overcome this issue.",
      "B) The fact that some researchers have made their datasets publicly available suggests that data sharing and reuse are already widespread practices in archaeobotany, and that the field is already well-established in terms of open science.",
      "C) The study's findings indicate that data sharing and reuse are not necessary for the advancement of archaeobotany, and that researchers can continue to rely on traditional methods of data collection and analysis.",
      "D) The study's results suggest that the lack of data sharing and reuse in archaeobotany is not a significant issue, and that researchers can focus on other aspects of the field, such as the application of digital methods, without worrying about data sharing and reuse."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Recent work on the availability of data on lithics assemblages found a low level of data sharing (Marwick & Pilaar Birch 2018) and there are perceptions of low levels of data reuse (Huggett 2018; Kintigh et al. 2018). Within zooarchaeology numerous studies have explored issues of data sharing and reuse (Kansa & Kansa 2013, 2014), and the sub-discipline is seen as one of the most advanced areas of archaeology in regards to open science (Cooper & Green 2016: 273). Beyond zooarchaeology, however, explicit discussion has remained limited. This paper assesses data sharing and reuse practices in archaeology through the case study of archaeobotany \u2013 a long established sub-discipline within archaeology which has well-established principles of data recording. Archaeobotany is an interesting case study for data sharing in archaeology as it straddles the division of archaeology between scientific and more traditional techniques. Quantitative data on archaeological plant remains are also of interest to a range of other fields, including ecology, environmental studies, biology and earth sciences. The key issues of data sharing and data reuse (Atici et al. 2013) have been touched upon in archaeobotany over the past decade within broader discussions on data quality (Van der Veen, Livarda & Hill 2007; Van der Veen, Hill & Livarda 2013). These earlier studies focussed on the quality and availability of archaeobotanical data from developer-funded excavations in Britain and Cultural Resource Management in North America (Vanderwarker et al. 2016: 156). However, no discussion of data-sharing and reuse in academic archaeobotany occurred. A recent review of digital methods in archaeobotany is the notable exception, with discussions of the challenges and methods of data sharing (Warinner & d\u2019Alpoim Guedes 2014). Currently, we have no evidence for the levels of data sharing and reuse within archaeobotany. This article provides the first quantitative assessment of 1) data publication in recent archaeobotanical journal articles 2) data citation in recent archaeobotanical meta-analysis 3) the reuse of archaeobotanical datasets, in order to assess whether practices need to change and how such changes can take place."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": null,
    "choices": null,
    "correct_answer": null,
    "documentation": [
      "Well, the civil war appears to be over. And Islamism won it. The loser, moderate Islam, is always deceptively well-represented on the level of the op-ed page and the public debate; elsewhere, it is supine and inaudible. We are not hearing from moderate Islam. Whereas Islamism, as a mover and shaper of world events, is pretty well all there is. So, to repeat, we respect Islam - the donor of countless benefits to mankind, and the possessor of a thrilling history. But Islamism? No, we can hardly be asked to respect a creedal wave that calls for our own elimination. More, we regard the Great Leap Backwards as a tragic development in Islam's story, and now in ours. Naturally we respect Islam. But we do not respect Islamism, just as we respect Muhammad and do not respect Muhammad Atta. \u0627\u0644\u0645\u0624\u0627\u0645\u0631\u0627\u062a \u0639\u0644\u0649 \u0627\u0644\u0623\u062f\u064a\u0627\u0646 \u0648\u062c\u0645\u064a\u0639 \u0627\u0644\u0627\u0646\u0642\u0644\u0627\u0628\u0627\u062a \u0627\u0644\u0645\u062e\u0631\u0628\u0629 \u0648\u0627\u0644\u062b\u0648\u0631\u0627\u062a \u0639\u0644\u0649 \u0627\u0644\u0642\u064a\u0645 \u0648\u0627\u0644\u0645\u0628\u0627\u062f\u0626 \u062e\u0631\u062c\u062a \u0645\u0646 \u0647\u0630\u0627 \u0627\u0644\u062a\u0631\u0627\u062b .. \u0648\u0627\u0646 \u0643\u0644 \u0645\u0639\u0648\u0644 \u0647\u062f\u0645 \u0643\u0627\u0646 \u0648\u0631\u0627\u0621\u0647 \u062a\u0648\u062c\u064a\u0647 \u064a\u0647\u0648\u062f\u064a. \u2022\u062a\u0630\u0643\u0631\u0648\u0627 \u0623\u0646 \u0627\u0644\u0634\u0639\u0628 \u0627\u0644\u0630\u064a \u0644\u0627 \u064a\u0647\u0644\u0643 \u063a\u064a\u0631\u0647 \u064a\u0647\u0644\u0643 \u0646\u0641\u0633\u0647. \u2022\u064a\u062c\u0628 \u0627\u0646 \u0646\u062e\u0644\u0642 \u0627\u0644\u062c\u064a\u0644 \u0627\u0644\u0630\u064a \u0644\u0627 \u064a\u062e\u062c\u0644 \u0645\u0646 \u0643\u0634\u0641 \u0639\u0648\u0631\u062a\u0647 (\u0623\u0644\u0627 \u062a\u0641\u0633\u0631 \u0644\u0646\u0627 \u0647\u0630\u0647 \u0627\u0644\u062c\u0645\u0644\u0629 \u0645\u0648\u062c\u0629 \u0627\u0644\u0639\u0631\u0649 \u0641\u064a \u0627\u0644\u0627\u0641\u0644\u0627\u0645 \u0648\u0627\u0644\u0645\u0648\u0636\u0627\u062a \u0627\u0644\u062a\u0649 \u062a\u0633\u0648\u062f \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0622\u0646). .\u0639\u0644\u064a\u0646\u0627 \u0627\u0646 \u0646\u0634\u0639\u0644 \u062d\u0631\u0628\u0627 \u0628\u064a\u0646 \u0627\u0644\u0634\u0639\u0648\u0628 \u0648\u0646\u0636\u0631\u0628 \u0627\u0644\u062f\u0648\u0644 \u0628\u0639\u0636\u0647\u0627 \u0628\u0628\u0639\u0636 \u0641\u0628\u0647\u0630\u0627 \u064a\u0635\u0628\u062d \u062c\u0645\u064a\u0639 \u0627\u0644\u0645\u062a\u062d\u0627\u0631\u0628\u064a\u0646 \u0641\u064a \u062d\u0627\u062c\u0629 \u0627\u0644\u0649 \u0623\u0645\u0648\u0627\u0644\u0646\u0627 \u0641\u0646\u0641\u0631\u0636 \u0639\u0644\u064a\u0647\u0645 \u0634\u0631\u0648\u0637\u0646\u0627. \u2022\u0627\u0644\u062c\u0645\u0627\u0647\u064a\u0631 \u0639\u0645\u064a\u0627\u0621 \u0641\u0627\u0634\u062a\u0631\u0648\u0647\u0627 \u0628\u0627\u0644\u0645\u0627\u0644 \u0648\u0633\u0648\u0642\u0648\u0647\u0627 \u0643\u0627\u0644\u0628\u0647\u0627\u0626\u0645 \u0627\u0644\u0649 \u0623\u0647\u062f\u0627\u0641\u0643\u0645. \u2022\u0633\u064a\u0637\u0631\u0648\u0627 \u0639\u0644\u0649 \u0627\u0644\u0627\u0646\u062a\u062e\u0627\u0628\u0627\u062a \u0648\u0648\u0633\u0627\u0626\u0644 \u0627\u0644\u0627\u0639\u0644\u0627\u0645 \u0648\u0627\u0644\u0635\u062d\u0627\u0641\u0629 (\u0648\u0647\u0645 \u0642\u062f \u0633\u064a\u0637\u0631\u0648\u0627 \u0639\u0644\u064a\u0647\u0627 \u0628\u0627\u0644\u0645\u0627\u0644 \u0648\u0627\u0644\u062c\u0646\u0633 \u0648\u0627\u0644\u0645\u0631\u0623\u0629 \u0641\u064a \u0627\u0644\u063a\u0631\u0628 \u0627\u0644\u0631\u0623\u0633\u0645\u0627\u0644\u064a \u0648\u0628\u0627\u0644\u062d\u0632\u0628 \u0648\u0627\u0644\u0633\u0644\u0637\u0629 \u0641\u064a \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0627\u0634\u062a\u0631\u0627\u0643\u064a). \u2022\u0627\u062f\u0641\u0639\u0648\u0627 \u0627\u0644\u062c\u0645\u0627\u0647\u064a\u0631 \u0627\u0644\u0639\u0645\u064a\u0627\u0621 \u0627\u0644\u0649 \u0627\u0644\u062b\u0648\u0631\u0629 \u0648\u0633\u0644\u0645\u0648\u0647\u0645 \u0645\u0642\u0627\u0644\u064a\u062f \u0627\u0644\u062d\u0643\u0645 \u0644\u064a\u062d\u0643\u0645\u0648\u0627 \u0641\u064a \u063a\u0648\u063a\u0627\u0626\u064a\u0629 \u0648\u063a\u0628\u0627\u0621 (\u0648\u0642\u062f \u0641\u0639\u0644\u0648\u0627 \u0647\u0630\u0647 \u0641\u064a \u0627\u0644\u062b\u0648\u0631\u0629 \u0627\u0644\u0641\u0631\u0646\u0633\u064a\u0629) \u0648\u062d\u064a\u0646\u0626\u0630 \u0646\u0623\u062a\u064a \u0646\u062d\u0646 \u0648\u0646\u0639\u062f\u0645\u0647\u0645 \u0641\u0646\u0643\u0648\u0646 \u0645\u0646\u0642\u0630\u064a\u0646 \u0644\u0644\u0639\u0627\u0644\u0645 (\u0648\u0642\u062f \u0627\u0639\u062f\u0645\u0648\u0647\u0645 \u062c\u0645\u064a\u0639\u0627\u064b \u0645\u0646 \u0631\u0648\u0628\u0633\u0628\u064a\u0631 \u0627\u0644\u0649 \u0645\u064a\u0631\u0627\u0628\u0648\u0627). \u2022\u0627\u0631\u0641\u0639\u0648\u0627 \u0634\u0639\u0627\u0631 \u0627\u0644\u062d\u0631\u064a\u0629 \u0648\u0627\u0647\u062f\u0645\u0648\u0627 \u0628\u0647\u0627 \u0627\u0644\u0627\u062e\u0644\u0627\u0642 \u0648\u0627\u0644\u0627\u0633\u0631\u0629 \u0648\u0627\u0644\u0642\u0648\u0645\u064a\u0629 \u0648\u0627\u0644\u0648\u0637\u0646\u064a\u0629. .\u0627\u0631\u0641\u0639\u0648\u0627 \u0634\u0639\u0627\u0631 \u0627\u0644\u0639\u0644\u0645 \u0648\u0627\u0647\u062f\u0645\u0648\u0627 \u0628\u0647 \u0627\u0644\u062f\u064a\u0646 .. \u0648\u0647\u0630\u0627 \u0645\u0627 \u0641\u0639\u0644\u0647 \u0643\u0645\u0627\u0644 \u0623\u062a\u0627\u062a\u0648\u0631\u0643 (\u062d\u0641\u064a\u062f \u0645\u0632\u0627\u0631\u0627\u062d\u064a) \u062d\u064a\u0646\u0645\u0627 \u0627\u0642\u0627\u0645 \u0627\u0644\u062f\u0648\u0644\u0629 \u0627\u0644\u0639\u0644\u0645\u0627\u0646\u064a\u0629 \u0641\u064a \u062a\u0631\u0643\u064a\u0627 \u0648\u0648\u0642\u0641 \u064a\u062e\u0637\u0628 \u0641\u064a \u0627\u0644\u0628\u0631\u0644\u0645\u0627\u0646 \u0627\u0644\u062a\u0631\u0643\u064a \u0639\u0627\u0645 1923 \u0633\u0627\u062e\u0631\u0627\u064b \u0645\u0646 \u0627\u0644\u0642\u0631\u0622\u0646. \u0646\u062d\u0646 \u0627\u0644\u0622\u0646 \u0641\u064a \u0627\u0644\u0642\u0631\u0646 \u0627\u0644\u0639\u0634\u0631\u064a\u0646 \u0644\u0627 \u0646\u0633\u062a\u0637\u064a\u0639 \u0627\u0646 \u0646\u0633\u064a\u0631 \u0648\u0631\u0627\u0621 \u0643\u062a\u0627\u0628 \u062a\u0634\u0631\u064a\u0639 \u064a\u0628\u062d\u062b \u0639\u0646 \u0627\u0644\u062a\u064a\u0646 \u0648\u0627\u0644\u0632\u064a\u062a\u0648\u0646. \u2022\u0627\u0644\u0630\u064a \u064a\u0639\u0631\u0642\u0644 \u0645\u0624\u0627\u0645\u0631\u0627\u062a\u0643\u0645 \u0627\u0648\u0642\u0639\u0648\u0647 \u0641\u064a \u0641\u0636\u0627\u0626\u062d \u062b\u0645 \u0647\u062f\u062f\u0648\u0647 \u0628\u0643\u0634\u0641\u0647\u0627 (\u0648\u0642\u062f \u0641\u0639\u0644\u0648\u0647\u0627 \u0641\u064a \u0648\u0648\u062a\u0631\u062c\u064a\u062a)"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A city's electrical grid is designed to provide both heating and cooling to its residents. However, the city's energy policy aims to reduce greenhouse gas emissions. Which of the following statements best describes the city's approach to heating and cooling?",
    "choices": [
      "A) The city's electrical grid is powered entirely by renewable energy sources, and the heating and cooling systems are designed to be highly efficient.",
      "B) The city's electrical grid is still reliant on fossil fuels, but the city has implemented a policy of \"load shifting\" to reduce peak demand during hot summer afternoons.",
      "C) The city's electrical grid is designed to provide heating and cooling through the use of electric heat pumps, which are more efficient than traditional heating systems.",
      "D) The city's electrical grid is designed to provide heating and cooling through the use of a combination of electric heat pumps and district heating, which utilizes waste heat from power stations to provide warmth to buildings."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Public utilities were set up in many cities targeting the burgeoning market for electrical lighting. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector. The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of resistive electric heating in new buildings. Electricity is however still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand for heating and cooling, the effects of which electricity utilities are increasingly obliged to accommodate. Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process. The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robot is navigating a grid world to reach a goal, with a mission time limit of 10 time steps. The robot's transition model is deterministic, and it can take diagonal actions. The human has selected actions to minimize a cost function Cg,\u03b8, where \u03b8 is defined as the length of the shortest path to the goal constrained by the preference. The robot receives a reward at each step, which is a sum of three components: a goal-specific reward, a preference-specific reward, and a penalty for taking an invalid transition. The robot's belief distribution over the unknown problem variables is updated at each step based on the observation received. Which of the following statements is true about the robot's behavior?",
    "choices": [
      "A) The robot will always take the shortest path to the goal, regardless of the preference constraints.",
      "B) The robot will take the path that minimizes the cost function Cg,\u03b8, which is the length of the shortest path to the goal constrained by the preference.",
      "C) The robot will take the path that minimizes the cost function Cg,\u03b8, but only if the preference constraints do not lead to a longer path.",
      "D) The robot will take the path that minimizes the cost function Cg,\u03b8, and also updates its belief distribution over the unknown problem variables at each step."
    ],
    "correct_answer": "D)",
    "documentation": [
      "EXPERIMENTS\n\nWe evaluate our model on a simulated navigation task where the robot must reach a goal that is unknown a priori while respecting the path preferences indicated by a human. The robot navigates in a grid world containing obstacles. The transition model is deterministic: the robot selects an adjacent location on the grid to reach at the next time step. The robot is also allowed to take diagonal actions. Each location s t in the map can be mapped to a vertex v t \u2208 G. Therefore, the actions leading to locations mapped to different vertices correspond to edges on the graph. We note f (s t , a t ) the edge crossed by taking action a t from location s t . The robot is given a mission time limit T max for reaching the goal. In this problem, we assume that the human selects actions to noisily minimize a cost function C g,\u03b8 , where \u03b8 is defined as per eq. ( ), corresponding to the length of the shortest path to the goal constrained by the preference (where the robot is only allowed to make transitions on G along preferred edges). More specifically, where \u03b4(s t , g | o t , p vt ) designates the length of the shortest path from s t to g passing by o t and constrained by preference p vt . This is a slight variant of the cost function proposed by Best and Fitch , where we add in a conditioning on the path preference. We compute costs by running the A path planning algorithm on the environment maps (grid worlds with diagonal actions) and impose preference constraints by pruning invalid transitions from the search tree. Reward model. At each step in time, the robot receives a reward which is a sum of three components: a goal-specific reward a preference-specific reward or penalty We compute solutions to the POMDP defined in section III-B with the online solver POMCP , and with the particularity that within the rollouts, the robot does not expect to collect human inputs. Each time a solution is computed, the robot takes an action and may receive an observation. If it does, it updates its belief distribution over the unknown problem variables and resolves the POMDP over a receding horizon."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The ancient Greeks' understanding of electricity was likely influenced by their observations of the natural world, including the behavior of electric fish and the properties of amber. However, their conclusions about the nature of electricity were not entirely accurate. Which of the following statements best summarizes the Greeks' understanding of electricity?",
    "choices": [
      "A) They believed that electricity was a type of magnetism that could be generated by rubbing amber with cat's fur.",
      "B) They thought that electric fish were capable of storing electrical energy, which they could then release through a process of electrical conduction.",
      "C) They believed that the numbing effect of electric shocks delivered by catfish and electric rays was due to a magnetic field that could be harnessed for medical purposes.",
      "D) They recognized that electricity was a form of energy that could be generated by the interaction between conducting objects and the natural world, but they did not fully understand its underlying mechanisms."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society. Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ra\u2018ad (\u0631\u0639\u062f) applied to the electric ray. Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the performance of a machine learning algorithm on a dataset with a complex structure. The algorithm is designed to optimize the sum of the $L_i$ values, which represent the likelihood of each data point given the model parameters. The researcher notices that the algorithm's performance improves significantly when it recursively sets to zero couplings that are estimated to be very small. However, the algorithm's performance also decreases abruptly at some point, indicating that relevant couplings are being decimated. What is the most likely reason for this abrupt decrease in performance?",
    "choices": [
      "A) The algorithm is overfitting to the training data, causing it to perform poorly on unseen data.",
      "B) The algorithm is underfitting, meaning it is not complex enough to capture the underlying patterns in the data.",
      "C) The algorithm is experiencing a phenomenon known as \"catastrophic forgetting\", where it is forgetting previously learned patterns in the data.",
      "D) The algorithm's performance is being hindered by the presence of a large number of non-decimated couplings, which are not contributing to the model's accuracy."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Moreover, in the technique there is the bias of the $l_2$ regularizer. Trying to overcome these problems, Decelle and Ricci-Tersenghi introduced a new method \\cite{Decelle14}, known as PLM + decimation: the algorithm maximizes the sum of the $L_i$,\n   \\begin{eqnarray}\n    {\\cal L}\\equiv \\frac{1}{N}\\sum_{i=1}^N \\mbox{L}_i\n    \\end{eqnarray}  \n    and, then, it recursively set to zero couplings which are estimated very small. We expect that as long as we are setting to zero couplings that are unnecessary to fit the data, there should be not much changing on ${\\cal L}$. Keeping on with decimation, a point is reached where ${\\cal L}$ decreases abruptly indicating  that relevant couplings are being decimated and under-fitting is taking place. Let us define  by $x$  the fraction of non-decimated couplings. To have a quantitative measure for the halt criterion of the decimation process, a tilted ${\\cal L}$ is defined as,\n   \\begin{eqnarray}\n  \\mathcal{L}_t &\\equiv& \\mathcal{L}  - x \\mathcal{L}_{\\textup{max}} - (1-x) \\mathcal{L}_{\\textup{min}} \\label{$t$PLF} \n   \\end{eqnarray}\n   where \n   \\begin{itemize}\n   \\item $\\mathcal{L}_{\\textup{min}}$ is the pseudolikelyhood of a model with independent variables. In the XY case: $\\mathcal{L}_{\\textup{min}}=-\\ln{2 \\pi}$.\n   \\item\n   $\\mathcal{L}_{\\textup{max}}$ is the pseudolikelyhood in the fully-connected model and it is maximized over all the $N(N-1)/2$ possible couplings. \n   \\end{itemize} At the first step, when $x=1$, $\\mathcal{L}$ takes value $\\mathcal{L}_{\\rm max}$ and  $\\mathcal{L}_t=0$. On the last step, for an empty graph, i.e., $x=0$, $\\mathcal{L}$ takes the value $\\mathcal{L}_{\\rm min}$ and, hence, again $\\mathcal{L}_t =0$. In the intermediate steps, during the decimation procedure, as $x$ is decreasing from $1$ to $0$, one observes firstly that $\\mathcal{L}_t$ increases linearly and, then, it displays an abrupt decrease indicating that from this point on relevant couplings are being decimated\\cite{Decelle14}. In Fig. \\ref{Jor1-$t$PLF} we give an instance of this behavior for the 2D short-range XY model with ordered couplings."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the impact of social media on mental health. They find that people who spend more time on social media are more likely to experience anxiety and depression. However, they also note that people who use social media to connect with others, such as friends and family, tend to have better mental health outcomes. What is the most likely explanation for this apparent paradox?",
    "choices": [
      "A) Social media platforms are designed to be addictive, and people who use them excessively are more likely to experience anxiety and depression due to the constant stream of information and the pressure to present a perfect online persona.",
      "B) Social media is a tool that can be used to improve mental health, but only if people use it in a way that is consistent with their offline social connections and activities.",
      "C) The researcher's findings are due to the fact that people who use social media to connect with others are more likely to have a strong support network, which is a key factor in maintaining good mental health.",
      "D) The relationship between social media use and mental health is complex, and people who use social media to connect with others may be more likely to experience anxiety and depression due to the constant comparison to others' curated online personas, which can lead to feelings of inadequacy and low self-esteem."
    ],
    "correct_answer": "D)",
    "documentation": [
      "In that this technology is a vehicle of communication, I believe that it is a vehicle of the truth, and as long as we keep it free, the truth will be heard that much more. Now I have kind of a question with a bit of a statement. I am a learning-disabled college student. I didn't ever finish high school. I had a freshmen education in high school, because of educational problems, and adjustment problems, I never really got too far beyond that. I write probably a fifth of the speed of anyone in this room and I have a real hard time doing math without a calculator. That's part of the reason why I wasn't able to do well in school. I read very well, fortunately, so I was able to go in when I was eighteen and take my GED just flat out without studying for it. I'm not dumb, or uneducated by any standards, but what has allowed me to get an associate's degree in college, and what has allowed me to approach graduation and get a bachelor's degree in college is the kind of technology that we are dealing with. I have never had easy access to that technology. The barriers that I have faced have been ones of order, regimentation, and where people try and say, \"Oh well, you don't fit in, you're not a CS student, you don't need those resources.\" I'm good with computers, I do a lot with them, I spend a lot of time with them. I hack, I don't do anything illegal, but I took a hacksaw to the frame of my nasty little 8088 about two years ago to cram some RAM into it, because that was the only way I could get it to fit and I needed it. Now I'm in a little bit better shape. I'm approaching the point where I would like to see ISDN real soon, because I need that kind of connectivity. You know, I'm doing interesting things that I find absolutely wonderful, but the idea that the kind of technology that is available to us, that is just there for the using, could be limited and unavailable to people, or that people would have to go through some of the things that I have had to go through, not being able to do well on tests, because I had no word processor available to me."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary role of the VC-10 Squadron under Goodwin's command during the Mariana Islands campaign?",
    "choices": [
      "A) To provide close air support to the initial landings of Marines on Saipan, and then to conduct bombing raids on Japanese airfields on Tinian and Guam.",
      "B) To engage in a series of intense dogfights with Japanese fighter planes, resulting in the destruction of several enemy aircraft.",
      "C) To conduct a naval blockade of the Japanese mainland, cutting off their supply lines and crippling their ability to wage war.",
      "D) To provide close air support to the initial landings of Marines on Saipan, and then to conduct a series of mercy flights over Allied prisoner-of-war camps in Japan, dropping food and medicine to the prisoners until the end of the war."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The air unit, VC-10 Squadron, under Goodwin's command gave close air support to the initial landings of Marines on Saipan on June 15, 1944, destroying enemy gun emplacements, troops, tanks, and trucks. On the 17th, her combat air patrol (CAP) shot down or turned back all but a handful of 47 enemy planes headed for her task group and her gunners shot down two of the three planes that did break through to attack her. Goodwin's carrier continued in providing of close ground support operations at Tinian during the end of July 1944, then turned her attention to Guam, where she gave identical aid to invading troops until mid-August that year. For his service during the Mariana Islands campaign, Goodwin was decorated with Bronze Star Medal with Combat \"V\". He was succeeded by Captain Walter V. R. Vieweg on August 18, 1944, and appointed Chief of Staff, Carrier Division Six under Rear admiral Arthur W. Radford. The Gambier Bay was sunk in the Battle off Samar on October 25, 1944, during the Battle of Leyte Gulf after helping turn back a much larger attacking Japanese surface force. Goodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and took part in the Battle of Leyte Gulf and operations supporting Leyte landings in late 1944. He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat \"V\". He was also entitled to wear two Navy Presidential Unit Citations and Navy Unit Commendation. Postwar service\n\nFollowing the surrender of Japan, Goodwin assumed command of Light aircraft carrier  on August 24, 1945. The ship was tasked with air missions over Japan became mercy flights over Allied prisoner-of-war camps, dropping food and medicine until the men could be rescued. She was also present at Tokyo Bay for the Japanese surrender on September 2, 1945."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary mechanism by which the strong coupling in ultracold Rydberg gases affects the hydrodynamics of plasma expansion?",
    "choices": [
      "A) The strong coupling leads to a suppression of three-body recombination, resulting in a more rapid expansion of the plasma.",
      "B) The strong coupling causes a significant increase in the number of ion correlations, leading to a more efficient recombination of ions and a slower expansion of the plasma.",
      "C) The strong coupling results in a decrease in the number of Penning electrons, leading to a slower expansion of the plasma.",
      "D) The strong coupling leads to a suppression of three-body recombination, resulting in a more rapid expansion of the plasma, due to the increased efficiency of electron impact between highly excited Rydberg states."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The Raithel group also found direct evidence for electron collisional $\\ell$-mixing in a Rb MOT \\cite{Dutta}, and used selective field ionization to monitor evolution to plasma on a microsecond timescale in ultracold $^{85}$Rb $65d$ Rydberg gases with densities as low as $10^8$ cm$^{-3}$ \\cite{WalzFlannigan}. Research by our group at UBC has observed very much the same dynamics in the relaxation of Xe Rydberg gases of similar density prepared in a molecular beam \\cite{Hung2014}. In both cases, the time evolution to avalanche is well-described by coupled rate equations (see below), assuming an initializing density of Penning electrons determined by Robicheaux's criterion \\cite{Robicheaux05}, applied to an Erlang distribution of Rydberg-Rydberg nearest neighbours. Theoretical investigations of ultracold plasma physics have focused for the most part on the long- and short-time dynamics of plasmas formed by direct photoionization \\cite{PhysRept,Lyon}. In addition to studies mentioned above, key insights on the evolution dynamics of Rydberg gases have been provided by studies of Pohl and coworkers exploring the effects of ion correlations and recombination-reionization on the hydrodynamics of plasma expansion \\cite{Pohl:2003,PPR}. Further research has drawn upon molecular dynamics (MD) simulations to reformulate rate coefficients for the transitions driven by electron impact between highly excited Rydberg states \\cite{PVS}, and describe an effect of strong coupling as it suppresses three-body recombination \\cite{Bannasch:2011}. MD simulations confirm the accuracy of coupled rate equation descriptions for systems with $\\Gamma$ as large as 0.3. Newer calculations suggest a strong connection between the order created by dipole blockade in Rydberg gases and the most favourable correlated distribution of ions in a corresponding strongly coupled ultracold plasma \\cite{Bannasch:2013}. Tate and coworkers have studied ultracold plasma avalanche and expansion theoretically as well as experimentally."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Society Committee on Education (SOCED) received an update from President Catherine Hunt on the thematic programming featured in Chicago focusing on the sustainability of energy, food, and water. This programming was part of the society's efforts to promote education in the field of chemistry. However, the committee also discussed the potential impact of the proposed changes to the Student Affiliates program on the society's overall financial performance. What is a likely consequence of the proposed changes to the Student Affiliates program on the society's financial performance, considering the favorable variance in 2006?",
    "choices": [
      "D) received an update from President Catherine Hunt on the thematic programming featured in Chicago focusing on the sustainability of energy, food, and water. This programming was part of the society's efforts to promote education in the field of chemistry. However, the committee also discussed the potential impact of the proposed changes to the Student Affiliates program on the society's overall financial performance. What is a likely consequence of the proposed changes to the Student Affiliates program on the society's financial performance, considering the favorable variance in 2006?",
      "A) The proposed changes would likely lead to a decrease in the society's overall financial performance, as the Student Affiliates program is a significant contributor to the society's revenue.",
      "B) The proposed changes would likely have no impact on the society's financial performance, as the Student Affiliates program is not a significant contributor to the society's expenses.",
      "C) The proposed changes would likely lead to an increase in the society's overall financial performance, as the Student Affiliates program is a significant contributor to the society's revenue and the favorable variance in 2006 was primarily attributable to higher than budgeted electronic services revenue and investment income."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The major actions taken by the board of directors and council during the national meeting in Chicago were reported in C&EN, April 30 (page 32). The Society Committee on Budget & Finance met on Saturday, March 24, to review the society's 2006 financial performance. The society ended 2006 with a net contribution from operations of $12.2 million, on revenues of $424.0 million and expenses of $411.8 million. This was $7.8 million favorable to the approved budget. After including the results of the Member Insurance Program and new ventures, the society's overall net contribution for 2006 was $11.5 million, which was $7.4 million favorable to the approved budget. The favorable variance was primarily attributable to higher than budgeted electronic services revenue and investment income, as well as expense savings from lower than budgeted health care costs and reduced IT spending. In addition, the society ended the year in compliance with the board-established financial guidelines. The Society Committee on Education (SOCED) received an update from President Catherine Hunt on the thematic programming featured in Chicago focusing on the sustainability of energy, food, and water. President-Elect Bruce Bursten solicited input from the committee pertaining to the central role of education in his agenda. SOCED received a presentation from the Membership Affairs Committee on its white paper on membership requirements. Committee members strongly support the proposal to include undergraduates as members of the society, but they requested that financial arrangements be clearly spelled out in the petition to ensure that the highly successful Student Affiliates program remains intact. The committee discussed the Education Division programs that were reviewed in 2006 and those that will be reviewed in 2007, under the auspices of the Program Review Advisory Group. SOCED received an update from the Committee on Professional Training regarding the draft ACS guidelines for approval of bachelor's degree programs in chemistry."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The experimental ZFC-FC curves reported in Fig. 10b suggest that the nanocolumns in the Ge matrix exhibit isotropic magnetic behavior. However, the fitting expression used to analyze the data, $\\chi_{\\parallel}^{-1}= \\chi_{\\perp}^{-1}\\approx 3k_{B}T/M(T)+\\mu_{0}H_{eff}(T)$, assumes isotropic or cubic anisotropy. What is the primary reason why the authors chose to use this expression, despite the possibility of anisotropic behavior?",
    "choices": [
      "A) The experimental data only showed a clear peak at low temperature for the in-plane susceptibility, indicating strong antiferromagnetic interactions between the nanocolumns.",
      "B) The authors assumed that the nanocolumns are isotropic because the Curie temperature does not exceed 170 K, and the temperature dependence of the saturation magnetization can be accounted for by writing $M(T)$.",
      "C) The fitting parameters, $M$ and $\\mu_{0}H_{eff}$, were chosen to minimize the difference between the experimental and fitted data, without considering the possibility of anisotropic behavior.",
      "D) The authors chose to use the isotropic expression because the matrix magnetic signal becomes very strong at 5 T and low temperature, making it difficult to distinguish between the nanocolumns' magnetic behavior and the matrix signal."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Moreover for low $T_{0}$ values, a peak appears at low temperature in FC curves which evidences strong antiferromagnetic interactions between the nanocolumns \\cite{Chan00}. \\begin{figure}[htb]\n\\center\n    \\includegraphics[width=.35\\linewidth]{./fig10a.eps}\n    \\includegraphics[width=.63\\linewidth]{./fig10b.eps}\n\\caption{(a) ZFC-FC measurements performed on a Ge$_{0.887}$Mn$_{0.113}$ sample grown at 115$^{\\circ}$C. The in-plane applied field is 0.015 T. Magnetization was recorded up to different T$_{0}$ temperatures: 30 K, 50 K, 100 K, 150 K, 200 K and 250 K. Curves are shifted up for more clarity. (b) ZFC-FC curves for in-plane and out-of-plane applied fields (0.015 T).}\n\\label{fig10}\n\\end{figure} In order to derive the magnetic size and anisotropy of the Mn-rich nanocolumns embedded in the Ge matrix, we have fitted the inverse normalized in-plane (resp. out-of-plane) susceptibility: $\\chi_{\\parallel}^{-1}$ (resp. $\\chi_{\\perp}^{-1}$). The corresponding experimental ZFC-FC curves are reported in Fig. 10b. Since susceptibility measurements are performed at low field (0.015 T), the matrix magnetic signal remains negligible. In order to normalize susceptibility data, we need to divide the magnetic moment by the saturated magnetic moment recorded at 5 T. However the matrix magnetic signal becomes very strong at 5 T and low temperature so that we need to subtract it from the saturated magnetic moment using a simple Curie function. From Fig. 10b, we can conclude that nanocolumns are isotropic. Therefore to fit experimental data we use the following expression well suited for isotropic systems or cubic anisotropy: $\\chi_{\\parallel}^{-1}= \\chi_{\\perp}^{-1}\\approx 3k_{B}T/M(T)+\\mu_{0}H_{eff}(T)$. $k_{B}$ is the Boltzmann constant, $M=M_{s}v$ is the magnetic moment of a single-domain nanostructure (macrospin approximation) where $M_{s}$ is its magnetization and $v$ its volume. The in-plane magnetic field is applied along $[110]$ or $[-110]$ crystal axes. Since the nanostructures Curie temperature does not exceed 170 K, the temperature dependence of the saturation magnetization is also accounted for by writting $M(T)$. Antiferromagnetic interactions between nanostructures are also considered by adding an effective field estimated in the mean field approximation \\cite{Fruc02}: $\\mu_{0}H_{eff}(T)$.\nThe only fitting parameters are the maximum magnetic moment (\\textit{i.e.} at low temperature) per nanostructure: $M$ (in Bohr magnetons $\\mu_{B}$) and the maximum interaction field (\\textit{i.e.} at low temperature): $\\mu_{0}H_{eff}$.\n\n\\begin{figure}[htb]\n\\center\n   \\includegraphics[width=.7\\linewidth]{./fig11.eps}\n\\caption{Temperature dependence of the inverse in-plane (open circles) and out-of-plane (open squares) normalized susceptibilities of a Ge$_{0.887}$Mn$_{0.113}$ sample grown at 115$^{\\circ}$C. Fits were performed assuming isotropic nanostructures or cubic anisotropy."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study analyzing the impact of fishing gear on shark populations found that the use of surface longlines resulted in significantly higher catch rates of threatened species compared to bottom longlines. However, the researchers also noted that the gear used by the vessels was not the only factor influencing catch rates, as fishing behavior and vessel type also played a role. Which of the following conclusions can be drawn from this study?",
    "choices": [
      "A) The use of surface longlines is the primary factor contributing to the decline of shark populations, and efforts to restrict their use would be sufficient to mitigate this impact.",
      "B) The study's findings suggest that the use of bottom longlines is more effective for catching threatened species, and therefore, fisheries managers should prioritize the use of this gear.",
      "C) The researchers' analysis revealed that the relationship between fishing gear and catch rates of threatened species is complex, and that other factors such as vessel type and fishing behavior also play a significant role.",
      "D) The study's results indicate that the use of surface longlines and bottom longlines has a synergistic effect on catch rates of threatened species, and that a combination of both gear types would be the most effective approach for managing shark populations."
    ],
    "correct_answer": "C)",
    "documentation": [
      "More detailed socioeconomic data were collected in a full household survey in 2016, as outlined in Lestari et al. . Shark landings data were collected by three experienced enumerators, who were trained in species identification and data collection methods during a two-day workshop and three weeks of field mentoring to ensure the accuracy of the data collected. Landings were recorded every morning at the Tanjung Luar shark auction facility where shark fishers usually landed dead sharks, from 5am to 10am from January 2014 to December 2015. The enumerators recorded data on catch composition and fishing behaviour (Table 1) from 52 different vessels across a total of 595 fishing trips. The enumerators also measured the weight of selected sharks to calculate biomass and length-weight relationship. Table 1. Types of data collected on fishing behaviour and catch composition during daily landings data collection at Tanjung Luar. From fishing behaviour and catch data we calculated the overall species composition of catch. We calculated catch per unit effort (CPUE) by number of individuals using both catch per set (hereafter CPUE per set) and catch per 100 hooks per set (hereafter standardised CPUE) [25,26]. This was deemed necessary since different vessels and gear-types systematically deploy different numbers of hooks, and standardised CPUE allows for a more meaningful comparison. To understand factors influencing overall CPUE we log transformed CPUE per trip to fit a normal distribution, and fitted linear models (LMs) of CPUE per trip to fishing behaviour variables (Table 1). We considered all variables and used minimum AIC values with stepwise analysis of variance to identify the best fit and most significant influencing variables. To inform the development of practical fisheries management measures (e.g. gear restrictions), we also specifically analysed differences in CPUE for surface and bottom longline gears employed in the fishery, using two-way ANOVAs. Factors affecting catch of threatened and regulated species."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A prominent researcher in the field of cognitive psychology has been studying the effects of social media on mental health. In a recent study, they found that individuals who spent more time on social media were more likely to experience anxiety and depression. However, the researcher also noted that the relationship between social media use and mental health is complex, and that other factors such as personality traits, socioeconomic status, and prior mental health history can also play a role. The researcher suggested that individuals who are already prone to anxiety and depression may be more susceptible to the negative effects of social media. What is the most likely explanation for why individuals who spend more time on social media are more likely to experience anxiety and depression?",
    "choices": [
      "A) Individuals who spend more time on social media are more likely to experience anxiety and depression because they are more likely to engage in pro-social behaviors, such as volunteering and donating to charity, which can have a positive impact on mental health.",
      "B) The relationship between social media use and mental health is causal, and individuals who spend more time on social media are more likely to experience anxiety and depression because they are more likely to be exposed to negative content, such as cyberbullying and online harassment.",
      "C) Individuals who spend more time on social media are more likely to experience anxiety and depression because they are more likely to have a higher socioeconomic status, which can provide access to better mental health resources and support.",
      "D) The relationship between social media use and mental health is bidirectional, and individuals who spend more time on social media are more likely to experience anxiety and depression because they are more likely to experience anxiety and depression, which can in turn lead to increased social media use as a coping mechanism."
    ],
    "correct_answer": "D)",
    "documentation": [
      "You wont always win, and nor will I. Try and get satisfaction form at least trying to win or putting up a good fight. About the limit on cubing. Please. I suggested that last year. Twice. Unfortunately multiscoping is allowed and so prevalent that it kind of ruins the idea. These are the facts: YOU and YOUR side threatened Faceless members who support London. One of the members threatened is actually London based. What do you expect him to do while his city is under attacked? A few of your members don't know when to keep quiet. They sent messages to us threatening specific players and telling them their zones will be dropped just because they have helped London. Basically if they help London then they get their zones wiped. And you have the cheek to call me a bully? We stood up to your members specifically because they were trying to bully. That is the reason we went strong months ago and took those big zones. How is this me bullying you? This is me answering your threats. I didn't attack those zones \"just because i can\", it's just because I should. Because of your threats. You can blame your members for the loss of those 2 or 3 big zones. These are the basics: You attack one of our zones. We look at the list of attackers and pick one of the players who deployed the most, find a zone of his or hers and attack it. We don't need to justify our attacks with \"because there are Faceless players within 30 miles\". What, every time we attack a zone we need to send some letter explaining why? You attack us or we attack you, for any reason. That's the game. When some of you were cheating and you could not win you complained, when you cube and lose you complain, when you invite all of Europe to attack and win you still complain. I just think you like to make a fuss. Last words (for now): You think I attack zones as a means of getting attention? I get enough of it form your threads. The only attention seeker here is you with your victim attitude and pity us posts. I've had some very angry emails today from a couple users who are upset their rival achieved the ability to switch factions freely having played for one of the factions for only 1 hour."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A theologian argues that the concept of original sin is not necessary for understanding the significance of Jesus' death on the cross. According to this view, the death of Jesus is sufficient to justify our salvation, regardless of our inherited sinfulness. However, this perspective overlooks the fact that Jesus' death was not just a means of atoning for our sins, but also a demonstration of God's power over sin and death. Furthermore, the Bible teaches that our relationship with sin is not just a matter of personal choice, but is also rooted in the Fall of humanity. Which of the following best captures the essence of this theologian's argument?",
    "choices": [
      "A) The death of Jesus is sufficient to justify our salvation, but it is not a demonstration of God's power over sin and death.",
      "B) The concept of original sin is not necessary for understanding the significance of Jesus' death on the cross, because the Bible teaches that our relationship with sin is a matter of personal choice.",
      "C) The death of Jesus is a demonstration of God's power over sin and death, but it is not sufficient to justify our salvation without the concept of original sin.",
      "D) The death of Jesus is both a means of atoning for our sins and a demonstration of God's power over sin and death, and this is only fully understood in the context of the Fall of humanity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Jesus, being fully God, lives a perfectly sinless life\u2014a life not meriting death\u2014and dies on our behalf, paying for all the sin of the world. Let that sink in for a moment: God dies. But the death of God becomes the death of sin, and the death of sin becomes the death of death. And death\u2019s final defeat is announced through the resurrection of God back from the dead. The God of life is alive! And He offers eternal life to all. As Tim Keller likes to put it, Jesus died the death we deserved so that we could live the life He deserved. Because Jesus submitted to death on our behalf, our relationship with death gets really complicated. It\u2019s still the enemy. It\u2019s still the wages of sin. It\u2019s still not good. But every good thing\u2014salvation, resurrection, eternal life, peace with God\u2014these all came from one great death: the Crucifixion. So now all death is bad, but that one death brought us everything good. We praise the God of life, but we celebrate His death. God took a horrible, terrible, rotten, no-good thing and redeemed it. I suppose that shouldn\u2019t surprise us either. We may sometimes look like we\u2019re rejoicing in death itself, but really we rejoice in that one death that God used to bring eternal life. Our problem isn\u2019t that we sing about death too much\u2014we probably don\u2019t sing about it enough! But we have to keep it in the context of the bigger story. We can\u2019t make any sense of the Crucifixion apart from the Fall, the Resurrection, and Return of Christ. This is the theme we see in the Book of Acts: God raised Jesus from the dead. It\u2019s all about resurrection now! We baptize in the likeness of His death\u2014and resurrection. We take the bread and cup to remember His death\u2014all the while waiting for His return. In Romans, death takes on a whole new meaning: since our sins were buried with Christ, we are now alive to God and dead to sin. Spiritual death is over now. Death has become just a metaphor for our relationship with sin. But make no mistake, death didn\u2019t just die spiritually. We might think that because we still see death all around us."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is simulating a non-equilibrium flow using the simplified Boltzmann equation. The equation is discretized by replacing the continuous velocity space with a limited number of particle velocities. However, the discretization process introduces a significant error in the calculation of the specific-heat ratio, \u03b3. What is the primary consequence of this error on the simulation results?",
    "choices": [
      "A) The simulation will produce more accurate results for high Kn numbers, as the discretization error is negligible in this regime.",
      "B) The simulation will produce more accurate results for low Kn numbers, as the discretization error is significant in this regime.",
      "C) The simulation will produce more accurate results for flows with a large number of extra degrees of freedom, as the discretization error is proportional to the number of degrees of freedom.",
      "D) The simulation will produce more accurate results if the discretization error is compensated by adjusting the value of the free parameter, \u03b7."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(ii) Discretization of the particle velocity space under the condition that the reserved kinetic moments keep their values unchanged. (iii) Checking the TNE state and extracting TNE information. (iv) The selection/design of the boundary conditions. Simplification and modification of the Boltzmann equation\n\nAs we know, the collision term in the original Boltzmann contains high dimensional distribution functions. Therefore, the direct solution to it needs too much computing consumption. The most common method to simplify the collision operator is to introduce a local equilibrium distribution function ( f eq ) and write the complex collision operator in a linearized form, i.e., the original BGK collision operator \u2212 1 \u03c4 ( f \u2212 f eq ), where \u03c4 is the relaxation time . The original BGK operator describes the situation where the system is always in the quasi-equilibrium state. Namely, it characterizes only the situation where the Kn number of the system is small enough and f \u2248 f eq . The currently used BGK operator for non-equilibrium flows in the field is a modified version incorporating the meanfield theory description . Based on the above considerations, the simplified Boltzmann equation describing the SBI process is where the two-dimensional equilibrium distribution function is ) where \u03c1, T , v, u, I, R, and \u03b7 are the mass density, temperature, particle velocity vector, flow velocity vector, the number of the extra degrees of freedom including molecular rotation and vibration inside the molecules, gas constant, and a free parameter that describes the energy of the extra degrees of freedom, respectively. The specific-heat ratio is flexible by adjusting parameter I, i.e., \u03b3 = (D + I + 2)/(D + I), where D = 2 represents the two-dimensional space. Discretization of the particle velocity space and determination of f \u03c3 ,eq i\n\nThe continuous Boltzmann equation should be discretized for simulating. Specifically, the continuous velocity space can be replaced by a limited number of particle velocities."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary reason why the authors were unable to improve the performance of the language identification model using the improved data sampling method?",
    "choices": [
      "A) The model was not fine-tuned for the specific dialects of the languages, leading to poor performance on those dialects.",
      "B) The model was over-sampled, resulting in an imbalance in the dataset that negatively impacted performance.",
      "C) The model was not able to generalize well to new languages, leading to poor performance on languages not present in the training data.",
      "D) The model was fine-tuned for 20 epochs, which was not sufficient to improve performance, and the learning rate was too high, causing overfitting."
    ],
    "correct_answer": "D)",
    "documentation": [
      "We tried to mitigate this imbalance using over-sampling and weighted sampling methods. However, the improved data sampling method did not affect the performance. System Description\n\nThis was a problem of multi-class classification having 9 classes for Track-1 and 6 classes for Track-2. The samples were belonging to 3 languages having 3 varieties each, so the classification pipeline was made in 2 stages. The Language Identification (LID) model which is the first stage classifies the sentence into 3 languages: English (EN), Spanish (ES) and Portuguese (PT). The LID is a pretrained XLM-RoBERTa that is fine-tuned for the task of language identification. It is able to classify the input sentence into 20 languages. We classify and separate the samples according to their language. The samples corresponding to the specific languages are then fed into the language specific models for dialect identification. For dialect identification we have used models like BERT and RoBERTa with a linear layer connected to the pooler output of the models. Then fine-tuning is done on the models for dialect identification   using the samples corresponding to the specific languages. For the task of dialect identification we experimented with several pretrained models like XLM-RoBERTa, BERT, ELECTRA, GPT-2 and RoBERTa. All models were fine-tuned for 20 epochs with a learning rate of 1e-6 and weight decay 1e-6 with a batch size of 8. The best performing model checkpoint was chosen according to the epoch-wise validation macro-F1 score. 5 Experiments and Results\n\nExperiments using Large Language Models\n\nFor the task of Dialect Identification we have tried various language specific models like XLM-RoBERTa, BERT, ELECTRA, RoBERTa and GPT- 2. The base variant of all these models were used and all the models were used through the Hugging-Face library. The pooler output of these models was passed through a linear layer and the models were fine-tuned. First, we experimented with different models for Track-1. All the models were trained for 20 epochs with learning rate 1e-6, weight decay 1e-6 and a batch size of 8."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In a study on the relationship between sleep duration and cognitive performance, researchers found that individuals who slept for less than 7 hours per night performed significantly worse on a memory recall task compared to those who slept for 7 hours or more. However, the study also revealed that the relationship between sleep duration and cognitive performance was moderated by the presence of a specific genetic variant. Which of the following statements best summarizes the implications of this finding?",
    "choices": [
      "A) The genetic variant had no effect on the relationship between sleep duration and cognitive performance.",
      "B) The genetic variant increased the cognitive performance of individuals who slept for less than 7 hours per night.",
      "C) The genetic variant reduced the cognitive performance of individuals who slept for 7 hours or more per night.",
      "D) The genetic variant eliminated the negative relationship between sleep duration and cognitive performance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "T h . For the second inequality Lemma 2(ii) is used. Applying Lemma 1.6 of then yields To bound the second term in (A.9), we use the moment bounds for \u03be k derived in Lemma 4. Then, for all integers > 1 Combining the error bounds (A.10) and (A.11) and choosing R=m \u22121/2 gives By assumption T = p \u03c5 and m = np (1\u2212\u03c5) for some fixed \u03c5 \u2208 ((4 \u2212 2 min{1, \u03b2})/3, 1). If is an integer such that \u2265 1/(1 \u2212 \u03c5), then where we used log(x) \u2264 x a /a with a = 1/(4 ). Consider 1/2 < \u03b2 \u2264 1 and let 0 < \u03c7 < 1 be a constant. Applying log(x) \u2264 x a /a twice with a = \u03c7/(2 ) yields For any fixed \u03c5\u2208((4 \u2212 2 min{1, \u03b2})/3, 1) one can find an integer which is independent of n, p such that the right side of (A.12) holds. Since p/n \u2192 c \u2208 (0, \u221e] and thus n/p = O(1) and p \u22121 = O(n \u22121 ), it follows for satisfying (A.12) that In total, choosing an integer Using the representation in Lemma 4 once more gives for each x \u2208 [0, 1] The bounds on k in Lemma 4 imply Consider the case that \u03b2 \u2265 1. In particular, q = \u03b3 and f (q) is \u03b1-H\u00f6lder continuous. Since f is a periodic function with f (x) \u2208 [\u03b4, M 0 ] and H(y) \u221d \u03c6(m/2)+ log (2y/m), it follows that {H(f )} (q) is also \u03b1-H\u00f6lder continuous. Extending g := H(f ) to the entire real line, we get Expanding g(t) in a Taylor series around x and using that h \u22121 K h is a kernel of order 2q, see Lemma 2(iii), it follows that for any x \u2208 [0, 1]\nwhere \u03be x,t is a point between x and t. Using the fact that the kernel K h decays exponentially and that g (q) is \u03b1-H\u00f6lder continuous on [\u03b4, M 0 ] with some constant L, the logarithm is Lipschitz continuous on a compact interval, it follows g = H(f ) is \u03b2-H\u00f6lder continuous. Expanding g to the entire line and using Lemma 2(iii) with\nIn a similar way as before, one obtains Note that T \u2212\u03b2 =o(h \u03b2 ) as \u03b2 > 1/2, T h \u2192 \u221e and h \u2192 0 by assumption. Since the derived bounds are uniform for x \u2208 [0, 1] it holds Putting the bounds A.13 and A.14 together gives If h > 0 such that h \u2192 0 and hT \u2192 \u221e, then with T = p \u03c5 for any \u03c5 \u2208 ((4 \u2212 2 min{1, \u03b2})/3, 1), the estimator f described in Section 3 with q = max{1, \u03b3} satisfies\nProof : By the mean value theorem, it holds for some function g between H(f ) and To show that the second term on the right hand side of (A.15) is negligible we use the moment generating function of H(f ) \u221e ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Benjamin Franklin's residence on Craven Street was a hub for intellectual and scientific discussions. What was a notable consequence of the Craven Street circle's interactions, as described in historical accounts?",
    "choices": [
      "A) The establishment of the Royal Society's London branch, with Benjamin Franklin as its president, due to the influential connections made during his time on Craven Street.",
      "B) The development of a new medical theory, as suggested by William Hewson's experiments with the Craven Street bones, which were later adopted by the medical community.",
      "C) The creation of a new scientific society, modeled after the American Philosophical Society, which was founded by Benjamin Franklin and his colleagues during his time on Craven Street.",
      "D) The publication of a influential book on electricity, written by Benjamin Franklin and based on his experiments conducted at his Craven Street residence, which helped to establish him as a leading figure in the scientific community."
    ],
    "correct_answer": "D)",
    "documentation": [
      "[46] Westminster Rate Books, Craven Street \u2013 1773, courtesy of the City of Westminster Archives. [47] S.W. Hillson et al., \u201cBenjamin Franklin, William Hewson, and the Craven Street Bones,\u201d Archaeology International, Vol. 2, (Nov. 22, 1998): 14-16. http://dx.doi.org/10.5334/ai.0206 [48] Westminster Rate Books, Craven Street \u2013 1774, 1775, courtesy of the City of Westminster Archives. [49] Survey of London, Historical Notes/No. 36, Craven Street (not sourced). [50] Gordon S. Wood, The Americanization of Benjamin Franklin, (New York: The Penguin Press, 2004), 261. [51] Pettigrew, Memoirs, 146 of Correspondence. [52] http://founders.archives.gov/documents/Franklin/01-22-02-0178, note 7. \u201cFalconar married Hewson\u2019s sister five months after the Doctor\u2019s death; most of the Craven Street circle attended the wedding, and BF gave away the bride: Polly to Barbara Hewson, Oct. 4, 1774, APS\u201d (American Philosophical Society); \u201cEngland Marriages, 1538\u20131973 ,\u201d database, FamilySearch (https://familysearch.org/ark:/61903/1:1:V52W-TGS : accessed September 15, 2015), Magnus Falconar and Dorothy Hewson, September 12, 1774; citing Saint Martin In The Fields, Westminster, London, England, reference ; FHL microfilm 561156, 561157, 561158, 942 B4HA V. 25, 942 B4HA V. 66. [53] I chose to rely on the Westminster Rate Books for the numbering system on Craven Street. The books were consistent throughout the eighteenth century in the ordering of residents on the street and were used as the basis for the 1792 re-numbering. For the most part, commercial directories aligned with them as well. If by chance a directory didn\u2019t initially align, it would inevitably produce future editions that did. Benjamin Franklin, Benjamin Franklin House, London\nMore from David Turnquist If one looked into Benjamin Franklin\u2019s time on Craven Street, they might... I think it\u2019s very ironic that on the street maps included in your excellent article, Craven Street is so close to Scotland Yard. Because following the back and forth juxtapositions of numbers 7, 27 and 36 Craven Street (throw in 75 Northumberland Court and 1 Craven Street, too) was a case that could confound Sherlock Holmes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Libyan rebels' claim that they have captured a key NATO airbase is being met with skepticism by some, who argue that the rebels' military capabilities are not sufficient to hold such a strategic location. However, others point out that the rebels have already demonstrated their ability to adapt and improvise in the face of adversity. What is the most likely reason why the Libyan rebels are able to hold the airbase, despite their limited military capabilities?",
    "choices": [
      "A) The rebels have been able to exploit a weakness in the NATO air defense system, which has allowed them to gain control of the airbase without a fight.",
      "B) The rebels have been able to negotiate a ceasefire with the remaining Gadhafi loyalists, who have agreed to stand down and allow the rebels to take control of the airbase.",
      "C) The rebels have been able to use their knowledge of the airbase's layout and infrastructure to their advantage, and have been able to disable the airbase's defenses without a direct confrontation.",
      "D) The rebels have been able to secure the airbase through a combination of diplomacy and military force, with NATO providing covert support to the rebels in order to prevent the airbase from falling into the hands of the Gadhafi regime."
    ],
    "correct_answer": "D)",
    "documentation": [
      "So one of them was a doctor, one of them was a medic. And we were in the hospital. And there was real anger at NATO, anger at the fact that when they needed those air strikes on the Gadhafi forces, they weren't getting them. And now, for the second time in a week, there's been another strike. Now, of course, we must stress that NATO says that they -- because they don't have enough boots on the ground, they can neither confirm nor deny this was a NATO strike. But certainly, speaking to eyewitnesses in the hospital, it certainly sounded like an air strike. And there are no other planes in the skies of Libya other than NATO planes -- Wolf.\nBLITZER: Ben Wedeman in Benghazi for us. The U.S. says Moammar Gadhafi is no longer the legitimate leader of Libya. So why not recognize the rebels? Why one U.S. official says it raises serious concerns. And a former U.S. Congressman in Libya armed with a message for the Libyan dictator. Will he get to meet with him face-to-face? My interview with Curt Weldon, that Republican former Congressman -- that's coming up, as well. BLITZER: Let's get right to Jack. He's got some nuclear concerns on his mind with The Cafferty File -- Jack. JACK CAFFERTY, THE CAFFERTY FILE: Well, they had another little temblor in Japan -- a 7.1 magnitude earthquake hit Northeastern Japan today, the strongest aftershock since that massive 9.0 quake and tsunami that followed devastated that nation four weeks ago. And this one today was in roughly the same area. One of the big concerns, of course, is possible further damage to the Fukushima Daiichi nuclear power plant. The Tokyo Electric Power Company, TEPCO, which operates the plant -- or what's left of it -- said there were no serious incidents as a result of today's aftershock. So they say. Radioactivity from that plant has poisoned the surrounding land, air and ocean. Millions of people have been exposed. Millions more could be, as radioactivity has been picked up in food and drinking water and detected in faraway places, like California."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The ACS bylaws change recommendations are likely to have a significant impact on the organization's future development. What is a potential consequence of the comprehensive petition's adoption?",
    "choices": [
      "A) The Project SEED program will be discontinued due to the increased administrative burden of implementing the new bylaws.",
      "B) The Committee on Ethics' efforts to organize a committee retreat will be hindered by the need to prioritize the new bylaws over other committee activities.",
      "C) The ACS will experience a significant increase in membership, as the new bylaws will make it easier for new members to join the organization.",
      "D) The ACS will need to allocate additional resources to support the implementation of the new bylaws, which will require significant changes to the organization's governance structure."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The committee received input from the Governance Review Task Force and its action teams, the Council Policy Committee, the board of directors, the Committee on Constitution & Bylaws, and several other committees between the San Francisco and Chicago meetings. These interactions have resulted in the current bylaw change recommendations. In Chicago, representatives from MAC attended several committee meetings and all seven councilor caucuses to summarize the current proposal for membership changes, answer questions, and seek input. In addition, all committee chairs were invited to have their respective committees review these bylaw changes and respond to MAC\u2014if possible\u2014before council met on Wednesday. MAC received 11 responses: eight supported the proposed changes as is, and three supported the proposed language with specified changes or considerations. The comprehensive petition will likely represent the most significant and voluminous change in the ACS bylaws that has occurred in decades, and MAC is proud to be among the leaders in its development and in efforts to get it right the first time. Hundreds of individuals have contributed to this major effort, since MAC began such discussions at the spring 2004 national meeting. The Committee on Ethics met in Chicago and discussed the possibility of organizing and scheduling a committee retreat in the near future to enable the committee to move from the current stage of exploring the needs and interests of ACS members to setting priorities for the next few years. The Project SEED program offers summer research opportunities for high school students from economically disadvantaged families. Since its inception in 1968, the program has had a significant impact on the lives of more than 8,400 students. At the selection meeting in March, the committee approved research projects for 340 SEED I students and 98 SEED II students for this summer in more than 100 institutions. The 2006 annual assessment surveys from 300 students indicate that 78% of the Project SEED participants are planning to major in a chemistry-related science, and 66% aspire to continue to graduate education."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A mother, who has been struggling with anxiety and depression, has made the decision to send her daughter to live with her friends' family for a few days a week. This decision was made after a family meeting with a counselor, where the daughter expressed her desire to escape the stress at home due to the mother's strict rules and behavior. However, the mother's husband, who is also the daughter's father, has expressed concerns about the daughter's well-being and the potential impact on her academic performance. What is the most likely motivation behind the mother's decision to send her daughter to live with her friends' family?",
    "choices": [
      "A) The mother wants to give her daughter more freedom and autonomy, but is struggling to cope with her own emotional needs.",
      "B) The mother is trying to punish her daughter for her poor academic performance and lack of discipline.",
      "C) The mother is trying to protect her daughter from the stress of living with her, but is not willing to make any changes to her own behavior.",
      "D) The mother is trying to give her daughter a break from the stress of living with her, but is also trying to maintain control over her daughter's life and academic performance."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Leading up to this I had been battling anxiety and depression which my husband found very hard to cope with. Over the years of our relationship I knew something was off but I just could not put my finger on it. I often felt a complete lack of validation and empathy. Communication was also difficult as my husband was defensive and unwilling to look at issues in our marriage. Please Mark could you help me validate some of this pain and try and make dense of 27 years of my life without drowning in fear guilt and despair about my future. Thank you for listening and your site. I have had problems with drunkenness, being late for school, not handing in school work, buying pot from a dealer etc. I chose to focus on the drinking and did the grounding then (grounding happened 3 times). I also stopped sleep overs at friends 100%. I have stopped handing out money for no reason or even buying treats like chocolate. I did lose it one evening (and didn't do the poker face) when I was trying to unplug the internet at midnight on a school night (she\u2019s always late for school so I am trying to get her to sleep at a reasonable hour). I was physically stopped and pushed around so I slapped my daughter (it was not hard). This ended up with her saying she didn\u2019t want to come home (the next day after school). By this stage, I also had enough and didn\u2019t go get her. I thought I am not begging. You will run out of money soon. It was quite a relief to have some peace. Daughter\u2019s Dad was in town (from another country) and called a family meeting with the counsellor. To cut a long story short, daughter and her counsellor put it on the table that daughter wants to go live somewhere else (with her friends family) because of the stress at home with me (we live on our own) (i.e. stricter rules and her bucking up against it). I didn\u2019t really want this but made a compromise that daughter would go there Tues morning \u2013 Friday afternoon as the friend is an A student whereas my daughter is failing. They do the same subjects. I made the decision at the end of the day based on what is good for me \u2013 some time away from the daughter."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A pregnant woman with a history of alpha thalassemia major is found to have a higher-than-normal level of fetal hemoglobin in her blood. Which of the following is the most likely reason for this finding?",
    "choices": [
      "A) She has been taking hydroxyurea as part of her treatment regimen to increase fetal hemoglobin production.",
      "B) Her spleen is enlarged due to splenomegaly, which can lead to increased fetal hemoglobin levels.",
      "C) She has a mutation in the HLA gene that affects the production of fetal hemoglobin.",
      "D) She has been screened for alpha thalassemia major and has been identified as a carrier, which is a common occurrence in her family."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Hemoglobin electrophoresis \u2014 A laboratory test that separates molecules based on their size, shape, or electrical charge. Hepatomegaly \u2014 An abnormally large liver. HLA type \u2014 Refers to the unique set of proteins called human leukocyte antigens. These proteins are present on each individual's cell and allow the immune system to recognize 'self' from 'foreign'. HLA type is particularly important in organ and tissue transplantation. Hydroxyurea \u2014 A drug that has been shown to induce production of fetal hemoglobin. Fetal hemoglobin has a pair of gamma-globin molecules in place of the typical beta-globins of adult hemoglobin. Higher-than-normal levels of fetal hemoglobin can ameliorate some of the symptoms of thalassemia. Iron overload \u2014 A side effect of frequent blood transfusions in which the body accumulates abnormally high levels of iron. Iron deposits can form in organs, particularly the heart, and cause life-threatening damage. Jaundice \u2014 Yellowing of the skin or eyes due to excess of bilirubin in the blood. Mutation \u2014 A permanent change in the genetic material that may alter a trait or characteristic of an individual, or manifest as disease, and can be transmitted to offspring. Placenta \u2014 The organ responsible for oxygen and nutrition exchange between a pregnant mother and her developing baby. Red blood cell \u2014 Hemoglobin-containing blood cells that transport oxygen from the lungs to tissues. In the tissues, the red blood cells exchange their oxygen for carbon dioxide, which is brought back to the lungs to be exhaled. Screening \u2014 Process through which carriers of a trait may be identified within a population. Splenomegaly \u2014 Enlargement of the spleen. Because alpha thalassemia major is most often a condition that is fatal in the prenatal or newborn period, treatment has previously been focused on identifying affected pregnancies in order to provide appropriate management to reduce potential maternal complications. Pregnancy termination provides one form of management. Increased prenatal surveillance and early treatment of maternal complications is an approach that is appropriate for mothers who wish to continue their pregnancy with the knowledge that the baby will most likely not survive."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A company develops a machine learning model to predict customer churn based on historical data. The model is trained on a dataset that includes information about customer demographics, purchase history, and social media activity. The company claims that the model is eligible for patent protection under the Bilski test, as it transforms the input data into a new, useful, and non-obvious output. However, the company's competitor argues that the model is not eligible for patent protection because it is simply a general-purpose computer program.",
    "choices": [
      "A) The company's model is eligible for patent protection because it is a machine that performs a specific function, namely predicting customer churn.",
      "B) The company's model is not eligible for patent protection because it is a machine that performs a general-purpose function, such as data analysis.",
      "C) The company's model is eligible for patent protection because it is a machine that performs a specific function, but the input data is not transformed in a way that is beyond the ordinary level of skill in the field.",
      "D) The company's model is eligible for patent protection because it is a machine that transforms the input data into a new, useful, and non-obvious output, and the input data includes information about customer demographics, purchase history, and social media activity."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Notably, while the independent claim failed the formation that would qualify under the \u2018\u2018transforma- machine-or-transformation test, its dependent claim tion\u2019\u2019 prong of Bilski. Given these disputed issues, the was eligible because it recited, \u2018\u2018further comprising us- ITC concluded that it was inappropriate to grant sum- ing the selected features in training a classifier for clas- mary judgment as to the patent eligibility of the claims. sifying data into categories.\u2019\u2019 In view of the specifica- A similar conclusion was reached in Versata Soft- tion, the board indicated that the \u2018\u2018classifier\u2019\u2019 was a par- ware Inc. v. Sun Microsystems Inc.,48 in which the dis- ticular machine \u2018\u2018in that it performs a particular data trict court denied the defendant\u2019s motion for summary classification function that is beyond mere general pur- judgment of invalidity under Section 101 based upon pose computing. \u2019\u201953 The board also concluded that the the Bilski court\u2019s refusal \u2018\u2018to adopt a broad exclusion claim \u2018\u2018transforms a particular article into a different over software or any other such category of subject state or thing, namely by transforming an untrained matter beyond the exclusion of claims drawn to funda- classifier into a trained classifier. \u2019\u201954 In Ex parte Casati,55 the board reversed the examin- Less stringent \u2018\u2018machine\u2019\u2019 prong analyses are also er\u2019s Section 101 rejection of a method claim reciting: found at the board level. For example, in Ex parteSchrader,50 the board held patent-eligible under Bilski A method of analyzing data and making predictions, reading process execution data from logs for a busi- A method for obtaining feedback from consumers re- ceiving an advertisement from an ad provided by anad provider through an interactive channel, the collecting the process execution data and storing the process execution data in a memory defining a ware-house; creating a feedback panel including at least one feed-back response concerning said advertisement; and analyzing the process execution data; generatingprediction models in response to the analyzing; and providing said feedback panel to said consumers, using the prediction models to predict an occurrence said feedback panel being activated by a consumer to of an exception in the business process."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new social media platform is considering implementing a feature that allows users to report and flag content that they believe to be misinformation. The platform's terms of service state that users who flag content in this manner will be rewarded with badges and other incentives. However, some critics argue that this feature could lead to the suppression of dissenting voices and the marginalization of minority groups. Others argue that the platform is simply trying to promote a more nuanced and informed public discourse. What is the most likely outcome of this feature, given the platform's stated goals and the potential risks and benefits?",
    "choices": [
      "A) The feature will lead to a significant increase in the number of users who report and flag content, resulting in a more homogeneous and conformist online community.",
      "B) The feature will have a net positive effect on the platform's user engagement and overall health, as users will be incentivized to participate in the moderation process and promote high-quality content.",
      "C) The feature will lead to a significant increase in the number of users who report and flag content, but will also result in a decrease in the number of users who feel comfortable expressing their opinions and engaging in online discussions.",
      "D) The feature will lead to a more nuanced and informed public discourse, as users will be incentivized to engage in constructive dialogue and debate with others, and to critically evaluate the information they encounter online."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Take away freedom and order will be overthrown -- witness the Soviet Union. Take away tradition, and modernization will be crushed -- witness Iran. The clearing must be respected and it must move. Just as Benjamin Cardozo of the U.S. Supreme Court said 65 years ago, the genius of the American system is its penchant for ordered liberty. When both halves of the equation work against each other and together in Hegelian terms, the clearing that they produce is, at any given time, a prevailing hypothesis, which is challenged by a new antithesis. Together they can produce a fresh synthesis. And all that is very familiar. What is new and trying is the sweep and pace of innovation today, plus -- and this is what we sometimes forget -- the political volatility of the value systems that this can induce. If you doubt that, consider the Buchanan campaign and what's been going on with the Endowment for the Arts and public broadcasting. These are signs of people running scared, and they can cause damage. So the answer for the 21st century is to proceed under power, but with restraint, to practice what Mitch Kapor in another connection called toleration for opposing forces and perspectives. We need each other to keep the enterprise together and on course. For computer practitioners represented in this room, this means restraint from provoking unnecessary and damaging social backlash. A good example might be New York telcos offering free per-call and per-line blocking with this caller identification service. For regulators and law enforcers, restraint means asking, \"Do you know enough to freeze emerging conduct in a particular form or pattern?\" I was very taken by the role reversal exercise organized by Michael Gibbons on Wednesday night. It led me to wonder what might have happened to the government's wiretapping and encryption proposals had they been subjected to a comparable advanced exercise before introduction. Sixteen years ago in Aspen, Colorado, I convened a gathering of federal policymakers and invited them to consider a suggested matrix of policy values and processes in the information society."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the interaction between a shock wave and a heavy-cylindrical bubble using the discrete Boltzmann method (DBM) found that the specific-heat ratio effects on the shock wave's interaction with the bubble can lead to a significant increase in the transmitted shock wave's intensity. However, the same study also revealed that the reflected shock wave's intensity is not necessarily affected by the specific-heat ratio. Which of the following statements best summarizes the relationship between the specific-heat ratio and the shock wave's interaction with the bubble?",
    "choices": [
      "A) The specific-heat ratio has a direct impact on the reflected shock wave's intensity, causing it to increase with an increase in the specific-heat ratio.",
      "B) The specific-heat ratio has a negligible effect on the transmitted shock wave's intensity, and the reflected shock wave's intensity remains unaffected.",
      "C) The specific-heat ratio affects the transmitted shock wave's intensity, but only in the case where the bubble's surface tension is high.",
      "D) The specific-heat ratio has a complex effect on the shock wave's interaction with the bubble, causing the transmitted shock wave's intensity to increase while the reflected shock wave's intensity decreases, but only when the bubble's surface tension is low."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Paper Info\n\nTitle: Specific-heat ratio effects on the interaction between shock wave and heavy-cylindrical bubble: based on discrete Boltzmann method\nPublish Date: May 29, 2023\nAuthor List: Yanbiao Gan (from School of Liberal Arts and Sciences, Hebei Key Laboratory of Trans-Media Aerial Underwater Vehicle, North China Institute of Aerospace Engineering), Yudong Zhang (from School of Mechanics and Safety Engineering, Zhengzhou University) Figure\n\nFigure 1: Research orientation and tasks of DBM. Figure 2: Sketch of D2V16 model. The numbers in the figure represent the index i in Eq. (3). Figure 3: The computational configuration of the shock-bubble interaction. In the figure, results from odd rows are experimental, and the even rows indicate DBM simulation results. The typical wave patterns and bubble's main characteristic structures are marked out in the figures. Numbers in the pictures represent the time in \u00b5s. Schlieren images of DBM results are calculated from the density gradient formula, i.e., |\u2207\u03c1|/|\u2207\u03c1| max , with |\u2207\u03c1| = (\u2202 \u03c1/\u2202 x) 2 + (\u2202 \u03c1/\u2202 y)2 .At t = 0\u00b5s, the incident shock wave impacts the upstream interface, and subsequently generates a transmitted shock (TS) propagating downstream in the bubble and a reflected shock wave moving upward in ambient gas. The incident shock wave travels downstream contin-\nThe definitions and the corresponding physical meanings of the common TNE quantities in DBM, where the operator \u2211 ix,iy indicates integrating over all the fluid units and multiply the unit area dxdy. From a certain perspective, the TNE strength is increasing; While from a different perspective, the TNE strength, on the other hand, may be decreasing. It is one of the concrete manifestations of the complexity of non-equilibrium flow behavior. Figure 4: Snapshots of schlieren images of the interaction between a shock wave and a heavy-cylindrical bubble. The odd rows represent experimental results from Ref. [31] with permission, and the even rows are DBM simulation results. The typical wave patterns and the bubble's main characteristic structure are marked out in the figures."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If the United States' withdrawal from the Trans Pacific Partnership Agreement (TPPA) is seen as a significant blow to the agreement's prospects, what can be inferred about the potential implications for New Zealand's trade relations with Europe?",
    "choices": null,
    "correct_answer": "D)",
    "documentation": [
      "Ng\u0101puhi have protested the Government's negotiation of the Trans Pacific Partnership Agreement (TPPA), which the iwi believe infringes upon M\u0101ori sovereignty, and thus does not adhere to the Treaty of Waitangi. English had been invited to attend in an official capacity; his non-attendance was criticised by a Ng\u0101puhi elder and Opposition leader Andrew Little. In his first overseas trip as Prime Minister, English travelled to Europe to discuss trade ties, including a prospective New Zealand\u2013European Union free trade agreement. He first travelled to London on 13 January 2017 to meet British Prime Minister Theresa May. Discussing trade relations, English said the two nations were \"natural partners\" and would \"continue to forge ties\" after the UK's withdrawal from the EU. He also arranged to meet with London Mayor Sadiq Khan, Belgian Prime Minister Charles Michel and German Chancellor Angela Merkel. In a meeting with Merkel, English received crucial backing from Germany for a trade deal with the EU. On 16 January, English stated that his government would continue to promote TPPA, despite the United States' decision to withdraw from the agreement. He explained that Southeast Asian countries would now be treated as a priority in negotiations\u2014he also asserted that the United States was ceding influence to China by its rejection of the trade pact. At a press conference at the Beehive on 1 February 2017, English announced that the 2017 general election would be held on 23 September. The Prime Minister later confirmed that his party would approach ACT, United Future and the M\u0101ori Party if confidence and supply agreements were required to form a government following the election. In his second cabinet reshuffle on 24 April, English appointed Gerry Brownlee as his new Foreign Affairs Minister; he also promoted Nikki Kaye to the portfolio of Education Minister, and moved Mark Mitchell into the cabinet to become Defence Minister. The reshuffle was perceived as an election preparation. On 13 February 2017, English welcomed Australian Prime Minister Malcolm Turnbull to Wellington."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary consequence of Samsung and TSMC's involvement in the ARM project, considering the historical context of the mobile industry's shift towards horizontal integration?",
    "choices": [
      "A) The increased competition between Samsung and TSMC will lead to a decrease in the overall market share of Intel and other manufacturers, as the new ARM-based SoCs will be more power-efficient and cost-effective.",
      "B) The partnership between Samsung and TSMC will result in a significant increase in the production capacity of ARM, allowing the company to dominate the market and reduce its reliance on Intel.",
      "C) The involvement of Samsung and TSMC in the ARM project will lead to a decrease in the number of phone manufacturers, as the new ARM-based SoCs will be more expensive and less compatible with existing designs.",
      "D) The partnership between Samsung and TSMC will put significant pressure on Intel to adapt to the changing market landscape, potentially leading to a shift in Intel's focus towards more power-efficient and cost-effective SoCs."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This will increase yields dramatically. Of course Samsung and TSMC will build ARM out, but it definitely puts quite a bit of pressure on all other manufacturers. As the article mentions Intel and Samsung are the only ones who control production top to bottom, and Samsung must share some of the benefits with ARM. As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP. Last edited by paul5ra on Wed Feb 13, 2013 11:06 am\nintroiboad wrote:I am happy to see Kanter here at Ars, I like his writing and he maintains Real World Tech, where Linus Torvalds often shows up to comment on CPU arch and other interesting topics. Indeed. Most tech writing in this area is atrocious. This piece is one of the few well informed articles I've read in a long time. ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A wealthy businessman, known for his ruthless tactics in the boardroom, has proposed a unique arrangement to his loyal employee: in exchange for a significant salary increase, the employee must agree to marry his daughter, who is currently engaged to another man. However, the employee's wife has recently passed away, leaving him with a young daughter of his own. The employee's daughter is known to be fiercely independent and has a strong sense of self-preservation. Which of the following is the most likely outcome of this proposal?",
    "choices": [
      "A) The employee's daughter will accept the proposal and marry her stepfather, recognizing the benefits of the increased salary and the security it provides for her and her young half-sister.",
      "B) The employee's daughter will reject the proposal, citing her own independence and the fact that she is already engaged to another man, and will instead pursue a career in business, using her father's wealth and influence to support herself.",
      "C) The employee's daughter will accept the proposal, but only if her stepfather agrees to provide her with a significant amount of financial support and protection, allowing her to maintain her independence and pursue her own interests.",
      "D) The employee's daughter will reject the proposal, but will instead propose a counteroffer: she will marry her stepfather, but only if he agrees to step down as CEO of the company and allow her to take over the reins, recognizing the benefits of the increased salary and the security it provides for her and her young half-sister."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Mistaken Mistress (2002)\n24. Outback Angel (2002)\n33. The Australian Tycoon's Proposal (2004)\n35. His Heiress Wife (2004)\n\nMarrying the Boss Series Multi-Author\nBoardroom Proposal (1999)\n\nContract Brides Series Multi-Author\nStrategy for Marriage (2002) Everlasting Love Series Multi-Author\nHidden Legacy (2008)\n\nDiamond Brides Series Multi-Author\nThe Australian's Society Bride (2008) Collections\nSummer Magic / Ring of Jade / Noonfire (1981)\nWife at Kimbara / Bridesmaid's Wedding (2005)\n\nOmnibus in Collaboration\nPretty Witch / Without Any Amazement / Storm Over Mandargi (1977) (with Lucy Gillen and Margaret Malcolm)\nDear Caliban / Heart of the Eagle / Swans' Reach (1978) (with Jane Donnelly and Elizabeth Graham)\nThe Bonds of Matrimony / Dragon Island / Reeds of Honey (1979) (with Elizabeth Hunter and Henrietta Reid)\nThe Man Outside / Castles in Spain / McCabe's Kingdom (1979) (with Jane Donnelly and Rebecca Stratton)\nWinds From The Sea / Island of Darkness / Wind River (1979) (with Margaret Pargeter and Rebecca Stratton)\nMoorland Magic / Tree of Idleness / Sweet Sundown (1980) (with Elizabeth Ashton and Elizabeth Hunter)\nThe Shifting Sands / Portrait of Jaime / Touched by Fire (1982) (with Jane Donnelly and Kay Thorpe)\nHead of Chancery / Wild Heart / One-Way Ticket (1986) (with Betty Beaty and Doris Smith)\nHeart of the Scorpion / The Winds of Heaven / Sweet Compulsion (1987) (with Janice Gray and Victoria Woolf) One Brief Sweet Hour / Once More With Feeling / Blue Lotus (1990) (with Jane Arbor and Natalie Sparks) Marry Me Cowboy (1995) (with Janet Dailey, Susan Fox and Anne McAllister)\nHusbands on Horseback (1996) (with Diana Palmer)\nWedlocked (1999) (with Day Leclaire and Anne McAllister)\nMistletoe Magic (1999) (with Betty Neels and Rebecca Winters) The Australians (2000) (with Helen Bianchin and Miranda Lee)\nWeddings Down Under (2001) (with Helen Bianchin and Jessica Hart)\nOutback Husbands (2002) (with Marion Lennox) The Mother's Day Collection (2002) (with Helen Dickson and Kate Hoffmann)\nAustralian Nights (2003) (with Miranda Lee)\nOutback Weddings (2003) (with Barbara Hannay)\nAustralian Playboys (2003) (with Helen Bianchin and Marion Lennox)\nAustralian Tycoons (2004) (with Emma Darcy and Marion Lennox)"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study comparing the performance of spiking neural networks (SNNs) and their convolutional neural network (CNN) counterparts on a specific task found that SNNs consistently outperformed CNNs by an average increase of 6.6%. However, the same study also reported that SEW ResNet18, a variant of SNN, performed worse than its CNN counterpart, ResNet18, on the RSA metric. Which of the following conclusions can be drawn from this information?",
    "choices": [
      "A) SNNs are more robust to variations in model architecture and depth than CNNs.",
      "B) The performance difference between SNNs and CNNs is due to the use of IF neurons in SNNs, which provide a more biologically plausible spike mechanism.",
      "C) The study's results suggest that SNNs are only superior to CNNs when the overall architecture and depth are the same.",
      "D) The performance difference between SNNs and CNNs is likely due to the incorporation of temporal features in SNNs, which allows them to capture the temporal aspects of visual information processing."
    ],
    "correct_answer": "D)",
    "documentation": [
      "However, the difference is not significant for RSA (t = 1.117, p = 0.327). Specifically, the similarity score of SEW ResNet152 is only slightly higher than that of ResNet152, and at the depth of 50 and 101, SEW ResNet's scores are lower than ResNet's. Macaque-Synthetic dataset. Similar to the results of Allen Brain dataset, no model performs best for all three metrics. SEW ResNet performs moderately better than ResNet (t = 3.354, p = 0.028; t = 3.824, p = 0.019; t = 2.343, p = 0.079). The only contrary is that SEW ResNet18 performs worse than ResNet18 for RSA. Further, to check the details of comparison between the SNNs and their CNN counterparts, we analyze the trajectories of similarity score across model layers (Figure ). As for ResNet and SEW ResNet with the same depth, the trends of their similarities across model layers are almost the same, but the former's trajectory is generally below the latter's. In other words, the similarity scores of SEW ResNet are higher than those of ResNet at almost all layers. Taken together, the results suggest that when the overall results that appear below also correspond to the three metrics in this order, unless the correspondence is stated in the text. architectures and depth are the same, SNNs with spiking neurons perform consistently better than their counterparts of CNNs with an average increase of 6.6%. Besides, SEW ResNet14 also outperforms the brain-like recurrent CNN, CORnet-S, with the same number of layers (see more details in Appendix B). Two properties of SNNs might contribute to the higher similarity scores. On the one hand, IF neurons are the basic neurons of spiking neural networks. The IF neuron uses several differential equations to roughly approximate the membrane potential dynamics of biological neurons, which provides a more biologically plausible spike mechanism for the network. On the other hand, the spiking neural network is able to capture the temporal features by incorporating both time and binary signals, just like the biological visual system during information processing."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is studying the effects of aging on synaptic plasticity in the hippocampus. They use a rodent hippocampal slice preparation to investigate long-term potentiation (LTP) and depression (LTD) in rats and transgenic mice. However, they notice that the slices from aged rats are more difficult to extract due to thicker skulls and tougher connective tissue. Which of the following statements best describes the potential consequence of this challenge on the researcher's findings?",
    "choices": [
      "D) in rats and transgenic mice. However, they notice that the slices from aged rats are more difficult to extract due to thicker skulls and tougher connective tissue. Which of the following statements best describes the potential consequence of this challenge on the researcher's findings?",
      "A) The use of aged rats will lead to more accurate measurements of LTP and LTD due to the increased difficulty in extracting slices.",
      "B) The researcher will be able to control for age-related differences in synaptic function and plasticity more effectively by using aged rats.",
      "C) The challenge of extracting slices from aged rats will negate the ability to detect real age-related differences in synaptic function and plasticity."
    ],
    "correct_answer": "C)",
    "documentation": [
      "This synapse assay is a valuable tool that can be widely utilized in the study of synaptic development. Neuroscience, Issue 45, synapse, immunocytochemistry, brain, neuron, astrocyte2270Play ButtonPreparation of Acute Hippocampal Slices from Rats and Transgenic Mice for the Study of Synaptic Alterations during Aging and Amyloid PathologyAuthors: Diana M. Mathis, Jennifer L. Furman, Christopher M. Norris. Institutions: University of Kentucky College of Public Health, University of Kentucky College of Medicine, University of Kentucky College of Medicine. The rodent hippocampal slice preparation is perhaps the most broadly used tool for investigating mammalian synaptic function and plasticity. The hippocampus can be extracted quickly and easily from rats and mice and slices remain viable for hours in oxygenated artificial cerebrospinal fluid. Moreover, basic electrophysisologic techniques are easily applied to the investigation of synaptic function in hippocampal slices and have provided some of the best biomarkers for cognitive impairments. The hippocampal slice is especially popular for the study of synaptic plasticity mechanisms involved in learning and memory. Changes in the induction of long-term potentiation and depression (LTP and LTD) of synaptic efficacy in hippocampal slices (or lack thereof) are frequently used to describe the neurologic phenotype of cognitively-impaired animals and/or to evaluate the mechanism of action of nootropic compounds. This article outlines the procedures we use for preparing hippocampal slices from rats and transgenic mice for the study of synaptic alterations associated with brain aging and Alzheimer's disease (AD)1-3. Use of aged rats and AD model mice can present a unique set of challenges to researchers accustomed to using younger rats and/or mice in their research. Aged rats have thicker skulls and tougher connective tissue than younger rats and mice, which can delay brain extraction and/or dissection and consequently negate or exaggerate real age-differences in synaptic function and plasticity."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new medication, Xylophia-IV, has been developed to treat a rare genetic disorder. The disorder, known as \"Luminous Syndrome,\" is characterized by an inability to produce a specific enzyme, leading to a buildup of toxic compounds in the body. The medication works by inhibiting the production of a specific protein, which is essential for the production of the enzyme. However, the protein is also involved in the regulation of cell growth and division. In a clinical trial, patients who received Xylophia-IV showed significant improvement in their symptoms, but the medication also caused an increase in the incidence of certain types of cancer. The manufacturer of the medication claims that the risk of cancer is still within acceptable limits, but the FDA has expressed concerns about the long-term effects of the medication.",
    "choices": [
      "A) The manufacturer's claim that the risk of cancer is within acceptable limits is supported by the fact that the medication has been shown to be effective in treating Luminous Syndrome, and the benefits of treatment outweigh the risks.",
      "B) The increase in cancer incidence in patients who received Xylophia-IV is likely due to the fact that the medication inhibits the production of a protein that is also involved in the regulation of cell growth and division, leading to uncontrolled cell proliferation.",
      "C) The FDA's concerns about the long-term effects of Xylophia-IV are unfounded, as the medication has been shown to be effective in treating Luminous Syndrome, and the benefits of treatment outweigh the risks, even if there is a small increase in cancer incidence.",
      "D) The manufacturer's claim that the risk of cancer is within acceptable limits is supported by the fact that the medication has been shown to be effective in treating Luminous Syndrome, and the benefits of treatment outweigh the risks, but the FDA's concerns about the long-term effects of the medication are valid, and further research is needed to fully understand the risks and benefits of Xylophia-IV."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\\\\n\n\\section{Acknowledgments}\nThis work was supported by the US Air Force Office of Scientific Research (Grant No. FA9550-17-1-0343), together with the Natural Sciences and Engineering research Council of Canada (NSERC), the Canada Foundation for Innovation (CFI) and the British Columbia Knowledge Development Fund (BCKDF)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher wants to compare the neural responses of mice and macaques to natural scene stimuli. Which of the following is a crucial consideration when designing the experiment?",
    "choices": [
      "A) To ensure that the mice are presented with a diverse range of natural scene stimuli, the researcher should use a dataset that includes images from multiple sources, such as natural scenes and synthetic images.",
      "B) Since the macaque visual cortex is more sensitive to face images than natural scene images, the researcher should only use the Macaque-Face dataset to compare the neural responses of mice and macaques.",
      "C) To account for the differences in the number of neurons recorded in the mouse and macaque datasets, the researcher should use a statistical method that takes into account the variance of the neural responses.",
      "D) Given that the mouse visual cortex is more responsive to natural scene stimuli than macaque visual cortex, the researcher should use the mouse dataset to make inferences about the neural responses of macaques."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Our work is conducted with three neural datasets. These datasets are recorded from two species under three types of stimuli. More specifically, there are neural responses of mouse visual cortex to natural scene stimuli, and responses of macaque visual cortex to face image and synthetic image stimuli. Allen Brain mouse dataset. It is part of the Allen Brain Observatory Visual Coding dataset ) col-lected using Neuropixel probes from 6 regions simultaneously in mouse visual cortex. Compared to two-photon calcium imaging, Neuropixel probes simultaneously record the spikes across many cortical regions with high temporal resolution. In these experiments, mice are presented with 118 250-ms natural scene stimuli in random orders for 50 times. Hundreds to thousands of neurons are recorded for each brain region. To get the stable neurons, we first concatenate the neural responses (average number of spikes in 10-ms bins across time) under 118 images for each neuron, and then preserve the neurons whose split-half reliability across 50 trials reaches at least 0.8. Macaque-Face dataset. This dataset ) is composed of neural responses of 159 neurons in the macaque anterior medial (AM) face patch under 2,100 real face stimuli, recorded with Tungsten electrodes. For this dataset, we compute the average number of spikes in a time window of 50-350ms after stimulus onset and exclude eleven neurons with noisy responses by assessing the neurons' noise ceiling. The details of the preprocessing procedure are the same as . Macaque-Synthetic dataset. This dataset is also about macaque neural responses which are recorded by electrodes under 3,200 synthetic image stimuli, and used for neural prediction in the initial version of Brain-Score . The image stimuli are generated by adding a 2D projection of a 3D object model to a natural background. The objects consist of eight categories, each with eight subclasses. The position, pose, and size of each object are randomly selected. 88 neurons of V4 and 168 neurons of IT are recorded."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A new company, X, is considering investing in a startup that has developed a revolutionary new material with potential applications in both the consumer electronics and automotive industries. The startup's CEO claims that the material has the potential to reduce energy consumption in smartphones by up to 30% and could also be used to improve fuel efficiency in cars by up to 25%. However, the startup's financial projections indicate that it will require significant investment to scale up production and commercialize the material. Which of the following statements best summarizes the potential risks and benefits of X's investment in the startup?",
    "choices": [
      "A) The startup's material has the potential to disrupt the entire electronics industry, and X's investment will be a wise decision regardless of the financial projections.",
      "B) The startup's material is likely to be a game-changer for the automotive industry, but its impact on the consumer electronics industry will be limited due to the existing dominance of established players.",
      "C) The startup's material is unlikely to have a significant impact on either industry, and X's investment will be a high-risk, high-reward proposition that requires careful consideration of the financial projections.",
      "D) The startup's material has the potential to improve fuel efficiency in cars, but its impact on energy consumption in smartphones will be limited due to the existing dominance of established players, and X's investment should focus on scaling up production and commercializing the material for the automotive industry."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Each of those manufacturers knew that smartphones would eventually be awesome, but none of them had the UI and software design to make a truly breakout product. Imagine if Motorola would have been smart enough to buy the Android guys instead of Google. Instead, Google bought a bunch of patents on that cellular black box to try to defend it's platform. And when you think about it, which consumes more man years of engineering effort per year at this point.... iterating that cellular black box or developing the OS, services and apps that power today's smartphones?\nIntel had better decide that they are competing in this space \"for real\", or they are screwed. They've already let the Atom languish for five years, during which ARM has completely caught up in performance. Just like Tim Cook said, if you don't cannibalize your own markets someone else will do it for you. Whether Intel will embrace that concept in time remains to be seen. Personally, I hope they don't; if Intel transforms into a chipless Fab company (like TSMC) everyone benefits. I still think Samsung has the advantage long term because they have both the SOC and the memory products. As mentioned in the article, TSV's (Through Silicon Via's) are going to be quite a disruption. Today, people normally stack an LPDDR2 package on top of their SOC package (POP or Package On Package). Within the LPDDR2 package, you could have a stack of DRAM die typically with wire bonding connecting the die within the package. Once you more to TSV's, you can have a LOT more connections between the SOC and its DRAM's. While this is being standardized through JEDEC (http://www.jedec.org/category/technolog ... a/3d-ics-0), Samsung has all the pieces in house to do whatever they want. You could see a 512 bit or higher bus from the SOC to the memory. The trick is that the memory and the SOC need to line up with each other when you stack them. This gives Samsung an inherent advantage. This isn't just going to impact mobile either. Take a look at that JEDEC link."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher investigates the relationship between the age of a cryptocurrency and its power-law exponent associated with positive returns. The researcher finds that the power-law exponent is significantly correlated with age, but the effect of market capitalization on the power-law exponent is not significant. However, the researcher notes that the power-law exponent is also correlated with the cryptocurrency's market capitalization, but only for cryptocurrencies with a certain level of age. Which of the following statements best summarizes the researcher's findings?",
    "choices": [
      "A) The power-law exponent is significantly correlated with both age and market capitalization, and the effect of age on the power-law exponent is stronger than the effect of market capitalization.",
      "B) The power-law exponent is only correlated with age, and the effect of market capitalization on the power-law exponent is due to the correlation between market capitalization and age.",
      "C) The power-law exponent is significantly correlated with age, but the effect of market capitalization on the power-law exponent is only significant for cryptocurrencies that are less than 5 years old.",
      "D) The power-law exponent is not significantly correlated with either age or market capitalization, and the researcher's findings suggest that the relationship between the two variables is complex and influenced by other factors."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Moreover, we have used the implementation available on the powerlaw Python package . In addition to obtaining the power-law exponents, we have also verified the adequacy of the power-law hypothesis using the procedure originally proposed by Clauset et al. as adapted by Preis et al. . This procedure consists of generating synthetic samples under the power-law hypothesis with the same properties of the empirical data under analysis (that is, same length and parameters \u03b1 and r min ), adjusting the simulated data with the power-law model via the Clauset-Shalizi-Newman method, and calculating the Kolmogorov-Smirnov statistic (\u03ba syn ) between the distributions obtained from the simulated samples and the adjusted power-law model. Next, the values of \u03ba syn are compared to the Kolmogorov-Smirnov statistic calculated between empirical data and the power-law model (\u03ba). Finally, a p-value is defined by calculating the fraction of times for which \u03ba syn > \u03ba. We have used one thousand synthetic samples for each position of the expanding time window and the more conservative 90% confidence level (instead of the more lenient and commonly used 95% confidence level), such that the power-law hypothesis is rejected whenever p-value \u2264 0.1. We have estimated the effects of age and market capitalization on the power-law exponents associated with positive or negative returns of a given cryptocurrency using the linear model where \u03b1 t represents the power-law exponent, log c t is the logarithm of the market capitalization, and y t is the age (in years) of the cryptocurrency at t-th observation. Moreover, K is the intercept of the association, while C and A are linear coefficients quantifying the effects of market capitalization and age, respectively. Finally, N (\u00b5, \u03c3 ) stands for the normal distribution with mean \u00b5 and standard deviation \u03c3 , such that the parameter \u03b5 accounts for the unobserved determinants in the dynamics of the power-law exponents. We have framed this problem using the hierarchical Bayesian approach such that each power-law exponent \u03b1 t is nested within a cryptocurrency with model parameters considered as random variables normally distributed with parameters that are also random variables."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A 15-year-old student, who has been diagnosed with high-functioning autism, is struggling to manage their daily life. They have a girlfriend who is unaware of their diagnosis, and their relationship is experiencing difficulties due to their struggles with social interactions and executive functioning. The student's parents are considering involving their girlfriend in their son's treatment plan to help improve their relationship and overall well-being. What are the potential benefits and drawbacks of involving the girlfriend in the treatment plan?",
    "choices": [
      "A) Involving the girlfriend could help the student feel more supported and understood, but it may also create unnecessary stress and pressure on the relationship.",
      "B) Involving the girlfriend could provide the student with additional emotional support and help them develop better communication skills, but it may also lead to feelings of resentment and frustration from the girlfriend.",
      "C) Involving the girlfriend could help the student's girlfriend understand and accommodate their needs, but it may also create unrealistic expectations and put additional pressure on the girlfriend to \"fix\" the student's problems.",
      "D) Involving the girlfriend could provide the student with a sense of autonomy and control over their treatment plan, but it may also lead to feelings of guilt and obligation from the girlfriend, potentially straining their relationship."
    ],
    "correct_answer": "D)",
    "documentation": [
      "Thank you for your assistance. I just listed to your tapes on dealing with an out of control, defiant teen. I'd like to ask your advice on a particular situation we have. Our 15 year old daughter is smoking pot almost every day at school. Because we had no way to control the situation, we told her, fine, go ahead and smoke weed. However, you will no longer receive the same support from us. You will not have your phone, lunch money to go off campus (she has an account at the school for the cafeteria she can use), and you will be grounded until you can pass a drug test. We will not be testing you except for when you tell us you are ready to be tested. She is now saying she's suicidal because she feels so isolated, yet she continues to smoke weed. In fact, she tried to sneak out last night but was foiled by our alarm system. For the particular drug test we have, I read it takes about 10 days of not smoking to pass the test. What would you do? Please advise. I am having a problem with my 18 year old son, Danny, with high functioning autism. We finally had him diagnosed when he was 16 years old. I always knew something was going on with him but the doctors misdiagnosed him as bipolar. It's been 2 years now and he will not accept his diagnosis. He won't talk about it and when I try to bring it up he gets very angry. I've tried telling him that it's not a bad thing, that there's been many, many very successful people with Aspergers. He won't tell anyone and refuses to learn about managing life with it. He once shared with me that the other kids at school use it as an insult, like saying someone is so autistic when they do something they don't approve of. So he doesn't want anyone to know. He's turned down services that could help him. He has a girlfriend, going on 8 months. He won't tell her and they're having problems arguing a lot and I wonder if it would help for her to know. I'm sad that he thinks it's a life sentence to something horrible instead of accepting, embracing it and learning about it more so he maybe can understand why he's struggling."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary advantage of using ion-induced defects to confine electrons and holes in semiconducting SWNTs, as demonstrated in the experimental setup?",
    "choices": [
      "A) The use of ion-induced defects allows for the creation of QDs with level spacings of the order of 100 meV and larger, which can be resolved using low-temperature STM/STS.",
      "B) The ion-induced defects provide a means to control the scattering strength of the defects, enabling the creation of QDs with specific energy levels.",
      "C) The Au(111) substrate plays a crucial role in the formation of QDs, and the use of ion-induced defects allows for the creation of QDs with well-defined energy levels.",
      "D) The combination of ion-induced defects with recent progress in controlling defect structure and localization offers a high potential for engineering a broad set of SWNT-based quantum devices operating at room temperature, as demonstrated in the experimental setup."
    ],
    "correct_answer": "D)",
    "documentation": [
      "\\\\\n\\indent Another technique for achieving confinement in SWNTs makes use of artificial defects such as covalently bound oxygen or aryl functionalization groups on the side walls of semiconducting SWNTs, inducing deep exciton trap states allowing for single-photon emission at room temperature~\\cite{Htoon_2015,tunable_QD_defects}. Also, carrier confinement between defect pairs acting as strong scattering centers has been reported for mechanically induced defects~\\cite{Postma_SET} as well as for ion-induced defects with reported level spacings up to 200 meV in metallic SWNTs~\\cite{Buchs_PRL}. The latter technique, combined with recent progress in controlling defects structure and localization~\\cite{Robertson_2012,Yoon_2016,Laser_writing_2017} offers a high potential for engineering a broad set of SWNT-based quantum devices operating at room temperature. \\\\\n\\indent Here, we demonstrate confinement of electrons and holes in sub-10 nm QD structures defined by ion-induced defect pairs along the axis of semiconducting SWNTs. Using low temperature scanning tunneling microscopy and spectroscopy (STM/STS), bound states with level spacings of the order of 100 meV and larger are resolved in energy and space. By solving the one-dimensional Schr\\\"odinger equation over a piecewise constant potential model, the effects of asymmetric defect scattering strength as well as the influence of the Au(111) substrate such as terrace edges on the bound states structure are remarkably well reproduced. By means of ab-initio calculations based on density functional theory and Green's functions, we find that single (SV) and double vacancies (DV) as well as chemisorbed nitrogen ad-atoms are good candidates to produce QDs with the experimentally observed features. These simulations also allow to study the scattering profile as a function of energy for different defect combinations. \\section{Experimental section}\n\nThe experiments have been performed in a commercial (Omicron) low temperature STM setup operating at $\\sim5$~K in ultra high vacuum."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic hand is designed to grasp and release objects of varying weights and materials. The controller uses force feedback integrated with conditional synergies to control the hand's movements. In the experiment, the robot is able to lift objects of different weights and materials while avoiding slip. However, when the weight of the object changes, the tangential force decreases, causing the normal force threshold to decrease as well. What is the primary reason for the decrease in normal force threshold when the weight of the object changes?",
    "choices": [
      "A) The robot's force controller is unable to generate sufficient force to maintain contact with the object.",
      "B) The object's weight causes the robot's hand to slip, reducing the normal force.",
      "C) The robot's controller adjusts the grasp size to compensate for the decrease in normal force threshold.",
      "D) The robot's force feedback system is unable to accurately detect changes in the object's weight."
    ],
    "correct_answer": "D)",
    "documentation": [
      "You can see the execution of the third experiment in the middle part of Figure . This experiment demonstrates the ability of the controller to perform robot to human handovers. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold, 2) the robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the vertical position, and 4) the robot enters the RELEASE phase, the arm stays still, the human grasps the object from the bottom and slightly pushes it up, the hand then detects that there is a supporting surface and starts to slowly release the object. You can see the execution of the fourth experiment in the bottom part of Figure . This experiment is similar to previous one, but the grasp type that the robot uses is a pinch grasp, that involves only the thumb and the index finger. To perform this we only had to alter the grasp type conditional variable that was given to the posture mapping function. You can see the execution of the fifth experiment in the bottom part of Figure . In the first part (blue) of the experiment the robot closes its grasp, by reducing the grasp size, until the normal force is below the force offset. In the next three parts (pink, green, red) the person throws coins in the cup to increase its weight. You can see in the signal plots that each time coins are added the tangential force decreases so the normal force threshold decreases too. The grasp sizes then decreases as well in order to apply more normal force. This experiment demonstrates the ability of the controller to handle perturbations in the weight of the object during grasping. CONCLUSION In summary, we presented a controller that uses force feedback integrated with conditional synergies to control a dexterous robotic hand to grasp and release objects. We demonstrated that our controller can lift objects of different weights and materials while avoiding slip, react online when the weight of the object changes, place them down on surfaces, and hand them over to humans."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher observes that the evolved learning rate of the moving agent decreases as the distance between the agent's initial weights and the optimal weights increases, but the effect of the environmental transition probability on the learning rate is less pronounced in the moving agents compared to the static agents. Which of the following statements best explains this observation?",
    "choices": [
      "A) The moving agents require more time to adapt to changes in the environment, resulting in a slower learning rate.",
      "B) The moving agents are more sensitive to the initial weights, leading to a greater dependence on the environment's ingredient values.",
      "C) The moving agents have a more limited lifetime, resulting in less exposure to the environment and a slower learning rate.",
      "D) The moving agents are able to adapt to changes in the environment more effectively, but the effect of the transition probability on the learning rate is masked by the agent's ability to learn from its experiences."
    ],
    "correct_answer": "D)",
    "documentation": [
      "The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights (Fig. ). After \u223c 100 evolutionary steps (Fig. ), the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr . At first, in order to simplify the experiment, we set the transition probability to 0, but fixed the initial weights to be the average of E 1 and E 2 , while the real state is E 2 . In this experiment, the distance between states d e indicates twice the distance between the agent's initial weights and the optimal weights (the environment's ingredient values) since the agent is initialized at the mean of the two environment distributions. Same as for the static agent, the learning rate increases with the distance d e (Fig. ). Then, we examine the effect of the environmental transition probability p tr on the evolved learning rate \u03b7 p . In order for an agent to get sufficient exposure to each environment, we scale down the probability p tr from the equivalent experiment for the static agents. We find that as the probability of transition increases, the evolved learning rate \u03b7 p decreases (Fig. ). This fits with the larger trend for the static agent, although there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agents. This could be due to much sparser data and possibly the insufficiently long lifetime of the moving agent (the necessity of scaling makes direct comparisons difficult). Nevertheless, overall we see that the associations observed in the static agents between environmental distance d e and transition probability p tr and the evolved learning rate \u03b7 p are largely maintained in the moving agents."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient with a history of hypertension and hyperlipidemia presents with chest pain and shortness of breath. The patient's EKG shows ST-segment elevation in leads V2-V4, and the patient has a history of a previous myocardial infarction. The patient's doctor orders a stress test to evaluate the patient's cardiac function. What is the most likely underlying cause of the patient's symptoms?",
    "choices": [
      "A) The patient's symptoms are likely due to a re-infarction of the previous MI, which is a common complication of hypertension and hyperlipidemia.",
      "B) The patient's symptoms are likely due to a pulmonary embolism, which is a common complication of deep vein thrombosis, which is a common complication of hypertension and hyperlipidemia.",
      "C) The patient's symptoms are likely due to a cardiac arrhythmia, which is a common complication of hypertension and hyperlipidemia, and can be exacerbated by the patient's previous MI.",
      "D) The patient's symptoms are likely due to a coronary artery spasm, which is a common cause of chest pain and shortness of breath in patients with hypertension and hyperlipidemia, and can be exacerbated by the patient's previous MI and the presence of ST-segment elevation on the EKG."
    ],
    "correct_answer": "D)",
    "documentation": [
      "There are many reasons for inverted T-waves, ranging from cardiac issues to completely benign conditions. One way of looking at this is choosing to believe that seeing a cardiologist will ease your mind one way or the other \u2013 so this is something to look forward to, not dread. If the cardiologist spots something suspicious, a treatment plan will be created. If not, you can wave goodbye and go back to happily living your life. Try thinking of this cardiology appointment just as you would if your car were making some frightening noises and you were bringing it to your mechanic for a check up. You could work yourself into a complete state worrying ahead of time if the car trouble is going to be serious, or you could look at this appointment as the solution \u2013 at last! \u2013 to figuring out what\u2019s wrong so the mechanic can recommend the next step. Thank you for this list of so many definitions provided in plain English. what a valuable resource this is. THANK YOU, I have been looking for translations FOR PATIENTS not med school graduates\u2013 like this for three years. My family doctor had me wear a 24 hr EKG. After reading the results, she has scheduled a scope to look inside my heart by a specialist. Completely forgoing a stress test. Said I have major changes in the EKG, what type of changes could they be looking at? Had LAD STENT INSERTED 7 YRS AGO \u2013 WHAT COULD THEY BE LOOKING FOR? This is a great wealth of information, Carolyn! I looked and did not see my diagnosis, which is aortic stenosis. I looked under aortic as well as stenosis. Did I just miss it somehow? I learned some new information, I am a bit familiar now, but not when I had my MI, it was like learning a new language. But, my favorite part was seeing SCAD on this list! Thank you.\nThanks and welcome! I was thinking of editing that SCAD definition actually: I suspect that that it isn\u2019t so much that SCAD is \u201crare\u201d, but it\u2019s more that it\u2019s \u201crarely correctly diagnosed\u201d. I totally agree that SCAD is not as rare as I believed for many years. Once awareness is spread to all medical staff, I believe many lives will be saved."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on shark fishing practices in Tanjung Luar, Indonesia, found that the majority of vessels targeting sharks are less than 10 gross tonnes (GT) in size. However, the researchers also noted that the Tanjung Luar port, where shark catch is landed, has a dedicated auction facility that handles a large volume of shark fins. Which of the following statements best describes the implications of this finding?",
    "choices": [
      "A) The study suggests that the majority of shark catch in Tanjung Luar is for human consumption, as the auction facility is primarily used for fin sales.",
      "B) The study implies that the large volume of shark fins handled at the auction facility is a result of the targeted shark fishing practices, which are likely to be unsustainable in the long term.",
      "C) The study's finding that most vessels targeting sharks are less than 10 GT in size suggests that the shark fishing industry in Tanjung Luar is primarily focused on small-scale, artisanal fishing practices.",
      "D) The presence of a dedicated auction facility at the Tanjung Luar port indicates that the shark fishing industry in the area is likely to be driven by a combination of economic and environmental factors, including the demand for shark fins and the need to manage shark populations sustainably."
    ],
    "correct_answer": "D)",
    "documentation": [
      "This work was conducted under a Memorandum of Understanding (MoU) and Technical Cooperation Agreement (TCA) between the Wildlife Conservation Society (WCS) and the Ministry of Environment and Forestry (MoEF), Ministry Marine Affairs and Fisheries (MMAF) and the Marine and Fisheries Agency (MFA) of West Nusa Tenggara Province. These documents were approved and signed by Sonny Partono (Director General of Conservation of Natural Resources and Ecosystem MoEF), Sjarief Widjaja (Secretary General MMAF), and Djoko Suprianto (Acting Head of MFA of West Nusa Tenggara Province). Due to this MoU and TCA no specific research permit was required. We collected data by measuring sharks that were already caught, dead, and landed by fishers in Tanjung Luar, with no incentives, compensation or specific requests for killing sharks for this study. WCS participates in the Conservation Initiative on Human Rights and the rules and guidelines of our Internal Review Board ensures that any research protects the rights of human subjects. We did not apply for an IRB permit for this study because our study design focused on collecting fish and fisheries data as opposed to personal socio-economic data. The FDGs and interviews were conducted to obtain early scoping information about fishing practices, and to establish protocols for more detailed fisheries data collection (as used in this study), and socio-economic data collection (as used in a later study (Lestari et al ), which underwent further ethical review due to the specific focus on human subjects). Tanjung Luar, located in East Lombok, West Nusa Tenggara Province, Indonesia (Fig 1), is a landing site for one of Indonesia\u2019s most well-known targeted shark fisheries. Tanjung Luar serves at least 1,000 vessels, and the majority of these are less than 10 gross tonnes (GT) in size . A group of specialised fishers operating from Tanjung Luar village and a neighbouring island, Gili Maringkik, specifically target sharks. Shark catch is landed in a dedicated auction facility at the Tanjung Luar port."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A researcher is analyzing the protein dynamics data of the yeast aquaporin (Aqy1) to identify the collective motions of the atoms responsible for the channel opening. The data consists of Euclidean coordinates of all 783 atoms of Aqy1 observed in a 100 nanosecond time frame, split into 20 000 equidistant observations. The diameter of the channel y_t at time t is given, measured by the distance between two centers of mass of certain residues of the protein. Which of the following statements about the analysis is most likely true?",
    "choices": [
      "A) The researcher assumes that the observations are independent over time, which is a reasonable assumption given the large number of time points.",
      "B) The researcher uses a linear model Y = X\u03b2 to model the response variable y_t, which is a reasonable choice given the complexity of the data.",
      "C) The researcher only considers the distance between the first atom and the first center of mass as a relevant feature, ignoring the distances between other atoms and base points.",
      "D) The researcher uses a Gaussian distribution to model the data, which is a reasonable assumption given the similarity of the results to those obtained from gamma and uniform distributions."
    ],
    "correct_answer": "D)",
    "documentation": [
      "(1) polynomial \u03c3 ( (  To test how robust our approach is to deviations from the Gaussian assumption, we simulated the data from gamma and uniform distributions and conducted a simulation study for the same scenarios and examples. The results are very similar to those of the Gaussian distribution, see supplementary materials for the details. Application to Protein Dynamics\n\nWe revisit the data analysis of protein dynamics performed in Krivobokova et al. (2012) and . We consider data generated by the molecular dynamics (MD) simulations for the yeast aquaporin (Aqy1) -the gated water channel of the yeast Pichi pastoris. MD simulations are an established tool for studying biological systems at the atomic level on timescales of nano-to microseconds. The data are given as Euclidean coordinates of all 783 atoms of Aqy1 observed in a 100 nanosecond time frame, split into 20 000 equidistant observations. Additionally, the diameter of the channel y t at time t is given, measured by the distance between two centers of mass of certain residues of the protein. The aim of the analysis is to identify the collective motions of the atoms responsible for the channel opening. In order to model the response variable y t , which is a distance, based on the motions of the protein atoms, we chose to represent the protein structure by distances between atoms and certain fixed base points instead of Euclidean coordinates. That is, we calculated where A t,i \u2208 R 3 , i = 1, . . . , 783 denotes the i-th atom of the protein at time t, B j \u2208 R 3 , j = 1, 2, 3, 4, is the j-th base point and d(\u2022, \u2022) is the Euclidean distance. Figure shows the diameter y t and the distance between the first atom and the first center of mass. It can therefore be concluded that a linear model Y = X\u03b2 + holds, where\n. This linear model has two specific features which are intrinsic to the problem: first, the observations are not independent over time and second, X t is high-dimensional at each t and only few columns of X are relevant for Y ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a system with $N$ spins, where the Hamiltonian is given by $\\mathcal{H}(\\bm{\\phi}) = \\sum_{i,j} J_{ij} \\sigma_i^x \\sigma_j^x + \\sum_{i,j} J_{ij} \\sigma_i^y \\sigma_j^y + \\sum_{i} \\Delta \\sigma_i^z$. The couplings $J_{ij}$ are unknown, and we want to infer them from the data. We generate the input data by Monte-Carlo simulations of the model, and we have the following information:\n\n* The Gibbs-Boltzmann distribution of the $N$ variables $\\bm{\\phi}$ is given by $P(\\bm{\\phi}) = \\frac{1}{Z} e^{-\\beta \\mathcal{H}(\\bm{\\phi})}$, where $\\beta = \\left( 2\\Delta^2 \\right)^{-1}$.\n* The rescaled couplings are $\\beta J_{ij}/2 \\rightarrow J_{ij}$.\n* We can compute the conditional probability distribution of one variable $\\phi_i$ given all other variables, $\\bm{\\phi}_{\\backslash i}$, using the formula $P(\\phi_i | \\bm{\\phi}_{\\backslash i}) = \\frac{1}{Z_i} \\exp \\left \\{ {H_i^x (\\bm{\\phi}_{\\backslash i})\n\t\\cos \\phi_i + H_i^y (\\bm{\\phi}_{\\backslash i}) \\sin \\phi_i } \\right \\}$.\n\nWhat is the correct inference of the couplings $J_{ij}$ from the data?",
    "choices": [
      "A) The couplings $J_{ij}$ are equal to the rescaled couplings $\\beta J_{ij}/2$, and can be inferred directly from the data.",
      "B) The couplings $J_{ij}$ are equal to the rescaled couplings $\\beta J_{ij}/2$, but only if the data is generated using a specific type of Monte-Carlo simulation.",
      "C) The couplings $J_{ij}$ are equal to the rescaled couplings $\\beta J_{ij}/2$, but only if the data is generated using a specific type of Hamiltonian.",
      "D) The couplings $J_{ij}$ cannot be inferred directly from the data, and require a more complex analysis that takes into account the Gibbs-Boltzmann distribution and the rescaled couplings."
    ],
    "correct_answer": "D)",
    "documentation": [
      "I think that to obtain the XY model, it is not necessary that the intensities are strictly quenched (that is also a quite unfeasible situation, I guess). Indeed eq (2) does not deal with the dynamics of the modes, but just connect the in and out ones. For this, what it is necessary to have the XY model, it is that the intensities are always the same on the different samples\n(so that the matrix $t_{ij}$ is the same for different phase data). If the intensities are fixed, then they can be incorporated in $t_{ij}$ and eq (2) can be written just for phases as described. \\\\\n}\n\\end{comment}\n\n\n  \\section{Pseudolikelihood Maximization}\n  \\label{sec:plm}\nThe inverse problem consists in the reconstruction of the parameters $J_{nm}$ of the Hamiltonian, Eq. (\\ref{eq:h_im}). Given a set of $M$ data configurations of $N$ spins\n $\\bm\\sigma = \\{ \\cos \\phi_i^{(\\mu)},\\sin \\phi_i^{(\\mu)} \\}$, $i = 1,\\dots,N$ and $\\mu=1,\\dots,M$, we want to \\emph{infer} the couplings:\n \\begin{eqnarray}\n\\bm \\sigma  \\rightarrow  \\mathbb{J} \n\\nonumber\n \\end{eqnarray}\n With this purpose in mind,\n in the rest of this section we implement the working equations for the techniques used. In order to test our methods, we generate the input data, i.e., the configurations, by Monte-Carlo simulations of the model. The joint probability distribution of the $N$ variables $\\bm{\\phi}\\equiv\\{\\phi_1,\\dots,\\phi_N\\}$, follows the Gibbs-Boltzmann distribution:\n \\begin{equation}\\label{eq:p_xy}\n P(\\bm{\\phi}) = \\frac{1}{Z} e^{-\\beta \\mathcal{H\\left(\\bm{\\phi}\\right)}} \\quad \\mbox{ where } \\quad Z = \\int \\prod_{k=1}^N d\\phi_k  e^{-\\beta \\mathcal{H\\left(\\bm{\\phi}\\right)}}  \n \\end{equation}\n and where we denote $\\beta=\\left( 2\\Delta^2 \\right)^{-1}$ with respect to Eq. (\\ref{def:Z}) formalism. In order to stick to usual statistical inference notation, in the following we will rescale the couplings by a factor $\\beta / 2$: $\\beta J_{ij}/2 \\rightarrow J_{ij}$. \n The main idea of the PLM is to work with the conditional probability distribution of one variable $\\phi_i$ given all other variables, \n $\\bm{\\phi}_{\\backslash i}$:\n \n  \\begin{eqnarray}\n\t\\nonumber\n   P(\\phi_i | \\bm{\\phi}_{\\backslash i}) &=& \\frac{1}{Z_i} \\exp \\left \\{ {H_i^x (\\bm{\\phi}_{\\backslash i})\n  \t\\cos \\phi_i + H_i^y (\\bm{\\phi}_{\\backslash i}) \\sin \\phi_i }"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  }
]