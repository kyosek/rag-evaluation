[
  {
    "question": "What can be inferred about Saif Al-Islam's role in Libya's human rights situation, based on the statements of the Libyan diplomat and the international reporting on mass murder and killings?",
    "choices": [
      "A) Saif Al-Islam's involvement in human rights violations is a well-documented fact, and he should be held accountable.",
      "B) The Libyan diplomat's experience with Saif Al-Islam suggests that he is a knowledgeable person who understands the need to interact with the West, but it does not necessarily imply that he is involved in human rights violations.",
      "C) The international reporting on mass murder and killings in Libya suggests that Saif Al-Islam's actions have been brutal and warrant an international tribunal to investigate human rights violations.",
      "D) The Libyan diplomat's statement that he has no firsthand evidence of Saif Al-Islam's involvement in human rights violations, combined with the international reporting, suggests that Saif Al-Islam's actions are a complex issue that requires further investigation."
    ],
    "correct_answer": "D",
    "documentation": [
      "And despite what Sarkozy said about resolving the issues of the Bulgarian nurses when they were sentenced to death twice, it was Saif who played a very critical role against some very powerful forces in this country that wanted to kill those people. You know, I don't know of any incidences where I, first hand, have seen evidence of him committing human rights violations, and if he did, he has to be held accountable like everyone else. And I have said that publicly and I will say that privately. So my judgment is just based upon my experience with him, the fact that he is a knowledgeable person, he understands the need to interact and interface with the West. I think he could be a viable candidate. But ultimately, my opinion is hopefully going to be the opinion of the Libyan people. BLITZER: Because you probably have seen all of the articles, the reports over the past month, month and a half, of mass murder, of killings, not only by Saif Al-Islam, but some of his brothers that have gone on, the atrocities that have been so widely reported. I hear what you're saying about his role over the recent years when the Bush administration, and later the Obama administration, was trying to improve relations with Libya, but over the past several weeks, based on all of the international reporting we have seen, it's been a brutal record that he has accomplished. WELDON: Well, again, I don't have firsthand evidence of that. I just got here two days ago. And I fully support an international tribunal to look at human rights violations on everyone in this country. That's necessary. And if they find evidence that he has been involved in that, then he should suffer the consequences of his actions. (END VIDEOTAPE) BLITZER: In our next hour, part two of the interview with former congressman Curt Weldon. There have been some questions raised about his motive. Is he in all of this for the money? You're going to find out his answer to that and more. Stand by. Also, Washington, D.C.'s congressional delegate is telling colleagues -- and I'm quoting her now -- \"Go to hell.\""
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary consequence of the non-local pairing induced by superconducting proximity effect on the Kondo screening in QDs?",
    "choices": [
      "A) The Kondo effect is significantly enhanced in the sub-gap transport regime.",
      "B) The Kondo effect is suppressed in the sub-gap transport regime due to the presence of SC pairing.",
      "C) The Kondo effect is screened due to the non-local pairing, which can occur with relatively small values of coupling to SC.",
      "D) The Kondo effect is not affected by the non-local pairing induced by superconducting proximity effect."
    ],
    "correct_answer": "C",
    "documentation": [
      "Such processes give rise to an exchange mechanism \\cite{Yao},\nthat we henceforth refer to as \\emph{the CAR exchange}, which can greatly modify\nthe low-temperature transport behavior of correlated hybrid nanostructures. The CAR exchange may be seen as RKKY-like interaction between\ntwo nearby impurities on SC surface \\cite{Yao}. The effect can be understood as a consequence\nof spin-dependent hybridization of the Yu-Shiba-Rusinov (YSR)\nstates \\cite{Yu,Shiba,Rusinov} in SC contact,\ncaused both by the overlap of their wave functions\nand their coupling to Cooper-pair condensate. This process is the most effective when the YSR states \nare close to the middle of the SC gap, {\\it e.g.} in the YSR-screened phase \\cite{YSRscreening}. The mechanism presented here is essentially the same,\nyet in the considered regime can be understood\nperturbatively without referring to YSR states,\nas a consequence of the non-local pairing induced by SC electrode. In particular, the presence of YSR bound states close to the Fermi level \nis not necessary for significant consequences for the Kondo physics, \nas long as some inter-dot pairing is present. The proximity of SC induces pairing in QDs \\cite{RozhkovArovas,Buitelaar} \nand tends to suppress the Kondo effect if the superconducting energy gap $2\\Delta$ \nbecomes larger than the relevant Kondo temperature $T_K$ \n\\cite{Buitelaar2002Dec,adatomsSC,Kondo_vs_SC1,Kondo_vs_SC2,Zitko_Kondo-Andreev,Zitko_S-QD-N,IW_Sau,YSRscreening}. Moreover, the strength of SC pairing can greatly affect the Kondo physics in the sub-gap transport regime: For QDs attached to SC and normal contacts, it can enhance the Kondo effect\n\\cite{DomanskiIW,KWIW,part1}, while\nfor DQD-based Cooper pair splitters, it tends to suppress both the $\\mathrm{SU}(2)$ and $\\mathrm{SU}(4)$ Kondo effects \\cite{IW_Kacper}. Our main result is that the non-local pairing induced by superconducting \nproximity effect, which gives rise to CAR exchange, can be the sole cause of the Kondo screening. Moreover, relatively small values of coupling to SC, $\\GS{}\\ll U$, are sufficient for the effect to occur."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The partition function $Z$ is defined as the integral over all possible electromagnetic field configurations, constrained by the total power received by the system and the total amount of energy transmitted through the medium. The cost function ${\\cal H}$ is a real-valued function that depends on the effective input-input coupling matrix $U_{kl}$ and the interaction matrix $\\mathbb{J}$. \n\nWhich of the following statements is true about the partition function $Z$?",
    "choices": [
      "A) $Z$ is a Gaussian integral, and the cost function ${\\cal H}$ is a quadratic form.",
      "B) $Z$ is not a Gaussian integral, and the cost function ${\\cal H}$ is a non-quadratic form.",
      "C) $Z$ is a Gaussian integral, and the cost function ${\\cal H}$ is a quadratic form, but the effective input-input coupling matrix $U_{kl}$ is not symmetric.",
      "D) $Z$ is not a Gaussian integral, and the cost function ${\\cal H}$ is a quadratic form, but the effective input-input coupling matrix $U_{kl}$ is symmetric."
    ],
    "correct_answer": "B",
    "documentation": [
      "(\\ref{eq:deltas}). Moreover, we move to consider the ensemble of all possible solutions of Eq. (\\ref{eq:transm}) at given $\\mathbb{T}$, looking at  all configurations of input fields. We, thus, define the function:\n \n   \\begin{eqnarray}\n  Z &\\equiv &\\int_{{\\cal S}_{\\rm in}} \\prod_{j=1}^{N_I}  dE^{\\rm in}_j \\int_{{\\cal S}_{\\rm out}}\\prod_{k=1}^{N_O} dE^{\\rm out}_k \n  \\label{def:Z}\n\\\\\n    \\times\n  &&\\prod_{k=1}^{N_O}\n   \\frac{1}{\\sqrt{2\\pi \\Delta^2}}  \\exp\\left\\{-\\frac{1}{2 \\Delta^2}\\left|\n  E^{\\rm out}_k -\\sum_{j=1}^{N_I}  t_{kj} E^{\\rm in}_j\\right|^2\n\\right\\} \n\\nonumber\n \\end{eqnarray} We stress that the integral of Eq. \\eqref{def:Z} is not exactly a Gaussian integral. Indeed, starting from Eq. \\eqref{eq:deltas}, two constraints on the electromagnetic field intensities must be taken into account. The space of solutions is  delimited by the total power ${\\cal P}$ received by system, i.e., \n  ${\\cal S}_{\\rm in}: \\{E^{\\rm in} |\\sum_k I^{\\rm in}_k = \\mathcal{P}\\}$, also implying  a constraint on the total amount of energy that is transmitted through the medium, i. e., \n  ${\\cal S}_{\\rm out}:\\{E^{\\rm out} |\\sum_k I^{\\rm out}_k=c\\mathcal{P}\\}$, where the attenuation factor  $c<1$ accounts for total losses. As we will see more in details in the following, being interested in inferring the transmission matrix through the PLM, we can omit to explicitly include these terms in Eq. \\eqref{eq:H_J} since they do not depend on $\\mathbb{T}$ not adding any information on the gradients with respect to the elements of $\\mathbb{T}$.\n  \n Taking the same number of incoming and outcoming channels, $N_I=N_O=N/2$, and  ordering the input fields in the first $N/2$ mode indices and the output fields in the last $N/2$ indices, we can drop the ``in'' and ``out'' superscripts and formally write $Z$  as a partition function\n    \\begin{eqnarray}\n        \\label{eq:z}\n && Z =\\int_{\\mathcal S} \\prod_{j=1}^{N} dE_j \\left(   \\frac{1}{\\sqrt{2\\pi \\Delta^2}} \\right)^{N/2} \n \\hspace*{-.4cm} \\exp\\left\\{\n  -\\frac{ {\\cal H} [\\{E\\};\\mathbb{T}] }{2\\Delta^2}\n  \\right\\}\n  \\\\\n&&{\\cal H} [\\{E\\};\\mathbb{T}] =\n-  \\sum_{k=1}^{N/2}\\sum_{j=N/2+1}^{N} \\left[E^*_j t_{jk} E_k + E_j t^*_{kj} E_k^* \n\\right]\n \\nonumber\n\\\\\n&&\\qquad\\qquad \\qquad + \\sum_{j=N/2+1}^{N} |E_j|^2+ \\sum_{k,l}^{1,N/2}E_k\nU_{kl} E_l^*\n \\nonumber\n \\\\\n \\label{eq:H_J}\n &&\\hspace*{1.88cm } = - \\sum_{nm}^{1,N} E_n J_{nm} E_m^*\n \\end{eqnarray}\n where ${\\cal H}$ is a real-valued function by construction, we have introduced the effective input-input coupling matrix\n\\begin{equation}\nU_{kl} \\equiv \\sum_{j=N/2+1}^{N}t^*_{lj} t_{jk} \n \\label{def:U}\n \\end{equation}\n and the whole interaction matrix reads (here $\\mathbb{T} \\equiv \\{ t_{jk} \\}$)\n \\begin{equation}\n \\label{def:J}\n \\mathbb J\\equiv \\left(\\begin{array}{ccc|ccc}\n \\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}&-\\mathbb{U} \\phantom{()}&\\phantom{()}&\\phantom{()}&{\\mathbb{T}}&\\phantom{()}\\\\\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\hline\n\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}&\\phantom{()}\\\\\n \\phantom{()}& \\mathbb   T^\\dagger&\\phantom{()}&\\phantom{()}& - \\mathbb{I} &\\phantom{()}\\\\\n\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}&\\phantom{a}\\\\\n \\end{array}\\right)\n \\end{equation}\n \n Determining the electromagnetic complex amplitude configurations that minimize the {\\em cost function} ${\\cal H}$, Eq.  (\\ref{eq:H_J}),  means to maximize the overall distribution peaked around the solutions of the transmission Eqs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A vehicle is equipped with a V2I link and is communicating with an RSU. The vehicle's position is given by $\\mathrm{p}_{n,t}=[{x}_{n,t},{y}_{n,t}]$, and the channel power gain from the vehicle to the RSU is given by $\\textrm{g}_{t,k}^{nR} = \\alpha_{t,k}^{nR} \\mathrm{h}_{t,k}^{nR}$. The vehicle's signal is transmitted at each time instant $t$ and is received by the RSU. The RSU aims to detect both the spoofer on the satellite link and the jammer on multiple V2I links. Which of the following statements is true about the joint GPS spoofing and jamming detection problem?",
    "choices": [
      "A) The RSU can detect the jammer by analyzing the channel power gain from the vehicle to the RSU, but cannot detect the spoofer.",
      "B) The RSU can detect the spoofer by analyzing the channel power gain from the vehicle to the RSU, but cannot detect the jammer.",
      "C) The RSU can detect both the spoofer and the jammer by analyzing the channel power gain from the vehicle to the RSU and the received signal at the RSU.",
      "D) The RSU cannot detect either the spoofer or the jammer, as the channel power gain and received signal are not sufficient to distinguish between the two."
    ],
    "correct_answer": "C",
    "documentation": [
      "The time-varying positions of the $n$-th vehicle is given by $\\mathrm{p}_{n,t}=[{x}_{n,t},{y}_{n,t}]$ where $n \\in N$. Among the $K$ orthogonal subchannels available for the Vehicle-to-Infrastructure (V2I) communications, RSU assigns one V2I link to each vehicle. Each vehicle exchanges messages composed of the vehicle's state (i.e., position and velocity) with RSU through the $k$-th V2I link by transmitting a signal $\\textrm{x}_{t,k}$ carrying those messages at each time instant $t$ where $k \\in K$. We consider a reactive RSJ that aims to attack the V2I link by injecting intentional interference to the communication link between vehicles and RSU to alter the transmitted signals by the vehicles. In contrast, the RSS purposes to mislead the vehicles by spoofing the GPS signal and so registering wrong GPS positions. RSU aims to detect both the spoofer on the satellite link and the jammer on multiple V2I links in order to take effective actions and protect the vehicular network. The joint GPS spoofing and jamming detection problem can be formulated as the following ternary hypothesis test:\n\\begin{equation}\n    \\begin{cases}\n        \\mathcal{H}_{0}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k} + \\mathrm{v}_{t,k}, \\\\\n        \\mathcal{H}_{1}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k} + \\mathrm{g}_{t,k}^{JR} \\mathrm{x}_{t,k}^{j} + \\mathrm{v}_{t,k}, \\\\\n        \\mathcal{H}_{2}: \\mathrm{z}_{t,k} = \\mathrm{g}_{t,k}^{nR} \\mathrm{x}_{t,k}^{*} + \\mathrm{v}_{t,k},\n    \\end{cases}\n\\end{equation}\nwhere $\\mathcal{H}_{0}$, $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ denote three hypotheses corresponding to the absence of both jammer and spoofer, the presence of the jammer, and the presence of the spoofer, respectively. $\\textrm{z}_{t,k}$ is the received signal at the RSU at $t$ over the $k$-th V2I link, $\\textrm{g}_{t,k}^{nR}$ is the channel power gain from vehicle $n$ to the RSU formulated as: $\\textrm{g}_{t,k}^{nR} = \\alpha_{t,k}^{nR} \\mathrm{h}_{t,k}^{nR}$, where $\\alpha_{t,k}^{nR}$ is the large-scale fading including path-loss and shadowing modeled as \\cite{8723178}: $\\alpha_{t,k}^{nR}=G\\beta d_{t,nR}^{-\\gamma}$.\n\\begin{figure}[t!]"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A human robot collaboration system is designed to navigate towards an unknown goal while accounting for a human's preference for a particular path in the presence of obstacles. The goal-only baseline and the proposed method differ in their approach to inferring path preferences. Which of the following statements best describes the key difference between the two approaches?",
    "choices": [
      "A) The goal-only baseline relies on the robot's own perception of the environment, while the proposed method uses a probabilistic approach to infer human preferences.",
      "B) The proposed method is more computationally expensive than the goal-only baseline due to its use of probabilistic models.",
      "C) The goal-only baseline is more accurate in predicting the correct goal, as it takes into account the robot's own perception of the environment.",
      "D) The proposed method is more accurate in predicting the correct goal, as it accounts for the human's preference for a particular path in the presence of obstacles."
    ],
    "correct_answer": "D",
    "documentation": [
      "The green robot selects actions according to the goal-only baseline, and the blue robot uses our proposed method to infer path preferences. The polytopes composing G are drawn in blue. Probability of correct goal. WLPHVWHS +J (c) Entropy of goal distribution g.\nFig. 6: Probability of the correct goal, fig.6b, and entropy of the goal belief distribution P (g), fig.6c, for the same problem setup, fig.6a. In this problem instance, the human's preference is to go to the goal by passing on the right side of the obstacle. Results are averaged over 50 runs and the area filled represents one standard deviation above and below the mean value. The goal-only baseline shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Success rates in the simple environment (Map 1).The results are averaged over 6 randomly sampled problem instances (start location, goal location, and goal possibilities), and over 50 runs per problem instance.∆T is the number of time steps separating two consecutive human inputs. The robot's mission time is Tmax = 30 time steps. We selected γ h = 1.5, corresponding to relatively noisy human inputs and making the problem more difficult to solve for the robot. Computation times for Goal Only and Path Preference methods on Map 1 (fig.5a),Map 2 (fig.5b), and Map 3 (fig.5c),averaged over 100 runs with randomly sampled problem instances. The 95 % confidence interval is provided with the mean. We evaluate computation time at the first iteration of each run (where the search depth takes on its highest value Tmax). abstract\n\nRobots that can effectively understand human intentions from actions are crucial for successful human-robot collaboration. In this work, we address the challenge of a robot navigating towards an unknown goal while also accounting for a human's preference for a particular path in the presence of obstacles."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the temporal evolutions of ṠNOEF and S NOMF in the given stages, what can be inferred about the relationship between the specific-heat ratio and the entropy production rates of NOMF and NOEF?",
    "choices": [
      "A) As the specific-heat ratio increases, the entropy production rate of NOMF decreases, while the entropy production rate of NOEF increases.",
      "B) The entropy production rate of NOMF increases with decreasing specific-heat ratio, whereas the entropy production rate of NOEF first decreases and then approaches a saturation value.",
      "C) The specific-heat ratio has a negligible effect on the entropy production rates of NOMF and NOEF.",
      "D) The entropy production rate of NOMF is more significant than that of NOEF when the specific-heat ratio is smaller than a threshold value γc."
    ],
    "correct_answer": "D",
    "documentation": [
      "In the first stage (t < 0.03), cases with different specific-heat ratios show various trends. At the stage where the bubble deformation is not very large, i.e., 0.03 < t < 0.06, values of ṠNOEF fluctuate near the average value. In the third stage (t > 0.06), evolutions of ṠNOEF in cases with larger specific-heat ratios show an apparent growing tendency. Differently, the values of ṠNOEF in cases with smaller specific-heat ratios remain almost unchanged. The influence of specific heat ratio on the ṠNOEF , similar with the effect on NOEF, is also affected by the heat conductivity and the temperature gradient. It can be seen that, except for the case of γ = 1.09, the larger the specific-heat ratio, the higher entropy production rate ṠNOEF . The temporal evolutions of ṠNOEF of case γ = 1.09 and case γ = 1.12 are very similar. Consequently, the specific-heat ratio increases the ṠNOEF by raising the temperature gradient. Further understanding can be seen in Fig. , where the entropy productions over this period are plotted. For convenience, the sum and difference between S NOMF and S NOEF are also plotted in the figure. The variation range of S NOEF is larger than that of S NOMF . It indicates that the influence of specific-heat ratio on S NOEF is more significant than that on S NOMF . Effects of specific-heat ratio on entropy production caused by NOMF and NOEF are contrary. Specifically, it can be seen that the entropy production contributed by NOMF increases with re- duced specific-heat ratio. But the entropy production caused by NOEF first reduces with decreasing specific-heat ratio and then approaches to a saturation value. The S NOEF in case γ = 1.09 is almost the same with it in case γ = 1.12. When the specificheat ratio γ is smaller than a threshold value γ c (γ c ≈ 1.315), the entropy production induced by NOEF is more significant than that caused by NOMF. However, in the case of γ > γ c , the situation reverses. The temporal evolution of the total entropy production (S NOMF +S NOEF ) is similar to the S NOEF profile."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the DBM modeling, when constructing a DBM that only retains the first-order term of the Knudsen number (i.e., only the first-order TNE effects are retained), what are the seven kinetic moments that should be reserved?",
    "choices": [
      "A) M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , M 5,3",
      "B) M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , M 5,3 , M 6",
      "C) M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , M 5,3 , M 7",
      "D) M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 , M 4 , M 5,3 , M 8"
    ],
    "correct_answer": "D",
    "documentation": [
      "In DBM modeling, the CE multiscale analysis is used to determine quickly the reserved kinetic moments. Specifically, when constructing a DBM which only the first order term of Kn number is retained (i.e., only the first order TNE effects are retained), seven kinetic moments should be reserved, i.e., the M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 . Two more kinetic moments ( M 4 and M 5,3 ) are needed when the second order TNE is considered . However, it should be noted that the function of CE analysis in DBM modeling is only to determine the kinetic moments that need to be preserved. Whether or not to derive the hydrodynamic equations does not affect the DBM simulation. The kinetic moments used in our physical modeling are shown in the Appendix B. Their expressions can be obtained by integrating v and η with continuous-form f eq . For better understanding, the Appendix C gives the two-fluid hydrodynamic equations recovered from the Boltzmann equation. The kinetic moments in Appendix B can be written in matrix form, i.e., C • f σ ,eq = fσ,eq , (\nwhere C is the matrix of discrete velocity and feq represents the kinetic moments. A proper discrete velocity model is needed to confirm the values of f σ ,eq i . The f σ ,eq can be obtained by solving the inverse matrix, i.e., f σ ,eq = C −1 • fσ,eq , where C −1 is the inverse matrix of C. It is very convenient to obtain the inverse matrix of C through some mathematical softwares such as Mathematica, etc. The D2V16 model is chosen in this paper, its sketches can be seen in Fig. . The specific values of D2V16 are given in the following equations: where \"cyc\" indicates cyclic permutation and c is an adjustable parameter of the discrete velocity model. The sketch of η in D2V16 is η i = η 0 for i = 1 − 4, and η i = 0 for i = 5 − 16. Checking the TNE state and extracting TNE information\n\nMany physical quantities can characterize the degree of TNE in a fluid system, such as relaxation time, Kn number, viscosity, heat conduction, the gradients of macroscopic quantity, etc."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the text, what is the fundamental relationship between the repulsive force between two charged spheres and the distance between their centers?",
    "choices": [
      "A) The repulsive force is directly proportional to the distance between the centers of the two spheres.",
      "B) The repulsive force is inversely proportional to the square of the distance between the centers of the two spheres.",
      "C) The repulsive force is directly proportional to the square of the distance between the centers of the two spheres.",
      "D) The repulsive force is directly proportional to the distance between the centers of the two spheres."
    ],
    "correct_answer": "B",
    "documentation": [
      "Annalen der Physik. 267 (8): S. 983–1000. Bibcode:1887AnP... 267..983H. doi:10.1002/andp.18872670827.\n^ \"The Nobel Prize in Physics 1921\". Nobel Foundation. Retrieved 2013-03-16. ^ John Sydney Blakemore, Solid state physics, pp. 1–3, Cambridge University Press, 1985 ISBN 0-521-31391-0.\n^ Richard C. Jaeger, Travis N. Blalock, Microelectronic circuit design, pp. 46–47, McGraw-Hill Professional, 2003 ISBN 0-07-250503-6.\n^ \"The repulsive force between two small spheres charged with the same type of electricity is inversely proportional to the square of the distance between the centres of the two spheres.\" Charles-Augustin de Coulomb, Histoire de l'Academie Royal des Sciences, Paris 1785. ^ Sewell, Tyson (1902), The Elements of Electrical Engineering, Lockwood, p. 18 . The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge'.\n^ a b Berkson, William (1974), Fields of Force: The Development of a World View from Faraday to Einstein, Routledge, p. 370, ISBN 0-7100-7626-6 Accounts differ as to whether this was before, during, or after a lecture.\n^ \"Lab Note #105 EMI Reduction – Unsuppressed vs. Suppressed\". Arc Suppression Technologies. April 2011. Retrieved March 7, 2012.\n^ Almost all electric fields vary in space. An exception is the electric field surrounding a planar conductor of infinite extent, the field of which is uniform. ^ Paul J. Nahin (9 October 2002). Oliver Heaviside: The Life, Work, and Times of an Electrical Genius of the Victorian Age. JHU Press. ISBN 978-0-8018-6909-9.\n^ \"The Bumpy Road to Energy Deregulation\". EnPowered. 2016-03-28.\n^ a b c d e f g h Van Riper, op.cit., p. 71. Look up electricity in Wiktionary, the free dictionary. Basic Concepts of Electricity chapter from Lessons In Electric Circuits Vol 1 DC book and series."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Graham Stewart's comments about BC's recruiting process and the development of players like Kevin Pierre-Louis suggest that the program prioritizes a patient approach to player development. However, this approach may not be effective in addressing the issue of players leaving the program for other schools. What is a potential consequence of BC's recruiting strategy, and how might it impact the program's ability to retain players?",
    "choices": [
      "A) BC's recruiting strategy may lead to a decrease in player retention, as players may feel that the program is not providing them with the necessary development and support to reach their full potential.",
      "B) The program's focus on developing players for life may actually increase player retention, as players will be more invested in the program's long-term goals and more likely to stay with the team.",
      "C) BC's approach to recruiting may not be effective in addressing the issue of players leaving the program for other schools, as the program's focus on development and support may not be enough to compete with the resources and opportunities offered by other schools.",
      "D) The program's patient approach to player development may actually lead to an increase in player retention, as players will be more likely to stay with the program and develop their skills over time."
    ],
    "correct_answer": "A",
    "documentation": [
      "But like Aboushi, his case was less about being seduced by a bigger, flashier program and more about not meeting BC's minimum standards for admissions at one point in time. Where these guys all over-hyped by the recruiting services? Where they wrong fits at their post-BC choices? I don't know. I do think a program like BC is probably more patient with players than some of the bigger programs. We won't rush a guy out of the program to free up a scholarship. We prefer to redshirt. And I think the nature of a program -- with good academic support and less of a big school mentality -- keeps kids from falling through the cracks. Every recruit thinks they are going to be a star, so selling them on development and a safety net doesn't sway many, but it should. If anything BC should use a guy like Marcus Grant -- a local kid who left a Big Ten program to come \"home\" as an example to Massachusetts recruits. Massachusetts kids keep leaving to play at the \"highest level.\" Our counter should be that we will develop them for life and the NFL (the real highest level) and not chew them up and spit them out like a football factory. I am sure that there will be a guy in the near future who decommits from BC and becomes a star. Or a guy we should have had who leads another team to glory. Right now I am just glad that we have very few regrets when it comes to old recruits. Our recruiting still has major challenges, but that's one area where things have broken our way. [Note to commentors: let me know if you think I missed any recruits who \"got away. \"]\nLabels: Graham Stewart, Joe Boisture, mike siravo, Recruiting, Spaz recruiting\nKey Players for 2012: Kevin Pierre-Louis\nJunior Linebacker, Kevin Pierre-Louis\nWhat he's been: BC's second-leading tackler. On most teams KPL would already be a star. But he played next to college football's tackling machine. There wasn't much room for headlines or an extra tackle with Luke Kuechly doing so much. Pierre-Louis also missed three games last year and played through pain in others."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a significant limitation of traditional methods for computing convolution integrals in the context of Volterra series identification?",
    "choices": [
      "A) They are computationally expensive due to the need for complex algebraic manipulations.",
      "B) They are limited to only three distinct domains: time, frequency, and Laplace.",
      "C) They require the use of time-delay neural networks for identification of Volterra kernels.",
      "D) They can be simplified by using the measured response to identify the kernel function."
    ],
    "correct_answer": "A",
    "documentation": [
      "For a long response with small time steps, the time domain methods are very costly in computational time. Volterra series is another widely used method, which is the extension of the Duhamel integral for linear systems . Volterra series can reproduce many nonlinear phenomena, but they are very complex due to higher-dimensional convolution integrals . Since 1980's, significant progress has been made in the general area of the Volterra series. The reader is referred to Ref. for a quite thorough literature review on the relevant topics. After 2017, most papers focus on Volterra series identification. De Paula and Marques proposed a method for the identification of Volterra kernels, which was based on time-delay neural networks. Son and Kim presented a method for a direct estimation of the Volterra kernel coefficients. Dalla Libera et al. introduced two new kernels for Volterra series identification. Peng et al. used the measured response to identify the kernel function and performed the nonlinear structural damage detection. Only a few papers concentrated on simplifying the computation of convolution integrals. Traditional methods for computing convolution integrals involved in the Volterra series have been performed in three distinct domains: time, frequency and Laplace. The time domain method based on Volterra series refers to discrete time convolution methods, which also suffer computational cost problems . Both the frequency domain method and the Laplace domain method based on the Volterra series consist of three steps: (1) Volterra series are transformed into an algebraic equation in the frequency domain or Laplace domain; the algebraic equation is solved by purely algebraic manipulations; and (3) the solution in Step ( ) is transformed back to the time domain. Many researchers have used the frequency domain method to compute the responses of nonlinear systems. Billings et al. developed a new method for identifying the generalized frequency response function (GFRF) of nonlinear systems and then predicted the nonlinear response based on these GFRFs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about Bill English's views on same-sex marriage based on his past voting record and his statement in December 2016?",
    "choices": [
      "A) He has always been in favor of same-sex marriage and has changed his mind.",
      "B) He has consistently opposed same-sex marriage and has not wavered in his views.",
      "C) He has a nuanced view on same-sex marriage, having initially opposed it but now being open to reconsideration.",
      "D) He has not made a public statement on same-sex marriage and therefore his views are unknown."
    ],
    "correct_answer": "C",
    "documentation": [
      "At the time of his re-election, English announced his intention to stay on as leader until the next general election. On 13 February 2018, however, he stood down as National Party leader due to personal reasons, and instructed the party to put into motion the processes to elect a new leader. He also retired from Parliament. English's resignation followed weeks of speculation that he would step aside for a new leader. On 27 February, he was succeeded as party leader by Simon Bridges as the result of the leadership election held that day. Post-premiership \nIn 2018, English joined the board of Australian conglomerate, Wesfarmers. English serves in Chairmanships of Mount Cook Alpine Salmon, Impact Lab Ltd and Manawanui Support Ltd. He is also a director of The Instillery, Centre for Independent Studies and The Todd Corporation Limited, and is a member of the Impact Advisory Group of Macquarie Infrastructure and Real Assets. Political and social views\n\nEnglish is regarded as more socially conservative than his predecessor, John Key. He has stated his opposition to voluntary euthanasia and physician-assisted suicide, same-sex civil unions, and the decriminalisation of prostitution. As Prime Minister he opposed any \"liberalisation\" of abortion law. In 2004, English voted against a bill to establish civil unions for both same-sex and opposite-sex couples. In 2005, he voted for the Marriage (Gender Clarification) Amendment Bill, which would have amended the Marriage Act to define marriage as only between a man and a woman. English voted against the Marriage (Definition of Marriage) Amendment Bill, a bill that legalised same-sex marriage in New Zealand. However, in December 2016 he stated, \"I'd probably vote differently now on the gay marriage issue. I don't think that gay marriage is a threat to anyone else's marriage\". In 2009, English voted against the Misuse of Drugs (Medicinal Cannabis) Amendment Bill, a bill aimed at amending the Misuse of Drugs Act so that cannabis could be used for medical purposes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Supreme Court's decision in Wayfair v. Amica Mutual Insurance Company effectively eliminated the doctrine of stare decisis in constitutional cases. What is the primary reason for this shift, according to the Court's reasoning?",
    "choices": [
      "A) The Court believes that Congress has the authority to update the Constitution through legislation, making stare decisis unnecessary.",
      "B) The Court argues that the Constitution's text does not explicitly mention stare decisis, allowing the Court to reinterpret its meaning.",
      "C) The Court asserts that the legislative and judicial branches have a realistic, working relationship, making stare decisis redundant.",
      "D) The Court claims that the Constitution's framers intended for the Court to have the final say in constitutional interpretation, rendering stare decisis obsolete."
    ],
    "correct_answer": "C",
    "documentation": [
      "Not anymore. Why? Because it is improper to “ask Congress to address a false constitutional premise of th[e] Court’s own creation. ”88× 88. Wayfair, 138 S. Ct. at 2096. The Latin for Wayfair’s doctrine is not stare decisis, which should reflect a realistic, working relationship between the legislative and judicial branches. It is mea culpa. In its zeal to update the Constitution for “the Cyber Age,”89× 89. Id. at 2097. the Court deleted Congress from stare decisis doctrine in constitutional cases. The Court had better options. It could have left Quill on Congress’s doorstep, as the dissent argued. Or it could have justified overruling Quill notwithstanding the special force of stare decisis. Instead, the Court reasoned that it doesn’t matter whether Congress is willing and able to do the job: a constitutional mess calls for a judicial clean-up crew. For constitutional default rules — a category of decisions embracing the dormant commerce clause and sweeping far beyond — Wayfair’s new theory of stare decisis makes the Court’s precedents less sticky and Congress less relevant."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Union Government intends to carry out development of a land for the purpose of its departments or offices or authorities. The officer-in-charge of the department shall inform the Director in writing, giving full particulars thereof, accompanied by such documents and plans as may be prescribed, at least thirty days before undertaking such development. However, the Director raises an objection to the proposed development on the ground that the development is not in conformity with the provisions of the development plan. What should the officer-in-charge do next?",
    "choices": [
      "A) Submit the proposal for development together with the objections raised by the Director to the State Government for decision, without making any modifications to the proposal.",
      "B) Make necessary modifications in the proposals for development to meet the objections raised by the Director, and then submit the proposal for development to the State Government for approval.",
      "C) Direct the Director to provide more information about the objections raised, before making any decisions about the proposal for development.",
      "D) Ignore the objections raised by the Director and proceed with the development as planned."
    ],
    "correct_answer": "B",
    "documentation": [
      "(f) for use for any purpose incidental to the use of building for human habitation, or any other building or land attached to such buildings;\n(g) for the construction of a road intended to give access to land solely for agricultural purposes\n28. Development undertaken on behalf of Union or State Government. - (1) When the Union Government or the State Government intends to carry out development of any land for the purpose of its departments or offices or authorities, the officer-in-charge thereof shall inform in writing to the Director the intention of the Government to do so, giving full particulars thereof, accompanied by such documents and plans as may be prescribed at least thirty days before undertaking such development. (2) Where the Director raises any objection to the proposed development on the ground that the development is not in conformity with the provisions of the development plan, the officer shall,-\n(i) make necessary modification in the proposals for development to meet the objections raised by the Director, or\n(ii) submit the proposal for development together with the objections raised by the Director to the State Government for decision: Provided that where no modification is proposed by the Director within thirty days of the receipt of the proposed plan by the Government, the plan will be presumed to have been approved.\n(3) The State Government, on receipt of the proposals for development together with the objections of the Director shall, approve the proposals with or without modifications or direct the officer to make such modifications in the proposals as it considers necessary in the circumstances. (4) The decision of the State Government under sub-section (3) shall be final and binding. 29. Development by local authority or by any authority constituted under this Act. - Where a local authority or any authority specially constituted under this Act intends to carry out development on any land for the purpose of that authority, the procedure applicable to the Union or State Government, under section 28 shall, mutatis' mutandis, apply in respect of such authority."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. However, when the side panels are bent and sloped to form the fuselage box section, the resulting shape is conical. As a result, the top longeron takes on an elliptical shape when built flat, bent to form a cylindrical section, and sloped to form a conical section. What is the primary reason for this elliptical shape?",
    "choices": [
      "A) The fuselage sides are not properly aligned with the top longeron.",
      "B) The side panels are bent and sloped to form a conical section, which is a result of the twist in the panel dimensions.",
      "C) The top longeron is not perpendicular to the plane of the fuselage box section.",
      "D) The fuselage box section is a result of the tumbled home method, which is a proven technique used in the marine trades."
    ],
    "correct_answer": "B",
    "documentation": [
      "Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel \"undevelopable\" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition \"developable\". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a \"compounded\" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home) , the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock. This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the underlying reason for Njoroge's relatively calm demeanor upon Mwihaki's departure, despite his initial embarrassment towards his father's actions towards Jacobo?",
    "choices": [
      "A) Njoroge's brothers' financial support for his education has made him less dependent on his father's approval.",
      "B) Njoroge's friendship with Mwihaki has provided him with a sense of stability and normalcy in his life.",
      "C) Njoroge's father's actions towards Jacobo have not had a significant impact on his relationship with Mwihaki.",
      "D) Njoroge's family's involvement in the Mau Mau movement has made him more aware of the consequences of his actions, leading to a sense of detachment."
    ],
    "correct_answer": "A",
    "documentation": [
      "Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father. Mwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school. For a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population. Jacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods. One day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition. Several months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A 65-year-old patient presents with chest pain and is found to have a normal sinus rhythm (NSR) on an electrocardiogram (EKG). Further testing reveals a myocardial infarction (MI) with evidence of new wall-motion abnormalities (NWMA) on an echocardiogram. The patient is also found to have peripheral artery disease (PAD) and is prescribed nitroglycerin (NTG) for angina. Which of the following is the most likely underlying cause of the patient's myocardial infarction?",
    "choices": [
      "A) Atherosclerosis of the coronary arteries",
      "B) Mitral valve prolapse",
      "C) Mitral valve stenosis",
      "D) Mitral regurgitation"
    ],
    "correct_answer": "A",
    "documentation": [
      "Murmur – Noises superimposed on normal heart sounds. They are caused by congenital defects or damaged heart valves that do not close properly and allow blood to leak back into the originating chamber. MV – Mitral Valve: The structure that controls blood flow between the heart’s left atrium (upper chamber) and left ventricle (lower chamber). Myocardial Infarction (MI, heart attack) – The damage or death of an area of the heart muscle (myocardium) resulting from a blocked blood supply to the area. The affected tissue dies, injuring the heart. Myocardium – The muscular tissue of the heart. New Wall-Motion Abnormalities – Results seen on an echocardiogram test report (see NWMA, below). Nitroglycerin – A medicine that helps relax and dilate arteries; often used to treat cardiac chest pain (angina). Also called NTG or GTN.\nNSR – Normal Sinus Rhythm: The characteristic rhythm of the healthy human heart. NSR is considered to be present if the heart rate is in the normal range, the P waves are normal on the EKG/ECG, and the rate does not vary significantly. NSTEMI – Non-ST-segment-elevation myocardial infarction: The milder form of the two main types of heart attack. An NSTEMI heart attack does not produce an ST-segment elevation seen on an electrocardiogram test (EKG). See also STEMI. Nuclear Stress Test – A diagnostic test that usually involves two exercise stress tests, one while you’re exercising on a treadmill/stationary bike or with medication that stresses your heart, and another set while you’re at rest. A nuclear stress test is used to gather information about how well your heart works during physical activity and at rest. See also: Exercise stress test, Nuclear perfusion test, MIBI. Open heart surgery – Any surgery in which the chest is opened and surgery is done on the heart muscle, valves, coronary arteries, or other parts of the heart (such as the aorta). See also CABG. Pacemaker – A surgically implanted electronic device that helps regulate the heartbeat. PAD – Peripheral Artery Disease: A common circulatory problem in which narrowed arteries reduce blood flow to the limbs, usually to the legs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The girlfriend is concerned about her boyfriend's handling of their daughter's behavior and wants to know if it's reasonable to ask him to take the child out of the house if she continues to exhibit bad behavior. The boyfriend believes that his approach is appropriate because everyone handles being upset differently. However, the girlfriend feels that he is being too lenient and giving in to her daughter's behavior. What is the most appropriate approach for the girlfriend to take in this situation?",
    "choices": [
      "A) Encourage the boyfriend to set clear boundaries and consequences for bad behavior, while also being understanding and empathetic towards the child's feelings.",
      "B) Suggest that the boyfriend should not have told the child that she wouldn't have been in trouble if she hadn't lied, as this could be seen as blaming her for the situation.",
      "C) Recommend that the girlfriend should take a more active role in managing the child's behavior, such as by setting limits and providing positive reinforcement for good behavior.",
      "D) Encourage the boyfriend to seek therapy for himself, as his approach to handling the child's behavior may be contributing to the problem."
    ],
    "correct_answer": "A",
    "documentation": [
      "She got excited that someone was at the door and ran downstairs to answer the door. In the process of getting the door, she fell and yelled at Carli. Carli became extremely upset. She was able to control her feelings at her friend's house, but when she came home, she proceeded to cry extremely loudly for over an hour. Her dad spent most of that time with her, talking to her and trying to calm her down. After an hour, I asked him if he could please tell her to be more quiet because the other members of the household were trying to go to sleep. My question is....how do I as the girlfriend, handle this? He did not like that I asked her to be quiet. We have a rule that if she is having bad behavior, and can't calm down in 5 minutes, he takes her out of the house because her yelling doesn't stop for a long time and is very upsetting to everyone in the household. I would like to ask him to do this with this kind of situation as well. Is this a reasonable request? His thought was that she shouldn't be made to calm down, because everyone handles being upset in a different way. But, she was literally sobbing and wailing very loudly. My other question is should she have been told that if she wouldn't have lied, this wouldn't have happened? She has a history of lying and of not accepting responsibility for her actions. My boyfriend became very upset with me when I brought this up. He was being very sympathetic and understanding to her. I feel like he was giving her negative attention, and being an over indulgent parent by not putting his foot gown and saying, \"you can't carry on like this, even though you are upset\". Please let me know how we can handle these situations better. I am contacting you for help with adult AS. I am taking initiative to pre screen potential therapists to help my current boyfriend get therapy and help with Adult AS. He has seen many therapists, but it seems like they aren’t really helping him with his problems. They don’t seem to understand how his (undiagnosed) AS would affect therapy approaches."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The conduction gap in graphene exhibits a strong dependence on the strain and its applied direction. According to the given equations, what is the condition for the zero conduction gap to occur?",
    "choices": [
      "A) The strain-induced shift of Dirac points in the $\\kappa_y$-axis is larger than the shift in the $\\kappa_x$-axis.",
      "B) The ratio of the strain-induced shifts of Dirac points in the $\\kappa_y$-axis and $\\kappa_x$-axis is equal to 1.",
      "C) The strain-induced shift of Dirac points in the $\\kappa_y$-axis is equal to the negative of the shift in the $\\kappa_x$-axis.",
      "D) The strain-induced shift of Dirac points in the $\\kappa_y$-axis is equal to the positive of the shift in the $\\kappa_x$-axis."
    ],
    "correct_answer": "C",
    "documentation": [
      "Hence, the data obtained for $\\phi$ ranging from $-30^\\circ$ to $30^\\circ$ and $\\theta  \\in \\left[ {0^\\circ ,180^\\circ } \\right]$ covers the properties of conduction gap in all possible cases. In Fig. 5, we present the maps of conduction gap with respect to the strain and its applied direction in two particular cases: the transport is either along the armchair ($\\phi = 0$) or the zigzag ($\\phi = 30^\\circ$) directions. Both tensile and compressive strains are considered. Let us first discuss the results obtained in the armchair case. Figs. 5(a,b) show that (i) a large conduction gap up to about 500 meV can open with a strain of 6 $\\%$ and (ii) again the conduction gap is strongly $\\theta$-dependent, in particular, its peaks occur at $\\theta = 0$ or $90^\\circ$ while the gap is zero at $\\theta \\approx 47^\\circ$ and $133^\\circ$ for tensile strain and at $\\theta \\approx 43^\\circ$ and $137^\\circ$ for compressive strain. In principle, the conduction gap is larger if the shift of Dirac points in the $\\kappa_y$-axis is larger, as discussed above about Figs. 3-4. We notice that the strain-induced shifts can be different for the six Dirac points of graphene \\cite{kitt12} and the gap is zero when there is any Dirac point observed at the same $\\kappa_y$ in the two graphene sections. From Eq. (9), we find that the Dirac points are determined by the following equations:\n\\begin{eqnarray*}\n  {\\cos}\\frac{\\kappa_y}{2} & =& \\pm \\frac{1}{2}\\sqrt{\\frac{{t_3^2 - {{\\left( {{t_1} - {t_2}} \\right)}^2}}}{{{t_1}{t_2}}}}, \\\\\n  \\cos \\frac{{\\kappa_x}}{2} &=& \\frac{{{t_1} + {t_2}}}{{\\left| {{t_3}} \\right|}}\\cos \\frac{{\\kappa_y}}{2},\\,\\,\\,\\sin \\frac{{\\kappa_x}}{2} = \\frac{{{t_2} - {t_1}}}{{\\left| {{t_3}} \\right|}}\\sin \\frac{{\\kappa_y}}{2},\n\\end{eqnarray*}\nwhich simplify into ${\\cos}\\frac{\\kappa_y}{2} = \\pm \\frac{1}{2}$ and, respectively, $\\cos \\left( {\\frac{{{\\kappa _x}}}{2}} \\right) = \\mp 1$ in the unstrained case. Hence, the zero conduction gap is obtained if\n\\begin{equation*}\n  \\frac{{t_3^2 - {{\\left( {{t_1} - {t_2}} \\right)}^2}}}{{4{t_1}{t_2}}} = \\frac{1}{4}\n\\end{equation*}\nAdditionally, it is observed that the effects of a strain $\\{\\sigma,\\theta\\}$ are qualitatively similar to those of a strain $\\{-\\sigma,\\theta+90^\\circ\\}$, i.e., the peaks and zero values of conduction gap are obtained at the same $\\theta$ in these two situations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In Ngũgĩ wa Thiong'o's novel \"Things Fall Apart\", the character of Kamba is portrayed as a leader who is both a traditionalist and a pragmatist. However, his actions are influenced by the British colonial authorities, who are trying to suppress the traditional Igbo way of life. What is the significance of the fact that Kamba's son, Mwanga, is educated in a Christian mission school, which is seen as a symbol of Westernization and cultural imperialism?",
    "choices": [
      "A) Kamba's decision to send Mwanga to the mission school is a sign of his own cultural imperialism, as he is trying to assimilate the Igbo people into Western culture.",
      "B) Kamba's decision to send Mwanga to the mission school is a pragmatic move, as he believes that Western education will provide his son with better job opportunities and a more secure future.",
      "C) Kamba's decision to send Mwanga to the mission school is a result of the British colonial authorities' efforts to suppress traditional Igbo culture, as they see the mission school as a way to assimilate the Igbo people into their own culture.",
      "D) Kamba's decision to send Mwanga to the mission school is a sign of his own resistance to Westernization, as he believes that traditional Igbo culture is superior to Western culture."
    ],
    "correct_answer": "C",
    "documentation": [
      "See also\n\nThings Fall Apart\nDeath and the King's Horseman\n\nReferences\n\nExternal links\nOfficial homepage of Ngũgĩ wa Thiong'o\nBBC profile of Ngũgĩ wa Thiong'o\nWeep Not, Child at Google Books\n\nBritish Empire in fiction\nNovels set in colonial Africa\nHistorical novels\nKenyan English-language novels\nNovels by Ngũgĩ wa Thiong'o\nNovels set in Kenya\n1964 novels\nHeinemann (publisher) books\nPostcolonial novels\nAfrican Writers Series\n1964 debut novels"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary effect of the CAR exchange mechanism on the energy splitting of the singlet and triplet states in a DQD system?",
    "choices": [
      "A) It decreases the energy splitting between the singlet and triplet states, leading to a more ferromagnetic behavior.",
      "B) It increases the energy splitting between the singlet and triplet states, leading to a more anti-ferromagnetic behavior.",
      "C) It has no effect on the energy splitting between the singlet and triplet states, as it only involves the re-emission of a Cooper pair.",
      "D) It generates a negative (ferromagnetic) sign in the RKKY interaction, leading to a more ferromagnetic behavior."
    ],
    "correct_answer": "D",
    "documentation": [
      "The proximity of SC gives rise to two further exchange mechanisms that\ndetermine the system's behavior. First of all, the (conventional)\n\\emph{RKKY interaction} appears, $J \\sim \\GS{}^2$ \\cite{RK,K,Y}. Moreover, the \\emph{CAR exchange} emerges as a consequence of finite $\\GS{}$ \\cite{Yao}. It can be understood on the basis \nof perturbation theory as follows. DQD in the inter-dot singlet state may absorb\nand re-emit a Cooper pair approaching from SC; see \\fig{system}(e)-(g). As a second-order\nprocess, it reduces the energy of the singlet, which is the ground state of isolated DQD. A similar process is not possible in the triplet state due to spin conservation. Therefore, the singlet-triplet energy splitting $J^{\\mathrm{eff}}$ is increased (or generated for $t=J=0$). More precisely, the leading ($2$nd-order in $t$ and $\\GS{}$) terms\nin the total exchange are \n\\begin{equation}\nJ^{\\mathrm{eff}} \t\\approx \tJ + \\frac{4t^2}{U-U'+\\frac{3}{4}J} + \\frac{4\\GS{}^2}{U+U'+\\frac{3}{4}J}. \\label{Jeff}\n\\end{equation}\nUsing this estimation, one can predict $T^*$ for finite $\\GS{}$, $t$ and $J$ with \\eq{Tstar}. Apparently, from three contributions corresponding to:\n(i) RKKY interaction, (ii) direct exchange and (iii) CAR exchange, only the first may bear a negative (ferromagnetic) sign. The two other contributions always have an anti-ferromagnetic nature. More accurate expression for $J^{\\mathrm{eff}}$ is derived in Appendix~\\ref{sec:downfolding} [see \\eq{A_J}] by the Hamiltonian down-folding procedure. The relevant terms differ \nby factors important only for large $\\GS{}/U$. Finally, it seems worth stressing that normal leads are not necessary for CAR exchange to occur. At least one of them is inevitable for the Kondo screening though, and two symmetrically coupled \nnormal leads allow for measurement of the normal conductance. It is also noteworthy that inter-dot Coulomb interactions\ndecrease the energy of intermediate states contributing to direct exchange [\\fig{system}(c)], while increasing the energy of intermediate\nstates causing the CAR exchange [\\fig{system}(f)]."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a key implication of the proposed framework for contour completion using deep structure priors (DSP) that differs from traditional approaches to contour completion?",
    "choices": [
      "A) The use of a strict definition of the mask is necessary for accurate reconstruction.",
      "B) The generator network must be pre-trained on a large dataset of complete images to learn the structure of the contours.",
      "C) The model can learn to connect disconnected contours without the need for a mask, but this requires careful tuning of the proportion coefficient α.",
      "D) The use of a novel loss metric allows the model to learn the perceivable illusory contours without any prior knowledge of the underlying structure."
    ],
    "correct_answer": "D",
    "documentation": [
      "In training, we use the MSE loss between the degraded image and the output of the network, and we optimize the loss using the ADAM optimizer and a learning rate equal to 0.01 . In our experiments, we also used α = 0.15 as an optimal proportion coefficient for reconstruction loss. Conclusion\n\nIn this work, we introduced a novel framework for contour completion using deep structure priors (DSP). This work offers a novel notion of a maskless grouping of fragmented contours. In our proposed framework, we introduced a novel loss metric that does not require a strict definition of the mask. Instead, it lets the model learn the perceivable illusory contours and connects those fragmented pieces using a generator network that is solely trained on just the single incomplete input image. Our model does not require any pre-training which demonstrates that the convolutional architecture of the hour-glass model is able to connect disconnected contours. We present an extended set of experiments that show the capability of our algorithm. We investigate the effect of each parameter introduced in our algorithm separately and show how one could possibly achieve the best result for their problem using this model. In future work, we plan to extend this model and try to see how it performs with real images. In particular, we want to determine whether we can inpaint real-world photographs while retaining perceptually aware scene structures. The importance of shape in perception by deep neural networks has been highlighted in many adversarial examples to appearance-based networks . The outcome of this work has strong potential to impact the designing and implementation of models that are robust to such perturbations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The speaker's son, Taylor, has a history of struggling with college courses, despite having a strong academic record in high school. The speaker is concerned about Taylor's future and is seeking more specific ideas on how to help him. What is a possible underlying reason for Taylor's struggles with college courses, based on the information provided?",
    "choices": [
      "A) Taylor's difficulties with social interactions and sensory sensitivities are making it hard for him to focus in class and complete assignments.",
      "B) Taylor's perfectionism and high expectations for himself are causing him to become overwhelmed and disengage from college life.",
      "C) Taylor's lack of motivation and interest in college courses is due to his difficulty with transitions and changes in routine, which is a common trait among individuals with Asperger's Syndrome.",
      "D) Taylor's struggles with college courses are a result of his parents' overindulgence and lack of boundaries, which is hindering his ability to develop independence and self-regulation skills."
    ],
    "correct_answer": "C",
    "documentation": [
      "We changed her diet and tried getting her involved with activities but she is anti-social and prefers reading than being social. She is terrified of change even in daily routine (even that will trigger prolonged crying). It frustrates me because I don't know what else to do with her behavior. I've tried acupuncture (she refused at the first session); she refuses massage too. She is an honor-roll student at school and has very minimal issues at school but if she has had a bad day it does result in a tantrum or crying and defiance. How can I get her tested for Asperger's Syndrome? Last night our 24 year old son with Aspergers told his dad and I that he is pulling out of the 4 college classes that he recetnly enrolled in because he has not been attending class or turning in his assignments. He paid $2800 (his own money) for tuition and I reminded him of this when he told us but it did not seem to bother him. This is the 3rd time he has started college courses and has not completed them. (He also took some concurrent college classes while he was in high school that he failed). This is a son who basically had a 4.0 grade point average through 10th grade and got a 34 on the ACT the first time he took it. With the news that he was once again not sticking with college courses I did not sleep well. When I got up this mornning I began looking online for help in how to deal with his situation. I found your \"Launching Adult Children With Aspergers\" and purchased it. Most of what is included are things we have done or did with our son throughout his life. I was hoping for more help so I am emailing you now in hopes of more specific ideas. We noticed some things with our son, Taylor, as a yound child but as we had not heard of Aspergers at that time we just did what we thought would help him. As a toddler and a child at pre-school he generally went off on his own to play. When I talked to his pre-school teacher about my concerns (that I was worried he would end up a hermit) she said she did not see him being a loner and that he seemed to interact fine with others in many situations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient with the hemoglobin H/Constant Spring form of the disease requires regular transfusions to manage their anemia. However, the patient's bone marrow is not functioning properly, and they are at risk of developing jaundice due to excessive bilirubin buildup. To address this issue, the patient's doctor recommends a bone marrow transplantation from a compatible donor. The doctor also prescribes desferoxamine to counteract the life-threatening buildup of iron in the patient's body. Which of the following is the primary reason for the doctor's recommendation of desferoxamine?",
    "choices": [
      "A) To increase the patient's red blood cell count",
      "B) To reduce the patient's bilirubin levels",
      "C) To prevent the buildup of iron in the patient's body",
      "D) To stimulate the patient's bone marrow to produce more hemoglobin"
    ],
    "correct_answer": "C",
    "documentation": [
      "For those with the hemoglobin H/Constant Spring form of the disease, the need for transfusions may be intermittent or ongoing, perhaps on a monthly basis and requiring desferoxamine treatment. Individuals with this more severe form of the disease may also have an increased chance of requiring removal of an enlarged and/or overactive spleen. Anemia — A blood condition in which the level of hemoglobin or the number of red blood cells falls below normal values. Common symptoms include paleness, fatigue, and shortness of breath. Bilirubin — A yellow pigment that is the end result of hemoglobin breakdown. This pigment is metabolized in the liver and excreted from the body through the bile. Bloodstream levels are normally low; however, extensive red cell destruction leads to excessive bilirubin formation and jaundice. Bone marrow — A spongy tissue located in the hollow centers of certain bones, such as the skull and hip bones. Bone marrow is the site of blood cell generation. Bone marrow transplantation — A medical procedure used to treat some diseases that arise from defective blood cell formation in the bone marrow. Healthy bone marrow is extracted from a donor to replace the marrow in an ailing individual. Proteins on the surface of bone marrow cells must be identical or very closely matched between a donor and the recipient. Desferoxamine — The primary drug used in iron chelation therapy. It aids in counteracting the life-threatening buildup of iron in the body associated with long-term blood transfusions. Globin — One of the component protein molecules found in hemoglobin. Normal adult hemoglobin has a pair each of alpha-globin and beta-globin molecules. Heme — The iron-containing molecule in hemoglobin that serves as the site for oxygen binding. Hemoglobin — Protein-iron compound in the blood that carries oxygen to the cells and carries carbon dioxide away from the cells. Hemoglobin A — Normal adult hemoglobin that contains a heme molecule, two alpha-globin molecules, and two beta-globin molecules."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a significant challenge in implementing effective shark conservation measures in Indonesia's shark fisheries, despite the country's commitment to CITES regulations?",
    "choices": [
      "A) The lack of awareness among fishers about the impact of their fishing practices on shark populations.",
      "B) The limited availability of alternative, sustainable marine-based livelihoods for small-scale fishers in poor coastal communities.",
      "C) The difficulty in enforcing CITES regulations at the point of export, given the country's extensive coastline and lack of effective monitoring systems.",
      "D) The need for significant investments in research and development of new, more selective fishing gear technologies to reduce bycatch and protect threatened species."
    ],
    "correct_answer": "B",
    "documentation": [
      "Indonesia is the world’s largest shark fishing nation [9, 14], and a global priority for shark conservation . Until recently Indonesia’s shark fishery has largely functioned as de facto open-access [12, 16]. However, in the past five years the Indonesian government has demonstrated a clear commitment to shark conservation and resource management, with domestic measures put in place to implement international obligations under CITES . Exploitation of all CITES-listed species is now regulated, either through full species protection or export controls (these species are hereafter referred to as ‘regulated’ species). However, CITES only affords protection to a small number of Indonesia’s 112 known shark species , of which 83 are threatened with extinction according to the IUCN Red List of Threatened Species (i.e. Vulnerable (VU), Endangered (EN) or Critically Endangered (CR) , these species are hereafter referred to as ‘threatened’ species), many of which continue to be landed throughout the country . Further, these policy measures predominantly regulate trade at the point of export, but do not necessarily influence fisher behaviour or local demand at the point of catch, such that the ‘trickle-down’ impacts on species mortality are unknown. In addition, effectively implementing species-specific shark mortality controls remains challenging due to the non-selectivity of fishing gears, and practical and cultural barriers to changing fisher preferences for certain gear-types and fishing methods. As such, existing regulations alone (e.g. Indonesian Law on Fisheries 31/2004 and its derivative regulations) will likely be insufficient to curb mortality of threatened and regulated species, as fishers must be both willing and able to change their fishing behaviour . Moreover, most of Indonesia’s shark fisheries are small-scale, and in relatively poor coastal communities where there are often no legal, sustainable marine-based alternatives to shark fishing that offer similar financial returns [22, 23]."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the prevalence of alpha-thalassemia major in Southeast Asian populations based on the information provided about the prevalence of beta-thalassemia major in the same populations?",
    "choices": [
      "A) The prevalence of alpha-thalassemia major is significantly higher than that of beta-thalassemia major in Southeast Asian populations.",
      "B) The prevalence of alpha-thalassemia major is significantly lower than that of beta-thalassemia major in Southeast Asian populations.",
      "C) The prevalence of alpha-thalassemia major is similar to that of beta-thalassemia major in Southeast Asian populations.",
      "D) The prevalence of alpha-thalassemia major cannot be determined from the information provided."
    ],
    "correct_answer": "B",
    "documentation": [
      "Survivors passed the mutation onto their offspring, and the trait became established throughout areas where malaria is common. As populations migrated, so did the thalassemia traits. Beta thalassemia trait is seen most commonly in people with the following ancestry: Mediterranean (including North African, and particularly Italian and Greek), Middle Eastern, Indian, African, Chinese, and Southeast Asian (including Vietnamese, Laotian, Thai, Singaporean, Filipino, Cambodian, Malaysian, Burmese, and Indonesian). Alpha-thalassemia trait is seen with increased frequency in the same ethnic groups. However, there are different types of alpha thalassemia traits within these populations. The frequency of hemoglobin H disease and alpha thalassemia major depends on the type of alpha thalassemia trait. The populations in which alpha thalassemia diseases are most common include Southeast Asians and Chinese (particularly Southern Chinese). It is difficult to obtain accurate prevalence figures for various types of thalassemia within different populations. This difficulty arises due to testing limitations in determining exact genetic diagnoses, as well as the fact that many studies have focused on small, biased hospital populations. Two studies reflect prevalence figures that can be helpful counseling families and determining who to screen for beta thalassemia. Between the years of 1990 and 1996, the State of California screened more than 3.1 million infants born in the state for beta thalassemia. Approximately 1 in 114,000 infants had beta thalassemia major, with prevalence rates being highest among Asian Indians (about one in 4,000), Southeast Asians (about one in 10,000), and Middle Easterners (about one in 7,000). Another type of beta thalassemia disease, E/beta thalassemia, was represented in approximately one in 110,000 births, all of which occurred in families of Southeast Asian ancestry. Among Southeast Asians, the prevalence of E/beta thalassemia was approximately one in 2,600 births. This is in keeping with the observation that hemoglobin E trait carrier rates are relatively high within the Southeast Asian population: 16% in a study of 768 immigrants to California, and up to 25% in some specific Southeast Asian populations such as Cambodians."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the discrete Boltzmann equation for component σ, what is the physical significance of the term M 3,1,α ( f σ ,(1) ) in the equation (C.15)?",
    "choices": [
      "A) It represents the total energy of the system, including both kinetic and potential energy.",
      "B) It represents the rate of change of the non-conserved moment corresponding to the α-th component of the velocity.",
      "C) It represents the total momentum of the system, including both linear and angular momentum.",
      "D) It represents the rate of change of the non-conserved moment corresponding to the α-th component of the temperature."
    ],
    "correct_answer": "B",
    "documentation": [
      "The discrete Boltzmann equation for component σ is (C.1) In Eq. C.1 there are two equilibrium distribution functions, i.e., f σ ,seq = f σ ,seq (ρ σ , u σ , T σ ) and f σ ,eq = f σ ,eq (ρ σ , u, T ). For convenience, S σ i is defined as We perform the CE expansion around the f σ ,seq . That is, the distribution function f σ i can be expanded as where ε is a coefficient referring to Knudsen number. The partial derivatives of time and space can also be expanded to Substituting the above four equations into Eq.\n(C.1), we can obtain (C.6) When retaining to ε terms, the following equation is obtained When retaining to ε 2 terms, we can obtain where M 2,αβ ( f σ ,(1) ) = ∑ i v iα v iβ f σ ,(1) i and M 3,1,α ( f σ ,(1) ) = . Substituting Eq. (C.14) into the above three equations, and replacing the time derivatives with the space derivatives, we obtain It should be noted that the ability to recover the corresponding level of macroscopic fluid mechanics equations is only part of the physical function of DBM. The corresponding to the physical functions of DBM is the EHEs, which, in addition to the conserved moments evolution equations corresponding to the three conservation laws of mass, momentum and energy, also includes some of the most closely related nonconserved moments evolution equations. We refer the EHEs derivation based on kinetic equation to as KMM. The necessity of the expanded part, the evolution equations of the relevant non-conserved moments, increases rapidly as increasing the degree of non-continuity/non-equilibrium. As the degree of non-continuity/non-equilibrium increases, the complexity will rapidly make KMM simulation studies, deriving and solving EHE, impossible."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on the metabolism of vitamin K in rats found that dietary phylloquinone is converted to tissue menaquinone-4 in the absence of gut bacteria. However, another study suggested that intestinal flora may play a role in the conversion of phylloquinone to menaquinone-4. Which of the following statements best summarizes the relationship between these two findings?",
    "choices": [
      "A) The first study's findings contradict the second study's suggestion that intestinal flora is involved in the conversion process.",
      "B) The first study's findings support the second study's suggestion that intestinal flora is involved in the conversion process, but only in certain circumstances.",
      "C) The first study's findings indicate that the conversion of phylloquinone to menaquinone-4 is independent of intestinal flora, and therefore, the second study's suggestion is incorrect.",
      "D) The first study's findings suggest that the conversion of phylloquinone to menaquinone-4 is dependent on intestinal flora, and therefore, the second study's suggestion is incorrect."
    ],
    "correct_answer": "B",
    "documentation": [
      "\"Vitamin K distribution in rat tissues: dietary phylloquinone is a source of tissue menaquinone-4\". The British Journal of Nutrition. 72 (3): 415–425. doi:10.1079/BJN19940043. PMID 7947656. ^ Will, B. H.; Usui, Y.; Suttie, J. W. (Dec 1992). \"Comparative metabolism and requirement of vitamin K in chicks and rats\". Journal of Nutrition. 122 (12): 2354–2360. PMID 1453219. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). \"Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria\". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). \"Intestinal flora is not an intermediate in the phylloquinone-menaquinone-4 conversion in the rat\". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Al Rajabi, Ala (2011). The Enzymatic Conversion of Phylloquinone to Menaquinone-4 (PhD thesis). Tufts University, Friedman School of Nutrition Science and Policy. ^ Furie, B.; Bouchard, B. A.; Furie, B. C. (Mar 1999). \"Vitamin K-dependent biosynthesis of gamma-carboxyglutamic acid\". Blood. 93 (6): 1798–1808. PMID 10068650. ^ Mann, K. G. (Aug 1999). \"Biochemistry and physiology of blood coagulation\". Thrombosis and Haemostasis. 82 (2): 165–174. PMID 10605701. ^ Price, P. A. (1988). \"Role of vitamin-K-dependent proteins in bone metabolism\". Annual Review of Nutrition. 8: 565–583. doi:10.1146/annurev.nu.08.070188.003025. PMID 3060178. ^ Coutu, D. L.; Wu, J. H.; Monette, A.; Rivard, G. E.; Blostein, M. D.; Galipeau, J (Jun 2008). \"Periostin, a member of a novel family of vitamin K-dependent proteins, is expressed by mesenchymal stromal cells\". Journal of Biological Chemistry. 283 (26): 17991–18001. doi:10.1074/jbc. M708029200. PMID 18450759. ^ Viegas, C. S.; Simes, D. C.; Laizé, V.; Williamson, M. K.; Price, P. A.; Cancela, M. L. (Dec 2008). \"Gla-rich protein (GRP), a new vitamin K-dependent protein identified from sturgeon cartilage and highly conserved in vertebrates\"."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider the case where $N + \\alpha - 2 > 0$ and the remaining inequality is not satisfied. In this scenario, what can be concluded about the existence of a suitable function $g(x)$ that satisfies the properties and a stable sub/super-solution $u$ for the appropriate equation?",
    "choices": [
      "A) There is no suitable function $g(x)$ that can be found, and the inequalities are sufficient to guarantee the non-existence of a stable sub/super-solution.",
      "B) The inequalities are necessary but not sufficient to guarantee the existence of a suitable function $g(x)$, and a stable sub/super-solution can be found.",
      "C) The inequalities are sufficient to guarantee the existence of a suitable function $g(x)$, and a stable sub/super-solution can be found.",
      "D) The inequalities are necessary but not sufficient to guarantee the existence of a suitable function $g(x)$, and a stable sub/super-solution cannot be found."
    ],
    "correct_answer": "B",
    "documentation": [
      "If one takes $ \\omega_1=\\omega_2=1$ in the above corollary, the results obtained for $(G)$ and  $(L)$,  and for some values of $p$ in $(M)$, are optimal, see \\cite{f2,f3,zz}. We now drop all monotonicity conditions on $ \\omega_1$.\n\n\\begin{cor} \\label{po} Suppose  $ \\omega_1 \\le C \\omega_2$ for big $x$, $ \\omega_2 \\in L^\\infty$, $ | \\nabla \\omega_1| \\le C \\omega_2$ for big $x$.\n\\begin{enumerate} \\item  There is no stable sub-solution of $(G)$ if $ N \\le 4$.\n\n\\item  There is no positive stable sub-solution of $(L)$ if $$N<1+\\frac{2}{p-1} \\left( p+\\sqrt{p(p-1)}  \\right).$$\n\n\\item There is no positive super-solution of $(M)$ if $$N<1+\\frac{2}{p+1} \\left( p+\\sqrt{p(p+1)}  \\right).$$\n\n\\end{enumerate}\n\n\\end{cor}\n\nSome of the conditions on $ \\omega_i$ in Corollary \\ref{po} seem somewhat artificial. If we shift over to the advection equation (and we take $ \\omega_1=\\omega_2$  for simplicity)\n\\[ -\\Delta u + \\nabla \\gamma \\cdot \\nabla u = f(u), \\] the conditions on $ \\gamma$ become: $ \\gamma$ is bounded from below and has a bounded gradient. In what follows we examine the case where $ \\omega_1(x) = (|x|^2 +1)^\\frac{\\alpha}{2}$ and $ \\omega_2(x)= g(x) (|x|^2 +1)^\\frac{\\beta}{2}$,  where $ g(x) $ is positive except at say a point, smooth and where $ \\lim_{|x| \\rightarrow \\infty} g(x) = C \\in (0,\\infty)$. For this class of weights we can essentially obtain optimal results. \\begin{thm} \\label{alpha_beta}   Take $ \\omega_1 $ and $ \\omega_2$ as above. \\begin{enumerate}\n\n\\item If $ N+ \\alpha - 2 <0$ then there is no stable sub-solution for $(G)$, $(L)$ (here we require it to be positive) and in the case of $(M)$ there is no positive  stable  super-solution. This case is the trivial case, see Remark \\ref{triv}. \\\\\n\n\n\n\\textbf{Assumption:} For the remaining cases we assume that $ N + \\alpha -2 > 0$.\n\n  \\item If  $N+\\alpha-2<4(\\beta-\\alpha+2)$ then there is no  stable sub-solution for $ (G)$.\n\n\\item If $N+\\alpha-2<\\frac{ 2(\\beta-\\alpha+2)   }{p-1} \\left( p+\\sqrt{p(p-1)}  \\right)$ then there is  no positive stable sub-solution of $(L)$.\n\n\\item If $N+\\alpha-2<\\frac{2(\\beta-\\alpha+2)   }{p+1} \\left( p+\\sqrt{p(p+1)}  \\right)$ then there is no positive stable super-solution of $(M)$.\n\n\\item Further more 2,3,4 are optimal in the sense if $ N + \\alpha -2 > 0$ and the remaining inequality is not satisfied (and in addition we assume we don't have equality in the inequality) then we can find a suitable function $ g(x)$ which satisfies the above properties and a stable sub/super-solution $u$ for the appropriate equation."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What type of device management system would be most suitable for a large enterprise that requires secure and centralized management of hundreds of IoT devices, including smartphones, tablets, PCs, wearables, and IoT devices?",
    "choices": [
      "A) A device-driven security system that focuses on individual device security features, such as built-in security applications.",
      "B) A platform-driven security system that relies on traditional IoT-MDM system providers for device management, but lacks centralized management capabilities.",
      "C) A location-driven security system that harnesses distributed and infrastructure security techniques, but is not designed for large-scale device management.",
      "D) An IoT-MDM system that provides centralized management and configuration of devices, including secure devices with built-in security features, and seamless upgrading capabilities."
    ],
    "correct_answer": "D",
    "documentation": [
      "While these service provisioning scenarios are relevant to overall 5G, they are also significantly related to the case of IoT-MDM system/services. Figure 3 Scenarios for 5G cybersecurity provisioning [Adapted from 35]. As a result, we find four major drivers of security, which potentially will come with new business opportunities in the 5G era. Device driven security comprises distributed and D2D security techniques. Platform driven security will focus on centralized and D2D security techniques. Whereas, network infrastructure driven security should focus on centralized and infrastructure security methods. Lastly, location driven security should harness distributed and infrastructure security techniques. In a quest to identify potential business entities operating in each of the quadrants relevant to device management, we recognize a ‘secure device manufacturer/provider’ focuses on device-driven security. Currently, multiple device manufacturers are developing devices where security features are built-in, regardless of which network or websites the users are accessing. This built-in security can be offered to multiple kinds of devices including smartphones, tablets, pcs, wearables, and even to IoT devices with communication and computation capability. These secure devices are built in a way that it will control access to potentially harmful networks, websites, and content; even without any commercially available security applications installed. However, when a customer enterprise (e.g. the future digital hospital) is buying a fleet of hundreds of such secure devices, the question raises how to manage and configure all these devices from time-to-time. In such a case, even the secure devices will need IoT-MDM services for proper management and seamless upgrading when needed. For the platform-driven security, we observe a ‘traditional IoT-MDM system provider’ can be a good example. In many cases, IoT-MDM system providers are selling there device management systems to enterprises directly."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a key consideration when designing a supercircuit for benchmarking a denoiser, particularly in terms of the dimensionality of the quasiprobabilistic mitigating ensemble?",
    "choices": [
      "A) The number of gate applications required to achieve a certain level of precision",
      "B) The type of two-qubit unitary channel used to parameterize the denoiser",
      "C) The choice of measurement and conditional preparation channels for the non-unitary part",
      "D) The overall circuit depth and its impact on the accuracy of the denoiser"
    ],
    "correct_answer": "D",
    "documentation": [
      "This circuit architecture is commonly used to simulate the time evolution of a quantum many-body system, until some time t, with controllable precision , and we will use it to benchmark the denoiser. In practice, we cannot directly implement a supercircuit, and so we have to utilize its interpretation as an ensemble of circuits. Essentially, after executing a shot of the noisy circuit we sample the denoiser and apply it. The goal is to construct the denoiser in a way that averaging over many of its samples cancels the accumulated errors and gives us a good approximation of the noiseless expectation values. It should be noted that our approach requires more gate applications on the quantum processor than with the gate-wise scheme, since there each sample from the mitigation quasiprobability distribution can be absorbed into the original circuit, whereas our approach increases the circuit depth. We take this into account by imposing the same noise on the denoiser. Furthermore, within our scheme, the dimensionality of the quasiprobabilistic mitigating ensemble can be controlled, in contrast to the gate-wise approach where it is equal to the gate count. To facilitate the stochastic interpretation we parameterize each two-qubit denoiser channel G i as a sum of CPTP maps, such that we can sample the terms in this sum and execute the sampled gate on the quantum processor. Concretely, we use a trace preserv-ing sum of a unitary and a non-unitary channel. For the unitary part we take a two-qubit unitary channel U( φ i ) = U ( φ i ) ⊗ U * ( φ i ), with U ( φ i ) a two-qubit unitary gate parameterized by φ i . For this we take the two-qubit ZZ rotation exp(−iα(σ z ⊗ σ z )) with angle α, which can be obtained from native gates on current hardware , and dress it with four general one-qubit unitaries, only two of which are independent if we want a circuit that is space inversion symmetric around every bond. The resulting gate has 7 real parameters φ i . For the non-unitary part, which is essential because D has to cancel the non-unitary accumulated noise to obtain the noiseless unitary circuit, we use a general onequbit measurement followed by conditional preparation channel M( , with V a general one-qubit unitary and each κ i a 3-dimensional vector, resulting in a real 9-dimensional ζ i ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the results from Fig. , what can be inferred about the relationship between the sampling overhead γ and the unitary channels in the denoisers?",
    "choices": [
      "A) As the number of unitary channels M increases, the sampling overhead γ remains relatively constant across different times t.",
      "B) The sampling overhead γ is directly proportional to the number of unitary channels M, and this relationship holds true for all values of M and t.",
      "C) The sampling overhead γ is inversely proportional to the number of unitary channels M, and this relationship is only observed for M = 2 and M = 8.",
      "D) The sampling overhead γ is influenced by the fluctuations in the α distributions, which in turn affect the difficulty in finding optimal deep denoisers."
    ],
    "correct_answer": "D",
    "documentation": [
      "The results are shown in Fig. , which contains histograms for the channel count N G versus α. The histograms are stacked, with the lightest color corresponding to the angles of the denoiser at t = 0.5 and the darkest at t = 5. The top four panels are for a denoiser with M = 2 and the bottom four with M = 8. We consider M trot = 8, 16, 32, 64. We see that in both cases the distribution widens upon increasing M trot , indicating that the unitary channels start deviating more from the identity. Moreover, while the M = 2 denoisers in all cases except M trot = 64 have ZZ contributions close to the identity, this is clearly not the case for M = 8. For simplicity, we did not focus on obtaining denoisers with the smallest sampling overhead γ, which is required to minimize the sign problem and hence ease the sampling of mitigated quantities. Instead, we let the optimization freely choose the η i in the denoiser parameterization, as defined in the main text. In Fig. we show the sampling overhead of the denoisers from Fig. of the main text. We see that for M = 1 and M = 2 the sampling overhead is relatively small and uniform across the different t, whereas for M > 2 the optimization sometimes yields a denoiser with large γ and other times with small γ. This could be related to the difference in α distributions from Fig. . The large fluctuations of γ appears to stem from the difficulty in finding optimal deep denoisers, and our optimization procedure likely only finds a local minimum in these cases. Here C(t) is the Trotter supercircuit for time t. In Fig. we show Z dw for the circuits from Fig."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the relationship between John Coakley Lettsom's medical practice and his involvement in the dissection of human remains, based on the information provided about his correspondence and the description of the buildings at 27 Craven Street?",
    "choices": [
      "A) Lettsom's medical practice was focused on obstetrics, and he used the buildings at 27 Craven Street as a site for lectures on the art of making preparations for childbirth.",
      "B) Lettsom's involvement in the dissection of human remains was a result of his collaboration with John Leake, who occupied the building at 35 (No. 26 in Franklin's time) and advertised lectures on anatomy.",
      "C) The buildings at 27 Craven Street were used for both domestic accommodation and as sites for lecturing and dissection, suggesting that Lettsom's medical practice involved the examination of human remains.",
      "D) Lettsom's correspondence with Benjamin Franklin and other individuals suggests that he was involved in the dissemination of anatomical knowledge, but there is no evidence to suggest that he used the buildings at 27 Craven Street for dissection."
    ],
    "correct_answer": "C",
    "documentation": [
      "http://babel.hathitrust.org/cgi/pt?id=wu.89072985302;view=1up;seq=4;size=150 [33] The Journal of the Society of Arts, Vol. LXII, No. 3,183, (Nov. 21, 1913): 18.\nhttp://babel.hathitrust.org/cgi/pt?id=mdp.39015058422968;view=1up;seq=26\n[36] Allen, “Dear and Serviceable,” 263-264. [37] Papers of Benjamin Franklin, 19:20. [38] Thomas Joseph Pettigrew, F. L. S., Memoirs of the Life and Writings of the Late John Coakley Lettsom With a Selection From His Correspondence, Vol. I, (London: Nichols, Son, and Bentley, 1817), 144 of Correspondence. [39] Papers of Benjamin Franklin, 19:321b. [40] Ibid., 19:314. [41] Ibid., 19:353a. [43] Simon David John Chaplin, John Hunter and the ‘museum oeconomy’, 1750-1800, Department of History, King’s College London. Thesis submitted for the degree of Doctor of Philosophy of the University of London., 202. “Following Falconar’s death [1778] the lease [27 Craven Street] was advertised, and the buildings were described as:\nA genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses…consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences. (Daily Advertiser, 27 August 1778)” [44] Simon Chaplin, “Dissection and Display in Eighteenth-Century London,” in Anatomical Dissection in Enlightenment England and Beyond: Autopsy, Pathology and Display, ed. Dr. Piers Mitchell, (Burlington: Ashgate Publishing Company, 2012), 108. “Given that a nearby building at 35 [ No. 26 in Franklin’s time] was occupied by the man-midwife John Leake, who advertised lectures – including lessons in the art of making preparations – at his ‘theatre’ between 1764 and 1788, it is possible that some facilities were shared. In both cases, however, the buildings [Leake’s residence at No. 26 and Hewson’s residence next door at 27] served a dual function as domestic accommodation and as sites for lecturing and dissection.” [45] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xviii."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the Minnesota Twins' decision to switch their flagship radio station from WCCO to KSTP in 2006?",
    "choices": [
      "A) KSTP's stronger signal in the Twin Cities market",
      "B) The Twins' desire to expand their radio network and reach a wider audience",
      "C) The need for a more directional broadcast to improve signal quality in rural areas",
      "D) The Twins' affiliation with the University of Minnesota's athletic department, which was already broadcasting on KSTP"
    ],
    "correct_answer": "C",
    "documentation": [
      "Station management cited the economic toll of the coronavirus for the changes. Sports broadcasting continues, primarily composed of ESPN radio network broadcasts. Sports Teams\n\nKSTP-AM served as the radio flagship for the Minnesota Vikings football team from 1970 to 1975. On August 1, 2006, the station announced that it would be the new flagship station for the Minnesota Twins baseball team, effective with the start of the 2007 season. The Twins had been on rival WCCO since arriving in Minnesota in 1961. KSTP served as the flagship for the Twins until the end of the 2012 season, when games moved to 96.3 KTWN-FM (now KMWA). The Twins have since returned to WCCO 830. The switch to a fairly weak FM station caused dissent among some listeners, particularly in communities that had trouble picking up KSTP 1500. Although KSTP is the state's second most powerful AM station, it must operate directionally at night, delivering a reduced signal to parts of the market. WCCO, by comparison, offers a signal with a wider coverage area during the day than KSTP does, with WCCO's non-directional 50,000 watt signal. In response, the Twins have expanded the number of affiliates. On March 9, 2011, KSTP announced it would be the new flagship for the University of Minnesota Golden Gophers men's and women's basketball and men's ice hockey, ending a 68-year run on WCCO. The rights have since moved to KFXN-FM, which already aired Gopher football. On March 2, 2017, KSTP announced it would be the first radio broadcaster for Minnesota United FC. The move brings live soccer action to 1500 AM. Previous logos\n\nReferences\n\nExternal links\nKSTP website\n\nFCC History Cards for KSTP (covering 1928-1980)\nRadiotapes.com Historic Minneapolis/St. Paul airchecks dating back to 1924 including KSTP and other Twin Cities radio stations. Rick Burnett's TwinCitiesRadioAirchecks.com has additional airchecks of KSTP and other Twin Cities radio stations from the '60s and '70s, including Chuck Knapp's 2nd show on KSTP. Hubbard Broadcasting\nESPN Radio stations\nPeabody Award winners\nRadio stations in Minneapolis–Saint Paul\nRadio stations established in 1925\n1925 establishments in Minnesota\nMinnesota Kicks\nSports radio stations in the United States\nClear-channel radio stations"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The developers of the game have made several changes to the Battlerealm, including the addition of new Health Potions and Revive Potions. However, these changes have also led to some unintended consequences. What is the result of the daily loyalty limit being set to refresh at 08:00UTC for all players?",
    "choices": [
      "A) The daily loyalty limit will be reset at the same time every day, allowing players to plan their loyalty activities accordingly.",
      "B) The daily loyalty limit will be reset at a random time, causing players to lose track of when it will be reset again.",
      "C) The daily loyalty limit will be reset at 08:00UTC every day, but players will not be notified when it is about to reset, leading to confusion and missed opportunities.",
      "D) The daily loyalty limit will be reset at 08:00UTC every day, and players will receive a notification when it is about to reset, allowing them to plan their loyalty activities accordingly."
    ],
    "correct_answer": "C",
    "documentation": [
      "Fixed a bug with Rocket Raccoon’s Dash attack being slower than intended. Added a confirmation popup when spending Units on stamina recharges and unlocking arenas. Regeneration no longer displays green Health values if you’re at full Health. Several new improvements to how status effects are displayed. AI opponents are no longer able to perform one unavoidable attack in response to a Special Attack 3. A new and improved look for all Health Potions in the Battlerealm. All Revive Potions now revive your Champions with +10% more Health. We’re adding so many new Champions, they could form their own Alliance! Some of your favourite heroes of the Marvel Cinematic Universe join The Contest!\nSummoner Mastery is on the horizon! Masteries provide beneficial effects for your Champions. Access Masteries through your Summoner Profile. Earn Mastery Points when you level up. Choose your Masteries wisely and strategically customize your benefits. Recover your points to try a new specialization as often as you’d like. Keep an eye on in-game messaging for more information. The daily loyalty limit has been set to refresh at 08:00UTC for all players. A timer has been added to show when the daily loyalty limit resets. Loyalty balance is now displayed in the Alliance menus. Ask for Versus help with a single tap on the ‘Help’ icon in Team Select. New Alliance Events are coming very soon! Work together with your Alliance to complete objectives and receive rewards! Muster your might, Alliance Arenas will soon open their gates! Competing in Alliance Arenas shares your points across your whole Alliance; work together to reach milestones and top ranks! Work together to amass a huge score, and defeat your competition in classic Arena combat! No slackers here either - if you don’t contribute to win the competition, you’re not eligible for the goods! All social features (Chat, Mail, and Friends) can now be accessed through the new Social Hub. Search for and add friends, and send private messages to Summoners on your Friends List."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The conflict optimizer's performance on geometric graphs is impressive, but what can be inferred about its performance on random graphs, given that it works extremely poorly on these graphs?",
    "choices": [
      "A) The conflict optimizer's performance on random graphs is comparable to its performance on geometric graphs.",
      "B) The conflict optimizer's performance on random graphs is likely to be worse than its performance on geometric graphs.",
      "C) The conflict optimizer's performance on random graphs is likely to be similar to its performance on geometric graphs, but with a larger number of colors.",
      "D) The conflict optimizer's performance on random graphs is likely to be better than its performance on geometric graphs, due to the optimizer's ability to adapt to different graph structures."
    ],
    "correct_answer": "B",
    "documentation": [
      "Notice also that HEAD algorithm provides 283 colors after one hour compared to less than 240 colors for the conflict optimizers. We ran the three implementations on three different servers and compared the results shown in Figure . For each implementation, the x coordinate is the running time in hours, while the y coordinate is the smallest number of colors found at that time. Results on DIMACS Graphs\n\nWe tested the implementation of each team on the DIMACS instances to gauge the performance of the conflict optimizer on other classes of graphs. We compared our results to the best known bounds and to the state of the art coloring algorithms HEAD and QACOL . The time limit for Lasa's algorithms is 1 hour. CWLS is Lasa's conflict optimizer with the neighbourhood presented in TABUCOL , while PWLS is the optimizer with the neighbourhood presented in PARTIALCOL . Gitastrophe algorithm ran 10 minutes after which the number of colors no longer decreases. Shadoks algorithm ran for 1 hour without the BDFS option (results with BDFS are worse). Results are presented in Table . We only kept the difficult DIMACS instances. For the other instances, all the results match the best known bounds. The DIMACS instances had comparatively few edges (on the order of thousands or millions); the largest intersection graphs considered in the CG:SHOP challenge had over 1.5 billion edges. We notice that the conflict optimizer works extremely poorly on random graphs, but it is fast and appears to perform well on geometric graphs (r250.5, r1000.1c, r1000.5, dsjr500.1c and dsjr500.5), matching the best-known results . Interestingly, these geometric graphs are not intersection graphs as in the CG:SHOP challenge, but are generated based on a distance threshold. On the DIMACS graphs, Lasa implementation shows better performance than the other implementations."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyze the passage to determine the likely outcome for Jesse Lazear, the American who contracted yellow fever in Cuba in 1900, based on the information provided about the disease's progression and the experiment described by Lazear.",
    "choices": [
      "A) Given that Lazear's logbook entry on September 13 described him as \"Guinea Pig No. 1\" and that he had been bitten by a mosquito that fed on yellow fever cases, it is likely that Lazear would have developed yellow fever and died.",
      "B) The fact that Lazear's logbook entry on September 13 mentioned a specific mosquito that fed on yellow fever cases suggests that he may have been able to avoid contracting the disease, as the mosquito was already infected.",
      "C) The passage states that about 80% of those with clinical disease recover at the end of the remission stage, which suggests that Lazear may have had a good chance of recovery if he had not developed the toxic stage of the disease.",
      "D) The description of the toxic stage of yellow fever, including symptoms such as black vomit, diarrhea, and failure of the kidneys, liver, and heart, suggests that Lazear's condition would have been severe and likely fatal."
    ],
    "correct_answer": "D",
    "documentation": [
      "But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them, considering all the factors involved. And no one can have 100% certainty that he has made the right decision, but that's life. But if you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito and getting, often dying, of yellow fever. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice. At the end of this stage there is a remission of two or three days. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the toxic stage dies, even now, even with the best of hospital care. The Faget's sign can also occur at the end of the first stage. You asked specifically about the symptoms of the Americans on Dr. Reed's team who got yellow fever in Cuba in 1900. I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. \"In his logbook, Lazear wrote an unusual entry on September 13. In all cases before those, page after page of records, Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. A one-line entry with a name and a date. On that day, however, in his elegant hand, Lazear did not write the soldier's name, but instead wrote 'Guinea Pig No. 1.' He went on to write that this guinea pig had been bitten by a mosquito that developed from an egg laid by a mosquito that developed from an egg laid by a mosquito that fed on a number of yellow fever cases: Suarez, Hernández, De Long, Ferández."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary reason for the increased expression of genes involved in the Integrated Stress Response (ISR) pathway in C2C12 cells treated with L-Histidinol, as compared to HeLa and HepG2 cells?",
    "choices": [
      "A) The presence of a specific transcription factor that is activated in C2C12 cells in response to L-Histidinol treatment",
      "B) The increased expression of genes involved in the mTOR pathway in C2C12 cells, which leads to an increase in ISR pathway activity",
      "C) The depletion of Met/Cys in C2C12 cells, which triggers the ISR pathway and leads to increased expression of ISR-related genes",
      "D) The increased expression of genes involved in the unfolded protein response (UPR) in C2C12 cells, which is a downstream effect of the ISR pathway"
    ],
    "correct_answer": "C",
    "documentation": [
      "Single AAs, Glucose, and NaHCO3 were from Sigma. Further details and amounts utilized are indicated in S1 Table. All media were supplemented with 10% dialyzed FBS (Invitrogen), 100 U/ml penicillin G (Invitrogen), 100 mg/ml streptomycin (Invitrogen), and G418 as required. HBSS was from Invitrogen. Cells were seeded at 10–30% of confluency; cells to be starved for 48 h were plated 2–3 times more confluent compared to the control. The following day, cells were washed and cultured in the appropriate medium, with or without EAA, for 24–48 h.\nL-Histidinol (HisOH), PP242, Integrated Stress Response Inhibitor (ISRIB), SP600125, Cycloheximide (CHX) were from Sigma; Salubrinal was from Tocris Bioscience; U0126 was from Promega. Drugs were used at the following final concentrations: HisOH at 4–16 mM; PP242 at 1–3 μM; ISRIB at 100 nM; SP600125 at 20 μM in HepG2 cells and 50 μM in HeLa cells; Cycloheximide (CHX) at 50 ug/ml in HepG2 cells and 100 ug/ml in HeLa cells; Salubrinal at 75 μM; U0126 at 50 μM. Vehicle was used as mock control. Treatments with drugs to be tested for their ability to inhibit transgene reactivation (ISRIB, SP600125 and U0126) were initiated 1h before the subsequent addition of L-Histidinol (ISRIB) or the subsequent depletion of Met/Cys (SP600125 and U0126). Total RNA was purified using the RNeasy Mini kit (Qiagen), according to manufacturer’s instructions. RNA concentration was determined by Nanodrop 8000 Spectrophotometer (Thermo Scientific). Equal amount (1 μg) of RNA from HeLa, HepG2 and C2C12 cells was reverse transcribed using the SuperScript First-Strand Synthesis System for RT-PCR (Invitrogen) using oligo-dT as primers, and diluted to 5 ng/μl. The cDNA (2 μl) was amplified by real-time PCR using SYBR green Master Mix on a Light Cycler 480 (Roche), according to manufacturer’s instructions. The thermal cycling conditions were: 1 cycle at 95°C for 5 min, followed by 40–45 cycles at 95° for 20 sec, 56° for 20 sec and 72° for 20 sec. The sequences, efficiencies and annealing temperatures of the primers are provided in S2 Table."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the consequences of less effective Andreev coupling in a T-shaped geometry setup, what is the expected outcome on the critical temperature $T^*$ in the presence of a strong suppression of the Cooper Andreev Reflection (CAR) exchange?",
    "choices": [
      "A) The critical temperature $T^*$ remains unchanged, as the suppression of CAR exchange does not affect the underlying superconducting gap $\\GS{}$.",
      "B) The critical temperature $T^*$ increases, as the suppression of CAR exchange leads to a stronger SC coupling necessary to observe the second stage of Kondo screening.",
      "C) The critical temperature $T^*$ decreases, as the suppression of CAR exchange reduces the residual low-temperature conductance at particle-hole symmetric case, leading to a narrowing of the local spectral density dip.",
      "D) The critical temperature $T^*$ remains unchanged, as the suppression of CAR exchange does not affect the underlying superconducting gap $\\GS{}$ and the related local spectral density dip."
    ],
    "correct_answer": "C",
    "documentation": [
      "This does not have to be the case in real setups, yet relaxing this assumption does not \nintroduce qualitative changes. Nevertheless, the model cannot be extended to inter-dot \ndistances much larger than the coherence length, where $\\GS{\\rm X}\\to 0$.\n\nTo quantitatively analyze the consequences of less effective Andreev coupling we define the \nCAR efficiency as $\\mathcal{C} \\equiv \\GS{\\rm X} / \\sqrt{\\GS{1}\\GS{2}}$ and analyze $\\mathcal{C} < 1$\nin the wide range of $\\GS{1}=\\GS{2}=\\GS{}$ and other parameters corresponding to \\fig{3}. The results are presented in \\fig{C}. Clearly, decreasing $\\mathcal{C}$ from $\\mathcal{C}=1$ causes diminishing of $\\GS{\\rm X}$, and consequently of CAR \nexchange. For a change as small as $\\mathcal{C}=0.9$, the consequences reduce to some shift of the \nconventional Kondo regime, compare \\fig{C}(a) with \\fig{3}. Stronger suppression of CAR may, \nhowever, increase the SC coupling necessary to observe the second stage of Kondo screening caused\nby CAR outside the experimentally achievable range, see \\fig{C}(b). Moreover, the reduced $T^*$\nleads to narrowing of the related local spectral density dip, while the\nincreased critical $\\GS{}$ necessary for the observation of the second stage of screening leads to the\nshallowing of the dip. This is visible especially in the inset in \\fig{C}(b). \\section{Conclusions}\n\\label{sec:conclusions}\n\nThe CAR exchange mechanism is present in any system comprising at least\ntwo QDs or magnetic impurities coupled to the same superconducting contact\nin a way allowing for crossed Andreev reflections. In the considered setup, comprised of two quantum dots in a T-shaped geometry \nwith respect to normal leads and proximized by superconductor,\nit leads to the two-stage Kondo\nscreening even in the absence of other exchange mechanisms. This CAR induced exchange screening is characterized by a residual \nlow-temperature conductance at particle-hole symmetric case. We have also shown that the competition between CAR exchange and RKKY\ninteraction may result in completely different Kondo screening scenarios."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the numbering system of Craven Street based on the Westminster Rate Books during Franklin's residence?",
    "choices": [
      "A) The numbering system was based on the distance from the northernmost house on the west side of the street.",
      "B) The numbering system was based on the list of residents in the Rate Books, with numbers increasing counter-clockwise down the west side and up the east side.",
      "C) The numbering system was based on the ownership of the houses, with the first resident listed under \"Craven Street\" being number one.",
      "D) The numbering system was based on the proximity to Court Street (later Craven Court, now Craven Passage) on the east side of the street."
    ],
    "correct_answer": "B",
    "documentation": [
      "Few of them were rated at more than a few shillings and many of them were unoccupied.”[1] The landowner, William, 5th Baron Craven, desiring to increase the profitability of his assets, tore down the derelict structures on Spur Alley around 1730 and leased the newly established lots to builders. By 1735, twenty brick houses in the Georgian style had been built on the west side and sixteen on the east side of the way now called Craven Street.[2] Figure 2. Craven Street 1746. (John Rocque London, Westminster and Southwark, First Edition 1746, Motco Enterprises Limited, motco.com)\nLetters to Franklin during his residence with Mrs. Margaret Stevenson, his landlady on Craven Street, were addressed rather vaguely; “Craven Street/Strand”, “Mrs. Stevensons in Craven Street”, or “Benjamin Franklin Esqr.” are but a few examples. Letters from Franklin referenced “London,” or sometimes “Cravenstreet,” but never included a number. Despite the absence of numbered addresses in Franklin’s correspondence, there was a sense of one’s place in the neighborhood based on entries in the Westminster Rate Books (tax assessments). The Rate Books did not list house numbers during Franklin’s time there, but they did list the residents of Craven Street in a particular order that became the default numbering system for the street. Number one was associated with the first resident listed under “Craven Street” in the Rate Books and was the northernmost house on the west side of the street. The numbers increased counter-clockwise down the west side and up the east side in accordance with the list of residents. In 1748, the first year of Margaret Stevenson’s (Stevens in the Rate Books for that year) residence on Craven Street, she is listed as the twenty-seventh resident, the second house north of Court Street (later Craven Court, now Craven Passage) on the east side of the street.[3]\nIn 1766, Parliament passed the London Paving and Lighting Act (6 Geo. 3 c. 26), “An act for the better paving, cleansing, and enlightening, the city of London, and the liberties thereof; and for preventing obstructions and annoyances within the same; and for other purposes therein mentioned."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a potential benefit of using newborn sibling umbilical cord blood for bone marrow transplantation in individuals with beta thalassemia?",
    "choices": [
      "A) Reduced risk of severe anemia due to the theoretical risk of certain medications and exposures.",
      "B) Improved HLA matching, which can increase the chances of a successful transplant.",
      "C) The ability to replace only a percentage of the affected individual's bone marrow, reducing the need for a full transplant.",
      "D) The potential to increase the amount of normal hemoglobin the body is able to make through gene therapy techniques."
    ],
    "correct_answer": "B",
    "documentation": [
      "The risk for specific individuals depends on current health status, age, and other factors. Because of the risks involved and the fact that beta thalassemia is a treatable condition, transplant physicians require a brother or sister donor who has an identically matched tissue type, called HLA type. HLA type refers to the unique set of proteins present on each individual's cells, which allows the immune system to recognize \"self\" from \"foreign.\" HLA type is genetically determined, so there is a 25% chance for two siblings to be a match. Transplant physicians and researchers are also investigating ways to improve the safety and effectiveness of bone marrow transplantation. Using newborn sibling umbilical cord blood—the blood from the placenta that is otherwise discarded after birth but contains cells that can go on to make bone marrow—seems to provide a safer and perhaps more effective source of donor cells. Donors and recipients may not have to be perfect HLA matches for a successful transplant using cord blood cells. Trials are also underway to determine the effectiveness of \"partial transplants,\" in which a safer transplant procedure is used to replace only a percentage of the affected individual's bone marrow. Other possible treatments on the horizon may include gene therapy techniques aimed at increasing the amount of normal hemoglobin the body is able to make. Hemoglobin H disease is a relatively mild form of thalassemia that may go unrecognized. It is not generally considered a condition that will reduce one's life expectancy. Education is an important part of managing the health of an individual with hemoglobin H disease. It is important to be able to recognize the signs of severe anemia that require medical attention. It is also important to be aware of the medications, chemicals, and other exposures to avoid due to the theoretical risk they pose of causing a severe anemia event. When severe anemia occurs, it is treated with blood transfusion therapy. For individuals with hemoglobin H disease, this is rarely required."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The concept of determinism suggests that every event, including human decisions, is the inevitable result of prior causes. However, some argue that indeterminism, or the idea that there is more than one possible outcome from a given state, provides a more nuanced understanding of human agency. According to neuroscientist Peter Tse, what is the most reasonable approach to take in a fully deterministic universe?",
    "choices": [
      "A) Attempt to make the best impact possible, regardless of the number of possible outcomes, as our actions still have consequences.",
      "B) Believe that indeterminism is true and that our choices are not entirely determined by prior causes.",
      "C) Assume that the universe is fundamentally probabilistic, with multiple possible outcomes, but still strive to make the best impact possible.",
      "D) Reject the idea of determinism and instead focus on individual freedom and moral responsibility."
    ],
    "correct_answer": "A",
    "documentation": [
      "(It should be noted, in relation to such models, that even if this is how our choices are made, our choice to choose one of these “alternative possibilities” will still be caused by prior causes that are ultimately completely beyond our own control. Nothing changes this fact, again because decision-making is the product of complex physical processes; it is not an uncaused event.) It is generally unclear what the purpose of such models is. Are they a hypotheses we should test? They do not seem to be. Generally, these models most of all seem like an attempt to make the world fit our preconceived intuitions, which most of all resembles pseudoscience. Fortunately, there is plenty of relief available to the libertarians and other people who have this fear, and it does not involve any unscientific models – neither two-stage, three-stage, nor any other number of stages. The source of this relief is the simple earlier-mentioned fact that we can never know whether there is just one or infinitely many possible outcomes from the present state of the universe. This simple fact gives us all the relief we could ask for, because it reveals that there is no reason to be sure that there is just one possible outcome from the present state of the universe. And, to repeat an important point, we are then left with the conclusion that the only reasonable thing to do is to try to make the best impact we can in the world, which is true no matter whether there is just one possible outcome from the present state of the universe or not, since our actions still have consequences and therefore still matter even in a fully deterministic universe. Some, especially libertarians, might want to object to the claim that we can never know whether determinism is true or not, and even claim that we in fact now know, or at least have good reasons to believe, that indeterminism is true. Here is neuroscientist Peter Tse expressing something along those lines: “Henceforth, I will accept the weight of evidence from modern physics, and assume ontological indeterminism to be the case.”"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the likely outcome if the Libyan delegation fails to respond to the US suggestions by midday tomorrow?",
    "choices": [
      "A) The US will immediately launch a military operation against Libya.",
      "B) The US will continue to engage in diplomatic efforts, including back-channel meetings, to find a peaceful resolution.",
      "C) The US will withdraw its support for the Libyan leader and allow the international community to take over.",
      "D) The US will impose economic sanctions on Libya to pressure them into making concessions."
    ],
    "correct_answer": "B",
    "documentation": [
      "And I said it's worth me coming over to support the administration and to try to let the leader know face to face that this is facing -- it's very grave timing in the situation and they have to have some movement fairly quickly or they're not going to be happy with the -- with the alternatives. BLITZER: What's taking so long? Why haven't you been able to meet with Gadhafi yet? WELDON: Well, it -- that's not unusual. I mean all three of the delegation trips that I led here in 2003 and 2004 -- or, actually, 2004 and 2005 -- they always make you wait until 30 minutes before the meeting and then you go. And some of those meetings were at 10:00 at night, some were at 5:00 in the afternoon. As you know from the excellent reporting being done by your folks here, there's a lot of security concerns, and they are very concerned where Gadhafi is at any given moment. That's one of the issues, but we have been making ourselves available. We have been doing a lot of back-channel meetings with friends and associates that I have here, and we have met with the chief of staff and one of the sons, and today with the prime minister, a very lengthy meeting for two hours. So, we're going to give them until tomorrow. We're not going to stay beyond that. And we have given them some suggestions, and we expect a response by midday tomorrow. And if we don't, we will done exit conversation with your people and let you know our feelings.\nBLITZER: What's the major headline that you got out of these meetings with other leaders? I take it you met with Saif Al-Islam Gadhafi, one of the sons of Moammar Gadhafi. What are they saying to you? WELDON: Well, we actually didn't meet with Saif. I have met with Saif probably 10 times over the past seven years, both in America and here in Libya. I have not yet met with Saif. I have offered, if he is available. I have met with Saadi. And the general thrust is obviously that they want peace and they want to find a way out of this. But as I have explained to them, there's certain things that have to be done according to our president and our secretary of state, who I'm here to support."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary challenge in implementing high-performance error mitigation schemes for quantum computing, as identified by the authors?",
    "choices": [
      "A) The need for a large number of samples to resolve the sign problem, which can be mitigated by increasing the number of terms in the denoiser or its depth.",
      "B) The difficulty in finding optimal parameterizations that minimize the sampling overhead, which is crucial for making the approach scalable.",
      "C) The requirement for a large number of gates in each denoiser sample to control the sign problem, which allows for tunability of the number of gates.",
      "D) The challenge of representing the inverse of a physical noise channel as a positive sum over CPTP maps, which is unphysical and cannot be represented as such."
    ],
    "correct_answer": "B",
    "documentation": [
      "All gates are affected by two-qubit depolarizing noise with p = 0.01. The non-denoised results are labelled with M = 0, and the noiseless values with p = 0. where sgn(η g ) is the sign of the sampled coefficient of the gth channel. γ = 1 means that all signs are positive. Observables Ô p=0 for the noiseless circuit are then approximated by resampling the observables from the denoiser ensemble\nwhere γ = N G g=1 γ g is the overall sampling overhead, with γ g the overhead of the gth gate. Clearly, a large γ implies a large variance of Ô p=0 for a given number of samples, with accurate estimation requiring the cancellation of large signed terms. The number of samples required to resolve this cancellation of signs is bounded by Hoeffding's inequality, which states that a sufficient number of samples to estimate Ô p=0 with error δ at probability 1 − ω is bounded by (2γ 2 /δ 2 ) ln(2/ω) . Since γ scales exponentially in γ g , it is clear that a denoiser with large M and γ 1 will require many samples. We observed that decompositions with γ > 1 are crucial for an accurate denoiser. Restricting to γ = 1 leads to large infidelity and no improvement upon increasing the number of terms in or the depth M of the denoiser. Simply put, probabilistic error cancellation of gate noise introduces a sign problem and it is crucial to find optimal parameterizations (1) which minimize γ to make the approach scalable. This issue arises in all high performance error mitigation schemes , because the inverse of a physical noise channel is unphysical and cannot be represented as a positive sum over CPTP maps. This is clearly visible in the spectra of the denoiser, which lies outside the unit circle (cf. Fig. ). This makes the tunability of the number of gates in each denoiser sample a crucial ingredient, which allows control over the sign problem, because we can freely choose the η i in . For the parametrization (1) of denoiser channels, we try to find a set of parameters for error mitigation by minimizing the normalized Frobenius distance between the noiseless and denoised supercircuits\nwhich bounds the distance of output density matrices and becomes zero for perfect denoising."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the role of astrocytes in the central nervous system based on the isolation and culture of mouse cortical astrocytes?",
    "choices": [
      "A) Astrocytes play a crucial role in the development of dendritic spines in the brain, and their isolation and culture can provide insights into their function in vivo.",
      "B) The use of cultured astrocytes can help to understand the molecular and functional characteristics of astrocytes, but it may not provide direct evidence of their role in dendritic spine morphologies.",
      "C) Astrocytes are not essential for the development of dendritic spines, and their isolation and culture can be used to study the effects of mutations or pharmacological treatments on dendritic spine morphologies.",
      "D) The isolation and culture of mouse cortical astrocytes can provide a model system to study the biological functions of astrocytes in detail, including their role in neuronal regeneration."
    ],
    "correct_answer": "D",
    "documentation": [
      "In order to study the potential role of these proteins in controlling dendritic spine morphologies/number, the use of cultured cortical neurons offers several advantages. Firstly, this system allows for high-resolution imaging of dendritic spines in fixed cells as well as time-lapse imaging of live cells. Secondly, this in vitro system allows for easy manipulation of protein function by expression of mutant proteins, knockdown by shRNA constructs, or pharmacological treatments. These techniques allow researchers to begin to dissect the role of disease-associated proteins and to predict how mutations of these proteins may function in vivo. Play ButtonIsolation and Culture of Mouse Cortical AstrocytesAuthors: Sebastian Schildge, Christian Bohrer, Kristina Beck, Christian Schachtrup. Institutions: University of Freiburg , University of Freiburg .Astrocytes are an abundant cell type in the mammalian brain, yet much remains to be learned about their molecular and functional characteristics. In vitro astrocyte cell culture systems can be used to study the biological functions of these glial cells in detail. This video protocol shows how to obtain pure astrocytes by isolation and culture of mixed cortical cells of mouse pups. The method is based on the absence of viable neurons and the separation of astrocytes, oligodendrocytes and microglia, the three main glial cell populations of the central nervous system, in culture. Representative images during the first days of culture demonstrate the presence of a mixed cell population and indicate the timepoint, when astrocytes become confluent and should be separated from microglia and oligodendrocytes. Moreover, we demonstrate purity and astrocytic morphology of cultured astrocytes using immunocytochemical stainings for well established and newly described astrocyte markers. This culture system can be easily used to obtain pure mouse astrocytes and astrocyte-conditioned medium for studying various aspects of astrocyte biology. Neuroscience, Issue 71, Neurobiology, Cellular Biology, Medicine, Molecular Biology, Anatomy, Physiology, brain, mouse, astrocyte culture, astrocyte, fibroblast, fibrinogen, chondroitin sulfate proteoglycan, neuronal regeneration, cell culture, animal model50079Play ButtonImaging Dendritic Spines of Rat Primary Hippocampal Neurons using Structured Illumination MicroscopyAuthors: Marijn Schouten, Giulia M. R. De Luca, Diana K. Alatriste González, Babette E. de Jong, Wendy Timmermans, Hui Xiong, Harm Krugers, Erik M. M. Manders, Carlos P. Fitzsimons."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient is diagnosed with hemophilia A, a disorder caused by a deficiency in factor VIII. The patient's doctor prescribes a treatment that involves administering a prothrombin complex concentrate (PCC) containing factor VIII, as well as factor IX and factor X. However, the patient also has a history of taking a medication called Romiplostim, which is a thrombopoietin receptor agonist. Which of the following statements is true about the patient's treatment?",
    "choices": [
      "A) The patient's treatment will increase the risk of thrombosis due to the presence of factor X in the PCC.",
      "B) The patient's treatment will not affect their bleeding time, as the Romiplostim will counteract the effects of the PCC.",
      "C) The patient's treatment will increase the risk of bleeding, as the Romiplostim will enhance the anticoagulant effects of the PCC.",
      "D) The patient's treatment will not affect their platelet count, as the Romiplostim will only stimulate the production of platelets."
    ],
    "correct_answer": "C",
    "documentation": [
      "doi:10.3181/00379727-37-9668P. ^ Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). \"Vitamin K dependent modifications of glutamic acid residues in prothrombin\". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. doi:10.1073/pnas.71.7.2730. PMC 388542. PMID 4528109. ^ Nelsestuen, G. L.; Zytkovicz, T. H.; Howard, J. B. (Oct 1974). \"The mode of action of vitamin K. Identification of gamma-carboxyglutamic acid as a component of prothrombin\" (PDF). Journal of Biological Chemistry. 249 (19): 6347–6350. PMID 4214105. ^ Magnusson, S.; Sottrup-Jensen, L.; Petersen, T. E.; Morris, H. R.; Dell, A. (Aug 1974). \"Primary structure of the vitamin K-dependent part of prothrombin\". FEBS Letters. 44 (2): 189–193. doi:10.1016/0014-5793(74)80723-4. PMID 4472513. Bibliography[edit]\nRhéaume-Bleue, Kate (2012). Vitamin K2 and the Calcium Paradox. John Wiley & Sons, Canada. ISBN 1-118-06572-7. External links[edit]\n\"Vitamin K: Another Reason to Eat Your Greens\". v\nTPP / ThDP (B1)\nFMN, FAD (B2)\nNAD+, NADH, NADP+, NADPH (B3)\nCoenzyme A (B5)\nPLP / P5P (B6)\nTHFA / H4FA, DHFA / H2FA, MTHF (B9)\nAdoCbl, MeCbl (B12)\nPhylloquinone (K1), Menaquinone (K2)\nnon-vitamins\nCoenzyme B\nHeme / Haem (A, B, C, O)\nMolybdopterin/Molybdenum cofactor\nTHMPT / H4MPT\nFe2+, Fe3+\nvitamins: see vitamins\nAntihemorrhagics (B02)\n(coagulation) Phytomenadione (K1)\nMenadione (K3)\nintrinsic: IX/Nonacog alfa\nVIII/Moroctocog alfa/Turoctocog alfa\nextrinsic: VII/Eptacog alfa\ncommon: X\nII/Thrombin\nI/Fibrinogen\nXIII/Catridecacog\ncombinations: Prothrombin complex concentrate (II, VII, IX, X, protein C and S)\nCarbazochrome\nthrombopoietin receptor agonist (Romiplostim\nEltrombopag) Tetragalacturonic acid hydroxymethylester\nEpinephrine/Adrenalone\namino acids (Aminocaproic acid\nAminomethylbenzoic acid)\nserpins (Aprotinin\nAlfa1 antitrypsin\nCamostat)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The city's plan to implement a new water management system has been met with resistance from the public, with many questioning the cost and effectiveness of the project. What is the underlying concern that has been reinforced by the discussion about the water issue, according to the author?",
    "choices": [
      "A) The lack of transparency in the city's decision-making process",
      "B) The need for more funding to support the project",
      "C) The extent to which elected officials and un-elected administrators can disregard laws and ignore citizen concerns",
      "D) The impact of the project on the local economy"
    ],
    "correct_answer": "C",
    "documentation": [
      "That is a good thing. So we can move the discussion ahead. Once, again this was handled incorrectly by our city custodians from the beginning. And now here we are. The public is not supporting this very expensive, very limited benefit project. As you said, until a plan is developed that the public can support, things don’t look good. All this discussion about the water issue has only reinforced my opinion the issue hasn’t been about water, only how the plan should be paid for. Or more specifically, to what extent do we allow our elected custodians and our un-elected GOD tzar decide which laws they will follow and which laws they will ignore. When the City GOD tzar tell citizens at a council meeting if we don’t agree with the City’s plan, then we should just sue him, and when the City Attorney explains to a citizen at a City Council meeting that she does have to respond to their questions because she does NOT work for them. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job providing direction to their employees and listening to and representing the CITIZENS. The subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyze the impact of Mennonite settlement on the cultural landscape of Kansas, considering the status of Mennonite women in their church and home relationships, as well as the physical and cultural environment of the region. How did the Mennonite settlement influence the social dynamics of women in Kansas, particularly in relation to their roles in the church and at home?",
    "choices": [
      "A) The Mennonite settlement led to an increase in women's participation in church leadership, resulting in a shift towards more egalitarian relationships between men and women in the community.",
      "B) The Mennonite settlement had a neutral impact on the status of women in Kansas, as their cultural practices and values were not significantly different from those of other settlers.",
      "C) The Mennonite settlement contributed to the marginalization of women, as their traditional roles in the church and home were reinforced, limiting their social and economic opportunities.",
      "D) The Mennonite settlement led to the establishment of women's rights organizations in Kansas, which challenged the patriarchal norms of the time."
    ],
    "correct_answer": "C",
    "documentation": [
      "(Download 6.8MB PDF eBook)\n\nMennonite Settlements\n Impact of Mennonite settlement on the cultural landscape of Kansas; Brenda Martin; Kansas State University; 1985/1988. Mennonite settlement : the relationship between the physical and cultural environment; Susan Movle; University of Utah; 1975/1886. Status of Mennonite women in Kansas in their church and home relationships; Eva Harshbarger; Bluffton College; 1925/1945. External links\n\nCounty\n \n McPherson County - Directory of Public Officials\nHistorical\n , from Hatteberg's People'' on KAKE TV news\nMaps\n McPherson County Maps: Current, Historic, KDOT\n Kansas Highway Maps: Current, Historic, KDOT\n Kansas Railroad Maps: Current, 1996, 1915, KDOT and Kansas Historical Society\n\n \nKansas counties\n1867 establishments in Kansas"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider the proof of Corollary \\ref{thing}. Suppose that $\\omega_1 \\le C \\omega_2$ for big $x$, $\\omega_2 \\in L^\\infty$, and $\\nabla \\omega_1(x) \\cdot x \\le 0$ for big $x$. Which of the following statements is a necessary condition for the non-existence of a stable solution to $(G)$?",
    "choices": [
      "A) $\\lim_{R \\rightarrow \\infty} I_G = 0$ for all $t$",
      "B) $\\lim_{R \\rightarrow \\infty} I_L = 0$ for all $t$",
      "C) $\\lim_{R \\rightarrow \\infty} I_G = 0$ for $t < 1$ and $\\lim_{R \\rightarrow \\infty} I_L = 0$ for $t > 1$",
      "D) $\\lim_{R \\rightarrow \\infty} I_G = 0$ for $t > 1$ and $\\lim_{R \\rightarrow \\infty} I_L = 0$ for $t < 1$"
    ],
    "correct_answer": "C",
    "documentation": [
      "So if we assume that $ \\nabla \\omega_1 \\cdot x \\le 0$ for big $x$ then we see that this last term is non-positive and hence we can drop the term. The the proof is as before but now we only require that $ \\lim_{R \\rightarrow \\infty} I_G=0$.\n\n (2). Suppose that $ u >0$ is a stable sub-solution of $(L)$  and so (\\ref{shit}) holds for all $  p - \\sqrt{p(p-1)} <t< p + \\sqrt{p(p-1)}$. Now we wish to use monotonicity to drop the term from (\\ref{shit}) involving the term $ \\nabla \\omega_1 \\cdot \\nabla \\phi$.      $ \\phi$ is chosen the same as in (1)  but here one notes that the co-efficient for this term changes sign at $ t=1$ and hence by restriction $t$ to the appropriate side of 1 (along with the above condition on $t$ and $\\omega_1$) we can drop the last term depending on which monotonicity we have and hence to obtain a contraction we only require that $ \\lim_{R \\rightarrow \\infty} I_L =0$. The result for the non-existence of a stable super-solution is similar be here one restricts $ 0 < t < \\frac{1}{2}$.\n\n\n(3). The proof here is similar to (1) and (2) and we omit the details. \\hfill $\\Box$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\textbf{Proof of Corollary \\ref{thing}.} We suppose  that $ \\omega_1 \\le C  \\omega_2$  for big $ x$, $ \\omega_2 \\in L^\\infty$,  $ \\nabla \\omega_1(x) \\cdot  x \\le 0$ for big $ x$.     \\\\\n(1). Since $ \\nabla \\omega_1 \\cdot x \\le 0$ for big $x$ we can apply Theorem \\ref{mono} to show the non-existence of a stable solution to $(G)$.   Note that  with the above assumptions on $ \\omega_i$ we have that\n\\[ I_G \\le \\frac{C R^N}{R^{4t+2}}.\\]  For $ N \\le 9$  we can take $ 0 <t<2$  but close enough to $2$ so the right hand side goes to zero as $ R \\rightarrow \\infty$.\n\nBoth (2) and (3) also follow directly from applying Theorem \\ref{mono}. Note that one can say more about (2) by taking the multiple cases as listed in Theorem \\ref{mono} but we have choice to leave this to the reader. \\hfill $ \\Box$\n\n\n\\textbf{Proof of Corollary \\ref{po}.} Since we have no monotonicity conditions now we will need both $I$ and $J$ to go to zero to show the non-existence of a stable solution."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for Jenny Shipley's resignation as leader of the National Party in October 2001?",
    "choices": [
      "A) Loss of support from the party caucus due to her handling of the economy",
      "B) Failure to deliver on her campaign promises, particularly in the area of healthcare",
      "C) A personal disagreement with Roger Sowry, her chosen deputy, over party strategy",
      "D) The party's poor performance in the 2002 election, which was seen as a result of her leadership"
    ],
    "correct_answer": "D",
    "documentation": [
      "English had been a supporter of Bolger as leader, but Shipley reappointed him Minister of Health in her new cabinet. English was promoted to Minister of Finance in a reshuffle in January 1999, a position which was at the time subordinate to the Treasurer, Bill Birch. After a few months, the pair switched positions as part of Birch's transition to retirement, with English assuming the senior portfolio. In early interviews, he emphasised his wish to be seen as a pragmatist rather than an ideologue, and said that the initiatives of some of his predecessors (Roger Douglas's \"Rogernomics\" and Ruth Richardson's \"Ruthanasia\") had focused on \"fruitless, theoretical debates\" when \"people just want to see problems solved\". Opposition (1999–2008)\n\nAfter the National Party lost the 1999 election to Helen Clark's Labour Party, English continued on in the shadow cabinet as National's spokesperson for finance. He was elected deputy leader of the party in February 2001, following the resignation of Wyatt Creech, with Gerry Brownlee being his unsuccessful opponent. Leader of the Opposition\nIn October 2001, after months of speculation, Jenny Shipley resigned as leader of the National Party after being told she no longer had the support of the party caucus. English was elected as her replacement unopposed (with Roger Sowry as his deputy), and consequently became Leader of the Opposition. However, he did not openly organise against Shipley, and according to The Southland Times \"there was almost an element of 'aw, shucks, I'll do it then' about Mr English's ascension\". Aged 39 when he was elected, English became the second-youngest leader in the National Party's history, after Jim McLay (who was 38 when elected in 1984). He also became only the third Southlander to lead a major New Zealand political party, after Joseph Ward and Adam Hamilton. However, English failed to improve the party's performance. In the 2002 election, National suffered its worst electoral defeat ever, gaining barely more than twenty percent of the vote."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A patient with familial hypercholesterolemia (FH) undergoes a coronary catheterization to evaluate for ischemic heart disease (IHD). During the procedure, a fractional flow reserve (FFR) test is performed to assess the pressure differences across a coronary artery stenosis. The results of the FFR test indicate a significant pressure drop, suggesting a critical stenosis. However, the patient's high cholesterol levels are not adequately controlled, and they have a history of heart failure. Which of the following is the most likely outcome for this patient?",
    "choices": [
      "A) The patient will require an immediate angioplasty and stenting procedure to relieve the stenosis.",
      "B) The patient will be referred to a cardiologist for further evaluation and management of their high cholesterol levels.",
      "C) The patient's heart failure will worsen due to the increased demand for oxygen by the heart muscle.",
      "D) The patient will be considered for an implantable cardioverter-defibrillator (ICD) to prevent life-threatening arrhythmias."
    ],
    "correct_answer": "A",
    "documentation": [
      "See also: MIBI, Echocardiogram, Nuclear Stress Test. Familial hypercholesterolemia (FH) – A genetic predisposition to dangerously high cholesterol levels. FH is an inherited disorder that can lead to aggressive and premature cardiovascular disease, including problems like heart attacks, strokes, or narrowing of the heart valves. Femoral Artery: a major artery in your groin/upper thigh area, through which a thin catheter is inserted, eventually making its way into the heart during angioplasty to implant a stent; currently the most widely used angioplasty approach in the United States, but many other countries now prefer the Radial Artery access in the wrist. FFR – Fractional Flow Reserve: A test used during coronary catheterization (angiogram) to measure pressure differences across a coronary artery stenosis (narrowing or blockage) defined as as the pressure behind a blockage relative to the pressure before the blockage. HC – High Cholesterol: When fatty deposits build up in your coronary arteries. HCTZ – Hydrochlorothiazide: A drug used to lower blood pressure; it acts by inhibiting the kidneys’ ability to retain water. Used to be called “water pills”. Heart Failure – a chronic progressive condition that affects the pumping power of your heart muscle. Sometimes called Congestive Heart Failure (CHF). Holter Monitor – A portable monitoring device that patients wear for recording heartbeats over a period of 24 hours or more. HTN – Hypertension: High blood pressure, the force of blood pushing against the walls of arteries as it flows through them. Hypokinesia – Decreased heart wall motion during each heartbeat, associated with cardiomyopathy, heart failure, or heart attack. Hypokinesia can involve small areas of the heart (segmental) or entire sections of heart muscle (global). Also called hypokinesis. ICD – Implantable Cardioverter Defibrillator: A surgically implanted electronic device to treat life-threatening heartbeat irregularities. IHD – Ischemic Heart Disease: heart problems caused by narrowing of the coronary arteries, causing a decreased blood supply to the heart muscle."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the article, what is a potential disadvantage for Intel in manufacturing SoCs on nodes that are a step or two behind the state of the art?",
    "choices": [
      "A) The increased cost of manufacturing on state-of-the-art nodes would be offset by the increased performance and efficiency of the 14nm process.",
      "B) The increased complexity of manufacturing on state-of-the-art nodes would lead to higher yields and more reliable products.",
      "C) The increased competition from other manufacturers on state-of-the-art nodes would make it difficult for Intel to maintain its market share.",
      "D) The increased cost of manufacturing on state-of-the-art nodes would be offset by the increased demand for high-performance SoCs in the mobile market."
    ],
    "correct_answer": "C",
    "documentation": [
      "In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art. I think the processing is a bigger advantage than many realize. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. All else being equal a 14nm chip should be significantly faster and more efficient than the same chip at 22nm. Add in the fact that yields increase geometrically - you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you have a very appealing proposition. And then add in the fact that Intel actually has a pretty good graphics stack and IP. My point was that Intel might have a one or two process advantage over the rest of the industry at the cutting edge but that doesn't mean that they can afford to manufacture on those processes for very low margin parts. If the SoC market becomes increasingly commoditised, there isn't going to be the money to justify making them in a state of the art fab. Remember that one of the big selling points of Itanium was that it would make use of process advantages that were effectively paid for by the mainstream x86 market. That didn't quite work out in practice and Itanium processors were often well behind Xeons in process technology. paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary factor contributing to the proliferation of pill mills in Florida during the early 2000s, despite efforts by other states to curb the issue?",
    "choices": [
      "A) The state's large population and urbanization, which created a high demand for pain management services.",
      "B) The lax regulatory environment and lack of oversight by state officials, which allowed rogue doctors to operate with impunity.",
      "C) The widespread availability of online resources and message boards that provided information on how to obtain prescription medications without a legitimate medical need.",
      "D) The influence of Governor Jeb Bush's personal connections to the pharmaceutical industry, which led to a conflict of interest in regulating the industry."
    ],
    "correct_answer": "B",
    "documentation": [
      "Kenneth Hammond didn’t make it back to his Knoxville, Tenn., home. He had a seizure after picking up prescriptions for 540 pills and died in an Ocala gas station parking lot. Keith Konkol didn’t make it back to Tennessee, either. His body was dumped on the side of a remote South Carolina road after he overdosed in the back seat of a car the same day of his clinic visit. He had collected eight prescriptions totaling 720 doses of oxycodone, methadone, Soma and Xanax. Konkol had every reason to believe he would get those prescriptions: In three previous visits to the Plantation clinic, he had picked up prescriptions for 1,890 pills. An estimated 60 percent of her patients were from out of state, a former medical assistant told the DEA. In 2015, Averill pleaded not guilty to eight manslaughter charges. She is awaiting trial in Broward County. Averill was just one doctor at just one clinic. In 2010, the year Averill’s patients overdosed, Florida received applications to open 1,026 more pain clinics. An online message board advising drug users summed it up: “Just go anywhere in South Florida and look for a ‘pain management clinic.’ It shouldn’t be too hard; you can’t swing a dead cat without hitting one.” Complain about anything from a back injury to a hangnail, it advised, “and they’ll set you right up.” By this time, Kentucky had reined in its pill mills. It didn’t matter, Ohio, Delaware, North Carolina, Connecticut acted as well, but other state’s efforts didn’t matter either, Florida continued ignoring the pill mills and rogue doctors feeding the nation’s oxycodone habit, the pills flowed. “There were folks down there, where if I had an opportunity to, get my hands around their throat, I would have wrung their neck,” said Huntington Mayor Steve Williams. On Florida’s inaction he stated, “There was total evidence as to what was happening. It lays at the foot, in my opinion, of the public officials there that allowed it to continue on.” Governor Jeb Bush Backed A Solution\nOne of the first dinners Florida Gov. Jeb Bush hosted after moving into the governor’s mansion in 1999 was a small one."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be said about the TNE strength/intensity/degree of a fluid system from different research perspectives?",
    "choices": [
      "A) It is always increasing, regardless of the perspective.",
      "B) It is always decreasing, regardless of the perspective.",
      "C) The TNE strength can be increasing or decreasing, depending on the perspective.",
      "D) The TNE strength is a fixed value that does not change with perspective."
    ],
    "correct_answer": "C",
    "documentation": [
      "They are all helpful to characterize the TNE strength and describe the TNE behaviors of a fluid system from their perspectives. But it is not enough only relying on these quantities. Besides the above physical quantities describing the TNE behaviors, in DBM modeling, we can also use the non-conservative moments of ( f − f eq ) to characterize the TNE state and extract TNE information from the fluid system. Fundamentally, four TNE quantities can be defined in a firstorder DBM, i.e., ∆ σ * 2 , ∆ σ * 3,1 , ∆ σ * 3 , and ∆ σ * 4,2 . Their definitions can be seen in Table , where v * i = v i − u represents the central velocity and u is the macro flow velocity of the mixture. Physically, ∆ σ * 2 = ∆ σ * 2,αβ e α e β and ∆ σ * 3,1 = ∆ σ * 3,1 e α represent the viscous stress tensor (or non-organized momentum flux, NOMF) and heat flux tensor (or non-organized energy flux, NOEF), respectively. The e α (e β ) is the unit vector in the α (β ) direction. The later two higher-order TNE quantities contain more condensed information. Specifically, and it indicates the flux information of ∆ σ * 2 . To describe the TNE strength of the whole fluid system, some TNE quantities contained more condensed information are also defined, i.e.,\nOther TNE quantities can be defined based on specific requirements. All the independent components of TNE characteristic quantities open a highdimensional phase space, and this space and its subspaces provide an intuitive image for characterizing the TNE state and understanding TNE behaviors . It should be emphasized that: (i) The TNE strength/intensity/degree is the most basic parameter of non-equilibrium flow description; And any definition of non-equilibrium strength/intensity/degree depends on the research perspective. (ii) The physical meaning of D * m,n is the TNE strength of this perspective. (iii) From a certain perspective, the TNE strength is increasing; While from a different perspective, the TNE strength, on the other hand, may be decreasing. It is normal, one of the concrete manifestations of the complexity of non-equilibrium flow behavior."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the passage, what is the significance of Jesus' actions before Andrew, Simon, James, and John follow him?",
    "choices": [
      "A) They demonstrate Jesus' power and authority to convince people to follow him.",
      "B) They show Jesus' willingness to wait for people to come to him, rather than actively seeking them out.",
      "C) They highlight the importance of Jesus' individual relationships with each of the four fishermen, emphasizing that he came specifically to each of them.",
      "D) They illustrate the idea that people must prove their worthiness to follow Jesus by showing him their skills and abilities."
    ],
    "correct_answer": "C",
    "documentation": [
      "This is a story about fishing. This isn’t only a story about four fisherman, or only a story about fishing. It’s also, and perhaps, most importantly, a story about God. If this is only a story about four fisherman who decide to follow Jesus, the pressures on you and me! After all, aren’t we too called to follow Jesus? Called to be his disciples? Wasn’t that the invitation you first heard when you first heard about Jesus? God has called us and we must decide. Jesus wants us all to follow him, to be like him, to walk in his footsteps, to do what he does. Of course this story is about that! And they do it, don’t they? Simon, Andrew, James, John, they do it! They decide and they do follow Jesus, imperfectly at that. Still, it’s a lot of pressure, a lot of responsibility. If life becomes all about what we do for Jesus, something is missing. If this is only a story about fishing, have some of us failed? Is it too late for us? Some of us might not be the best at fishing, not all the great about casting Jesus’ loving net to our brothers and sisters. His net is sometimes, or maybe more than sometimes, a bit more expansive than we might be comfortable with. He calls us to be like him and fish for people, and yet, sometimes we can barely get the net into the water. Perhaps for others, we aren’t even convinced that Jesus would include us in the net at all, no matter how deep into the water he goes. He can really mean me? Would his net really reach me? There’s still more to the story. This is a story about God, who God is, how God acts, what God does. Before Andrew, Simon, James and John follow Jesus, Jesus finds them. Before they follow Jesus, Jesus comes to them! They don’t have to go searching, they have been found. Jesus saw. Jesus spoke. Jesus called. Jesus said, “Come.“ We don’t follow Jesus in order to find him, to prove our worthiness with what we do, or even by showing Jesus how big our nets are. We follow Jesus because he first came to us. He came down to the beach to meet these four fishermen. He came specifically for Simon and for Andrew, for James and for John, for you and me."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the response of a system to a sinusoidal excitation, what can be inferred about the contribution of higher-order responses when the excitation frequency is far from the linear natural frequency?",
    "choices": [
      "A) The higher-order responses will always be significant and cannot be neglected.",
      "B) The higher-order responses will only be significant when the excitation frequency is close to the linear natural frequency.",
      "C) The higher-order responses will be negligible and can be ignored, except in cases where the response nonlinearity is high.",
      "D) The higher-order responses will be the dominant component of the response, and their contributions cannot be approximated by the first-order response."
    ],
    "correct_answer": "C",
    "documentation": [
      "All cases have same amplitudes. The poles of a sinusoidal excitation are λ 1,2 = ±iΩ, and the residues are α 1,2 = ∓iA/2. Numerical values of excitation poles and residues for different cases are listed in Table . Table : Parameter values, poles and residues of the sinusoidal excitation Substituting poles and residues of the excitation, as well as those of the system into Eqs.\n20 and 19, response coefficients β p i ,k corresponding to system poles −a i and response coefficients γ p i ,ℓ corresponding to excitation poles λ ℓ are calculated, respectively. According to Eq. 22, the first three orders of responses for each case in Table are calculated. Figures )-15(a) show the comparison of responses obtained by the proposed method and the fourth-order Runge-Kutta method with ∆t = 10 −4 . For Cases 1 and 2, the first-order responses agree well with the total responses obtained by the Runge-Kutta method, and the higher-order responses only slightly improve the transient parts. For Cases 3-5, the sum of the first three orders of responses is in good agreement with the Runge-Kutta solution. When the response nonlinearity increases, higher-order responses need to be considered. In other words, the proposed method can accurately compute the nonlinear responses by choosing a small number N of Volterra series terms. Figures )-15(b) show the contributions of the three response components for the five cases. In each case, the first-order response is the most dominant component, and the contributions of secondand third-order responses are much less than those of the first-order response. Especially for Cases 1 and 2, whose excitation frequencies are far from the linear natural frequency, second-and thirdorder responses are close to zero. This may be because the QFRF and CFRF approach zero when the frequency is larger than 4 rad/s (see Figs. ). Furthermore, the mean values of the first-order responses are approximately zero, and those of the second-order responses are always smaller than zero, which are the difference frequency components in Eq. 27."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A genetic study found that a new mutation for alpha-thalassemia was discovered for the first time among Iranian patients in 2004. This mutation is a result of a substitution of a single nucleotide, which leads to a structurally altered hemoglobin. The study also found that the mutation is more common in individuals who have a history of malaria infection. What is the likely reason for the increased prevalence of this mutation in individuals who have a history of malaria infection?",
    "choices": [
      "A) The mutation provides protection against malaria infection, which is why it is more common in individuals who have a history of malaria.",
      "B) The mutation is a result of a genetic adaptation to the malaria parasite, which is why it is more common in individuals who have a history of malaria.",
      "C) The mutation is a result of a genetic mutation that occurred in response to the malaria parasite, which is why it is more common in individuals who have a history of malaria.",
      "D) The mutation is a result of a genetic mutation that occurred in response to the malaria parasite, but it does not provide protection against malaria infection, which is why it is more common in individuals who have a history of malaria."
    ],
    "correct_answer": "B",
    "documentation": [
      "Scientists continue to study the causes. For instance, a new mutation for alpha-thalassemia was discovered for the first time among Iranian patients in 2004. BETA-THALASSEMIA. Most individuals have two normal copies of the beta globin gene, which is located on chromosome 11 and makes the beta globin component of normal adult hemoglobin, hemoglobin A. There are approximately 100 genetic mutations that have been described that cause beta thalassemia, designated as either beta0 or beta + mutations. No beta globin is produced with a beta0 mutation, and only a small fraction of the normal amount of beta globin is produced with a beta + mutation. When an individual has one normal beta globin gene and one with a beta thalassemia mutation, he or she is said to carry the beta thalassemia trait. Beta thalassemia trait, like other hemoglobin traits, is protective against malaria infection. Trait status is generally thought not to cause health problems, although some women with beta thalassemia trait may have an increased tendency toward anemia during pregnancy. When two members of a couple carry the beta thalassemia trait, there is a 25% chance that each of their children will inherit beta thalassemia disease by inheriting two beta thalassemia mutations, one from each parent. The clinical severity of the beta thalassemia disease—whether an individual has beta thalassemia intermedia or beta thalassemia major—will depend largely on whether the mutations inherited are beta0 thalassemia or beta + thalassemia mutations. Two beta0 mutations generally lead to beta thalassemia major, and two beta+ thalassemia mutations generally lead to beta thalassemia intermedia. Inheritance of one beta0 and one beta + thalassemia mutation tends to be less predictable. Although relatively uncommon, there are other thalassemia-like mutations that can affect the beta globin gene. Hemoglobin E is the result of a substitution of a single nucleotide. This change results in a structurally altered hemoglobin that is produced in decreased amounts."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The philosopher DragonFly argues that libertarians accept the incompatibility premise that determinism is incompatible with human freedom. However, some critics argue that this premise is based on a flawed understanding of libertarianism. Which of the following statements best captures the essence of DragonFly's argument?",
    "choices": [
      "A) Libertarians believe that free will is absolute and applies to every action, making them morally responsible for all their actions.",
      "B) DragonFly claims that libertarians accept the incompatibility premise because they believe that free will is incompatible with determinism, but this premise is actually based on a misunderstanding of libertarianism.",
      "C) The concept of free will is inherently tied to the idea of a \"self-made\" individual, and DragonFly argues that libertarians' acceptance of free will is incompatible with this idea.",
      "D) DragonFly's argument is that libertarians' acceptance of free will is based on a flawed understanding of the concept of determinism, which is incompatible with human freedom."
    ],
    "correct_answer": "D",
    "documentation": [
      "[/quote]\nDragonFly » April 21st, 2018, 3:57 pm wrote: Yes, as I said, some is indeterminate, so there is no ignoring. Incorrect. You did not say \"some is indeterminate.\" So either you do not write well, cannot understand the logic of your own words, or you make up things as an excuse to attack other people. In fact, this can be identified with a logical fallacy. \"Whatever is indeterminate diminishes our modeling\" means our modeling is diminished IF there is anything indeterminate. If A then B does not allow you affirm A, so by equating these two you have committed a logical fallacy. Furthermore it is amazing how far out on a limb you go to concoct such an attack. You said, \"we cannot know if everything is deterministic,\" which is utterly inconsistent with a clam that \"some is indeterminate,\" because if some is indeterminate then you would know that it is NOT deterministic. DragonFly » April 21st, 2018, 3:57 pm wrote: Total libertarians do claim that they are first cause, self made people at every instant. The philosophers who claim that we have free actions are called libertarians. The radical opposition that libertarians pose to the determinist position is their acceptance of free actions. Libertarians accept the incompatibility premise that holds agents morally responsible for free actions. Incompatibilism maintains that determinism is incompatible with human freedom. Libertarians accept that there are free actions, and in doing so, believe that we are morally responsible for some of our actions, namely, the free ones. The libertarian ONLY claims that we do have free will actions and affirm the incompatibility of determinism with free will. There is no claim here that free will is absolute, inviolable, and applies to every action and thus that people are \"self made at every instance. \"\nThus in the following it is clear you are burning an absurd strawman. DragonFly » April 21st, 2018, 3:57 pm wrote: How does this work? A theory of conscious intentions happening without any underlying physical processes ('you') behind them is the toughest sell of all proposals on the will, so it's no wonder that this 'being free of the will' can't be shown."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the fishing practices of larger vessels compared to smaller vessels based on the information provided?",
    "choices": [
      "A) Larger vessels tend to use bottom longlines more frequently than smaller vessels.",
      "B) Smaller vessels spend more time at sea than larger vessels, but have a higher number of sets per trip.",
      "C) Larger vessels primarily fish in southern Sumbawa and Sumba Islands, while smaller vessels fish in the Java Sea.",
      "D) Smaller vessels tend to employ surface longlines more frequently than larger vessels."
    ],
    "correct_answer": "A",
    "documentation": [
      "These vessels are operated by approximately 150 highly-specialised shark fishers, from Tanjung Luar village and Gili Maringkik, who make up roughly 5% of the local fisher population. The shark industry is more profitable than non-shark fisheries, and shark fishers report high household dependency on shark resources, low occupational diversity, and limited capacity and aspirations to move into other fisheries or industries. Surface and bottom longlines are used as the primary fishing gears to target sharks, with pelagic fish (e.g. Euthynnus spp., Rastrellinger spp.) used as bait. Surface and bottom longlines systematically vary in length, depth deployed, number of sets, number of hooks used, and soak times (Table 2). Gear types are typically associated with certain vessel types, and fishers–captain and crew—tend to exhibit preferences for specific gear types. Shark fishers also use gillnets and troll lines as secondary gears, to catch bait and opportunistically target other species, such as grouper, snapper, skipjack and mackerel tuna. Table 2. Characteristics of surface and bottom longlines. The shark fishing vessels can be divided into two broad categories according to fishing behaviour: larger vessels (≥14 m) with higher horsepower (HP) engines spend more time at sea than smaller vessels (≤12m) (p<0.001), and reach fishing grounds outside of West Nusa Tenggara. These vessels primarily fish in southern Sumbawa and Sumba Islands, however, they also reach as far as eastern Flores, Timor Island, and the Java Sea (Fig 1). Larger, higher HP vessels also tend to employ surface longlines (p<0.001), and since they spend more time at sea, have a higher number of sets per trip than smaller vessels (p<0.001). Smaller vessels (≤12 m) with smaller engines tend to remain in waters around West Nusa Tenggara only, carrying out shorter fishing trips using bottom longlines (Table 3). Table 3. Characterisation of the different fishing vessels used to target sharks in Tanjung Luar. During the study period we recorded shark catch from a total of 595 fishing trips."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The analysis of CPUE (catch per unit effort) for regulated species in Tanjung Luar's targeted shark fishery reveals significant variations in spatial and temporal patterns. Which of the following statements best summarizes the implications of these findings for fisheries management?",
    "choices": [
      "A) The use of standardized CPUE measures is crucial for comparing fishing efforts across different gears and vessel types, as it allows for more accurate assessments of fishing effectiveness.",
      "B) The spatial and temporal patterns of CPUE for regulated species suggest that fisheries management should focus on reducing fishing efforts in areas with lower shark abundance, such as West Nusa Tenggara Province.",
      "C) The positive relationship between engine power and unstandardized CPUE indicates that larger engines are more efficient for catching sharks, and therefore should be encouraged in fisheries management.",
      "D) The significant differences in unstandardized CPUE between January and other months suggest that the west monsoon season is a critical period for shark conservation efforts."
    ],
    "correct_answer": "A",
    "documentation": [
      "The most significant factors influencing the likelihood of catching regulated species were month (January was significantly lower: p<0.001), number of hooks (p<0.001) and engine power (<0.01). Significant factors associated with standardised CPUE of regulated species were number of hooks (p<0.001), fishing gear (<0.001), number of sets (p<0.001), engine power (p<0.01) and month (November and January: p<0.05) (Table 5 and Fig 4). Plots of most significant factors affecting standardised CPUE (number of individuals per 100 hooks per set) of regulated species: a) hook number, b) gear type, c) number of sets. Although Tanjung Luar’s targeted shark fishery is small in scale, considerable numbers of shark are landed, including a large proportion of threatened and regulated species. A key finding is that measures of CPUE, for all sharks and for threatened and regulated species, vary spatially and temporally, and with several aspects of fishing effort including gear type, hook number, engine power and number of sets. Moreover, the relationships between CPUE and fishing behaviour variables are different for different measures of CPUE (CPUE per trip, CPUE per set, CPUE per 100 hooks per set). This highlights the importance of using appropriate standardisation for meaningful comparisons of CPUE across different gears and vessel types, and has important implications for fisheries management. Unstandardised CPUE (individuals per set) was significantly lower in January. This is during the west monsoon season, which is characterised by high rainfall and adverse conditions at sea for fishing. Unstandardised CPUE was also significantly lower in West Nusa Tenggara Province (WNTP) than East Nusa Tenggara Province (ENTP) and other provinces, suggesting a lower abundance of sharks in this area. Engine power had a significant positive influence on unstandardised CPUE, and was also associated with longer trips and more sets, which was likely due to the ability of vessels with larger engines to travel longer distances, over longer time periods, and with higher numbers of sets, to favoured fishing grounds."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "As the government teeters on the brink of a shutdown, what is a potential consequence of the impending crisis for the city of Washington, D.C., which is heavily reliant on federal funding?",
    "choices": [
      "A) The city's infrastructure will be severely impacted, with major roads and highways being closed for an extended period.",
      "B) The city's economy will experience a significant boost, with tourists flocking to the nation's capital to take advantage of the free services.",
      "C) The city's residents will be forced to rely on private contractors to provide essential services, such as trash collection and public transportation.",
      "D) The city's cultural institutions, such as museums and galleries, will be spared from the effects of the shutdown, as they are funded separately from the federal budget."
    ],
    "correct_answer": "C",
    "documentation": [
      "CNN.com - Transcripts\nTensions Boil Over possible government shutdown; New trouble targeting Gadhafi; Libyan Rebels in Panicked Retreat; Should U.S. Recognize the Rebels?; Meeting With Gadhafi; Washington, D.C. to Feel Burden of Shutdown; Religious Leaders Fast to Protests Cuts for Poor\nWOLF BLITZER, HOST: Don, thanks very much. Happening now, the top U.S. general in charge of the military mission in Libya now expressing doubts that the opposition has the manpower to topple Moammar Gadhafi, as deadly new air strikes force rebel fighters into another retreat. This hour, I'll speak with a former Republican Congressman who's in Tripoli right now trying to get Gadhafi to step down. Also, growing outrage across the United States, amidst new signs tomorrow's potential government shutdown may -- repeat may be unavoidable. Why one lawmaker is telling Congress -- and I'm quoting right now -- \"go straight to hell. \"\nAnd possible presidential hopeful, Donald Trump, on a mission to tell President Obama, \"you're fired.\" We're fact checking his controversial investigation into the president's birth. Up first, the political showdown over the budget, as tensions reach a boiling point about 31 hours until impending government shutdown. Just hours from now, President Obama will meet with Republican House speaker, John Boehner, and the Democratic Senate majority leader, Harry Reid, for further negotiations. Those talks scheduled to begin 7:00 p.m. Eastern. Hundreds of thousands of people across the country will be impacted by the shutdown. And we'll be bringing you examples throughout the next two hours. One place it would be felt heavily is right here in Congress' backyard, the city of Washington. Washington, DC -- its spending is tied to the federal budget. And this major metropolitan area could lose millions of dollars while a number of critical services, like trash collection, for example, would be suspended for at least a week. Today, an enraged Eleanor Holmes, the delegate representing Washington, DC, lit into Congress over the stalemate."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "If the government awards licenses for HDTV, which of the following is a likely outcome?",
    "choices": [
      "A) The number of broadcast television stations will decrease, leading to less diversity of points of view.",
      "B) The public will become more engaged and participatory in the media, leading to more informed decision-making.",
      "C) The existing regime of licensed broadcast television will be replaced by a new regime of unlicensed, community-driven media outlets.",
      "D) The proliferation of HDTV will lead to a decrease in the number of information middlemen, such as editors and packagers."
    ],
    "correct_answer": "C",
    "documentation": [
      "I just want things to be able to sort themselves out in a much more equitable fashion. We have this enormous, artificial scarcity today over the means of communication, because the government awards licenses which self-perpetuate. They are about to do the same thing, and give every broadcast television station another license for HDTV. So if you've got a license today, you get a second one; if you don't have one, you get nothing. That is going to be our policy about HDTV. I think it would be a lot better if we had more markets, more choices, and better values. I don't know how to do better values, but we know how to do more choices. So the point is, we'll wind up with some new regime which I don't think that we can particularly predict. I don't think that it is going to be chaotic or anarchic. I think there is something about people as social animals or creatures -- we will create some new forms of social organization. There will be information middlemen; there will be the equivalent of editors and packagers. There will be trusted intermediaries who help organize these new media. If you open it up and equalize things so that everybody can participate, you will get more diversity of points of view, you will get less homogenization. One of the reasons that tons of people have just dropped out, or are in terminal couch-potato-dom is that the sets of choices and the values that come across the tube are not ones that stir the human heart. And people know that. They can't figure out what to do about that, so they sort of fuzz out on drugs and alcohol. I say let's edit TV, which is the electronic drug. Let's do something about that. DAVIES: I like your idea, Mitch. I think it's sweet. (laughter) The problem is that I really worry that the ultimate test of the future is going to be the outcome of the quest, the battle between those who are looking for the sort of vision you've got of the right of the individual, the individual being the producer. And that, probably, is the way we solve our problems on this planet."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary advantage of using a probabilistic error cancellation scheme in the context of second-order Trotter circuits?",
    "choices": [
      "A) It allows for the efficient compression of unitary circuits, reducing the number of gates required for noiseless time evolution.",
      "B) It enables the mitigation of errors in local noise channels, reducing the need for complex noiseless circuit compression.",
      "C) It provides a means to control the sign problem inherent to probabilistic error cancellation, allowing for more efficient optimization of denoiser parameters.",
      "D) It facilitates the use of matrix product operators to formulate the optimization procedure, making it more convenient for practical denoiser design."
    ],
    "correct_answer": "C",
    "documentation": [
      "In the Supplementary Material we show the spectra for a p = 0.036 denoiser, where we observe a clustering of eigenvalues reminiscent of Refs. . There we also investigate the channel entropy of the various supercircuits . Conclusion. -We have introduced a probabilistic error cancellation scheme, where a classically determined denoiser mitigates the accumulated noise of a (generally non-Clifford) local noise channel. The required number of mitigation gates, i.e. the dimensionality of the corresponding quasiprobability distribution, is tunable and the parameterization of the corresponding channels provides control over the sign problem that is inherent to probabilistic error cancellation. We have shown that a denoiser with one layer can already significantly mitigate errors for second-order Trotter circuits with up to 64 layers. This effectiveness of low-depth compressed circuits for denoising, in contrast with the noiseless time evolution operator compression from , can be understood from the non-unitarity of the denoiser channels. In particu-lar, measurements can have non-local effects, since the measurement of a single qubit can reduce some highly entangled state (e.g. a GHZ state) to a product state, whereas in unitary circuits the spreading of correlations forms a light-cone. To optimize a denoiser with convenience at L > 8, the optimization can be formulated in terms of matrix product operators or channels , which is convenient because the circuit calculations leading to the normalized distance and its gradient are easily formulated in terms of tensor contractions and singular value decompositions . This provides one route to a practical denoiser, which is relevant because the targeted noiseless circuit and the accompanying noisy variant in (4) need to be simulated classically, confining the optimization procedure to limited system sizes with an exact treatment or limited entanglement with tensor networks. Nonetheless, we can use e.g. matrix product operators to calculate (4) for some relatively small t, such that the noiseless and denoised supercircuits in (4) have relatively small entanglement, and then stack the final denoised supercircuit on a quantum processor to generate classically intractable states."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that the circulating levels of menaquinone-7 and menaquinone-8, congeners of vitamin K2, increase with age. However, the same study also found that the intake of fermented soybeans, which is a rich source of these congeners, is associated with reduced bone loss in postmenopausal women. Additionally, a review of the literature on vitamin K2 found that the use of warfarin, a medication that requires vitamin K2 to prevent bleeding, is associated with an increased risk of fractures in older adults. Which of the following statements best summarizes the relationship between vitamin K2 intake and bone health in older adults?",
    "choices": [
      "A) Vitamin K2 intake is associated with an increased risk of fractures in older adults.",
      "B) Vitamin K2 intake is associated with reduced bone loss in postmenopausal women, but not in older adults.",
      "C) Vitamin K2 intake is not associated with bone health in older adults, and the relationship between vitamin K2 and bone loss is complex and influenced by multiple factors.",
      "D) Vitamin K2 intake is associated with reduced bone loss in older adults, and the use of warfarin is not a significant risk factor for fractures."
    ],
    "correct_answer": "D",
    "documentation": [
      "\"Age-related changes in the circulating levels of congeners of vitamin K2, menaquinone-7 and menaquinone-8\". Clinical Science. 78 (1): 63–66. PMID 2153497. ^ \"Vitamin K\". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press. 2001. p. 162–196. ^ Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006 ^ a b Rhéaume-Bleue, p. 42\n^ \"Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K\" (PDF). National Institutes of Health Clinical Center. ^ \"Nutrition Facts and Information for Parsley, raw\". Nutritiondata.com. Retrieved 21 Apr 2013. ^ \"Nutrition facts, calories in food, labels, nutritional information and analysis\". Nutritiondata.com. 13 Feb 2008. Retrieved 21 Apr 2013. ^ \"Vitamin K\". Vivo.colostate.edu. 2 Jul 1999. Retrieved 21 Apr 2013. ^ \"Vitamin K\". Micronutrient Data Centre. ^ Ikeda, Y.; Iki, M.; Morita, A.; Kajita, E.; Kagamimori, S.; Kagawa, Y.; Yoneshima, H. (May 2006). \"Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women: Japanese Population-Based Osteoporosis (JPOS) Study\". Journal of Nutrition. 136 (5): 1323–1328. PMID 16614424. ^ Katsuyama, H.; Ideguchi, S.; Fukunaga, M.; Saijoh, K.; Sunami, S. (Jun 2002). \"Usual dietary intake of fermented soybeans (Natto) is associated with bone mineral density in premenopausal women\". Journal of Nutritional Science and Vitaminology. 48 (3): 207–215. doi:10.3177/jnsv.48.207. PMID 12350079. ^ Sano, M.; Fujita, H.; Morita, I.; Uematsu, H.; Murota, S. (Dec 1999). \"Vitamin K2 (menatetrenone) induces iNOS in bovine vascular smooth muscle cells: no relationship between nitric oxide production and gamma-carboxylation\". Journal of Nutritional Science and Vitaminology. 45 (6): 711–723. doi:10.3177/jnsv.45.711. PMID 10737225. ^ Gast, G. C ; de Roos, N. M.; Sluijs, I.; Bots, M. L.; Beulens, J. W.; Geleijnse, J. M.; Witteman, J. C.; Grobbee, D. E.; Peeters, P. H.; van der Schouw, Y. T. (Sep 2009)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a scenario where a light source with $N_I$ incoming channels and $N_O$ outgoing channels is used to transmit electromagnetic fields. The transmission matrix $\\mathbb{T}$, which represents the propagation of these fields, is composed of $4 \\times 4$ M{\\\"u}ller matrices. If the source is polarized and the observation is made on the same polarization, a scalar model can be used to simplify the transmission process. However, in the presence of noise, it is more meaningful to use a probabilistic approach to account for the uncertainty in the observed data.\n\nWhich of the following statements best describes the implications of using a probabilistic approach to account for noise in the transmission process?",
    "choices": [
      "A) The transmission matrix $\\mathbb{T}$ can be simplified to a single scalar value that represents the overall transmission efficiency.",
      "B) The use of a probabilistic approach allows for the incorporation of noise into the transmission process, which can lead to more accurate predictions of the observed data.",
      "C) The scalar model can be used to estimate the transmission matrix $\\mathbb{T}$ by minimizing the difference between the observed and predicted data.",
      "D) The probabilistic approach can be used to estimate the transmission matrix $\\mathbb{T}$ by maximizing the likelihood of the observed data given the transmission process."
    ],
    "correct_answer": "B",
    "documentation": [
      "Consider the case in which there are $N_I$ incoming channels and $N_O$ outgoing ones; we can indicate with $E^{\\rm in,out}_k$ the input/output electromagnetic field phasors of channel $k$. In the most general case, i.e., without making any particular assumptions on the field polarizations, each light mode and its polarization polarization state can be represented by means of the $4$-dimensional Stokes vector. Each $ t_{ki}$ element of $\\mathbb{T}$, thus, is a $4 \\times 4$ M{\\\"u}ller matrix. If, on the other hand, we know that the source is polarized and the observation is made on the same polarization, one can use a scalar model and adopt Jones calculus \\cite{Goodman85,Popoff10a,Akbulut11}:\n   \\begin{eqnarray}\n E^{\\rm out}_k = \\sum_{i=1}^{N_I}  t_{ki} E^{\\rm in}_i \\qquad \\forall~ k=1,\\ldots,N_O\n \\label{eq:transm}\n \\end{eqnarray}\n  We recall that the elements of the transmission matrix are random complex coefficients\\cite{Popoff10a}. For the case of completely unpolarized modes, we can also use a scalar model similar to Eq. \\eqref{eq:transm}, but whose variables are  the intensities of the outgoing/incoming fields, rather than the fields themselves.\\\\ \nIn the following, for simplicity, we will consider Eq. (\\ref{eq:transm}) as our starting point,\nwhere $E^{\\rm out}_k$, $E^{\\rm in}_i$ and $t_{ki}$ are all complex scalars. If Eq. \\eqref{eq:transm} holds for any $k$, we can write:\n  \\begin{eqnarray}\n  \\int \\prod_{k=1}^{N_O} dE^{\\rm out}_k \\prod_{k=1}^{N_O}\\delta\\left(E^{\\rm out}_k - \\sum_{j=1}^{N_I}  t_{kj} E^{\\rm in}_j \\right) = 1\n  \\nonumber\n  \\\\\n  \\label{eq:deltas}\n  \\end{eqnarray}\n\n Observed data are a noisy representation of the true values of the fields. Therefore, in inference problems it is statistically more meaningful to take that noise into account in a probabilistic way, \n rather than looking  at the precise solutions of the exact equations (whose parameters are unknown). To this aim we can introduce Gaussian distributions whose limit for zero variance are the Dirac deltas in Eq."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the problem diagram, what is the assumption made about the conditional independence of observations and preferences?",
    "choices": [
      "A) The observation o t and the preference for any other vertex are conditionally independent given the robot location s t and the goal g.",
      "B) The observation o t and the preference for the corresponding vertex p v in the graph are conditionally independent given the robot location s t and the goal g.",
      "C) The observation o t and the preference for any other vertex are conditionally independent given the robot location s t and the goal g, but the preference for the corresponding vertex p v in the graph is not.",
      "D) The observation o t and the preference for the corresponding vertex p v in the graph are conditionally independent given the robot location s t and the goal g, but the observation o t and the preference for any other vertex are not."
    ],
    "correct_answer": "A",
    "documentation": [
      "In other words, as the robot continually moves in space, the first hyperplane that it will cross upon exiting the polytope will correspond to one of the polytope's nonredundant constraints. Vincent and Schwager outline an iterative method for removing redundant constraints by solving n linear programs. We use this method in practice for computing α j e for each polytope. We can now characterize each polytope by a vector α j e ∈ {−1, 1} n j e , where n j e ≤ n is the number of essential constraints of the polytope. The polytopes P j partition the environment into a hyperplane arrangement. Path Preference\n\nIn this section, we provide a definition of preference θ according to a graphical representation of the environment based on the hyperplane arrangement. Under this representation, a path preference corresponds to a set of preferred transitions. In other words, for each polytope in the space, the human will have a preference to which neighboring polytope they wish to transition. Let G := (V, E) be an undirected graph, where vertices are obstacle-free polytopes, and edges connect two adjacent polytopes. Each polytope is described by a unique vector α j as defined in eq. ( ). Two polytopes are adjacent if they share non-redundant constraints (rows in eq. ( )) corresponding to the same hyperplane (i.e. they are on opposite sides of the hyperplane). Let N (v) be the set of neighbors of a vertex v. For each vertex, we denote p v the discrete-valued random variable describing which edge in N (v) the human intends to transition to. Using this formalism, we define a path preference as the set of preferred transitions over all nodes in the graph, Let m θ = v∈V |N (v)| be the cardinality of Θ, and m g = |Ω g | the number of possible goals. A priori, the number of Bayesian updates required to update the belief at every iteration should be m θ × m g . Now, let us assume the conditional independence relationships described by the new problem diagram in fig. . More specifically, we introduce the assumption that conditioned on a robot location s t , the goal g, and the preference for the corresponding vertex p v in the graph, the observation o t and the preference for any other vertex are conditionally independent."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robotic hand is designed to perform dexterous manipulation tasks, such as lifting and placing objects. The control algorithm for this hand uses tactile feedback from the force sensors on the fingertips to decide the forces that need to be applied to the object in each step of the task. Given the desired forces to be applied, the size of the grasp will be computed. However, previous works have relied on simplifying assumptions about the friction coefficient between the object and the fingertip. Which of the following statements best describes the approach taken in this work?",
    "choices": [
      "A) The control algorithm uses a fixed friction coefficient for all object materials, as determined by empirical testing.",
      "B) The control algorithm uses a multi-modal tactile sensor, such as the BioTac sensor, to predict the friction coefficient between the object and the fingertip.",
      "C) The control algorithm uses Coulomb's law to model the contacts and friction, but assumes a constant friction coefficient for all object materials.",
      "D) The control algorithm uses a machine learning approach to learn the friction coefficient for each object material from a dataset of experiments."
    ],
    "correct_answer": "C",
    "documentation": [
      "Our control algorithm could also be applied to different hands as it does not depend on the hands configuration. Finally, in previous approaches only lifting tasks had been considered. In our work we demonstrate that our approach can be used to perform more complex tasks, such as placing objects on surfaces and performing handovers, which was not done in previous works. Our goal in this work is to design a control algorithm for an anthropomorphic robotic hand to perform dexterous manipulation skills such as lifting and placing down objects. Our control algorithm will use tactile feedback from the force sensors on the fingertips of the hand to decide the forces that need to be applied to the object in each step of the task. Given the desired forces to be applied, the size of the grasp will be computed. Given the grasp size and a desired grasp type, the posture generator will generate a grasp posture, i.e. the hand configuration, such that the force constraints are satisfied. To model the contacts and friction we use Coulombs' law, which states that in order to avoid slip, the normal contact force f n to the contact surface of an object, times the fiction coefficient µ, has to be larger than the tangential force f t :\nµf n ≥ f t You can see an example in Figure , where an object is pressed against a wall by an applied normal force f n , and we have the tangential force f t = mg due to gravity. In order for the object to remain stable we need to apply a normal force: where µ is the friction coefficient between the object and the wall. In the case of a dexterous hand manipulating an object, we want the normal forces applied by all fingers to be greater than the tangential force divided by the friction coefficient of the materials of the object and the fingertip. Since it is hard to accurately compute the friction coefficient between all possible object materials previous works have used multi-modal tactile sensors like the BioTac sensor, which provides information about the pressure, skin deformation, and temperature, to predict slip and based on that signal to increase the applied normal force."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a potential long-term consequence of the US's efforts to ensure a long-term military partnership with Iraq, as requested by the next Iraqi government?",
    "choices": [
      "A) The US will be forced to withdraw its troops from Iraq, leading to a significant reduction in its military presence in the region.",
      "B) The US will be able to establish a permanent military base in Iraq, allowing for greater strategic flexibility and control over the region.",
      "C) The US will be able to increase its economic influence in Iraq, leading to a significant boost in trade and investment between the two countries.",
      "D) The US will be able to improve its relationships with other countries in the region, particularly Iran, by demonstrating its commitment to stability and security in Iraq."
    ],
    "correct_answer": "B",
    "documentation": [
      "In reality, constant American pressure was applied to Maliki, Allawi, Kurdish leaders and other prominent Iraqi politicians throughout the entire nine-month process to form a cabinet. The US intervention included numerous personal phone calls and visits to Baghdad by both President Barack Obama and Vice President Joe Biden. The key objective of the Obama administration has been to ensure that the next Iraqi government will \"request\" a long-term military partnership with the US when the current Status of Forces Agreement (SOFA) expires at the end of 2011. The SOFA is the legal basis upon which some 50,000 American troops remain in Iraq, operating from large strategic air bases such as Balad and Tallil and Al Asad. US imperialism spent billions of dollars establishing these advanced bases as part of its wider strategic plans and has no intention of abandoning them. Cogan's only the second person to include the SOFA in his report. Some are impressed with the 'feat' of taking nearly ten months to form a government, stringing the country along for ten months while no decisions could go through. The editorial board of the Washington Post, for example, was full of praise yesterday. Today they're joined by Iran's Ambassador to Iraq, Hassan Danaiifar. The Tehran Times reports that Danaiifar was full of praise today hailing the \"positive and final step which ended the 10-month political limbo in Iraq.\" However, Danaiifar was less pie-in-the-sky than the Post editorial board because he can foresee future problems as evidenced by his statement, \"We may witness the emergence of some problems after one and half of a year -- for example, some ministers may be impeached.\" Of course, there are already many clouds on the horizon, even if Iranian diplomats and Post editorial boards can't suss them out. For example, Ben Bendig (Epoch Times) noted the objection of Iraq's female politicians to Nouri al-Maliki's decision to nominate only one woman (so far) to his Cabinet: \"Some 50 female lawmakers went to the country's top leadership, the United Nations and the Arab League to voice their concern and desire for increased representation.\""
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Veterans for Peace event in front of the White House was a solemn occasion, marked by a silent procession and a human link of bodies across the 'picture postcard' tableau in front of the White House. What can be inferred about the participants' motivations for their actions, based on the accounts of Nate Goldshlag and Linda LeTendre?",
    "choices": [
      "A) The participants were primarily motivated by a desire to protest the presence of the White House, rather than to honor the fallen veterans.",
      "B) The participants were united in their opposition to the Iraq and Afghan wars, and saw the event as a way to honor the memories of those who had been killed in combat.",
      "C) The participants were primarily motivated by a sense of patriotism and a desire to show respect for the American flag, rather than to make a political statement.",
      "D) The participants were driven by a sense of anger and frustration towards the government, and saw the event as a way to express their dissent."
    ],
    "correct_answer": "B",
    "documentation": [
      "A group of veterans under the leadership of Veterans for Peace members Tarak Kauff, Will Covert and Elaine Brower, mother of a Marine who has served three tours of duty in Iraq, sponsored the event with the explicit purpose of putting their bodies on the line. Many participants were Vietnam War veterans; others ranged from Iraq and Afghanistan war veterans in their 20s and 30s to World War II vets in their 80s and older. They were predominately white; men outnumbered women by at least three to one. After a short rally in Lafayette Park, they formed a single-file procession, walking across Pennsylvania Avenue to the solemn beat of a drum. As they reached the police barricade (erected to prevent them from chaining themselves to the gate, a plan they announced on their web site), the activists stood shoulder to shoulder, their bodies forming a human link across the 'picture postcard' tableau in front of the White House.\" Maria Chutchian (Arlington Advocate) quotes, participant Nate Goldshlag (Vietnam veteran) stating, \"\"There was a silent, single file march around Lafayette Park to a drum beat. Then we went in front of the White House,. There were barricades set up in front of white house fence. So when we got there, we jumped over barricades and were able to get right next to the White House fence.\" Participant Linda LeTendre (Daily Gazette) reports: At the end of the rally, before the silent, solemn procession to the White House fence, in honor of those killed in Iraq and Afghan wars of lies and deceptions, the VFP played taps and folded an American flag that had been left behind at a recent funeral for the veteran of one of those wars. Two attendees in full dress uniform held and folded the flag. I had the image of all of the people who stood along the roads and bridges when the bodies of the two local men, Benjamin Osborn and David Miller, were returned to the Capital District. I thought if all of those people were here now or spoke out against war these two fine young men might still be with us."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The developers have made several balance changes to the game's Champions, including adjustments to their abilities and scaling. However, these changes have raised questions about the impact on gameplay. Which of the following statements best summarizes the developers' reasoning for increasing the frequency of Black Bolt's \"Provocation\" ability?",
    "choices": [
      "A) To compensate for the reduced frequency of Special Attacks, which were used too infrequently at lower levels.",
      "B) To ensure that Provocation would trigger more frequently, especially against opponents with high Critical Hit rates, in order to maintain a more balanced gameplay experience.",
      "C) To create a new synergy between Black Bolt's abilities and the Alliance Crystal, which now provides temporary boosts to Attack, Health, and XP.",
      "D) To address the issue of Gamora's \"Assassination\" ability being too strong at lower levels, where fewer counters to Regeneration exist."
    ],
    "correct_answer": "B",
    "documentation": [
      "We’ve compared our notes with the feedback you’ve been sending us and are making some balance changes to them. Thanks for your feedback!\nSlightly reduced the frequency and duration of Juggernaut’s “Unstoppable” ability. • He was indeed a bit too...unstoppable. We’ve toned down the frequency this ability triggers, as well as reduced the duration it’s active for when it does trigger. We feel Juggernaut is still a powerful Champion despite these revisions. Take care! Slightly reduced the starting values of Wolverine's “Cellular Regeneration”. • We found that Cellular Regeneration was too strong at lower levels where fewer counters to Regeneration exist. Re-scaled Gamora's “Assassination” to start higher but scale slower. • At lower levels, Special Attacks were used too infrequently, giving this powerful ability little visibility. We’ve adjusted the scaling to better match Special Attack usage at all levels. Increased the frequency that Black Bolt’s “Provocation” triggers. • Due to the varying Critical Hit rates across all Champions, in some cases Provocation would trigger rarely or not at all within a fight. We’ve increased the frequency to ensure you’ll see it every match – but especially so against opponents with high Critical Hit rates. We’ll continue to follow the effect of these new abilities on gameplay. Please keep your feedback coming! Hey everyone! We have been hard at work on improving the game and have prepared a big update inspired in part by your great community feedback. Please keep letting us know what you think! • Fixed many Dash, Medium, Heavy and Special Attacks missing or failing to execute. • Added Alliances and a new Alliance Crystal. • Rocket Raccoon and Unstoppable Colossus join The Contest. • Temporary Boosts to Attack, Health, and XP are now available from the Alliance Crystal. • Rewards for completing and exploring Chapters and Acts. Earn a guaranteed 3-Star hero crystal for each fully explored Act! This is retroactive, just complete any quest to claim them. • A new Fight Menu combines The Arenas, Story Quests and Event Quest menus."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the provided text, what can be inferred about the relationship between Heart Rate Variability (HRV) and Heart Rate (HR) based on the signal processing methods described?",
    "choices": [
      "A) HRV and HR are directly proportional, meaning that an increase in HR should increase HRV.",
      "B) HRV and HR are inversely proportional, meaning that an increase in HR should decrease HRV.",
      "C) The filtering method used to remove noise from the PPG signal does not affect the relationship between HRV and HR.",
      "D) The use of a 5-Hz 8th-order Butterworth low-pass filter to refine the PPG signal does not impact the relationship between HRV and HR."
    ],
    "correct_answer": "B",
    "documentation": [
      "We use Periodic Moving Average Filter (PMAF) to remove motion artifacts and noises \\cite{lee07}. We first segment the PPG signal on periodic boundaries and then average the $m^{th}$ samples of each period. After filtering the input PPG signal with a 5-Hz $8^{th}$-order Butterworth low-pass filter, we estimate the maximum and minimum value of each period. The mean of each period are obtained from the maximum and minimum values applying the zero crossing method. These points of the means help determine the boundaries of each period. Then, interpolation or decimation is performed to ensure that each period had the same number of samples \\cite{lee07}. \\subsubsection{Heart Rate and Heart Rate Variability Estimation} We first apply PMAF on PPG signal to remove noises and motion artifacts, refine PPG by smoothing the signal using 1-dimensional Gaussian Filter and Convolution, calculate first derivative of the convoluted signal and finally find the differences between two consecutive peak values which is called HRV \\cite{sel08}. The occurrences of total peak values (R-peak or beat) in each minute is called Heart Rate (HR) with an unit of Beat Per Minute. The signal value property of HRV and HR are inversely proportional which means the mental arousal that increases HR should decrease HRV in the time segment window. Fig~\\ref{fig:ppg_artifact_removal} shows a sample of the noisy and filtered PPG signal and their corresponding Instant Heart Rate. \\begin{figure}[!htb]\n\\vspace{-.1in}\n\\begin{center}\n   \\epsfig{file=ppg_artifact_removal.pdf,height=1.4in, width=3.5in}\n   \\vspace{-.15in}\n\\caption{Top figure illustrates the noisy signal (dotted line) and filtered signal from PPG sensor based on our filtering method. Bottom figure illustrates instant heart rate calculated from noisy signal (dotted line) and filtered signal}\n   \\label{fig:ppg_artifact_removal}\n\\end{center}\n\\vspace{-.15in}\n\\end{figure}\n\\subsection{Physiological Sensor Signal Feature Extraction}\nUsing the above mentioned methods, we removed the noises and motion artifacts from EDA and PPG signals and generated two time series signal from EDA (tonic and phasic components) and one time series signal from PPG (HRV)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user, \"John\", has created a Broadjam account and has subscribed to the \"Premium\" service. According to the terms of service, John is responsible for any unauthorized use of his account, and he agrees to immediately notify Broadjam of any breach of security. However, John has recently noticed that his account has been accessed from a different location than his usual IP address. What is the most likely explanation for this, given the information provided?",
    "choices": [
      "A) John's account has been compromised by a third-party hacker, and the access was made from a nearby location.",
      "B) John has been using a public Wi-Fi network and his IP address was spoofed, allowing the access to appear from a different location.",
      "C) John's account has been accessed by a Broadjam employee, who is using a proxy server to mask their IP address.",
      "D) John's subscription has been automatically renewed for a new term, and the access was made from a different location due to a technical glitch."
    ],
    "correct_answer": "B",
    "documentation": [
      "Broadjam is not liable for any harm caused by or related to the theft of your Username, your disclosure of your Username, or your authorization to allow another person to access and use the Site or any Service using your Username. Furthermore, you are solely and entirely responsible for any and all activities that occur under your account, including, but not limited to, any charges incurred relating to the Site or any Service. You agree to immediately notify us of any unauthorized use of your account or any other breach of security known to you. You acknowledge that the complete privacy of your data transmitted while using the Site or any Service cannot be guaranteed. The term of any Subscription Service shall commence when the Subscriber initiates payment for such Subscription Service or, if the Subscription Service is complimentary, when the Subscriber registers for such Subscription Service. All Subscription Services will extend for an initial period of oneyear (the \"Term\") and, unless terminated as provided herein, shall renew automatically for successive one-year periods. During the Term, the Subscriber shall be afforded the full use and benefit of the applicable Subscription Service as described on the Site (the \"Service Benefits\"), which Service Benefits may be revised by Broadjam from time to time without notice to the Subscriber. Due to technical considerations, certain Service Benefits may not be available to the Subscriber immediately upon commencement of the Term, but shall be provided to the Subscriber as soon as commercially reasonable. Please direct any questions about Subscription Services or Service Benefits to Broadjam by email at: customerservice@broadjam.com or by US mail at: Broadjam Inc., 100 S. Baldwin St. Ste. #204, Madison, WI 53703, Attn: Customer Service.\n(b) maintain and update such information as needed to keep it current, complete and accurate. Subscriber acknowledges that Broadjam relies and will rely upon the accuracy of such information as supplied by Subscriber."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The authors' work on Fokker-Planck acceleration has implications for our understanding of the behavior of particles in complex systems. In particular, they have shown that the acceleration of particles in a system can be described by a Fokker-Planck equation. However, this equation assumes a certain level of symmetry in the system, which may not always be the case. Furthermore, the authors have also discussed the importance of considering the effects of noise on the behavior of particles in complex systems. What are the potential consequences of neglecting the effects of noise on the behavior of particles in a system that exhibits Fokker-Planck acceleration?",
    "choices": [
      "A) The system will exhibit a more rapid increase in entropy over time.",
      "B) The system will exhibit a more rapid decrease in entropy over time.",
      "C) The system will exhibit a more rapid change in the distribution of particles over time.",
      "D) The system will exhibit a more rapid convergence to a stable equilibrium."
    ],
    "correct_answer": "C",
    "documentation": [
      "The authors would also like to thank Dr.~Anil Prinja for discussions involving Fokker-Planck acceleration."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The reconstruction error curve for the Ising model displays a minimum at a low temperature, close to the critical point, in cases where a critical behavior occurs. This minimum is enhanced by the decimation method, which appears to reduce the reconstruction error by almost an order of magnitude compared to other methods. What is the likely outcome of using the decimation method in the field of light waves propagation through random media, where the goal is to reconstruct an object seen through an opaque medium or a disordered optical fiber?",
    "choices": [
      "A) The reconstruction error will increase significantly as the temperature decreases.",
      "B) The reconstruction error will decrease significantly as the temperature decreases, allowing for more accurate reconstruction of the object.",
      "C) The reconstruction error will remain constant, regardless of the temperature, due to the limitations of the decimation method.",
      "D) The reconstruction error will increase as the temperature approaches zero, making it impossible to reconstruct the object."
    ],
    "correct_answer": "B",
    "documentation": [
      "The behavior of the inference quality in temperature and in the size of data samples is also investigated, basically confirming the low $T$ behavior hinted by Nguyen and Berg \\cite{Nguyen12b} for the Ising model. In temperature, in particular, the reconstruction error curve displays a minimum at a low temperature, close to the critical point in those cases in which a critical behavior occurs, and a sharp increase as temperature goes to zero. The decimation method, once again, appears to enhance this minimum of the reconstruction error of almost an order of magnitude with respect to other methods. The techniques displayed and the results obtained in this work can be of use in any of the many systems whose theoretical representation is given by Eq. \\eqref{eq:HXY} or Eq. \\eqref{eq:h_im}, some of which are recalled in Sec. \\ref{sec:model}. In particular, a possible application can be the field of light waves propagation through random media and the corresponding problem of the  reconstruction of an object seen through an opaque medium or a disordered optical fiber \\cite{Vellekoop07,Vellekoop08a,Vellekoop08b, Popoff10a,Akbulut11,Popoff11,Yilmaz13,Riboli14}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary concern of Brooksley Born, the former Commodity Futures Trading Commission (CFTC) chair, as revealed in her 2009 testimony before the Senate Banking Committee, which was later cited as a warning about the impending credit crisis?",
    "choices": [
      "A) The need for stricter regulations on the derivatives market to prevent excessive speculation.",
      "B) The importance of increasing transparency in financial markets to prevent insider trading.",
      "C) The potential risks of unchecked leverage in the financial system, which could lead to a systemic crisis.",
      "D) The requirement for greater oversight of hedge funds to prevent their misuse of derivatives."
    ],
    "correct_answer": "C",
    "documentation": [
      "Interview: Brooksley Born for \"PBS Frontline: The Warning\", PBS, (streaming VIDEO 1 hour), October 20, 2009. Articles\nManuel Roig-Franzia. \"Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On\", The Washington Post, May 26, 2009\n Taibbi, Matt. \"The Great American Bubble Machine\", Rolling Stone'', July 9–23, 2009\n\n1940 births\nAmerican women lawyers\nArnold & Porter people\nClinton administration personnel\nColumbus School of Law faculty\nCommodity Futures Trading Commission personnel\nHeads of United States federal agencies\nLawyers from San Francisco\nLiving people\nStanford Law School alumni\n21st-century American women\nStanford University alumni"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for KSTP's rebranding as \"SKOR North\" in 2019, and how did this change impact the station's programming schedule?",
    "choices": [
      "A) The station's ownership was sold to a new company, leading to a shift in target audience, and the rebranding was a result of this change. As a result, the station began airing more sports-focused programming, including local shows from 12 pm to 7 pm, which was previously not the case.",
      "B) The station's management decided to rebrand as a sports-focused station to attract a younger demographic, and this change led to the suspension of most local programming and the laying off of nearly all local staff. The rebranding resulted in a shift towards more sports-focused programming, including local shows from 12 pm to 7 pm.",
      "C) The station's rebranding was a result of a dispute between the station's management and its parent company, Clear Channel, over the direction of the station's programming. As a result, the station began airing more sports-focused programming, including local shows from 12 pm to 7 pm, which was previously not the case.",
      "D) The station's rebranding was a result of a partnership with the Minnesota Vikings, and this change led to the suspension of most local programming and the laying off of nearly all local staff. The rebranding resulted in a shift towards more sports-focused programming, including local shows from 12 pm to 7 pm."
    ],
    "correct_answer": "B",
    "documentation": [
      "These broadcasters were supported by producers such as Bruce Huff, Rob Pendleton, Alison Brown, Jean Bjorgen, David Elvin (who Vogel dubbed the \"Steven Spielberg of Talk Radio\"), Mitch Berg and others. The station has, for the most part, emphasized local hosts over the years. But in 1988, KSTP was one of Rush Limbaugh's first affiliates when his conservative talk show was rolled out for national syndication. (Clear Channel-owned KTLK-FM took over rights to Limbaugh's show in January 2006). Other syndicated hosts previously heard on KSTP include Sean Hannity, Bruce Williams, Larry King, and Owen Spann. Sports Radio\nKSTP switched to Sports Radio on February 15, 2010. As the station had to wait for ESPN's contract with rival KFAN and its sister station KFXN to expire, it did not become an ESPN Radio affiliate until April 12, the same day that the Minnesota Twins were scheduled to play the first game in their new ball park, Target Field, against the Boston Red Sox. As a result Coast to Coast AM and Live on Sunday Night, it's Bill Cunningham were retained during this period. One ESPN Radio network program, The Herd with Colin Cowherd, was picked up by KSTP immediately following the format change. In 2018, the station was approved for an FM translator on 94.1 FM, broadcasting from a transmitter atop the IDS Center in downtown Minneapolis. The two-watt signal threw most of its power to the west, preventing interference to low powered FM stations on the same channel including WFNU-LP in St. Paul. With only two watts of power, however, the signal was limited to the immediate downtown area surrounding the IDS Center. It later acquired a 250 watt translator, K235BP at 94.9 MHz. The original translator was discontinued. On January 15, 2019, KSTP rebranded as \"SKOR North\" (a reference to the Vikings team song/chant, \"Skol, Vikings\"), with local programming between 12 noon and 7 pm. About a year later, in May of 2020, KSTP suspended most of its local programming and laid off nearly all of its local staff."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When building the fuselage of a Rand Robinson KR series plane, what is the primary reason for the \"banana\" shape that occurs when the completed fuselage sides are laid into position to form the fuselage box section?",
    "choices": [
      "A) The builder failed to properly clamp the sides during construction, causing them to bow up from the building surface.",
      "B) The preformed fiberglass parts used in the build process are not designed to accommodate the curved shape of the fuselage, leading to the \"banana\" shape.",
      "C) The builder did not accurately measure the distances given in the plans, resulting in a \"foreshortened\" view that does not accurately represent the true shape of the fuselage.",
      "D) The builder attempted to force the longerons into place, causing them to become misaligned and create the \"banana\" shape."
    ],
    "correct_answer": "C",
    "documentation": [
      "Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go \"perfectly.\" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane. This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes. While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying \"banana\" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built. First understand that the plans show the finished form of the plane. They show the \"projected\" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are \"foreshortened\" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to \"develop\" the \"true\" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Carli's behavior of making up a story to get her friend's sister off the computer can be seen as a manifestation of her own anxiety and need for control. However, her father's recent withdrawal and his struggles with anxiety and depression may also be contributing factors. Considering the complex dynamics at play, what is the most likely underlying reason for Carli's behavior, and how can her father's situation inform his approach to supporting Carli?",
    "choices": [
      "A) Carli's behavior is a result of her own anxiety and need for control, and her father's situation is irrelevant to her behavior.",
      "B) Carli's behavior is a coping mechanism for her father's recent withdrawal and struggles with anxiety and depression, and her father should focus on addressing his own mental health.",
      "C) Carli's behavior is a manifestation of her own behavioral issues, and her father's situation is a red herring that distracts from the real issue at hand.",
      "D) Carli's behavior is a result of her father's own anxiety and depression, and he should take steps to address his own mental health in order to better support Carli."
    ],
    "correct_answer": "B",
    "documentation": [
      "He’s highly intelligent and successful, a pattern seeker, has a tendency to focus on the project to hand to the total exclusion of all else for as long sit takes (work or home) socially awkward (has learned coping strategies), sensitive to loud noise, high anxiety with control strategies, black and white thinking etc. He’s currently not working and I’ve seen a slow withdrawal over the last 6 weeks, including the need to ‘escape’ and leave a situation at least once. He also has a bipolar ex overseas who has primary custody one daughter where there has been ongoing patterns of drama which has recently increased. Over the past couple of months (since stopping work and drama increase) I’ve gone from being ‘wonderful’ in his eyes to him now being sorry and not having the ‘urge’ to spend close/intimate time with me and offering friendship. Since he shared that with me in a message he’s stonewalled and has retreated to the safety of minimal messages and talks about not knowing what best to say and not being able to find the right words somehow. He’s a good kind man who I feel is struggling. I’m concerned about his anxiety and possibly the risk of depression. I’m fairly resilient and whilst i’m disappointed he doesn’t want to pursue a relationship with me, i’m concerned for him and his well being. One of his very few close friends is also just leaving the country to live overseas. The strategy I’ve used so far is simply to back off and give him space. I’ve asked to take him up on an original offer he made to talk but haven’t pushed it. I also haven’t been aggressive or accusatory in the few messages i’ve sent. Any advise you could give would be greatly appreciated,\nCarli who is 10 years old and has had behavioral issues her whole life. The other night she came home very upset after having a conflict with a friend. She was at her friend's house and her and her friend wanted to get on the computer and the older sister was using it. Carli made up a story that someone was at the door to get the older sister off the computer. Her friend didn't understand that she was making up a story to get the sister off the computer."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study published in the Journal of Biological Chemistry found that a protein called Gla-rich protein accumulates at sites of pathological calcifications, and is vitamin K-dependent. Another study published in the American Journal of Pathology found that a protein called Gas6 is a ligand for the Axl receptor tyrosine kinase subfamily, and is also vitamin K-dependent. However, a third study published in the Proceedings of the National Academy of Sciences found that a protein called proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein. Which of the following statements is most consistent with the findings of these studies?",
    "choices": [
      "A) Vitamin K is essential for the regulation of calcification in the body.",
      "B) The Axl receptor tyrosine kinase subfamily plays a key role in the regulation of vitamin K-dependent proteins.",
      "C) Proline-rich Gla protein 2 is a major contributor to the accumulation of vitamin K-dependent proteins at sites of pathological calcifications.",
      "D) Vitamin K-dependent proteins are primarily involved in the regulation of bone metabolism."
    ],
    "correct_answer": "C",
    "documentation": [
      "Journal of Biological Chemistry. 283 (52): 36655–36664. doi:10.1074/jbc. M802761200. PMC 2605998. PMID 18836183. ^ Viegas, C. S.; Cavaco, S.; Neves, P. L.; Ferreira, A.; João, A.; Williamson, M. K.; Price, P. A.; Cancela, M. L.; Simes, D. C. (Dec 2009). \"Gla-rich protein is a novel vitamin K-dependent protein present in serum that accumulates at sites of pathological calcifications\". American Journal of Pathology. 175 (6): 2288–2298. doi:10.2353/ajpath.2009.090474. PMC 2789615. PMID 19893032. ^ Hafizi, S.; Dahlbäck, B. (Dec 2006). \"Gas6 and protein S. Vitamin K-dependent ligands for the Axl receptor tyrosine kinase subfamily\". The FEBS Journal. 273 (23): 5231–5244. doi:10.1111/j.1742-4658.2006.05529.x. PMID 17064312. ^ Kulman, J. D.; Harris, J. E.; Xie, L.; Davie, E. W. (May 2007). \"Proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein\". Proceedings of the National Academy of Sciences of the United States of America. 104 (21): 8767–8772. doi:10.1073/pnas.0703195104. PMC 1885577. PMID 17502622. ^ \"Vitamin K\". MedlinePlus. US National Library of Medicine, National Institutes of Health. Sep 2016. Retrieved 26 May 2009. ^ Conly, J; Stein, K. (Dec 1994). \"Reduction of vitamin K2 concentrations in human liver associated with the use of broad spectrum antimicrobials\". Clinical and Investigative Medicine. 17 (6): 531–539. PMID 7895417. ^ Ferland, G.; Sadowski, J. A.; O'Brien, M. E. (Apr 1993). \"Dietary induced subclinical vitamin K deficiency in normal human subjects\". Journal of Clinical Investigation. 91 (4): 1761–1768. doi:10.1172/JCI116386. PMC 288156. PMID 8473516. ^ Holden, R. M.; Morton, A. R.; Garland, J. S.; Pavlov, A.; Day, A. G.; Booth, S. L. (Apr 2010). \"Vitamins K and D status in stages 3-5 chronic kidney disease\". Clinical Journal of the American Society of Nephrology. 5 (4): 590–597. doi:10.2215/CJN.06420909. PMC 2849681. PMID 20167683. ^ Hodges, S. J.; Pilkington, M. J.; Shearer, M. J.; Bitensky, L.; Chayen, J (Jan 1990)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When performing complex manipulation tasks in unstructured environments, humans rely on tactile feedback from their fingers to adjust their grasp configuration. What is a key advantage of using synergy spaces in robotic grasping, as described in the provided text?",
    "choices": [
      "A) It allows for independent control of each finger joint, enabling precise movements.",
      "B) It enables the robot to adapt to changes in the object's weight by adjusting the synergy space.",
      "C) It provides a shared space for all hand joints, facilitating the use of tactile afferents for precise force control.",
      "D) It enables the robot to use machine learning models to predict slip and update desired normal forces."
    ],
    "correct_answer": "C",
    "documentation": [
      "Using this framework we are able to control the hand during different grasp types using only one variable, the grasp size, which we define as the distance between the tip of the thumb and the index finger. Instead of controlling the finger limbs independently, our controller generates control signals for all the hand joints in a (lowdimensional) shared space (i.e.\nsynergy space). In addition, our approach is modular, which allows to execute various types of precision grips, by changing the synergy space according to the type of grasp. We show that our controller is able to lift objects of various weights and materials, adjust the grasp configuration during changes in the object's weight, and perform object placements and object handovers. INTRODUCTION\n\nTo perform complex manipulation tasks in unstructured environments, humans use tactile feedback from their fingers. This feedback is provided by tactile afferents located in the skin of the hand. Particularly, for handling small objects with precise movements, the afferents located in the fingertips are used, which have high density and adapt fast to pressure changes . These afferents provide information about the characteristics of the exerted contact forces, such as the magnitude and the direction. For anthropomorphic robots to be able to perform dexterous tasks similar force feedback signals must be used to alleviate problems arising from uncertainty in measurements, and handle external perturbations. For example, using open-loop position control to lift a heavy object may fail due to slip without any feedback mechanism to provide tactile information. Previous works have used tactile sensors to design force controllers that use slip prediction to update the desired normal forces applied by the fingertips. The slip predictors are based on machine learning models such as neural networks and random forests to classify multi-modal signals from a tactile sensor. In all previous works, each finger was separately controlled by an independent force controller."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The recent incident involving the compromised accounts in the game has raised concerns about the security of password storage. What can be inferred about the password storage system of QONQR based on the information provided?",
    "choices": [
      "A) The system stores passwords in plaintext, making them vulnerable to hacking.",
      "B) The system uses a two-factor authentication method, requiring both a password and a second form of verification.",
      "C) The system uses a hashing algorithm to store passwords, but the hashing process is not one-way, allowing for potential decryption.",
      "D) The system uses a combination of hashing and encryption to store passwords, with access to the encrypted database restricted to authorized personnel."
    ],
    "correct_answer": "D",
    "documentation": [
      "More money from various players might mean they can limit the players who spend a ton and still generate a healthy income. The main issue i see with limiting refreshes is someone multiscoping and spending money. He now has two, three, four accounts to refresh with and gets the advantage. Its tricky. ey dun new ho to yet it uff. Yet you complain almost everyday here, on your website, Twitter, and YouTube channel that the game needs to change because cubing has such an impact. It was fun. Swarm had me scared at first, but it turned into kind of a bullying match between us and legion. Last hour became p obvious which way it was gonna go. Legion rly stepped up their game in the end there, respect. We are investigating this. Here is what we know: Several of the accounts used the same password. Most of the accounts belonged to people who knew each other personally. The accounts were all switched from the same IP Addresses. The person who logged in, got into each account on the first attempt, so they knew the password for each account. What you should know: QONQR never stores passwords, not even in the logs. Passwords are hashed (one way encrypted) and can never be decrypted When you authenticate to our servers, we hash the password you gave us and compare it to the encrypted password in the database to see if they match. Access to our database in the could is restricted tightly and we are confident no one breached the system. What you should do: Don't use the same password as other people you play with. Don't share your password with anyone. I heard all the French players fled to the UK after one German player accidentally shot a single missile into France. Most factions now use GroupMe or Line as their means of communication, the forums are too slow as a means of communication and insecure for specific faction conversations. Think of the forums are more of a gaming information resource rather than a means of communication. Contact the top players of your faction in the leader boards of your state and they will likely point you in the right direction to chatting with your local faction."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The proposed algorithm for the MISO channel estimation is an adaptable step-size LMS that performs well in both stationary and tracking scenarios. However, the choice of parameters for the algorithm can be challenging. What are the advantages of using a probabilistic interpretation of the LMS algorithm, and how does it address the issue of parameter tuning?",
    "choices": [
      "A) The probabilistic interpretation allows for a more intuitive understanding of the algorithm's behavior, but it requires more parameters to be tuned, which can lead to overfitting.",
      "B) The probabilistic interpretation provides a clear physical meaning to the algorithm's parameters, making it easier to tune them, but it may not be as effective in tracking scenarios.",
      "C) The probabilistic interpretation enables the algorithm to adapt to changing channel conditions, reducing the need for parameter tuning, but it may not be as effective in stationary scenarios.",
      "D) The probabilistic interpretation provides a more descriptive model of the channel, allowing for more accurate parameter estimation, but it may not be as effective in tracking scenarios due to the increased number of parameters."
    ],
    "correct_answer": "C",
    "documentation": [
      "More details on the setup can be found in \\cite{gutierrez2011frequency}. Fig. \\ref{fig_2} shows the real part of one of the channels, and the estimate of the proposed algorithm. The shaded area represents the estimated uncertainty for each prediction, i.e. $\\hat{\\mu}_k\\pm2\\hat{\\sigma}_k$. Since the experimental setup does not allow us to obtain the optimal values for the parameters, we fix these parameters to their values that optimize the steady-state mean square deviation (MSD). \\hbox{Table \\ref{tab:table_MSD}} shows this steady-state MSD of the estimate of the MISO channel with different methods. As can be seen, the best tracking performance is obtained by standard LMS and the proposed method. \n\n\n\n\n\n\\section{Conclusions and Opened Extensions}\n\\label{sec:conclusions}\n\n{We have presented a probabilistic interpretation of the least-mean-square filter. The resulting algorithm is an adaptable step-size LMS that performs well both in stationary and tracking scenarios. Moreover, it has fewer free parameters than previous approaches and these parameters have a clear physical meaning. Finally, as stated in the introduction, one of the advantages of having a probabilistic model is that it is easily extensible:}\n\n\\begin{itemize}\n\\item If, instead of using an isotropic Gaussian distribution in the approximation, we used a Gaussian with diagonal covariance matrix, we would obtain a similar algorithm with different step sizes and measures of uncertainty, for each component of ${\\bf w}_k$. Although this model can be more descriptive, it needs more parameters to be tuned, and the parallelism with LMS vanishes. \\item Similarly, if we substitute the transition model of \\eqref{eq:trans_eq} by an Ornstein-Uhlenbeck process, \n\n\\begin{equation}\np({\\bf w}_k|{\\bf w}_{k-1})= \\mathcal{N}({\\bf w}_k;\\lambda {\\bf w}_{k-1}, \\sigma_d^2), \\nonumber\n\\label{eq:trans_eq_lambda}\n\\end{equation}\na similar algorithm is obtained but with a forgetting factor $\\lambda$ multiplying ${\\bf w}_{k-1}^{(LMS)}$ in \\eqref{eq:lms}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a time series with a spectral density function f that is Hölder continuous with exponent β = γ + α > 0. The autocovariances σ k of the time series are the Fourier coefficients of f. According to Lemma 1, what is the relationship between the entries of the Discrete Cosine Transform (DCT-I) matrix I and the spectral density function f?",
    "choices": [
      "A) The entries of I are equal to the spectral density function f evaluated at the frequencies x j = (j − 1)/(p − 1), j = 1, ..., p.",
      "B) The entries of I are equal to the inverse Fourier transform of the spectral density function f.",
      "C) The entries of I are equal to the spectral norm of the spectral density function f, denoted by Σ ≤ f ∞ := sup x∈ |f (x)|.",
      "D) The entries of I are equal to the k-th Fourier coefficient of the function whose γ-th derivative is α-Hölder continuous, multiplied by √2."
    ],
    "correct_answer": "D",
    "documentation": [
      "The sample size n may tend to infinity or to be a constant. The case n = 1 corresponds to a single observation of a stationary time series, and in this case the data are simply denoted by Y ∼N p (0 p , Σ). The dimension p is assumed to grow. The spectral density function f , corresponding to a Toeplitz covariance matrix Σ, is given by so that for f ∈ L 2 (−π, π) the inverse Fourier transform implies Hence, Σ is completely characterized by f , and the non-negativity of the spectral density function implies the positive definiteness of the covariance matrix. Moreover, the decay of the autocovariance σ k is directly connected to the smoothness of f . Finally, the convergence rate of a Toeplitz covariance estimator and that of the corresponding spectral density estimator are directly related via Σ ≤ f ∞ := sup x∈ |f (x)|, where • denotes the spectral norm (see . As in , we introduce a class of positive definite Toeplitz covariance matrices with Hölder continuous spectral densities. For β = γ + α > 0, where The optimal convergence rate for estimating Toeplitz covariance matrices over P β (M 0 , M 1 ) depends crucially on β. It is well known that the k-th Fourier coefficient of a function whose γ-th derivative is α-Hölder continuous decays at least with order O(k −β ) (see . Hence, β determines the decay rate of the autocovariances σ k , which are the Fourier coefficients of the spectral density f , as k → ∞. In particular, this implies that for β ∈ (0, 1], the class P β (M 0 , M 1 ) includes Toeplitz covariance matrices corresponding to long-memory processes with bounded spectral densities, since the sequence of corresponding autocovariances is not summable. A connection between Toeplitz covariance matrices and their spectral densities is further exploited in the following lemma. Lemma 1. Let Σ ∈ P β (M 0 , M 1 ) and let x j = (j − 1)/(p − 1), j = 1, ..., p, then where δ i,j is the Kroneker delta, O(•) terms are uniform over i, j = 1, . . . , p and divided by √ 2 when i, j ∈ {1, p} is the Discrete Cosine Transform I (DCT-I) matrix."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What are the primary concerns in the numerical research on SBI that macroscopic models based on the continuity hypothesis cannot address?",
    "choices": [
      "A) The limitations of the computational costs of microscopic modeling methods and the need for more efficient algorithms to simulate the evolution of shock-accelerated heavy bubbles.",
      "B) The challenges posed by the non-continuity/non-equilibrium flows and the need for kinetic modeling to describe these phenomena.",
      "C) The importance of considering the effects of small structures and fast-changing patterns on the behavior of the system, and the need for improved methods to describe these phenomena.",
      "D) The potential applications of mesoscopic modeling methods, such as the Direct Simulation Monte Carlo method, in simulating the evolution of bubbles and flows morphology."
    ],
    "correct_answer": "B",
    "documentation": [
      "Generally, there are three kinds of physical modeling methods (or models) for SBI numerical research, i.e., the macroscopic, mesoscopic, and microscopic modeling methods. Most of the existing numerical researches on SBI are related to the macroscopic modeling methods (such as the Euler and Navier-Stokes (NS) models) based on the continuous hypothesis (or equilibrium and nearequilibrium hypothesis) . For example, presented the computational results on the evolution of the shock-accelerated heavy bubbles through the multi-fluid Eulerian equation . There also exist a few SBI works based on the mesoscopic modeling method, such as the Direct Simulation Monte Carlo method . The microscopic modeling methods such as the Molecular dynamics (MD) simulation, is capable of capturing much more flow behaviors but restricted to smaller spatiotemporal scales because of its huge computing costs. In the numerical research on SBI, three points need to be concerned. (i) Investigation of kinetic modeling that describes the non-continuity/non-equilibrium flows. Most of the current researches are based on macroscopic models. However, there exist abundant small structure (and fast-changing patterns) behaviors and effects such as the shock wave, boundary layer, material defects, etc. For cases with small structures, the mean free path of molecules cannot be ignored compared to the characteristic length, i.e., the non-continuity (discreteness) of the system is pronounced, which challenge the rationality and physical function of the macroscopic models based on the continuity hypothesis. For cases with fast-changing patterns, the system dose not have enough time to relax to the thermodynamic equilibrium state, i.e., the system may significantly deviate from the thermodynamic equilibrium state. Therefore, the rational-ity and physical function of the macroscopic models based on the hypothesis of thermodynamic equilibrium (or near thermodynamic equilibrium) will be challenged. (ii) Improvement of method that describes the evolution characteristics of bubbles and flows morphology."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a plausible explanation for the observed deficits in astrocyte-mediated spine and synaptic pathology in Downs syndrome, based on the information provided in the two studies?",
    "choices": [
      "A) Astrocytes in DS patients have impaired mitochondrial function, leading to metabolic alterations in protein processing and secretion, which in turn disrupts the complex network of proteins linking extracellular signals with the actin cytoskeleton, resulting in abnormal dendritic spine morphology.",
      "B) The reduced number and aberrant architecture of dendritic spines in DS patients are a result of impaired astrocyte function, which leads to a decrease in the release of soluble factors and physical contact with neurons, ultimately causing a decrease in excitatory connections within the brain.",
      "C) The deficits in astrocyte function in DS patients lead to an increase in the number of dendritic spines, which in turn causes an overabundance of excitatory connections within the brain, resulting in abnormal neural circuitry and processing.",
      "D) The abnormalities in dendritic spine morphology and numbers in DS patients are a result of genetic mutations in synaptic proteins, which disrupt the complex network of proteins linking extracellular signals with the actin cytoskeleton, leading to impaired astrocyte function."
    ],
    "correct_answer": "A",
    "documentation": [
      "JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols\nA role for thrombospondin-1 deficits in astrocyte-mediated spine and synaptic pathology in Downs syndrome. Octavio Garcia, Maria Torres, Pablo Helguera, Pinar Coskun, Jorge Busciglio. PUBLISHED: 07-02-2010\tDowns syndrome (DS) is the most common genetic cause of mental retardation. Reduced number and aberrant architecture of dendritic spines are common features of DS neuropathology. However, the mechanisms involved in DS spine alterations are not known. In addition to a relevant role in synapse formation and maintenance, astrocytes can regulate spine dynamics by releasing soluble factors or by physical contact with neurons. We have previously shown impaired mitochondrial function in DS astrocytes leading to metabolic alterations in protein processing and secretion. In this study, we investigated whether deficits in astrocyte function contribute to DS spine pathology. Analysis of Dendritic Spine Morphology in Cultured CNS Neurons Authors: Deepak P. Srivastava, Kevin M. Woolfrey, Peter Penzes. Published: 07-13-2011 JoVE Neuroscience\nDendritic spines are the sites of the majority of excitatory connections within the brain, and form the post-synaptic compartment of synapses. These structures are rich in actin and have been shown to be highly dynamic. In response to classical Hebbian plasticity as well as neuromodulatory signals, dendritic spines can change shape and number, which is thought to be critical for the refinement of neural circuits and the processing and storage of information within the brain. Within dendritic spines, a complex network of proteins link extracellular signals with the actin cyctoskeleton allowing for control of dendritic spine morphology and number. Neuropathological studies have demonstrated that a number of disease states, ranging from schizophrenia to autism spectrum disorders, display abnormal dendritic spine morphology or numbers. Moreover, recent genetic studies have identified mutations in numerous genes that encode synaptic proteins, leading to suggestions that these proteins may contribute to aberrant spine plasticity that, in part, underlie the pathophysiology of these disorders."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a critical consideration when administering supplemental vitamin K to patients taking warfarin, and how does it relate to the timing of dosing?",
    "choices": [
      "A) The timing of dosing should be based solely on the patient's age, as older adults may require more vitamin K to counteract warfarin's effects.",
      "B) The proper anticoagulant action of warfarin and vitamin K is a function of vitamin K intake and drug dose, and should be individualized for each patient based on their unique metabolic profile.",
      "C) Patients taking warfarin should only receive supplemental vitamin K if they have a history of bleeding disorders, as this can help prevent further bleeding episodes.",
      "D) The maximum effect of warfarin and vitamin K is achieved within 24 hours of dosing, and therefore, patients should receive supplemental vitamin K immediately after taking their warfarin."
    ],
    "correct_answer": "B",
    "documentation": [
      "Even doses in rats as high as 250 mg/kg, body weight did not alter the tendency for blood-clot formation to occur.[14]\nUnlike the safe natural forms of vitamin K1 and vitamin K2 and their various isomers, a synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells.[2]\nPhylloquinone (K1)[15][16] or menaquinone (K2) are capable of reversing the anticoagulant activity of the anticoagulant warfarin (tradename Coumadin). Warfarin works by blocking recycling of vitamin K, so that the body and tissues have lower levels of active vitamin K, and thus a deficiency of vitamin K.\nSupplemental vitamin K (for which oral dosing is often more active than injectable dosing in human adults) reverses the vitamin K deficiency caused by warfarin, and therefore reduces the intended anticoagulant action of warfarin and related drugs.[17] Sometimes small amounts of vitamin K are given orally to patients taking warfarin so that the action of the drug is more predictable.[17] The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient.[citation needed] The action of warfarin and vitamin K both require two to five days after dosing to have maximum effect, and neither warfarin or vitamin K shows much effect in the first 24 hours after they are given.[18]\nThe newer anticoagulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K, and may be taken with supplemental vitamin K.[19][20]\nVitamin K2 (menaquinone). In menaquinone, the side chain is composed of a varying number of isoprenoid residues. The most common number of these residues is four, since animal enzymes normally produce menaquinone-4 from plant phylloquinone. A sample of phytomenadione for injection, also called phylloquinone\nThe three synthetic forms of vitamin K are vitamins K3 (menadione), K4, and K5, which are used in many areas, including the pet food industry (vitamin K3) and to inhibit fungal growth (vitamin K5).[21]\nConversion of vitamin K1 to vitamin K2[edit]\nVitamin K1 (phylloquinone) – both forms of the vitamin contain a functional naphthoquinone ring and an aliphatic side chain."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "As a preacher, you want to create a compelling sermon that engages your congregation. You've developed a process to achieve this, which involves several steps. However, you've noticed that if you skip one of these steps, the entire process is compromised. Which step is most crucial to the overall effectiveness of your sermon?",
    "choices": [
      "A) Reading the sermon out loud to identify awkward phrasing",
      "B) Memorizing the entire sermon to ensure smooth delivery",
      "C) Creating a detailed outline to structure the sermon",
      "D) Choreographing your movements to convey confidence and authority"
    ],
    "correct_answer": "B",
    "documentation": [
      "In order to make it presentable, I have a few more steps I go through, and these typically take me a week all by themselves. My goal is to make the sermon sound as natural and engaging as possible. First, I read the sermon out loud and mark anything that doesn’t sound like me. Maybe I was copying someone’s tone, or more likely my tone was too formal or too informal for the moment. I also italicize the words I want to emphasize. It’s all about the sound. Second, I memorize the sermon. (Yes, the whole thing.) This is what they trained us to do in seminary, and I thought it was overkill. Yes, you can get better eye contact, step away from the podium, I get that. But what I’ve discovered is that when I memorize my work it polishes the sermon like nothing else. If I can’t remember what I’m about to say, how can I expect the congregation to remember? Memorizing forces me to find the best words for the job. It also helps me on a structural level, because if I can’t remember what I was about to say next, it shows that there’s a weak connection between the two points. In a compelling script, the next thing has to follow the last. Once you know why the two are married, you can go back and make it more obvious to the congregation. As I memorize, I boil down the transcript into a preaching outline, which has just enough structure and content to cue me if my mind goes blank in the pulpit. It will have the necessary structural elements, markers for key phrases, and all condensed so that it fits on just a few pages on the platform. (One danger is if I don’t use it in practice, it’s less helpful on Sunday.) Third—and frankly this is the step I’m most likely to skip—I try to choreograph my movements. I believe good preaching is theater, but not in the sense that you’re dramatizing the text. Your whole body is communicating whether you want it to or not, so your gestures should be purposeful. Use the space to organize thoughts, repeat certain motions when you repeat the same thought, make sure you’re not sending mixed signals."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "When a robot and a human collaborate to move an object from one end of a room to the other, what is a potential risk of inferring the goal and the preferred path as separate problems?",
    "choices": [
      "A) The robot may become over-confident in its belief about the human's preference for a particular path, leading to a failure in completing the task.",
      "B) The robot may only consider the human's input up to the current point in time, without taking into account the robot's own knowledge of the environment.",
      "C) The robot may incorrectly assume that the human's preferred path is the most efficient route to the goal, leading to a suboptimal solution.",
      "D) The robot may become stuck in an infinite loop of recomputing the set of homotopies at every iteration, making the belief update challenging."
    ],
    "correct_answer": "A",
    "documentation": [
      "Bhattacharya propose an efficient algorithm for solving pathplanning problems under homotopic constraints. However, the number of homotopy classes for a given problem can be infinite, and as the robot changes location and updates its representation of the world, carrying out inference over homotopy classes in a dynamic environment requires recomputing the set of homotopies at every iteration, making the belief update challenging. Prior work has addressed the challenge of shared autonomy by considering how robots can infer a human's intended goal, or how they can infer the preferred path to a goal. However, we argue that inferring the goal and the path as separate problems can lead to over-confidence in incorrect beliefs about the user's preferences. To illustrate this point, consider the following scenario: a robot and a human are collaborating to move an object from one end of a room to Fig. : Using the hyperplanes composing the H-representation of each obstacle, we construct a hyperplane arrangement of the obstacle-free space (a). We define the human's preference for the robot's one step action choices as the posterior distribution (given all human input up to that point) over transitions from the current to the neighboring polytopes, i.e. edges on the graph. Each time the robot transitions to a new polytope, the set of neighbor polytopes and the distribution over human preferences are updated. another, but there is an obstacle in the way. The human would like the robot to take a path around the obstacle on the left, even though the goal is on the right. If the robot only infers the goal from the human's inputs, it may incorrectly assume that the goal is on the right, and become over-confident in this belief. On the other hand, if the robot only infers the preferred path, it may mistakenly assume that the goal is on the left, leading to a failure in completing the task. To overcome these challenges, our work proposes a joint inference approach that considers both the human's intended goal and their preferred path to that goal."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the NFPA method, what is the role of the consistency term $\\hat{D}_F$ in the LO equation?",
    "choices": [
      "A) To account for large-angle scattering in the HO equation",
      "B) To force the transport and modified FP equations to be consistent",
      "C) To solve the HO equation using Picard iteration",
      "D) To integrate the LO equation into multiphysics models"
    ],
    "correct_answer": "B",
    "documentation": [
      "\\end{equation}\nThe role of $\\hat{D}_F$ is to force the transport and modified FP equations to be consistent. Subtracting \\cref{mfp1} from \\cref{transport1} and rearranging, we obtain the consistency term\n\\begin{equation}\n\\label{dfp}\n\\hat{D}_F = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_l - \\frac{\\sigma_{tr}}{2}\\frac{\\partial}{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi}{\\partial \\mu} - \\sigma_{s,0} \\psi\\,. \\end{equation}\n\nThe NFPA method is given by the following equations:\n\\begin{subequations}\\label[pluraleq]{holocons}\n\\begin{align}\n\\label{HO1}\n\\text{HO}&: \\mu\\frac{\\partial \\psi_{HO}}{\\partial x} + \\sigma_t \\psi_{HO} = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_{l, LO} + Q\\,,\\\\\n\\label{LO11}\n\\text{LO}&: \\mu\\frac{\\partial \\psi_{LO}}{\\partial x} + \\sigma_a \\psi_{LO} = \\frac{\\sigma_{tr}}{2}\\frac{\\partial }{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi_{LO}}{\\partial \\mu} + \\hat{D}_F + Q\\,,\\\\\n\\label{con1}\n\\text{Consistency term}&: \\hat{D}_F = \\sum_{l=0}^L \\frac{(2l+1)}{2} P_l \\sigma_l \\phi_{l, HO}^m - \\frac{\\sigma_{tr}}{2}\\frac{\\partial }{\\partial \\mu} (1-\\mu^2) \\frac{\\partial \\psi_{HO}}{\\partial \\mu} - \\sigma_{s,0} \\psi_{HO}\\,,\n\\end{align}\n\\end{subequations}\nwhere $\\psi_{HO}$ is the angular flux obtained from the HO equation and $\\psi_{LO}$ is the angular flux obtained from the LO equation. The nonlinear HOLO-plus-consistency system given by \\cref{holocons} can be solved using any nonlinear solution technique \\cite{kelley}. Note that the NFPA scheme returns a FP equation that is consistent with HO transport. Moreover, this modified FP equation accounts for large-angle scattering which the standard FP equation does not. The LO equation (\\ref{fp1}) can then be integrated into multiphysics models in a similar fashion to standard HOLO schemes \\cite{patelFBR}. To solve the HOLO-plus-consistency system above, we use Picard iteration \\cite{kelley}:\n\\begin{subequations}\n\\begin{align}\n\\label{H1}\n\\text{Transport Sweep for HO}&:\n\\mathcal{L} \\psi_{HO}^{k+1} = \\mathcal{S} \\psi_{LO}^{k} + Q, \\\\\n\\label{L1}\n\\text{Evaluate Consistency Term}&: \\hat{D}_F^{k+1} = \\left(\\mathcal{S} - \\mathcal{F} - \\sigma_{s,0}\\mathcal{I}\\right) \\psi_{HO}^{k+1}, \\\\\n\\label{c1}\n\\text{Solve LO Equation}&: \\psi_{LO}^{k+1} = \\mathcal{P}^{-1} \\left(\\hat{D}_F^{k+1} + Q\\right), \n\\end{align}\n\\end{subequations}\nwhere $\\mathcal{L}$ and $\\mathcal{S}$ are given in \\cref{trans1}, $\\mathcal{P}$ and $\\mathcal{F}$ are given in \\cref{FPSAsi1}, $\\mathcal{I}$ is the identity operator, and $k$ is the iteration index."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A robot is navigating through a cluttered office space with a goal location marked on the floor. The robot receives direction indications from a human that indicate which path it should take to get to the goal. The human wants the robot to go around the desks on the right side of the classroom. However, the robot's path preference is not explicitly stated. The robot's navigation system uses a probabilistic representation of human preference to adapt to the human's recommendations. What is the primary challenge in using homotopy classes to encode and infer human preferences in this scenario?",
    "choices": [
      "A) The complexity of the navigation system increases with the number of obstacles in the space.",
      "B) The robot's path preference is not explicitly stated, making it difficult to infer human preferences.",
      "C) The use of homotopies can pose computational challenges when encoding and inferring human preferences.",
      "D) The human's preference is not taken into account when the robot's goal is unknown."
    ],
    "correct_answer": "C",
    "documentation": [
      "To optimize the use of human input and quickly infer the human's preference, Fig. : An autonomous robot navigates in a simulated classroom towards a goal location (pink circle). At the start of its mission, it receives direction indications (arrows) from a human that indicate which path it should take to get to the goal. In this scenario, the human wants the robot to go around the desks on the right side of the classroom. A robot that does not reason over path preferences (green) will take the shortest path to the goal regardless of the human's input. Our method (blue) infers the human's path preference from these indications and adapts to their recommendations. we propose an approach that leverages probabilistic representations of human preference and incorporates real-time feedback. Previous research by Bajcsy et al. considered an online adaptation problem in a manipulation task, where the person can apply forces to the robot to indicate their preferences. By allowing the robot to continue its task while taking into account a probabilistic representation of human preference, their approach does not require frequent inputs. Building on this idea, we adopt a similar approach to adapt to a human's preference in the context of a robot autonomously navigating through a known environment, such as a cluttered office space. Specifically, we focus on allowing the human to influence the robot's trajectory with respect to obstacles, by providing guidance on preferred routes or paths, while the robot continues to execute its task. Paths can be represented using homotopy classes . However, homotopies can pose computational challenges when used to encode and infer human preferences. When the robot maintains a belief over homotopy classes, the inference problem can become exponentially complex with the number of obstacles in the space. Additionally, when the goal is unknown, the number of variables increases with the number of candidate destinations. This complexity can render the decision-making problem intractable."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A local authority in a rural area, falling within a Planning Area, wants to construct a new community center that includes a school, a library, and a community hall. The community center will be built on land that is currently used for agriculture. The local authority has already obtained permission from the Director for the construction of the community center, but they need to ensure that the development complies with the Interim Development Plan and the Development Plan for the area.\n\nWhich of the following is a necessary condition for the local authority to proceed with the construction of the community center?",
    "choices": [
      "A) The community center must be built on land that is currently used for commercial activities, such as a farm or a shop.",
      "B) The community center must be built on land that is already zoned for residential activities, such as a farm-house or a residential house.",
      "C) The community center must be built on land that is within the limits of the Interim Development Plan and the Development Plan for the area.",
      "D) The community center must be built on land that is owned by the local authority or a special authority constituted under this Act."
    ],
    "correct_answer": "C",
    "documentation": [
      "30. Application for permission for development by others. - (1) Any person, not being the Union Government, State Government, a local authority or a special authority constituted under this Act intending to carry out any development on any land, shall make an application in writing to the Director for permission, in such form and containing such particulars and accompanied by such documents as may be prescribed. (2) Such application shall also be accompanied by such fee as may be prescribed. [30A. Exemption from development permission in rural areas falling within Planning or Special Area. - (1) Any person who owns land in rural areas, falling within Planning or Special Areas wherein neither Interim Development Plan nor Development Plan has been notified, shall be exempted from permission under this Act for the following development activities up to the limits as may be prescribed: -\n(i) Residential activities such as farm-houses and residential houses up to three storeys, cattle shed, toilet, septic tank, kitchen, store, parking shed or garage and rain shelter;\n(ii) Commercial activities such as basic commercial activities like shops of general merchandise, cobbler, barber, tailoring, fruit, vegetable, tea or sweet, eating places and dhabas, chemist and farm produce sale depot;\n(iii) Service Industries such as cottage or house-hold, service industries like carpentry, knitting, weaving, blacksmith, goldsmith, atta-chakki with capacity up to five horse-power, water mill, agriculture equipments or machinery repair, electrical, electronic and house-hold appliances;\n(iv) Public amenities such as public amenities like panchayat offices, schools, mahila mandals, yuvak mandals, community halls, post offices, dispensaries and clinics (including health, veterinary and Indian System of Medicines) information technology kiosks, patwar khanas, guard huts, anganwaries, electricity and telephone installations and connections, roads and paths, ropeways, water tanks, rain harvesting tanks, overhead or underground water tan."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the distance to XMMSL1~J060636.2-694933 based on the uncertainty in the estimated peak magnitude at outburst?",
    "choices": [
      "A) The distance estimate is likely to be within 10 kpc of the Sun.",
      "B) The distance estimate is likely to be between 50 and 100 kpc of the Sun.",
      "C) The distance estimate is likely to be outside of the Milky Way galaxy.",
      "D) The distance estimate is likely to be within 10 kpc of the center of the Milky Way."
    ],
    "correct_answer": "C",
    "documentation": [
      "We have no\ninformation over the 12 days between the data point of maximum\nbrightness and the lower limit prior to this (Fig.\\,\\ref{optlc}), and\ntherefore we have no exact outburst date, nor exact apparent\nmagnitude at outburst. Assuming for the moment though that we have\ncaught the outburst exactly in the Sep.~30, 2005 observation, then we\ncan estimate (Sect.~5.3) $t_{2}$ to be 8$\\pm$2\\,days, and using this,\nwe can estimate (Della Valle \\& Livio 1995) the absolute magnitude at\nmaximum brightness $M_{V}$ to be --8.7$\\pm$0.6. An absolute magnitude\nof $M_{V}$=--8.7 implies a peak luminosity $\\sim$7 times the Eddington\nluminosity for a 1\\,$M_{\\odot}$ white dwarf. This is quite typical of\nnovae. With $A_{V}$=0.39$^{+0.05}_{-0.09}$ (90\\% error), as derived (Predehl\n\\& Schmitt 1995) from $N_{\\rm\n  H}$=6.9$^{+1.0}_{-1.6}\\times10^{20}$\\,cm$^{-2}$ (from the highest\nstatistic spectral fit; the XMM-Newton ToO observation), and with\n$M_{V}$=--8.7$\\pm$0.6, and a peak $m_{V}$ of 12.0, we can derive a\ndistance to XMMSL1~J060636.2-694933 of 115$^{+43}_{-30}$\\,kpc. As\ndiscussed above however, we are unsure as to the exact outburst date\nand the maximum brightness at outburst. Our assumed peak $m_{V}$ of\n12.0 is almost certainly an underestimation. Although we have no\ninformation in the 12 days prior to Sep.~30, 2005, a simple linear\nextrapolation of the early October lightcurve back prior to Sep.~30,\n2005 suggests that the actual peak $m_{V}$ was somewhere between 9 and\n12. The corresponding distance estimates are then between 29 and\n115\\,kpc (with a mid-point $m_{V}$=10.5 value yielding a distance\nestimate of 58\\,kpc). Many methods have been used to estimate the\ndistance to the LMC (e.g.  Kovacs 2000, Nelson et al.\\ 2000), but a\nvalue of around 50\\,kpc appears to be quite robust. Our distance\nestimate is certainly consistent with that of the LMC, though the\nerrors are quite large. It does appear to be the case however, that\nour distance estimate places the source far outside of our own Galaxy. This, together with the source's position on the sky (at the eastern\nedge of the LMC) and the sizable ($\\sim$Galactic) X-ray hydrogen\ncolumn densities obtained from the spectral fits, suggest strongly\nthat XMMSL1~J060636.2-694933 lies within the LMC itself."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider the asymptotic order of E[exp{λ H(f ) ∞ }] for n, p → ∞, where λ > 0 may depend on n, p or not. By the exponential decay property of the kernel K stated in Lemma 2, we can bound the moment generating function of H(f ) ∞ as follows:",
    "choices": [
      "A) √ 2 exp{λ H(f ) ∞ } ≤ exp{λ ∑_{k=1}^T γ h (x_k, x_k) } ≤ √ 2 exp{λ H(f ) ∞ } for all x_k",
      "B) √ 2 exp{λ H(f ) ∞ } ≤ exp{λ ∑_{k=1}^T γ h (x_k, x_k) } ≤ √ 2 exp{λ H(f ) ∞ } for all x_k, where γ h (x_k, x_k) = (T h)^-1",
      "C) √ 2 exp{λ H(f ) ∞ } ≤ exp{λ ∑_{k=1}^T γ h (x_k, x_k) } ≤ √ 2 exp{λ H(f ) ∞ } for all x_k, where γ h (x_k, x_k) = (T h)^-1 and T h is a constant independent of n, p",
      "D) √ 2 exp{λ H(f ) ∞ } ≤ exp{λ ∑_{k=1}^T γ h (x_k, x_k) } ≤ √ 2 exp{λ H(f ) ∞ } for all x_k, where γ h (x_k, x_k) = (T h)^-1 and T h is a constant independent of n, p, and λ"
    ],
    "correct_answer": "C",
    "documentation": [
      "In the next paragraph, we derive the asymptotic order of E[exp{λ H(f ) ∞ }] for n, p → ∞, where λ > 0 may depend on n, p or not. By the exponential decay property of the kernel K stated in Lemma 2 holds First, H(f ) ∞ is bounded with the maximum over a finite number of points. Calculating the derivative of s : Since δ δx s(x) > 0 almost surely for x = x k , the extrema occur at x k , k = 1, ..., T . Thus, for λ > 0 the moment generating function of H(f ) ∞ is bounded by\nLet M j = ( T h) −1 T k=1 γ h (x j , x k ), which by Lemma 2 is bounded uniformly in j by some global constant M > 0. By the convexity of the exponential function we obtain √ 2 and by assumption 0 ≤ δ ≤ f ≤ M 0 . Using Lemma 3, Q k can be written as a sum of m = np/T independent gamma random variables, i.e.\nThe moment generating function of | log(X)| when X follows a Γ(a, b)-distribution is given by where Γ(a) is the gamma function and γ(a, b) is the lower incomplete gamma function. In particular, To derive the asymptotic order of E[exp{λ H(f ) ∞ }] for n, p → ∞ we first establish the asymptotic order of the ratio Γ(a + t)/Γ(a) for a → ∞.\nWe distinguish the two cases where t is independent of a and where t linearly depends on a. Thus, for 0 < t < a and t independent of a, equation (A.17) implies for a → ∞ that Γ(a + t)/Γ(a) = O(a t ). Similarly, it can be seen that Γ(a − t)/Γ(a) = O(a −t ). If 0 < t < a and t linearly depends on a, i.e. t = ca for some constant c ∈ (0, 1), then we get Γ(a ± t)/Γ(a) = O(a ±t exp{a}) for a → ∞.\nHence, for a fixed λ not depending on n, p and such that 0 < λ < m/( √ 2M j ) we get for sufficiently large n, p If λ = cm such that 0 < λ < m/( √ 2M j ), then for sufficiently large n, p b∈{cδ/m,cM 0 /m} (bm/2) for some constant L > 1. Set K = min j=1,..., T 1/( √ 2M j ) which is a constant independent of n, p. Altogether, we showed that for 0 < λ < Km and n, p → ∞\nBounding the right hand side of (A.15) for some constants c 0 , c 1 > 0 and n, p → ∞ Since g lies between H(f ) and H(f ), and f almost surely pointwise."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The philosopher's assertion that the power of an object to produce a human experience is a fundamental aspect of its nature can be seen as a statement of the obvious. However, this idea has been challenged by a school of western philosophy. According to the philosopher, what is the primary reason why we can trust our perceptions of the physical world, including the existence of buses?",
    "choices": [
      "A) Because our senses are enhanced by instruments that allow us to observe the physical world in greater detail.",
      "B) Because the multiplicity of observers and their reports provides evidence for the external world.",
      "C) Because the physical world is governed by laws that can be verified through science, and our senses are capable of detecting these laws.",
      "D) Because our experiences, including perceptions of the physical world, are preserved over time as long-term memories."
    ],
    "correct_answer": "D",
    "documentation": [
      "This question veritably answers itself. Only a madman would deny the evidence of his own senses. It is essential to understand that the correspondence of which I speak depends on the reality of motion [from which we derive the ideas of time and space]. To keep ourselves safe, it is necessary that we have the ability to know when a material object is moving closer or further from us and to be able to recognize an object as a danger. This, the senses give us, for perceptions like all other experiences are memories [are preserved over time]. An object is recognized as a danger through prior sensory experiences preserved as long-term memories. In order to be recognized and remembered as a danger, a material object must have the power to produce a particular human experience of it. That power is part of the nature of the object and is thus truly reflected in the perception of it—even though there may be more to the object than its power to yield a human perception. To the reasonable mind, the above comments may properly be seen as statements of the obvious. The curious fact, however, is that a whole school of western philosophy has labored mightily to deny the obvious. I agree; I'm only delving into the inner experience to see how it works and what may become of that.\nby TheVat on April 22nd, 2018, 11:57 am\nRJG, this tablet ate the quoted part of your post and somehow hid the submit button, so sorry about the missing comment.... No, I was not assuming the in-the-moment knowledge, but rather that facts about buses are physically verifiable when science is applied. It is not difficult to verify that I was neither dreaming nor hallucinating. We are saved from solipsism by the multiplicity of observers and their reports. We can open a book and read complex prose (something that can't be done in dreams) that reveals areas of knowledge utterly unknown to us and beyond our previous experiences. We have senses enhanced by instruments that can show us photons leaving the photosphere of the sun and bouncing off solid objects like buses in particular and regular patterns, etc."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study on hand gesture and postural activity recognition using Feature Weighted Naive Bayes (FWNB) classification achieved significant improvements over baseline approaches. According to the results shown in Fig~\\ref{fig:hand_gesture_accuracy}, what is the accuracy of the FWNB method for 8-hand gestural activity recognition?",
    "choices": [
      "A) 85%",
      "B) 92%",
      "C) 95%",
      "D) 98%"
    ],
    "correct_answer": "B",
    "documentation": [
      "Then, the start/end duration error is 9 minutes ($|$5 minutes delayed start$|$ + $|$4 minutes hastened end$|$), in an overall error of e.g., 30\\% (9/30=0.3). We measure cross-participant accuracy using leave-two-participants-out method for performance metrics, i.e., we take out two of the participants' data points from the entire dataset, train our proposed classification models, test the model accuracy on the two left-out participants relevant data points, and continue the process for entire dataset. \\begin{figure*}[!htb]\n\\begin{minipage}{0.45\\textwidth}\n\\begin{center}\n   \\epsfig{file=hand_gesture_accuracy.pdf,height=1.6in, width=3in}\n\\caption{Feature Weighted Naive Bayes (FWNB) classification accuracy comparisons with baseline approaches (graphical signatures of all hand gestures are shown).}\n   \\label{fig:hand_gesture_accuracy}\n\\end{center}\n\\end{minipage}\n\\begin{minipage}{0.29\\textwidth}\n\\begin{center}\n\\vspace{-.12in}\n   \\epsfig{file=posture_accuracy_normal.pdf,height=1.6in, width=2.1in}\n\\caption{4-class postural level activity recognition performance and comparisons with baseline method}\n   \\label{fig:posture_accuracy_normal}\n\\end{center}\n\\end{minipage}\n\\begin{minipage}{0.25\\textwidth}\n \\begin{center}\n \\vspace{-.12in}\n   \\epsfig{file=posture_accuracy_extended.pdf,height=1.6in, width=2.1in}\n\\caption{6-class diverse postural activity recognition framework accuracy comparisons with the baseline approach.}\n   \\label{fig:posture_accuracy_extended}\n\\end{center}\n \\end{minipage}\n\\end{figure*}\n\nFig~\\ref{fig:hand_gesture_accuracy} displays Feature Weighted Naive Bayes (FWNB) based the 8-hand gestural activity recognition accuracies comparisons with the baseline methods which clearly depicts the outperformance of our method (5\\% improvement) with an overall accuracy of 92\\% (FP rate 6.7\\%) in RCC dataset. For postural activity recognition, dataset achieving 91\\% postural activity recognition accuracy (FP rate 9.5\\%) which outperforms the baseline approach significantly (8\\% improvement)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the consolidation of WAMD and KFOY into a single station, KSTP, in 1928?",
    "choices": [
      "A) To create a more efficient broadcasting schedule",
      "B) To take advantage of the newly available 1360 kHz frequency",
      "C) To comply with the Federal Radio Commission's (FRC) General Order 40, which assigned KSTP to a \"high-powered regional\" frequency of 1460 kHz",
      "D) To allow for the expansion of KSTP's broadcasting capabilities to include high-fidelity audio programming"
    ],
    "correct_answer": "C",
    "documentation": [
      "Beginning on November 24, 1927 the WAMD broadcasts, still on 1330 kHz, were shifted to KFOY's facility in St. Paul. (At this time KFOY was assigned to 1050 kHz). The next day it was announced that National Battery had purchased KFOY, and as of December 1, 1927 both KFOY and WAMD were reassigned to 1350 kHz. WAMD continued making regular broadcasts until the end of March 1928, while KFOY, although it continued to be licensed for a few more months on a time-sharing basis with WAMD, ceased operations at this point. National Battery Company\nIn mid-December 1927, the National Battery Company announced it had received permission from the Federal Radio Commission (FRC) to build a new station, with the call letters KSTP, operating from a transmitter site to be constructed three miles south of Wescott. The next month it was reported that the new station, still under construction, had been assigned to 1360 kHz. KSTP made its debut broadcast on March 29, 1928. Although technically it was a separate station from WAMD and KFOY, both of which were formally deleted on April 30, 1928, overall KSTP was treated as the direct successor to a consolidated WAMD and KFOY. Hubbard became the merged station's general manager, acquiring controlling interest in 1941. A month after the merger, KSTP became an affiliate for the NBC Red Network. It remained with NBC for 46 years. On November 11, 1928, under the provisions of the FRC's General Order 40, KSTP was assigned to a \"high-powered regional\" frequency of 1460 kHz. The only other station assigned to this frequency was WTFF in Mount Vernon Hills, Virginia (later WJSV, now WFED, Washington, D.C.). On February 7, 1933, the FRC authorized KSTP to increase its daytime power to 25 KW. In 1938 and 1939 KSTP also operated a high-fidelity AM \"experimental audio broadcasting station\" Apex station, W9XUP, originally on 25,950 kHz and later on 26,150 kHz. In 1941, as part of the implementation of the North American Regional Broadcasting Agreement, KSTP was assigned to its current \"clear channel\" frequency of 1500 kHz, with the provision that it and WJSV, as \"Class I-B\" stations, had to maintain directional antennas at night in order to mutually protect each other from interference."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The author of the text describes a process for creating a fiberglass skin for a model aircraft. However, the text also mentions the importance of masking off the fuselage to prevent epoxy from getting on it. What is the likely consequence of not masking off the fuselage during the application of epoxy?",
    "choices": [
      "A) The epoxy will be evenly distributed and provide a strong bond between the fiberglass skin and the fuselage.",
      "B) The epoxy will seep into the small gaps between the fiberglass skin and the fuselage, creating a weak bond and potentially causing the skin to delaminate.",
      "C) The epoxy will not adhere to the fuselage at all, resulting in a weak and fragile bond between the skin and the fuselage.",
      "D) The epoxy will harden too quickly, making it difficult to remove from the fuselage."
    ],
    "correct_answer": "B",
    "documentation": [
      "Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo. After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade. After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The author's response to the RSPH pamphlet can be characterized as a form of \"epistemic resistance\" to the vaccine program. This resistance is rooted in a deep-seated skepticism about the germ theory of disease and the safety of vaccines. The author's argument is that the outcome of disease is not solely determined by the virulence of the pathogen, but also by the individual's immune system. This perspective is based on the idea that the immune system can be compromised by various factors, including environmental toxins and other pathogens. The author's skepticism is also fueled by their perception that the vaccine program is driven by a desire to silence critics and maintain the products' safety through public relations rather than scientific evidence.",
    "choices": [
      "A) The author's skepticism about the germ theory of disease is a result of their personal experience with a severe illness that they overcame as a child.",
      "B) The author's argument that the outcome of disease is not solely determined by the virulence of the pathogen is based on their interpretation of the quote from the European Parliament, which suggests that the immune system plays a crucial role in determining the outcome of disease.",
      "C) The author's resistance to the vaccine program is driven by their concern that the vaccines are not safe and may cause more harm than good, as evidenced by their mention of the \"billions of viruses\" that humans are exposed to every day.",
      "D) The author's skepticism about the vaccine program is rooted in their distrust of the pharmaceutical industry and the government's role in promoting vaccines, as suggested by their mention of Paul Offit and Senator Pan."
    ],
    "correct_answer": "B",
    "documentation": [
      "It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be \"independent\" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so. Please help give the ICAN letter the widest possible distribution, particularly to politicians. \"The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system.\"\nNope. This makes no sense. Lots of people who seemed vibrant will get a very severe case of the same illness that a vulnerable baby overcomes in a day. And under the germ theory it doesn't matter how strong your immune system *was*. Once it's been overcome by the pathogen it is every bit as weak as anybody else's with that pathogen. What you say makes no sense. There's no reason for me to reply to you again. \"Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared?\"\nWhy do you keep asking this question when I've already provided the answer hundreds of times? Why are you so desperate to believe the people who you already recognize are harming our children? Why would Walter Reed be any more trustworthy than Paul Offit or Senator Pan? Why would Jenner or Pasteur? And you went no way to explaining my arguments against germ theory. If we are attacked by billions of viruses every day then if even a tiny fraction of them are pathogenic then we couldn't possibly survive."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What significant event in Hugh H. Goodwin's early life led to his decision to enlist in the United States Navy in 1917, despite not having completed high school?",
    "choices": [
      "A) The sinking of the passenger ship Lusitania, which was a major concern for the United States at the time, and Goodwin felt a strong sense of duty to serve.",
      "B) The outbreak of World War I, which drew the United States into the conflict, and Goodwin saw it as an opportunity to serve his country and gain valuable experience.",
      "C) The United States' entry into World War I, which led to a surge in recruitment efforts, and Goodwin was eager to join the military and make a difference.",
      "D) The Navy's need for additional personnel to man its ships, and Goodwin saw an opportunity to serve his country and gain a sense of purpose."
    ],
    "correct_answer": "B",
    "documentation": [
      "Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War. Following the War, he remained in the Navy and rose to the flag rank and held several important commands including Vice Commander, Military Air Transport Service, Commander, Carrier Division Two and Commander, Naval Air Forces, Continental Air Defense Command. Early life and career\n\nHugh H. Goodwin was born on December 21, 1900, in Monroe, Louisiana and attended Monroe High School there (now Neville High School). Following the United States' entry into World War I in April 1917, Goodwin left the school without receiving the diploma in order to see some combat and enlisted the United States Navy on May 7, 1917. He completed basic training and was assigned to the battleship . Goodwin participated in the training of armed guard crews and engine room personnel as the Atlantic Fleet prepared to go to war and in November 1917, he sailed with the rest of Battleship Division 9, bound for Britain to reinforce the Grand Fleet in the North Sea. Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the United States Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname \"Huge\" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Robert E. Blick Jr., Herbert S. Duckworth, Clayton C. Jerome, James P. Riseley, James A. Stuart, Frank Peak Akers, Sherman Clark, Raymond P. Coffman, Delbert S. Cornwell, Frederick J. Eckhoff, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy, William S. Parsons, Harold R. Stevens, John P. Whitney, Lyman G. Miller and George J. O'Shea."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyze the relationship between deliberative democracy and the concept of \"the people\" in the context of contemporary democratic theory. How do the ideas of Helene Landemore and Jeffrey Edward Green on democratic epistemology and the meaning of the people intersect with the deliberative democratic approach of Amy Gutmann and Dennis Thompson?",
    "choices": [
      "A) Landemore's emphasis on epistemic democracy highlights the importance of individual expertise in shaping public opinion, which is in tension with the collective deliberation of \"the people\" in Gutmann and Thompson's model.",
      "B) Green's notion of \"the people\" as a collective entity is compatible with Gutmann and Thompson's emphasis on deliberative democracy, as both approaches prioritize the collective decision-making process over individual interests.",
      "C) The deliberative democratic approach of Gutmann and Thompson relies on the idea of \"the people\" as a homogeneous entity, which is challenged by Landemore's epistemic democracy that emphasizes the diversity of individual perspectives.",
      "D) Landemore's epistemic democracy is actually a form of \"oracular democracy\" that undermines the democratic process by relying on individual experts rather than collective deliberation."
    ],
    "correct_answer": "C",
    "documentation": [
      "This is a graduate seminar on contemporary topics in democratic theory. Topics to be covered include: democratic epistemology; deliberative democracy; the meaning of the people; oracular democracy; agonistic democracy; and possibly new theories of republicanism, representation and partisanship. Texts (tentative) Helene Landemore, Democratic Reason\nJeffrey Edward Green, The Eyes of the People\nAmy Gutmann and Dennis Thompson, Why Deliberative Democracy? Alan Keenan, Democracy in Question\nJason Frank, Constituent Moments\nJason Frank, Publius and Political Imagination\nNadia Urbanati, Democracy Disfigured\nRussell Muirhead, Partisanship in a Polarized Age\nBryan Garsten, manuscript\nActive seminar participation; an annotated bibliography or review essay; a research/analytic paper. GOV 310L • American Government-Honors 37615 • Fall 2015 Meets TTH 2:00PM-3:30PM BEN 1.106 show description\nTTH 2-3:30/BEN 1.106\nBruce Ackerman,Before the Next Attack: Preserving Civil Liberties in an Age of Terrorism GOV 370L • Presidency In Constitutl Order 37845 • Fall 2015 Meets TTH 5:00PM-6:30PM PAR 310 show description\nGOV 370L (37845) TTH 5-6:30 PAR 310\nThe Presidency in the Constitutional Order\nA study of the place of the presidency in the American political order that stresses tension between power and accountability inherent in the office and the system. Topics include: separation of powers, presidential selection, impeachment, relations with Congress and bureaucracy, emergency powers, presidential character, and leadership. This is a very demanding writing flag class. If you are enrolling in this class just in order to satisfy the writing flag, you are in the wrong class. Interest in political theory and willingness to work very hard are necessary for success in this class. Joseph M. Bessette, The Constitutional Presidency\nAndrew Rudalevige, The New Imperial Presidency\nBruce Ackerman, The Rise and Decline of the American Republic\nMichael Nelson, ed., The Presidency in the Political System\nMichael Nelson, ed., The Evolving Presidency\nLouis Fisher, Constitutional Conflicts Between Congress and the President\nActive and prepared class participation\nRegular quizzes on the reading\nFour analytic essays (approximately 1200 words)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the context of the SLAS system, what is the primary objective of the speed and lane advisory system, and how does it balance travel time minimization with passenger comfort?",
    "choices": [
      "A) To minimize travel time at the expense of passenger comfort, by dynamically adjusting the ego vehicle's speed and lane position based on real-time traffic conditions and passenger preferences.",
      "B) To prioritize passenger comfort over travel time minimization, by adjusting the ego vehicle's speed and lane position to ensure a smooth and safe ride, while also considering the impact on travel time.",
      "C) To optimize both travel time and passenger comfort simultaneously, by using a receding horizon mixed-integer optimization approach that takes into account real-time traffic conditions, passenger preferences, and the ego vehicle's speed and lane position.",
      "D) To dynamically adjust the ego vehicle's speed and lane position based on the ego vehicle's own comfort level, rather than considering external factors such as traffic conditions or passenger preferences."
    ],
    "correct_answer": "C",
    "documentation": [
      "Paper Info\n\nTitle: SLAS: Speed and Lane Advisory System for Highway Navigation\nPublish Date: Unkown\nAuthor List: Faizan Tariq, David Isele, John Baras, Sangjae Bae\n\nFigure\n\nFig. 1.Motivational Example. With a slow moving vehicle ahead, the ego vehicle (in blue) may decide to either change lane to the fast moving lane (left) to minimize travel time or adjust its speed without changing lanes to preserve safety but it would be unwise for it to switch to the slow moving lane (right) as that would not benefit travel time or safety.\nFig. 3. Simulation Setup.Scenario Runner sets up the scenario for the CARLA Simulator, which then communicates with the SLAS and the Planning and Control ROS (Robot Operating System) nodes through the ROS bridge node. Fig. 4. Testing scenario with three lanes: lane 0 (left), lane 1 (center) and lane 2 (right).The expected motion of the ego vehicle, over the course of the simulation, is shown with numbered frames. The right most lane (lane 3) is reserved for merging traffic so it is not utilized in our simulation. Fig. 5. Left: Travel time comparison. Center: Lane choice (lateral position) comparison. The center lines of lanes 0 (left), 1 (center) and 2 (right) have fixed lateral displacements of 0m, 3.5m and 7m respectively. Right: Headway comparison. With no leading vehicle, the headway is restricted by the visibility range of 50m.\n\nabstract\n\nThis paper proposes a hierarchical autonomous vehicle navigation architecture, composed of a high-level speed and lane advisory system (SLAS) coupled with low-level trajectory generation and trajectory following modules. Specifically, we target a multi-lane highway driving scenario where an autonomous ego vehicle navigates in traffic. We propose a novel receding horizon mixed-integer optimization based method for SLAS with the objective to minimize travel time while accounting for passenger comfort. We further incorporate various modifications in the proposed approach to improve the overall computational efficiency and achieve real-time performance."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the complicity of doctors and pharmacies in fueling the opioid epidemic in Ohio, based on the information provided about David L. Kidd's ring?",
    "choices": [
      "A) The doctors and pharmacies in Florida were aware of Kidd's ring and actively participated in it.",
      "B) The doctors and pharmacies in Florida were unaware of Kidd's ring and did not knowingly fuel the opioid epidemic in Ohio.",
      "C) The doctors and pharmacies in Florida were complicit in Kidd's ring, but only because they were also victims of the opioid epidemic themselves.",
      "D) The doctors and pharmacies in Florida were not involved in Kidd's ring, but they did provide prescriptions to patients who went on to overdose."
    ],
    "correct_answer": "A",
    "documentation": [
      "Superior Pharmacy not only filled oxycodone prescriptions for pain clinics, it shared waiting room space with a pain clinic in a Temple Terrace strip mall outside Tampa. Neither Masters nor Superior had so much as Googled the background of pain clinic doctors writing those prescriptions, the DEA later said. Had they done so, the DEA dryly noted, they “would likely have come across a press release” announcing one of the doctors had been arrested and charged with trafficking in prescription drugs. Hundreds of thousands of oxycodone pills were sent from Ohio distributors to Florida pharmacies. Unknown thousands of pills headed right back up to Ohio. When Ohio police burst into Christopher Thompson’s home outside Columbus, they found an assault rifle, $80,000 in cash and oxycodone from his Florida deals. A construction worker whose own pill habit started at age 14, Thompson oversaw a ring of 15 Ohio buyers who traveled to Florida to pick up oxycodone to resell in Central Ohio. Two hours to the west in Martin’s Ferry, David L. Kidd orchestrated a ring of buyers traveling to West Palm Beach and Central Florida to pick up oxycodone for resale on the streets of eastern Ohio and West Virginia. Doctors and pharmacies from Florida were complicit with Kidd’s ring in fueling Ohio’s opioid epidemic, wrote the U.S. attorney for West Virginia after Kidd’s 2011 arrest: “The steady flow of pain pills into the Ohio Valley from Florida must stop.” Driving To Pick Up Death By Rx\nWith more drugs came more deaths, in January 2010, say police, Fort Lauderdale pathologist Dr. Lynn Averill started a seven-month oxycodone shopping spree, buying 437,880 oxycodone pills from drug distributors. The same month, Matthew Koutouzis drove from Toms River, N.J., to see Averill in her Broward County pain clinic. The 26-year-old collected prescriptions for 390 pills and overdosed two days later. Brian Moore traveled 13 hours from his Laurel County, Ky., home to see Averill. He left with prescriptions for 600 pills and also overdosed within 48 hours."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the likely outcome for Njoroge's family, given the events of the novel and the ideology of nationalism that drives the plot?",
    "choices": [
      "A) The family will be reunited and Njoroge's aspirations will be fulfilled through the leadership of Jomo Kenyatta.",
      "B) The family will be torn apart by the violence of the Mau Mau rebels and the colonial government, leading to further alienation and disillusionment.",
      "C) Njoroge's family will find salvation through the blood of their ancestors, as prophesied by the author, and will rise to lead the people against the colonial government.",
      "D) The family will be saved by the white man, Mr. Howlands, who will provide them with the custom and traditions purified by learning and travel."
    ],
    "correct_answer": "B",
    "documentation": [
      "It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as \"entering politics\") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo. Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother. Jacobo: Mwihaki's father and an important landowner. Chief of the village. Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school. Themes and motifs\nWeep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt. The novel also ponders the role of saviours and salvation. The author notes in his The River Between: \"Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people.\" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, \"Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel.\" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the electronic structure of 5OS from the DFT-calculated bond lengths and the open-shell resonance structure?",
    "choices": [
      "A) The bond lengths indicate a strong covalent bond between the metal and the molecule, while the open-shell resonance structure suggests a delocalized π system.",
      "B) The bond lengths suggest a weak covalent bond between the metal and the molecule, while the open-shell resonance structure indicates a localized π system.",
      "C) The bond lengths indicate a delocalized π system, while the open-shell resonance structure suggests a localized π system.",
      "D) The bond lengths suggest a localized π system, while the open-shell resonance structure indicates a delocalized π system."
    ],
    "correct_answer": "A",
    "documentation": [
      "In the main text, we focus on the generation and characterization of 5 on insulating surfaces. Generation and characterization of 5 on coinage metal surfaces is shown in Supplementary Fig. . ). Blue and orange colors represent spin up and spin down densities, respectively. c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -Å -3 ). d, DFT-calculated bond lengths of 5OS. e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra. Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side). Acquisition position of the spectra is shown in Supplementary Fig. . f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.\nAlso shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible. Scanning parameters: I = 0.3 pA (V = -1.2 V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3\nÅ. The tip-height offset Δz for each panel is provided with respect to the STM setpoint, and positive (negative) values of Δz denote tip approach (retraction) from the STM setpoint. f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island. The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively. Scale bars: 10 Å (f) and 5 Å (g). To experimentally explore the electronic structure of 5, we used bilayer NaCl films on coinage metal surfaces to electronically decouple the molecule from the metal surfaces. Before presenting the experimental findings, we summarize the results of our theoretical calculations performed on 5 in the neutral charge state (denoted as 5 0 ). We start by performing DFT calculations on 5 0 in the gas phase. Geometry optimization performed at the spin-unrestricted UB3LYP/6-31G level of theory leads to one local minimum, 5OS, the geometry of which corresponds to the open-shell resonance structure of 5 (Fig. , the label OS denotes open-shell)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The author's argument about the effectiveness of vaccines is based on the observation that many routine childhood diseases, such as pertussis, measles, and mumps, have almost entirely disappeared in developed countries. However, the author also notes that the newer acellular vaccines, such as DTaP, are less effective than the older DPT vaccine. What is the most plausible explanation for the author's seeming contradiction between the disappearance of childhood diseases and the ineffectiveness of the newer vaccines?",
    "choices": [
      "A) The author is arguing that the newer vaccines are not effective because they are not being used correctly, and that proper vaccination schedules and administration techniques are crucial for their effectiveness.",
      "B) The author is suggesting that the disappearance of childhood diseases is due to the fact that the diseases are not as contagious as they once were, and that the newer vaccines are not necessary to prevent them.",
      "C) The author is pointing out that the newer vaccines are not as effective as the older DPT vaccine, but that this is because they are designed to be safer and less likely to cause serious side effects, and that the benefits of vaccination still outweigh the risks.",
      "D) The author is arguing that the disappearance of childhood diseases is due to the fact that the diseases are being eradicated by other means, such as improved hygiene and sanitation, and that the newer vaccines are not necessary to prevent them."
    ],
    "correct_answer": "C",
    "documentation": [
      "Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. .. Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. .. (Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.\" As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing. Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Which of the following models achieves the highest average precision at the 50% threshold, while also achieving the highest average precision at the 75% threshold, and has the lowest FLOPS?",
    "choices": [
      "A) Cascade R-CNN with R-50 backbone and 77.54G FLOPS",
      "B) Grid R-CNN with R-50 backbone and 176.67G FLOPS",
      "C) RepPoints with R-50 backbone and 48.54G FLOPS",
      "D) FSAF with R-50 backbone and 51.82G FLOPS"
    ],
    "correct_answer": "B",
    "documentation": [
      "\\label{ben}\n\\begin{tabular}{|l|l|c|c|c|ccc|ccc|cccc|}\n\\hline\nMethod&Backbone&Param.&FLOPs&FPS&AP&AP$_{50}$&AP$_{75}$&AP$_{S}$&AP$_{M}$&AP$_{L}$&AP$_{Ho}$&AP$_{Ec}$&AP$_{Sc}$&AP$_{St}$ \\\\ \n\\hline \n\\emph{multi-stage:} &&&&&&&&&&&&&& \\\\\n\n\\multirow{3}{*}{Faster R-CNN \\cite{Ren2015Faster}}\n&R-18&28.14M&49.75G&5.7&50.1&72.6&57.8&42.9&51.9&48.7&49.1&60.1&31.6&59.7\\\\\n&R-50&41.14M&63.26G&4.7&54.8&75.9&63.1&53.0&56.2&53.8&55.5&62.4&38.7&62.5\\\\\n&R-101&60.13M&82.74G&3.7&53.8&75.4&61.6&39.0&55.2&52.8&54.3&62.0&38.5&60.4\\\\\n\\hline\n\n\\multirow{3}{*}{Cascade R-CNN \\cite{Cai_2019}}\n&R-18&55.93M&77.54G&3.4&52.7&73.4&60.3&\\bf 49.0&54.7&50.9&51.4&62.3&34.9&62.3\\\\\n&R-50&68.94M&91.06G&3.0&55.6&75.5&63.8&44.9&57.4&54.4&56.8&63.6&38.7&63.5\\\\\n&R-101&87.93M&110.53G&2.6&56.0&76.1&63.6&51.2&57.5&54.7&56.2&63.9&41.3&62.6\\\\\n\\hline\n\n\\multirow{3}{*}{Grid R-CNN \\cite{lu2019grid}}\n&R-18&51.24M&163.15G&3.9&51.9&72.1&59.2&40.4&54.2&50.1&50.7&61.8&33.3&61.9\\\\\n&R-50&64.24M&176.67G&3.4&55.9&75.8&64.3&40.9&57.5&54.8&56.7&62.9&39.5&64.4\\\\\n&R-101&83.24M&196.14G&2.8&55.6&75.6&62.9&45.6&57.1&54.5&55.5&62.9&41.0&62.9\\\\\n\\hline\n\n\\multirow{3}{*}{RepPoints \\cite{yang2019reppoints}}\n&R-18&20.11M&\\bf 35.60G&5.6&51.7&76.9&57.8&43.8&54.0&49.7&50.8&63.3&33.6&59.2\\\\\n&R-50&36.60M&48.54G&4.8&56.0&80.2&63.1&40.8&58.5&53.7&56.7&65.7&39.3&62.3\\\\\n&R-101&55.60M&68.02G&3.8&55.4&79.0&62.6&42.2&57.3&53.9&56.0&65.8&39.0&60.9\\\\\n\\hline \n\\hline \n\\emph{one-stage:} &&&&&&&&&&&&&& \\\\\n\\multirow{3}{*}{RetinaNet \\cite{Lin2017Focal}}\n&R-18&19.68M&39.68G&7.1&44.7&66.3&50.7&29.3&47.6&42.5&46.9&54.2&23.9&53.8\\\\\n&R-50&36.17M&52.62G&5.9&49.3&70.3&55.4&36.5&51.9&47.6&54.4&56.6&27.8&58.3\\\\\n&R-101&55.16M&72.10G&4.5&50.4&71.7&57.3&34.6&52.8&49.0&54.6&57.0&33.7&56.3\\\\\n\\hline \n\n\\multirow{3}{*}{FreeAnchor \\cite{2019arXiv190902466Z}}\n&R-18&19.68M&39.68G&6.8&49.0&71.9&55.3&38.6&51.7&46.7&47.2&62.8&28.6&57.6\\\\\n&R-50&36.17M&52.62G&5.8&54.4&76.6&62.5&38.1&55.7&53.4&55.3&65.2&35.3&61.8\\\\\n&R-101&55.16M&72.10G&4.4&54.6&76.9&62.9&36.5&56.5&52.9&54.0&65.1&38.4&60.7\\\\\n\\hline \n\n\\multirow{3}{*}{FoveaBox \\cite{DBLP:journals/corr/abs-1904-03797}}\n&R-18&21.20M&44.75G&6.7&51.6&74.9&57.4&40.0&53.6&49.8&51.0&61.9&34.6&59.1\\\\\n&R-50&37.69M&57.69G&5.5&55.3&77.8&62.3&44.7&57.4&53.4&57.9&64.2&36.4&62.8\\\\\n&R-101&56.68M&77.16G&4.2&54.7&77.3&62.3&37.7&57.1&52.4&55.3&63.6&38.9&60.8\\\\\n\\hline \n\n\\multirow{3}{*}{PAA \\cite{2020arXiv200708103K}}\n&R-18&\\bf 18.94M&38.84G&3.0&52.6&75.3&58.8&41.3&55.1&50.2&49.9&64.6&35.6&60.5\\\\\n&R-50&31.89M&51.55G&2.9&56.8&79.0&63.8&38.9&58.9&54.9&56.5&66.9&39.9&64.0\\\\\n&R-101&50.89M&71.03G&2.4&56.5&78.5&63.7&40.9&58.7&54.5&55.8&66.5&42.0&61.6\\\\\n\\hline \n\n\\multirow{3}{*}{FSAF \\cite{zhu2019feature}}\n&R-18&19.53M&38.88G&\\bf 7.4&49.6&74.3&55.1&43.4&51.8&47.5&45.5&63.5&30.3&58.9\\\\\n&R-50&36.02M&51.82G&6.0&54.9&79.3&62.1&46.2&56.7&53.3&53.7&66.4&36.8&62.5\\\\\n&R-101&55.01M&55.01G&4.5&54.6&78.7&61.9&46.0&57.1&52.2&53.0&66.3&38.2&61.1\\\\\n\\hline \n\n\\multirow{3}{*}{FCOS \\cite{DBLP:journals/corr/abs-1904-01355}}\n&R-18&\\bf 18.94M&38.84G&6.5&48.4&72.8&53.7&30.7&50.9&46.3&46.5&61.5&29.1&56.6\\\\\n&R-50&31.84M&50.34G&5.4&53.0&77.1&59.9&39.7&55.6&50.5&52.3&64.5&35.2&60.0\\\\\n&R-101&50.78M&69.81G&4.2&53.2&77.3&60.1&43.4&55.4&51.2&51.7&64.1&38.5&58.5\\\\\n\\hline \n\n\\multirow{3}{*}{ATSS \\cite{zhang2019bridging}}\n&R-18&\\bf 18.94M&38.84G&6.0&54.0&76.5&60.9&44.1&56.6&51.4&52.6&65.5&35.8&61.9\\\\\n&R-50&31.89M&51.55G&5.2&58.2&\\bf 80.1&66.5&43.9&60.6&55.9&\\bf 58.6&67.6&41.8&64.6\\\\\n&R-101&50.89M&71.03G&3.8&57.6&79.4&65.3&46.5&60.3&55.0&57.7&67.2&42.6&62.9\\\\\n\\hline \n\n\\multirow{3}{*}{GFL \\cite{li2020generalized}}\n&R-18&19.09M&39.63G&6.3&54.4&75.5&61.9&35.0&57.1&51.8&51.8&66.9&36.5&62.5\\\\\n&R-50&32.04M&52.35G&5.5&\\bf 58.6&79.3&\\bf 66.7&46.5&\\bf 61.6&55.6&\\bf 58.6&\\bf 69.1&41.3&\\bf 65.3\\\\\n&R-101&51.03M&71.82G&4.1&58.3&79.3&65.5&45.1&60.5&\\bf 56.3&57.0&\\bf"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a key advantage of using the rotational normalization method proposed in the AutoCogniSys framework for reducing the complexity of ground truth labeling in hand gesture recognition?",
    "choices": [
      "A) It allows for the collection of data from a wider range of postures, including those of older adults using walkers or wheelchairs.",
      "B) It enables the use of a larger number of hand gestures, resulting in a more comprehensive gesture dictionary.",
      "C) It reduces the need for extensive labeling efforts by merging hand gestures subject to directional differences.",
      "D) It improves the accuracy of the FWNB classifier by incorporating more features from the 3-axis data."
    ],
    "correct_answer": "C",
    "documentation": [
      "\\section{Activity Recognition}\nWe aim to detect single wrist-worn ACC sensor based hand gesture and postural activities and insert these into an HDBN graphical model in conjunction with ambient and object sensor values for complex activity recognition. We consider the recognition problem asan activity tupple of $\\langle gesture,posture,ambient,object \\rangle$. Though, Alam et. al. provides significant performance improvement for single wrist-worn ACC sensor aided 18-hand gesture based postural activity recognition in lab environment \\cite{alam17}, it faces some practical challenges in real-time smart environment with older adults due to the diversity of their postures. For example, some older adults use walker, double walking sticks or wheel chair for walking in which cases collecting 18 hand gestures and corresponding postural activities for training requires endless efforts and carefulness. To reduce the complexity of ground truth labeling and later state space explosion for graphical model (HDBN), we propose to use rotational normalization method that can merge some hand-gestures subject to directional differences and forms an 8-hand gesture model. However, our proposed Feature Weight Naive Bayes (FWNB) classifier adds significant improvement on Alam et. al. proposed sparse-deconvolution method as well as recognition in diverse postural environment. \\begin{figure}[!htb]\n\\begin{center}\n   \\epsfig{file=hand_gestures.pdf,height=0.5in, width=3in}\n   \\vspace{-.2in}\n\\caption{8 hand gesture dictionary with direction}\n   \\label{fig:hand_gestures}\n   \\vspace{-.2in}\n\\end{center}\n\\end{figure}\n\\subsection{Hand Gesture Recognition}\n\\label{sec:hand_gesture}\n\\emph{AutoCogniSys} proposes an 8-gesture dictionary (as shown in Fig. \\ref{fig:hand_gestures}) and a Feature Weighted Naive Bayesian (FWNB) framework for building, modeling and recognizing hand gestures. The method comprises of the following steps: (i) \\emph{Preprocessing:} wrist-worn ACC sensor provided 3-axis data are passed through 0.4Hz low-pass filter to remove the data drift."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a probabilistic extension of the KS-equation, where the underlying linear dynamic is approximated using amortized Variational Inference. According to the text, what is the primary advantage of this approach in terms of quantifying uncertainty in predictions?",
    "choices": [
      "A) It allows for the estimation of the prior distribution on the model parameters θ.",
      "B) It enables the computation of Monte Carlo estimates of the gradient of the Evidence Lower Bound (ELBO) with respect to φ and θ.",
      "C) It provides a way to capture the full dynamics of the KS-equation for a longer time horizon, despite the chaotic nature of the system.",
      "D) It facilitates the use of the reparametrization trick to optimize the ELBO."
    ],
    "correct_answer": "B",
    "documentation": [
      "In particular we are employing a probabilistic decoder The encoder is used to infer the state variables z based on the given data and thus defined in the inference and learning section. Inference and Learning\n\nGiven the probabilistic relations , our goal is to infer the latent variables z 0:T as well as all model parameters θ. We follow a hybrid Bayesian approach in which the posterior of the state variables is approximated using amortized Variational Inference and Maximum-A-Posteriori (MAP) point-estimates for θ are computed. The application of Bayes' rule for each data sequence x 0:T leads to the following posterior: where p(θ) denotes the prior on the model parameters. In the context of variational inference, we use the following factorization of the approximate posterior i.e. we infer only the mean µ and variance σ for each state variable based on the given data points. This conditional density used for inference is the encoder-counterpart to the probabilistic decoder defined in the section before. It can be readily shown that the optimal parameter values are found by maximizing the Evidence Lower Bound (ELBO) F(q φ (z 0:T ), θ) which is derived in Appendix B. We compute Monte Carlo estimates of the gradient of the ELBO with respect to φ and θ with the help of the reparametrization trick and carry out stochastic optimization with the ADAM algorithm .\n\nResults for the probabilistic extension\n\nWe applied our probabilistic version to the KS-equation. We used the same settings as for the deterministic approach but considered up to 10 complex latent variables. The obtained λ's are in Figure . The probabilistic model allows us to quantify the uncertainty in predictions. In Figure predictions for various time-steps and the respective uncertainty bounds are shown for an unseen initial condition. Due to the chaotic nature of the KS-equation and the small amount of training data, the underlying linear dynamic of our model is only able to capture the full dynamics for a limited time horizon."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the discontinuation of KSTP-FM 102.1 in 1952, despite its initial success in the early 1940s?",
    "choices": [
      "A) Lack of funding due to the post-war economic downturn",
      "B) Technical difficulties in broadcasting FM signals during the 1940s",
      "C) The rise of television as a more popular form of entertainment",
      "D) The limited availability of FM radios in the Minnesota market"
    ],
    "correct_answer": "B",
    "documentation": [
      "An FM station, KSTP-FM, was founded in 1946 but shut down in 1952. Hubbard reportedly acquired an RCA TV camera in 1939, and started experimenting with television broadcasts. But World War II put a hold on the development of television. In 1948, with the war over, KSTP-TV became the first television station in Minnesota. With KSTP 1500 already associated with NBC Radio, KSTP-TV became an NBC Television Network affiliate. From 1946 to 1952, KSTP also had an FM counterpart. KSTP-FM 102.1 was only on the air four years. There were few radios equipped to receive FM signals in that era, and management decided to discontinue FM broadcasts. MOR and Top 40\nAs network programming moved from radio to television, KSTP programmed a full service Middle of the Road (MOR) radio format, in the shadow of its chief competitor, CBS Radio affiliate 830 WCCO. In 1965, a new FM station, reviving the KSTP-FM call sign, was put on the air, largely simulcasting the AM station. But by the late 1960s, KSTP-FM began a separate format of beautiful music. KSTP was the radio home of the Minnesota Vikings football team from 1970 to 1975. In 1973, KSTP broke away from its longtime adult MOR sound and became one of four area stations at the time to program a Top 40 format. \"15 KSTP, The Music Station\" competed with Top 40 AM rivals WDGY, KDWB and later, WYOO. The competition would eventually shake itself out, with outrageous rocker WYOO dropping out after being sold in 1976, and then the staid WDGY switching to country music the following year. As for uptempo hits station 15 KSTP, it went from a tight Top 40 format to leaning adult rock in 1978, to leaning adult contemporary in 1979, to evolving into adult contemporary/talk by 1980. In 1982, it officially shifted to talk. Most Top 40 rock music, by this time, had moved to the FM band. Past Personalities\n\nNotable hosts who have been on KSTP include John Hines, Jesse Ventura, Larry Carolla, Tom Barnard, Big Al Davis, Don Vogel, John MacDougall, Griff, Mike Edwards, Geoff Charles, Joe Soucheray, James Lileks, Leigh Kamman, Barbara Carlson, Peter Thiele, Tom Mischke, Jason Lewis, Chuck Knapp, Machine Gun Kelly, Charle Bush, Mark O'Connell and Paul Brand."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Consider a two-dimensional environment composed of 5 polytopic obstacles, each defined by their half-space representation (H-representation). The obstacles are arranged in a way that forms a hyperplane arrangement, as shown in fig. The preference θ is spatially decomposed into a set of preferred neighboring polytopes per region of the space. Within each polytope j, the human preference pj is a discrete distribution over the preferred neighbor in N (j).\n\nWhich of the following statements is true about the reduced H-representation of a polytope j, assuming that the non-redundant constraints correspond to edges of the polytope?",
    "choices": [
      "A) The reduced H-representation can be written as where A j e is a square matrix, b j e is a vector, and α j e is a vector of length 2.",
      "B) The reduced H-representation can be written as where A j e is a square matrix, b j e is a vector, and α j e is a vector of length 2, and the constraints are redundant.",
      "C) The reduced H-representation can be written as where A j e is a square matrix, b j e is a vector, and α j e is a vector of length 2, and the constraints are non-redundant.",
      "D) The reduced H-representation can be written as where A j e is a square matrix, b j e is a vector, and α j e is a vector of length 2, and the constraints are non-redundant, but the matrix A j e is not square."
    ],
    "correct_answer": "C",
    "documentation": [
      "Hyperplane arrangements have been used by Vincent and Schwager in the context of Neural Network verification. In our setting, we leverage this representation to define path preferences as preferred transitions between adjacent regions of the space. Hyperplane Arrangement\n\nWe assume a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation) where A i ∈ R di×2 and b i ∈ R di , and where d i is the number of edges (hyperplanes) composing polytope i. Let n = i d i be the total number of hyperplanes. We leverage each obstacle's H-representation to construct a hyperplane arrangement of the environment as shown in fig.\n.e. a partitioning of the space into polytopes. More specifically, each location in space belongs to a polytope j for which we can write an H-representation of the form where α j i ∈ {−1, 1} di is a vector specific to polytope j and obstacle i corresponding to the relative position of any point in the set with respect to each hyperplane in O i . Fig. : Intent inference model in a hyperplane arrangement of the obstacle free space. We spatially decompose the preference θ into a set of preferred neighboring polytopes per region of the space. Within each polytope j, the human preference pj is a discrete distribution over the preferred neighbor in N (j). We assume that for a location st belonging to polytope j, and given goal g and preference pj, the observation ot and any other preference p i,i =j are conditionally independent. Concatenating elements from each obstacle's Hrepresentation, we can write polytope j's H-representation as where Some of the constraints in eq. ( ) (corresponding to rows of A, b and α j ) are redundant, i.e. the set P j does not change upon their removal. We can further reduce the Hrepresentation of a polytope to include only non-redundant constraints. By removing the rows corresponding to redundant constraints, we obtain new matrices A j e , b j e and α j e such that we can write the polytope's reduced H-representation as The non-redundant constraints correspond to edges of the polytope."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The author of the text describes a process of reinforcing the turtledeck of an aircraft model. What is the primary purpose of adding the 2-inch tapes to the bulkheads inside and out?",
    "choices": [
      "A) To provide additional structural support to the turtledeck",
      "B) To create a smooth surface for painting the turtledeck",
      "C) To prevent the turtledeck from becoming too heavy",
      "D) To create a composite stringer system for added stability"
    ],
    "correct_answer": "D",
    "documentation": [
      "When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out. At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the \"backbone\" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The C$_2$H lines are detected independent of the evolutionary stage of the sources, but the line-widths between the different sub-groups show a significant difference. What can be inferred about the spatial structure of C$_2$H during the different evolutionary stages?",
    "choices": [
      "A) The C$_2$H lines are more sensitive to the distances of the sources, and the broader line-widths of the UCH{\\sc ii}s indicate a more complex spatial structure.",
      "B) The C$_2$H lines are more sensitive to the luminosities of the sources, and the marginal difference between the IRDCs and HMPOs in terms of line-widths suggests a similar spatial structure.",
      "C) The C$_2$H lines are more sensitive to the evolutionary stage of the sources, and the significantly broader line-widths of the UCH{\\sc ii}s indicate a more complex spatial structure.",
      "D) The C$_2$H lines are more sensitive to the evolutionary stage of the sources, and the marginal difference between the IRDCs and HMPOs in terms of line-widths suggests a similar spatial structure, but the UCH{\\sc ii}s exhibit a more complex spatial structure due to their higher luminosities."
    ],
    "correct_answer": "C",
    "documentation": [
      "Table\n\\ref{sample} lists the observed sources, their coordinates, distances,\nluminosities and a first order classification into the evolutionary\nsub-groups IRDCs, HMPOs and UCH{\\sc ii}s based on the previously\navailable data. Although this classification is only based on a\nlimited set of data, here we are just interested in general\nevolutionary trends. Hence, the division into the three main classes\nis sufficient. Figure \\ref{spectra} presents sample spectra toward one source of each\nevolutionary group. While we see several CH$_3$OH lines as well as\nSO$_2$ and H$_2$CS toward some of the HMPOs and UCH{\\sc ii}s but not\ntoward the IRDCs, the surprising result of this comparison is the\npresence of the C$_2$H lines around 349.4\\,GHz toward all source types\nfrom young IRDCs via the HMPOs to evolved UCH{\\sc ii}s. Table\n\\ref{sample} lists the peak brightness temperatures, the integrated\nintensities and the FWHM line-widths of the C$_2$H line blend at\n349.399\\,GHz. The separation of the two lines of 1.375\\,MHz already\ncorresponds to a line-width of 1.2\\,km\\,s$^{-1}$. We have three C$_2$H\nnon-detections (2 IRDCs and 1 HMPO), however, with no clear trend with\nrespect to the distances or the luminosities (the latter comparison is\nonly possible for the HMPOs). While IRDCs are on average colder than\nmore evolved sources, and have lower brightness temperatures, the\nnon-detections are more probable due to the relatively low sensitivity\nof the short observations (\\S\\ref{obs}). Hence, the data indicate\nthat the C$_2$H lines are detected independent of the evolutionary\nstage of the sources in contrast to the situation with other\nmolecules. When comparing the line-widths between the different\nsub-groups, one finds only a marginal difference between the IRDCs and\nthe HMPOs (the average $\\Delta v$ of the two groups are 2.8 and\n3.1\\,km\\,s$^{-1}$). However, the UCH{\\sc ii}s exhibit significantly\nbroader line-widths with an average value of 5.5\\,km\\,s$^{-1}$.\n\nIntrigued by this finding, we wanted to understand the C$_2$H spatial\nstructure during the different evolutionary stages."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The proposed formulation for the SLAS module can accommodate an arbitrary number of lanes at any given time instant k, as stated in Remark 3. However, the computational complexity of the original formulation is high due to the integer decision variables. To address this, the problem is reformulated with binary variables, which reduces the computational complexity. What is the primary benefit of using binary variables in the reformulated problem?",
    "choices": [
      "A) It allows for the consideration of non-linear relationships between lanes.",
      "B) It enables the incorporation of additional constraints related to lane changes.",
      "C) It reduces the computational complexity of the original formulation by replacing integer variables with binary variables.",
      "D) It introduces an auxiliary variable to handle the big-M method."
    ],
    "correct_answer": "C",
    "documentation": [
      "\"\nFurthermore, the absolute value constraint can be decomposed into linear constraints by the application of big-M method and the introduction of an auxiliary variable, as shown in the Appendix. Remark 3: The proposed formulation can accommodate arbitrary number of lanes at any given time instant k. This means that if at any given time, the number of available lanes for traveling either increases or decreases, the proposed formulation will still continue to hold. This is an important consideration since many a times on highways, some lanes are blocked due to various unanticipated situations such as road accidents, roadwork, narrowing of road etc. 2) Computational Complexity Reduction: This section details the optimization problem reformulation with binary variables, optimization warm start technique and lazy constraint implementation, all of which combine to improve the computational complexity of our SLAS module. Binary Variables: The proposed formulation in Section III-B has relatively high computation complexity (computation time of ∼ 2s in the worst case scenario -slow moving traffic blocking all the lanes) due to the integer decision variables yielding a mixed-integer optimization problem . To circumvent the computational overload, we reformulate the problem with binary variables that replace the integer variables, as follows:\nwhere the Lk (i, j) represents the modified target lane variable, indexed by the lane (i) as well as the planning step (j) and Lk (a, b) = 1 represents the choice of lane a ∈ L as the target lane at planning step b ∈ Z . Then, some of the constraints from the SLAS formulation in Section III-B are modified as follows:\nHere, initializes the target lane, (21) restricts the target lane at any planning step to the set of available lanes, restricts the lane change between consecutive planning steps to the adjacent lanes, and ( ) represents the augmented safety constraint. The implication ( =⇒ ) in ( ) can easily be transformed into a linear constraint (see Appendix)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A user visits the Agency Spotter website and clicks on a link to a third-party service. The user's browser sends a request to the Agency Spotter server, which logs the request, including the user's IP address, browser type, and cookies. The user then clicks on an ad on the third-party service's website, which uses cookies to track the user's browsing behavior. The user returns to the Agency Spotter website and uploads a file, which is stored on the Agency Spotter server. The server logs the file upload, including the date and time of the upload, the amount of time spent on the upload page, and the pages viewed before and after the upload. The Agency Spotter server also shares the user's information with the third-party service, which uses it to personalize advertising content. However, the user has set their browser to limit the placement of cookies by advertising networks. Which of the following statements is true?",
    "choices": [
      "A) The Agency Spotter server logs all user activity, including the user's browsing behavior on the third-party service's website.",
      "B) The user's information is shared with the third-party service only if the user has given explicit permission.",
      "C) The Agency Spotter server stores all user data, including the user's browsing history, on its servers in the United States.",
      "D) The user's browser cookies are deleted after the user uploads a file to the Agency Spotter website."
    ],
    "correct_answer": "C",
    "documentation": [
      "In addition, when you use the Site, our servers automatically record certain information that your web browser sends whenever you visit any website. These server logs may include information such as your web request, Internet Protocol address, browser type, browser language, referring/exit pages and URLs, platform type, number of clicks, domain names, landing pages, pages viewed and the order of those pages, the amount of time spent on particular pages, the date and time of your request, and one or more cookies that may uniquely identify your browser. Information from third party services and other websites. Do not upload or insert any information to or into the Site or Services that you do not want to be shared or used in the manner described in this section. Advertisements. Advertisers who present ads on the Site may use technological methods to measure the effectiveness of their ads and to personalize advertising content. You may use your browser cookie settings to limit or prevent the placement of cookies by advertising networks. Agency Spotter does not share personally identifiable information with advertisers unless we get your permission.\nLinks. When you click on links on Agency Spotter you may leave our site. We are not responsible for the privacy practices of other sites, and we encourage you to read their privacy statements. If we are requested to disclose your information to a government agency or official, we will do so if we believe in good faith, after considering your privacy interests and other relevant factors, that such disclosure is necessary to: (i) conform to legal requirements or comply with a legal process with which we are involved; (ii) protect our rights or property or the rights or property of our affiliated companies; (iii) prevent a crime or protect national security; or (iv) protect the personal safety of Site users or the public. Because Agency Spotter is a United States limited liability company and information collected on our Site is stored in whole or in part in the United States, your information may be subject to U.S. law."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary consequence of Secretary Johnson's proposed reorganization of the United States Armed Forces in April 1949?",
    "choices": [
      "A) The Navy's budget was increased, allowing for more ships and personnel to be added to the fleet.",
      "B) The Marine Corps was merged into the Army, and the Navy was reduced to a convoy-escort force, as proposed by Secretary Johnson.",
      "C) The Joint Chiefs of Staff were given more autonomy to make decisions on arms policies, leading to a more decentralized military command structure.",
      "D) The Navy's role in the war effort was diminished, leading to a decrease in naval personnel and equipment."
    ],
    "correct_answer": "B",
    "documentation": [
      "Goodwin returned with San Jacinto to the United States in mid-September 1945 and he was detached in January 1946. He subsequently served in the office of the Chief of Naval Operations until May that year, when he entered the instruction at National War College. Goodwin graduated in June 1947 and served on Secretary's committee for Research on Reorganization. Upon promotion to Rear admiral on April 1, 1949, Goodwin was appointed Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet under Admiral William H. P. Blandy. Revolt of the Admirals\n\nIn April 1949, the budget's cuts and proposed reorganization of the United States Armed Forces by the Secretary of Defense Louis A. Johnson launched the wave of discontent between senior commanders in the United States Navy. Johnson proposed the merging of the Marine Corps into the Army, and reduce the Navy to a convoy-escort force. Goodwin's superior officer, Admiral Blandy was call to testify before the House Committee on Armed Services and his harsh statements for the defense of the Navy, costed him his career. Goodwin shared his views and openly criticized Secretary Johnson for having power concentrated in a single civilian executive, who is an appointee of the Government and not an elected representative of the people. He also criticized aspects of defense unification which permitted the Joint Chiefs of Staff to vote on arms policies of individual services, and thus \"rob\" the branches of autonomy. The outbreak of the Korean War in summer 1950 proved the proposal of Secretary Johnson as incorrect and he resigned in September that year. Also Secretary of the Navy, Francis P. Matthews resigned one month earlier. Later service\n\nDue to the Revolts of the admirals, Blandy was forced to retire in February 1950 and Goodwin was ordered to Newport, Rhode Island for temporary duty as Chief of Staff and Aide to the President of the Naval War College under Vice admiral Donald B. Beary in April 1950. Goodwin was detached from that assignment two months and appointed member of the General Board of the Navy."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "In the experiment where the robot receives human input and the goal-only baseline, what is the primary effect of accounting for path preference on the robot's performance?",
    "choices": [
      "A) The robot becomes more overconfident in its predictions, leading to a higher chance of getting stuck in local optima.",
      "B) The robot's belief entropy decreases more steadily, allowing it to leverage the human's latest observations and reach the goal successfully.",
      "C) The robot's computation time increases due to the additional complexity of the path preference model.",
      "D) The robot's performance improves significantly, but only when the human provides information that aligns with the shortest path to the goal."
    ],
    "correct_answer": "B",
    "documentation": [
      "We find that in these runs, accounting for path preference consistently improves performance compared with the goal-only baseline. Results also show that blending the user's input with the robot's policy (Path Preference + Blend) when the human provides information leads to improved performance. Belief entropy. Figure shows a challenging problem instance where the directions the human provides do not align directly with the shortest path to the goal. By ignoring the effects of preferences in the problem model (goal only), the robot quickly infers from observations that the upper left goal is less likely than others (P (g) drops). The strong decrease in entropy shows that the robot becomes overconfident in this prediction. Overconfidence in an incorrect goal will prevent the agent from finding the correct goal once the human's indications directly align with it, as it needs to correct for the wrong predictions, as shown in the path realization (fig.\n). In this realization, the goal-only method (green robot) fails to search the upper left area within the allotted time. By accounting for path preferences in its model, the blue robot's entropy over the goal distribution decreases more steadily, allowing for it to leverage the human's latest observations and reach the goal successfully. shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Computation time. In table II we provide the time required to solve the POMDP, and the time required to update the robot's belief as it receives new observations. We compute solutions on three maps: a simple 10 × 10 grid world with 8 polytopes (fig. ), a 10 × 10 grid world with 56 polytopes (fig. ), and a 20×20 grid world with 73 polytopes (fig. ). The latter environment being larger, we increase the mission time and the depth of the search tree in POMCP from T max = 30 (Map 1 and Map 2) to T max = 60 (Map 3)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The author's argument about the effectiveness of vaccines is based on the observation that many routine childhood diseases, such as pertussis, measles, and mumps, have almost entirely disappeared in developed countries. However, the author also notes that the newer acellular vaccines, such as DTaP, are less effective than the older DPT vaccine. What is the most plausible explanation for the author's seeming contradiction between the disappearance of childhood diseases and the ineffectiveness of the newer vaccines?",
    "choices": [
      "A) The author is arguing that the newer vaccines are not effective because they are not being used correctly, and that proper vaccination schedules and administration techniques are crucial for their effectiveness.",
      "B) The author is suggesting that the disappearance of childhood diseases is due to the fact that the diseases are not as contagious as they once were, and that the newer vaccines are not necessary to prevent them.",
      "C) The author is pointing out that the newer vaccines are not as effective as the older DPT vaccine, but that this is because they are designed to be safer and less likely to cause serious side effects, and that the benefits of vaccination still outweigh the risks.",
      "D) The author is arguing that the disappearance of childhood diseases is due to the fact that the diseases are being eradicated by other means, such as improved hygiene and sanitation, and that the newer vaccines are not necessary to prevent them."
    ],
    "correct_answer": "C",
    "documentation": [
      "Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. .. Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. .. (Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.\" As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing. Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A nonlinear oscillator, whose governing equation of motion is given by , can be represented by the Volterra series . If the energy of excitation f (t) is limited, the nonlinear response under zero initial conditions (i.e., zero displacement and zero velocity) can be represented by the Volterra series . What is the relationship between the order of the Volterra series and the convergence of the Volterra series representation?",
    "choices": [
      "A) The order of the Volterra series is directly proportional to the convergence of the Volterra series representation.",
      "B) The order of the Volterra series is inversely proportional to the convergence of the Volterra series representation.",
      "C) The order of the Volterra series is equal to the convergence of the Volterra series representation.",
      "D) The order of the Volterra series is independent of the convergence of the Volterra series representation."
    ],
    "correct_answer": "B",
    "documentation": [
      "Compared to Hu et al. , which was regarded as an efficient tool to compute responses of linear systems, the generalized pole-residue method in this paper is introduced to compute responses of nonlinear systems. The proposed method involves two steps: (1) the Volterra kernels are decoupled in terms of Laguerre polynomials, and (2) the partial response related to a single Laguerre polynomial is obtained analytically in terms of the pole-residue method. Compared to the traditional pole-residue method for a linear system, one of the novelties of the generalized pole-residue method is how to deal with the higher-order poles and their corresponding coefficients. Similar to the Taylor series, the Volterra series representation is an infinite series, and convergence conditions are needed to assure that the representation is meaningful. Because the proposed method is based on the Volterra series, only the system with convergent Volterra series representation can be treated by the proposed method. The paper is organized as follows. In Section 2, the nonlinear response is modelled by a Volterra series, and Volterra kernel functions are decoupled by Laguerre polynomials. Then, the pole-residue method for computing explicit responses is developed in Section 3. Numerical studies and discussions are given in Section 4. Finally, the conclusions are drawn in Section 5. Response calculation based on Volterra series\n\nA nonlinear oscillator, whose governing equation of motion is given by where z(t, y, ẏ) represents an arbitrary nonlinear term; m, c, and k are the mass, damping and linear stiffness, respectively; y(t), ẏ(t) and ÿ(t) are the displacement, velocity and acceleration, respectively; and f (t) is the time-dependent excitation. If the energy of excitation f (t) is limited, the nonlinear response under zero initial conditions (i.e., zero displacement and zero velocity) can be represented by the Volterra series : where N is the order of Volterra series and In Eq. 3, h 1 (τ ) is called the first-order Volterra kernel function, which represents the linear behaviour of the system; h n (τ 1 , . ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the minimum duration of a Hosting Service suspension due to suspected spam, as per Broadjam's policies?",
    "choices": [
      "A) 1 day",
      "B) 2 days",
      "C) 30 days",
      "D) 60 days"
    ],
    "correct_answer": "B",
    "documentation": [
      "Upon request by Broadjam, conclusive proof of optin may be required for an email address or fax number. (d) If Broadjam determines that Hosting Services are being used in association with spam, Broadjam will re-direct, suspend, or cancel such Hosting Service for a period of no less than 2 days. The Hosting Subscriber will be required to respond by email to Broadjam stating that Hosting Subscriber will cease to send spam and/or have spam sent on their behalf. Broadjam will require a non-refundable reactivation fee to be paid before Hosting Subscriber's Website, email boxes and/or other Hosting Services are reactivated. In the event Broadjam determines the abuse has not stopped after services have been restored the first time, Broadjam may terminate all Services associated with the Hosting Subscriber. This Article IV applies to all Users. Fees and prices appearing on the Site are based on United States dollars. Payments for any Service or purchase made on or through the Site shall be made to Broadjam in United States dollars, except as provided in Section 4.05 herein. You agree to pay for all fees and charges incurred under your Broadjam account or Username. If you have configured the account associated with your Username (your \"Account\") to pay for Services or purchases with a credit or debit card or similar form of payment (a \"Card\" payment method), you authorize any and all charges and fees incurred under your Account to be billed from time to time to your Card account. Regardless of the method of payment, it is your sole responsibility to advise Broadjam of any billing problems or discrepancies within thirty (30) days after such discrepancies or problems become known to you. Your Card issuer agreement governs the use of your designated Card account in connection with any fee, purchase or Service; you must refer exclusively to such issuer agreement, and not this Agreement, to determine your rights and liabilities as a Cardholder. If you submit a payment that results in Broadjam being charged non-sufficient funds, chargeback fees, or other similar fees, you agree to reimburse all such fees."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the interaction between a planar shock wave and a 2-D heavy-cylindrical bubble, what can be inferred about the effect of the shock wave on the bubble's TNE (Transition Number Effect) behavior?",
    "choices": [
      "A) The shock wave will increase the TNE effect due to the increased pressure and temperature gradients.",
      "B) The shock wave will have no effect on the TNE effect, as the bubble's composition is primarily SF6, which is not affected by the shock wave.",
      "C) The shock wave will decrease the TNE effect due to the formation of a boundary layer on the bubble's surface, which will reduce the bubble's ability to respond to the shock wave.",
      "D) The shock wave will not affect the TNE effect, as the bubble's initial conditions are in a state of hydrostatic equilibrium, and the shock wave will not disturb this equilibrium."
    ],
    "correct_answer": "C",
    "documentation": [
      "Strictly speaking, those TNE intensity and effect descriptions that do not account for the research perspective are not correct. Do not explain the research perspective, the corresponding is not dependent on the research perspective. Numerical simulations and results\n\nIn this section, we first validate the DBM code by comparing the DBM results with experimental results. Then, the effects of specific-heat ratio on the dynamic process and TNE behaviors on SBI are investigated. Comparison with experimental results\n\nIn the following part, we use a first-order two-fluid DBM to simulate the interaction between a planar shock wave with a 2-D heavy-cylindrical bubbles, and compare the DBM results with the experimental results from Ref. . The computational configuration can be seen in Fig. . In a flow field which is filled with Air, there is a static bubble composed of 26% Air and 74% SF 6 . A shock with Ma = 1.2 would pass through the bubble from left to right. The initial conditions of ambient gas are ρ 0 = 1.29kg/m 3 , T 0 = 293K, p 0 = 101.3kPa. Ignoring the pressure difference between interior gas and ambient gas, the initial parameters of the bubble are ρ bubble = 4.859kg/m 3 , p bubble = 101.3kPa,\nand T 0 = 293K. For simulating, these actual physical quantities should be transferred to dimensionless parameters. This process can refer to the Appendix A. The dimensionless conditions of macroscopic quantities of the fluid field in initial time are (ρ, T, u x , u y ) bubble = (4.0347, 1.0, 0.0, 0.0), (ρ, T, u x , u y ) 1 = (1.3416,\n1.128, 0.3616, 0.0), (ρ, T, u x , u y ) 0 = (1.0, 1.0, 0.0, 0.0), where the subscript \"0\" (\"1\") represents downstream (upstream) region. In two-fluid DBM code, the distribution function f Air is used to describe the ambient gas, i.e., Air. The f bubble characters the bubble which is a mixture that composed of Air and SF 6 . The grid number is N x × N y = 800 × 400, where the N x and N y are grid number in x and y direction, respectively. This grid size has passed the mesh convergence test."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the effect of a uniaxial strain of angle θ on the hopping parameters t_{1,2,3} of the strained graphene part of the structure?",
    "choices": [
      "A) The hopping parameters t_{1,2,3} are unchanged, and the strain only affects the bond vectors.",
      "B) The hopping parameters t_{1,2,3} are scaled by a factor of (1 + γ)sinθcosθ, where γ is the Poisson ratio.",
      "C) The hopping parameters t_{1,2,3} are scaled by a factor of (1 + γ)sin^2θ, where γ is the Poisson ratio.",
      "D) The hopping parameters t_{1,2,3} are scaled by a factor of (1 + γ)(cos^2θ - sin^2θ), where γ is the Poisson ratio."
    ],
    "correct_answer": "D",
    "documentation": [
      "= \\sum\\nolimits_{nm} {{t_{nm}}c_n^\\dag {c_m}}$ where $t_{nm}$ is the hopping energy between nearest neighbor \\emph{n}th and \\emph{m}th atoms. The application of a uniaxial strain of angle $\\theta$ causes the following changes in the $C-C$ bond vectors:\n\\begin{eqnarray}\n {{\\vec r}_{nm}}\\left( \\sigma \\right) &=& \\left\\{ {1 + {M_s}\\left( \\sigma, \\theta \\right)} \\right\\}{{\\vec r}_{nm}}\\left( 0 \\right) \\\\\n {M_s}\\left( \\sigma, \\theta \\right) &=& \\sigma \\left[ {\\begin{array}{*{20}{c}}\n {{{\\cos }^2}\\theta  - \\gamma {{\\sin }^2}\\theta }&{\\left( {1 + \\gamma } \\right)\\sin \\theta \\cos \\theta }\\\\\n {\\left( {1 + \\gamma } \\right)\\sin \\theta \\cos \\theta }&{{{\\sin }^2}\\theta  - \\gamma {{\\cos }^2}\\theta }\n \\end{array}} \\right] \\nonumber\n\\end{eqnarray}\nwhere $\\sigma$ represents the strain and $\\gamma \\simeq 0.165$ is the Poisson ratio \\cite{blak70}. The hopping parameters are defined as $t_{nm} \\left( \\sigma \\right) = t_0 \\exp\\left[-3.37\\left(r_{nm} \\left( \\sigma \\right) /r_0 - 1\\right)\\right]$, where the hopping energy $t_0 = -2.7$ $eV$ and the bond length $r_{nm} \\left( 0 \\right) \\equiv r_0 = 0.142$ $nm$ in the unstrained case. Therefore, there are three different hoping parameters $t_{1,2,3}$ corresponding to three bond vectors ${\\vec r}_{1,2,3}$, respectively, in the strained graphene part of the structure (see Fig. 1). Here, we assume a 1D profile of applied strain, i.e., the strain tensor is a function of position along the transport direction Ox while it is constant along the Oy-axis. The transport direction, $\\phi$, and strain direction, $\\theta$, are determined as schematized in Fig. 1. Based on this tight binding model, two methods described below can be used to investigate the conduction gap of the considered strained junctions. \\textbf{Green's function calculations.} First, we split the graphene sheet into the smallest possible unit cells periodically repeated along the Ox/Oy directions with the indices $p/q$, respectively (similarly, see the details in \\cite{hung12}). The tight-binding Hamiltonian can therefore be expressed in the following form:\n\\begin{eqnarray}\n{H_{tb}} = \\sum\\limits_{p,q} {\\left( {{H_{p,q}} + \\sum\\limits_{{p_1},{q_1}} {{H_{p,q \\to p_1,q_1}}} } \\right)}\n\\end{eqnarray}\nwhere $H_{p,q}$ is the Hamiltonian of cell $\\{p,q\\}$, and $H_{p,q \\to p_1,q_1}$ denotes the coupling of cell $\\{p,q\\}$ to its nearest neighbor cell $\\{p_1,q_1\\}$."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The proposed method's efficiency under non-zero initial conditions can be understood by considering the following:\n\nChunk0: The efficiency of the proposed method was verified by the fourth-order Runge-Kutta method. This paper only computes the response under zero initial conditions. The response under non-zero initial conditions will be investigated in our future work.\n\nChunk1: The fourth-order Runge-Kutta method is known for its high accuracy and stability, especially when dealing with non-linear systems. However, its computational complexity can be a limitation for real-time applications.\n\nChunk2: In the context of non-linear systems, the response to initial conditions can be highly sensitive to the choice of method. A more efficient method might be necessary to capture the dynamics of the system accurately.",
    "choices": [
      "A) The proposed method's efficiency under non-zero initial conditions can be improved by using a more efficient numerical method, such as the trapezoidal rule, which is known for its lower computational complexity.",
      "B) The response under non-zero initial conditions will be similar to the response under zero initial conditions, as the system's dynamics are primarily driven by the initial conditions.",
      "C) The proposed method's efficiency under non-zero initial conditions can be understood by considering the system's non-linearity and the need for a more efficient numerical method to capture its dynamics accurately.",
      "D) The response under non-zero initial conditions will be more stable than the response under zero initial conditions, as the system's non-linearity will lead to a more robust response."
    ],
    "correct_answer": "C",
    "documentation": [
      "The efficiency of the proposed method was verified by the fourth-order Runge-Kutta method. This paper only computes the response under zero initial conditions. The response under non-zero initial conditions will be investigated in our future work."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The parallel least mean square-partial parallel interference cancelation (PLMS-PPIC) method improves upon the LMS-PPIC method in which aspect?",
    "choices": [
      "A) It uses a single NLMS algorithm with a fixed step-size, whereas PLMS-PPIC uses multiple NLMS algorithms with different step-sizes.",
      "B) It assumes that the receiver knows the phases of all user channels, whereas PLMS-PPIC assumes that the receiver only knows a quarter of the channel phases.",
      "C) It tries to keep the magnitude of all weight elements in all stages equal to unity, whereas PLMS-PPIC does not have this property.",
      "D) It uses a different algorithm for estimating the parameter of the NLMS algorithm, whereas PLMS-PPIC uses the same algorithm as LMS-PPIC."
    ],
    "correct_answer": "A",
    "documentation": [
      "\\section{Introduction}\\label{S1}\n\nThe multiple access interferences (MAI) is the root of user\nlimitation in CDMA systems \\cite{R1,R3}. The parallel least mean\nsquare-partial parallel interference cancelation (PLMS-PPIC) method\nis a multiuser detector for code division multiple access (CDMA)\nreceivers which reduces the effect of MAI in bit detection. In this\nmethod and similar to its former versions like LMS-PPIC \\cite{R5}\n(see also \\cite{RR5}), a weighted value of the MAI of other users is\nsubtracted before making the decision for a specific user in\ndifferent stages \\cite{cohpaper}. In both of these methods, the\nnormalized least mean square (NLMS) algorithm is engaged\n\\cite{Haykin96}. The $m^{\\rm th}$ element of the weight vector in\neach stage is the true transmitted binary value of the $m^{\\rm th}$\nuser divided by its hard estimate value from the previous stage. The\nmagnitude of all weight elements in all stages are equal to unity. Unlike the LMS-PPIC, the PLMS-PPIC method tries to keep this\nproperty in each iteration by using a set of NLMS algorithms with\ndifferent step-sizes instead of one NLMS algorithm used in LMS-PPIC. In each iteration, the parameter estimate of the NLMS algorithm is\nchosen whose element magnitudes of cancelation weight estimate have\nthe best match with unity. In PLMS-PPIC implementation it is assumed\nthat the receiver knows the phases of all user channels. However in\npractice, these phases are not known and should be estimated. In\nthis paper we improve the PLMS-PPIC procedure \\cite{cohpaper} in\nsuch a way that when there is only a partial information of the\nchannel phases, this modified version simultaneously estimates the\nphases and the cancelation weights. The partial information is the\nquarter of each channel phase in $(0,2\\pi)$.\n\nThe rest of the paper is organized as follows: In section \\ref{S4}\nthe modified version of PLMS-PPIC with capability of channel phase\nestimation is introduced. In section \\ref{S5} some simulation\nexamples illustrate the results of the proposed method."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary consequence of the Prestige System in the game, and how does it impact the rewards for Alliance Quests?",
    "choices": [
      "A) The Prestige System increases the difficulty of Alliance Quests, resulting in higher rewards for Alliance members who succeed in completing them.",
      "B) The Prestige System decreases the difficulty of Alliance Quests, resulting in lower rewards for Alliance members who succeed in completing them.",
      "C) The Prestige System has no impact on the difficulty of Alliance Quests, but rather adjusts the rewards based on the Alliance's overall performance.",
      "D) The Prestige System only affects the rewards for Story Quests, not Alliance Quests, and has no impact on the difficulty of Alliance Quests."
    ],
    "correct_answer": "C",
    "documentation": [
      "Redesigned chat and mail screens. Take on other Summoners’ top Champions for bragging rights and prizes in 1-on-1 Duels! A new series of special Ultron quests are available, starting with the first Chapter. Fight back against Ultron’s infection alongside the Summoner, and team up with some of Marvel’s finest! New quests unlock each week! The Spider-Man Champion gate has been removed from Act 1, Chapter 1, Quest 5. • Fixed an issue where chat snapped to the most recent message. • Fixed several issues where Hero Rating would fluctuate. • Various improvements to the Summoner Mastery screens and descriptions. • Increased the ISO8 awarded by duplicate 2-Star Champions. Quest through the new single-player campaign, Ant-Man’s Adventure! In addition to Ant-Man and Yellowjacket feuding throughout the Battlerealm, additional new Champions will be joining The Contest! Access more Masteries in the new Utility Mastery tree! Please note, these changes may result in a loss of Hero Rating as incorrect effects are restored back to normal levels. Improved and polished combat mechanics to reduce the amount of stutters and lost input. Fixed and optimized rendering related issues with Metal enabled devices. Team up with Ant-Man, and put a stop to Yellowjacket’s mysterious mission! All Alliance Quests only last for a specified amount of time, defeat the boss with your Alliance before it expires! New Prestige System - A dynamic difficulty and score setting that adjusts as you and your Alliance succeed in harder quests. The better you do and the tougher your Alliance is, the higher the prestige. The higher the prestige, the better the rewards!\nChoose your teams carefully as Champions within Alliance Quests cannot be used in other Story or Event Quests. Act 4 has been released! Play Chapter 1 now! Summoner level maximum has been increased to level 60! 5-Star Champions are coΩming to The Contest! These are the most powerful Champions yet! Additional improvements have been made to the UI, Versus Arenas, Synergy Bonuses, the Stash & Items Store."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Intel's strategy for maintaining market share in the face of stagnating desktop/laptop sales and increasing tablet sales will likely involve a significant shift in focus towards the tablet market. However, this shift will be hindered by Intel's current technology gap with ARM-based processors. What is the most likely outcome for Intel's tablet sales in the next 2-3 years?",
    "choices": [
      "A) Intel's tablet sales will increase by 20% annually, driven by the growing demand for high-performance tablets.",
      "B) Intel's tablet sales will stagnate at current levels, as the company struggles to close the technology gap with ARM-based processors.",
      "C) Intel's tablet sales will decline by 10% annually, as the company fails to adapt to the changing market landscape and the increasing competition from ARM-based processors.",
      "D) Intel's tablet sales will remain stable, as the company focuses on developing new products that can compete with ARM-based processors on both power and performance."
    ],
    "correct_answer": "B",
    "documentation": [
      "Intel has a large benefit of having a relatively \"good name\" when it comes to CPUs, so they can effectively charge a brand-name premium. I'm sure there are other reasons, and probably better reasons, but these are the main ones that I think of. Mabsark wrote:Actually, that trend will not simply keep increasing going forward. The reason desktop/laptop sales are stagnating/decreasing is due to the fact that most people already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease. The PC market is saturated and in a couple of years, the tablet market will be saturated too. Basically, in order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product. That's true as long as most people are still buying both a tablet and a laptop when each needs to be replaced. I think the assumption is that, as you say, the tablet market will saturate, with people just replacing existing ones, but the desktop/laptop market could decrease much farther than that, if most people stop replacing them at all. I'm not sure of the likelihood of that, but I think that's where this idea comes from. ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward. But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that. The upcoming Haswell chip is showing to consume 1/3 the power of IvyBridge at peak, consumes 1/20th the power at idle, all the while maintaining Identical or better performance. This chip should actually compete with ARM CPUs on both power/performance and idle."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. However, when the side panels are bent and sloped to form the fuselage box section, the resulting shape is conical. As a result, the top longeron takes on an elliptical shape when built flat, bent to form a cylindrical section, and sloped to form a conical section. What is the primary reason for this elliptical shape?",
    "choices": [
      "A) The fuselage sides are not properly aligned with the top longeron.",
      "B) The side panels are bent and sloped to form a conical section, which is a result of the twist in the panel dimensions.",
      "C) The top longeron is not perpendicular to the plane of the fuselage box section.",
      "D) The fuselage box section is a result of the tumbled home method, which is a proven technique used in the marine trades."
    ],
    "correct_answer": "B",
    "documentation": [
      "Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel \"undevelopable\" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition \"developable\". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a \"compounded\" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home) , the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock. This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the expected behavior of the intrinsic Hall conductivity in twisted bilayers of tMoTe2 at twist angles deviating from the commensurate value of 21.8°, as indicated by the moiré pattern in Figure S4?",
    "choices": [
      "A) The conductivity will remain unchanged, as the moiré pattern is a result of the twist angle deviation and does not affect the intrinsic Hall conductivity.",
      "B) The conductivity will increase by a factor of 2, due to the spin degeneracy of the energy windows ∼ ω around band degeneracies highlighted in Figure S4.",
      "C) The conductivity will exhibit a periodic variation with the twist angle, as the moiré pattern in Figure S4 suggests a periodic modulation of the layer composition σz.",
      "D) The conductivity will decrease by a factor of 2, due to the reduced mobility of electrons in the moiré pattern regions marked by A, B, and C in Figure S4."
    ],
    "correct_answer": "C",
    "documentation": [
      "Paper Info\n\nTitle: Crossed Nonlinear Dynamical Hall Effect in Twisted Bilayers\nPublish Date: 17 Mar 2023\nAuthor List: Figure\n\nFIG. 1.(a) Schematics of experimental setup.(b, c) Valence band structure and intrinsic Hall conductivity with respect to in-plane input for tMoTe2 at twist angles (b) θ = 1.2 • and (c) θ = 2 • in +K valley. Color coding in (b) and (c) denotes the layer composition σ z n (k).\nFIG. 2. (a) The interlayer BCP G, and (b) its vorticity [∂ k × G]z on the first valence band from +K valley of 1.2 • tMoTe2.Background color and arrows in (a) denote the magnitude and vector flow, respectively. Grey curves in (b) show energy contours at 1/2 and 3/4 of the band width. The black dashed arrow denotes direction of increasing hole doping level. Black dashed hexagons in (a, b) denote the boundary of moiré Brillouin zone (mBZ). FIG. 3. (a-c) Three high-symmetry stacking registries for tBG with a commensurate twist angle θ = 21.8 • .Lattice geometries with rotation center on an overlapping atomic site (a, b) and hexagonal center (c).(d) Schematic of the moiré pattern when the twist angle slightly deviates from 21.8 • , here θ = 21 • .Red squares marked by A, B and C are the local regions that resemble commensurate 21.8 • patterns in (a), (b) and (c), respectively.(e, f) Low-energy band structures and intrinsic Hall conductivity of the two geometries [(a) and (b) are equivalent].The shaded areas highlight energy windows ∼ ω around band degeneracies where interband transitions, not considered here, may quantitatively affect the conductivity measured.\nFIG. S4.Band structure and layer composition σ z n in +K valley of tBG (left panel) and the intrinsic Hall conductivity (right panel) at three different twist angle θ. The shaded areas highlight energy windows ∼ ω around band degeneracies in which the conductivity results should not be considered. Here σH should be multiplied by a factor of 2 accounting for spin degeneracy. abstract\n\nWe propose an unconventional nonlinear dynamical Hall effect characteristic of twisted bilayers."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a key challenge in dialect classification, particularly when dealing with multilingual datasets?",
    "choices": [
      "A) Handling codemixed data with high accuracy",
      "B) Achieving near-perfect test accuracy of 99.6% with language identification models",
      "C) Overcoming the imbalance ratio of 1:8 between classes in the dataset",
      "D) Using statistical methods like Gaussian Mixture Models and Frame Selection Decoding"
    ],
    "correct_answer": "C",
    "documentation": [
      "Lastly, language specific models like Spanish BERT (la Rosa y Eduardo G. Ponferrada y Manu Romero y Paulo Villegas y Pablo González de Prado Salas y María Grandury, 2022) and Portuguese BERT are available as well. Our winning solution makes use of these large language models trained on specific languages. Language Identification Models\n\nMany multilingual language identification models have been developed in order to classify the language of the input sentence beforehand. Even though the initial works used n-gram models and generative mixture models or even conditional random fields and other classical machine learning methods like naive bayes , modern methods have shifted to the use of deep learning for language identification . Recent works have mainly focused on deep learning based language identification, where handling codemixed data is a big challenge in the domain. For our experiments, we use a version of XLM-RoBERTa finetuned on a language identification dataset 2 . This model has near-perfect test accuracy of 99.6%. Dialect Classification\n\nDialect classification has been previously solved using statistical methods like Gaussian Mixture Models and Frame Selection Decoding or Support Vector Machines (SVM) . It has been explored relatively sparsely, mostly in the case for local languages . Deep learning approaches have been explored in previous editions of the VarDial workshop shared tasks and otherwise . Dialect classification was also explored previously as a part of other shared tasks . We want to stress that given the multilingual nature of the dataset, using the present methods directly was not an option. In our work, although we take inspiration from the previous works, we propose a novel system that surpasses the performance of the previous systems by a large margin. Data\n\nThe dataset  We observed that the class PT-BR had the most number of samples (2,724) and the class EN had the least number of samples (349), and thus the imbalance ratio was almost 1:8. We have illustrated the data distribution in Figure ."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The authors derive an inequality for the kinetic energy of plasma blobs in a compressible flow, which is absent in the incompressible flow case. They also show that the acceleration of the blobs is proportional to the total radial particle flux. However, they assume that the results hold for both depletions and amplifications of the density field. What is the main assumption underlying this claim?",
    "choices": [
      "A) The authors assume that the results hold for both depletions and amplifications of the density field because the numerical results show that the inequality is satisfied for both cases.",
      "B) The authors assume that the results hold for both depletions and amplifications of the density field because the inequality is derived using the Cauchy-Schwartz inequality, which is valid for both cases.",
      "C) The authors assume that the results hold for both depletions and amplifications of the density field because the density field is initialized with a seeded blob of radius $\\ell$ and amplitude $\\triangle n$, which is a common assumption in plasma physics.",
      "D) The authors assume that the results hold for both depletions and amplifications of the density field because the acceleration of the blobs is proportional to the total radial particle flux, which is a fundamental property of compressible flows."
    ],
    "correct_answer": "C",
    "documentation": [
      "Since both $S(t)\\geq 0$ and $E(t)\\geq 0$ we further derive from Eq.~\\eqref{eq:energya} and Eq.~\\eqref{eq:energyb} that the kinetic energy\nis bounded by $E(t) \\leq T_eS(t) + E(t) = T_e S(0)$; a feature absent from the gravitational system with \nincompressible flows, where $S(t) = S(0)$. \n\nWe now show that the invariants Eqs.~\\eqref{eq:energya} and \\eqref{eq:energyb} present restrictions on the velocity and\nacceleration of plasma blobs. First, we define the blobs' center of mass (COM) via $X(t):= \\int\\mathrm{dA}\\, x(n-n_0)/M$ and \nits COM velocity as $V(t):=\\d X(t)/\\d t$. The latter is proportional to the total radial particle flux~\\cite{Garcia_Bian_Fundamensky_POP_2006, Held2016a}. We assume\nthat $n>n_0$ and $(n-n_0)^2/2 \\leq [ n\\ln (n/n_0) - (n-n_0)]n $ to show for both systems \n\\begin{align}\n  (MV)^2 &= \\left( \\int \\mathrm{dA}\\, n{\\phi_y}/{B} \\right)^2\n  = \\left( \\int \\mathrm{dA}\\, (n-n_0){\\phi_y}/{B} \\right)^2\\nonumber\\\\\n \n&\\leq 2 \\left( \\int \\mathrm{dA}\\, \\left[n\\ln (n/n_0) -(n-n_0)\\right]^{1/2}\\sqrt{n}{\\phi_y}/{B}\\right)^2\\nonumber\\\\\n \n  &\\leq 4 S(0) E(t)/m_i \n \n  \\label{eq:inequality}\n\\end{align} Here we use the Cauchy-Schwartz inequality and \n$\\phi_y:=\\partial\\phi/\\partial y$. \nNote that although we derive the inequality Eq.~\\eqref{eq:inequality} only for amplitudes $\\triangle n >0$  we assume that the results also hold for depletions. This is justified by our numerical results later in this letter. If we initialize our density field with a seeded blob of radius $\\ell$ and amplitude $\\triangle n$ as \n\\begin{align}\n  n(\\vec x, 0) &= n_0 + \\triangle n \\exp\\left( -\\frac{\\vec x^2}{2\\ell^2} \\right), \\label{eq:inita}\n \n \n\\end{align}\nand  \n$\\phi(\\vec x, 0 ) = 0$,\nwe immediately have $M := M(0) = 2\\pi \\ell^2 \\triangle n$, $E(0) = G(0) = 0$ and \n$S(0) = 2\\pi \\ell^2 f(\\triangle n)$, where $f(\\triangle n)$ captures the amplitude dependence of \nthe integral for $S(0)$. \n\nThe acceleration for both incompressible and compressible flows can be estimated\nby assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$~\\cite{Held2016a} and using \n$E(t) = G(t)"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the demographics of McPherson County based on the information provided about the county's population and household characteristics?",
    "choices": [
      "A) The county has a high percentage of young families with children under the age of 18, given the median age of 38 years and the average household size of 2.49.",
      "B) The county has a significant proportion of elderly residents, with 17.30% of the population being 65 years of age or older, and a median age of 38 years.",
      "C) The county has a relatively low poverty rate, with only 4.20% of families and 6.60% of the population below the poverty line, considering the median income for a household and family.",
      "D) The county has a high percentage of males, given the ratio of males to females in the population and the median income for males being higher than for females."
    ],
    "correct_answer": "B",
    "documentation": [
      "25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 2.99. In the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 95.90 males. For every 100 females age 18 and over, there were 92.90 males. The median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921. About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over. Government\n\nPresidential elections\nMcPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson. Laws\nFollowing amendment to the Kansas Constitution in 1986, the county remained a prohibition, or \"dry\", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement. Education\n\nColleges\n McPherson College in McPherson\n Bethany College in Lindsborg\n Central Christian College in McPherson\n\nUnified school districts\n Smoky Valley USD 400\n McPherson USD 418\n Canton-Galva USD 419\n Moundridge USD 423\n Inman USD 448\n\nSchool district office in neighboring county\n Goessel USD 411\n Little River-Windom USD 444\n\nMuseums\n Birger Sandzén Memorial Gallery in Lindsborg\n McCormick-Deering Days Museum in Inman\n McPherson Museum in McPherson\n Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg\n Kansas Motorcycle Museum in Marquette\n\nCommunities\n\nCities\n\n Canton\n Galva\n Inman\n Lindsborg\n Marquette\n McPherson (county seat) \n Moundridge\n Windom\n\nUnincorporated communities\n† means a Census-Designated Place (CDP) by the United States Census Bureau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the performance of NFPA and FPSA solvers in comparison to GMRES and DSA solvers, given the results from the HGK kernels for problems 1 and 2?",
    "choices": [
      "A) NFPA and FPSA consistently outperform GMRES and DSA across all values of g.",
      "B) NFPA and FPSA require fewer iterations than GMRES and DSA for problem 1, but GMRES and DSA require fewer iterations for problem 2.",
      "C) NFPA and FPSA require more time and iterations than GMRES and DSA for problem 1, but GMRES and DSA require more time and iterations for problem 2.",
      "D) NFPA and FPSA outperform GMRES and DSA in iteration count for problem 1, but GMRES and DSA outperform NFPA and FPSA in runtime for problem 2."
    ],
    "correct_answer": "D",
    "documentation": [
      "The three kernels tested are shown in \\cref{HGK}. GMRES, DSA, FPSA, and NFPA all converged to the same solution for problems 1 and 2.\n\\Cref{HGK_plots} shows the solutions for HGK with $g = 0.99$.\nThe results of each solver are shown in \\cref{HGKresults1,HGKresults2}. \\begin{table}[h]\n\\begin{center}\n\\scalebox{0.8}{\n\\begin{tabular}{c || c || c || c} \\hline \nParameter & Solver & Runtime (s) & Iterations \\\\ \\hline \\hline\n\\multirow{4}{*}{$g=0.9$} & GMRES & 9.88 & 76 \\\\\n& DSA & 24.5 & 554 \\\\\n& FPSA & 1.50 & 32 \\\\ \n& NFPA & 1.39 & 27 \\\\ \\hline \n\\multirow{4}{*}{$g=0.95$} & GMRES & 12.2 & 131 \\\\\n& DSA & 47.7 & 1083 \\\\\n& FPSA & 1.75 & 38 \\\\ \n& NFPA & 1.83 & 35 \\\\ \\hline \n\\multirow{4}{*}{$g=0.99$} & GMRES & 40.0 & 27 \\\\\n& DSA & 243 & 5530  \\\\\n& FPSA & 3.38 & 74 \\\\ \n& NFPA & 3.93 & 73 \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{Runtime and Iteration Counts for Problem 1 with HGK}\n\\label{HGKresults1} \n\\end{table}\n\\begin{table}[h]\n\\begin{center}\n\\scalebox{0.8}{\n\\begin{tabular}{c || c || c || c} \\hline \nParameter & Solver & Runtime (s) & Iterations \\\\ \\hline \\hline\n\\multirow{4}{*}{$g=0.9$} & GMRES & 24.3 & 135 \\\\\n& DSA & 14.8 & 336  \\\\\n& FPSA & 1.15 & 23 \\\\ \n& NFPA & 1.35 & 24 \\\\ \\hline \n\\multirow{4}{*}{$g=0.95$} & GMRES & 31.3 & 107 \\\\\n& DSA & 29.7 & 675 \\\\\n& FPSA & 1.56 & 32 \\\\ \n& NFPA & 1.90 & 33 \\\\ \\hline \n\\multirow{4}{*}{$g=0.99$} & GMRES & 41.4 & 126 \\\\\n& DSA & 146 & 3345 \\\\\n& FPSA & 3.31 & 67 \\\\ \n& NFPA & 3.99 & 67 \\\\ \\hline  \n\\end{tabular}}\n\\end{center}\n\\caption{Runtime and Iteration Counts for Problem 2 with HGK}\n\\label{HGKresults2} \n\\end{table} Here we see that NFPA and FPSA do not perform as well compared to their results for the SRK and EK. Contrary to what happened in those cases, both solvers require more time and iterations as the problem becomes more anisotropic. This is somewhat expected, due to HGK not having a valid Fokker-Planck limit. However, both NFPA and FPSA continue to greatly outperform GMRES and DSA. Moreover, NFPA outperforms FPSA in iteration count for problem 1."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyze the implications of the Supreme Court's role in resolving disputes between the President and Congress over custody of their respective powers. How might this impact the balance of power in the American constitutional order?",
    "choices": [
      "A) The Court's involvement would lead to a more stable and predictable system of government, as it would provide a clear framework for resolving conflicts between the branches. This would allow for more effective policy-making and a reduction in stalemates.",
      "B) The Court's role would undermine the system of checks and balances, as it would give the judiciary too much power to dictate the outcome of disputes between the branches. This would lead to a concentration of power in the hands of the judiciary and a decrease in the effectiveness of the other branches.",
      "C) The Court's involvement would not significantly impact the balance of power, as it would only be called upon to resolve disputes that are already contentious and difficult to resolve through other means. This would allow the other branches to continue to function effectively, even if the Court's decisions are not always popular.",
      "D) The Court's role would lead to a shift in the balance of power towards the executive branch, as the President would be able to use the Court to block legislation and appointments that are opposed by Congress. This would allow the President to exert significant influence over the legislative process and undermine the authority of Congress."
    ],
    "correct_answer": "B",
    "documentation": [
      "Four take home writing assignments. Analytic essays, each 1000-1500 words. (Grades weighted: 10%, 25%, 25%, and 25%) Late essays will not be accepted, except with a doctor’s excuse or a Dean’s excuse for family emergency. Regular preparation and class participation: 15%. OR as an option: By prior arrangement with me by the due date of the second analytic essay, students may substitute one longer research paper (15 – 20 pages) for two of the last three analytic papers This paper will be on a topic of the students choosing , if I approve, and the due date will be the same as the last assigned analytic essay. This project would count 50% of the students course grade. Selected writings by Frederick Douglass, W.E.B. Dubois, Ralph Ellison, James Baldwin\nSolzhenitsyn, “A World Split Apart”\nTocqueville, Democracy in America GOV 382M • Tocqueville 39150 • Spring 2011 Meets T 6:30PM-9:30PM BAT 5.102 show description\nSee syllabus GOV 370L • President, Congress, And Court 38695 • Fall 2010 Meets TTH 8:00AM-9:30AM UTC 3.112 show description\nCourse Description: A Study of the political relationship of the President, Congress and Court in the American constitutional order. Has this relationship changed over the course of American history? Is American national politics prone to stalemate or deadlock between the branches regarding major issues of public policy? Do we have a new “imperial presidency?” Should the Court arbitrate disputes between the President and Congress over custody of their respective powers? Has Congress abdicated its constitutional responsibilities? We will examine questions like these in light of practical problems such as executive privilege and secrecy, the war on terror, budget politics and controversies regarding appointments to the Supreme Court. Grading: Three in class essay tests, for which study questions will be distributed in advance. The exam questions will be chosen from the list of study questions. (25% each) One short take home essay (10% each). Class participation and attendance (15%)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the reconstruction error plots in Fig. \\ref{PL-err-Jor1}, what can be inferred about the effect of the number of samples on the reconstruction error for the 2XY model at different temperatures?",
    "choices": [
      "A) The reconstruction error decreases as the number of samples increases, regardless of the temperature.",
      "B) The reconstruction error decreases as the number of samples increases, but only for temperatures above the Kosterlitz-Thouless transition temperature.",
      "C) The reconstruction error remains high for all temperatures when only 64 samples are considered, but decreases as the number of samples increases for temperatures above the Kosterlitz-Thouless transition temperature.",
      "D) The reconstruction error decreases as the number of samples increases, but only for temperatures below the Kosterlitz-Thouless transition temperature."
    ],
    "correct_answer": "C",
    "documentation": [
      "Even though the jump is smeared, the difference between inferred couplings corresponding to the set of non-zero couplings \n     and to the set of zero couplings can be clearly appreciated. Similarly, the plots in the right column of Fig. \\ref{PL-Jor1} show the results obtained for the case with  bimodal disordered couplings, for the same working temperature and number of samples. In particular, note that the algorithm infers half positive and half negative couplings, as expected.\n     \n     \n\\begin{figure}\n\\centering\n\\includegraphics[width=1\\linewidth]{Jor11_2D_l2_errJ_varT_varM}\n\\caption{Reconstruction error $\\mbox{err}_J$, cf. Eq. (\\ref{eq:errj}), plotted as a function of temperature (left) for three values of the number of samples $M$ and  as a function $M$ (right) for three values of temperature in the ordered system, i.e., $J_{ij}=0,1$. \nThe system size is $N=64$.}\n\\label{PL-err-Jor1}\n\\end{figure} In order to analyze the effects of the number of samples and of the temperature regimes, we plot in Fig. \\ref{PL-err-Jor1} the reconstruction error, Eq. (\\ref{err}), as a function of temperature for three different sample sizes $M=64,128$ and $512$. The error is seen to sharply rise al low temperature, incidentally, in the ordered case, for  $T<T_c \\sim 0.893$, which is the Kosterlitz-Thouless transition temperature of the 2XY model\\cite{Olsson92}. However, we can see that if only $M=64$ samples are considered, $\\mbox{err}_J$ remains high independently on the working temperature. In the right plot of Fig. \\ref{PL-err-Jor1},  $\\mbox{err}_J$ is plotted as a function of $M$ for three different working temperatures $T/J=0.4,0.7$ and $1.3$. As we expect, \n $\\mbox{err}_J$  decreases as $M$ increases. This effect was observed also with mean-field inference techniques on the same model\\cite{Tyagi15}. To better understand the performances of the algorithms, in Fig. \\ref{PL-varTP-Jor1} we show several True Positive (TP) curves obtained for various values of $M$ at three different temperatures $T$. As $M$ is large and/or temperature is not too small,  we are able to reconstruct correctly all the couplings present in the system (see bottom plots)."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The development of plasticity mechanisms in agents is influenced by various factors, including environmental variability, sensory system reliability, and the complexity of the motor network. However, the relationship between these factors is not always straightforward. Which of the following statements best summarizes the findings of the study on the impact of environmental variability on the development of plasticity mechanisms in agents?",
    "choices": [
      "A) Environmental variability has no significant impact on the development of plasticity mechanisms in agents.",
      "B) A diverse environment and a reliable sensory system are necessary conditions for the development of plasticity mechanisms in agents, but environmental variability has no effect on this process.",
      "C) Environmental variability significantly increases the development of plasticity mechanisms in agents, but only in the presence of a complex motor network.",
      "D) The interaction between a static motor network and a plastic sensory network gives rise to a greater variety of well-functioning learning rules, but environmental variability has no effect on this process."
    ],
    "correct_answer": "C",
    "documentation": [
      "The agents perform equally well in this variation of the task as before (Fig. ), but now, the evolved plasticity rules seem to be more structured (Fig. ). Moreover, the variance of the learned weights in the bestperforming agents is significantly reduced (Fig. ), which indicates that the bottleneck in the sensory network is in-creasing selection pressure for rules that learn the environment's food distribution accurately. We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity. Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. Our results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents. Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way. Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability. Our findings on the effect of environmental variability concur with the findings of previous studies that have identified the constraints that environmental variability places on the evolutionary viability of learning behaviors."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the historical development of McPherson County, Kansas, based on the fact that the cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships?",
    "choices": [
      "A) The county's population has been steadily increasing since the late 19th century, with Lindsborg and McPherson being major contributors to the growth.",
      "B) The county's economy was primarily driven by agriculture and industry in the late 19th and early 20th centuries, with Lindsborg and McPherson serving as major commercial centers.",
      "C) The county's government has historically been decentralized, with Lindsborg and McPherson having a significant degree of autonomy and self-governance.",
      "D) The county's population has been declining since the mid-20th century, with Lindsborg and McPherson being major contributors to the out-migration of younger residents."
    ],
    "correct_answer": "C",
    "documentation": [
      "Conway\n Elyria†\n Groveland\n Johnstown\n New Gottland\n Roxbury†\n\nGhost towns\n Alta Mills\n Battle Hill\n Christian\n Doles Park\n Elivon\n King City\n Sweadal\n\nTownships\nMcPherson County is divided into twenty-five townships. The cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships. In the following table, the population center is the largest city (or cities) included in that township's population total, if it is of a significant size. See also\n List of people from McPherson County, Kansas\n National Register of Historic Places listings in McPherson County, Kansas\n McPherson Valley Wetlands\n Maxwell Wildlife Refuge\n\nReferences\n\nNotes\n\nFurther reading\n\n Wheeler, Wayne Leland. \"An Analysis of Social Change in a Swedish-Immigrant Community: The Case of Lindsborg, Kansas.\" (PhD dissertation, University of Missouri-Columbia; ProQuest Dissertations Publishing, 1959. 5905657). County\n Through the Years: A Pictorial History of McPherson County; McPherson Sentinel' Heritage House Publishing Co; 1992. McPherson County First Courthouse Built About 1869 or 1870; Lindsborg News-Record; March 30, 1959. Pioneer Life and Lore of McPherson County, Kansas; Edna Nyquist; Democratic-Opinion Press; 1932. A History of the Church of the Brethren in Kansas (includes McPherson College history); Elmer LeRoy Craik; McPherson Daily; Republican Press; 397 pages; 1922. Portrait and Biographical Record of Dickinson, Saline, McPherson, and Marion Counties, Kansas; Chapman Bros; 614 pages; 1893. Standard Atlas of McPherson County, Kansas; Geo. A. Ogle & Co; 82 pages; 1921. Plat Book of McPherson County, Kansas; North West Publishing Co; 50 pages; 1903. Edwards' Atlas of McPherson County, Kansas; John P. Edwards; 51 pages; 1884. Trails\n The Story of the Marking of the Santa Fe Trail by the Daughters of the American Revolution in Kansas and the State of Kansas; Almira Cordry; Crane Co; 164 pages; 1915. (Download 4MB PDF eBook)\n The National Old Trails Road To Southern California, Part 1 (LA to KC); Automobile Club Of Southern California; 64 pages; 1916."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the temporal evolution of entropy production rates, what can be inferred about the impact of the shock wave on the bubble's deformation and the resulting entropy generation rate?",
    "choices": [
      "A) The shock wave's passage through the bubble causes a rapid increase in entropy production, followed by a decrease as the contact interface widens, and finally, a rapid drop as the shock wave exits the flow field.",
      "B) The shock wave's passage through the bubble causes a rapid increase in entropy production, followed by a decrease as the contact interface widens, and finally, a stable increase as the shock wave exits the flow field.",
      "C) The shock wave's passage through the bubble causes a rapid decrease in entropy production, followed by a decrease as the contact interface widens, and finally, a stable increase as the shock wave exits the flow field.",
      "D) The shock wave's passage through the bubble causes a rapid increase in entropy production, followed by a decrease as the contact interface widens, and finally, a stable decrease as the shock wave exits the flow field."
    ],
    "correct_answer": "A",
    "documentation": [
      "They are key factors in compression science field. The former is induced by temperature gradient and the NOEF (∆ * 3,1 ). The latter is affected by velocity gradient and the NOMF (∆ * 2 ). The entropy production rates are defined by the following formulas : Integrating the ṠNOEF and ṠNOMF over time t, the entropy generations over this period of time are obtained, i.e., S NOEF = t 0 ṠNOEF dt and S NOMF = t 0 ṠNOMF dt. Plotted in Fig. ) and 14(b) are the temporal evolution of ṠNOMF and ṠNOEF , respectively. The evolution of entropy generation rate is related to two aspects: (i) the propagation of the shock wave, and (ii) the deformation of the bubble. The former generates a macroscopic quantity gradient, and the latter makes the contact interface wider, longer, and deformed. Depending on the location of the shock wavefront, there exist two critical moments in this SBI process: (i) at around t = 0.03, the shock wave just sweeps through the bubble, and (ii) at t = 0.06, the shock wave exits the flow field. Therefore, the temporal evolution of the entropy production rate shows three stages, i.e., t < 0.03, 0.03 < t < 0.06, and t > 0.06. At the stage t < 0.03, the shock compression stage, the shock effects compress the bubble. It generates the large macroscopic quantity gradients, resulting in a quick increase of ṠNOMF . At around t = 0.03, the shock wave passed through the bubble. So the values of ṠNOMF decreases. The values of ṠNOMF would continue to decrease due to the gradually wider contact interface caused by the diffusion effect. At around t = 0.06, the shock wave comes out of the flow field so that the values of ṠNOMF drops rapidly. In the third stage, i.e., t > 0.06, because of the diffusive effect, the general trend of ṠNOMF is downward. However, it shows an oscillatory trend due to the influence of various reflected shock waves. The specific heat ratio indirectly changes the value of ṠNOMF by changing the velocity gradient. The smaller the specific-heat ratio, the larger ṠNOMF . Different understanding can be seen in Fig. , where the temporal evolution of ṠNOEF is plotted."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A paramecium is a biological machine that operates through self-organization and evolution. It lacks sentience and is not a product of design. However, its behavior is influenced by the interactions of organic chemistry. What is the primary reason for the distinction between a paramecium and a machine?",
    "choices": [
      "A) The paramecium's behavior is shaped by its own internal goals and motivations, whereas machines are programmed by external designers.",
      "B) The paramecium's behavior is the result of its own evolution and self-organization, whereas machines are created through deliberate design and engineering.",
      "C) The paramecium's behavior is influenced by its environment and the interactions of organic chemistry, whereas machines are isolated from their environment and operate independently.",
      "D) The paramecium's behavior is shaped by its own internal processes, such as cilia movement and vacuole dumping, whereas machines are controlled by external inputs and feedback loops."
    ],
    "correct_answer": "B",
    "documentation": [
      "A paramecium is not full of Schnitt. It is not measuring or having goals or anything else. It is an automaton. To think otherwise would be to invite some sort of Bergsonian \"elan vital\" or other dualistic essence. The problem with the term \"observation\" is that it's prejudicial in common parlance. It implies some sort of sentient observer. It implies a subjective aspect. So a more neutral term would be needed when a microbe registers a chance in its environment and contracts or expands or fires up the cilia or dumps the vacuoles or whatever. Or when a Bose Einstein condensate loses its coherence in a wet noisy puddle. Braininvat » April 24th, 2018 , 12:52 pm wrote: It seems likely a paramecium does no representing to a self, and is pretty much a cellular machine lacking sentience. But it is not a machine for the simple reason that it is not a product of design. The only reasons for which it does things are its own reasons. It is a product of self organization, and the learning process which is evolution. I certainly agree with the term \"biological machinery,\" which is to say that there is no reason to distinguish things simply on the basis that one uses the interactions of organic chemistry. Thus I think the locus of difference between the living organism and the machine has to do with origins whether it is by design or by learning, evolution, and self-organization. Braininvat » April 24th, 2018, 12:52 pm wrote: The problem with the term \"observation\" is that it's prejudicial in common parlance. It implies some sort of sentient observer. It implies a subjective aspect. So a more neutral term would be needed when a microbe registers a chance in its environment and contracts or expands or fires up the cilia or dumps the vacuoles or whatever. But the problem with this is that the prejudice in language goes both ways with the presumption of an uncrossable divide between the sentient and the non-sentient, when all the evidence points to a continuum going all the way from the non-living to the living to the sentient."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The Supreme Court's decision in Wayfair v. Amex challenged the physical presence rule, a constitutional precedent for stare decisis. However, the Court's reasoning was based on a flawed understanding of the relationship between stare decisis and the Constitution. What is the primary flaw in the Court's stare decisis analysis in Wayfair?",
    "choices": [
      "A) The Court's reliance on the physical presence rule as a constitutional precedent ignores the potential for Congress to correct an erroneous decision.",
      "B) The Court's emphasis on the formalism of stare decisis overlooks the importance of considering the Constitution's text and history.",
      "C) The Court's rejection of the physical presence rule as a constitutional precedent is inconsistent with its own previous decisions on stare decisis.",
      "D) The Court's stare decisis analysis in Wayfair fails to account for the potential consequences of overruling a constitutional precedent."
    ],
    "correct_answer": "B",
    "documentation": [
      "Or it could have argued that new facts on the ground — namely, the blast of e-commerce that hit like a comet after Quill — overpowered stare decisis of any force, special or plain.61× 61. Two recent studies of stare decisis highlighted the physical presence rule as exemplifying a precedent that may reasonably be overruled due to changed facts. See Bryan A. Garner et al., The Law of Judicial Precedent 364–65 (2016); Randy J. Kozel, Settled Versus Right: A Theory of Precedent 112–13 (2017). It should be noted that the authors of The Law of Judicial Precedent classify the physical presence rule as a constitutional precedent for stare decisis purposes, thus anticipating the Court’s misstep in Wayfair. Garner et al., supra, at 354–65. Because even statutory precedents may sometimes be overruled,62× 62. See Patterson v. McLean Credit Union, 491 U.S. 164, 173–74 (1989) (discussing justifications for overruling statutory precedents). Contra Lawrence C. Marshall, “Let Congress Do It”: The Case for an Absolute Rule of Statutory Stare Decisis, 88 Mich. L. Rev. 177 (1989). the Court could have killed Quill without first planting its constitutional kiss of death.63× 63. Cf. Thomas R. Lee, Stare Decisis in Historical Perspective: From the Founding Era to the Rehnquist Court, 52 Vand. L. Rev. 647, 704 (1999) (“Justice Brandeis’ . . . memorable prose has since become a mandatory part of the burial rite for any constitutional precedent.”). The Court resisted such arguments. Instead, Wayfair reasoned that Congress’s total ability to correct an erroneous decision counts for nothing when the Court gets the Constitution wrong. That such a theory sprouts from a case like Wayfair, which repudiated a “formalistic distinction,”64× 64. Wayfair, 138 S. Ct. at 2092. is ironic. Wayfair’s stare decisis analysis resorts to the formalism of making constitutional a “magic” word65× 65. See Transcript of Oral Argument, supra note 21, at 12. rather than asking whether Congress can step in. Moreover, the Court’s new thinking on stare decisis threatens other constitutional default rules."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a fundamental consequence of the fast processes associated with disorder-induced heating in the formation of ultracold plasmas?",
    "choices": [
      "A) The electron temperature remains constant throughout the plasma evolution.",
      "B) The ion temperature is significantly higher than the initial electron temperature due to disorder-induced heating.",
      "C) The effective initial electron temperature is constrained to a narrow range, but the ion temperature can still be elevated to around one degree Kelvin.",
      "D) The disorder-induced heating process leads to a complete loss of ion temperature, resulting in a plasma with a uniform electron temperature."
    ],
    "correct_answer": "B",
    "documentation": [
      "\\section{Introduction}\n\nUltracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \\cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \\cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \\cite{Rousse,Esarey}. Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field. The relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s. A series of reviews affords a good overview of progress in the last twenty years \\cite{Gallagher,Killian_Science,PhysRept,Lyon}. Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas. While molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \\cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \\cite{Killian}. This work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions. Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \\cite{Kuzmin,Hanson,Laha}."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a likely consequence of the bipartisan efforts to delay and ultimately repeal the mandated sale of Plum Island, given the potential for gypsy moth outbreak in the region?",
    "choices": [
      "A) The sale of Plum Island will be accelerated to prevent the spread of gypsy moths.",
      "B) The delay in selling Plum Island will lead to increased funding for gypsy moth control measures in the surrounding areas.",
      "C) The preservation of Plum Island will be compromised due to the economic pressures of maintaining the island's ecological value.",
      "D) The repeal of the sale of Plum Island will result in a significant reduction in the number of gypsy moth outbreaks in the region."
    ],
    "correct_answer": "B",
    "documentation": [
      "Current law states that Plum Island must be sold publicly to help finance the new research facility. Aerial view of Plum Island. The lawmakers joint statement explained, “The amendment will prevent the federal agency in charge of the island from moving forward with a sale by prohibiting it from using any of its operational funding provided by Congress for that purpose,” concluding, ” This will not be the end of the fight to preserve Plum Island, but this will provide us with more time to find a permanent solution for protecting the Island for generations to come.” For several years, members from both sides of Long Island Sound have been working in a bipartisan manner to delay and, ultimately, repeal the mandated sale of this ecological treasure. Earlier this year, the representatives, along with the whole Connecticut delegation, cosponsored legislation that passed the House unanimously to delay the sale of Plum Island. Filed Under: Outdoors July 1 Update: Aquatic Treatment Planned for Rogers Lake, July 5 July 1, 2016 by admin Leave a Comment We received this updated information from the Old Lyme Selectman’s office at 11:05 a.m. this morning:\nFiled Under: Lyme, Old Lyme, Outdoors, Town Hall They’re Everywhere! All About Gypsy Moth Caterpillars — Advice from CT Agricultural Experiment Station June 2, 2016 by Adina Ripin Leave a Comment Gypsy moth caterpillars – photo by Peter Trenchard, CAES. The potential for gypsy moth outbreak exists every year in our community. Dr. Kirby Stafford III, head of the Department of Entomology at the Connecticut Agricultural Experiment Station, has written a fact sheet on the gypsy moth available on the CAES website. The following information is from this fact sheet. The gypsy moth, Lymantria dispar, was introduced into the US (Massachusetts) by Etienne Leopold Trouvelot in about 1860. The escaped larvae led to small outbreaks in the area in 1882, increasing rapidly. It was first detected in Connecticut in 1905. By 1952, it had spread to 169 towns. In 1981, 1.5 million acres were defoliated in Connecticut."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What was the primary reason for the transfer of operator responsibility from ConocoPhillips to Statoil in 2013, in connection with the removal of the H7 platform?",
    "choices": [
      "A) The discovery of the Ekofisk field in 1969 led to an increase in oil production, making the platform less critical to the Norwegian gas transport network.",
      "B) The collision between Reint and H7 in 1995 highlighted the need for improved safety measures, leading to a reevaluation of operator responsibility.",
      "C) The shutdown of the H7 platform in 2007 and the completion of removal in 2013 marked the end of an era, and Statoil was chosen to take over as the company in charge of the project on Gassco's behalf.",
      "D) The award of the first exploration licences in 1965 led to a surge in oil-related growth in the region, but the actual work was still carried out by ConocoPhillips as a technical service provider to Gassco, making Statoil's involvement unnecessary."
    ],
    "correct_answer": "C",
    "documentation": [
      "Control panels on turbines and compressors were replaced and metering instruments installed to conduct measurements in this equipment. While the nearest neighbour to B11 was a Danish oil field, H7 stood in the middle of the shipping channel. M/S Hero broke down 15 nautical miles west of the latter platform at around 13.00 on 12 November 1977. By 21.00, the ship was still adrift and heading directly for H7, and all 14 crew on the platform made ready to evacuate by helicopter – the waves were too high for the lifeboats. The wreck passed at 21.40 with a clearance of 400 metres. German cargo carrier Reint collided with H7 on 30 September 1995, despite efforts by the standby ship to avert the threat. Production was halted as a safety measure, but the platform luckily suffered only minor damage. The collision was caused by inadequate watchkeeping on the ship’s bridge. Operator responsibility for B11 and H7 was transferred at the beginning of 2003 to Norway’s state-owned Gassco company, which runs the Norwegian gas transport network. This change had little significance for operation of the platforms, since the actual work was still carried out by ConocoPhillips as a technical service provider to Gassco. H7 was shut down in 2007, and removal had been completed in 2013. In connection with preparations to remove the structure, operator responsibility was transferred to Statoil as the company in charge of the project on Gassco’s behalf. Published 24. August 2016 • Updated 22. October 2019\nPhillips inundates Sola with oil revenues\nperson by Kristin Øye Gjerde\nStavanger and neighbouring Sola were the first Norwegian local authorities to experience fantastic oil-related growth after the award of the first exploration licences in 1965. — Phillips er i ferd med å etablere seg på Norscobasen nederst til høyre Ca 1972 Foto: Norsk fly og flyfoto/Norsk Oljemuseum\nThe Shell refinery at Risavika in Sola was completed two years later, while the Norsco base in Tananger became operational as early as 1966. But things really took off once the Ekofisk field had been discovered in the autumn of 1969 and started trial production on 14 July 1971."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the $B_s$ oscillation frequency from the CDF result on fully reconstructed decays, given that the D\\O\\ results on opposite and same side muon tagging are consistent with each other but differ from the CDF result?",
    "choices": [
      "A) The CDF result is consistent with the D\\O\\ results, indicating a possible discrepancy in the measurement of $\\Delta m_d$.",
      "B) The CDF result is consistent with the D\\O\\ results, but the discrepancy in $\\Delta m_d$ values may be due to differences in the reconstruction methods.",
      "C) The CDF result is inconsistent with the D\\O\\ results, indicating a possible difference in the measurement of $\\Delta m_d$.",
      "D) The CDF result is consistent with the D\\O\\ results, but the difference in $\\Delta m_d$ values may be due to the fact that the CDF result is based on fully reconstructed decays, while the D\\O\\ results are based on partially reconstructed decays."
    ],
    "correct_answer": "D",
    "documentation": [
      "As a benchmark for future $B_s$ oscillation measurement, both groups\nstudy  $B_d$ mixing, gaining an understanding of the different components\nof a $B$ mixing analysis (sample composition, flavor tagging, vertexing,\nasymmetry fitting). For a sample of partially reconstructed decays\n$B\\rightarrow D^*(2010)^+\\mu^-X$, D\\O\\ obtains \n$\\Delta m_d = 0.506 \\pm 0.055 (stat) \\pm  0.049 (syst))$ ps$^{-1}$ and\n$\\Delta m_d = 0.488 \\pm 0.066 (stat) \\pm  0.044 (syst))$ ps$^{-1}$\nwhen employing  opposite side muon tagging and the same side tagging,\nrespectively. The CDF result for semileptonic channels is\n$\\Delta m_d = 0.536 \\pm 0.037 (stat) \\pm  0.009 (s.c.) \\pm 0.015 (syst)$ ps$^{-1}$.\nCDF also reports a result on $B$ oscillations using fully reconstructed\ndecays:\n$\\Delta m_d = 0.526 \\pm 0.056 (stat) \\pm  0.005 (syst))$ ps$^{-1}$.\n\nReconstructing $B_s$ decays into different final states is another\nimportant\n step in the ${B_s^0}$ -${\\overline{B}_s^0}$ ~mixing analysis. Thanks to the  large muon and tracking coverage,   D\\O\\ is accumulating\na  high statistics sample of semileptonic $B_s$ decays. D\\O\\ reconstructs the $B_s \\rightarrow D^+_s \\mu^- X$ decays, with\n$D^+_s \\rightarrow \\phi \\pi^+ $ and\n$D^+_s \\rightarrow K^* K^+ $,\nat a rate of $\\approx$ 40(25) events per pb$^{-1}$,  respectively. Figure \\ref{fig:d0_bsdsphipi} shows the mass distribution of the\n$D^+_s \\rightarrow \\phi \\pi$ candidates. \\begin{figure}[htb]\n\\vspace*{-5mm}\n\\includegraphics[height=0.3\\textheight,width=8.0cm]  {blds-250.eps}\n\\vspace*{-1.2cm}\n\\caption{  $D^+_s \\rightarrow \\phi \\pi^+$  signal. (D\\O)}\n\\label{fig:d0_bsdsphipi}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\vspace*{-10mm}\n\\hspace*{-4mm}\n\\includegraphics[height=0.35\\textheight,width=7.9cm]  {cdf_Bs-DsPi-PhiPi.eps}\n\n\\vspace*{-1.0cm}\n\\caption{ $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$  signal. (CDF)}\n\\label{fig:cdf_bsdsphipi}\n\\end{figure}\n\n\nCDF has clean signals for fully hadronic, flavor-specific  $B_s$ decays,\nproviding the best sensitivity to $B_s$ oscillations at high\n$\\Delta m_s$. Figure \\ref{fig:cdf_bsdsphipi} shows the signal for\nthe best channel, $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$.\n\n\\clearpage\n\n\n\\subsection{Rare decays}\n\nThe purely leptonic decays $B_{d,s}^0 \\rightarrow \\mu^+\n\\mu^-$ are flavor-changing neutral current (FCNC) processes."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "The libertarian notion of free will is often criticized for its inability to account for the causal chain of events that lead to our actions. According to critics, what is the primary motivation behind libertarians' defense of this notion?",
    "choices": [
      "A) A desire to promote moral responsibility and retributive justice",
      "B) A fear of determinism and the loss of personal agency",
      "C) A need to justify the existence of moral responsibility in a deterministic universe",
      "D) A misunderstanding of the concept of free will and its relationship to indeterminism"
    ],
    "correct_answer": "B",
    "documentation": [
      "Libertarians do not necessarily accept that this argument shows that we do not have free will, and the reason, or at least a big part of it, should not surprise anyone at this point: they simply define free will differently. According to libertarians, such as Robert Nozick and Robert Kane, one has free will if one could have acted otherwise than one did, and if indeterminism is true, then it may be true that we could have “acted” differently than we did under the exact same circumstances, and that we thereby might have free will in this sense. It should be pointed out, though, that critics of libertarianism are“rightly skeptical about the relevance of this kind of free will. First of all, the free will that libertarians endorse is, unlike what many libertarians seem to think, not an ethically relevant kind of freedom, and it does not have anything to do with the freedom of action that we by definition want. Second, the hard incompatibilist is right that no matter what is true about the degree to which the universe is deterministic, our actions are still caused by prior causes ultimately beyond our own control, which few of those who identify themselves as libertarians seem to want to acknowledge. And lastly, the fact that our actions are caused by causes ultimately beyond our own control does, if we truly appreciated, undermine our intuition of retributive justice, an intuition that libertarians generally seem to want to defend intellectually. So, as many have pointed out already, libertarians are simply on a failed mission. Together with the want to defend retributive blame and punishment, what seems to be the main motivation for people who defend a libertarian notion of free will seems to be a fear of predeterminism, a fear of there being just one possible outcome from the present state of the universe, which would imply that we ultimately cannot do anything to cause a different outcome than the one possible. Libertarians and others with the same fear have artfully tried to make various models to help them overcome this fear, for instance so-called two-stage models that propose that our choices consist of an indeterministic stage of generation of possible actions, and then our non-random choice of one of them."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary advantage of using conditional postural synergies in the proposed force feedback control algorithm for a dexterous robotic hand?",
    "choices": [
      "A) It allows for more precise control over the grasp size, enabling the robot to manipulate objects with greater accuracy.",
      "B) It enables the robot to adapt to changing environmental conditions, such as varying object textures or temperatures, by adjusting the grasp postures accordingly.",
      "C) It facilitates the integration of sensory feedback from the fingertips, allowing the robot to refine its grasp postures in real-time based on tactile sensations.",
      "D) It enables the robot to learn from experience and improve its grasp postures over time through trial and error, without the need for explicit programming."
    ],
    "correct_answer": "C",
    "documentation": [
      "Paper Info\n\nTitle: Force Feedback Control For Dexterous Robotic Hands Using Conditional Postural Synergies\nPublish Date: Unkown\nAuthor List: Dimitrios Dimou, José Santos-Victor, Plinio Moreno\n\nFigure\n\nFig. 1.Example of modeling the contacts and friction during manipulation. Fig. 2. Schematic representation of the proposed force controller. The input is the state (GRASP or RELEASE) and the force readings. Based on that the grasp size is adjusted by a value C and is given to the posture mapping function along with the desired grasp type. A finger configuration is then generated and commanded to the robot. Fig. 3. Our control algorithm in Python-like pseudocode.\nFig. 4. Our first experiment. The robot picks up a bottle, transports it, and places down on the desk. In the bottom part of the figure, you can see the control signals during this task. Fig. 5.The household objects used in our experiments. Under the pictures of the execution you can see the signals recorded by the controller: the average normal force applied by all fingers (blue line), the thresholds f threshold high n .(purple dashed line) and f threshold low n.(yellow dashed line), the average tangential force (green), and the grasp size used in each time-step (red).The task is divided four stages: 1) (red part) the initial grasp of the object, in this stage the force controller closes the grasp until the applied normal\nFig.6.In the upper row of images, you can see our second experiment. The robot picks up the chips can, rotates it 90 degrees, and places back down. In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp. abstract\n\nWe present a force feedback controller for a dexterous robotic hand equipped with force sensors on its fingertips. Our controller uses the conditional postural synergies framework to generate the grasp postures, i.e. the finger configuration of the robot, at each time step based on forces measured on the robot's fingertips."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is a critical consideration for the Director when preparing the draft regional plan, as per Section 8 of the Act?",
    "choices": [
      "A) The Director should prioritize the allocation of land for residential purposes, as it is the most populous sector.",
      "B) The Director should ensure that the draft plan includes a comprehensive analysis of the region's natural resources, including forests and water bodies.",
      "C) The Director should allocate a minimum of 20% of the region's land for conservation and development of natural resources, as per Section 7 of the Act.",
      "D) The Director should prioritize the development of transport and communication facilities, as it is essential for the region's economic growth."
    ],
    "correct_answer": "C",
    "documentation": [
      "6. Survey. - (1) The Director shall, with a view to prepare the existing land use map, and other maps as are necessary for the purpose of regional plan,-\n(a) carry out such surveys as may be necessary;\n(b) obtain from any department of Government and any local authority such maps, survey reports and land records as may be necessary for the purpose. (2) It shall be the duty of every Government department and local authority to furnish, as soon as may be possible, maps, reports and record, as may be required by the Director. 7. Contents of regional plan. - The regional plan shall indicate the manner in which land in the region should be used, the phasing of development, the net work of communications and transport, the proposals for conservation and development of natural resources, and in particular-\n(a) allocation of land to such purposes as residential, industrial; agricultural or as forests or for mineral exploitation;\n(b) reservation of open spaces for recreational purposes, gardens, tree belts, and animal sanctuaries;\n(c) access or development of transport and communication facilities such as roads, railways, water ways, and the allocation and development of airports;\n(d) requirements and suggestions for development of public utilities such as water supply, drainage and electricity;\n(e) allocation of areas to be developed as \"Special Areas� wherein new towns, townships, large industrial estates or any other type of large development projects may be established;\n(f) landscaping and the preservation of areas in their natural state,\n(g) measures relating to the prevention of erosion, including rejuvenation of forest areas;\n(h) proposals relating to irrigation, water supply or flood control works. 8. Preparation of regional plan. - (1) After preparation of the existing land use map, the Director shall cause to be prepared a draft regional plan and publish it by making a copy thereof available for inspection and publishing a notice in such form and manner as may be prescribed inviting objections and suggestions from any person with respect to the draft plan before such date as may be specified in the notice, such date not being earlier than sixty days from the publication of the notice."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the demographics of McPherson County based on the information provided about the county's population and household characteristics?",
    "choices": [
      "A) The county has a high percentage of young families with children under the age of 18, given the median age of 38 years and the average household size of 2.49.",
      "B) The county has a significant proportion of elderly residents, with 17.30% of the population being 65 years of age or older, and a median age of 38 years.",
      "C) The county has a relatively low poverty rate, with only 4.20% of families and 6.60% of the population below the poverty line, considering the median income for a household and family.",
      "D) The county has a high percentage of males, given the ratio of males to females in the population and the median income for males being higher than for females."
    ],
    "correct_answer": "B",
    "documentation": [
      "25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older. The average household size was 2.49 and the average family size was 2.99. In the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 95.90 males. For every 100 females age 18 and over, there were 92.90 males. The median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921. About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over. Government\n\nPresidential elections\nMcPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson. Laws\nFollowing amendment to the Kansas Constitution in 1986, the county remained a prohibition, or \"dry\", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement. Education\n\nColleges\n McPherson College in McPherson\n Bethany College in Lindsborg\n Central Christian College in McPherson\n\nUnified school districts\n Smoky Valley USD 400\n McPherson USD 418\n Canton-Galva USD 419\n Moundridge USD 423\n Inman USD 448\n\nSchool district office in neighboring county\n Goessel USD 411\n Little River-Windom USD 444\n\nMuseums\n Birger Sandzén Memorial Gallery in Lindsborg\n McCormick-Deering Days Museum in Inman\n McPherson Museum in McPherson\n Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg\n Kansas Motorcycle Museum in Marquette\n\nCommunities\n\nCities\n\n Canton\n Galva\n Inman\n Lindsborg\n Marquette\n McPherson (county seat) \n Moundridge\n Windom\n\nUnincorporated communities\n† means a Census-Designated Place (CDP) by the United States Census Bureau."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What can be inferred about the future of the mobile industry based on the decreasing cost of SoCs and the economics of device production?",
    "choices": [
      "A) Intel will continue to dominate the market with high-end processors, as the cost of manufacturing cutting-edge SoCs will remain prohibitively expensive for them to compete with the likes of Qualcomm and Samsung.",
      "B) The decreasing cost of SoCs will lead to a shift towards more commoditized devices, where Intel will struggle to justify its involvement in the market due to the lack of profit margins.",
      "C) The mobile industry will see a resurgence of innovation, driven by the decreasing cost of SoCs and the increased competition among manufacturers, leading to a new wave of high-end devices with advanced features.",
      "D) The economics of device production will lead to a consolidation of the market, where only a few major players will remain, and the rest will be forced to exit the market due to the lack of profitability."
    ],
    "correct_answer": "B",
    "documentation": [
      "I am expecting a large war. Apple once again is dictating the performance in the mobile industry. Nice to see others being able to keep the pace, as well.\npaul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple evolutionary path by the SoC providers since then. Yeah, and most of the innovation in the automobile industry came about before Henry Ford came into the business. Doesn't change the fact that cars would probably have been an asterisk in the history books under \"toys for rich people\" if it weren't for him. The same applies to to mobile computing for Apple, Samsung, et al.\nSheldonRoss wrote:Lagrange wrote:The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each?The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "According to the NFPA algorithm, what is the primary advantage of setting up the LO equation in the described fashion?",
    "choices": [
      "A) It allows for the stiffness matrix to be inverted multiple times, leading to improved convergence.",
      "B) It enables the stiffness matrix to be inverted only once, just as with FPSA, resulting in a significant impact on the method's performance.",
      "C) It facilitates the use of a different scattering kernel for each iteration, leading to more accurate results.",
      "D) It allows for the use of a different parameter set for each iteration, resulting in improved stability."
    ],
    "correct_answer": "B",
    "documentation": [
      "Iteration is done until a convergence criterion is met. The main advantage of setting up the LO equation in this fashion is that the stiffness matrix for LO needs to be setup and inverted \\textit{only once}, just as with FPSA \\cite{JapanFPSA, japanDiss}. This has a large impact on the method's performance. A flowchart of this algorithm is shown in \\cref{Nalgorithm}.\n\n\\begin{figure}[H]\n\\centering\n\\begin{tikzpicture}[node distance = 3cm, auto]\n   \n    \\node [block] (init) {Initial guess of flux moments};\n    \\node [cloud_HO, right of=init, node distance=4cm] (HOm) {HO};\n    \\node [cloud_LO, below of=HOm, node distance=2cm] (LOm) {LO};\n    \\node [HO, below of=init] (transport) { One sweep in transport equation};\n    \\node [decision, below of=transport,node distance=4cm] (decide) {Flux moments converged?}; \\node [LO, left of=decide, node distance=4cm] (dterm) {Solve for consistency term}; \\node [LO, left of=dterm, node distance=3cm] (MFP) {Solve for FP angular flux}; \\node [LO, above of=MFP, node distance=4cm] (moments) {Convert angular flux to moments};\n    \\node [block, right of=decide, node distance=4cm] (stop) {Stop};\n   \n    \\path [line] (init) -- (transport);\n    \\path [line] (transport) -- (decide);\n    \\path [line] (decide) -- node {no} (dterm);\n    \\path [line] (dterm) -- (MFP);\n    \\path [line] (MFP) -- (moments);\n    \\path [line] (moments) -- (transport);\n    \\path [line] (decide) -- node {yes}(stop);\n\\end{tikzpicture}\n\\caption{NFPA algorithm}\n\\label{Nalgorithm}\n\\end{figure}\n\n\\section{Numerical Experiments}\\label{sec3} In \\cref{sec31} we describe the discretization methods used to implement the algorithms. In \\cref{sec32} we provide numerical results for 2 different choices of source $Q$ and boundary conditions. For each choice we solve the problem using 3 different scattering kernels, applying 3 different choices of parameters for each kernel. We provide NFPA numerical results for these 18 cases and compare them against those obtained from FPSA and other standard methods. All numerical experiments were performed using MATLAB."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "Analyzing the distribution of power-law exponents for the top 2000 and top 200 cryptocurrencies by market capitalization, what can be inferred about the typical scale of large price movements in the cryptocurrency market?",
    "choices": [
      "A) The variance of large price movements is finite for both positive and negative returns, with a median exponent of 2.78 for positive returns and 3.11 for negative returns, indicating a shift towards smaller values for positive returns.",
      "B) The power-law hypothesis is always rejected for the 9 cryptoassets with relatively small market capitalization, suggesting that these assets exhibit non-power-law behavior.",
      "C) The distribution of typical power-law exponents is shifted towards larger values for large negative returns, indicating that negative returns are more susceptible to large variations.",
      "D) The median exponent for Bitcoin is 4.50 for positive returns and 2.99 for negative returns, suggesting that the power-law hypothesis is well-supported for this asset."
    ],
    "correct_answer": "A",
    "documentation": [
      "Panels (c) and (d) show the distributions of these median exponents when considering the top 2000 and the top 200 cryptocurrencies by market capitalization, respectively. We observe that the distributions of α+ and α− tend to shift toward larger values when considering the largest cryptoassets. number of return observations (between 100 and 300 days) and filtering out cryptoassets with missing observations (Supplementary Figures ). Still, it is worth noticing the existence of a few cryptoassets (9 of them) with relatively small market capitalization (ranking below the top 1000) for which the power-law hypothesis is always rejected (Supplementary Table ). Having verified that large price movements in the cryptocurrency market are generally well-described by powerlaw distributions, we now focus on the power-law exponents that typically characterize each cryptoasset. To do so, we select all exponent estimates over the entire history of each digital asset for which the power-law hypothesis is not rejected and calculate their median values for both the positive ( α+ ) and negative ( α− ) returns. The dashed lines in Fig. ) show these median values for Bitcoin where α+ = 4.50 and α− = 2.99. It is worth noticing that the variance of large price movements σ 2 is finite only for α > 3, as the integral σ 2 ∼ ∞ r min r 2 p(r)dr diverges outside this interval. Thus, while the typical variance of large positive returns is finite for Bitcoin, negative returns are at the limit of not having a typical scale and are thus susceptible to much larger variations. Figure shows the probability distribution for the median power-law exponents of all cryptoassets grouped by large positive and negative returns. We note that the distribution of typical power-law exponents associated with large positive returns is shifted to smaller values when compared with the distribution of exponents related to large negative returns. The medians of these typical exponents are respectively 2.78 and 3.11 for positive and negative returns."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "A study found that fruit consumption in Amazonian riparian communities was associated with lower levels of mercury in the blood and hair of fish-eaters. However, another study in Spain found that cooking predatory fish increased mercury bioaccessibility, leading to higher mercury exposure. Which of the following conclusions can be drawn from these two studies?",
    "choices": [
      "A) Fruit consumption is the primary factor in reducing mercury exposure in fish-eating communities.",
      "B) Cooking predatory fish is the most effective way to reduce mercury exposure in fish-eaters.",
      "C) The relationship between fruit consumption and mercury exposure is dependent on the cooking method used.",
      "D) Fruit consumption and cooking methods for fish are both important factors in reducing mercury exposure in fish-eaters."
    ],
    "correct_answer": "D",
    "documentation": [
      "consumption influences mercury: Topics by WorldWideScience.org\nSample records for consumption influences mercury\nEpidemiologic confirmation that fruit consumption influences mercury exposure in riparian communities in the Brazilian Amazon\nSousa Passos, Carlos Jose; Mergler, Donna; Fillion, Myriam; Lemire, Melanie; Mertens, Frederic; Guimaraes, Jean Remy Davee; Philibert, Aline\nSince deforestation has recently been associated with increased mercury load in the Amazon, the problem of mercury exposure is now much more widespread than initially thought. A previous exploratory study suggested that fruit consumption may reduce mercury exposure. The objectives of the study were to determine the effects of fruit consumption on the relation between fish consumption and bioindicators of mercury (Hg) exposure in Amazonian fish-eating communities. A cross-sectional dietary survey based on a 7-day recall of fish and fruit consumption frequency was conducted within 13 riparian communities from the Tapajos River, Brazilian Amazon. Hair samples were collected from 449 persons, and blood samples were collected from a subset of 225, for total and inorganic mercury determination by atomic absorption spectrometry. On average, participants consumed 6.6 fish meals/week and ate 11 fruits/week. The average blood Hg (BHg) was 57.1Â±36.3 Î¼g/L (median: 55.1 Î¼g/L), and the average hair-Hg (HHg) was 16.8Â±10.3 Î¼g/g (median: 15.7 Î¼g/g). There was a positive relation between fish consumption and BHg (r=0.48; P 2 =36.0%) and HHg levels (fish: Î²=1.2, P 2 =21.0%). ANCOVA models showed that for the same number of fish meals, persons consuming fruits more frequently had significantly lower blood and HHg concentrations. For low fruit consumers, each fish meal contributed 9.8 Î¼g/ L Hg increase in blood compared to only 3.3 Î¼g/ L Hg increase for the high fruit consumers. In conclusion, fruit consumption may provide a protective effect for Hg exposure in Amazonian riparians. Prevention strategies that seek to maintain fish consumption while reducing Hg exposure in fish-eating communities should be pursued\nInfluence of mercury bioaccessibility on exposure assessment associated with consumption of cooked predatory fish in Spain."
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  },
  {
    "question": "What is the primary consequence of the Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance on the stability of the ultracold plasma?",
    "choices": [
      "A) The plasma's kinetic energy is directly proportional to the ion-Rydberg relative motion.",
      "B) The ambipolar expansion quenches the electron kinetic energy, leading to a stable plasma-high Rydberg equilibrium.",
      "C) The momentum matching between ions and Rydberg molecules effectively channels electron energy into the overall $\\pm x$ motion of gas volumes.",
      "D) The internal kinetic energy of the plasma is defined almost entirely by the ion-Rydberg relative motion, leading to a stable plasma-high Rydberg equilibrium."
    ],
    "correct_answer": "D",
    "documentation": [
      "}\n\\end{figure}\n\nFigure \\ref{fig:w2_spectra} shows a series of $\\omega_2$ late-signal excitation spectra for a set of initial densities. Here, we see a clear consequence of the higher-order dependence of Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - on $n_0$, the $\\omega_2$-selected Rydberg gas initial principal quantum number. This Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance appears necessary as a critical factor in achieving the long ultracold plasma lifetime required to produce this signal. We are progressing in theoretical work that explains the stability apparently conferred by this balance.  \n\n\n\\subsection{Bifurcation and arrested relaxation}\n\nAmbipolar expansion quenches electron kinetic energy as the initially formed plasma expands. Core ions follow electrons into the wings of the Rydberg gas. There, recurring charge exchange between NO$^+$ ions and NO$^*$ Rydberg molecules redistributes the ambipolar force of the expanding electron gas, equalizing ion and Rydberg velocities. This momentum matching effectively channels electron energy through ion motion into the overall $\\pm x$ motion of gas volumes in the laboratory. The internal kinetic energy of the plasma, which at this point is defined almost entirely by the ion-Rydberg relative motion, falls. Spatial correlation develops, and over a period of 500 ns, the system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure \\ref{fig:SFI}. \\begin{figure}[h!] \\centering\n\\includegraphics[width= .4 \\textwidth]{Bifurcation.pdf}\n   \\caption{$x,y$ detector images of ultracold plasma volumes produced by 2:1 aspect ratio ellipsoidal Rydberg gases with selected initial state, $40f(2)$ after a flight time of 402 $\\mu$s over a distance of 575 mm. Lower frame displays the distribution in $x$ of the charge integrated in $y$ and $z$.  Both images represent the unadjusted raw signal acquired in each case after 250 shots.  \n   }\n\\label{fig:bifurcation}\n\\end{figure}"
    ],
    "metadata": {
      "num_chunks_used": 1
    }
  }
]