{"0": {"documentation": {"title": "Control of planar nonlinear guided waves and spatial solitons with a\n  left-handed medium", "source": "Allan D. Boardman, Peter Egan, Larry Velasco, Neil King", "docs_id": "cond-mat/0603159", "section": ["cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Control of planar nonlinear guided waves and spatial solitons with a\n  left-handed medium. The evidence that double negative media, with an effective negative permittivity, and an effective negative permeability, can be manufactured to operate at frequencies ranging from microwave to optical is ushering in a new era of metamaterials. They are referred to here as 'left-handed', even though a variety of names is evident from the literature. In anticipation of a demand for highly structured integrated practical waveguides, this paper addresses the impact of this type of medium upon waveguides that can be also nonlinear. After an interesting historical overview and an exposure of some straightforward concepts, a planar guide is investigated, in which the waveguide is a slab consisting of a left-handed medium sandwiched between a substrate and cladding that are simple dielectrics. The substrate and cladding display a Kerr-type nonlinear response. Because of the nonlinear properties of the Kerr media, the power flow direction can be controlled by the intensity of the electric field. A comprehensive finite-difference-time-domain (FDTD) analysis is presented that concentrates upon spatial soliton behaviour. An interesting soliton-lens arrangement is investigated that lends itself to a novel cancellation effect."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a left-handed medium in a planar waveguide, as described in the paper, and how does it impact the behavior of nonlinear guided waves and spatial solitons?\n\n**A)** The left-handed medium allows for the creation of a waveguide with a negative refractive index, which enables the propagation of waves in the opposite direction of conventional waveguides. This property enables the control of power flow direction by the intensity of the electric field.\n\n**B)** The left-handed medium enables the creation of a waveguide with a negative permeability, which allows for the confinement of electromagnetic waves in a specific region. This property enables the creation of a waveguide with a high degree of spatial confinement.\n\n**C)** The left-handed medium enables the creation of a waveguide with a negative permittivity, which allows for the creation of a waveguide with a high degree of spatial confinement. This property enables the creation of a waveguide with a high degree of spatial confinement.\n\n**D)** The left-handed medium enables the creation of a waveguide with a negative refractive index, which allows for the creation of a waveguide with a high degree of spatial confinement. This property enables the creation of a waveguide with a high degree of spatial confinement.\n\n**Correct Answer:** A) The left-handed medium allows for the creation of a waveguide with a negative refractive index, which enables the propagation of waves in the opposite direction of conventional waveguides. This property enables the control of power flow direction by the intensity of the electric field.\n\n**Explanation:** The correct answer is A) because the left-handed medium is characterized by a negative refractive index, which enables the propagation of waves in the opposite direction of conventional waveguides. This property allows for the control of power flow direction by the intensity of the electric field, as described in the paper. The other options are incorrect because they do not accurately describe the primary advantage of using a left-handed medium in a planar waveguide. Option B is incorrect because the left-handed medium is characterized by a negative permeability, not a negative permittivity. Option C is incorrect because it is a repetition of option B. Option D is incorrect because it does not accurately describe the primary advantage of using a left-handed medium in a planar waveguide."}, "1": {"documentation": {"title": "Learning Character Strings via Mastermind Queries, with a Case Study\n  Involving mtDNA", "source": "Michael T. Goodrich", "docs_id": "0904.4458", "section": ["cs.DS", "cs.CR", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Character Strings via Mastermind Queries, with a Case Study\n  Involving mtDNA. We study the degree to which a character string, $Q$, leaks details about itself any time it engages in comparison protocols with a strings provided by a querier, Bob, even if those protocols are cryptographically guaranteed to produce no additional information other than the scores that assess the degree to which $Q$ matches strings offered by Bob. We show that such scenarios allow Bob to play variants of the game of Mastermind with $Q$ so as to learn the complete identity of $Q$. We show that there are a number of efficient implementations for Bob to employ in these Mastermind attacks, depending on knowledge he has about the structure of $Q$, which show how quickly he can determine $Q$. Indeed, we show that Bob can discover $Q$ using a number of rounds of test comparisons that is much smaller than the length of $Q$, under reasonable assumptions regarding the types of scores that are returned by the cryptographic protocols and whether he can use knowledge about the distribution that $Q$ comes from. We also provide the results of a case study we performed on a database of mitochondrial DNA, showing the vulnerability of existing real-world DNA data to the Mastermind attack."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A character string $Q$ is compared to a set of strings provided by a querier, Bob, using a cryptographic protocol that produces no additional information other than the scores assessing the degree of match. What is the primary concern for the security of $Q$ in this scenario, and how can Bob potentially exploit this vulnerability to learn the complete identity of $Q$?\n\n**A)** The primary concern is that the comparison protocol may leak information about the structure of $Q$, allowing Bob to deduce its identity through pattern recognition. Bob can exploit this by using a brute-force approach to test all possible strings.\n\n**B)** The primary concern is that the comparison protocol may reveal information about the distribution of $Q$ from which it was drawn, allowing Bob to infer its identity. Bob can exploit this by analyzing the scores returned by the protocol and using knowledge about the distribution to make educated guesses.\n\n**C)** The primary concern is that the comparison protocol may be vulnerable to a Mastermind-style attack, where Bob can use a set of test comparisons to deduce the complete identity of $Q$. Bob can exploit this by using a strategic approach to select the most informative test comparisons.\n\n**D)** The primary concern is that the comparison protocol may be susceptible to a side-channel attack, where Bob can exploit information about the computational resources required to compute the scores. Bob can exploit this by analyzing the timing or power consumption of the protocol.\n\n**Correct Answer:** C) The primary concern is that the comparison protocol may be vulnerable to a Mastermind-style attack, where Bob can use a strategic approach to select the most informative test comparisons.\n\n**Explanation:** The correct answer is C) because the documentation states that Bob can use a Mastermind-style attack to learn the complete identity of $Q$ by employing a strategic approach to select the most informative test comparisons. This is a key finding of the study, and the correct answer requires an understanding of the Mastermind attack and its implications for the security of $Q$. The other options are incorrect because they do not accurately reflect the primary concern for the security of $Q$ in this scenario."}, "2": {"documentation": {"title": "Optimal Mixed Discrete-Continuous Planning for Linear Hybrid Systems", "source": "Jingkai Chen, Brian Williams, Chuchu Fan", "docs_id": "2102.08261", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Mixed Discrete-Continuous Planning for Linear Hybrid Systems. Planning in hybrid systems with both discrete and continuous control variables is important for dealing with real-world applications such as extra-planetary exploration and multi-vehicle transportation systems. Meanwhile, generating high-quality solutions given certain hybrid planning specifications is crucial to building high-performance hybrid systems. However, since hybrid planning is challenging in general, most methods use greedy search that is guided by various heuristics, which is neither complete nor optimal and often falls into blind search towards an infinite-action plan. In this paper, we present a hybrid automaton planning formalism and propose an optimal approach that encodes this planning problem as a Mixed Integer Linear Program (MILP) by fixing the action number of automaton runs. We also show an extension of our approach for reasoning over temporally concurrent goals. By leveraging an efficient MILP optimizer, our method is able to generate provably optimal solutions for complex mixed discrete-continuous planning problems within a reasonable time. We use several case studies to demonstrate the extraordinary performance of our hybrid planning method and show that it outperforms a state-of-the-art hybrid planner, Scotty, in both efficiency and solution qualities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in hybrid planning, and how does the proposed approach in the paper \"Optimal Mixed Discrete-Continuous Planning for Linear Hybrid Systems\" address this challenge?\n\nA) Hybrid planning is challenging due to the complexity of the planning problem, and the proposed approach uses a heuristic search algorithm to find a solution.\nB) Hybrid planning is challenging because it involves both discrete and continuous control variables, and the proposed approach encodes this planning problem as a Mixed Integer Linear Program (MILP) to fix the action number of automaton runs.\nC) Hybrid planning is challenging due to the need for real-time decision-making, and the proposed approach uses a greedy search algorithm to find a solution quickly.\nD) Hybrid planning is challenging because it involves multiple agents with conflicting goals, and the proposed approach uses a game-theoretic approach to find a solution.\n\n**Correct Answer:** B) Hybrid planning is challenging because it involves both discrete and continuous control variables, and the proposed approach encodes this planning problem as a Mixed Integer Linear Program (MILP) to fix the action number of automaton runs.\n\n**Explanation:** The correct answer is B) because the paper specifically states that hybrid planning is challenging due to the presence of both discrete and continuous control variables, and that the proposed approach addresses this challenge by encoding the planning problem as a MILP. The other options are incorrect because they do not accurately reflect the main challenge in hybrid planning and the proposed approach's solution. Option A is incorrect because the paper does not mention using a heuristic search algorithm. Option C is incorrect because the paper does not mention real-time decision-making as a challenge. Option D is incorrect because the paper does not mention multiple agents with conflicting goals as a challenge."}, "3": {"documentation": {"title": "Superfluidity in the AdS/CFT Correspondence", "source": "Nick Evans and Michela Petrini", "docs_id": "hep-th/0108052", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Superfluidity in the AdS/CFT Correspondence. A chemical potential may be introduced into the AdS/CFT correspondence by setting the D3 branes of the construction spinning. In the field theory the fermionic modes are expected to condense as Cooper pairs, although at zero temperature the chemical potential destabilizes the scalar sector of the N=4 theory obscuring this phenomena. We show, in the case where a chemical potential is introduced for a small number of the gauge colours, that there is a metastable vacuum for the scalar fields where fermionic Cooper pairing is apparently manifest. In this vacuum the D3 branes expand non-commutatively (to balance the centrifugal force) into a D5 brane, in a mechanism analogous to Harmark and Savvidy's (M)atrix theory construction of a spinning D2 brane. We show that the D5 brane acts as a source for the RR 3-form whose UV scaling and symmetries are those of a fermion bilinear. The D5 brane rotates within the S^5 and so decays by the emission of RR fields which we interpret as the metastable vacuum decaying via higher dimension operators."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the AdS/CFT correspondence, consider a D3 brane spinning in the presence of a chemical potential. Describe the behavior of the scalar fields in the N=4 theory and the implications for the metastable vacuum.\n\nA) The scalar fields condense into Cooper pairs, but the chemical potential destabilizes the vacuum, preventing the formation of a metastable state.\n\nB) The scalar fields remain uncondensed, and the chemical potential only affects the fermionic modes, which form Cooper pairs in the absence of the chemical potential.\n\nC) The scalar fields condense into Cooper pairs, but the D3 brane expands non-commutatively into a D5 brane, which acts as a source for the RR 3-form, leading to a metastable vacuum.\n\nD) The scalar fields remain uncondensed, and the chemical potential only affects the D3 brane's motion, causing it to rotate within the S^5, leading to a decay via higher dimension operators.\n\nCorrect Answer: C) The scalar fields condense into Cooper pairs, but the D3 brane expands non-commutatively into a D5 brane, which acts as a source for the RR 3-form, leading to a metastable vacuum.\n\nExplanation: The correct answer is C) because the documentation states that in the metastable vacuum, the D3 branes expand non-commutatively into a D5 brane, which acts as a source for the RR 3-form. This is analogous to Harmark and Savvidy's (M)atrix theory construction of a spinning D2 brane. The scalar fields condense into Cooper pairs, but this is not the primary focus of the question. The correct answer requires an understanding of the AdS/CFT correspondence and the behavior of the D3 brane in the presence of a chemical potential."}, "4": {"documentation": {"title": "Mechanical control of the directional stepping dynamics of the kinesin\n  motor", "source": "Changbong Hyeon and Jos\\'e N. Onuchic", "docs_id": "0710.4558", "section": ["cond-mat.soft", "cond-mat.stat-mech", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mechanical control of the directional stepping dynamics of the kinesin\n  motor. Among the multiple steps constituting the kinesin's mechanochemical cycle, one of the most interesting events is observed when kinesins move an 8-nm step from one microtubule (MT)-binding site to another. The stepping motion that occurs within a relatively short time scale (~100 microsec) is, however, beyond the resolution of current experiments, therefore a basic understanding to the real-time dynamics within the 8-nm step is still lacking. For instance, the rate of power stroke (or conformational change), that leads to the undocked-to-docked transition of neck-linker, is not known, and the existence of a substep during the 8-nm step still remains a controversial issue in the kinesin community. By using explicit structures of the kinesin dimer and the MT consisting of 13 protofilaments (PFs), we study the stepping dynamics with varying rates of power stroke (kp). We estimate that 1/kp <~ 20 microsec to avoid a substep in an averaged time trace. For a slow power stroke with 1/kp>20 microsec, the averaged time trace shows a substep that implies the existence of a transient intermediate, which is reminiscent of a recent single molecule experiment at high resolution. We identify the intermediate as a conformation in which the tethered head is trapped in the sideway binding site of the neighboring PF. We also find a partial unfolding (cracking) of the binding motifs occurring at the transition state ensemble along the pathways prior to binding between the kinesin and MT."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the estimated time scale for the power stroke (or conformational change) in the kinesin mechanochemical cycle, and what implications does this have for the existence of a substep during the 8-nm step?\n\nA) The power stroke occurs within a time scale of 1/kp <~ 20 microsec, which suggests that a substep is unlikely to occur, and the intermediate state is a conformation where the tethered head is trapped in the sideway binding site of the neighboring PF.\n\nB) The power stroke occurs within a time scale of 1/kp > 20 microsec, which implies that a substep is likely to occur, and the intermediate state is a conformation where the binding motifs are partially unfolded.\n\nC) The power stroke occurs within a time scale of 1/kp <~ 20 microsec, which suggests that a substep is unlikely to occur, but the intermediate state is a conformation where the tethered head is not trapped in the sideway binding site of the neighboring PF.\n\nD) The power stroke occurs within a time scale of 1/kp > 20 microsec, which implies that a substep is likely to occur, but the intermediate state is a conformation where the binding motifs are not partially unfolded.\n\n**Correct Answer:** A) The power stroke occurs within a time scale of 1/kp <~ 20 microsec, which suggests that a substep is unlikely to occur, and the intermediate state is a conformation where the tethered head is trapped in the sideway binding site of the neighboring PF.\n\n**Explanation:**\n\nThe correct answer is A) because the text states that \"We estimate that 1/kp <~ 20 microsec to avoid a substep in an averaged time trace.\" This implies that a power stroke with a time scale of 1/kp <~ 20 microsec is unlikely to result in a substep, and the intermediate state is indeed a conformation where the tethered head is trapped in the sideway binding site of the neighboring PF.\n\nThe incorrect answers are B, C, and D because they either imply that a substep is likely to occur (B and D) or do not accurately describe the intermediate state (C)."}, "5": {"documentation": {"title": "Geometry-induced Casimir suspension of oblate bodies in fluids", "source": "Alejandro W. Rodriguez, M. T. Homer Reid, Francesco Intravaia,\n  Alexander Woolf, Diego A. R. Dalvit, Federico Capasso, Steven G. Johnson", "docs_id": "1310.4396", "section": ["cond-mat.other", "cond-mat.mtrl-sci", "physics.gen-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometry-induced Casimir suspension of oblate bodies in fluids. We predict that a low-permittivity oblate body (disk-shaped object) above a thin metal substrate (plate with a hole) immersed in a fluidof intermediate permittivity will experience a meta-stable equilibrium (restoring force) near the center of the hole. Stability is the result of a geometry-induced transition in the sign of the force, from repulsive to attractive, that occurs as the disk approaches the hole---in planar or nearly-planar geometries, the same material combination yields a repulsive force at all separations in accordance with the Dzyaloshinskii--Lifshitz--Pitaevskii condition of fluid-induced repulsion between planar bodies. We explore the stability of the system with respect to rotations and lateral translations of the disks, and demonstrate interesting transitions (bifurcations) in the rotational stability of the disks as a function of their size. Finally, we consider the reciprocal situation in which the disk--plate materials are interchanged, and find that in this case the system also exhibits meta-stability. The forces in the system are sufficiently large to be observed in experiments and should enable measurements based on the diffusion dynamics of the suspended bodies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism behind the meta-stable equilibrium of an oblate body (disk-shaped object) above a thin metal substrate (plate with a hole) immersed in a fluid of intermediate permittivity?\n\nA) The Dzyaloshinskii-Lifshitz-Pitaevskii condition of fluid-induced repulsion between planar bodies\nB) The geometry-induced transition in the sign of the force, from repulsive to attractive, as the disk approaches the hole\nC) The diffusion dynamics of the suspended bodies\nD) The material properties of the disk and plate\n\n**Correct Answer:** B) The geometry-induced transition in the sign of the force, from repulsive to attractive, as the disk approaches the hole\n\n**Explanation:** The correct answer is B) The geometry-induced transition in the sign of the force, from repulsive to attractive, as the disk approaches the hole. This is because the documentation states that the system exhibits a meta-stable equilibrium near the center of the hole, which is a result of a geometry-induced transition in the sign of the force. This transition occurs as the disk approaches the hole, and it is this transition that gives rise to the meta-stable equilibrium.\n\n**Candidate A (Dzyaloshinskii-Lifshitz-Pitaevskii condition)** is incorrect because while the Dzyaloshinskii-Lifshitz-Pitaevskii condition is mentioned in the documentation as a condition for fluid-induced repulsion between planar bodies, it is not the primary mechanism behind the meta-stable equilibrium of the oblate body above the thin metal substrate.\n\n**Candidate C (diffusion dynamics of the suspended bodies)** is incorrect because while the documentation mentions that the forces in the system are sufficiently large to be observed in experiments and should enable measurements based on the diffusion dynamics of the suspended bodies, this is not the primary mechanism behind the meta-stable equilibrium.\n\n**Candidate D (material properties of the disk and plate)** is incorrect because while the material properties of the disk and plate are mentioned in the documentation, they are not the primary mechanism behind the meta-stable equilibrium. The meta-stable equilibrium is a result of the geometry-induced transition in the sign of the force, not the material properties of the disk and plate."}, "6": {"documentation": {"title": "Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms:\n  A Case with Bounded Regret", "source": "A. \\\"Omer Sar{\\i}ta\\c{c} and Cem Tekin", "docs_id": "1707.07443", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms:\n  A Case with Bounded Regret. In this paper, we study the combinatorial multi-armed bandit problem (CMAB) with probabilistically triggered arms (PTAs). Under the assumption that the arm triggering probabilities (ATPs) are positive for all arms, we prove that a class of upper confidence bound (UCB) policies, named Combinatorial UCB with exploration rate $\\kappa$ (CUCB-$\\kappa$), and Combinatorial Thompson Sampling (CTS), which estimates the expected states of the arms via Thompson sampling, achieve bounded regret. In addition, we prove that CUCB-$0$ and CTS incur $O(\\sqrt{T})$ gap-independent regret. These results improve the results in previous works, which show $O(\\log T)$ gap-dependent and $O(\\sqrt{T\\log T})$ gap-independent regrets, respectively, under no assumptions on the ATPs. Then, we numerically evaluate the performance of CUCB-$\\kappa$ and CTS in a real-world movie recommendation problem, where the actions correspond to recommending a set of movies, the arms correspond to the edges between the movies and the users, and the goal is to maximize the total number of users that are attracted by at least one movie. Our numerical results complement our theoretical findings on bounded regret. Apart from this problem, our results also directly apply to the online influence maximization (OIM) problem studied in numerous prior works."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms: A Case with Bounded Regret, what is the theoretical guarantee on the regret incurred by the Combinatorial UCB with exploration rate $\\kappa$ (CUCB-$\\kappa$) policy, and how does it compare to the regret incurred by the Combinatorial Thompson Sampling (CTS) policy?\n\n**A)** CUCB-$\\kappa$ incurs $O(\\log T)$ gap-dependent regret, while CTS incurs $O(\\sqrt{T\\log T})$ gap-independent regret.\n\n**B)** CUCB-$\\kappa$ incurs $O(\\sqrt{T})$ gap-independent regret, while CTS incurs $O(\\log T)$ gap-dependent regret.\n\n**C)** CUCB-$\\kappa$ incurs $O(\\log T)$ gap-dependent regret, while CTS incurs $O(\\sqrt{T})$ gap-independent regret.\n\n**D)** CUCB-$\\kappa$ incurs $O(\\sqrt{T\\log T})$ gap-independent regret, while CTS incurs $O(\\log T)$ gap-dependent regret.\n\n**Correct Answer:** B) CUCB-$\\kappa$ incurs $O(\\sqrt{T})$ gap-independent regret, while CTS incurs $O(\\log T)$ gap-dependent regret.\n\n**Explanation:** According to the documentation, CUCB-$\\kappa$ and CTS achieve bounded regret, and CUCB-$0$ and CTS incur $O(\\sqrt{T})$ gap-independent regret. This matches option B, which states that CUCB-$\\kappa$ incurs $O(\\sqrt{T})$ gap-independent regret, while CTS incurs $O(\\log T)$ gap-dependent regret."}, "7": {"documentation": {"title": "Estimating Experimental Dispersion Curves from Steady-State Frequency\n  Response Measurements", "source": "V. V. N. Sriram Malladi, Mohammad I. Albakri, Manu Krishnan, Serkan\n  Gugercin, Pablo A. Tarazaga", "docs_id": "2101.00155", "section": ["physics.data-an", "cond-mat.mtrl-sci", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating Experimental Dispersion Curves from Steady-State Frequency\n  Response Measurements. Dispersion curves characterize the frequency dependence of the phase and the group velocities of propagating elastic waves. Many analytical and numerical techniques produce dispersion curves from physics-based models. However, it is often challenging to accurately model engineering structures with intricate geometric features and inhomogeneous material properties. For such cases, this paper proposes a novel method to estimate group velocities from experimental data-driven models. Experimental frequency response functions (FRFs) are used to develop data-driven models, {which are then used to estimate dispersion curves}. The advantages of this approach over other traditionally used transient techniques stem from the need to conduct only steady-state experiments. In comparison, transient experiments often need a higher-sampling rate for wave-propagation applications and are more susceptible to noise. The vector-fitting (VF) algorithm is adopted to develop data-driven models from experimental in-plane and out-of-plane FRFs of a one-dimensional structure. The quality of the corresponding data-driven estimates is evaluated using an analytical Timoshenko beam as a baseline. The data-driven model (using the out-of-plane FRFs) estimates the anti-symmetric ($A_0$) group velocity with a maximum error of $4\\%$ over a 40~kHz frequency band. In contrast, group velocities estimated from transient experiments resulted in a maximum error of $6\\%$ over the same frequency band."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A researcher is interested in estimating the dispersion curves of a one-dimensional structure with intricate geometric features and inhomogeneous material properties. Which of the following methods is proposed in the paper \"Estimating Experimental Dispersion Curves from Steady-State Frequency Response Measurements\" to estimate group velocities from experimental data-driven models?\n\nA) Vector-fitting (VF) algorithm using in-plane FRFs\nB) Vector-fitting (VF) algorithm using out-of-plane FRFs\nC) Transient wave-propagation techniques with high-sampling rates\nD) Analytical Timoshenko beam models with steady-state frequency response functions\n\n**Correct Answer:** B) Vector-fitting (VF) algorithm using out-of-plane FRFs\n\n**Explanation:** The paper proposes using the vector-fitting (VF) algorithm to develop data-driven models from experimental in-plane and out-of-plane FRFs of a one-dimensional structure. The out-of-plane FRFs are specifically mentioned as being used to estimate the anti-symmetric ($A_0$) group velocity. This method is advantageous over other traditionally used transient techniques because it requires only steady-state experiments, which are less susceptible to noise and require a lower sampling rate."}, "8": {"documentation": {"title": "The anomalous tango of hemocyte migration in Drosophila melanogaster\n  embryos", "source": "Nickolay Korabel, Giuliana D. Clemente, Daniel Han, Felix Feldman, Tom\n  H. Millard and Thomas Andrew Waigh", "docs_id": "2109.03797", "section": ["physics.bio-ph", "cond-mat.stat-mech", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The anomalous tango of hemocyte migration in Drosophila melanogaster\n  embryos. Drosophila melanogaster hemocytes are highly motile cells that are crucial for successful embryogenesis and have important roles in the organism's immunological response. Hemocyte motion was measured using selective plane illumination microscopy. Every hemocyte cell in one half of an embryo was tracked during embryogenesis and analysed using a deep learning neural network. The anomalous transport of the cells was well described by fractional Brownian motion that was heterogeneous in both time and space. Hemocyte motion became less persistent over time. LanB1 and SCAR mutants disrupted the collective cellular motion and reduced its persistence due to the modification of viscoelasticity and actin-based motility respectively. The anomalous motility of the hemocytes oscillated in time with alternating epoques of varying persistent motion. Touching hemocytes experience synchronised contact inhibition of locomotion; an anomalous tango. A quantitative statistical framework is presented for hemocyte motility which provides new biological insights."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism underlying the anomalous tango of hemocyte migration in Drosophila melanogaster embryos, as described in the study?\n\nA) Fractional Brownian motion with heterogeneous time and space properties\nB) Collective cellular motion disrupted by LanB1 and SCAR mutants\nC) Synchronized contact inhibition of locomotion due to hemocyte touching\nD) Viscoelasticity and actin-based motility modified by LanB1 and SCAR mutants, respectively\n\n**Correct Answer:** A) Fractional Brownian motion with heterogeneous time and space properties\n\n**Explanation:**\n\nThe study describes the anomalous tango of hemocyte migration in Drosophila melanogaster embryos as being well-described by fractional Brownian motion, which is a type of anomalous diffusion process. This process is heterogeneous in both time and space, meaning that the motion of the hemocytes is not only random but also exhibits long-range correlations and non-Gaussian distributions. This is in contrast to traditional Brownian motion, which is homogeneous and Gaussian.\n\nThe other options are incorrect because:\n\n* B) Collective cellular motion is disrupted by LanB1 and SCAR mutants, but this is not the primary mechanism underlying the anomalous tango.\n* C) Synchronized contact inhibition of locomotion due to hemocyte touching is a phenomenon observed in the study, but it is not the primary mechanism underlying the anomalous tango.\n* D) Viscoelasticity and actin-based motility are modified by LanB1 and SCAR mutants, respectively, but this is not the primary mechanism underlying the anomalous tango.\n\nThis question requires the test-taker to have a good understanding of the study's findings and to be able to distinguish between different mechanisms underlying the anomalous tango of hemocyte migration. It also requires the test-taker to be able to analyze the information provided and to identify the correct answer based on the evidence presented in the study."}, "9": {"documentation": {"title": "Critical point in the phase diagram of primordial quark-gluon matter\n  from black hole physics", "source": "Renato Critelli (1), Jorge Noronha (1), Jacquelyn Noronha-Hostler\n  (2,3), Israel Portillo (3), Claudia Ratti (3), Romulo Rougemont (4) ((1) Sao\n  Paulo U., (2) Rutgers U., (3) Houston U., (4) IIP, Brazil)", "docs_id": "1706.00455", "section": ["nucl-th", "hep-lat", "hep-ph", "hep-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Critical point in the phase diagram of primordial quark-gluon matter\n  from black hole physics. Strongly interacting matter undergoes a crossover phase transition at high temperatures $T\\sim 10^{12}$ K and zero net-baryon density. A fundamental question in the theory of strong interactions, Quantum Chromodynamics (QCD), is whether a hot and dense system of quarks and gluons displays critical phenomena when doped with more quarks than antiquarks, where net-baryon number fluctuations diverge. Recent lattice QCD work indicates that such a critical point can only occur in the baryon dense regime of the theory, which defies a description from first principles calculations. Here we use the holographic gauge/gravity correspondence to map the fluctuations of baryon charge in the dense quark-gluon liquid onto a numerically tractable gravitational problem involving the charge fluctuations of holographic black holes. This approach quantitatively reproduces ab initio results for the lowest order moments of the baryon fluctuations and makes predictions for the higher order baryon susceptibilities and also for the location of the critical point, which is found to be within the reach of heavy ion collision experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of using the holographic gauge/gravity correspondence to study the phase transition in strongly interacting matter, and what are the implications of this approach for our understanding of the critical point in the phase diagram?\n\nA) To calculate the entropy of the system and determine the temperature at which the phase transition occurs, which is crucial for understanding the behavior of quarks and gluons in high-energy collisions.\nB) To map the fluctuations of baryon charge in the dense quark-gluon liquid onto a numerically tractable gravitational problem, which allows for the prediction of the critical point and the study of higher-order baryon susceptibilities.\nC) To investigate the role of quantum gravity in the early universe and the behavior of black holes in the presence of matter, which is relevant for understanding the origins of the universe.\nD) To develop a new theoretical framework for describing the behavior of strongly interacting matter, which is based on the principles of quantum field theory and general relativity.\n\nCorrect Answer: B) To map the fluctuations of baryon charge in the dense quark-gluon liquid onto a numerically tractable gravitational problem, which allows for the prediction of the critical point and the study of higher-order baryon susceptibilities.\n\nExplanation: The correct answer is B) because the holographic gauge/gravity correspondence is used to map the fluctuations of baryon charge in the dense quark-gluon liquid onto a gravitational problem, which allows for the prediction of the critical point and the study of higher-order baryon susceptibilities. This approach is described in the provided documentation as a way to \"quantitatively reproduce ab initio results for the lowest order moments of the baryon fluctuations\" and to make predictions for the higher-order baryon susceptibilities. The other options are not directly related to the primary goal of using the holographic gauge/gravity correspondence to study the phase transition in strongly interacting matter."}, "10": {"documentation": {"title": "Vacuum Instability in Electric Fields via AdS/CFT: Euler-Heisenberg\n  Lagrangian and Planckian Thermalization", "source": "Koji Hashimoto and Takashi Oka", "docs_id": "1307.7423", "section": ["hep-th", "cond-mat.str-el", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vacuum Instability in Electric Fields via AdS/CFT: Euler-Heisenberg\n  Lagrangian and Planckian Thermalization. We analyze vacuum instability of strongly coupled gauge theories in a constant electric field using AdS/CFT correspondence. The model is the N=2 1-flavor supersymmetric large N_c QCD in the strong 't Hooft coupling limit. We calculate the Euler-Heisenberg effective Lagrangian L(E), which encodes the nonlinear response and the quantum decay rate of the vacuum in a background electric field E, from the complex D-brane action in AdS/CFT. We find that the decay rate given by Im L(E) becomes nonzero above a critical electric field set by the confining force between quarks. A large-E expansion of Im L(E) is found to coincide with that of the Schwinger effects in QED, replacing its electron mass by the confining force. Then, the time-dependent response of the system in a strong electric field is solved non-perturbatively, and we observe a universal thermalization at a shortest timescale \"Planckian thermalization time\" t ~ 1/T ~ E^{-1/2}. Here, T is an effective temperature which quarks feel in the nonequilibrium state with nonzero electric current, calculated in AdS/CFT as a Hawking temperature. Stronger electric fields accelerate the thermalization, and for a realistic value of the electric field in RHIC experiment, we obtain t ~ 1 [fm/c], which is consistent with the believed value."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the AdS/CFT correspondence, what is the relationship between the Planckian thermalization time and the electric field strength, and how does it compare to the thermalization time in the Schwinger effects in QED?\n\nA) The Planckian thermalization time is inversely proportional to the square of the electric field strength, and it is identical to the thermalization time in the Schwinger effects in QED.\n\nB) The Planckian thermalization time is inversely proportional to the electric field strength, and it is proportional to the square of the electric field strength in the Schwinger effects in QED.\n\nC) The Planckian thermalization time is inversely proportional to the electric field strength, and it is identical to the thermalization time in the Schwinger effects in QED, but with a different constant of proportionality.\n\nD) The Planckian thermalization time is directly proportional to the electric field strength, and it is inversely proportional to the square of the electric field strength in the Schwinger effects in QED.\n\nCorrect Answer: C) The Planckian thermalization time is inversely proportional to the electric field strength, and it is identical to the thermalization time in the Schwinger effects in QED, but with a different constant of proportionality.\n\nExplanation: The correct answer is C) because the Planckian thermalization time is given by t ~ 1/T ~ E^{-1/2}, which shows that it is inversely proportional to the electric field strength. Additionally, the thermalization time in the Schwinger effects in QED is given by t ~ 1/(eE), which is also inversely proportional to the electric field strength. However, the constant of proportionality is different, as the Planckian thermalization time has a factor of 1/E^{-1/2}, whereas the thermalization time in the Schwinger effects has a factor of 1/eE."}, "11": {"documentation": {"title": "Machine-Learning Tests for Effects on Multiple Outcomes", "source": "Jens Ludwig, Sendhil Mullainathan, Jann Spiess", "docs_id": "1707.01473", "section": ["stat.ML", "econ.EM", "stat.AP", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Machine-Learning Tests for Effects on Multiple Outcomes. In this paper we present tools for applied researchers that re-purpose off-the-shelf methods from the computer-science field of machine learning to create a \"discovery engine\" for data from randomized controlled trials (RCTs). The applied problem we seek to solve is that economists invest vast resources into carrying out RCTs, including the collection of a rich set of candidate outcome measures. But given concerns about inference in the presence of multiple testing, economists usually wind up exploring just a small subset of the hypotheses that the available data could be used to test. This prevents us from extracting as much information as possible from each RCT, which in turn impairs our ability to develop new theories or strengthen the design of policy interventions. Our proposed solution combines the basic intuition of reverse regression, where the dependent variable of interest now becomes treatment assignment itself, with methods from machine learning that use the data themselves to flexibly identify whether there is any function of the outcomes that predicts (or has signal about) treatment group status. This leads to correctly-sized tests with appropriate $p$-values, which also have the important virtue of being easy to implement in practice. One open challenge that remains with our work is how to meaningfully interpret the signal that these methods find."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Machine-Learning Tests for Effects on Multiple Outcomes\", what is the primary challenge that economists face when conducting randomized controlled trials (RCTs), and how does the proposed solution address this challenge?\n\n**A)** Economists face the challenge of collecting too much data, which leads to overfitting and inaccurate results. The proposed solution addresses this challenge by using machine learning methods to identify the most relevant outcome measures.\n\n**B)** Economists face the challenge of inference in the presence of multiple testing, which leads to incorrect p-values and failed to detect significant effects. The proposed solution addresses this challenge by combining reverse regression with machine learning methods to flexibly identify the relationships between treatment assignment and outcome measures.\n\n**C)** Economists face the challenge of interpreting the results of RCTs, which leads to difficulties in generalizing findings to real-world settings. The proposed solution addresses this challenge by providing a framework for meaningfully interpreting the signal that machine learning methods find.\n\n**D)** Economists face the challenge of designing effective policy interventions, which leads to difficulties in identifying the most relevant outcome measures. The proposed solution addresses this challenge by providing a discovery engine for data from RCTs.\n\n**Correct Answer:** B) Economists face the challenge of inference in the presence of multiple testing, which leads to incorrect p-values and failed to detect significant effects. The proposed solution addresses this challenge by combining reverse regression with machine learning methods to flexibly identify the relationships between treatment assignment and outcome measures.\n\n**Explanation:** The correct answer is B) because the paper specifically mentions that economists usually wind up exploring just a small subset of the hypotheses that the available data could be used to test due to concerns about inference in the presence of multiple testing. The proposed solution addresses this challenge by using machine learning methods to flexibly identify the relationships between treatment assignment and outcome measures, leading to correctly-sized tests with appropriate p-values."}, "12": {"documentation": {"title": "Quasifree Kaon Photoproduction on Nuclei", "source": "F.X. Lee, T. Mart, C. Bennhold, H. Haberzettl, L.E. Wright", "docs_id": "nucl-th/9907119", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quasifree Kaon Photoproduction on Nuclei. Investigations of the quasifree reaction A$(\\gamma, K Y)$B are presented in the distorted wave impulse approximation (DWIA). For this purpose, we present a revised tree-level model of elementary kaon photoproduction that incorporates hadronic form factors consistent with gauge invariance, uses SU(3) values for the Born couplings and uses resonances consistent with multi-channel analyses. The potential of exclusive quasifree kaon photoproduction on nuclei to reveal details of the hyperon-nucleus interaction is examined. Detailed predictions for the coincidence cross section, the photon asymmetry, and the hyperon polarization and their sensitivities to the ingredients of the model are obtained for all six production channels. Under selected kinematics these observables are found to be sensitive to the hyperon-nucleus final state interaction. Some polarization observables are found to be insensitive to distortion effects, making them ideal tools to search for possible medium modifications of the elementary amplitude."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the revised tree-level model of elementary kaon photoproduction in the distorted wave impulse approximation (DWIA) for investigating quasifree kaon photoproduction on nuclei?\n\nA) It provides a more accurate description of the hyperon-nucleus interaction\nB) It allows for the incorporation of hadronic form factors consistent with gauge invariance\nC) It enables the examination of the potential of exclusive quasifree kaon photoproduction to reveal details of the hyperon-nucleus interaction\nD) It facilitates the calculation of coincidence cross sections and photon asymmetry\n\nCorrect Answer: B) It allows for the incorporation of hadronic form factors consistent with gauge invariance\n\nExplanation: The revised tree-level model of elementary kaon photoproduction incorporates hadronic form factors consistent with gauge invariance, which is a key aspect of the model. This allows for a more accurate description of the kaon photoproduction process and enables the examination of the potential of exclusive quasifree kaon photoproduction to reveal details of the hyperon-nucleus interaction. The other options are not the primary advantage of the model, although they may be related to its use in DWIA."}, "13": {"documentation": {"title": "American Hate Crime Trends Prediction with Event Extraction", "source": "Songqiao Han, Hailiang Huang, Jiangwei Liu, Shengsheng Xiao", "docs_id": "2111.04951", "section": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "American Hate Crime Trends Prediction with Event Extraction. Social media platforms may provide potential space for discourses that contain hate speech, and even worse, can act as a propagation mechanism for hate crimes. The FBI's Uniform Crime Reporting (UCR) Program collects hate crime data and releases statistic report yearly. These statistics provide information in determining national hate crime trends. The statistics can also provide valuable holistic and strategic insight for law enforcement agencies or justify lawmakers for specific legislation. However, the reports are mostly released next year and lag behind many immediate needs. Recent research mainly focuses on hate speech detection in social media text or empirical studies on the impact of a confirmed crime. This paper proposes a framework that first utilizes text mining techniques to extract hate crime events from New York Times news, then uses the results to facilitate predicting American national-level and state-level hate crime trends. Experimental results show that our method can significantly enhance the prediction performance compared with time series or regression methods without event-related factors. Our framework broadens the methods of national-level and state-level hate crime trends prediction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the FBI's Uniform Crime Reporting (UCR) Program in predicting American national-level and state-level hate crime trends, and how does the proposed framework in the paper address this limitation?\n\n**A)** The UCR Program's reports are released yearly, which can lead to delayed analysis and decision-making. The proposed framework addresses this limitation by utilizing text mining techniques to extract hate crime events from New York Times news, allowing for more timely and accurate predictions.\n\n**B)** The UCR Program's reports are limited to collecting data on confirmed hate crimes, which can lead to underreporting of hate crimes. The proposed framework addresses this limitation by incorporating empirical studies on the impact of confirmed crimes, providing a more comprehensive understanding of hate crime trends.\n\n**C)** The UCR Program's reports are not publicly available, which can limit the ability of law enforcement agencies and lawmakers to access valuable data. The proposed framework addresses this limitation by providing a publicly available framework for predicting hate crime trends.\n\n**D)** The UCR Program's reports are not comprehensive, as they only collect data on hate crimes reported to the police. The proposed framework addresses this limitation by utilizing text mining techniques to extract hate crime events from New York Times news, providing a more holistic understanding of hate crime trends.\n\n**Correct Answer:** A) The UCR Program's reports are released yearly, which can lead to delayed analysis and decision-making. The proposed framework addresses this limitation by utilizing text mining techniques to extract hate crime events from New York Times news, allowing for more timely and accurate predictions.\n\n**Explanation:** The correct answer is A) because the paper highlights the limitation of the UCR Program's reports being released yearly, which can lead to delayed analysis and decision-making. The proposed framework addresses this limitation by utilizing text mining techniques to extract hate crime events from New York Times news, allowing for more timely and accurate predictions. The other options are incorrect because they do not accurately reflect the limitation of the UCR Program's reports and the proposed framework's address to this limitation."}, "14": {"documentation": {"title": "Surface Terms of Quartic Quasitopological Gravity and Thermodynamics of\n  Nonlinear Charged Rotating Black Branes", "source": "A. Bazrafshan, M. H. Dehghani and M. Ghanaatian", "docs_id": "1209.0246", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Surface Terms of Quartic Quasitopological Gravity and Thermodynamics of\n  Nonlinear Charged Rotating Black Branes. As in the case of Einstein or Lovelock gravity, the action of quartic quasitopological gravity has not a well-defined variational principle. In this paper, we first introduce a surface term that makes the variation of quartic quasitopological gravity well defined. Second, we present the static charged solutions of quartic quasitopological gravity in the presence of a non linear electromagnetic field. One of the branch of these solutions presents a black brane with one or two horizons or a naked singularity depending on the charge and mass of the solution. The thermodynamic of these black branes are investigated through the use of the Gibbs free energy. In order to do this, we calculate the finite action by use of the counterterm method inspired by AdS/CFT correspondence. Introducing a Smarr-type formula, we also show that the conserved and thermodynamics quantities of these solutions satisfy the first law of thermodynamics. Finally, we present the charged rotating black branes in $(n+1)$ dimensions with $k\\leq [n/2]$ rotation parameters and investigate their thermodynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of quartic quasitopological gravity, what is the physical significance of the surface term introduced to make the variation of the action well-defined, and how does it relate to the thermodynamic properties of the resulting black branes?\n\nA) The surface term is a regularization technique to avoid divergences in the action, and it does not affect the thermodynamic properties of the black branes. The thermodynamic properties are determined solely by the finite action calculated using the counterterm method.\n\nB) The surface term is a source of additional energy that affects the thermodynamic properties of the black branes, and it must be taken into account when calculating the Gibbs free energy. The finite action calculated using the counterterm method is a measure of the total energy of the black branes.\n\nC) The surface term is a manifestation of the non-linear electromagnetic field, and it is responsible for the formation of the black branes. The thermodynamic properties of the black branes are determined by the surface term, which is a measure of the energy density of the electromagnetic field.\n\nD) The surface term is a Smarr-type formula that relates the conserved and thermodynamic quantities of the black branes to the finite action calculated using the counterterm method. The surface term is a measure of the entropy of the black branes, and it is used to calculate the Gibbs free energy.\n\nCorrect Answer: D) The surface term is a Smarr-type formula that relates the conserved and thermodynamic quantities of the black branes to the finite action calculated using the counterterm method. The surface term is a measure of the entropy of the black branes, and it is used to calculate the Gibbs free energy.\n\nExplanation: The surface term introduced in the paper is a Smarr-type formula that relates the conserved and thermodynamic quantities of the black branes to the finite action calculated using the counterterm method. This formula is used to calculate the Gibbs free energy of the black branes, which is a measure of their thermodynamic properties. The correct answer is D) because it accurately describes the physical significance of the surface term and its relation to the thermodynamic properties of the black branes."}, "15": {"documentation": {"title": "Cities in a world of diminishing transport costs", "source": "Tomoya Mori, Minoru Osawa", "docs_id": "2012.12503", "section": ["econ.GN", "nlin.PS", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cities in a world of diminishing transport costs. Economic activities favor mutual geographical proximity and concentrate spatially to form cities. In a world of diminishing transport costs, however, the advantage of physical proximity is fading, and the role of cities in the economy may be declining. To provide insights into the long-run evolution of cities, we analyzed Japan's census data over the 1970--2015 period. We found that fewer and larger cities thrived at the national scale, suggesting an eventual mono-centric economy with a single megacity; simultaneously, each larger city flattened out at the local scale, suggesting an eventual extinction of cities. We interpret this multi-scale phenomenon as an instance of pattern formation by self-organization, which is widely studied in mathematics and biology. However, cities' dynamics are distinct from mathematical or biological mechanisms because they are governed by economic interactions mediated by transport costs between locations. Our results call for the synthesis of knowledge in mathematics, biology, and economics to open the door for a general pattern formation theory that is applicable to socioeconomic phenomena."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nCities in a world of diminishing transport costs. Economic activities favor mutual geographical proximity and concentrate spatially to form cities. In a world of diminishing transport costs, however, the advantage of physical proximity is fading, and the role of cities in the economy may be declining. To provide insights into the long-run evolution of cities, we analyzed Japan's census data over the 1970--2015 period. We found that fewer and larger cities thrived at the national scale, suggesting an eventual mono-centric economy with a single megacity; simultaneously, each larger city flattened out at the local scale, suggesting an eventual extinction of cities. We interpret this multi-scale phenomenon as an instance of pattern formation by self-organization, which is widely studied in mathematics and biology. However, cities' dynamics are distinct from mathematical or biological mechanisms because they are governed by economic interactions mediated by transport costs between locations. Our results call for the synthesis of knowledge in mathematics, biology, and economics to open the door for a general pattern formation theory that is applicable to socioeconomic phenomena.\n\nWhat is the primary implication of the study on the long-run evolution of cities, and how does it relate to the concept of pattern formation by self-organization?\n\nA) The study suggests that cities will continue to thrive and expand indefinitely, leading to a poly-centric economy with multiple megacities. This is an instance of pattern formation by self-organization, where cities' dynamics are driven by economic interactions mediated by transport costs.\n\nB) The study indicates that cities will eventually become extinct due to the decline of physical proximity and the rise of mono-centric economies. This is an instance of pattern formation by self-organization, where cities' dynamics are driven by economic interactions mediated by transport costs.\n\nC) The study reveals that the role of cities in the economy is already declining, and this decline will continue indefinitely, leading to a mono-centric economy with a single megacity. This is an instance of pattern formation by self-organization, where cities' dynamics are driven by economic interactions mediated by transport costs.\n\nD) The study suggests that the advantage of physical proximity is still a dominant factor in the economy, and cities will continue to thrive and expand indefinitely, leading to a poly-centric economy with multiple megacities. This is not an instance of pattern formation by self-organization, as the dynamics of cities are driven by economic interactions mediated by transport costs.\n\nCorrect Answer: C) The study reveals that the role of cities in the economy is already declining, and this decline will continue indefinitely, leading to a mono-centric economy with a single megacity. This is an instance of pattern formation by self-organization, where cities' dynamics are driven by economic interactions mediated by transport costs.\n\nExplanation: The correct answer is C) because the study found that fewer and larger cities thrived at the national scale, suggesting an eventual mono-centric economy with a single megacity, and each larger city flattened out at the local scale, suggesting an eventual extinction of cities. This multi-scale phenomenon is an instance of pattern formation by self-organization, where cities' dynamics are driven by economic interactions mediated by transport costs. The other options are incorrect because they do not accurately reflect the findings of the study. Option A is incorrect because the study suggests that cities will eventually become extinct, not thrive and expand indefinitely. Option B is incorrect because the study suggests that cities will eventually become extinct, not continue to decline indefinitely. Option D is incorrect because the study suggests that the advantage of physical proximity is fading, not still a dominant factor in the economy."}, "16": {"documentation": {"title": "Transferable Representation Learning in Vision-and-Language Navigation", "source": "Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel\n  Magalhaes, Jason Baldridge, Eugene Ie", "docs_id": "1908.03409", "section": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transferable Representation Learning in Vision-and-Language Navigation. Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the sequence alignment task in the Transferable Representation Learning in Vision-and-Language Navigation approach?\n\nA) To determine whether an instruction corresponds to a sequence of visual frames\nB) To determine whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space\nC) To adapt pre-trained vision and language representations to relevant in-domain tasks\nD) To improve the success rate weighted by path length (SPL) metric in VLN tasks\n\nCorrect Answer: A) To determine whether an instruction corresponds to a sequence of visual frames\n\nExplanation: The sequence alignment task is one of the two tasks that the model adapts pre-trained vision and language representations to solve. The primary goal of this task is to determine whether an instruction corresponds to a sequence of visual frames, which is a crucial step in understanding the relationship between language and vision in VLN tasks. This task is essential for the model to learn effective representations that can be used for navigation.\n\nNote: The other options are incorrect because:\n\n* B) is a task mentioned in the sequence coherence task, not the sequence alignment task.\n* C) is a broader goal of the approach, but not the specific goal of the sequence alignment task.\n* D) is a metric used to evaluate the performance of VLN agents, but not the primary goal of the sequence alignment task."}, "17": {"documentation": {"title": "The number of accessible paths in the hypercube", "source": "Julien Berestycki, \\'Eric Brunet, Zhan Shi", "docs_id": "1304.0246", "section": ["math.PR", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The number of accessible paths in the hypercube. Motivated by an evolutionary biology question, we study the following problem: we consider the hypercube $\\{0,1\\}^L$ where each node carries an independent random variable uniformly distributed on $[0,1]$, except $(1,1,\\ldots,1)$ which carries the value $1$ and $(0,0,\\ldots,0)$ which carries the value $x\\in[0,1]$. We study the number $\\Theta$ of paths from vertex $(0,0,\\ldots,0)$ to the opposite vertex $(1,1,\\ldots,1)$ along which the values on the nodes form an increasing sequence. We show that if the value on $(0,0,\\ldots,0)$ is set to $x=X/L$ then $\\Theta/L$ converges in law as $L\\to\\infty$ to $\\mathrm{e}^{-X}$ times the product of two standard independent exponential variables. As a first step in the analysis, we study the same question when the graph is that of a tree where the root has arity $L$, each node at level 1 has arity $L-1$, \\ldots, and the nodes at level $L-1$ have only one offspring which are the leaves of the tree (all the leaves are assigned the value 1, the root the value $x\\in[0,1]$)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a hypercube graph with $L$ nodes, where each node carries an independent random variable uniformly distributed on $[0,1]$, except for the root node which carries the value $x=X/L$. We want to study the number of paths from the root node to the opposite node $(1,1,\\ldots,1)$ along which the values on the nodes form an increasing sequence.\n\n**A)** The number of such paths is given by the binomial distribution with parameters $L$ and $x$.\n\n**B)** The number of such paths is given by the Poisson distribution with parameter $Lx$.\n\n**C)** The number of such paths is given by the normal distribution with mean $Lx$ and variance $Lx^2$.\n\n**D)** The number of such paths is given by the exponential distribution with parameter $Lx$.\n\n**Correct Answer:** D) The number of such paths is given by the exponential distribution with parameter $Lx$.\n\n**Explanation:** The problem statement mentions that the value on $(0,0,\\ldots,0)$ is set to $x=X/L$, and that the number $\\Theta$ of paths from vertex $(0,0,\\ldots,0)$ to the opposite vertex $(1,1,\\ldots,1)$ along which the values on the nodes form an increasing sequence converges in law as $L\\to\\infty$ to $\\mathrm{e}^{-X}$ times the product of two standard independent exponential variables. This suggests that the number of paths is exponentially distributed with parameter $Lx$."}, "18": {"documentation": {"title": "YBa2Cu3O7/LaXMnO3 (X: Ca, Sr) based\n  Superconductor/Ferromagnet/Superconductor junctions with memory functionality", "source": "R. de Andres Prada, T. Golod, O. M. Kapran, E. A. Borodianskyi, Ch.\n  Bernhard, and V. M. Krasnov", "docs_id": "1904.03951", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "YBa2Cu3O7/LaXMnO3 (X: Ca, Sr) based\n  Superconductor/Ferromagnet/Superconductor junctions with memory functionality. Complex oxides exhibit a variety of unusual physical properties, which can be used for designing novel electronic devices. Here we fabricate and study experimentally nano-scale Superconductor/ Ferromagnet/Superconductor junctions with the high-Tc cuprate superconductor YBa2Cu3O7 and the colossal magnetoresistive (CMR) manganite ferromagnets LaXMnO3 (X: Ca or Sr). We demonstrate that in a broad temperature range the magnetization of a manganite nanoparticle, forming the junction interface, switches abruptly in a mono-domain manner. The CMR phenomenon translates the magnetization loop into a hysteretic magnetoresistance loop. The latter facilitates a memory functionality of such a junction with just a single CMR ferromagnetic layer. The orientation of the magnetization (stored information) can be read out by simply measuring the junction resistance in an applied magnetic field. The CMR facilitates a large read-out signal in a small applied field. We argue that such a simple single layer CMR junction can operate as a memory cell both in the superconducting state at cryogenic temperatures and in the normal state up to room temperature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a colossal magnetoresistive (CMR) manganite ferromagnet in a Superconductor/Ferromagnet/Superconductor junction, and how does it enable memory functionality in such a junction?\n\nA) The CMR phenomenon allows for a large read-out signal in a small applied field, making it suitable for high-speed applications.\nB) The CMR phenomenon enables the storage of information in a single layer CMR junction, facilitating a simple and compact memory cell.\nC) The CMR phenomenon translates the magnetization loop into a hysteretic magnetoresistance loop, allowing for the detection of changes in the stored information.\nD) The CMR phenomenon enables the junction to operate in both the superconducting and normal states, making it suitable for applications requiring both high and low temperatures.\n\nCorrect Answer: C) The CMR phenomenon translates the magnetization loop into a hysteretic magnetoresistance loop, allowing for the detection of changes in the stored information.\n\nExplanation: The CMR phenomenon is the key to enabling memory functionality in the junction. By translating the magnetization loop into a hysteretic magnetoresistance loop, the CMR phenomenon allows for the detection of changes in the stored information, which can be read out by measuring the junction resistance in an applied magnetic field. This enables the junction to function as a simple and compact memory cell."}, "19": {"documentation": {"title": "New Approach to Determine the Quality of Graphene", "source": "Soo-Whan Kim, Hyun-Kyung Kim, Sunghun Lee, Kyujoon Lee, Joong Tark\n  Han, Kwang-Bum Kim, Kwang Chul Roh, Myung-Hwa Jung", "docs_id": "1709.09879", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New Approach to Determine the Quality of Graphene. The reduction of graphene oxide is one of the most facile methods to fabricate a large amount of graphene and the reduction rate of graphene oxide is related with the quality of synthesized graphene for its possible application. The reduction rate is usually determined by using various spectroscopy measurements such as Raman spectroscopy, Fourier transform infrared spectroscopy, and X-ray photoelectron spectroscopy. Here we propose that the magnetic data can be used as a means of determining the quality of graphene oxide (GO) and reduced graphene oxide (RGO) by the investigation of close relation between magnetic moment and chemical bonding state. Our experimental findings and previous theoretical studies suggest that hydroxyl functional groups in GO mainly contribute to Langevin paramagnetism, carboxyl functional groups in RGO1 act as the source for Pauli paramagnetism, and sp2 bonding state in RGO2 plays a major role on the diamagnetism. Especially in terms of mass production, the magnetic data is useful for decomposing the chemical bonding electronic states in graphene-like samples and judging their quality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary method used to determine the quality of graphene oxide (GO) and reduced graphene oxide (RGO) in terms of their magnetic properties?\n\nA) Fourier transform infrared spectroscopy\nB) X-ray photoelectron spectroscopy\nC) Raman spectroscopy\nD) Magnetic moment analysis\n\nCorrect Answer: D) Magnetic moment analysis\n\nExplanation: The correct answer is D) Magnetic moment analysis. The question states that the magnetic data can be used to determine the quality of graphene oxide (GO) and reduced graphene oxide (RGO) by investigating the close relation between magnetic moment and chemical bonding state. This implies that the magnetic moment analysis is the primary method used to determine the quality of these materials.\n\nCandidate A) Fourier transform infrared spectroscopy is incorrect because it is a spectroscopic method used to determine the chemical bonding state, but not specifically the magnetic properties.\n\nCandidate B) X-ray photoelectron spectroscopy is incorrect because it is a spectroscopic method used to determine the electronic structure, but not specifically the magnetic properties.\n\nCandidate C) Raman spectroscopy is incorrect because it is a spectroscopic method used to determine the chemical bonding state and defects in graphene, but not specifically the magnetic properties.\n\nCandidate D) Magnetic moment analysis is the correct answer because it is the method used to investigate the close relation between magnetic moment and chemical bonding state, which is essential for determining the quality of GO and RGO."}, "20": {"documentation": {"title": "Review of results using heavy ion collisions at CMS", "source": "Georgios Konstantinos Krintiras (on behalf of the CMS Collaboration)", "docs_id": "2006.05556", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Review of results using heavy ion collisions at CMS. Ultrarelativistic heavy ion collisions at the laboratory provide a unique chance to study quantum chromodynamics (QCD) under extreme temperature (${\\approx}150\\,\\mathrm{MeV}$) and density (${\\approx}1\\,\\mathrm{GeV}/\\mathrm{fm}^3$) conditions. Over the past decade, experimental results from LHC have shown further evidence for the formation of the quark-gluon plasma (QGP), a phase that is thought to permeate the early Universe and is formed in the high-density neutron-star cores. Various QCD predictions that model the behavior of the low-$x$ gluon nuclear density, a poorly explored region, are also tested. Since the photon flux per ion scales as the square of the emitting electric charge $Z^2$, cross sections of so far elusive photon-induced processes are extremely enhanced as compared to nucleon-nucleon collisions. Here, we review recent progress on CMS measurements of particle production with large transverse momentum or mass, photon-initiated processes, jet-induced medium response, and heavy quark production. These high-precision data, along with novel approaches, offer stringent constraints on initial state, QGP formation and transport parameters, and even parametrizations beyond the standard model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What can be inferred about the conditions under which ultrarelativistic heavy ion collisions at the laboratory occur, based on the given information about the temperature and density conditions?\n\nA) The temperature and density conditions are similar to those found in the early Universe.\nB) The temperature and density conditions are similar to those found in neutron-star cores.\nC) The temperature and density conditions are similar to those found in normal nuclear reactions.\nD) The temperature and density conditions are similar to those found in high-energy particle collisions.\n\n**Correct Answer:** B) The temperature and density conditions are similar to those found in neutron-star cores.\n\n**Explanation:** The text states that the ultrarelativistic heavy ion collisions at the laboratory provide a unique chance to study quantum chromodynamics (QCD) under extreme temperature (${\\approx}150\\,\\mathrm{MeV}$) and density (${\\approx}1\\,\\mathrm{GeV}/\\mathrm{fm}^3$) conditions. Additionally, it mentions that various QCD predictions that model the behavior of the low-$x$ gluon nuclear density, a poorly explored region, are also tested. The text also mentions that the formation of the quark-gluon plasma (QGP) is thought to permeate the early Universe and is formed in the high-density neutron-star cores. Therefore, it can be inferred that the temperature and density conditions under which ultrarelativistic heavy ion collisions occur are similar to those found in neutron-star cores.\n\n**Note:** The other options are incorrect because:\n\n* A) The temperature and density conditions are not similar to those found in the early Universe, as the early Universe is thought to have been in a state of thermal equilibrium, whereas the heavy ion collisions are at a much higher energy scale.\n* C) The temperature and density conditions are not similar to those found in normal nuclear reactions, as normal nuclear reactions do not involve the extreme conditions of ultrarelativistic heavy ion collisions.\n* D) The temperature and density conditions are not similar to those found in high-energy particle collisions, as high-energy particle collisions do not involve the same level of density and temperature as ultrarelativistic heavy ion collisions."}, "21": {"documentation": {"title": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families", "source": "Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton", "docs_id": "1705.08360", "section": ["stat.ML", "cs.LG", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families. We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite-dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nystr\\\"om-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Efficient and principled score estimation with Nystr\\\"om kernel exponential families\", what is the primary advantage of the proposed method over an earlier solution, and how does it relate to the computational efficiency of the model?\n\n**A)** The proposed method avoids the need to compute a normalization constant, which reduces the computational complexity of the model.\n**B)** The proposed method uses a low-rank, Nystr\\\"om-like solution, which reduces the storage requirements of the model.\n**C)** The proposed method retains the consistency and convergence rates of the full-rank solution, while providing a more efficient and data-efficient estimator for the score.\n**D)** The proposed method provides statistical guarantees on the degree of cost and storage reduction, while also being more interpretable and principled than existing score learning approaches.\n\n**Correct Answer:** C) The proposed method retains the consistency and convergence rates of the full-rank solution, while providing a more efficient and data-efficient estimator for the score.\n\n**Explanation:** The correct answer is C) because the proposed method improves the computational efficiency of the model by retaining the consistency and convergence rates of the full-rank solution, while also providing a more efficient and data-efficient estimator for the score. This is stated in the paper as \"Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nystr\\\"om-like solution.\" The other options are incorrect because they either focus on a different aspect of the method (A and B) or provide a statement that is not supported by the paper (D)."}, "22": {"documentation": {"title": "Strategy equilibrium in dilemma games with off-diagonal payoff\n  perturbations", "source": "Marco A. Amaral and Marco A. Javarone", "docs_id": "2003.12823", "section": ["physics.soc-ph", "cond-mat.stat-mech", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strategy equilibrium in dilemma games with off-diagonal payoff\n  perturbations. We analyse the strategy equilibrium of dilemma games considering a payoff matrix affected by small and random perturbations on the off-diagonal. Notably, a recent work [1] reported that, while cooperation is sustained by perturbations acting on the main diagonal, a less clear scenario emerges when perturbations act on the off-diagonal. Thus, the second case represents the core of this investigation, aimed at completing the description of the effects that payoff perturbations have on the dynamics of evolutionary games. Our results, achieved by analysing the proposed model under a variety of configurations, as different update rules, suggest that off-diagonal perturbations actually constitute a non-trivial form of noise. In particular, the most interesting effects are detected near the phase transition, as perturbations tend to move the strategy distribution towards non-ordered states of equilibrium, supporting cooperation when defection is pervading the population, and supporting defection in the opposite case. To conclude, we identified a form of noise that, under controlled conditions, could be used to enhance cooperation, and greatly delay its extinction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of dilemma games with off-diagonal payoff perturbations, what is the primary effect of off-diagonal perturbations on the strategy equilibrium, according to the analysis presented in the study?\n\nA) Off-diagonal perturbations lead to a decrease in cooperation and an increase in defection.\nB) Off-diagonal perturbations have no significant effect on the strategy equilibrium.\nC) Off-diagonal perturbations tend to move the strategy distribution towards non-ordered states of equilibrium, supporting cooperation when defection is pervading the population.\nD) Off-diagonal perturbations only affect the main diagonal of the payoff matrix.\n\n**Correct Answer:** C) Off-diagonal perturbations tend to move the strategy distribution towards non-ordered states of equilibrium, supporting cooperation when defection is pervading the population.\n\n**Explanation:** The study analyzed the effects of off-diagonal perturbations on the strategy equilibrium in dilemma games and found that these perturbations actually constitute a non-trivial form of noise. The most interesting effects were detected near the phase transition, where perturbations tend to move the strategy distribution towards non-ordered states of equilibrium. This movement supports cooperation when defection is pervading the population, and supports defection in the opposite case. Therefore, option C is the correct answer."}, "23": {"documentation": {"title": "Temperature effects on nuclear pseudospin symmetry in the\n  Dirac-Hartree-Bogoliubov formalism", "source": "R. Lisboa, P. Alberto, B. V. Carlson, and M. Malheiro", "docs_id": "1708.09511", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temperature effects on nuclear pseudospin symmetry in the\n  Dirac-Hartree-Bogoliubov formalism. We present finite temperature Dirac-Hartree-Bogoliubov (FTDHB) calculations for the tin isotope chain to study the dependence of pseudospin on the nuclear temperature. In the FTDHB calculation, the density dependence of the self-consistent relativistic mean fields, the pairing, and the vapor phase that takes into account the unbound nucleon states are considered self-consistently. The mean field potentials obtained in the FTDHB calculations are fit by Woods-Saxon (WS) potentials to examine how the WS parameters are related to the energy splitting of the pseudospin pairs as the temperature increases. We find that the nuclear potential surface diffuseness is the main driver for the pseudospin splittings and that it increases as the temperature grows. We conclude that pseudospin symmetry is better realized when the nuclear temperature increases. The results confirm the findings of previous works using RMF theory at $T=0$, namely that the correlation between the pseudospin splitting and the parameters of the Woods-Saxon potentials implies that pseudospin symmetry is a dynamical symmetry in nuclei. We show that the dynamical nature of the pseudospin symmetry remains when the temperature is considered in a realistic calculation of the tin isotopes, such as that of the Dirac-Hartree-Bogoliubov formalism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main factor that drives the pseudospin splittings in nuclei as the temperature increases, according to the FTDHB calculations for the tin isotope chain?\n\nA) The pairing energy\nB) The density dependence of the self-consistent relativistic mean fields\nC) The vapor phase that takes into account the unbound nucleon states\nD) The energy splitting of the pseudospin pairs\n\nCorrect Answer: D) The energy splitting of the pseudospin pairs\n\nExplanation: The correct answer is D) The energy splitting of the pseudospin pairs. The FTDHB calculations show that the nuclear potential surface diffuseness is the main driver for the pseudospin splittings, and that it increases as the temperature grows. This is because the energy splitting of the pseudospin pairs is directly related to the diffuseness of the nuclear potential surface.\n\nThe other options are incorrect because:\n\nA) The pairing energy is not mentioned as a factor that drives the pseudospin splittings in the FTDHB calculations.\n\nB) The density dependence of the self-consistent relativistic mean fields is considered in the FTDHB calculation, but it is not the main factor that drives the pseudospin splittings.\n\nC) The vapor phase that takes into account the unbound nucleon states is also considered in the FTDHB calculation, but it is not directly related to the pseudospin splittings.\n\nThis question requires the test-taker to carefully read and understand the documentation, and to identify the correct factor that drives the pseudospin splittings in nuclei as the temperature increases. It requires analysis and critical thinking skills, as well as the ability to distinguish between relevant and irrelevant information."}, "24": {"documentation": {"title": "Phase-separated symmetry-breaking vortex-lattice in a binary\n  Bose-Einstein condensate", "source": "S. K. Adhikari", "docs_id": "1908.07848", "section": ["cond-mat.quant-gas", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase-separated symmetry-breaking vortex-lattice in a binary\n  Bose-Einstein condensate. We study spontaneous-symmetry-breaking circularly-asymmetric phase separation of vortex lattices in a rapidly rotating harmonically-trapped quasi-two-dimensional (quasi-2D) binary Bose-Einstein condensate (BEC) with repulsive inter- and intra-species interactions. The phase separated vortex lattices of the components appear in different regions of space with no overlap between the vortices of the two components, which will permit an efficient experimental observation of such vortices and accurate study of the effect of atomic interaction on such vortex lattice. Such phase separation takes place when the intra-species interaction energies of the two components are equal or nearly equal with relatively strong inter-species repulsion. When the intra-species energies are equal, the two phase-separated vortex lattices have identical semicircular shapes with one being the parity conjugate of the other. When the intra-species energies are nearly equal, the phase separation is also complete but the vortex lattices have different shapes. We demonstrate our claim with a numerical solution of the mean-field Gross-Pitaevskii equation for a rapidly rotating quasi-2D binary BEC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the condition under which the phase-separated vortex lattices of the two components in a rapidly rotating harmonically-trapped quasi-two-dimensional (quasi-2D) binary Bose-Einstein condensate (BEC) will have identical semicircular shapes, one being the parity conjugate of the other?\n\nA) When the inter-species interaction energies are equal and the intra-species interaction energies are equal.\nB) When the intra-species interaction energies are nearly equal and the inter-species interaction energies are relatively weak.\nC) When the intra-species interaction energies are equal or nearly equal with relatively strong inter-species repulsion.\nD) When the inter-species interaction energies are equal and the intra-species interaction energies are nearly equal.\n\nCorrect Answer: C) When the intra-species interaction energies are equal or nearly equal with relatively strong inter-species repulsion.\n\nExplanation: According to the documentation, when the intra-species interaction energies of the two components are equal or nearly equal with relatively strong inter-species repulsion, the phase-separated vortex lattices will have identical semicircular shapes, one being the parity conjugate of the other. This is because the phase separation takes place in this condition, allowing for an efficient experimental observation of such vortices and accurate study of the effect of atomic interaction on such vortex lattice."}, "25": {"documentation": {"title": "The physics of hearing: fluid mechanics and the active process of the\n  inner ear", "source": "T. Reichenbach, A. J. Hudspeth", "docs_id": "1408.2085", "section": ["q-bio.NC", "nlin.AO", "physics.bio-ph", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The physics of hearing: fluid mechanics and the active process of the\n  inner ear. Most sounds of interest consist of complex, time-dependent admixtures of tones of diverse frequencies and variable amplitudes. To detect and process these signals, the ear employs a highly nonlinear, adaptive, real-time spectral analyzer: the cochlea. Sound excites vibration of the eardrum and the three miniscule bones of the middle ear, the last of which acts as a piston to initiate oscillatory pressure changes within the liquid-filled chambers of the cochlea. The basilar membrane, an elastic band spiraling along the cochlea between two of these chambers, responds to these pressures by conducting a largely independent traveling wave for each frequency component of the input. Because the basilar membrane is graded in mass and stiffness along its length, however, each traveling wave grows in magnitude and decreases in wavelength until it peaks at a specific, frequency-dependent position: low frequencies propagate to the cochlear apex, whereas high frequencies culminate at the base. The oscillations of the basilar membrane deflect hair bundles, the mechanically sensitive organelles of the ear's sensory receptors, the hair cells. As mechanically sensitive ion channels open and close, each hair cell responds with an electrical signal that is chemically transmitted to an afferent nerve fiber and thence into the brain. In addition to transducing mechanical inputs, hair cells amplify them [...]"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Describe the mechanism by which the cochlea processes complex sounds, including the role of the basilar membrane and the hair cells in transducing mechanical inputs into electrical signals.\n\n**A)** The cochlea processes complex sounds by using a single, rigid basilar membrane that responds to pressure changes in a uniform manner, resulting in a single electrical signal for each frequency component.\n\n**B)** The cochlea employs a highly nonlinear, adaptive, real-time spectral analyzer, the cochlea, which uses the basilar membrane to conduct independent traveling waves for each frequency component of the input, resulting in a graded response that peaks at specific frequency-dependent positions.\n\n**C)** The cochlea processes complex sounds by using a single, elastic band-like structure that responds to pressure changes in a uniform manner, resulting in a single electrical signal for each frequency component.\n\n**D)** The cochlea employs a highly nonlinear, adaptive, real-time spectral analyzer, the cochlea, which uses the basilar membrane to conduct independent traveling waves for each frequency component of the input, resulting in a graded response that peaks at specific frequency-dependent positions, and amplifies the signals through the hair cells.\n\n**Correct Answer:** D) The cochlea employs a highly nonlinear, adaptive, real-time spectral analyzer, the cochlea, which uses the basilar membrane to conduct independent traveling waves for each frequency component of the input, resulting in a graded response that peaks at specific frequency-dependent positions, and amplifies the signals through the hair cells.\n\n**Explanation:** The correct answer, D, accurately describes the mechanism by which the cochlea processes complex sounds. The cochlea uses the basilar membrane to conduct independent traveling waves for each frequency component of the input, resulting in a graded response that peaks at specific frequency-dependent positions. Additionally, the hair cells amplify the signals, allowing for the transduction of mechanical inputs into electrical signals. The other options are incorrect because they do not accurately describe the mechanism by which the cochlea processes complex sounds. Option A is incorrect because the basilar membrane is graded in mass and stiffness along its length, resulting in a non-uniform response. Option B is incorrect because it does not mention the amplification of signals through the hair cells. Option C is incorrect because it does not accurately describe the structure and function of the basilar membrane."}, "26": {"documentation": {"title": "Neighbor List Collision-Driven Molecular Dynamics Simulation for\n  Nonspherical Particles. I. Algorithmic Details II. Applications to Ellipses\n  and Ellipsoids", "source": "Aleksandar Donev and Salvatore Torquato and Frank H. Stillinger", "docs_id": "physics/0405089", "section": ["physics.comp-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neighbor List Collision-Driven Molecular Dynamics Simulation for\n  Nonspherical Particles. I. Algorithmic Details II. Applications to Ellipses\n  and Ellipsoids. In the first part of a series of two papers, we present in considerable detail a collision-driven molecular dynamics algorithm for a system of nonspherical particles, within a parallelepiped simulation domain, under both periodic or hard-wall boundary conditions. The algorithm extends previous event-driven molecular dynamics algorithms for spheres. We present a novel partial-update near-neighbor list (NNL) algorithm that is superior to previous algorithms at high densities, without compromising the correctness of the algorithm. This efficiency of the algorithm is further increased for systems of very aspherical particles by using bounding sphere complexes (BSC). In the second part of this series of papers we apply the algorithm presented in the first part of this series of papers to systems of hard ellipses and ellipsoids. The theoretical machinery needed to treat such particles, including the overlap potentials, is developed in full detail. We describe an algorithm for predicting the time of collision for two moving ellipses or ellipsoids. We present performance results for our implementation of the algorithm. The practical utility of the algorithm is demonstrated by presenting several interesting physical applications, including the generation of jammed packings inside spherical containers, the study of contact force chains in jammed packings, and melting the densest-known equilibrium crystals of prolate spheroids."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the novel partial-update near-neighbor list (NNL) algorithm presented in the paper, and how does it improve the efficiency of the collision-driven molecular dynamics algorithm for systems of nonspherical particles?\n\nA) It reduces the computational complexity of the algorithm by a factor of 2, making it suitable for large-scale simulations.\nB) It uses a more efficient data structure to store the near-neighbor list, allowing for faster lookups and updates.\nC) It incorporates a bounding sphere complex (BSC) to further reduce the computational complexity of the algorithm, making it suitable for systems of very aspherical particles.\nD) It eliminates the need for periodic boundary conditions, allowing for more realistic simulations of nonspherical particles.\n\nCorrect Answer: B) It uses a more efficient data structure to store the near-neighbor list, allowing for faster lookups and updates.\n\nExplanation: The paper states that the novel partial-update NNL algorithm is \"superior to previous algorithms at high densities, without compromising the correctness of the algorithm.\" This suggests that the algorithm uses a more efficient data structure to store the near-neighbor list, allowing for faster lookups and updates. This is a key advantage of the algorithm, as it enables more efficient simulations of nonspherical particles at high densities."}, "27": {"documentation": {"title": "Mass sensing for the advanced fabrication of nanomechanical resonators", "source": "G. Gruber (1), C. Urgell (1), A. Tavernarakis (1), A. Stavrinadis (1),\n  S. Tepsic (1), C. Magen (2 and 3), S. Sangiao (2 and 3), J. M. de Teresa (2\n  and 3), P. Verlot (4), A. Bachtold (1) ((1) ICFO, Castelldefels (Barcelona),\n  Spain, (2) ICMA, Universidad de Zaragoza, Zaragoza, Spain, (3) LMA,\n  Universidad de Zaragoza, Zaragoza, Spain, (4) School of Physics and\n  Astronomy, The University of Nottingham, Nottingham, United Kingdom)", "docs_id": "2101.09201", "section": ["physics.app-ph", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mass sensing for the advanced fabrication of nanomechanical resonators. We report on a nanomechanical engineering method to monitor matter growth in real time via e-beam electromechanical coupling. This method relies on the exceptional mass sensing capabilities of nanomechanical resonators. Focused electron beam induced deposition (FEBID) is employed to selectively grow platinum particles at the free end of singly clamped nanotube cantilevers. The electron beam has two functions: it allows both to grow material on the nanotube and to track in real time the deposited mass by probing the noise-driven mechanical resonance of the nanotube. On the one hand, this detection method is highly effective as it can resolve mass deposition with a resolution in the zeptogram range; on the other hand, this method is simple to use and readily available to a wide range of potential users, since it can be operated in existing commercial FEBID systems without making any modification. The presented method allows to engineer hybrid nanomechanical resonators with precisely tailored functionality. It also appears as a new tool for studying growth dynamics of ultra-thin nanostructures, opening new opportunities for investigating so far out-of-reach physics of FEBID and related methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary advantages of using focused electron beam induced deposition (FEBID) in conjunction with nanomechanical resonators for real-time monitoring of matter growth, and how does this method compare to other mass sensing techniques?\n\nA) FEBID allows for high-resolution imaging and precise control over material deposition, but its sensitivity is limited to the nanogram range.\nB) FEBID enables real-time monitoring of matter growth with a resolution in the zeptogram range, and its simplicity makes it accessible to a wide range of users without requiring significant modifications to existing commercial FEBID systems.\nC) FEBID is a complex and time-consuming process that requires specialized equipment and expertise, but its high sensitivity makes it suitable for studying ultra-thin nanostructures.\nD) FEBID is a relatively new technique that has not been extensively tested, and its reliability and accuracy are still uncertain.\n\nCorrect Answer: B) FEBID enables real-time monitoring of matter growth with a resolution in the zeptogram range, and its simplicity makes it accessible to a wide range of users without requiring significant modifications to existing commercial FEBID systems.\n\nExplanation: The correct answer highlights the dual advantages of FEBID in conjunction with nanomechanical resonators: its ability to monitor matter growth in real-time with high resolution, and its simplicity, which makes it accessible to a wide range of users without requiring significant modifications to existing commercial FEBID systems. This answer demonstrates an understanding of the method's benefits and its potential applications."}, "28": {"documentation": {"title": "Binary-driven stellar rotation evolution at the main-sequence turn-off\n  in star clusters", "source": "Weijia Sun, Richard de Grijs, Licai Deng, Michael D. Albrow", "docs_id": "2102.02352", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Binary-driven stellar rotation evolution at the main-sequence turn-off\n  in star clusters. The impact of stellar rotation on the morphology of star cluster colour-magnitude diagrams is widely acknowledged. However, the physics driving the distribution of the equatorial rotation velocities of main-sequence turn-off (MSTO) stars is as yet poorly understood. Using Gaia Data Release 2 photometry and new Southern African Large Telescope medium-resolution spectroscopy, we analyse the intermediate-age ($\\sim1\\,$Gyr-old) Galactic open clusters NGC 3960, NGC 6134 and IC 4756 and develop a novel method to derive their stellar rotation distributions based on SYCLIST stellar rotation models. Combined with literature data for the open clusters NGC 5822 and NGC 2818, we find a tight correlation between the number ratio of slow rotators and the clusters' binary fractions. The blue-main-sequence stars in at least two of our clusters are more centrally concentrated than their red-main-sequence counterparts. The origin of the equatorial stellar rotation distribution and its evolution remains as yet unidentified. However, the observed correlation in our open cluster sample suggests a binary-driven formation mechanism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism driving the distribution of equatorial rotation velocities of main-sequence turn-off (MSTO) stars in star clusters, according to the study?\n\nA) Gravitational interactions with the cluster's central black hole\nB) Binary interactions with other stars in the cluster\nC) Magnetic field variations within the star itself\nD) Stellar evolution through nuclear reactions in the core\n\nCorrect Answer: B) Binary interactions with other stars in the cluster\n\nExplanation: The study suggests a binary-driven formation mechanism as the primary driver of the equatorial stellar rotation distribution. This is supported by the observed correlation between the number ratio of slow rotators and the clusters' binary fractions. While other factors such as gravitational interactions and magnetic field variations may also play a role, the study's findings point to binary interactions as the primary mechanism driving the observed distribution of equatorial rotation velocities."}, "29": {"documentation": {"title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of\n  Channels", "source": "Zan Shen, Jiang Qian, Bojin Zhuang, Shaojun Wang, Jing Xiao", "docs_id": "2003.09821", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of\n  Channels. One-Shot methods have evolved into one of the most popular methods in Neural Architecture Search (NAS) due to weight sharing and single training of a supernet. However, existing methods generally suffer from two issues: predetermined number of channels in each layer which is suboptimal; and model averaging effects and poor ranking correlation caused by weight coupling and continuously expanding search space. To explicitly address these issues, in this paper, a Broadening-and-Shrinking One-Shot NAS (BS-NAS) framework is proposed, in which `broadening' refers to broadening the search space with a spring block enabling search for numbers of channels during training of the supernet; while `shrinking' refers to a novel shrinking strategy gradually turning off those underperforming operations. The above innovations broaden the search space for wider representation and then shrink it by gradually removing underperforming operations, followed by an evolutionary algorithm to efficiently search for the optimal architecture. Extensive experiments on ImageNet illustrate the effectiveness of the proposed BS-NAS as well as the state-of-the-art performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What are the two primary issues that existing One-Shot methods in Neural Architecture Search (NAS) generally suffer from, and how does the proposed BS-NAS framework address these issues?\n\nA) Predetermined number of channels in each layer is suboptimal, and model averaging effects are caused by weight coupling and continuously expanding search space. BS-NAS addresses this by broadening the search space with a spring block and shrinking it by gradually removing underperforming operations.\n\nB) Predetermined number of channels in each layer is suboptimal, and poor ranking correlation is caused by weight sharing and single training of a supernet. BS-NAS addresses this by broadening the search space with a spring block and shrinking it by gradually removing underperforming operations.\n\nC) Model averaging effects are caused by weight coupling and continuously expanding search space, and poor ranking correlation is caused by predetermined number of channels in each layer. BS-NAS addresses this by shrinking the search space with a spring block and broadening it by gradually adding underperforming operations.\n\nD) Predetermined number of channels in each layer is suboptimal, and poor ranking correlation is caused by weight sharing and single training of a supernet. BS-NAS addresses this by shrinking the search space with a spring block and broadening it by gradually removing underperforming operations.\n\nCorrect Answer: A) Predetermined number of channels in each layer is suboptimal, and model averaging effects are caused by weight coupling and continuously expanding search space. BS-NAS addresses this by broadening the search space with a spring block and shrinking it by gradually removing underperforming operations.\n\nExplanation: The correct answer is A because it accurately summarizes the two primary issues that existing One-Shot methods in NAS generally suffer from, which are predetermined number of channels in each layer being suboptimal and model averaging effects caused by weight coupling and continuously expanding search space. The proposed BS-NAS framework addresses these issues by broadening the search space with a spring block and shrinking it by gradually removing underperforming operations, as described in the documentation."}, "30": {"documentation": {"title": "Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks", "source": "Agnes Korcsak-Gorzo, Michael G. M\\\"uller, Andreas Baumbach, Luziwei\n  Leng, Oliver Julien Breitwieser, Sacha J. van Albada, Walter Senn, Karlheinz\n  Meier, Robert Legenstein, Mihai A. Petrovici", "docs_id": "2006.11099", "section": ["q-bio.NC", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks. Brains need to deal with an uncertain world. Often, this requires visiting multiple interpretations of the available information or multiple solutions to an encountered problem. This gives rise to the so-called mixing problem: since all of these \"valid\" states represent powerful attractors, but between themselves can be very dissimilar, switching between such states can be difficult. We propose that cortical oscillations can be effectively used to overcome this challenge. By acting as an effective temperature, background spiking activity modulates exploration. Rhythmic changes induced by cortical oscillations can then be interpreted as a form of simulated tempering. We provide a rigorous mathematical discussion of this link and study some of its phenomenological implications in computer simulations. This identifies a new computational role of cortical oscillations and connects them to various phenomena in the brain, such as sampling-based probabilistic inference, memory replay, multisensory cue combination and place cell flickering."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary function of cortical oscillations in spiking neural networks, according to the proposed mechanism, and how do they address the mixing problem in the brain?\n\n**A)** Cortical oscillations act as a form of noise reduction, suppressing the activity of neurons and preventing the mixing of states. This is achieved through the modulation of exploration by background spiking activity, which effectively reduces the temperature of the system.\n\n**B)** Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks, allowing the brain to effectively visit multiple interpretations of available information or multiple solutions to encountered problems. This is achieved through the simulated tempering effect of rhythmic changes induced by cortical oscillations, which modulate exploration and facilitate the mixing of states.\n\n**C)** Cortical oscillations are responsible for the replay of memories in the brain, allowing the brain to revisit and consolidate previously experienced events. This is achieved through the synchronized activity of neurons, which creates a \"memory trace\" that can be replayed and modified.\n\n**D)** Cortical oscillations are a form of neural plasticity, allowing the brain to reorganize and adapt its connections in response to changing environments. This is achieved through the dynamic modulation of synaptic strength, which is influenced by the rhythmic activity of cortical oscillations.\n\n**Correct Answer:** B) Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks, allowing the brain to effectively visit multiple interpretations of available information or multiple solutions to encountered problems. This is achieved through the simulated tempering effect of rhythmic changes induced by cortical oscillations, which modulate exploration and facilitate the mixing of states.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the proposed mechanism of cortical oscillations in spiking neural networks. Cortical oscillations are proposed to act as an effective temperature, modulating exploration and facilitating the mixing of states. This is achieved through the simulated tempering effect of rhythmic changes induced by cortical oscillations, which allows the brain to effectively visit multiple interpretations of available information or multiple solutions to encountered problems. The other options are incorrect because they either misrepresent the function of cortical oscillations or propose alternative mechanisms that are not supported by the documentation."}, "31": {"documentation": {"title": "Generalizations of Szpilrajn's Theorem in economic and game theories", "source": "Athanasios Andrikopoulos", "docs_id": "1708.04711", "section": ["q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalizations of Szpilrajn's Theorem in economic and game theories. Szpilrajn's Lemma entails that each partial order extends to a linear order. Dushnik and Miller use Szpilrajn's Lemma to show that each partial order has a relizer. Since then, many authors utilize Szpilrajn's Theorem and the Well-ordering principle to prove more general existence type theorems on extending binary relations. Nevertheless, we are often interested not only in the existence of extensions of a binary relation $R$ satisfying certain axioms of orderability, but in something more: (A) The conditions of the sets of alternatives and the properties which $R$ satisfies to be inherited when one passes to any member of a subfamily of the family of extensions of $R$ and: (B) The size of a family of ordering extensions of $R$, whose intersection is $R$, to be the smallest one. The key to addressing these kinds of problems is the szpilrajn inherited method. In this paper, we define the notion of $\\Lambda(m)$-consistency, where $m$ can reach the first infinite ordinal $\\omega$, and we give two general inherited type theorems on extending binary relations, a Szpilrajn type and a Dushnik-Miller type theorem, which generalize all the well known existence and inherited type extension theorems in the literature. \\keywords{Consistent binary relations, Extension theorems, Intersection of binary relations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a family of binary relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$, where $\\Lambda$ is a partially ordered set. Suppose that each $R_\\alpha$ is $\\Lambda(m)$-consistent, where $m$ is the first infinite ordinal $\\omega$. Let $R$ be the intersection of all $R_\\alpha$. Prove that $R$ is also $\\Lambda(m)$-consistent.\n\n**A)** The intersection of all $\\Lambda(m)$-consistent relations is $\\Lambda(m)$-consistent.\n\n**B)** The family of relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$ has a minimal element with respect to the partial order $\\Lambda$.\n\n**C)** The intersection of all $\\Lambda(m)$-consistent relations is $\\Lambda(m)$-consistent if and only if the family of relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$ has a maximal element with respect to the partial order $\\Lambda$.\n\n**D)** The intersection of all $\\Lambda(m)$-consistent relations is $\\Lambda(m)$-consistent if and only if the family of relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$ has a least upper bound with respect to the partial order $\\Lambda$.\n\n**Correct Answer:** C) The intersection of all $\\Lambda(m)$-consistent relations is $\\Lambda(m)$-consistent if and only if the family of relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$ has a maximal element with respect to the partial order $\\Lambda$.\n\n**Explanation:** This question requires the application of the Szpilrajn inherited method, which is a generalization of Szpilrajn's Lemma. The correct answer relies on the fact that the intersection of all $\\Lambda(m)$-consistent relations is $\\Lambda(m)$-consistent if and only if the family of relations $\\{R_\\alpha\\}_{\\alpha \\in \\Lambda}$ has a maximal element with respect to the partial order $\\Lambda$. This is because the Szpilrajn inherited method allows us to extend a partial order to a linear order, and the intersection of all $\\Lambda(m)$-consistent relations is the smallest extension of the original relation that is $\\Lambda(m)$-consistent. The other options are incorrect because they do not accurately reflect the relationship between the intersection of $\\Lambda(m)$-consistent relations and the maximal element of the family of relations."}, "32": {"documentation": {"title": "The equation of state in (2+1)-flavor QCD", "source": "A. Bazavov, Tanmoy Bhattacharya, C. DeTar, H.-T. Ding, Steven\n  Gottlieb, Rajan Gupta, P. Hegde, U.M. Heller, F. Karsch, E. Laermann, L.\n  Levkova, Swagato Mukherjee, P. Petreczky, C. Schmidt, C. Schroeder, R.A.\n  Soltz, W. Soeldner, R. Sugar, M. Wagner, P. Vranas", "docs_id": "1407.6387", "section": ["hep-lat", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The equation of state in (2+1)-flavor QCD. We present results for the equation of state in (2+1)-flavor QCD using the highly improved staggered quark action and lattices with temporal extent $N_{\\tau}=6,~8,~10$, and $12$. We show that these data can be reliably extrapolated to the continuum limit and obtain a number of thermodynamic quantities and the speed of sound in the temperature range $(130-400)$ MeV. We compare our results with previous calculations, and provide an analytic parameterization of the pressure, from which other thermodynamic quantities can be calculated, for use in phenomenology. We show that the energy density in the crossover region, $145~ {\\rm MeV} \\leq T \\leq 163$ MeV, defined by the chiral transition, is $\\epsilon_c=(0.18-0.5)~{\\rm GeV}/{\\rm fm}^3$, $i.e.$, $(1.2-3.1)\\ \\epsilon_{\\rm nuclear}$. At high temperatures, we compare our results with resummed and dimensionally reduced perturbation theory calculations. As a byproduct of our analyses, we obtain the values of the scale parameters $r_0$ from the static quark potential and $w_0$ from the gradient flow."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper on the equation of state in (2+1)-flavor QCD present results for the energy density in the crossover region, which is defined by the chiral transition. They report that the energy density is given by the range $(0.18-0.5)~{\\rm GeV}/{\\rm fm}^3$, which is equivalent to $(1.2-3.1)\\ \\epsilon_{\\rm nuclear}$. \n\n**A)** The energy density in the crossover region is a constant value of $0.35~{\\rm GeV}/{\\rm fm}^3$.\n\n**B)** The energy density in the crossover region is a function of the lattice spacing, and the authors report that it is independent of the lattice spacing for $N_{\\tau}=6,~8,~10$, and $12$.\n\n**C)** The energy density in the crossover region is a measure of the \"nuclear\" energy density, and the authors report that it is $(1.2-3.1)\\ \\epsilon_{\\rm nuclear}$.\n\n**D)** The energy density in the crossover region is a measure of the \"critical\" energy density, and the authors report that it is $(0.18-0.5)~{\\rm GeV}/{\\rm fm}^3$.\n\n**Correct Answer:** C) The energy density in the crossover region is a measure of the \"nuclear\" energy density, and the authors report that it is $(1.2-3.1)\\ \\epsilon_{\\rm nuclear}$.\n\n**Explanation:** The correct answer is C) because the energy density in the crossover region is indeed a measure of the \"nuclear\" energy density, and the authors report that it is equivalent to $(1.2-3.1)\\ \\epsilon_{\\rm nuclear}$. The other options are incorrect because the energy density is not a constant value (A), it is not independent of the lattice spacing (B), and it is not a measure of the \"critical\" energy density (D)."}, "33": {"documentation": {"title": "Electron transport parameters in CO$_2$: scanning drift tube\n  measurements and kinetic computations", "source": "M. Vass, I. Korolov, D. Loffhagen, N. Pinhao, Z. Donko", "docs_id": "1611.07447", "section": ["physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electron transport parameters in CO$_2$: scanning drift tube\n  measurements and kinetic computations. This work presents transport coefficients of electrons (bulk drift velocity, longitudinal diffusion coefficient, and effective ionization frequency) in CO2 measured under time-of-flight conditions over a wide range of the reduced electric field, 15Td <= E/N <= 2660Td in a scanning drift tube apparatus. The data obtained in the experiments are also applied to determine the effective steady-state Townsend ionization coefficient. These parameters are compared to the results of previous experimental studies, as well as to results of various kinetic computations: solutions of the electron Boltzmann equation under different approximations (multiterm and density gradient expansions) and Monte Carlo simulations. The experimental data extend the range of E/N compared with previous measurements and are consistent with most of the transport parameters obtained in these earlier studies. The computational results point out the range of applicability of the respective approaches to determine the different measured transport properties of electrons in CO2. They demonstrate as well the need for further improvement of the electron collision cross section data for CO2 taking into account the present experimental data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion drawn from the comparison of experimental data with kinetic computations for electron transport parameters in CO2, as presented in the study?\n\nA) The experimental data are inconsistent with most of the transport parameters obtained in previous studies.\nB) The kinetic computations are more accurate than the experimental data for determining electron transport properties in CO2.\nC) The experimental data are consistent with most of the transport parameters obtained in previous studies, but the kinetic computations highlight the need for further improvement of the electron collision cross section data for CO2.\nD) The study suggests that the electron Boltzmann equation under multiterm and density gradient expansions is more suitable for determining electron transport properties in CO2 than Monte Carlo simulations.\n\n**Correct Answer:** C) The experimental data are consistent with most of the transport parameters obtained in previous studies, but the kinetic computations highlight the need for further improvement of the electron collision cross section data for CO2.\n\n**Explanation:** The correct answer is C) because the study concludes that the experimental data are consistent with most of the transport parameters obtained in previous studies, but the kinetic computations reveal limitations and areas for improvement in the current understanding of electron transport properties in CO2. Specifically, the study highlights the need for further improvement of the electron collision cross section data for CO2, which is a key aspect of the kinetic computations. The other options are incorrect because they do not accurately reflect the conclusions drawn from the comparison of experimental data with kinetic computations."}, "34": {"documentation": {"title": "Disorder raises the critical temperature of a cuprate superconductor", "source": "Maxime Leroux, Vivek Mishra, Jacob P.C. Ruff, Helmut Claus, Matthew P.\n  Smylie, Christine Opagiste, Pierre Rodi\\`ere, Asghar Kayani, G. D. Gu, John\n  M. Tranquada, Wai-Kwong Kwok, Zahirul Islam, Ulrich Welp", "docs_id": "1808.05984", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Disorder raises the critical temperature of a cuprate superconductor. With the discovery of charge density waves (CDW) in most members of the cuprate high temperature superconductors, the interplay between superconductivity and CDW has become a key point in the debate on the origin of high temperature superconductivity. Some experiments in cuprates point toward a CDW state competing with superconductivity, but others raise the possibility of a CDW-superconductivity intertwined order, or more elusive pair-density wave (PDW). Here we have used proton irradiation to induce disorder in crystals of La$_{1.875}$Ba$_{0.125}$CuO$_4$ and observed a striking 50% increase of $T_\\mathrm{c}$ accompanied by a suppression of the CDW. This is in clear contradiction with the behaviour expected of a d-wave superconductor for which both magnetic and non-magnetic defects should suppress $T_\\mathrm{c}$. Our results thus make an unambiguous case for the strong detrimental effect of the CDW on bulk superconductivity in La$_{1.875}$Ba$_{0.125}$CuO$_4$. Using tunnel diode oscillator (TDO) measurements, we find evidence for dynamic layer decoupling in PDW phase. Our results establish irradiation-induced disorder as a particularly relevant tuning parameter for the many families of superconductors with coexisting density waves, which we demonstrate on superconductors such as the dichalcogenides and Lu$_5$Ir$_4$Si$_{10}$."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary conclusion drawn from the study on the interplay between superconductivity and charge density waves (CDW) in cuprate high temperature superconductors, as demonstrated by the experiment on La$_{1.875}$Ba$_{0.125}$CuO$_4$?\n\nA) The presence of CDW is necessary for the emergence of superconductivity.\nB) The CDW state competes with superconductivity, but the latter is more stable.\nC) The CDW state is detrimental to the emergence of superconductivity, and its suppression leads to a significant increase in the critical temperature.\nD) The study provides no conclusive evidence for the interplay between superconductivity and CDW.\n\nCorrect Answer: C) The CDW state is detrimental to the emergence of superconductivity, and its suppression leads to a significant increase in the critical temperature.\n\nExplanation: The study found that proton irradiation-induced disorder in La$_{1.875}$Ba$_{0.125}$CuO$_4$ resulted in a 50% increase in the critical temperature, accompanied by a suppression of the CDW. This contradicts the expected behavior of a d-wave superconductor, which should be suppressed by both magnetic and non-magnetic defects. Therefore, the primary conclusion is that the CDW state is detrimental to the emergence of superconductivity, and its suppression leads to an increase in the critical temperature."}, "35": {"documentation": {"title": "Color-magnetic flux tubes in quark matter cores of neutron stars", "source": "Mark G. Alford (Washington U., St Louis), Armen Sedrakian (Frankfurt\n  U.)", "docs_id": "1001.3346", "section": ["astro-ph.SR", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Color-magnetic flux tubes in quark matter cores of neutron stars. We argue that if color-superconducting quark matter exists in the core of a neutron star, it may contain a high density of flux tubes, carrying flux that is mostly color-magnetic, with a small admixture of ordinary magnetic flux. We focus on the two-flavor color-superconducting (\"2SC\") phase, and assume that the flux tubes are energetically stable, although this has not yet been demonstrated. The density of flux tubes depends on the nature of the transition to the color-superconducting phase, and could be within an order of magnitude of the density of magnetic flux tubes that would be found if the core were superconducting nuclear matter. We calculate the cross-section for Aharonov-Bohm scattering of gapless fermions off the flux tubes, and the associated collision time and frictional force on a moving flux tube. We discuss the other forces on the flux tube, and find that if we take in to account only the forces that arise within the 2SC core region then the timescale for expulsion of the color flux tubes from the 2SC core is of order 10^10 years."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of color-superconducting quark matter cores of neutron stars, what is the estimated timescale for expulsion of color flux tubes from the 2SC core, assuming only forces within the 2SC core region are considered?\n\n**A)** 10^6 years\n**B)** 10^8 years\n**C)** 10^10 years\n**D)** 10^12 years\n\n**Correct Answer:** C) 10^10 years\n\n**Explanation:** The question requires the test-taker to recall the specific calculation mentioned in the documentation, which states that the timescale for expulsion of the color flux tubes from the 2SC core is of order 10^10 years. This requires the test-taker to carefully read and understand the text, and to identify the correct answer among the options.\n\nNote: The other options (A, B, and D) are plausible but incorrect, and may be intended to test the test-taker's ability to distinguish between different orders of magnitude or to recognize the correct answer among distractors."}, "36": {"documentation": {"title": "Topology-dependent density optima for efficient simultaneous network\n  exploration", "source": "Daniel B. Wilson and Ruth E. Baker and Francis G. Woodhouse", "docs_id": "1709.08706", "section": ["math.OC", "cond-mat.stat-mech", "math.PR", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topology-dependent density optima for efficient simultaneous network\n  exploration. A random search process in a networked environment is governed by the time it takes to visit every node, termed the cover time. Often, a networked process does not proceed in isolation but competes with many instances of itself within the same environment. A key unanswered question is how to optimise this process: how many concurrent searchers can a topology support before the benefits of parallelism are outweighed by competition for space? Here, we introduce the searcher-averaged parallel cover time (APCT) to quantify these economies of scale. We show that the APCT of the networked symmetric exclusion process is optimised at a searcher density that is well predicted by the spectral gap. Furthermore, we find that non-equilibrium processes, realised through the addition of bias, can support significantly increased density optima. Our results suggest novel hybrid strategies of serial and parallel search for efficient information gathering in social interaction and biological transport networks."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary limitation of a networked process in a competitive environment, and how does the searcher-averaged parallel cover time (APCT) address this limitation?\n\n**A)** The primary limitation is the physical constraints of the environment, and APCT is a measure of the average time it takes for a searcher to visit every node in the network.\n\n**B)** The primary limitation is the competition for space among multiple instances of the same process, and APCT is a measure of the optimal number of concurrent searchers that a network can support.\n\n**C)** The primary limitation is the lack of parallelism in the search process, and APCT is a measure of the average time it takes for a searcher to visit every node in the network, taking into account the effects of parallelism.\n\n**D)** The primary limitation is the emergence of non-equilibrium processes, and APCT is a measure of the average time it takes for a searcher to visit every node in the network, taking into account the effects of bias.\n\n**Correct Answer:** B) The primary limitation is the competition for space among multiple instances of the same process, and APCT is a measure of the optimal number of concurrent searchers that a network can support.\n\n**Explanation:** The correct answer is B) because the question asks about the primary limitation of a networked process in a competitive environment, which is the competition for space among multiple instances of the same process. The APCT is a measure of the optimal number of concurrent searchers that a network can support before the benefits of parallelism are outweighed by competition for space. The other options are incorrect because they either misstate the primary limitation (A and C) or introduce a different concept (D)."}, "37": {"documentation": {"title": "Nonlinear Dipole Inversion (NDI) enables Quantitative Susceptibility\n  Mapping (QSM) without parameter tuning", "source": "Daniel Polak, Itthi Chatnuntawech, Jaeyeon Yoon, Siddharth Srinivasan\n  Iyer, Jongho Lee, Peter Bachert, Elfar Adalsteinsson, Kawin Setsompop, Berkin\n  Bilgic", "docs_id": "1909.13692", "section": ["eess.IV", "cs.LG", "eess.SP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear Dipole Inversion (NDI) enables Quantitative Susceptibility\n  Mapping (QSM) without parameter tuning. We propose Nonlinear Dipole Inversion (NDI) for high-quality Quantitative Susceptibility Mapping (QSM) without regularization tuning, while matching the image quality of state-of-the-art reconstruction techniques. In addition to avoiding over-smoothing that these techniques often suffer from, we also obviate the need for parameter selection. NDI is flexible enough to allow for reconstruction from an arbitrary number of head orientations, and outperforms COSMOS even when using as few as 1-direction data. This is made possible by a nonlinear forward-model that uses the magnitude as an effective prior, for which we derived a simple gradient descent update rule. We synergistically combine this physics-model with a Variational Network (VN) to leverage the power of deep learning in the VaNDI algorithm. This technique adopts the simple gradient descent rule from NDI and learns the network parameters during training, hence requires no additional parameter tuning. Further, we evaluate NDI at 7T using highly accelerated Wave-CAIPI acquisitions at 0.5 mm isotropic resolution and demonstrate high-quality QSM from as few as 2-direction data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the Nonlinear Dipole Inversion (NDI) technique in Quantitative Susceptibility Mapping (QSM) reconstruction, and how does it differ from other reconstruction techniques?\n\nA) NDI requires a large number of head orientations to achieve high-quality QSM, whereas other techniques can use fewer orientations.\nB) NDI avoids over-smoothing and obviates the need for parameter selection, unlike other techniques that often suffer from these issues.\nC) NDI is limited to using only 1-direction data, whereas other techniques can use multiple directions.\nD) NDI is only suitable for high-field MRI scanners, whereas other techniques can be used on lower-field scanners.\n\nCorrect Answer: B) NDI avoids over-smoothing and obviates the need for parameter selection, unlike other techniques that often suffer from these issues.\n\nExplanation: The correct answer is B) NDI avoids over-smoothing and obviates the need for parameter selection, unlike other techniques that often suffer from these issues. This is stated in the documentation as one of the key advantages of NDI, which allows for high-quality QSM reconstruction without the need for regularization tuning or parameter selection. The other options are incorrect because they do not accurately reflect the advantages of NDI. Option A is incorrect because NDI can actually use fewer orientations than other techniques. Option C is incorrect because NDI can use an arbitrary number of head orientations, not just 1-direction data. Option D is incorrect because there is no mention of NDI being limited to high-field MRI scanners."}, "38": {"documentation": {"title": "Multi-task Learning for Voice Trigger Detection", "source": "Siddharth Sigtia, Pascal Clark, Rob Haynes, Hywel Richards, John\n  Bridle", "docs_id": "2001.09519", "section": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-task Learning for Voice Trigger Detection. We describe the design of a voice trigger detection system for smart speakers. In this study, we address two major challenges. The first is that the detectors are deployed in complex acoustic environments with external noise and loud playback by the device itself. Secondly, collecting training examples for a specific keyword or trigger phrase is challenging resulting in a scarcity of trigger phrase specific training data. We describe a two-stage cascaded architecture where a low-power detector is always running and listening for the trigger phrase. If a detection is made at this stage, the candidate audio segment is re-scored by larger, more complex models to verify that the segment contains the trigger phrase. In this study, we focus our attention on the architecture and design of these second-pass detectors. We start by training a general acoustic model that produces phonetic transcriptions given a large labelled training dataset. Next, we collect a much smaller dataset of examples that are challenging for the baseline system. We then use multi-task learning to train a model to simultaneously produce accurate phonetic transcriptions on the larger dataset \\emph{and} discriminate between true and easily confusable examples using the smaller dataset. Our results demonstrate that the proposed model reduces errors by half compared to the baseline in a range of challenging test conditions \\emph{without} requiring extra parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the second-pass detectors in the proposed two-stage cascaded architecture for voice trigger detection?\n\n**A)** To improve the accuracy of phonetic transcriptions on the larger dataset\n**B)** To reduce the computational power required for the detection process\n**C)** To verify that the detected audio segment contains the trigger phrase and improve overall accuracy\n**D)** To collect a larger dataset of examples for training the general acoustic model\n\n**Correct Answer:** C) To verify that the detected audio segment contains the trigger phrase and improve overall accuracy\n\n**Explanation:** The correct answer is C) To verify that the detected audio segment contains the trigger phrase and improve overall accuracy. The second-pass detectors are designed to re-score the candidate audio segment to verify that it contains the trigger phrase, which is a crucial step in improving the overall accuracy of the voice trigger detection system. The other options are incorrect because they do not accurately describe the primary goal of the second-pass detectors. Option A is incorrect because the primary goal is not to improve the accuracy of phonetic transcriptions on the larger dataset, but rather to verify the trigger phrase. Option B is incorrect because the second-pass detectors do not aim to reduce computational power. Option D is incorrect because the second-pass detectors are not responsible for collecting a larger dataset of examples."}, "39": {"documentation": {"title": "Gamma, Gaussian and Poisson approximations for random sums using\n  size-biased and generalized zero-biased couplings", "source": "Fraser Daly", "docs_id": "2011.13815", "section": ["math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gamma, Gaussian and Poisson approximations for random sums using\n  size-biased and generalized zero-biased couplings. Let $Y=X_1+\\cdots+X_N$ be a sum of a random number of exchangeable random variables, where the random variable $N$ is independent of the $X_j$, and the $X_j$ are from the generalized multinomial model introduced by Tallis (1962). This relaxes the classical assumption that the $X_j$ are independent. We use zero-biased coupling and its generalizations to give explicit error bounds in the approximation of $Y$ by a Gaussian random variable in Wasserstein distance when either the random variables $X_j$ are centred or $N$ has a Poisson distribution. We further establish an explicit bound for the approximation of $Y$ by a gamma distribution in stop-loss distance for the special case where $N$ is Poisson. Finally, we briefly comment on analogous Poisson approximation results that make use of size-biased couplings. The special case of independent $X_j$ is given special attention throughout. As well as establishing results which extend beyond the independent setting, our bounds are shown to be competitive with known results in the independent case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $Y=X_1+\\cdots+X_N$ be a sum of a random number of exchangeable random variables, where the random variable $N$ is independent of the $X_j$, and the $X_j$ are from the generalized multinomial model introduced by Tallis (1962). Suppose that the $X_j$ are centred and have a Poisson distribution with parameter $\\lambda$. Find the explicit error bound in Wasserstein distance for the approximation of $Y$ by a Gaussian random variable, and explain why this bound is competitive with known results in the independent case.\n\n**A)** The error bound is $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2)$.\n\n**B)** The error bound is $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2) + \\frac{\\lambda}{2}$.\n\n**C)** The error bound is $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2) + \\frac{\\lambda}{2} + \\frac{\\lambda^2}{2N}$.\n\n**D)** The error bound is $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2) + \\frac{\\lambda}{2} + \\frac{\\lambda^2}{2N} + \\frac{\\lambda^3}{2N^2}$.\n\n**Correct Answer:** C) The error bound is $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2) + \\frac{\\lambda}{2} + \\frac{\\lambda^2}{2N}$.\n\n**Explanation:** The correct answer can be obtained by applying the results from the documentation, specifically the explicit error bounds for the approximation of $Y$ by a Gaussian random variable in Wasserstein distance when the $X_j$ are centred and $N$ has a Poisson distribution. The bound is given by $\\sqrt{\\frac{\\lambda}{2\\pi}} \\sum_{j=1}^N E(X_j^2) + \\frac{\\lambda}{2} + \\frac{\\lambda^2}{2N}$, which is obtained by combining the results from the documentation and applying the appropriate mathematical manipulations."}, "40": {"documentation": {"title": "Dynamics in the production of superheavy nuclei in low-energy heavy-ion\n  collisions", "source": "Zhao-Qing Feng, Gen-Ming Jin, Jun-Qing Li", "docs_id": "1105.5224", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics in the production of superheavy nuclei in low-energy heavy-ion\n  collisions. We present a review of the recent progress of theoretical models on the description of the formation of superheavy nuclei in collisions of heavy systems. Two sorts of reactions that are the fusion-evaporation mechanism and the massive damped collisions to produce superheavy nuclei are discussed. Problems and further improvements of the capture of colliding partners, the formation of compound nucleus and the de-excitation process are pointed out. Possible combinations in the synthesis of the gap of the cold fusion and $^{48}$Ca induced reactions are proposed by the calculations based on the dinuclear system model and also compared with other approaches. The synthesis of neutron-rich heavy isotopes near sub-shell closure N=162 via transfer reactions in the damped collisions of two actinides and the influence of shell closure on the production of heavy isotopes are investigated. Prospective possibility to reach superheavy nuclei near N=184 via neutron-rich radioactive beams of high intensity in the future is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism proposed in the dinuclear system model for the synthesis of superheavy nuclei, and how does it differ from the fusion-evaporation mechanism?\n\nA) The dinuclear system model proposes the fusion-evaporation mechanism, which involves the capture of colliding partners and the formation of a compound nucleus, followed by de-excitation.\nB) The dinuclear system model suggests that massive damped collisions can produce superheavy nuclei through the transfer of nucleons, leading to the synthesis of neutron-rich heavy isotopes near sub-shell closure.\nC) The dinuclear system model proposes that the synthesis of superheavy nuclei occurs through the combination of cold fusion and $^{48}$Ca induced reactions, which involve the capture of colliding partners and the formation of a compound nucleus.\nD) The dinuclear system model proposes that the synthesis of superheavy nuclei occurs through the massive damped collisions of two actinides, resulting in the transfer of nucleons and the formation of neutron-rich heavy isotopes near sub-shell closure.\n\nCorrect Answer: B) The dinuclear system model suggests that massive damped collisions can produce superheavy nuclei through the transfer of nucleons, leading to the synthesis of neutron-rich heavy isotopes near sub-shell closure.\n\nExplanation: The correct answer is B) because the dinuclear system model proposes that massive damped collisions can produce superheavy nuclei through the transfer of nucleons, leading to the synthesis of neutron-rich heavy isotopes near sub-shell closure. This is in contrast to the fusion-evaporation mechanism, which involves the capture of colliding partners and the formation of a compound nucleus, followed by de-excitation. The other options are incorrect because they either describe the fusion-evaporation mechanism (A) or propose a combination of cold fusion and $^{48}$Ca induced reactions (C), which is not the primary mechanism proposed by the dinuclear system model. Option D) is also incorrect because it describes a different mechanism, the massive damped collisions of two actinides, which is not the primary mechanism proposed by the dinuclear system model."}, "41": {"documentation": {"title": "Five Starter Pieces: Quantum Information Science via Semi-definite\n  Programs", "source": "Vikesh Siddhu, Sridhar Tayur", "docs_id": "2112.08276", "section": ["quant-ph", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Five Starter Pieces: Quantum Information Science via Semi-definite\n  Programs. As the title indicates, this manuscript presents a brief, self-contained introduction to five fundamental problems in Quantum Information Science (QIS) that are especially well-suited to be formulated as Semi-definite Programs (SDP). We have in mind two audiences. The primary audience comprises of Operations Research (and Computer Science) graduate students who have familiarity with SDPs, but have found it daunting to become even minimally conversant with pre-requisites of QIS. The second audience consists of Physicists (and Electrical Engineers) already knowledgeable with modeling of QIS via SDP but interested in computational tools that are applicable more generally. For both audiences, we strive for rapid access to the unfamiliar material. For the first, we provide just enough required background material (from Quantum Mechanics, treated via matrices, and mapping them in Dirac notation) and simultaneously for the second audience we recreate, computationally in Jupyter notebooks, known closed-form solutions. We hope you will enjoy this little manuscript and gain understanding of the marvelous connection between SDP and QIS by self-study, or as a short seminar course. Ultimately, we hope this disciplinary outreach will fuel advances in QIS through their fruitful study via SDPs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a quantum information science problem that can be formulated as a Semi-definite Program (SDP). What is the primary motivation behind the authors' decision to provide a self-contained introduction to five fundamental problems in Quantum Information Science (QIS) via SDPs, and what is the intended audience for this manuscript?\n\nA) The authors aim to provide a comprehensive review of SDPs and QIS, targeting experts in the field. \nB) The authors strive to provide rapid access to unfamiliar material, targeting Operations Research (and Computer Science) graduate students and Physicists (and Electrical Engineers) who are already knowledgeable with modeling of QIS via SDPs.\nC) The authors aim to recreate known closed-form solutions computationally in Jupyter notebooks, targeting experts in both fields.\nD) The authors aim to provide a detailed analysis of the mathematical prerequisites of QIS, targeting students with no prior knowledge of the subject.\n\nCorrect Answer: B) The authors strive to provide rapid access to unfamiliar material, targeting Operations Research (and Computer Science) graduate students and Physicists (and Electrical Engineers) who are already knowledgeable with modeling of QIS via SDPs."}, "42": {"documentation": {"title": "Interference Avoidance in UAV-Assisted Networks: Joint 3D Trajectory\n  Design and Power Allocation", "source": "Ali Rahmati, Seyyedali Hosseinalipour, Yavuz Yapici, Xiaofan He,\n  Ismail Guvenc, Huaiyu Dai, Arupjyoti Bhuyan", "docs_id": "1904.07781", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interference Avoidance in UAV-Assisted Networks: Joint 3D Trajectory\n  Design and Power Allocation. The use of the unmanned aerial vehicle (UAV) has been foreseen as a promising technology for the next generation communication networks. Since there are no regulations for UAVs deployment yet, most likely they form a network in coexistence with an already existed network. In this work, we consider a transmission mechanism that aims to improve the data rate between a terrestrial base station (BS) and user equipment (UE) through deploying multiple UAVs relaying the desired data flow. Considering the coexistence of this network with other established communication networks, we take into account the effect of interference, which is incurred by the existing nodes. Our primary goal is to optimize the three-dimensional (3D) trajectories and power allocation for the relaying UAVs to maximize the data flow while keeping the interference to existing nodes below a predefined threshold. An alternating-maximization strategy is proposed to solve the joint 3D trajectory design and power allocation for the relaying UAVs. To this end, we handle the information exchange within the network by resorting to spectral graph theory and subsequently address the power allocation through convex optimization techniques. Simulation results show that our approach can considerably improve the information flow while the interference threshold constraint is met."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Interference Avoidance in UAV-Assisted Networks: Joint 3D Trajectory Design and Power Allocation\", what is the primary goal of the proposed alternating-maximization strategy for optimizing the 3D trajectories and power allocation for relaying UAVs?\n\nA) To minimize the power consumption of the UAVs while maximizing the data flow.\nB) To maximize the data flow while keeping the interference to existing nodes below a predefined threshold.\nC) To optimize the 3D trajectories and power allocation for the relaying UAVs to minimize the interference to existing nodes.\nD) To design a network architecture that can support multiple UAVs and terrestrial base stations simultaneously.\n\n**Correct Answer:** B) To maximize the data flow while keeping the interference to existing nodes below a predefined threshold.\n\n**Explanation:** The correct answer is B) To maximize the data flow while keeping the interference to existing nodes below a predefined threshold. This is stated in the introduction of the paper as the primary goal of the work. The alternating-maximization strategy is proposed to achieve this goal by optimizing the 3D trajectories and power allocation for the relaying UAVs.\n\n**Candidate A (Incorrect):** A) To minimize the power consumption of the UAVs while maximizing the data flow. While power consumption is an important consideration in UAV-assisted networks, it is not the primary goal of the proposed strategy.\n\n**Candidate B (Incorrect):** B) To maximize the data flow while keeping the interference to existing nodes below a predefined threshold. This is the correct answer, but it is not a complete statement of the primary goal. The paper also mentions that the strategy aims to optimize the 3D trajectories and power allocation.\n\n**Candidate C (Incorrect):** C) To optimize the 3D trajectories and power allocation for the relaying UAVs to minimize the interference to existing nodes. This statement is close to the correct answer, but it is not the primary goal. The paper actually aims to maximize the data flow while minimizing interference.\n\n**Candidate D (Incorrect):** D) To design a network architecture that can support multiple UAVs and terrestrial base stations simultaneously. This statement is not relevant to the primary goal of the proposed strategy, which is to optimize the 3D trajectories and power allocation for relaying UAVs."}, "43": {"documentation": {"title": "Mathematical modelling of variable porosity coatings for controlled drug\n  release", "source": "Sean McGinty, David King, Giuseppe Pontrelli", "docs_id": "1701.04989", "section": ["physics.flu-dyn", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mathematical modelling of variable porosity coatings for controlled drug\n  release. In this paper we investigate the extent to which variable porosity drug-eluting coatings can provide better control over drug release than coatings where the porosity is constant throughout. In particular, we aim to establish the potential benefits of replacing a single-layer with a two-layer coating of identical total thickness and initial drug mass. In our study, what distinguishes the layers (other than their individual thickness and initial drug loading) is the underlying microstructure, and in particular the effective porosity and the tortuosity of the material. We consider the effect on the drug release profile of varying the initial distribution of drug, the relative thickness of the layers and the relative resistance to diffusion offered by each layer's composition. Our results indicate that the contrast in properties of the two layers can be used as a means of better controlling the release, and that the quantity of drug delivered in the early stages can be modulated by varying the distribution of drug across the layers. We conclude that microstructural and loading differences between multi-layer variable porosity coatings can be used to tune the properties of the coating materials to obtain the desired drug release profile for a given application."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of variable porosity coatings for controlled drug release, what is the primary advantage of using a two-layer coating with distinct microstructures, as opposed to a single-layer coating with constant porosity throughout?\n\n**A)** The two-layer coating allows for a more uniform distribution of drug across the surface.\n**B)** The two-layer coating enables the modulation of the early stages of drug release by varying the distribution of drug across the layers.\n**C)** The two-layer coating provides a more consistent and predictable drug release profile.\n**D)** The two-layer coating offers improved mechanical strength and durability.\n\n**Correct Answer:** B) The two-layer coating enables the modulation of the early stages of drug release by varying the distribution of drug across the layers.\n\n**Explanation:** The correct answer is B) because the paper states that \"the quantity of drug delivered in the early stages can be modulated by varying the distribution of drug across the layers.\" This indicates that the two-layer coating allows for the intentional variation of drug release rates, which is a key advantage of this design. The other options are incorrect because they do not accurately reflect the benefits of the two-layer coating as described in the paper."}, "44": {"documentation": {"title": "Structural distortion and frustrated magnetic interactions in the\n  layered copper oxychloride [CuCl]LaNb(2)O(7)", "source": "Alexander A. Tsirlin, Helge Rosner", "docs_id": "0901.0154", "section": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Structural distortion and frustrated magnetic interactions in the\n  layered copper oxychloride [CuCl]LaNb(2)O(7). We present a computational study of the layered copper oxychloride [CuCl]LaNb(2)O(7) that has been recently proposed as a spin-1/2 frustrated square lattice compound. Our results evidence an orbitally degenerate ground state for the reported tetragonal crystal structure and reveal a Jahn-Teller-type structural distortion. This distortion heavily changes the local environment of copper -- CuO(2)Cl(2) plaquettes are formed instead of CuO(2)Cl(4) octahedra -- and restores the single-orbital scenario typical for copper oxides and oxyhalides. The calculated distortion is consistent with the available diffraction data and the experimental results on the electric field gradients for the Cu and Cl sites. The band structure suggests a complex three-dimensional spin model with the interactions up to the fourth neighbors. Despite the layered structure of (CuCl)LaNb(2)O(7), the spin system has pronounced one-dimensional features. Yet, sizable interchain interactions lead to the strong frustration and likely cause the spin-gap behavior. Computational estimates of individual exchange couplings are in qualitative agreement with the experimental data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary consequence of the Jahn-Teller-type structural distortion in the layered copper oxychloride [CuCl]LaNb(2)O(7), and how does it affect the local environment of copper ions in the compound?\n\nA) The distortion leads to a decrease in the number of CuO(2)Cl(4) octahedra, resulting in a more disordered local environment for copper ions.\nB) The distortion restores the single-orbital scenario typical for copper oxides and oxyhalides, forming CuO(2)Cl(2) plaquettes instead of CuO(2)Cl(4) octahedra.\nC) The distortion has no significant effect on the local environment of copper ions, maintaining the original CuO(2)Cl(4) octahedral structure.\nD) The distortion leads to an increase in the number of CuO(2)Cl(4) octahedra, resulting in a more ordered local environment for copper ions.\n\nCorrect Answer: B) The distortion restores the single-orbital scenario typical for copper oxides and oxyhalides, forming CuO(2)Cl(2) plaquettes instead of CuO(2)Cl(4) octahedra.\n\nExplanation: The correct answer is B) because the Jahn-Teller-type structural distortion in [CuCl]LaNb(2)O(7) leads to a change in the local environment of copper ions, restoring the single-orbital scenario typical for copper oxides and oxyhalides. This is evident from the text, which states that the distortion \"heavily changes the local environment of copper -- CuO(2)Cl(2) plaquettes are formed instead of CuO(2)Cl(4) octahedra -- and restores the single-orbital scenario typical for copper oxides and oxyhalides.\""}, "45": {"documentation": {"title": "Complexity of Equilibrium in Diffusion Games on Social Networks", "source": "Seyed Rasoul Etesami, Tamer Basar", "docs_id": "1403.3881", "section": ["cs.GT", "cs.CC", "cs.DC", "cs.DM", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity of Equilibrium in Diffusion Games on Social Networks. In this paper, we consider the competitive diffusion game, and study the existence of its pure-strategy Nash equilibrium when defined over general undirected networks. We first determine the set of pure-strategy Nash equilibria for two special but well-known classes of networks, namely the lattice and the hypercube. Characterizing the utility of the players in terms of graphical distances of their initial seed placements to other nodes in the network, we show that in general networks the decision process on the existence of pure-strategy Nash equilibrium is an NP-hard problem. Following this, we provide some necessary conditions for a given profile to be a Nash equilibrium. Furthermore, we study players' utilities in the competitive diffusion game over Erdos-Renyi random graphs and show that as the size of the network grows, the utilities of the players are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network. Finally, we obtain a lower bound for the maximum social welfare of the game with two players, and study sub-modularity of the players' utilities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the competitive diffusion game on social networks, what is the relationship between the size of the network and the concentration of players' utilities around their expectation?\n\n**A)** As the size of the network grows, the utilities of the players become more dispersed and less concentrated around their expectation.\n\n**B)** The utilities of the players in the competitive diffusion game over Erdos-Renyi random graphs are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network.\n\n**C)** The size of the network has no effect on the concentration of players' utilities around their expectation.\n\n**D)** The utilities of the players in the competitive diffusion game over Erdos-Renyi random graphs become more dispersed and less concentrated around their expectation as the size of the network grows.\n\n**Correct Answer:** B) The utilities of the players in the competitive diffusion game over Erdos-Renyi random graphs are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network.\n\n**Explanation:** The correct answer is based on the statement in the paper that \"as the size of the network grows, the utilities of the players are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network.\" This indicates that the size of the network has a positive effect on the concentration of players' utilities around their expectation."}, "46": {"documentation": {"title": "Simultaneous Identification and Control Using Active Signal Injection\n  for Series Hybrid Electric Vehicles based on Dynamic Programming", "source": "Haojie Zhu, Ziyou Song, Jun Hou, Heath Hofmann, Jing Sun", "docs_id": "1909.08062", "section": ["eess.SY", "cs.SY", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simultaneous Identification and Control Using Active Signal Injection\n  for Series Hybrid Electric Vehicles based on Dynamic Programming. Hybrid electric vehicles (HEVs) have an over-actuated system by including two power sources, a battery pack and an internal combustion engine. This feature of HEV is exploited in this paper to simultaneously achieve accurate identification of battery parameters/states. By actively injecting current signals, state of charge, state of health, and other battery parameters can be estimated in a specific sequence to improve the identification performance when compared to the case where all parameters and states are estimated concurrently using the baseline current signals. A dynamic programming strategy is developed to provide the benchmark results about how to balance the conflicting objectives corresponding to identification and system efficiency. The tradeoff between different objectives is presented to optimize the current profile so that the richness of signal can be ensured and the fuel economy can be optimized. In addition, simulation results show that the Root-Mean-Square error of the estimation can be decreased by up to 100% at a cost of less than 2% increase in fuel consumption. With the proposed simultaneous identification and control algorithm, the parameters/states of the battery can be monitored to ensure safe and efficient application of the battery for HEVs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Series Hybrid Electric Vehicles (HEVs), what is the primary advantage of using an active signal injection strategy for simultaneous identification and control of battery parameters/states, as proposed in the paper \"Simultaneous Identification and Control Using Active Signal Injection\"?\n\n**A)** The active signal injection strategy improves the accuracy of battery state of charge estimation by 20%.\n\n**B)** The active signal injection strategy reduces the Root-Mean-Square error of battery parameter estimation by up to 100% while increasing fuel consumption by less than 2%.\n\n**C)** The active signal injection strategy allows for concurrent estimation of all battery parameters and states, resulting in improved system efficiency.\n\n**D)** The active signal injection strategy is limited to estimating only the state of health of the battery.\n\n**Correct Answer:** B) The active signal injection strategy reduces the Root-Mean-Square error of battery parameter estimation by up to 100% while increasing fuel consumption by less than 2%.\n\n**Explanation:** The correct answer is B) because the paper states that the simulation results show that the Root-Mean-Square error of the estimation can be decreased by up to 100% at a cost of less than 2% increase in fuel consumption. This indicates that the active signal injection strategy is effective in improving the accuracy of battery parameter estimation while also optimizing fuel economy."}, "47": {"documentation": {"title": "On The Projection Operator to A Three-view Cardinality Constrained Set", "source": "Haichuan Yang, Shupeng Gui, Chuyang Ke, Daniel Stefankovic, Ryohei\n  Fujimaki, and Ji Liu", "docs_id": "1703.07345", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On The Projection Operator to A Three-view Cardinality Constrained Set. The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardinality constraints. In this paper, we consider the scenario where the overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection into a linear programming, and show that for TVCS, the vertex solution of this linear programming is the solution for the original projection problem. We further prove that such solution can be found with the complexity proportional to the number of variables and constraints. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a cardinality constrained problem with multiple overlapped constraints, where the constraints satisfy the Three-view Cardinality Structure (TVCS). What is the complexity of finding the vertex solution to the linear programming formulation of this problem?\n\n**A)** O(n^2), where n is the number of variables\n**B)** O(n), where n is the number of variables\n**C)** O(n log n), where n is the number of variables\n**D)** O(n^3), where n is the number of variables\n\n**Correct Answer:** B) O(n), where n is the number of variables\n\n**Explanation:** According to the paper, the complexity of finding the vertex solution to the linear programming formulation of the problem is proportional to the number of variables and constraints. Since the number of variables is n, the complexity is O(n). The TVCS assumption allows for a more efficient solution, but the overall complexity remains linear in the number of variables."}, "48": {"documentation": {"title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for\n  End-to-End Monaural Speech Separation", "source": "Jingjing Chen, Qirong Mao, Dong Liu", "docs_id": "2007.13975", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dual-Path Transformer Network: Direct Context-Aware Modeling for\n  End-to-End Monaural Speech Separation. The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPTNet) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of the dominant speech separation models, according to the paper, and how does the proposed Dual-Path Transformer Network (DPTNet) address this limitation?\n\nA) The dominant speech separation models are limited by the need for positional encodings in the transformer architecture, which prevents them from modeling speech sequences with direct context-awareness. DPTNet addresses this limitation by incorporating a recurrent neural network into the original transformer.\nB) The dominant speech separation models are limited by the need for complex recurrent or convolutional neural networks, which model speech sequences indirectly and lead to suboptimal separation performance. DPTNet addresses this limitation by introducing a direct context-aware modeling approach using a transformer architecture.\nC) The dominant speech separation models are limited by the need for extremely long speech sequence modeling, which is addressed by the dual-path structure of DPTNet, which enables efficient modeling of long speech sequences.\nD) The dominant speech separation models are limited by the need for intermediate states in recurrent neural networks, which are unnecessary for direct context-aware modeling. DPTNet addresses this limitation by eliminating the need for intermediate states.\n\nCorrect Answer: B) The dominant speech separation models are limited by the need for complex recurrent or convolutional neural networks, which model speech sequences indirectly and lead to suboptimal separation performance. DPTNet addresses this limitation by introducing a direct context-aware modeling approach using a transformer architecture."}, "49": {"documentation": {"title": "The statistical geometry of material loops in turbulence", "source": "Lukas Bentkamp, Theodore D. Drivas, Cristian C. Lalescu, Michael\n  Wilczek", "docs_id": "2106.11622", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The statistical geometry of material loops in turbulence. Material elements - which are lines, surfaces, or volumes behaving as passive, non-diffusive markers of dye - provide an inherently geometric window into the intricate dynamics of chaotic flows. Their stretching and folding dynamics has immediate implications for mixing in the oceans or the atmosphere, as well as the emergence of self-sustained dynamos in astrophysical settings. Here, we uncover robust statistical properties of an ensemble of material loops in a turbulent environment. Our approach combines high-resolution direct numerical simulations of Navier-Stokes turbulence, stochastic models, and dynamical systems techniques to reveal predictable, universal features of these complex objects. We show that the loop curvature statistics become stationary through a dynamical formation process of high-curvature slings, leading to distributions with power-law tails whose exponents are determined by the large-deviations statistics of finite-time Lyapunov exponents of the background flow. This prediction applies to advected material lines in a broad range of chaotic flows. To complement this dynamical picture, we confirm our theory in the analytically tractable Kraichnan model with an exact Fokker-Planck approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mechanism that leads to the formation of high-curvature slings in material loops in turbulent environments, and how does it relate to the large-deviations statistics of finite-time Lyapunov exponents of the background flow?\n\nA) The material loops undergo a stochastic process that leads to the formation of high-curvature slings, resulting in power-law tails in their curvature statistics.\nB) The high-curvature slings are a result of the material loops interacting with the background flow, leading to a dynamical formation process that is governed by the large-deviations statistics of finite-time Lyapunov exponents.\nC) The material loops are advected by the background flow, which leads to the formation of high-curvature slings through a process that is determined by the large-deviations statistics of finite-time Lyapunov exponents.\nD) The high-curvature slings are a result of the material loops undergoing a process of stretching and folding, which is governed by the Navier-Stokes equations and leads to power-law tails in their curvature statistics.\n\nCorrect Answer: B) The high-curvature slings are a result of the material loops interacting with the background flow, leading to a dynamical formation process that is governed by the large-deviations statistics of finite-time Lyapunov exponents.\n\nExplanation: The correct answer is B) because the documentation states that the loop curvature statistics become stationary through a dynamical formation process of high-curvature slings, which is led by the interaction between the material loops and the background flow. This process is governed by the large-deviations statistics of finite-time Lyapunov exponents of the background flow, which is a key concept in the paper. The other options are incorrect because they either oversimplify the process or misrepresent the relationship between the material loops and the background flow."}, "50": {"documentation": {"title": "Escaping Arrow's Theorem: The Advantage-Standard Model", "source": "Wesley H. Holliday and Mikayla Kelley", "docs_id": "2108.01134", "section": ["econ.TH", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Escaping Arrow's Theorem: The Advantage-Standard Model. There is an extensive literature in social choice theory studying the consequences of weakening the assumptions of Arrow's Impossibility Theorem. Much of this literature suggests that there is no escape from Arrow-style impossibility theorems unless one drastically violates the Independence of Irrelevant Alternatives (IIA). In this paper, we present a more positive outlook. We propose a model of comparing candidates in elections, which we call the Advantage-Standard (AS) model. The requirement that a collective choice rule (CCR) be rationalizable by the AS model is in the spirit of but weaker than IIA; yet it is stronger than what is known in the literature as weak IIA (two profiles alike on x, y cannot have opposite strict social preferences on x and y). In addition to motivating violations of IIA, the AS model makes intelligible violations of another Arrovian assumption: the negative transitivity of the strict social preference relation P. While previous literature shows that only weakening IIA to weak IIA or only weakening negative transitivity of P to acyclicity still leads to impossibility theorems, we show that jointly weakening IIA to AS rationalizability and weakening negative transitivity of P leads to no such impossibility theorems. Indeed, we show that several appealing CCRs are AS rationalizable, including even transitive CCRs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a voting system where three candidates, A, B, and C, are running for office. The voters' preferences are as follows:\n\n* Voter 1 prefers A > B > C\n* Voter 2 prefers B > A > C\n* Voter 3 prefers C > A > B\n\nWhich of the following collective choice rules (CCRs) is AS-rationalizable?\n\nA) The CCR that always chooses the candidate with the most first-place votes.\nB) The CCR that chooses the candidate with the highest average ranking.\nC) The CCR that chooses the candidate with the most second-place votes.\nD) The CCR that chooses the candidate with the highest average ranking, unless two or more candidates are tied, in which case it chooses the candidate with the most first-place votes.\n\nCorrect Answer: B) The CCR that chooses the candidate with the highest average ranking.\n\nExplanation: The AS model requires that the CCR be rationalizable by the Advantage-Standard model, which is weaker than IIA but stronger than weak IIA. The CCR in option B is AS-rationalizable because it is based on a simple and intuitive criterion (average ranking) that is not sensitive to irrelevant alternatives. In contrast, the other options are not AS-rationalizable because they either prioritize first-place votes (A) or second-place votes (C), which can lead to violations of IIA, or are ambiguous in the case of ties (D)."}, "51": {"documentation": {"title": "Arbitrary Pattern Formation by Opaque Fat Robots with Lights", "source": "Kaustav Bose, Ranendu Adhikary, Manash Kumar Kundu, Buddhadeb Sau", "docs_id": "1910.02706", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Arbitrary Pattern Formation by Opaque Fat Robots with Lights. Arbitrary Pattern Formation is a widely studied problem in autonomous robot systems. The problem asks to design a distributed algorithm that moves a team of autonomous, anonymous and identical mobile robots to form any arbitrary pattern given as input. The majority of the existing literature investigates this problem for robots with unobstructed visibility. In a few recent works, the problem has been studied in the obstructed visibility model, where the view of a robot can be obstructed by the presence of other robots. However, in these works, the robots have been modelled as dimensionless points in the plane. In this paper, we have considered the problem in the more realistic setting where the robots have a physical extent. In particular, the robots are modelled as opaque disks. Furthermore, the robots operate under a fully asynchronous scheduler. They do not have access to any global coordinate system, but agree on the direction and orientation of one coordinate axis. Each robot is equipped with an externally visible light which can assume a constant number of predefined colors. In this setting, we have given a complete characterization of initial configurations from where any arbitrary pattern can be formed by a deterministic distributed algorithm."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Arbitrary Pattern Formation problem, what is the key difference between the existing literature on the problem and the approach taken in the paper \"Arbitrary Pattern Formation by Opaque Fat Robots with Lights\"?\n\nA) The existing literature assumes a fully asynchronous scheduler, while the paper assumes a deterministic scheduler.\nB) The existing literature models robots as dimensionless points, while the paper models robots as opaque disks.\nC) The existing literature investigates the problem for robots with unobstructed visibility, while the paper investigates the problem for robots with obstructed visibility.\nD) The existing literature assumes a constant number of predefined colors for the robots' lights, while the paper assumes a variable number of colors.\n\n**Correct Answer:** C) The existing literature investigates the problem for robots with unobstructed visibility, while the paper investigates the problem for robots with obstructed visibility.\n\n**Explanation:** The correct answer is C because the paper specifically mentions that the majority of existing literature investigates the problem for robots with unobstructed visibility, whereas the paper addresses the problem in the obstructed visibility model. This highlights the key difference between the two approaches.\n\n**Explanation for incorrect options:**\n\nA) The paper does not assume a deterministic scheduler, but rather a fully asynchronous scheduler, which is the same as the existing literature.\n\nB) While the paper does model robots as opaque disks, this is not the key difference between the existing literature and the paper.\n\nD) The paper does not assume a variable number of colors for the robots' lights, but rather a constant number of predefined colors."}, "52": {"documentation": {"title": "Fourier spectra from exoplanets with polar caps and ocean glint", "source": "P.M. Visser and F.J. van de Bult", "docs_id": "1503.05059", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fourier spectra from exoplanets with polar caps and ocean glint. The weak orbital-phase dependent reflection signal of an exoplanet contains information on the planet surface, such as the distribution of continents and oceans on terrestrial planets. This light curve is usually studied in the time domain, but because the signal from a stationary surface is (quasi)periodic, analysis of the Fourier series may provide an alternative, complementary approach. We study Fourier spectra from reflected light curves for geometrically simple configurations. Depending on its atmospheric properties, a rotating planet in the habitable zone could have circular polar ice caps. Tidally locked planets, on the other hand, may have symmetric circular oceans facing the star. These cases are interesting because the high-albedo contrast at the sharp edges of the ice-sheets and the glint from the host star in the ocean may produce recognizable light curves with orbital periodicity, which could also be interpreted in the Fourier domain. We derive a simple general expression for the Fourier coefficients of a quasiperiodic light curve in terms of the albedo map of a Lambertian planet surface. Analytic expressions for light curves and their spectra are calculated for idealized situations, and dependence of spectral peaks on the key parameters inclination, obliquity, and cap size is studied."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the Fourier spectra from exoplanets with polar caps and ocean glint, considering the effects of inclination, obliquity, and cap size on the spectral peaks. What is the relationship between the Fourier coefficients and the albedo map of a Lambertian planet surface?\n\n**A)** The Fourier coefficients are directly proportional to the square of the albedo map, with no dependence on inclination, obliquity, or cap size.\n\n**B)** The Fourier coefficients are inversely proportional to the albedo map, with a dependence on inclination, obliquity, and cap size that can be described by a simple analytical expression.\n\n**C)** The Fourier coefficients are independent of the albedo map, with a dependence on inclination, obliquity, and cap size that can be described by a complex analytical expression involving multiple parameters.\n\n**D)** The Fourier coefficients are proportional to the albedo map, with a dependence on inclination, obliquity, and cap size that can be described by a simple linear expression.\n\n**Correct Answer:** B) The Fourier coefficients are inversely proportional to the albedo map, with a dependence on inclination, obliquity, and cap size that can be described by a simple analytical expression.\n\n**Explanation:** The correct answer is based on the fact that the Fourier coefficients are inversely proportional to the albedo map, as derived in the study. The dependence on inclination, obliquity, and cap size is also considered, and the analytical expression is found to be simple, making option B the correct answer."}, "53": {"documentation": {"title": "Improving the Integrality Gap for Multiway Cut", "source": "Krist\\'of B\\'erczi, Karthekeyan Chandrasekaran, Tam\\'as Kir\\'aly,\n  Vivek Madan", "docs_id": "1807.09735", "section": ["cs.DS", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving the Integrality Gap for Multiway Cut. In the multiway cut problem, we are given an undirected graph with non-negative edge weights and a collection of $k$ terminal nodes, and the goal is to partition the node set of the graph into $k$ non-empty parts each containing exactly one terminal so that the total weight of the edges crossing the partition is minimized. The multiway cut problem for $k\\ge 3$ is APX-hard. For arbitrary $k$, the best-known approximation factor is $1.2965$ due to [Sharma and Vondr\\'{a}k, 2014] while the best known inapproximability factor is $1.2$ due to [Angelidakis, Makarychev and Manurangsi, 2017]. In this work, we improve on the lower bound to $1.20016$ by constructing an integrality gap instance for the CKR relaxation. A technical challenge in improving the gap has been the lack of geometric tools to understand higher-dimensional simplices. Our instance is a non-trivial $3$-dimensional instance that overcomes this technical challenge. We analyze the gap of the instance by viewing it as a convex combination of $2$-dimensional instances and a uniform 3-dimensional instance. We believe that this technique could be exploited further to construct instances with larger integrality gap. One of the ingredients of our proof technique is a generalization of a result on \\emph{Sperner admissible labelings} due to [Mirzakhani and Vondr\\'{a}k, 2015] that might be of independent combinatorial interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Improving the Integrality Gap for Multiway Cut\n\n**A)** The best-known approximation factor for the multiway cut problem is 1.2965, which was obtained by Sharma and Vondr\\'{a}k in 2014.\n\n**B)** The multiway cut problem for $k\\ge 3$ is APX-hard, and the best-known inapproximability factor is 1.2, due to Angelidakis, Makarychev, and Manurangsi in 2017.\n\n**C)** The author of the paper improves on the lower bound to 1.20016 by constructing an integrality gap instance for the CKR relaxation, which is a non-trivial 3-dimensional instance.\n\n**D)** The technique used to analyze the gap of the instance involves viewing it as a convex combination of 2-dimensional instances and a uniform 3-dimensional instance, which might be exploited further to construct instances with larger integrality gap.\n\n**Correct Answer:** C) The author of the paper improves on the lower bound to 1.20016 by constructing an integrality gap instance for the CKR relaxation, which is a non-trivial 3-dimensional instance.\n\n**Explanation:** This question requires the test-taker to understand the main contribution of the paper, which is to improve the lower bound of the integrality gap for the multiway cut problem. The correct answer is C, as it accurately summarizes the main result of the paper. The other options are incorrect because they either provide outdated information (A), mention a different aspect of the problem (B), or describe a different technique used in the paper (D)."}, "54": {"documentation": {"title": "The spatial dissemination of COVID-19 and associated socio-economic\n  consequences", "source": "Yafei Zhang, Lin Wang, Jonathan J. H. Zhu, Xiaofan Wang", "docs_id": "2104.08213", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The spatial dissemination of COVID-19 and associated socio-economic\n  consequences. The ongoing coronavirus disease 2019 (COVID-19) pandemic has wreaked havoc worldwide with millions of lives claimed, human travel restricted, and economic development halted. Leveraging city-level mobility and case data across mainland China, our analysis shows that the spatial dissemination of COVID-19 in mainland China can be well explained by the human migration from Wuhan and there will be very different outcomes if the COVID-19 outbreak occurred in other cities. For example, the outbreak in Beijing or Guangzhou would result in a $\\sim$90% increase of COVID-19 cases at the end of the Chinese New Year holiday. After the implementation of a series of control measures, human mobility had experienced substantial changes toward containing the spread of COVID-19. Our results also suggest an inequality of economic deprivation as less developed areas generally suffered more severe economic recession during the COVID-19. Intuitively, it's anticipated that cities with more confirmed cases would suffer more economic losses. However, for cities outside of Hubei province, we don't observe such a phenomenon. Our work has important implications for the mitigation of disease and the reevaluation of the social and economic consequences of COVID-19 on our society."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the spatial dissemination of COVID-19 in mainland China, and explain the implications of the results for the mitigation of disease and the reevaluation of social and economic consequences.\n\n**A)** The study suggests that the spatial dissemination of COVID-19 in mainland China can be attributed solely to human migration from Wuhan, and that this factor is the primary driver of the pandemic's spread.\n\n**B)** The analysis reveals that cities outside of Hubei province experienced more severe economic recession during the COVID-19 pandemic, highlighting the importance of regional disparities in economic development.\n\n**C)** The study's findings indicate that the implementation of control measures had a significant impact on containing the spread of COVID-19, and that human mobility patterns played a crucial role in mitigating the pandemic's effects.\n\n**D)** The research suggests that the socio-economic consequences of COVID-19 are more pronounced in less developed areas, and that the pandemic has exacerbated existing inequalities in economic development.\n\n**Correct Answer:** C) The study's findings indicate that the implementation of control measures had a significant impact on containing the spread of COVID-19, and that human mobility patterns played a crucial role in mitigating the pandemic's effects.\n\n**Explanation:** The correct answer is supported by the text, which states that \"human mobility had experienced substantial changes toward containing the spread of COVID-19\" after the implementation of control measures. Additionally, the text notes that \"our results also suggest an inequality of economic deprivation as less developed areas generally suffered more severe economic recession during the COVID-19.\" This suggests that the study found that control measures were effective in mitigating the pandemic's effects, and that human mobility patterns played a crucial role in this process."}, "55": {"documentation": {"title": "The nature of the progenitor of the M31 North-western stream: globular\n  clusters as milestones of its orbit", "source": "Takanobu Kirihara, Yohei Miki, Masao Mori", "docs_id": "1705.03030", "section": ["astro-ph.GA", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The nature of the progenitor of the M31 North-western stream: globular\n  clusters as milestones of its orbit. We examine the nature, possible orbits and physical properties of the progenitor of the North-western stellar stream (NWS) in the halo of the Andromeda galaxy (M31). The progenitor is assumed to be an accreting dwarf galaxy with globular clusters (GCs). It is, in general, difficult to determine the progenitor's orbit precisely because of many necessary parameters. Recently, Veljanoski et al. 2014 reported five GCs whose positions and radial velocities suggest an association with the stream. We use this data to constrain the orbital motions of the progenitor using test-particle simulations. Our simulations split the orbit solutions into two branches according to whether the stream ends up in the foreground or in the background of M31. Upcoming observations that will determine the distance to the NWS will be able to reject one of the two branches. In either case, the solutions require that the pericentric radius of any possible orbit be over 2 kpc. We estimate the efficiency of the tidal disruption and confirm the consistency with the assumption for the progenitor being a dwarf galaxy. The progenitor requires the mass $\\ga 2\\times10^6 M_{\\sun}$ and half-light radius $\\ga 30$ pc. In addition, $N$-body simulations successfully reproduce the basic observed features of the NWS and the GCs' line-of-sight velocities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the minimum mass required for the progenitor of the North-western stellar stream (NWS) in the halo of the Andromeda galaxy (M31) to be a dwarf galaxy, according to the test-particle simulations?\n\nA) $10^6 M_{\\sun}$\nB) $2\\times10^6 M_{\\sun}$\nC) $5\\times10^6 M_{\\sun}$\nD) $10^7 M_{\\sun}$\n\nCorrect Answer: B) $2\\times10^6 M_{\\sun}$\n\nExplanation: The text states that the progenitor requires a mass of $\\ga 2\\times10^6 M_{\\sun}$ to be a dwarf galaxy. This is a key constraint on the nature of the progenitor, and is used to constrain the orbital motions of the progenitor using test-particle simulations."}, "56": {"documentation": {"title": "How Covid-19 Pandemic Changes the Theory of Economics?", "source": "Matti Estola", "docs_id": "2012.04571", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How Covid-19 Pandemic Changes the Theory of Economics?. During its history, the ultimate goal of economics has been to develop similar frameworks for modeling economic behavior as invented in physics. This has not been successful, however, and current state of the process is the neoclassical framework that bases on static optimization. By using a static framework, however, we cannot model and forecast the time paths of economic quantities because for a growing firm or a firm going into bankruptcy, a positive profit maximizing flow of production does not exist. Due to these problems, we present a dynamic theory for the production of a profit-seeking firm where the adjustment may be stable or unstable. This is important, currently, because we should be able to forecast the possible future bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can solve the time moment of bankruptcy of a firm as a function of several parameters. The proposed model is mathematically identical with Newtonian model of a particle moving in a resisting medium, and so the model explains the reasons that stop the motion too. The frameworks for modeling dynamic events in physics are thus applicable in economics, and we give reasons why physics is more important for the development of economics than pure mathematics. (JEL D21, O12) Keywords: Limitations of neoclassical framework, Dynamics of production, Economic force, Connections between economics and physics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"How Covid-19 Pandemic Changes the Theory of Economics?\" argue that the neoclassical framework in economics is limited in modeling dynamic economic behavior. They propose a dynamic theory for the production of a profit-seeking firm, which is mathematically identical to the Newtonian model of a particle moving in a resisting medium. What are the key implications of this new dynamic theory for the modeling of economic behavior, and how does it address the limitations of the neoclassical framework?\n\n**A)** The new dynamic theory implies that economic behavior is subject to random fluctuations, which can lead to unpredictable outcomes. This challenges the traditional assumption of rational behavior in economics.\n\n**B)** The new dynamic theory suggests that economic systems are subject to external forces, such as government policies and technological changes, which can affect the stability of economic growth. This highlights the importance of considering external factors in economic modeling.\n\n**C)** The new dynamic theory implies that economic agents are not solely driven by self-interest, but are also influenced by social and environmental factors. This challenges the traditional assumption of individual rationality in economics.\n\n**D)** The new dynamic theory provides a framework for modeling the time paths of economic quantities, including the possibility of bankruptcy and firm failure. This allows for more accurate forecasting and policy-making in the face of economic uncertainty.\n\n**Correct Answer:** D) The new dynamic theory provides a framework for modeling the time paths of economic quantities, including the possibility of bankruptcy and firm failure. This allows for more accurate forecasting and policy-making in the face of economic uncertainty.\n\n**Explanation:** The correct answer is D) because the new dynamic theory, as proposed by the authors, aims to address the limitations of the neoclassical framework by modeling the time paths of economic quantities, including the possibility of bankruptcy and firm failure. This allows for more accurate forecasting and policy-making in the face of economic uncertainty, which is particularly relevant in the context of the Covid-19 pandemic. The other options, while related to the topic, do not accurately capture the main implication of the new dynamic theory."}, "57": {"documentation": {"title": "Generation of photoluminescent ultrashort carbon nanotubes through\n  nanoscale exciton localization at sp3 -defect sites", "source": "No\\'emie Dann\\'e (LP2N), Mijin Kim, Antoine Godin (LP2N), Hyejin Kwon,\n  Zhenghong Gao (LP2N), Xiaojian Wu, Nicolai Hartmann (LANL), Stephen Doorn\n  (LANL), Brahim Lounis (LP2N), Yuhuang Wang, Laurent Cognet (LP2N)", "docs_id": "1803.03535", "section": ["physics.app-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generation of photoluminescent ultrashort carbon nanotubes through\n  nanoscale exciton localization at sp3 -defect sites. The intrinsic near-infrared photoluminescence observed in long single walled carbon nanotubes is systematically quenched in ultrashort single-walled carbon nanotubes (usCNTs, below 100 nm length) due to their short dimension as compared to the exciton diffusion length. It would however be key for number of applications to have such tiny nanostructure displaying photoluminescence emission to complement their unique physical, chemical and biological properties. Here we demonstrate that intense photoluminescence can be created in usCNTs (~40 nm length) upon incorporation of emissive sp3-defect sites in order to trap excitons. Using super-resolution imaging at <25 nm resolution, we directly reveal the localization of excitons at the defect sites on individual usCNTs. They are found preferentially localized at nanotube ends which can be separated by less than 40 nm and behave as independent emitters. The demonstration and control of bright near-infrared photoluminescence in usCNTs through exciton trapping opens the possibility to engineering tiny carbon nanotubes for applications in various domains of research including quantum optics and bioimaging."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the systematic quenching of intrinsic near-infrared photoluminescence in ultrashort single-walled carbon nanotubes (usCNTs) compared to long single-walled carbon nanotubes?\n\nA) The short dimension of usCNTs causes a decrease in the number of excitons available for emission.\nB) The short dimension of usCNTs leads to a decrease in the exciton diffusion length, resulting in a loss of photoluminescence.\nC) The short dimension of usCNTs causes a decrease in the surface area available for exciton localization.\nD) The short dimension of usCNTs leads to an increase in the number of defects, which quench the photoluminescence.\n\nCorrect Answer: B) The short dimension of usCNTs leads to a decrease in the exciton diffusion length, resulting in a loss of photoluminescence.\n\nExplanation: The correct answer is based on the understanding that the exciton diffusion length is a critical factor in determining the photoluminescence of carbon nanotubes. The documentation states that the intrinsic near-infrared photoluminescence observed in long single-walled carbon nanotubes is systematically quenched in ultrashort single-walled carbon nanotubes due to their short dimension as compared to the exciton diffusion length. This implies that the short dimension of usCNTs leads to a decrease in the exciton diffusion length, resulting in a loss of photoluminescence."}, "58": {"documentation": {"title": "Decentralized multi-agent reinforcement learning with shared actions", "source": "Rajesh K Mishra, Deepanshu Vasal, Sriram Vishwanath", "docs_id": "2003.10185", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decentralized multi-agent reinforcement learning with shared actions. In this paper, we propose a novel model-free reinforcement learning algorithm to compute the optimal policies for a multi-agent system with $N$ cooperative agents where each agent privately observes it's own private type and publicly observes each others' actions. The goal is to maximize their collective reward. The problem belongs to the broad class of decentralized control problems with partial information. We use the common agent approach wherein some fictitious common agent picks the best policy based on a belief on the current states of the agents. These beliefs are updated individually for each agent from their current belief and action histories. Belief state updates without the knowledge of system dynamics is a challenge. In this paper, we employ particle filters called the bootstrap filter distributively across agents to update the belief. We provide a model-free reinforcement learning (RL) method for this multi-agent partially observable Markov decision processes using the particle filter and sampled trajectories to estimate the optimal policies for the agents. We showcase our results with the help of a smartgrid application where the users strive to reduce collective cost of power for all the agents in the grid. Finally, we compare the performances for model and model-free implementation of the RL algorithm establishing the effectiveness of particle filter (pf) method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Decentralized multi-agent reinforcement learning with shared actions\", what is the primary challenge in updating the belief state of the fictitious common agent, and how does the proposed particle filter method address this challenge?\n\n**A)** The primary challenge is the lack of knowledge of system dynamics, and the particle filter method addresses this by using a distributed approach to update the belief state.\n\n**B)** The primary challenge is the need for a centralized controller to update the belief state, and the particle filter method addresses this by using a decentralized approach to update the belief state.\n\n**C)** The primary challenge is the high dimensionality of the state space, and the particle filter method addresses this by using a hierarchical representation of the state space.\n\n**D)** The primary challenge is the need for a model of the system dynamics, and the particle filter method addresses this by using a model-free approach to update the belief state.\n\n**Correct Answer:** A) The primary challenge is the lack of knowledge of system dynamics, and the particle filter method addresses this by using a distributed approach to update the belief state.\n\n**Explanation:** The paper mentions that \"Belief state updates without the knowledge of system dynamics is a challenge\" in the context of decentralized control problems with partial information. The particle filter method is proposed as a solution to address this challenge, and it is described as being \"distributively across agents\" to update the belief state. Therefore, option A is the correct answer."}, "59": {"documentation": {"title": "Relative Distributed Formation and Obstacle Avoidance with Multi-agent\n  Reinforcement Learning", "source": "Yuzi Yan, Xiaoxiang Li, Xinyou Qiu, Jiantao Qiu, Jian Wang, Yu Wang,\n  Yuan Shen", "docs_id": "2111.07334", "section": ["eess.SY", "cs.AI", "cs.LG", "cs.MA", "cs.RO", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relative Distributed Formation and Obstacle Avoidance with Multi-agent\n  Reinforcement Learning. Multi-agent formation as well as obstacle avoidance is one of the most actively studied topics in the field of multi-agent systems. Although some classic controllers like model predictive control (MPC) and fuzzy control achieve a certain measure of success, most of them require precise global information which is not accessible in harsh environments. On the other hand, some reinforcement learning (RL) based approaches adopt the leader-follower structure to organize different agents' behaviors, which sacrifices the collaboration between agents thus suffering from bottlenecks in maneuverability and robustness. In this paper, we propose a distributed formation and obstacle avoidance method based on multi-agent reinforcement learning (MARL). Agents in our system only utilize local and relative information to make decisions and control themselves distributively. Agent in the multi-agent system will reorganize themselves into a new topology quickly in case that any of them is disconnected. Our method achieves better performance regarding formation error, formation convergence rate and on-par success rate of obstacle avoidance compared with baselines (both classic control methods and another RL-based method). The feasibility of our method is verified by both simulation and hardware implementation with Ackermann-steering vehicles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of multi-agent systems, what is the primary limitation of classic control methods such as Model Predictive Control (MPC) and Fuzzy Control in achieving robust and maneuverable formations, and how does the proposed MARL approach address this limitation?\n\n**A)** Classic control methods require precise global information, which is not accessible in harsh environments, leading to limitations in formation error and convergence rate. MARL approaches, on the other hand, utilize local and relative information to make decisions, allowing for more robust and maneuverable formations.\n\n**B)** Classic control methods are limited by their inability to adapt to changing environments, whereas MARL approaches can reorganize themselves into new topologies quickly in response to disconnections. However, this comes at the cost of sacrificing collaboration between agents.\n\n**C)** The proposed MARL approach achieves better performance regarding formation error and convergence rate compared to baselines, but its success rate in obstacle avoidance is comparable to another RL-based method. This suggests that MARL approaches may not be the most effective solution for all multi-agent systems.\n\n**D)** The primary limitation of classic control methods is their inability to handle complex, dynamic environments, whereas MARL approaches can effectively utilize local and relative information to make decisions. However, this approach may not be suitable for applications where precise global information is required.\n\n**Correct Answer:** A) Classic control methods require precise global information, which is not accessible in harsh environments, leading to limitations in formation error and convergence rate. MARL approaches, on the other hand, utilize local and relative information to make decisions, allowing for more robust and maneuverable formations.\n\n**Explanation:** This question requires the test-taker to understand the limitations of classic control methods in achieving robust and maneuverable formations, and how the proposed MARL approach addresses these limitations. The correct answer highlights the key advantage of MARL approaches, which is their ability to utilize local and relative information to make decisions, allowing for more robust and maneuverable formations. The incorrect answers provide distractors that test the test-taker's understanding of the topic, but ultimately lead to incorrect conclusions."}}