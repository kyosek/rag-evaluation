{"0": {"documentation": {"title": "Dual-domain Cascade of U-nets for Multi-channel Magnetic Resonance Image\n  Reconstruction", "source": "Roberto Souza, Mariana Bento, Nikita Nogovitsyn, Kevin J. Chung, R.\n  Marc Lebel and Richard Frayne", "docs_id": "1911.01458", "section": ["eess.IV", "cs.LG", "physics.med-ph", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dual-domain Cascade of U-nets for Multi-channel Magnetic Resonance Image\n  Reconstruction. The U-net is a deep-learning network model that has been used to solve a number of inverse problems. In this work, the concatenation of two-element U-nets, termed the W-net, operating in k-space (K) and image (I) domains, were evaluated for multi-channel magnetic resonance (MR) image reconstruction. The two element network combinations were evaluated for the four possible image-k-space domain configurations: a) W-net II, b) W-net KK, c) W-net IK, and d) W-net KI were evaluated. Selected promising four element networks (WW-nets) were also examined. Two configurations of each network were compared: 1) Each coil channel processed independently, and 2) all channels processed simultaneously. One hundred and eleven volumetric, T1-weighted, 12-channel coil k-space datasets were used in the experiments. Normalized root mean squared error, peak signal to noise ratio, visual information fidelity and visual inspection were used to assess the reconstructed images against the fully sampled reference images. Our results indicated that networks that operate solely in the image domain are better suited when processing individual channels of multi-channel data independently. Dual domain methods are more advantageous when simultaneously reconstructing all channels of multi-channel data. Also, the appropriate cascade of U-nets compared favorably (p < 0.01) to the previously published, state-of-the-art Deep Cascade model in in three out of four experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using dual-domain methods for multi-channel magnetic resonance image reconstruction, as opposed to processing individual channels independently?\n\nA) Dual-domain methods are more computationally efficient.\nB) Dual-domain methods are better suited for processing individual channels independently.\nC) Dual-domain methods are more advantageous when simultaneously reconstructing all channels of multi-channel data.\nD) Dual-domain methods are more suitable for reconstructing images with high spatial resolution.\n\nCorrect Answer: C) Dual-domain methods are more advantageous when simultaneously reconstructing all channels of multi-channel data.\n\nExplanation: According to the documentation, the results showed that dual-domain methods are more advantageous when simultaneously reconstructing all channels of multi-channel data, whereas networks that operate solely in the image domain are better suited when processing individual channels independently. This suggests that the correct answer is C."}, "1": {"documentation": {"title": "Isoscalar neutron-proton pairing and SU(4)-symmetry breaking in\n  Gamow-Teller transitions", "source": "K. Kaneko, Y. Sun, T. Mizusaki", "docs_id": "1805.06136", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isoscalar neutron-proton pairing and SU(4)-symmetry breaking in\n  Gamow-Teller transitions. The isoscalar neutron-proton pairing is thought to be important for nuclei with equal number of protons and neutrons but its manifestation in structure properties remains to be understood. We investigate the Gamow-Teller (GT) transitions for the f7/2-shell nuclei in large-scale shell-model calculations with the realistic Hamiltonian. We show that the isoscalar T=0, J=1+ neutron-proton pairing interaction plays a decisive role for the concentration of GT strengths at the first-excited 1+ state in 42Sc, and that the suppression of these strengths in 46V, 50Mn, and 54Co is mainly caused by the spin-orbit force supplemented by the quadrupole-quadrupole interaction. Based on the good reproduction of the charge-exchange reaction data, we further analyze the interplay between the isoscalar and isovector pairing correlations. We conclude that even for the most promising A=42 nuclei where the SU(4) isoscalar-isovector-pairing symmetry is less broken, the probability of forming an isoscalar neutron-proton pairing condensation is less than 60% as compared to the expectation at the SU(4)-symmetry limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the suppression of Gamow-Teller strengths in nuclei such as 46V, 50Mn, and 54Co, according to the study?\n\nA) The isoscalar neutron-proton pairing interaction is too weak\nB) The spin-orbit force and quadrupole-quadrupole interaction dominate the GT transition\nC) The SU(4)-symmetry is too strong, leading to a suppression of GT strengths\nD) The shell-model calculations are not realistic enough\n\nCorrect Answer: B) The spin-orbit force and quadrupole-quadrupole interaction dominate the GT transition\n\nExplanation: The study concludes that the suppression of GT strengths in certain nuclei is mainly caused by the spin-orbit force supplemented by the quadrupole-quadrupole interaction. This is evident from the statement \"the suppression of these strengths in 46V, 50Mn, and 54Co is mainly caused by the spin-orbit force supplemented by the quadrupole-quadrupole interaction.\""}, "2": {"documentation": {"title": "Multimodal Multipart Learning for Action Recognition in Depth Videos", "source": "Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang", "docs_id": "1507.08761", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multimodal Multipart Learning for Action Recognition in Depth Videos. The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary approach used in the proposed joint sparse regression based learning method for action recognition in depth videos, and how does it utilize structured sparsity to model each action?\n\nA) The method uses a heterogeneous set of depth and skeleton-based features to represent dynamics and appearance of body parts, but does not employ structured sparsity.\nB) The method employs a hierarchical mixed norm to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection.\nC) The method uses a sparse set of body parts to analyze actions based on partial descriptors, but does not utilize structured sparsity.\nD) The method uses a combination of depth and skeleton-based features to represent body parts, but does not employ a hierarchical mixed norm.\n\nCorrect Answer: B) The method employs a hierarchical mixed norm to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection.\n\nExplanation: The correct answer is B) because the proposed method utilizes a hierarchical mixed norm to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. This is stated in the documentation as \"to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection\". The other options are incorrect because they either omit the use of structured sparsity (A and C) or incorrectly describe the method's approach (D)."}, "3": {"documentation": {"title": "Lockout: Sparse Regularization of Neural Networks", "source": "Gilmer Valdes, Wilmer Arbelo, Yannet Interian, and Jerome H. Friedman", "docs_id": "2107.07160", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lockout: Sparse Regularization of Neural Networks. Many regression and classification procedures fit a parameterized function $f(x;w)$ of predictor variables $x$ to data $\\{x_{i},y_{i}\\}_1^N$ based on some loss criterion $L(y,f)$. Often, regularization is applied to improve accuracy by placing a constraint $P(w)\\leq t$ on the values of the parameters $w$. Although efficient methods exist for finding solutions to these constrained optimization problems for all values of $t\\geq0$ in the special case when $f$ is a linear function, none are available when $f$ is non-linear (e.g. Neural Networks). Here we present a fast algorithm that provides all such solutions for any differentiable function $f$ and loss $L$, and any constraint $P$ that is an increasing monotone function of the absolute value of each parameter. Applications involving sparsity inducing regularization of arbitrary Neural Networks are discussed. Empirical results indicate that these sparse solutions are usually superior to their dense counterparts in both accuracy and interpretability. This improvement in accuracy can often make Neural Networks competitive with, and sometimes superior to, state-of-the-art methods in the analysis of tabular data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a neural network with a non-linear function $f(x;w)$, where $x$ is a vector of predictor variables and $w$ is a vector of parameters. The loss criterion $L(y,f)$ is a differentiable function of the output $y$ and the parameters $w$. Suppose we want to apply a sparsity-inducing regularization constraint $P(w) \\leq t$ to the parameters $w$, where $P$ is an increasing monotone function of the absolute value of each parameter. What is the key assumption that the Lockout algorithm makes about the constraint $P$ in order to provide a solution to the constrained optimization problem?\n\n**A)** The constraint $P$ must be a linear function of the absolute value of each parameter.\n**B)** The constraint $P$ must be a convex function of the absolute value of each parameter.\n**C)** The constraint $P$ must be an increasing monotone function of the absolute value of each parameter, and the optimization problem must be convex.\n**D)** The constraint $P$ must be a decreasing monotone function of the absolute value of each parameter, and the optimization problem must be concave.\n\n**Correct Answer:** C) The constraint $P$ must be an increasing monotone function of the absolute value of each parameter, and the optimization problem must be convex.\n\n**Explanation:** The Lockout algorithm assumes that the constraint $P$ is an increasing monotone function of the absolute value of each parameter, which means that as the absolute value of a parameter increases, the constraint value also increases. Additionally, the optimization problem must be convex, which means that the loss function $L(y,f)$ and the constraint $P(w)$ must satisfy certain properties that ensure the existence of a solution. This assumption is crucial for the algorithm to provide a solution to the constrained optimization problem."}, "4": {"documentation": {"title": "Effects of neutrino oscillations on nucleosynthesis and neutrino signals\n  for an 18 M supernova model", "source": "Meng-Ru Wu, Yong-Zhong Qian, Gabriel Martinez-Pinedo, Tobias Fischer,\n  Lutz Huther", "docs_id": "1412.8587", "section": ["astro-ph.HE", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of neutrino oscillations on nucleosynthesis and neutrino signals\n  for an 18 M supernova model. In this paper, we explore the effects of neutrino flavor oscillations on supernova nucleosynthesis and on the neutrino signals. Our study is based on detailed information about the neutrino spectra and their time evolution from a spherically-symmetric supernova model for an 18 M progenitor. We find that collective neutrino oscillations are not only sensitive to the detailed neutrino energy and angular distributions at emission, but also to the time evolution of both the neutrino spectra and the electron density profile. We apply the results of neutrino oscillations to study the impact on supernova nucleosynthesis and on the neutrino signals from a Galactic supernova. We show that in our supernova model, collective neutrino oscillations enhance the production of rare isotopes 138La and 180Ta but have little impact on the nu p-process nucleosynthesis. In addition, the adiabatic MSW flavor transformation, which occurs in the C/O and He shells of the supernova, may affect the production of light nuclei such as 7Li and 11B. For the neutrino signals, we calculate the rate of neutrino events in the Super-Kamiokande detector and in a hypothetical liquid argon detector. Our results suggest the possibility of using the time profiles of the events in both detectors, along with the spectral information of the detected neutrinos, to infer the neutrino mass hierarchy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary effect of collective neutrino oscillations on the production of rare isotopes in a supernova model, according to the study presented in the Arxiv paper?\n\nA) Collective neutrino oscillations enhance the production of rare isotopes 138La and 180Ta, but have little impact on the nu p-process nucleosynthesis.\nB) Collective neutrino oscillations have no effect on the production of rare isotopes in a supernova model.\nC) Collective neutrino oscillations only affect the production of light nuclei such as 7Li and 11B.\nD) Collective neutrino oscillations reduce the production of rare isotopes 138La and 180Ta, and increase the production of light nuclei.\n\n**Correct Answer:** A) Collective neutrino oscillations enhance the production of rare isotopes 138La and 180Ta, but have little impact on the nu p-process nucleosynthesis.\n\n**Explanation:** The study presented in the Arxiv paper finds that collective neutrino oscillations enhance the production of rare isotopes 138La and 180Ta, but have little impact on the nu p-process nucleosynthesis. This is evident from the statement \"We show that in our supernova model, collective neutrino oscillations enhance the production of rare isotopes 138La and 180Ta but have little impact on the nu p-process nucleosynthesis.\""}, "5": {"documentation": {"title": "Reference Signal-Aided Channel Estimation in Spatial Media-Based\n  Modulation Systems", "source": "Akif Kabac{\\i}, Mehmet Ba\\c{s}aran and Hakan Ali \\c{C}{\\i}rpan", "docs_id": "2009.13231", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reference Signal-Aided Channel Estimation in Spatial Media-Based\n  Modulation Systems. In this letter, we study the reference signal-aided channel estimation concept which is a crucial requirement to address the realistic performance of spatial media-based modulation (SMBM) systems where the radio frequency mirrors are deployed along with the multiple transmit antennas. Accordingly, least squares and linear minimum mean square error-based channel estimation schemes are proposed for MBM-based systems for the first time in the literature where former studies mainly assume either perfect channel state information or an error model on channel coefficients. In addition, corresponding symbol detection performance is studied. To measure the efficiency of the proposed channel estimation approaches, the theoretical upper bounds on average bit error rate are derived and shown to be well overlapped with the computer simulations for the medium and high signal-to-noise ratio regions. This study is important due to the implementation of channel estimation as well as the theoretical derivation of detection bounds for MBM-based communication systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of spatial media-based modulation (SMBM) systems, what is the primary advantage of using reference signal-aided channel estimation, and how does it differ from traditional channel estimation schemes that assume perfect channel state information or an error model on channel coefficients?\n\n**A)** Reference signal-aided channel estimation provides a more accurate estimation of channel coefficients, allowing for better symbol detection performance in the presence of multipath fading.\n\n**B)** The proposed scheme in the literature assumes a more realistic error model on channel coefficients, which is essential for the implementation of SMBM-based communication systems.\n\n**C)** The use of reference signals enables the derivation of theoretical upper bounds on average bit error rate, which are well overlapped with computer simulations for medium and high signal-to-noise ratio regions.\n\n**D)** The least squares and linear minimum mean square error-based channel estimation schemes proposed in the literature are more computationally efficient than traditional schemes, making them suitable for real-time implementation in SMBM-based systems.\n\n**Correct Answer:** C) The use of reference signals enables the derivation of theoretical upper bounds on average bit error rate, which are well overlapped with computer simulations for medium and high signal-to-noise ratio regions.\n\n**Explanation:** The correct answer is C) because the study in the literature demonstrates that the theoretical upper bounds on average bit error rate derived using the proposed reference signal-aided channel estimation scheme are well overlapped with computer simulations for medium and high signal-to-noise ratio regions. This suggests that the scheme is effective in achieving reliable communication in these regimes. The other options are incorrect because they either focus on the accuracy of channel coefficient estimation (A), assume a more realistic error model (B), or highlight computational efficiency (D), which are not the primary advantages of the proposed scheme."}, "6": {"documentation": {"title": "Securing of Unmanned Aerial Systems (UAS) against security threats using\n  human immune system", "source": "Reza Fotohi", "docs_id": "2003.04984", "section": ["cs.CR", "cs.AI", "cs.PF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Securing of Unmanned Aerial Systems (UAS) against security threats using\n  human immune system. UASs form a large part of the fighting ability of the advanced military forces. In particular, these systems that carry confidential information are subject to security attacks. Accordingly, an Intrusion Detection System (IDS) has been proposed in the proposed design to protect against the security problems using the human immune system (HIS). The IDSs are used to detect and respond to attempts to compromise the target system. Since the UASs operate in the real world, the testing and validation of these systems with a variety of sensors is confronted with problems. This design is inspired by HIS. In the mapping, insecure signals are equivalent to an antigen that are detected by antibody-based training patterns and removed from the operation cycle. Among the main uses of the proposed design are the quick detection of intrusive signals and quarantining their activity. Moreover, SUAS-HIS method is evaluated here via extensive simulations carried out in NS-3 environment. The simulation results indicate that the UAS network performance metrics are improved in terms of false positive rate, false negative rate, detection rate, and packet delivery rate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the proposed design of an Intrusion Detection System (IDS) using the human immune system (HIS) to secure Unmanned Aerial Systems (UAS) against security threats?\n\n**A)** To improve the packet delivery rate of UAS networks\n**B)** To enhance the detection rate of intrusive signals in UAS networks\n**C)** To reduce the false positive rate of IDS systems\n**D)** To utilize the human immune system's ability to recognize and respond to antigens, thereby improving the overall security of UAS networks\n\n**Correct Answer:** D) To utilize the human immune system's ability to recognize and respond to antigens, thereby improving the overall security of UAS networks\n\n**Explanation:** The proposed design of the IDS using HIS is inspired by the human immune system's ability to recognize and respond to antigens. In this context, insecure signals are equivalent to antigens that are detected by antibody-based training patterns and removed from the operation cycle. This approach aims to improve the overall security of UAS networks by quickly detecting and quarantining intrusive signals. While options A, B, and C may be related to the performance of the IDS system, they do not accurately capture the primary motivation behind the proposed design."}, "7": {"documentation": {"title": "On time reversal in photoacoustic tomography for tissue similar to water", "source": "Richard Kowar", "docs_id": "1308.0498", "section": ["math.AP", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On time reversal in photoacoustic tomography for tissue similar to water. This paper is concerned with time reversal in \\emph{photoacoustic tomography} (PAT) of dissipative media that are similar to water. Under an appropriate condition, it is shown that the time reversal method in \\cite{Wa11,AmBrGaWa11} based on the non-causal thermo-viscous wave equation can be used if the non-causal data is replaced by a \\emph{time shifted} set of causal data. We investigate a similar imaging functional for time reversal and an operator equation with the time reversal image as right hand side. If required, an enhanced image can be obtained by solving this operator equation. Although time reversal (for noise-free data) does not lead to the exact initial pressure function, the theoretical and numerical results of this paper show that regularized time reversal in dissipative media similar to water is a valuable method. We note that the presented time reversal method can be considered as an alternative to the causal approach in \\cite{KaSc13} and a similar operator equation may hold for their approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of photoacoustic tomography (PAT) of dissipative media similar to water, what is the primary advantage of using regularized time reversal in PAT, as discussed in the paper on time reversal in PAT?\n\n**A)** It allows for the exact reconstruction of the initial pressure function.\n**B)** It provides an alternative to the causal approach in PAT, as mentioned in the paper.\n**C)** It enables the use of non-causal data, which can be replaced by a time-shifted set of causal data.\n**D)** It enhances the image quality by solving an operator equation with the time reversal image as the right-hand side.\n\n**Correct Answer:** D) It enhances the image quality by solving an operator equation with the time reversal image as the right-hand side.\n\n**Explanation:** The correct answer is D) because the paper states that \"Although time reversal (for noise-free data) does not lead to the exact initial pressure function, the theoretical and numerical results of this paper show that regularized time reversal in dissipative media similar to water is a valuable method.\" This implies that the primary advantage of regularized time reversal is that it can enhance the image quality by solving an operator equation with the time reversal image as the right-hand side.\n\nThe other options are incorrect because:\n\n* A) The paper states that time reversal does not lead to the exact initial pressure function, so option A is incorrect.\n* B) While the paper does mention that the presented time reversal method can be considered as an alternative to the causal approach, this is not the primary advantage of regularized time reversal.\n* C) The paper does not mention the use of non-causal data, so option C is incorrect."}, "8": {"documentation": {"title": "Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks", "source": "Ejder Ba\\c{s}tu\\u{g}, Mehdi Bennis, M\\'erouane Debbah", "docs_id": "1405.5974", "section": ["cs.NI", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks. This article explores one of the key enablers of beyond $4$G wireless networks leveraging small cell network deployments, namely proactive caching. Endowed with predictive capabilities and harnessing recent developments in storage, context-awareness and social networks, peak traffic demands can be substantially reduced by proactively serving predictable user demands, via caching at base stations and users' devices. In order to show the effectiveness of proactive caching, we examine two case studies which exploit the spatial and social structure of the network, where proactive caching plays a crucial role. Firstly, in order to alleviate backhaul congestion, we propose a mechanism whereby files are proactively cached during off-peak demands based on file popularity and correlations among users and files patterns. Secondly, leveraging social networks and device-to-device (D2D) communications, we propose a procedure that exploits the social structure of the network by predicting the set of influential users to (proactively) cache strategic contents and disseminate them to their social ties via D2D communications. Exploiting this proactive caching paradigm, numerical results show that important gains can be obtained for each case study, with backhaul savings and a higher ratio of satisfied users of up to $22\\%$ and $26\\%$, respectively. Higher gains can be further obtained by increasing the storage capability at the network edge."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of proactive caching in 5G wireless networks, what is the primary benefit of leveraging social networks and device-to-device (D2D) communications in caching strategy?\n\n**A)** Reduced backhaul congestion by caching popular files\n**B)** Increased storage capacity at the network edge\n**C)** Improved prediction of user demands by exploiting social structure\n**D)** Enhanced content dissemination through D2D communications\n\n**Correct Answer:** C) Improved prediction of user demands by exploiting social structure\n\n**Explanation:** The correct answer is C) Improved prediction of user demands by exploiting social structure. The article proposes a procedure that exploits the social structure of the network by predicting the set of influential users to (proactively) cache strategic contents and disseminate them to their social ties via D2D communications. This approach leverages social networks to improve the prediction of user demands, which is a key enabler of proactive caching in 5G wireless networks.\n\nThe other options are incorrect because:\n\n* A) Reduced backhaul congestion by caching popular files is a benefit of proactive caching in general, but not specifically related to social networks and D2D communications.\n* B) Increased storage capacity at the network edge is a benefit of increasing storage capability, but not directly related to the social network and D2D communication approach.\n* D) Enhanced content dissemination through D2D communications is a benefit of the proposed procedure, but not the primary benefit of leveraging social networks and D2D communications in caching strategy."}, "9": {"documentation": {"title": "Tumbling and Technicolor Theory", "source": "Noriaki Kitazawa", "docs_id": "hep-ph/9401231", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tumbling and Technicolor Theory. The extended technicolor theory is a candidate of the physics beyond the standard model. To explain the mass hierarchy of the quarks and leptons, the extended technicolor gauge symmetry must hierarchically break to the technicolor gauge symmetry. Tumbling gauge theory is considered as a candidate of the dynamics of such hierarchical symmetry breaking, since the sequential self-breaking of the gauge symmetry (``tumbling'') can be expected in that theory. It is well known that the extended technicolor theory induces too strong flavor-changing neutral current interactions to be consistent with the experiments. This problem can be solved if the technicolor dynamics is the special one with very large anomalous dimension of the composite operator ${\\bar T}T$ composed by the technifermion field $T$. Two types of the models with large anomalous dimension have been proposed. One is the gauge theory with slowly running coupling, another is the gauge theory with strong four fermion interaction. It is expected that the large anomalous dimension is realized in the tumbling gauge theory. In this thesis we systematically estimate the strength of the effective four fermion interactions induced in the tumbling gauge theory"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the extended technicolor theory, what is the expected outcome of the hierarchical symmetry breaking of the technicolor gauge symmetry, and how does the tumbling gauge theory address the issue of strong flavor-changing neutral current interactions?\n\nA) The hierarchical symmetry breaking leads to a strong suppression of flavor-changing neutral current interactions, which can be resolved by introducing a large anomalous dimension in the composite operator ${\\bar T}T$.\n\nB) The sequential self-breaking of the gauge symmetry (tumbling) in the tumbling gauge theory is expected to induce a strong suppression of flavor-changing neutral current interactions, which can be resolved by introducing a large anomalous dimension in the composite operator ${\\bar T}T$.\n\nC) The hierarchical symmetry breaking leads to a strong enhancement of flavor-changing neutral current interactions, which can be resolved by introducing a large anomalous dimension in the composite operator ${\\bar T}T$.\n\nD) The sequential self-breaking of the gauge symmetry (tumbling) in the tumbling gauge theory is expected to induce a strong enhancement of flavor-changing neutral current interactions, which can be resolved by introducing a large anomalous dimension in the composite operator ${\\bar T}T$.\n\n**Correct Answer:** B) The sequential self-breaking of the gauge symmetry (tumbling) in the tumbling gauge theory is expected to induce a strong suppression of flavor-changing neutral current interactions, which can be resolved by introducing a large anomalous dimension in the composite operator ${\\bar T}T$.\n\n**Explanation:** The correct answer is B) because the tumbling gauge theory is expected to induce a sequential self-breaking of the gauge symmetry, which leads to a strong suppression of flavor-changing neutral current interactions. This is in line with the idea that the large anomalous dimension in the composite operator ${\\bar T}T$ can resolve the issue of strong flavor-changing neutral current interactions. The other options are incorrect because they either suggest a strong enhancement of flavor-changing neutral current interactions (C) or a weak suppression (A and D)."}, "10": {"documentation": {"title": "Scaling properties of extreme price fluctuations in Bitcoin markets", "source": "Stjepan Begu\\v{s}i\\'c, Zvonko Kostanj\\v{c}ar, H. Eugene Stanley, and\n  Boris Podobnik", "docs_id": "1803.08405", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling properties of extreme price fluctuations in Bitcoin markets. Detection of power-law behavior and studies of scaling exponents uncover the characteristics of complexity in many real world phenomena. The complexity of financial markets has always presented challenging issues and provided interesting findings, such as the inverse cubic law in the tails of stock price fluctuation distributions. Motivated by the rise of novel digital assets based on blockchain technology, we study the distributions of cryptocurrency price fluctuations. We consider Bitcoin returns over various time intervals and from multiple digital exchanges, in order to investigate the existence of universal scaling behavior in the tails, and ascertain whether the scaling exponent supports the presence of a finite second moment. We provide empirical evidence on slowly decaying tails in the distributions of returns over multiple time intervals and different exchanges, corresponding to a power-law. We estimate the scaling exponent and find an asymptotic power-law behavior with 2 < {\\alpha} < 2.5 suggesting that Bitcoin returns, in addition to being more volatile, also exhibit heavier tails than stocks, which are known to be around 3. Our results also imply the existence of a finite second moment, thus providing a fundamental basis for the usage of standard financial theories and covariance-based techniques in risk management and portfolio optimization scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on the scaling properties of extreme price fluctuations in Bitcoin markets suggests that the distribution of returns exhibits a power-law behavior with a scaling exponent of 2 < {\\alpha} < 2.5. What can be inferred about the tails of the distribution of returns, and how does this compare to the tails of stock price fluctuation distributions?\n\n**A)** The tails of the distribution of returns are lighter than those of stock price fluctuation distributions, and the scaling exponent suggests a finite second moment.\n\n**B)** The tails of the distribution of returns are heavier than those of stock price fluctuation distributions, and the scaling exponent suggests a finite second moment.\n\n**C)** The tails of the distribution of returns are the same as those of stock price fluctuation distributions, and the scaling exponent suggests a non-finite second moment.\n\n**D)** The tails of the distribution of returns are lighter than those of stock price fluctuation distributions, and the scaling exponent suggests a non-finite second moment.\n\n**Correct Answer:** B) The tails of the distribution of returns are heavier than those of stock price fluctuation distributions, and the scaling exponent suggests a finite second moment.\n\n**Explanation:** The study suggests that the distribution of returns exhibits a power-law behavior with a scaling exponent of 2 < {\\alpha} < 2.5, which implies that the tails of the distribution are heavier than those of stock price fluctuation distributions, which have a scaling exponent of around 3. Additionally, the study finds that the scaling exponent supports the presence of a finite second moment, which is a fundamental requirement for the usage of standard financial theories and covariance-based techniques in risk management and portfolio optimization scenarios."}, "11": {"documentation": {"title": "Trace formula for linear Hamiltonian systems with its applications to\n  elliptic Lagrangian solutions", "source": "Xijun Hu, Yuwei Ou, Penghui Wang", "docs_id": "1308.4745", "section": ["math-ph", "math.DS", "math.FA", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Trace formula for linear Hamiltonian systems with its applications to\n  elliptic Lagrangian solutions. In the present paper, we build up trace formulas for both the linear Hamiltonian systems and Sturm-Liouville systems. The formula connects the monodromy matrix of a symmetric periodic orbit with the infinite sum of eigenvalues of the Hessian of the action functional. A natural application is to study the non-degeneracy of linear Hamiltonian systems. Precisely, by the trace formula, we can give an estimation for the upper bound such that the non-degeneracy preserves. Moreover, we could estimate the relative Morse index by the trace formula. Consequently, a series of new stability criteria for the symmetric periodic orbits is given. As a concrete application, the trace formula is used to study the linear stability of elliptic Lagrangian solutions of the classical planar three-body problem. It is well known that the linear stability of elliptic Lagrangian solutions depends on the mass parameter $\\bb=27(m_1m_2+m_2m_3+m_3m_1)/(m_1+m_2+m_3)^2\\in [0,9]$ and the eccentricity $e\\in [0,1)$. Based on the trace formula, we estimate the stable region and hyperbolic region of the elliptic Lagranian solutions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a linear Hamiltonian system with a symmetric periodic orbit. The trace formula for this system is given by:\n\n\u2211_{i=1}^\u221e \u03bb_i = Tr(M)\n\nwhere \u03bb_i are the eigenvalues of the Hessian of the action functional, and M is the monodromy matrix of the symmetric periodic orbit.\n\nWhat can be concluded about the non-degeneracy of the linear Hamiltonian system based on the trace formula?\n\nA) The non-degeneracy is preserved if the sum of the eigenvalues is greater than 1.\nB) The non-degeneracy is preserved if the sum of the eigenvalues is less than 1.\nC) The non-degeneracy is preserved if the sum of the eigenvalues is equal to 1.\nD) The non-degeneracy is not preserved, regardless of the sum of the eigenvalues.\n\nCorrect Answer: B) The non-degeneracy is preserved if the sum of the eigenvalues is less than 1.\n\nExplanation: The trace formula provides a connection between the monodromy matrix of the symmetric periodic orbit and the infinite sum of eigenvalues of the Hessian of the action functional. By analyzing this connection, it can be concluded that the non-degeneracy of the linear Hamiltonian system is preserved if the sum of the eigenvalues is less than 1. This is because a smaller sum of eigenvalues indicates a more stable system, which is a characteristic of non-degenerate systems."}, "12": {"documentation": {"title": "Fingerprints of spin-fermion pairing in cuprates", "source": "Ar. Abanov, Andrey V. Chubukov, and J\\\"org Schmalian", "docs_id": "cond-mat/0010403", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fingerprints of spin-fermion pairing in cuprates. We demonstrate that the feedback effect from bosonic excitations on fermions, which in the past allowed one to verify the phononic mechanism of a conventional, $s-$wave superconductivity, may also allow one to experimentally detect the ``fingerprints'' of the pairing mechanism in cuprates. We argue that for spin-mediated $d-$wave superconductivity, the fermionic spectral function, the density of states, the tunneling conductance through an insulating junction, and the optical conductivity are affected by the interaction with collective spin excitations, which below $T_c$ are propagating, magnon-like quasiparticles with gap $\\Delta_s$. We show that the interaction with a propagating spin excitation gives rise to singularities at frequencies $\\Delta + \\Delta_s$ for the spectral function and the density of states, and at $2\\Delta + \\Delta_s$ for tunneling and optical conductivities, where $\\Delta$ is the maximum value of the $d-$wave gap. We further argue that recent optical measurements also allow one to detect subleading singularities at $4\\Delta$ and $2\\Delta + 2\\Delta_s$. We consider the experimental detection of these singularities as a strong evidence in favor of the magnetic scenario for superconductivity in cuprates."}, "answer": "Here is the difficult multi-form question based on the provided documentation:\n\n**Question:** What is the predicted signature of spin-mediated $d-$wave superconductivity in cuprates, and how does it differ from the phononic mechanism of conventional $s-$wave superconductivity?\n\nA) The spectral function and density of states exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively, with no subleading singularities.\n\nB) The tunneling conductance through an insulating junction and optical conductivity exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively, with subleading singularities at $4\\Delta$ and $2\\Delta + 2\\Delta_s$.\n\nC) The fermionic spectral function and density of states exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively, with subleading singularities at $4\\Delta$ and $2\\Delta + 2\\Delta_s$, and the tunneling conductance through an insulating junction and optical conductivity exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively.\n\nD) The interaction with collective spin excitations gives rise to singularities at frequencies $\\Delta + \\Delta_s$ for the spectral function and density of states, and at $2\\Delta + \\Delta_s$ for tunneling and optical conductivities, with no subleading singularities.\n\n**Correct Answer:** C) The fermionic spectral function and density of states exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively, with subleading singularities at $4\\Delta$ and $2\\Delta + 2\\Delta_s$, and the tunneling conductance through an insulating junction and optical conductivity exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively.\n\n**Explanation:**\n\nThe correct answer, C, accurately reflects the predicted signatures of spin-mediated $d-$wave superconductivity in cuprates. The interaction with collective spin excitations gives rise to singularities in the fermionic spectral function and density of states at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively. Additionally, the tunneling conductance through an insulating junction and optical conductivity exhibit singularities at frequencies $\\Delta + \\Delta_s$ and $2\\Delta + \\Delta_s$, respectively. Furthermore, recent optical measurements also allow for the detection of subleading singularities at $4\\Delta$ and $2\\Delta + 2\\Delta_s$. This answer choice accurately captures the predicted signatures of spin-mediated $d-$wave superconductivity in cuprates, as described in the provided documentation."}, "13": {"documentation": {"title": "The start of the Abiogenesis: Preservation of homochirality in proteins\n  as a necessary and sufficient condition for the establishment of the\n  metabolism", "source": "Soren Toxvaerd", "docs_id": "1803.01560", "section": ["q-bio.BM", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The start of the Abiogenesis: Preservation of homochirality in proteins\n  as a necessary and sufficient condition for the establishment of the\n  metabolism. Biosystems contain an almost infinite amount of vital important details, which together ensure their life. There are, however, some common structures and reactions in the systems: the homochirality of carbohydrates and proteins, the metabolism and the genetics. The Abiogenesis, or the origin of life, is probably not a result of a series of single events, but rather the result of a gradual process with increasing complexity of molecules and chemical reactions, and the prebiotic synthesis of molecules might not have left a trace of the establishment of structures and reactions at the beginning of the evolution. But alternatively, one might be able to determine some order in the formation of the chemical denominators in the Abiogenesis. Here we review experimental results and present a model of the start of the Abionenesis, where the spontaneous formation of homochirality in proteins is the precondition for the establishment of homochirality of carbohydrates and for the metabolism at the start of the Abiogenesis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary condition that is believed to be necessary and sufficient for the establishment of metabolism in the origin of life (Abiogenesis), according to the reviewed experimental results and presented model?\n\nA) The spontaneous formation of homochirality in carbohydrates\nB) The prebiotic synthesis of nucleotides\nC) The preservation of homochirality in proteins\nD) The emergence of self-replicating molecules\n\n**Correct Answer:** C) The preservation of homochirality in proteins\n\n**Explanation:** The correct answer is C) The preservation of homochirality in proteins, as stated in the abstract: \"The start of the Abiogenesis: Preservation of homochirality in proteins as a necessary and sufficient condition for the establishment of the metabolism.\" This indicates that the preservation of homochirality in proteins is a crucial step in the origin of life, and that it is necessary and sufficient for the establishment of metabolism.\n\nThe other options are incorrect because:\n\nA) The spontaneous formation of homochirality in carbohydrates is not mentioned as a necessary condition for metabolism.\n\nB) The prebiotic synthesis of nucleotides is not mentioned as a necessary condition for metabolism.\n\nD) The emergence of self-replicating molecules is not mentioned as a necessary condition for metabolism, and is actually not mentioned at all in the provided documentation."}, "14": {"documentation": {"title": "Renormalization and motivic Galois theory", "source": "Alain Connes (College de France) and Matilde Marcolli (MPIM Bonn)", "docs_id": "math/0409306", "section": ["math.NT", "hep-th", "math-ph", "math.AG", "math.MP", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Renormalization and motivic Galois theory. We investigate the nature of divergences in quantum field theory, showing that they are organized in the structure of a certain `` motivic Galois group'', which is uniquely determined and universal with respect to the set of physical theories. The renormalization group can be identified canonically with a one parameter subgroup. The group is obtained through a Riemann-Hilbert correspondence. Its representations classify equisingular flat vector bundles, where the equisingularity condition is a geometric formulation of the fact that in quantum field theory the counterterms are independent of the choice of a unit of mass. As an algebraic group scheme, it is a semi-direct product by the multiplicative group of a pro-unipotent group scheme whose Lie algebra is freely generated by one generator in each positive integer degree. There is a universal singular frame in which all divergences disappear. When computed as iterated integrals, its coefficients are certain rational numbers that appear in the local index formula of Connes-Moscovici. When working with formal Laurent series over the field of rational numbers, the data of equisingular flat vector bundles define a Tannakian category whose properties are reminiscent of a category of mixed Tate motives."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the motivic Galois group associated with the renormalization group in quantum field theory. What is the universal singular frame in which all divergences disappear, and what are the coefficients of its iterated integrals, as described in the local index formula of Connes-Moscovici?\n\nA) The universal singular frame is a formal Laurent series over the field of rational numbers, and its coefficients are the rational numbers that appear in the local index formula of Connes-Moscovici, which are classified by equisingular flat vector bundles.\n\nB) The universal singular frame is a Riemann-Hilbert correspondence, and its coefficients are the rational numbers that appear in the local index formula of Connes-Moscovici, which are classified by equisingular flat vector bundles.\n\nC) The universal singular frame is a semi-direct product by the multiplicative group of a pro-unipotent group scheme, and its coefficients are the rational numbers that appear in the local index formula of Connes-Moscovici, which are classified by equisingular flat vector bundles.\n\nD) The universal singular frame is a formal Laurent series over the field of rational numbers, and its coefficients are the rational numbers that appear in the local index formula of Connes-Moscovici, which are classified by equisingular flat vector bundles, but the coefficients are not necessarily rational.\n\nCorrect Answer: A) The universal singular frame is a formal Laurent series over the field of rational numbers, and its coefficients are the rational numbers that appear in the local index formula of Connes-Moscovici, which are classified by equisingular flat vector bundles."}, "15": {"documentation": {"title": "Practical GFDM-based Linear Receivers", "source": "Ahmad Nimr, Marwa Chafii and Gerhard Fettweis", "docs_id": "1812.05919", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Practical GFDM-based Linear Receivers. The conventional receiver designs of generalized frequency division multiplexing (GFDM) consider a large scale multiple-input multiple-output (MIMO) system with a block circular matrix of combined channel and modulation. Exploiting this structure, several approaches have been proposed for low complexity joint linear minimum mean squared error (LMMSE) receiver. However, the joint design is complicated and inappropriate for hardware implementation. In this paper, we define the concept of GFDM-based linear receivers, which first performs channel equalization (CEq) and afterwards the equalized signal is processed with GFDM demodulator. We show that the optimal joint LMMSE receiver is equivalent to a GFDM-based one, that applies LMMSE-CEq and zero-forcing demodulation. For orthogonal modulation, the optimal LMMSE receiver has an implementation-friendly structure. For the non-orthogonal case, we propose two practical designs that approach the performance of the joint LMMSE. Finally, we analytically prove that GFDM-based receivers achieve equal signal-to-interference-plus-noise ratio per subsymbols within the same subcarrier."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key difference between the conventional GFDM receiver designs and the proposed GFDM-based linear receivers, and how does the latter approach achieve equal signal-to-interference-plus-noise ratio per subsymbols within the same subcarrier?\n\n**A)** The conventional GFDM receiver designs consider a large scale MIMO system, whereas the proposed GFDM-based linear receivers perform channel equalization (CEq) first and then apply GFDM demodulation. This approach allows for a more efficient and hardware-friendly implementation.\n\n**B)** The conventional GFDM receiver designs use a joint LMMSE-CEq approach, whereas the proposed GFDM-based linear receivers employ LMMSE-CEq and zero-forcing demodulation. This distinction enables the latter to achieve better performance in the non-orthogonal case.\n\n**C)** The conventional GFDM receiver designs are limited by their complexity and inappropriateness for hardware implementation, whereas the proposed GFDM-based linear receivers overcome these limitations by decoupling the channel equalization and demodulation processes.\n\n**D)** The conventional GFDM receiver designs rely on a block circular matrix of combined channel and modulation, whereas the proposed GFDM-based linear receivers exploit the structure of GFDM to achieve equal signal-to-interference-plus-noise ratio per subsymbols within the same subcarrier.\n\n**Correct Answer:** C) The conventional GFDM receiver designs are limited by their complexity and inappropriateness for hardware implementation, whereas the proposed GFDM-based linear receivers overcome these limitations by decoupling the channel equalization and demodulation processes.\n\n**Explanation:** The correct answer, C, highlights the key difference between the conventional GFDM receiver designs and the proposed GFDM-based linear receivers. The conventional designs are indeed limited by their complexity and inappropriateness for hardware implementation, whereas the proposed GFDM-based linear receivers decouple the channel equalization and demodulation processes, making them more efficient and hardware-friendly. This distinction is crucial in achieving equal signal-to-interference-plus-noise ratio per subsymbols within the same subcarrier."}, "16": {"documentation": {"title": "Breathing Relativistic Rotators and Fundamental Dynamical Systems", "source": "{\\L}ukasz Bratek", "docs_id": "0907.3956", "section": ["math-ph", "gr-qc", "hep-th", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Breathing Relativistic Rotators and Fundamental Dynamical Systems. Recently, it was shown, that the mechanical model of a massive spinning particle proposed by Kuzenko, Lyakhovich and Segal in 1994, which is also the fundamental relativistic rotator rediscovered independently 15 years later by Staruszkiewicz in quite a different context, is defective as a dynamical system, that is, its Cauchy problem is not well posed. This dynamical system is fundamental, since its mass and spin are parameters, not arbitrary constants of motion, which is a classical counterpart of quantum irreducibility. It is therefore desirable to find other objects which, apart from being fundamental, would also have well posed Cauchy problem. For that purpose, a class of breathing rotators is considered. A breathing rotator consists of a single null vector associated with position and moves in accordance with some relativistic laws of motion. Surprisingly, breathing rotators which are fundamental, are also defective as dynamical systems. More generally, it has been shown, that the necessary condition for a breathing rotator to be similarly defective, is functional dependence of its Casimir invariants of the Poincar{\\'e} group."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a relativistic rotator with a null vector associated with position, which moves in accordance with some relativistic laws of motion. What is the necessary condition for this rotator to be defective as a dynamical system, and how does it relate to the functional dependence of its Casimir invariants of the Poincar\u00e9 group?\n\n**A)** The necessary condition is that the rotator's mass and spin are arbitrary constants of motion, which implies that the system is not fundamental.\n\n**B)** The necessary condition is that the rotator's Casimir invariants of the Poincar\u00e9 group are independent of the system's parameters, which implies that the system is well-posed.\n\n**C)** The necessary condition is that the rotator's Casimir invariants of the Poincar\u00e9 group are functionally dependent on the system's parameters, which implies that the system is defective as a dynamical system.\n\n**D)** The necessary condition is that the rotator's mass and spin are parameters, not arbitrary constants of motion, which implies that the system is fundamental.\n\n**Correct Answer:** C) The necessary condition is that the rotator's Casimir invariants of the Poincar\u00e9 group are functionally dependent on the system's parameters, which implies that the system is defective as a dynamical system.\n\n**Explanation:** The correct answer is based on the statement from the documentation that \"More generally, it has been shown, that the necessary condition for a breathing rotator to be similarly defective, is functional dependence of its Casimir invariants of the Poincar\u00e9 group.\" This implies that if the Casimir invariants of the Poincar\u00e9 group are functionally dependent on the system's parameters, then the system is defective as a dynamical system."}, "17": {"documentation": {"title": "Orientation-aware Semantic Segmentation on Icosahedron Spheres", "source": "Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla", "docs_id": "1907.12849", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orientation-aware Semantic Segmentation on Icosahedron Spheres. We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed orientation-aware CNN framework for semantic segmentation on icosahedron spheres, as described in the paper \"Orientation-aware Semantic Segmentation on Icosahedron Spheres\"?\n\nA) It requires significant memory and parameters, enabling execution only at very low resolutions.\nB) It simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere.\nC) It uses a pre-trained model on perspective data, allowing for direct transfer of feature weights.\nD) It is rotation invariant, enabling classification and segmentation tasks without additional processing.\n\nCorrect Answer: B) It simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere.\n\nExplanation: The correct answer is B) because the paper states that the proposed representation \"allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere.\" This indicates that the primary advantage of the framework is its ability to simplify standard network operations while still considering the unique properties of the spherical domain."}, "18": {"documentation": {"title": "Mobility Management for Cellular-Connected UAVs: A Learning-Based\n  Approach", "source": "Md Moin Uddin Chowdhury, Walid Saad, and Ismail Guvenc", "docs_id": "2002.01546", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mobility Management for Cellular-Connected UAVs: A Learning-Based\n  Approach. The pervasiveness of the wireless cellular network can be a key enabler for the deployment of autonomous unmanned aerial vehicles (UAVs) in beyond visual line of sight scenarios without human control. However, traditional cellular networks are optimized for ground user equipment (GUE) such as smartphones which makes providing connectivity to flying UAVs very challenging. Moreover, ensuring better connectivity to a moving cellular-connected UAV is notoriously difficult due to the complex air-to-ground path loss model. In this paper, a novel mechanism is proposed to ensure robust wireless connectivity and mobility support for cellular-connected UAVs by tuning the downtilt (DT) angles of all the GBSs. By leveraging tools from reinforcement learning (RL), DT angles are dynamically adjusted by using a model-free RL algorithm. The goal is to provide efficient mobility support in the sky by maximizing the received signal quality at the UAV while also maintaining good throughput performance of the ground users. Simulation results show that the proposed RL-based mobility management (MM) technique can reduce the number of handovers while maintaining the performance goals, compared to the baseline MM scheme in which the network always keeps the DT angle fixed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in providing wireless connectivity to autonomous unmanned aerial vehicles (UAVs) in beyond visual line of sight scenarios, and how does the proposed RL-based mobility management (MM) technique address this challenge?\n\n**A)** The primary challenge is the complexity of the air-to-ground path loss model, and the proposed technique addresses this by using a model-based approach to optimize DT angles.\n\n**B)** The primary challenge is the pervasiveness of the wireless cellular network, which is optimized for ground user equipment, and the proposed technique addresses this by leveraging tools from reinforcement learning (RL) to dynamically adjust DT angles.\n\n**C)** The primary challenge is the limited range of cellular networks, and the proposed technique addresses this by using a fixed DT angle for all GBSs.\n\n**D)** The primary challenge is the high power consumption of UAVs, and the proposed technique addresses this by using a power-saving approach to optimize DT angles.\n\n**Correct Answer:** B) The pervasiveness of the wireless cellular network, which is optimized for ground user equipment, and the proposed technique addresses this by leveraging tools from reinforcement learning (RL) to dynamically adjust DT angles.\n\n**Explanation:** The correct answer is B) because the documentation states that \"traditional cellular networks are optimized for ground user equipment (GUE) such as smartphones, which makes providing connectivity to flying UAVs very challenging.\" The proposed RL-based mobility management (MM) technique addresses this challenge by leveraging tools from reinforcement learning (RL) to dynamically adjust DT angles, as stated in the paper."}, "19": {"documentation": {"title": "Thermal conductivity of the side ledge in aluminium electrolysis cells:\n  compounds as a function of temperature and grain size", "source": "Aimen E. Gheribi and Patrice Chartrand", "docs_id": "1609.08023", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermal conductivity of the side ledge in aluminium electrolysis cells:\n  compounds as a function of temperature and grain size. In aluminium electrolysis cells, a ledge of frozen electrolyte is formed, attached to the sides of the cell. The control of the side ledge thickness is essential in ensuring a reasonable lifetime for the cells. Numerical modelling of the side ledge thickness requires an accurate knowledge of the thermal transport properties as a function of temperature. Unfortunately, there is a considerable lack of experimental data for the large majority of the phases constituting the side ledge. The aim of this work is to provide, for each phase possibly present in the side ledge, a formulation of the thermal conductivity as a function of both temperature and size. To achieve this, we consider reliable physical models linking the density of the lattice vibration energy and the phonon mean free path to key parameters: the high temperature limit of the Debye temperature and the Gruneisen constant. These model parameters can be obtained by simultaneous fitting of (i) the heat capacity, (ii) the thermal expansion tensor coefficient and (iii) the adiabatic elastic constants, on relevant physical models. Where data is missing, first principles (ab initio) calculations are used to determine directly the model parameters. For compounds for which data is available, the model's predictions are found to be in very good agreement with the reported experimental data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the work presented in the Arxiv documentation, and what approach is taken to determine the thermal conductivity of the side ledge in aluminium electrolysis cells?\n\n**A)** The primary objective is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach is to use experimental data and first principles calculations to determine the model parameters.\n\n**B)** The primary objective is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach is to use numerical modelling and simultaneous fitting of heat capacity, thermal expansion tensor coefficient, and adiabatic elastic constants to determine the model parameters.\n\n**C)** The primary objective is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach is to use first principles calculations to determine the model parameters, with a focus on compounds for which data is available.\n\n**D)** The primary objective is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach is to use experimental data and first principles calculations to determine the model parameters, but with a focus on compounds for which data is not available.\n\n**Correct Answer:** B) The primary objective is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach is to use numerical modelling and simultaneous fitting of heat capacity, thermal expansion tensor coefficient, and adiabatic elastic constants to determine the model parameters.\n\n**Explanation:** The correct answer is B) because the primary objective of the work is to provide a formulation of the thermal conductivity as a function of temperature and grain size, and the approach taken is to use numerical modelling and simultaneous fitting of heat capacity, thermal expansion tensor coefficient, and adiabatic elastic constants to determine the model parameters. This is evident from the introduction of the work, which states that the aim is to provide a formulation of the thermal conductivity as a function of both temperature and size, and that numerical modelling is used to achieve this. The simultaneous fitting of multiple physical models is also mentioned as a key step in determining the model parameters."}, "20": {"documentation": {"title": "Inflation in an effective gravitational model & asymptotic safety", "source": "Lei-Hua Liu, Tomislav Prokopec and Alexei A. Starobinsky", "docs_id": "1806.05407", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inflation in an effective gravitational model & asymptotic safety. We consider an inflationary model motivated by quantum effects of gravitational and matter fields near the Planck scale. Our Lagrangian is a re-summed version of the effective Lagrangian recently obtained by Demmel, Saueressig and Zanusso~\\cite{Demmel:2015oqa} in the context of gravity as an asymptotically safe theory. It represents a refined Starobinsky model, ${\\cal L}_{\\rm eff}=M_{\\rm P}^2 R/2 + (a/2)R^2/[1+b\\ln(R/\\mu^2)]$, where $R$ is the Ricci scalar, $a$ and $b$ are constants and $\\mu$ is an energy scale. By implementing the COBE normalisation and the Planck constraint on the scalar spectrum, we show that increasing $b$ leads to an increased value of both the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$. Requiring $n_s$ to be consistent with the Planck collaboration upper limit, we find that $r$ can be as large as $r\\simeq 0.01$, the value possibly measurable by Stage IV CMB ground experiments and certainly from future dedicated space missions. The predicted running of the scalar spectral index $\\alpha=d n_s/d\\ln(k)$ is still of the order $-5\\times 10^{-4}$ (as in the Starobinsky model), about one order of magnitude smaller than the current observational bound."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the effective gravitational model and asymptotic safety, what is the relationship between the constant $b$ and the predicted values of the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$?\n\nA) Increasing $b$ leads to a decrease in both $n_s$ and $r$.\nB) Increasing $b$ leads to an increase in $n_s$ and a decrease in $r$.\nC) Increasing $b$ leads to an increase in both $n_s$ and $r$.\nD) Increasing $b$ leads to a decrease in $n_s$ and an increase in $r$.\n\nCorrect Answer: C) Increasing $b$ leads to an increase in both $n_s$ and $r$.\n\nExplanation: According to the text, increasing $b$ leads to an increased value of both the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$. This is stated in the sentence: \"By implementing the COBE normalisation and the Planck constraint on the scalar spectrum, we show that increasing $b$ leads to an increased value of both the scalar spectral index $n_s$ and the tensor-to-scalar ratio $r$.\""}, "21": {"documentation": {"title": "Topological quasiparticles and the holographic bulk-edge relation in\n  2+1D string-net models", "source": "Tian Lan, Xiao-Gang Wen", "docs_id": "1311.1784", "section": ["cond-mat.str-el", "math.CT", "math.QA", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological quasiparticles and the holographic bulk-edge relation in\n  2+1D string-net models. String-net models allow us to systematically construct and classify 2+1D topologically ordered states which can have gapped boundaries. We can use a simple ideal string-net wavefunction, which is described by a set of F-matrices [or more precisely, a unitary fusion category (UFC)], to study all the universal properties of such a topological order. In this paper, we describe a finite computational method -- Q-algebra approach, that allows us to compute the non-Abelian statistics of the topological excitations [or more precisely, the unitary modular tensor category (UMTC)], from the string-net wavefunction (or the UFC). We discuss several examples, including the topological phases described by twisted gauge theory (i.e., twisted quantum double $D^\\alpha(G)$). Our result can also be viewed from an angle of holographic bulk-boundary relation. The 1+1D anomalous topological orders, that can appear as edges of 2+1D topological states, are classified by UFCs which describe the fusion of quasiparticles in 1+1D. The 1+1D anomalous edge topological order uniquely determines the 2+1D bulk topological order (which are classified by UMTC). Our method allows us to compute this bulk topological order (i.e., the UMTC) from the anomalous edge topological order (i.e., the UFC)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the topological phases described by twisted gauge theory and the 2+1D topological states, as revealed by the Q-algebra approach in string-net models?\n\nA) The twisted gauge theory phases are equivalent to the 2+1D topological states, but with a different classification.\nB) The twisted gauge theory phases are a subset of the 2+1D topological states, but with distinct non-Abelian statistics.\nC) The twisted gauge theory phases are uniquely determined by the 2+1D topological states, and can be computed from the UFC.\nD) The twisted gauge theory phases are mutually exclusive with the 2+1D topological states, and cannot be reconciled by the Q-algebra approach.\n\nCorrect Answer: C) The twisted gauge theory phases are uniquely determined by the 2+1D topological states, and can be computed from the UFC.\n\nExplanation: The Q-algebra approach in string-net models allows for the computation of the non-Abelian statistics of topological excitations from the UFC. The twisted gauge theory phases are classified by UFCs, which describe the fusion of quasiparticles in 1+1D. The 1+1D anomalous edge topological order uniquely determines the 2+1D bulk topological order, which is classified by UMTC. Therefore, the twisted gauge theory phases can be computed from the UFC, and are uniquely determined by the 2+1D topological states."}, "22": {"documentation": {"title": "Predicting Code Review Completion Time in Modern Code Review", "source": "Moataz Chouchen, Jefferson Olongo, Ali Ouni, Mohamed Wiem Mkaouer", "docs_id": "2109.15141", "section": ["cs.SE", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predicting Code Review Completion Time in Modern Code Review. Context. Modern Code Review (MCR) is being adopted in both open source and commercial projects as a common practice. MCR is a widely acknowledged quality assurance practice that allows early detection of defects as well as poor coding practices. It also brings several other benefits such as knowledge sharing, team awareness, and collaboration. Problem. In practice, code reviews can experience significant delays to be completed due to various socio-technical factors which can affect the project quality and cost. For a successful review process, peer reviewers should perform their review tasks in a timely manner while providing relevant feedback about the code change being reviewed. However, there is a lack of tool support to help developers estimating the time required to complete a code review prior to accepting or declining a review request. Objective. Our objective is to build and validate an effective approach to predict the code review completion time in the context of MCR and help developers better manage and prioritize their code review tasks. Method. We formulate the prediction of the code review completion time as a learning problem. In particular, we propose a framework based on regression models to (i) effectively estimate the code review completion time, and (ii) understand the main factors influencing code review completion time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Modern Code Review (MCR), what is the primary objective of the proposed framework for predicting code review completion time, as stated in the paper \"Predicting Code Review Completion Time in Modern Code Review\"?\n\n**A)** To develop a tool that automatically assigns code review tasks to reviewers based on their past performance.\n**B)** To estimate the time required to complete a code review prior to accepting or declining a review request, and to understand the main factors influencing code review completion time.\n**C)** To improve the quality of code reviews by providing more detailed feedback to reviewers.\n**D)** To reduce the number of code reviews required by automating the review process.\n\n**Correct Answer:** B) To estimate the time required to complete a code review prior to accepting or declining a review request, and to understand the main factors influencing code review completion time.\n\n**Explanation:** The correct answer is B) because the paper explicitly states that the objective is to build and validate an effective approach to predict the code review completion time in the context of MCR, and to help developers better manage and prioritize their code review tasks. The proposed framework is based on regression models to estimate the code review completion time and understand the main factors influencing it."}, "23": {"documentation": {"title": "Network-based Referral Mechanism in a Crowdfunding-based Marketing\n  Pattern", "source": "Yongli Li, Zhi-Ping Fan, and Wei Zhang", "docs_id": "1808.03070", "section": ["econ.TH", "econ.GN", "math.OC", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Network-based Referral Mechanism in a Crowdfunding-based Marketing\n  Pattern. Crowdfunding is gradually becoming a modern marketing pattern. By noting that the success of crowdfunding depends on network externalities, our research aims to utilize them to provide an applicable referral mechanism in a crowdfunding-based marketing pattern. In the context of network externalities, measuring the value of leading customers is chosen as the key to coping with the research problem by considering that leading customers take a critical stance in forming a referral network. Accordingly, two sequential-move game models (i.e., basic model and extended model) were established to measure the value of leading customers, and a skill of matrix transformation was adopted to solve the model by transforming a complicated multi-sequence game into a simple simultaneous-move game. Based on the defined value of leading customers, a network-based referral mechanism was proposed by exploring exactly how many awards are allocated along the customer sequence to encourage the leading customers' actions of successful recommendation and by demonstrating two general rules of awarding the referrals in our model setting. Moreover, the proposed solution approach helps deepen an understanding of the effect of the leading position, which is meaningful for designing more numerous referral approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Network-based Referral Mechanism in a Crowdfunding-based Marketing Pattern, what is the primary objective of measuring the value of leading customers, and how does it relate to the formation of a referral network?\n\n**A)** To determine the optimal number of awards to be allocated to customers based on their social influence.\n**B)** To identify the most influential customers in the network and encourage their actions of successful recommendation.\n**C)** To analyze the impact of leading positions on the overall performance of the crowdfunding campaign.\n**D)** To develop a skill of matrix transformation to simplify sequential-move games into simultaneous-move games.\n\n**Correct Answer:** B) To identify the most influential customers in the network and encourage their actions of successful recommendation.\n\n**Explanation:** The correct answer is B) To identify the most influential customers in the network and encourage their actions of successful recommendation. According to the documentation, measuring the value of leading customers is chosen as the key to coping with the research problem, as leading customers take a critical stance in forming a referral network. The proposed solution approach aims to encourage the leading customers' actions of successful recommendation by allocating awards along the customer sequence."}, "24": {"documentation": {"title": "The Role of Social Networks in Information Diffusion", "source": "Eytan Bakshy, Itamar Rosenn, Cameron Marlow, Lada Adamic", "docs_id": "1201.4145", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Role of Social Networks in Information Diffusion. Online social networking technologies enable individuals to simultaneously share information with any number of peers. Quantifying the causal effect of these technologies on the dissemination of information requires not only identification of who influences whom, but also of whether individuals would still propagate information in the absence of social signals about that information. We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends' information sharing among 253 million subjects in situ. Those who are exposed are significantly more likely to spread information, and do so sooner than those who are not exposed. We further examine the relative role of strong and weak ties in information propagation. We show that, although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of online information diffusion, what is the primary finding regarding the role of weak ties in the dissemination of novel information, as reported in the study \"The Role of Social Networks in Information Diffusion\"?\n\nA) Stronger ties are more influential in the propagation of novel information.\nB) Weak ties are individually more influential in the propagation of novel information.\nC) The abundance of weak ties is more important than the strength of weak ties in the dissemination of novel information.\nD) The study found no significant difference in the role of strong and weak ties in information propagation.\n\n**Correct Answer:** C) The abundance of weak ties is more important than the strength of weak ties in the dissemination of novel information.\n\n**Explanation:** The study found that although stronger ties are individually more influential, it is the more abundant weak ties who are responsible for the propagation of novel information. This suggests that the sheer number of weak ties, rather than their individual strength, plays a more significant role in the dissemination of information online."}, "25": {"documentation": {"title": "Emotions in Online Content Diffusion", "source": "Yifan Yu, Shan Huang, Yuchen Liu, Yong Tan", "docs_id": "2011.09003", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emotions in Online Content Diffusion. Social media-transmitted online information, particularly content that is emotionally charged, shapes our thoughts and actions. In this study, we incorporate social network theories and analyses to investigate how emotions shape online content diffusion, using a computational approach. We rigorously quantify and characterize the structural properties of diffusion cascades, in which more than six million unique individuals transmitted 387,486 articles in a massive-scale online social network, WeChat. We detected the degree of eight discrete emotions (i.e., surprise, joy, anticipation, love, anxiety, sadness, anger, and disgust) embedded in these articles, using a newly generated domain-specific and up-to-date emotion lexicon. We found that articles with a higher degree of anxiety and love reached a larger number of individuals and diffused more deeply, broadly, and virally, whereas sadness had the opposite effect. Age and network degree of the individuals who transmitted an article and, in particular, the social ties between senders and receivers, significantly mediated how emotions affect article diffusion. These findings offer valuable insight into how emotions facilitate or hinder information spread through social networks and how people receive and transmit online content that induces various emotions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the study \"Emotions in Online Content Diffusion\", the authors found that articles with a higher degree of anxiety and love reached a larger number of individuals and diffused more deeply, broadly, and virally. However, sadness had the opposite effect. What is the underlying mechanism that explains why anxiety and love facilitate information spread, while sadness hinders it?\n\n**A)** The authors suggest that the emotional intensity of anxiety and love triggers a stronger social network effect, leading to increased transmission and diffusion.\n\n**B)** The authors propose that the emotional valence of anxiety and love is more closely tied to the social ties between senders and receivers, which in turn influences the likelihood of article transmission.\n\n**C)** The authors argue that the cognitive biases associated with anxiety and love, such as the \"availability heuristic\" and \"emotional contagion\", lead to a greater likelihood of article transmission and diffusion.\n\n**D)** The authors suggest that the emotional arousal of anxiety and love triggers a stronger \"emotional resonance\" with the target audience, leading to increased engagement and transmission.\n\n**Correct Answer:** B) The authors propose that the emotional valence of anxiety and love is more closely tied to the social ties between senders and receivers, which in turn influences the likelihood of article transmission.\n\n**Explanation:** The correct answer is supported by the text, which states that \"Age and network degree of the individuals who transmitted an article and, in particular, the social ties between senders and receivers, significantly mediated how emotions affect article diffusion.\" This suggests that the social connections between individuals play a crucial role in determining how emotions influence the spread of information."}, "26": {"documentation": {"title": "Modeling the optical/UV polarization while flying around the tilted\n  outflows of NGC 1068", "source": "Frederic Marin, Rene W. Goosmann, Michal Dovciak", "docs_id": "1204.0936", "section": ["astro-ph.HE", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling the optical/UV polarization while flying around the tilted\n  outflows of NGC 1068. Recent modeling of multi-waveband spectroscopic and maser observations suggests that the ionized outflows in the nuclear region of the archetypal Seyfert-2 galaxy NGC 1068 are inclined with respect to the vertical axis of the obscuring torus. Based on this suggestion, we build a complex reprocessing model of NGC 1068 for the optical/UV band. We apply the radiative transfer code STOKES to compute polarization spectra and images. The effects of electron and dust scattering and the radiative coupling occurring in the inner regions of the multi-component object are taken into account and evaluated at different polar and azimuthal viewing angles. The observed type-1/type-2 polarization dichotomy of active galactic nuclei is reproduced. At the assumed observer's inclination toward NGC 1068, the polarization is dominated by scattering in the polar outflows and therefore it indicates their tilting angle with respect to the torus axis. While a detailed analysis of our model results is still in progress, we briefly discuss how they relate to existing polarization observations of NGC 1068."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on NGC 1068's optical/UV polarization suggest that the ionized outflows in the nuclear region of the galaxy are inclined with respect to the vertical axis of the obscuring torus. What is the primary mechanism responsible for the polarization observed in the polar outflows, according to the authors' model?\n\nA) Radiative coupling between the outflows and the torus\nB) Dust scattering in the polar outflows\nC) Electron scattering in the polar outflows\nD) Maser emission in the polar outflows\n\n**Correct Answer:** C) Electron scattering in the polar outflows\n\n**Explanation:** The authors mention that the polarization is dominated by scattering in the polar outflows, and they evaluate the effects of electron and dust scattering. Therefore, the correct answer is electron scattering in the polar outflows. The other options are incorrect because radiative coupling is mentioned as a factor in the inner regions of the multi-component object, but not specifically related to the polarization in the polar outflows; dust scattering is mentioned as a factor, but not as the primary mechanism; and maser emission is not mentioned as a factor in the polarization."}, "27": {"documentation": {"title": "Parameter estimation and bifurcation analysis of stochastic models of\n  gene regulatory networks: tensor-structured methods", "source": "Shuohao Liao, Tomas Vejchodsky, Radek Erban", "docs_id": "1406.7825", "section": ["q-bio.MN", "math.NA", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parameter estimation and bifurcation analysis of stochastic models of\n  gene regulatory networks: tensor-structured methods. Stochastic modelling provides an indispensable tool for understanding how random events at the molecular level influence cellular functions. In practice, the common challenge is to calibrate a large number of model parameters against the experimental data. A related problem is to efficiently study how the behaviour of a stochastic model depends on its parameters, i.e. whether a change in model parameters can lead to a significant qualitative change in model behaviour (bifurcation). In this paper, tensor-structured parametric analysis (TPA) is presented. It is based on recently proposed low-parametric tensor-structured representations of classical matrices and vectors. This approach enables simultaneous computation of the model properties for all parameter values within a parameter space. This methodology is exemplified to study the parameter estimation, robustness, sensitivity and bifurcation structure in stochastic models of biochemical networks. The TPA has been implemented in Matlab and the codes are available at http://www.stobifan.org ."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of stochastic models of gene regulatory networks, what is the primary challenge in calibrating model parameters against experimental data, and how does the tensor-structured parametric analysis (TPA) approach address this challenge?\n\n**A)** The primary challenge is to estimate the mean and variance of the model parameters, and TPA addresses this by using low-parametric tensor-structured representations of classical matrices and vectors to compute model properties for all parameter values within a parameter space.\n\n**B)** The primary challenge is to identify the most significant qualitative changes in model behavior due to changes in model parameters, and TPA addresses this by using a combination of parameter estimation and bifurcation analysis to study the robustness and sensitivity of the model.\n\n**C)** The primary challenge is to efficiently study the behavior of stochastic models for a large number of parameter values, and TPA addresses this by using a tensor-structured approach to compute model properties simultaneously for all parameter values within a parameter space.\n\n**D)** The primary challenge is to estimate the model parameters using a deterministic approach, and TPA addresses this by using a probabilistic approach to estimate the model parameters and compute model properties.\n\n**Correct Answer:** C) The primary challenge is to efficiently study the behavior of stochastic models for a large number of parameter values, and TPA addresses this by using a tensor-structured approach to compute model properties simultaneously for all parameter values within a parameter space.\n\n**Explanation:** The correct answer is C) because the documentation states that the primary challenge in calibrating model parameters against experimental data is to efficiently study the behavior of stochastic models for a large number of parameter values. The tensor-structured parametric analysis (TPA) approach addresses this challenge by using a tensor-structured approach to compute model properties simultaneously for all parameter values within a parameter space. This allows for a comprehensive study of the model's behavior and properties, which is essential for understanding how random events at the molecular level influence cellular functions."}, "28": {"documentation": {"title": "Channel Estimation for Opportunistic Spectrum Access: Uniform and Random\n  Sensing", "source": "Quanquan Liang, Mingyan Liu", "docs_id": "1005.2544", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Channel Estimation for Opportunistic Spectrum Access: Uniform and Random\n  Sensing. The knowledge of channel statistics can be very helpful in making sound opportunistic spectrum access decisions. It is therefore desirable to be able to efficiently and accurately estimate channel statistics. In this paper we study the problem of optimally placing sensing times over a time window so as to get the best estimate on the parameters of an on-off renewal channel. We are particularly interested in a sparse sensing regime with a small number of samples relative to the time window size. Using Fisher information as a measure, we analytically derive the best and worst sensing sequences under a sparsity condition. We also present a way to derive the best/worst sequences without this condition using a dynamic programming approach. In both cases the worst turns out to be the uniform sensing sequence, where sensing times are evenly spaced within the window. With these results we argue that without a priori knowledge, a robust sensing strategy should be a randomized strategy. We then compare different random schemes using a family of distributions generated by the circular $\\beta$ ensemble, and propose an adaptive sensing scheme to effectively track time-varying channel parameters. We further discuss the applicability of compressive sensing for this problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of opportunistic spectrum access, what is the primary advantage of using a randomized sensing strategy, as argued in the paper \"Channel Estimation for Opportunistic Spectrum Access: Uniform and Random Sensing\"?\n\n**A)** It allows for more accurate estimation of channel statistics by reducing the impact of noise.\n**B)** It enables the use of compressive sensing to reduce the number of samples required for estimation.\n**C)** It provides a robust sensing strategy that can effectively track time-varying channel parameters, even without prior knowledge of the channel statistics.\n**D)** It reduces the computational complexity of channel estimation by reducing the number of sensing times.\n\n**Correct Answer:** C) It provides a robust sensing strategy that can effectively track time-varying channel parameters, even without prior knowledge of the channel statistics.\n\n**Explanation:** The paper argues that without prior knowledge of channel statistics, a robust sensing strategy should be a randomized strategy. This is because the worst-case scenario is that the sensing sequence is uniform, which can lead to inaccurate estimates of channel statistics. The randomized sensing strategy can help mitigate this issue by providing a more robust estimate of channel parameters. The correct answer, C, highlights this key advantage of randomized sensing. The other options are incorrect because they do not accurately capture the primary advantage of randomized sensing in this context."}, "29": {"documentation": {"title": "Cluster size in bond percolation on the Platonic solids", "source": "Nicolas Lanchier and Axel La Salle", "docs_id": "2012.01508", "section": ["math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cluster size in bond percolation on the Platonic solids. The main objective of this paper is to study the size of a typical cluster of bond percolation on each of the five Platonic solids: the tetrahedron, the cube, the octahedron, the dodecahedron and the icosahedron. Looking at the clusters from a dynamical point of view, i.e., comparing the clusters with birth processes, we first prove that the first and second moments of the cluster size are bounded by their counterparts in a certain branching process, which results in explicit upper bounds that are accurate when the density of open edges is small. Using that vertices surrounded by closed edges cannot be reached by an open path, we also derive upper bounds that, on the contrary, are accurate when the density of open edges is large. These upper bounds hold in fact for all regular graphs. Specializing in the five~Platonic solids, the exact value of (or lower bounds for) the first and second moments are obtained from the inclusion-exclusion principle and a computer program. The goal of our program is not to simulate the stochastic process but to compute exactly sums of integers that are too large to be computed by hand so these results are analytical, not numerical."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the cluster size in bond percolation on the Platonic solids and the branching process, and how do the upper bounds for the cluster size change depending on the density of open edges?\n\nA) The cluster size is directly proportional to the branching process, and the upper bounds are accurate for all densities.\nB) The cluster size is bounded by the branching process, but the upper bounds are only accurate for small densities of open edges.\nC) The cluster size is not related to the branching process, and the upper bounds are accurate for all densities of open edges.\nD) The cluster size is bounded by the branching process, but the upper bounds are only accurate for large densities of open edges.\n\nCorrect Answer: B) The cluster size is bounded by the branching process, but the upper bounds are only accurate for small densities of open edges.\n\nExplanation: The paper proves that the first and second moments of the cluster size are bounded by their counterparts in a certain branching process. However, the accuracy of these upper bounds depends on the density of open edges. For small densities, the upper bounds are accurate, but for large densities, they are not. This is because vertices surrounded by closed edges cannot be reached by an open path, which affects the accuracy of the upper bounds."}, "30": {"documentation": {"title": "Taming Chimeras in Networks through Multiplexing Delays", "source": "Saptarshi Ghosh, Leonhard Sch\\\"ulen, Ajay Deep Kachhvah, Anna\n  Zakharova and Sarika Jalan", "docs_id": "1907.10031", "section": ["nlin.AO", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Taming Chimeras in Networks through Multiplexing Delays. Chimera referring to a coexistence of coherent and incoherent states, is traditionally very difficult to control due to its peculiar nature. Here, we provide a recipe to construct chimera states in the multiplex networks with the aid of multiplexing-delays. The chimera state in multiplex networks is produced by introducing heterogeneous delays in a fraction of inter-layer links, referred as multiplexing-delay, in a sequence. Additionally, the emergence of the incoherence in the chimera state can be regulated by making appropriate choice of both inter- and intra-layer coupling strengths, whereas the extent and the position of the incoherence regime can be regulated by appropriate placing and {strength} of the multiplexing delays. The proposed technique to construct such {engineered} chimera equips us with multiplex network's structural parameters as tools in gaining both qualitative- and quantitative-control over the incoherent section of the chimera states and, in turn, the chimera. Our investigation can be of worth in controlling dynamics of multi-level delayed systems and attain desired chimeric patterns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of multiplex networks, what is the primary challenge in controlling chimera states, and how can the proposed technique address this challenge?\n\nA) The chimera state is difficult to control due to the complexity of its dynamics, but the proposed technique can regulate the emergence of incoherence by adjusting inter- and intra-layer coupling strengths.\nB) The chimera state is difficult to control due to the peculiar nature of its coexistence of coherent and incoherent states, but the proposed technique can regulate the extent and position of the incoherence regime by placing and adjusting multiplexing delays.\nC) The chimera state is difficult to control due to the heterogeneity of its delays, but the proposed technique can regulate the structural parameters of the multiplex network to gain control over the incoherent section of the chimera states.\nD) The chimera state is difficult to control due to the lack of understanding of its dynamics, but the proposed technique can provide a recipe to construct chimera states in multiplex networks with the aid of multiplexing-delays.\n\n**Correct Answer:** B) The chimera state is difficult to control due to the peculiar nature of its coexistence of coherent and incoherent states, but the proposed technique can regulate the extent and position of the incoherence regime by placing and adjusting multiplexing delays.\n\n**Explanation:** The correct answer is B) because the documentation states that the chimera state is traditionally difficult to control due to its peculiar nature of coexistence of coherent and incoherent states. The proposed technique addresses this challenge by regulating the emergence of incoherence in the chimera state by making appropriate choices of both inter- and intra-layer coupling strengths, and by placing and adjusting multiplexing delays to regulate the extent and position of the incoherence regime."}, "31": {"documentation": {"title": "Deep Lagrangian connectivity in the global ocean inferred from Argo\n  floats", "source": "Ryan Abernathey, Christopher Bladwell, Gary Froyland, and Konstantinos\n  Sakellariou", "docs_id": "2108.00683", "section": ["math.DS", "math.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Lagrangian connectivity in the global ocean inferred from Argo\n  floats. We describe the application of a new technique from nonlinear dynamical systems to infer the Lagrangian connectivity of the deep global ocean. We approximate the dynamic Laplacian using Argo trajectories from January 2011 to January 2017 and extract the eight dominant coherent (or dynamically self-connected) regions at 1500m depth. Our approach overcomes issues such as sparsity of observed data, and floats continually leaving and entering the dataset; only 10\\% of floats record for the full six years. The identified coherent regions maximally trap water within them over the six-year time frame, providing a distinct analysis of the deep global ocean, and relevant information for planning future float deployment. While our study is concerned with ocean circulation at a multi-year, global scale, the dynamic Laplacian approach may be applied at any temporal or spatial scale to identify coherent structures in ocean flow from positional time series information arising from observations or models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the dynamic Laplacian approach to infer Lagrangian connectivity in the global ocean, as described in the study \"Deep Lagrangian connectivity in the global ocean inferred from Argo floats\"?\n\nA) It allows for the estimation of ocean currents at a high spatial resolution\nB) It overcomes issues related to data sparsity and temporal variability in Argo float trajectories\nC) It provides a direct measure of ocean circulation at a global scale\nD) It enables the identification of coherent structures in ocean flow using positional time series information\n\nCorrect Answer: B) It overcomes issues related to data sparsity and temporal variability in Argo float trajectories\n\nExplanation: The study highlights the challenges of working with Argo float data, including the sparsity of observed data and the continuous leaving and entering of floats from the dataset. The dynamic Laplacian approach is presented as a solution to these issues, allowing for the identification of coherent regions in the ocean that trap water over a six-year time frame. This approach is therefore the primary advantage of using the dynamic Laplacian approach to infer Lagrangian connectivity in the global ocean."}, "32": {"documentation": {"title": "Special Functions of Mathematical Physics: A Unified Lagrangian\n  Formalism", "source": "Zdzislaw Musielak, Niyousha Davachi, Marialis Rosario-Franco", "docs_id": "1902.01013", "section": ["math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Special Functions of Mathematical Physics: A Unified Lagrangian\n  Formalism. Lagrangian formalism is established for differential equations with special functions of mathematical physics as solutions. Formalism is based on either standard or non-standard Lagrangians. This work shows that the procedure of deriving the standard Lagrangians leads to Lagrangians for which the Euler--Lagrange equation vanishes identically, and that only some of these Lagrangians become the null Lagrangians with the well-defined gauge functions. It is also demonstrated that the non-standard Lagrangians require that the Euler--Lagrange equations are amended by the auxiliary conditions, which is a new phenomenon in the calculus of variations. The~existence of the auxiliary conditions has profound implications on the validity of the Helmholtz conditions. The obtained results are used to derive the Lagrangians for the Airy, Bessel, Legendre and Hermite equations. The presented examples clearly demonstrate that the developed Lagrangian formalism is applicable to all considered differential equations, including the Airy (and other similar) equations, and that the regular and modified Bessel equations are the only ones with the gauge functions. Possible implications of the existence of the gauge functions for these equations are~discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary implication of the existence of gauge functions for the Airy and modified Bessel equations, as demonstrated by the Lagrangian formalism in the context of special functions of mathematical physics?\n\nA) The Euler-Lagrange equation vanishes identically for all Lagrangians, regardless of the gauge functions.\nB) The non-standard Lagrangians require the introduction of auxiliary conditions to ensure the validity of the Helmholtz conditions.\nC) The gauge functions imply that the Euler-Lagrange equation is satisfied for all solutions, including those that are not null Lagrangians.\nD) The existence of gauge functions for the Airy and modified Bessel equations has no significant implications for the validity of the Euler-Lagrange equation.\n\nCorrect Answer: B) The non-standard Lagrangians require the introduction of auxiliary conditions to ensure the validity of the Helmholtz conditions.\n\nExplanation: The correct answer is B) because the documentation states that the non-standard Lagrangians require the introduction of auxiliary conditions, which is a new phenomenon in the calculus of variations. This has profound implications on the validity of the Helmholtz conditions. The other options are incorrect because they do not accurately reflect the implications of the existence of gauge functions for the Airy and modified Bessel equations. Option A is incorrect because the Euler-Lagrange equation does not vanish identically for all Lagrangians. Option C is incorrect because the gauge functions do not imply that the Euler-Lagrange equation is satisfied for all solutions. Option D is incorrect because the existence of gauge functions does have significant implications for the validity of the Euler-Lagrange equation."}, "33": {"documentation": {"title": "Monte Carlo Glauber model with meson cloud: predictions for 5.44 TeV\n  Xe+Xe collisions", "source": "B.G. Zakharov", "docs_id": "1804.05405", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Monte Carlo Glauber model with meson cloud: predictions for 5.44 TeV\n  Xe+Xe collisions. We study, within the Monte-Carlo Glauber model, centrality dependence of the midrapidity charged multiplicity density $dN_{ch}/d\\eta$ and of the anisotropy coefficients $\\varepsilon_{2,3}$ in Pb+Pb collisions at $\\sqrt{s}=5.02$ TeV and in Xe+Xe collisions at $\\sqrt{s}=5.44$ TeV. Calculations are performed for versions with and without nucleon meson cloud. The fraction of the binary collisions, $\\alpha$, has been fitted to the data on $dN_{ch}/d\\eta$ in Pb+Pb collisions. We obtain $\\alpha\\approx 0.09(0.13)$ with (without) meson cloud. The effect of meson cloud on the $dN_{ch}/d\\eta$ is relatively small. For Xe+Xe collisions for $0$-$5$\\% centrality bin we obtain $dN_{ch}/d\\eta\\approx 1149$ and $1134$ with and without meson cloud, respectively. We obtain $\\varepsilon_2(\\mbox{Xe})/\\varepsilon_2(\\mbox{Pb})\\sim 1.45$ for most central collisions, and $\\varepsilon_2(\\mbox{Xe})/\\varepsilon_2(\\mbox{Pb})$ close to unity at $c\\gtrsim 20$\\%. We find a noticeable increase of the eccentricity in Xe+Xe collisions at small centralities due to the prolate shape of the Xe nucleus. The triangularity in Xe+Xe collisions is bigger than in Pb+Pb collisions at $c\\lesssim 70$\\%. We obtain $\\varepsilon_3(\\mbox{Xe})/\\varepsilon_3(\\mbox{Pb})\\sim 1.3$ at $c\\lesssim 1$\\%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Compare the centrality dependence of the midrapidity charged multiplicity density $dN_{ch}/d\\eta$ and the anisotropy coefficients $\\varepsilon_{2,3}$ in Pb+Pb collisions at $\\sqrt{s}=5.02$ TeV and in Xe+Xe collisions at $\\sqrt{s}=5.44$ TeV, considering the effects of the nucleon meson cloud.\n\nA) The centrality dependence of $dN_{ch}/d\\eta$ and $\\varepsilon_{2,3}$ is similar in both Pb+Pb and Xe+Xe collisions, with a noticeable increase in eccentricity in Xe+Xe collisions at small centralities.\n\nB) The effect of the nucleon meson cloud on $dN_{ch}/d\\eta$ is relatively small, but it leads to a significant increase in the eccentricity of Xe+Xe collisions at small centralities.\n\nC) The centrality dependence of $dN_{ch}/d\\eta$ and $\\varepsilon_{2,3}$ is similar in both Pb+Pb and Xe+Xe collisions, with a noticeable increase in triangularity in Xe+Xe collisions at $c\\lesssim 70\\%$.\n\nD) The nucleon meson cloud has a significant effect on the centrality dependence of $dN_{ch}/d\\eta$ and $\\varepsilon_{2,3}$ in Xe+Xe collisions, leading to a larger eccentricity and triangularity compared to Pb+Pb collisions.\n\nCorrect Answer: B) The effect of the nucleon meson cloud on $dN_{ch}/d\\eta$ is relatively small, but it leads to a significant increase in the eccentricity of Xe+Xe collisions at small centralities."}, "34": {"documentation": {"title": "Power spectrum and diffusion of the Amari neural field", "source": "Luca Salasnich", "docs_id": "1504.07523", "section": ["q-bio.NC", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Power spectrum and diffusion of the Amari neural field. We study the power spectrum of a space-time dependent neural field which describes the average membrane potential of neurons in a single layer. This neural field is modelled by a dissipative integro-differential equation, the so-called Amari equation. By considering a small perturbation with respect to a stationary and uniform configuration of the neural field we derive a linearized equation which is solved for a generic external stimulus by using the Fourier transform into wavevector-freqency domain, finding an analytical formula for the power spectrum of the neural field. In addition, after proving that for large wavelengths the linearized Amari equation is equivalent to a diffusion equation which admits space-time dependent analytical solutions, we take into account the nonlinearity of the Amari equation. We find that for large wavelengths a weak nonlinearity in the Amari equation gives rise to a reaction-diffusion equation which can be formally derived from a neural action functional by introducing a dual neural field. For some initial conditions, we discuss analytical solutions of this reaction-diffusion equation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Amari neural field, what is the relationship between the linearized Amari equation and the diffusion equation for large wavelengths, and how does the introduction of a dual neural field lead to the derivation of a reaction-diffusion equation?\n\n**A)** The linearized Amari equation is equivalent to the diffusion equation for large wavelengths, and the introduction of a dual neural field does not affect the diffusion equation.\n\n**B)** The linearized Amari equation is equivalent to the diffusion equation for large wavelengths, and the introduction of a dual neural field leads to the derivation of a reaction-diffusion equation that can be solved using the Fourier transform.\n\n**C)** The linearized Amari equation is equivalent to the diffusion equation for large wavelengths, but the introduction of a dual neural field leads to the derivation of a reaction-diffusion equation that can be solved using the method of characteristics.\n\n**D)** The linearized Amari equation is not equivalent to the diffusion equation for large wavelengths, and the introduction of a dual neural field does not lead to the derivation of a reaction-diffusion equation.\n\n**Correct Answer:** B) The linearized Amari equation is equivalent to the diffusion equation for large wavelengths, and the introduction of a dual neural field leads to the derivation of a reaction-diffusion equation that can be solved using the Fourier transform.\n\n**Explanation:** The correct answer is B) because the documentation states that for large wavelengths, the linearized Amari equation is equivalent to a diffusion equation, and that the introduction of a dual neural field leads to the derivation of a reaction-diffusion equation. The Fourier transform is also mentioned as a method for solving the reaction-diffusion equation. The other options are incorrect because they either misstate the relationship between the linearized Amari equation and the diffusion equation, or incorrectly describe the effect of introducing a dual neural field."}, "35": {"documentation": {"title": "Relativistic particle in a box: Klein-Gordon vs Dirac Equations", "source": "Pedro Alberto, Saurya Das, Elias C. Vagenas", "docs_id": "1711.06313", "section": ["quant-ph", "gr-qc", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relativistic particle in a box: Klein-Gordon vs Dirac Equations. The problem of a particle in a box is probably the simplest problem in quantum mechanics which allows for significant insight into the nature of quantum systems and thus is a cornerstone in the teaching of quantum mechanics. In relativistic quantum mechanics this problem allows also to highlight the implications of special relativity for quantum physics, namely the effect that spin has on the quantized energy spectra. To illustrate this point, we solve the problem of a spin zero relativistic particle in a one- and three-dimensional box using the Klein-Gordon equation in the Feshbach-Villars formalism. We compare the solutions and the energy spectra obtained with the corresponding ones from the Dirac equation for a spin one-half relativistic particle. We note the similarities and differences, in particular the spin effects in the relativistic energy spectrum. As expected, the non-relativistic limit is the same for both kinds of particles, since, for a particle in a box, the spin contribution to the energy is a relativistic effect."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the relativistic particle in a box problem, compare the energy spectra obtained from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box with the energy spectra obtained from the Dirac equation for a spin-one-half particle in a three-dimensional box. How do the spin effects manifest in the relativistic energy spectra, and what is the non-relativistic limit for both cases?\n\n**A)** The energy spectra from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box are identical to the energy spectra from the Dirac equation for a spin-one-half particle in a three-dimensional box, with no spin effects.\n\n**B)** The energy spectra from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box exhibit a relativistic energy spectrum with spin effects, while the energy spectra from the Dirac equation for a spin-one-half particle in a three-dimensional box exhibit a relativistic energy spectrum with spin effects, but with a different energy spacing.\n\n**C)** The non-relativistic limit for the energy spectra from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box is the same as the non-relativistic limit for the energy spectra from the Dirac equation for a spin-one-half particle in a three-dimensional box, with no spin effects.\n\n**D)** The energy spectra from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box exhibit a relativistic energy spectrum with spin effects, while the energy spectra from the Dirac equation for a spin-one-half particle in a three-dimensional box exhibit a relativistic energy spectrum with spin effects, and the non-relativistic limit for the spin-zero particle is different from the non-relativistic limit for the spin-one-half particle.\n\n**Correct Answer:** D) The energy spectra from the Klein-Gordon equation in the Feshbach-Villars formalism for a spin-zero particle in a one-dimensional box exhibit a relativistic energy spectrum with spin effects, while the energy spectra from the Dirac equation for a spin-one-half particle in a three-dimensional box exhibit a relativistic energy spectrum with spin effects, and the non-relativistic limit for the spin-zero particle is different from the non-relativistic limit for the spin-one-half particle.\n\n**Explanation:** The correct answer is D) because it accurately describes the energy spectra obtained from both the Klein-Gordon equation in the Feshbach-Villars formalism and the Dirac equation, including the relativistic energy spectrum with spin effects and the non-relativistic limit for both cases. The other options are incorrect because they either ignore the spin effects or incorrectly describe the non-relativistic limit."}, "36": {"documentation": {"title": "VeriSmart: A Highly Precise Safety Verifier for Ethereum Smart Contracts", "source": "Sunbeom So, Myungho Lee, Jisu Park, Heejo Lee, Hakjoo Oh", "docs_id": "1908.11227", "section": ["cs.PL", "cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "VeriSmart: A Highly Precise Safety Verifier for Ethereum Smart Contracts. We present VeriSmart, a highly precise verifier for ensuring arithmetic safety of Ethereum smart contracts. Writing safe smart contracts without unintended behavior is critically important because smart contracts are immutable and even a single flaw can cause huge financial damage. In particular, ensuring that arithmetic operations are safe is one of the most important and common security concerns of Ethereum smart contracts nowadays. In response, several safety analyzers have been proposed over the past few years, but state-of-the-art is still unsatisfactory; no existing tools achieve high precision and recall at the same time, inherently limited to producing annoying false alarms or missing critical bugs. By contrast, VeriSmart aims for an uncompromising analyzer that performs exhaustive verification without compromising precision or scalability, thereby greatly reducing the burden of manually checking undiscovered or incorrectly-reported issues. To achieve this goal, we present a new domain-specific algorithm for verifying smart contracts, which is able to automatically discover and leverage transaction invariants that are essential for precisely analyzing smart contracts. Evaluation with real-world smart contracts shows that VeriSmart can detect all arithmetic bugs with a negligible number of false alarms, far outperforming existing analyzers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of VeriSmart, a highly precise safety verifier for Ethereum smart contracts, and how does it differ from existing safety analyzers?\n\nA) To achieve high precision and recall at the same time, while minimizing false alarms and maximizing scalability.\nB) To produce exhaustive verification without compromising precision or scalability, thereby reducing the burden of manually checking undiscovered or incorrectly-reported issues.\nC) To leverage transaction invariants to automatically discover and analyze smart contracts, while achieving high recall and minimizing false alarms.\nD) To achieve high recall and precision at the same time, while minimizing false alarms and maximizing scalability.\n\nCorrect Answer: B) To produce exhaustive verification without compromising precision or scalability, thereby reducing the burden of manually checking undiscovered or incorrectly-reported issues.\n\nExplanation: The correct answer is B) because the documentation states that VeriSmart aims for an \"uncompromising analyzer that performs exhaustive verification without compromising precision or scalability\". This means that VeriSmart's primary goal is to produce exhaustive verification without sacrificing precision or scalability, which is a key difference from existing safety analyzers that are inherently limited to producing false alarms or missing critical bugs."}, "37": {"documentation": {"title": "Dirac's magnetic monopole and the Kontsevich star product", "source": "Michael A. Soloviev (Lebedev Inst.)", "docs_id": "1708.05030", "section": ["math-ph", "math.MP", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dirac's magnetic monopole and the Kontsevich star product. We examine relationships between various quantization schemes for an electrically charged particle in the field of a magnetic monopole. Quantization maps are defined in invariant geometrical terms, appropriate to the case of nontrivial topology, and are constructed for two operator representations. In the first setting, the quantum operators act on the Hilbert space of sections of a nontrivial complex line bundle associated with the Hopf bundle, whereas the second approach uses instead a quaternionic Hilbert module of sections of a trivial quaternionic line bundle. We show that these two quantizations are naturally related by a bundle morphism and, as a consequence, induce the same phase-space star product. We obtain explicit expressions for the integral kernels of star-products corresponding to various operator orderings and calculate their asymptotic expansions up to the third order in the Planck constant $\\hbar$. We also show that the differential form of the magnetic Weyl product corresponding to the symmetric ordering agrees completely with the Kontsevich formula for deformation quantization of Poisson structures and can be represented by Kontsevich's graphs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the magnetic monopole and the Kontsevich star product as described in the Arxiv documentation. Show that the two quantization schemes for an electrically charged particle in the field of a magnetic monopole induce the same phase-space star product, and explain the relationship between the two quantizations in terms of bundle morphisms.\n\nA) The two quantizations are related by a bundle morphism if and only if the integral kernels of the star-products corresponding to the two operator orderings are equal.\n\nB) The two quantizations are related by a bundle morphism if and only if the differential form of the magnetic Weyl product corresponding to the symmetric ordering agrees completely with the Kontsevich formula for deformation quantization of Poisson structures.\n\nC) The two quantizations are related by a bundle morphism if and only if the Hilbert space of sections of the nontrivial complex line bundle associated with the Hopf bundle is isomorphic to the Hilbert module of sections of the trivial quaternionic line bundle.\n\nD) The two quantizations are related by a bundle morphism if and only if the magnetic Weyl product corresponding to the symmetric ordering can be represented by Kontsevich's graphs.\n\nCorrect Answer: B) The two quantizations are related by a bundle morphism if and only if the differential form of the magnetic Weyl product corresponding to the symmetric ordering agrees completely with the Kontsevich formula for deformation quantization of Poisson structures.\n\nExplanation: The correct answer is B) because the documentation states that the two quantizations are naturally related by a bundle morphism, and as a consequence, induce the same phase-space star product. Furthermore, the documentation shows that the differential form of the magnetic Weyl product corresponding to the symmetric ordering agrees completely with the Kontsevich formula for deformation quantization of Poisson structures. This implies that the two quantizations are related by a bundle morphism if and only if the differential form of the magnetic Weyl product corresponding to the symmetric ordering agrees completely with the Kontsevich formula."}, "38": {"documentation": {"title": "HIV and TB in Eastern and Southern Africa: Evidence for behaviour change\n  and the impact of ART", "source": "Brian G. Williams", "docs_id": "1406.6912", "section": ["q-bio.QM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "HIV and TB in Eastern and Southern Africa: Evidence for behaviour change\n  and the impact of ART. The United Nations Joint Programme on HIV/AIDS (UNAIDS) has set a target to ensure that 15 million HIV-positive people in the world would be receiving combination anti-retroviral treatment (ART) by 2015. This target is likely to be reached and new targets for 2020 and 2030 are needed. Eastern and Southern Africa (ESA) account for approximately half of all people living with HIV in the world and it will be especially important to set reachable and affordable targets for this region. In order to make future projections of HIV and TB prevalence, incidence and mortality assuming different levels of ART scale-up and coverage, it is first necessary to assess the current state of the epidemic. Here we review national data on the prevalence of HIV, the coverage of ART and the notification rates of TB to provide a firm basis for making future projections. We use the data to assess the extent to which behaviour change and ART have reduced the number of people living with HIV who remain infectious."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of ART on the reduction of infectious HIV cases in Eastern and Southern Africa, considering the role of behavior change in this process. How do the UNAIDS targets for ART coverage in 2015, 2020, and 2030 relate to the current state of the epidemic in this region?\n\n**A)** The UNAIDS targets for ART coverage in 2015, 2020, and 2030 are likely to be met, as the current prevalence of HIV in Eastern and Southern Africa is decreasing due to increased access to ART. However, behavior change is a crucial factor in reducing infectious cases, as it can lead to increased adherence to treatment and reduced transmission rates.\n\n**B)** The UNAIDS targets for ART coverage in 2015, 2020, and 2030 are unlikely to be met, as the current notification rates of TB and high HIV prevalence in Eastern and Southern Africa indicate a need for increased investment in ART and behavior change interventions.\n\n**C)** The impact of ART on reducing infectious HIV cases in Eastern and Southern Africa is limited by the lack of behavior change, which can lead to continued transmission and high rates of HIV prevalence. The UNAIDS targets for ART coverage in 2015, 2020, and 2030 should be revised to prioritize behavior change interventions.\n\n**D)** The UNAIDS targets for ART coverage in 2015, 2020, and 2030 are sufficient to reduce infectious HIV cases in Eastern and Southern Africa, as the region's high HIV prevalence is largely driven by factors other than ART coverage, such as poverty and lack of access to healthcare.\n\n**Correct Answer:** A) The UNAIDS targets for ART coverage in 2015, 2020, and 2030 are likely to be met, as the current prevalence of HIV in Eastern and Southern Africa is decreasing due to increased access to ART. However, behavior change is a crucial factor in reducing infectious cases, as it can lead to increased adherence to treatment and reduced transmission rates.\n\n**Explanation:** This answer choice is correct because it acknowledges the progress made in increasing ART coverage in Eastern and Southern Africa, which has contributed to a decrease in HIV prevalence. However, it also recognizes the importance of behavior change in reducing infectious cases, which is a critical factor in controlling the epidemic. This answer choice demonstrates an understanding of the complex interplay between ART coverage and behavior change in reducing infectious HIV cases."}, "39": {"documentation": {"title": "A k-mer Based Approach for SARS-CoV-2 Variant Identification", "source": "Sarwan Ali, Bikram Sahoo, Naimat Ullah, Alexander Zelikovskiy, Murray\n  Patterson, Imdadullah Khan", "docs_id": "2108.03465", "section": ["q-bio.QM", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A k-mer Based Approach for SARS-CoV-2 Variant Identification. With the rapid spread of the novel coronavirus (COVID-19) across the globe and its continuous mutation, it is of pivotal importance to design a system to identify different known (and unknown) variants of SARS-CoV-2. Identifying particular variants helps to understand and model their spread patterns, design effective mitigation strategies, and prevent future outbreaks. It also plays a crucial role in studying the efficacy of known vaccines against each variant and modeling the likelihood of breakthrough infections. It is well known that the spike protein contains most of the information/variation pertaining to coronavirus variants. In this paper, we use spike sequences to classify different variants of the coronavirus in humans. We show that preserving the order of the amino acids helps the underlying classifiers to achieve better performance. We also show that we can train our model to outperform the baseline algorithms using only a small number of training samples ($1\\%$ of the data). Finally, we show the importance of the different amino acids which play a key role in identifying variants and how they coincide with those reported by the USA's Centers for Disease Control and Prevention (CDC)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach used in the paper \"A k-mer Based Approach for SARS-CoV-2 Variant Identification\" to classify different variants of the coronavirus, and what is the significance of preserving the order of amino acids in this approach?\n\n**A)** The paper proposes a machine learning approach using a k-mer based method to classify variants, where the order of amino acids is not preserved. This approach is significant because it allows for the use of a large dataset to train the model.\n\n**B)** The paper uses a deep learning approach to classify variants, where the order of amino acids is preserved to capture the sequential information in the spike protein sequences. This approach is significant because it enables the model to achieve better performance and identify key amino acids that play a crucial role in variant identification.\n\n**C)** The paper proposes a rule-based approach to classify variants, where the order of amino acids is not preserved. This approach is significant because it allows for the identification of known variants and the prediction of unknown variants.\n\n**D)** The paper uses a k-mer based approach to classify variants, where the order of amino acids is preserved, but the approach is limited to a small number of training samples. This approach is significant because it enables the model to outperform baseline algorithms using a limited dataset.\n\n**Correct Answer:** B) The paper uses a deep learning approach to classify variants, where the order of amino acids is preserved to capture the sequential information in the spike protein sequences. This approach is significant because it enables the model to achieve better performance and identify key amino acids that play a crucial role in variant identification.\n\n**Explanation:** The correct answer is B) because the paper specifically states that preserving the order of amino acids helps the underlying classifiers to achieve better performance. This is a key insight in the paper, and the correct answer reflects this. The other options are incorrect because they either misrepresent the approach used in the paper or do not accurately capture the significance of preserving the order of amino acids."}, "40": {"documentation": {"title": "Arctic Amplification of Anthropogenic Forcing: A Vector Autoregressive\n  Analysis", "source": "Philippe Goulet Coulombe and Maximilian G\\\"obel", "docs_id": "2005.02535", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Arctic Amplification of Anthropogenic Forcing: A Vector Autoregressive\n  Analysis. On September 15th 2020, Arctic sea ice extent (SIE) ranked second-to-lowest in history and keeps trending downward. The understanding of how feedback loops amplify the effects of external CO2 forcing is still limited. We propose the VARCTIC, which is a Vector Autoregression (VAR) designed to capture and extrapolate Arctic feedback loops. VARs are dynamic simultaneous systems of equations, routinely estimated to predict and understand the interactions of multiple macroeconomic time series. The VARCTIC is a parsimonious compromise between full-blown climate models and purely statistical approaches that usually offer little explanation of the underlying mechanism. Our completely unconditional forecast has SIE hitting 0 in September by the 2060's. Impulse response functions reveal that anthropogenic CO2 emission shocks have an unusually durable effect on SIE -- a property shared by no other shock. We find Albedo- and Thickness-based feedbacks to be the main amplification channels through which CO2 anomalies impact SIE in the short/medium run. Further, conditional forecast analyses reveal that the future path of SIE crucially depends on the evolution of CO2 emissions, with outcomes ranging from recovering SIE to it reaching 0 in the 2050's. Finally, Albedo and Thickness feedbacks are shown to play an important role in accelerating the speed at which predicted SIE is heading towards 0."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the implications of the proposed VARCTIC model on the Arctic sea ice extent (SIE) and the feedback loops that amplify the effects of anthropogenic CO2 forcing. How do the findings of the study relate to the current trends in Arctic sea ice extent, and what are the potential consequences of the predicted SIE hitting 0 in the 2060s?\n\n**A)** The VARCTIC model suggests that the Arctic sea ice extent will continue to decline at a steady rate, with no significant feedback loops amplifying the effects of CO2 forcing. This is consistent with current trends in Arctic sea ice extent, which have been declining steadily over the past few decades.\n\n**B)** The study's findings indicate that the Albedo- and Thickness-based feedbacks play a crucial role in accelerating the speed at which SIE is heading towards 0. This is consistent with the predicted SIE hitting 0 in the 2060s, as the feedback loops will continue to amplify the effects of CO2 forcing. However, the study also suggests that the future path of SIE crucially depends on the evolution of CO2 emissions, which could lead to a recovery of SIE in the 2050s.\n\n**C)** The VARCTIC model implies that the Arctic sea ice extent will reach 0 in the 2050s, rather than the 2060s, due to the dominant role of Albedo and Thickness feedbacks in amplifying the effects of CO2 forcing. This is consistent with the study's finding that the future path of SIE crucially depends on the evolution of CO2 emissions.\n\n**D)** The study's findings suggest that the VARCTIC model is unable to capture the complex interactions between multiple macroeconomic time series, and therefore provides little explanation of the underlying mechanism of Arctic feedback loops. This is inconsistent with the study's claim that the VARCTIC model is a parsimonious compromise between full-blown climate models and purely statistical approaches.\n\n**Correct Answer:** B) The study's findings indicate that the Albedo- and Thickness-based feedbacks play a crucial role in accelerating the speed at which SIE is heading towards 0. This is consistent with the predicted SIE hitting 0 in the 2060s, as the feedback loops will continue to amplify the effects of CO2 forcing. However, the study also suggests that the future path of SIE crucially depends on the evolution of CO2 emissions, which could lead to a recovery of SIE in the 2050s.\n\n**Explanation:** The correct answer, B, requires the test-taker to carefully read and analyze the study's findings, identifying the key role of Albedo and Thickness feedbacks in amplifying the effects of CO2 forcing. The test-taker must also consider the implications of the predicted SIE hitting 0 in the 2060s, and how the evolution of CO2 emissions could impact the future path of SIE. The incorrect answers, A, C, and D, require the test-taker to misinterpret or oversimplify the study's findings, demonstrating a lack of understanding of the complex interactions between Arctic feedback loops and CO2 forcing."}, "41": {"documentation": {"title": "A queueing system with on-demand servers: local stability of fluid\n  limits", "source": "Lam M. Nguyen, Alexander Stolyar", "docs_id": "1609.02611", "section": ["math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A queueing system with on-demand servers: local stability of fluid\n  limits. We study a system, where a random flow of customers is served by servers (called agents) invited on-demand. Each invited agent arrives into the system after a random time; after each service completion, an agent returns to the system or leaves it with some fixed probabilities. Customers and/or agents may be impatient, that is, while waiting in queue, they leave the system at a certain rate (which may be zero). We consider the queue-length-based feedback scheme, which controls the number of pending agent invitations, depending on the customer and agent queue lengths and their changes. The basic objective is to minimize both customer and agent waiting times. We establish the system process fluid limits in the asymptotic regime where the customer arrival rate goes to infinity. We use the machinery of switched linear systems and common quadratic Lyapunov functions to approach the stability of fluid limits at the desired equilibrium point, and derive a variety of sufficient local stability conditions. For our model, we conjecture that local stability is in fact sufficient for global stability of fluid limits; the validity of this conjecture is supported by numerical and simulation experiments. When local stability conditions do hold, simulations show good overall performance of the scheme."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a queueing system with on-demand servers, where customers and agents may be impatient and leave the system at a certain rate. The system process fluid limits are established in the asymptotic regime where the customer arrival rate goes to infinity. What is the primary objective of the control scheme, and what is the underlying assumption that supports the conjecture that local stability is sufficient for global stability of fluid limits?\n\n**A)** Minimize customer waiting times while maximizing agent waiting times, assuming that the system is globally stable.\n**B)** Minimize both customer and agent waiting times, assuming that the system is locally stable and the fluid limits are asymptotically stable.\n**C)** Maximize customer waiting times while minimizing agent waiting times, assuming that the system is globally stable.\n**D)** Minimize customer waiting times while minimizing agent waiting times, assuming that the system is locally stable but not necessarily globally stable.\n\n**Correct Answer:** B) Minimize both customer and agent waiting times, assuming that the system is locally stable and the fluid limits are asymptotically stable.\n\n**Explanation:** The primary objective of the control scheme is to minimize both customer and agent waiting times. The underlying assumption that supports the conjecture is that the system is locally stable, meaning that the fluid limits are asymptotically stable, and that the queue-length-based feedback scheme effectively controls the number of pending agent invitations. This assumption is supported by the use of switched linear systems and common quadratic Lyapunov functions to approach the stability of fluid limits."}, "42": {"documentation": {"title": "Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments", "source": "Markus Wulfmeier, Alex Bewley and Ingmar Posner", "docs_id": "1712.07436", "section": ["stat.ML", "cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments. Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the incremental adversarial domain adaptation approach presented in the paper, and how does it differ from traditional unsupervised domain adaptation methods?\n\n**A)** The approach uses a single alignment step to adapt to large appearance changes, whereas traditional methods require multiple iterations to converge.\n**B)** The incremental approach benefits from unsupervised alignment to a series of intermediate domains, which successively diverge from the labelled source domain, allowing for more effective adaptation to changing environments.\n**C)** The deployment module can be rendered fully independent of retaining large amounts of source training data by approximating the feature distribution with a generative adversarial network.\n**D)** The incremental approach is limited to adapting to minor shifts, whereas traditional methods can handle massive differences over time.\n\n**Correct Answer:** B) The incremental approach benefits from unsupervised alignment to a series of intermediate domains, which successively diverge from the labelled source domain, allowing for more effective adaptation to changing environments.\n\n**Explanation:** The correct answer highlights the key advantage of the incremental approach, which is its ability to adapt to changing environments by leveraging unsupervised alignment to intermediate domains. This allows the model to effectively handle large appearance changes, such as day to night, over time. The other options are incorrect because they either misrepresent the approach (A and D) or only partially capture its benefits (C)."}, "43": {"documentation": {"title": "Calculation of Doublet Capture Rate for Muon Capture in Deuterium within\n  Chiral Effective Field Theory", "source": "J. Adam, Jr., M. Tater, E. Truhlik, E. Epelbaum, R.Machleidt, P. Ricci", "docs_id": "1110.3183", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Calculation of Doublet Capture Rate for Muon Capture in Deuterium within\n  Chiral Effective Field Theory. The doublet capture rate of the negative muon capture in deuterium is calculated employing the nuclear wave functions generated from accurate nucleon-nucleon potentials constructed at next-to-next-to-next-to-leading order of heavy-baryon chiral perturbation theory and the weak meson exchange current operator derived within the same formalism. All but one of the low-energy constants that enter the calculation were fixed from pion-nucleon and nucleon-nucleon scattering data. The low-energy constant d^R (c_D), which cannot be determined from the purely two-nucleon data, was extracted recently from the triton beta-decay and the binding energies of the three-nucleon systems. The calculated values of the doublet capture rates show a rather large spread for the used values of the d^R. Precise measurement of the doublet capture rate in the future will not only help to constrain the value of d^R, but also provide a highly nontrivial test of the nuclear chiral EFT framework. Besides, the precise knowledge of the constant d^R will allow for consistent calculations of other two-nucleon weak processes, such as proton-proton fusion and solar neutrino scattering on deuterons, which are important for astrophysics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation for precise measurement of the doublet capture rate in the future, and what are the potential benefits of determining the constant d^R?\n\nA) To constrain the value of d^R and test the nuclear chiral EFT framework, which will also provide a highly nontrivial test of the framework and enable consistent calculations of other two-nucleon weak processes.\n\nB) To constrain the value of d^R and test the nuclear chiral EFT framework, which will also provide a highly nontrivial test of the framework and enable consistent calculations of other two-nucleon weak processes, but only for astrophysics applications.\n\nC) To constrain the value of d^R and test the nuclear chiral EFT framework, which will also provide a highly nontrivial test of the framework, but only for particle physics applications.\n\nD) To constrain the value of d^R and test the nuclear chiral EFT framework, which will also provide a highly nontrivial test of the framework, but only for laboratory experiments.\n\nCorrect Answer: A) To constrain the value of d^R and test the nuclear chiral EFT framework, which will also provide a highly nontrivial test of the framework and enable consistent calculations of other two-nucleon weak processes.\n\nExplanation: The correct answer is A) because the documentation states that precise measurement of the doublet capture rate will help to constrain the value of d^R, which is a crucial constant in the nuclear chiral EFT framework. Additionally, the precise knowledge of d^R will enable consistent calculations of other two-nucleon weak processes, such as proton-proton fusion and solar neutrino scattering on deuterons, which are important for astrophysics. The other options are incorrect because they either omit the astrophysics application or limit the scope of the measurement to only particle physics or laboratory experiments."}, "44": {"documentation": {"title": "Phase reconstruction from amplitude spectrograms based on\n  von-Mises-distribution deep neural network", "source": "Shinnosuke Takamichi and Yuki Saito and Norihiro Takamune and Daichi\n  Kitamura and Hiroshi Saruwatari", "docs_id": "1807.03474", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase reconstruction from amplitude spectrograms based on\n  von-Mises-distribution deep neural network. This paper presents a deep neural network (DNN)-based phase reconstruction from amplitude spectrograms. In audio signal and speech processing, the amplitude spectrogram is often used for processing, and the corresponding phase spectrogram is reconstructed from the amplitude spectrogram on the basis of the Griffin-Lim method. However, the Griffin-Lim method causes unnatural artifacts in synthetic speech. Addressing this problem, we introduce the von-Mises-distribution DNN for phase reconstruction. The DNN is a generative model having the von Mises distribution that can model distributions of a periodic variable such as a phase, and the model parameters of the DNN are estimated on the basis of the maximum likelihood criterion. Furthermore, we propose a group-delay loss for DNN training to make the predicted group delay close to a natural group delay. The experimental results demonstrate that 1) the trained DNN can predict group delay accurately more than phases themselves, and 2) our phase reconstruction methods achieve better speech quality than the conventional Griffin-Lim method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a von-Mises-distribution deep neural network (DNN) for phase reconstruction from amplitude spectrograms, and how does it address the limitations of the Griffin-Lim method?\n\n**A)** The von-Mises-distribution DNN is designed to improve the accuracy of phase reconstruction, but it does not address the unnatural artifacts caused by the Griffin-Lim method. Instead, it introduces a new loss function to minimize these artifacts.\n\n**B)** The von-Mises-distribution DNN is a generative model that can model distributions of a periodic variable such as a phase, and it is trained on the basis of the maximum likelihood criterion to estimate the model parameters. This approach addresses the limitations of the Griffin-Lim method by providing a more accurate and natural-sounding phase reconstruction.\n\n**C)** The von-Mises-distribution DNN is primarily used for speech processing, and its development is motivated by the need to improve the quality of synthetic speech. The Griffin-Lim method is limited in its ability to produce natural-sounding speech, and the von-Mises-distribution DNN is designed to address this limitation.\n\n**D)** The von-Mises-distribution DNN is a type of convolutional neural network (CNN) that is specifically designed for phase reconstruction from amplitude spectrograms. Its development is motivated by the need to improve the accuracy of phase reconstruction, and it addresses the limitations of the Griffin-Lim method by providing a more accurate and natural-sounding phase reconstruction.\n\n**Correct Answer:** B) The von-Mises-distribution DNN is a generative model that can model distributions of a periodic variable such as a phase, and it is trained on the basis of the maximum likelihood criterion to estimate the model parameters. This approach addresses the limitations of the Griffin-Lim method by providing a more accurate and natural-sounding phase reconstruction.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the primary motivation behind the development of the von-Mises-distribution DNN. The DNN is a generative model that can model distributions of a periodic variable such as a phase, and it is trained on the basis of the maximum likelihood criterion to estimate the model parameters. This approach addresses the limitations of the Griffin-Lim method by providing a more accurate and natural-sounding phase reconstruction. The other options are incorrect because they either do not accurately summarize the primary motivation behind the development of the von-Mises-distribution DNN (A and D) or are too vague or misleading (C)."}, "45": {"documentation": {"title": "On radiating solitary waves in bi-layers with delamination and coupled\n  Ostrovsky equations", "source": "K. R. Khusnutdinova and M. R. Tranter", "docs_id": "1702.07575", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On radiating solitary waves in bi-layers with delamination and coupled\n  Ostrovsky equations. We study the scattering of a long longitudinal radiating bulk strain solitary wave in the delaminated area of a two-layered elastic structure with soft (`imperfect') bonding between the layers within the scope of the coupled Boussinesq equations. The direct numerical modelling of this and similar problems is challenging and has natural limitations. We develop a semi-analytical approach, based on the use of several matched asymptotic multiple-scale expansions and averaging with respect to the fast space variable, leading to the coupled Ostrovsky equations in bonded regions and uncoupled Korteweg-de Vries equations in the delaminated region. We show that the semi-analytical approach agrees well with direct numerical simulations and use it to study the nonlinear dynamics and scattering of the radiating solitary wave in a wide range of bi-layers with delamination. The results indicate that radiating solitary waves could help us to control the integrity of layered structures with imperfect interfaces."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary approach used to study the scattering of a long longitudinal radiating bulk strain solitary wave in a two-layered elastic structure with soft bonding between the layers, as described in the given Arxiv documentation?\n\nA) Direct numerical modelling using finite element methods\nB) Semi-analytical approach based on matched asymptotic multiple-scale expansions and averaging\nC) Coupled Boussinesq equations with uncoupled Korteweg-de Vries equations\nD) Linearized perturbation theory with small amplitude assumptions\n\nCorrect Answer: B) Semi-analytical approach based on matched asymptotic multiple-scale expansions and averaging\n\nExplanation: The correct answer is B) Semi-analytical approach based on matched asymptotic multiple-scale expansions and averaging. The documentation states that the authors develop a semi-analytical approach using this method to study the scattering of the radiating solitary wave in a wide range of bi-layers with delamination. This approach is described as a combination of matched asymptotic multiple-scale expansions and averaging, which is a complex mathematical technique used to solve nonlinear problems.\n\nThe other options are incorrect because:\n\nA) Direct numerical modelling using finite element methods is not mentioned in the documentation as the primary approach.\n\nC) Coupled Boussinesq equations with uncoupled Korteweg-de Vries equations are mentioned as the equations used in the bonded and delaminated regions, respectively, but not as the primary approach.\n\nD) Linearized perturbation theory with small amplitude assumptions is not mentioned in the documentation as a relevant approach."}, "46": {"documentation": {"title": "Anisotropic diffusion and hydrodynamic effects on lamellar relaxation\n  and grain boundary motion in a model of a block copolymer", "source": "Chi-Deuk Yoo and Jorge Vinals", "docs_id": "1106.4282", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anisotropic diffusion and hydrodynamic effects on lamellar relaxation\n  and grain boundary motion in a model of a block copolymer. We consider the effects of anisotropic diffusion and hydrodynamic flows on the relaxation time scales of the lamellar phase of a diblock copolymer. We first extend the two-fluid model of a polymer solution to a block copolymer, and include a tensor mobility for the diffusive relaxation of monomer composition which is consistent with the uniaxial symmetry of the lamellar phase. The resulting equation is coupled to the momentum conservation equation, allowing also for a dissipative stress tensor for a uniaxial fluid. We then study the linear relaxation of weakly perturbed lamellae, and the motion of a tilt grain boundary separating two semi-infinite domains. We find that anisotropic diffusion has a negligible effect on the linear relaxation of the layered phase (in the long wavelenght limit), whereas the introduction of hydrodynamic flows considerably speeds the decay to a rate proportional to $Q^{2}$, where $Q\\ll 1$ is the wavenumber of a transverse perturbation to the lamellar phase (diffusive relaxation scales as $Q^{4}$ instead). On the other hand, grain boundary motion is siginificantly affected by anisotropic diffusion because of the coupling between undulation and permeation diffusive modes within the grain boundary region."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the two-fluid model of a block copolymer, what is the effect of introducing hydrodynamic flows on the relaxation time scales of the lamellar phase, and how does this compare to the effect of anisotropic diffusion?\n\n**A)** The introduction of hydrodynamic flows has a negligible effect on the relaxation time scales of the lamellar phase, whereas anisotropic diffusion speeds up the decay to a rate proportional to $Q^{2}$.\n\n**B)** Anisotropic diffusion has a negligible effect on the relaxation time scales of the lamellar phase, whereas the introduction of hydrodynamic flows speeds up the decay to a rate proportional to $Q^{4}$.\n\n**C)** The introduction of hydrodynamic flows has a significant effect on the relaxation time scales of the lamellar phase, whereas anisotropic diffusion has a negligible effect on the grain boundary motion.\n\n**D)** Anisotropic diffusion has a significant effect on the relaxation time scales of the lamellar phase, whereas the introduction of hydrodynamic flows speeds up the decay to a rate proportional to $Q^{2}$.\n\n**Correct Answer:** A) The introduction of hydrodynamic flows has a negligible effect on the relaxation time scales of the lamellar phase, whereas anisotropic diffusion speeds up the decay to a rate proportional to $Q^{2}$.\n\n**Explanation:** The correct answer is A) because the documentation states that \"anisotropic diffusion has a negligible effect on the linear relaxation of the layered phase (in the long wavelenght limit)\", whereas \"the introduction of hydrodynamic flows considerably speeds the decay to a rate proportional to $Q^{2}$\". This indicates that hydrodynamic flows have a significant effect on the relaxation time scales of the lamellar phase, whereas anisotropic diffusion has a negligible effect."}, "47": {"documentation": {"title": "Search for Second Neutral Pion", "source": "W. A. Perkins", "docs_id": "hep-ph/0110053", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for Second Neutral Pion. There is evidence of a second neutral pion from: (1) the anomalous branching ratios in the reactions p-bar p -> pi pi and p-bar d -> pi pi N, and (2) the 1960's results of Tsai-Chu et al. for antinucleon annihilation stars in emulsions. The anomaly of (1) is eliminated if the two neutral pions in the reactions p-bar p -> pi_0 pi_0 and p-bar d -> pi_0 pi_0 n are not identical. Tsai-Chu et al. observed a second neutral pion that ``decays more rapidly into electron pairs with larger opening angles and more frequently into double pairs.'' One antineutron annihilation event produced three neutral particles (each with a mass of 135 +/- 14 MeV), and each decayed into four electrons with much wider opening angles than those of the internal conversion electrons seen in pi-zero decays. The larger opening angles and much more frequent double pair production could be caused by neutral pions with a lifetime so short that they sometimes decay into photon pairs before they can leave the annihilation nucleus (e.g., Ag) of the emulsion. We discuss several methods of searching for this second neutral pion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most plausible explanation for the observed anomalies in the reactions p-bar p -> pi pi and p-bar d -> pi pi N, and how does it relate to the results of Tsai-Chu et al. for antinucleon annihilation stars in emulsions?\n\nA) The anomalies are due to the presence of a second neutral pion that decays rapidly into electron pairs with larger opening angles and more frequently into double pairs, causing the observed effects.\n\nB) The anomalies are caused by the presence of a second neutral pion that decays into photon pairs before leaving the annihilation nucleus, resulting in the observed larger opening angles and more frequent double pair production.\n\nC) The anomalies are due to the presence of a second neutral pion that is identical to the pi-zero, causing the observed effects.\n\nD) The anomalies are caused by the presence of a second neutral pion that decays into a different type of particle, such as a kaon or a proton.\n\nCorrect Answer: B) The anomalies are caused by the presence of a second neutral pion that decays into photon pairs before leaving the annihilation nucleus, resulting in the observed larger opening angles and more frequent double pair production.\n\nExplanation: The correct answer is B) because the text states that the larger opening angles and more frequent double pair production could be caused by neutral pions with a lifetime so short that they sometimes decay into photon pairs before they can leave the annihilation nucleus. This explanation is supported by the observation of three neutral particles decaying into four electrons with much wider opening angles than those of the internal conversion electrons seen in pi-zero decays."}, "48": {"documentation": {"title": "Mapping Organization Knowledge Network and Social Media Based Reputation\n  Management", "source": "Andry Alamsyah, Maribella Syawiluna", "docs_id": "2102.12337", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mapping Organization Knowledge Network and Social Media Based Reputation\n  Management. Knowledge management is an important aspect of an organization, especially in the ICT industry. Having more control of it is essentials for the organization to stay competitive in the business. One way to assess the organization's knowledge capital is by measuring employee knowledge networks and their personal reputation in social media. Using this measurement, we see how employees build relationships around their peer networks or clients virtually. We are also able to see how knowledge networks support organizational performance. The research objective is to map knowledge network and reputation formulation in order to fully understand how knowledge flow and whether employee reputation has a higher degree of influence in the organization's knowledge network. We particularly develop formulas to measure knowledge networks and personal reputation based on their social media activities. As a case study, we pick an Indonesian ICT company that actively build their business around their employee peer knowledge outside the company. For the knowledge network, we perform data collection by conducting interviews. For reputation management, we collect data from several popular social media. We base our work on Social Network Analysis (SNA) methodology. The result shows that employees' knowledge is directly proportional to their reputation, but there are different reputations level on different social media observed in this research."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the research on Mapping Organization Knowledge Network and Social Media Based Reputation Management, what is the primary objective of measuring employee knowledge networks and personal reputation in social media?\n\n**A)** To assess the organization's knowledge capital and stay competitive in the business.\n**B)** To evaluate the effectiveness of employee training programs.\n**C)** To identify the most influential employees in the organization's knowledge network.\n**D)** To develop formulas to measure knowledge networks and personal reputation based on social media activities.\n\n**Correct Answer:** A) To assess the organization's knowledge capital and stay competitive in the business.\n\n**Explanation:** The research objective is to map knowledge networks and reputation formulation to understand how knowledge flow and whether employee reputation has a higher degree of influence in the organization's knowledge network. This implies that the primary objective is to assess the organization's knowledge capital and stay competitive in the business, which is a key aspect of knowledge management in the ICT industry."}, "49": {"documentation": {"title": "COVID-19 Induced Economic Uncertainty: A Comparison between the United\n  Kingdom and the United States", "source": "Ugur Korkut Pata", "docs_id": "2007.07839", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "COVID-19 Induced Economic Uncertainty: A Comparison between the United\n  Kingdom and the United States. The purpose of this study is to investigate the effects of the COVID-19 pandemic on economic policy uncertainty in the US and the UK. The impact of the increase in COVID-19 cases and deaths in the country, and the increase in the number of cases and deaths outside the country may vary. To examine this, the study employs bootstrap ARDL cointegration approach from March 8, 2020 to May 24, 2020. According to the bootstrap ARDL results, a long-run equilibrium relationship is confirmed for five out of the 10 models. The long-term coefficients obtained from the ARDL models suggest that an increase in COVID-19 cases and deaths outside of the UK and the US has a significant effect on economic policy uncertainty. The US is more affected by the increase in the number of COVID-19 cases. The UK, on the other hand, is more negatively affected by the increase in the number of COVID-19 deaths outside the country than the increase in the number of cases. Moreover, another important finding from the study demonstrates that COVID-19 is a factor of great uncertainty for both countries in the short-term."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on COVID-19 induced economic uncertainty in the UK and the US. How do the results of the bootstrap ARDL cointegration approach support the conclusions drawn by the study?\n\n**A)** The study's use of a bootstrap ARDL cointegration approach provides strong evidence for the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries. However, the results do not provide conclusive evidence for the relative impact of COVID-19 cases and deaths on economic policy uncertainty in the two countries.\n\n**B)** The study's findings suggest that the increase in COVID-19 cases outside of the UK has a significant effect on economic policy uncertainty in the US, while the increase in COVID-19 deaths outside of the UK has a significant negative effect on economic policy uncertainty in the UK. However, the study's use of a bootstrap ARDL cointegration approach does not provide conclusive evidence for the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries.\n\n**C)** The study's results indicate that COVID-19 is a factor of great uncertainty for both countries in the short-term, but the bootstrap ARDL cointegration approach does not provide conclusive evidence for the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries. Furthermore, the study's findings suggest that the UK is more negatively affected by the increase in COVID-19 deaths outside of the country than the increase in COVID-19 cases.\n\n**D)** The study's use of a bootstrap ARDL cointegration approach provides strong evidence for the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries. The results also suggest that the US is more affected by the increase in COVID-19 cases, while the UK is more negatively affected by the increase in COVID-19 deaths outside of the country. Moreover, the study's findings indicate that COVID-19 is a factor of great uncertainty for both countries in the short-term.\n\n**Correct Answer:** D) The study's use of a bootstrap ARDL cointegration approach provides strong evidence for the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries. The results also suggest that the US is more affected by the increase in COVID-19 cases, while the UK is more negatively affected by the increase in COVID-19 deaths outside of the country. Moreover, the study's findings indicate that COVID-19 is a factor of great uncertainty for both countries in the short-term.\n\n**Explanation:** The correct answer is D) because it accurately summarizes the main findings of the study. The study's use of a bootstrap ARDL cointegration approach confirms the existence of a long-run equilibrium relationship between economic policy uncertainty and COVID-19 cases and deaths in both countries. The results also suggest that the US is more affected by the increase in COVID-19 cases, while the UK is more negatively affected by the increase in COVID-19 deaths outside of the country. Furthermore, the study's findings indicate that COVID-19 is a factor of great uncertainty for both countries in the short-term."}, "50": {"documentation": {"title": "Compact Graphene Plasmonic Slot Photodetector on Silicon-on-insulator\n  with High Responsivity", "source": "Zhizhen Ma, Kazuya Kikunage, Hao Wang, Shuai Sun, Rubab Amin, Mohammad\n  Tahersima, Rishi Maiti, Mario Miscuglio, Hamed Dalir, Volker J. Sorger", "docs_id": "1812.00894", "section": ["physics.app-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compact Graphene Plasmonic Slot Photodetector on Silicon-on-insulator\n  with High Responsivity. Graphene has extraordinary electro-optic properties and is therefore a promising candidate for monolithic photonic devices such as photodetectors. However, the integration of this atom-thin layer material with bulky photonic components usually results in a weak light-graphene interaction leading to large device lengths limiting electro-optic performance. In contrast, here we demonstrate a plasmonic slot graphene photodetector on silicon-on-insulator platform with high-responsivity given the 5 um-short device length. We observe that the maximum photocurrent, and hence the highest responsivity, scales inversely with the slot gap width. Using a dual-lithography step, we realize 15 nm narrow slots that show a 15-times higher responsivity per unit device-length compared to photonic graphene photodetectors. Furthermore, we reveal that the back-gated electrostatics is overshadowed by channel-doping contributions induced by the contacts of this ultra-short channel graphene photodetector. This leads to quasi charge neutrality, which explains both the previously-unseen offset between the maximum photovoltaic-based photocurrent relative to graphenes Dirac point and the observed non-ambipolar transport. Such micrometer compact and absorption-efficient photodetectors allow for short-carrier pathways in next-generation photonic components, while being an ideal testbed to study short-channel carrier physics in graphene optoelectronics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the observed quasi charge neutrality in the ultra-short channel graphene photodetector, and how does it impact the device's performance?\n\nA) The back-gated electrostatics is overshadowed by channel-doping contributions from the contacts, leading to quasi charge neutrality and a non-ambipolar transport.\n\nB) The slot gap width is inversely proportional to the maximum photocurrent, resulting in a higher responsivity per unit device-length.\n\nC) The graphene layer is doped with impurities, causing a shift in the Dirac point and leading to quasi charge neutrality.\n\nD) The device length is too short, causing the carriers to recombine before reaching the contacts, resulting in quasi charge neutrality.\n\nCorrect Answer: A) The back-gated electrostatics is overshadowed by channel-doping contributions from the contacts, leading to quasi charge neutrality and a non-ambipolar transport.\n\nExplanation: The correct answer is A) because the documentation states that the back-gated electrostatics is overshadowed by channel-doping contributions induced by the contacts of the ultra-short channel graphene photodetector, leading to quasi charge neutrality. This is the primary reason for the observed quasi charge neutrality in the device. The other options are incorrect because they do not accurately describe the reason for quasi charge neutrality in the device. Option B is incorrect because the responsivity is inversely proportional to the slot gap width, not the maximum photocurrent. Option C is incorrect because there is no mention of impurities in the graphene layer. Option D is incorrect because the device length is not too short, but rather, it is ultra-short, which is actually beneficial for the device's performance."}, "51": {"documentation": {"title": "A Core of E-Commerce Customer Experience based on Conversational Data\n  using Network Text Methodology", "source": "Andry Alamsyah, Nurlisa Laksmiani, Lies Anisa Rahimi", "docs_id": "2102.09107", "section": ["econ.GN", "cs.CY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Core of E-Commerce Customer Experience based on Conversational Data\n  using Network Text Methodology. E-commerce provides an efficient and effective way to exchange goods between sellers and customers. E-commerce has been a popular method for doing business, because of its simplicity of having commerce activity transparently available, including customer voice and opinion about their own experience. Those experiences can be a great benefit to understand customer experience comprehensively, both for sellers and future customers. This paper applies to e-commerces and customers in Indonesia. Many Indonesian customers expressed their voice to open social network services such as Twitter and Facebook, where a large proportion of data is in the form of conversational data. By understanding customer behavior through open social network service, we can have descriptions about the e-commerce services level in Indonesia. Thus, it is related to the government's effort to improve the Indonesian digital economy ecosystem. A method for finding core topics in large-scale internet unstructured text data is needed, where the method should be fast but sufficiently accurate. Processing large-scale data is not a straightforward job, it often needs special skills of people and complex software and hardware computer system. We propose a fast methodology of text mining methods based on frequently appeared words and their word association to form network text methodology. This method is adapted from Social Network Analysis by the model relationships between words instead of actors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the authors' proposal of a network text methodology for analyzing conversational data in e-commerce, and how does this approach relate to the government's efforts to improve the Indonesian digital economy ecosystem?\n\nA) To identify the most influential customers in the e-commerce industry and provide personalized marketing strategies.\nB) To understand customer behavior and sentiment towards e-commerce services in Indonesia, which can inform policy decisions to improve the digital economy ecosystem.\nC) To develop a new algorithm for sentiment analysis of text data, which can be applied to various industries beyond e-commerce.\nD) To create a social network analysis model that can predict the likelihood of customer churn in e-commerce businesses.\n\nCorrect Answer: B) To understand customer behavior and sentiment towards e-commerce services in Indonesia, which can inform policy decisions to improve the digital economy ecosystem.\n\nExplanation: The question requires the test-taker to understand the context of the paper and the motivation behind the authors' proposal. The correct answer, B, is supported by the text, which states that the authors aim to \"understand customer experience comprehensively, both for sellers and future customers\" and that this can inform the government's efforts to improve the Indonesian digital economy ecosystem. The other options are incorrect because they do not accurately reflect the primary motivation behind the authors' proposal. Option A is too narrow in focus, option C is unrelated to the context of the paper, and option D is not mentioned in the text as a motivation for the authors' proposal."}, "52": {"documentation": {"title": "A unified meshfree pseudospectral method for solving both classical and\n  fractional PDEs", "source": "John Burkardt, Yixuan Wu, Yanzhi Zhang", "docs_id": "2009.10811", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A unified meshfree pseudospectral method for solving both classical and\n  fractional PDEs. In this paper, we propose a meshfree method based on the Gaussian radial basis function (RBF) to solve both classical and fractional PDEs. The proposed method takes advantage of the analytical Laplacian of Gaussian functions so as to accommodate the discretization of the classical and fractional Laplacian in a single framework and avoid the large computational cost for numerical evaluation of the fractional derivatives. These important merits distinguish it from other numerical methods for fractional PDEs. Moreover, our method is simple and easy to handle complex geometry and local refinement, and its computer program implementation remains the same for any dimension $d \\ge 1$. Extensive numerical experiments are provided to study the performance of our method in both approximating the Dirichlet Laplace operators and solving PDE problems. Compared to the recently proposed Wendland RBF method, our method exactly incorporates the Dirichlet boundary conditions into the scheme and is free of the Gibbs phenomenon as observed in the literature. Our studies suggest that to obtain good accuracy the shape parameter cannot be too small or too big, and the optimal shape parameter might depend on the RBF center points and the solution properties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed meshfree pseudospectral method in accommodating the discretization of both classical and fractional Laplacians in a single framework?\n\nA) It reduces the computational cost for numerical evaluation of the fractional derivatives.\nB) It allows for the incorporation of Dirichlet boundary conditions into the scheme.\nC) It enables the use of Gaussian radial basis functions for both classical and fractional PDEs.\nD) It provides a simple and easy-to-handle method for complex geometry and local refinement.\n\nCorrect Answer: B) It allows for the incorporation of Dirichlet boundary conditions into the scheme.\n\nExplanation: The correct answer is B) It allows for the incorporation of Dirichlet boundary conditions into the scheme. This is stated in the documentation as \"our method exactly incorporates the Dirichlet boundary conditions into the scheme and is free of the Gibbs phenomenon as observed in the literature.\" This is a key advantage of the proposed method, as it allows for more accurate solutions to PDE problems. The other options are incorrect because while they may be related to the method, they are not the primary advantage. Option A is incorrect because the method actually avoids the large computational cost for numerical evaluation of the fractional derivatives. Option C is incorrect because while the method does use Gaussian radial basis functions, this is not the primary advantage. Option D is incorrect because while the method is simple and easy to handle complex geometry and local refinement, this is not the primary advantage."}, "53": {"documentation": {"title": "Achieving Small Test Error in Mildly Overparameterized Neural Networks", "source": "Shiyu Liang, Ruoyu Sun and R. Srikant", "docs_id": "2104.11895", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Achieving Small Test Error in Mildly Overparameterized Neural Networks. Recent theoretical works on over-parameterized neural nets have focused on two aspects: optimization and generalization. Many existing works that study optimization and generalization together are based on neural tangent kernel and require a very large width. In this work, we are interested in the following question: for a binary classification problem with two-layer mildly over-parameterized ReLU network, can we find a point with small test error in polynomial time? We first show that the landscape of loss functions with explicit regularization has the following property: all local minima and certain other points which are only stationary in certain directions achieve small test error. We then prove that for convolutional neural nets, there is an algorithm which finds one of these points in polynomial time (in the input dimension and the number of data points). In addition, we prove that for a fully connected neural net, with an additional assumption on the data distribution, there is a polynomial time algorithm."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Achieving Small Test Error in Mildly Overparameterized Neural Networks\", what is the main goal of the authors, and what are the two aspects that recent theoretical works on over-parameterized neural nets have focused on?\n\n**A)** The authors aim to develop a new optimization algorithm for neural networks, and the two aspects are optimization and generalization. However, the authors also require a very large width for the neural network.\n**B)** The authors are interested in finding a point with small test error in polynomial time for a binary classification problem with two-layer mildly over-parameterized ReLU networks, and the two aspects are optimization and generalization.\n**C)** The authors aim to prove that neural networks can achieve small test error with a small number of parameters, and the two aspects are optimization and generalization. However, the authors also require a very large width for the neural network.\n**D)** The authors are interested in developing a new neural network architecture that can achieve small test error with a small number of parameters, and the two aspects are optimization and generalization.\n\n**Correct Answer:** B) The authors are interested in finding a point with small test error in polynomial time for a binary classification problem with two-layer mildly over-parameterized ReLU networks, and the two aspects are optimization and generalization.\n\n**Explanation:** The correct answer is B) because the authors' main goal is to find a point with small test error in polynomial time for a binary classification problem with two-layer mildly over-parameterized ReLU networks. The two aspects that recent theoretical works on over-parameterized neural nets have focused on are optimization and generalization, which are also the focus of the authors' work. The other options are incorrect because they either misstate the authors' goal or require a very large width for the neural network, which is not mentioned in the paper."}, "54": {"documentation": {"title": "The longest observation of a low intensity state from a Supergiant Fast\n  X-ray Transient: Suzaku observes IGRJ08408-4503", "source": "L. Sidoli (1), P. Esposito (2,3), L. Ducci (1,4) ((1) INAF-IASF\n  Milano, Italy, (2) INAF, Osservatorio Astronomico di Cagliari, Italy, (3)\n  INFN Pavia, Italy, (4) Dipartimento di Fisica e Matematica, Universita'\n  dell'Insubria, Como, Italy)", "docs_id": "1007.1091", "section": ["astro-ph.HE", "astro-ph.GA", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The longest observation of a low intensity state from a Supergiant Fast\n  X-ray Transient: Suzaku observes IGRJ08408-4503. We report here on the longest deep X-ray observation of a SFXT outside outburst, with an average luminosity level of 1E33 erg/s (assuming 3 kpc distance). This observation was performed with Suzaku in December 2009 and was targeted on IGRJ08408-4503, with a net exposure with the X-ray imaging spectrometer (XIS, 0.4-10 keV) and the hard X-ray detector (HXD, 15-100 keV) of 67.4 ks and 64.7 ks, respectively, spanning about three days. The source was caught in a low intensity state characterized by an initially average X-ray luminosity level of 4E32 erg/s (0.5-10 keV) during the first 120 ks, followed by two long flares (about 45 ks each) peaking at a flux a factor of about 3 higher than the initial pre-flare emission. Both XIS spectra (initial emission and the two subsequent long flares) can be fitted with a double component spectrum, with a soft thermal plasma model together with a power law, differently absorbed. The spectral characteristics suggest that the source is accreting matter even at this very low intensity level. From the HXD observation we place an upper limit of 6E33 erg/s (15-40 keV; 3 kpc distance) to the hard X-ray emission, which is the most stringent constrain to the hard X-ray emission during a low intensity state in a SFXT, to date. The timescale observed for the two low intensity long flares is indicative of an orbital separation of the order of 1E13 cm in IGRJ08408-4503."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the X-ray emission characteristics of IGRJ08408-4503 during its longest deep observation outside of outburst. What can be inferred about the accretion process in this Supergiant Fast X-ray Transient (SFXT) based on the spectral analysis of the XIS and HXD observations?\n\n**A)** The source is in a state of maximum accretion, with a hard X-ray emission that is not constrained by the data.\n\n**B)** The source is accreting matter at a low rate, even during the long flares, and the spectral characteristics suggest a double component spectrum with a soft thermal plasma model and a power law, differently absorbed.\n\n**C)** The source is in a state of quiescence, with a hard X-ray emission that is not detectable during the observation.\n\n**D)** The source is experiencing a burst of accretion, with a hard X-ray emission that is significantly higher than the initial pre-flare emission.\n\n**Correct Answer:** B) The source is accreting matter at a low rate, even during the long flares, and the spectral characteristics suggest a double component spectrum with a soft thermal plasma model and a power law, differently absorbed.\n\n**Explanation:** The correct answer is based on the analysis of the XIS and HXD spectra, which show a double component spectrum with a soft thermal plasma model and a power law, differently absorbed. This suggests that the source is accreting matter at a low rate, even during the long flares, and that the spectral characteristics are consistent with a SFXT in a low intensity state. The other options are incorrect because they do not accurately reflect the analysis of the X-ray emission characteristics of IGRJ08408-4503 during its longest deep observation outside of outburst."}, "55": {"documentation": {"title": "An efficient approximation to the correlated Nakagami-m sums and its\n  application in equal gain diversity receivers", "source": "Nikola Zlatanov, Zoran Hadzi-Velkov and George Karagiannidis", "docs_id": "1005.0734", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An efficient approximation to the correlated Nakagami-m sums and its\n  application in equal gain diversity receivers. There are several cases in wireless communications theory where the statistics of the sum of independent or correlated Nakagami-m random variables (RVs) is necessary to be known. However, a closed-form solution to the distribution of this sum does not exist when the number of constituent RVs exceeds two, even for the special case of Rayleigh fading. In this paper, we present an efficient closed-form approximation for the distribution of the sum of arbitrary correlated Nakagami-m envelopes with identical and integer fading parameters. The distribution becomes exact for maximal correlation, while the tightness of the proposed approximation is validated statistically by using the Chi-square and the Kolmogorov-Smirnov goodness-of-fit tests. As an application, the approximation is used to study the performance of equal-gain combining (EGC) systems operating over arbitrary correlated Nakagami-m fading channels, by utilizing the available analytical results for the error-rate performance of an equivalent maximal-ratio combining (MRC) system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a wireless communication system with an equal-gain combining (EGC) receiver operating over a Nakagami-m fading channel with correlated envelopes. The fading parameters are given as follows:\n\n* The number of correlated Nakagami-m RVs is N = 4.\n* The fading parameters are identical, i.e., m1 = m2 = ... = mn = m.\n* The correlation coefficient between the RVs is given by \u03c1 = 0.8.\n\nAssuming that the Nakagami-m RVs are independent and identically distributed (i.i.d.), derive the probability density function (pdf) of the sum of the RVs, denoted as S. Then, evaluate the expected value of S, E[S], and the variance of S, Var[S].\n\n**A)** The pdf of S is given by a Chi-square distribution with 4 degrees of freedom, i.e., S ~ \u03c7\u00b2(4). The expected value of S is E[S] = 4m, and the variance of S is Var[S] = 4m(1 + \u03c1).\n\n**B)** The pdf of S is given by a Nakagami-m distribution with parameters N = 4 and m = m. The expected value of S is E[S] = 4m, and the variance of S is Var[S] = 4m(1 + \u03c1).\n\n**C)** The pdf of S is given by a truncated Nakagami-m distribution with parameters N = 4, m = m, and a correlation coefficient \u03c1 = 0.8. The expected value of S is E[S] = 4m, and the variance of S is Var[S] = 4m(1 + \u03c1).\n\n**D)** The pdf of S is given by a generalized Gamma distribution with parameters \u03b1 = 4, \u03b2 = m, and \u03c1 = 0.8. The expected value of S is E[S] = 4m, and the variance of S is Var[S] = 4m(1 + \u03c1).\n\n**Correct Answer:** B) The pdf of S is given by a Nakagami-m distribution with parameters N = 4 and m = m. The expected value of S is E[S] = 4m, and the variance of S is Var[S] = 4m(1 + \u03c1).\n\n**Explanation:** The correct answer is B) because the sum of correlated Nakagami-m RVs with identical and integer fading parameters follows a Nakagami-m distribution. The expected value and variance of the sum can be derived using the properties of the Nakagami-m distribution. The other options are incorrect because they do not accurately describe the distribution of the sum of correlated Nakagami-m RVs."}, "56": {"documentation": {"title": "On Periodic solutions for a reduction of Benney chain", "source": "Michael (Misha) Bialy", "docs_id": "0804.2187", "section": ["math.SG", "math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Periodic solutions for a reduction of Benney chain. We study periodic solutions for a quasi-linear system, which is the so called dispersionless Lax reduction of the Benney moments chain. This question naturally arises in search of integrable Hamiltonian systems of the form $ H=p^2/2+u(q,t) $ Our main result classifies completely periodic solutions for 3 by 3 system. We prove that the only periodic solutions have the form of traveling waves, so in particular, the potential $u$ is a function of a linear combination of $t$ and $q$. This result implies that the there are no nontrivial cases of existence of the fourth power integral of motion for $H$: if it exists, then it is equal necessarily to the square of the quadratic one. Our method uses two new general observations. The first is the genuine non-linearity of the maximal and minimal eigenvalues for the system. The second observation uses the compatibility conditions of Gibonns-Tsarev in order to give certain exactness for the system in Riemann invariants. This exactness opens a possibility to apply the Lax analysis of blow up of smooth solutions, which usually does not work for systems of higher order."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a quasi-linear system of the form $H=p^2/2+u(q,t)$, which is the dispersionless Lax reduction of the Benney moments chain. What can be concluded about the existence of nontrivial fourth power integrals of motion for $H$, based on the classification of periodic solutions for the 3x3 system?\n\nA) There are nontrivial fourth power integrals of motion for $H$, which can be expressed as a linear combination of the quadratic integral of motion.\nB) The only fourth power integral of motion for $H$ is the square of the quadratic integral of motion.\nC) The existence of nontrivial fourth power integrals of motion for $H$ is undetermined by the classification of periodic solutions.\nD) The classification of periodic solutions implies that the fourth power integral of motion for $H$ is always zero.\n\nCorrect Answer: B) The only fourth power integral of motion for $H$ is the square of the quadratic integral of motion.\n\nExplanation: The correct answer follows from the main result of the paper, which states that the only periodic solutions have the form of traveling waves, and therefore the potential $u$ is a function of a linear combination of $t$ and $q$. This implies that the fourth power integral of motion for $H$ is necessarily the square of the quadratic integral of motion. The other options are incorrect because they do not follow from the classification of periodic solutions. Option A is incorrect because it implies that there are nontrivial fourth power integrals of motion that are not related to the quadratic integral of motion. Option C is incorrect because the classification of periodic solutions does provide information about the existence of nontrivial fourth power integrals of motion. Option D is incorrect because it implies that the fourth power integral of motion is always zero, which is not necessarily true."}, "57": {"documentation": {"title": "Skewness of local logarithmic exports", "source": "Sung-Gook Choi and Deok-Sun Lee", "docs_id": "2012.15487", "section": ["physics.soc-ph", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Skewness of local logarithmic exports. The distributions of trade values and relationships among countries and product categories reflect how countries select their trade partners and design export portfolios. Here we consider the exporter-importer network and the exporter-product network with directed links weighted by the logarithm of the corresponding export values each year from 1962 to 2018, and study how the weights of the outgoing links from each country are distributed. Such local logarithmic export distributions by destinations and products are found to follow approximately the Gaussian distribution across exporters and time, implying random assignment of export values on logarithmic scale. However, a non-zero skewness is identified, changing from positive to negative as exporters have more partner importers and more product categories in their portfolios. Seeking the origin, we analyze how local exports depend on the out-degree of exporter and the in-degrees of destinations/products and formulate their quantitative and measurable relation incorporating randomness, which uncovers the fundamental nature of the export strategies of individual countries."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the skewness of local logarithmic exports in the context of international trade networks. How do the weights of outgoing links from each country's exporter-importer network and exporter-product network change over time, and what factors influence these changes?\n\n**A)** The weights of outgoing links from each country's exporter-importer network and exporter-product network follow a Gaussian distribution across exporters and time, indicating a random assignment of export values on a logarithmic scale. However, the skewness of these distributions changes from positive to negative as exporters have more partner importers and more product categories in their portfolios.\n\n**B)** The skewness of local logarithmic exports is influenced by the out-degree of exporters and the in-degrees of destinations/products, but the relationship between these factors and export values is not quantitatively measurable.\n\n**C)** The weights of outgoing links from each country's exporter-importer network and exporter-product network are positively correlated with the number of partner importers and product categories in their portfolios, indicating a strategic approach to international trade.\n\n**D)** The skewness of local logarithmic exports is independent of the out-degree of exporters and the in-degrees of destinations/products, and can be explained solely by the random assignment of export values on a logarithmic scale.\n\n**Correct Answer:** A) The weights of outgoing links from each country's exporter-importer network and exporter-product network follow a Gaussian distribution across exporters and time, indicating a random assignment of export values on a logarithmic scale. However, the skewness of these distributions changes from positive to negative as exporters have more partner importers and more product categories in their portfolios.\n\n**Explanation:** The correct answer is based on the analysis of the skewness of local logarithmic exports in the provided documentation. The documentation states that the distributions of trade values and relationships among countries and product categories reflect how countries select their trade partners and design export portfolios. The analysis reveals that the weights of outgoing links from each country's exporter-importer network and exporter-product network follow a Gaussian distribution across exporters and time, indicating a random assignment of export values on a logarithmic scale. However, the skewness of these distributions changes from positive to negative as exporters have more partner importers and more product categories in their portfolios. This indicates that the relationship between the weights of outgoing links and the factors influencing these changes is complex and influenced by multiple factors."}, "58": {"documentation": {"title": "Impact of Metal ns2 Lone Pair on Luminescence Efficiency in\n  Low-Dimensional Halide Perovskites", "source": "Hongliang Shi, Dan Han, Shiyou Chen, and Mao-Hua Du", "docs_id": "1902.04700", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Impact of Metal ns2 Lone Pair on Luminescence Efficiency in\n  Low-Dimensional Halide Perovskites. Based on first-principles calculations, we show that chemically active metal ns2 lone pairs play an important role in exciton relaxation and dissociation in low-dimensional halide perovskites. We studied excited-state properties of several recently discovered luminescent all-inorganic and hybrid organic-inorganic zero-dimensional (0D) Sn and Pb halides. The results show that, despite the similarity in ground-state electronic structure between Sn and Pb halide perovskites, the chemically more active Sn2+ lone pair leads to stronger excited-state structural distortion and larger Stokes shift in Sn halides. The enhanced Stokes shift hinders excitation energy transport, which reduces energy loss to defects and increases the photoluminescence quantum efficiency (PLQE). The presence of the ns2 metal cations in the 0D halide perovskites also promotes the exciton dissociation into electron and hole polarons especially in all-inorganic compounds, in which the coupling between metal-halide clusters is significant."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of chemically active metal ns2 lone pairs on the luminescence efficiency in low-dimensional halide perovskites?\n\nA) They lead to a decrease in Stokes shift, resulting in increased energy loss to defects.\nB) They promote the formation of exciton-polaron pairs, leading to enhanced energy loss to defects.\nC) They cause a stronger excited-state structural distortion, resulting in a larger Stokes shift and increased photoluminescence quantum efficiency (PLQE).\nD) They have no significant impact on the luminescence efficiency in low-dimensional halide perovskites.\n\nCorrect Answer: C) They cause a stronger excited-state structural distortion, resulting in a larger Stokes shift and increased photoluminescence quantum efficiency (PLQE).\n\nExplanation: The correct answer is based on the statement in the documentation that \"the chemically more active Sn2+ lone pair leads to stronger excited-state structural distortion and larger Stokes shift in Sn halides.\" This distortion results in a larger Stokes shift, which hinders excitation energy transport and reduces energy loss to defects, ultimately increasing the photoluminescence quantum efficiency (PLQE)."}, "59": {"documentation": {"title": "A Simple 1-1/e Approximation for Oblivious Bipartite Matching", "source": "Zhihao Gavin Tang, Xiaowei Wu, Yuhao Zhang", "docs_id": "2002.06037", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Simple 1-1/e Approximation for Oblivious Bipartite Matching. We study the oblivious matching problem, which aims at finding a maximum matching on a graph with unknown edge set. Any algorithm for the problem specifies an ordering of the vertex pairs. The matching is then produced by probing the pairs following the ordering, and including a pair if both of them are unmatched and there exists an edge between them. The unweighted (Chan et al. (SICOMP 2018)) and the vertex-weighted (Chan et al. (TALG 2018)) versions of the problem are well studied. In this paper, we consider the edge-weighted oblivious matching problem on bipartite graphs, which generalizes the stochastic bipartite matching problem. Very recently, Gamlath et al. (SODA 2019) studied the stochastic bipartite matching problem, and proposed an (1-1/e)-approximate algorithm. We give a very simple algorithm adapted from the Ranking algorithm by Karp et al. (STOC 1990), and show that it achieves the same (1-1/e) approximation ratio for the oblivious matching problem on bipartite graph."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the paper \"A Simple 1-1/e Approximation for Oblivious Bipartite Matching\" in the context of the oblivious matching problem on bipartite graphs?\n\n**A)** The authors propose a new algorithm that achieves a 1-1/e approximation ratio for the oblivious matching problem on bipartite graphs, which generalizes the stochastic bipartite matching problem.\n\n**B)** The authors provide a new lower bound for the oblivious matching problem on bipartite graphs, which shows that any algorithm must achieve a 1-1/e approximation ratio.\n\n**C)** The authors adapt the Ranking algorithm by Karp et al. (STOC 1990) to solve the oblivious matching problem on bipartite graphs and show that it achieves the same (1-1/e) approximation ratio as the stochastic bipartite matching problem.\n\n**D)** The authors prove that the oblivious matching problem on bipartite graphs is NP-hard and show that any algorithm must achieve a 1-1/e approximation ratio.\n\n**Correct Answer:** C) The authors adapt the Ranking algorithm by Karp et al. (STOC 1990) to solve the oblivious matching problem on bipartite graphs and show that it achieves the same (1-1/e) approximation ratio as the stochastic bipartite matching problem.\n\n**Explanation:** The correct answer is C) because the paper states that the authors \"give a very simple algorithm adapted from the Ranking algorithm by Karp et al. (STOC 1990), and show that it achieves the same (1-1/e) approximation ratio for the oblivious matching problem on bipartite graph.\" This indicates that the main contribution of the paper is to adapt the Ranking algorithm to solve the oblivious matching problem on bipartite graphs and achieve the same approximation ratio as the stochastic bipartite matching problem."}}