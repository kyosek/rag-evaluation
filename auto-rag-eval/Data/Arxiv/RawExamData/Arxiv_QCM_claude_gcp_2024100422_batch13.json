{"0": {"documentation": {"title": "Anomalous switching in Nb/Ru/Sr2RuO4 topological junctions by chiral\n  domain wall motion", "source": "M. S. Anwar, Taketomo Nakamura, S. Yonezawa, M. Yakabe, R. Ishiguro,\n  H. Takayanagi and Y. Maeno", "docs_id": "1308.2460", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anomalous switching in Nb/Ru/Sr2RuO4 topological junctions by chiral\n  domain wall motion. A spontaneous symmetry breaking in a system often results in domain wall formation. The motion of such domain walls is utilized to realize novel devices like racetrack-memories, in which moving ferromagnetic domain walls store and carry information. Superconductors breaking time reversal symmetry can also form domains with degenerate chirality of their superconducting order parameter. Sr2RuO4 is the leading candidate of a chiral p-wave superconductor, expected to be accompanied by chiral domain structure. Here, we present that Nb/Ru/Sr2RuO4 topological superconducting-junctions, with which the phase winding of order parameter can be effectively probed by making use of real-space topology, exhibit unusual switching between higher and lower critical current states. This switching is well explained by chiral-domain-wall dynamics. The switching can be partly controlled by external parameters such as temperature, magnetic field and current. These results open up a possibility to utilize the superconducting chiral domain wall motion for future novel superconducting devices."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the Nb/Ru/Sr2RuO4 topological superconducting junctions, what is the primary mechanism proposed to explain the unusual switching between higher and lower critical current states?\n\nA) Ferromagnetic domain wall motion\nB) Chiral domain wall dynamics\nC) Racetrack memory formation\nD) Time reversal symmetry breaking\n\nCorrect Answer: B\n\nExplanation: The correct answer is B) Chiral domain wall dynamics. The documentation explicitly states that the unusual switching observed in Nb/Ru/Sr2RuO4 topological superconducting junctions \"is well explained by chiral-domain-wall dynamics.\" This mechanism is specific to the chiral p-wave superconductor Sr2RuO4, which is expected to form chiral domains.\n\nA) is incorrect because ferromagnetic domain wall motion is mentioned in the context of racetrack memories, not the Sr2RuO4 junctions.\n\nC) is incorrect as racetrack memories are mentioned as an example of a different application of domain wall motion, not the mechanism in Sr2RuO4.\n\nD) is incorrect because while time reversal symmetry breaking is a property of the superconductor, it is not the direct mechanism causing the switching behavior.\n\nThis question tests the student's ability to identify the key mechanism from the given information and distinguish it from related but incorrect concepts mentioned in the text."}, "1": {"documentation": {"title": "Defect Level Switching for Highly-Nonlinear and Hysteretic Electronic\n  Devices", "source": "Han Yin, Abinash Kumar, James M. LeBeau, R. Jaramillo", "docs_id": "2005.07935", "section": ["physics.app-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Defect Level Switching for Highly-Nonlinear and Hysteretic Electronic\n  Devices. Nonlinear and hysteretic electrical devices are needed for applications from circuit protection to next-generation computing. Widely-studied devices for resistive switching are based on mass transport, such as the drift of ions in an electric field, and on collective phenomena, such as insulator-metal transitions. We ask whether the large photoconductive response known in many semiconductors can be stimulated in the dark and harnessed to design electrical devices. We design and test devices based on photoconductive CdS, and our results are consistent with the hypothesis that resistive switching arises from point defects that switch between deep- and shallow-donor configurations: defect level switching (DLS). This new electronic device design principle - photoconductivity without photons - leverages decades of research on photoconductivity and defect spectroscopy. It is easily generalized and will enable the rational design of new nonlinear, hysteretic devices for future electronics."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the principle of Defect Level Switching (DLS) and its potential impact on electronic device design?\n\nA) DLS is based on the drift of ions in an electric field and aims to improve existing mass transport-based resistive switching devices.\n\nB) DLS utilizes collective phenomena such as insulator-metal transitions to create nonlinear and hysteretic electrical devices.\n\nC) DLS harnesses the photoconductive response of semiconductors without the need for photons, enabling the design of novel nonlinear and hysteretic devices.\n\nD) DLS is a technique that enhances the photoconductivity of materials like CdS by increasing their exposure to light, leading to improved electrical device performance.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation describes Defect Level Switching (DLS) as a new electronic device design principle that leverages the large photoconductive response known in semiconductors, but stimulates this response in the dark. This approach is described as \"photoconductivity without photons\" and involves point defects switching between deep- and shallow-donor configurations.\n\nAnswer A is incorrect because it confuses DLS with mass transport-based resistive switching, which is mentioned as a different, widely-studied approach.\n\nAnswer B is incorrect as it mistakenly associates DLS with collective phenomena like insulator-metal transitions, which is another separate approach mentioned in the text.\n\nAnswer D is incorrect because it misinterprets the concept of DLS. Rather than enhancing photoconductivity through increased light exposure, DLS aims to stimulate a photoconductive-like response in the dark.\n\nThe correct answer highlights the novel aspect of DLS: harnessing photoconductive-like responses without actual photons, which opens up new possibilities for designing nonlinear and hysteretic electrical devices."}, "2": {"documentation": {"title": "Review on Master Patient Index", "source": "Dr W.G Prabath Jayatissa, Prof Vajira H W Dissanayake, Dr Roshan\n  Hewapathirane", "docs_id": "1803.05994", "section": ["cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Review on Master Patient Index. In today's health care establishments there is a great diversity of information systems. Each with different specificities and capacities, proprietary communication methods, and hardly allow scalability. This set of characteristics hinders the interoperability of all these systems, in the search for the good of the patient. It is vulgar that, when we look at all the databases of each of these information systems, we come across different registers that refer to the same person; records with insufficient data; records with erroneous data due to errors or misunderstandings when inserting patient data; and records with outdated data. These problems cause duplicity, incoherence, discontinuation and dispersion in patient data. With the intention of minimizing these problems that the concept of a Master Patient Index is necessary. A Master Patient Index proposes a centralized repository, which indexes all patient records of a given set of information systems. Which is composed of a set of demographic data sufficient to unambiguously identify a person and a list of identifiers that identify the various records that the patient has in the repositories of each information system. This solution allows for synchronization between all the actors, minimizing incoherence, out datedness, lack of data, and a decrease in duplicate registrations. The Master Patient Index is an asset to patients, the medical staff and health care providers."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following best describes the primary function and benefit of a Master Patient Index (MPI) in healthcare information systems?\n\nA) It creates a new, separate database that replaces all existing patient information systems in a healthcare establishment.\n\nB) It serves as a centralized repository that indexes patient records across multiple information systems, improving data coherence and reducing duplications.\n\nC) It's a software tool that automatically corrects all errors in patient demographic data across various healthcare databases.\n\nD) It's a standardized communication protocol that allows different proprietary healthcare information systems to directly share patient data.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The Master Patient Index (MPI) serves as a centralized repository that indexes patient records across multiple information systems, improving data coherence and reducing duplications. This accurately reflects the main purpose and benefit of an MPI as described in the given text.\n\nOption A is incorrect because an MPI doesn't replace existing systems; instead, it works alongside them to improve data management.\n\nOption C is incorrect because while an MPI can help identify inconsistencies, it doesn't automatically correct all errors in patient data.\n\nOption D is incorrect because an MPI is not a communication protocol. While it does improve interoperability, it does so by creating a central index, not by enabling direct communication between different systems.\n\nThe correct answer captures the essence of an MPI: a centralized solution that indexes patient records across various systems, helping to minimize problems like data incoherence, outdated information, and duplicate registrations."}, "3": {"documentation": {"title": "Improving living biomass C-stock loss estimates by combining optical\n  satellite, airborne laser scanning, and NFI data", "source": "Johannes Breidenbach, Janis Ivanovs, Annika Kangas, Thomas\n  Nord-Larsen, Mats Nilson, Rasmus Astrup", "docs_id": "2012.07921", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving living biomass C-stock loss estimates by combining optical\n  satellite, airborne laser scanning, and NFI data. Policy measures and management decisions aiming at enhancing the role of forests in mitigating climate-change require reliable estimates of C-stock dynamics in greenhouse gas inventories (GHGIs). Aim of this study was to assemble design-based estimators to provide estimates relevant for GHGIs using national forest inventory (NFI) data. We improve basic expansion (BE) estimates of living-biomass C-stock loss using field-data only, by leveraging with remotely-sensed auxiliary data in model-assisted (MA) estimates. Our case studies from Norway, Sweden, Denmark, and Latvia covered an area of >70 Mha. Landsat-based Forest Cover Loss (FCL) and one-time wall-to-wall airborne laser scanning (ALS) data served as auxiliary data. ALS provided information on the C-stock before a potential disturbance indicated by FCL. The use of FCL in MA estimators resulted in considerable efficiency gains which in most cases were further increased by using ALS in addition. A doubling of efficiency was possible for national estimates and even larger efficiencies were observed at the sub-national level. Average annual estimates were considerably more precise than pooled estimates using NFI data from all years at once. The combination of remotely-sensed with NFI field data yields reliable estimates which is not necessarily the case when using remotely-sensed data without reference observations."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which combination of data sources and estimation methods provided the most significant improvement in estimating living biomass C-stock loss, as described in the study?\n\nA) National Forest Inventory (NFI) data with Basic Expansion (BE) estimates\nB) NFI data combined with Landsat-based Forest Cover Loss (FCL) using Model-Assisted (MA) estimates\nC) NFI data combined with both FCL and Airborne Laser Scanning (ALS) data using MA estimates\nD) ALS data alone using BE estimates\n\nCorrect Answer: C\n\nExplanation: The study aimed to improve living biomass C-stock loss estimates by combining different data sources. While NFI data with BE estimates provided a baseline, the introduction of remotely-sensed auxiliary data in MA estimates led to significant improvements. The combination of NFI data with Landsat-based FCL using MA estimates resulted in considerable efficiency gains. However, the most significant improvement came from using NFI data combined with both FCL and ALS data in MA estimates. This combination allowed for information on C-stock before potential disturbances (from ALS) and detection of forest cover loss (from FCL), leading to the highest efficiency gains, particularly at sub-national levels. The study explicitly states that \"The use of FCL in MA estimators resulted in considerable efficiency gains which in most cases were further increased by using ALS in addition.\" Therefore, option C represents the most comprehensive and effective approach described in the study."}, "4": {"documentation": {"title": "Modulation of Control Authority in Adaptive HapticShared Control\n  Paradigms", "source": "Vahid Izadi, Amir H. Ghasemi", "docs_id": "2007.07436", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modulation of Control Authority in Adaptive HapticShared Control\n  Paradigms. This paper presents an adaptive haptic shared control framework wherein a driver and an automation system are physically connected through a motorized steering wheel. The automation system is modeled as an intelligent agent that is not only capable of making decisions but also monitoring the human's behavior and adjusting its behavior accordingly. To enable the automation system to smoothly exchange the control authority with the human partner, this paper introduces a novel self-regulating impedance controller for the automation system. To determine an optimal modulation policy, a cost function is defined. The terms of the cost function are assigned to minimize the performance error and reduce the disagreement between the human and automation system. To solve the optimal control problem, we employed a nonlinear model predictive approach and used the continuation generalized minimum residual method to solve the nonlinear cost function. To demonstrate the effectiveness of the proposed approach, simulation studies consider a scenario where the human and the automation system both detect an obstacle and negotiate on controlling the steering wheel so that the obstacle can be avoided safely. The simulations involve four interaction modes addressing the cooperation status (cooperative and uncooperative) and the desired direction of the control transfer (active safety and autopilot)."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the adaptive haptic shared control framework described, which of the following best characterizes the primary purpose of the self-regulating impedance controller introduced for the automation system?\n\nA) To maximize the disagreement between the human and automation system\nB) To enable smooth exchange of control authority between human and automation\nC) To increase the performance error of the overall system\nD) To eliminate the need for human input in obstacle avoidance scenarios\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper introduces a \"novel self-regulating impedance controller for the automation system\" with the specific purpose of enabling \"the automation system to smoothly exchange the control authority with the human partner.\" This is a key feature of the adaptive haptic shared control framework presented in the paper.\n\nOption A is incorrect because the system aims to reduce, not maximize, disagreement between human and automation, as indicated by the cost function that includes a term to \"reduce the disagreement between the human and automation system.\"\n\nOption C is also incorrect. The cost function is designed to \"minimize the performance error,\" not increase it.\n\nOption D is incorrect because the framework is based on shared control and cooperation between human and automation, not on eliminating human input. The simulation studies specifically involve scenarios where \"the human and the automation system both detect an obstacle and negotiate on controlling the steering wheel.\""}, "5": {"documentation": {"title": "Implications of a frame dependent gravitational effective action for\n  perturbations on the Robertson-Walker Metric", "source": "Stephen L. Adler", "docs_id": "1704.00388", "section": ["gr-qc", "astro-ph.CO", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of a frame dependent gravitational effective action for\n  perturbations on the Robertson-Walker Metric. In earlier work we showed that a frame dependent effective action motivated by the postulates of three-space general coordinate invariance and Weyl scaling invariance exactly mimics a cosmological constant in Robertson-Walker (RW) spacetimes. Here we study the implications of this effective action for small fluctuations around a spatially flat RW background geometry. The equations for the conserving extension of the modified stress-energy tensor can be integrated in closed form, and involve only the metric perturbation $h_{00}$. Hence the equations for tensor and vector perturbations are unmodified, but there are Hubble scale additions to the scalar perturbation equations, which nonetheless admit no propagating wave solutions. Consequently, there are no modifications to standard gravitational wave propagation theory, but there may be observable implications for cosmology. We give a self-contained discussion, including an analysis of the restricted class of gauge transformations that act when a frame dependent effective action is present."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of a frame dependent gravitational effective action for perturbations on the Robertson-Walker Metric, which of the following statements is correct?\n\nA) The equations for tensor and vector perturbations are significantly modified, leading to changes in gravitational wave propagation theory.\n\nB) The modified stress-energy tensor equations can only be solved numerically and involve all components of the metric perturbation tensor.\n\nC) The scalar perturbation equations receive Hubble scale additions but still admit propagating wave solutions.\n\nD) The equations for the conserving extension of the modified stress-energy tensor can be integrated in closed form and involve only the metric perturbation h_{00}.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that \"The equations for the conserving extension of the modified stress-energy tensor can be integrated in closed form, and involve only the metric perturbation h_{00}.\" This is a key feature of the frame dependent effective action's implications.\n\nAnswer A is incorrect because the documentation clearly states that \"the equations for tensor and vector perturbations are unmodified\" and there are \"no modifications to standard gravitational wave propagation theory.\"\n\nAnswer B is wrong on two counts. First, the equations can be integrated in closed form, not just numerically. Second, they involve only h_{00}, not all components of the metric perturbation tensor.\n\nAnswer C is partially correct in mentioning the Hubble scale additions to scalar perturbation equations, but it's wrong in stating that these admit propagating wave solutions. The documentation specifically mentions that they \"admit no propagating wave solutions.\""}, "6": {"documentation": {"title": "Birdsong dialect patterns explained using magnetic domains", "source": "James Burridge and Steven Kenney", "docs_id": "1603.04429", "section": ["q-bio.PE", "q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Birdsong dialect patterns explained using magnetic domains. The songs and calls of many bird species, like human speech, form distinct regional dialects. We suggest that the process of dialect formation is analogous to the physical process of magnetic domain formation. We take the coastal breeding grounds of the Puget Sound white crowned sparrow as an example. Previous field studies suggest that birds of this species learn multiple songs early in life, and when establishing a territory for the first time, retain one of these dialects in order to match the majority of their neighbours. We introduce a simple lattice model of the process, showing that this matching behaviour can produce single dialect domains provided the death rate of adult birds is sufficiently low. We relate death rate to thermodynamic temperature in magnetic materials, and calculate the critical death rate by analogy with the Ising model. Using parameters consistent with the known behavior of these birds we show that coastal dialect domain shapes may be explained by viewing them as low temperature \"stripe states\"."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: The formation of birdsong dialects is compared to which physical process, and what critical factor determines the formation of single dialect domains in the model described?\n\nA) Crystallization; the rate of song learning\nB) Magnetic domain formation; the death rate of adult birds\nC) Phase transitions; the population density of the species\nD) Quantum entanglement; the frequency of territorial disputes\n\nCorrect Answer: B\n\nExplanation: The documentation explicitly states that \"the process of dialect formation is analogous to the physical process of magnetic domain formation.\" This directly corresponds to option B. Furthermore, the text mentions that \"this matching behaviour can produce single dialect domains provided the death rate of adult birds is sufficiently low.\" The death rate of adult birds is identified as the critical factor in determining whether single dialect domains form, which is analogous to temperature in magnetic materials. Options A, C, and D introduce concepts not mentioned in the given text or misattribute the critical factor in dialect formation."}, "7": {"documentation": {"title": "Studies of light nicleus clustering in relativistic multifragmentation\n  processes", "source": "V. Bradnova, M.M. Chernyavsky, A.Sh. Gaitinov, L.A. Goncharova, L.\n  Just, S.P. Kharlamov, A.D. Kovalenko, M. Haiduc, V.G. Larionova, F.G.\n  Lepekhin, A.I. Malakhov, G.I. Orlova, N.G. Peresadko, N.G. Polukhina, P.A.\n  Rukoyatkin, V.V. Rusakova, N.A. Salmanova, B.B. Simonov, S. Vokal, P.I.\n  Zarubin, and I.G. Zarubina", "docs_id": "nucl-ex/0604009", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Studies of light nicleus clustering in relativistic multifragmentation\n  processes. We give an overview of results and prospects of nuclear clustering studies on the grounds of the observations of interactions of light stable and radioactive nuclei with an initial energy above 1 A GeV in nuclear emulsions. Thank to the best spatial resolution and the full solid angle acceptance provided by nuclear emulsions, such an approach allows one to obtain unique and evident observations reecting cluster-like features in light nuclear structures. New results on dissociation of $^7$Be in very peripheral interactions with emulsion nuclei are presented. The importance of this research for the physics of few body nuclear systems and the related problems of nucleosynthesis is noted. The paper is illustrated with characteristic images obtained by means of a microscope equipped with a CCD camera. The discussed explorations are provided with the beams of the Synchrophasotron and Nuclotron of JINR, Dubna. Future investigations are suggested to be carried out in relativistic beams of He, Be, B, C, and N isotopes."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the advantages of using nuclear emulsions in studying light nucleus clustering in relativistic multifragmentation processes?\n\nA) Nuclear emulsions provide the highest energy resolution for detecting nuclear fragments.\nB) Nuclear emulsions allow for the detection of only stable light nuclei.\nC) Nuclear emulsions offer the best spatial resolution and full solid angle acceptance.\nD) Nuclear emulsions are particularly useful for studying heavy nuclei interactions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that \"Thank to the best spatial resolution and the full solid angle acceptance provided by nuclear emulsions, such an approach allows one to obtain unique and evident observations reflecting cluster-like features in light nuclear structures.\" This highlights the key advantages of using nuclear emulsions in these studies.\n\nOption A is incorrect because while nuclear emulsions offer good resolution, the text specifically mentions spatial resolution, not energy resolution.\n\nOption B is false because the documentation mentions studies of both stable and radioactive nuclei, not just stable ones.\n\nOption D is incorrect because the text focuses on light nuclei, not heavy nuclei. The document specifically mentions light stable and radioactive nuclei with energies above 1 A GeV.\n\nThis question tests the student's understanding of the experimental techniques used in nuclear physics research and their ability to extract key information from scientific documentation."}, "8": {"documentation": {"title": "BuildingNet: Learning to Label 3D Buildings", "source": "Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova,\n  Melinos Averkiou, Andreas Andreou, Siddhartha Chaudhuri, Evangelos\n  Kalogerakis", "docs_id": "2110.04955", "section": ["cs.CV", "cs.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BuildingNet: Learning to Label 3D Buildings. We introduce BuildingNet: (a) a large-scale dataset of 3D building models whose exteriors are consistently labeled, (b) a graph neural network that labels building meshes by analyzing spatial and structural relations of their geometric primitives. To create our dataset, we used crowdsourcing combined with expert guidance, resulting in 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles. We include a benchmark for evaluating mesh and point cloud labeling. Buildings have more challenging structural complexity compared to objects in existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that our dataset can nurture the development of algorithms that are able to cope with such large-scale geometric data for both vision and graphics tasks e.g., 3D semantic segmentation, part-based generative models, correspondences, texturing, and analysis of point cloud data acquired from real-world buildings. Finally, we show that our mesh-based graph neural network significantly improves performance over several baselines for labeling 3D meshes."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements most accurately describes the significance and unique aspects of the BuildingNet dataset and associated graph neural network?\n\nA) It primarily focuses on interior building layouts and is limited to residential structures.\n\nB) It contains 513K annotated mesh primitives across 292K semantic part components, covering various building types, and introduces a graph neural network that outperforms baselines in labeling 3D meshes.\n\nC) It is designed exclusively for point cloud labeling and does not include mesh data or semantic segmentation capabilities.\n\nD) It is a small-scale dataset of 2K building models, focused mainly on simplistic geometric shapes without consideration for structural complexity.\n\nCorrect Answer: B\n\nExplanation: Option B correctly captures the key aspects of BuildingNet as described in the documentation. The dataset contains 513K annotated mesh primitives grouped into 292K semantic part components across 2K building models, covering various building types such as houses, churches, skyscrapers, and more. Additionally, it introduces a graph neural network that significantly improves performance over baselines for labeling 3D meshes by analyzing spatial and structural relations of geometric primitives.\n\nOption A is incorrect because BuildingNet focuses on exterior, not interior, building layouts and covers various building types, not just residential structures.\n\nOption C is wrong because while BuildingNet does include a benchmark for point cloud labeling, it is not exclusively designed for this purpose. It also includes mesh data and capabilities for semantic segmentation.\n\nOption D is incorrect as it understates the scale and complexity of the dataset. BuildingNet is described as a large-scale dataset with 2K building models, emphasizing the challenging structural complexity compared to existing benchmarks."}, "9": {"documentation": {"title": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II", "source": "Jiahao Chen and Nancy Makri", "docs_id": "0812.1552", "section": ["cond-mat.other", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II. The extent of exciton coherence in protein-pigment complexes has significant implications for the initial light harvesting step in photosynthetic organisms. In this work we model the main antenna protein of photosystem II, namely light harvesting complex II (LHC-II), with a single-exciton Hamiltonian with sites coupled via dipole-dipole interaction, with linear coupling to a dissipative phonon bath. With appropriate parameters, Monte Carlo path integral (MCPI) results of the exciton coherence length from 1 K to 500 K show that at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites, with persistent short-range coherence over the A2-B2 pair, A3-B3 pair and B1-B5-B6 triplet. Quasi-adiabatic path integral (QUAPI) calculations of the subsystems mentioned above show a smooth, incoherent relaxation towards thermodynamic equilibrium. The results obtained imply that with the exception of small coherent subsystems at cryogenic temperatures, excitons in LHC-II are more localized than in the analogous light harvesting complex II (LH-II) of the purple bacterium Rs. molischianum, which may be expected from the lower symmetry of the former."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Based on the Monte Carlo path integral (MCPI) results for exciton coherence in Light Harvesting Complex II (LHC-II) at thermodynamic equilibrium, which of the following statements is most accurate?\n\nA) Excitons are delocalized across the entire LHC-II complex at all temperatures.\nB) Excitons are primarily localized on 2 single chlorophyll pigment sites, with coherence persisting only in specific small subsystems.\nC) Excitons show long-range coherence across multiple chlorophyll sites at low temperatures.\nD) Excitons exhibit complete localization on individual chlorophyll sites with no coherence between sites.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that MCPI results show \"at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites, with persistent short-range coherence over the A2-B2 pair, A3-B3 pair and B1-B5-B6 triplet.\" This indicates that excitons are primarily localized on two sites, but small coherent subsystems persist.\n\nAnswer A is incorrect because the excitons are not delocalized across the entire complex, but rather localized on mostly two sites.\n\nAnswer C is incorrect because the coherence is described as \"short-range\" and limited to specific pairs or triplets, not long-range across multiple sites.\n\nAnswer D is incorrect because while excitons are mostly localized, there is still some coherence between specific sites, not complete localization with no coherence.\n\nThis question tests the student's ability to interpret complex scientific findings and distinguish between subtle differences in exciton behavior in photosynthetic systems."}, "10": {"documentation": {"title": "Energetics and Structural Properties of Trapped Two-Component Fermi\n  Gases", "source": "J. von Stecher, Chris H. Greene, D. Blume", "docs_id": "0801.2747", "section": ["cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energetics and Structural Properties of Trapped Two-Component Fermi\n  Gases. Using two different numerical methods, we study the behavior of two-component Fermi gases interacting through short-range s-wave interactions in a harmonic trap. A correlated Gaussian basis-set expansion technique is used to determine the energies and structural properties, i.e., the radial one-body densities and pair distribution functions, for small systems with either even or odd $N$, as functions of the s-wave scattering length and the mass ratio $\\kappa$ of the two species. Particular emphasis is put on a discussion of the angular momentum of the system in the BEC-BCS crossover regime. At unitarity, the excitation spectrum of the four-particle system with total angular momentum L=0 is calculated as a function of the mass ratio $\\kappa$. The results are analyzed from a hyperspherical perspective, which offers new insights into the problem. Additionally, fixed-node diffusion Monte Carlo calculations are performed for equal-mass Fermi gases with up to N=30 atoms. We focus on the odd-even oscillations of the ground state energy of the equal-mass unitary system having up to N=30 particles, which are related to the excitation gap of the system. Furthermore, we present a detailed analysis of the structural properties of these systems."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a study of two-component Fermi gases in a harmonic trap, which of the following statements is most accurate regarding the research methods and focus areas?\n\nA) The study exclusively used a correlated Gaussian basis-set expansion technique for all system sizes and focused primarily on the BEC-BCS crossover regime for large N.\n\nB) Fixed-node diffusion Monte Carlo calculations were employed for systems with up to N=100 atoms, with a primary focus on even-N systems at unitarity.\n\nC) The research combined correlated Gaussian basis-set expansion for small systems and fixed-node diffusion Monte Carlo for larger systems, with emphasis on angular momentum at unitarity and odd-even energy oscillations.\n\nD) The study used only hyperspherical analysis techniques and concentrated on the excitation spectrum of systems with total angular momentum L>0 as a function of mass ratio.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation clearly states that two different numerical methods were used: a correlated Gaussian basis-set expansion technique for small systems, and fixed-node diffusion Monte Carlo calculations for larger systems (up to N=30 atoms). The study emphasized angular momentum in the BEC-BCS crossover regime, calculated the excitation spectrum at unitarity for the four-particle system, and focused on odd-even oscillations of the ground state energy for the equal-mass unitary system. This combination of methods and focus areas is accurately reflected in option C.\n\nOption A is incorrect because it mentions only one method and incorrectly states a focus on large N systems. Option B is wrong as it overstates the number of atoms studied (N=100 instead of N=30) and incorrectly focuses only on even-N systems. Option D is incorrect because it mentions only hyperspherical analysis, which was used for insight but not as the primary method, and it incorrectly states a focus on L>0 systems when the document specifically mentions L=0 for the four-particle system analysis."}, "11": {"documentation": {"title": "Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions", "source": "Wen Yan and Robert Blackwell", "docs_id": "2010.15155", "section": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions. Many different simulation methods for Stokes flow problems involve a common computationally intense task -- the summation of a kernel function over $O(N^2)$ pairs of points. One popular technique is the Kernel Independent Fast Multipole Method (KIFMM), which constructs a spatial adaptive octree for all points and places a small number of equivalent multipole and local equivalent points around each octree box, and completes the kernel sum with $O(N)$ cost, using these equivalent points. Simpler kernels can be used between these equivalent points to improve the efficiency of KIFMM. Here we present further extensions and applications to this idea, to enable efficient summations and flexible boundary conditions for various kernels. We call our method the Kernel Aggregated Fast Multipole Method (KAFMM), because it uses different kernel functions at different stages of octree traversal. We have implemented our method as an open-source software library STKFMM based on the high performance library PVFMM, with support for Laplace kernels, the Stokeslet, regularized Stokeslet, Rotne-Prager-Yamakawa (RPY) tensor, and the Stokes double-layer and traction operators. Open and periodic boundary conditions are supported for all kernels, and the no-slip wall boundary condition is supported for the Stokeslet and RPY tensor. The package is designed to be ready-to-use as well as being readily extensible to additional kernels."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: The Kernel Aggregated Fast Multipole Method (KAFMM) improves upon the Kernel Independent Fast Multipole Method (KIFMM) by:\n\nA) Eliminating the need for an octree structure\nB) Using different kernel functions at different stages of octree traversal\nC) Increasing the computational cost to O(N^2)\nD) Implementing only Laplace kernels\n\nCorrect Answer: B\n\nExplanation: The Kernel Aggregated Fast Multipole Method (KAFMM) is an extension of the Kernel Independent Fast Multipole Method (KIFMM). The key improvement, as stated in the text, is that KAFMM \"uses different kernel functions at different stages of octree traversal.\" This allows for more efficient summations and flexible boundary conditions for various kernels.\n\nOption A is incorrect because KAFMM still uses an octree structure, as mentioned in the text: \"constructs a spatial adaptive octree for all points.\"\n\nOption C is incorrect because KAFMM aims to reduce computational cost, not increase it. The text mentions that KIFMM completes the kernel sum with O(N) cost, and KAFMM builds upon this efficiency.\n\nOption D is too limited. While KAFMM does support Laplace kernels, it also supports various other kernels such as Stokeslet, regularized Stokeslet, Rotne-Prager-Yamakawa tensor, and Stokes double-layer and traction operators."}, "12": {"documentation": {"title": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information", "source": "Farouq Abdulaziz Masoudy", "docs_id": "1801.06966", "section": ["q-fin.GN", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information. Since exchange economy considerably varies in the market assets, asset prices have become an attractive research area for investigating and modeling ambiguous and uncertain information in today markets. This paper proposes a new generative uncertainty mechanism based on the Bayesian Inference and Correntropy (BIC) technique for accurately evaluating asset pricing in markets. This technique examines the potential processes of risk, ambiguity, and variations of market information in a controllable manner. We apply the new BIC technique to a consumption asset-pricing model in which the consumption variations are modeled using the Bayesian network model with observing the dynamics of asset pricing phenomena in the data. These dynamics include the procyclical deviations of price, the countercyclical deviations of equity premia and equity volatility, the leverage impact and the mean reversion of excess returns. The key findings reveal that the precise modeling of asset information can estimate price changes in the market effectively."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the key innovation and findings of the research paper on asset pricing evaluation?\n\nA) The paper introduces a new consumption-based asset pricing model that accurately predicts market crashes.\n\nB) The research proposes a generative uncertainty mechanism based on Bayesian Inference and Correntropy (BIC) to evaluate asset pricing, finding that precise modeling of asset information can effectively estimate price changes.\n\nC) The study develops a novel machine learning algorithm to predict equity premia and volatility in exchange economies.\n\nD) The paper presents a comprehensive review of existing asset pricing models and their limitations in uncertain market conditions.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately captures the main innovation and findings described in the text. The paper introduces a \"new generative uncertainty mechanism based on the Bayesian Inference and Correntropy (BIC) technique\" for evaluating asset pricing. The key finding mentioned is that \"precise modeling of asset information can estimate price changes in the market effectively.\"\n\nAnswer A is incorrect because while the paper does deal with asset pricing, it doesn't specifically mention predicting market crashes.\n\nAnswer C is incorrect because although the paper does consider equity premia and volatility, it doesn't mention developing a machine learning algorithm for prediction.\n\nAnswer D is incorrect because the text doesn't indicate that the paper is a review of existing models. Instead, it proposes a new technique."}, "13": {"documentation": {"title": "High-Power and High-Capacity Mobile Optical SWIPT", "source": "Mingliang Xiong, Qingwen Liu, Shengli Zhou, Shun Han, and Mingqing Liu", "docs_id": "2107.09299", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-Power and High-Capacity Mobile Optical SWIPT. The increasing demands of power supply and data rate for mobile devices promote the research of simultaneous wireless information and power transfer (SWIPT). Optical SWIPT, as known as simultaneous light information and power transfer (SLIPT), has the potential for providing high-capacity communication and high-power wireless charging. However, SLIPT technologies based on light-emitting diodes have low efficiency due to energy dissipation over the air. Laser-based SLIPT technologies need strict positioning accuracy and scanning resolution, which may lead to the increase of costs and complexity. In this paper, we propose a mobile SLIPT scheme based on spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation. The power and data are transferred via separated frequencies, while they share the same self-aligned resonant beam path, without the needs of receiver positioning and beam steering. We establish the analysis model of the resonant beam power and its second harmonic power. Numerical results show that the proposed system can achieve watt-level battery charging power and above 10-bit/s/Hz achievable rate at 8-m distance, which satisfies the requirements of most indoor mobile devices."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the advantages of the proposed mobile SLIPT scheme based on spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation over existing optical SWIPT technologies?\n\nA) It provides higher power transfer efficiency than LED-based SLIPT systems due to reduced energy dissipation over the air.\n\nB) It achieves stricter positioning accuracy and higher scanning resolution compared to laser-based SLIPT technologies.\n\nC) It allows for power and data transfer via separated frequencies while sharing the same self-aligned resonant beam path, eliminating the need for receiver positioning and beam steering.\n\nD) It can only achieve milliwatt-level battery charging power but compensates with extremely high data rates exceeding 100-bit/s/Hz at short distances.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the proposed mobile SLIPT scheme based on SSLR and intra-cavity second harmonic generation allows for power and data transfer via separated frequencies while sharing the same self-aligned resonant beam path. This unique feature eliminates the need for receiver positioning and beam steering, which are significant challenges in other laser-based SLIPT technologies.\n\nOption A is incorrect because while the proposed system may be more efficient than LED-based systems, this is not explicitly stated as its main advantage in the given information.\n\nOption B is incorrect because the proposed system actually eliminates the need for strict positioning accuracy and high scanning resolution, which are limitations of other laser-based SLIPT technologies.\n\nOption D is incorrect because the proposed system can achieve watt-level battery charging power (not just milliwatt-level) and an achievable rate above 10-bit/s/Hz (not exceeding 100-bit/s/Hz) at 8-m distance, according to the given information."}, "14": {"documentation": {"title": "Lovasz Convolutional Networks", "source": "Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth,\n  Arun Rajkumar, Partha Talukdar", "docs_id": "1805.11365", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lovasz Convolutional Networks. Semi-supervised learning on graph structured data has received significant attention with the recent introduction of Graph Convolution Networks (GCN). While traditional methods have focused on optimizing a loss augmented with Laplacian regularization framework, GCNs perform an implicit Laplacian type regularization to capture local graph structure. In this work, we propose Lovasz Convolutional Network (LCNs) which are capable of incorporating global graph properties. LCNs achieve this by utilizing Lovasz's orthonormal embeddings of the nodes. We analyse local and global properties of graphs and demonstrate settings where LCNs tend to work better than GCNs. We validate the proposed method on standard random graph models such as stochastic block models (SBM) and certain community structure based graphs where LCNs outperform GCNs and learn more intuitive embeddings. We also perform extensive binary and multi-class classification experiments on real world datasets to demonstrate LCN's effectiveness. In addition to simple graphs, we also demonstrate the use of LCNs on hyper-graphs by identifying settings where they are expected to work better than GCNs."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the key innovation of Lovasz Convolutional Networks (LCNs) compared to Graph Convolution Networks (GCNs)?\n\nA) LCNs use Laplacian regularization while GCNs do not.\nB) LCNs incorporate global graph properties using Lovasz's orthonormal embeddings.\nC) LCNs are specifically designed for hypergraphs, unlike GCNs.\nD) LCNs perform implicit Laplacian regularization to capture local graph structure.\n\nCorrect Answer: B\n\nExplanation: The key innovation of Lovasz Convolutional Networks (LCNs) is their ability to incorporate global graph properties, which they achieve by utilizing Lovasz's orthonormal embeddings of the nodes. This is in contrast to Graph Convolution Networks (GCNs), which primarily capture local graph structure through implicit Laplacian-type regularization.\n\nOption A is incorrect because both LCNs and GCNs use forms of Laplacian regularization, though GCNs do so implicitly.\n\nOption C is incorrect because while LCNs can be used on hypergraphs, this is not their primary distinguishing feature from GCNs.\n\nOption D describes a feature of GCNs, not LCNs, and thus is incorrect.\n\nThis question tests the student's understanding of the key differences between LCNs and GCNs, particularly in terms of their ability to capture global versus local graph properties."}, "15": {"documentation": {"title": "A hierarchy of Palm measures for determinantal point processes with\n  gamma kernels", "source": "Alexander I. Bufetov and Grigori Olshanski", "docs_id": "1904.13371", "section": ["math.PR", "math-ph", "math.CO", "math.FA", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A hierarchy of Palm measures for determinantal point processes with\n  gamma kernels. The gamma kernels are a family of projection kernels $K^{(z,z')}=K^{(z,z')}(x,y)$ on a doubly infinite $1$-dimensional lattice. They are expressed through Euler's gamma function and depend on two continuous parameters $z,z'$. The gamma kernels initially arose from a model of random partitions via a limit transition. On the other hand, these kernels are closely related to unitarizable representations of the Lie algebra $\\mathfrak{su}(1,1)$. Every gamma kernel $K^{(z,z')}$ serves as a correlation kernel for a determinantal measure $M^{(z,z')}$, which lives on the space of infinite point configurations on the lattice. We examine chains of kernels of the form $$ \\ldots, K^{(z-1,z'-1)}, \\; K^{(z,z')},\\; K^{(z+1,z'+1)}, \\ldots, $$ and establish the following hierarchical relations inside any such chain: Given $(z,z')$, the kernel $K^{(z,z')}$ is a one-dimensional perturbation of (a twisting of) the kernel $K^{(z+1,z'+1)}$, and the one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to $M^{(z+1,z'+1)}$. We also explicitly compute the corresponding Radon-Nikod\\'ym derivatives and show that they are given by certain normalized multiplicative functionals."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Consider a chain of gamma kernels $K^{(z,z')}$ for determinantal point processes. Which of the following statements is correct regarding the hierarchical relations between kernels $K^{(z,z')}$ and $K^{(z+1,z'+1)}$?\n\nA) $K^{(z,z')}$ is a two-dimensional perturbation of $K^{(z+1,z'+1)}$, and the two-point Palm distributions for $M^{(z,z')}$ are absolutely continuous with respect to $M^{(z+1,z'+1)}$.\n\nB) $K^{(z,z')}$ is a one-dimensional perturbation of $K^{(z+1,z'+1)}$, and the one-point Palm distributions for $M^{(z,z')}$ are absolutely continuous with respect to $M^{(z+1,z'+1)}$.\n\nC) $K^{(z,z')}$ is a one-dimensional perturbation of $K^{(z+1,z'+1)}$, and the Radon-Nikod\u00fdm derivatives between $M^{(z,z')}$ and $M^{(z+1,z'+1)}$ are given by unnormalized additive functionals.\n\nD) $K^{(z,z')}$ is a two-dimensional perturbation of the twisting of $K^{(z+1,z'+1)}$, and the Radon-Nikod\u00fdm derivatives between $M^{(z,z')}$ and $M^{(z+1,z'+1)}$ are given by normalized multiplicative functionals.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the given information, for any pair $(z,z')$, the kernel $K^{(z,z')}$ is a one-dimensional perturbation of (a twisting of) the kernel $K^{(z+1,z'+1)}$. Additionally, the one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to $M^{(z+1,z'+1)}$. \n\nOption A is incorrect because it mentions a two-dimensional perturbation and two-point Palm distributions, which are not mentioned in the given information. \n\nOption C is incorrect because, while it correctly states that $K^{(z,z')}$ is a one-dimensional perturbation of $K^{(z+1,z'+1)}$, it incorrectly describes the Radon-Nikod\u00fdm derivatives as unnormalized additive functionals. The information states that they are given by certain normalized multiplicative functionals.\n\nOption D is incorrect because it mentions a two-dimensional perturbation, which is not consistent with the given information. However, it correctly states that the Radon-Nikod\u00fdm derivatives are given by normalized multiplicative functionals."}, "16": {"documentation": {"title": "Single and multiple index functional regression models with\n  nonparametric link", "source": "Dong Chen, Peter Hall, Hans-Georg M\\\"uller", "docs_id": "1211.5018", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single and multiple index functional regression models with\n  nonparametric link. Fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. This difficulty has led to an emphasis on the so-called functional linear model, which is much more flexible than common linear models in finite dimension, but nevertheless imposes structural constraints on the relationship between predictors and responses. Recent advances have extended the linear approach by using it in conjunction with link functions, and by considering multiple indices, but the flexibility of this technique is still limited. For example, the link may be modeled parametrically or on a grid only, or may be constrained by an assumption such as monotonicity; multiple indices have been modeled by making finite-dimensional assumptions. In this paper we introduce a new technique for estimating the link function nonparametrically, and we suggest an approach to multi-index modeling using adaptively defined linear projections of functional data. We show that our methods enable prediction with polynomial convergence rates. The finite sample performance of our methods is studied in simulations, and is illustrated by an application to a functional regression problem."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the advantages and limitations of the approach introduced in the paper for functional regression models?\n\nA) It achieves exponential convergence rates and is limited to single-index models with parametric link functions.\n\nB) It allows for nonparametric estimation of the link function and multi-index modeling, but still has slower than nonparametric convergence rates.\n\nC) It achieves polynomial convergence rates and enables both nonparametric link function estimation and adaptive multi-index modeling using linear projections of functional data.\n\nD) It is restricted to functional linear models but improves upon them by incorporating multiple indices with finite-dimensional assumptions.\n\nCorrect Answer: C\n\nExplanation: The paper introduces a new technique that allows for nonparametric estimation of the link function and suggests an approach to multi-index modeling using adaptively defined linear projections of functional data. Importantly, the authors state that their methods enable prediction with polynomial convergence rates, which is an improvement over fully nonparametric methods that have slower convergence rates for high-dimensional functions. This approach overcomes limitations of previous methods that often used parametric link functions or made finite-dimensional assumptions for multiple indices. Option C accurately captures these key aspects of the paper's contribution."}, "17": {"documentation": {"title": "Information Design in Optimal Auctions", "source": "Yi-Chun, Chen and Xiangqian, Yang", "docs_id": "2010.08990", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information Design in Optimal Auctions. We study the information design problem in a single-unit auction setting. The information designer controls independent private signals according to which the buyers infer their binary private values. Assuming that the seller adopts the optimal auction due to Myerson (1981) in response, we characterize both the buyer-optimal information structure, which maximizes the buyers' surplus, and the sellerworst information structure, which minimizes the seller's revenue. We translate both information design problems into finite-dimensional, constrained optimization problems in which one can explicitly solve for the optimal information structures. In contrast to the case with one buyer (Roesler and Szentes, 2017 and Du, 2018), we show that with two or more buyers, the symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure. The good is always sold under the seller-worst information structure but not under the buyer-optimal information structure. Nevertheless, as the number of buyers goes to infinity, both symmetric information structures converge to no disclosure. We also show that in an ex ante symmetric setting, an asymmetric information structure is never seller-worst but can generate a strictly higher surplus for the buyers than the symmetric buyer-optimal information structure."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of information design in optimal auctions, which of the following statements is NOT true according to the research findings?\n\nA) The buyer-optimal and seller-worst information structures are identical when there is only one buyer.\nB) As the number of buyers approaches infinity, both symmetric buyer-optimal and seller-worst information structures converge to full disclosure.\nC) In an ex ante symmetric setting, an asymmetric information structure can yield higher buyer surplus than the symmetric buyer-optimal structure.\nD) The good is always sold under the seller-worst information structure but not necessarily under the buyer-optimal information structure.\n\nCorrect Answer: B\n\nExplanation:\nA) is true according to the passage, which mentions that with one buyer, the buyer-optimal and seller-worst structures are the same (referencing Roesler and Szentes, 2017 and Du, 2018).\n\nB) is false. The passage states that as the number of buyers goes to infinity, both symmetric information structures converge to no disclosure, not full disclosure.\n\nC) is true. The passage explicitly states that an asymmetric information structure can generate strictly higher surplus for buyers than the symmetric buyer-optimal structure in an ex ante symmetric setting.\n\nD) is true. The passage directly states that the good is always sold under the seller-worst information structure but not under the buyer-optimal information structure.\n\nTherefore, B is the correct answer as it is the only statement that is NOT true according to the research findings presented in the passage."}, "18": {"documentation": {"title": "Statistical Decision Properties of Imprecise Trials Assessing COVID-19\n  Drugs", "source": "Charles F. Manski and Aleksey Tetenov", "docs_id": "2006.00343", "section": ["econ.EM", "q-bio.QM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Decision Properties of Imprecise Trials Assessing COVID-19\n  Drugs. As the COVID-19 pandemic progresses, researchers are reporting findings of randomized trials comparing standard care with care augmented by experimental drugs. The trials have small sample sizes, so estimates of treatment effects are imprecise. Seeing imprecision, clinicians reading research articles may find it difficult to decide when to treat patients with experimental drugs. Whatever decision criterion one uses, there is always some probability that random variation in trial outcomes will lead to prescribing sub-optimal treatments. A conventional practice when comparing standard care and an innovation is to choose the innovation only if the estimated treatment effect is positive and statistically significant. This practice defers to standard care as the status quo. To evaluate decision criteria, we use the concept of near-optimality, which jointly considers the probability and magnitude of decision errors. An appealing decision criterion from this perspective is the empirical success rule, which chooses the treatment with the highest observed average patient outcome in the trial. Considering the design of recent and ongoing COVID-19 trials, we show that the empirical success rule yields treatment results that are much closer to optimal than those generated by prevailing decision criteria based on hypothesis tests."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the context of clinical trials for COVID-19 treatments, why might the empirical success rule be considered superior to conventional hypothesis testing for making treatment decisions?\n\nA) It always results in selecting the most effective treatment without fail.\nB) It eliminates the need for statistical analysis in clinical trials.\nC) It considers both the probability and magnitude of decision errors, leading to near-optimal treatment choices.\nD) It allows for smaller sample sizes in clinical trials without compromising accuracy.\n\nCorrect Answer: C\n\nExplanation: The empirical success rule is considered superior because it uses the concept of near-optimality, which jointly considers the probability and magnitude of decision errors. This approach leads to treatment decisions that are much closer to optimal compared to conventional hypothesis testing methods.\n\nOption A is incorrect because no decision rule can guarantee always selecting the most effective treatment without fail, especially in trials with small sample sizes and imprecise estimates.\n\nOption B is false because the empirical success rule still relies on statistical analysis; it just uses a different approach to interpret the results.\n\nOption D is not supported by the text. While the empirical success rule may perform better with small sample sizes, there's no indication that it allows for smaller samples without compromising accuracy.\n\nThe correct answer, C, accurately reflects the text's description of the empirical success rule and its advantages in making treatment decisions based on imprecise trial data."}, "19": {"documentation": {"title": "A Contextual Risk Model for the Ellsberg Paradox", "source": "Diederik Aerts and Sandro Sozzo", "docs_id": "1105.1814", "section": ["physics.soc-ph", "q-fin.GN", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Contextual Risk Model for the Ellsberg Paradox. The Allais and Ellsberg paradoxes show that the expected utility hypothesis and Savage's Sure-Thing Principle are violated in real life decisions. The popular explanation in terms of 'ambiguity aversion' is not completely accepted. On the other hand, we have recently introduced a notion of 'contextual risk' to mathematically capture what is known as 'ambiguity' in the economics literature. Situations in which contextual risk occurs cannot be modeled by Kolmogorovian classical probabilistic structures, but a non-Kolmogorovian framework with a quantum-like structure is needed. We prove in this paper that the contextual risk approach can be applied to the Ellsberg paradox, and elaborate a 'sphere model' within our 'hidden measurement formalism' which reveals that it is the overall conceptual landscape that is responsible of the disagreement between actual human decisions and the predictions of expected utility theory, which generates the paradox. This result points to the presence of a 'quantum conceptual layer' in human thought which is superposed to the usually assumed 'classical logical layer'."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the approach and findings of the contextual risk model in relation to the Ellsberg paradox?\n\nA) The model proves that ambiguity aversion is the sole explanation for the Ellsberg paradox and can be fully modeled using classical Kolmogorovian probability structures.\n\nB) The contextual risk approach demonstrates that a quantum-like conceptual layer in human thought, superposed on classical logic, is responsible for the discrepancies between human decisions and expected utility theory predictions in the Ellsberg paradox.\n\nC) The sphere model within the hidden measurement formalism shows that the Ellsberg paradox can be resolved by applying traditional expected utility hypothesis and Savage's Sure-Thing Principle more rigorously.\n\nD) The contextual risk model concludes that the Ellsberg paradox is a result of experimental error and does not actually violate the principles of expected utility theory.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately reflects the key points and conclusions presented in the documentation. The contextual risk approach introduces a quantum-like structure to model situations that cannot be explained by classical probabilistic frameworks. The research suggests that a \"quantum conceptual layer\" in human thought, superposed on classical logic, is responsible for the discrepancies between actual human decisions and the predictions of expected utility theory in the Ellsberg paradox.\n\nOption A is incorrect because the document states that ambiguity aversion is not completely accepted as an explanation, and the model specifically uses non-Kolmogorovian frameworks.\n\nOption C is incorrect because the research does not suggest resolving the paradox through more rigorous application of expected utility hypothesis and Savage's Sure-Thing Principle. Instead, it proposes a new approach using contextual risk and quantum-like structures.\n\nOption D is incorrect as the document does not suggest that the Ellsberg paradox is a result of experimental error. On the contrary, it acknowledges the paradox as a real phenomenon that violates expected utility theory and proposes a new model to explain it."}, "20": {"documentation": {"title": "Theory of gravitation theories: a no-progress report", "source": "Thomas P Sotiriou, Valerio Faraoni, Stefano Liberati", "docs_id": "0707.2748", "section": ["gr-qc", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theory of gravitation theories: a no-progress report. Already in the 1970s there where attempts to present a set of ground rules, sometimes referred to as a theory of gravitation theories, which theories of gravity should satisfy in order to be considered viable in principle and, therefore, interesting enough to deserve further investigation. From this perspective, an alternative title of the present paper could be ``why are we still unable to write a guide on how to propose viable alternatives to general relativity?''. Attempting to answer this question, it is argued here that earlier efforts to turn qualitative statements, such as the Einstein Equivalence Principle, into quantitative ones, such as the metric postulates, stand on rather shaky grounds -- probably contrary to popular belief -- as they appear to depend strongly on particular representations of the theory. This includes ambiguities in the identification of matter and gravitational fields, dependence of frequently used definitions, such as those of the stress-energy tensor or classical vacuum, on the choice of variables, etc. Various examples are discussed and possible approaches to this problem are pointed out. In the course of this study, several common misconceptions related to the various forms of the Equivalence Principle, the use of conformal frames and equivalence between theories are clarified."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best represents the main challenge in developing a comprehensive \"theory of gravitation theories\" as discussed in the article?\n\nA) The Einstein Equivalence Principle is too vague to be useful in evaluating alternative theories of gravity.\n\nB) There is a lack of experimental data to test alternative theories of gravity against General Relativity.\n\nC) Attempts to quantify qualitative principles like the Einstein Equivalence Principle are hindered by their dependence on particular representations of the theory.\n\nD) The metric postulates are insufficient to describe all possible viable alternatives to General Relativity.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The article argues that the main challenge in developing a comprehensive \"theory of gravitation theories\" lies in the difficulty of turning qualitative statements, such as the Einstein Equivalence Principle, into quantitative ones. This process is complicated by the fact that these quantifications appear to depend strongly on particular representations of the theory, including ambiguities in identifying matter and gravitational fields, and the dependence of common definitions on the choice of variables.\n\nOption A is incorrect because the article does not suggest that the Einstein Equivalence Principle is too vague, but rather that attempts to quantify it are problematic.\n\nOption B is not mentioned in the given text and does not address the theoretical challenges discussed.\n\nOption D, while related to the topic, does not capture the main challenge as precisely as option C. The article focuses more on the difficulties in quantifying principles rather than the inadequacy of specific postulates."}, "21": {"documentation": {"title": "Orbital eccentricity of WASP-12 and WASP-14 from new radial-velocity\n  monitoring with SOPHIE", "source": "Nawal Husnoo, Frederic Pont, Guillaume Hebrard, Elaine Simpson, Tsevi\n  Mazeh, Francois Bouchy, Claire Moutou, Luc Arnold, Isabelle Boisse, Rodrigo\n  Diaz, Anne Eggenberger and Avi Shporer", "docs_id": "1004.1809", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orbital eccentricity of WASP-12 and WASP-14 from new radial-velocity\n  monitoring with SOPHIE. As part of the long-term radial velocity monitoring of known transiting planets -- designed to measure orbital eccentricities, spin-orbit alignments and further planetary companions -- we have acquired radial velocity data for the two transiting systems WASP-12 and WASP-14, each harbouring gas giants on close orbits (orbital period of 1.09 and 2.24 days respectively). In both cases, the initial orbital solution suggested a significant orbital eccentricity, 0.049+-0.015 for WASP-12 and 0.091+-0.003 for WASP-14. Since then, measurements of the secondary eclipse of WASP-12 in the infrared have indicated that one projection of the eccentricity (e cos w) was very close to zero, casting doubt on the eccentricity from the initial radial velocity orbit. Our measurements confirm that the initial eccentricity detection could be spurious, and show that the radial velocity data is compatible with a circular orbit. A MCMC analysis taking into account the presence of correlated systematic noise in both the radial velocity and photometric data gives e=0.017 (+0.015-0.011). By contrast, we confirm the orbital eccentricity of WASP-14, and refine its value to e=0.088+-0.003. WASP-14 is thus the closest presently known planet with a confirmed eccentric orbit."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Based on the radial velocity monitoring study of WASP-12 and WASP-14, which of the following statements is most accurate?\n\nA) The initial orbital eccentricity measurements for both WASP-12 and WASP-14 were confirmed to be accurate by the new data.\n\nB) WASP-12's orbit was confirmed to be circular, while WASP-14's eccentricity was refined to a lower value than initially reported.\n\nC) WASP-12's orbital eccentricity was found to be potentially spurious, while WASP-14's eccentricity was confirmed and refined to e=0.088\u00b10.003.\n\nD) Both WASP-12 and WASP-14 were found to have circular orbits after accounting for correlated systematic noise in the data.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the study's results for both exoplanets. Option A is incorrect because the initial measurements were not confirmed for both planets. Option B is wrong because while WASP-12's eccentricity was questioned, WASP-14's was actually confirmed, not lowered. Option D is incorrect because only WASP-12's orbit was found to be potentially circular, not both. Option C correctly summarizes the findings: WASP-12's initial eccentricity measurement was cast into doubt and found to be compatible with a circular orbit, while WASP-14's eccentricity was confirmed and refined to e=0.088\u00b10.003, making it the closest known planet with a confirmed eccentric orbit."}, "22": {"documentation": {"title": "Temporal Markov Processes for Transport in Porous Media: Random Lattice\n  Networks", "source": "Amir H. Delgoshaie, Patrick Jenny, Hamdi A. Tchelepi", "docs_id": "1708.04173", "section": ["physics.comp-ph", "cs.CE", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal Markov Processes for Transport in Porous Media: Random Lattice\n  Networks. Monte Carlo (MC) simulations of transport in random porous networks indicate that for high variances of the log-normal permeability distribution, the transport of a passive tracer is non-Fickian. Here we model this non-Fickian dispersion in random porous networks using discrete temporal Markov models. We show that such temporal models capture the spreading behavior accurately. This is true despite the fact that the slow velocities are strongly correlated in time, and some studies have suggested that the persistence of low velocities would render the temporal Markovian model inapplicable. Compared to previously proposed temporal stochastic differential equations with case specific drift and diffusion terms, the models presented here require fewer modeling assumptions. Moreover, we show that discrete temporal Markov models can be used to represent dispersion in unstructured networks, which are widely used to model porous media. A new method is proposed to extend the state space of temporal Markov models to improve the model predictions in the presence of extremely low velocities in particle trajectories and extend the applicability of the model to higher temporal resolutions. Finally, it is shown that by combining multiple transitions, temporal models are more efficient for computing particle evolution compared to correlated CTRW with spatial increments that are equal to the lengths of the links in the network."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements most accurately describes the key finding of the study regarding temporal Markov models for non-Fickian dispersion in random porous networks?\n\nA) Temporal Markov models are ineffective for capturing spreading behavior in networks with high variances of log-normal permeability distribution.\n\nB) Temporal Markov models accurately capture spreading behavior despite strong temporal correlations in slow velocities.\n\nC) Temporal Markov models are only applicable to structured networks and cannot represent dispersion in unstructured networks.\n\nD) Temporal Markov models require more modeling assumptions compared to previously proposed temporal stochastic differential equations.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text explicitly states that \"We show that such temporal models capture the spreading behavior accurately. This is true despite the fact that the slow velocities are strongly correlated in time, and some studies have suggested that the persistence of low velocities would render the temporal Markovian model inapplicable.\"\n\nOption A is incorrect because the study found that temporal Markov models are effective, not ineffective, for capturing spreading behavior in these networks.\n\nOption C is incorrect because the text mentions that \"discrete temporal Markov models can be used to represent dispersion in unstructured networks, which are widely used to model porous media.\"\n\nOption D is incorrect as the text states that compared to previously proposed temporal stochastic differential equations, \"the models presented here require fewer modeling assumptions.\""}, "23": {"documentation": {"title": "Dynamics of domain-wall Dirac fermions on a topological insulator: a\n  chiral fermion beam splitter", "source": "Ren\\'e Hammer and Walter P\\\"otz", "docs_id": "1306.6139", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of domain-wall Dirac fermions on a topological insulator: a\n  chiral fermion beam splitter. The intersection of two ferromagnetic domain walls placed on the surface of topological insulators provides a one-way beam splitter for domain-wall Dirac fermions. Based on an analytic expression for a static two-soliton magnetic texture we perform a systematic numerical study of the propagation of Dirac wave packets along such intersections. A single-cone staggered-grid finite difference lattice scheme is employed in the numerical analysis. It is shown that the angle of intersection plays a decisive role in determining the splitting ratio of the fermion beam. For a non-rectangular intersection, the width and, to a lesser extent, the type of domain walls, e.g. Bloch or N{\\'e}el, determine the properties of the splitter. As the ratio between domain-wall width and transverse localization length of the Dirac fermion is increased its propagation behavior changes from quantum-mechanical (wave-like) to classical ballistic (particle-like). An electric gate placed near the intersection offers a dynamic external control knob for adjusting the splitting ratio."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: A researcher is studying the propagation of Dirac fermions at the intersection of two ferromagnetic domain walls on a topological insulator surface. Which combination of factors would most likely result in quantum-mechanical (wave-like) behavior of the fermions at the intersection?\n\nA) Wide domain walls, large angle of intersection, and high fermion energy\nB) Narrow domain walls, small angle of intersection, and low fermion energy\nC) Wide domain walls, rectangular intersection, and high fermion energy\nD) Narrow domain walls, non-rectangular intersection, and low fermion energy\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of how various factors affect the propagation behavior of Dirac fermions at domain wall intersections on topological insulators.\n\nOption B is correct because:\n1. Narrow domain walls: The documentation states that \"as the ratio between domain-wall width and transverse localization length of the Dirac fermion is increased its propagation behavior changes from quantum-mechanical (wave-like) to classical ballistic (particle-like).\" Therefore, narrower domain walls promote wave-like behavior.\n2. Small angle of intersection: The text mentions that \"the angle of intersection plays a decisive role in determining the splitting ratio of the fermion beam.\" A smaller angle would likely lead to more interference and wave-like behavior.\n3. Low fermion energy: Although not explicitly stated, lower energy typically corresponds to longer wavelengths, which would enhance quantum effects.\n\nOption A is incorrect because wide domain walls and large intersection angles would promote more classical, particle-like behavior.\n\nOption C is incorrect because wide domain walls would lead to more classical behavior, and a rectangular intersection is not specified as promoting wave-like propagation.\n\nOption D is incorrect because, while narrow domain walls are correct, a non-rectangular intersection is not specifically associated with wave-like behavior in the text.\n\nThis question requires synthesizing information from various parts of the given text and making some inferences about quantum behavior, making it a challenging exam question."}, "24": {"documentation": {"title": "Robust transformations of firing patterns for neural networks", "source": "Karlis Kanders, Tom Lorimer, Yoko Uwate, Willi-Hans Steeb and Ruedi\n  Stoop", "docs_id": "1708.04168", "section": ["q-bio.NC", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust transformations of firing patterns for neural networks. As a promising computational paradigm, occurrence of critical states in artificial and biological neural networks has attracted wide-spread attention. An often-made explicit or implicit assumption is that one single critical state is responsible for two separate notions of criticality (avalanche criticality and dynamical edge of chaos criticality). Previously, we provided an isolated counter-example for co-occurrence. Here, we reveal a persistent paradigm of structural transitions that such networks undergo, as the overall connectivity strength is varied over its biologically meaningful range. Among these transitions, only one avalanche critical point emerges, with edge of chaos failing to co-occur. Our observations are based on ensembles of networks obtained from variations of network configuration and their neurons. This suggests that not only non-coincidence of criticality, but also the persistent paradigm of network structural changes in function of the overall connectivity strength, could be generic features of a large class of biological neural networks."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements most accurately reflects the findings of the study regarding criticality in neural networks?\n\nA) A single critical state is responsible for both avalanche criticality and dynamical edge of chaos criticality.\n\nB) Multiple critical states exist, with avalanche criticality and edge of chaos criticality always co-occurring.\n\nC) Only one avalanche critical point emerges as network connectivity strength is varied, and edge of chaos criticality does not co-occur with it.\n\nD) The study found no evidence of any form of criticality in neural networks.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the key findings from the study. Option A is incorrect because the study challenges the assumption of a single critical state responsible for both types of criticality. Option B is also incorrect as the study specifically mentions that edge of chaos criticality fails to co-occur with avalanche criticality. Option D is incorrect because the study does find evidence of criticality, specifically one avalanche critical point. Option C correctly summarizes the main finding: as the overall connectivity strength is varied, only one avalanche critical point emerges, and the edge of chaos criticality does not co-occur with it. This option also aligns with the study's conclusion about the non-coincidence of these two types of criticality in neural networks."}, "25": {"documentation": {"title": "Dynamically generated inflationary two-field potential via\n  non-Riemannian volume forms", "source": "David Benisty, Eduardo I. Guendelman, Emil Nissimov, Svetlana Pacheva", "docs_id": "1907.07625", "section": ["astro-ph.CO", "gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamically generated inflationary two-field potential via\n  non-Riemannian volume forms. We consider a simple model of modified gravity interacting with a single scalar field $\\varphi$ with weakly coupled exponential potential within the framework of non-Riemannian spacetime volume-form formalism. The specific form of the action is fixed by the requirement of invariance under global Weyl-scale symmetry. Upon passing to the physical Einstein frame we show how the non-Riemannian volume elements create a second canonical scalar field $u$ and dynamically generate a non-trivial two-scalar-field potential $U_{\\rm eff}(u,\\varphi)$ with two remarkable features: (i) it possesses a large flat region for large $u$ describing a slow-roll inflation; (ii) it has a stable low-lying minimum w.r.t. $(u,\\varphi)$ representing the dark energy density in the \"late universe\". We study the corresponding two-field slow-roll inflation and show that the pertinent slow-roll inflationary curve $\\varphi = \\varphi(u)$ in the two-field space $(u,\\varphi)$ has a very small curvature, i.e., $\\varphi$ changes very little during the inflationary evolution of $u$ on the flat region of $U_{\\rm eff}(u,\\varphi)$. Explicit expressions are found for the slow-roll parameters which differ from those in the single-field inflationary counterpart. Numerical solutions for the scalar spectral index and the tensor-to-scalar ratio are derived agreeing with the observational data."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the described model of modified gravity with non-Riemannian volume forms, what are the key characteristics of the dynamically generated two-scalar-field potential U_eff(u,\u03c6)?\n\nA) It has a single minimum representing dark energy and a steep region for slow-roll inflation.\nB) It possesses multiple flat regions for different inflationary epochs and several stable minima.\nC) It has a large flat region for slow-roll inflation at small u values and an unstable maximum representing dark energy.\nD) It features a large flat region for slow-roll inflation at large u values and a stable low-lying minimum representing dark energy density.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that the dynamically generated two-scalar-field potential U_eff(u,\u03c6) has two remarkable features: (i) it possesses a large flat region for large u describing a slow-roll inflation; (ii) it has a stable low-lying minimum with respect to (u,\u03c6) representing the dark energy density in the \"late universe\". This directly corresponds to option D.\n\nOption A is incorrect because it mentions a steep region for slow-roll inflation, which contradicts the flat region described in the text. Option B is wrong as it suggests multiple flat regions and minima, which are not mentioned in the documentation. Option C is incorrect because it places the flat region at small u values (instead of large u) and describes an unstable maximum for dark energy, contrary to the stable minimum mentioned in the text."}, "26": {"documentation": {"title": "Active Sensing for Search and Tracking: A Review", "source": "Luca Varotto, Angelo Cenedese, and Andrea Cavallaro", "docs_id": "2112.02381", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active Sensing for Search and Tracking: A Review. Active Position Estimation (APE) is the task of localizing one or more targets using one or more sensing platforms. APE is a key task for search and rescue missions, wildlife monitoring, source term estimation, and collaborative mobile robotics. Success in APE depends on the level of cooperation of the sensing platforms, their number, their degrees of freedom and the quality of the information gathered. APE control laws enable active sensing by satisfying either pure-exploitative or pure-explorative criteria. The former minimizes the uncertainty on position estimation; whereas the latter drives the platform closer to its task completion. In this paper, we define the main elements of APE to systematically classify and critically discuss the state of the art in this domain. We also propose a reference framework as a formalism to classify APE-related solutions. Overall, this survey explores the principal challenges and envisages the main research directions in the field of autonomous perception systems for localization tasks. It is also beneficial to promote the development of robust active sensing methods for search and tracking applications."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the relationship between pure-exploitative and pure-explorative criteria in Active Position Estimation (APE) control laws?\n\nA) Pure-exploitative criteria focus on task completion, while pure-explorative criteria minimize position uncertainty.\nB) Pure-exploitative criteria minimize position uncertainty, while pure-explorative criteria drive the platform closer to task completion.\nC) Pure-exploitative criteria maximize sensor coverage, while pure-explorative criteria optimize energy efficiency.\nD) Pure-exploitative and pure-explorative criteria are interchangeable terms in APE control laws.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the provided information, APE control laws enable active sensing by satisfying either pure-exploitative or pure-explorative criteria. The text explicitly states that pure-exploitative criteria minimize the uncertainty on position estimation, whereas pure-explorative criteria drive the platform closer to its task completion. This directly corresponds to option B.\n\nOption A is incorrect because it reverses the roles of pure-exploitative and pure-explorative criteria. Option C introduces concepts (sensor coverage and energy efficiency) that are not mentioned in the given text in relation to these criteria. Option D is incorrect because the text clearly differentiates between pure-exploitative and pure-explorative criteria, showing they are not interchangeable.\n\nThis question tests the reader's understanding of the specific roles of different criteria in APE control laws, requiring careful attention to detail and the ability to distinguish between closely related concepts."}, "27": {"documentation": {"title": "Classical versus quantum views of intense laser pulse propagation in\n  gases", "source": "S.A. Berman, C. Chandre, J. Dubois, M. Perin, and T. Uzer", "docs_id": "1901.06354", "section": ["physics.atom-ph", "nlin.CD", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Classical versus quantum views of intense laser pulse propagation in\n  gases. We study the behavior of reduced models for the propagation of intense laser pulses in atomic gases. The models we consider incorporate ionization, blueshifting, and other nonlinear propagation effects in an ab initio manner, by explicitly taking into account the microscopic electron dynamics. Numerical simulations of the propagation of ultrashort linearly-polarized and elliptically-polarized laser pulses over experimentally-relevant propagation distances are presented. We compare the behavior of models where the electrons are treated classically with those where they are treated quantum-mechanically. A classical equivalent to the ground state is found, which maximizes the agreement between the quantum and classical predictions of the single-atom ionization probability as a function of laser intensity. We show that this translates into quantitative agreement between the quantum and classical models for the laser field evolution during propagation through gases of ground-state atoms. This agreement is exploited to provide a classical perspective on low- and high-order harmonic generation in linearly-polarized fields. In addition, we demonstrate the stability of the polarization of a nearly-linearly-polarized pulse using a two-dimensional model."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of intense laser pulse propagation in gases, what key finding was made regarding the comparison between classical and quantum models?\n\nA) Quantum models always outperformed classical models in predicting ionization probabilities.\nB) A classical equivalent to the ground state was found that maximized agreement with quantum predictions for single-atom ionization probability.\nC) Classical models were unable to accurately predict low-order harmonic generation in linearly-polarized fields.\nD) Two-dimensional models showed that nearly-linearly-polarized pulses are inherently unstable during propagation.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that \"A classical equivalent to the ground state is found, which maximizes the agreement between the quantum and classical predictions of the single-atom ionization probability as a function of laser intensity.\" This finding is significant because it allows for quantitative agreement between quantum and classical models in predicting laser field evolution during propagation through gases of ground-state atoms.\n\nOption A is incorrect because the study does not claim that quantum models always outperform classical models. Instead, it shows that under certain conditions, classical models can agree with quantum predictions.\n\nOption C is incorrect. The documentation actually suggests that the classical perspective can be used to understand both low- and high-order harmonic generation in linearly-polarized fields, not that classical models were unable to predict these phenomena.\n\nOption D is also incorrect. The study demonstrates \"the stability of the polarization of a nearly-linearly-polarized pulse using a two-dimensional model,\" which is the opposite of what this option claims."}, "28": {"documentation": {"title": "POD-Galerkin reduced order models and physics-informed neural networks\n  for solving inverse problems for the Navier-Stokes equations", "source": "Saddam Hijazi, Melina Freitag and Niels Landwehr", "docs_id": "2112.11950", "section": ["physics.flu-dyn", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "POD-Galerkin reduced order models and physics-informed neural networks\n  for solving inverse problems for the Navier-Stokes equations. We present a Reduced Order Model (ROM) which exploits recent developments in Physics Informed Neural Networks (PINNs) for solving inverse problems for the Navier--Stokes equations (NSE). In the proposed approach, the presence of simulated data for the fluid dynamics fields is assumed. A POD-Galerkin ROM is then constructed by applying POD on the snapshots matrices of the fluid fields and performing a Galerkin projection of the NSE (or the modified equations in case of turbulence modeling) onto the POD reduced basis. A $\\textit{POD-Galerkin PINN ROM}$ is then derived by introducing deep neural networks which approximate the reduced outputs with the input being time and/or parameters of the model. The neural networks incorporate the physical equations (the POD-Galerkin reduced equations) into their structure as part of the loss function. Using this approach, the reduced model is able to approximate unknown parameters such as physical constants or the boundary conditions. A demonstration of the applicability of the proposed ROM is illustrated by two cases which are the steady flow around a backward step and the unsteady turbulent flow around a surface mounted cubic obstacle."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the POD-Galerkin PINN ROM approach for solving inverse problems for the Navier-Stokes equations, which of the following statements is NOT correct?\n\nA) The method assumes the availability of simulated data for fluid dynamics fields.\n\nB) The neural networks in this approach directly solve the full Navier-Stokes equations without any dimensionality reduction.\n\nC) The ROM can approximate unknown parameters such as physical constants or boundary conditions.\n\nD) The approach combines POD-Galerkin reduced order modeling with Physics Informed Neural Networks.\n\nCorrect Answer: B\n\nExplanation: \nA is correct as the documentation states \"the presence of simulated data for the fluid dynamics fields is assumed.\"\n\nB is incorrect and thus the right answer to this question. The method does not directly solve the full Navier-Stokes equations with neural networks. Instead, it uses POD to reduce the dimensionality of the problem and then applies neural networks to the reduced order model.\n\nC is correct as the text mentions \"the reduced model is able to approximate unknown parameters such as physical constants or the boundary conditions.\"\n\nD is correct as the approach is described as a \"POD-Galerkin PINN ROM\" which combines POD-Galerkin reduced order modeling with Physics Informed Neural Networks."}, "29": {"documentation": {"title": "On Interpretability and Similarity in Concept-Based Machine Learning", "source": "L\\'eonard Kwuida and Dmitry I. Ignatov", "docs_id": "2102.12723", "section": ["cs.LG", "cs.AI", "cs.DM", "math.CO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Interpretability and Similarity in Concept-Based Machine Learning. Machine Learning (ML) provides important techniques for classification and predictions. Most of these are black-box models for users and do not provide decision-makers with an explanation. For the sake of transparency or more validity of decisions, the need to develop explainable/interpretable ML-methods is gaining more and more importance. Certain questions need to be addressed: How does an ML procedure derive the class for a particular entity? Why does a particular clustering emerge from a particular unsupervised ML procedure? What can we do if the number of attributes is very large? What are the possible reasons for the mistakes for concrete cases and models? For binary attributes, Formal Concept Analysis (FCA) offers techniques in terms of intents of formal concepts, and thus provides plausible reasons for model prediction. However, from the interpretable machine learning viewpoint, we still need to provide decision-makers with the importance of individual attributes to the classification of a particular object, which may facilitate explanations by experts in various domains with high-cost errors like medicine or finance. We discuss how notions from cooperative game theory can be used to assess the contribution of individual attributes in classification and clustering processes in concept-based machine learning. To address the 3rd question, we present some ideas on how to reduce the number of attributes using similarities in large contexts."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of interpretable machine learning, which of the following approaches is most closely aligned with the method described in the text for assessing the importance of individual attributes in classification and clustering processes?\n\nA) Random forest feature importance\nB) SHAP (SHapley Additive exPlanations) values\nC) LIME (Local Interpretable Model-agnostic Explanations)\nD) Gradient-based saliency maps\n\nCorrect Answer: B\n\nExplanation: The correct answer is B) SHAP (SHapley Additive exPlanations) values. The text mentions using \"notions from cooperative game theory to assess the contribution of individual attributes in classification and clustering processes.\" This directly aligns with the concept of SHAP values, which are based on Shapley values from cooperative game theory. SHAP values provide a way to distribute the prediction among the features, similar to how the text describes assessing the importance of individual attributes.\n\nOption A is incorrect because random forest feature importance, while useful, doesn't leverage cooperative game theory concepts.\n\nOption C, LIME, is a model-agnostic method that provides local explanations but doesn't specifically use cooperative game theory concepts as mentioned in the text.\n\nOption D, gradient-based saliency maps, are typically used in deep learning models and don't align with the cooperative game theory approach described in the text."}, "30": {"documentation": {"title": "On the role of extrinsic noise in microRNA-mediated bimodal gene\n  expression", "source": "Marco Del Giudice, Stefano Bo, Silvia Grigolon and Carla Bosia", "docs_id": "1611.00666", "section": ["q-bio.MN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the role of extrinsic noise in microRNA-mediated bimodal gene\n  expression. Several studies highlighted the relevance of extrinsic noise in shaping cell decision making and differentiation in molecular networks. Experimental evidences of phenotypic differentiation are given by the presence of bimodal distributions of gene expression levels, where the modes of the distribution often correspond to different physiological states of the system. We theoretically address the presence of bimodal phenotypes in the context of microRNA (miRNA)-mediated regulation. MiRNAs are small noncoding RNA molecules that downregulate the expression of their target mRNAs. The nature of this interaction is titrative and induces a threshold effect: below a given target transcription rate no mRNAs are free and available for translation. We investigate the effect of extrinsic noise on the system by introducing a fluctuating miRNA-transcription rate. We find that the presence of extrinsic noise favours the presence of bimodal target distributions which can be observed for a wider range of parameters compared to the case with intrinsic noise only and for lower miRNA-target interaction strength. Our results suggest that combining threshold-inducing interactions with extrinsic noise provides a simple and robust mechanism for obtaining bimodal populations not requiring fine tuning. We furthermore characterise the protein distributions dependence on protein half-life."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of microRNA-mediated regulation, which of the following statements best describes the role of extrinsic noise in promoting bimodal gene expression?\n\nA) Extrinsic noise decreases the range of parameters for which bimodal target distributions can be observed.\n\nB) Extrinsic noise requires a higher miRNA-target interaction strength to induce bimodality compared to intrinsic noise alone.\n\nC) Extrinsic noise, introduced as fluctuations in miRNA transcription rate, facilitates bimodal target distributions over a broader parameter range and at lower miRNA-target interaction strengths.\n\nD) Extrinsic noise has no significant impact on the threshold effect induced by the titrative nature of miRNA-mRNA interactions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"the presence of extrinsic noise favours the presence of bimodal target distributions which can be observed for a wider range of parameters compared to the case with intrinsic noise only and for lower miRNA-target interaction strength.\" This directly supports option C, indicating that extrinsic noise, introduced through fluctuations in miRNA transcription rate, promotes bimodality across a broader range of parameters and at lower interaction strengths.\n\nOption A is incorrect because the documentation suggests that extrinsic noise increases, not decreases, the range of parameters for bimodal distributions.\n\nOption B is false because the text states that bimodality is observed for lower miRNA-target interaction strength in the presence of extrinsic noise, not higher.\n\nOption D is incorrect because the documentation clearly indicates that extrinsic noise has a significant impact on the system, particularly in promoting bimodality.\n\nThis question tests the student's understanding of the complex interplay between extrinsic noise and miRNA-mediated regulation in gene expression, requiring careful analysis of the provided information."}, "31": {"documentation": {"title": "The Need to Reconcile Concepts that Characterize Systems Withstanding\n  Threats", "source": "Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, Igor Linkov", "docs_id": "1912.10457", "section": ["physics.soc-ph", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Need to Reconcile Concepts that Characterize Systems Withstanding\n  Threats. Desirable system performance in the face of threats and disruptions has been characterized by various management concepts. Through semi-structured interviews with editors of journals in the fields of emergency response and systems management, a literature review, and professional judgment, we identified nine related and often interchangeably-used system performance concepts: adaptability, agility, reliability, resilience, resistance, robustness, safety, security, and sustainability. We analysed expert responses and reviewed the linguistic definitions and mathematical framing of these concepts to understand their applications. We found a lack of consensus on their usage between interview subjects, but using a mathematical framing enriched the linguistic definitions and enabled formulating comparative visualizations and system-specific definitions for the concepts. We propose a conceptual framing to relate the concepts for management purposes. A better understanding of these concepts will allow system planners to pursue management strategies best suited to their unique system dynamics and specific objectives of 'goodness' that all these concepts bring."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best represents the key findings and recommendations of the study on system performance concepts?\n\nA) The study found complete consensus among experts on the usage of the nine identified system performance concepts, suggesting standardized definitions across fields.\n\nB) The research concluded that linguistic definitions alone are sufficient for understanding and applying these concepts in system management.\n\nC) The study recommends abandoning the use of multiple system performance concepts in favor of a single, unified term to avoid confusion.\n\nD) The research suggests that mathematical framing can enhance understanding of the concepts, and proposes a conceptual framework to relate them for management purposes.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The study identified nine related system performance concepts (adaptability, agility, reliability, resilience, resistance, robustness, safety, security, and sustainability) and found a lack of consensus on their usage among experts. However, the research discovered that using mathematical framing enriched the linguistic definitions and allowed for better visualization and system-specific definitions. The study proposes a conceptual framing to relate these concepts for management purposes, aiming to help system planners choose strategies best suited to their specific system dynamics and objectives.\n\nOption A is incorrect because the study found a lack of consensus, not complete agreement. Option B is wrong as the research emphasizes the value of mathematical framing in addition to linguistic definitions. Option C contradicts the study's approach, which seeks to reconcile and relate the concepts rather than abandon them."}, "32": {"documentation": {"title": "The proton radius (puzzle?) and its relatives", "source": "Clara Peset, Antonio Pineda, and Oleksandr Tomalak", "docs_id": "2106.00695", "section": ["hep-ph", "nucl-th", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The proton radius (puzzle?) and its relatives. We review determinations of the electric proton charge radius from a diverse set of low-energy observables. We explore under which conditions it can be related to Wilson coefficients of appropriate effective field theories. This discussion is generalized to other low-energy constants. This provides us with a unified framework to deal with a set of low-energy constants of the proton associated with its electromagnetic interactions. Unambiguous definitions of these objects are given, as well as their relation with expectation values of QCD operators. We show that the proton radius obtained from spectroscopy and lepton-proton scattering (when both the lepton and proton move with nonrelativistic velocities) is related to the same object of the underlying field theory with ${\\cal O}(\\alpha)$ precision. The model dependence of these analyses is discussed. The prospects of constructing effective field theories valid for the kinematic configuration of present, or near-future, lepton-proton scattering experiments are discussed."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the relationship between the proton radius obtained from spectroscopy and lepton-proton scattering, according to the review?\n\nA) The proton radius from these methods is related to the same object in the underlying field theory with perfect precision.\n\nB) The proton radius from these methods is related to the same object in the underlying field theory with O(\u03b1\u00b2) precision.\n\nC) The proton radius from these methods is related to the same object in the underlying field theory with O(\u03b1) precision, but only when both the lepton and proton move with relativistic velocities.\n\nD) The proton radius from these methods is related to the same object in the underlying field theory with O(\u03b1) precision, but only when both the lepton and proton move with nonrelativistic velocities.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The text states: \"We show that the proton radius obtained from spectroscopy and lepton-proton scattering (when both the lepton and proton move with nonrelativistic velocities) is related to the same object of the underlying field theory with O(\u03b1) precision.\" This directly corresponds to option D, which accurately captures the conditions (nonrelativistic velocities) and the precision (O(\u03b1)) mentioned in the text.\n\nOption A is incorrect because it claims perfect precision, which is not stated in the text. Option B is incorrect because it mentions O(\u03b1\u00b2) precision, which is different from the O(\u03b1) precision stated in the text. Option C is incorrect because it specifies relativistic velocities, whereas the text explicitly mentions nonrelativistic velocities."}, "33": {"documentation": {"title": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report)", "source": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "docs_id": "1811.00694", "section": ["cs.SE", "cs.FL", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report). Improving patient care safety is an ultimate objective for medical cyber-physical systems. A recent study shows that the patients' death rate can be significantly reduced by computerizing medical best practice guidelines. To facilitate the development of computerized medical best practice guidelines, statecharts are often used as a modeling tool because of their high resemblances to disease and treatment models and their capabilities to provide rapid prototyping and simulation for clinical validations. However, some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics which makes it difficult to model certain functionalities that are essential in modeling medical guidelines, such as two-way communications and configurable execution orders. Rather than introducing new statechart elements or changing the statechart implementation's underline semantics, we use existing basic statechart elements to design model patterns for the commonly occurring issues. In particular, we show the design of model patterns for two-way communications and configurable execution orders and formally prove the correctness of these model patterns. We further use a simplified airway laser surgery scenario as a case study to demonstrate how the developed model patterns address the two-way communication and configurable execution order issues and their impact on validation and verification of medical safety properties."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of modeling medical best practice guidelines using statecharts, which of the following statements best describes the challenge and proposed solution for implementing two-way communications and configurable execution orders?\n\nA) The challenge is that statecharts inherently support two-way communications, but the solution proposes new statechart elements to improve this functionality.\n\nB) The challenge is that priority-based statecharts with synchronous execution semantics make it difficult to model these functionalities, and the solution is to change the underlying semantics of statechart implementations.\n\nC) The challenge is that statecharts lack the ability to model medical guidelines effectively, and the solution is to develop an entirely new modeling paradigm.\n\nD) The challenge is that priority-based statecharts with synchronous execution semantics make it difficult to model these functionalities, and the solution is to design model patterns using existing basic statechart elements.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics, which makes it challenging to model two-way communications and configurable execution orders. These functionalities are essential for modeling medical guidelines. Instead of introducing new statechart elements or changing the underlying semantics, the proposed solution is to design model patterns using existing basic statechart elements to address these issues. This approach allows for the use of statecharts while overcoming their limitations in modeling specific aspects of medical guidelines.\n\nOption A is incorrect because the challenge is not that statecharts inherently support two-way communications; in fact, the difficulty in modeling two-way communications is part of the problem.\n\nOption B is partially correct in identifying the challenge but is incorrect in the proposed solution. The documentation explicitly states that they avoid changing the underlying semantics of statechart implementations.\n\nOption C is incorrect as it overstates the problem and proposes a solution that is not mentioned in the documentation. The goal is to improve the use of statecharts, not to replace them entirely."}, "34": {"documentation": {"title": "CoCoLasso for High-dimensional Error-in-variables Regression", "source": "Abhirup Datta and Hui Zou", "docs_id": "1510.07123", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CoCoLasso for High-dimensional Error-in-variables Regression. Much theoretical and applied work has been devoted to high-dimensional regression with clean data. However, we often face corrupted data in many applications where missing data and measurement errors cannot be ignored. Loh and Wainwright (2012) proposed a non-convex modification of the Lasso for doing high-dimensional regression with noisy and missing data. It is generally agreed that the virtues of convexity contribute fundamentally the success and popularity of the Lasso. In light of this, we propose a new method named CoCoLasso that is convex and can handle a general class of corrupted datasets including the cases of additive measurement error and random missing data. We establish the estimation error bounds of CoCoLasso and its asymptotic sign-consistent selection property. We further elucidate how the standard cross validation techniques can be misleading in presence of measurement error and develop a novel corrected cross-validation technique by using the basic idea in CoCoLasso. The corrected cross-validation has its own importance. We demonstrate the superior performance of our method over the non-convex approach by simulation studies."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: In the context of high-dimensional error-in-variables regression, which of the following statements about CoCoLasso is NOT correct?\n\nA) It is a convex modification of the Lasso algorithm designed to handle corrupted datasets.\nB) It can address both additive measurement error and random missing data scenarios.\nC) It relies on standard cross-validation techniques for optimal performance.\nD) It demonstrates superior performance compared to non-convex approaches in simulation studies.\n\nCorrect Answer: C\n\nExplanation: \nA is correct: CoCoLasso is described as a \"new method\" that is \"convex\" and can handle \"a general class of corrupted datasets.\"\n\nB is correct: The text explicitly states that CoCoLasso can handle \"a general class of corrupted datasets including the cases of additive measurement error and random missing data.\"\n\nC is incorrect: The text actually states that \"standard cross validation techniques can be misleading in presence of measurement error\" and that the authors developed \"a novel corrected cross-validation technique\" for use with CoCoLasso.\n\nD is correct: The passage concludes by stating that the authors \"demonstrate the superior performance of our method over the non-convex approach by simulation studies.\"\n\nThe correct answer is C because it contradicts the information provided in the text about cross-validation techniques."}, "35": {"documentation": {"title": "Predictions on the alpha decay half lives of Superheavy nuclei with Z =\n  113 in the range 255 l.e. A l.e. 314", "source": "K. P. Santhosh, A. Augustine, C. Nithya and B. Priyanka", "docs_id": "1604.00296", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predictions on the alpha decay half lives of Superheavy nuclei with Z =\n  113 in the range 255 l.e. A l.e. 314. An intense study of the alpha decay properties of the isotopes of superheavy element Z=113 have been performed within the Coulomb and proximity potential model for deformed nuclei (CPPMDN) within the wide range 255 l.e. A l.e. 314. The predicted alpha decay half lives of $^{278}$113 and $^{282}$113 and the alpha half lives of their decay products are in good agreement with the experimental data. 6{\\alpha) chains and 4{\\alpha} chains predicted respectively for $^{278}$113 and $^{282}$113 are in agreement with the experimental observation. Our study shows that the isotopes in the mass range 278 l.e. A l.e. 286 will survive fission and can be synthesized and detected in the laboratory via alpha decay. In our study, we have predicted 6{\\alpha} chains from $^{279}$113, 4{\\alpha} chains from $^{286}$113, 3{\\alpha} chains from $^{280,281,283}$113, 2{\\alpha} chains from $^{284}$113 and 1{\\alpha} chain from $^{285}$113. We hope that these predictions will be a guideline for future experimental investigations."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Based on the Coulomb and proximity potential model for deformed nuclei (CPPMDN), which of the following statements about the alpha decay chains of isotopes of element 113 is correct?\n\nA) The model predicts 5\u03b1 chains for \u00b2\u2077\u2078113 and 3\u03b1 chains for \u00b2\u2078\u00b2113\nB) The model predicts 6\u03b1 chains for \u00b2\u2077\u2079113 and 2\u03b1 chains for \u00b2\u2078\u2074113\nC) The model predicts 4\u03b1 chains for \u00b2\u2078\u2076113 and 3\u03b1 chains for \u00b2\u2078\u00b2113\nD) The model predicts 1\u03b1 chain for \u00b2\u2078\u2075113 and 5\u03b1 chains for \u00b2\u2078\u00b3113\n\nCorrect Answer: B\n\nExplanation: The question tests the student's ability to carefully read and interpret complex scientific information. The correct answer is B because the documentation explicitly states that the study predicts \"6\u03b1 chains from \u00b2\u2077\u2079113\" and \"2\u03b1 chains from \u00b2\u2078\u2074113\". Answer A is incorrect as it mentions 5\u03b1 chains for \u00b2\u2077\u2078113, which is not stated in the text (it actually predicts 6\u03b1 chains for this isotope). Answer C is partially correct about 4\u03b1 chains for \u00b2\u2078\u2076113, but incorrectly states 3\u03b1 chains for \u00b2\u2078\u00b2113 (the text mentions 4\u03b1 chains for this isotope). Answer D is partially correct about 1\u03b1 chain for \u00b2\u2078\u2075113, but incorrectly states 5\u03b1 chains for \u00b2\u2078\u00b3113 (the text mentions 3\u03b1 chains for this isotope)."}, "36": {"documentation": {"title": "Bayesian Projected Calibration of Computer Models", "source": "Fangzheng Xie, Yanxun Xu", "docs_id": "1803.01231", "section": ["stat.ME", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Projected Calibration of Computer Models. We develop a Bayesian approach called Bayesian projected calibration to address the problem of calibrating an imperfect computer model using observational data from a complex physical system. The calibration parameter and the physical system are parametrized in an identifiable fashion via $L_2$-projection. The physical process is assigned a Gaussian process prior, which naturally induces a prior distribution on the calibration parameter through the $L_2$-projection constraint. The calibration parameter is estimated through its posterior distribution, which provides a natural and non-asymptotic way for the uncertainty quantification. We provide a rigorous large sample justification for the proposed approach by establishing the asymptotic normality of the posterior of the calibration parameter with the efficient covariance matrix. In addition, two efficient computational algorithms based on stochastic approximation are designed with theoretical guarantees. Through extensive simulation studies and two real-world datasets analyses, we show that the Bayesian projected calibration can accurately estimate the calibration parameters, appropriately calibrate the computer models, and compare favorably to alternative approaches."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the Bayesian projected calibration approach for computer model calibration, which of the following statements is NOT correct?\n\nA) The physical process is assigned a Gaussian process prior.\nB) The calibration parameter is estimated through its posterior distribution.\nC) The approach provides asymptotic uncertainty quantification for the calibration parameter.\nD) The method uses L2-projection to parametrize the calibration parameter and physical system in an identifiable fashion.\n\nCorrect Answer: C\n\nExplanation:\nA) is correct. The documentation explicitly states that \"The physical process is assigned a Gaussian process prior.\"\n\nB) is correct. The text mentions that \"The calibration parameter is estimated through its posterior distribution.\"\n\nC) is incorrect and therefore the correct answer to this question. The documentation states that the approach provides a \"natural and non-asymptotic way for the uncertainty quantification.\" This contradicts the statement in option C which suggests asymptotic uncertainty quantification.\n\nD) is correct. The text states that \"The calibration parameter and the physical system are parametrized in an identifiable fashion via L2-projection.\"\n\nThis question tests the reader's understanding of key aspects of the Bayesian projected calibration approach, particularly focusing on its non-asymptotic nature for uncertainty quantification, which is a crucial feature of this method."}, "37": {"documentation": {"title": "Ab initio electronic structure and prospects for the formation of\n  ultracold calcium--alkali-metal-atom molecular ions", "source": "Wissem Zrafi, Hela Ladjimi, Halima Said, Hamid Berriche, Micha{\\l}\n  Tomza", "docs_id": "2003.02813", "section": ["physics.atom-ph", "cond-mat.quant-gas", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ab initio electronic structure and prospects for the formation of\n  ultracold calcium--alkali-metal-atom molecular ions. Experiments with cold ion-atom mixtures have recently opened the way for the production and application of ultracold molecular ions. Here, in a comparative study, we theoretically investigate ground and several excited electronic states and prospects for the formation of molecular ions composed of a calcium ion and an alkali-metal atom: CaAlk$^{+}$ (Alk=Li, Na, K, Rb, Cs). We use a quantum chemistry approach based on non-empirical pseudopotential, operatorial core-valence correlation, large Gaussian basis sets, and full configuration interaction method for valence electrons. Adiabatic potential energy curves, spectroscopic constants, and transition and permanent electric dipole moments are determined and analyzed for the ground and excited electronic states. We examine the prospects for ion-neutral reactive processes and the production of molecular ions via spontaneous radiative association and laser-induced photoassociation. After that, spontaneous and stimulated blackbody radiation transition rates are calculated and used to obtain radiative lifetimes of vibrational states of the ground and first-excited electronic states. The present results pave the way for the formation and spectroscopy of calcium--alkali-metal-atom molecular ions in modern experiments with cold ion-atom mixtures."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about the theoretical study of CaAlk+ molecular ions is NOT correct?\n\nA) The study uses a quantum chemistry approach with non-empirical pseudopotential and full configuration interaction method for valence electrons.\n\nB) The research investigates both ground and excited electronic states of molecular ions composed of a calcium ion and an alkali-metal atom.\n\nC) The study calculates and analyzes spectroscopic constants and permanent electric dipole moments for the examined electronic states.\n\nD) The research concludes that laser-induced photoassociation is the only viable method for producing these molecular ions in cold ion-atom mixtures.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D because the documentation mentions multiple prospects for the production of these molecular ions, including spontaneous radiative association and laser-induced photoassociation. It does not state that laser-induced photoassociation is the only viable method. \n\nOptions A, B, and C are all correct statements based on the information provided in the documentation. The study does use a quantum chemistry approach with the mentioned methods (A), investigates both ground and excited states (B), and analyzes spectroscopic constants and dipole moments (C).\n\nThis question tests the reader's ability to carefully analyze the given information and identify a statement that contradicts or overstates the content of the research documentation."}, "38": {"documentation": {"title": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study", "source": "Artur Strzelecki", "docs_id": "2003.10998", "section": ["cs.CY", "cs.IR", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study. The recent emergence of a new coronavirus, COVID-19, has gained extensive coverage in public media and global news. As of 24 March 2020, the virus has caused viral pneumonia in tens of thousands of people in Wuhan, China, and thousands of cases in 184 other countries and territories. This study explores the potential use of Google Trends (GT) to monitor worldwide interest in this COVID-19 epidemic. GT was chosen as a source of reverse engineering data, given the interest in the topic. Current data on COVID-19 is retrieved from (GT) using one main search topic: Coronavirus. Geographical settings for GT are worldwide, China, South Korea, Italy and Iran. The reported period is 15 January 2020 to 24 March 2020. The results show that the highest worldwide peak in the first wave of demand for information was on 31 January 2020. After the first peak, the number of new cases reported daily rose for 6 days. A second wave started on 21 February 2020 after the outbreaks were reported in Italy, with the highest peak on 16 March 2020. The second wave is six times as big as the first wave. The number of new cases reported daily is rising day by day. This short communication gives a brief introduction to how the demand for information on coronavirus epidemic is reported through GT."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements accurately reflects the findings of the Google Trends study on worldwide interest in COVID-19?\n\nA) The first wave of information demand peaked on February 21, 2020, coinciding with outbreaks in Italy.\n\nB) The second wave of interest was approximately six times larger than the first wave and peaked on March 16, 2020.\n\nC) The study focused on Google Trends data from China, South Korea, Italy, and Iran exclusively.\n\nD) The number of new COVID-19 cases reported daily decreased for 6 days after the first peak in worldwide interest.\n\nCorrect Answer: B\n\nExplanation: \nOption A is incorrect because the first wave peaked on January 31, 2020, not February 21.\nOption B is correct as the passage states, \"A second wave started on 21 February 2020 after the outbreaks were reported in Italy, with the highest peak on 16 March 2020. The second wave is six times as big as the first wave.\"\nOption C is incorrect because while the study did focus on those countries, it also included worldwide data.\nOption D is incorrect because the passage mentions that the number of new cases rose for 6 days after the first peak, not decreased."}, "39": {"documentation": {"title": "Finite-Sample Concentration of the Multinomial in Relative Entropy", "source": "Rohit Agrawal", "docs_id": "1904.02291", "section": ["cs.IT", "math.IT", "math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finite-Sample Concentration of the Multinomial in Relative Entropy. We show that the moment generating function of the Kullback-Leibler divergence (relative entropy) between the empirical distribution of $n$ independent samples from a distribution $P$ over a finite alphabet of size $k$ (i.e. a multinomial distribution) and $P$ itself is no more than that of a gamma distribution with shape $k - 1$ and rate $n$. The resulting exponential concentration inequality becomes meaningful (less than 1) when the divergence $\\varepsilon$ is larger than $(k-1)/n$, whereas the standard method of types bound requires $\\varepsilon > \\frac{1}{n} \\cdot \\log{\\binom{n+k-1}{k-1}} \\geq (k-1)/n \\cdot \\log(1 + n/(k-1))$, thus saving a factor of order $\\log(n/k)$ in the standard regime of parameters where $n\\gg k$. As a consequence, we also obtain finite-sample bounds on all the moments of the empirical divergence (equivalently, the discrete likelihood-ratio statistic), which are within constant factors (depending on the moment) of their asymptotic values. Our proof proceeds via a simple reduction to the case $k = 2$ of a binary alphabet (i.e. a binomial distribution), and has the property that improvements in the case of $k = 2$ directly translate to improvements for general $k$. In particular, we conjecture a bound on the binomial moment generating function that would almost close the quadratic gap between our finite-sample bound and the asymptotic moment generating function bound from Wilks' theorem (which does not hold for finite samples)."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Consider a multinomial distribution over a finite alphabet of size k, with n independent samples. According to the research, how does the new bound on the Kullback-Leibler divergence (\u03b5) compare to the standard method of types bound, and what is the primary advantage of this new bound?\n\nA) The new bound requires \u03b5 > (k-1)/n, while the standard bound requires \u03b5 > (k-1)/n \u00b7 log(1 + n/(k-1)). The new bound is tighter by a factor of order log(n/k) when n \u226b k.\n\nB) The new bound requires \u03b5 > k/n, while the standard bound requires \u03b5 > (k-1)/n. The new bound is tighter by a constant factor for all values of n and k.\n\nC) The new bound requires \u03b5 > (k-1)/n, while the standard bound requires \u03b5 > k/n. The new bound is looser but applies to a wider range of distributions.\n\nD) The new bound and the standard bound both require \u03b5 > (k-1)/n, but the new bound provides tighter concentration for small values of n.\n\nCorrect Answer: A\n\nExplanation: The correct answer is A. The documentation states that the new bound becomes meaningful when \u03b5 > (k-1)/n, while the standard method of types bound requires \u03b5 > (1/n) \u00b7 log(binom(n+k-1, k-1)), which is greater than or equal to (k-1)/n \u00b7 log(1 + n/(k-1)). The key advantage of the new bound is that it saves a factor of order log(n/k) in the standard regime where n \u226b k (n is much larger than k). This improvement is significant as it provides a tighter concentration inequality for the Kullback-Leibler divergence between the empirical distribution and the true distribution."}, "40": {"documentation": {"title": "Mori-Zwanzig reduced models for uncertainty quantification II: Initial\n  condition uncertainty", "source": "Panagiotis Stinis", "docs_id": "1212.6360", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mori-Zwanzig reduced models for uncertainty quantification II: Initial\n  condition uncertainty. In a recent preprint (arXiv:1211.4285v1) we addressed the problem of constructing reduced models for time-dependent systems described by differential equations which involve uncertain parameters. In the current work, we focus on the construction of reduced models for systems of differential equations when the initial condition is uncertain. While for both cases the reduced models are constructed through the Mori-Zwanzig formalism, the necessary estimation of the memory parameters is quite different. For the case of uncertain initial conditions we present an algorithm which allows to estimate on the fly the parameters appearing in the reduced model. The first part of the algorithm evolves the full system until the estimation of the parameters for the reduced model has converged. At the time instant that this happens, the algorithm switches to the evolution of only the reduced model with the estimated parameter values from the first part of the algorithm. The viscous Burgers equation with uncertain initial condition is used to illustrate the construction."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of Mori-Zwanzig reduced models for uncertainty quantification with uncertain initial conditions, which of the following statements best describes the algorithm presented in the paper?\n\nA) The algorithm evolves only the reduced model from the beginning, using pre-computed parameter estimates.\n\nB) The algorithm alternates between evolving the full system and the reduced model throughout the entire simulation.\n\nC) The algorithm first evolves the full system until parameter estimation converges, then switches to evolving only the reduced model with the estimated parameters.\n\nD) The algorithm evolves the full system and the reduced model simultaneously, continuously updating parameter estimates.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that the algorithm has two parts. In the first part, it evolves the full system until the estimation of the parameters for the reduced model has converged. Once convergence is achieved, the algorithm switches to the second part, where it evolves only the reduced model using the parameter values estimated in the first part.\n\nAnswer A is incorrect because the algorithm doesn't use pre-computed estimates but calculates them on the fly.\n\nAnswer B is incorrect as the algorithm doesn't alternate between the full and reduced models throughout the simulation. It has a clear transition point.\n\nAnswer D is incorrect because the algorithm doesn't evolve both models simultaneously. It first evolves the full system, then switches to the reduced model.\n\nThis question tests the student's understanding of the unique approach presented in the paper for handling uncertainty in initial conditions within the Mori-Zwanzig framework."}, "41": {"documentation": {"title": "Mixed Precision DNN Qunatization for Overlapped Speech Separation and\n  Recognition", "source": "Junhao Xu, Jianwei Yu, Xunying Liu, Helen Meng", "docs_id": "2111.14479", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mixed Precision DNN Qunatization for Overlapped Speech Separation and\n  Recognition. Recognition of overlapped speech has been a highly challenging task to date. State-of-the-art multi-channel speech separation system are becoming increasingly complex and expensive for practical applications. To this end, low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. However, current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different model components to quantization errors. In this paper, novel mixed precision DNN quantization methods are proposed by applying locally variable bit-widths to individual TCN components of a TF masking based multi-channel speech separation system. The optimal local precision settings are automatically learned using three techniques. The first two approaches utilize quantization sensitivity metrics based on either the mean square error (MSE) loss function curvature, or the KL-divergence measured between full precision and quantized separation models. The third approach is based on mixed precision neural architecture search. Experiments conducted on the LRS3-TED corpus simulated overlapped speech data suggest that the proposed mixed precision quantization techniques consistently outperform the uniform precision baseline speech separation systems of comparable bit-widths in terms of SI-SNR and PESQ scores as well as word error rate (WER) reductions up to 2.88% absolute (8% relative)."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the novel approach to DNN quantization for overlapped speech separation and recognition as presented in the paper?\n\nA) It applies a uniform bit-width across all components of the TF masking based multi-channel speech separation system.\n\nB) It uses mixed precision quantization with locally variable bit-widths for individual TCN components, determined by manual tuning.\n\nC) It employs mixed precision quantization with locally variable bit-widths for individual TCN components, automatically learned using three distinct techniques.\n\nD) It focuses solely on reducing model size without considering the impact on separation performance or word error rate.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The paper introduces a novel approach using mixed precision DNN quantization methods that apply locally variable bit-widths to individual TCN components of a TF masking based multi-channel speech separation system. The optimal local precision settings are automatically learned using three techniques: \n1) A quantization sensitivity metric based on the mean square error (MSE) loss function curvature\n2) A metric based on the KL-divergence between full precision and quantized separation models\n3) Mixed precision neural architecture search\n\nAnswer A is incorrect because the paper specifically moves away from uniform precision quantization. Answer B is incorrect because the bit-widths are automatically learned, not manually tuned. Answer D is incorrect because the approach not only reduces model size but also considers and improves separation performance and word error rate."}, "42": {"documentation": {"title": "Contextual First-Price Auctions with Budgets", "source": "Santiago Balseiro, Christian Kroer, Rachitesh Kumar", "docs_id": "2102.10476", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contextual First-Price Auctions with Budgets. The internet advertising market is a multi-billion dollar industry, in which advertisers buy thousands of ad placements every day by repeatedly participating in auctions. In recent years, the industry has shifted to first-price auctions as the preferred paradigm for selling advertising slots. Another important and ubiquitous feature of these auctions is the presence of campaign budgets, which specify the maximum amount the advertisers are willing to pay over a specified time period. In this paper, we present a new model to study the equilibrium bidding strategies in first-price auctions for advertisers who satisfy budget constraints on average. Our model dispenses with the common, yet unrealistic assumption that advertisers' values are independent and instead assumes a contextual model in which advertisers determine their values using a common feature vector. We show the existence of a natural value-pacing-based Bayes-Nash equilibrium under very mild assumptions, and study its structural properties. Furthermore, we generalize the existence result to standard auctions and prove a revenue equivalence showing that all standard auctions yield the same revenue even in the presence of budget constraints."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of first-price auctions with budget constraints in internet advertising, which of the following statements is most accurate regarding the model and findings presented in the paper?\n\nA) The model assumes independent advertiser values and proves that first-price auctions always generate more revenue than second-price auctions under budget constraints.\n\nB) The paper introduces a contextual model where advertisers use a common feature vector to determine values, and demonstrates that all standard auctions yield the same revenue under budget constraints.\n\nC) The study focuses exclusively on second-price auctions and shows that budget constraints lead to unpredictable equilibrium bidding strategies.\n\nD) The model proves that first-price auctions with budget constraints always result in higher advertiser satisfaction compared to other auction formats.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the paper introduces a contextual model where advertisers use a common feature vector to determine their values, moving away from the assumption of independent values. Additionally, the paper proves a revenue equivalence theorem, showing that all standard auctions (including first-price and second-price) yield the same revenue even with budget constraints. \n\nOption A is incorrect because the model specifically does not assume independent advertiser values, and it doesn't claim first-price auctions generate more revenue.\n\nOption C is incorrect as the paper focuses on first-price auctions, not second-price auctions, and it does show the existence of equilibrium bidding strategies.\n\nOption D is incorrect because the paper doesn't make claims about advertiser satisfaction, instead focusing on equilibrium strategies and revenue equivalence."}, "43": {"documentation": {"title": "Aspects of Jamming in Two-Dimensional Frictionless Systems", "source": "C. Reichhardt and C.J. Olson Reichhardt", "docs_id": "1312.5367", "section": ["cond-mat.soft", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Aspects of Jamming in Two-Dimensional Frictionless Systems. In this work we provide an overview of jamming transitions in two dimensional systems focusing on the limit of frictionless particle interactions in the absence of thermal fluctuations. We first discuss jamming in systems with short range repulsive interactions, where the onset of jamming occurs at a critical packing density and where certain quantities show a divergence indicative of critical behavior. We describe how aspects of the dynamics change as the jamming density is approached and how these dynamics can be explored using externally driven probes. Different particle shapes can produce jamming densities much lower than those observed for disk-shaped particles, and we show how jamming exhibits fragility for some shapes while for other shapes this is absent. Next we describe the effects of long range interactions and jamming behavior in systems such as charged colloids, vortices in type-II superconductors, and dislocations. We consider the effect of adding obstacles to frictionless jamming systems and discuss connections between this type of jamming and systems that exhibit depinning transitions. Finally, we discuss open questions such as whether the jamming transition in all these different systems can be described by the same or a small subset of universal behaviors, as well as future directions for studies of jamming transitions in two dimensional systems, such as jamming in self-driven or active matter systems."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about jamming in two-dimensional frictionless systems is NOT correct according to the given information?\n\nA) The onset of jamming in systems with short-range repulsive interactions occurs at a specific critical packing density.\n\nB) Jamming behavior is consistent across all particle shapes, with disk-shaped particles serving as a universal model.\n\nC) Long-range interactions, such as those in charged colloids and vortices in type-II superconductors, can affect jamming behavior.\n\nD) The addition of obstacles to frictionless jamming systems may create connections to systems exhibiting depinning transitions.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it contradicts the information provided in the documentation. The text states that \"Different particle shapes can produce jamming densities much lower than those observed for disk-shaped particles, and we show how jamming exhibits fragility for some shapes while for other shapes this is absent.\" This indicates that jamming behavior is not consistent across all particle shapes, and disk-shaped particles do not serve as a universal model.\n\nOptions A, C, and D are all correct statements based on the given information:\nA) The documentation mentions that \"the onset of jamming occurs at a critical packing density\" for systems with short-range repulsive interactions.\nC) The text explicitly discusses \"the effects of long range interactions and jamming behavior in systems such as charged colloids, vortices in type-II superconductors, and dislocations.\"\nD) The document states that it considers \"the effect of adding obstacles to frictionless jamming systems and discuss connections between this type of jamming and systems that exhibit depinning transitions.\""}, "44": {"documentation": {"title": "Threshold learning dynamics in social networks", "source": "J. C. Gonz\\'alez-Avella, V. M. Egu\\'iluz, M. Marsili, F. Vega-Redondo\n  and M. San Miguel", "docs_id": "1008.3083", "section": ["physics.soc-ph", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Threshold learning dynamics in social networks. Social learning is defined as the ability of a population to aggregate information, a process which must crucially depend on the mechanisms of social interaction. Consumers choosing which product to buy, or voters deciding which option to take respect to an important issues, typically confront external signals to the information gathered from their contacts. Received economic models typically predict that correct social learning occurs in large populations unless some individuals display unbounded influence. We challenge this conclusion by showing that an intuitive threshold process of individual adjustment does not always lead to such social learning. We find, specifically, that three generic regimes exist. And only in one of them, where the threshold is within a suitable intermediate range, the population learns the correct information. In the other two, where the threshold is either too high or too low, the system either freezes or enters into persistent flux, respectively. These regimes are generally observed in different social networks (both complex or regular), but limited interaction is found to promote correct learning by enlarging the parameter region where it occurs."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: According to the research on threshold learning dynamics in social networks, which of the following statements is true regarding the conditions for correct social learning to occur?\n\nA) Correct social learning always occurs in large populations unless some individuals have unbounded influence.\n\nB) The threshold for individual adjustment must be either very high or very low for the population to learn the correct information.\n\nC) Limited interaction between individuals in a network hinders the occurrence of correct social learning.\n\nD) Correct social learning is most likely to occur when the threshold for individual adjustment falls within an intermediate range.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the key findings from the research on threshold learning dynamics in social networks. Option A is incorrect because the research challenges this traditional economic model. Option B is the opposite of what the research found; very high or very low thresholds lead to system freezing or persistent flux, not correct learning. Option C is incorrect because the text states that limited interaction actually promotes correct learning. Option D is correct, as the research specifically states that correct social learning occurs when \"the threshold is within a suitable intermediate range.\" This option accurately reflects the main finding of the study regarding the conditions necessary for a population to learn the correct information."}, "45": {"documentation": {"title": "Dynamics of contentment", "source": "Alexey A. Burluka", "docs_id": "2101.05655", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of contentment. A continuous variable changing between 0 and 1 is introduced to characterise contentment, or satisfaction with life, of an individual and an equation governing its evolution is postulated from analysis of several factors likely to affect the contentment. As contentment is strongly affected by material well-being, a similar equation is formulated for wealth of an individual and from these two equations derived an evolution equation for the joint distribution of individuals' wealth and contentment within a society. The equation so obtained is used to compute evolution of this joint distribution in a society with initially low variation of wealth and contentment over a long period time. As illustration of this model capabilities, effects of the wealth tax rate are simulated and it is shown that a higher taxation in the longer run may lead to a wealthier and more content society. It is also shown that lower rates of the wealth tax lead to pronounced stratification of the society in terms of both wealth and contentment and that there is no direct relationship between the average values of these two variables."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the model described, which of the following statements is most accurate regarding the relationship between wealth tax rates and societal outcomes?\n\nA) Higher wealth tax rates always lead to decreased contentment and wealth inequality in the short term.\n\nB) Lower wealth tax rates result in a more homogeneous distribution of wealth and contentment across society over time.\n\nC) There is a direct linear relationship between average wealth and average contentment in a society regardless of tax rates.\n\nD) Higher wealth tax rates may, in the long run, contribute to both increased overall wealth and higher levels of contentment in society.\n\nCorrect Answer: D\n\nExplanation: The documentation states that \"a higher taxation in the longer run may lead to a wealthier and more content society.\" It also mentions that lower tax rates lead to \"pronounced stratification of the society in terms of both wealth and contentment.\" Furthermore, it explicitly states that \"there is no direct relationship between the average values of these two variables,\" refuting options A, B, and C. Option D correctly captures the nuanced relationship between higher tax rates and long-term societal outcomes as described in the model."}, "46": {"documentation": {"title": "Cellular buckling in stiffened plates", "source": "M. Ahmer Wadee, Maryam Farsi", "docs_id": "1405.0216", "section": ["cond-mat.mtrl-sci", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cellular buckling in stiffened plates. An analytical model based on variational principles for a thin-walled stiffened plate subjected to axial compression is presented. A system of nonlinear differential and integral equations is derived and solved using numerical continuation. The results show that the system is susceptible to highly unstable local--global mode interaction after an initial instability is triggered. Moreover, snap-backs in the response showing sequential destabilization and restabilization, known as cellular buckling or snaking, arise. The analytical model is compared to static finite element models for joint conditions between the stiffener and the main plate that have significant rotational restraint. However, it is known from previous studies that the behaviour, where the same joint is insignificantly restrained rotationally, is captured better by an analytical approach than by standard finite element methods; the latter being unable to capture cellular buckling behaviour even though the phenomenon is clearly observed in laboratory experiments."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of cellular buckling in stiffened plates, which of the following statements is most accurate regarding the comparison between analytical models and finite element methods?\n\nA) Finite element models consistently outperform analytical models in capturing cellular buckling behavior for all joint conditions.\n\nB) Analytical models are superior to finite element methods only when modeling joints with significant rotational restraint.\n\nC) For joints with insignificant rotational restraint, analytical approaches better capture cellular buckling behavior compared to standard finite element methods.\n\nD) Both analytical models and finite element methods equally fail to represent cellular buckling phenomena observed in laboratory experiments.\n\nCorrect Answer: C\n\nExplanation: The passage states that \"it is known from previous studies that the behaviour, where the same joint is insignificantly restrained rotationally, is captured better by an analytical approach than by standard finite element methods.\" This directly supports answer C. The text also mentions that standard finite element methods are \"unable to capture cellular buckling behaviour even though the phenomenon is clearly observed in laboratory experiments\" for joints with insignificant rotational restraint. This further reinforces why C is the correct answer and eliminates options A, B, and D.\n\nOption A is incorrect because the passage indicates that finite element models do not always outperform analytical models, especially for joints with insignificant rotational restraint. Option B is the opposite of what the passage suggests, as it states that analytical models are better for insignificantly restrained joints, not significantly restrained ones. Option D is incorrect because while finite element methods fail to capture cellular buckling in certain conditions, analytical approaches are said to capture this behavior better, and the phenomenon is observed in laboratory experiments."}, "47": {"documentation": {"title": "The Cepheid mass discrepancy and pulsation-driven mass loss", "source": "Hilding R. Neilson, Matteo Cantiello, Norbert Langer (AIfA)", "docs_id": "1104.1638", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Cepheid mass discrepancy and pulsation-driven mass loss. Context. A longstanding challenge for understanding classical Cepheids is the Cepheid mass discrepancy, where theoretical mass estimates using stellar evolution and stellar pulsation calculations have been found to differ by approximately 10 - 20%. Aims. We study the role of pulsation-driven mass loss during the Cepheid stage of evolution as a possible solution to this mass discrepancy. Methods. We computed stellar evolution models with a Cepheid mass-loss prescription and various amounts of convective core overshooting. The contribution of mass loss towards the mass discrepancy is determined using these models, Results. Pulsation-driven mass loss is found to trap Cepheid evolution on the instability strip, allowing them to lose about 5 - 10% of their total mass when moderate convective core overshooting, an amount consistent with observations of other stars, is included in the stellar models. Conclusions. We find that the combination of moderate convective core overshooting and pulsation-driven mass loss can solve the Cepheid mass discrepancy."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which combination of factors does the study suggest as a solution to the longstanding Cepheid mass discrepancy problem?\n\nA) Pulsation-driven mass loss alone\nB) Moderate convective core overshooting alone\nC) Pulsation-driven mass loss combined with significant convective core overshooting\nD) Pulsation-driven mass loss combined with moderate convective core overshooting\n\nCorrect Answer: D\n\nExplanation: The study concludes that the combination of moderate convective core overshooting and pulsation-driven mass loss can solve the Cepheid mass discrepancy. The research found that pulsation-driven mass loss traps Cepheid evolution on the instability strip, allowing them to lose about 5-10% of their total mass when moderate convective core overshooting is included in the stellar models. This amount of overshooting is described as consistent with observations of other stars. Option A is incorrect because pulsation-driven mass loss alone is not suggested as the complete solution. Option B is incorrect because moderate convective core overshooting alone is not sufficient. Option C is incorrect because the study specifically mentions moderate, not significant, convective core overshooting."}, "48": {"documentation": {"title": "Bridging Physics-based and Data-driven modeling for Learning Dynamical\n  Systems", "source": "Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang, Rose Yu", "docs_id": "2011.10616", "section": ["cs.LG", "physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bridging Physics-based and Data-driven modeling for Learning Dynamical\n  Systems. How can we learn a dynamical system to make forecasts, when some variables are unobserved? For instance, in COVID-19, we want to forecast the number of infected and death cases but we do not know the count of susceptible and exposed people. While mechanics compartment models are widely used in epidemic modeling, data-driven models are emerging for disease forecasting. We first formalize the learning of physics-based models as AutoODE, which leverages automatic differentiation to estimate the model parameters. Through a benchmark study on COVID-19 forecasting, we notice that physics-based mechanistic models significantly outperform deep learning. Our method obtains a 57.4% reduction in mean absolute errors for 7-day ahead COVID-19 forecasting compared with the best deep learning competitor. Such performance differences highlight the generalization problem in dynamical system learning due to distribution shift. We identify two scenarios where distribution shift can occur: changes in data domain and changes in parameter domain (system dynamics). Through systematic experiments on several dynamical systems, we found that deep learning models fail to forecast well under both scenarios. While much research on distribution shift has focused on changes in the data domain, our work calls attention to rethink generalization for learning dynamical systems."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the context of learning dynamical systems for forecasting, which of the following statements best describes the relationship between physics-based models and data-driven models, particularly in the case of COVID-19 forecasting?\n\nA) Data-driven models consistently outperform physics-based models due to their ability to handle unobserved variables.\n\nB) Physics-based models, implemented as AutoODE, showed a 57.4% reduction in mean absolute errors compared to the best deep learning model for 7-day ahead COVID-19 forecasting.\n\nC) Deep learning models demonstrated superior generalization capabilities under both data domain and parameter domain changes.\n\nD) The performance of physics-based and data-driven models was roughly equivalent, with neither approach showing a significant advantage.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage explicitly states that physics-based mechanistic models significantly outperformed deep learning models in COVID-19 forecasting. Specifically, it mentions that their method (which leverages physics-based models implemented as AutoODE) achieved a 57.4% reduction in mean absolute errors for 7-day ahead COVID-19 forecasting compared to the best deep learning competitor.\n\nOption A is incorrect because the passage indicates that physics-based models outperformed data-driven models, not the other way around.\n\nOption C is incorrect because the passage highlights that deep learning models failed to forecast well under both scenarios of distribution shift: changes in data domain and changes in parameter domain (system dynamics).\n\nOption D is incorrect because the passage clearly states that there was a significant performance difference between physics-based and data-driven models, with physics-based models showing a clear advantage.\n\nThis question tests the reader's comprehension of the key findings presented in the passage, particularly the comparative performance of physics-based and data-driven models in dynamical system learning and forecasting."}, "49": {"documentation": {"title": "Hadron Collider Tests of Neutrino Mass-Generating Mechanisms", "source": "Richard E. Ruiz", "docs_id": "1509.06375", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hadron Collider Tests of Neutrino Mass-Generating Mechanisms. The Standard Model of particle physics (SM) is presently the best description of nature at small distances and high energies. However, with tiny but nonzero neutrino masses, a Higgs boson mass unstable under radiative corrections, and little guidance on understanding the hierarchy of fermion masses, the SM remains an unsatisfactory description of nature. Well-motivated scenarios that resolve these issues exist but also predict extended gauge (e.g., Left-Right Symmetric Models), scalar (e.g., Supersymmetry), and/or fermion sectors (e.g., Seesaw Models). Hence, discovering such new states would have far-reaching implications. After reviewing basic tenets of the SM and collider physics, several beyond the SM (BSM) scenarios that alleviate these shortcomings are investigated. Emphasis is placed on the production of a heavy Majorana neutrinos at hadron colliders in the context of low-energy, effective theories that simultaneously explain the origin of neutrino masses and their smallness compared to other elementary fermions, the so-called Seesaw Mechanisms. As probes of new physics, rare top quark decays to Higgs bosons in the context of the SM, the Types I and II Two Higgs Doublet Model (2HDM), and the semi-model independent framework of Effective Field Theory (EFT) have also been investigated. Observation prospects and discovery potentials of these models at current and future collider experiments are quantified."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the relationship between the Standard Model (SM) of particle physics and the proposed beyond Standard Model (BSM) scenarios discussed in the text?\n\nA) BSM scenarios completely replace the Standard Model, offering entirely new explanations for all particle interactions.\n\nB) BSM scenarios are minor adjustments to the Standard Model that don't introduce any new particles or forces.\n\nC) BSM scenarios extend the Standard Model by introducing new particles and interactions to address its shortcomings while retaining its successful aspects.\n\nD) BSM scenarios focus solely on explaining neutrino masses without addressing other issues in the Standard Model.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text indicates that while the Standard Model is currently the best description of nature at small distances and high energies, it has shortcomings such as unexplained neutrino masses and the hierarchy problem. The BSM scenarios discussed (e.g., Left-Right Symmetric Models, Supersymmetry, Seesaw Models) aim to address these issues by extending the SM with new particles, interactions, or symmetries. These scenarios don't completely replace the SM (ruling out A), nor are they minor adjustments (ruling out B). While neutrino masses are a key focus, the BSM theories discussed address multiple issues beyond just neutrinos (ruling out D). The correct answer reflects that BSM scenarios build upon the successful aspects of the SM while introducing new elements to resolve its shortcomings."}, "50": {"documentation": {"title": "CP Factor Model for Dynamic Tensors", "source": "Yuefeng Han, Cun-Hui Zhang and Rong Chen", "docs_id": "2110.15517", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CP Factor Model for Dynamic Tensors. Observations in various applications are frequently represented as a time series of multidimensional arrays, called tensor time series, preserving the inherent multidimensional structure. In this paper, we present a factor model approach, in a form similar to tensor CP decomposition, to the analysis of high-dimensional dynamic tensor time series. As the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on Tucker-type tensor decomposition. The model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes, making it much more convenient to study the underlying dynamics of the time series. A new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures commonly used in Tucker-type tensor factor model and general tensor CP decomposition procedures. Theoretical investigation provides statistical error bounds for the proposed methods, which shows the significant advantage of utilizing the special model structure. Simulation study is conducted to further demonstrate the finite sample properties of the estimators. Real data application is used to illustrate the model and its interpretations."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the context of the CP Factor Model for Dynamic Tensors, which of the following statements is NOT true?\n\nA) The model allows for a set of uncorrelated one-dimensional latent dynamic factor processes.\n\nB) The loading vectors in this model are uniquely defined and always orthogonal.\n\nC) The model structure is similar to tensor CP decomposition.\n\nD) A new high order projection estimator is proposed, utilizing the special structure and higher order orthogonal iteration procedures.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation specifically states that \"As the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on Tucker-type tensor decomposition.\" This contradicts the statement in option B that the loading vectors are always orthogonal.\n\nOption A is true according to the text: \"The model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes.\"\n\nOption C is also true, as the document mentions: \"we present a factor model approach, in a form similar to tensor CP decomposition.\"\n\nOption D is correct as well, as stated in the text: \"A new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures.\"\n\nThis question tests the reader's understanding of the key differences between the CP Factor Model and other tensor decomposition models, particularly with regard to the properties of the loading vectors."}, "51": {"documentation": {"title": "Bare Action and Regularized Functional Integral of Asymptotically Safe\n  Quantum Gravity", "source": "Elisa Manrique, Martin Reuter", "docs_id": "0811.3888", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bare Action and Regularized Functional Integral of Asymptotically Safe\n  Quantum Gravity. Investigations of Quantum Einstein Gravity (QEG) based upon the effective average action employ a flow equation which does not contain any ultraviolet (UV) regulator. Its renormalization group trajectories emanating from a non-Gaussian fixed point define asymptotically safe quantum field theories. A priori these theories are, somewhat unusually, given in terms of their effective rather than bare action. In this paper we construct a functional integral representation of these theories. We fix a regularized measure and show that every trajectory of effective average actions, depending on an IR cutoff only, induces an associated trajectory of bare actions which depend on a UV cutoff. Together with the regularized measure these bare actions give rise to a functional integral which reproduces the prescribed effective action when the UV cutoff is removed. In this way we are able to reconstruct the underlying microscopic (\"classical\") system and identify its fundamental degrees of freedom and interactions. The bare action of the Einstein-Hilbert truncation is computed and its flow is analyzed as an example. Various conceptual issues related to the completion of the asymptotic safety program are discussed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Asymptotically Safe Quantum Gravity, what is the primary purpose of constructing a functional integral representation of the theories defined by renormalization group trajectories emanating from a non-Gaussian fixed point?\n\nA) To introduce an ultraviolet (UV) regulator into the flow equation\nB) To derive the Einstein-Hilbert truncation from first principles\nC) To reconstruct the underlying microscopic system and identify its fundamental degrees of freedom and interactions\nD) To prove the existence of a non-Gaussian fixed point in Quantum Einstein Gravity\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that the construction of a functional integral representation allows for the reconstruction of the underlying microscopic (\"classical\") system and the identification of its fundamental degrees of freedom and interactions. This is a crucial step in connecting the effective theory approach of asymptotically safe quantum gravity to a more traditional quantum field theory formulation.\n\nAnswer A is incorrect because the flow equation in this approach explicitly does not contain any UV regulator, as mentioned in the text.\n\nAnswer B is not the primary purpose of constructing the functional integral representation. While the Einstein-Hilbert truncation is mentioned as an example, deriving it is not the main goal of this approach.\n\nAnswer D, while related to asymptotic safety, is not the purpose of constructing the functional integral representation. The existence of the non-Gaussian fixed point is assumed in this context, not proved by this particular construction."}, "52": {"documentation": {"title": "Search for Anomalous Couplings in Top Decay at Hadron Colliders", "source": "S. Tsuno, I. Nakano, Y. Sumino, R. Tanaka", "docs_id": "hep-ex/0512037", "section": ["hep-ex", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for Anomalous Couplings in Top Decay at Hadron Colliders. We present a quantitative study on sensitivities to the top-decay anomalous couplings, taking into account realistic experimental conditions expected at Tevatron and LHC. A double angular distribution of W and charged lepton in the top decay is analyzed, using ttbar events in the lepton+jets channel. In order to improve sensitivities to the anomalous couplings, we apply two techniques: (1) We use a likelihood fitting method for full kinematical reconstruction of each top event. (2) We develop a new effective spin reconstruction method for leptonically-decayed top quarks; this method does not require spin information of the antitop side. For simplicity, we neglect couplings of right-handed bottom quark as well as CP violating couplings. The 95% C.L. estimated bound on a ratio of anomalous couplings reads -0.81 < f_2/f_1 < -0.70, -0.12<f_2/f_1<0.14 using 1000 reconstructed top events at Tevatron, while -0.74<f_2/f_1<-0.72, -0.01<f_2/f_1<0.01 is expected with 100k reconstructed top events at LHC, where only statistical errors are taken into account. A two-fold ambiguity in the allowed range remains when the number of events exceeds a few hundred."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of anomalous couplings in top decay at hadron colliders, which of the following statements is correct regarding the estimated bounds on the ratio of anomalous couplings (f_2/f_1) at the LHC, assuming 100k reconstructed top events and considering only statistical errors?\n\nA) -0.81 < f_2/f_1 < -0.70 and -0.12 < f_2/f_1 < 0.14\nB) -0.74 < f_2/f_1 < -0.72 and -0.01 < f_2/f_1 < 0.01\nC) -0.81 < f_2/f_1 < -0.70 and -0.01 < f_2/f_1 < 0.01\nD) -0.74 < f_2/f_1 < -0.72 and -0.12 < f_2/f_1 < 0.14\n\nCorrect Answer: B\n\nExplanation: The question tests the understanding of the specific results presented in the documentation for different colliders and event numbers. According to the text, the estimated bound on the ratio of anomalous couplings (f_2/f_1) at the LHC with 100k reconstructed top events is -0.74 < f_2/f_1 < -0.72 and -0.01 < f_2/f_1 < 0.01. This corresponds exactly to option B. \n\nOption A is incorrect as it represents the bounds for the Tevatron with 1000 reconstructed events. Options C and D are incorrect as they mix the results from Tevatron and LHC. The question specifically asks about the LHC results with 100k events, making B the only correct answer."}, "53": {"documentation": {"title": "Group Secret Key Generation Algorithms", "source": "Chunxuan Ye and Alex Reznik", "docs_id": "cs/0701124", "section": ["cs.IT", "cs.CR", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Group Secret Key Generation Algorithms. We consider a pair-wise independent network where every pair of terminals in the network observes a common pair-wise source that is independent of all the sources accessible to the other pairs. We propose a method for secret key agreement in such a network that is based on well-established point-to-point techniques and repeated application of the one-time pad. Three specific problems are investigated. 1) Each terminal's observations are correlated only with the observations of a central terminal. All these terminals wish to generate a common secret key. 2) In a pair-wise independent network, two designated terminals wish to generate a secret key with the help of other terminals. 3) All the terminals in a pair-wise independent network wish to generate a common secret key. A separate protocol for each of these problems is proposed. Furthermore, we show that the protocols for the first two problems are optimal and the protocol for the third problem is efficient, in terms of the resulting secret key rates."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In a pair-wise independent network where every pair of terminals observes a common pair-wise source independent of other pairs, which of the following statements is NOT true regarding the proposed secret key generation methods?\n\nA) The protocol for generating a common secret key between a central terminal and multiple terminals with correlated observations is proven to be optimal in terms of secret key rates.\n\nB) The protocol for two designated terminals generating a secret key with help from other terminals in the network is shown to be optimal in terms of secret key rates.\n\nC) The protocol for all terminals in the network to generate a common secret key is proven to be optimal in terms of secret key rates.\n\nD) The proposed method for secret key agreement in such networks is based on established point-to-point techniques and repeated application of the one-time pad.\n\nCorrect Answer: C\n\nExplanation: The question asks for the statement that is NOT true. Options A, B, and D are all correct according to the given information. The document states that the protocols for the first two problems (corresponding to options A and B) are optimal, and that the method is based on point-to-point techniques and one-time pad (option D). However, for the third problem where all terminals wish to generate a common secret key (option C), the protocol is described as \"efficient\" but not explicitly stated to be optimal in terms of secret key rates. Therefore, C is the correct answer as it is not a true statement based on the given information."}, "54": {"documentation": {"title": "Path Integral Renormalization of Flow through Random Porous Media", "source": "Umut C. \\\"Ozer, Peter R. King, Dimitri D. Vvedensky", "docs_id": "1911.11218", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Path Integral Renormalization of Flow through Random Porous Media. The path integral for Darcy's law with a stochastic conductivity, which characterizes flow through random porous media, is used as a basis for Wilson renormalization-group (RG) calculations in momentum space. A coarse graining procedure is implemented by integrating over infinitesimal shells of large momenta corresponding to the elimination of the small scale modes of the theory. The resulting one-loop $\\beta$-functions are solved exactly to obtain an effective conductivity in a coarse grained theory over successively larger length scales. We first carry out a calculation with uncorrelated Gaussian conductivity fluctuations to illustrate the RG procedure before considering the effect of a finite correlation length of conductivity fluctuations. We conclude by discussing applications and extensions of our calculations, including comparisons with the numerical evaluation of path integrals, non-Gaussian fluctuations, and multiphase flow, for which the path integral formulation should prove particularly useful."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the path integral renormalization approach to flow through random porous media, what is the primary purpose of integrating over infinitesimal shells of large momenta in the Wilson renormalization-group calculations?\n\nA) To eliminate large scale modes of the theory\nB) To increase the complexity of the conductivity fluctuations\nC) To implement a coarse graining procedure by eliminating small scale modes\nD) To introduce non-Gaussian fluctuations into the model\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"A coarse graining procedure is implemented by integrating over infinitesimal shells of large momenta corresponding to the elimination of the small scale modes of the theory.\" This process is a key step in the Wilson renormalization-group calculations, allowing the researchers to develop an effective conductivity model over increasingly larger length scales.\n\nOption A is incorrect because the process eliminates small scale modes, not large scale modes. Option B is incorrect as the integration doesn't increase complexity but rather simplifies the model through coarse graining. Option D is incorrect because the introduction of non-Gaussian fluctuations is mentioned as a potential extension of the work, not a result of this integration process."}, "55": {"documentation": {"title": "Supernova bounds on axion-like particles coupled with nucleons and\n  electrons", "source": "Francesca Calore, Pierluca Carenza, Maurizio Giannotti, Joerg Jaeckel,\n  Giuseppe Lucente, Alessandro Mirizzi", "docs_id": "2107.02186", "section": ["hep-ph", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supernova bounds on axion-like particles coupled with nucleons and\n  electrons. We investigate the potential of type II supernovae (SNe) to constrain axion-like particles (ALPs) coupled simultaneously to nucleons and electrons. ALPs coupled to nucleons can be efficiently produced in the SN core via nucleon-nucleon bremsstrahlung and, for a wide range of parameters, leave the SN unhindered, producing a large ALP flux. For masses exceeding 1 MeV, these ALPs would decay into electron-positron pairs, generating a positron flux. In the case of Galactic SNe, the annihilation of the created positrons with the electrons present in the Galaxy would contribute to the 511 keV annihilation line. Using the SPI (SPectrometer on INTEGRAL) observation of this line, allows us to exclude a wide range of the axion-electron coupling, $10^{-19} \\lesssim g_{ae} \\lesssim 10^{-11}$, for $g_{ap}\\sim 10^{-9}$. Additionally, ALPs from extra-galactic SNe decaying into electron-positron pairs would yield a contribution to the cosmic X-ray background. In this case, we constrain the ALP-electron coupling down to $g_{ae} \\sim 10^{-20}$."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: A researcher is studying axion-like particles (ALPs) with a mass of 2 MeV that couple to both nucleons and electrons. The nucleon coupling constant is g\u2090\u209a ~ 10\u207b\u2079, and the researcher wants to determine the allowed range for the electron coupling constant g\u2090\u2091 based on supernova constraints. Which of the following statements is most accurate regarding the constraints on g\u2090\u2091?\n\nA) The allowed range for g\u2090\u2091 is approximately 10\u207b\u00b9\u2079 < g\u2090\u2091 < 10\u207b\u00b9\u00b9, based solely on the 511 keV gamma-ray line observations from Galactic supernovae.\n\nB) The allowed range for g\u2090\u2091 is g\u2090\u2091 < 10\u207b\u00b2\u2070, based solely on the contribution to the cosmic X-ray background from extra-galactic supernovae.\n\nC) There are no constraints on g\u2090\u2091 for ALPs with masses exceeding 1 MeV, as they cannot be produced efficiently in supernova cores.\n\nD) The allowed range for g\u2090\u2091 is g\u2090\u2091 < 10\u207b\u00b2\u2070 or g\u2090\u2091 > 10\u207b\u00b9\u00b9, based on a combination of constraints from the 511 keV line and the cosmic X-ray background.\n\nCorrect Answer: D\n\nExplanation: The question combines multiple aspects of the given information to create a challenging scenario. The correct answer is D because:\n\n1. For g\u2090\u209a ~ 10\u207b\u2079, the 511 keV gamma-ray line observations from Galactic supernovae exclude the range 10\u207b\u00b9\u2079 \u2272 g\u2090\u2091 \u2272 10\u207b\u00b9\u00b9.\n\n2. The cosmic X-ray background from extra-galactic supernovae provides an additional constraint, limiting g\u2090\u2091 down to ~10\u207b\u00b2\u2070.\n\n3. Combining these constraints, we can conclude that g\u2090\u2091 must be either smaller than ~10\u207b\u00b2\u2070 or larger than ~10\u207b\u00b9\u00b9 to satisfy both observational limits.\n\nOption A is incorrect because it doesn't account for the extra-galactic supernova constraint. Option B is incomplete as it doesn't consider the upper limit from Galactic supernovae. Option C is false, as the document states that ALPs with masses exceeding 1 MeV can be efficiently produced and constrained."}, "56": {"documentation": {"title": "Dynamical Formation of Scalarized Black Holes and Neutron Stars through\n  Stellar Core Collapse", "source": "Hao-Jui Kuan, Daniela D. Doneva, Stoytcho S. Yazadjiev", "docs_id": "2103.11999", "section": ["gr-qc", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Formation of Scalarized Black Holes and Neutron Stars through\n  Stellar Core Collapse. In a certain class of scalar-Gauss-Bonnet gravity, the black holes and the neutron stars can undergo spontaneous scalarization -- a strong gravity phase transition triggered by a tachyonic instability due to the nonminimal coupling between the scalar field and the spacetime curvature. Studies of this phenomenon have, so far, been restricted mainly to the study of the tachyonic instability and stationary scalarized black holes and neutron stars. To date, no realistic physical mechanism for the formation of isolated scalarized black holes and neutron stars has been proposed. We study, for the first time, the spherically symmetric fully nonlinear stellar core collapse to a black hole and a neutron star in scalar-Gauss-Bonnet theories allowing for a spontaneous scalarization. We show that the core collapse can produce scalarized black holes and scalarized neutron stars starting with a nonscalarized progenitor star. The possible paths to reach the end (non)scalarized state are quite rich leading to interesting possibilities for observational manifestations."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In the context of scalar-Gauss-Bonnet gravity, which of the following statements is correct regarding the dynamical formation of scalarized black holes and neutron stars through stellar core collapse?\n\nA) Spontaneous scalarization is a weak gravity phenomenon that occurs gradually over long periods of cosmic time.\n\nB) The formation of scalarized black holes and neutron stars requires an initially scalarized progenitor star.\n\nC) The study demonstrates that core collapse can produce scalarized compact objects from non-scalarized progenitors, revealing multiple evolutionary pathways.\n\nD) The research concludes that spontaneous scalarization is impossible during the core collapse process, contradicting previous static studies.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that this study, for the first time, shows that core collapse can produce scalarized black holes and neutron stars starting with a non-scalarized progenitor star. It also mentions that the paths to reach the end (non)scalarized state are quite rich, implying multiple evolutionary pathways.\n\nOption A is incorrect because spontaneous scalarization is described as a strong gravity phase transition, not a weak gravity phenomenon.\n\nOption B is incorrect because the study demonstrates that scalarized objects can form from non-scalarized progenitors, contradicting this statement.\n\nOption D is incorrect because the research actually confirms the possibility of spontaneous scalarization during core collapse, rather than contradicting previous studies."}, "57": {"documentation": {"title": "Quark production, Bose-Einstein condensates and thermalization of the\n  quark-gluon plasma", "source": "Jean-Paul Blaizot, Bin Wu, and Li Yan", "docs_id": "1402.5049", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quark production, Bose-Einstein condensates and thermalization of the\n  quark-gluon plasma. In this paper, we study the thermalization of gluons and N_f flavors of massless quarks and antiquarks in a spatially homogeneous system. First, two coupled transport equations for gluons and quarks (and antiquarks) are derived within the diffusion approximation of the Boltzmann equation, with only 2<-> 2 processes included in the collision term. Then, these transport equations are solved numerically in order to study the thermalization of the quark-gluon plasma. At initial time, we assume that no quarks or antiquarks are present and we choose the gluon distribution in the form f = f_0 theta (1-p/Q_s) with Q_s the saturation momentum and f_0 a constant. The subsequent evolution of systems may, or may not, lead to the formation of a (transient) Bose condensate, depending on the value of f_0. In fact, we observe, depending on the value of f_0, three different patterns: (a) thermalization without gluon Bose-Einstein condensate (BEC) for f_0 < f_{0t}, (b) thermalization with transient BEC for f_{0t} < f_0 < f_{0c}, and (c) thermalization with BEC for f_{0c} < f_0. The values of f_{0t} and f_{0c} depend on N_f. When f_0> 1 > f_{0c}, the onset of BEC occurs at a finite time t_c ~ 1/((alpha_s f_0)^2 Q_s). We also find that quark production slows down the thermalization process: the equilibration time for N_f = 3 is typically about 5 to 6 times longer than that for N_f = 0 at the same Q_s."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the study of quark-gluon plasma thermalization with initial gluon distribution f = f_0 \u03b8(1-p/Q_s), which of the following statements is correct regarding the formation of Bose-Einstein condensates (BEC) and the impact of quark production?\n\nA) The formation of BEC is independent of the value of f_0 and always occurs during thermalization.\n\nB) For f_0 > f_{0c}, BEC formation begins at t_c ~ 1/((\u03b1_s f_0)^2 Q_s), and quark production accelerates the thermalization process.\n\nC) Three patterns are observed: no BEC formation (f_0 < f_{0t}), transient BEC formation (f_{0t} < f_0 < f_{0c}), and stable BEC formation (f_{0c} < f_0), with quark production slowing down thermalization.\n\nD) The equilibration time for N_f = 3 is typically about 2 to 3 times longer than that for N_f = 0 at the same Q_s, regardless of BEC formation.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key findings from the Arxiv documentation. The paper describes three patterns of thermalization depending on the value of f_0: no BEC formation, transient BEC formation, and stable BEC formation, with the thresholds f_{0t} and f_{0c}. Additionally, it states that quark production slows down the thermalization process, with N_f = 3 taking about 5 to 6 times longer to equilibrate compared to N_f = 0. \n\nOption A is incorrect because BEC formation is dependent on f_0. Option B is partially correct about the onset time of BEC but wrongly states that quark production accelerates thermalization. Option D is incorrect about the equilibration time difference between N_f = 3 and N_f = 0."}, "58": {"documentation": {"title": "Nonlinear coupling of flow harmonics: Hexagonal flow and beyond", "source": "Giuliano Giacalone, Li Yan, and Jean-Yves Ollitrault", "docs_id": "1803.00253", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear coupling of flow harmonics: Hexagonal flow and beyond. Higher Fourier harmonics of anisotropic flow ($v_4$ and beyond) get large contributions induced by elliptic and triangular flow through nonlinear response. We present a general framework of nonlinear hydrodynamic response which encompasses the existing one, and allows to take into account the mutual correlation between the nonlinear couplings affecting Fourier harmonics of any order. Using Large Hadron Collider data on Pb+Pb collisions at ~$\\sqrt[]{s}=2.76$ TeV, we perform an application of our formalism to hexagonal flow, $v_6$, a coefficient affected by several nonlinear contributions which are of the same order of magnitude. We obtain the first experimental measure of the coefficient $\\chi_{624}$, which couples $v_6$ to $v_2$ and $v_4$. This is achieved by putting together the information from several analyses: event-plane correlations, symmetric cumulants, as well as new higher-order moments recently analyzed by the ALICE collaboration. The value of $\\chi_{624}$ extracted from data is in fair agreement with hydrodynamic calculations, although with large error bars, which would be dramatically reduced by a dedicated analysis. We argue that within our formalism the nonlinear structure of a given higher harmonic can be determined more accurately than the harmonic itself, and we emphasize potential applications to future measurements of $v_7$ and $v_8$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of nonlinear hydrodynamic response in heavy-ion collisions, which of the following statements about hexagonal flow (v6) is correct?\n\nA) v6 is primarily determined by linear response and is not significantly affected by nonlinear couplings.\n\nB) The coefficient \u03c7624, which couples v6 to v2 and v4, can be accurately measured using only event-plane correlations.\n\nC) The nonlinear structure of v6 can be determined more precisely than the magnitude of v6 itself.\n\nD) Hydrodynamic calculations predict a value for \u03c7624 that is significantly different from experimental measurements.\n\nCorrect Answer: C\n\nExplanation: \nA is incorrect because the documentation states that v6 is affected by several nonlinear contributions of similar magnitude.\n\nB is incorrect because the measurement of \u03c7624 requires combining information from multiple analyses, including event-plane correlations, symmetric cumulants, and higher-order moments.\n\nC is correct. The document explicitly states: \"We argue that within our formalism the nonlinear structure of a given higher harmonic can be determined more accurately than the harmonic itself.\"\n\nD is incorrect. The documentation mentions that the experimentally extracted value of \u03c7624 is in fair agreement with hydrodynamic calculations, although with large error bars."}, "59": {"documentation": {"title": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM", "source": "Yipu Zhao, Justin S. Smith, Patricio A. Vela", "docs_id": "2008.10123", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM. The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical characteristic of resource-limited applications. While hardware and algorithm advances have been significantly improved the cost-efficiency of VSLAM front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This paper describes a novel, rigorous method to improve the cost-efficiency of local BA in a BA-based VSLAM back-end. An efficient algorithm, called Good Graph, is developed to select size-reduced graphs optimized in local BA with condition preservation. To better suit BA-based VSLAM back-ends, the Good Graph predicts future estimation needs, dynamically assigns an appropriate size budget, and selects a condition-maximized subgraph for BA estimation. Evaluations are conducted on two scenarios: 1) VSLAM as standalone process, and 2) VSLAM as part of closed-loop navigation system. Results from the first scenario show Good Graph improves accuracy and robustness of VSLAM estimation, when computational limits exist. Results from the second scenario, indicate that Good Graph benefits the trajectory tracking performance of VSLAM-based closed-loop navigation systems, which is a primary application of VSLAM."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of visual SLAM (VSLAM) systems, the Good Graph algorithm is designed to address which of the following challenges?\n\nA) Improving the efficiency of the front-end feature extraction process\nB) Enhancing the accuracy of inertial measurement unit (IMU) integration\nC) Optimizing the cost-efficiency of local bundle adjustment in the back-end\nD) Increasing the frame rate of the visual odometry component\n\nCorrect Answer: C\n\nExplanation: The correct answer is C) Optimizing the cost-efficiency of local bundle adjustment in the back-end. The document explicitly states that the Good Graph algorithm is developed to improve the cost-efficiency of local bundle adjustment (BA) in a BA-based VSLAM back-end. It addresses the bottleneck in VSLAM back-ends by selecting size-reduced graphs for optimization in local BA while preserving conditions.\n\nAnswer A is incorrect because the document mentions that front-end efficiency has already been significantly improved through hardware and algorithm advances, and is not the focus of the Good Graph algorithm.\n\nAnswer B is incorrect as the document does not discuss enhancing IMU integration accuracy.\n\nAnswer D is incorrect because the Good Graph algorithm focuses on the back-end optimization process, not on increasing the frame rate of visual odometry, which is typically a front-end component.\n\nThe Good Graph algorithm specifically targets the back-end optimization process, predicting future estimation needs, dynamically assigning size budgets, and selecting condition-maximized subgraphs for BA estimation, all aimed at improving the cost-efficiency of local BA in VSLAM back-ends."}}