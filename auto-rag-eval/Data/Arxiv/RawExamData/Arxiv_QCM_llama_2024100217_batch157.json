{"0": {"documentation": {"title": "Neutral and charged mesons in magnetic fields: A resonance gas in a\n  non-relativistic quark model", "source": "Toru Kojo", "docs_id": "2104.00376", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutral and charged mesons in magnetic fields: A resonance gas in a\n  non-relativistic quark model. We analyze mesons in constant magnetic fields ($B$) within a non-relativistic constituent quark model. Our quark model contains a harmonic oscillator type confining potential, and we perturbatively treat short range correlations to account for the spin-flavor energy splittings. We study both neutral and charged mesons taking into account the internal quark dynamics. The neutral states are labelled by two-dimensional momenta for magnetic translations, while the charged states by two discrete indices related to angular momenta. For $B \\ll \\Lambda_{\\rm QCD}^2$ ($\\Lambda_{\\rm QCD} \\sim 200$ MeV: the QCD scale), the analyses proceed as in usual quark models, while special precautions are needed for strong fields, $B \\sim \\Lambda_{QCD}^2$, especially when we treat short range correlations such as the Fermi-Breit-Pauli interactions. We compute the energy spectra of mesons up to energies of $\\sim 2.5$ GeV and use them to construct the meson resonance gas. Within the assumption that the constituent quark masses are insensitive to magnetic fields, the phase space enhancement for mesons significantly increases the entropy, assisting a transition from a hadron gas to a quark gluon plasma. We confront our results with the lattice data, finding reasonable agreement for the low-lying spectra and the entropy density at low temperature less than $\\sim 100$ MeV, but our results at higher energy scale suffer from artifacts of our confining potential and non-relativistic treatments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Arxiv paper \"Neutral and charged mesons in magnetic fields: A resonance gas in a non-relativistic quark model\", what is the main assumption made about the constituent quark masses in the presence of a magnetic field, and how does this assumption impact the phase space enhancement for mesons?\n\n**A)** The constituent quark masses are assumed to be insensitive to magnetic fields, leading to a significant increase in phase space enhancement and a transition from a hadron gas to a quark gluon plasma.\n\n**B)** The constituent quark masses are assumed to be directly affected by magnetic fields, resulting in a decrease in phase space enhancement and a transition from a quark gluon plasma to a hadron gas.\n\n**C)** The constituent quark masses are assumed to be independent of magnetic fields, but the confining potential and non-relativistic treatments lead to artifacts that affect the results at higher energy scales.\n\n**D)** The constituent quark masses are assumed to be constant and unchanging, regardless of the presence of a magnetic field, and this assumption is not explicitly addressed in the paper.\n\n**Correct Answer:** A) The constituent quark masses are assumed to be insensitive to magnetic fields, leading to a significant increase in phase space enhancement and a transition from a hadron gas to a quark gluon plasma.\n\n**Explanation:** The correct answer is A) because the paper states that \"Within the assumption that the constituent quark masses are insensitive to magnetic fields, the phase space enhancement for mesons significantly increases the entropy, assisting a transition from a hadron gas to a quark gluon plasma.\" This assumption is crucial in understanding the results of the paper, as it allows for a significant increase in phase space enhancement and a transition to a quark gluon plasma. The other options are incorrect because they either contradict the assumption made in the paper or are not explicitly addressed in the paper."}, "1": {"documentation": {"title": "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards", "source": "Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov", "docs_id": "2006.15454", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards. Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing cross-lingual text summarization methods, according to the authors of the paper?\n\n**A)** They are not effective in generating fluent sentences.\n**B)** They rely on machine translation to synthesize training data, leading to error propagation.\n**C)** They are not suitable for low-resource languages.\n**D)** They are not scalable for large datasets.\n\n**Correct Answer:** B) They rely on machine translation to synthesize training data, leading to error propagation.\n\n**Explanation:** The authors of the paper mention that existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. This is the primary limitation of existing cross-lingual text summarization methods, according to the authors."}, "2": {"documentation": {"title": "Efficient Solution of Backward Jump-Diffusion PIDEs with Splitting and\n  Matrix Exponentials", "source": "Andrey Itkin", "docs_id": "1304.3159", "section": ["q-fin.CP", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Solution of Backward Jump-Diffusion PIDEs with Splitting and\n  Matrix Exponentials. We propose a new, unified approach to solving jump-diffusion partial integro-differential equations (PIDEs) that often appear in mathematical finance. Our method consists of the following steps. First, a second-order operator splitting on financial processes (diffusion and jumps) is applied to these PIDEs. To solve the diffusion equation, we use standard finite-difference methods, which for multi-dimensional problems could also include splitting on various dimensions. For the jump part, we transform the jump integral into a pseudo-differential operator. Then for various jump models we show how to construct an appropriate first and second order approximation on a grid which supersets the grid that we used for the diffusion part. These approximations make the scheme to be unconditionally stable in time and preserve positivity of the solution which is computed either via a matrix exponential, or via P{\\'a}de approximation of the matrix exponent. Various numerical experiments are provided to justify these results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a jump-diffusion PIDE of the form:\n\n\u2202u/\u2202t + \u2207\u22c5(D\u2207u) + \u222b[0,\u221e) (\u03bc(x,t)u(x,t) - \u03bb(x,t)u(x,t-\u03c3))dN(t) = 0\n\nwhere u = u(x,t) is the solution, D is the diffusion coefficient, \u03bc and \u03bb are the jump intensity and jump measure, and N(t) is a Poisson process.\n\n**A)** The proposed method in the paper suggests using a second-order operator splitting on financial processes to solve this PIDE. What is the first step in this method, and how is it related to the standard finite-difference methods for solving the diffusion equation?\n\n**B)** The paper mentions that the jump integral can be transformed into a pseudo-differential operator. What is the form of this operator, and how is it used to construct an approximation for the jump part of the PIDE?\n\n**C)** The paper claims that the resulting scheme is unconditionally stable in time and preserves positivity of the solution. What is the condition on the parameters of the scheme that ensures this stability and positivity?\n\n**D)** The paper provides numerical experiments to justify the results. What is the specific numerical experiment mentioned in the paper that demonstrates the unconditional stability and positivity of the scheme?\n\n**Correct Answer:** A) The first step in the proposed method is to apply a second-order operator splitting on financial processes, which involves splitting the PIDE into two equations: one for the diffusion equation and one for the jump equation. The diffusion equation is solved using standard finite-difference methods, while the jump equation is solved using a pseudo-differential operator.\n\n**Explanation:** The correct answer is A) because the first step in the proposed method is indeed to apply a second-order operator splitting on financial processes, which involves splitting the PIDE into two equations: one for the diffusion equation and one for the jump equation. The diffusion equation is solved using standard finite-difference methods, while the jump equation is solved using a pseudo-differential operator. The other options are incorrect because they refer to later steps in the method or provide incorrect information about the scheme's stability and positivity."}, "3": {"documentation": {"title": "Paternalism, Autonomy, or Both? Experimental Evidence from Energy Saving\n  Programs", "source": "Takanori Ida, Takunori Ishihara, Koichiro Ito, Daido Kido, Toru\n  Kitagawa, Shosei Sakaguchi and Shusaku Sasaki", "docs_id": "2112.09850", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Paternalism, Autonomy, or Both? Experimental Evidence from Energy Saving\n  Programs. Identifying who should be treated is a central question in economics. There are two competing approaches to targeting - paternalistic and autonomous. In the paternalistic approach, policymakers optimally target the policy given observable individual characteristics. In contrast, the autonomous approach acknowledges that individuals may possess key unobservable information on heterogeneous policy impacts, and allows them to self-select into treatment. In this paper, we propose a new approach that mixes paternalistic assignment and autonomous choice. Our approach uses individual characteristics and empirical welfare maximization to identify who should be treated, untreated, and decide whether to be treated themselves. We apply this method to design a targeting policy for an energy saving programs using data collected in a randomized field experiment. We show that optimally mixing paternalistic assignments and autonomous choice significantly improves the social welfare gain of the policy. Exploiting random variation generated by the field experiment, we develop a method to estimate average treatment effects for each subgroup of individuals who would make the same autonomous treatment choice. Our estimates confirm that the estimated assignment policy optimally allocates individuals to be treated, untreated, or choose themselves based on the relative merits of paternalistic assignments and autonomous choice for individuals types."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Paternalism, Autonomy, or Both? Experimental Evidence from Energy Saving Programs\", what is the primary advantage of the proposed approach that mixes paternalistic assignment and autonomous choice?\n\n**A)** It allows policymakers to target individuals based solely on observable characteristics, without considering unobservable information.\n**B)** It enables individuals to self-select into treatment, without any consideration for the potential benefits of paternalistic assignment.\n**C)** It optimally allocates individuals to be treated, untreated, or choose themselves based on a combination of individual characteristics and empirical welfare maximization.\n**D)** It eliminates the need for randomized field experiments, relying solely on observational data.\n\n**Correct Answer:** C) It optimally allocates individuals to be treated, untreated, or choose themselves based on a combination of individual characteristics and empirical welfare maximization.\n\n**Explanation:** The correct answer, C, reflects the main contribution of the paper, which is to propose a new approach that combines the benefits of paternalistic assignment and autonomous choice. This approach uses individual characteristics and empirical welfare maximization to identify who should be treated, untreated, and decide whether to be treated themselves. The other options are incorrect because they either oversimplify the approach (A), misrepresent its focus (B), or eliminate the need for experimental design (D)."}, "4": {"documentation": {"title": "Microscopy Image Restoration with Deep Wiener-Kolmogorov filters", "source": "Valeriya Pronina, Filippos Kokkinos, Dmitry V. Dylov, Stamatios\n  Lefkimmiatis", "docs_id": "1911.10989", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microscopy Image Restoration with Deep Wiener-Kolmogorov filters. Microscopy is a powerful visualization tool in biology, enabling the study of cells, tissues, and the fundamental biological processes; yet, the observed images typically suffer from blur and background noise. In this work, we propose a unifying framework of algorithms for Gaussian image deblurring and denoising. These algorithms are based on deep learning techniques for the design of learnable regularizers integrated into the Wiener-Kolmogorov filter. Our extensive experimentation line showcases that the proposed approach achieves a superior quality of image reconstruction and surpasses the solutions that rely either on deep learning or on optimization schemes alone. Augmented with the variance stabilizing transformation, the proposed reconstruction pipeline can also be successfully applied to the problem of Poisson image deblurring, surpassing the state-of-the-art methods. Moreover, several variants of the proposed framework demonstrate competitive performance at low computational complexity, which is of high importance for real-time imaging applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed framework in the paper \"Microscopy Image Restoration with Deep Wiener-Kolmogorov filters\"?\n\nA) To develop a new optimization scheme for image deblurring and denoising\nB) To design a deep learning model for image reconstruction\nC) To create a unifying framework for Gaussian image deblurring and denoising\nD) To apply variance stabilizing transformation to image deblurring\n\n**Correct Answer:** C) To create a unifying framework for Gaussian image deblurring and denoising\n\n**Explanation:** The question requires the test-taker to understand the main objective of the paper, which is to propose a unifying framework for Gaussian image deblurring and denoising. The correct answer, option C, is supported by the introduction of the paper, which states that the authors propose a \"unifying framework of algorithms for Gaussian image deblurring and denoising\". The other options are incorrect because they either focus on a specific aspect of the framework (option A) or are unrelated to the primary goal of the paper (options B and D). \n\n**Additional question:** What is the significance of the variance stabilizing transformation in the proposed reconstruction pipeline?\n\nA) It is used to improve the computational complexity of the algorithm\nB) It is used to enhance the quality of image reconstruction\nC) It is used to stabilize the variance of the reconstructed images\nD) It is used to reduce the noise in the images\n\n**Correct Answer:** C) It is used to stabilize the variance of the reconstructed images\n\n**Explanation:** The question requires the test-taker to understand the role of variance stabilizing transformation in the proposed reconstruction pipeline. The correct answer, option C, is supported by the paper, which states that the variance stabilizing transformation is \"augmented with the proposed reconstruction pipeline\" and that it \"can also be successfully applied to the problem of Poisson image deblurring\". The other options are incorrect because they either misrepresent the purpose of variance stabilizing transformation (options A and D) or are unrelated to its significance (option B)."}, "5": {"documentation": {"title": "DyLoc: Dynamic Localization for Massive MIMO Using Predictive Recurrent\n  Neural Networks", "source": "Farzam Hejazi, Katarina Vuckovic, Nazanin Rahnavard", "docs_id": "2101.07848", "section": ["cs.CV", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DyLoc: Dynamic Localization for Massive MIMO Using Predictive Recurrent\n  Neural Networks. This paper presents a data-driven localization framework with high precision in time-varying complex multipath environments, such as dense urban areas and indoors, where GPS and model-based localization techniques come short. We consider the angle-delay profile (ADP), a linear transformation of channel state information (CSI), in massive MIMO systems and show that ADPs preserve users' motion when stacked temporally. We discuss that given a static environment, future frames of ADP time-series are predictable employing a video frame prediction algorithm. We express that a deep convolutional neural network (DCNN) can be employed to learn the background static scattering environment. To detect foreground changes in the environment, corresponding to path blockage or addition, we introduce an algorithm taking advantage of the trained DCNN. Furthermore, we present DyLoc, a data-driven framework to recover distorted ADPs due to foreground changes and to obtain precise location estimations. We evaluate the performance of DyLoc in several dynamic scenarios employing DeepMIMO dataset to generate geo-tagged CSI datasets for indoor and outdoor environments. We show that previous DCNN-based techniques fail to perform with desirable accuracy in dynamic environments, while DyLoc pursues localization precisely. Moreover, simulations show that as the environment gets richer in terms of the number of multipath, DyLoc gets more robust to foreground changes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the angle-delay profile (ADP) in massive MIMO systems, and how does it relate to the proposed DyLoc framework?\n\n**A)** ADP is a more accurate method for estimating channel state information (CSI) in time-varying environments, but it requires more complex signal processing algorithms. DyLoc uses ADP to improve localization accuracy in dynamic environments.\n\n**B)** ADP is a linear transformation of CSI that preserves users' motion when stacked temporally, allowing for more accurate location estimations in time-varying environments. DyLoc leverages this property to develop a data-driven localization framework.\n\n**C)** ADP is a more efficient method for estimating CSI in static environments, but it is less effective in dynamic environments. DyLoc addresses this limitation by employing a deep convolutional neural network (DCNN) to learn the background static scattering environment.\n\n**D)** ADP is a more robust method for estimating CSI in environments with multiple multipath components, but it is less accurate in environments with few multipath components. DyLoc improves upon this by using a video frame prediction algorithm to predict future frames of ADP.\n\n**Correct Answer:** B) ADP is a linear transformation of CSI that preserves users' motion when stacked temporally, allowing for more accurate location estimations in time-varying environments. DyLoc leverages this property to develop a data-driven localization framework.\n\n**Explanation:** The correct answer is B) because the documentation states that ADP preserves users' motion when stacked temporally, which is a key advantage of using ADP in massive MIMO systems. Additionally, DyLoc leverages this property to develop a data-driven localization framework, making B) the most accurate answer. The other options are incorrect because they either misrepresent the advantages of ADP or the purpose of DyLoc."}, "6": {"documentation": {"title": "Diffusive behavior of multiplex networks", "source": "Giulia Cencetti, Federico Battiston", "docs_id": "1904.00265", "section": ["physics.soc-ph", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diffusive behavior of multiplex networks. Diffusion describes the motion of microscopic entities from regions of high concentration to regions of low concentration. In multiplex networks, flows can occur both within and across layers, and super-diffusion, a regime where the time scale of the multiplex to reach equilibrium is smaller than that of single networks in isolation, can emerge due to the interplay of these two mechanisms. In the limits of strong and weak inter-layer couplings multiplex diffusion has been linked to the spectrum of the supra-Laplacian associated to the system. However, a general theory for the emergence of this behavior is still lacking. Here we shed light on how the structural and dynamical features of the multiplex affect the Laplacian spectral properties. For instance, we find that super-diffusion emerges the earliest in systems with poorly diffusive layers, and that its onset is independent from the presence of overlap, which only influences the maximum relative intensity of the phenomenon. Moreover, a uniform allocation of resources to enhance diffusion within layers is preferable, as highly intra-layer heterogenous flows might hamper super-diffusion. Last, in multiplex networks formed by many layers, diffusion is best promoted by strengthening inter-layer flows across dissimilar layers. Our work can turn useful for the design of interconnected infrastructures in real-world transportation systems, clarifying the determinants able to drive the system towards the super-diffusive regime."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of multiplex networks, what is the primary factor that determines the emergence of super-diffusion, and how does it relate to the structural and dynamical features of the multiplex?\n\n**A)** The presence of overlap between layers, which influences the maximum relative intensity of super-diffusion.\n**B)** The uniform allocation of resources to enhance diffusion within layers, which is preferable for promoting super-diffusion.\n**C)** The strength of inter-layer couplings, which determines the time scale of the multiplex to reach equilibrium.\n**D)** The structural and dynamical features of the multiplex, including poorly diffusive layers and heterogeneous flows, which are linked to the spectrum of the supra-Laplacian associated with the system.\n\n**Correct Answer:** D) The structural and dynamical features of the multiplex, including poorly diffusive layers and heterogeneous flows, which are linked to the spectrum of the supra-Laplacian associated with the system.\n\n**Explanation:** The correct answer is D) because the documentation states that \"super-diffusion emerges the earliest in systems with poorly diffusive layers, and that its onset is independent from the presence of overlap, which only influences the maximum relative intensity of the phenomenon.\" Additionally, the correct answer is linked to the spectrum of the supra-Laplacian associated with the system, which is mentioned in the introduction as a relevant concept in understanding multiplex diffusion. The other options are incorrect because they either focus on a specific aspect of super-diffusion (A and B) or are unrelated to the emergence of super-diffusion (C)."}, "7": {"documentation": {"title": "An ultra-deep ISOCAM observation through a cluster-lens", "source": "B. Altieri, L. Metcalfe, J.P. Kneib, B. McBreen, H. Aussel, A.\n  Biviano, M. Delaney, D. Elbaz, K. Leech, L. L\\'emonon, K. Okumura, R.\n  Pell\\'o, B. Schulz", "docs_id": "astro-ph/9810480", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An ultra-deep ISOCAM observation through a cluster-lens. We present results of ultra-deep ISOCAM observations through a cluster-lens at 7 and 15 micron with the Infrared Space Observatory (ISO) satellite. These observations reveal a large number of luminous Mid-Infrared (MIR) sources. Cross-identification in the optical and Near-Infrared (NIR) wavebands shows that about half of the 7 micron sources are cluster galaxies. The other 7 micron and almost all 15 micron sources are identified as lensed distant galaxies. Thanks to the gravitational amplification they constitute the faintest MIR detected sources, allowing us to extend the number counts in both the 7 and 15 micron bands. In particular, we find that the 15 micron counts have a steep slope alpha_15 = -1.5 +/- 0.3 and are large, with N_15 (>30 microJy}) = 13 +/- 5 per square arcmin. These numbers rule out non-evolutionary models and favour very strong evolution. Down to our counts limit, we found that the resolved 7 and 15 microns background radiation intensity is respectively (2 +/-0.5) 10^(-9) and (5 +/-1) 10^(-9) W m^(-2) sr^(-1)."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the implications of the observed MIR source counts in the 7 and 15 micron bands on the evolution of the universe. What can be inferred about the nature of the sources and the underlying cosmological model?\n\n**A)** The observed steep slope of the 15 micron counts suggests that the universe is experiencing a period of rapid star formation, leading to a strong evolutionary signal. However, the lack of evolution in the 7 micron counts implies that the universe is also experiencing a period of quiescence, which may be indicative of a non-evolutionary model.\n\n**B)** The large number of faint MIR sources detected in the 15 micron band, combined with the steep slope of the counts, suggests that the universe is experiencing a strong evolutionary signal, with a significant increase in the number of luminous MIR sources over time. This is consistent with a model of strong evolution, where the universe is experiencing a rapid increase in star formation and galaxy evolution.\n\n**C)** The observed MIR source counts in the 7 and 15 micron bands can be explained by the presence of a large population of faint, distant galaxies that are being lensed by the cluster, resulting in an amplification of their MIR emission. This would lead to a steep slope in the counts, but would not necessarily imply a strong evolutionary signal.\n\n**D)** The observed counts in the 7 and 15 micron bands are consistent with a non-evolutionary model, where the number of luminous MIR sources remains constant over time. The steep slope of the 15 micron counts may be due to the presence of a large population of faint, distant galaxies that are being lensed by the cluster, rather than a strong evolutionary signal.\n\n**Correct Answer:** B) The large number of faint MIR sources detected in the 15 micron band, combined with the steep slope of the counts, suggests that the universe is experiencing a strong evolutionary signal, with a significant increase in the number of luminous MIR sources over time. This is consistent with a model of strong evolution, where the universe is experiencing a rapid increase in star formation and galaxy evolution.\n\n**Explanation:** The correct answer is B) because the observed MIR source counts in the 7 and 15 micron bands show a steep slope, indicating a strong evolutionary signal. The large number of faint MIR sources detected in the 15 micron band, combined with this steep slope, suggests that the universe is experiencing a rapid increase in star formation and galaxy evolution. This is consistent with a model of strong evolution, where the universe is experiencing a rapid increase in the number of luminous MIR sources over time. The other options do not provide a consistent explanation for the observed counts, and therefore are incorrect."}, "8": {"documentation": {"title": "Isotopic equilibrium constants for very low-density and low-temperature\n  nuclear matter", "source": "J. B. Natowitz, H. Pais, G. Roepke, J. Gauthier, K. Hagel, M. Barbui,\n  R. Wada", "docs_id": "2009.05200", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isotopic equilibrium constants for very low-density and low-temperature\n  nuclear matter. Yields of equatorially emitted light isotopes, $1\\le Z\\le 14$, observed in ternary fission in the reaction $^{241}$Pu($n_{\\rm th}$,f) are employed to determine apparent chemical equilibrium constants for low-temperature and low-density nuclear matter. The degree of liberation and role of medium modifications are probed through a comparison of experimentally derived reaction quotients with equilibrium constants calculated using a relativistic mean-field model employing a universal medium modification correction for the attractive $\\sigma$ meson coupling. The results of these comparisons indicate that equilibrium is achieved for the lighter ternary fission isotopes. For the heavier isotopes experimental reaction quotients are well below calculated equilibrium constants. This is attributed to a dynamical limitation reflecting insufficient time for full equilibrium to develop. The role of medium effects leading to yield reductions is discussed as is the apparent enhancement of yields for $^8$He and other very neutron rich exotic nuclei."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred from the comparison of experimentally derived reaction quotients with equilibrium constants calculated using a relativistic mean-field model for the ternary fission reaction $^{241}$Pu($n_{\\rm th}$,f) in low-temperature and low-density nuclear matter?\n\nA) The reaction is highly sensitive to medium modifications, leading to significant yield reductions.\nB) The heavier ternary fission isotopes are more likely to achieve equilibrium than the lighter ones.\nC) The dynamical limitation of the reaction prevents full equilibrium from developing, resulting in lower reaction quotients for heavier isotopes.\nD) The relativistic mean-field model overestimates the medium modifications, leading to an overestimation of the equilibrium constants.\n\nCorrect Answer: C) The dynamical limitation of the reaction prevents full equilibrium from developing, resulting in lower reaction quotients for heavier isotopes.\n\nExplanation: The correct answer is based on the statement in the documentation that \"experimental reaction quotients are well below calculated equilibrium constants for the heavier isotopes\" and that this is attributed to \"a dynamical limitation reflecting insufficient time for full equilibrium to develop\". This implies that the reaction is limited by the time available for equilibrium to develop, resulting in lower reaction quotients for heavier isotopes."}, "9": {"documentation": {"title": "Cooperative Cargo Transport by Several Molecular Motors", "source": "Stefan Klumpp and Reinhard Lipowsky", "docs_id": "q-bio/0512011", "section": ["q-bio.SC", "cond-mat.stat-mech", "physics.bio-ph", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cooperative Cargo Transport by Several Molecular Motors. The transport of cargo particles which are pulled by several molecular motors in a cooperative manner is studied theoretically. The transport properties depend primarily on the maximal number, $N$, of motor molecules that may pull simultaneously on the cargo particle. Since each motor must unbind from the filament after a finite number of steps but can also rebind to it again, the actual number of pulling motors is not constant but varies with time between zero and $N$. An increase in the maximal number $N$ leads to a strong increase of the average walking distance (or run length) of the cargo particle. If the cargo is pulled by up to $N$ kinesin motors, e.g., the walking distance is estimated to be $5^{N-1}/N$ micrometers which implies that seven or eight kinesin molecules are sufficient to attain an average walking distance in the centimeter range. If the cargo particle is pulled against an external load force, this force is shared between the motors which provides a nontrivial motor-motor coupling and a generic mechanism for nonlinear force-velocity relationships. With increasing load force, the probability distribution of the instantenous velocity is shifted towards smaller values, becomes broader, and develops several peaks. Our theory is consistent with available experimental data and makes quantitative predictions that are accessible to systematic in vitro experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The transport of cargo particles by several molecular motors in a cooperative manner is studied theoretically. According to the given documentation, what is the estimated average walking distance of the cargo particle when pulled by up to N kinesin motors?\n\n**A)** $5^{N}/N$ micrometers\n**B)** $5^{N-1}/N$ micrometers\n**C)** $5^{N}/N^2$ micrometers\n**D)** $5^{N-1}/N^2$ micrometers\n\n**Correct Answer:** B) $5^{N-1}/N$ micrometers\n\n**Explanation:** The documentation states that the walking distance is estimated to be $5^{N-1}/N$ micrometers when the cargo is pulled by up to N kinesin motors. This is a key result of the theoretical study, and option B is the correct answer. Option A is incorrect because it has an extra factor of N in the denominator. Option C and D are also incorrect because they have an extra factor of N^2 in the denominator, which is not present in the original equation."}, "10": {"documentation": {"title": "Photophoresis in a Dilute, Optically Thick Medium and Dust Motion in\n  Protoplanetary Disks", "source": "Colin P. McNally and Alexander Hubbard", "docs_id": "1510.03427", "section": ["astro-ph.EP", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photophoresis in a Dilute, Optically Thick Medium and Dust Motion in\n  Protoplanetary Disks. We derive expressions for the photophoretic force on opaque spherical particles in a dilute gas in the optically thick regime where the radiation field is in local thermal equilibrium. Under those conditions, the radiation field has a simple form, leading to well defined analytical approximations for the photophoretic force that also consider both the internal thermal conduction within the particle, and the effects of heat conduction and radiation to the surrounding gas. We derive these results for homogeneous spherical particles; and for the double layered spheres appropriate for modeling solid grains with porous aggregate mantles. Then, as a specific astrophysical application of these general physical results, we explore the parameter space relevant to the photophoresis driven drift of dust in protoplanetary disks. We show that highly porous silicate grains have sufficiently low thermal conductivities that photophoretic effects, such as significant relative velocities between particles with differing porosity or levitation above the midplane, are expected to occur."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary assumption made about the radiation field in the optically thick regime, and how does it lead to well-defined analytical approximations for the photophoretic force?\n\nA) The radiation field is in a state of thermal equilibrium, but its exact form is unknown.\nB) The radiation field is in local thermal equilibrium, and its form is simple enough to be approximated analytically.\nC) The radiation field is in a state of thermal equilibrium, but its effects on the particle are negligible.\nD) The radiation field is in a state of thermal equilibrium, but its form is too complex to be approximated analytically.\n\nCorrect Answer: B) The radiation field is in local thermal equilibrium, and its form is simple enough to be approximated analytically.\n\nExplanation: The correct answer is B) because the documentation states that \"Under those conditions, the radiation field has a simple form, leading to well-defined analytical approximations for the photophoretic force...\" This indicates that the primary assumption made about the radiation field is that it is in local thermal equilibrium, and its simple form allows for analytical approximations to be made."}, "11": {"documentation": {"title": "No-arbitrage concepts in topological vector lattices", "source": "Eckhard Platen and Stefan Tappe", "docs_id": "2005.04923", "section": ["math.FA", "math.PR", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "No-arbitrage concepts in topological vector lattices. We provide a general framework for no-arbitrage concepts in topological vector lattices, which covers many of the well-known no-arbitrage concepts as particular cases. The main structural condition we impose is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone. As one consequence of our approach, the concepts NUPBR, NAA$_1$ and NA$_1$ may fail to be equivalent in our general setting. Furthermore, we derive abstract versions of the fundamental theorem of asset pricing (FTAP), including an abstract FTAP on Banach function spaces, and investigate when the FTAP is warranted in its classical form with a separating measure. We also consider a financial market with semimartingales which does not need to have a num\\'{e}raire, and derive results which show the links between the no-arbitrage concepts by only using the theory of topological vector lattices and well-known results from stochastic analysis in a sequence of short proofs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of no-arbitrage concepts in topological vector lattices, what is the main structural condition imposed in the general framework, and how does it relate to the equivalence of no-arbitrage concepts such as NUPBR, NAA$_1$, and NA$_1$?\n\n**A)** The main structural condition is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone, and this condition implies the equivalence of NUPBR, NAA$_1$, and NA$_1$.\n\n**B)** The main structural condition is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone, but this condition does not imply the equivalence of NUPBR, NAA$_1$, and NA$_1$.\n\n**C)** The main structural condition is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone, and this condition implies the non-equivalence of NUPBR, NAA$_1$, and NA$_1$.\n\n**D)** The main structural condition is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone, but this condition is not related to the equivalence of NUPBR, NAA$_1$, and NA$_1$.\n\n**Correct Answer:** B) The main structural condition is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone, but this condition does not imply the equivalence of NUPBR, NAA$_1$, and NA$_1$.\n\n**Explanation:** The correct answer is B) because the documentation states that the main structural condition imposed is that the outcomes of trading strategies with initial wealth zero and those with positive initial wealth have the structure of a convex cone. However, it also notes that this condition may fail to imply the equivalence of NUPBR, NAA$_1$, and NA$_1$ in the general setting. This is in line with the correct answer."}, "12": {"documentation": {"title": "Pattern representation and recognition with accelerated analog\n  neuromorphic systems", "source": "Mihai A. Petrovici, Sebastian Schmitt, Johann Kl\\\"ahn, David\n  St\\\"ockel, Anna Schroeder, Guillaume Bellec, Johannes Bill, Oliver\n  Breitwieser, Ilja Bytschok, Andreas Gr\\\"ubl, Maurice G\\\"uttler, Andreas\n  Hartel, Stephan Hartmann, Dan Husmann, Kai Husmann, Sebastian Jeltsch, Vitali\n  Karasenko, Mitja Kleider, Christoph Koke, Alexander Kononov, Christian Mauch,\n  Eric M\\\"uller, Paul M\\\"uller, Johannes Partzsch, Thomas Pfeil, Stefan\n  Schiefer, Stefan Scholze, Anand Subramoney, Vasilis Thanasoulis, Bernhard\n  Vogginger, Robert Legenstein, Wolfgang Maass, Ren\\'e Sch\\\"uffny, Christian\n  Mayr, Johannes Schemmel, Karlheinz Meier", "docs_id": "1703.06043", "section": ["q-bio.NC", "cs.NE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pattern representation and recognition with accelerated analog\n  neuromorphic systems. Despite being originally inspired by the central nervous system, artificial neural networks have diverged from their biological archetypes as they have been remodeled to fit particular tasks. In this paper, we review several possibilites to reverse map these architectures to biologically more realistic spiking networks with the aim of emulating them on fast, low-power neuromorphic hardware. Since many of these devices employ analog components, which cannot be perfectly controlled, finding ways to compensate for the resulting effects represents a key challenge. Here, we discuss three different strategies to address this problem: the addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures and a training method for hardware-emulated networks that functions without perfect knowledge of the system's dynamics and parameters. For all three scenarios, we corroborate our theoretical considerations with experimental results on accelerated analog neuromorphic platforms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the three strategies proposed in the paper \"Pattern representation and recognition with accelerated analog neuromorphic systems\" to address the challenge of compensating for the effects of imperfectly controlled analog components in neuromorphic hardware?\n\n**A)** The addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures, and a training method that relies on perfect knowledge of the system's dynamics and parameters.\n\n**B)** The addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures, and a training method that functions without perfect knowledge of the system's dynamics and parameters.\n\n**C)** The addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures, and a training method that relies on perfect knowledge of the system's parameters but not its dynamics.\n\n**D)** The addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures, and a training method that functions without knowledge of the system's parameters or dynamics.\n\n**Correct Answer:** B) The addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures, and a training method that functions without perfect knowledge of the system's dynamics and parameters.\n\n**Explanation:** The correct answer is B) because the paper discusses three strategies to address the challenge of imperfectly controlled analog components: adding auxiliary network components for stabilizing activity, utilizing inherently robust architectures, and developing a training method that can function without perfect knowledge of the system's dynamics and parameters. The other options are incorrect because they either omit one or more of the strategies or incorrectly describe the relationship between the strategies and the system's knowledge."}, "13": {"documentation": {"title": "The application of multivariate classification in evaluating the\n  regional differentiation by population income in Russia", "source": "Natalia A. Sadovnikova, Olga A. Zolotareva", "docs_id": "2010.07403", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The application of multivariate classification in evaluating the\n  regional differentiation by population income in Russia. The article presents the results of multivariate classification of Russian regions by the indicators characterizing the population income and their concentration. The clusterization was performed upon an author approach to selecting the characteristics which determines the academic novelty in the evaluation of regional differentiation by population income and the interconnected characteristics. The performed analysis was aimed at the evaluation of the real scale of disproportions in spatial development of the country territories by the considered characteristics. The clusterization results allowed to formulate the condition of a relatively \"strong\" position of a group of high-income regions (the changes in the array of regions constituting it is highly unlikely in the foreseeable future). Additionally there has been revealed a group of Russian regions that the population is struggling to live on quite low income. These so-called \"poor\" regions, within the crisis conditions caused by Covid-19 are in need of additional public support, without which their population will impoverish."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The article presents the results of multivariate classification of Russian regions by population income and concentration. According to the author's approach, which of the following characteristics were selected to determine the academic novelty in evaluating regional differentiation by population income?\n\nA) Only GDP per capita\nB) Only poverty rates\nC) A combination of GDP per capita, poverty rates, and other socio-economic indicators\nD) Only regional autonomy and political stability\n\n**Correct Answer:** C) A combination of GDP per capita, poverty rates, and other socio-economic indicators\n\n**Explanation:** The correct answer is C) A combination of GDP per capita, poverty rates, and other socio-economic indicators. The article states that the author's approach to selecting characteristics was academic novelty in evaluating regional differentiation by population income, implying that a combination of multiple indicators was used. The other options are incorrect because they only mention a single characteristic, which is not supported by the text."}, "14": {"documentation": {"title": "Learning the Morphology of Brain Signals Using Alpha-Stable\n  Convolutional Sparse Coding", "source": "Mainak Jas and Tom Dupr\\'e La Tour and Umut \\c{S}im\\c{s}ekli and\n  Alexandre Gramfort", "docs_id": "1705.08006", "section": ["stat.ML", "q-bio.NC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning the Morphology of Brain Signals Using Alpha-Stable\n  Convolutional Sparse Coding. Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such 'shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call $\\alpha$CSC, lies a family of heavy-tailed distributions called $\\alpha$-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, $\\alpha$CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed $\\alpha$CSC model in learning the morphology of brain signals, and how does it address the limitations of existing algorithms?\n\n**A)** The primary goal is to extract shift-invariant atoms from raw neural signals, and it addresses the limitations of existing algorithms by using a family of heavy-tailed distributions called $\\alpha$-stable distributions to improve robustness to artifacts.\n\n**B)** The primary goal is to reduce the dimensionality of neural time-series data, and it addresses the limitations of existing algorithms by using a computationally efficient Monte Carlo expectation-maximization algorithm for inference.\n\n**C)** The primary goal is to classify neural signals into different categories, and it addresses the limitations of existing algorithms by using a weighted CSC problem optimization algorithm to improve convergence speeds.\n\n**D)** The primary goal is to detect anomalies in neural signals, and it addresses the limitations of existing algorithms by using a novel, computationally efficient algorithm to extract spike bursts and oscillations.\n\n**Correct Answer:** A) The primary goal is to extract shift-invariant atoms from raw neural signals, and it addresses the limitations of existing algorithms by using a family of heavy-tailed distributions called $\\alpha$-stable distributions to improve robustness to artifacts.\n\n**Explanation:** The correct answer is A) because the primary goal of the proposed $\\alpha$CSC model is to extract shift-invariant atoms from raw neural signals, which is a key aspect of analyzing neural time-series data. The model addresses the limitations of existing algorithms by using a family of heavy-tailed distributions called $\\alpha$-stable distributions, which improves robustness to artifacts. The other options are incorrect because they do not accurately describe the primary goal or the addressing of limitations of the proposed model."}, "15": {"documentation": {"title": "Statistical Inference on Tree Swallow Migrations with Random Forests", "source": "Tim Coleman, Lucas Mentch, Daniel Fink, Frank La Sorte, Giles Hooker,\n  Wesley Hochachka, David Winkler", "docs_id": "1710.09793", "section": ["q-bio.PE", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Inference on Tree Swallow Migrations with Random Forests. Bird species' migratory patterns have typically been studied through individual observations and historical records. In recent years however, the eBird citizen science project, which solicits observations from thousands of bird watchers around the world, has opened the door for a data-driven approach to understanding the large-scale geographical movements. Here, we focus on the North American Tree Swallow (\\textit{Tachycineta bicolor}) occurrence patterns throughout the eastern United States. Migratory departure dates for this species are widely believed by both ornithologists and casual observers to vary substantially across years, but the reasons for this are largely unknown. In this work, we present evidence that maximum daily temperature is a major factor influencing Tree Swallow occurrence. Because it is generally understood that species occurrence is a function of many complex, high-order interactions between ecological covariates, we utilize the flexible modeling approach offered by random forests. Making use of recent asymptotic results, we provide formal hypothesis tests for predictive significance various covariates and also develop and implement a permutation-based approach for formally assessing interannual variations by treating the prediction surfaces generated by random forests as functional data. Each of these tests suggest that maximum daily temperature has a significant effect on migration patterns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Statistical Inference on Tree Swallow Migrations with Random Forests\" aim to understand the factors influencing the migratory patterns of the North American Tree Swallow. According to the study, what is the primary factor influencing the occurrence of Tree Swallows, as suggested by the authors' analysis?\n\n**A)** The amount of rainfall in the eastern United States\n**B)** The maximum daily temperature in the eastern United States\n**C)** The latitude of the breeding grounds of the Tree Swallow\n**D)** The number of bird watchers participating in the eBird citizen science project\n\n**Correct Answer:** B) The maximum daily temperature in the eastern United States\n\n**Explanation:** The study suggests that maximum daily temperature is a major factor influencing Tree Swallow occurrence patterns. This is supported by the authors' use of random forests, a flexible modeling approach that can handle complex interactions between ecological covariates. The study also provides formal hypothesis tests for predictive significance of various covariates, including maximum daily temperature, which indicates its importance in shaping Tree Swallow migration patterns.\n\n**Additional explanation:** The other options are incorrect because:\n\n* A) Rainfall is not mentioned as a factor influencing Tree Swallow occurrence in the study.\n* C) Latitude is not mentioned as a factor influencing Tree Swallow occurrence in the study.\n* D) The number of bird watchers participating in the eBird citizen science project is not mentioned as a factor influencing Tree Swallow occurrence in the study."}, "16": {"documentation": {"title": "CELLS: Cost-Effective Evolution in Latent Space for Goal-Directed\n  Molecular Generation", "source": "Zhiyuan Chen, Xiaomin Fang, Fan Wang, Xiaotian Fan, Hua Wu, Haifeng\n  Wang", "docs_id": "2112.00905", "section": ["cs.LG", "cs.AI", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CELLS: Cost-Effective Evolution in Latent Space for Goal-Directed\n  Molecular Generation. Efficiently discovering molecules that meet various property requirements can significantly benefit the drug discovery industry. Since it is infeasible to search over the entire chemical space, recent works adopt generative models for goal-directed molecular generation. They tend to utilize the iterative processes, optimizing the parameters of the molecular generative models at each iteration to produce promising molecules for further validation. Assessments are exploited to evaluate the generated molecules at each iteration, providing direction for model optimization. However, most previous works require a massive number of expensive and time-consuming assessments, e.g., wet experiments and molecular dynamic simulations, leading to the lack of practicability. To reduce the assessments in the iterative process, we propose a cost-effective evolution strategy in latent space, which optimizes the molecular latent representation vectors instead. We adopt a pre-trained molecular generative model to map the latent and observation spaces, taking advantage of the large-scale unlabeled molecules to learn chemical knowledge. To further reduce the number of expensive assessments, we introduce a pre-screener as the proxy to the assessments. We conduct extensive experiments on multiple optimization tasks comparing the proposed framework to several advanced techniques, showing that the proposed framework achieves better performance with fewer assessments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed cost-effective evolution strategy in latent space, CELLS, in the context of goal-directed molecular generation?\n\n**A)** To optimize the parameters of the molecular generative models at each iteration to produce promising molecules for further validation.\n**B)** To reduce the number of expensive and time-consuming assessments required for evaluating generated molecules.\n**C)** To introduce a pre-screener as a proxy to the assessments to further reduce the number of assessments.\n**D)** To adopt a pre-trained molecular generative model to map the latent and observation spaces, taking advantage of large-scale unlabeled molecules to learn chemical knowledge.\n\n**Correct Answer:** D) To adopt a pre-trained molecular generative model to map the latent and observation spaces, taking advantage of large-scale unlabeled molecules to learn chemical knowledge.\n\n**Explanation:** The correct answer is D) because the CELLS framework aims to reduce the number of expensive assessments by leveraging a pre-trained molecular generative model to map the latent and observation spaces. This allows the framework to learn chemical knowledge from large-scale unlabeled molecules, reducing the need for costly assessments. The other options are incorrect because they either describe a secondary aspect of the framework (A and C) or a different goal altogether (B)."}, "17": {"documentation": {"title": "The influence of metallicity on stellar differential rotation and\n  magnetic activity", "source": "Christoffer Karoff, Travis S. Metcalfe, Angela R. G. Santos, Benjamin\n  T. Montet, Howard Isaacson, Veronika Witzke, Alexander I. Shapiro, Savita\n  Mathur, Guy R. Davies, Mikkel N. Lund, Rafael A. Garcia, Allan S. Brun, David\n  Salabert, Pedro P. Avelino, Jennifer van Saders, Ricky Egeland, Margarida S.\n  Cunha, Tiago L. Campante, William J. Chaplin, Natalie Krivova, Sami K.\n  Solanki, Maximilian Stritzinger and Mads F. Knudsen", "docs_id": "1711.07716", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The influence of metallicity on stellar differential rotation and\n  magnetic activity. Observations of Sun-like stars over the last half-century have improved our understanding of how magnetic dynamos, like that responsible for the 11-year solar cycle, change with rotation, mass and age. Here we show for the first time how metallicity can affect a stellar dynamo. Using the most complete set of observations of a stellar cycle ever obtained for a Sun-like star, we show how the solar analog HD 173701 exhibits solar-like differential rotation and a 7.4-year activity cycle. While the duration of the cycle is comparable to that generated by the solar dynamo, the amplitude of the brightness variability is substantially stronger. The only significant difference between HD 173701 and the Sun is its metallicity, which is twice the solar value. Therefore, this provides a unique opportunity to study the effect of the higher metallicity on the dynamo acting in this star and to obtain a comprehensive understanding of the physical mechanisms responsible for the observed photometric variability. The observations can be explained by the higher metallicity of the star, which is predicted to foster a deeper outer convection zone and a higher facular contrast, resulting in stronger variability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that explains the stronger photometric variability observed in the Sun-like star HD 173701 compared to the Sun, according to the study?\n\nA) The star's higher rotation rate\nB) The star's lower metallicity\nC) The star's higher metallicity, which fosters a deeper outer convection zone and a higher facular contrast\nD) The star's older age\n\nCorrect Answer: C) The star's higher metallicity, which fosters a deeper outer convection zone and a higher facular contrast\n\nExplanation: The study suggests that the higher metallicity of HD 173701 is the primary factor responsible for the stronger photometric variability observed in the star. This is because higher metallicity is predicted to lead to a deeper outer convection zone and a higher facular contrast, resulting in stronger variability. The other options are incorrect because the study does not suggest that the star's rotation rate, metallicity, or age are the primary factors contributing to the observed variability."}, "18": {"documentation": {"title": "Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial\n  Networks", "source": "Can Pu, Runzi Song, Radim Tylecek, Nanbo Li, Robert B Fisher", "docs_id": "1803.06657", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial\n  Networks. Refining raw disparity maps from different algorithms to exploit their complementary advantages is still challenging. Uncertainty estimation and complex disparity relationships among pixels limit the accuracy and robustness of existing methods and there is no standard method for fusion of different kinds of depth data. In this paper, we introduce a new method to fuse disparity maps from different sources, while incorporating supplementary information (intensity, gradient, etc.) into a refiner network to better refine raw disparity inputs. A discriminator network classifies disparities at different receptive fields and scales. Assuming a Markov Random Field for the refined disparity map produces better estimates of the true disparity distribution. Both fully supervised and semi-supervised versions of the algorithm are proposed. The approach includes a more robust loss function to inpaint invalid disparity values and requires much less labeled data to train in the semi-supervised learning mode. The algorithm can be generalized to fuse depths from different kinds of depth sources. Experiments explored different fusion opportunities: stereo-monocular fusion, stereo-ToF fusion and stereo-stereo fusion. The experiments show the superiority of the proposed algorithm compared with the most recent algorithms on public synthetic datasets (Scene Flow, SYNTH3, our synthetic garden dataset) and real datasets (Kitti2015 dataset and Trimbot2020 Garden dataset)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the proposed Sdf-GAN algorithm in the paper \"Semi-supervised Depth Fusion with Multi-scale Adversarial Networks\"?\n\nA) To improve the accuracy of disparity maps by fusing data from different sources\nB) To reduce the amount of labeled data required for training disparity refinement networks\nC) To estimate the true disparity distribution by modeling complex relationships among pixels\nD) To classify disparities at different receptive fields and scales using a discriminator network\n\nCorrect Answer: A) To improve the accuracy of disparity maps by fusing data from different sources\n\nExplanation: The paper introduces a new method to fuse disparity maps from different sources, incorporating supplementary information into a refiner network to refine raw disparity inputs. The primary goal of the algorithm is to improve the accuracy of disparity maps by combining the strengths of different disparity sources. While options B and D are related to the algorithm's architecture and functionality, they are not the primary goal. Option C is a consequence of the algorithm's approach, but it is not the main objective."}, "19": {"documentation": {"title": "Multiple scattering effects on inclusive particle production in the\n  large-x regime", "source": "Zhong-Bo Kang, Ivan Vitev, Hongxi Xing", "docs_id": "1307.3557", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiple scattering effects on inclusive particle production in the\n  large-x regime. We study the multiple scattering effects on inclusive particle production in p+A and $\\gamma$+A collisions. Specifically, we concentrate on the region where the parton momentum fraction in the nucleus $x\\sim {\\cal O}(1)$ and incoherent multiple interactions are relevant. By taking into account both initial-state and final-state double scattering, we derive the nuclear size-enhanced power corrections to the differential cross section for single inclusive hadron production in p+A and $\\gamma$+A reactions, and for prompt photon production in p+A reactions. We find that the final result can be written in a simple compact form in terms of four-parton correlation functions, in which the second-derivative, first-derivative and non-derivative terms of the correlation distributions share the same hard-scattering functions. We expect our result to be especially relevant for understanding the nuclear modification of particle production in the backward rapidity regions in p+A and e+A collisions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of multiple scattering on inclusive particle production in the large-x regime, as studied in p+A and \u03b3+A collisions?\n\nA) The nuclear size-enhanced power corrections to the differential cross section for single inclusive hadron production are solely due to initial-state double scattering.\nB) The final result can be written in a simple compact form in terms of four-parton correlation functions, where the second-derivative, first-derivative, and non-derivative terms of the correlation distributions share the same hard-scattering functions.\nC) Incoherent multiple interactions are the primary mechanism responsible for the nuclear modification of particle production in the backward rapidity regions in p+A and e+A collisions.\nD) The study focuses on the region where the parton momentum fraction in the nucleus x is much smaller than 1, and neglects the effects of multiple scattering.\n\nCorrect Answer: B) The final result can be written in a simple compact form in terms of four-parton correlation functions, where the second-derivative, first-derivative, and non-derivative terms of the correlation distributions share the same hard-scattering functions.\n\nExplanation: The correct answer is B, as the study derives the nuclear size-enhanced power corrections to the differential cross section for single inclusive hadron production in p+A and \u03b3+A reactions, and finds that the final result can be written in a simple compact form in terms of four-parton correlation functions, where the second-derivative, first-derivative, and non-derivative terms of the correlation distributions share the same hard-scattering functions. This is a key finding of the study, and is relevant to understanding the nuclear modification of particle production in the backward rapidity regions in p+A and e+A collisions."}, "20": {"documentation": {"title": "Lattice constants and expansivities of gas hydrates from 10K up to the\n  stability limit", "source": "Thomas C. Hansen, Andrzej Falenty, M. Mangir Murshed, Werner F. Kuhs", "docs_id": "1510.06901", "section": ["cond-mat.mtrl-sci", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lattice constants and expansivities of gas hydrates from 10K up to the\n  stability limit. In a combination of neutron and synchrotron diffraction the lattice constants and expansivities of hydrogenated and deuterated CH4-, CO2-, Xe- (structure type I) and N2-hydrate (structure type II) from 10 K up to the stability limit under pressure were established. Some important results emerge from our analysis: (1) Despite the larger guest-size of CO2 as compared to methane, CO2- hydrate has the smaller lattice constants at low temperatures which we ascribe to the larger attractive guest-host interaction of the CO2-water system. (2) The expansivity of CO2-hydrate is larger than for CH4-hydrate which leads to larger lattice constants for the former at temperatures above ~ 150 K; this is likely due to the higher motional degrees of freedom of the CO2 guest molecules. (3) The cage filling does not affect significantly the lattice constants in CH4- and CO2-hydrate in contrast to Xe-hydrate for which the effect is quantitatively established. (4) Similar to ice Ih, the deuterated compounds have slightly larger lattice constants for all investigated systems which can be ascribed to the somewhat weaker H-bonding; the isotopic difference is smallest for the Xesystem,in which the large Xe atoms lead to an increase of averaged H-bond distances. (5) Compared to ice Ih the high temperature expansivities are about 50% larger; in contrast to ice Ih, there is no negative thermal expansion at low temperature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the lattice constants and expansivities of hydrogenated and deuterated CH4-, CO2-, Xe-, and N2-hydrate systems at low temperatures, and explain the differences in their behavior.\n\n**A)** The lattice constants of CO2-hydrate are smaller than those of CH4-hydrate at low temperatures due to the stronger attractive guest-host interaction between CO2 and water molecules.\n\n**B)** The expansivity of Xe-hydrate is larger than that of CH4-hydrate due to the higher motional degrees of freedom of the Xe guest molecules.\n\n**C)** The cage filling does not affect the lattice constants of CH4- and CO2-hydrate, but has a significant effect on the lattice constants of Xe-hydrate.\n\n**D)** The deuterated compounds have slightly larger lattice constants than the hydrogenated compounds due to the weaker H-bonding between water molecules.\n\n**Correct Answer:** C) The cage filling does not affect the lattice constants of CH4- and CO2-hydrate, but has a significant effect on the lattice constants of Xe-hydrate.\n\n**Explanation:** According to the text, the cage filling does not affect the lattice constants of CH4- and CO2-hydrate, but has a significant effect on the lattice constants of Xe-hydrate. This is stated in point (3) of the documentation: \"The cage filling does not affect significantly the lattice constants in CH4- and CO2-hydrate in contrast to Xe-hydrate for which the effect is quantitatively established.\""}, "21": {"documentation": {"title": "Sampling and Estimation for (Sparse) Exchangeable Graphs", "source": "Victor Veitch and Daniel M. Roy", "docs_id": "1611.00843", "section": ["math.ST", "cs.SI", "math.CO", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sampling and Estimation for (Sparse) Exchangeable Graphs. Sparse exchangeable graphs on $\\mathbb{R}_+$, and the associated graphex framework for sparse graphs, generalize exchangeable graphs on $\\mathbb{N}$, and the associated graphon framework for dense graphs. We develop the graphex framework as a tool for statistical network analysis by identifying the sampling scheme that is naturally associated with the models of the framework, and by introducing a general consistent estimator for the parameter (the graphex) underlying these models. The sampling scheme is a modification of independent vertex sampling that throws away vertices that are isolated in the sampled subgraph. The estimator is a dilation of the empirical graphon estimator, which is known to be a consistent estimator for dense exchangeable graphs; both can be understood as graph analogues to the empirical distribution in the i.i.d. sequence setting. Our results may be viewed as a generalization of consistent estimation via the empirical graphon from the dense graph regime to also include sparse graphs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the graphex framework, and how does it relate to the graphon framework for dense graphs?\n\nA) To develop a new method for estimating the parameters of sparse exchangeable graphs, and to generalize the graphon framework to include sparse graphs.\nB) To introduce a new sampling scheme for sparse graphs that is similar to independent vertex sampling, and to develop a consistent estimator for the graphex underlying these models.\nC) To generalize the empirical graphon estimator for dense exchangeable graphs to sparse graphs, and to develop a new framework for statistical network analysis.\nD) To modify the graphon framework for dense graphs to include sparse graphs, and to introduce a new method for estimating the parameters of sparse exchangeable graphs.\n\n**Correct Answer:** C) To generalize the empirical graphon estimator for dense exchangeable graphs to sparse graphs, and to develop a new framework for statistical network analysis.\n\n**Explanation:** The graphex framework is a generalization of the graphon framework for dense graphs to sparse graphs. The primary goal of the graphex framework is to develop a new framework for statistical network analysis by identifying the sampling scheme that is naturally associated with the models of the framework, and by introducing a general consistent estimator for the parameter (the graphex) underlying these models. The graphex framework generalizes the empirical graphon estimator for dense exchangeable graphs to sparse graphs, which is a key result of the paper."}, "22": {"documentation": {"title": "On the self-similarity of diffracting gaseous detonations and the\n  critical channel width problem", "source": "Matei Ioan Radulescu, Remy Mevel, Qiang Xiao and Stany Gallier", "docs_id": "2105.04481", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the self-similarity of diffracting gaseous detonations and the\n  critical channel width problem. One strategy for arresting propagating detonation waves in pipes is by imposing a sudden area enlargement, which provides a rapid lateral divergence of the gases in the reaction zone and attenuates the leading shock. For sufficiently small tube diameter, the detonation decays to a deflagration and the shock decays to negligible strengths. This is known as the critical tube diameter problem. In the present study, we provide a closed form model to predict the detonation quenching for 2D channels. Whitham's geometric shock dynamics, coupled with a shock evolution law based on shocks sustained by a constant source obtained by the shock change equations of Radulescu, is shown to capture the lateral shock dynamics response to the failure wave originating at the expansion corner. A criterion for successful detonation transmission to open space is that the lateral strain rate provided by the failure wave not exceed the critical strain rate of steady curved detonations. Using the critical lateral strain rate obtained by He and Clavin, a closed form solution is obtained for the critical channel opening permitting detonation transmission. The predicted critical channel width is found in very good agreement with our recent experiments and simulations of diffracting H$_2$/O$_2$/Ar detonations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which detonation waves in pipes can be arrested, and what is the key criterion for successful detonation transmission to open space?\n\nA) Imposing a sudden area reduction, which provides a rapid lateral convergence of the gases in the reaction zone and amplifies the leading shock.\nB) Imposing a sudden area enlargement, which provides a rapid lateral divergence of the gases in the reaction zone and attenuates the leading shock.\nC) Imposing a constant pressure gradient, which provides a steady lateral strain rate and allows for detonation transmission.\nD) Imposing a constant temperature gradient, which provides a steady lateral strain rate and allows for detonation transmission.\n\nCorrect Answer: B) Imposing a sudden area enlargement, which provides a rapid lateral divergence of the gases in the reaction zone and attenuates the leading shock.\n\nExplanation: The correct answer is B) Imposing a sudden area enlargement, which is the strategy mentioned in the introduction of the study. This approach provides a rapid lateral divergence of the gases in the reaction zone, which attenuates the leading shock and allows for detonation quenching. The other options are incorrect because imposing a sudden area reduction (A) would amplify the leading shock, imposing a constant pressure gradient (C) or temperature gradient (D) would not directly affect the detonation wave's behavior in the reaction zone."}, "23": {"documentation": {"title": "Theoretical Analysis of Double Differential Cross Section of Proton,\n  Deuteron and Triton for $p+^7$Li Reaction at 14 MeV", "source": "Jiaqi Hu, Xiaojun Sun, Jingshang Zhang, Sheng Wang, Sheng Wang, Yinlu\n  Han", "docs_id": "2001.02891", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical Analysis of Double Differential Cross Section of Proton,\n  Deuteron and Triton for $p+^7$Li Reaction at 14 MeV. Based on the statistical theory of light nucleus reactions (STLN), the description of the complicated emission processes of proton and light composite charged particles are further improved through considering the effects of Coulomb barriers both in incident and different outgoing reaction channels. And the analysis of the reaction channels including the sequential and simultaneous emission processes for $p + ^7$Li reaction is performed in detail. So the partial spectra of all of outgoing particles are also obtained for different reaction processes. The calculated double differential cross sections of total outgoing proton, deuteron and triton at $E_p = 14$ MeV agree well with the available experimental data for different outgoing angles. The ENDF-6 formatted data, which includes all of the reaction cross sections, elastic angular distributions, double differential cross sections of nucleon and light composite charged particles for $p + ^7$Li reaction, are also obtained by PUNF code."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary approach used in the theoretical analysis of the double differential cross section of proton, deuteron, and triton for the $p+^7$Li reaction at 14 MeV?\n\nA) The statistical theory of light nucleus reactions (STLN) is used to describe the emission processes, but the effects of Coulomb barriers are not considered.\nB) The statistical theory of light nucleus reactions (STLN) is used to describe the emission processes, and the effects of Coulomb barriers are considered in both the incident and outgoing reaction channels.\nC) The statistical theory of light nucleus reactions (STLN) is used to describe the emission processes, but the effects of Coulomb barriers are only considered in the incident reaction channel.\nD) The statistical theory of light nucleus reactions (STLN) is not used, and the effects of Coulomb barriers are considered using a different approach.\n\nCorrect Answer: B) The statistical theory of light nucleus reactions (STLN) is used to describe the emission processes, and the effects of Coulomb barriers are considered in both the incident and outgoing reaction channels.\n\nExplanation: The question requires the test-taker to understand the main approach used in the theoretical analysis of the double differential cross section of proton, deuteron, and triton for the $p+^7$Li reaction at 14 MeV. The correct answer, B, indicates that the statistical theory of light nucleus reactions (STLN) is used to describe the emission processes, and the effects of Coulomb barriers are considered in both the incident and outgoing reaction channels. This requires the test-taker to analyze the information provided in the documentation and identify the key aspects of the approach used in the analysis."}, "24": {"documentation": {"title": "Agent-based mapping of credit risk for sustainable microfinance", "source": "Joung-Hun Lee, Marko Jusup, Boris Podobnik, Yoh Iwasa", "docs_id": "1504.05737", "section": ["q-fin.RM", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Agent-based mapping of credit risk for sustainable microfinance. Inspired by recent ideas on how the analysis of complex financial risks can benefit from analogies with independent research areas, we propose an unorthodox framework for mapping microfinance credit risk---a major obstacle to the sustainability of lenders outreaching to the poor. Specifically, using the elements of network theory, we constructed an agent-based model that obeys the stylised rules of microfinance industry. We found that in a deteriorating economic environment confounded with adverse selection, a form of latent moral hazard may cause a regime shift from a high to a low loan repayment probability. An after-the-fact recovery, when possible, required the economic environment to improve beyond that which led to the shift in the first place. These findings suggest a small set of measurable quantities for mapping microfinance credit risk and, consequently, for balancing the requirements to reasonably price loans and to operate on a fully self-financed basis. We illustrate how the proposed mapping works using a 10-year monthly data set from one of the best-known microfinance representatives, Grameen Bank in Bangladesh. Finally, we discuss an entirely new perspective for managing microfinance credit risk based on enticing spontaneous cooperation by building social capital."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Agent-based mapping of credit risk for sustainable microfinance\" propose an unorthodox framework for mapping microfinance credit risk using elements of network theory. What is the primary mechanism by which a deteriorating economic environment can lead to a regime shift from a high to a low loan repayment probability, according to the authors?\n\nA) Adverse selection leads to a decrease in loan quality, causing a shift in repayment probability.\nB) Latent moral hazard causes borrowers to default on loans, leading to a shift in repayment probability.\nC) Economic downturns lead to a decrease in social capital, causing borrowers to default on loans.\nD) Interest rate changes cause a shift in the loan repayment probability, independent of economic conditions.\n\n**Correct Answer:** B) Latent moral hazard causes borrowers to default on loans, leading to a shift in repayment probability.\n\n**Explanation:** The authors suggest that in a deteriorating economic environment, adverse selection can lead to a decrease in loan quality, but this is not the primary mechanism by which a regime shift occurs. Instead, they propose that latent moral hazard, which refers to the tendency of borrowers to default on loans when they are not fully informed about the risks, can cause a shift in repayment probability. This shift can occur even if the economic environment is not deteriorating, and can lead to a regime shift from a high to a low loan repayment probability. The authors also note that an after-the-fact recovery, when possible, requires the economic environment to improve beyond that which led to the shift in the first place."}, "25": {"documentation": {"title": "Null infinity and extremal horizons in AdS-CFT", "source": "Andrew Hickling, James Lucietti and Toby Wiseman", "docs_id": "1408.3417", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Null infinity and extremal horizons in AdS-CFT. We consider AdS gravity duals to CFT on background spacetimes with a null infinity. Null infinity on the conformal boundary may extend to an extremal horizon in the bulk. For example it does so for Poincare-AdS, although does not for planar Schwarzschild-AdS. If null infinity does extend into an extremal horizon in the bulk, we show that the bulk near-horizon geometry is determined by the geometry of the boundary null infinity. Hence the `infra-red' geometry of the bulk is fixed by the large scale behaviour of the CFT spacetime. In addition the boundary stress tensor must have a particular decay at null infinity. As an application, we argue that for CFT on asymptotically flat backgrounds, any static bulk dual containing an extremal horizon extending from the boundary null infinity, must have the near-horizon geometry of Poincare-AdS. We also discuss a class of boundary null infinity that cannot extend to a bulk extremal horizon, although we give evidence that they can extend to an analogous null surface in the bulk which possesses an associated scale-invariant `near-geometry'."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a CFT on a Poincare-AdS background with a null infinity that extends to an extremal horizon in the bulk. What is the near-horizon geometry of the bulk, and what is the required decay of the boundary stress tensor at null infinity?\n\nA) The near-horizon geometry is determined by the geometry of the boundary null infinity, and the boundary stress tensor must decay as 1/r^2 at null infinity.\n\nB) The near-horizon geometry is determined by the geometry of the boundary null infinity, and the boundary stress tensor must decay as 1/r^2 at null infinity, but only for the radial direction.\n\nC) The near-horizon geometry is determined by the geometry of the boundary null infinity, and the boundary stress tensor must decay as 1/r^2 at null infinity, but only for the spatial directions.\n\nD) The near-horizon geometry is not determined by the geometry of the boundary null infinity, and the boundary stress tensor must decay as 1/r^2 at null infinity, but only for the temporal direction.\n\nCorrect Answer: A) The near-horizon geometry is determined by the geometry of the boundary null infinity, and the boundary stress tensor must decay as 1/r^2 at null infinity.\n\nExplanation: The correct answer is A) because the documentation states that the bulk near-horizon geometry is determined by the geometry of the boundary null infinity. Additionally, the documentation mentions that the boundary stress tensor must have a particular decay at null infinity, which is consistent with the 1/r^2 decay mentioned in option A. Options B, C, and D are incorrect because they introduce additional constraints on the decay of the boundary stress tensor that are not mentioned in the documentation."}, "26": {"documentation": {"title": "Charged lepton mixing and oscillations from neutrino mixing in the early\n  Universe", "source": "D. Boyanovsky, C. M. Ho", "docs_id": "hep-ph/0510214", "section": ["hep-ph", "astro-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Charged lepton mixing and oscillations from neutrino mixing in the early\n  Universe. Charged lepton mixing as a consequence of neutrino mixing is studied for two generations $e,\\mu$ in the temperature regime $m_\\mu \\ll T \\ll M_W$ in the early Universe. We state the general criteria for charged lepton mixing, critically reexamine aspects of neutrino equilibration and provide arguments to suggest that neutrinos may equilibrate as mass eigenstates in the temperature regime \\emph{prior} to flavor equalization. We assume this to be the case, and that neutrino mass eigenstates are in equilibrium with different chemical potentials. Charged lepton self-energies are obtained to leading order in the electromagnetic and weak interactions. The upper bounds on the neutrino asymmetry parameters from CMB and BBN without oscillations, combined with the fit to the solar and KamLAND data for the neutrino mixing angle, suggest that for the two generation case there is resonant \\emph{charged lepton} mixing in the temperature range $T \\sim 5 \\mathrm{GeV}$. In this range the charged lepton oscillation frequency is of the same order as the electromagnetic damping rate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the early Universe, what is the approximate temperature range in which resonant charged lepton mixing is expected to occur, and what is the significance of this range in the context of neutrino oscillations?\n\n**A)** $T \\sim 10 \\mathrm{GeV}$, as this range corresponds to the neutrino mass eigenstates being in equilibrium with different chemical potentials, leading to a significant impact on the CMB and BBN.\n\n**B)** $T \\sim 5 \\mathrm{GeV}$, as this range is where the charged lepton oscillation frequency is of the same order as the electromagnetic damping rate, allowing for resonant mixing to occur.\n\n**C)** $T \\sim 1 \\mathrm{GeV}$, as this range is where the neutrino mass eigenstates are in equilibrium with different chemical potentials, leading to a significant impact on the solar and KamLAND data.\n\n**D)** $T \\sim 100 \\mathrm{GeV}$, as this range corresponds to the neutrino mass eigenstates being in equilibrium with different chemical potentials, leading to a significant impact on the CMB and BBN.\n\n**Correct Answer:** B) $T \\sim 5 \\mathrm{GeV}$, as this range is where the charged lepton oscillation frequency is of the same order as the electromagnetic damping rate, allowing for resonant mixing to occur.\n\n**Explanation:** The correct answer is B) $T \\sim 5 \\mathrm{GeV}$ because this range is where the charged lepton oscillation frequency is of the same order as the electromagnetic damping rate, allowing for resonant mixing to occur. This is a critical aspect of the early Universe, as it has implications for the CMB and BBN, as well as the solar and KamLAND data. The other options are incorrect because they do not accurately describe the temperature range in which resonant charged lepton mixing is expected to occur."}, "27": {"documentation": {"title": "Towards Bursting Filter Bubble via Contextual Risks and Uncertainties", "source": "Rikiya Takahashi and Shunan Zhang", "docs_id": "1706.09985", "section": ["stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Bursting Filter Bubble via Contextual Risks and Uncertainties. A rising topic in computational journalism is how to enhance the diversity in news served to subscribers to foster exploration behavior in news reading. Despite the success of preference learning in personalized news recommendation, their over-exploitation causes filter bubble that isolates readers from opposing viewpoints and hurts long-term user experiences with lack of serendipity. Since news providers can recommend neither opposite nor diversified opinions if unpopularity of these articles is surely predicted, they can only bet on the articles whose forecasts of click-through rate involve high variability (risks) or high estimation errors (uncertainties). We propose a novel Bayesian model of uncertainty-aware scoring and ranking for news articles. The Bayesian binary classifier models probability of success (defined as a news click) as a Beta-distributed random variable conditional on a vector of the context (user features, article features, and other contextual features). The posterior of the contextual coefficients can be computed efficiently using a low-rank version of Laplace's method via thin Singular Value Decomposition. Efficiencies in personalized targeting of exceptional articles, which are chosen by each subscriber in test period, are evaluated on real-world news datasets. The proposed estimator slightly outperformed existing training and scoring algorithms, in terms of efficiency in identifying successful outliers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of preference learning in personalized news recommendation, and how does the proposed Bayesian model address this issue?\n\n**A)** Preference learning can only recommend articles that are already popular, leading to a lack of serendipity in news reading. The proposed model addresses this by incorporating uncertainty-aware scoring and ranking.\n\n**B)** Preference learning can only recommend articles that are likely to be clicked, but it fails to consider the diversity of opinions. The proposed model addresses this by modeling the probability of success as a Beta-distributed random variable.\n\n**C)** Preference learning can lead to filter bubbles that isolate readers from opposing viewpoints. The proposed model addresses this by using a Bayesian binary classifier that models the probability of success conditional on a vector of context features.\n\n**D)** Preference learning can only recommend articles that are predicted to be popular, but it fails to consider the variability in click-through rates. The proposed model addresses this by incorporating high variability (risks) or high estimation errors (uncertainties) into the scoring and ranking process.\n\n**Correct Answer:** C) Preference learning can lead to filter bubbles that isolate readers from opposing viewpoints. The proposed model addresses this by using a Bayesian binary classifier that models the probability of success conditional on a vector of context features.\n\n**Explanation:** The correct answer is C) because the proposed Bayesian model addresses the limitation of preference learning by incorporating context features into the scoring and ranking process. This allows the model to recommend articles that are likely to be clicked, while also considering the diversity of opinions and the variability in click-through rates. The other options are incorrect because they either oversimplify the issue (A) or focus on a different aspect of the problem (B and D)."}, "28": {"documentation": {"title": "Asymmetric Stokes flow induced by a transverse point-force acting near a\n  finite-sized elastic membrane", "source": "Abdallah Daddi-Moussa-Ider", "docs_id": "2006.14375", "section": ["physics.flu-dyn", "cond-mat.soft", "physics.class-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric Stokes flow induced by a transverse point-force acting near a\n  finite-sized elastic membrane. A deep understanding of the physical interactions between nanoparticles and target cell membranes is important in designing efficient nanocarrier systems for drug delivery applications. Here, we present a theoretical framework to describe the hydrodynamic flow field induced by a point-force singularity (Stokeslet) directed parallel to a finite-sized elastic membrane endowed with shear and bending rigidities. We formulate the elastohydrodynamic problem as a mixed-boundary-value problem, which we then reduce into a well-behaved system of integro-differential equations. It follows that shear and bending linearly decouple so that the solution of the overall flow problem can be obtained by linear superposition of the contributions arising from these modes of deformation. Additionally, we probe the effect of the membrane on the hydrodynamic drag acting on a nearby particle, finding that, in a certain range of parameters, translational motion near an elastic membrane with only energetic resistance toward shear can, surprisingly, be sped up compared to bulk fluid. Our results may find applications in microrheological characterizations of colloidal systems near elastic confinements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary assumption made in the theoretical framework presented in the paper, and how does it impact the formulation of the elastohydrodynamic problem?\n\nA) The primary assumption is that the membrane is infinitely large, and this assumption leads to a decoupling of shear and bending modes. This decoupling allows for the solution of the overall flow problem by linear superposition of the contributions from these modes.\n\nB) The primary assumption is that the membrane is rigid and non-deformable, and this assumption leads to a decoupling of shear and bending modes. This decoupling allows for the solution of the overall flow problem by linear superposition of the contributions from these modes.\n\nC) The primary assumption is that the membrane is elastic and deformable, and this assumption leads to a decoupling of shear and bending modes. This decoupling allows for the solution of the overall flow problem by linear superposition of the contributions from these modes.\n\nD) The primary assumption is that the membrane is a perfect fluid, and this assumption leads to a decoupling of shear and bending modes. This decoupling allows for the solution of the overall flow problem by linear superposition of the contributions from these modes.\n\nCorrect Answer: C) The primary assumption is that the membrane is elastic and deformable, and this assumption leads to a decoupling of shear and bending modes. This decoupling allows for the solution of the overall flow problem by linear superposition of the contributions from these modes.\n\nExplanation: The correct answer is C) because the paper states that \"shear and bending linearly decouple\" in the elastohydrodynamic problem, and this decoupling is a direct result of the membrane being elastic and deformable. The other options are incorrect because they either misstate the assumption or introduce additional assumptions not present in the paper."}, "29": {"documentation": {"title": "Subleading-N_c corrections in non-linear small-x evolution", "source": "Yuri V. Kovchegov, Janne Kuokkanen, Kari Rummukainen and Heribert\n  Weigert", "docs_id": "0812.3238", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Subleading-N_c corrections in non-linear small-x evolution. We explore the subleading-N_c corrections to the large-N_c Balitsky-Kovchegov (BK) evolution equation by comparing its solution to that of the all-N_c Jalilian-Marian-Iancu-McLerran-Weigert-Leonidov-Kovner (JIMWLK) equation. In earlier simulations it was observed that the difference between the solutions of JIMWLK and BK is unusually small for a quark dipole scattering amplitude, of the order of 0.1%, which is two orders of magnitude smaller than the naively expected 1/N_c^2 or 11%. In this paper we argue that this smallness is not accidental. We show that saturation effects and correlator coincidence limits fixed by group theory constraints conspire with the particular structure of the dipole kernel to suppress subleading-N_c corrections reducing the difference between the solutions of JIMWLK and BK to 0.1%. We solve the JIMWLK equation with improved numerical accuracy and verify that the remaining 1/N_c corrections, while small, still manage to slow down the rapidity-dependence of JIMWLK evolution compared to that of BK. We demonstrate that a truncation of JIMWLK evolution in the form of a minimal Gaussian generalization of the BK equation captures some of the remaining 1/N_c contributions leading to an even better agreement with JIMWLK evolution. As the 1/N_c corrections to BK include multi-reggeon exchanges one may conclude that the net effect of multi-reggeon exchanges on the dipole amplitude is rather small."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary argument made in the paper regarding the small difference between the solutions of the Balitsky-Kovchegov (BK) and Jalilian-Marian-Iancu-McLerran-Weigert-Leonidov-Kovner (JIMWLK) equations, and how do the authors address the remaining 1/N_c corrections?\n\nA) The paper argues that the small difference is due to the specific structure of the dipole kernel, which suppresses subleading-N_c corrections, and that the remaining 1/N_c corrections are negligible.\nB) The paper argues that the small difference is due to the saturation effects and correlator coincidence limits fixed by group theory constraints, which conspire to suppress subleading-N_c corrections, and that the remaining 1/N_c corrections are significant.\nC) The paper argues that the small difference is due to the truncation of JIMWLK evolution in the form of a minimal Gaussian generalization of the BK equation, which captures some of the remaining 1/N_c contributions, and that the remaining 1/N_c corrections are small.\nD) The paper argues that the small difference is due to the multi-reggeon exchanges in the 1/N_c corrections to BK, which have a small net effect on the dipole amplitude, and that the remaining 1/N_c corrections are negligible.\n\nCorrect Answer: A) The paper argues that the small difference is due to the specific structure of the dipole kernel, which suppresses subleading-N_c corrections, and that the remaining 1/N_c corrections are negligible.\n\nExplanation: The paper argues that the small difference between the solutions of BK and JIMWLK is not accidental, but rather due to the specific structure of the dipole kernel, which suppresses subleading-N_c corrections. The authors also show that the remaining 1/N_c corrections are negligible, as evidenced by the improved numerical accuracy of the JIMWLK equation and the minimal Gaussian generalization of the BK equation."}, "30": {"documentation": {"title": "Auditory streaming emerges from fast excitation and slow delayed\n  inhibition", "source": "Andrea Ferrario, James Rankin", "docs_id": "2006.14537", "section": ["math.DS", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Auditory streaming emerges from fast excitation and slow delayed\n  inhibition. In the auditory streaming paradigm alternating sequences of pure tones can be perceived as a single galloping rhythm (integration) or as two sequences with separated low and high tones (segregation). Although studied for decades, the neural mechanisms underlining this perceptual grouping of sound remains a mystery. With the aim of identifying a plausible minimal neural circuit that captures this phenomenon, we propose a firing rate model with two periodically forced neural populations coupled by fast direct excitation and slow delayed inhibition. By analyzing the model in a non-smooth, slow-fast regime we analytically prove the existence of a rich repertoire of dynamical states and of their parameter dependent transitions. We impose plausible parameter restrictions and link all states with perceptual interpretations. Regions of stimulus parameters occupied by states linked with each percept matches those found in behavioral experiments. Our model suggests that slow inhibition masks the perception of subsequent tones during segregation (forward masking), while fast excitation enables integration for large pitch differences between the two tones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism underlying the auditory streaming phenomenon, as proposed by the authors of the study, and how does it relate to the perception of galloping rhythms versus segregated tones?\n\nA) The neural populations are coupled by slow delayed inhibition and fast direct excitation, which enables the integration of alternating sequences of pure tones into a single rhythm.\n\nB) The neural populations are decoupled by fast direct inhibition and slow delayed excitation, which masks the perception of subsequent tones during segregation.\n\nC) The neural populations are coupled by fast direct excitation and slow delayed inhibition, which enables the perception of segregated tones with separated low and high pitches.\n\nD) The neural populations are decoupled by fast direct excitation and slow delayed inhibition, which enables the integration of alternating sequences of pure tones into a single rhythm.\n\nCorrect Answer: C) The neural populations are coupled by fast direct excitation and slow delayed inhibition, which enables the perception of segregated tones with separated low and high pitches.\n\nExplanation: The correct answer is C) because the study proposes that the neural populations are coupled by fast direct excitation and slow delayed inhibition, which enables the perception of segregated tones with separated low and high pitches. This is in line with the authors' statement that \"fast excitation enables integration for large pitch differences between the two tones\" and \"slow inhibition masks the perception of subsequent tones during segregation\". The other options are incorrect because they describe a decoupling of the neural populations, which is not supported by the study."}, "31": {"documentation": {"title": "Vorticity moments in four numerical simulations of the 3D Navier-Stokes\n  equations", "source": "D. Donzis, J. D. Gibbon, A. Gupta, R. M. Kerr, R. Pandit and D.\n  Vincenzi", "docs_id": "1302.1768", "section": ["nlin.CD", "math-ph", "math.MP", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vorticity moments in four numerical simulations of the 3D Navier-Stokes\n  equations. The issue of intermittency in numerical solutions of the 3D Navier-Stokes equations on a periodic box $[0,\\,L]^{3}$ is addressed through four sets of numerical simulations that calculate a new set of variables defined by $D_{m}(t) = \\left(\\varpi_{0}^{-1}\\Omega_{m}\\right)^{\\alpha_{m}}$ for $1 \\leq m \\leq \\infty$ where $\\alpha_{m}= \\frac{2m}{4m-3}$ and $\\left[\\Omega_{m}(t)\\right]^{2m} = L^{-3}\\I |\\bom|^{2m}dV$ with $\\varpi_{0} = \\nu L^{-2}$. All four simulations unexpectedly show that the $D_{m}$ are ordered for $m = 1\\,,...,\\,9$ such that $D_{m+1} < D_{m}$. Moreover, the $D_{m}$ squeeze together such that $D_{m+1}/D_{m}\\nearrow 1$ as $m$ increases. The first simulation is of very anisotropic decaying turbulence\\,; the second and third are of decaying isotropic turbulence from random initial conditions and forced isotropic turbulence at constant Grashof number respectively\\,; the fourth is of very high Reynolds number forced, stationary, isotropic turbulence at up to resolutions of $4096^{3}$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on vorticity moments in four numerical simulations of the 3D Navier-Stokes equations observe that the $D_{m}$ variables are ordered in a specific way. What can be inferred about the relationship between the values of $D_{m}$ and $D_{m+1}$ based on the given information?\n\nA){The values of $D_{m}$ and $D_{m+1}$ are equal, as indicated by the fact that $D_{m+1}/D_{m}\\nearrow 1$ as $m$ increases.}\nB){The values of $D_{m}$ and $D_{m+1}$ are inversely related, such that $D_{m+1} < D_{m}$ for all $m$.}\nC){The values of $D_{m}$ and $D_{m+1}$ are related by a power law, such that $D_{m+1} = D_{m}^{\\alpha}$ for some constant $\\alpha$.}\nD){The values of $D_{m}$ and $D_{m+1}$ are independent of each other, and there is no discernible pattern in their relationship.}\n\n**Correct Answer:** C){The values of $D_{m}$ and $D_{m+1}$ are related by a power law, such that $D_{m+1} = D_{m}^{\\alpha}$ for some constant $\\alpha$.}\n\n**Explanation:** The correct answer is C) because the text states that the values of $D_{m+1}/D_{m}$ \"squeez together such that $D_{m+1}/D_{m}\\nearrow 1$ as $m$ increases\", which implies that the relationship between $D_{m}$ and $D_{m+1}$ is a power law of the form $D_{m+1} = D_{m}^{\\alpha}$. The value of $\\alpha$ is not explicitly given, but it can be inferred to be $\\alpha_{m} = \\frac{2m}{4m-3}$, which is the definition of $\\alpha_{m}$ in the text."}, "32": {"documentation": {"title": "Deep learning stochastic processes with QCD phase transition", "source": "Lijia Jiang, Lingxiao Wang, and Kai Zhou", "docs_id": "2103.04090", "section": ["nucl-th", "nucl-ex", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep learning stochastic processes with QCD phase transition. It is non-trivial to recognize phase transitions and track dynamics inside a stochastic process because of its intrinsic stochasticity. In this paper, we employ the deep learning method to classify the phase orders and predict the damping coefficient of fluctuating systems under Langevin's description. As a concrete set-up, we demonstrate this paradigm for the scalar condensation in QCD matter near the critical point, in which the order parameter of chiral phase transition can be characterized in a $1+1$-dimensional Langevin equation for $\\sigma$ field. In a supervised learning manner, the Convolutional Neural Networks(CNNs) accurately classify the first-order phase transition and crossover based on $\\sigma$ field configurations with fluctuations. Noise in the stochastic process does not significantly hinder the performance of the well-trained neural network for phase order recognition. For mixed dynamics with diverse dynamical parameters, we further devise and train the machine to predict the damping coefficients $\\eta$ in a broad range. The results show that it is robust to extract the dynamics from the bumpy field configurations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Deep learning stochastic processes with QCD phase transition\", what is the primary advantage of employing deep learning methods to classify phase orders and predict damping coefficients in fluctuating systems?\n\n**A)** The ability to handle complex, high-dimensional data without significant loss of information\n**B)** The capacity to accurately classify phase transitions and predict damping coefficients even in the presence of significant noise\n**C)** The potential to improve the accuracy of phase order recognition by leveraging the strengths of convolutional neural networks\n**D)** The ability to train machine learning models to predict damping coefficients in a broad range of dynamical parameters\n\n**Correct Answer:** C) The potential to improve the accuracy of phase order recognition by leveraging the strengths of convolutional neural networks\n\n**Explanation:** The correct answer, C, highlights the key benefit of using deep learning methods in this context. The paper demonstrates that convolutional neural networks (CNNs) can accurately classify phase transitions and predict damping coefficients in fluctuating systems, even in the presence of noise. This is a significant advantage, as it enables the extraction of dynamics from bumpy field configurations. The other options, while related to the topic, do not capture the primary advantage of deep learning methods in this specific context."}, "33": {"documentation": {"title": "Supersymmetric Quantum Spherical Spins", "source": "L. G. dos Santos, L. V. T. Tavares, P. F. Bienzobaz, and Pedro R. S.\n  Gomes", "docs_id": "1806.05656", "section": ["cond-mat.stat-mech", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supersymmetric Quantum Spherical Spins. In this work we investigate properties of a supersymmetric extension of the quantum spherical model from an off-shell formulation directly in the superspace. This is convenient to safely handle the constraint structure of the model in a way compatible with supersymmetry. The model is parametrized by an interaction energy, $U_{{\\bf r},{\\bf r}'}$, which governs the interactions between the superfields of different sites. We briefly discuss some consequences when $U_{{\\bf r},{\\bf r}'}$ corresponds to the case of first-neighbor interactions. After computing the partition function via saddle point method for a generic interaction, $U_{{\\bf r},{\\bf r}'}\\equiv U(|{\\bf r}-{\\bf r}'|)$, we focus in the mean-field version, which reveals an interesting critical behavior. In fact, the mean-field supersymmetric model exhibits a quantum phase transition without breaking supersymmetry at zero temperature, as well as a phase transition at finite temperature with broken supersymmetry. We compute critical exponents of the usual magnetization and susceptibility in both cases of zero and finite temperature. Concerning the susceptibility, there are two regimes in the case of finite temperature characterized by distinct critical exponents. The entropy is well behaved at low temperature, vanishing as $T \\rightarrow 0$"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the supersymmetric quantum spherical model, what is the critical behavior of the mean-field model at zero temperature, and how does it differ from the behavior at finite temperature?\n\n**A)** The mean-field model exhibits a phase transition at zero temperature with broken supersymmetry, and at finite temperature with unbroken supersymmetry.\n\n**B)** The mean-field model exhibits a quantum phase transition without breaking supersymmetry at zero temperature, and a phase transition at finite temperature with broken supersymmetry.\n\n**C)** The mean-field model exhibits a phase transition at zero temperature with unbroken supersymmetry, and a quantum phase transition at finite temperature with broken supersymmetry.\n\n**D)** The mean-field model exhibits a quantum phase transition at zero temperature with broken supersymmetry, and a phase transition at finite temperature with unbroken supersymmetry.\n\n**Correct Answer:** B) The mean-field model exhibits a quantum phase transition without breaking supersymmetry at zero temperature, and a phase transition at finite temperature with broken supersymmetry.\n\n**Explanation:** The correct answer is B) because the documentation states that the mean-field supersymmetric model exhibits a quantum phase transition without breaking supersymmetry at zero temperature, and a phase transition at finite temperature with broken supersymmetry. This is a key result of the work, and the correct answer reflects this finding. The other options are incorrect because they either misstate the behavior at zero temperature or at finite temperature, or they incorrectly combine the two behaviors."}, "34": {"documentation": {"title": "On the Nuclear Modification Factor at RHIC and LHC", "source": "Andrey Kormilitzin, Eugene Levin, Amir H. Rezaeian", "docs_id": "1011.1248", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Nuclear Modification Factor at RHIC and LHC. We show that pQCD factorization incorporated with pre-haronization energy-loss effect naturally leads to flatness of the nuclear modification factor R_{AA} for produced hadrons at high transverse momentum p_T. We consider two possible scenarios for the pre-hadronization: In scenario 1, the produced gluon propagates through dense QCD medium and loses energy. In scenario 2, all gluons first decay to quark-antiquark pairs and then each pair loses energy as propagating through the medium. We show that the estimates of the energy-loss in these two different models lead to very close values and is able to explain the suppression of high-p_T hadrons in nucleus-nucleus collisions at RHIC. We show that the onset of the flatness of R_{AA} for the produced hadron in central collisions at midrapidity is about p_T\\approx 15 and 25 GeV at RHIC and the LHC energies, respectively. We show that the smallness (R_{AA}<0.5) and the high-p_T flatness of R_{AA} obtained from the k_T factorization supplemented with the Balitsky-Kovchegov (BK) equation is rather generic and it does not strongly depend on the details of the BK solutions. We show that energy-loss effect reduces the nuclear modification factor obtained from the k_T factorization about 30\\div 50% at moderate p_T."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the approximate p_T value at which the nuclear modification factor R_{AA} for produced hadrons becomes flat in nucleus-nucleus collisions at RHIC and the LHC, according to the pre-hadronization energy-loss effect described in the given Arxiv documentation?\n\nA) p_T \u2248 5 GeV\nB) p_T \u2248 15 GeV\nC) p_T \u2248 25 GeV\nD) p_T \u2248 50 GeV\n\nCorrect Answer: C) p_T \u2248 25 GeV\n\nExplanation: According to the documentation, the onset of the flatness of R_{AA} for the produced hadron in central collisions at midrapidity is about p_T \u2248 25 GeV at the LHC energies. This is a key finding in the study, and it highlights the importance of considering the pre-hadronization energy-loss effect in understanding the nuclear modification factor at high transverse momentum p_T."}, "35": {"documentation": {"title": "Exact soliton solutions and nonlinear modulation instability in spinor\n  Bose-Einstein condensates", "source": "Lu Li, Zaidong Li, Boris A. Malomed, Dumitru Mihalache, and W. M. Liu", "docs_id": "nlin/0603027", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact soliton solutions and nonlinear modulation instability in spinor\n  Bose-Einstein condensates. We find one-, two-, and three-component solitons of the polar and ferromagnetic (FM) types in the general (non-integrable) model of a spinor (three-component) model of the Bose-Einstein condensate (BEC), based on a system of three nonlinearly coupled Gross-Pitaevskii equations. The stability of the solitons is studied by means of direct simulations, and, in a part, analytically, using linearized equations for small perturbations. Global stability of the solitons is considered by means of the energy comparison. As a result, ground-state and metastable soliton states of the FM and polar types are identified. For the special integrable version of the model, we develop the Darboux transformation (DT). As an application of the DT, analytical solutions are obtained that display full nonlinear evolution of the modulational instability (MI) of a continuous-wave (CW) state seeded by a small spatially periodic perturbation. Additionally, by dint of direct simulations, we demonstrate that solitons of both the polar and FM types, found in the integrable system, are structurally stable, i.e., they are robust under random changes of the relevant nonlinear coefficient in time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the spinor Bose-Einstein condensate model, what is the primary method used to study the stability of the solitons, and what is the significance of the energy comparison approach in this context?\n\n**A)** The stability of solitons is studied using direct simulations, and the energy comparison approach is used to identify the ground-state and metastable soliton states.\n\n**B)** The stability of solitons is studied using linearized equations for small perturbations, and the energy comparison approach is used to demonstrate the global stability of the solitons.\n\n**C)** The stability of solitons is studied using direct simulations, and the energy comparison approach is used to analyze the modulational instability of a continuous-wave state.\n\n**D)** The stability of solitons is studied using linearized equations for small perturbations, and the energy comparison approach is used to demonstrate the structural stability of the solitons.\n\n**Correct Answer:** A) The stability of solitons is studied using direct simulations, and the energy comparison approach is used to identify the ground-state and metastable soliton states.\n\n**Explanation:** The correct answer is A) because the documentation states that the stability of solitons is studied using direct simulations, and the energy comparison approach is used to identify the ground-state and metastable soliton states. This is a key finding in the study of solitons in the spinor Bose-Einstein condensate model. The other options are incorrect because they either misattribute the method used to study stability (B and D) or misrepresent the significance of the energy comparison approach (C)."}, "36": {"documentation": {"title": "Theoretical analysis of a possible observation of the chiral magnetic\n  effect in Au + Au collisions within the RHIC beam energy scan program", "source": "V. D. Toneev, V. Voronyuk, E. L. Bratkovskaya, W. Cassing, V. P.\n  Konchakovski and S. A. Voloshin", "docs_id": "1112.2595", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical analysis of a possible observation of the chiral magnetic\n  effect in Au + Au collisions within the RHIC beam energy scan program. In terms of the hadron-string-dynamics (HSD) approach we investigate the correlation function in the azimuthal angle $\\psi$ of charged hadrons that is expected to be sensitive to a signal of local strong parity violation. Our analysis of Au+Au collisions is based on the recent STAR data within the RHIC Beam-Energy-Scan (BES) program. The HSD model reasonably reproduces STAR data for $\\sqrt{s_{NN}}=$7.7 GeV, while there are some deviations from the experiment at the collision energy of 11.5 GeV and an increase of deviations between theory and experiment at $\\sqrt{s_{NN}}=$39 GeV. For reference, the results for $\\sqrt{s_{NN}}=$ 200 GeV are given as well. The role of the retarded electromagnetic field is discussed and a compensation effect for the action of its electric and magnetic components is pointed out. We conclude that the recent RHIC BES data at $\\sqrt{s_{NN}}=$7.7 and 11.5 GeV can be understood on the hadronic level without involving the idea of a strong parity violation; however, at $\\sqrt{s_{NN}}\\sim$40 GeV and above one needs to take into consideration explicit partonic (quark-qluon) degrees-of-freedom for a proper treatment of the dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the role of the retarded electromagnetic field in the HSD model, and how does it relate to the treatment of strong parity violation in Au+Au collisions at different collision energies?\n\nA) The retarded electromagnetic field plays a crucial role in the HSD model, and its compensation effect is essential for understanding the dynamics at $\\sqrt{s_{NN}}=$7.7 and 11.5 GeV. However, at $\\sqrt{s_{NN}}\\sim$40 GeV and above, the explicit partonic degrees-of-freedom become necessary.\n\nB) The retarded electromagnetic field is not a significant factor in the HSD model, and its compensation effect is not relevant to the dynamics at any collision energy.\n\nC) The compensation effect of the retarded electromagnetic field is only relevant at $\\sqrt{s_{NN}}=$7.7 and 11.5 GeV, and it is not necessary to consider explicit partonic degrees-of-freedom at higher energies.\n\nD) The retarded electromagnetic field is not a necessary component of the HSD model, and its compensation effect is not relevant to the treatment of strong parity violation in Au+Au collisions.\n\nCorrect Answer: A) The retarded electromagnetic field plays a crucial role in the HSD model, and its compensation effect is essential for understanding the dynamics at $\\sqrt{s_{NN}}=$7.7 and 11.5 GeV. However, at $\\sqrt{s_{NN}}\\sim$40 GeV and above, the explicit partonic degrees-of-freedom become necessary.\n\nExplanation: The correct answer is A) because the documentation states that the HSD model reasonably reproduces STAR data for $\\sqrt{s_{NN}}=$7.7 GeV, but there are some deviations from the experiment at the collision energy of 11.5 GeV. Additionally, the role of the retarded electromagnetic field is discussed, and a compensation effect for the action of its electric and magnetic components is pointed out. This suggests that the retarded electromagnetic field plays a crucial role in the HSD model, particularly at lower collision energies. However, at higher collision energies ($\\sqrt{s_{NN}}\\sim$40 GeV and above), the explicit partonic degrees-of-freedom become necessary to properly treat the dynamics."}, "37": {"documentation": {"title": "Bonnet's type theorems in the relative differential geometry of the\n  4-dimensional space", "source": "Stylianos Stamatakis and Ioannis Kaffas", "docs_id": "1707.07549", "section": ["math.DG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bonnet's type theorems in the relative differential geometry of the\n  4-dimensional space. We deal with hypersurfaces in the framework of the relative differential geometry in $\\mathbb{R}^4$. We consider a hypersurface $\\varPhi$ in $\\mathbb{R}^4$ with position vector field $\\vect{x}$ which is relatively normalized by a relative normalization $\\vect{y}$. Then $\\vect{y}$ is also a relative normalization of every member of the one-parameter family $\\mathcal{F}$ of hypersurfaces $\\varPhi_\\mu$ with position vector field $\\vect{x}_\\mu = \\vect{x} + \\mu \\, \\vect{y}$, where $\\mu$ is a real constant. We call every hypersurface $\\varPhi_\\mu \\in \\mathcal{F}$ relatively parallel to $\\varPhi$. This consideration includes both Euclidean and Blaschke hypersurfaces of the affine differential geometry. In this paper we express the relative mean curvature's functions of a hypersurface $\\varPhi_\\mu$ relatively parallel to $\\varPhi$ by means of the ones of $\\varPhi$ and the \"relative distance\" $\\mu$. Then we prove several Bonnet's type theorems. More precisely, we show that if two relative mean curvature's functions of $\\varPhi$ are constant, then there exists at least one relatively parallel hypersurface with a constant relative mean curvature's function."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a hypersurface $\\varPhi$ in $\\mathbb{R}^4$ with position vector field $\\vect{x}$ and a relative normalization $\\vect{y}$. Let $\\mathcal{F}$ be the one-parameter family of hypersurfaces $\\varPhi_\\mu$ with position vector field $\\vect{x}_\\mu = \\vect{x} + \\mu \\, \\vect{y}$, where $\\mu$ is a real constant. Suppose that the relative mean curvature's functions of $\\varPhi$ are constant, and denote them by $H_\\varPhi$. Prove that there exists at least one relatively parallel hypersurface $\\varPhi_\\mu$ with a constant relative mean curvature's function, and show that this constant relative mean curvature's function is equal to $H_\\varPhi$.\n\nA) The relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi + \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$.\n\nB) The relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi - \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$.\n\nC) The relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi + \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$, and the constant relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi$.\n\nD) The relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi - \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$, and the constant relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi$.\n\nCorrect Answer: C) The relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi + \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$, and the constant relative mean curvature's function of $\\varPhi_\\mu$ is equal to $H_\\varPhi$.\n\nExplanation: To solve this problem, we need to use the definition of relative mean curvature's function and the properties of the one-parameter family of hypersurfaces $\\varPhi_\\mu$. We start by expressing the relative mean curvature's function of $\\varPhi_\\mu$ in terms of the relative mean curvature's function of $\\varPhi$ and the \"relative distance\" $\\mu$. Using the chain rule, we can write:\n\n$$H_{\\varPhi_\\mu} = H_\\varPhi + \\mu \\, \\frac{\\partial H_\\varPhi}{\\partial \\mu}$$\n\nThis expression shows that the relative mean curvature's function of $\\varPhi_\\mu$ is equal to the relative mean curvature's function of $\\varPhi$ plus the \"relative distance\" $\\mu$ times the derivative of the relative mean curvature's function of $\\varPhi$ with respect to $\\mu$. Since the relative mean curvature's functions of $\\varPhi$ are constant, we have:\n\n$$\\frac{\\partial H_\\varPhi}{\\partial \\mu} = 0$$\n\nTherefore, the relative mean curvature's function of $\\varPhi_\\mu$ is equal to the relative mean curvature's function of $\\varPhi$, which is constant. This proves that there exists at least one relatively parallel hypersurface $\\varPhi_\\mu$ with a constant relative mean curvature's function, and that this constant relative mean curvature's function is equal to $H_\\varPhi$."}, "38": {"documentation": {"title": "Epidemic response to physical distancing policies and their impact on\n  the outbreak risk", "source": "Fabio Vanni, David Lambert, and Luigi Palatella", "docs_id": "2007.14620", "section": ["physics.soc-ph", "econ.GN", "q-bio.PE", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Epidemic response to physical distancing policies and their impact on\n  the outbreak risk. We introduce a theoretical framework that highlights the impact of physical distancing variables such as human mobility and physical proximity on the evolution of epidemics and, crucially, on the reproduction number. In particular, in response to the coronavirus disease (CoViD-19) pandemic, countries have introduced various levels of 'lockdown' to reduce the number of new infections. Specifically we use a collisional approach to an infection-age structured model described by a renewal equation for the time homogeneous evolution of epidemics. As a result, we show how various contributions of the lockdown policies, namely physical proximity and human mobility, reduce the impact of SARS-CoV-2 and mitigate the risk of disease resurgence. We check our theoretical framework using real-world data on physical distancing with two different data repositories, obtaining consistent results. Finally, we propose an equation for the effective reproduction number which takes into account types of interactions among people, which may help policy makers to improve remote-working organizational structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on epidemic response to physical distancing policies and their impact on the outbreak risk propose a theoretical framework that highlights the impact of physical distancing variables on the evolution of epidemics. According to the study, what is the primary contribution of lockdown policies in reducing the impact of SARS-CoV-2 and mitigating the risk of disease resurgence?\n\nA) Human mobility\nB) Physical proximity\nC) Age structure of the population\nD) Vaccination rates\n\n**Correct Answer:** B) Physical proximity\n\n**Explanation:** The study uses a collisional approach to an infection-age structured model to analyze the impact of physical distancing variables on the evolution of epidemics. The authors specifically highlight the role of physical proximity in reducing the impact of SARS-CoV-2 and mitigating the risk of disease resurgence. This is evident from the statement \"we show how various contributions of the lockdown policies, namely physical proximity and human mobility, reduce the impact of SARS-CoV-2 and mitigate the risk of disease resurgence.\"\n\n**Candidate A (Human mobility):** While human mobility is mentioned as a contributing factor, it is not the primary contribution of lockdown policies in reducing the impact of SARS-CoV-2.\n\n**Candidate B (Physical proximity):** This is the correct answer, as the study highlights the role of physical proximity in reducing the impact of SARS-CoV-2 and mitigating the risk of disease resurgence.\n\n**Candidate C (Age structure of the population):** The study does not mention the age structure of the population as a contributing factor to the impact of lockdown policies.\n\n**Candidate D (Vaccination rates):** Vaccination rates are not mentioned in the study as a contributing factor to the impact of lockdown policies."}, "39": {"documentation": {"title": "Predicting the evolution of the COVID-19 epidemic with the A-SIR model:\n  Lombardy, Italy and S\\~ao Paulo state, Brazil", "source": "Armando G. M. Neves and Gustavo Guerrero", "docs_id": "2005.11182", "section": ["q-bio.PE", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predicting the evolution of the COVID-19 epidemic with the A-SIR model:\n  Lombardy, Italy and S\\~ao Paulo state, Brazil. The presence of a large number of infected individuals with few or no symptoms is an important epidemiological difficulty and the main mathematical feature of COVID-19. The A-SIR model, i.e. a SIR (Susceptible-Infected-Removed) model with a compartment for infected individuals with no symptoms or few symptoms was proposed by Giuseppe Gaeta, arXiv:2003.08720 [q-bio.PE] (2020). In this paper we investigate a slightly generalized version of the same model and propose a scheme for fitting the parameters of the model to real data using the time series only of the deceased individuals. The scheme is applied to the concrete cases of Lombardy, Italy and S\\~ao Paulo state, Brazil, showing different aspects of the epidemics. For each case we show that we may have good fits to the data up to the present, but with very large differences in the future behavior. The reasons behind such disparate outcomes are the uncertainty on the value of a key parameter, the probability that an infected individual is fully symptomatic, and on the intensity of the social distancing measures adopted. This conclusion enforces the necessity of trying to determine the real number of infected individuals in a population, symptomatic or asymptomatic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper on predicting the evolution of the COVID-19 epidemic with the A-SIR model investigate a slightly generalized version of the SIR model, which includes a compartment for infected individuals with no symptoms or few symptoms. However, the model's performance varies significantly between two case studies: Lombardy, Italy and S\\~ao Paulo state, Brazil. What are the three main factors that contribute to the disparate outcomes in the model's future behavior, according to the authors?\n\nA) The uncertainty on the value of the basic reproduction number (R0), the probability that an infected individual is fully symptomatic, and the intensity of the social distancing measures adopted.\nB) The uncertainty on the value of the transmission rate, the probability that an infected individual is fully symptomatic, and the effectiveness of the contact tracing measures.\nC) The uncertainty on the value of the recovery rate, the probability that an infected individual is fully symptomatic, and the impact of the weather on the spread of the disease.\nD) The uncertainty on the value of the incubation period, the probability that an infected individual is fully symptomatic, and the effectiveness of the vaccination campaign.\n\n**Correct Answer:** A) The uncertainty on the value of the basic reproduction number (R0), the probability that an infected individual is fully symptomatic, and the intensity of the social distancing measures adopted.\n\n**Explanation:** The authors mention that the uncertainty on the value of the key parameter, the probability that an infected individual is fully symptomatic, and the intensity of the social distancing measures adopted are the reasons behind the disparate outcomes in the model's future behavior. The basic reproduction number (R0) is also mentioned as a key parameter, but it is not explicitly stated as one of the contributing factors. Therefore, option A is the correct answer."}, "40": {"documentation": {"title": "The Impacts of Mobility on Covid-19 Dynamics: Using Soft and Hard Data", "source": "Leonardo Martins and Marcelo C. Medeiros", "docs_id": "2110.00597", "section": ["stat.AP", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Impacts of Mobility on Covid-19 Dynamics: Using Soft and Hard Data. This paper has the goal of evaluating how changes in mobility has affected the infection spread of Covid-19 throughout the 2020-2021 years. However, identifying a \"clean\" causal relation is not an easy task due to a high number of non-observable (behavioral) effects. We suggest the usage of Google Trends and News-based indexes as controls for some of these behavioral effects and we find that a 1\\% increase in residential mobility (i.e. a reduction in overall mobility) have significant impacts for reducing both Covid-19 cases (at least 3.02\\% on a one-month horizon) and deaths (at least 2.43\\% at the two-weeks horizon) over the 2020-2021 sample. We also evaluate the effects of mobility on Covid-19 spread on the restricted sample (only 2020) where vaccines were not available. The results of diminishing mobility over cases and deaths on the restricted sample are still observable (with similar magnitudes in terms of residential mobility) and cumulative higher, as the effects of restricting workplace mobility turns to be also significant: a 1\\% decrease in workplace mobility diminishes cases around 1\\% and deaths around 2\\%."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Evaluate the impact of changes in mobility on the spread of Covid-19, considering the limitations of identifying causal relations due to non-observable behavioral effects. How do the results of the study differ between the unrestricted sample (2020-2021) and the restricted sample (only 2020), and what implications do these findings have for understanding the effects of mobility on Covid-19 spread?\n\n**A)** The study's use of Google Trends and News-based indexes as controls for behavioral effects is a flawed approach, as it may not accurately capture the complex relationships between mobility and Covid-19 spread.\n\n**B)** The results of the study suggest that a 1% decrease in workplace mobility leads to a 1% reduction in Covid-19 cases and a 2% reduction in deaths, indicating a significant impact of mobility on the spread of the disease.\n\n**C)** The study's findings are limited by the fact that the restricted sample (only 2020) does not account for the effects of vaccines on Covid-19 spread, which may have influenced the results.\n\n**D)** The study's use of a 1% increase in residential mobility as a control for behavioral effects is a reasonable approach, as it allows for the examination of the impact of mobility on Covid-19 spread while controlling for other factors that may influence the relationship between the two variables.\n\n**Correct Answer:** D) The study's use of a 1% increase in residential mobility as a control for behavioral effects is a reasonable approach, as it allows for the examination of the impact of mobility on Covid-19 spread while controlling for other factors that may influence the relationship between the two variables.\n\n**Explanation:** The correct answer is D) because the study uses a 1% increase in residential mobility as a control for behavioral effects, which allows for the examination of the impact of mobility on Covid-19 spread while controlling for other factors that may influence the relationship between the two variables. This approach is reasonable because it acknowledges the limitations of identifying causal relations due to non-observable behavioral effects and attempts to mitigate these limitations by using external controls. The other options are incorrect because they either misrepresent the study's approach (A), misinterpret the results (B), or fail to account for the study's limitations (C)."}, "41": {"documentation": {"title": "Giant perpendicular magnetic anisotropy enhancement in MgO-based\n  magnetic tunnel junction by using Co/Fe composite layer", "source": "Libor Voj\\'a\\v{c}ek, Fatima Ibrahim, Ali Hallal, Bernard Dieny,\n  Mairbek Chshiev", "docs_id": "2007.15940", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Giant perpendicular magnetic anisotropy enhancement in MgO-based\n  magnetic tunnel junction by using Co/Fe composite layer. Magnetic tunnel junctions with perpendicular anisotropy form the basis of the spin-transfer torque magnetic random-access memory (STT-MRAM), which is non-volatile, fast, dense, and has quasi-infinite write endurance and low power consumption. Based on density functional theory (DFT) calculations, we propose an alternative design of magnetic tunnel junctions comprising Fe(n)Co(m)Fe(n)/MgO storage layers with greatly enhanced perpendicular magnetic anisotropy (PMA) up to several mJ/m2, leveraging the interfacial perpendicular anisotropy of Fe/MgO along with a stress-induced bulk PMA discovered within bcc Co. This giant enhancement dominates the demagnetizing energy when increasing the film thickness. The tunneling magnetoresistance (TMR) estimated from the Julliere model is comparable with that of the pure Fe/MgO case. We discuss the advantages and pitfalls of a real-life fabrication of the structure and propose the Fe(3ML)Co(4ML)Fe(3ML) as a storage layer for MgO-based STT-MRAM cells. The large PMA in strained bcc Co is explained in the framework of Bruno's model by the MgO-imposed strain and consequent changes in the energies of dyz and dz2 minority-spin bands."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a Fe(n)Co(m)Fe(n)/MgO storage layer in magnetic tunnel junctions, as proposed in the study, and how does it relate to the demagnetizing energy?\n\nA) The Fe(n)Co(m)Fe(n) layer enhances the tunneling magnetoresistance (TMR) by increasing the film thickness, leading to a higher demagnetizing energy.\nB) The Fe(n)Co(m)Fe(n) layer reduces the demagnetizing energy by decreasing the film thickness, resulting in a lower TMR.\nC) The Fe(n)Co(m)Fe(n) layer increases the demagnetizing energy due to the stress-induced bulk PMA in the bcc Co, which dominates the interfacial perpendicular anisotropy of Fe/MgO.\nD) The Fe(n)Co(m)Fe(n) layer maintains the demagnetizing energy at a constant level, allowing for a stable and efficient magnetic tunnel junction.\n\nCorrect Answer: C) The Fe(n)Co(m)Fe(n) layer increases the demagnetizing energy due to the stress-induced bulk PMA in the bcc Co, which dominates the interfacial perpendicular anisotropy of Fe/MgO.\n\nExplanation: The correct answer is C) because the study proposes using a Fe(n)Co(m)Fe(n) layer to enhance the perpendicular magnetic anisotropy (PMA) in magnetic tunnel junctions. The PMA is increased due to the stress-induced bulk PMA in the bcc Co, which dominates the interfacial perpendicular anisotropy of Fe/MgO. This leads to a higher demagnetizing energy, which is a key advantage of the proposed design. The other options are incorrect because they do not accurately describe the relationship between the Fe(n)Co(m)Fe(n) layer and the demagnetizing energy."}, "42": {"documentation": {"title": "A Search for X-ray Counterparts of Radio Pulsars", "source": "Tobias Prinz (MPE Garching) and Werner Becker (MPE Garching and MPIfR\n  Bonn)", "docs_id": "1511.07713", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Search for X-ray Counterparts of Radio Pulsars. We describe a systematic search for X-ray counterparts of radio pulsars. The search was accomplished by cross-correlating the radio timing positions of all radio pulsars from the ATNF pulsar database (version 1.54) with archival XMM-Newton and Chandra observations publicly released by July 1st 2016. In total, 178 of the archival XMM-Newton observations and 213 of the archival Chandra datasets where found to have a radio pulsar serendipitously in the field of view. From the 288 radio pulsars covered by these datasets we identified 20 previously undetected X-ray counterparts. For 6 of them the statistics was sufficient to model the energy spectrum with one- or two-component models. For the remaining new detections and for those pulsars for which we determined an upper limit to their counting rate we computed the energy flux by assuming a Crab-like spectrum. Additionally, we derived upper limits on the neutron stars' surface temperature and on the non-thermal X-ray efficiency for those pulsars for which the spin-down energy was known. The temperature upper limits where compared with predictions from various neutron star cooling models and where found to be in agreement with the minimal cooling paradigm"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"A Search for X-ray Counterparts of Radio Pulsars\" used archival XMM-Newton and Chandra observations to search for X-ray counterparts of radio pulsars. What was the total number of archival XMM-Newton observations and Chandra datasets that were found to have a radio pulsar serendipitously in the field of view?\n\nA) 178 XMM-Newton observations and 213 Chandra datasets\nB) 213 XMM-Newton observations and 178 Chandra datasets\nC) 288 XMM-Newton observations and 178 Chandra datasets\nD) 213 XMM-Newton observations and 288 Chandra datasets\n\n**Correct Answer:** A) 178 XMM-Newton observations and 213 Chandra datasets\n\n**Explanation:** According to the text, \"In total, 178 of the archival XMM-Newton observations and 213 of the archival Chandra datasets where found to have a radio pulsar serendipitously in the field of view.\" This indicates that there were 178 XMM-Newton observations and 213 Chandra datasets that contained radio pulsars in their field of view."}, "43": {"documentation": {"title": "Transitions in optimal adaptive strategies for populations in\n  fluctuating environments", "source": "Andreas Mayer, Thierry Mora, Olivier Rivoire, Aleksandra M. Walczak", "docs_id": "1703.09780", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transitions in optimal adaptive strategies for populations in\n  fluctuating environments. Biological populations are subject to fluctuating environmental conditions. Different adaptive strategies can allow them to cope with these fluctuations: specialization to one particular environmental condition, adoption of a generalist phenotype that compromise between conditions, or population-wise diversification (bet-hedging). Which strategy provides the largest selective advantage in the long run depends on the range of accessible phenotypes and the statistics of the environmental fluctuations. Here, we analyze this problem in a simple mathematical model of population growth. First, we review and extend a graphical method to identify the nature of the optimal strategy when the environmental fluctuations are uncorrelated. Temporal correlations in environmental fluctuations open up new strategies that rely on memory but are mathematically challenging to study: we present here new analytical results to address this challenge. We illustrate our general approach by analyzing optimal adaptive strategies in the presence of trade-offs that constrain the range of accessible phenotypes. Our results extend several previous studies and have applications to a variety of biological phenomena, from antibiotic resistance in bacteria to immune responses in vertebrates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of optimal adaptive strategies for populations in fluctuating environments, what is the primary factor that determines the long-term selective advantage of a particular strategy, and how does it relate to the range of accessible phenotypes and the statistics of environmental fluctuations?\n\n**A)** The range of accessible phenotypes and the statistics of environmental fluctuations determine the long-term selective advantage of a strategy, which is influenced by the degree of specialization or generalization of the phenotype.\n\n**B)** The range of accessible phenotypes and the statistics of environmental fluctuations determine the long-term selective advantage of a strategy, which is influenced by the degree of diversification or bet-hedging within the population.\n\n**C)** The range of accessible phenotypes and the statistics of environmental fluctuations determine the long-term selective advantage of a strategy, which is influenced by the degree of temporal correlations in environmental fluctuations.\n\n**D)** The range of accessible phenotypes and the statistics of environmental fluctuations determine the long-term selective advantage of a strategy, which is influenced by the degree of specialization or generalization of the population as a whole.\n\n**Correct Answer:** A) The range of accessible phenotypes and the statistics of environmental fluctuations determine the long-term selective advantage of a strategy, which is influenced by the degree of specialization or generalization of the phenotype.\n\n**Explanation:** The correct answer is A) because the documentation states that \"Which strategy provides the largest selective advantage in the long run depends on the range of accessible phenotypes and the statistics of the environmental fluctuations.\" This suggests that the range of accessible phenotypes and the statistics of environmental fluctuations are the primary factors that determine the long-term selective advantage of a strategy. Additionally, the documentation mentions that specialization to one particular environmental condition, adoption of a generalist phenotype, or population-wise diversification (bet-hedging) are different adaptive strategies that can provide a selective advantage, which is influenced by the degree of specialization or generalization of the phenotype."}, "44": {"documentation": {"title": "Deep calibration of rough stochastic volatility models", "source": "Christian Bayer, Benjamin Stemper", "docs_id": "1810.03399", "section": ["q-fin.PR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep calibration of rough stochastic volatility models. Sparked by Al\\`os, Le\\'on, and Vives (2007); Fukasawa (2011, 2017); Gatheral, Jaisson, and Rosenbaum (2018), so-called rough stochastic volatility models such as the rough Bergomi model by Bayer, Friz, and Gatheral (2016) constitute the latest evolution in option price modeling. Unlike standard bivariate diffusion models such as Heston (1993), these non-Markovian models with fractional volatility drivers allow to parsimoniously recover key stylized facts of market implied volatility surfaces such as the exploding power-law behaviour of the at-the-money volatility skew as time to maturity goes to zero. Standard model calibration routines rely on the repetitive evaluation of the map from model parameters to Black-Scholes implied volatility, rendering calibration of many (rough) stochastic volatility models prohibitively expensive since there the map can often only be approximated by costly Monte Carlo (MC) simulations (Bennedsen, Lunde, & Pakkanen, 2017; McCrickerd & Pakkanen, 2018; Bayer et al., 2016; Horvath, Jacquier, & Muguruza, 2017). As a remedy, we propose to combine a standard Levenberg-Marquardt calibration routine with neural network regression, replacing expensive MC simulations with cheap forward runs of a neural network trained to approximate the implied volatility map. Numerical experiments confirm the high accuracy and speed of our approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of rough stochastic volatility models, such as the rough Bergomi model, and how do these models differ from standard bivariate diffusion models like the Heston model?\n\n**A)** To reduce the computational complexity of option pricing models and recover key stylized facts of market implied volatility surfaces.\n**B)** To introduce non-Markovian dynamics and fractional volatility drivers to better capture market microstructure effects.\n**C)** To provide a more flexible and parsimonious framework for option pricing, allowing for the recovery of key stylized facts of market implied volatility surfaces.\n**D)** To introduce artificial noise into the volatility process to better capture market volatility clustering.\n\n**Correct Answer:** C) To provide a more flexible and parsimonious framework for option pricing, allowing for the recovery of key stylized facts of market implied volatility surfaces.\n\n**Explanation:** The correct answer is C) because the documentation states that rough stochastic volatility models, such as the rough Bergomi model, constitute the latest evolution in option price modeling, and that these models allow to parsimoniously recover key stylized facts of market implied volatility surfaces, such as the exploding power-law behaviour of the at-the-money volatility skew as time to maturity goes to zero. This suggests that the primary motivation behind the development of these models is to provide a more flexible and parsimonious framework for option pricing, which is option C."}, "45": {"documentation": {"title": "Prohibitions caused by nonlocality for Alice-Bob Boussinesq-KdV type\n  systems", "source": "S. Y. Lou", "docs_id": "1806.07559", "section": ["nlin.SI", "math-ph", "math.MP", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prohibitions caused by nonlocality for Alice-Bob Boussinesq-KdV type\n  systems. It is found that two different celebrate models, the Korteweg de-Vrise (KdV) equation and the Boussinesq equation, are linked to a same model equation but with different nonlocalities. The model equation is called the Alice-Bob KdV (ABKdV) equation which was derived from the usual KdV equation via the so-called consistent correlated bang (CCB) companied by the shifted parity (SP) and delayed time reversal (DTR). The same model can be called as the Alice-Bob Boussinesq (ABB) system if the nonlocality is changed as only one of SP and DTR. For the ABB systems, with help of the bilinear approach and recasting the multi-soliton solutions of the usual Boussinesq equation to an equivalent novel form, the multi-soliton solutions with even numbers and the head on interactions are obtained. However, the multi-soliton solutions with odd numbers and the multi-soliton solutions with even numbers but with pursuant interactions are prohibited. For the ABKdV equation, the multi-soliton solutions exhibit many more structures because an arbitrary odd function of $x+t$ can be introduced as background waves of the usual KdV equation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Alice-Bob KdV (ABKdV) equation, which is derived from the usual KdV equation via the consistent correlated bang (CCB) method accompanied by the shifted parity (SP) and delayed time reversal (DTR). What is the condition under which the multi-soliton solutions of the ABKdV equation are prohibited, and how does this condition differ from that of the Alice-Bob Boussinesq (ABB) system?\n\nA) The multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an odd number of solitons.\nB) The multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an even number of solitons with pursuant interactions.\nC) The multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an arbitrary odd function of $x+t$ as background waves.\nD) The multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an even number of solitons with head-on interactions.\n\nCorrect Answer: B) The multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an even number of solitons with pursuant interactions.\n\nExplanation: According to the provided documentation, the multi-soliton solutions of the ABKdV equation are prohibited when the nonlocality is changed to only one of SP and DTR, and the solution exhibits an even number of solitons with pursuant interactions. This is in contrast to the ABB system, where the multi-soliton solutions with even numbers and the head-on interactions are allowed."}, "46": {"documentation": {"title": "Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks", "source": "Tung-Wei Kuo, Kuang-Che Lee, Kate Ching-Ju Lin and Ming-Jer Tsai", "docs_id": "1404.6041", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks. In multiuser MIMO (MU-MIMO) LANs, the achievable throughput of a client depends on who are transmitting concurrently with it. Existing MU-MIMO MAC protocols however enable clients to use the traditional 802.11 contention to contend for concurrent transmission opportunities on the uplink. Such a contention-based protocol not only wastes lots of channel time on multiple rounds of contention, but also fails to maximally deliver the gain of MU-MIMO because users randomly join concurrent transmissions without considering their channel characteristics. To address such inefficiency, this paper introduces MIMOMate, a leader-contention-based MU-MIMO MAC protocol that matches clients as concurrent transmitters according to their channel characteristics to maximally deliver the MU-MIMO gain, while ensuring all users to fairly share concurrent transmission opportunities. Furthermore, MIMOMate elects the leader of the matched users to contend for transmission opportunities using traditional 802.11 CSMA/CA. It hence requires only a single contention overhead for concurrent streams, and can be compatible with legacy 802.11 devices. A prototype implementation in USRP-N200 shows that MIMOMate achieves an average throughput gain of 1.42x and 1.52x over the traditional contention-based protocol for 2-antenna and 3-antenna AP scenarios, respectively, and also provides fairness for clients."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks, what is the primary advantage of using MIMOMate over traditional contention-based MU-MIMO MAC protocols?\n\n**A)** MIMOMate reduces the overhead of channel time wasted on multiple rounds of contention.\n**B)** MIMOMate ensures that all users have an equal share of concurrent transmission opportunities.\n**C)** MIMOMate maximally delivers the MU-MIMO gain by matching clients as concurrent transmitters according to their channel characteristics.\n**D)** MIMOMate is incompatible with legacy 802.11 devices.\n\n**Correct Answer:** C) MIMOMate maximally delivers the MU-MIMO gain by matching clients as concurrent transmitters according to their channel characteristics.\n\n**Explanation:** The correct answer is C) because the paper states that MIMOMate \"maximally deliver the MU-MIMO gain, while ensuring all users to fairly share concurrent transmission opportunities\" by matching clients according to their channel characteristics. This is the primary advantage of using MIMOMate over traditional contention-based protocols.\n\nThe other options are incorrect because:\n\n* A) While MIMOMate does reduce the overhead of channel time wasted on multiple rounds of contention, this is not its primary advantage.\n* B) MIMOMate does ensure fairness for clients, but this is not its primary advantage.\n* D) MIMOMate is actually compatible with legacy 802.11 devices, as stated in the paper."}, "47": {"documentation": {"title": "Families of Bragg-grating solitons in a cubic-quintic medium", "source": "Javid Atai and Boris A. Malomed", "docs_id": "nlin/0105039", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Families of Bragg-grating solitons in a cubic-quintic medium. We investigate the existence and stability of solitons in an optical waveguide equipped with a Bragg grating (BG) in which nonlinearity contains both cubic and quintic terms. The model has straightforward realizations in both temporal and spatial domains, the latter being most realistic. Two different families of zero-velocity solitons, which are separated by a border at which solitons do not exist, are found in an exact analytical form. One family may be regarded as a generalization of the usual BG solitons supported by the cubic nonlinearity, while the other family, dominated by the quintic nonlinearity, includes novel ``two-tier'' solitons with a sharp (but nonsingular) peak. These soliton families also differ in the parities of their real and imaginary parts. A stability region is identified within each family by means of direct numerical simulations. The addition of the quintic term to the model makes the solitons very robust: simulating evolution of a strongly deformed pulse, we find that a larger part of its energy is \\emph{retained} in the process of its evolution into a soliton shape, only a small share of the energy being lost into radiation, which is opposite to what occurs in the usual BG model with cubic nonlinearity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Bragg-grating solitons in a cubic-quintic medium, what is the primary effect of adding the quintic nonlinearity term to the model, and how does it compare to the usual Bragg-grating solitons supported by the cubic nonlinearity?\n\n**A)** The addition of the quintic term leads to a decrease in the stability of the solitons, resulting in a higher loss of energy into radiation.\n\n**B)** The quintic term makes the solitons more susceptible to deformation, leading to a larger loss of energy into radiation.\n\n**C)** The quintic term significantly enhances the robustness of the solitons, allowing a larger part of the energy to be retained during the evolution of a strongly deformed pulse.\n\n**D)** The quintic term has no effect on the stability of the solitons, and the solitons behave similarly to those in the cubic nonlinearity model.\n\n**Correct Answer:** C) The quintic term significantly enhances the robustness of the solitons, allowing a larger part of the energy to be retained during the evolution of a strongly deformed pulse.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"simulating evolution of a strongly deformed pulse, we find that a larger part of its energy is \\emph{retained} in the process of its evolution into a soliton shape, only a small share of the energy being lost into radiation...\" This indicates that the addition of the quintic term leads to a more robust soliton, which retains more energy during the evolution process."}, "48": {"documentation": {"title": "Distributionally Robust Graphical Models", "source": "Rizal Fathony, Ashkan Rezaei, Mohammad Ali Bashiri, Xinhua Zhang,\n  Brian D. Ziebart", "docs_id": "1811.02728", "section": ["stat.ML", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributionally Robust Graphical Models. In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods---probabilistic graphical models and large margin methods---have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary drawback of Conditional Random Fields (CRFs) in structured prediction problems, according to the provided documentation?\n\n**A)** They do not permit integration of customized loss metrics into their learning process.\n**B)** They are not Fisher consistent.\n**C)** They are computationally expensive to train.\n**D)** They are not suitable for large-scale datasets.\n\n**Correct Answer:** A) They do not permit integration of customized loss metrics into their learning process.\n\n**Explanation:** The documentation states that CRFs are Fisher consistent, but they do not allow for the integration of customized loss metrics into their learning process. This is a significant drawback, as it limits the flexibility of CRFs in addressing specific problems. The other options are incorrect because they do not accurately reflect the primary drawback of CRFs mentioned in the documentation."}, "49": {"documentation": {"title": "Cauchy problems for Keller-Segel type time-space fractional diffusion\n  equation", "source": "Lei Li and Jian-Guo Liu and Li-zhen Wang", "docs_id": "1712.02298", "section": ["math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cauchy problems for Keller-Segel type time-space fractional diffusion\n  equation. This paper investigates Cauchy problems for nonlinear fractional time-space generalized Keller-Segel equation $^c_0D_t^\\beta\\rho+(-\\triangle)^{\\frac{\\alpha}{2}}\\rho+\\nabla\\cdot(\\rho B(\\rho))=0$, where Caputo derivative $^c_0D_t^\\beta\\rho$ models memory effects in time, fractional Laplacian $(-\\triangle)^{\\frac{\\alpha}{2}}\\rho$ represents L\\'evy diffusion and $B(\\rho)=-s_{n,\\gamma}\\int_{R^n}\\frac{x-y}{|x-y|^{n-\\gamma+2}}\\rho(y)dy $ is the general potential with a singular kernel which takes into account the long rang interaction. We first establish $L^r-L^q$ estimates and weighted estimates of the fundamental solutions $(P(x,t), Y(x,t))$ (or equivalently, the solution operators $(S_\\alpha^\\beta(t), T_\\alpha^\\beta(t))$). Then, we prove the existence and uniqueness of the mild solutions when initial data are in $L^p$ spaces, or the weighted spaces. Similar to Keller-Segel equations, if the initial data are small in critical space $L^{p_c}(\\mathbb{R}^n)$ ($p_c=\\frac{n}{\\alpha+\\gamma-2}$), we construct the global existence. Furthermore, we prove the $L^1$ integrability and integral preservation when the initial data are in $L^1(\\mathbb{R}^n)\\cap L^p(\\mathbb{R}^n)$ or $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$. Finally, some important properties of the mild solutions including the nonnegativity preservation, mass conservation and blowup behaviors are established."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Investigate the properties of the mild solutions to the Keller-Segel type time-space fractional diffusion equation, specifically the preservation of nonnegativity, mass conservation, and blowup behaviors.\n\n**A)** The mild solutions preserve nonnegativity and mass conservation, but may exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$.\n\n**B)** The mild solutions preserve nonnegativity and mass conservation, and exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$.\n\n**C)** The mild solutions do not preserve nonnegativity, but conserve mass and exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$.\n\n**D)** The mild solutions preserve nonnegativity, conserve mass, and exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$.\n\n**Correct Answer:** D) The mild solutions preserve nonnegativity, conserve mass, and exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$.\n\n**Explanation:** The correct answer is D) because the documentation states that the mild solutions preserve nonnegativity and mass conservation, and exhibit blowup behavior for initial data in $L^1(\\mathbb{R}^n)\\cap L^{p_c}(\\mathbb{R}^n)$. This is a consequence of the existence and uniqueness of the mild solutions, as well as the properties of the fundamental solutions and the solution operators."}, "50": {"documentation": {"title": "Detailed illustration of accuracy of presently used nuclear-mass models", "source": "Adam Sobiczewski, Yuri A. Litvinov, Michal Palczewski", "docs_id": "1903.00886", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detailed illustration of accuracy of presently used nuclear-mass models. The accuracy of description of measured nuclear masses by presently used nuclear-mass models is studied. Twelve models of various kinds are considered, eleven of the global character and one local model specially adapted to description of heavy nuclei. To decrease the number of nuclei over which the accuracy is averaged, the global region (Z,N >= 8) is divided into four subregions, in which the accuracy is studied separately. Still, to reach the best precision, the accuracy is finally investigated without any averaging, for each nucleus separately. The results are presented in a form of colored maps, large enough to be easily and accurately read. Besides the accuracy of the studied models, also their predictive power is considered. It is found that the accuracy of description of mass strongly depends on a nuclear-mass model and on the region of nuclei to which the model is applied. The best accuracy is obtained by the recent two Chinese models WS3+ and WS4+. Generally, no clear, strong correlation between the accuracy of description of already known masses by a given model and its predictive power for new masses is observed. Still, such correlation is found for separate models and in separate regions of nuclei. More often for the macroscopic-microscopic models than for the other approaches."}, "answer": "Here is the difficult multi-form question:\n\nWhat are the key findings regarding the accuracy of presently used nuclear-mass models, as reported in the study on Arxiv?\n\nA) The accuracy of description of measured nuclear masses by presently used nuclear-mass models is found to be highly correlated with the predictive power of the models for new masses.\n\nB) The study reveals that the accuracy of description of mass strongly depends on the region of nuclei to which the model is applied, but not on the model itself.\n\nC) The recent two Chinese models WS3+ and WS4+ are found to have the best accuracy in describing the masses of heavy nuclei, with no correlation between accuracy and predictive power.\n\nD) The study shows that the accuracy of description of mass is generally higher for macroscopic-microscopic models than for other approaches, but no correlation is found between accuracy and predictive power for individual models.\n\nCorrect Answer: D) The study shows that the accuracy of description of mass is generally higher for macroscopic-microscopic models than for other approaches, but no correlation is found between accuracy and predictive power for individual models.\n\nExplanation: The correct answer is D) because the study actually finds that the accuracy of description of mass depends on the nuclear-mass model and the region of nuclei to which the model is applied, and that macroscopic-microscopic models tend to have higher accuracy than other approaches, but no clear correlation is found between accuracy and predictive power for individual models."}, "51": {"documentation": {"title": "On Optimal Battery Sizing for Households Participating in Demand-Side\n  Management Schemes", "source": "Matthias Pilz, Omar Ellabban, and Luluwah Al-Fagih", "docs_id": "1905.00097", "section": ["physics.soc-ph", "cs.GT", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Optimal Battery Sizing for Households Participating in Demand-Side\n  Management Schemes. The smart grid with its two-way communication and bi-directional power layers is a cornerstone in the combat against global warming. It allows for the large scale adoption of distributed (individually-owned) renewable energy resources such as solar photovoltaic systems. Their intermittency poses a threat to the stability of the grid which can be addressed by the introduction of energy storage systems. Determining the optimal capacity of a battery has been an active area of research in recent years. In this research an in-depth analysis of the relation between optimal capacity, and demand and generation patterns is performed for households taking part in a community-wide demand-side management scheme. The scheme is based on a non-cooperative dynamic game approach in which participants compete for the lowest electricity bill by scheduling their energy storage systems. The results are evaluated based on self-consumption, the peak-to-average ratio of the aggregated load, and potential cost reductions. Furthermore, the difference between individually-owned batteries to a centralised community energy storage system serving the whole community is investigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the smart grid and demand-side management schemes, what is the primary motivation for households to participate in a community-wide demand-side management scheme, and how does the scheme's non-cooperative dynamic game approach affect the optimal capacity of a battery?\n\n**A)** Households participate to reduce their carbon footprint and increase their self-consumption of renewable energy, and the scheme's non-cooperative approach leads to a decrease in battery capacity due to increased competition for the lowest electricity bill.\n\n**B)** Households participate to reduce their electricity bills and increase their energy independence, and the scheme's non-cooperative approach leads to an increase in battery capacity due to the need for households to schedule their energy storage systems to minimize their energy consumption.\n\n**C)** Households participate to reduce their peak-to-average ratio of the aggregated load and increase their potential cost reductions, and the scheme's non-cooperative approach leads to a decrease in battery capacity due to the need for households to optimize their energy storage systems to minimize their energy consumption.\n\n**D)** Households participate to reduce their peak-to-average ratio of the aggregated load and increase their potential cost reductions, and the scheme's non-cooperative approach leads to an increase in battery capacity due to the need for households to schedule their energy storage systems to minimize their energy consumption and reduce their energy bills.\n\n**Correct Answer:** D) Households participate to reduce their peak-to-average ratio of the aggregated load and increase their potential cost reductions, and the scheme's non-cooperative approach leads to an increase in battery capacity due to the need for households to schedule their energy storage systems to minimize their energy consumption and reduce their energy bills.\n\n**Explanation:** The correct answer is D) because the scheme's non-cooperative dynamic game approach leads to an increase in battery capacity due to the need for households to schedule their energy storage systems to minimize their energy consumption and reduce their energy bills. This is because households compete with each other to achieve the lowest electricity bill, leading to an increase in battery capacity as households invest in energy storage systems to optimize their energy consumption and reduce their energy bills. The other options are incorrect because they do not accurately reflect the motivations and outcomes of the scheme's non-cooperative dynamic game approach."}, "52": {"documentation": {"title": "XMM-Newton observations of the eclipsing polar V2301 Oph", "source": "Gavin Ramsay and Mark Cropper (MSSL/UCL)", "docs_id": "0705.2936", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "XMM-Newton observations of the eclipsing polar V2301 Oph. We present XMM-Newton observations of the eclipsing polar V2301 Oph which cover nearly 2.5 binary orbital cycles and 2 eclipses. This polar is believed to have the lowest magnetic field strength (7 MG) of any known polar. We find evidence for structure in the X-ray eclipse profile which shows a `standstill' feature lasting 26+/-4 sec. This allows us to place an upper limit on the mass of the white dwarf of ~1.2 Msun. We find no evidence for QPOs in the frequency range 0.02-10 Hz. This coupled with the absence of QPOs in RXTE data suggest that, if present, any oscillations in the shock front have a minimal effect on the resultant X-ray flux. We find no evidence for a distinct soft X-ray component in its spectrum - it therefore joins another 7 systems which do not show this component. We suggest that those systems which are asynchronous, have low mass transfer rates, or have accretion occurring over a relatively large fraction of the white dwarf are more likely to show this effect. We find that the specific mass transfer rate has to be close to 0.1 g cm^-2 s^-1 to predict masses which are consistent with that derived from our eclipse analysis. This maybe due to the fact that the low magnetic field strength allows accretion to take place along a wide range of azimuth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the accretion process in the polar V2301 Oph based on the XMM-Newton observations?\n\nA) The accretion process is highly variable and affects the X-ray flux significantly.\nB) The accretion process is minimal and has a minimal effect on the resultant X-ray flux.\nC) The accretion process is asynchronous and occurs over a relatively large fraction of the white dwarf.\nD) The accretion process is synchronized with the binary orbital cycle and affects the X-ray flux significantly.\n\nCorrect Answer: B) The accretion process is minimal and has a minimal effect on the resultant X-ray flux.\n\nExplanation: The question is based on the statement in the documentation that \"We find no evidence for QPOs in the frequency range 0.02-10 Hz. This coupled with the absence of QPOs in RXTE data suggest that, if present, any oscillations in the shock front have a minimal effect on the resultant X-ray flux.\" This suggests that the accretion process has a minimal effect on the X-ray flux, making option B the correct answer."}, "53": {"documentation": {"title": "Confidently Comparing Estimators with the c-value", "source": "Brian L. Trippe, Sameer K. Deshpande, Tamara Broderick", "docs_id": "2102.09705", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Confidently Comparing Estimators with the c-value. Modern statistics provides an ever-expanding toolkit for estimating unknown parameters. Consequently, applied statisticians frequently face a difficult decision: retain a parameter estimate from a familiar method or replace it with an estimate from a newer or complex one. While it is traditional to compare estimators using risk, such comparisons are rarely conclusive in realistic settings. In response, we propose the \"c-value\" as a measure of confidence that a new estimate achieves smaller loss than an old estimate on a given dataset. We show that it is unlikely that a computed c-value is large and that the new estimate has larger loss than the old. Therefore, just as a small p-value provides evidence to reject a null hypothesis, a large c-value provides evidence to use a new estimate in place of the old. For a wide class of problems and estimators, we show how to compute a c-value by first constructing a data-dependent high-probability lower bound on the difference in loss. The c-value is frequentist in nature, but we show that it can provide a validation of Bayesian estimates in real data applications involving hierarchical models and Gaussian processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of comparing estimators with the c-value, what is the primary advantage of using this measure of confidence over traditional risk-based comparisons?\n\n**A)** It provides a more nuanced understanding of the relationship between the old and new estimates, allowing for a more informed decision about which estimate to use.\n**B)** It is more computationally efficient than traditional risk-based comparisons, making it a more practical choice for large datasets.\n**C)** It is a frequentist measure, which provides a more objective and unbiased assessment of the estimates, whereas traditional risk-based comparisons may be influenced by prior beliefs or assumptions.\n**D)** It is a Bayesian measure, which allows for a more flexible and adaptive approach to estimating uncertainty, whereas traditional risk-based comparisons may be too rigid and inflexible.\n\n**Correct Answer:** C) It is a frequentist measure, which provides a more objective and unbiased assessment of the estimates, whereas traditional risk-based comparisons may be influenced by prior beliefs or assumptions.\n\n**Explanation:** The c-value is described as a \"frequentist in nature\", which means that it is based on the frequency of occurrence of certain events, rather than on prior beliefs or assumptions. This makes it a more objective and unbiased measure of confidence, which is particularly useful in situations where prior knowledge or assumptions may be limited or uncertain. In contrast, traditional risk-based comparisons may be influenced by prior beliefs or assumptions, which can lead to biased or inaccurate conclusions."}, "54": {"documentation": {"title": "Skyrmion Quantization and the Decay of the Delta", "source": "Nicholas Dorey, James Hughes and Michael Mattis", "docs_id": "hep-ph/9404274", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Skyrmion Quantization and the Decay of the Delta. We present the complete solution to the so-called ``Yukawa problem'' of the Skyrme model. This refers to the perceived difficulty of reproducing---purely from soliton physics---the usual pseudovector pion-nucleon coupling, echoed by pion coupling to the higher spin/isospin baryons $(I=J=3/2 , 5/2 , \\cdots , N_c/2 )$ in a manner fixed by large-$N_c$ group theory. The solution involves surprisingly elegant interplay between the classical and quantum properties of a new configuration, the ``new improved skyrmion''. This is the near-hedgehog obtained by minimizing the usual skyrmion mass functional augmented by an all-important isorotational kinetic term. The numerics are pleasing: a $\\Delta$ decay width within a few MeV of its measured value, and furthermore, the higher-spin baryons $(I=J \\ge 5/2 )$ with widths so large ($\\Gamma > 800 MeV$) that these undesirable large-$N_c$ artifacts effectively drop out of the spectrum, and pose no phenomenological problem. Beyond these specific results, we ground the Skyrme model in the Feynman Path Integral, and set up a transparent collective coordinate formalism that makes maximal use of the $1/N_c$ expansion. This approach elucidates the connection between skyrmions on the one hand, and Feynman diagrams in an effective field theory on the other."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Skyrme model, what is the significance of the \"new improved skyrmion\" configuration, and how does it relate to the Feynman Path Integral and the $1/N_c$ expansion?\n\nA) The new improved skyrmion configuration is a manifestation of the Skyrme model's failure to reproduce the pion-nucleon coupling, and its significance lies in its ability to generate a $\\Delta$ decay width within a few MeV of its measured value.\n\nB) The new improved skyrmion configuration is a result of the Skyrme model's application of the $1/N_c$ expansion, and its significance lies in its ability to elucidate the connection between skyrmions and Feynman diagrams in an effective field theory.\n\nC) The new improved skyrmion configuration is a near-hedgehog obtained by minimizing the skyrmion mass functional augmented by an isorotational kinetic term, and its significance lies in its ability to reproduce the pion-nucleon coupling and higher-spin baryons with widths so large that they pose no phenomenological problem.\n\nD) The new improved skyrmion configuration is a result of the Skyrme model's application of large-$N_c$ group theory, and its significance lies in its ability to generate a $\\Delta$ decay width within a few MeV of its measured value and to elucidate the connection between skyrmions and Feynman diagrams in an effective field theory.\n\nCorrect Answer: D) The new improved skyrmion configuration is a result of the Skyrme model's application of large-$N_c$ group theory, and its significance lies in its ability to generate a $\\Delta$ decay width within a few MeV of its measured value and to elucidate the connection between skyrmions and Feynman diagrams in an effective field theory.\n\nExplanation: The correct answer, D, highlights the significance of the new improved skyrmion configuration in the context of the Skyrme model. The configuration is a result of the model's application of large-$N_c$ group theory, which is a key aspect of the Skyrme model's formulation. The new improved skyrmion configuration is significant because it generates a $\\Delta$ decay width within a few MeV of its measured value, and it elucidates the connection between skyrmions and Feynman diagrams in an effective field theory. This connection is crucial in understanding the Skyrme model's ability to reproduce the pion-nucleon coupling and higher-spin baryons with widths so large that they pose no phenomenological problem."}, "55": {"documentation": {"title": "Topological Data Analysis with $\\epsilon$-net Induced Lazy Witness\n  Complex", "source": "Naheed Anjum Arafat, Debabrota Basu, St\\'ephane Bressan", "docs_id": "1906.06122", "section": ["cs.CG", "math.AT", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological Data Analysis with $\\epsilon$-net Induced Lazy Witness\n  Complex. Topological data analysis computes and analyses topological features of the point clouds by constructing and studying a simplicial representation of the underlying topological structure. The enthusiasm that followed the initial successes of topological data analysis was curbed by the computational cost of constructing such simplicial representations. The lazy witness complex is a computationally feasible approximation of the underlying topological structure of a point cloud. It is built in reference to a subset of points, called landmarks, rather than considering all the points as in the \\v{C}ech and Vietoris-Rips complexes. The choice and the number of landmarks dictate the effectiveness and efficiency of the approximation. We adopt the notion of $\\epsilon$-cover to define $\\epsilon$-net. We prove that $\\epsilon$-net, as a choice of landmarks, is an $\\epsilon$-approximate representation of the point cloud and the induced lazy witness complex is a $3$-approximation of the induced Vietoris-Rips complex. Furthermore, we propose three algorithms to construct $\\epsilon$-net landmarks. We establish the relationship of these algorithms with the existing landmark selection algorithms. We empirically validate our theoretical claims. We empirically and comparatively evaluate the effectiveness, efficiency, and stability of the proposed algorithms on synthetic and real datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main advantage of using the lazy witness complex in Topological Data Analysis, and how does it relate to the choice of landmarks?\n\nA) The lazy witness complex is a more efficient approximation of the underlying topological structure when the number of landmarks is large.\nB) The lazy witness complex is a more accurate approximation of the underlying topological structure when the number of landmarks is small.\nC) The lazy witness complex is a more computationally feasible approximation of the underlying topological structure when the choice of landmarks is optimal.\nD) The lazy witness complex is a more stable approximation of the underlying topological structure when the choice of landmarks is random.\n\n**Correct Answer:** C) The lazy witness complex is a more computationally feasible approximation of the underlying topological structure when the choice of landmarks is optimal.\n\n**Explanation:** The correct answer is C) because the lazy witness complex is a computationally feasible approximation of the underlying topological structure when the choice of landmarks is optimal. This is stated in the documentation as \"The lazy witness complex is a computationally feasible approximation of the underlying topological structure of a point cloud.\" The other options are incorrect because they do not accurately reflect the relationship between the lazy witness complex and the choice of landmarks. Option A is incorrect because the documentation does not mention the relationship between the number of landmarks and the efficiency of the approximation. Option B is incorrect because the documentation does not mention the relationship between the number of landmarks and the accuracy of the approximation. Option D is incorrect because the documentation does not mention the relationship between the choice of landmarks and the stability of the approximation."}, "56": {"documentation": {"title": "Carries and the arithmetic progression structure of sets", "source": "Francesco Monopoli, Imre Z. Ruzsa", "docs_id": "1506.08869", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Carries and the arithmetic progression structure of sets. If we want to represent integers in base $m$, we need a set $A$ of digits, which needs to be a complete set of residues modulo $m$. When adding two integers with last digits $a_1, a_2 \\in A$, we find the unique $a \\in A$ such that $a_1 + a_2 \\equiv a$ mod $m$, and call $(a_1 + a_2 -a)/m$ the carry. Carries occur also when addition is done modulo $m^2$, with $A$ chosen as a set of coset representatives for the cyclic group $\\mathbb{Z}/m \\mathbb{Z} \\subseteq \\mathbb{Z}/m^2\\mathbb{Z}$. It is a natural to look for sets $A$ which minimize the number of different carries. In a recent paper, Diaconis, Shao and Soundararajan proved that, when $m=p$, $p$ prime, the only set $A$ which induces two distinct carries, i. e. with $A+A \\subseteq \\{ x, y \\}+A$ for some $x, y \\in \\mathbb{Z}/p^2\\mathbb{Z}$, is the arithmetic progression $[0, p-1]$, up to certain linear transformations. We present a generalization of the result above to the case of generic modulus $m^2$, and show how this is connected to the uniqueness of the representation of sets as a minimal number of arithmetic progression of same difference."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a set $A$ of digits used to represent integers in base $m^2$, where $m$ is a generic modulus. Suppose that $A$ is chosen as a set of coset representatives for the cyclic group $\\mathbb{Z}/m \\mathbb{Z} \\subseteq \\mathbb{Z}/m^2\\mathbb{Z}$. What is the condition under which the set $A$ induces the minimum number of distinct carries when adding two integers with last digits $a_1, a_2 \\in A$?\n\nA) The set $A$ is an arithmetic progression of the form $[0, m-1]$.\nB) The set $A$ is a complete set of residues modulo $m$.\nC) The set $A$ is a minimal number of arithmetic progressions of the same difference.\nD) The set $A$ is a union of disjoint arithmetic progressions of the same difference.\n\nCorrect Answer: A) The set $A$ is an arithmetic progression of the form $[0, m-1]$.\n\nExplanation: According to the documentation, when $m=p$, $p$ prime, the only set $A$ which induces two distinct carries is the arithmetic progression $[0, p-1]$, up to certain linear transformations. This result has been generalized to the case of generic modulus $m^2$. Therefore, the correct answer is A) The set $A$ is an arithmetic progression of the form $[0, m-1]$."}, "57": {"documentation": {"title": "Counting Defiers", "source": "Amanda Kowalski", "docs_id": "1908.05811", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Counting Defiers. The LATE monotonicity assumption of Imbens and Angrist (1994) precludes \"defiers,\" individuals whose treatment always runs counter to the instrument, in the terminology of Balke and Pearl (1993) and Angrist et al. (1996). I allow for defiers in a model with a binary instrument and a binary treatment. The model is explicit about the randomization process that gives rise to the instrument. I use the model to develop estimators of the counts of defiers, always takers, compliers, and never takers. I propose separate versions of the estimators for contexts in which the parameter of the randomization process is unspecified, which I intend for use with natural experiments with virtual random assignment. I present an empirical application that revisits Angrist and Evans (1998), which examines the impact of virtual random assignment of the sex of the first two children on subsequent fertility. I find that subsequent fertility is much more responsive to the sex mix of the first two children when defiers are allowed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the LATE monotonicity assumption, what is the primary concern when allowing for \"defiers\" in a model with a binary instrument and a binary treatment?\n\nA) Defiers may lead to biased estimates of treatment effects due to their counterfactual behavior.\nB) Defiers may lead to underestimation of treatment effects due to their non-compliance.\nC) Defiers may lead to overestimation of treatment effects due to their non-random assignment.\nD) Defiers may lead to a loss of identification of treatment effects due to their non-compliance.\n\n**Correct Answer:** A) Defiers may lead to biased estimates of treatment effects due to their counterfactual behavior.\n\n**Explanation:** The LATE monotonicity assumption assumes that the instrument is monotonic in the treatment effect, meaning that the treatment effect is non-decreasing in the instrument. However, when defiers are allowed, their counterfactual behavior can lead to biased estimates of treatment effects, as their behavior is not monotonic in the instrument. This is because defiers may have a treatment effect that is opposite to the one predicted by the model, leading to biased estimates of the treatment effect.\n\n**Candidate B's response:** Defiers may lead to underestimation of treatment effects due to their non-compliance. (Incorrect) This response is incorrect because defiers may actually lead to overestimation of treatment effects, not underestimation.\n\n**Candidate C's response:** Defiers may lead to overestimation of treatment effects due to their non-random assignment. (Incorrect) This response is incorrect because defiers are not necessarily related to non-random assignment, and the concern is more about their counterfactual behavior.\n\n**Candidate D's response:** Defiers may lead to a loss of identification of treatment effects due to their non-compliance. (Incorrect) This response is incorrect because the concern is not about identification, but about biased estimates of treatment effects due to defiers' counterfactual behavior."}, "58": {"documentation": {"title": "A convolutional neural network for defect classification in Bragg\n  coherent X-ray diffraction", "source": "Bruce Lim, Ewen Bellec, Maxime Dupraz, Steven Leake, Andrea Resta,\n  Alessandro Coati, Michael Sprung, Ehud Almog, Eugen Rabkin, Tobias Sch\\\"ulli\n  and Marie-Ingrid Richard", "docs_id": "2106.16179", "section": ["cond-mat.mtrl-sci", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A convolutional neural network for defect classification in Bragg\n  coherent X-ray diffraction. Coherent diffraction imaging enables the imaging of individual defects, such as dislocations or stacking faults, in materials.These defects and their surrounding elastic strain fields have a critical influence on the macroscopic properties and functionality of materials. However, their identification in Bragg coherent diffraction imaging remains a challenge and requires significant data mining. The ability to identify defects from the diffraction pattern alone would be a significant advantage when targeting specific defect types and accelerates experiment design and execution. Here, we exploit a computational tool based on a three-dimensional (3D) parametric atomistic model and a convolutional neural network to predict dislocations in a crystal from its 3D coherent diffraction pattern. Simulated diffraction patterns from several thousands of relaxed atomistic configurations of nanocrystals are used to train the neural network and to predict the presence or absence of dislocations as well as their type(screw or edge). Our study paves the way for defect recognition in 3D coherent diffraction patterns for material science"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a convolutional neural network to predict dislocations in a crystal from its 3D coherent diffraction pattern, as described in the study?\n\nA) It enables the identification of defects in real-time during experiments\nB) It accelerates data mining and experiment design by allowing for the prediction of defect types and locations\nC) It provides a detailed 3D visualization of the crystal structure\nD) It enables the simulation of complex material properties and behavior\n\nCorrect Answer: B) It accelerates data mining and experiment design by allowing for the prediction of defect types and locations\n\nExplanation: The study highlights the challenge of identifying defects in Bragg coherent diffraction imaging and the potential benefits of using a convolutional neural network to predict defect types and locations. The correct answer, B, is supported by the text, which states that \"the ability to identify defects from the diffraction pattern alone would be a significant advantage when targeting specific defect types and accelerates experiment design and execution.\" The other options are incorrect because they do not accurately reflect the primary advantage of using a convolutional neural network in this context."}, "59": {"documentation": {"title": "Measurement of strange baryon-antibaryon interactions with femtoscopic\n  correlations", "source": "ALICE Collaboration", "docs_id": "1903.06149", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of strange baryon-antibaryon interactions with femtoscopic\n  correlations. Two-particle correlation functions were measured for $\\rm p\\overline{p}$, $\\rm p\\overline{\\Lambda}$, $\\rm \\overline{p}\\Lambda$, and $\\Lambda\\overline{\\Lambda}$ pairs in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}=2.76$ TeV and $\\sqrt{s_{\\rm NN}}=5.02$ TeV recorded by the ALICE detector. From a simultaneous fit to all obtained correlation functions, real and imaginary components of the scattering lengths, as well as the effective ranges, were extracted for combined $\\rm p\\overline{\\Lambda}$ and $\\rm \\overline{p}\\Lambda$ pairs and, for the first time, for $\\Lambda\\overline{\\Lambda}$ pairs. Effective averaged scattering parameters for heavier baryon-antibaryon pairs, not measured directly, are also provided. The results reveal similarly strong interaction between measured baryon-antibaryon pairs, suggesting that they all annihilate in the same manner at the same pair relative momentum $k^{*}$. Moreover, the reported significant non-zero imaginary part and negative real part of the scattering length provide motivation for future baryon-antibaryon bound state searches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the implications of the measurement of strange baryon-antibaryon interactions with femtoscopic correlations on the understanding of baryon-antibaryon interactions in high-energy collisions.\n\nA) The results suggest that the strong interaction between baryon-antibaryon pairs is due to the formation of a common bound state, which is not supported by the data.\n\nB) The simultaneous fit to all obtained correlation functions reveals that the imaginary and real components of the scattering lengths are identical for all measured baryon-antibaryon pairs.\n\nC) The reported significant non-zero imaginary part and negative real part of the scattering length provide motivation for future searches for baryon-antibaryon bound states, which would require a more detailed analysis of the data.\n\nD) The effective averaged scattering parameters for heavier baryon-antibaryon pairs, not measured directly, are consistent with the idea that all baryon-antibaryon pairs annihilate in the same manner at the same pair relative momentum k*.\n\nCorrect Answer: D) The effective averaged scattering parameters for heavier baryon-antibaryon pairs, not measured directly, are consistent with the idea that all baryon-antibaryon pairs annihilate in the same manner at the same pair relative momentum k*."}}