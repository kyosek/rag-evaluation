{"0": {"documentation": {"title": "An Extended Integral Unit Commitment Formulation and an Iterative\n  Algorithm for Convex Hull Pricing", "source": "Yanan Yu, Yongpei Guan, Yonghong Chen", "docs_id": "1910.12994", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Extended Integral Unit Commitment Formulation and an Iterative\n  Algorithm for Convex Hull Pricing. To increase market transparency, independent system operators (ISOs) have been working on minimizing uplift payments based on convex hull pricing theorems. However, the large-scale complex systems for ISOs bring computational challenges to the existing convex hull pricing algorithms. In this paper, based on the analysis of specific generator features in the Midcontinent ISO (MISO) system, besides reviewing integral formulations for several special cases, we develop two integral formulations of a single generator that can capture these features. We then build a compact convex hull pricing formulation based on these integral formulations. Meanwhile, to improve the computational efficiency, we propose innovative iterative algorithms with convergence properties, plus a complementary algorithm, to obtain a convex hull price. The computational results indicate that our approach leads to an exact convex hull price on MISO instances with and without transmission constraints and the solutions can be obtained within 20 minutes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** An independent system operator (ISO) is working to minimize uplift payments based on convex hull pricing theorems for the Midcontinent ISO (MISO) system. However, the large-scale complex systems for ISOs bring computational challenges to the existing convex hull pricing algorithms. What are the two integral formulations developed in the paper to capture specific generator features in the MISO system, and how do they contribute to building a compact convex hull pricing formulation?\n\n**A)** The two integral formulations are the \"Generator-Transmission\" formulation and the \"Generator-Load\" formulation, which capture the specific features of generator output and transmission constraints, respectively. These formulations are then used to build a compact convex hull pricing formulation that takes into account both generator features and transmission constraints.\n\n**B)** The two integral formulations are the \"Generator-Load\" formulation and the \"Generator-Transmission\" formulation, which capture the specific features of generator output and transmission constraints, respectively. These formulations are then used to build a compact convex hull pricing formulation that takes into account both generator features and transmission constraints.\n\n**C)** The two integral formulations are the \"Generator-Transmission\" formulation and the \"Generator-Load\" formulation, which capture the specific features of generator output and transmission constraints, respectively. However, these formulations are not used to build a compact convex hull pricing formulation, but rather are used as a basis for developing an iterative algorithm to obtain a convex hull price.\n\n**D)** The two integral formulations are the \"Generator-Transmission\" formulation and the \"Generator-Load\" formulation, which capture the specific features of generator output and transmission constraints, respectively. However, these formulations are not used to build a compact convex hull pricing formulation, but rather are used as a basis for developing a complementary algorithm to obtain a convex hull price.\n\n**Correct Answer:** A) The two integral formulations are the \"Generator-Transmission\" formulation and the \"Generator-Load\" formulation, which capture the specific features of generator output and transmission constraints, respectively. These formulations are then used to build a compact convex hull pricing formulation that takes into account both generator features and transmission constraints.\n\n**Explanation:** The correct answer is A) because the paper develops two integral formulations, \"Generator-Transmission\" and \"Generator-Load\", to capture specific generator features in the MISO system. These formulations are then used to build a compact convex hull pricing formulation that takes into account both generator features and transmission constraints. The other options are incorrect because they either omit the use of the integral formulations to build the compact convex hull pricing formulation or incorrectly describe the formulations."}, "1": {"documentation": {"title": "Bounds for Treatment Effects in the Presence of Anticipatory Behavior", "source": "Aibo Gong", "docs_id": "2111.06573", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bounds for Treatment Effects in the Presence of Anticipatory Behavior. It is often the case in program evaluation that units will often anticipate the implementation of a new policy before it occurs. Such anticipatory behavior can lead to units' outcomes becoming dependent on their future treatment assignments. In this paper, I employ a potential outcomes framework to analyze the treatment effect with anticipation. I start with a classical difference-in-differences model with two time periods and provide partially identified sets with easy-to-implement estimation and inference strategies for causal parameters. I consider generalizations on including covariates and longitudinal models. I also analyze cases with imperfect anticipation and nonlinear outcomes. I further illustrate my results by analyzing the effect of an early retirement incentive program for teachers, which was likely to be anticipated by the target units, on student achievement. The empirical results demonstrate the potential pitfalls of failing to consider anticipation in policy evaluation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of program evaluation, what is the primary concern when units anticipate the implementation of a new policy, and how does this anticipation affect the estimation of treatment effects?\n\n**A)** Units' anticipation leads to a decrease in treatment effects, as they may adjust their behavior before the policy is implemented.\n\n**B)** Units' anticipation leads to an increase in treatment effects, as they may be more motivated to participate in the program.\n\n**C)** Units' anticipation leads to a decrease in the variance of treatment effects, as the anticipation reduces the uncertainty associated with the policy implementation.\n\n**D)** Units' anticipation leads to a decrease in the bias of treatment effects, as the anticipation reduces the correlation between units' outcomes and future treatment assignments.\n\n**Correct Answer:** D) Units' anticipation leads to a decrease in the bias of treatment effects, as the anticipation reduces the correlation between units' outcomes and future treatment assignments.\n\n**Explanation:** The correct answer is D) because when units anticipate the implementation of a new policy, their outcomes become dependent on their future treatment assignments, which can lead to biased estimates of treatment effects if not accounted for. By considering anticipation in the analysis, researchers can reduce the bias in treatment effect estimates and obtain more accurate causal inference. The other options are incorrect because anticipation does not necessarily lead to a decrease in treatment effects (A), increase in treatment effects (B), or decrease in the variance of treatment effects (C)."}, "2": {"documentation": {"title": "METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving", "source": "Rohan Chandra, Mridul Mahajan, Rahul Kala, Rishitha Palugulla,\n  Chandrababu Naidu, Alok Jain, and Dinesh Manocha", "docs_id": "2109.07648", "section": ["cs.CV", "cs.AI", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving. We present a new and complex traffic dataset, METEOR, which captures traffic patterns in unstructured scenarios in India. METEOR consists of more than 1000 one-minute video clips, over 2 million annotated frames with ego-vehicle trajectories, and more than 13 million bounding boxes for surrounding vehicles or traffic agents. METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics. Furthermore, we provide annotations for rare and interesting driving behaviors such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane changing, running traffic signals, driving in the wrong lanes, taking wrong turns, lack of right-of-way rules at intersections, etc. We also present diverse traffic scenarios corresponding to rainy weather, nighttime driving, driving in rural areas with unmarked roads, and high-density traffic scenarios. We use our novel dataset to evaluate the performance of object detection and behavior prediction algorithms. We show that state-of-the-art object detectors fail in these challenging conditions and also propose a new benchmark test: action-behavior prediction with a baseline mAP score of 70.74."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the creation of the METEOR dataset, and how does it differ from existing traffic datasets in terms of capturing traffic patterns?\n\n**A)** METEOR was created to evaluate the performance of object detection algorithms in low-light conditions, and it differs from existing datasets in its focus on heterogeneous traffic characteristics.\n\n**B)** METEOR was created to provide a comprehensive dataset for autonomous driving, and it differs from existing datasets in its emphasis on capturing rare and interesting driving behaviors.\n\n**C)** METEOR was created to evaluate the performance of behavior prediction algorithms in complex traffic scenarios, and it differs from existing datasets in its focus on capturing macroscopic traffic characteristics.\n\n**D)** METEOR was created to provide a benchmark for object detection algorithms, and it differs from existing datasets in its emphasis on capturing microscopic traffic characteristics.\n\n**Correct Answer:** B) METEOR was created to provide a comprehensive dataset for autonomous driving, and it differs from existing datasets in its emphasis on capturing rare and interesting driving behaviors.\n\n**Explanation:** The correct answer is B) because the documentation states that METEOR is a \"new and complex traffic dataset\" that captures traffic patterns in unstructured scenarios in India, and it provides annotations for rare and interesting driving behaviors such as cut-ins, yielding, overtaking, etc. This suggests that the primary motivation behind the creation of METEOR is to provide a comprehensive dataset for autonomous driving, and it differs from existing datasets in its emphasis on capturing these rare and interesting driving behaviors."}, "3": {"documentation": {"title": "Label-Consistent Backdoor Attacks", "source": "Alexander Turner, Dimitris Tsipras, Aleksander Madry", "docs_id": "1912.02771", "section": ["stat.ML", "cs.CR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Label-Consistent Backdoor Attacks. Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is a critical requirement for a backdoor attack to remain undetected in deep neural networks, according to the concept of label-consistency?\n\n**A)** The injected inputs must be mislabeled to raise suspicion.\n**B)** The injected inputs must be consistent with their labels to avoid detection.\n**C)** The model must be trained on a large dataset to increase its robustness.\n**D)** The backdoor trigger must be designed to be easily detectable by humans.\n\n**Correct Answer:** B) The injected inputs must be consistent with their labels to avoid detection.\n\n**Explanation:** The correct answer is B) The injected inputs must be consistent with their labels to avoid detection. This is because label-consistency is a crucial requirement for a backdoor attack to remain undetected. If the injected inputs are mislabeled, they may raise suspicion and be detected by humans. In contrast, if the injected inputs are consistent with their labels, they can be injected into the training set without arousing suspicion, allowing the adversary to plant a backdoor into the model."}, "4": {"documentation": {"title": "What can be learned from a future supernova neutrino detection?", "source": "Shunsaku Horiuchi, James P Kneller", "docs_id": "1709.01515", "section": ["astro-ph.HE", "hep-ex", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "What can be learned from a future supernova neutrino detection?. This year marks the thirtieth anniversary of the only supernova from which we have detected neutrinos - SN 1987A. The twenty or so neutrinos that were detected were mined to great depth in order to determine the events that occurred in the explosion and to place limits upon all manner of neutrino properties. Since 1987 the scale and sensitivity of the detectors capable of identifying neutrinos from a Galactic supernova have grown considerably so that current generation detectors are capable of detecting of order ten thousand neutrinos for a supernova at the Galactic Center. Next generation detectors will increase that yield by another order of magnitude. Simultaneous with the growth of neutrino detection capability, our understanding of how massive stars explode and how the neutrino interacts with hot and dense matter has also increased by a tremendous degree. The neutrino signal will contain much information on all manner of physics of interest to a wide community. In this review we describe the expected features of the neutrino signal, the detectors which will detect it, and the signatures one might try to look for in order to get at these physics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the expected neutrino signal from a future supernova detection, and how will it be affected by the growth of neutrino detection capability and our understanding of massive star explosions?\n\nA) The neutrino signal will contain information on the properties of dark matter, but will not be affected by the growth of detection capability.\nB) The neutrino signal will contain information on the physics of massive star explosions, but will be overwhelmed by background noise due to the increased detection capability.\nC) The neutrino signal will contain information on the properties of neutrinos, including their interactions with hot and dense matter, and will be detectable by current generation detectors.\nD) The neutrino signal will contain information on the properties of black holes, but will not be detectable by current generation detectors due to their limited sensitivity.\n\nCorrect Answer: C) The neutrino signal will contain information on the properties of neutrinos, including their interactions with hot and dense matter, and will be detectable by current generation detectors.\n\nExplanation: The question requires the test-taker to analyze the information provided in the documentation and make an inference about the expected neutrino signal from a future supernova detection. The correct answer, C, is supported by the text, which states that the neutrino signal will contain information on the physics of interest to a wide community, including the properties of neutrinos and their interactions with hot and dense matter. The text also notes that current generation detectors are capable of detecting of order ten thousand neutrinos for a supernova at the Galactic Center, indicating that they are sensitive enough to detect the neutrino signal."}, "5": {"documentation": {"title": "Statistical Assemblies of Particles with Spin", "source": "G. Ramachandran", "docs_id": "1909.03931", "section": ["nucl-th", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Assemblies of Particles with Spin. Spin, $s$ in quantum theory can assume only half odd integer or integer values. For a given $s$, there exist $n=2s+1$ states $|s,m\\rangle$, $m=s,s-1,........,-s$. A statistical assembly of particles (like a beam or target employed in experiments in physics) with the lowest value of spin $s=\\frac {1}{2}$ can be described in terms of probabilities $p_m$ assigned to the two states $m=\\pm \\frac {1}{2}$. A generalization of this concept to higher spins $s>\\frac {1}{2}$ leads only to a particularly simple category of statistical assemblies known as `Oriented systems'. To provide a comprehensive description of all realizable categories of statistical assemblies in experiments, it is advantageous to employ the generators of the Lie group $SU(n)$. The probability domain then gets identified to the interior of regular polyhedra in $\\Re^{n-1},$ where the centre corresponds to an unpolarized assembly and the vertices represent `pure' states. All the other interior points correspond to `mixed' states. The higher spin system has embedded within itself a set of $s(2s+1)$ independent axes, which are determinable empirically. Only when all these axes turn out to be collinear, the simple category of `Oriented systems' is realized, where probabilities $p_m$ are assigned to the states $|s,m\\rangle$. The simplest case of higher spin $s=1$ provides an illustrative example, where additional features of `aligned' and more general `non oriented' categories are displayed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of statistical assemblies of particles with spin, what is the relationship between the spin $s$ and the number of states $n$ that can be described, and how does this relationship impact the description of statistical assemblies in experiments?\n\n**A)** For a given spin $s$, the number of states $n$ is equal to $2s+1$, and this relationship is a direct consequence of the properties of spin in quantum theory.\n\n**B)** The number of states $n$ is equal to $2s+1$, but this relationship only holds for spin $s=\\frac{1}{2}$, and higher spin values require a more complex description.\n\n**C)** The number of states $n$ is equal to $2s+1$, but this relationship is only applicable to statistical assemblies that are oriented, and higher spin values require a more general description.\n\n**D)** The number of states $n$ is equal to $2s+1$, but this relationship is a consequence of the Lie group $SU(n)$, and higher spin values require a more nuanced understanding of the generators of this group.\n\n**Correct Answer:** A) For a given spin $s$, the number of states $n$ is equal to $2s+1$, and this relationship is a direct consequence of the properties of spin in quantum theory.\n\n**Explanation:** The correct answer is A) because the documentation states that \"For a given $s$, there exist $n=2s+1$ states $|s,m\\rangle$\". This relationship is a fundamental property of spin in quantum theory, and it is not specific to spin $s=\\frac{1}{2}$ or higher spin values. The other options are incorrect because they either limit the relationship to specific spin values or imply that the relationship is more complex than it actually is."}, "6": {"documentation": {"title": "Valid Causal Inference with (Some) Invalid Instruments", "source": "Jason Hartford, Victor Veitch, Dhanya Sridhar, Kevin Leyton-Brown", "docs_id": "2006.11386", "section": ["stat.ME", "cs.LG", "econ.EM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Valid Causal Inference with (Some) Invalid Instruments. Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable \"exclusion\" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is \"black-box\" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of instrumental variable methods for causal inference, what is the primary challenge when applying these methods, and how does the proposed approach in the paper \"Valid Causal Inference with (Some) Invalid Instruments\" address this challenge?\n\n**A)** The primary challenge is the reliance on untestable \"exclusion\" assumptions, and the proposed approach addresses this by requiring all candidate instruments to be valid.\n\n**B)** The primary challenge is the difficulty in estimating the treatment effect when there are multiple candidate instruments, and the proposed approach addresses this by using an estimate of the modal prediction from an ensemble of instrumental variable estimators.\n\n**C)** The primary challenge is the need for a large sample size to estimate the causal effect, and the proposed approach addresses this by using a machine-learning based estimator that allows for the estimation of conditional average treatment effects.\n\n**D)** The primary challenge is the requirement for a strong relationship between the instrument variable and the response variable, and the proposed approach addresses this by using a simple and \"black-box\" approach that can be applied to any instrumental variable estimator.\n\n**Correct Answer:** B) The primary challenge is the difficulty in estimating the treatment effect when there are multiple candidate instruments, and the proposed approach addresses this by using an estimate of the modal prediction from an ensemble of instrumental variable estimators.\n\n**Explanation:** The correct answer is B) because the paper proposes a solution to the challenge of estimating the causal effect when there are multiple candidate instruments. The approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators, which allows for the estimation of the causal effect even when only a majority of the candidate instruments are valid. This is in contrast to the other options, which do not accurately capture the solution proposed in the paper."}, "7": {"documentation": {"title": "Soft-core particles freezing to form a quasicrystal and a crystal-liquid\n  phase", "source": "A.J. Archer and A.M. Rucklidge and E. Knobloch", "docs_id": "1507.01682", "section": ["cond-mat.soft", "cond-mat.stat-mech", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Soft-core particles freezing to form a quasicrystal and a crystal-liquid\n  phase. Systems of soft-core particles interacting via a two-scale potential are studied. The potential is responsible for peaks in the structure factor of the liquid state at two different but comparable length scales, and a similar bimodal structure is evident in the dispersion relation. Dynamical density functional theory in two dimensions is used to identify two novel states of this system, the crystal-liquid state, in which the majority of the particles are located on lattice sites but a minority remains free and so behaves like a liquid, and a 12-fold quasicrystalline state. Both are present even for deeply quenched liquids and are found in a regime in which the liquid is unstable with respect to modulations on the smaller scale only. As a result the system initially evolves towards a small scale crystal state; this state is not a minimum of the free energy, however, and so the system subsequently attempts to reorganize to generate the lower energy larger scale crystals. This dynamical process generates a disordered state with quasicrystalline domains, and takes place even when this large scale is linearly stable, i.e., it is a nonlinear process. With controlled initial conditions a perfect quasicrystal can form. The results are corroborated using Brownian dynamics simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary outcome of the dynamical process described in the study, and how does it relate to the stability of the system?\n\nA) The system evolves towards a stable crystal state, which is a minimum of the free energy.\nB) The system generates a disordered state with quasicrystalline domains, even when the large scale is linearly stable.\nC) The system undergoes a phase transition from a liquid to a crystal state, with the quasicrystal being a metastable state.\nD) The system remains in a liquid state, with no changes in the structure or dynamics.\n\nCorrect Answer: B) The system generates a disordered state with quasicrystalline domains, even when the large scale is linearly stable.\n\nExplanation: The study describes a dynamical process that generates a disordered state with quasicrystalline domains, even when the large scale is linearly stable. This is a nonlinear process that occurs even when the system is initially unstable with respect to modulations on the smaller scale. The correct answer requires an understanding of the study's findings and the ability to distinguish between the different outcomes of the dynamical process.\n\nNote: The other options are incorrect because:\n\n* A) The system does not evolve towards a stable crystal state, as the study describes a process that generates a disordered state.\n* C) The phase transition from a liquid to a crystal state is not the primary outcome of the dynamical process, as the study focuses on the generation of quasicrystalline domains.\n* D) The system does not remain in a liquid state, as the study describes a process that generates a disordered state with quasicrystalline domains."}, "8": {"documentation": {"title": "How do governments determine policy priorities? Studying development\n  strategies through spillover networks", "source": "Omar A. Guerrero, Gonzalo Casta\\~neda and Florian Ch\\'avez-Ju\\'arez", "docs_id": "1902.00432", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How do governments determine policy priorities? Studying development\n  strategies through spillover networks. Determining policy priorities is a challenging task for any government because there may be, for example, a multiplicity of objectives to be simultaneously attained, a multidimensional policy space to be explored, inefficiencies in the implementation of public policies, interdependencies between policy issues, etc. Altogether, these factor s generate a complex landscape that governments need to navigate in order to reach their goals. To address this problem, we develop a framework to model the evolution of development indicators as a political economy game on a network. Our approach accounts for the --recently documented-- network of spillovers between policy issues, as well as the well-known political economy problem arising from budget assignment. This allows us to infer not only policy priorities, but also the effective use of resources in each policy issue. Using development indicators data from more than 100 countries over 11 years, we show that the country-specific context is a central determinant of the effectiveness of policy priorities. In addition, our model explains well-known aggregate facts about the relationship between corruption and development. Finally, this framework provides a new analytic tool to generate bespoke advice on development strategies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Governments face numerous challenges when determining policy priorities, including multiple objectives, a multidimensional policy space, and interdependencies between policy issues. How do you think the framework developed in the study \"How do governments determine policy priorities? Studying development strategies through spillover networks\" addresses these challenges, and what are the key findings of the study?\n\n**A)** The framework accounts for the complexity of policy priorities by modeling the evolution of development indicators as a political economy game on a network, which allows for the effective use of resources in each policy issue. However, the study's findings are limited to country-specific contexts and do not provide a comprehensive understanding of global development strategies.\n\n**B)** The framework is overly simplistic, as it fails to consider the nuances of corruption and its impact on development. The study's findings are also based on a limited dataset of over 100 countries, which may not be representative of the global development landscape.\n\n**C)** The framework is a valuable tool for generating bespoke advice on development strategies, but it does not provide a clear understanding of how policy priorities are determined in practice. The study's findings are also influenced by the assumption that country-specific contexts are a central determinant of policy effectiveness.\n\n**D)** The framework is a significant improvement over traditional approaches to policy prioritization, as it accounts for the network of spillovers between policy issues and the political economy problem of budget assignment. The study's findings are robust and generalizable to a wide range of countries and development contexts.\n\n**Correct Answer:** D) The framework is a significant improvement over traditional approaches to policy prioritization, as it accounts for the network of spillovers between policy issues and the political economy problem of budget assignment. The study's findings are robust and generalizable to a wide range of countries and development contexts.\n\n**Explanation:** The correct answer, D, highlights the key strengths of the framework developed in the study. By accounting for the network of spillovers between policy issues and the political economy problem of budget assignment, the framework provides a more nuanced understanding of policy prioritization. The study's findings are also robust and generalizable, as they are based on a large dataset of over 100 countries and a 11-year time period. This suggests that the framework can be applied to a wide range of development contexts, making it a valuable tool for policymakers and development strategists."}, "9": {"documentation": {"title": "A network model for cellular aging", "source": "Hong Qin", "docs_id": "1305.5784", "section": ["q-bio.MN", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A network model for cellular aging. What is aging? Mechanistic answers to this question remain elusive despite decades of research. Here, we propose a mathematical model of cellular aging based on a model gene interaction network. Our network model is made of only non-aging components - the biological functions of gene interactions decrease with a constant mortality rate. Death of a cell occurs in the model when an essential gene loses all of its interactions to other genes, equivalent to the deletion of an essential gene. Gene interactions are stochastic based on a binomial distribution. We show that the defining characteristic of biological aging, the exponential increase of mortality rate over time, can arise from this gene network model during the early stage of aging. Hence, we demonstrate that cellular aging is an emergent property of this model network. Our model predicts that the rate of aging, defined by the Gompertz coefficient, is approximately proportional to the average number of active interactions per gene and that the stochastic heterogeneity of gene interactions is an important factor in the dynamics of the aging process. This theoretic framework offers a mechanistic foundation for the pleiotropic nature of aging and can provide insights on cellular aging."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying assumption of the proposed network model for cellular aging, and how does it relate to the concept of aging?\n\nA) The model assumes that gene interactions are deterministic and follow a linear progression, whereas aging is an emergent property of the network.\nB) The model assumes that gene interactions are stochastic and follow a binomial distribution, whereas aging is a result of the exponential increase of mortality rate over time.\nC) The model assumes that gene interactions are constant and do not decrease with age, whereas aging is a result of the deletion of essential genes.\nD) The model assumes that gene interactions are deterministic and follow a linear progression, whereas aging is an emergent property of the network that arises from the stochastic heterogeneity of gene interactions.\n\nCorrect Answer: B) The model assumes that gene interactions are stochastic and follow a binomial distribution, whereas aging is a result of the exponential increase of mortality rate over time.\n\nExplanation: The correct answer is B) because the documentation states that \"Gene interactions are stochastic based on a binomial distribution\" and that \"the defining characteristic of biological aging, the exponential increase of mortality rate over time, can arise from this gene network model during the early stage of aging.\" This suggests that the model assumes stochastic gene interactions, which is a key aspect of the proposed network model for cellular aging."}, "10": {"documentation": {"title": "Variable selection in doubly truncated regression", "source": "Ming Zheng, Chanjuan Lin, Wen Yu", "docs_id": "2110.10393", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variable selection in doubly truncated regression. Doubly truncated data arise in many areas such as astronomy, econometrics, and medical studies. For the regression analysis with doubly truncated response variables, the existence of double truncation may bring bias for estimation as well as affect variable selection. We propose a simultaneous estimation and variable selection procedure for the doubly truncated regression, allowing a diverging number of regression parameters. To remove the bias introduced by the double truncation, a Mann-Whitney-type loss function is used. The adaptive LASSO penalty is then added into the loss function to achieve simultaneous estimation and variable selection. An iterative algorithm is designed to optimize the resulting objective function. We establish the consistency and the asymptotic normality of the proposed estimator. The oracle property of the proposed selection procedure is also obtained. Some simulation studies are conducted to show the finite sample performance of the proposed approach. We also apply the method to analyze a real astronomical data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of doubly truncated regression, what is the primary goal of using a Mann-Whitney-type loss function, and how does it address the bias introduced by double truncation?\n\nA) To minimize the mean squared error (MSE) and reduce the impact of outliers, thereby improving the accuracy of the regression model.\nB) To introduce an adaptive penalty term that can handle a diverging number of regression parameters, thereby facilitating simultaneous estimation and variable selection.\nC) To reduce the variance of the estimator and improve its asymptotic normality, thereby enhancing the reliability of the regression results.\nD) To incorporate a regularization term that can handle high-dimensional data and reduce the risk of overfitting, thereby improving the generalizability of the model.\n\n**Correct Answer:** B) To introduce an adaptive penalty term that can handle a diverging number of regression parameters, thereby facilitating simultaneous estimation and variable selection.\n\n**Explanation:** The correct answer is B) because the Mann-Whitney-type loss function is used to remove the bias introduced by double truncation, and the adaptive LASSO penalty is added to achieve simultaneous estimation and variable selection. This approach allows for a diverging number of regression parameters, which is a key feature of doubly truncated regression. The other options are incorrect because they do not accurately describe the primary goal of using a Mann-Whitney-type loss function in doubly truncated regression."}, "11": {"documentation": {"title": "GNN-PT: Enhanced Prediction of Compound-protein Interactions by\n  Integrating Protein Transformer", "source": "Jingtao Wang, Xi Li, and Hua Zhang", "docs_id": "2009.00805", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "GNN-PT: Enhanced Prediction of Compound-protein Interactions by\n  Integrating Protein Transformer. The prediction of protein interactions (CPIs) is crucial for the in-silico screening step in drug discovery. Recently, many end-to-end representation learning methods using deep neural networks have achieved significantly better performance than traditional machine learning algorithms. Much effort has focused on the compound representation or the information extraction from the compound-protein interaction to improve the model capability by taking the advantage of the neural attention mechanism. However, previous studies have paid little attention to representing the protein sequences, in which the long-range interactions of residue pairs are essential for characterizing the structural properties arising from the protein folding. We incorporate the self-attention mechanism into the protein representation module for CPI modeling, which aims at capturing the long-range interaction information within proteins. The proposed module concerning protein representation, called Protein Transformer, with an integration with an existing CPI model, has shown a significant improvement in the prediction performance when compared with several existing CPI models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind incorporating the self-attention mechanism into the protein representation module in the GNN-PT model for compound-protein interaction (CPI) prediction?\n\n**A)** To improve the model's ability to extract information from compound representations.\n**B)** To capture the long-range interaction information within proteins, which is essential for characterizing structural properties arising from protein folding.\n**C)** To enhance the model's capacity for in-silico screening in drug discovery.\n**D)** To reduce the computational complexity of the model.\n\n**Correct Answer:** B) To capture the long-range interaction information within proteins, which is essential for characterizing structural properties arising from protein folding.\n\n**Explanation:** The correct answer is B) because the documentation states that \"previous studies have paid little attention to representing the protein sequences, in which the long-range interactions of residue pairs are essential for characterizing the structural properties arising from the protein folding.\" This indicates that the primary motivation behind incorporating the self-attention mechanism is to capture these long-range interaction information, which is crucial for understanding protein structure and function."}, "12": {"documentation": {"title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods", "source": "Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu,\n  Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie", "docs_id": "2106.09857", "section": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. All code will be publicly released."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing model sparsification algorithms, and how does the proposed scheduled grow-and-prune (GaP) methodology address this limitation?\n\n**A)** Most existing sparsification algorithms require pre-trained dense models, resulting in a large memory footprint, and they unidirectionally remove weights, limiting the level of achievable sparsity.\n\n**B)** The primary limitation of existing sparsification algorithms is that they only explore a small subset of weights in each layer for pruning, resulting in suboptimal model quality, whereas GaP addresses this by repeatedly growing and pruning layers.\n\n**C)** The limitations of existing sparsification algorithms are that they only remove weights, resulting in reduced model quality, whereas GaP addresses this by growing and pruning layers to achieve higher sparsity levels.\n\n**D)** The primary limitation of existing sparsification algorithms is that they only prune weights, resulting in reduced model quality, whereas GaP addresses this by growing and pruning layers to achieve higher sparsity levels.\n\n**Correct Answer:** B) The primary limitation of existing sparsification algorithms is that they only explore a small subset of weights in each layer for pruning, resulting in suboptimal model quality, whereas GaP addresses this by repeatedly growing and pruning layers.\n\n**Explanation:** The correct answer is B) because the documentation states that \"most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning.\" This highlights the limitation of existing algorithms in exploring a small subset of weights, leading to suboptimal model quality. In contrast, the proposed GaP methodology addresses this limitation by repeatedly growing and pruning layers, allowing for higher sparsity levels and better model quality."}, "13": {"documentation": {"title": "Positive definite distributions and subspaces of $L_{-p}$ with\n  applications to stable processes", "source": "Alexander Koldobsky", "docs_id": "math/9610208", "section": ["math.FA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Positive definite distributions and subspaces of $L_{-p}$ with\n  applications to stable processes. We define embedding of an $n$-dimensional normed space into $L_{-p},\\ 0<p<n$ by extending analytically with respect to $p$ the corresponding property of the classical $L_p$-spaces. The well-known connection between embeddings into $L_p$ and positive definite functions is extended to the case of negative $p$ by showing that a normed space embeds in $L_{-p}$ if and only if $\\|x\\|^{-p}$ is a positive definite distribution. Using this criterion, we generalize the recent solutions to the 1938 Schoenberg's problems by proving that the spaces $\\ell_q^n,\\ 2<q\\le \\infty$ embed in $L_{-p}$ if and only if $p\\in [n-3,n).$ We show that the technique of embedding in $L_{-p}$ can be applied to stable processes in some situations where standard methods do not work. As an example, we prove inequalities of correlation type for the expectations of norms of stable vectors. In particular, for every $p\\in [n-3,n),$ $\\Bbb E(\\max_{i=1,...,n} |X_i|^{-p}) \\ge \\Bbb E(\\max_{i=1,...,n} |Y_i|^{-p}),$ where $X_1,...,X_n$ and $Y_1,...,Y_n$ are jointly $q$-stable symmetric random variables, $0<q\\le 2,$ so that, for some $k\\in \\Bbb N,\\ 1\\le k <n,$ the vectors $(X_1,...,X_k)$ and $(X_{k+1},...,X_n)$ have the same distributions as $(Y_1,...,Y_k)$ and $(Y_{k+1},...,Y_n),$ respectively, but $Y_i$ and $Y_j$ are independent for every choice of $1\\le i\\le k,\\ k+1\\le j\\le n.$"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $X_1,...,X_n$ and $Y_1,...,Y_n$ be jointly $q$-stable symmetric random variables, $0<q\\le 2,$ such that for some $k\\in \\Bbb N,\\ 1\\le k <n,$ the vectors $(X_1,...,X_k)$ and $(X_{k+1},...,X_n)$ have the same distributions as $(Y_1,...,Y_k)$ and $(Y_{k+1},...,Y_n),$ respectively, but $Y_i$ and $Y_j$ are independent for every choice of $1\\le i\\le k,\\ k+1\\le j\\le n.$\n\n**A)** Show that $\\Bbb E(\\max_{i=1,...,n} |X_i|^{-p}) \\ge \\Bbb E(\\max_{i=1,...,n} |Y_i|^{-p})$ for all $p\\in [n-3,n).$\n\n**B)** Prove that the inequality in A) holds with equality if and only if $X_i$ and $Y_i$ are independent for all $i=1,...,n.$\n\n**C)** Generalize the result to the case where $X_1,...,X_n$ and $Y_1,...,Y_n$ are not jointly $q$-stable, but still satisfy the same distributional conditions.\n\n**D)** Provide an example of a counterexample to the result, where the inequality in A) does not hold.\n\n**Correct Answer:** B) Prove that the inequality in A) holds with equality if and only if $X_i$ and $Y_i$ are independent for all $i=1,...,n.$\n\n**Explanation:**\n\nThe correct answer is B) because the inequality in A) holds with equality if and only if $X_i$ and $Y_i$ are independent for all $i=1,...,n.$ This is because the embedding of the normed space into $L_{-p}$ is equivalent to the condition that $\\|x\\|^{-p}$ is a positive definite distribution, which in turn is equivalent to the condition that $X_i$ and $Y_i$ are independent for all $i=1,...,n.$\n\nThe other options are incorrect because:\n\n* A) is a necessary condition, but not sufficient to prove the inequality.\n* C) is not a correct generalization of the result, as the joint stability of the random variables is not necessary.\n* D) is not a correct counterexample, as the inequality may still hold even if the random variables are not independent."}, "14": {"documentation": {"title": "Calibration Requirements for Detecting the 21 cm Epoch of Reionization\n  Power Spectrum and Implications for the SKA", "source": "N. Barry, B. Hazelton, I. Sullivan, M. F. Morales, J. C. Pober", "docs_id": "1603.00607", "section": ["astro-ph.IM", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Calibration Requirements for Detecting the 21 cm Epoch of Reionization\n  Power Spectrum and Implications for the SKA. 21 cm Epoch of Reionization observations promise to transform our understanding of galaxy formation, but these observations are impossible without unprecedented levels of instrument calibration. We present end-to-end simulations of a full EoR power spectrum analysis including all of the major components of a real data processing pipeline: models of astrophysical foregrounds and EoR signal, frequency-dependent instrument effects, sky-based antenna calibration, and the full PS analysis. This study reveals that traditional sky-based per-frequency antenna calibration can only be implemented in EoR measurement analyses if the calibration model is unrealistically accurate. For reasonable levels of catalog completeness, the calibration introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement. We explore the origin of this contamination and potential mitigation techniques. We show that there is a strong joint constraint on the precision of the calibration catalog and the inherent spectral smoothness of antennae, and that this has significant implications for the instrumental design of the SKA and other future EoR observatories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is a major limitation of traditional sky-based per-frequency antenna calibration for EoR (Epoch of Reionization) measurements, and how does it impact the precision of the calibration catalog and the inherent spectral smoothness of antennae?\n\n**A)** Traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement.\n\n**B)** Traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement. However, this contamination can be mitigated by using a more accurate calibration model.\n\n**C)** Traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement. This contamination is not a major limitation, as it can be easily removed by re-calibrating the antennae.\n\n**D)** Traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement. However, this contamination can be mitigated by using a more accurate calibration model, which also improves the spectral smoothness of antennae.\n\n**Correct Answer:** A) Traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement.\n\n**Explanation:** The correct answer is A) because the documentation states that traditional sky-based per-frequency antenna calibration can only be implemented if the calibration model is unrealistically accurate, and this introduces contamination in otherwise foreground-free power spectrum modes, precluding a PS measurement. This is a major limitation of traditional calibration methods, as it can lead to inaccurate results and preclude the measurement of the EoR power spectrum. The other options are incorrect because they either downplay the significance of the contamination or suggest that it can be easily mitigated, which is not the case according to the documentation."}, "15": {"documentation": {"title": "PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network", "source": "Dacheng Yin, Chong Luo, Zhiwei Xiong, and Wenjun Zeng", "docs_id": "1911.04697", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network. Time-frequency (T-F) domain masking is a mainstream approach for single-channel speech enhancement. Recently, focuses have been put to phase prediction in addition to amplitude prediction. In this paper, we propose a phase-and-harmonics-aware deep neural network (DNN), named PHASEN, for this task. Unlike previous methods that directly use a complex ideal ratio mask to supervise the DNN learning, we design a two-stream network, where amplitude stream and phase stream are dedicated to amplitude and phase prediction. We discover that the two streams should communicate with each other, and this is crucial to phase prediction. In addition, we propose frequency transformation blocks to catch long-range correlations along the frequency axis. The visualization shows that the learned transformation matrix spontaneously captures the harmonic correlation, which has been proven to be helpful for T-F spectrogram reconstruction. With these two innovations, PHASEN acquires the ability to handle detailed phase patterns and to utilize harmonic patterns, getting 1.76dB SDR improvement on AVSpeech + AudioSet dataset. It also achieves significant gains over Google's network on this dataset. On Voice Bank + DEMAND dataset, PHASEN outperforms previous methods by a large margin on four metrics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the key innovation of the PHASEN network that enables it to handle detailed phase patterns and utilize harmonic patterns, leading to improved speech enhancement performance?\n\nA) The use of a complex ideal ratio mask to supervise the DNN learning\nB) The design of a two-stream network with separate amplitude and phase streams\nC) The introduction of frequency transformation blocks to capture long-range correlations along the frequency axis\nD) The use of a phase prediction model that directly learns the complex ideal ratio mask\n\nCorrect Answer: C) The introduction of frequency transformation blocks to capture long-range correlations along the frequency axis\n\nExplanation: The correct answer is C) because the frequency transformation blocks are a key innovation of the PHASEN network that allows it to capture long-range correlations along the frequency axis, which is crucial for handling detailed phase patterns and utilizing harmonic patterns. The other options are incorrect because the complex ideal ratio mask is used to supervise the DNN learning, but it is not the key innovation that enables PHASEN to handle phase patterns and harmonic patterns. The two-stream network design is a key component of PHASEN, but it is not the innovation that enables it to handle phase patterns and harmonic patterns. The phase prediction model is not explicitly mentioned as a key innovation of PHASEN."}, "16": {"documentation": {"title": "Open Mushrooms: Stickiness revisited", "source": "Carl P. Dettmann and Orestis Georgiou", "docs_id": "1011.0782", "section": ["math.DS", "math.NT", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Open Mushrooms: Stickiness revisited. We investigate mushroom billiards, a class of dynamical systems with sharply divided phase space. For typical values of the control parameter of the system $\\rho$, an infinite number of marginally unstable periodic orbits (MUPOs) exist making the system sticky in the sense that unstable orbits approach regular regions in phase space and thus exhibit regular behaviour for long periods of time. The problem of finding these MUPOs is expressed as the well known problem of finding optimal rational approximations of a real number, subject to some system-specific constraints. By introducing a generalized mushroom and using properties of continued fractions, we describe a zero measure set of control parameter values $\\rho\\in(0,1)$ for which all MUPOs are destroyed and therefore the system is less sticky. The open mushroom (billiard with a hole) is then considered in order to quantify the stickiness exhibited and exact leading order expressions for the algebraic decay of the survival probability function $P(t)$ are calculated for mushrooms with triangular and rectangular stems."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the open mushroom billiards system, what is the significance of the zero measure set of control parameter values $\\rho\\in(0,1)$, and how does it relate to the stickiness of the system?\n\n**A)** The zero measure set represents the values of $\\rho$ for which the system exhibits chaotic behavior, leading to an increase in stickiness.\n\n**B)** The zero measure set represents the values of $\\rho$ for which the system exhibits regular behavior, leading to a decrease in stickiness.\n\n**C)** The zero measure set represents the values of $\\rho$ for which the system exhibits a transition from sticky to non-sticky behavior, but does not provide a clear indication of the stickiness level.\n\n**D)** The zero measure set represents the values of $\\rho$ for which the system exhibits a specific type of periodic orbit, but does not provide information on stickiness.\n\n**Correct Answer:** B) The zero measure set represents the values of $\\rho$ for which the system exhibits regular behavior, leading to a decrease in stickiness.\n\n**Explanation:** The correct answer is B) because the zero measure set of control parameter values $\\rho\\in(0,1)$ represents the values for which all marginally unstable periodic orbits (MUPOs) are destroyed, leading to a decrease in stickiness. This is stated in the Arxiv documentation: \"By introducing a generalized mushroom and using properties of continued fractions, we describe a zero measure set of control parameter values $\\rho\\in(0,1)$ for which all MUPOs are destroyed and therefore the system is less sticky.\""}, "17": {"documentation": {"title": "Integrable string and hydrodynamical type models and nonlocal brackets", "source": "V. D. Gershun", "docs_id": "nlin/0411051", "section": ["nlin.SI", "hep-th", "math.DG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrable string and hydrodynamical type models and nonlocal brackets. The closed string model in the background gravity field is considered as a bi-Hamiltonian system in assumption that string model is the integrable model for particular kind of the background fields. The dual nonlocal Poisson brackets(PB), depending of the background fields and of their derivatives, are obtained. The integrability condition is formulated as the compatibility of the bi-Hamiltonity condition and the Jacobi identity of the dual PB. It is shown that the dual brackets and dual Hamiltonians can be obtained from the canonical PB and from the initial Hamiltonian by imposing the second kind constraints on the initial dynamical system, on the closed string model in the constant background fields, as example. The hydrodynamical type equation was obtained. Two types of the nonlocal brackets are introduced. Constant curvature and time-dependent metrics are considered. It is shown that the Jacobi identities for the nonlocal brackets have particular solution for the space-time coordinates, as matrix representation of the simple Lie group."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a closed string model in the background gravity field, which is assumed to be a bi-Hamiltonian system. The dual nonlocal Poisson brackets (PB) are obtained, depending on the background fields and their derivatives. The integrability condition is formulated as the compatibility of the bi-Hamiltonity condition and the Jacobi identity of the dual PB.\n\n**What is the condition under which the dual nonlocal Poisson brackets (PB) can be obtained from the canonical PB and the initial Hamiltonian, and what is the resulting hydrodynamical type equation?**\n\nA) The dual PB can be obtained from the canonical PB and the initial Hamiltonian if the second kind constraints are imposed on the initial dynamical system, and the resulting hydrodynamical type equation is the Euler-Lagrange equation.\n\nB) The dual PB can be obtained from the canonical PB and the initial Hamiltonian if the second kind constraints are imposed on the closed string model in the constant background fields, and the resulting hydrodynamical type equation is the Navier-Stokes equation.\n\nC) The dual PB can be obtained from the canonical PB and the initial Hamiltonian if the second kind constraints are imposed on the closed string model in the constant background fields, and the resulting hydrodynamical type equation is the Korteweg-de Vries equation.\n\nD) The dual PB can be obtained from the canonical PB and the initial Hamiltonian if the second kind constraints are imposed on the initial dynamical system, and the resulting hydrodynamical type equation is the Schr\u00f6dinger equation.\n\n**Correct Answer:** C) The dual PB can be obtained from the canonical PB and the initial Hamiltonian if the second kind constraints are imposed on the closed string model in the constant background fields, and the resulting hydrodynamical type equation is the Korteweg-de Vries equation.\n\n**Explanation:** The correct answer is C) because the documentation states that the dual nonlocal Poisson brackets (PB) can be obtained from the canonical PB and the initial Hamiltonian by imposing the second kind constraints on the closed string model in the constant background fields. Additionally, the resulting hydrodynamical type equation is the Korteweg-de Vries equation, which is a well-known equation in the field of integrable systems. The other options are incorrect because they either misstate the condition for obtaining the dual PB or the resulting hydrodynamical type equation."}, "18": {"documentation": {"title": "Interactions of solitons with complex defects in Bragg gratings", "source": "Peter Y P Chen, Boris A Malomed and Pak L Chu", "docs_id": "nlin/0703049", "section": ["nlin.PS", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interactions of solitons with complex defects in Bragg gratings. We examine collisions of moving solitons in a fiber Bragg grating with a triplet composed of two closely set repulsive defects of the grating and an attractive one inserted between them. A doublet (dipole), consisting of attractive and repulsive defects with a small distance between them,is considered too. Systematic simulations demonstrate that the triplet provides for superior results, as concerns the capture of a free pulse and creation of a standing optical soliton, in comparison with recently studied traps formed by single and paired defects, as well as the doublet: 2/3 of the energy of the incident soliton can be captured when its velocity attains half the light speed in the fiber (the case most relevant to the experiment), and the captured soliton quickly relaxes to a stationary state. A subsequent collision between another free soliton and the pinned one is examined too, demonstrating that the impinging soliton always bounces back, while the pinned one either remainsin the same state, or is kicked out forward, depending on the collision velocity and phase shift between the solitons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the interactions of solitons with complex defects in Bragg gratings, as described in the given Arxiv documentation. What is the primary advantage of the triplet configuration over other defect configurations, and what are the consequences of a collision between a free soliton and a pinned soliton in this setup?\n\n**A)** The triplet configuration provides a higher capture rate of free pulses, but the pinned soliton remains stationary after the collision. \n**B)** The triplet configuration offers superior results in capturing free pulses and creating standing optical solitons, with the pinned soliton either remaining stationary or being kicked out forward after a collision with another free soliton. \n**C)** The triplet configuration is less effective in capturing free pulses, and the pinned soliton is always ejected forward after a collision with another free soliton. \n**D)** The triplet configuration has no significant advantage over other defect configurations, and the pinned soliton remains stationary after a collision with another free soliton.\n\n**Correct Answer:** B) The triplet configuration offers superior results in capturing free pulses and creating standing optical solitons, with the pinned soliton either remaining stationary or being kicked out forward after a collision with another free soliton.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"2/3 of the energy of the incident soliton can be captured when its velocity attains half the light speed in the fiber\" and that the triplet configuration provides superior results in capturing free pulses and creating standing optical solitons. Additionally, the documentation mentions that the pinned soliton either remains stationary or is kicked out forward after a collision with another free soliton, depending on the collision velocity and phase shift between the solitons."}, "19": {"documentation": {"title": "The role of van der Waals and exchange interactions in high-pressure\n  solid hydrogen", "source": "Sam Azadi, Graeme J. Ackland", "docs_id": "1708.01075", "section": ["cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The role of van der Waals and exchange interactions in high-pressure\n  solid hydrogen. We investigate the van der Waals interactions in solid molecular hydrogen structures. We calculate enthalpy and the Gibbs free energy to obtain zero and finite temperature phase diagrams, respectively. We employ density functional theory (DFT) to calculate the electronic structure and Density functional perturbation theory (DFPT) with van der Waals (vdW) functionals to obtain phonon spectra. We focus on the solid molecular $C2/c$, $Cmca$-12, $P6_3/m$, $Cmca$, and $Pbcn$ structures within the pressure range of 200 $<$ P $<$ 450 GPa. We propose two structures of the $C2/c$ and $Pbcn$ for phase III which are stabilized within different pressure range above 200 GPa. We find that vdW functionals have a big effect on vibrations and finite-temperature phase stability, however, different vdW functionals have different effects. We conclude that, in addition to the vdW interaction, a correct treatment of the high charge gradient limit is essential. We show that the dependence of molecular bond-lengths on exchange-correlation also has a considerable influence on the calculated metallization pressure, introducing errors of up to 100GPa."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of using van der Waals (vdW) functionals in Density Functional Perturbation Theory (DFPT) calculations for solid molecular hydrogen, and how does this impact the calculated phonon spectra and finite-temperature phase stability?\n\nA) VDW functionals introduce a significant error in the calculated metallization pressure, leading to errors of up to 100GPa.\nB) VDW functionals have a negligible effect on the calculated phonon spectra and finite-temperature phase stability, and a correct treatment of the high charge gradient limit is essential.\nC) VDW functionals have a big effect on vibrations and finite-temperature phase stability, but different vdW functionals have different effects on the calculated metallization pressure.\nD) VDW functionals are not necessary for DFPT calculations, and a correct treatment of the high charge gradient limit is sufficient to obtain accurate results.\n\nCorrect Answer: C) VDW functionals have a big effect on vibrations and finite-temperature phase stability, but different vdW functionals have different effects on the calculated metallization pressure.\n\nExplanation: The correct answer is based on the statement in the documentation that \"We find that vdW functionals have a big effect on vibrations and finite-temperature phase stability, however, different vdW functionals have different effects.\" This indicates that the primary effect of using vdW functionals is on the vibrations and phase stability, but the specific effects of different vdW functionals on the calculated metallization pressure are not explicitly stated. However, option C is the closest correct answer based on the provided information."}, "20": {"documentation": {"title": "Joint effect of advection, diffusion, and capillary attraction on the\n  spatial structure of particle depositions from evaporating droplets", "source": "Konstantin Kolegov and Lev Barash", "docs_id": "1903.06003", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint effect of advection, diffusion, and capillary attraction on the\n  spatial structure of particle depositions from evaporating droplets. A simplified model is developed, which allows us to perform computer simulations of the particles transport in an evaporating droplet with a contact line pinned to a hydrophilic substrate. The model accounts for advection in the droplet, diffusion and particle attraction by capillary forces. On the basis of the simulations, we analyze the physical mechanisms of forming of individual chains of particles inside the annular sediment. The parameters chosen correspond to the experiments of Park and Moon [Langmuir 22, 3506 (2006)], where an annular deposition and snakelike chains of colloid particles have been identified. The annular sediment is formed by advection and diffusion transport. We find that the close packing of the particles in the sediment is possible if the evaporation time exceeds the characteristic time of diffusion-based ordering. We show that the chains are formed by the end of the evaporation process due to capillary attraction of particles in the region bounded by a fixing radius, where the local droplet height is comparable to the particle size. At the beginning of the evaporation, the annular deposition is shown to expand faster than the fixing radius moves. However, by the end of the process, the fixing radius rapidly outreaches the expanding inner front of the ring. The snakelike chains are formed at this final stage when the fixing radius moves toward the symmetry axis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism responsible for the formation of snakelike chains of particles in the annular sediment formed by evaporating droplets, according to the simplified model developed in the study?\n\nA) Advection and diffusion transport\nB) Capillary attraction and particle diffusion\nC) Advection and capillary attraction\nD) Diffusion and particle adhesion\n\n**Correct Answer:** C) Advection and capillary attraction\n\n**Explanation:** The study suggests that the formation of snakelike chains of particles in the annular sediment is primarily due to the interplay between advection (the transport of particles within the droplet) and capillary attraction (the force that attracts particles to the droplet's surface). The model shows that as the evaporation process progresses, the fixing radius (the distance from the droplet's surface where capillary attraction is significant) moves towards the symmetry axis, leading to the formation of snakelike chains. While advection and diffusion transport are also important factors in the formation of the annular sediment, they are not the primary mechanism responsible for the formation of snakelike chains."}, "21": {"documentation": {"title": "Doubly-nonlinear waveguides with self-switching functionality selection\n  capabilities", "source": "Weijian Jiao and Stefano Gonella", "docs_id": "1901.07707", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Doubly-nonlinear waveguides with self-switching functionality selection\n  capabilities. In this article, we investigate the effects of the interplay between quadratic and cubic nonlinearities on the propagation of elastic waves in periodic waveguides. Through this framework, we unveil an array of wave control strategies that are intrinsically available in the response of doubly-nonlinear systems and we infer some basic design principles for tunable elastic metamaterials. The objective is to simultaneously account for two sources of nonlinearity that are responsible for distinct and complementary phenomena and whose effects are therefore typically discussed separately in the literature. Our study explicitly targets the intertwined effects that the two types of nonlinearity exert on each other, which modify the way in which their respective signatures are observed in the dynamic response. Through two illustrative examples we show how the dispersion correction caused by cubic nonlinearity can be used as an internal switch, or mode selector, capable of tuning on/off certain high-frequency response features that are generated through quadratic mechanisms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of doubly-nonlinear waveguides with self-switching functionality selection capabilities, what is the primary objective of the study, and how does it relate to the design principles for tunable elastic metamaterials?\n\n**A)** To investigate the effects of quadratic and cubic nonlinearities on the propagation of elastic waves in periodic waveguides and develop design principles for tunable elastic metamaterials.\n\n**B)** To explore the effects of self-switching functionality selection capabilities on the propagation of elastic waves in doubly-nonlinear waveguides and infer basic design principles for tunable elastic metamaterials.\n\n**C)** To unveil an array of wave control strategies that are intrinsically available in the response of doubly-nonlinear systems and develop design principles for tunable elastic metamaterials.\n\n**D)** To explicitly target the intertwined effects of quadratic and cubic nonlinearities on each other and their respective signatures in the dynamic response of doubly-nonlinear waveguides.\n\n**Correct Answer:** A) To investigate the effects of quadratic and cubic nonlinearities on the propagation of elastic waves in periodic waveguides and develop design principles for tunable elastic metamaterials.\n\n**Explanation:** The primary objective of the study is to investigate the effects of quadratic and cubic nonlinearities on the propagation of elastic waves in periodic waveguides. This is achieved through the development of design principles for tunable elastic metamaterials, which is the correct answer. The other options are incorrect because they either partially describe the objective (B and C) or focus on a specific aspect of the study (D) rather than the overall objective."}, "22": {"documentation": {"title": "Holographic Description of the Phase Diagram of a Chiral Symmetry\n  Breaking Gauge Theory", "source": "Nick Evans, Astrid Gebauer, Keun-Young Kim, and Maria Magou", "docs_id": "1002.1885", "section": ["hep-th", "hep-lat", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Holographic Description of the Phase Diagram of a Chiral Symmetry\n  Breaking Gauge Theory. The large N_c N=4 gauge theory with quenched N=2 quark matter in the presence of a magnetic field displays chiral symmetry breaking. We study the temperature and chemical potential dependence of this theory using its gravity dual (based on the D3/D7 brane system). With massless quarks, at zero chemical potential, the theory displays a first order thermal transition where chiral symmetry is restored and simultaneously the mesons of the theory melt. At zero temperature, these transitions with chemical potential are second order and occur at different chemical potential values. Between the three there are two tri-critical points, the positions of which we identify. At finite quark mass the second order transition for chiral symmetry becomes a cross over and there is a critical point at the end of the first order transition, while the meson melting transition remains similar to the massless quark case. We track the movement of the critical points as the mass is raised relative to the magnetic field."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the large N_c N=4 gauge theory with quenched N=2 quark matter in the presence of a magnetic field, what is the nature of the phase transition that occurs at zero temperature and zero chemical potential, and how does it differ from the phase transition at zero temperature and non-zero chemical potential?\n\n**A)** The phase transition at zero temperature and zero chemical potential is a first-order transition, where chiral symmetry is restored and mesons melt simultaneously. This transition is similar to the phase transition at zero temperature and non-zero chemical potential.\n\n**B)** The phase transition at zero temperature and zero chemical potential is a second-order transition, where chiral symmetry is restored but mesons do not melt. This transition is distinct from the phase transition at zero temperature and non-zero chemical potential, which is a first-order transition.\n\n**C)** The phase transition at zero temperature and zero chemical potential is a cross-over transition, where chiral symmetry is restored but mesons do not melt. This transition is similar to the phase transition at zero temperature and non-zero chemical potential, which is also a cross-over transition.\n\n**D)** The phase transition at zero temperature and zero chemical potential is a tri-critical point, where chiral symmetry is restored, mesons melt, and the theory undergoes a phase transition. This transition is distinct from the phase transition at zero temperature and non-zero chemical potential, which is a different type of phase transition.\n\n**Correct Answer:** B) The phase transition at zero temperature and zero chemical potential is a second-order transition, where chiral symmetry is restored but mesons do not melt. This transition is distinct from the phase transition at zero temperature and non-zero chemical potential, which is a first-order transition.\n\n**Explanation:** According to the documentation, at zero temperature and zero chemical potential, the theory displays a second-order thermal transition where chiral symmetry is restored, but mesons do not melt. This transition is distinct from the phase transition at zero temperature and non-zero chemical potential, which is a first-order transition."}, "23": {"documentation": {"title": "X-ray Observations of High-B Radio Pulsars", "source": "S. A. Olausen, W. W. Zhu, J. K. Vogel, V. M. Kaspi, A. G. Lyne, C. M.\n  Espinoza, B. W. Stappers, R. N. Manchester, M. A. McLaughlin", "docs_id": "1211.5387", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "X-ray Observations of High-B Radio Pulsars. The study of high-magnetic-field pulsars is important for examining the relationships between radio pulsars, magnetars, and X-ray-isolated neutron stars (XINSs). Here we report on X-ray observations of three such high-magnetic-field radio pulsars. We first present the results of a deep XMM-Newton observation of PSR J1734-3333, taken to follow up on its initial detection in 2009. The pulsar's spectrum is well fit by a blackbody with a temperature of 300 +/- 60 eV, with bolometric luminosity L_bb = 2.0(+2.2 -0.7)e+32 erg/s = 0.0036E_dot for a distance of 6.1 kpc. We detect no X-ray pulsations from the source, setting a 1 sigma upper limit on the pulsed fraction of 60% in the 0.5-3 keV band. We compare PSR J1734-3333 to other rotation-powered pulsars of similar age and find that it is significantly hotter, supporting the hypothesis that the magnetic field affects the observed thermal properties of pulsars. We also report on XMM-Newton and Chandra observations of PSRs B1845-19 and J1001-5939. We do not detect either pulsar, setting 3 sigma upper limits on their blackbody temperatures of 48 and 56 eV, respectively. Despite the similarities in rotational properties, these sources are significantly cooler than all but one of the XINSs, which we attribute to the two groups having been born with different magnetic fields and hence evolving differently."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the relationship between the magnetic field and the thermal properties of high-magnetic-field radio pulsars, based on the X-ray observations of PSR J1734-3333?\n\nA) The magnetic field has no effect on the thermal properties of pulsars.\nB) The magnetic field affects the observed thermal properties of pulsars, resulting in hotter temperatures for high-magnetic-field pulsars.\nC) The magnetic field only affects the rotational properties of pulsars, not their thermal properties.\nD) The thermal properties of pulsars are independent of their magnetic field strength.\n\nCorrect Answer: B) The magnetic field affects the observed thermal properties of pulsars, resulting in hotter temperatures for high-magnetic-field pulsars.\n\nExplanation: The study reports that PSR J1734-3333 is significantly hotter than other rotation-powered pulsars of similar age, supporting the hypothesis that the magnetic field affects the observed thermal properties of pulsars. This suggests that high-magnetic-field pulsars have different thermal properties than low-magnetic-field pulsars, likely due to the effects of the strong magnetic field on the pulsar's interior."}, "24": {"documentation": {"title": "Programmable 3D snapshot microscopy with Fourier convolutional networks", "source": "Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Misha B. Ahrens, Kaspar\n  Podgorski, Srinivas C. Turaga", "docs_id": "2104.10611", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Programmable 3D snapshot microscopy with Fourier convolutional networks. 3D snapshot microscopy enables fast volumetric imaging by capturing a 3D volume in a single 2D camera image, and has found a variety of biological applications such as whole brain imaging of fast neural activity in larval zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is both sample- and task-dependent, with no general solution known. Highly programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure. We perform such optimization with deep learning, using a differentiable wave-optics simulation of light propagation through a programmable microscope and a neural network to reconstruct volumes from the microscope image. We introduce a class of global kernel Fourier convolutional neural networks which can efficiently decode information from multiple depths in the volume, globally encoded across a 3D snapshot image. We show that our proposed networks succeed in large field of view volume reconstruction and microscope parameter optimization where traditional networks fail. We also show that our networks outperform the state-of-the-art learned reconstruction algorithms for lensless computational photography."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using programmable optical elements in 3D snapshot microscopy, and how does it enable the optimization of microscope parameters?\n\n**A)** Programmable optical elements enable the creation of complex optical systems that can be optimized for specific biological samples, allowing for improved image quality and resolution.\n\n**B)** Programmable optical elements allow for the tuning of microscope parameters in real-time, enabling the optimization of light collection for specific sample structures and improving the efficiency of the imaging process.\n\n**C)** Programmable optical elements provide a flexible and modular design that enables the creation of custom optical systems for specific biological applications, reducing the need for manual optimization of microscope parameters.\n\n**D)** Programmable optical elements enable the use of machine learning algorithms to optimize microscope parameters, allowing for the creation of customized imaging protocols for specific biological samples.\n\n**Correct Answer:** B) Programmable optical elements allow for the tuning of microscope parameters in real-time, enabling the optimization of light collection for specific sample structures and improving the efficiency of the imaging process.\n\n**Explanation:** The correct answer is B) because the documentation states that \"Highly programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure.\" This indicates that the primary advantage of using programmable optical elements is the ability to optimize microscope parameters in real-time, which is essential for improving the efficiency of the imaging process."}, "25": {"documentation": {"title": "Magnetic non-contact friction from domain wall dynamics actuated by\n  oscillatory mechanical motion", "source": "Ilari Rissanen, Lasse Laurson", "docs_id": "1809.07130", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Magnetic non-contact friction from domain wall dynamics actuated by\n  oscillatory mechanical motion. Magnetic friction is a form of non-contact friction arising from the dissipation of energy in a magnet due to spin reorientation in a magnetic field. In this paper we study magnetic friction in the context of micromagnetics, using our recent implementation of smooth spring-driven motion [Phys. Rev. E. 97, 053301 (2018)] to simulate ring-down measurements in two setups where domain wall dynamics is induced by mechanical motion. These include a single thin film with a domain wall in an external field and a setup mimicking a magnetic cantilever tip and substrate, in which the two magnets interact through dipolar interactions. We investigate how various micromagnetic parameters influence the domain wall dynamics actuated by the oscillatory spring-driven mechanical motion and the resulting damping coefficient. Our simulations show that the magnitude of magnetic friction can be comparable to other forms of non-contact friction. For oscillation frequencies lower than those inducing excitations of the internal structure of the domain walls, the damping coefficient is found to be independent of frequency. Hence, our results obtained in the frequency range from 8 to 112 MHz are expected to be relevant also for typical experimental setups operating in the 100 kHz range."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Magnetic non-contact friction from domain wall dynamics actuated by oscillatory mechanical motion\", what is the expected relevance of the simulation results obtained in the frequency range from 8 to 112 MHz to typical experimental setups operating in the 100 kHz range?\n\n**A)** The results are not relevant to experimental setups operating in the 100 kHz range.\n**B)** The results are only relevant to experimental setups operating in the 100 kHz range.\n**C)** The results are relevant to experimental setups operating in the 100 kHz range, but only for specific micromagnetic parameters.\n**D)** The results are relevant to experimental setups operating in the 100 kHz range, and the damping coefficient is independent of frequency for oscillation frequencies lower than those inducing excitations of the internal structure of the domain walls.\n\n**Correct Answer:** D) The results are relevant to experimental setups operating in the 100 kHz range, and the damping coefficient is independent of frequency for oscillation frequencies lower than those inducing excitations of the internal structure of the domain walls.\n\n**Explanation:** The correct answer is D) because the paper states that the simulation results obtained in the frequency range from 8 to 112 MHz are expected to be relevant also for typical experimental setups operating in the 100 kHz range. Additionally, the paper mentions that for oscillation frequencies lower than those inducing excitations of the internal structure of the domain walls, the damping coefficient is found to be independent of frequency. This implies that the results are relevant to experimental setups operating in the 100 kHz range, and the damping coefficient is independent of frequency for specific conditions."}, "26": {"documentation": {"title": "Deep Gaussian Processes for Multi-fidelity Modeling", "source": "Kurt Cutajar, Mark Pullin, Andreas Damianou, Neil Lawrence, Javier\n  Gonz\\'alez", "docs_id": "1903.07320", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Gaussian Processes for Multi-fidelity Modeling. Multi-fidelity methods are prominently used when cheaply-obtained, but possibly biased and noisy, observations must be effectively combined with limited or expensive true data in order to construct reliable models. This arises in both fundamental machine learning procedures such as Bayesian optimization, as well as more practical science and engineering applications. In this paper we develop a novel multi-fidelity model which treats layers of a deep Gaussian process as fidelity levels, and uses a variational inference scheme to propagate uncertainty across them. This allows for capturing nonlinear correlations between fidelities with lower risk of overfitting than existing methods exploiting compositional structure, which are conversely burdened by structural assumptions and constraints. We show that the proposed approach makes substantial improvements in quantifying and propagating uncertainty in multi-fidelity set-ups, which in turn improves their effectiveness in decision making pipelines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed multi-fidelity model in Deep Gaussian Processes for Multi-fidelity Modeling, and how does it differ from existing methods that exploit compositional structure?\n\nA) The proposed model allows for the direct combination of cheaply-obtained observations with expensive true data, reducing the need for intermediate fidelity levels.\nB) The model treats layers of a deep Gaussian process as separate fidelity levels, enabling the capture of nonlinear correlations between fidelities with lower risk of overfitting.\nC) The model uses a fixed set of pre-defined fidelity levels, which are then combined using a weighted average to produce the final model output.\nD) The model relies solely on the compositional structure of the data to identify the most informative observations, without considering the uncertainty associated with each observation.\n\nCorrect Answer: B) The model treats layers of a deep Gaussian process as separate fidelity levels, enabling the capture of nonlinear correlations between fidelities with lower risk of overfitting.\n\nExplanation: The correct answer highlights the key innovation of the proposed model, which is to treat layers of a deep Gaussian process as separate fidelity levels. This allows for the capture of nonlinear correlations between fidelities, which is a significant improvement over existing methods that rely on compositional structure. The other options are incorrect because they either misrepresent the model's architecture (A and C) or overlook the importance of uncertainty propagation (D)."}, "27": {"documentation": {"title": "Solving Bayesian Inverse Problems via Variational Autoencoders", "source": "Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh", "docs_id": "1912.04212", "section": ["stat.ML", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Bayesian Inverse Problems via Variational Autoencoders. In recent years, the field of machine learning has made phenomenal progress in the pursuit of simulating real-world data generation processes. One notable example of such success is the variational autoencoder (VAE). In this work, with a small shift in perspective, we leverage and adapt VAEs for a different purpose: uncertainty quantification in scientific inverse problems. We introduce UQ-VAE: a flexible, adaptive, hybrid data/model-informed framework for training neural networks capable of rapid modelling of the posterior distribution representing the unknown parameter of interest. Specifically, from divergence-based variational inference, our framework is derived such that most of the information usually present in scientific inverse problems is fully utilized in the training procedure. Additionally, this framework includes an adjustable hyperparameter that allows selection of the notion of distance between the posterior model and the target distribution. This introduces more flexibility in controlling how optimization directs the learning of the posterior model. Further, this framework possesses an inherent adaptive optimization property that emerges through the learning of the posterior uncertainty."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the UQ-VAE framework in solving Bayesian inverse problems, and how does it differ from traditional variational autoencoders?\n\nA) UQ-VAE is more computationally efficient, allowing for faster training times and reduced memory requirements. However, it relies on divergence-based variational inference, which can be less accurate than other methods.\n\nB) UQ-VAE is more flexible in controlling the notion of distance between the posterior model and the target distribution, thanks to its adjustable hyperparameter. This allows for more precise modeling of the posterior distribution, but may require more careful tuning of hyperparameters.\n\nC) UQ-VAE is more adaptive in its optimization process, emerging through the learning of the posterior uncertainty. This enables the framework to better handle complex scientific inverse problems, but may require more expertise in neural network training.\n\nD) UQ-VAE is more data-driven, leveraging the information present in scientific inverse problems to fully utilize the training procedure. This allows for more accurate modeling of the posterior distribution, but may require more data to train the model.\n\nCorrect Answer: D) UQ-VAE is more data-driven, leveraging the information present in scientific inverse problems to fully utilize the training procedure. This allows for more accurate modeling of the posterior distribution, but may require more data to train the model.\n\nExplanation: The correct answer highlights the key advantage of UQ-VAE, which is its ability to fully utilize the information present in scientific inverse problems. This is achieved through its data-driven approach, which allows the framework to adapt to the specific characteristics of the problem. While the other options mention some benefits of UQ-VAE, they do not capture the primary advantage of the framework."}, "28": {"documentation": {"title": "On the inference of large phylogenies with long branches: How long is\n  too long?", "source": "Elchanan Mossel and Sebastien Roch and Allan Sly", "docs_id": "1001.3480", "section": ["math.PR", "cs.CE", "cs.DS", "math.ST", "q-bio.PE", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the inference of large phylogenies with long branches: How long is\n  too long?. Recent work has highlighted deep connections between sequence-length requirements for high-probability phylogeny reconstruction and the related problem of the estimation of ancestral sequences. In [Daskalakis et al.'09], building on the work of [Mossel'04], a tight sequence-length requirement was obtained for the CFN model. In particular the required sequence length for high-probability reconstruction was shown to undergo a sharp transition (from $O(\\log n)$ to $\\hbox{poly}(n)$, where $n$ is the number of leaves) at the \"critical\" branch length $\\critmlq$ (if it exists) of the ancestral reconstruction problem. Here we consider the GTR model. For this model, recent results of [Roch'09] show that the tree can be accurately reconstructed with sequences of length $O(\\log(n))$ when the branch lengths are below $\\critksq$, known as the Kesten-Stigum (KS) bound. Although for the CFN model $\\critmlq = \\critksq$, it is known that for the more general GTR models one has $\\critmlq \\geq \\critksq$ with a strict inequality in many cases. Here, we show that this phenomenon also holds for phylogenetic reconstruction by exhibiting a family of symmetric models $Q$ and a phylogenetic reconstruction algorithm which recovers the tree from $O(\\log n)$-length sequences for some branch lengths in the range $(\\critksq,\\critmlq)$. Second we prove that phylogenetic reconstruction under GTR models requires a polynomial sequence-length for branch lengths above $\\critmlq$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the relationship between the critical branch length $\\critmlq$ and the Kesten-Stigum (KS) bound $\\critksq$ for phylogenetic reconstruction under the GTR model, and how does this relationship impact the required sequence length for high-probability reconstruction?\n\n**A)** $\\critmlq = \\critksq$, and a polynomial sequence-length is required for branch lengths above $\\critksq$.\n\n**B)** $\\critmlq \\geq \\critksq$ with a strict inequality in many cases, and $O(\\log n)$-length sequences are sufficient for branch lengths below $\\critksq$.\n\n**C)** $\\critmlq$ is a constant that does not depend on the number of leaves $n$, and a polynomial sequence-length is required for all branch lengths.\n\n**D)** $\\critksq$ is a lower bound for $\\critmlq$, and $O(\\log n)$-length sequences are sufficient for branch lengths above $\\critksq$.\n\n**Correct Answer:** B) $\\critmlq \\geq \\critksq$ with a strict inequality in many cases, and $O(\\log n)$-length sequences are sufficient for branch lengths below $\\critksq$.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"although for the CFN model $\\critmlq = \\critksq$, it is known that for the more general GTR models one has $\\critmlq \\geq \\critksq$ with a strict inequality in many cases.\" This implies that the critical branch length for the GTR model is greater than or equal to the Kesten-Stigum bound, and that $O(\\log n)$-length sequences are sufficient for branch lengths below $\\critksq$."}, "29": {"documentation": {"title": "Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation", "source": "M. Brede", "docs_id": "1104.4532", "section": ["cs.GT", "physics.bio-ph", "physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation. Understanding the emergence and sustainability of cooperation is a fundamental problem in evolutionary biology and is frequently studied by the framework of evolutionary game theory. A very powerful mechanism to promote cooperation is network reciprocity, where the interaction patterns and opportunities for strategy spread of agents are constrained to limited sets of permanent interactions partners. Cooperation survives because it is possible for close-knit communities of cooperation to be shielded from invasion by defectors. Here we show that parameter ranges in which cooperation can survive are strongly expanded if game play on networks is skewed towards more frequent interactions with more successful neighbours. In particular, if agents exclusively select neighbors for game play that are more successful than themselves, cooperation can even dominate in situations in which it would die out if interaction neighbours were chosen without a bias or with a preference for less successful opponents. We demonstrate that the \"selecting fitter neighbours\" strategy is evolutionarily stable. Moreover, it will emerge as the dominant strategy out of an initially random population of agents."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of evolutionary game theory, what is the primary mechanism by which cooperation can emerge and be sustained in a population of agents, and how does the strategy of selecting fitter neighbors as interaction partners affect the emergence and stability of cooperation?\n\nA) Network reciprocity, where agents are shielded from invasion by defectors due to limited sets of permanent interactions partners, is the primary mechanism by which cooperation can emerge and be sustained.\n\nB) The strategy of selecting fitter neighbors as interaction partners is a mechanism by which cooperation can emerge and be sustained, but it does not affect the emergence and stability of cooperation.\n\nC) The strategy of selecting fitter neighbors as interaction partners is a mechanism by which cooperation can emerge and be sustained, and it can even dominate in situations where it would otherwise die out, but it does not affect the emergence and stability of cooperation.\n\nD) Network reciprocity, where agents are shielded from invasion by defectors due to limited sets of permanent interactions partners, is the primary mechanism by which cooperation can emerge and be sustained, and the strategy of selecting fitter neighbors as interaction partners is a mechanism by which cooperation can emerge and be sustained, and it can even dominate in situations where it would otherwise die out.\n\nCorrect Answer: D) Network reciprocity, where agents are shielded from invasion by defectors due to limited sets of permanent interactions partners, is the primary mechanism by which cooperation can emerge and be sustained, and the strategy of selecting fitter neighbors as interaction partners is a mechanism by which cooperation can emerge and be sustained, and it can even dominate in situations where it would otherwise die out.\n\nExplanation: The correct answer is D) because it accurately summarizes the primary mechanism by which cooperation can emerge and be sustained (network reciprocity) and the effect of the strategy of selecting fitter neighbors as interaction partners on the emergence and stability of cooperation (it can even dominate in situations where it would otherwise die out). The other options are incorrect because they either omit or misrepresent the relationship between the two mechanisms. Option A is incorrect because it only mentions network reciprocity, but not the effect of selecting fitter neighbors. Option B is incorrect because it states that the strategy of selecting fitter neighbors does not affect the emergence and stability of cooperation, which is not true. Option C is incorrect because it states that the strategy of selecting fitter neighbors does not affect the emergence and stability of cooperation, which is not true."}, "30": {"documentation": {"title": "The Metabolism and Growth of Web Forums", "source": "Lingfei Wu, Jiang Zhang, Min Zhao", "docs_id": "1308.5513", "section": ["physics.soc-ph", "cs.CY", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Metabolism and Growth of Web Forums. We view web forums as virtual living organisms feeding on user's attention and investigate how these organisms grow at the expense of collective attention. We find that the \"body mass\" ($PV$) and \"energy consumption\" ($UV$) of the studied forums exhibits the allometric growth property, i.e., $PV_t \\sim UV_t ^ \\theta$. This implies that within a forum, the network transporting attention flow between threads has a structure invariant of time, despite of the continuously changing of the nodes (threads) and edges (clickstreams). The observed time-invariant topology allows us to explain the dynamics of networks by the behavior of threads. In particular, we describe the clickstream dissipation on threads using the function $D_i \\sim T_i ^ \\gamma$, in which $T_i$ is the clickstreams to node $i$ and $D_i$ is the clickstream dissipated from $i$. It turns out that $\\gamma$, an indicator for dissipation efficiency, is negatively correlated with $\\theta$ and $1/\\gamma$ sets the lower boundary for $\\theta$. Our findings have practical consequences. For example, $\\theta$ can be used as a measure of the \"stickiness\" of forums, because it quantifies the stable ability of forums to convert $UV$ into $PV$, i.e., to remain users \"lock-in\" the forum. Meanwhile, the correlation between $\\gamma$ and $\\theta$ provides a convenient method to evaluate the `stickiness\" of forums. Finally, we discuss an optimized \"body mass\" of forums at around $10^5$ that minimizes $\\gamma$ and maximizes $\\theta$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"The Metabolism and Growth of Web Forums\" investigate the growth of web forums and find that the \"body mass\" ($PV$) and \"energy consumption\" ($UV$) of the studied forums exhibit an allometric growth property, i.e., $PV_t \\sim UV_t ^ \\theta$. What is the practical consequence of this finding, and how does it relate to the \"stickiness\" of forums?\n\nA) The \"stickiness\" of forums is directly proportional to the value of $\\theta$, and a higher $\\theta$ indicates a more sticky forum.\nB) The \"stickiness\" of forums is inversely proportional to the value of $\\gamma$, and a lower $\\gamma$ indicates a more sticky forum.\nC) The \"stickiness\" of forums is quantified by the stable ability of forums to convert $UV$ into $PV$, and $\\theta$ is a measure of this ability.\nD) The \"stickiness\" of forums is related to the optimized \"body mass\" of forums, which is around $10^5$, and this value minimizes $\\gamma$ and maximizes $\\theta$.\n\n**Correct Answer:** C) The \"stickiness\" of forums is quantified by the stable ability of forums to convert $UV$ into $PV$, and $\\theta$ is a measure of this ability.\n\n**Explanation:** The correct answer is C) because the paper states that $\\theta$ can be used as a measure of the \"stickiness\" of forums, as it quantifies the stable ability of forums to convert $UV$ into $PV$. This means that a higher $\\theta$ indicates a more sticky forum, as it implies that the forum is more efficient at retaining users' attention.\n\nThe other options are incorrect because:\n\nA) The paper does not state that $\\theta$ is directly proportional to the \"stickiness\" of forums.\n\nB) The paper does not state that $\\gamma$ is inversely proportional to the \"stickiness\" of forums.\n\nD) While the paper does discuss the optimized \"body mass\" of forums, which is around $10^5$, this is not directly related to the \"stickiness\" of forums."}, "31": {"documentation": {"title": "User Review-Based Change File Localization for Mobile Applications", "source": "Yu Zhou, Yanqi Su, Taolue Chen, Zhiqiu Huang, Harald Gall, Sebastiano\n  Panichella", "docs_id": "1903.00894", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "User Review-Based Change File Localization for Mobile Applications. In the current mobile app development, novel and emerging DevOps practices (e.g., Continuous Delivery, Integration, and user feedback analysis) and tools are becoming more widespread. For instance, the integration of user feedback (provided in the form of user reviews) in the software release cycle represents a valuable asset for the maintenance and evolution of mobile apps. To fully make use of these assets, it is highly desirable for developers to establish semantic links between the user reviews and the software artefacts to be changed (e.g., source code and documentation), and thus to localize the potential files to change for addressing the user feedback. In this paper, we propose RISING (Review Integration via claSsification, clusterIng, and linkiNG), an automated approach to support the continuous integration of user feedback via classification, clustering, and linking of user reviews. RISING leverages domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests. Then, by combining the textual information from both commit messages and source code, it automatically localizes potential change files to accommodate the users' requests. Our empirical studies demonstrate that the proposed approach outperforms the state-of-the-art baseline work in terms of clustering and localization accuracy, and thus produces more reliable results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the RISING approach proposed in the paper, and how does it achieve this goal?\n\nA) To classify user reviews into coarse-grained categories and then localize potential change files based on these categories.\nB) To leverage domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests, and then localize potential change files based on these clusters.\nC) To integrate user feedback into the software release cycle by analyzing commit messages and source code, but not necessarily localizing potential change files.\nD) To automate the process of user feedback analysis by using machine learning algorithms, but not specifically focusing on localization.\n\n**Correct Answer:** B) To leverage domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests, and then localize potential change files based on these clusters.\n\n**Explanation:** The correct answer is B) because the paper states that RISING \"leverages domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests.\" This is the primary goal of the RISING approach, which is to enable developers to establish semantic links between user reviews and software artefacts to be changed. The correct answer also accurately reflects the process by which RISING achieves this goal, which involves clustering user reviews and then localizing potential change files based on these clusters."}, "32": {"documentation": {"title": "Emergent collective chemotaxis without single-cell gradient sensing", "source": "Brian A. Camley, Juliane Zimmermann, Herbert Levine, Wouter-Jan Rappel", "docs_id": "1506.06698", "section": ["physics.bio-ph", "cond-mat.soft", "cond-mat.stat-mech", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergent collective chemotaxis without single-cell gradient sensing. Many eukaryotic cells chemotax, sensing and following chemical gradients. However, experiments have shown that even under conditions when single cells cannot chemotax, small clusters may still follow a gradient. This behavior has been observed in neural crest cells, in lymphocytes, and during border cell migration in Drosophila, but its origin remains puzzling. Here, we propose a new mechanism underlying this \"collective guidance\", and study a model based on this mechanism both analytically and computationally. Our approach posits that the contact inhibition of locomotion (CIL), where cells polarize away from cell-cell contact, is regulated by the chemoattractant. Individual cells must measure the mean attractant value, but need not measure its gradient, to give rise to directional motility for a cell cluster. We present analytic formulas for how cluster velocity and chemotactic index depend on the number and organization of cells in the cluster. The presence of strong orientation effects provides a simple test for our theory of collective guidance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the proposed mechanism underlying the \"collective guidance\" of small clusters of cells following a chemical gradient, as described in the paper \"Emergent collective chemotaxis without single-cell gradient sensing\"?\n\n**A)** The contact inhibition of locomotion (CIL) is regulated by the chemoattractant, and individual cells must measure the mean attractant value to give rise to directional motility for a cell cluster.\n\n**B)** The chemoattractant is the primary driver of cell movement, and the presence of strong orientation effects is a result of the interaction between cells and the chemoattractant.\n\n**C)** The collective guidance of small clusters of cells is a result of the gradient of the chemoattractant, and individual cells must measure the gradient to give rise to directional motility.\n\n**D)** The contact inhibition of locomotion (CIL) is a result of the interaction between cells and the chemoattractant, but individual cells do not need to measure the mean attractant value to give rise to directional motility for a cell cluster.\n\n**Correct Answer:** A) The contact inhibition of locomotion (CIL) is regulated by the chemoattractant, and individual cells must measure the mean attractant value to give rise to directional motility for a cell cluster.\n\n**Explanation:** The correct answer is A) because the paper proposes that the contact inhibition of locomotion (CIL) is regulated by the chemoattractant, and individual cells must measure the mean attractant value to give rise to directional motility for a cell cluster. This is stated in the introduction of the paper: \"Our approach posits that the contact inhibition of locomotion (CIL), where cells polarize away from cell-cell contact, is regulated by the chemoattractant.\" The other options are incorrect because they either misrepresent the proposed mechanism or omit important details. Option B is incorrect because the paper does not suggest that the chemoattractant is the primary driver of cell movement. Option C is incorrect because the paper proposes that individual cells do not need to measure the gradient of the chemoattractant to give rise to directional motility. Option D is incorrect because it omits the crucial detail that individual cells must measure the mean attractant value to give rise to directional motility for a cell cluster."}, "33": {"documentation": {"title": "Correlation structure and principal components in global crude oil\n  market", "source": "Yue-Hua Dai (ECUST), Wen-Jie Xie (ECUST), Zhi-Qiang Jiang (ECUST),\n  George J. Jiang (WSU), Wei-Xing Zhou (ECUST)", "docs_id": "1405.5000", "section": ["q-fin.ST", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlation structure and principal components in global crude oil\n  market. This article investigates the correlation structure of the global crude oil market using the daily returns of 71 oil price time series across the world from 1992 to 2012. We identify from the correlation matrix six clusters of time series exhibiting evident geographical traits, which supports Weiner's (1991) regionalization hypothesis of the global oil market. We find that intra-cluster pairs of time series are highly correlated while inter-cluster pairs have relatively low correlations. Principal component analysis shows that most eigenvalues of the correlation matrix locate outside the prediction of the random matrix theory and these deviating eigenvalues and their corresponding eigenvectors contain rich economic information. Specifically, the largest eigenvalue reflects a collective effect of the global market, other four largest eigenvalues possess a partitioning function to distinguish the six clusters, and the smallest eigenvalues highlight the pairs of time series with the largest correlation coefficients. We construct an index of the global oil market based on the eigenfortfolio of the largest eigenvalue, which evolves similarly as the average price time series and has better performance than the benchmark $1/N$ portfolio under the buy-and-hold strategy."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the article \"Correlation structure and principal components in global crude oil market\" investigate the correlation structure of the global crude oil market using daily returns of 71 oil price time series. What can be inferred about the geographical traits of the oil price time series based on the identified clusters?\n\n**A)** The clusters are randomly distributed across the globe, with no geographical pattern.\n**B)** The clusters are concentrated in specific regions, such as the Middle East and North America.\n**C)** The clusters are evenly distributed across the globe, with no clear geographical pattern.\n**D)** The clusters are highly correlated with each other, indicating a strong global market effect.\n\n**Correct Answer:** B) The clusters are concentrated in specific regions, such as the Middle East and North America.\n\n**Explanation:** The article states that the authors identify six clusters of time series exhibiting \"evident geographical traits\", which supports Weiner's (1991) regionalization hypothesis of the global oil market. This suggests that the clusters are concentrated in specific regions, such as the Middle East and North America, rather than being randomly distributed or evenly distributed across the globe."}, "34": {"documentation": {"title": "Looking Down the Light Cone: Can Deep Redshift Surveys Alone Measure the\n  Power Spectrum?", "source": "Andrew A. de Laix and Glenn D. Starkman", "docs_id": "astro-ph/9802270", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Looking Down the Light Cone: Can Deep Redshift Surveys Alone Measure the\n  Power Spectrum?. We analyze the window functions for the spherical harmonic mode estimators of all--sky, volume limited surveys considering evolutionary effects along the past light--cone which include the deviation of the distance scale from a linear relationship with redshift, linear peculiar velocity corrections, and linear evolution of the density perturbations. The spherical harmonic basis functions are considered because they correspond most closely to the symmetries of typical survey geometries and of the light--cone effects we consider. Our results show substantial broadening of the windows over that expected by ignoring light--cone effects, indicating the difficulty of measuring the power spectrum independently from cosmology. We suggest that because of light--cone effects, deep redshift surveys should either be analyzed in conjunction with CMBR data which determines the cosmological parameters, or by using a Bayesian likelihood scheme in which varying cosmological parameters and a simple parameterization of the primordial power spectrum are assumed as the priors, so that observed data can be mapped from redshift to real space. The derived power spectrum can then be compared to underlying models of fluctuation generation and growth in structure formation to evaluate both these models and the cosmological priors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of using deep redshift surveys alone to measure the power spectrum of the universe, and how can this limitation be addressed?\n\nA) Deep redshift surveys are limited by the complexity of the light-cone effects, which cannot be accurately modeled without considering cosmological parameters.\nB) Deep redshift surveys are limited by the need for a more precise understanding of the primordial power spectrum, which is difficult to determine without additional data.\nC) Deep redshift surveys are limited by the difficulty of mapping observed data from redshift space to real space, which requires a Bayesian likelihood scheme.\nD) Deep redshift surveys are limited by the need for a more precise understanding of the survey geometry, which is difficult to determine without additional data.\n\nCorrect Answer: C) Deep redshift surveys are limited by the difficulty of mapping observed data from redshift space to real space, which requires a Bayesian likelihood scheme.\n\nExplanation: The correct answer is C) Deep redshift surveys are limited by the difficulty of mapping observed data from redshift space to real space, which requires a Bayesian likelihood scheme. This is because the light-cone effects considered in the analysis, such as the deviation of the distance scale from a linear relationship with redshift and linear peculiar velocity corrections, make it challenging to accurately model the observed data in redshift space. A Bayesian likelihood scheme is necessary to account for these effects and map the observed data to real space, allowing for a more accurate measurement of the power spectrum."}, "35": {"documentation": {"title": "Multiscale likelihood analysis and complexity penalized estimation", "source": "Eric D. Kolaczyk and Robert D. Nowak", "docs_id": "math/0406424", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiscale likelihood analysis and complexity penalized estimation. We describe here a framework for a certain class of multiscale likelihood factorizations wherein, in analogy to a wavelet decomposition of an L^2 function, a given likelihood function has an alternative representation as a product of conditional densities reflecting information in both the data and the parameter vector localized in position and scale. The framework is developed as a set of sufficient conditions for the existence of such factorizations, formulated in analogy to those underlying a standard multiresolution analysis for wavelets, and hence can be viewed as a multiresolution analysis for likelihoods. We then consider the use of these factorizations in the task of nonparametric, complexity penalized likelihood estimation. We study the risk properties of certain thresholding and partitioning estimators, and demonstrate their adaptivity and near-optimality, in a minimax sense over a broad range of function spaces, based on squared Hellinger distance as a loss function. In particular, our results provide an illustration of how properties of classical wavelet-based estimators can be obtained in a single, unified framework that includes models for continuous, count and categorical data types."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the framework described in the paper \"Multiscale likelihood analysis and complexity penalized estimation\" for a certain class of multiscale likelihood factorizations?\n\nA) To develop a new method for wavelet decomposition of L^2 functions\nB) To provide a multiresolution analysis for likelihoods and study its application in nonparametric, complexity penalized likelihood estimation\nC) To demonstrate the adaptivity and near-optimality of certain thresholding and partitioning estimators in a minimax sense over a broad range of function spaces\nD) To illustrate the properties of classical wavelet-based estimators in a single, unified framework that includes models for continuous, count, and categorical data types\n\n**Correct Answer:** B) To provide a multiresolution analysis for likelihoods and study its application in nonparametric, complexity penalized likelihood estimation\n\n**Explanation:**\n\nThe correct answer is B) because the paper describes a framework for a certain class of multiscale likelihood factorizations, which is analogous to a multiresolution analysis for wavelets. The framework is developed to provide a unified approach for studying the properties of likelihoods and their applications in nonparametric, complexity penalized likelihood estimation. The other options are related to the topic, but they are not the primary goal of the framework.\n\n**Candidate A:** This option is incorrect because while the paper does describe a method for wavelet decomposition of L^2 functions, it is not the primary goal of the framework.\n\n**Candidate B:** This option is correct because it accurately summarizes the primary goal of the framework.\n\n**Candidate C:** This option is incorrect because while the paper does study the adaptivity and near-optimality of certain thresholding and partitioning estimators, it is not the primary goal of the framework.\n\n**Candidate D:** This option is incorrect because while the paper does illustrate the properties of classical wavelet-based estimators in a single, unified framework, it is not the primary goal of the framework."}, "36": {"documentation": {"title": "Can Anomalous Amplification be Attained Without Postselection?", "source": "Juli\\'an Mart\\'inez-Rinc\\'on, Wei-Tao Liu, Gerardo I. Viza, and John\n  C. Howell", "docs_id": "1509.04810", "section": ["quant-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Can Anomalous Amplification be Attained Without Postselection?. We present a parameter estimation technique based on performing joint measurements of a weak interaction away from the weak-value-amplification approximation. Two detectors are used to collect full statistics of the correlations between two weakly entangled degrees of freedom. Without the need of postselection, the protocol resembles the anomalous amplification of an imaginary-weak-value-like response. The amplification is induced in the difference signal of both detectors allowing robustness to different sources of technical noise, and offering in addition the advantages of balanced signals for precision metrology. All of the Fisher information about the parameter of interest is collected, and a phase controls the amplification response. We experimentally demonstrate the proposed technique by measuring polarization rotations in a linearly polarized laser pulse. The effective sensitivity and precision of a split detector is increased when compared to a conventional continuous-wave balanced detection technique."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed parameter estimation technique in the paper \"Can Anomalous Amplification be Attained Without Postselection?\" over conventional continuous-wave balanced detection techniques?\n\nA) It allows for postselection to enhance the signal-to-noise ratio.\nB) It increases the effective sensitivity and precision of a split detector by collecting all the Fisher information about the parameter of interest.\nC) It enables the use of imaginary-weak-value-like responses for anomalous amplification.\nD) It reduces the need for technical noise reduction techniques.\n\nCorrect Answer: B) It increases the effective sensitivity and precision of a split detector by collecting all the Fisher information about the parameter of interest.\n\nExplanation: The correct answer is B) because the paper states that the proposed technique \"collects all of the Fisher information about the parameter of interest\" and that it \"increases the effective sensitivity and precision of a split detector\" compared to conventional continuous-wave balanced detection techniques. This suggests that the primary advantage of the technique is its ability to collect all the available information about the parameter, leading to improved sensitivity and precision."}, "37": {"documentation": {"title": "Resummation and NLO Matching of Event Shapes with Effective Field Theory", "source": "Matthew D. Schwartz", "docs_id": "0709.2709", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resummation and NLO Matching of Event Shapes with Effective Field Theory. The resummed differential thrust rate in e+e- annihilation is calculated using Soft-Collinear Effective Theory (SCET). The resulting distribution in the two-jet region T~1 is found to agree with the corresponding expression derived by the standard approach. A matching procedure to account for finite corrections at T < 1 is then described. There are two important advantages of the SCET approach. First, SCET manifests a dynamical seesaw scale q = p^2/Q in addition to the center-of-mass energy Q and the jet mass scale p ~ Q ~ sqrt(1 - T). Thus, the resummation of logs of p/q can be cleanly distinguished from the resummation of logs of Q/p. Second, finite parts of loop amplitudes appear in specific places in the perturbative distribution: in the matching to the hard function, at the scale Q, in matching to the jet function, at the scale p, and in matching to the soft function, at the scale q. This allows for a consistent merger of fixed order corrections and resummation. In particular, the total NLO e+e- cross section is reproduced from these finite parts without having to perform additional infrared regulation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Soft-Collinear Effective Theory (SCET), what is the significance of the dynamical seesaw scale q = p^2/Q, and how does it impact the resummation of logs of p/q versus logs of Q/p?\n\nA) The dynamical seesaw scale q = p^2/Q is a manifestation of the center-of-mass energy Q, and its resummation is equivalent to the resummation of logs of Q/p.\n\nB) The dynamical seesaw scale q = p^2/Q is a scale that arises from the finite corrections at T < 1, and its resummation is necessary to distinguish it from the resummation of logs of Q/p.\n\nC) The dynamical seesaw scale q = p^2/Q is a scale that is introduced in the SCET approach to account for the finite parts of loop amplitudes, and its resummation is crucial for a consistent merger of fixed order corrections and resummation.\n\nD) The dynamical seesaw scale q = p^2/Q is a scale that is not relevant to the resummation of logs of p/q versus logs of Q/p, and its resummation is not necessary.\n\nCorrect Answer: C) The dynamical seesaw scale q = p^2/Q is a scale that is introduced in the SCET approach to account for the finite parts of loop amplitudes, and its resummation is crucial for a consistent merger of fixed order corrections and resummation.\n\nExplanation: The correct answer, C, highlights the significance of the dynamical seesaw scale q = p^2/Q in the SCET approach. This scale is introduced to account for the finite parts of loop amplitudes, which appear in specific places in the perturbative distribution, such as in the matching to the hard function, at the scale Q, in matching to the jet function, at the scale p, and in matching to the soft function, at the scale q. The resummation of logs of p/q versus logs of Q/p is crucial for a consistent merger of fixed order corrections and resummation, and the dynamical seesaw scale q = p^2/Q plays a key role in this process."}, "38": {"documentation": {"title": "Beyond Quantum Mechanics", "source": "Sam Powers, Dejan Stojkovic", "docs_id": "2110.13617", "section": ["physics.gen-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beyond Quantum Mechanics. In this paper, an alternative formalism for modeling physics is proposed. The motivation for this approach arises from the tension between the countable (discrete) nature of empirical data and the uncountable sets (continuous functions) that form the foundations of modern physical theories. The foundation of this alternative formalism is the set of all base-2 sequences of length n. While this set is countable for finite n, it becomes uncountable in the limit that n goes to infinity, providing a viable pathway to correspondence with current theories. The mathematical construction necessary to model physics is developed by considering relationships among different base-2 sequences. Upon choosing a reference base-2 sequence, a relational system of numbers can be defined. Based on the properties of these relational numbers, the rules of angular momentum addition in quantum mechanics can be derived from first principles along with an alternative representation of the Clebsch-Gordan coefficients. These results can then be employed to model basic physics such as spin, as well as simple geometric elements such as directed edges. The simultaneous emergence of these modeling tools within this construction give hope that models for both matter and space-time may be supported by a single formalism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation for developing an alternative formalism for modeling physics, as proposed in the paper \"Beyond Quantum Mechanics\", and how does it relate to the foundations of modern physical theories?\n\nA) The need to reconcile the discrete nature of empirical data with the continuous nature of physical laws, in order to develop a more comprehensive understanding of the universe.\nB) The desire to create a new mathematical framework for describing the behavior of subatomic particles, independent of the principles of quantum mechanics.\nC) The goal of developing a more intuitive and accessible representation of the mathematical structures underlying modern physical theories.\nD) The aim of creating a formalism that can model the behavior of complex systems, such as black holes, using a novel combination of discrete and continuous mathematical structures.\n\nCorrect Answer: A) The need to reconcile the discrete nature of empirical data with the continuous nature of physical laws, in order to develop a more comprehensive understanding of the universe.\n\nExplanation: The paper \"Beyond Quantum Mechanics\" proposes an alternative formalism for modeling physics, motivated by the tension between the countable (discrete) nature of empirical data and the uncountable (continuous) sets that form the foundations of modern physical theories. This tension arises from the fact that empirical data is discrete, while the mathematical structures underlying modern physical theories are continuous. The alternative formalism aims to reconcile this tension by using a set of base-2 sequences, which are countable for finite n but become uncountable in the limit that n goes to infinity. This provides a viable pathway to correspondence with current theories, and allows for the development of a more comprehensive understanding of the universe."}, "39": {"documentation": {"title": "Approximate methods for the generation of dark matter halo catalogs in\n  the age of precision cosmology", "source": "Pierluigi Monaco (Trieste University, INAF-OATs, INFN-Trieste)", "docs_id": "1605.07752", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approximate methods for the generation of dark matter halo catalogs in\n  the age of precision cosmology. Precision cosmology has recently triggered new attention on the topic of approximate methods for the clustering of matter on large scales, whose foundations date back to the period from late '60s to early '90s. Indeed, although the prospect of reaching sub-percent accuracy in the measurement of clustering poses a challenge even to full N-body simulations, an accurate estimation of the covariance matrix of clustering statistics, not to mention the sampling of parameter space, requires usage of a large number (hundreds in the most favourable cases) of simulated (mock) galaxy catalogs. Combination of few N-body simulations with a large number of realizations performed with approximate methods gives the most promising approach to solve these problems with a reasonable amount of resources. In this paper I review this topic, starting from the foundations of the methods, then going through the pioneering efforts of the '90s, and finally presenting the latest extensions and a few codes that are now being used in present-generation surveys and thoroughly tested to assess their performance in the context of future surveys."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation for using approximate methods for the generation of dark matter halo catalogs in the age of precision cosmology, and how do these methods address the challenges posed by the need for sub-percent accuracy in clustering measurements?\n\n**A)** The primary motivation is to reduce computational resources required for N-body simulations, while approximate methods can provide a reasonable estimate of the covariance matrix of clustering statistics. However, this approach may not fully address the challenge of sampling parameter space.\n\n**B)** The primary motivation is to overcome the limitation of full N-body simulations in achieving sub-percent accuracy in clustering measurements, and approximate methods can provide a large number of simulated galaxy catalogs to assess the performance of these methods. However, this approach may not fully address the challenge of estimating the covariance matrix of clustering statistics.\n\n**C)** The primary motivation is to utilize a large number of simulated galaxy catalogs to assess the performance of approximate methods, while these methods can provide a reasonable estimate of the covariance matrix of clustering statistics. However, this approach may not fully address the challenge of sampling parameter space.\n\n**D)** The primary motivation is to reduce the computational resources required for N-body simulations, while approximate methods can provide a large number of simulated galaxy catalogs to assess the performance of these methods. However, this approach may not fully address the challenge of estimating the covariance matrix of clustering statistics and sampling parameter space.\n\n**Correct Answer:** B) The primary motivation is to overcome the limitation of full N-body simulations in achieving sub-percent accuracy in clustering measurements, and approximate methods can provide a large number of simulated galaxy catalogs to assess the performance of these methods. However, this approach may not fully address the challenge of estimating the covariance matrix of clustering statistics.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the motivation for using approximate methods in the age of precision cosmology. The primary motivation is to overcome the limitation of full N-body simulations in achieving sub-percent accuracy in clustering measurements, and approximate methods can provide a large number of simulated galaxy catalogs to assess the performance of these methods. However, this approach may not fully address the challenge of estimating the covariance matrix of clustering statistics, which is a critical aspect of precision cosmology."}, "40": {"documentation": {"title": "Fast Micron-Scale 3D Printing with a Resonant-Scanning Two-Photon\n  Microscope", "source": "Benjamin W Pearre, Christos Michas, Jean-Marc Tsang, Timothy J.\n  Gardner, Timothy M. Otchy", "docs_id": "1803.07135", "section": ["physics.app-ph", "physics.ins-det", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast Micron-Scale 3D Printing with a Resonant-Scanning Two-Photon\n  Microscope. 3D printing allows rapid fabrication of complex objects from digital designs. One 3D-printing process, direct laser writing, polymerises a light-sensitive material by steering a focused laser beam through the shape of the object to be created. The highest-resolution direct laser writing systems use a femtosecond laser to effect two-photon polymerisation. The focal (polymerisation) point is steered over the shape of the desired object with mechanised stages or galvanometer-controlled mirrors. Here we report a new high-resolution direct laser writing system that employs a resonant mirror scanner to achieve a significant increase in printing speed over galvanometer- or piezo-based methods while maintaining resolution on the order of a micron. This printer is based on a software modification to a commerically available resonant-scanning two-photon microscope. We demonstrate the complete process chain from hardware configuration and control software to the printing of objects of approximately $400\\times 400\\times 350\\;\\mu$m, and validate performance with objective benchmarks. Released under an open-source license, this work makes micro-scale 3D printing available the large community of two-photon microscope users, and paves the way toward widespread availability of precision-printed devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a resonant mirror scanner in the new high-resolution direct laser writing system described in the paper, compared to galvanometer- or piezo-based methods?\n\nA) Increased resolution due to improved laser beam steering\nB) Enhanced printing speed due to reduced mechanical vibrations\nC) Improved accuracy due to precise mirror movement control\nD) Increased material throughput due to optimized laser beam focusing\n\nCorrect Answer: B) Enhanced printing speed due to reduced mechanical vibrations\n\nExplanation: The paper states that the resonant mirror scanner achieves a \"significant increase in printing speed\" compared to galvanometer- or piezo-based methods. This suggests that the primary advantage of using a resonant mirror scanner is indeed the enhanced printing speed, which is likely due to reduced mechanical vibrations that can slow down the printing process in other systems."}, "41": {"documentation": {"title": "Biased-sample empirical likelihood weighting: an alternative to inverse\n  probability weighting", "source": "Yukun Liu and Yan Fan", "docs_id": "2111.12955", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Biased-sample empirical likelihood weighting: an alternative to inverse\n  probability weighting. Inverse probability weighting (IPW) is widely used in many areas when data are subject to unrepresentativeness, missingness, or selection bias. An inevitable challenge with the use of IPW is that the IPW estimator can be remarkably unstable if some probabilities are very close to zero. To overcome this problem, at least three remedies have been developed in the literature: stabilizing, thresholding, and trimming. However the final estimators are still IPW type estimators, and inevitably inherit certain weaknesses of the naive IPW estimator: they may still be unstable or biased. We propose a biased-sample empirical likelihood weighting (ELW) method to serve the same general purpose as IPW, while completely overcoming the instability of IPW-type estimators by circumventing the use of inverse probabilities. The ELW weights are always well defined and easy to implement. We show theoretically that the ELW estimator is asymptotically normal and more efficient than the IPW estimator and its stabilized version for missing data problems and unequal probability sampling without replacement. Its asymptotic normality is also established under unequal probability sampling with replacement. Our simulation results and a real data analysis indicate that the ELW estimator is shift-equivariant, nearly unbiased, and usually outperforms the IPW-type estimators in terms of mean square error."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a survey study where participants are selected using unequal probability sampling without replacement. The data are subject to missingness, and the researcher wants to estimate the population mean using an empirical likelihood weighting (ELW) method. However, the ELW estimator is known to be sensitive to the choice of the weighting function. Which of the following statements about the ELW estimator is true?\n\nA) The ELW estimator is asymptotically normal and more efficient than the inverse probability weighting (IPW) estimator for missing data problems and unequal probability sampling without replacement.\n\nB) The ELW estimator is shift-equivariant, but its asymptotic normality is only established under equal probability sampling with replacement.\n\nC) The ELW estimator is nearly unbiased, but its mean square error is always higher than that of the IPW-type estimators.\n\nD) The ELW estimator is asymptotically normal and more efficient than the IPW estimator, but its asymptotic normality is only established under unequal probability sampling with replacement.\n\n**Correct Answer:** A) The ELW estimator is asymptotically normal and more efficient than the inverse probability weighting (IPW) estimator for missing data problems and unequal probability sampling without replacement.\n\n**Explanation:** The correct answer is A) because the documentation states that the ELW estimator is asymptotically normal and more efficient than the IPW estimator for missing data problems and unequal probability sampling without replacement. This is a key advantage of the ELW method, which is designed to overcome the instability of IPW-type estimators. The other options are incorrect because they either misstate the properties of the ELW estimator or introduce additional assumptions that are not supported by the documentation."}, "42": {"documentation": {"title": "Parametric Sensitivity Analysis for Stochastic Molecular Systems using\n  Information Theoretic Metrics", "source": "Anastasios Tsourtis, Yannis Pantazis, Markos A. Katsoulakis, Vagelis\n  Harmandaris", "docs_id": "1412.6482", "section": ["cs.IT", "math.IT", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parametric Sensitivity Analysis for Stochastic Molecular Systems using\n  Information Theoretic Metrics. In this paper we extend the parametric sensitivity analysis (SA) methodology proposed in Ref. [Y. Pantazis and M. A. Katsoulakis, J. Chem. Phys. 138, 054115 (2013)] to continuous time and continuous space Markov processes represented by stochastic differential equations and, particularly, stochastic molecular dynamics as described by the Langevin equation. The utilized SA method is based on the computation of the information-theoretic (and thermodynamic) quantity of relative entropy rate (RER) and the associated Fisher information matrix (FIM) between path distributions. A major advantage of the pathwise SA method is that both RER and pathwise FIM depend only on averages of the force field therefore they are tractable and computable as ergodic averages from a single run of the molecular dynamics simulation both in equilibrium and in non-equilibrium steady state regimes. We validate the performance of the extended SA method to two different molecular stochastic systems, a standard Lennard-Jones fluid and an all-atom methane liquid and compare the obtained parameter sensitivities with parameter sensitivities on three popular and well-studied observable functions, namely, the radial distribution function, the mean squared displacement and the pressure. Results show that the RER-based sensitivities are highly correlated with the observable-based sensitivities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Parametric Sensitivity Analysis for Stochastic Molecular Systems using Information Theoretic Metrics, what is the primary advantage of the pathwise SA method in terms of computational tractability?\n\n**A)** It requires multiple runs of the molecular dynamics simulation to compute the relative entropy rate (RER) and Fisher information matrix (FIM).\n\n**B)** It is only applicable to equilibrium steady state regimes and cannot be used for non-equilibrium systems.\n\n**C)** Both RER and FIM depend only on averages of the force field, making them computable as ergodic averages from a single run of the molecular dynamics simulation.\n\n**D)** It is limited to studying only observable functions such as the radial distribution function, mean squared displacement, and pressure.\n\n**Correct Answer:** C) Both RER and FIM depend only on averages of the force field, making them computable as ergodic averages from a single run of the molecular dynamics simulation.\n\n**Explanation:** The correct answer is C) because the pathwise SA method has the advantage of being computationally tractable, as both the relative entropy rate (RER) and Fisher information matrix (FIM) depend only on averages of the force field, which can be computed from a single run of the molecular dynamics simulation. This makes it a valuable tool for sensitivity analysis of stochastic molecular systems."}, "43": {"documentation": {"title": "On spinodal decomposition in alnico---a transmission electron microscopy\n  and atom probe tomography study", "source": "Lin Zhou, Wei Guo, Jonathan D. Poplawsky, Liqin Ke, Wei Tang, Iver E.\n  Anderson, Matthew J. Kramer", "docs_id": "1810.12580", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On spinodal decomposition in alnico---a transmission electron microscopy\n  and atom probe tomography study. Alnico is a prime example of a finely tuned nanostructure whose magnetic properties are intimately connected to magnetic annealing (MA) during spinodal transformation and subsequent lower temperature annealing (draw) cycles. Using a combination of transmission electron microscopy and atom probe tomography, we show how these critical processing steps affect the local composition and nanostructure evolution with impact on magnetic properties. The nearly 2-fold increase of intrinsic coercivity ($H_\\text{ci}$) during the draw cycle is not adequately explained by chemical refinement of the spinodal phases. Instead, increased Fe-Co phase ($\\alpha_1$) isolation, development of Cu-rich spheres/rods/blades and additional $\\alpha_1$ rod precipitation that occurs during the MA and draw, likely play a key role in $H_\\text{ci}$ enhancement. Chemical ordering of the Al-Ni-phase ($\\alpha_2$) and formation of Ni-rich ($\\alpha_3$) may also contribute. Unraveling of the subtle effect of these nano-scaled features is crucial to understanding on how to improve shape anisotropy in alnico magnets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism responsible for the nearly 2-fold increase in intrinsic coercivity (Hci) during the draw cycle in Alnico, and how do the nanostructure features formed during this process contribute to this enhancement?\n\n**A)** Chemical refinement of the spinodal phases and increased Fe-Co phase isolation are the primary mechanisms, with additional effects from Cu-rich spheres/rods/blades and \u03b11 rod precipitation.\n\n**B)** Chemical ordering of the Al-Ni-phase (\u03b12) and formation of Ni-rich (\u03b13) phases are the primary mechanisms, with increased Fe-Co phase isolation and Cu-rich spheres/rods/blades playing a secondary role.\n\n**C)** Increased Fe-Co phase isolation, development of Cu-rich spheres/rods/blades, and additional \u03b11 rod precipitation during the MA and draw cycles are the primary mechanisms, with chemical refinement of the spinodal phases and chemical ordering of the Al-Ni-phase (\u03b12) contributing to a lesser extent.\n\n**D)** The nearly 2-fold increase in Hci during the draw cycle is primarily due to the formation of Ni-rich (\u03b13) phases, with increased Fe-Co phase isolation and Cu-rich spheres/rods/blades playing a secondary role.\n\n**Correct Answer:** C) Increased Fe-Co phase isolation, development of Cu-rich spheres/rods/blades, and additional \u03b11 rod precipitation during the MA and draw cycles are the primary mechanisms, with chemical refinement of the spinodal phases and chemical ordering of the Al-Ni-phase (\u03b12) contributing to a lesser extent.\n\n**Explanation:** The correct answer is based on the text, which states that \"increased Fe-Co phase (\u03b11) isolation, development of Cu-rich spheres/rods/blades and additional \u03b11 rod precipitation that occurs during the MA and draw, likely play a key role in Hci enhancement.\" This suggests that the primary mechanisms are related to the nanostructure features formed during the MA and draw cycles, rather than chemical refinement of the spinodal phases or chemical ordering of the Al-Ni-phase."}, "44": {"documentation": {"title": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data", "source": "Katsuya Ito, Kei Nakagawa", "docs_id": "2002.00724", "section": ["q-fin.ST", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data. In time-series analysis, the term \"lead-lag effect\" is used to describe a delayed effect on a given time series caused by another time series. lead-lag effects are ubiquitous in practice and are specifically critical in formulating investment strategies in high-frequency trading. At present, there are three major challenges in analyzing the lead-lag effects. First, in practical applications, not all time series are observed synchronously. Second, the size of the relevant dataset and rate of change of the environment is increasingly faster, and it is becoming more difficult to complete the computation within a particular time limit. Third, some lead-lag effects are time-varying and only last for a short period, and their delay lengths are often affected by external factors. In this paper, we propose NAPLES (Negative And Positive lead-lag EStimator), a new statistical measure that resolves all these problems. Through experiments on artificial and real datasets, we demonstrate that NAPLES has a strong correlation with the actual lead-lag effects, including those triggered by significant macroeconomic announcements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of time-series analysis, what are the three major challenges in analyzing lead-lag effects, as identified in the paper \"NAPLES\"?\n\n**A)** Insufficient dataset size, computational complexity, and non-stationarity of the time series.\n**B)** Non-synchronous observation of time series, high-frequency data, and limited computational resources.\n**C)** Presence of outliers, non-linear relationships, and time-varying lead-lag effects.\n**D)** Inability to account for external factors, non-stationarity of the lead-lag effects, and limited dataset size.\n\n**Correct Answer:** B) Non-synchronous observation of time series, high-frequency data, and limited computational resources.\n\n**Explanation:** The paper highlights three major challenges in analyzing lead-lag effects: (1) non-synchronous observation of time series, (2) high-frequency data, and (3) limited computational resources. These challenges are critical in formulating investment strategies in high-frequency trading. The correct answer requires the candidate to identify the correct challenges mentioned in the paper.\n\n**Additional Marking Scheme:**\n\n* 1 mark for correctly identifying the three challenges (non-synchronous observation, high-frequency data, and limited computational resources)\n* 0.5 marks for correctly identifying at least two of the challenges\n* 0 marks for incorrectly identifying any of the challenges or providing an unrelated answer."}, "45": {"documentation": {"title": "Engineering the spectral bandwidth of quantum cascade laser frequency\n  combs", "source": "Maximilian Beiser, Nikola Opa\\v{c}ak, Johannes Hillbrand, Gottfried\n  Strasser, Benedikt Schwarz", "docs_id": "2103.04307", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Engineering the spectral bandwidth of quantum cascade laser frequency\n  combs. Quantum cascade lasers (QCLs) facilitate compact optical frequency comb sources that operate in the mid-infrared and terahertz spectral regions, where many molecules have their fundamental absorption lines. Enhancing the optical bandwidth of these chip-sized lasers is of paramount importance to address their application in broadband high-precision spectroscopy. In this work, we provide a numerical and experimental investigation of the comb spectral width and show how it can be optimized to obtain its maximum value defined by the laser gain bandwidth. The interplay of nonoptimal values of the resonant Kerr nonlinearity and the cavity dispersion can lead to significant narrowing of the comb spectrum and reveals the best approach for dispersion compensation. The implementation of high mirror losses is shown to be favourable and results in proliferation of the comb sidemodes. Ultimately, injection locking of QCLs by modulating the laser bias around the roundtrip frequency provides a stable external knob to control the FM comb state and recover the maximum spectral width of the unlocked laser state."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of implementing high mirror losses in quantum cascade lasers, and how does it impact the spectral width of the frequency comb?\n\nA) High mirror losses lead to a decrease in the spectral width of the frequency comb, resulting in a narrower bandwidth.\nB) High mirror losses lead to a proliferation of sidemodes in the frequency comb, resulting in a wider bandwidth.\nC) High mirror losses have no significant impact on the spectral width of the frequency comb.\nD) High mirror losses lead to a decrease in the stability of the frequency comb, resulting in a narrower bandwidth.\n\nCorrect Answer: B) High mirror losses lead to a proliferation of sidemodes in the frequency comb, resulting in a wider bandwidth.\n\nExplanation: The documentation states that \"the implementation of high mirror losses is shown to be favourable and results in proliferation of the comb sidemodes.\" This implies that high mirror losses lead to an increase in the number of sidemodes in the frequency comb, which in turn results in a wider bandwidth. This is a key advantage in achieving the maximum spectral width of the frequency comb."}, "46": {"documentation": {"title": "KrigHedge: Gaussian Process Surrogates for Delta Hedging", "source": "Mike Ludkovski and Yuri Saporito", "docs_id": "2010.08407", "section": ["q-fin.CP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "KrigHedge: Gaussian Process Surrogates for Delta Hedging. We investigate a machine learning approach to option Greeks approximation based on Gaussian process (GP) surrogates. The method takes in noisily observed option prices, fits a nonparametric input-output map and then analytically differentiates the latter to obtain the various price sensitivities. Our motivation is to compute Greeks in cases where direct computation is expensive, such as in local volatility models, or can only ever be done approximately. We provide a detailed analysis of numerous aspects of GP surrogates, including choice of kernel family, simulation design, choice of trend function and impact of noise. We further discuss the application to Delta hedging, including a new Lemma that relates quality of the Delta approximation to discrete-time hedging loss. Results are illustrated with two extensive case studies that consider estimation of Delta, Theta and Gamma and benchmark approximation quality and uncertainty quantification using a variety of statistical metrics. Among our key take-aways are the recommendation to use Matern kernels, the benefit of including virtual training points to capture boundary conditions, and the significant loss of fidelity when training on stock-path-based datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind using Gaussian Process (GP) surrogates for Delta hedging, as proposed in the KrigHedge paper?\n\nA) To reduce the computational complexity of option Greeks approximation\nB) To improve the accuracy of Delta hedging by incorporating noise into the estimation process\nC) To provide a more robust and efficient method for estimating option Greeks in local volatility models\nD) To develop a new framework for option pricing that incorporates virtual training points\n\n**Correct Answer:** C) To provide a more robust and efficient method for estimating option Greeks in local volatility models\n\n**Explanation:** The KrigHedge paper aims to address the challenge of computing option Greeks in local volatility models, where direct computation is expensive or only possible approximately. The authors propose using GP surrogates as a machine learning approach to approximate option Greeks, which is motivated by the need for a more robust and efficient method. While options A and B are related to the topic, they are not the primary motivation behind the KrigHedge approach. Option D is incorrect because virtual training points are actually used to capture boundary conditions, not as a primary motivation for the approach."}, "47": {"documentation": {"title": "Contact-line deposits from multiple evaporating droplets", "source": "Alexander W. Wray, Patrick S. Wray, Brian R. Duffy, Stephen K. Wilson", "docs_id": "2103.07221", "section": ["physics.flu-dyn", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contact-line deposits from multiple evaporating droplets. Building on the recent theoretical work of Wray, Duffy and Wilson [J. Fluid Mech. 884, A45 (2020)] concerning the competitive diffusion-limited evaporation of multiple thin sessile droplets in proximity to each other, we obtain theoretical predictions for the spatially non-uniform densities of the contact-line deposits (often referred to as \"coffee stains\" or \"ring stains\") left on the substrate after such droplets containing suspended solid particles have completely evaporated. Neighbouring droplets interact via their vapour fields, which results in a spatially non-uniform \"shielding\" effect. We give predictions for the deposits from a pair of identical droplets, which show that the deposit is reduced the most where the droplets are closest together, and demonstrate excellent quantitative agreement with experimental results of Pradhan and Panigrahi [Coll. Surf. A 482, 562-567 (2015)]. We also give corresponding predictions for a triplet of identical droplets arranged in an equilateral triangle, which show that the effect of shielding on the deposit is more subtle in this case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the theoretical work of Wray, Duffy, and Wilson (2020), what is the expected effect on the spatially non-uniform densities of contact-line deposits left on a substrate after multiple evaporating droplets containing suspended solid particles have completely evaporated?\n\n**A)** The deposits will be uniform and unaffected by the proximity of neighboring droplets.\n**B)** The deposits will be reduced the most where the droplets are closest together due to a \"shielding\" effect.\n**C)** The deposits will be increased in density where the droplets are farthest apart, resulting in a \"boosting\" effect.\n**D)** The deposits will be unaffected by the arrangement of the droplets, with the same density regardless of proximity.\n\n**Correct Answer:** B) The deposits will be reduced the most where the droplets are closest together due to a \"shielding\" effect.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the deposit is reduced the most where the droplets are closest together, and demonstrate excellent quantitative agreement with experimental results of Pradhan and Panigrahi [Coll. Surf. A 482, 562-567 (2015)].\" This indicates that the proximity of neighboring droplets leads to a \"shielding\" effect, resulting in a reduction in the density of the contact-line deposits."}, "48": {"documentation": {"title": "Events-to-Video: Bringing Modern Computer Vision to Event Cameras", "source": "Henri Rebecq, Ren\\'e Ranftl, Vladlen Koltun, Davide Scaramuzza", "docs_id": "1904.08298", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Events-to-Video: Bringing Modern Computer Vision to Event Cameras. Event cameras are novel sensors that report brightness changes in the form of asynchronous \"events\" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks. A video of the experiments is available at https://youtu.be/IdYrC4cUO0I"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of event cameras over conventional cameras, according to the authors of the paper, and how does this advantage impact the development of algorithms for processing event data?\n\nA) Event cameras have a higher resolution than conventional cameras, but are more prone to motion blur.\nB) Event cameras have a higher temporal resolution, high dynamic range, and no motion blur, making them ideal for applications that require real-time processing and high-quality images.\nC) Event cameras are more expensive than conventional cameras, but offer better low-light performance.\nD) Event cameras are more susceptible to noise and interference than conventional cameras, but are more compact and portable.\n\nCorrect Answer: B) Event cameras have a higher temporal resolution, high dynamic range, and no motion blur, making them ideal for applications that require real-time processing and high-quality images.\n\nExplanation: The question requires the test-taker to understand the advantages of event cameras and how they impact the development of algorithms for processing event data. The correct answer highlights the key benefits of event cameras, including high temporal resolution, high dynamic range, and no motion blur, which are essential for real-time processing and high-quality images. The incorrect options (A, C, and D) are designed to test the test-taker's ability to distinguish between relevant and irrelevant information, as well as their understanding of the subject matter."}, "49": {"documentation": {"title": "A Measurement of Stellar Surface Gravity Hidden in Radial Velocity\n  Differences of Co-moving Stars", "source": "Matthew Moschella, Oren Slone, Jeff A. Dror, Matteo Cantiello, Hagai\n  B. Perets", "docs_id": "2102.01079", "section": ["astro-ph.SR", "astro-ph.GA", "astro-ph.IM", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Measurement of Stellar Surface Gravity Hidden in Radial Velocity\n  Differences of Co-moving Stars. The gravitational redshift induced by stellar surface gravity is notoriously difficult to measure for non-degenerate stars, since its amplitude is small in comparison with the typical Doppler shift induced by stellar radial velocity. In this study, we make use of the large observational data set of the Gaia mission to achieve a significant reduction of noise caused by these random stellar motions. By measuring the differences in velocities between the components of pairs of co-moving stars and wide binaries, we are able to statistically measure gravitational redshift and nullify the effect of the peculiar motions of the stars. For the subset of stars considered in this study, we find a positive correlation between the observed differences in Gaia radial velocities and the differences in surface gravity inferred from effective temperature and luminosity measurements. This corresponds to the first ever measurement of extra-Solar surface gravity induced gravitational redshift in non-degenerate stars. Additionally, we study the sub-dominant effects of convective blueshifting of emission lines, effects of binary motion, and possible systematic errors in radial velocity measurements within Gaia. Results from the technique presented in this study are expected to improve significantly with data from the next Gaia data release. Such improvements could be used to constrain the mass-luminosity relation and stellar models which predict the magnitude of convective blueshift."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the differences in velocities between co-moving stars and wide binaries to measure the gravitational redshift induced by stellar surface gravity?\n\nA) It allows for the direct measurement of stellar surface gravity.\nB) It enables the cancellation of the effects of peculiar motions of the stars.\nC) It provides a more accurate measurement of stellar radial velocities.\nD) It enables the detection of convective blueshifting of emission lines.\n\nCorrect Answer: B) It enables the cancellation of the effects of peculiar motions of the stars.\n\nExplanation: The correct answer is B) It enables the cancellation of the effects of peculiar motions of the stars. The study uses the differences in velocities between co-moving stars and wide binaries to statistically measure gravitational redshift and nullify the effect of the peculiar motions of the stars. This is a key advantage of the technique presented in the study, as it allows for a more accurate measurement of the gravitational redshift.\n\nThe other options are incorrect because:\n\nA) The direct measurement of stellar surface gravity is not the primary advantage of the technique, although it is a related benefit.\n\nC) The measurement of stellar radial velocities is not the primary focus of the study, although it is a necessary step in the process.\n\nD) The detection of convective blueshifting of emission lines is a sub-dominant effect that is studied in the context of the technique, but it is not the primary advantage."}, "50": {"documentation": {"title": "Utility Optimal Scheduling in Energy Harvesting Networks", "source": "Longbo Huang, Michael J. Neely", "docs_id": "1012.1945", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Utility Optimal Scheduling in Energy Harvesting Networks. In this paper, we show how to achieve close-to-optimal utility performance in energy harvesting networks with only finite capacity energy storage devices. In these networks, nodes are capable of harvesting energy from the environment. The amount of energy that can be harvested is time varying and evolves according to some probability law. We develop an \\emph{online} algorithm, called the Energy-limited Scheduling Algorithm (ESA), which jointly manages the energy and makes power allocation decisions for packet transmissions. ESA only has to keep track of the amount of energy left at the network nodes and \\emph{does not require any knowledge} of the harvestable energy process. We show that ESA achieves a utility that is within $O(\\epsilon)$ of the optimal, for any $\\epsilon>0$, while ensuring that the network congestion and the required capacity of the energy storage devices are \\emph{deterministically} upper bounded by bounds of size $O(1/\\epsilon)$. We then also develop the Modified-ESA algorithm (MESA) to achieve the same $O(\\epsilon)$ close-to-utility performance, with the average network congestion and the required capacity of the energy storage devices being only $O([\\log(1/\\epsilon)]^2)$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Energy-limited Scheduling Algorithm (ESA), what is the relationship between the utility performance achieved by ESA and the optimal utility, in terms of the given constant $\\epsilon$?\n\n**A)** ESA achieves a utility that is within $O(\\epsilon^2)$ of the optimal utility.\n**B)** ESA achieves a utility that is within $O(\\epsilon)$ of the optimal utility.\n**C)** ESA achieves a utility that is within $O(1/\\epsilon^2)$ of the optimal utility.\n**D)** ESA achieves a utility that is within $O(1/\\epsilon)$ of the optimal utility.\n\n**Correct Answer:** B) ESA achieves a utility that is within $O(\\epsilon)$ of the optimal utility.\n\n**Explanation:** According to the paper, ESA achieves a utility that is within $O(\\epsilon)$ of the optimal, for any $\\epsilon>0$. This means that the difference between the utility achieved by ESA and the optimal utility is bounded by a constant that depends on $\\epsilon$, but not on the optimal utility itself."}, "51": {"documentation": {"title": "Viscoelastic properties and flow instabilities of aqueous suspensions of\n  cellulosic fibers", "source": "Jing He, Stephanie S. Lee, Manuk Colakyan and Dilhan M. Kalyon", "docs_id": "1903.07736", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Viscoelastic properties and flow instabilities of aqueous suspensions of\n  cellulosic fibers. Processing of concentrated lignocellulosic biomass suspensions typically involves the conversion of the cellulose into sugars and sugars into ethanol. Biomass is usually pre-processed (i.e. via comminution, steam explosion, etc.) to form fine cellulosic fibers to be dispersed into an aqueous phase for further treatment. The resulting cellulose suspensions need to be pressurized and pumped into and out of various processing vessels without allowing the development of flow instabilities that are typically associated with the demixing, i.e., the segregation of the cellulosic biomass from the aqueous phase. Here, it is demonstrated that the use of a gelation agent, hydroxypropyl guar gum (HPG) at the relatively low concentration of 0.5 wt% significantly affects the development of viscoelastic material functions of cellulosic suspensions, and improves the dispersive mixing of the cellulose fibers within the aqueous phase. This results in the reduction of the flow instabilities and associated demixing effects that are ubiquitously observed during the pressurization of cellulosic suspensions in Poiseuille and compressive squeeze flows."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of adding hydroxypropyl guar gum (HPG) to cellulosic suspensions at a concentration of 0.5 wt% in terms of their viscoelastic material functions and flow instabilities?\n\nA) It increases the viscosity of the suspensions, leading to enhanced flow instabilities.\nB) It reduces the viscosity of the suspensions, leading to improved dispersive mixing and reduced flow instabilities.\nC) It has no significant effect on the viscoelastic properties of the suspensions.\nD) It increases the compressive strength of the suspensions, leading to improved flow instabilities.\n\nCorrect Answer: B) It reduces the viscosity of the suspensions, leading to improved dispersive mixing and reduced flow instabilities.\n\nExplanation: The correct answer is B) It reduces the viscosity of the suspensions, leading to improved dispersive mixing and reduced flow instabilities. This is because the addition of HPG at a concentration of 0.5 wt% significantly affects the development of viscoelastic material functions of cellulosic suspensions, and improves the dispersive mixing of the cellulose fibers within the aqueous phase, resulting in reduced flow instabilities and associated demixing effects."}, "52": {"documentation": {"title": "A Unified Theory and Fundamental Rules of Strain-dependent Doping\n  Behaviors in Semiconductors", "source": "Xiaolan Yan, Pei Li, Su-Huai Wei, and Bing Huang", "docs_id": "2008.09343", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Unified Theory and Fundamental Rules of Strain-dependent Doping\n  Behaviors in Semiconductors. Enhancing the dopability of semiconductors via strain engineering is critical to improving their functionalities, which is, however, largely hindered by the lack of fundamental rules. In this Letter, for the first time, we develop a unified theory to understand the total energy changes of defects (or dopants) with different charge states under strains, which can exhibit either parabolic or superlinear behaviors, determined by the size of defect-induced local volume change ({\\Delta}V). In general, {\\Delta}V increases (decreases) when an electron is added (removed) to (from) the defect site. Consequently, in terms of this unified theory, three fundamental rules can be obtained to further understand or predict the diverse strain-dependent doping behaviors, i.e., defect formation energies, charge-state transition levels, and Fermi pinning levels, in semiconductors. These three fundamental rules could be generally applied to improve the doping performance or overcome the doping bottlenecks in various semiconductors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying principle that governs the relationship between the size of defect-induced local volume change (\u0394V) and the strain-dependent doping behaviors in semiconductors?\n\nA) The size of \u0394V directly determines the charge state of the defect, leading to parabolic or superlinear behaviors.\nB) The size of \u0394V is inversely proportional to the defect formation energy, resulting in superlinear behaviors.\nC) The size of \u0394V increases (decreases) when an electron is added (removed) to (from) the defect site, leading to parabolic or superlinear behaviors.\nD) The size of \u0394V is a constant value that remains unchanged under different strains, resulting in parabolic behaviors.\n\nCorrect Answer: C) The size of \u0394V increases (decreases) when an electron is added (removed) to (from) the defect site, leading to parabolic or superlinear behaviors.\n\nExplanation: The correct answer is based on the statement in the documentation that \"In general, \u0394V increases (decreases) when an electron is added (removed) to (from) the defect site.\" This principle is the underlying reason for the parabolic or superlinear behaviors observed in strain-dependent doping behaviors in semiconductors."}, "53": {"documentation": {"title": "Fault diagnosis for linear heterodirectional hyperbolic ODE-PDE systems\n  using backstepping-based trajectory planning", "source": "Ferdinand Fischer and Joachim Deutscher", "docs_id": "2010.11526", "section": ["eess.SY", "cs.SY", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fault diagnosis for linear heterodirectional hyperbolic ODE-PDE systems\n  using backstepping-based trajectory planning. This paper is concerned with the fault diagnosis problem for general linear heterodirectional hyperbolic ODE-PDE systems. A systematic solution is presented for additive time-varying actuator, process and sensor faults in the presence of disturbances. The faults and disturbances are represented by the solutions of finite-dimensional signal models, which allow to take a large class of signals into account. For disturbances, that are only bounded, a threshold for secured fault diagnosis is derived. By applying integral transformations to the system an algebraic fault detection equation to detect faults in finite time is obtained. The corresponding integral kernels result from the realization of a finite-time transition between a non-equilibrium initial state and a vanishing final state of a hyperbolic ODE-PDE system. For this new challenging problem, a systematic trajectory planning approach is presented. In particular, this problem is facilitated by mapping the kernel equations into backstepping coordinates and tracing the solution of the transition problem back to a simple trajectory planning. The fault diagnosis for a $4\\times 4$ heterodirectional hyperbolic system coupled with a second order ODE demonstrates the results of the paper."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a linear heterodirectional hyperbolic ODE-PDE system with the following state-space representation:\n\nx' = Ax + Bu + Fv\ny = Cx + Du + Ev\n\nwhere x \u2208 \u211d\u207f, u \u2208 \u211d\u207f, v \u2208 \u211d\u207f, y \u2208 \u211d\u207f, A, B, C, D, and E are matrices of appropriate dimensions, and F is a matrix representing the disturbances.\n\n**A)** Derive the algebraic fault detection equation for detecting additive time-varying actuator, process, and sensor faults in the presence of disturbances, assuming that the disturbances are bounded.\n\n**B)** Explain why the realization of a finite-time transition between a non-equilibrium initial state and a vanishing final state of a hyperbolic ODE-PDE system is necessary for obtaining the integral kernels.\n\n**C)** Describe the mapping of the kernel equations into backstepping coordinates and explain how this facilitates the fault diagnosis for the given system.\n\n**D)** Discuss the significance of the threshold for secured fault diagnosis for disturbances that are only bounded, and provide a mathematical expression for this threshold.\n\n**Correct Answer:** A\n\n**Explanation:**\n\nThe correct answer is A) Derive the algebraic fault detection equation for detecting additive time-varying actuator, process, and sensor faults in the presence of disturbances, assuming that the disturbances are bounded.\n\nThe algebraic fault detection equation is a key result in the paper, which allows for the detection of faults in finite time. To derive this equation, one needs to apply integral transformations to the system and use the finite-dimensional signal models to represent the faults and disturbances. This involves solving a system of differential equations and using the properties of hyperbolic ODE-PDE systems to obtain the desired equation.\n\nThe other options are incorrect because:\n\n* B) is incorrect because the realization of a finite-time transition is a necessary step in obtaining the integral kernels, but it is not the main goal of the paper.\n* C) is incorrect because the mapping of kernel equations into backstepping coordinates is a tool used to facilitate the fault diagnosis, but it is not the main focus of the paper.\n* D) is incorrect because the threshold for secured fault diagnosis is not explicitly derived in the paper, and even if it were, it would not be a mathematical expression for a specific value."}, "54": {"documentation": {"title": "Graphene on silicon: effects of the silicon surface orientation on the\n  work function and carrier density of graphene", "source": "Y. W. Sun, D. Holec, D. Gehringer, L. Li, O. Fenwick, D. J. Dunstan\n  and C. J. Humphreys", "docs_id": "2010.02551", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graphene on silicon: effects of the silicon surface orientation on the\n  work function and carrier density of graphene. Density functional theory has been employed to study graphene on the (111), (100) and (110) surfaces of silicon (Si) substrates. There are several interesting findings. First, carbon atoms in graphene form covalent bonds with Si atoms, when placed close enough on Si (111) and (100) surfaces, but not on the (110) surface. The presence of a Si (111) surface shifts the Fermi level of graphene into its conduction band, resulting in an increase of the work function by 0.29 eV and of the electron density by three orders of magnitude. The carrier density of graphene can also be increased by eighty times on a Si (100) substrate without doping, due to the modification of the density of states near the Dirac point. No interfacial covalent bond can be formed on Si (110). These striking effects that different orientations of a silicon substrate can have on the properties of graphene are related to the surface density of the silicon surface. Applying the results to a real device of a specific orientation requires further consideration of surface reconstructions, lattice mismatch, temperature, and environmental effects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the significant increase in the work function of graphene on a Si (111) substrate, and how does this affect the carrier density of graphene?\n\nA) The formation of interfacial covalent bonds between carbon and silicon atoms, resulting in a shift of the Fermi level into the conduction band.\nB) The modification of the density of states near the Dirac point due to the presence of a Si (100) substrate, leading to an increase in the electron density.\nC) The surface reconstruction of the Si (110) substrate, causing a decrease in the work function and an increase in the carrier density.\nD) The lattice mismatch between graphene and the Si (111) substrate, resulting in a decrease in the work function and a decrease in the carrier density.\n\nCorrect Answer: A) The formation of interfacial covalent bonds between carbon and silicon atoms, resulting in a shift of the Fermi level into the conduction band.\n\nExplanation: The correct answer is A) because the documentation states that carbon atoms in graphene form covalent bonds with Si atoms on the (111) and (100) surfaces, but not on the (110) surface. This results in a shift of the Fermi level of graphene into its conduction band, leading to an increase in the work function by 0.29 eV. This, in turn, affects the carrier density of graphene, increasing it by three orders of magnitude on the Si (111) substrate."}, "55": {"documentation": {"title": "Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data", "source": "Jin-Xing Liu, Chun-Mei Feng, Xiang-Zhen Kong, Yong Xu", "docs_id": "1901.06794", "section": ["q-bio.GN", "cs.CE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data. In the context of cancer, internal \"checkerboard\" structures are normally found in the matrices of gene expression data, which correspond to genes that are significantly up- or down-regulated in patients with specific types of tumors. In this paper, we propose a novel method, called dual graph-regularization principal component analysis (DGPCA). The main innovation of this method is that it simultaneously considers the internal geometric structures of the condition manifold and the gene manifold. Specifically, we obtain principal components (PCs) to represent the data and approximate the cluster membership indicators through Laplacian embedding. This new method is endowed with internal geometric structures, such as the condition manifold and gene manifold, which are both suitable for bi-clustering. A closed-form solution is provided for DGPCA. We apply this new method to simultaneously cluster genes and conditions (e.g., different samples) with the aim of finding internal \"checkerboard\" structures on gene expression data, if they exist. Then, we use this new method to identify regulatory genes under the particular conditions and to compare the results with those of other state-of-the-art PCA-based methods. Promising results on gene expression data have been verified by extensive experiments"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main innovation of the proposed method, Dual Graph-Laplacian PCA (DGPCA), in the context of cancer gene expression data analysis?\n\nA) It uses a traditional PCA approach to identify clusters of genes and conditions separately.\nB) It incorporates Laplacian embedding to approximate cluster membership indicators and simultaneously considers the internal geometric structures of the condition manifold and the gene manifold.\nC) It employs a machine learning algorithm to predict gene expression levels based on external features.\nD) It uses a dimensionality reduction technique to reduce the number of genes in the analysis.\n\n**Correct Answer:** B) It incorporates Laplacian embedding to approximate cluster membership indicators and simultaneously considers the internal geometric structures of the condition manifold and the gene manifold.\n\n**Explanation:** The correct answer is B) because the documentation states that the main innovation of DGPCA is that it \"simultaneously considers the internal geometric structures of the condition manifold and the gene manifold\" through Laplacian embedding. This allows the method to approximate cluster membership indicators and identify internal \"checkerboard\" structures on gene expression data, which is a key aspect of the paper's contribution."}, "56": {"documentation": {"title": "Spinning superconducting electrovacuum soliton", "source": "Irina Dymnikova", "docs_id": "hep-th/0607174", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spinning superconducting electrovacuum soliton. In nonlinear electrodynamics coupled to general relativity and satisfying the weak energy condition, a spherically symmetric electrically charged electrovacuum soliton has obligatory de Sitter center in which the electric field vanishes while the energy density of electromagnetic vacuum achieves its maximal value. De Sitter vacuum supplies a particle with the finite positive electromagnetic mass related to breaking of space-time symmetry from the de Sitter group in the origin. By the G\\\"urses-G\\\"ursey algorithm based on the Newman-Trautman technique it is transformed into a spinning electrovacuum soliton asymptotically Kerr-Newman for a distant observer. De Sitter center becomes de Sitter equatorial disk which has both perfect conductor and ideal diamagnetic properties. The interior de Sitter vacuum disk displays superconducting behavior within a single spinning soliton. This behavior found for an arbitrary nonlinear lagrangian ${\\cal L}(F)$, is generic for the class of regular spinning electrovacuum solutions describing both black holes and particle-like structures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a spherically symmetric electrovacuum soliton in nonlinear electrodynamics coupled to general relativity, satisfying the weak energy condition. What is the nature of the de Sitter center at the origin of this soliton, and how does it relate to the electromagnetic mass of a particle that can be formed from this vacuum?\n\nA) The de Sitter center is a perfect conductor, and the electromagnetic mass is zero.\nB) The de Sitter center is a de Sitter equatorial disk with perfect conductor and ideal diamagnetic properties, and the electromagnetic mass is finite and positive.\nC) The de Sitter center is a black hole, and the electromagnetic mass is infinite.\nD) The de Sitter center is a cosmological singularity, and the electromagnetic mass is undefined.\n\nCorrect Answer: B) The de Sitter center is a de Sitter equatorial disk with perfect conductor and ideal diamagnetic properties, and the electromagnetic mass is finite and positive.\n\nExplanation: According to the documentation, the de Sitter center at the origin of the electrovacuum soliton is a de Sitter equatorial disk with both perfect conductor and ideal diamagnetic properties. Additionally, the energy density of the electromagnetic vacuum achieves its maximal value at this center, which is related to the formation of a particle with a finite positive electromagnetic mass."}, "57": {"documentation": {"title": "Breaking limitation of quantum annealer in solving optimization problems\n  under constraints", "source": "Masayuki Ohzeki", "docs_id": "2002.05298", "section": ["quant-ph", "cond-mat.dis-nn", "cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Breaking limitation of quantum annealer in solving optimization problems\n  under constraints. Quantum annealing is a generic solver for optimization problems that uses fictitious quantum fluctuation. The most groundbreaking progress in the research field of quantum annealing is its hardware implementation, i.e., the so-called quantum annealer, using artificial spins. However, the connectivity between the artificial spins is sparse and limited on a special network known as the chimera graph. Several embedding techniques have been proposed, but the number of logical spins, which represents the optimization problems to be solved, is drastically reduced. In particular, an optimization problem including fully or even partly connected spins suffers from low embeddable size on the chimera graph. In the present study, we propose an alternative approach to solve a large-scale optimization problem on the chimera graph via a well-known method in statistical mechanics called the Hubbard-Stratonovich transformation or its variants. The proposed method can be used to deal with a fully connected Ising model without embedding on the chimera graph and leads to nontrivial results of the optimization problem. We tested the proposed method with a number of partition problems involving solving linear equations and the traffic flow optimization problem in Sendai and Kyoto cities in Japan."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of quantum annealing in solving optimization problems under constraints, and how does the proposed method in the study address this limitation?\n\n**A)** The primary limitation is the sparse connectivity between artificial spins on the chimera graph, which reduces the number of logical spins that can be used to represent optimization problems. The proposed method uses the Hubbard-Stratonovich transformation to overcome this limitation.\n\n**B)** The primary limitation is the inability of quantum annealing to handle fully connected Ising models, which are common in optimization problems. The proposed method uses a variant of the Hubbard-Stratonovich transformation to deal with these models.\n\n**C)** The primary limitation is the need for embedding techniques to reduce the number of logical spins, which is necessary for solving optimization problems on the chimera graph. The proposed method uses the Hubbard-Stratonovich transformation to eliminate the need for embedding.\n\n**D)** The primary limitation is the lack of scalability of quantum annealing to solve large-scale optimization problems. The proposed method uses the Hubbard-Stratonovich transformation to improve the scalability of quantum annealing.\n\n**Correct Answer:** A) The primary limitation is the sparse connectivity between artificial spins on the chimera graph, which reduces the number of logical spins that can be used to represent optimization problems. The proposed method uses the Hubbard-Stratonovich transformation to overcome this limitation.\n\n**Explanation:** The study proposes an alternative approach to solve large-scale optimization problems on the chimera graph using the Hubbard-Stratonovich transformation. This transformation allows for the use of fully connected Ising models, which are common in optimization problems, without the need for embedding techniques. The transformation enables the use of a larger number of logical spins, which improves the scalability of quantum annealing. Therefore, option A is the correct answer."}, "58": {"documentation": {"title": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation", "source": "Xingchen Ma, Matthew B. Blaschko", "docs_id": "2010.03171", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation. Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem, on pruning pre-trained VGG16 and ResNet50 models as well as on searching activation functions of ResNet20. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Bayesian optimization, what is the primary advantage of the proposed additive tree-structured covariance function, as described in the paper \"Additive Tree-Structured Conditional Parameter Spaces in Bayesian Optimization: A Novel Covariance Function and a Fast Implementation\"?\n\n**A)** It allows for the incorporation of prior knowledge about the parameter space into the optimization process.\n**B)** It enables the optimization of the acquisition function in a low-dimensional space, leading to improved sample efficiency.\n**C)** It provides a more flexible and generalizable framework for modeling conditional parameter spaces.\n**D)** It reduces the computational complexity of the optimization algorithm, making it more scalable.\n\n**Correct Answer:** B) It enables the optimization of the acquisition function in a low-dimensional space, leading to improved sample efficiency.\n\n**Explanation:** The correct answer is B) because the paper highlights the development of a parallel algorithm to optimize the acquisition function in a low-dimensional space, which leads to improved sample efficiency. This is a key advantage of the proposed additive tree-structured covariance function. The other options, while related to the topic, are not the primary advantage of the proposed method. Option A is related to the incorporation of prior knowledge, but it is not the main benefit. Option C is related to flexibility and generalizability, but it is not the primary advantage. Option D is related to computational complexity, but it is not the main benefit of the proposed method."}, "59": {"documentation": {"title": "Ab initio study of 2p-core level x-ray photoemission spectra in\n  ferromagnetic transition metals", "source": "Manabu Takahashi and Jun-ichi Igarashi", "docs_id": "1203.0361", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ab initio study of 2p-core level x-ray photoemission spectra in\n  ferromagnetic transition metals. We study the 2p-core level x-ray photoemission spectra in ferromagnetic transition metals, Fe, Co, and Ni using a recently developed ab initio method.The excited final states are set up by distributing electrons on the one-electron states calculated under the fully screened potential in the presence of the core hole. We evaluate the overlap between these excited states and the ground state by using one-electron wave functions, and obtain the spectral curves as a function of binding energy. The calculated spectra reproduce well the observed spectra displaying interesting dependence on the element and on the spin of the removed core electron. The origin of the spectral shapes is elucidated in terms of the one-electron states screening the core hole. The magnetic splitting of the threshold energy is also estimated by using the coherent potential approximation within the fully screened potential approximation. It decreases more rapidly than the local spin moment with moving from Fe to Ni. It is estimated to be almost zero for Ni despite the definite local moment about 0.6\\mu_B, in agreement with the experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism underlying the magnetic splitting of the threshold energy in the 2p-core level x-ray photoemission spectra of ferromagnetic transition metals, as elucidated by the study?\n\nA) The coherent potential approximation within the fully screened potential approximation\nB) The overlap between excited final states and the ground state evaluated using one-electron wave functions\nC) The screening of the core hole by one-electron states\nD) The local spin moment of the removed core electron\n\nCorrect Answer: A) The coherent potential approximation within the fully screened potential approximation\n\nExplanation: The study uses the coherent potential approximation within the fully screened potential approximation to estimate the magnetic splitting of the threshold energy. This approximation is used to evaluate the effect of the core hole on the one-electron states, which in turn affects the spectral curves. The correct answer is A) The coherent potential approximation within the fully screened potential approximation, as it is the primary mechanism underlying the magnetic splitting of the threshold energy.\n\nNote: The other options are incorrect because:\n\n* B) The overlap between excited final states and the ground state is an important aspect of the study, but it is not the primary mechanism underlying the magnetic splitting of the threshold energy.\n* C) The screening of the core hole by one-electron states is a relevant factor in the study, but it is not the primary mechanism underlying the magnetic splitting of the threshold energy.\n* D) The local spin moment of the removed core electron is not directly related to the magnetic splitting of the threshold energy, and is therefore an incorrect answer."}}