{"0": {"documentation": {"title": "A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration\n  Framework", "source": "Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao\n  Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, and Yanzhi Wang", "docs_id": "2003.06513", "section": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration\n  Framework. Weight pruning of deep neural networks (DNNs) has been proposed to satisfy the limited storage and computing capability of mobile edge devices. However, previous pruning methods mainly focus on reducing the model size and/or improving performance without considering the privacy of user data. To mitigate this concern, we propose a privacy-preserving-oriented pruning and mobile acceleration framework that does not require the private training dataset. At the algorithm level of the proposed framework, a systematic weight pruning technique based on the alternating direction method of multipliers (ADMM) is designed to iteratively solve the pattern-based pruning problem for each layer with randomly generated synthetic data. In addition, corresponding optimizations at the compiler level are leveraged for inference accelerations on devices. With the proposed framework, users could avoid the time-consuming pruning process for non-experts and directly benefit from compressed models. Experimental results show that the proposed framework outperforms three state-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN, with speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy loss, while preserving data privacy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the proposed privacy-preserving-oriented pruning and mobile acceleration framework, and how does it address the limitations of previous pruning methods?\n\n**A)** The framework aims to improve the performance of DNNs on mobile edge devices, but it does not consider the privacy of user data. Previous pruning methods focused on reducing model size and improving performance without addressing data privacy concerns.\n\n**B)** The framework is designed to reduce the storage and computing requirements of DNNs on mobile edge devices, but it does not require the private training dataset, thereby preserving user data privacy.\n\n**C)** The framework is intended to improve the accuracy of DNNs on mobile edge devices, but it does not consider the limitations of previous pruning methods, which mainly focused on reducing model size and improving performance.\n\n**D)** The framework is designed to improve the performance of DNNs on mobile edge devices, but it does not address the limitations of previous pruning methods, which mainly focused on reducing model size and improving performance without considering data privacy.\n\n**Correct Answer:** B) The framework is designed to reduce the storage and computing requirements of DNNs on mobile edge devices, but it does not require the private training dataset, thereby preserving user data privacy.\n\n**Explanation:** The correct answer is B) because the proposed framework is motivated by the need to address the limitations of previous pruning methods, which mainly focused on reducing model size and improving performance without considering data privacy. The framework's design, which uses a systematic weight pruning technique based on ADMM and leverages compiler optimizations for inference accelerations, is intended to preserve user data privacy by not requiring the private training dataset."}, "1": {"documentation": {"title": "Jiffy: A Lock-free Skip List with Batch Updates and Snapshots", "source": "Tadeusz Kobus, Maciej Kokoci\\'nski, Pawe{\\l} T. Wojciechowski", "docs_id": "2102.01044", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jiffy: A Lock-free Skip List with Batch Updates and Snapshots. In this paper we introduce Jiffy, the first lock-free, linearizable ordered key-value index that offers both (1) batch updates, which are put and remove operations that are executed atomically, and (2) consistent snapshots used by, e.g., range scan operations. Jiffy is built as a multiversioned lock-free skip list and relies on CPU's Time Stamp Counter register to generate version numbers at minimal cost. For faster skip list traversals and better utilization of the CPU caches, key-value entries are grouped into immutable objects called revisions. Moreover, by changing the size of revisions and thus modifying the synchronization granularity, our index can adapt to varying contentions levels (smaller revisions are more suited for write-heavy workloads whereas large revisions benefit read-dominated workloads, especially when they feature many range scan operations). Structure modifications to the index, which result in changing the size of revisions, happen through (lock-free) skip list node split and merge operations that are carefully coordinated with the update operations. Despite rich semantics, Jiffy offers highly scalable performance, which is comparable or exceeds the performance of the state-of-the-art lock-free ordered indices that feature linearizable range scan operations. Compared to its (lock-based) rivals that also support batch updates, Jiffy can execute large batch updates up to 7.4x more efficiently."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of Jiffy's revision grouping mechanism, and how does it impact the performance of the index in different workloads?\n\n**A)** Jiffy's revision grouping mechanism allows for more efficient use of CPU caches, leading to faster skip list traversals, but it does not adapt to varying contentions levels.\n\n**B)** Jiffy's revision grouping mechanism enables the index to adapt to varying contentions levels by changing the size of revisions, which in turn affects the performance of the index in write-heavy and read-dominated workloads.\n\n**C)** Jiffy's revision grouping mechanism is not related to the performance of the index, and its primary purpose is to improve the scalability of the index.\n\n**D)** Jiffy's revision grouping mechanism allows for more efficient execution of large batch updates, but it does not impact the performance of the index in read-dominated workloads.\n\n**Correct Answer:** B) Jiffy's revision grouping mechanism enables the index to adapt to varying contentions levels by changing the size of revisions, which in turn affects the performance of the index in write-heavy and read-dominated workloads.\n\n**Explanation:** The correct answer is B) because the documentation states that Jiffy's revision grouping mechanism allows the index to adapt to varying contentions levels by changing the size of revisions, which in turn affects the performance of the index in write-heavy and read-dominated workloads. This is a key advantage of Jiffy's design, as it enables the index to optimize its performance for different workloads. The other options are incorrect because they either misrepresent the primary advantage of Jiffy's revision grouping mechanism or ignore its impact on performance."}, "2": {"documentation": {"title": "Micron-size two-dimensional methylammonium lead halide perovskites", "source": "Eugen Klein, Andres Black, \\\"Oznur Tokmak, Christian Strelow,\n  Rostyslav Lesyuk, Christian Klinke", "docs_id": "1907.02360", "section": ["physics.app-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Micron-size two-dimensional methylammonium lead halide perovskites. Hybrid lead halide perovskites with 2D stacking structures have recently emerged as promising materials for optoelectronic applications. We report a method for growing 2D nanosheets of hybrid lead halide perovskites (I, Br and Cl), with tunable lateral sizes ranging from 0.05 to 8 microns, and a structure consisting of n stacked monolayers separated by long alkylamines, tunable from bulk down to n=1. The key to obtaining such a wide range of perovskite properties hinged on utilizing the respective lead halide nanosheets as precursors in a hot-injection synthesis that afforded careful control over all process parameters. The layered, quantum confined (n small than 4) nanosheets were comprised of major and minor fractions with differing n. Energy funneling from low to high n (high to low energy) regions within a single sheet, mediated by the length of the ligands between stacks, produced photoluminescent quantum yields as high as 49 percent. These large, tunable 2D nanosheets could serve as convenient platforms for future high efficiency optoelectronic devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using hybrid lead halide perovskites with 2D stacking structures in optoelectronic applications, and how do the authors achieve this advantage in their synthesis method?\n\nA) The use of 2D stacking structures allows for the creation of perovskites with tunable optical properties, which can be achieved by carefully controlling the length of the alkylamines between stacks.\n\nB) The authors' hot-injection synthesis method enables the growth of perovskites with high photoluminescent quantum yields, which is a result of the energy funneling from low to high n regions within a single sheet.\n\nC) The 2D stacking structures of the perovskites allow for the creation of nanosheets with tunable lateral sizes, which can be achieved by controlling the size of the lead halide nanosheets.\n\nD) The use of 2D stacking structures enables the creation of perovskites with high efficiency optoelectronic devices, which is a result of the layered, quantum confined nanosheets.\n\nCorrect Answer: A) The use of 2D stacking structures allows for the creation of perovskites with tunable optical properties, which can be achieved by carefully controlling the length of the alkylamines between stacks.\n\nExplanation: The correct answer is A) because the authors mention that the energy funneling from low to high n regions within a single sheet, mediated by the length of the ligands between stacks, produces photoluminescent quantum yields as high as 49 percent. This suggests that the tunable optical properties of the perovskites are a result of the careful control of the alkylamine length between stacks. The other options are incorrect because they do not accurately summarize the primary advantage of using 2D stacking structures in optoelectronic applications."}, "3": {"documentation": {"title": "Phase-space study of surface-electrode Paul traps: Integrable, chaotic,\n  and mixed motions", "source": "V. Roberdel, D. Leibfried, D. Ullmo, H. Landa", "docs_id": "1804.01952", "section": ["quant-ph", "nlin.CD", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase-space study of surface-electrode Paul traps: Integrable, chaotic,\n  and mixed motions. We present a comprehensive phase-space treatment of the motion of charged particles in electrodynamic traps. Focusing on five-wire surface-electrode Paul traps, we study the details of integrable and chaotic motion of a single ion. We introduce appropriate phase-space measures and give a universal characterization of the trap effectiveness as a function of the parameters. We rigorously derive the commonly used (time-independent) pseudopotential approximation, quantify its regime of validity and analyze the mechanism of its breakdown within the time-dependent potential. The phase space approach that we develop gives a general framework for describing ion dynamics in a broad variety of surface Paul traps. To probe this framework experimentally, we propose and analyze, using numerical simulations, an experiment that can be realized with an existing four-wire trap. We predict a robust experimental signature of the existence of trapping pockets within a mixed regular and chaotic phase-space structure. Intricately rich escape dynamics suggest that surface traps give access to exploring microscopic Hamiltonian transport phenomena in phase space."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the phase-space study of surface-electrode Paul traps presented in the Arxiv documentation, and what type of motion is expected to be observed in the trap?\n\n**A)** To study the effects of surface-electrode Paul traps on charged particles in terms of their energy levels, and to predict the type of motion that will occur in the trap.\n**B)** To develop a universal characterization of the trap effectiveness as a function of the parameters, and to analyze the mechanism of breakdown of the commonly used pseudopotential approximation.\n**C)** To explore the microscopic Hamiltonian transport phenomena in phase space using surface traps, and to predict a robust experimental signature of the existence of trapping pockets within a mixed regular and chaotic phase-space structure.\n**D)** To compare the performance of five-wire surface-electrode Paul traps with four-wire traps, and to determine the optimal configuration for trapping charged particles.\n\n**Correct Answer:** B) To develop a universal characterization of the trap effectiveness as a function of the parameters, and to analyze the mechanism of breakdown of the commonly used pseudopotential approximation.\n\n**Explanation:**\n\nThe primary goal of the phase-space study is to develop a universal characterization of the trap effectiveness as a function of the parameters, which is stated in the introduction of the documentation. Additionally, the authors aim to analyze the mechanism of breakdown of the commonly used pseudopotential approximation, which is a key aspect of the study. This is evident from the title of the documentation, which mentions the \"pseudopotential approximation\" and its \"regime of validity\". Therefore, option B is the correct answer.\n\nOption A is incorrect because while the study does discuss the effects of surface-electrode Paul traps on charged particles, its primary goal is not to study energy levels. Option C is also incorrect because while the study does explore microscopic Hamiltonian transport phenomena in phase space, it is not the primary goal of the study. Option D is incorrect because the study does not compare the performance of five-wire and four-wire traps, and the optimal configuration is not a primary concern of the study."}, "4": {"documentation": {"title": "Measurements of $\\gamma \\gamma \\to \\mbox{Higgs}$ and $\\gamma \\gamma \\to\n  W^{+}W^{-}$ in $e^{+}e^{-}$ collisions at the Future Circular Collider", "source": "David d'Enterria, Patricia Rebello Teles, Daniel E. Martins", "docs_id": "1712.07023", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurements of $\\gamma \\gamma \\to \\mbox{Higgs}$ and $\\gamma \\gamma \\to\n  W^{+}W^{-}$ in $e^{+}e^{-}$ collisions at the Future Circular Collider. The measurements of the two-photon production of the Higgs boson and of $W^{\\pm}$ boson pairs in $e^{+}e^{-}$ collisions at the Future Circular Collider (FCC-ee) are investigated. The processes $e^{+}e^{-}\\xrightarrow{\\gamma \\gamma}e^+\\,{\\rm H}\\,e^-,e^+\\,{\\rm W^+W^-}\\,e^-$ are computed using the effective photon approximation for electron-positron beams, and studied in their ${\\rm H}\\to b\\bar{b}$ and ${\\rm W^+W^-}\\to 4j$ decay final-states including parton showering and hadronization, jet reconstruction, $e^\\pm$ forward tagging, and realistic experimental cuts. After selection criteria, up to 75 Higgs bosons and 6600 $\\rm W^{\\pm}$ pairs will be reconstructed on top of controllable continuum backgrounds at $\\sqrt{s} = $240 and 350 GeV for the total expected integrated luminosities, by tagging the scattered $e^\\pm$ with near-beam detectors. A 5$\\sigma$ observation of $\\gamma \\gamma \\to$H is thereby warranted, as well as high-statistics studies of triple $\\rm \\gamma WW$ and quartic $\\rm \\gamma\\gamma WW$ electroweak couplings, improving by at least factors of 2 and 10 the current limits on dimension-6 anomalous quartic gauge couplings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Future Circular Collider (FCC-ee) experiment, what is the expected integrated luminosity required to achieve a 5\u03c3 observation of the process \u03b3\u03b3 \u2192 H, and how does this compare to the expected number of reconstructed Higgs bosons and W+W- pairs?\n\n**A)** 100 fb^-1, 75 Higgs bosons, 3300 W+W- pairs\n**B)** 50 fb^-1, 75 Higgs bosons, 6600 W+W- pairs\n**C)** 200 fb^-1, 75 Higgs bosons, 6600 W+W- pairs\n**D)** 150 fb^-1, 75 Higgs bosons, 3300 W+W- pairs\n\n**Correct Answer:** B) 50 fb^-1, 75 Higgs bosons, 6600 W+W- pairs\n\n**Explanation:** According to the documentation, the total expected integrated luminosities for the FCC-ee experiment are 240 and 350 GeV, and up to 75 Higgs bosons and 6600 W+W- pairs will be reconstructed on top of controllable continuum backgrounds. To achieve a 5\u03c3 observation of the process \u03b3\u03b3 \u2192 H, the expected integrated luminosity is 50 fb^-1, which corresponds to the lower luminosity value of 240 GeV."}, "5": {"documentation": {"title": "Riemannian isometries of twisted magnetic flux tubes and stable\n  current-carrying solar loops", "source": "Garcia de Andrade", "docs_id": "astro-ph/0702300", "section": ["astro-ph", "gr-qc", "physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Riemannian isometries of twisted magnetic flux tubes and stable\n  current-carrying solar loops. Two examples of the use of differential geometry in plasma physics are given: The first is the computation and solution of the constraint equations obtained from the Riemann metric isometry of the twisted flux tube. In this case a constraint between the Frenet torsion and curvature is obtained for inhomogeneous helical magnetic flux tube axis. In the second one, geometrical and topological constraints on the current-carrying solar loops are obtained by assuming that the plasma filament is stable. This is analogous to early computations by Liley [(Plasma Physics (1964)] in the case of hydromagnetic equilibria of magnetic surfaces. It is shown that exists a relationship between the ratio of the current components along and cross the plasma filament and the Frenet torsion and curvature. The computations are performed for the helical plasma filaments where torsion and curvature are proportional. The constraints imposed on the electric currents by the energy stability condition are used to solve the remaining magnetohydrodynamical (MHD) equations which in turn allows us to compute magnetic helicity and from them the twist and writhe topological numbers. Magnetic energy is also computed from the solutions of MHD equations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Riemannian isometries of twisted magnetic flux tubes and stable current-carrying solar loops, what is the relationship between the Frenet torsion and curvature for inhomogeneous helical magnetic flux tubes, and how does it relate to the ratio of current components along and cross the plasma filament?\n\n**A)** The Frenet torsion and curvature are inversely proportional, with the torsion being zero when the current components are equal.\n\n**B)** The Frenet torsion and curvature are directly proportional, with the torsion being maximum when the current components are equal.\n\n**C)** The Frenet torsion and curvature are related by a constraint equation, which is given by \u2207_\u03bc \u2207_\u03bd T = -R_\u03bc\u03bd, where T is the Frenet torsion and R is the Riemann curvature tensor.\n\n**D)** The Frenet torsion and curvature are related by a topological constraint, which is given by \u222b T dA = 0, where dA is the area element of the plasma filament.\n\n**Correct Answer:** C) The Frenet torsion and curvature are related by a constraint equation, which is given by \u2207_\u03bc \u2207_\u03bd T = -R_\u03bc\u03bd, where T is the Frenet torsion and R is the Riemann curvature tensor.\n\n**Explanation:** The correct answer is based on the constraint equation obtained from the Riemann metric isometry of the twisted flux tube, which is \u2207_\u03bc \u2207_\u03bd T = -R_\u03bc\u03bd. This equation relates the Frenet torsion and curvature of the plasma filament, and is a fundamental result in the study of Riemannian isometries of twisted magnetic flux tubes and stable current-carrying solar loops."}, "6": {"documentation": {"title": "Fast Bayesian Record Linkage With Record-Specific Disagreement\n  Parameters", "source": "Thomas Stringham", "docs_id": "2003.04238", "section": ["stat.ME", "econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast Bayesian Record Linkage With Record-Specific Disagreement\n  Parameters. Researchers are often interested in linking individuals between two datasets that lack a common unique identifier. Matching procedures often struggle to match records with common names, birthplaces or other field values. Computational feasibility is also a challenge, particularly when linking large datasets. We develop a Bayesian method for automated probabilistic record linkage and show it recovers more than 50% more true matches, holding accuracy constant, than comparable methods in a matching of military recruitment data to the 1900 US Census for which expert-labelled matches are available. Our approach, which builds on a recent state-of-the-art Bayesian method, refines the modelling of comparison data, allowing disagreement probability parameters conditional on non-match status to be record-specific in the smaller of the two datasets. This flexibility significantly improves matching when many records share common field values. We show that our method is computationally feasible in practice, despite the added complexity, with an R/C++ implementation that achieves significant improvement in speed over comparable recent methods. We also suggest a lightweight method for treatment of very common names and show how to estimate true positive rate and positive predictive value when true match status is unavailable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A researcher is tasked with linking individuals between two datasets, one containing military recruitment data and the other containing the 1900 US Census. The datasets lack a common unique identifier, and matching procedures often struggle to match records with common names, birthplaces, or other field values. The researcher develops a Bayesian method for automated probabilistic record linkage, which refines the modelling of comparison data to allow for record-specific disagreement probability parameters conditional on non-match status. This flexibility significantly improves matching when many records share common field values.\n\n**Which of the following statements best describes the key innovation of the Bayesian method developed by the researcher?**\n\nA) The method uses a traditional frequency-based approach to model comparison data, without incorporating record-specific disagreement probability parameters.\nB) The method uses a Bayesian approach to model comparison data, but does not allow for record-specific disagreement probability parameters conditional on non-match status.\nC) The method refines the modelling of comparison data to allow for record-specific disagreement probability parameters conditional on non-match status, significantly improving matching when many records share common field values.\nD) The method uses a non-parametric approach to model comparison data, without relying on any specific distribution or probability model.\n\n**Correct Answer:** C) The method refines the modelling of comparison data to allow for record-specific disagreement probability parameters conditional on non-match status, significantly improving matching when many records share common field values.\n\n**Explanation:** The correct answer, C, highlights the key innovation of the Bayesian method developed by the researcher. By allowing for record-specific disagreement probability parameters conditional on non-match status, the method can better capture the complexities of matching records with common field values, leading to improved matching accuracy. The other options are incorrect because they do not accurately describe the key innovation of the method. Option A is incorrect because the method uses a Bayesian approach, not a traditional frequency-based approach. Option B is incorrect because the method does allow for record-specific disagreement probability parameters conditional on non-match status. Option D is incorrect because the method uses a Bayesian approach, not a non-parametric approach."}, "7": {"documentation": {"title": "Identification and Estimation of Average Partial Effects in\n  Semiparametric Binary Response Panel Models", "source": "Laura Liu, Alexandre Poirier, and Ji-Liang Shiu", "docs_id": "2105.12891", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification and Estimation of Average Partial Effects in\n  Semiparametric Binary Response Panel Models. Average partial effects (APEs) are generally not point-identified in binary response panel models with unrestricted unobserved heterogeneity. We show their point-identification under an index sufficiency assumption on the unobserved heterogeneity, even when the error distribution is unspecified. This assumption does not impose parametric restrictions on the unobserved heterogeneity. We then construct a three-step semiparametric estimator for the APE. In the first step, we estimate the common parameters using either a conditional logit or smoothed maximum score estimator. In the second step, we estimate the conditional expectation of the outcomes given the indices and a generated regressor that depends on first-step estimates. In the third step, we average derivatives of this conditional expectation to obtain a partial mean that estimates the APE. We show that this proposed three-step APE estimator is consistent and asymptotically normal. We evaluate its finite-sample properties in Monte Carlo simulations. We then illustrate our estimator in a study of determinants of married women's labor supply."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Identification and Estimation of Average Partial Effects in Semiparametric Binary Response Panel Models\", what is the key assumption required for the point-identification of Average Partial Effects (APEs) under the three-step semiparametric estimator?\n\nA) The error distribution must be specified as normal.\nB) The unobserved heterogeneity must be index sufficient, and the error distribution is unspecified.\nC) The common parameters must be estimated using a maximum likelihood estimator.\nD) The conditional expectation of the outcomes must be estimated using a linear regression model.\n\n**Correct Answer:** B) The unobserved heterogeneity must be index sufficient, and the error distribution is unspecified.\n\n**Explanation:** The paper states that the index sufficiency assumption on the unobserved heterogeneity is crucial for the point-identification of APEs, even when the error distribution is unspecified. This assumption does not impose parametric restrictions on the unobserved heterogeneity, allowing for a more flexible estimation approach."}, "8": {"documentation": {"title": "Direct Numerical Simulations of Type Ia Supernovae Flames I: The\n  Landau-Darrieus Instability", "source": "J. B. Bell, M. S. Day, C. A. Rendleman, S. E. Woosley, M. Zingale", "docs_id": "astro-ph/0311543", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct Numerical Simulations of Type Ia Supernovae Flames I: The\n  Landau-Darrieus Instability. Planar flames are intrinsically unstable in open domains due to the thermal expansion across the burning front--the Landau-Darrieus instability. This instability leads to wrinkling and growth of the flame surface, and corresponding acceleration of the flame, until it is stabilized by cusp formation. We look at the Landau-Darrieus instability for C/O thermonuclear flames at conditions relevant to the late stages of a Type Ia supernova explosion. Two-dimensional direct numerical simulations of both single-mode and multi-mode perturbations using a low Mach number hydrodynamics code are presented. We show the effect of the instability on the flame speed as a function of both the density and domain size, demonstrate the existence of the small scale cutoff to the growth of the instability, and look for the proposed breakdown of the non-linear stabilization at low densities. The effects of curvature on the flame as quantified through measurements of the growth rate and computation of the corresponding Markstein number. While accelerations of a few percent are observed, they are too small to have any direct outcome on the supernova explosion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of the Landau-Darrieus instability on the flame speed in Type Ia supernovae explosions, and how does it relate to the proposed breakdown of non-linear stabilization at low densities?\n\nA) The instability leads to a significant increase in flame speed, which is stabilized by cusp formation at low densities.\nB) The instability causes a decrease in flame speed, which is accelerated by the growth of the flame surface and corresponding acceleration of the flame.\nC) The instability has no effect on flame speed, and the growth of the flame surface is solely driven by thermal expansion across the burning front.\nD) The instability leads to a breakdown of non-linear stabilization at low densities, resulting in a significant increase in flame speed.\n\nCorrect Answer: D) The instability leads to a breakdown of non-linear stabilization at low densities, resulting in a significant increase in flame speed.\n\nExplanation: The correct answer is D) because the documentation states that the Landau-Darrieus instability leads to the breakdown of non-linear stabilization at low densities, resulting in a significant increase in flame speed. This is supported by the fact that accelerations of a few percent are observed, which are too small to have any direct outcome on the supernova explosion. The other options are incorrect because they either misrepresent the effect of the instability on flame speed (A and B) or incorrectly state the relationship between the instability and non-linear stabilization (C)."}, "9": {"documentation": {"title": "Tracer Diffusion on a Crowded Random Manhattan Lattice", "source": "Carlos Mej\\'ia-Monasterio, Sergei Nechaev, Gleb Oshanin, and Oleg\n  Vasilyev", "docs_id": "1912.03169", "section": ["cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tracer Diffusion on a Crowded Random Manhattan Lattice. We study by extensive numerical simulations the dynamics of a hard-core tracer particle (TP) in presence of two competing types of disorder - frozen convection flows on a square random Manhattan lattice and a crowded dynamical environment formed by a lattice gas of mobile hard-core particles. The latter perform lattice random walks, constrained by a single-occupancy condition of each lattice site, and are either insensitive to random flows (model A) or choose the jump directions as dictated by the local directionality of bonds of the random Manhattan lattice (model B). We focus on the TP disorder-averaged mean-squared displacement, (which shows a super-diffusive behaviour $\\sim t^{4/3}$, $t$ being time, in all the cases studied here), on higher moments of the TP displacement, and on the probability distribution of the TP position $X$ along the $x$-axis. Our analysis evidences that in absence of the lattice gas particles the latter has a Gaussian central part $\\sim \\exp(- u^2)$, where $u = X/t^{2/3}$, and exhibits slower-than-Gaussian tails $\\sim \\exp(-|u|^{4/3})$ for sufficiently large $t$ and $u$. Numerical data convincingly demonstrate that in presence of a crowded environment the central Gaussian part and non-Gaussian tails of the distribution persist for both models."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the behavior of the tracer particle's displacement distribution in the presence of a crowded dynamical environment, specifically model B, where the lattice gas particles choose jump directions dictated by the local directionality of bonds of the random Manhattan lattice. How does the persistence of non-Gaussian tails in the distribution compare to the case without the lattice gas particles?\n\n**A)** The presence of the lattice gas particles in model B leads to a significant increase in the amplitude of the non-Gaussian tails, resulting in a distribution that is more skewed than the Gaussian central part.\n\n**B)** The non-Gaussian tails in the distribution persist for both models, but the amplitude of the tails is reduced in model B due to the random walk constraints imposed by the lattice gas particles.\n\n**C)** The distribution in model B exhibits a power-law decay of the tails, similar to the Gaussian central part, and the non-Gaussian tails are not significantly affected by the presence of the lattice gas particles.\n\n**D)** The non-Gaussian tails in the distribution are completely suppressed in model B due to the random walk constraints imposed by the lattice gas particles, resulting in a distribution that is Gaussian-like.\n\n**Correct Answer:** B) The non-Gaussian tails in the distribution persist for both models, but the amplitude of the tails is reduced in model B due to the random walk constraints imposed by the lattice gas particles.\n\n**Explanation:** The correct answer is B) because the documentation states that the non-Gaussian tails persist for both models, but the amplitude of the tails is reduced in model B due to the random walk constraints imposed by the lattice gas particles. This is evident from the fact that the distribution in model B exhibits slower-than-Gaussian tails, but the amplitude of these tails is reduced compared to the case without the lattice gas particles."}, "10": {"documentation": {"title": "On perturbative limits of quadrupole evolution in QCD at high energy", "source": "Jamal Jalilian-Marian", "docs_id": "1111.3936", "section": ["hep-ph", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On perturbative limits of quadrupole evolution in QCD at high energy. We consider the perturbative (weak field) limit of the small $x$ QCD evolution equation for quadrupole, the normalized trace of four Wilson lines in the fundamental representation, which appears in di-hadron angular correlation in high energy collisions. We linearize the quadrupole evolution equation and then expand the Wilson lines in powers of $g\\, A_{\\mu}$ where $A_{\\mu}$ is the gauge field. The quadratic terms in the expansion ($\\sim g^2\\, A^2$) satisfy the BFKL equation as has been recently shown. We then consider the quartic terms ($\\sim g^4\\, A^4$) in the expansion and show that the linearized quadrupole evolution equation, written in terms of color charge density $\\rho$, reduces to the well-known BJKP equation for the imaginary part of four-reggeized gluon exchange amplitude. We comment on the possibility that the BJKP equation for the evolution of a $n$-reggeized gluon state can be obtained from the JIMWLK evolution equation for the normalized trace of $n$ fundamental Wilson lines when non-linear (recombination) terms are neglected."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the perturbative limit of the small $x$ QCD evolution equation for quadrupole, which is given by:\n\n$$\\frac{\\partial \\mathcal{Q}}{\\partial \\xi} = \\frac{g^2}{2} \\mathcal{Q} \\mathcal{Q}$$\n\nwhere $\\mathcal{Q}$ is the quadrupole operator and $\\xi$ is the scaling variable. Expand the quadrupole operator in powers of $g\\, A_{\\mu}$, where $A_{\\mu}$ is the gauge field, and identify the terms that satisfy the BFKL equation.\n\nA) The quadratic terms ($\\sim g^2\\, A^2$) satisfy the BFKL equation.\n\nB) The quartic terms ($\\sim g^4\\, A^4$) satisfy the BFKL equation.\n\nC) The linearized quadrupole evolution equation, written in terms of color charge density $\\rho$, reduces to the BJKP equation for the imaginary part of four-reggeized gluon exchange amplitude.\n\nD) The BFKL equation for the evolution of a $n$-reggeized gluon state can be obtained from the JIMWLK evolution equation for the normalized trace of $n$ fundamental Wilson lines when non-linear (recombination) terms are neglected.\n\nCorrect Answer: C) The linearized quadrupole evolution equation, written in terms of color charge density $\\rho$, reduces to the BJKP equation for the imaginary part of four-reggeized gluon exchange amplitude.\n\nExplanation: The correct answer is C) because the linearized quadrupole evolution equation, written in terms of color charge density $\\rho$, reduces to the BJKP equation for the imaginary part of four-reggeized gluon exchange amplitude. This is shown in the original documentation, where the quartic terms ($\\sim g^4\\, A^4$) are considered and the linearized quadrupole evolution equation is reduced to the BJKP equation. The other options are incorrect because they do not accurately describe the relationship between the BFKL equation and the evolution equation for quadrupole. Option A is incorrect because the quadratic terms do not satisfy the BFKL equation. Option B is incorrect because the quartic terms do not satisfy the BFKL equation. Option D is incorrect because the BFKL equation for the evolution of a $n$-reggeized gluon state cannot be obtained from the JIMWLK evolution equation for the normalized trace of $n$ fundamental Wilson lines when non-linear (recombination) terms are neglected."}, "11": {"documentation": {"title": "Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles", "source": "Shanyu Zhou and Hulya Seferoglu", "docs_id": "1608.07352", "section": ["cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles. We consider a transportation system of heterogeneously connected vehicles, where not all vehicles are able to communicate. Heterogeneous connectivity in transportation systems is coupled to practical constraints such that (i) not all vehicles may be equipped with devices having communication interfaces, (ii) some vehicles may not prefer to communicate due to privacy and security reasons, and (iii) communication links are not perfect and packet losses and delay occur in practice. In this context, it is crucial to develop control algorithms by taking into account the heterogeneity. In this paper, we particularly focus on making traffic phase scheduling decisions. We develop a connectivity-aware traffic phase scheduling algorithm for heterogeneously connected vehicles that increases the intersection efficiency (in terms of the average number of vehicles that are allowed to pass the intersection) by taking into account the heterogeneity. The simulation results show that our algorithm significantly improves the efficiency of intersections as compared to the baselines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of connectivity-aware traffic phase scheduling for heterogeneously connected vehicles, what is the primary motivation for developing control algorithms that take into account the heterogeneity of the transportation system?\n\n**A)** To improve the accuracy of traffic flow predictions\n**B)** To increase the intersection efficiency by optimizing traffic phase scheduling decisions\n**C)** To reduce the energy consumption of vehicles\n**D)** To enhance the overall safety of the transportation system\n\n**Correct Answer:** B) To increase the intersection efficiency by optimizing traffic phase scheduling decisions\n\n**Explanation:** The question requires the test-taker to understand the main objective of the paper, which is to develop a connectivity-aware traffic phase scheduling algorithm to improve the efficiency of intersections in a transportation system with heterogeneous connectivity. The correct answer, B, is supported by the paper's statement that the algorithm \"increases the intersection efficiency (in terms of the average number of vehicles that are allowed to pass the intersection) by taking into account the heterogeneity.\" The other options are incorrect because they are not the primary motivation for developing such an algorithm."}, "12": {"documentation": {"title": "Splitting Sensitivity of the Ground and 7.6 eV Isomeric States of 229Th", "source": "A.C. Hayes, J.L Friar, P. Moller", "docs_id": "0805.2454", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Splitting Sensitivity of the Ground and 7.6 eV Isomeric States of 229Th. The lowest-known excited state in nuclei is the 7.6 eV isomer of 229Th. This energy is within the range of laser-based investigations that could allow accurate measurements of possible temporal variation of this energy splitting. This in turn could probe temporal variation of the fine-structure constant or other parameters in the nuclear Hamiltonian. We investigate the sensitivity of this transition energy to these quantities. We find that the two states are predicted to have identical deformations and thus the same Coulomb energies within the accuracy of the model (viz., within roughly 30 keV). We therefore find no enhanced sensitivity to variation of the fine-structure constant. In the case of the strong interaction the energy splitting is found to have a complicated dependence on several parameters of the interaction, which makes an accurate prediction of sensitivity to temporal changes of fundamental constants problematical. Neither the strong- nor Coulomb-interaction contributions to the energy splitting of this doublet can be constrained within an accuracy better than a few tens of keV, so that only upper limits can be set on the possible sensitivity to temporal variations of the fundamental constants."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be concluded about the sensitivity of the 7.6 eV isomer transition energy of 229Th to temporal variations of the fine-structure constant and other parameters in the nuclear Hamiltonian?\n\nA) The energy splitting is highly sensitive to variations of the fine-structure constant, and any changes would be detectable with high accuracy.\nB) The energy splitting is predicted to have identical deformations and Coulomb energies for both states, making it insensitive to variations of the fine-structure constant.\nC) The energy splitting is only sensitive to variations of the strong interaction parameters, and not to the fine-structure constant.\nD) The energy splitting is highly sensitive to variations of the strong interaction parameters, but the sensitivity to the fine-structure constant is unknown.\n\nCorrect Answer: B) The energy splitting is predicted to have identical deformations and Coulomb energies for both states, making it insensitive to variations of the fine-structure constant.\n\nExplanation: The text states that the two states are predicted to have identical deformations and thus the same Coulomb energies within the accuracy of the model, which means that the energy splitting is not sensitive to variations of the fine-structure constant. This is a key conclusion of the study, and option B accurately reflects this finding."}, "13": {"documentation": {"title": "Properties and characteristics of the WFIRST H4RG-10 detectors", "source": "Gregory Mosby, Jr., Bernard J. Rauscher, Chris Bennett, Edward .S.\n  Cheng, Stephanie Cheung, Analia Cillis, David Content, Dave Cottingham, Roger\n  Foltz, John Gygax, Robert J. Hill, Jeffrey W. Kruk, Jon Mah, Lane Meier,\n  Chris Merchant, Laddawan Miko, Eric C. Piquette, Augustyn Waczynski, Yiting\n  Wen", "docs_id": "2005.00505", "section": ["astro-ph.IM", "astro-ph.CO", "astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Properties and characteristics of the WFIRST H4RG-10 detectors. The Wide-Field Infrared Survey Telescope (WFIRST) will answer fundamental questions about the evolution of dark energy over time and expand the catalog of known exoplanets into new regions of parameter space. Using a Hubble-sized mirror and 18 newly developed HgCdTe 4K x 4K photodiode arrays (H4RG-10), WFIRST will measure the positions and shapes of hundreds of millions of galaxies, the light curves of thousands of supernovae, and the microlensing signals of over a thousand exoplanets toward the bulge of the Galaxy. These measurements require unprecedented sensitivity and characterization of the Wide Field Instrument (WFI), particularly its detectors. The WFIRST project undertook an extensive detector development program to create focal plane arrays that meet these science requirements. These prototype detectors have been characterized and their performance demonstrated in a relevant space-like environment (thermal vacuum, vibration, acoustic, and radiation testing), advancing the H4RG-10's technology readiness level (TRL) to TRL-6. We present the performance characteristics of these TRL-6 demonstration devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the WFIRST project in terms of the Wide Field Instrument (WFI) detectors, and what is the significance of the H4RG-10 detectors in achieving this goal?\n\n**A)** The WFIRST project aims to measure the positions and shapes of millions of galaxies, and the H4RG-10 detectors are designed to achieve this goal with unprecedented sensitivity. However, the primary goal is not explicitly stated in the provided documentation.\n\n**B)** The WFIRST project aims to measure the positions and shapes of hundreds of millions of galaxies, the light curves of thousands of supernovae, and the microlensing signals of over a thousand exoplanets toward the bulge of the Galaxy. The H4RG-10 detectors are crucial in achieving this goal, as they provide the necessary sensitivity and characterization of the WFI.\n\n**C)** The WFIRST project aims to expand the catalog of known exoplanets into new regions of parameter space, and the H4RG-10 detectors are designed to achieve this goal by providing high-resolution images of the exoplanet microlensing signals. However, the primary goal is not explicitly stated in the provided documentation.\n\n**D)** The WFIRST project aims to answer fundamental questions about the evolution of dark energy over time, and the H4RG-10 detectors are designed to achieve this goal by providing high-resolution images of the galaxy positions and shapes. However, the primary goal is not explicitly stated in the provided documentation.\n\n**Correct Answer:** B) The WFIRST project aims to measure the positions and shapes of hundreds of millions of galaxies, the light curves of thousands of supernovae, and the microlensing signals of over a thousand exoplanets toward the bulge of the Galaxy. The H4RG-10 detectors are crucial in achieving this goal, as they provide the necessary sensitivity and characterization of the WFI.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the primary goal of the WFIRST project and the significance of the H4RG-10 detectors in achieving this goal. The other options are incorrect because they either omit or misstate the primary goal of the WFIRST project, or they focus on secondary aspects of the project."}, "14": {"documentation": {"title": "A level-set approach to the control of state-constrained McKean-Vlasov\n  equations: application to renewable energy storage and portfolio selection", "source": "Maximilien Germain (EDF R&D OSIRIS, EDF R&D, EDF, LPSM), Huy\\^en Pham\n  (LPSM, CREST, FiME Lab), Xavier Warin (EDF R&D OSIRIS, EDF R&D, EDF, FiME\n  Lab)", "docs_id": "2112.11059", "section": ["math.OC", "math.PR", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A level-set approach to the control of state-constrained McKean-Vlasov\n  equations: application to renewable energy storage and portfolio selection. We consider the control of McKean-Vlasov dynamics (or mean-field control) with probabilistic state constraints. We rely on a level-set approach which provides a representation of the constrained problem in terms of an unconstrained one with exact penalization and running maximum or integral cost. The method is then extended to the common noise setting. Our work extends (Bokanowski, Picarelli, and Zidani, SIAM J. Control Optim. 54.5 (2016), pp. 2568--2593) and (Bokanowski, Picarelli, and Zidani, Appl. Math. Optim. 71 (2015), pp. 125--163) to a mean-field setting. The reformulation as an unconstrained problem is particularly suitable for the numerical resolution of the problem, that is achieved from an extension of a machine learning algorithm from (Carmona, Lauri{\\`e}re, arXiv:1908.01613 to appear in Ann. Appl. Prob., 2019). A first application concerns the storage of renewable electricity in the presence of mean-field price impact and another one focuses on a mean-variance portfolio selection problem with probabilistic constraints on the wealth. We also illustrate our approach for a direct numerical resolution of the primal Markowitz continuous-time problem without relying on duality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a McKean-Vlasov dynamics system with probabilistic state constraints, where the control objective is to minimize the expected running maximum cost subject to a constraint on the state. The system is subject to common noise, and the state constraints are represented using a level-set approach.\n\n**A)** The optimal control problem can be reformulated as an unconstrained problem using exact penalization, where the penalty term is proportional to the maximum value of the state constraint.\n\n**B)** The level-set approach can be used to extend the mean-field control problem to a common noise setting, where the noise is modeled as a perturbation of the state dynamics.\n\n**C)** The reformulation of the optimal control problem as an unconstrained problem is particularly suitable for numerical resolution using machine learning algorithms, such as those based on neural networks.\n\n**D)** The McKean-Vlasov dynamics system can be solved using a direct numerical resolution approach, where the primal Markowitz continuous-time problem is solved without relying on duality.\n\n**Correct Answer:** C) The reformulation of the optimal control problem as an unconstrained problem is particularly suitable for numerical resolution using machine learning algorithms, such as those based on neural networks.\n\n**Explanation:** The correct answer is C) because the reformulation of the optimal control problem as an unconstrained problem is particularly suitable for numerical resolution using machine learning algorithms, such as those based on neural networks. This is stated in the documentation as \"The method is then extended to the common noise setting. Our work extends (Bokanowski, Picarelli, and Zidani, SIAM J. Control Optim. 54.5 (2016), pp. 2568--2593) and (Bokanowski, Picarelli, and Zidani, Appl. Math. Optim. 71 (2015), pp. 125--163) to a mean-field setting. The reformulation as an unconstrained problem is particularly suitable for the numerical resolution of the problem, that is achieved from an extension of a machine learning algorithm from (Carmona, Lauri{\\`e}re, arXiv:1908.01613 to appear in Ann. Appl. Prob., 2019)\"."}, "15": {"documentation": {"title": "Nutritional Regulation Influencing Colony Dynamics and Task Allocations\n  in Social Insect Colonies", "source": "Feng Rao, Marisabel Rodriguez Messan, Angelica Marquez, Nathan Smith,\n  Yun Kang", "docs_id": "2005.03432", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nutritional Regulation Influencing Colony Dynamics and Task Allocations\n  in Social Insect Colonies. In this paper, we use an adaptive modeling framework to model and study how nutritional status (measured by the protein to carbohydrate ratio) may regulate population dynamics and foraging task allocation of social insect colonies. Mathematical analysis of our model shows that both investment to brood rearing and brood nutrition are important for colony survival and dynamics. When division of labor and/or nutrition are in an intermediate value range, the model undergoes a backward bifurcation and creates multiple attractors due to bistability. This bistability implies that there is a threshold population size required for colony survival. When the investment in brood is large enough or nutritional requirements are less strict the colony tends to survive, otherwise the colony faces collapse. Our model suggests that the needs of colony survival are shaped by the brood survival probability, which requires good nutritional status. As a consequence, better nutritional status can lead to a better survival rate of larvae, and thus a larger worker population."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Nutritional Regulation Influencing Colony Dynamics and Task Allocations\" in Social Insect Colonies, what is the implication of the model's bistability and backward bifurcation on the colony's survival dynamics?\n\n**A)** The colony's survival is guaranteed above a certain threshold population size, regardless of nutritional status.\n**B)** The colony's survival is dependent on the balance between investment in brood rearing and nutritional requirements, with bistability creating multiple attractors.\n**C)** The colony's survival is only possible when the investment in brood rearing is extremely high, and nutritional requirements are minimal.\n**D)** The colony's survival is not affected by the protein to carbohydrate ratio, and is solely determined by the division of labor.\n\n**Correct Answer:** B) The colony's survival is dependent on the balance between investment in brood rearing and nutritional requirements, with bistability creating multiple attractors.\n\n**Explanation:** The correct answer is B) because the model's bistability and backward bifurcation imply that there is a threshold population size required for colony survival, and that the balance between investment in brood rearing and nutritional requirements is crucial for this survival. When this balance is disrupted, the colony may undergo a backward bifurcation, creating multiple attractors and potentially leading to colony collapse. This requires an understanding of the model's dynamics and the implications of bistability on colony survival."}, "16": {"documentation": {"title": "Neural Topic Model via Optimal Transport", "source": "He Zhao, Dinh Phung, Viet Huynh, Trung Le, Wray Buntine", "docs_id": "2008.13537", "section": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neural Topic Model via Optimal Transport. Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a new neural topic model via optimal transport (OT), and how does it address the shortcomings of existing Neural Topic Models (NTMs)?\n\n**A)** The new model is designed to improve the reparameterisation requirement of NTMs, allowing for more flexible training. However, this does not address the issue of achieving good document representation and coherent/diverse topics simultaneously.\n\n**B)** The new model is intended to overcome the limitations of NTMs in discovering coherent and diverse topics, particularly in short documents. By directly minimising the OT distance to the document's word distributions, the model can learn the topic distribution in a more efficient and effective manner.\n\n**C)** The new model is proposed to alleviate the performance degradation of NTMs on short documents by using a differentiable loss function. However, this does not address the issue of achieving coherent and diverse topics.\n\n**D)** The new model is designed to improve the representation quality of NTMs by using a cost matrix that models the weights between topics and words. However, this does not address the issue of discovering coherent and diverse topics.\n\n**Correct Answer:** B) The new model is intended to overcome the limitations of NTMs in discovering coherent and diverse topics, particularly in short documents. By directly minimising the OT distance to the document's word distributions, the model can learn the topic distribution in a more efficient and effective manner.\n\n**Explanation:** The correct answer is B) because the new model is specifically designed to address the shortcomings of existing NTMs, including the difficulty of achieving good document representation and coherent/diverse topics simultaneously, particularly in short documents. The use of OT distance to directly learn the topic distribution is a key innovation that enables the model to overcome these limitations."}, "17": {"documentation": {"title": "Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of\n  Shared Servers for Low Delay", "source": "Arpan Mukhopadhyay, A. Karthik, Ravi R. Mazumdar", "docs_id": "1502.05786", "section": ["cs.DC", "cs.PF", "cs.SY", "math.PR", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of\n  Shared Servers for Low Delay. We consider the job assignment problem in a multi-server system consisting of $N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$) different types according to their processing capacity or speed. Jobs of random sizes arrive at the system according to a Poisson process with rate $N \\lambda$. Upon each arrival, a small number of servers from each type is sampled uniformly at random. The job is then assigned to one of the sampled servers based on a selection rule. We propose two schemes, each corresponding to a specific selection rule that aims at reducing the mean sojourn time of jobs in the system. We first show that both methods achieve the maximal stability region. We then analyze the system operating under the proposed schemes as $N \\to \\infty$ which corresponds to the mean field. Our results show that asymptotic independence among servers holds even when $M$ is finite and exchangeability holds only within servers of the same type. We further establish the existence and uniqueness of stationary solution of the mean field and show that the tail distribution of server occupancy decays doubly exponentially for each server type. When the estimates of arrival rates are not available, the proposed schemes offer simpler alternatives to achieving lower mean sojourn time of jobs, as shown by our numerical studies."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the job assignment problem in a multi-server system, consider the proposed schemes for reducing the mean sojourn time of jobs in the system. What is the limiting behavior of the system as the number of servers (N) approaches infinity, and what implications does this have for the independence and exchangeability of servers?\n\n**A)** As N approaches infinity, the system exhibits asymptotic independence among servers, and exchangeability holds within servers of the same type. The mean field analysis reveals that the tail distribution of server occupancy decays exponentially.\n\n**B)** As N approaches infinity, the system exhibits asymptotic independence among servers, but exchangeability holds only within servers of the same type. The mean field analysis reveals that the tail distribution of server occupancy decays doubly exponentially.\n\n**C)** As N approaches infinity, the system exhibits asymptotic independence among servers, but exchangeability holds only within servers of the same type. The mean field analysis reveals that the tail distribution of server occupancy decays polynomially.\n\n**D)** As N approaches infinity, the system exhibits asymptotic independence among servers, but exchangeability holds only within servers of the same type. The mean field analysis reveals that the tail distribution of server occupancy decays exponentially, but with a non-zero probability of server occupancy.\n\n**Correct Answer:** B) As N approaches infinity, the system exhibits asymptotic independence among servers, but exchangeability holds only within servers of the same type. The mean field analysis reveals that the tail distribution of server occupancy decays doubly exponentially.\n\n**Explanation:** The correct answer is based on the analysis of the system as N approaches infinity, which reveals that asymptotic independence among servers holds, but exchangeability holds only within servers of the same type. Additionally, the mean field analysis shows that the tail distribution of server occupancy decays doubly exponentially, which is a key result of the paper."}, "18": {"documentation": {"title": "The different energy loss mechanisms of inclusive and b-tagged\n  reconstructed jets within ultra-relativistic heavy-ion collisions", "source": "Florian Senzel, Jan Uphoff, Zhe Xu, Carsten Greiner", "docs_id": "1602.05086", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The different energy loss mechanisms of inclusive and b-tagged\n  reconstructed jets within ultra-relativistic heavy-ion collisions. The phenomenon of jet quenching provides essential information about the properties of hot and dense matter created in ultra-relativistic heavy-ion collisions. Recent results from experiments at the Large Hadron Collider (LHC) show evidence for an unexpectedly similar suppression of both light and heavy flavor jets. Furthermore, the role of radiative energy loss of heavy quarks is still under active discussion within the theoretical community. By employing the parton cascade Boltzmann Approach to Multi-Parton Scatterings (BAMPS), which numerically solves the 3+1D Boltzmann equation both for light and heavy flavor partons, we calculate the nuclear modification factor of inclusive and b-tagged reconstructed jets in 0-10% central $\\sqrt{s_{\\rm LHC}}$ = 2.76 ATeV Pb+Pb collisions. Based on perturbative QCD cross sections we find a suppression of both light and heavy flavor jets. While the inclusive jets are slightly too strong suppressed within BAMPS in comparison with data, both elastic+radiative and only elastic interactions lead to a realistic b-tagged jet suppression. To further investigate light and heavy flavor energy loss we predict the R dependence of inclusive and b-tagged jet suppression. Furthermore, we propose the medium modification of b-tagged jet shapes as an observable for discriminating between different heavy quark energy loss scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the study on the nuclear modification factor of inclusive and b-tagged reconstructed jets in ultra-relativistic heavy-ion collisions, and how does it relate to the role of radiative energy loss of heavy quarks?\n\nA) The study suggests that radiative energy loss of heavy quarks is the primary mechanism responsible for jet quenching, and that this process is more significant for heavy flavor jets than for light flavor jets.\nB) The study indicates that the nuclear modification factor of inclusive jets is slightly too strong suppressed within the BAMPS model, while b-tagged jets exhibit a more realistic suppression, suggesting that radiative energy loss plays a crucial role in heavy quark energy loss.\nC) The study reveals that the R dependence of inclusive and b-tagged jet suppression is consistent with the predictions of the BAMPS model, and that this dependence can be used to discriminate between different heavy quark energy loss scenarios.\nD) The study concludes that the suppression of both light and heavy flavor jets is equally significant, and that the difference in suppression between the two is due to the presence of radiative energy loss in heavy quark interactions.\n\nCorrect Answer: B) The study indicates that the nuclear modification factor of inclusive jets is slightly too strong suppressed within the BAMPS model, while b-tagged jets exhibit a more realistic suppression, suggesting that radiative energy loss plays a crucial role in heavy quark energy loss.\n\nExplanation: The correct answer is B) because the study finds that the inclusive jets are slightly too strong suppressed within the BAMPS model, while b-tagged jets exhibit a more realistic suppression. This suggests that radiative energy loss plays a crucial role in heavy quark energy loss, and that the BAMPS model is able to capture this effect more accurately for b-tagged jets. The other options are incorrect because they either misinterpret the results of the study or fail to account for the differences in suppression between inclusive and b-tagged jets."}, "19": {"documentation": {"title": "The Chiral Gross-Neveu model on the lattice via a Landau-forbidden phase\n  transition", "source": "Gertian Roose, Jutho Haegeman, Karel Van Acoleyen, Laurens\n  Vanderstraeten and Nick Bultinck", "docs_id": "2111.14652", "section": ["hep-th", "cond-mat.str-el", "hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Chiral Gross-Neveu model on the lattice via a Landau-forbidden phase\n  transition. We study the phase diagram of the $(1+1)$-dimensional Gross-Neveu model with both $g_x^2(\\bar{\\psi}\\psi)^2$ and $g_y^2(\\bar{\\psi}i\\gamma_5\\psi)^2$ interaction terms on a spatial lattice. The continuous chiral symmetry, which is present in the continuum model when $g_x^2=g_y^2$, has a mixed 't~Hooft anomaly with the charge conservation symmetry, which guarantees the existence of a massless mode. However, the same 't~Hooft anomaly also implies that the continuous chiral symmetry is broken explicitly in our lattice model. Nevertheless, from numerical matrix product state simulations we find that, for certain parameters of the lattice model, the continuous chiral symmetry reemerges in the infrared fixed point theory, even at strong coupling. We argue that, to understand this phenomenon, it is crucial to go beyond mean-field theory (or, equivalently, beyond the leading order term in a $1/N$ expansion). Interestingly, on the lattice, the chiral Gross-Neveu model appears at a Landau-forbidden second order phase transition separating two distinct and unrelated symmetry-breaking orders. We point out the crucial role of two different 't Hooft anomalies or Lieb-Schultz-Mattis obstructions for this Landau-forbidden phase transition to occur."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Chiral Gross-Neveu model on a lattice, what is the role of the 't~Hooft anomaly in the phase diagram, and how does it relate to the emergence of a massless mode and the reemergence of continuous chiral symmetry in the infrared fixed point theory?\n\nA) The 't~Hooft anomaly is responsible for the breaking of continuous chiral symmetry, leading to the emergence of a massless mode, but does not play a role in the reemergence of continuous chiral symmetry in the infrared fixed point theory.\n\nB) The 't~Hooft anomaly is a mixed 't~Hooft anomaly with charge conservation symmetry, which guarantees the existence of a massless mode, but does not affect the reemergence of continuous chiral symmetry in the infrared fixed point theory.\n\nC) The 't~Hooft anomaly is a Lieb-Schultz-Mattis obstruction that prevents the emergence of a massless mode, but does not play a role in the reemergence of continuous chiral symmetry in the infrared fixed point theory.\n\nD) The 't~Hooft anomaly is a mixed 't~Hooft anomaly with charge conservation symmetry, which guarantees the existence of a massless mode, and plays a crucial role in the reemergence of continuous chiral symmetry in the infrared fixed point theory, even at strong coupling.\n\nCorrect Answer: D) The 't~Hooft anomaly is a mixed 't~Hooft anomaly with charge conservation symmetry, which guarantees the existence of a massless mode, and plays a crucial role in the reemergence of continuous chiral symmetry in the infrared fixed point theory, even at strong coupling.\n\nExplanation: The correct answer is D) because the 't~Hooft anomaly is indeed a mixed 't~Hooft anomaly with charge conservation symmetry, which guarantees the existence of a massless mode. Additionally, the same 't~Hooft anomaly also implies that the continuous chiral symmetry is broken explicitly in the lattice model, but the numerical matrix product state simulations find that the continuous chiral symmetry reemerges in the infrared fixed point theory, even at strong coupling. This suggests that the 't~Hooft anomaly plays a crucial role in the reemergence of continuous chiral symmetry in the infrared fixed point theory."}, "20": {"documentation": {"title": "Highly intensive data dissemination in complex networks", "source": "Gabriele D'Angelo, Stefano Ferretti", "docs_id": "1507.08417", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Highly intensive data dissemination in complex networks. This paper presents a study on data dissemination in unstructured Peer-to-Peer (P2P) network overlays. The absence of a structure in unstructured overlays eases the network management, at the cost of non-optimal mechanisms to spread messages in the network. Thus, dissemination schemes must be employed that allow covering a large portion of the network with a high probability (e.g.~gossip based approaches). We identify principal metrics, provide a theoretical model and perform the assessment evaluation using a high performance simulator that is based on a parallel and distributed architecture. A main point of this study is that our simulation model considers implementation technical details, such as the use of caching and Time To Live (TTL) in message dissemination, that are usually neglected in simulations, due to the additional overhead they cause. Outcomes confirm that these technical details have an important influence on the performance of dissemination schemes and that the studied schemes are quite effective to spread information in P2P overlay networks, whatever their topology. Moreover, the practical usage of such dissemination mechanisms requires a fine tuning of many parameters, the choice between different network topologies and the assessment of behaviors such as free riding. All this can be done only using efficient simulation tools to support both the network design phase and, in some cases, at runtime."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of data dissemination in unstructured Peer-to-Peer (P2P) network overlays, what is the primary advantage of using gossip-based approaches, and how do these approaches address the challenge of non-optimal message spreading in unstructured networks?\n\n**A)** Gossip-based approaches provide a structured framework for message dissemination, allowing for more efficient spreading of information in unstructured networks. However, this comes at the cost of increased network management complexity.\n\n**B)** Gossip-based approaches enable the covering of a large portion of the network with a high probability, thereby addressing the challenge of non-optimal message spreading in unstructured networks. This is achieved through the use of caching and Time To Live (TTL) in message dissemination.\n\n**C)** Gossip-based approaches are limited in their ability to spread information in unstructured networks, as they rely on a structured framework that is not present in these networks. This leads to inefficient message dissemination and poor network performance.\n\n**D)** Gossip-based approaches are not effective in spreading information in unstructured networks, as they do not account for the absence of a structure in these networks. This results in non-optimal message spreading and poor network performance.\n\n**Correct Answer:** B) Gossip-based approaches enable the covering of a large portion of the network with a high probability, thereby addressing the challenge of non-optimal message spreading in unstructured networks. This is achieved through the use of caching and Time To Live (TTL) in message dissemination.\n\n**Explanation:** The correct answer, B, highlights the primary advantage of gossip-based approaches in addressing the challenge of non-optimal message spreading in unstructured networks. The use of caching and Time To Live (TTL) in message dissemination enables the covering of a large portion of the network with a high probability, thereby improving the efficiency of message spreading. This is a key finding of the study presented in the Arxiv paper."}, "21": {"documentation": {"title": "A new approach to barrier-top fission dynamics", "source": "G.F. Bertsch and J.M. Mehlhaff", "docs_id": "1511.01936", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new approach to barrier-top fission dynamics. We proposed a calculational framework for describing induced fission that avoids the Bohr-Wheeler assumption of well-defined fission channels. The building blocks of our approach are configurations that form a discrete, orthogonal basis and can be characterized by both energy and shape. The dynamics is to be determined by interaction matrix elements between the states rather than by a Hill-Wheeler construction of a collective coordinate. Within our approach, several simple limits can be seen: diffusion; quantized conductance; and ordinary decay through channels. The specific proposal for the discrete basis is to use the $K^\\pi$ quantum numbers of the axially symmetric Hartree-Fock approximation to generate the configurations. Fission paths would be determined by hopping from configuration to configuration via the residual interaction. We show as an example the configurations needed to describe a fictitious fission decay $^{32}{\\rm S} \\rightarrow ^{16}{\\rm O} + ^{16}{\\rm O}$. We also examine the geometry of the path for fission of $^{236}$U, measuring distances by the number of jumps needed to go to a new $K^\\pi$ partition."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the key assumption that the proposed calculational framework for induced fission dynamics avoids, and how does it differ from the traditional Hill-Wheeler approach?\n\nA) The framework assumes a continuous energy landscape, whereas the Hill-Wheeler approach assumes a discrete set of fission channels.\nB) The framework assumes a well-defined fission channel, whereas the Hill-Wheeler approach assumes a continuous energy landscape.\nC) The framework assumes a quantized conductance, whereas the Hill-Wheeler approach assumes a diffusion process.\nD) The framework assumes a Hill-Wheeler construction of a collective coordinate, whereas the Hill-Wheeler approach assumes a discrete set of fission channels.\n\nCorrect Answer: B) The framework assumes a well-defined fission channel, whereas the Hill-Wheeler approach assumes a continuous energy landscape.\n\nExplanation: The proposed framework avoids the Bohr-Wheeler assumption of well-defined fission channels, which is a key assumption of the traditional Hill-Wheeler approach. Instead, the framework uses a discrete, orthogonal basis to describe the configurations, and the dynamics is determined by interaction matrix elements between the states. This approach allows for a more flexible and accurate description of induced fission dynamics."}, "22": {"documentation": {"title": "A Monte Carlo Study of Multiplicity Fluctuations in Pb-Pb Collisions at\n  LHC Energies", "source": "Ramni Gupta", "docs_id": "1501.03773", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Monte Carlo Study of Multiplicity Fluctuations in Pb-Pb Collisions at\n  LHC Energies. With large volumes of data available from LHC, it has become possible to study the multiplicity distributions for the various possible behaviours of the multiparticle production in collisions of relativistic heavy ion collisions, where a system of dense and hot partons has been created. In this context it is important and interesting as well to check how well the Monte Carlo generators can describe the properties or the behaviour of multiparticle production processes. One such possible behaviour is the self-similarity in the particle production, which can be studied with the intermittency studies and further with chaoticity/erraticity, in the heavy ion collisions. We analyse the behaviour of erraticity index in central Pb-Pb collisions at centre of mass energy of 2.76 TeV per nucleon using the AMPT monte carlo event generator, following the recent proposal by R.C. Hwa and C.B. Yang, concerning the local multiplicity fluctuation study as a signature of critical hadronization in heavy-ion collisions. We report the values of erraticity index for the two versions of the model with default settings and their dependence on the size of the phase space region. Results presented here may serve as a reference sample for the experimental data from heavy ion collisions at these energies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary purpose of the study on multiplicity fluctuations in Pb-Pb collisions at LHC energies, as described in the given Arxiv documentation?\n\nA) To investigate the effects of particle production on the behavior of relativistic heavy ion collisions\nB) To evaluate the performance of Monte Carlo generators in simulating multiparticle production processes\nC) To analyze the self-similarity in particle production and its relation to chaoticity/erraticity in heavy ion collisions\nD) To compare the results of different Monte Carlo event generators in simulating multiparticle production\n\nCorrect Answer: B) To evaluate the performance of Monte Carlo generators in simulating multiparticle production processes\n\nExplanation: The correct answer is B) To evaluate the performance of Monte Carlo generators in simulating multiparticle production processes. The study aims to check how well the Monte Carlo generators can describe the properties or behavior of multiparticle production processes, which is a crucial aspect of understanding the complex phenomena in heavy ion collisions. The other options, while related to the topic, are not the primary purpose of the study. Option A is too broad, option C is a specific aspect of the study, and option D is not the main focus of the study."}, "23": {"documentation": {"title": "Causality constraints in Quadratic Gravity", "source": "Jose D. Edelstein, Rajes Ghosh, Alok Laddha and Sudipta Sarkar", "docs_id": "2107.07424", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causality constraints in Quadratic Gravity. Classifying consistent effective field theories for the gravitational interaction has recently been the subject of intense research. Demanding the absence of causality violation in high energy graviton scattering processes has led to a hierarchy of constraints on higher derivative terms in the Lagrangian. Most of these constraints have relied on analysis that is performed in general relativistic backgrounds, as opposed to a generic solution to the equations of motion which are perturbed by higher curvature operators. Hence, these constraints are necessary but may not be sufficient to ensure that the theory is consistent. In this context, we explore the so-called CEMZ causality constraints on Quadratic Gravity in a space of shock wave solutions beyond GR. We show that the Shapiro time delay experienced by a graviton is polarization-independent and positive, regardless of the strength of the gravitational couplings. Our analysis shows that as far as the causality constraints are concerned, albeit inequivalent to General Relativity due to additional propagating modes, Quadratic Gravity is causal as per as the diagnostic proposed by CEMZ."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Quadratic Gravity, what is the implication of the CEMZ causality constraints on the Shapiro time delay experienced by a graviton, and how does this relate to the consistency of the theory?\n\nA) The Shapiro time delay is polarization-dependent and negative, indicating a violation of causality, which would necessitate the introduction of additional propagating modes to restore causality.\n\nB) The Shapiro time delay is polarization-independent and positive, regardless of the strength of the gravitational couplings, indicating that Quadratic Gravity is causal and consistent with the diagnostic proposed by CEMZ.\n\nC) The Shapiro time delay is only positive for high-energy gravitons, while being negative for low-energy gravitons, which would imply a violation of causality for low-energy processes.\n\nD) The Shapiro time delay is a measure of the gravitational coupling strength, and its positivity or negativity is a diagnostic for the consistency of the theory.\n\nCorrect Answer: B) The Shapiro time delay is polarization-independent and positive, regardless of the strength of the gravitational couplings, indicating that Quadratic Gravity is causal and consistent with the diagnostic proposed by CEMZ.\n\nExplanation: The correct answer is B) because the text states that the Shapiro time delay experienced by a graviton is \"polarization-independent and positive, regardless of the strength of the gravitational couplings\". This implies that the theory is causal and consistent with the diagnostic proposed by CEMZ, as the Shapiro time delay is a diagnostic for causality in high-energy graviton scattering processes. The other options are incorrect because they either imply a violation of causality (A and C) or misinterpret the Shapiro time delay as a measure of gravitational coupling strength (D)."}, "24": {"documentation": {"title": "General Principles of Learning-Based Multi-Agent Systems", "source": "David H. Wolpert, Kevin R. Wheeler, Kagan Tumer", "docs_id": "cs/9905005", "section": ["cs.MA", "nlin.AO", "cond-mat.stat-mech", "cs.DC", "cs.LG", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "General Principles of Learning-Based Multi-Agent Systems. We consider the problem of how to design large decentralized multi-agent systems (MAS's) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to ``work at cross-purposes'' as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur's bar problem (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Artificial Collective Intelligence (COIN) framework, and how does it address the issue of agents working at cross-purposes in achieving a global goal?\n\n**A)** To minimize the number of agents in a decentralized system, while maximizing the overall performance of the system.\n**B)** To design a framework that automatically sets and updates reward functions for each agent to achieve a global goal, while preventing agents from working at cross-purposes.\n**C)** To develop a mathematical framework for analyzing the behavior of decentralized multi-agent systems, without providing any solutions for improving their performance.\n**D)** To create a new type of reinforcement learning algorithm that can be used to solve complex optimization problems.\n\n**Correct Answer:** B) To design a framework that automatically sets and updates reward functions for each agent to achieve a global goal, while preventing agents from working at cross-purposes.\n\n**Explanation:** The correct answer is B) because the COIN framework is designed to address the problem of agents working at cross-purposes in achieving a global goal. The framework uses reinforcement learning algorithms for each agent, and the goal is to automatically set and update the reward functions to ensure that the agents work together to achieve the global goal. This is in contrast to the other options, which do not accurately describe the primary goal of the COIN framework."}, "25": {"documentation": {"title": "Bell correlations between light and vibration at ambient conditions", "source": "Santiago Tarrago Velez, Vivishek Sudhir, Nicolas Sangouard, Christophe\n  Galland", "docs_id": "1912.04502", "section": ["quant-ph", "cond-mat.mes-hall", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bell correlations between light and vibration at ambient conditions. Time-resolved Raman spectroscopy techniques offer various ways to study the dynamics of molecular vibrations in liquids or gases and optical phonons in crystals. While these techniques give access to the coherence time of the vibrational modes, they are not able to reveal the fragile quantum correlations that are spontaneously created between light and vibration during the Raman interaction. Here, we present a scheme leveraging universal properties of spontaneous Raman scattering to demonstrate Bell correlations between light and a collective molecular vibration. We measure the decay of these hybrid photon-phonon Bell correlations with sub-picosecond time resolution and find that they survive over several hundred oscillations at ambient conditions. Our method offers a universal approach to generate entanglement between light and molecular vibrations. Moreover, our results pave the way for the study of quantum correlations in more complex solid-state and molecular systems in their natural state."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of time-resolved Raman spectroscopy techniques in studying the dynamics of molecular vibrations in liquids or gases, and how does the proposed scheme in the paper address this limitation?\n\nA) Time-resolved Raman spectroscopy techniques are unable to resolve the fragile quantum correlations between light and vibration, but they can measure the coherence time of vibrational modes. The proposed scheme does not address this limitation.\n\nB) Time-resolved Raman spectroscopy techniques can measure the coherence time of vibrational modes, but they are unable to reveal the fragile quantum correlations between light and vibration. The proposed scheme leverages universal properties of spontaneous Raman scattering to demonstrate Bell correlations between light and a collective molecular vibration.\n\nC) Time-resolved Raman spectroscopy techniques can measure the coherence time of vibrational modes and reveal the fragile quantum correlations between light and vibration. The proposed scheme does not address this limitation.\n\nD) Time-resolved Raman spectroscopy techniques are unable to measure the coherence time of vibrational modes, but they can reveal the fragile quantum correlations between light and vibration. The proposed scheme does not address this limitation.\n\n**Correct Answer:** B) Time-resolved Raman spectroscopy techniques can measure the coherence time of vibrational modes, but they are unable to reveal the fragile quantum correlations between light and vibration. The proposed scheme leverages universal properties of spontaneous Raman scattering to demonstrate Bell correlations between light and a collective molecular vibration.\n\n**Explanation:** The question requires the test-taker to understand the limitations of time-resolved Raman spectroscopy techniques and how the proposed scheme in the paper addresses this limitation. The correct answer, B, highlights the fact that the techniques can measure the coherence time of vibrational modes, but they are unable to reveal the fragile quantum correlations between light and vibration. The proposed scheme, on the other hand, leverages universal properties of spontaneous Raman scattering to demonstrate Bell correlations between light and a collective molecular vibration, thereby addressing this limitation."}, "26": {"documentation": {"title": "Monitoring Large Crowds With WiFi: A Privacy-Preserving Approach", "source": "Jean-Fran\\c{c}ois Determe and Sophia Azzagnuni and Utkarsh Singh and\n  Fran\\c{c}ois Horlin and Philippe De Doncker", "docs_id": "2010.10370", "section": ["eess.SY", "cs.AR", "cs.CR", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Monitoring Large Crowds With WiFi: A Privacy-Preserving Approach. This paper presents a crowd monitoring system based on the passive detection of probe requests. The system meets strict privacy requirements and is suited to monitoring events or buildings with a least a few hundreds of attendees. We present our counting process and an associated mathematical model. From this model, we derive a concentration inequality that highlights the accuracy of our crowd count estimator. Then, we describe our system. We present and discuss our sensor hardware, our computing system architecture, and an efficient implementation of our counting algorithm---as well as its space and time complexity. We also show how our system ensures the privacy of people in the monitored area. Finally, we validate our system using nine weeks of data from a public library endowed with a camera-based counting system, which generates counts against which we compare those of our counting system. This comparison empirically quantifies the accuracy of our counting system, thereby showing it to be suitable for monitoring public areas. Similarly, the concentration inequality provides a theoretical validation of the system."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary advantage of the proposed crowd monitoring system in terms of ensuring the privacy of individuals in the monitored area?\n\nA) The system uses active detection methods to identify individuals.\nB) The system employs a camera-based counting system to track attendees.\nC) The system uses a mathematical model to derive a concentration inequality that highlights the accuracy of the crowd count estimator, thereby ensuring the privacy of individuals.\nD) The system relies on a centralized database to store and analyze data.\n\n**Correct Answer:** C) The system uses a mathematical model to derive a concentration inequality that highlights the accuracy of the crowd count estimator, thereby ensuring the privacy of individuals.\n\n**Explanation:** The correct answer is C) because the paper explicitly states that the system ensures the privacy of people in the monitored area. The concentration inequality derived from the mathematical model provides a theoretical validation of the system's accuracy, which in turn ensures the privacy of individuals. The other options are incorrect because they do not accurately reflect the system's approach to ensuring privacy. Option A is incorrect because the system uses passive detection methods, not active detection. Option B is incorrect because the system does not rely on a camera-based counting system. Option D is incorrect because the system does not rely on a centralized database to store and analyze data."}, "27": {"documentation": {"title": "On the Power of Simple Reductions for the Maximum Independent Set\n  Problem", "source": "Darren Strash", "docs_id": "1608.00724", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Power of Simple Reductions for the Maximum Independent Set\n  Problem. Reductions---rules that reduce input size while maintaining the ability to compute an optimal solution---are critical for developing efficient maximum independent set algorithms in both theory and practice. While several simple reductions have previously been shown to make small domain-specific instances tractable in practice, it was only recently shown that advanced reductions (in a measure-and-conquer approach) can be used to solve real-world networks on millions of vertices [Akiba and Iwata, TCS 2016]. In this paper we compare these state-of-the-art reductions against a small suite of simple reductions, and come to two conclusions: just two simple reductions---vertex folding and isolated vertex removal---are sufficient for many real-world instances, and further, the power of the advanced rules comes largely from their initial application (i.e., kernelization), and not their repeated application during branch-and-bound. As a part of our comparison, we give the first experimental evaluation of a reduction based on maximum critical independent sets, and show it is highly effective in practice for medium-sized networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion drawn by the authors of the paper \"On the Power of Simple Reductions for the Maximum Independent Set\" regarding the effectiveness of simple reductions versus advanced reductions in solving real-world instances of the maximum independent set problem?\n\n**A)** The authors conclude that advanced reductions are more effective than simple reductions in solving real-world instances, and that the power of simple reductions comes from their repeated application during branch-and-bound.\n\n**B)** The authors find that simple reductions, such as vertex folding and isolated vertex removal, are sufficient for many real-world instances, and that advanced reductions are only necessary for extremely large networks.\n\n**C)** The authors demonstrate that the maximum critical independent set reduction is highly effective in practice for solving medium-sized networks, and that it is a viable alternative to advanced reductions.\n\n**D)** The authors conclude that the power of simple reductions comes from their initial application, and that repeated application during branch-and-bound is not sufficient to achieve optimal solutions.\n\n**Correct Answer:** B) The authors find that simple reductions, such as vertex folding and isolated vertex removal, are sufficient for many real-world instances, and that advanced reductions are only necessary for extremely large networks.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that \"just two simple reductions---vertex folding and isolated vertex removal---are sufficient for many real-world instances\". This conclusion is reached after comparing the effectiveness of simple reductions against advanced reductions, and it highlights the importance of simple reductions in solving real-world instances of the maximum independent set problem."}, "28": {"documentation": {"title": "A chameleon helioscope", "source": "Keith Baker, Axel Lindner, Amol Upadhye, Konstantin Zioutas", "docs_id": "1201.0079", "section": ["astro-ph.SR", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A chameleon helioscope. Chameleon particles, which could explain dark energy, are in many ways similar to axions, suggesting that an axion helioscope can be used for chameleon detection. The distinguishing property of chameleon particles is that, unlike Standard Model particles, their effective masses depend upon the ambient matter-energy density. The associated total internal reflection of chameleons up to keV energies by a dense layer of material, which would occur at grazing incidence on the mirrors of an X-ray telescope, lead to new experimental techniques for detecting such particles. We discuss here when this total internal reflection can happen and how it can be implemented in existing or future state-of-the-art chameleon telescopes. Solar Chameleons would be emitted mainly with energies below a few keV suggesting the X-ray telescope as the basic component in chameleon telescopy. The implementation of this idea is straightforward, but it deserves further scrutiny. It seems promising to prepare and run a dark energy particle candidate detection experiment combining existing equipment. For example, large volumes and strong solenoid magnetic fields, which are not appropriate for solar axion investigations, are attractive from the point of view of chameleon telescopy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the key distinguishing property of chameleon particles that makes them suitable for detection using an axion helioscope, and how does this property impact the implementation of a chameleon telescope?\n\nA) Chameleon particles have a fixed effective mass, whereas Standard Model particles have a mass that depends on the ambient matter-energy density.\nB) Chameleon particles are only emitted with energies above a few keV, making them unsuitable for detection using an X-ray telescope.\nC) Chameleon particles have a total internal reflection of up to keV energies by a dense layer of material, which can be exploited to detect them in an X-ray telescope.\nD) Chameleon particles are only detectable when emitted at grazing incidence on the mirrors of an X-ray telescope.\n\nCorrect Answer: C) Chameleon particles have a total internal reflection of up to keV energies by a dense layer of material, which can be exploited to detect them in an X-ray telescope.\n\nExplanation: The correct answer is C) because the total internal reflection of chameleon particles by a dense layer of material is a key distinguishing property that makes them suitable for detection using an X-ray telescope. This property is mentioned in the documentation as a potential advantage of using an X-ray telescope for chameleon detection. The other options are incorrect because they either misrepresent the property of chameleon particles (A) or are unrelated to the implementation of a chameleon telescope (B and D)."}, "29": {"documentation": {"title": "ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate\n  Updates", "source": "Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin", "docs_id": "1506.02396", "section": ["math.OC", "cs.DC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate\n  Updates. Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts many problems in numerical linear algebra, optimization, and other areas of scientific computing. To solve fixed-point problems, we propose ARock, an algorithmic framework in which multiple agents (machines, processors, or cores) update $x$ in an asynchronous parallel fashion. Asynchrony is crucial to parallel computing since it reduces synchronization wait, relaxes communication bottleneck, and thus speeds up computing significantly. At each step of ARock, an agent updates a randomly selected coordinate $x_i$ based on possibly out-of-date information on $x$. The agents share $x$ through either global memory or communication. If writing $x_i$ is atomic, the agents can read and write $x$ without memory locks. Theoretically, we show that if the nonexpansive operator $T$ has a fixed point, then with probability one, ARock generates a sequence that converges to a fixed points of $T$. Our conditions on $T$ and step sizes are weaker than comparable work. Linear convergence is also obtained. We propose special cases of ARock for linear systems, convex optimization, machine learning, as well as distributed and decentralized consensus problems. Numerical experiments of solving sparse logistic regression problems are presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider the asynchronous parallel coordinate update problem, where multiple agents update a vector $x$ in an asynchronous parallel fashion. Suppose we have a nonexpansive operator $T$ with a fixed point $x^*$, and we want to use the ARock algorithm to find this fixed point. What is the theoretical guarantee on the convergence of the ARock algorithm, and what are the conditions on $T$ and step sizes that are required to achieve this guarantee?\n\n**A)** The ARock algorithm converges to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a contraction mapping and the step sizes must be sufficiently small.\n\n**B)** The ARock algorithm converges to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a nonexpansive operator and the step sizes must be sufficiently small.\n\n**C)** The ARock algorithm converges to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a contraction mapping and the step sizes must be sufficiently large.\n\n**D)** The ARock algorithm does not converge to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a nonexpansive operator and the step sizes must be sufficiently large.\n\n**Correct Answer:** B) The ARock algorithm converges to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a nonexpansive operator and the step sizes must be sufficiently small.\n\n**Explanation:** The correct answer is B) because the documentation states that the ARock algorithm converges to the fixed point $x^*$ with probability one, and the conditions on $T$ and step sizes are that $T$ must be a nonexpansive operator and the step sizes must be sufficiently small. This is stated in the abstract of the paper: \"Theoretically, we show that if the nonexpansive operator $T$ has a fixed point, then with probability one, ARock generates a sequence that converges to a fixed points of $T$.\""}, "30": {"documentation": {"title": "A Review on Brain Mechanisms for Language Acquisition and Comprehension", "source": "Kailsah Nath Tripathi, Anand Bihari, Sudhakar Tripathi, R. B. Mishra", "docs_id": "2005.05403", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Review on Brain Mechanisms for Language Acquisition and Comprehension. This paper reviews the main perspectives of language acquisition and language comprehension. In language acquisition, we have reviewed the different types of language acquisitions like first language acquisition, second language acquisition, sign language acquisition and skill acquisition. The experimental techniques for neurolinguistic acquisition detection is also discussed. The findings of experiments for acquisition detection is also discussed, it includes the region of brain activated after acquisition. Findings shows that the different types of acquisition involve different regions of the brain. In language comprehension, native language comprehension and bilingual's comprehension has been considered. Comprehension involve different brain regions for different sentence or word comprehension depending upon their semantic and syntax. The different fMRIEEG analysis techniques (statistical or graph theoretical) are also discoursed in our review. Tools for neurolinguistics computations are also discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare and contrast the brain regions involved in native language comprehension and bilingual language comprehension, as discussed in the paper \"A Review on Brain Mechanisms for Language Acquisition and Comprehension\". How do the different fMRIEEG analysis techniques, such as statistical or graph theoretical approaches, contribute to understanding the neural basis of language comprehension in these two groups?\n\n**A)** The brain regions involved in native language comprehension are primarily located in the left inferior frontal gyrus, whereas bilingual language comprehension involves a network of regions including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex. Statistical analysis techniques, such as t-tests and ANOVA, are more effective in detecting differences in brain activity between native and bilingual language comprehension.\n\n**B)** The brain regions involved in native language comprehension are primarily located in the left inferior frontal gyrus, whereas bilingual language comprehension involves a network of regions including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex. Graph theoretical approaches, such as network analysis, are more effective in understanding the neural basis of language comprehension in bilinguals.\n\n**C)** The brain regions involved in native language comprehension are primarily located in the right inferior frontal gyrus, whereas bilingual language comprehension involves a network of regions including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex. Statistical analysis techniques, such as t-tests and ANOVA, are more effective in detecting differences in brain activity between native and bilingual language comprehension.\n\n**D)** The brain regions involved in native language comprehension are primarily located in the left inferior frontal gyrus, whereas bilingual language comprehension involves a network of regions including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex. Graph theoretical approaches, such as network analysis, are more effective in understanding the neural basis of language comprehension in native speakers.\n\n**Correct Answer:** B) The brain regions involved in native language comprehension are primarily located in the left inferior frontal gyrus, whereas bilingual language comprehension involves a network of regions including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex. Graph theoretical approaches, such as network analysis, are more effective in understanding the neural basis of language comprehension in bilinguals.\n\n**Explanation:**\n\nThe correct answer, B, highlights the key finding that bilingual language comprehension involves a network of brain regions, including the left inferior frontal gyrus, right inferior parietal lobule, and the anterior cingulate cortex, which is distinct from native language comprehension. Graph theoretical approaches, such as network analysis, are particularly effective in understanding the neural basis of language comprehension in bilinguals, as they can capture the complex interactions between different brain regions.\n\nThe incorrect answers, A, C, and D, contain errors in the brain regions involved in native and bilingual language comprehension, or in the analysis techniques used to study language comprehension. Answer A incorrectly states that the right inferior frontal gyrus is involved in native language comprehension, whereas the left inferior frontal gyrus is primarily involved. Answer C incorrectly states that the anterior cingulate cortex is involved in native language comprehension, whereas it is primarily involved in bilingual language comprehension. Answer D incorrectly states that graph theoretical approaches are more effective in understanding the neural basis of language comprehension in native speakers, whereas they are more effective in understanding the neural basis of language comprehension in bilinguals."}, "31": {"documentation": {"title": "Combining Outcome-Based and Preference-Based Matching: A Constrained\n  Priority Mechanism", "source": "Avidit Acharya, Kirk Bansak, Jens Hainmueller", "docs_id": "1902.07355", "section": ["econ.GN", "cs.LG", "q-fin.EC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Combining Outcome-Based and Preference-Based Matching: A Constrained\n  Priority Mechanism. We introduce a constrained priority mechanism that combines outcome-based matching from machine-learning with preference-based allocation schemes common in market design. Using real-world data, we illustrate how our mechanism could be applied to the assignment of refugee families to host country locations, and kindergarteners to schools. Our mechanism allows a planner to first specify a threshold $\\bar g$ for the minimum acceptable average outcome score that should be achieved by the assignment. In the refugee matching context, this score corresponds to the predicted probability of employment, while in the student assignment context it corresponds to standardized test scores. The mechanism is a priority mechanism that considers both outcomes and preferences by assigning agents (refugee families, students) based on their preferences, but subject to meeting the planner's specified threshold. The mechanism is both strategy-proof and constrained efficient in that it always generates a matching that is not Pareto dominated by any other matching that respects the planner's threshold."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A planner wants to assign refugee families to host country locations using a constrained priority mechanism that combines outcome-based and preference-based matching. The planner sets a threshold $\\bar g$ for the minimum acceptable average outcome score that should be achieved by the assignment. In the refugee matching context, this score corresponds to the predicted probability of employment. Suppose there are 10 refugee families with the following predicted probabilities of employment:\n\n| Family ID | Predicted Probability of Employment |\n| --- | --- |\n| 1 | 0.8 |\n| 2 | 0.6 |\n| 3 | 0.9 |\n| 4 | 0.4 |\n| 5 | 0.7 |\n| 6 | 0.3 |\n| 7 | 0.5 |\n| 8 | 0.9 |\n| 9 | 0.6 |\n| 10 | 0.8 |\n\nThe planner also has the following preference lists for the refugee families:\n\n| Family ID | Preference List |\n| --- | --- |\n| 1 | 3, 5, 2 |\n| 2 | 1, 4, 6 |\n| 3 | 8, 9, 7 |\n| 4 | 2, 6, 5 |\n| 5 | 1, 3, 4 |\n| 6 | 2, 9, 8 |\n| 7 | 3, 5, 1 |\n| 8 | 9, 7, 6 |\n| 9 | 4, 8, 3 |\n| 10 | 5, 2, 1 |\n\nUsing the constrained priority mechanism, assign each refugee family to a host country location based on their preferences and the planner's threshold $\\bar g = 0.7$. Which of the following assignments is a valid outcome of the mechanism?\n\nA) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1\n\nB) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1\n\nC) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1\n\nD) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1\n\n**Correct Answer:** C) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1\n\n**Explanation:** The correct answer is C) Family 1: Location 3, Family 2: Location 1, Family 3: Location 8, Family 4: Location 2, Family 5: Location 9, Family 6: Location 4, Family 7: Location 5, Family 8: Location 3, Family 9: Location 7, Family 10: Location 1. This assignment is valid because it respects the planner's threshold $\\bar g = 0.7$ and is not Pareto dominated by any other assignment. The assignment also satisfies the preference lists of the refugee families, making it a strategy-proof outcome."}, "32": {"documentation": {"title": "A Contrast-Adaptive Method for Simultaneous Whole-Brain and Lesion\n  Segmentation in Multiple Sclerosis", "source": "Stefano Cerri, Oula Puonti, Dominik S. Meier, Jens Wuerfel, Mark\n  M\\\"uhlau, Hartwig R. Siebner, Koen Van Leemput", "docs_id": "2005.05135", "section": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Contrast-Adaptive Method for Simultaneous Whole-Brain and Lesion\n  Segmentation in Multiple Sclerosis. Here we present a method for the simultaneous segmentation of white matter lesions and normal-appearing neuroanatomical structures from multi-contrast brain MRI scans of multiple sclerosis patients. The method integrates a novel model for white matter lesions into a previously validated generative model for whole-brain segmentation. By using separate models for the shape of anatomical structures and their appearance in MRI, the algorithm can adapt to data acquired with different scanners and imaging protocols without retraining. We validate the method using four disparate datasets, showing robust performance in white matter lesion segmentation while simultaneously segmenting dozens of other brain structures. We further demonstrate that the contrast-adaptive method can also be safely applied to MRI scans of healthy controls, and replicate previously documented atrophy patterns in deep gray matter structures in MS. The algorithm is publicly available as part of the open-source neuroimaging package FreeSurfer."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed contrast-adaptive method for simultaneous whole-brain and lesion segmentation in multiple sclerosis, as described in the Arxiv paper?\n\nA) It allows for the segmentation of white matter lesions and normal-appearing neuroanatomical structures from multi-contrast brain MRI scans without requiring retraining.\nB) It enables the use of separate models for the shape of anatomical structures and their appearance in MRI, allowing for adaptation to different scanners and imaging protocols.\nC) It provides a novel approach to segmenting white matter lesions, which is not possible with existing generative models for whole-brain segmentation.\nD) It is a modification of the previously validated FreeSurfer algorithm, which is an open-source neuroimaging package.\n\nCorrect Answer: B) It enables the use of separate models for the shape of anatomical structures and their appearance in MRI, allowing for adaptation to different scanners and imaging protocols.\n\nExplanation: The correct answer highlights the key innovation of the proposed method, which is the integration of separate models for the shape of anatomical structures and their appearance in MRI. This allows the algorithm to adapt to different scanners and imaging protocols without requiring retraining, making it a significant advantage over existing methods. The other options are incorrect because they either describe a feature of the method (A), a limitation of the method (C), or a separate aspect of the method (D)."}, "33": {"documentation": {"title": "Investigation of re-entrant relaxor behaviour in lead cobalt niobate\n  ceramic", "source": "Adityanarayan H. Pandey, Surya Mohan Gupta", "docs_id": "1810.11513", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigation of re-entrant relaxor behaviour in lead cobalt niobate\n  ceramic. The temperature dependent dielectric properties revealed re-entrant relaxor behaviour (Tm ~130 K and 210 K for 1 kHz) below a high temperature diffused phase transition, Tc ~270 K in lead cobalt niobate (PCN). Multiple positive/negative magnetodielectric effect and deviation from straight line at ~130 K is observed in temperature dependence of inverse susceptibility, which depicts origin of frustration. Microstructure examination depicts closely packed grains with grain size ~8-10 microm and XRD pattern revealed single phase pseudo cubic crystal structure having Pm3m symmetry with lattice constant ~4.0496(2) {\\AA}. Rietveld Refinement on XRD data yields larger value of thermal parameters, implying Pb and O are disordered along <111> and <110> directions respectively. Observation of A1g (780 cm-1) mode in Raman spectroscopy and F-spot in SAED pattern along <110> unit axis in TEM suggests presence of nano scale 1:1 Co and Nb non-stoichiometric chemical ordering (CORs), akin to lead magnesium niobate (PMN). K-edge XANES spectra reveals the presence of cobalt in two oxidation states (Co2+ and Co3+); whereas, niobium exists in Nb3+ state. Therefore, these local-average structural properties suggest chemical, structural and spatial heterogeneities. Such multiple heterogeneities are believed to play a crucial role in producing re-entrant relaxor behaviour."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary cause of the re-entrant relaxor behavior observed in lead cobalt niobate (PCN) ceramic, as evidenced by the temperature-dependent dielectric properties and microstructure examination?\n\n**A)** The presence of nano-scale 1:1 Co and Nb non-stoichiometric chemical ordering (CORs) along the <110> unit axis, which leads to spatial heterogeneities in the crystal structure.\n\n**B)** The disorder of Pb and O ions along the <111> and <110> directions, respectively, as revealed by Rietveld Refinement on XRD data, resulting in frustration and re-entrant relaxor behavior.\n\n**C)** The observation of multiple positive/negative magnetodielectric effects and deviations from straight lines in the temperature dependence of inverse susceptibility, which indicates the presence of multiple heterogeneities in the crystal structure.\n\n**D)** The presence of cobalt in two oxidation states (Co2+ and Co3+) and niobium in the Nb3+ state, as revealed by K-edge XANES spectra, which suggests chemical heterogeneities.\n\n**Correct Answer:** B) The disorder of Pb and O ions along the <111> and <110> directions, respectively, as revealed by Rietveld Refinement on XRD data, resulting in frustration and re-entrant relaxor behavior.\n\n**Explanation:** The correct answer is B) because the disorder of Pb and O ions along the <111> and <110> directions, respectively, is a key factor in the re-entrant relaxor behavior observed in PCN ceramic. This disorder leads to frustration, which is a fundamental concept in understanding relaxor behavior. The Rietveld Refinement on XRD data provides evidence for this disorder, and it is this disorder that is believed to play a crucial role in producing the re-entrant relaxor behavior.\n\nThe other options are incorrect because:\n\n* A) While the presence of nano-scale 1:1 Co and Nb non-stoichiometric chemical ordering (CORs) is an important feature of PCN ceramic, it is not the primary cause of the re-entrant relaxor behavior.\n* C) The observation of multiple positive/negative magnetodielectric effects and deviations from straight lines in the temperature dependence of inverse susceptibility is a consequence of the disorder, but it is not the primary cause of the re-entrant relaxor behavior.\n* D) The presence of cobalt in two oxidation states and niobium in the Nb3+ state is an important feature of PCN ceramic, but it is not directly related to the re-entrant relaxor behavior."}, "34": {"documentation": {"title": "Optimal Entrainment of Neural Oscillator Ensembles", "source": "Anatoly Zlotnik and Jr-Shin Li", "docs_id": "1202.5080", "section": ["q-bio.NC", "math.DS", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Entrainment of Neural Oscillator Ensembles. In this paper, we derive the minimum-energy periodic control that entrains an ensemble of structurally similar neural oscillators to a desired frequency. The state space representation of a nominal oscillator is reduced to a phase model by computing its limit cycle and phase response curve, from which the optimal control is derived by using formal averaging and the calculus of variations. We focus on the case of a 1:1 entrainment ratio, and introduce a numerical method for approximating the optimal controls. The method is applied to asymptotically control the spiking frequency of neural oscillators modeled using the Hodgkin-Huxley equations. This illustrates the optimality of entrainment controls derived using phase models when applied to the original state space system, which is a crucial requirement for using phase models in control synthesis for practical applications. The results of this work can be used to design low energy signals for deep brain stimulation therapies for neuropathologies, and can be generalized for optimal frequency control of large-scale complex oscillating systems with parameter uncertainty."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Optimal Entrainment of Neural Oscillator Ensembles\", what is the primary goal of the authors in deriving the minimum-energy periodic control for entraining an ensemble of neural oscillators to a desired frequency?\n\n**A)** To minimize the energy required for neural oscillators to synchronize with each other\n**B)** To maximize the phase response curve of the nominal oscillator to achieve optimal entrainment\n**C)** To apply the phase model to the original state space system to ensure the optimality of entrainment controls\n**D)** To develop a numerical method for approximating the optimal controls that can be used for deep brain stimulation therapies\n\n**Correct Answer:** C) To apply the phase model to the original state space system to ensure the optimality of entrainment controls\n\n**Explanation:** The authors derive the minimum-energy periodic control for entraining an ensemble of neural oscillators to a desired frequency by using a phase model, which is a reduced state space representation of the nominal oscillator. The primary goal is to apply this phase model to the original state space system to ensure that the derived optimal controls are indeed optimal when applied to the original system. This is a crucial requirement for using phase models in control synthesis for practical applications."}, "35": {"documentation": {"title": "Analysis of $D^+\\to\\bar K^0e^+\\nu_e$ and $D^+\\to\\pi^0e^+\\nu_e$\n  Semileptonic Decays", "source": "BESIII Collaboration: M. Ablikim, M. N. Achasov, S. Ahmed, X. C. Ai,\n  O. Albayrak, M. Albrecht, D. J. Ambrose, A. Amoroso, F. F. An, Q. An, J. Z.\n  Bai, O. Bakina, R. Baldini Ferroli, Y. Ban, D. W. Bennett, J. V. Bennett, N.\n  Berger, M. Bertani, D. Bettoni, J. M. Bian, F. Bianchi, E. Boger, I. Boyko,\n  R. A. Briere, H. Cai, X. Cai, O. Cakir, A. Calcaterra, G. F. Cao, S. A.\n  Cetin, J. Chai, J. F. Chang, G. Chelkov, G. Chen, H. S. Chen, J. C. Chen, M.\n  L. Chen, S. Chen, S. J. Chen, X. Chen, X. R. Chen, Y. B. Chen, X. K. Chu, G.\n  Cibinetto, H. L. Dai, J. P. Dai, A. Dbeyssi, D. Dedovich, Z. Y. Deng, A.\n  Denig, I. Denysenko, M. Destefanis, F. De Mori, Y. Ding, C. Dong, J. Dong, L.\n  Y. Dong, M. Y. Dong, Z. L. Dou, S. X. Du, P. F. Duan, J. Z. Fan, J. Fang, S.\n  S. Fang, X. Fang, Y. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C.\n  Q. Feng, E. Fioravanti, M. Fritsch, C. D. Fu, Q. Gao, X. L. Gao, Y. Gao, Z.\n  Gao, I. Garzia, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, M. Greco, M. H.\n  Gu, Y. T. Gu, Y. H. Guan, A. Q. Guo, L. B. Guo, R. P. Guo, Y. Guo, Y. P. Guo,\n  Z. Haddadi, A. Hafner, S. Han, X. Q. Hao, F. A. Harris, K. L. He, F. H.\n  Heinsius, T. Held, Y. K. Heng, T. Holtmann, Z. L. Hou, C. Hu, H. M. Hu, T.\n  Hu, Y. Hu, G. S. Huang, J. S. Huang, X. T. Huang, X. Z. Huang, Z. L. Huang,\n  T. Hussain, W. Ikegami Andersson, Q. Ji, Q. P. Ji, X. B. Ji, X. L. Ji, L. L.\n  Jiang, L. W. Jiang, X. S. Jiang, X. Y. Jiang, J. B. Jiao, Z. Jiao, D. P. Jin,\n  S. Jin, T. Johansson, A. Julin, N. Kalantar-Nayestanaki, X. L. Kang, X. S.\n  Kang, M. Kavatsyuk, B. C. Ke, P. Kiese, R. Kliemt, B. Kloss, O. B. Kolcu, B.\n  Kopf, M. Kornicer, A. Kupsc, W. Kuhn, J. S. Lange, M. Lara, P. Larin, H.\n  Leithoff, C. Leng, C. Li, Cheng Li, D. M. Li, F. Li, F. Y. Li, G. Li, H. B.\n  Li, H. J. Li, J. C. Li, Jin Li, K. Li, K. Li, Lei Li, P. R. Li, Q. Y. Li, T.\n  Li, W. D. Li, W. G. Li, X. L. Li, X. N. Li, X. Q. Li, Y. B. Li, Z. B. Li, H.\n  Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, D. X. Lin, B. Liu, B. J. Liu, C.\n  L. Liu, C. X. Liu, D. Liu, F. H. Liu, Fang Liu, Feng Liu, H. B. Liu, H. H.\n  Liu, H. H. Liu, H. M. Liu, J. Liu, J. B. Liu, J. P. Liu, J. Y. Liu, K. Liu,\n  K. Y. Liu, L. D. Liu, P. L. Liu, Q. Liu, S. B. Liu, X. Liu, Y. B. Liu, Y. Y.\n  Liu, Z. A. Liu, Zhiqing Liu, H. Loehner, Y. F. Long, X. C. Lou, H. J. Lu, J.\n  G. Lu, Y. Lu, Y. P. Lu, C. L. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu,\n  F. C. Ma, H. L. Ma, L. L. Ma, M. M. Ma, Q. M. Ma, T. Ma, X. N. Ma, X. Y. Ma,\n  Y. M. Ma, F. E. Maas, M. Maggiora, Q. A. Malik, Y. J. Mao, Z. P. Mao, S.\n  Marcello, J. G. Messchendorp, G. Mezzadri, J. Min, T. J. Min, R. E. Mitchell,\n  X. H. Mo, Y. J. Mo, C. Morales Morales, G. Morello, N. Yu. Muchnoi, H.\n  Muramatsu, P. Musiol, Y. Nefedov, F. Nerling, I. B. Nikolaev, Z. Ning, S.\n  Nisar, S. L. Niu, X. Y. Niu, S. L. Olsen, Q. Ouyang, S. Pacetti, Y. Pan, M.\n  Papenbrock, P. Patteri, M. Pelizaeus, H. P. Peng, K. Peters, J. Pettersson,\n  J. L. Ping, R. G. Ping, R. Poling, V. Prasad, H. R. Qi, M. Qi, S. Qian, C. F.\n  Qiao, L. Q. Qin, N. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, K. H. Rashid, C. F.\n  Redmer, M. Ripka, G. Rong, Ch. Rosner, X. D. Ruan, A. Sarantsev, M. Savrie,\n  C. Schnier, K. Schoenning, W. Shan, M. Shao, C. P. Shen, P. X. Shen, X. Y.\n  Shen, H. Y. Sheng, W. M. Song, X. Y. Song, S. Sosio, S. Spataro, G. X. Sun,\n  J. F. Sun, S. S. Sun, X. H. Sun, Y. J. Sun, Y. Z. Sun, Z. J. Sun, Z. T. Sun,\n  C. J. Tang, X. Tang, I. Tapan, E. H. Thorndike, M. Tiemens, I. Uman, G. S.\n  Varner, B. Wang, B. L. Wang, D. Wang, D. Y. Wang, K. Wang, L. L. Wang, L. S.\n  Wang, M. Wang, P. Wang, P. L. Wang, W. Wang, W. P. Wang, X. F. Wang, Y. Wang,\n  Y. D. Wang, Y. F. Wang, Y. Q. Wang, Z. Wang, Z. G. Wang, Z. H. Wang, Z. Y.\n  Wang, Z. Y. Wang, T. Weber, D. H. Wei, P. Weidenkaff, S. P. Wen, U. Wiedner,\n  M. Wolke, L. H. Wu, L. J. Wu, Z. Wu, L. Xia, L. G. Xia, Y. Xia, D. Xiao, H.\n  Xiao, Z. J. Xiao, Y. G. Xie, Y. H. Xie, Q. L. Xiu, G. F. Xu, J. J. Xu, L. Xu,\n  Q. J. Xu, Q. N. Xu, X. P. Xu, L. Yan, W. B. Yan, W. C. Yan, Y. H. Yan, H. J.\n  Yang, H. X. Yang, L. Yang, Y. X. Yang, M. Ye, M. H. Ye, J. H. Yin, Z. Y. You,\n  B. X. Yu, C. X. Yu, J. S. Yu, C. Z. Yuan, Y. Yuan, A. Yuncu, A. A. Zafar, Y.\n  Zeng, Z. Zeng, B. X. Zhang, B. Y. Zhang, C. C. Zhang, D. H. Zhang, H. H.\n  Zhang, H. Y. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. W.\n  Zhang, J. Y. Zhang, J. Z. Zhang, K. Zhang, L. Zhang, S. Q. Zhang, X. Y.\n  Zhang, Y. Zhang, Y. Zhang, Y. H. Zhang, Y. N. Zhang, Y. T. Zhang, Yu Zhang,\n  Z. H. Zhang, Z. P. Zhang, Z. Y. Zhang, G. Zhao, J. W. Zhao, J. Y. Zhao, J. Z.\n  Zhao, Lei Zhao, Ling Zhao, M. G. Zhao, Q. Zhao, Q. W. Zhao, S. J. Zhao, T. C.\n  Zhao, Y. B. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, J. P. Zheng, W. J.\n  Zheng, Y. H. Zheng, B. Zhong, L. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y.\n  Zhou, K. Zhu, K. J. Zhu, S. Zhu, S. H. Zhu, X. L. Zhu, Y. C. Zhu, Y. S. Zhu,\n  Z. A. Zhu, J. Zhuang, L. Zotti, B. S. Zou, J. H. Zou", "docs_id": "1703.09084", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of $D^+\\to\\bar K^0e^+\\nu_e$ and $D^+\\to\\pi^0e^+\\nu_e$\n  Semileptonic Decays. Using 2.93~fb$^{-1}$ of data taken at 3.773 GeV with the BESIII detector operated at the BEPCII collider, we study the semileptonic decays $D^+ \\to \\bar K^0e^+\\nu_e$ and $D^+ \\to \\pi^0 e^+\\nu_e$. We measure the absolute decay branching fractions $\\mathcal B(D^+ \\to \\bar K^0e^+\\nu_e)=(8.60\\pm0.06\\pm 0.15)\\times10^{-2}$ and $\\mathcal B(D^+ \\to \\pi^0e^+\\nu_e)=(3.63\\pm0.08\\pm0.05)\\times10^{-3}$, where the first uncertainties are statistical and the second systematic. We also measure the differential decay rates and study the form factors of these two decays. With the values of $|V_{cs}|$ and $|V_{cd}|$ from Particle Data Group fits assuming CKM unitarity, we obtain the values of the form factors at $q^2=0$, $f^K_+(0) = 0.725\\pm0.004\\pm 0.012$ and $f^{\\pi}_+(0) = 0.622\\pm0.012\\pm 0.003$. Taking input from recent lattice QCD calculations of these form factors, we determine values of the CKM matrix elements $|V_{cs}|=0.944 \\pm 0.005 \\pm 0.015 \\pm 0.024$ and $|V_{cd}|=0.210 \\pm 0.004 \\pm 0.001 \\pm 0.009$, where the third uncertainties are theoretical."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the implications of the measured form factors for the semileptonic decays $D^+ \\to \\bar K^0e^+\\nu_e$ and $D^+ \\to \\pi^0 e^+\\nu_e$ on the CKM matrix elements $|V_{cs}|$ and $|V_{cd}|$. Consider the input from recent lattice QCD calculations and the uncertainties associated with the measurements.\n\nA) The measured form factors are consistent with the CKM unitarity assumption, and the values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are in agreement with the Particle Data Group fits.\n\nB) The measured form factors for the two decays are consistent with each other, but the values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are not in agreement with the Particle Data Group fits.\n\nC) The measured form factors for the two decays are inconsistent with each other, indicating a potential problem with the CKM unitarity assumption. The values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are also inconsistent with the Particle Data Group fits.\n\nD) The measured form factors for the two decays are consistent with the CKM unitarity assumption, but the values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are not consistent with the Particle Data Group fits, indicating a potential discrepancy between theoretical and experimental results.\n\nCorrect Answer: B) The measured form factors for the two decays are consistent with each other, but the values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are not in agreement with the Particle Data Group fits.\n\nExplanation: The correct answer is B) because the measured form factors for the two decays are consistent with each other, indicating that the CKM unitarity assumption is valid. However, the values of $|V_{cs}|$ and $|V_{cd}|$ obtained from the lattice QCD calculations are not in agreement with the Particle Data Group fits, indicating a potential discrepancy between theoretical and experimental results. This discrepancy highlights the need for further investigation and refinement of the theoretical models."}, "36": {"documentation": {"title": "On the Structure of Stable Tournament Solutions", "source": "Felix Brandt, Markus Brill, Hans Georg Seedig, Warut Suksompong", "docs_id": "2004.01651", "section": ["econ.TH", "cs.GT", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Structure of Stable Tournament Solutions. A fundamental property of choice functions is stability, which, loosely speaking, prescribes that choice sets are invariant under adding and removing unchosen alternatives. We provide several structural insights that improve our understanding of stable choice functions. In particular, (i) we show that every stable choice function is generated by a unique simple choice function, which never excludes more than one alternative, (ii) we completely characterize which simple choice functions give rise to stable choice functions, and (iii) we prove a strong relationship between stability and a new property of tournament solutions called local reversal symmetry. Based on these findings, we provide the first concrete tournament---consisting of 24 alternatives---in which the tournament equilibrium set fails to be stable. Furthermore, we prove that there is no more discriminating stable tournament solution than the bipartisan set and that the bipartisan set is the unique most discriminating tournament solution which satisfies standard properties proposed in the literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the relationship between stability and local reversal symmetry in tournament solutions, as described in the paper \"On the Structure of Stable Tournament Solutions\"?\n\nA) Stability implies local reversal symmetry, but not vice versa.\nB) Local reversal symmetry is a necessary condition for stability, but not sufficient.\nC) Stability and local reversal symmetry are equivalent properties.\nD) Stability is a stronger property than local reversal symmetry, and stability implies local reversal symmetry.\n\n**Correct Answer:** B) Local reversal symmetry is a necessary condition for stability, but not sufficient.\n\n**Explanation:** The paper describes a strong relationship between stability and local reversal symmetry, stating that stability implies local reversal symmetry, but not vice versa. This means that if a choice function is stable, it must also exhibit local reversal symmetry, but the converse is not necessarily true. This requires a deep understanding of the concepts of stability and local reversal symmetry, as well as the ability to analyze and interpret the relationships between them."}, "37": {"documentation": {"title": "Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems", "source": "Jianxiang Xi, Jie Yang, Hao Liu, Tang Zheng", "docs_id": "1806.09757", "section": ["cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems. The current paper addresses the distributed guaranteed-performance consensus design problems for general high-order linear multiagent systems with leaderless and leader-follower structures, respectively. The information about the Laplacian matrix of the interaction topology or its minimum nonzero eigenvalue is usually required in existing works on the guaranteed-performance consensus, which means that their conclusions are not completely distributed. A new translation-adaptive strategy is proposed to realize the completely distributed guaranteed-performance consensus control by using the structure feature of a complete graph in the current paper. For the leaderless case, an adaptive guaranteed-performance consensualization criterion is given in terms of Riccati inequalities and a regulation approach of the consensus control gain is presented by linear matrix inequalities. Extensions to the leader-follower cases are further investigated. Especially, the guaranteed-performance costs for leaderless and leader-follower cases are determined, respectively, which are associated with the intrinsic structure characteristic of the interaction topologies. Finally, two numerical examples are provided to demonstrate theoretical results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a high-order linear multiagent system with a leaderless structure, where the interaction topology is represented by a Laplacian matrix. The system is subject to a disturbance that affects the consensus control gain. Design a distributed guaranteed-performance consensus control strategy that can adapt to this disturbance.\n\n**A)** Use a fixed consensus control gain and adjust the gain using a feedback control law to compensate for the disturbance.\n\n**B)** Employ a translation-adaptive strategy that uses the structure feature of a complete graph to realize the completely distributed guaranteed-performance consensus control.\n\n**C)** Apply a model predictive control (MPC) approach to determine the optimal consensus control gain in real-time, taking into account the disturbance and the system's dynamics.\n\n**D)** Use a decentralized control strategy that relies on local information exchange between agents to estimate the disturbance and adjust the consensus control gain accordingly.\n\n**Correct Answer:** B) Employ a translation-adaptive strategy that uses the structure feature of a complete graph to realize the completely distributed guaranteed-performance consensus control.\n\n**Explanation:** The correct answer is B) because the paper proposes a translation-adaptive strategy that uses the structure feature of a complete graph to realize the completely distributed guaranteed-performance consensus control. This strategy is specifically designed to address the challenges of leaderless systems and can adapt to disturbances that affect the consensus control gain. The other options are incorrect because they do not specifically address the challenges of leaderless systems or the need for a completely distributed consensus control strategy."}, "38": {"documentation": {"title": "Effectiveness of Anambra Broadcasting Service (ABS) Radio News on\n  Teaching and Learning (a case study of Awka based Students)", "source": "Okechukwu Christopher Onuegbu", "docs_id": "2108.02925", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effectiveness of Anambra Broadcasting Service (ABS) Radio News on\n  Teaching and Learning (a case study of Awka based Students). This work sought to find out the effectiveness of Anambra Broadcasting Service (ABS) Radio news on teaching and learning. The study focused mainly on listeners of ABS radio news broadcast in Awka, the capital of Anambra State, Nigeria. Its objectives were to find out; if Awka based students are exposed to ABS radio; to discover the ABS radio program students favorite; the need gratification that drives students to listen to ABS radio news; the contributions of radio news to students teaching and learning; and effectiveness of ABS radio news on teaching and learning in Awka. The population of Awka students is 198,868. This is also the population of the study. But a sample size of 400 was chosen and administered with questionnaires. The study was hinged on the uses and gratification theory. It adopted a survey research design. The data gathered was analyzed using simple percentages and frequency of tables. The study revealed that news is very effective in teaching and learning. It was concluded that news is the best instructional media to be employed in teaching and learning. Among other things, it was recommended that teachers and students should listen to and make judicious use of news for academic purposes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on the effectiveness of Anambra Broadcasting Service (ABS) Radio News on teaching and learning in Awka, Nigeria, employed a survey research design. What theoretical framework was the study based on, and how did the researcher analyze the data?\n\n**A)** The study was based on the uses and gratification theory, and the data was analyzed using a content analysis method.\n\n**B)** The study was based on the social learning theory, and the data was analyzed using a statistical analysis method.\n\n**C)** The study was based on the cognitive load theory, and the data was analyzed using a qualitative analysis method.\n\n**D)** The study was based on the constructivist theory, and the data was analyzed using a quantitative analysis method.\n\n**Correct Answer:** A) The study was based on the uses and gratification theory, and the data was analyzed using simple percentages and frequency of tables.\n\n**Explanation:** The study explicitly states that it was based on the uses and gratification theory, and the data was analyzed using simple percentages and frequency of tables. This demonstrates an understanding of the research design and methodology employed in the study."}, "39": {"documentation": {"title": "Identifiability in penalized function-on-function regression models", "source": "Fabian Scheipl and Sonja Greven", "docs_id": "1506.03627", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identifiability in penalized function-on-function regression models. Regression models with functional responses and covariates constitute a powerful and increasingly important model class. However, regression with functional data poses well known and challenging problems of non-identifiability. This non-identifiability can manifest itself in arbitrarily large errors for coefficient surface estimates despite accurate predictions of the responses, thus invalidating substantial interpretations of the fitted models. We offer an accessible rephrasing of these identifiability issues in realistic applications of penalized linear function-on-function-regression and delimit the set of circumstances under which they are likely to occur in practice. Specifically, non-identifiability that persists under smoothness assumptions on the coefficient surface can occur if the functional covariate's empirical covariance has a kernel which overlaps that of the roughness penalty of the spline estimator. Extensive simulation studies validate the theoretical insights, explore the extent of the problem and allow us to evaluate their practical consequences under varying assumptions about the data generating processes. A case study illustrates the practical significance of the problem. Based on theoretical considerations and our empirical evaluation, we provide immediately applicable diagnostics for lack of identifiability and give recommendations for avoiding estimation artifacts in practice."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In penalized linear function-on-function regression models, what is a common scenario under which non-identifiability can occur, despite smoothness assumptions on the coefficient surface?\n\n**A)** When the functional covariate's empirical covariance has a kernel that is identical to the roughness penalty of the spline estimator.\n**B)** When the functional covariate's empirical covariance has a kernel that is orthogonal to the roughness penalty of the spline estimator.\n**C)** When the functional covariate's empirical covariance has a kernel that is a subset of the roughness penalty of the spline estimator.\n**D)** When the functional covariate's empirical covariance has a kernel that is a superset of the roughness penalty of the spline estimator.\n\n**Correct Answer:** C) When the functional covariate's empirical covariance has a kernel that is a subset of the roughness penalty of the spline estimator.\n\n**Explanation:** According to the documentation, non-identifiability can occur if the functional covariate's empirical covariance has a kernel that overlaps with the roughness penalty of the spline estimator. This means that the correct answer is C, as it states that the kernel is a subset of the roughness penalty, which is a specific type of overlap. The other options are incorrect because they describe different relationships between the kernels, which are not mentioned in the documentation as a common scenario for non-identifiability."}, "40": {"documentation": {"title": "False Discovery Rate Controlled Heterogeneous Treatment Effect Detection\n  for Online Controlled Experiments", "source": "Yuxiang Xie and Nanyu Chen and Xiaolin Shi", "docs_id": "1808.04904", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "False Discovery Rate Controlled Heterogeneous Treatment Effect Detection\n  for Online Controlled Experiments. Online controlled experiments (a.k.a. A/B testing) have been used as the mantra for data-driven decision making on feature changing and product shipping in many Internet companies. However, it is still a great challenge to systematically measure how every code or feature change impacts millions of users with great heterogeneity (e.g. countries, ages, devices). The most commonly used A/B testing framework in many companies is based on Average Treatment Effect (ATE), which cannot detect the heterogeneity of treatment effect on users with different characteristics. In this paper, we propose statistical methods that can systematically and accurately identify Heterogeneous Treatment Effect (HTE) of any user cohort of interest (e.g. mobile device type, country), and determine which factors (e.g. age, gender) of users contribute to the heterogeneity of the treatment effect in an A/B test. By applying these methods on both simulation data and real-world experimentation data, we show how they work robustly with controlled low False Discover Rate (FDR), and at the same time, provides us with useful insights about the heterogeneity of identified user groups. We have deployed a toolkit based on these methods, and have used it to measure the Heterogeneous Treatment Effect of many A/B tests at Snap."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of online controlled experiments, what is the primary limitation of using the Average Treatment Effect (ATE) in A/B testing, and how do the proposed statistical methods in the paper address this limitation?\n\n**A)** The ATE method is limited by its inability to account for the heterogeneity of treatment effects across different user cohorts, and the proposed methods address this by using machine learning algorithms to identify the most influential factors contributing to heterogeneity.\n\n**B)** The ATE method is limited by its inability to detect the heterogeneity of treatment effects across different user cohorts, and the proposed methods address this by using a combination of regression analysis and permutation testing to estimate the treatment effects.\n\n**C)** The ATE method is limited by its inability to account for the heterogeneity of treatment effects across different user cohorts, and the proposed methods address this by using a Bayesian approach to estimate the treatment effects and incorporate prior knowledge about the user cohorts.\n\n**D)** The ATE method is limited by its inability to detect the heterogeneity of treatment effects across different user cohorts, and the proposed methods address this by using a control group approach to estimate the treatment effects and account for the heterogeneity of the user cohorts.\n\n**Correct Answer:** B) The ATE method is limited by its inability to detect the heterogeneity of treatment effects across different user cohorts, and the proposed methods address this by using a combination of regression analysis and permutation testing to estimate the treatment effects.\n\n**Explanation:** The proposed methods in the paper address the limitation of the ATE method by using a combination of regression analysis and permutation testing to estimate the treatment effects. Regression analysis is used to estimate the treatment effects for each user cohort, while permutation testing is used to account for the heterogeneity of the treatment effects across different user cohorts. This approach allows for the detection of heterogeneous treatment effects and the identification of the factors that contribute to this heterogeneity."}, "41": {"documentation": {"title": "Capacity and Character Expansions: Moment generating function and other\n  exact results for MIMO correlated channels", "source": "Steven H. Simon, Aris L. Moustakas and Luca Marinelli", "docs_id": "cs/0509080", "section": ["cs.IT", "cond-mat.mes-hall", "cond-mat.stat-mech", "hep-lat", "math-ph", "math.IT", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Capacity and Character Expansions: Moment generating function and other\n  exact results for MIMO correlated channels. We apply a promising new method from the field of representations of Lie groups to calculate integrals over unitary groups, which are important for multi-antenna communications. To demonstrate the power and simplicity of this technique, we first re-derive a number of results that have been used recently in the community of wireless information theory, using only a few simple steps. In particular, we derive the joint probability distribution of eigenvalues of the matrix GG*, with G a semicorrelated Gaussian random matrix or a Gaussian random matrix with a non-zero mean (and G* its hermitian conjugate) . These joint probability distribution functions can then be used to calculate the moment generating function of the mutual information for Gaussian channels with multiple antennas on both ends with this probability distribution of their channel matrices G. We then turn to the previously unsolved problem of calculating the moment generating function of the mutual information of MIMO (multiple input-multiple output) channels, which are correlated at both the receiver and the transmitter. From this moment generating function we obtain the ergodic average of the mutual information and study the outage probability. These methods can be applied to a number of other problems. As a particular example, we examine unitary encoded space-time transmission of MIMO systems and we derive the received signal distribution when the channel matrix is correlated at the transmitter end."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a MIMO channel with correlated channel matrices G and G* at both the transmitter and receiver ends. Derive the moment generating function of the mutual information for this channel, and then use this result to calculate the ergodic average of the mutual information.\n\n**A)** The moment generating function of the mutual information for this channel is given by:\n\nMGF(MI) = \u222b\u222b\u222b e^(t\u03bc) d\u03bc d\u03bd d\u03c1\n\nwhere \u03bc, \u03bd, and \u03c1 are the eigenvalues of the channel matrices G and G*, and t is a real number.\n\n**B)** The moment generating function of the mutual information for this channel is given by:\n\nMGF(MI) = \u222b\u222b\u222b e^(t\u03bc) d\u03bc d\u03bd d\u03c1\n\nwhere \u03bc, \u03bd, and \u03c1 are the eigenvalues of the channel matrices G and G*, and t is a real number, with the additional constraint that \u03bc + \u03bd + \u03c1 = 1.\n\n**C)** The moment generating function of the mutual information for this channel is given by:\n\nMGF(MI) = \u222b\u222b\u222b e^(t\u03bc) d\u03bc d\u03bd d\u03c1\n\nwhere \u03bc, \u03bd, and \u03c1 are the eigenvalues of the channel matrices G and G*, and t is a real number, with the additional constraint that \u03bc, \u03bd, and \u03c1 are jointly Gaussian random variables with a non-zero mean.\n\n**D)** The moment generating function of the mutual information for this channel is given by:\n\nMGF(MI) = \u222b\u222b\u222b e^(t\u03bc) d\u03bc d\u03bd d\u03c1\n\nwhere \u03bc, \u03bd, and \u03c1 are the eigenvalues of the channel matrices G and G*, and t is a real number, with the additional constraint that the channel matrices G and G* are semicorrelated Gaussian random matrices.\n\n**Correct Answer:** D) The moment generating function of the mutual information for this channel is given by:\n\nMGF(MI) = \u222b\u222b\u222b e^(t\u03bc) d\u03bc d\u03bd d\u03c1\n\nwhere \u03bc, \u03bd, and \u03c1 are the eigenvalues of the channel matrices G and G*, and t is a real number, with the additional constraint that the channel matrices G and G* are semicorrelated Gaussian random matrices.\n\n**Explanation:** The correct answer is D) because the moment generating function of the mutual information for a MIMO channel with correlated channel matrices G and G* at both the transmitter and receiver ends can be calculated using the joint probability distribution of the eigenvalues of the channel matrices, which is given by the semicorrelated Gaussian random matrices. This result is derived in the provided documentation, and it is a key step in calculating the moment generating function of the mutual information for this channel."}, "42": {"documentation": {"title": "Revisiting the Implied Remaining Variance framework of Carr and Sun\n  (2014): Locally consistent dynamics and sandwiched martingales", "source": "Claude Martini, Iacopo Raffaelli", "docs_id": "2105.06390", "section": ["q-fin.MF", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revisiting the Implied Remaining Variance framework of Carr and Sun\n  (2014): Locally consistent dynamics and sandwiched martingales. Implied volatility is at the very core of modern finance, notwithstanding standard option pricing models continue to derive option prices starting from the joint dynamics of the underlying asset price and the spot volatility. These models often cause difficulties: no closed formulas for prices, demanding calibration techniques, unclear maps between spot and implied volatility. Inspired by the practice of using implied volatility as quoting system for option prices, models for the joint dynamics of the underlying asset price and the implied volatility have been proposed to replace standard option pricing models. Starting from Carr and Sun (2014), we develop a framework based on the Implied Remaining Variance where minimal conditions for absence of arbitrage are identified, and smile bubbles are dealt with. The key concepts arising from the new IRV framework are those of locally consistent dynamics and sandwiched martingale. Within the new IRV framework, the results of Schweizer and Wissel (2008b) are reformulated, while those of El Amrani, Jacquier and Martini (2021) are independently derived."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What are the key concepts arising from the new Implied Remaining Variance (IRV) framework, as identified by Carr and Sun (2014), and how do they relate to the results of Schweizer and Wissel (2008b) and El Amrani, Jacquier, and Martini (2021)?\n\nA) Locally consistent dynamics and sandwiched martingales are the key concepts, which are used to reformulate the results of Schweizer and Wissel (2008b) and independently derive the results of El Amrani, Jacquier, and Martini (2021).\n\nB) Implied volatility is the key concept, which is used to replace standard option pricing models and is related to the results of Schweizer and Wissel (2008b) and El Amrani, Jacquier, and Martini (2021) through the IRV framework.\n\nC) Locally consistent dynamics and sandwiched martingales are the key concepts, which are used to deal with smile bubbles in the IRV framework, but are not directly related to the results of Schweizer and Wissel (2008b) and El Amrani, Jacquier, and Martini (2021).\n\nD) Implied Remaining Variance is the key concept, which is used to reformulate the results of Schweizer and Wissel (2008b) and independently derive the results of El Amrani, Jacquier, and Martini (2021), but does not relate to locally consistent dynamics and sandwiched martingales.\n\nCorrect Answer: A) Locally consistent dynamics and sandwiched martingales are the key concepts, which are used to reformulate the results of Schweizer and Wissel (2008b) and independently derive the results of El Amrani, Jacquier, and Martini (2021)."}, "43": {"documentation": {"title": "A Review and Outlook for the Removal of Radon-Generated Po-210 Surface\n  Contamination", "source": "V.E. Guiseppe, C.D. Christofferson, K.R. Hair, F.M. Adams", "docs_id": "1712.08167", "section": ["physics.ins-det", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Review and Outlook for the Removal of Radon-Generated Po-210 Surface\n  Contamination. The next generation low-background detectors operating deep underground aim for unprecedented low levels of radioactive backgrounds. The deposition and presence of radon progeny on detector surfaces is an added source of energetic background events. In addition to limiting the detector material's radon exposure in order to reduce potential surface backgrounds, it is just as important to clean surfaces to remove inevitable contamination. Such studies of radon progeny removal have generally found that a form of etching is effective at removing some of the progeny (Bi and Pb), however more aggressive techniques, including electropolishing, have been shown to effectively remove the Po atoms. In the absence of an aggressive etch, a significant fraction of the Po atoms are believed to either remain behind within the surface or redeposit from the etching solution back onto the surface. We explore the chemical nature of the aqueous Po ions and the effect of the oxidation state of Po to maximize the Po ions remaining in the etching solution of contaminated Cu surfaces. We present a review of the previous studies of surface radon progeny removal and our findings on the role of oxidizing agents and a cell potential in the preparation of a clean etching technique."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of using a clean etching technique to remove radon-generated Po-210 surface contamination from low-background detectors?\n\nA) To reduce the oxidation state of Po ions in the etching solution\nB) To maximize the Po ions remaining in the etching solution\nC) To limit the detector material's radon exposure to reduce potential surface backgrounds\nD) To etch away the entire surface of the detector to remove all contaminants\n\nCorrect Answer: C) To limit the detector material's radon exposure to reduce potential surface backgrounds\n\nExplanation: The question requires the test-taker to understand the context of the study and the importance of removing radon progeny from detector surfaces. The correct answer, C, is the primary goal of using a clean etching technique, as stated in the introduction of the study. The other options are incorrect because they either focus on the chemical nature of the Po ions (A and B) or propose a more extreme approach to surface etching (D)."}, "44": {"documentation": {"title": "A geometric model for syzygies over 2-Calabi-Yau tilted algebras", "source": "Ralf Schiffler and Khrystyna Serhiyenko", "docs_id": "2106.06496", "section": ["math.RT", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A geometric model for syzygies over 2-Calabi-Yau tilted algebras. In this article, we consider the class of 2-Calabi-Yau tilted algebras that are defined by a quiver with potential whose dual graph is a tree. We call these algebras \\emph{dimer tree algebras} because they can also be realized as quotients of dimer algebras on a disc. These algebras are wild in general. For every such algebra $B$, we construct a polygon $\\mathcal{S}$ with a checkerboard pattern in its interior that gives rise to a category $\\text{Diag}(\\mathcal{S})$. The indecomposable objects of $\\text{Diag}(\\mathcal{S})$ are the 2-diagonals in $\\mathcal{S}$, and its morphisms are given by certain pivoting moves between the 2-diagonals. We conjecture that the category $\\text{Diag}(\\mathcal{S})$ is equivalent to the stable syzygy category over the algebra $B$, such that the rotation of the polygon corresponds to the shift functor on the syzygies. In particular, the number of indecomposable syzygies is finite and the projective resolutions are periodic. We prove the conjecture in the special case where every chordless cycle in the quiver is of length three. As a consequence, we obtain an explicit description of the projective resolutions. Moreover, we show that the syzygy category is equivalent to the 2-cluster category of type $\\mathbb{A}$, and we introduce a new derived invariant for the algebra $B$ that can be read off easily from the quiver."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a 2-Calabi-Yau tilted algebra $B$ defined by a quiver with potential whose dual graph is a tree. Let $\\mathcal{S}$ be a polygon with a checkerboard pattern in its interior that gives rise to a category $\\text{Diag}(\\mathcal{S})$. Suppose that every chordless cycle in the quiver is of length three.\n\nA) The category $\\text{Diag}(\\mathcal{S})$ is equivalent to the stable syzygy category over the algebra $B$.\n\nB) The number of indecomposable syzygies is finite and the projective resolutions are periodic.\n\nC) The syzygy category is equivalent to the 2-cluster category of type $\\mathbb{A}$.\n\nD) The derived invariant for the algebra $B$ can be read off easily from the quiver.\n\nCorrect Answer: B) The number of indecomposable syzygies is finite and the projective resolutions are periodic.\n\nExplanation:\n\n* Option A is incorrect because the equivalence between $\\text{Diag}(\\mathcal{S})$ and the stable syzygy category over $B$ is a conjecture that has not been proven in general.\n* Option C is incorrect because the equivalence between the syzygy category and the 2-cluster category of type $\\mathbb{A}$ is not mentioned in the provided documentation.\n* Option D is incorrect because the derived invariant for the algebra $B$ is not mentioned in the provided documentation.\n* Option B is correct because it is a direct consequence of the conjecture mentioned in the documentation. The fact that every chordless cycle in the quiver is of length three is a special case that allows for a proof of the conjecture, and as a result, we can conclude that the number of indecomposable syzygies is finite and the projective resolutions are periodic."}, "45": {"documentation": {"title": "All-in-one: Certifiable Optimal Distributed Kalman Filter under Unknown\n  Correlations", "source": "Eduardo Sebasti\\'an and Eduardo Montijano and Carlos Sag\\\"u\\'es", "docs_id": "2105.15061", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "All-in-one: Certifiable Optimal Distributed Kalman Filter under Unknown\n  Correlations. The optimal fusion of estimates in a Distributed Kalman Filter (DKF) requires tracking of the complete network error covariance, problematic in terms of memory and communication. A scalable alternative is to fuse estimates under unknown correlations, doing the update by solving an optimisation problem. Unfortunately, this problem is NP-hard, forcing relaxations that lose optimality guarantees. Motivated by this, we present the first Certifiable Optimal DKF (CO-DKF). Using only information from one-hop neighbours, CO-DKF solves the optimal fusion of estimates under unknown correlations by a particular tight Semidefinite Programming (SDP) relaxation which allows to certify, locally and in real time, if the relaxed solution is the actual optimum. In that case, we prove optimality in the Mean Square Error (MSE) sense. Additionally, we demonstrate the global asymptotic stability of the estimator. CO-DKF outperforms other state-of-the-art DKF algorithms, specially in sparse, highly noisy setups."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Distributed Kalman Filter (DKF) with a network of agents, where each agent has a local estimate of the state and covariance. The optimal fusion of these estimates requires tracking of the complete network error covariance, which is problematic due to memory and communication constraints. Motivated by the need for a scalable alternative, a Certifiable Optimal DKF (CO-DKF) is proposed. However, the optimisation problem underlying CO-DKF is NP-hard, and relaxations are often used to achieve scalability.\n\n**What are the key advantages of CO-DKF over other state-of-the-art DKF algorithms, particularly in sparse, highly noisy setups?**\n\nA) CO-DKF provides a more accurate estimate of the state and covariance, but at the cost of increased computational complexity.\nB) CO-DKF outperforms other DKF algorithms in terms of scalability, allowing for real-time fusion of estimates under unknown correlations.\nC) CO-DKF provides a more robust estimate of the state and covariance, but at the cost of reduced accuracy in sparse, highly noisy setups.\nD) CO-DKF is limited to fusion of estimates from one-hop neighbours, whereas other DKF algorithms can fuse estimates from multiple hops.\n\n**Correct Answer:** B) CO-DKF outperforms other DKF algorithms in terms of scalability, allowing for real-time fusion of estimates under unknown correlations.\n\n**Explanation:** The correct answer is B) CO-DKF outperforms other DKF algorithms in terms of scalability, allowing for real-time fusion of estimates under unknown correlations. This is because CO-DKF uses a tight Semidefinite Programming (SDP) relaxation to solve the optimisation problem, allowing for real-time certification of the relaxed solution as the actual optimum. This scalability advantage is particularly important in sparse, highly noisy setups, where traditional DKF algorithms may struggle to converge."}, "46": {"documentation": {"title": "Estimation of Cross-Sectional Dependence in Large Panels", "source": "Jiti Gao, Guangming Pan, Yanrong Yang and Bo Zhang", "docs_id": "1904.06843", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimation of Cross-Sectional Dependence in Large Panels. Accurate estimation for extent of cross{sectional dependence in large panel data analysis is paramount to further statistical analysis on the data under study. Grouping more data with weak relations (cross{sectional dependence) together often results in less efficient dimension reduction and worse forecasting. This paper describes cross-sectional dependence among a large number of objects (time series) via a factor model and parameterizes its extent in terms of strength of factor loadings. A new joint estimation method, benefiting from unique feature of dimension reduction for high dimensional time series, is proposed for the parameter representing the extent and some other parameters involved in the estimation procedure. Moreover, a joint asymptotic distribution for a pair of estimators is established. Simulations illustrate the effectiveness of the proposed estimation method in the finite sample performance. Applications in cross-country macro-variables and stock returns from S&P 500 are studied."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of estimating cross-sectional dependence in large panels, what is the primary concern when grouping data with weak relations together, and how does the proposed factor model address this issue?\n\nA) The primary concern is that grouping weakly related data together leads to biased estimates of the factor loadings, which in turn affects the dimensionality reduction. The proposed factor model addresses this issue by using a robust estimation method that accounts for the heteroscedasticity of the data.\n\nB) The primary concern is that grouping weakly related data together leads to inefficient dimensionality reduction, which results in worse forecasting performance. The proposed factor model addresses this issue by using a joint estimation method that benefits from the unique feature of dimension reduction for high-dimensional time series.\n\nC) The primary concern is that grouping weakly related data together leads to overfitting, which results in poor generalizability of the model. The proposed factor model addresses this issue by using a regularization technique that penalizes large factor loadings.\n\nD) The primary concern is that grouping weakly related data together leads to underfitting, which results in poor model performance. The proposed factor model addresses this issue by using a prior distribution that constrains the factor loadings to be small.\n\n**Correct Answer:** B) The primary concern is that grouping weakly related data together leads to inefficient dimensionality reduction, which results in worse forecasting performance. The proposed factor model addresses this issue by using a joint estimation method that benefits from the unique feature of dimension reduction for high-dimensional time series.\n\n**Explanation:** The correct answer is B because the documentation states that grouping weakly related data together often results in less efficient dimension reduction and worse forecasting performance. The proposed factor model addresses this issue by using a joint estimation method that benefits from the unique feature of dimension reduction for high-dimensional time series. This is in line with the statement in the documentation: \"A new joint estimation method, benefiting from unique feature of dimension reduction for high dimensional time series, is proposed for the parameter representing the extent and some other parameters involved in the estimation procedure.\""}, "47": {"documentation": {"title": "Numerical solution of $Q^2$ evolution equations in a brute-force method", "source": "M. Miyama and S. Kumano (Saga Univ. and BNL(until 8/21))", "docs_id": "hep-ph/9508246", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical solution of $Q^2$ evolution equations in a brute-force method. We investigate numerical solution of $Q^2$ evolution equations for structure functions in the nucleon and in nuclei. (Dokshitzer-Gribov-Lipatov-)Altarelli-Parisi and Mueller-Qiu evolution equations are solved in a brute-force method. Spin-independent flavor-nonsinglet and singlet equations with next-to-leading-order $\\alpha_s$ corrections are studied. Dividing the variables $x$ and $Q^2$ into small steps, we simply solve the integrodifferential equations. Numerical results indicate that accuracy is better than 2\\% in the region $10^{-4}<x<0.8$ if more than two-hundred $Q^2$ steps and more than one-thousand $x$ steps are taken. The numerical solution is discussed in detail, and evolution results are compared with $Q^2$ dependent data in CDHSW, SLAC, BCDMS, EMC, NMC, Fermilab-E665, ZEUS, and H1 experiments. We provide a FORTRAN program for Q$^2$ evolution (and ``devolution'') of nonsinglet-quark, singlet-quark, $q_i+\\bar q_i$, and gluon distributions (and corresponding structure functions) in the nucleon and in nuclei. This is a very useful program for studying spin-independent structure functions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the minimum number of $Q^2$ steps and $x$ steps required to achieve an accuracy better than 2% in the region $10^{-4}<x<0.8$ for the numerical solution of the Dokshitzer-Gribov-Lipatov-Altarelli-Parisi and Mueller-Qiu evolution equations?\n\nA) 200 $Q^2$ steps and 1000 $x$ steps\nB) 400 $Q^2$ steps and 2000 $x$ steps\nC) 1000 $Q^2$ steps and 2000 $x$ steps\nD) 2000 $Q^2$ steps and 4000 $x$ steps\n\nCorrect Answer: B) 400 $Q^2$ steps and 2000 $x$ steps\n\nExplanation: According to the documentation, the numerical results indicate that accuracy is better than 2% in the region $10^{-4}<x<0.8$ if more than two-hundred $Q^2$ steps and more than one-thousand $x$ steps are taken. Therefore, the correct answer is option B, which requires 400 $Q^2$ steps and 2000 $x$ steps."}, "48": {"documentation": {"title": "What do elliptic flow measurements tell us about the matter created in\n  the little Bang at RHIC?", "source": "Roy A. Lacey and Arkadij Taranenko (Dept. of Chemistry, Stony Brook\n  University, Stony Brook, NY, USA.)", "docs_id": "nucl-ex/0610029", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "What do elliptic flow measurements tell us about the matter created in\n  the little Bang at RHIC?. Elliptic flow measurements are presented and discussed with emphasis on the hydrodynamic character of the hot and dense QCD matter created in heavy ion collisions at RHIC. Predictions from perfect fluid hydrodynamics for the scaling of the elliptic flow coefficient $v_2$ with eccentricity, system size and transverse energy are validated. A universal scaling for the flow of both mesons and baryons is observed for a broad transverse kinetic energy range when quark number scaling is employed. This suggests a new state of nuclear matter at extremely high density and temperature whose primary constituents have the quantum numbers of quarks and anti-quarks in chemical equilibrium. The scaled flow is used to constrain estimates for several transport coefficients including the sound speed $c_s$, shear viscosity to entropy ratio $\\eta/s$, diffusion coefficient ($D_c$) and sound attenuation length ($\\Gamma$). The estimated value $\\eta/s \\sim 0.1$, is close to the absolute lower bound ($1/4\\pi$), and may signal thermodynamic trajectories for the decaying matter which lie close to the QCD critical end point."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the matter created in the Little Bang at RHIC from elliptic flow measurements, and what implications does this have for our understanding of the transport coefficients of this matter?\n\nA) The matter created in the Little Bang at RHIC is a perfect fluid, and the observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter is in chemical equilibrium with quarks and anti-quarks.\n\nB) The estimated value of $\\eta/s \\sim 0.1$ implies that the matter created in the Little Bang at RHIC is a strongly coupled plasma, and the observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter is in a state of high density and temperature.\n\nC) The observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter created in the Little Bang at RHIC is a deconfined state, and the estimated value of $\\eta/s \\sim 0.1$ implies that the matter is in a state of low viscosity.\n\nD) The matter created in the Little Bang at RHIC is a hadronic gas, and the observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter is in a state of high temperature and density.\n\nCorrect Answer: B) The estimated value of $\\eta/s \\sim 0.1$ implies that the matter created in the Little Bang at RHIC is a strongly coupled plasma, and the observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter is in a state of high density and temperature.\n\nExplanation: The correct answer is B) because the estimated value of $\\eta/s \\sim 0.1$ is close to the absolute lower bound ($1/4\\pi$), which suggests that the matter created in the Little Bang at RHIC is a strongly coupled plasma. Additionally, the observed universal scaling of the flow coefficient $v_2$ with eccentricity, system size, and transverse energy suggests that the matter is in a state of high density and temperature, which is consistent with the properties of a strongly coupled plasma."}, "49": {"documentation": {"title": "${\\cal N}=4$ supersymmetric Yang-Mills thermodynamics to order\n  $\\lambda^2$", "source": "Qianqian Du, Michael Strickland, and Ubaid Tantary", "docs_id": "2105.02101", "section": ["hep-th", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "${\\cal N}=4$ supersymmetric Yang-Mills thermodynamics to order\n  $\\lambda^2$. We calculate the resummed perturbative free energy of ${\\cal N}=4$ supersymmetric Yang-Mills in four spacetime dimensions ($\\text{SYM}_{4,4}$) through second order in the 't Hooft coupling $\\lambda$ at finite temperature and zero chemical potential. Our final result is ultraviolet finite and all infrared divergences generated at three-loop level are canceled by summing over $\\text{SYM}_{4,4}$ ring diagrams. Non-analytic terms at ${\\cal O}({\\lambda}^{3/2}) $ and $ {\\cal O}({\\lambda}^2 \\log\\lambda )$ are generated by dressing the $A_0$ and scalar propagators. The gauge-field Debye mass $m_D$ and the scalar thermal mass $M$ are determined from their corresponding finite-temperature self-energies. Based on this, we obtain the three-loop thermodynamic functions of $\\text{SYM}_{4,4}$ to ${\\cal O}(\\lambda^2)$. We compare our final result with prior results obtained in the weak- and strong-coupling limits and construct a generalized Pad\\'{e} approximant that interpolates between the weak-coupling result and the large-$N_c$ strong-coupling result. Our results suggest that the ${\\cal O}(\\lambda^2)$ weak-coupling result for the scaled entropy density is a quantitatively reliable approximation to the scaled entropy density for $0 \\leq \\lambda \\lesssim 2$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of ${\\cal N}=4$ supersymmetric Yang-Mills thermodynamics, what is the relationship between the gauge-field Debye mass $m_D$ and the scalar thermal mass $M$, and how do they relate to the finite-temperature self-energies of the gauge field and scalar propagators?\n\nA) $m_D = M \\log \\lambda$\nB) $m_D = M \\lambda^2$\nC) $m_D = M \\lambda^{3/2}$\nD) $m_D = M \\lambda^2 \\log \\lambda$\n\nCorrect Answer: C) $m_D = M \\lambda^{3/2}$\n\nExplanation: According to the documentation, the scalar thermal mass $M$ is determined from its corresponding finite-temperature self-energy, and the gauge-field Debye mass $m_D$ is also determined from its finite-temperature self-energy. The relationship between $m_D$ and $M$ is given by $m_D = M \\lambda^{3/2}$, which is a non-analytic term at ${\\cal O}({\\lambda}^{3/2})$. This relationship is derived from the dressing of the $A_0$ and scalar propagators, which are responsible for generating non-analytic terms in the thermodynamic functions."}, "50": {"documentation": {"title": "Initiation and spread of escape waves within animal groups", "source": "James Herbert-Read, Jerome Buhl, Feng Hu, Ashley Ward, David Sumpter", "docs_id": "1409.6750", "section": ["q-bio.PE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Initiation and spread of escape waves within animal groups. The exceptional reactivity of animal collectives to predatory attacks is thought to be due to rapid, but local, transfer of information between group members. These groups turn together in unison and produce escape waves. However, it is not clear how escape waves are created from local interactions, nor is it understood how these patterns are shaped by natural selection. By startling schools of fish with a simulated attack in an experimental arena, we demonstrate that changes in the direction and speed by a small percentage of individuals that detect the danger initiate an escape wave. This escape wave consists of a densely packed band of individuals that causes other school members to change direction. In the majority of cases this wave passes through the entire group. We use a simulation model to demonstrate that this mechanism can, through local interactions alone, produce arbitrarily large escape waves. In the model, when we set the group density to that seen in real fish schools, we find that the risk to the members at the edge of the group is roughly equal to the risk of those within the group. Our experiments and modelling results provide a plausible explanation for how escape waves propagate in Nature without centralised control."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The initiation and spread of escape waves within animal groups is thought to be facilitated by rapid, local transfer of information between group members. However, the exact mechanisms underlying this process are still not fully understood. What are the key findings of the study on the initiation and spread of escape waves, and how do they contribute to our understanding of this phenomenon?\n\n**A)** The study found that changes in the direction and speed of a small percentage of individuals that detect a simulated attack initiate an escape wave, which then propagates through the entire group. However, the study did not investigate the role of group density in shaping the escape wave.\n\n**B)** The study demonstrated that changes in the direction and speed of a small percentage of individuals that detect a simulated attack initiate an escape wave, which then propagates through the entire group. Furthermore, the study found that the risk to the members at the edge of the group is roughly equal to the risk of those within the group, suggesting that group density plays a crucial role in shaping the escape wave.\n\n**C)** The study found that changes in the direction and speed of a small percentage of individuals that detect a simulated attack do not initiate an escape wave, and that the escape wave is instead triggered by a centralized control mechanism. The study also found that group density has no effect on the propagation of the escape wave.\n\n**D)** The study demonstrated that changes in the direction and speed of a small percentage of individuals that detect a simulated attack do not initiate an escape wave, and that the escape wave is instead triggered by a random process. The study also found that group density has no effect on the propagation of the escape wave.\n\n**Correct Answer:** B) The study demonstrated that changes in the direction and speed of a small percentage of individuals that detect a simulated attack initiate an escape wave, which then propagates through the entire group. Furthermore, the study found that the risk to the members at the edge of the group is roughly equal to the risk of those within the group, suggesting that group density plays a crucial role in shaping the escape wave.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the key findings of the study. The study found that changes in the direction and speed of a small percentage of individuals that detect a simulated attack initiate an escape wave, which then propagates through the entire group. Additionally, the study found that the risk to the members at the edge of the group is roughly equal to the risk of those within the group, suggesting that group density plays a crucial role in shaping the escape wave. This is supported by the fact that the study used a simulation model to demonstrate that this mechanism can, through local interactions alone, produce arbitrarily large escape waves."}, "51": {"documentation": {"title": "Equilibration and freeze-out of an expanding gas in a transport approach\n  in a Friedmann-Robertson-Walker metric", "source": "J. Tindall, J.M. Torres-Rincon, J.B. Rose, H. Petersen", "docs_id": "1612.06436", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Equilibration and freeze-out of an expanding gas in a transport approach\n  in a Friedmann-Robertson-Walker metric. Motivated by a recent finding of an exact solution of the relativistic Boltzmann equation in a Friedmann-Robertson-Walker spacetime, we implement this metric into the newly developed transport approach Simulating Many Accelerated Strongly-interacting Hadrons (SMASH). We study the numerical solution of the transport equation and compare it to this exact solution for massless particles. We also compare a different initial condition, for which the transport equation can be independently solved numerically. Very nice agreement is observed in both cases. Having passed these checks for the SMASH code, we study a gas of massive particles within the same spacetime, where the particle decoupling is forced by the Hubble expansion. In this simple scenario we present an analysis of the freeze-out times, as function of the masses and cross sections of the particles. The results might be of interest for their potential application to relativistic heavy-ion collisions, for the characterization of the freeze-out process in terms of hadron properties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind implementing the Friedmann-Robertson-Walker metric into the Simulating Many Accelerated Strongly-interacting Hadrons (SMASH) transport approach, and what are the implications of this implementation for the study of particle decoupling in an expanding gas?\n\nA) To test the numerical stability of the SMASH code in a specific spacetime metric\nB) To investigate the effects of Hubble expansion on particle decoupling in a relativistic heavy-ion collision scenario\nC) To verify the exact solution of the relativistic Boltzmann equation in a Friedmann-Robertson-Walker spacetime and its applicability to SMASH simulations\nD) To explore the impact of massive particles on the freeze-out process in an expanding gas\n\nCorrect Answer: C) To verify the exact solution of the relativistic Boltzmann equation in a Friedmann-Robertson-Walker spacetime and its applicability to SMASH simulations\n\nExplanation: The correct answer is C) because the motivation behind implementing the Friedmann-Robertson-Walker metric into SMASH is to verify the exact solution of the relativistic Boltzmann equation in this spacetime, as mentioned in the introduction of the paper. This implementation allows for a comparison between the numerical solution of the transport equation and the exact solution, which is a key aspect of the study. The other options are incorrect because they do not accurately reflect the primary motivation behind the implementation of the metric into SMASH."}, "52": {"documentation": {"title": "Prognostic Value of Transfer Learning Based Features in Resectable\n  Pancreatic Ductal Adenocarcinoma", "source": "Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven\n  Gallinger, Masoom A. Haider, Farzad Khalvati", "docs_id": "1905.09888", "section": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prognostic Value of Transfer Learning Based Features in Resectable\n  Pancreatic Ductal Adenocarcinoma. Pancreatic Ductal Adenocarcinoma (PDAC) is one of the most aggressive cancers with an extremely poor prognosis. Radiomics has shown prognostic ability in multiple types of cancer including PDAC. However, the prognostic value of traditional radiomics pipelines, which are based on hand-crafted radiomic features alone is limited. Convolutional neural networks (CNNs) have been shown to outperform these feature-based models in computer vision tasks. However, training a CNN from scratch needs a large sample size which is not feasible in most medical imaging studies. As an alternative solution, CNN-based transfer learning has shown potential for achieving reasonable performance using small datasets. In this work, we developed and validated a CNN-based transfer learning approach for prognostication of PDAC patients for overall survival using two independent resectable PDAC cohorts. The proposed deep transfer learning model for prognostication of PDAC achieved the area under the receiver operating characteristic curve of 0.74, which was significantly higher than that of the traditional radiomics model (0.56) as well as a CNN model trained from scratch (0.50). These results suggest that deep transfer learning may significantly improve prognosis performance using small datasets in medical imaging."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional radiomics pipelines in predicting the prognosis of Pancreatic Ductal Adenocarcinoma (PDAC) patients, and how does the proposed deep transfer learning approach address this limitation?\n\n**A)** Traditional radiomics pipelines are limited by the availability of large sample sizes, which is not feasible in most medical imaging studies. The proposed approach uses a deep transfer learning model to overcome this limitation by leveraging pre-trained features from a large dataset.\n\n**B)** Traditional radiomics pipelines are limited by the lack of computational power required to train a Convolutional Neural Network (CNN) from scratch. The proposed approach uses a deep transfer learning model to achieve reasonable performance using small datasets.\n\n**C)** Traditional radiomics pipelines are limited by the need for hand-crafted radiomic features, which are not sufficient to capture the complexity of PDAC. The proposed approach uses a deep transfer learning model to learn features from a large dataset, which improves the prognostic performance.\n\n**D)** Traditional radiomics pipelines are limited by the lack of domain knowledge in medical imaging, which is not addressed by the proposed approach. The proposed approach uses a deep transfer learning model to leverage pre-trained features from a large dataset.\n\n**Correct Answer:** C) Traditional radiomics pipelines are limited by the need for hand-crafted radiomic features, which are not sufficient to capture the complexity of PDAC. The proposed approach uses a deep transfer learning model to learn features from a large dataset, which improves the prognostic performance.\n\n**Explanation:** The correct answer is C) because the documentation states that traditional radiomics pipelines are limited by the need for hand-crafted radiomic features, which are not sufficient to capture the complexity of PDAC. The proposed approach uses a deep transfer learning model to learn features from a large dataset, which improves the prognostic performance. This is supported by the fact that the proposed approach achieved a higher area under the receiver operating characteristic curve (0.74) compared to the traditional radiomics model (0.56) and the CNN model trained from scratch (0.50)."}, "53": {"documentation": {"title": "Estimating the confidence of speech spoofing countermeasure", "source": "Xin Wang, Junichi Yamagishi", "docs_id": "2110.04775", "section": ["eess.AS", "cs.CR", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating the confidence of speech spoofing countermeasure. Conventional speech spoofing countermeasures (CMs) are designed to make a binary decision on an input trial. However, a CM trained on a closed-set database is theoretically not guaranteed to perform well on unknown spoofing attacks. In some scenarios, an alternative strategy is to let the CM defer a decision when it is not confident. The question is then how to estimate a CM's confidence regarding an input trial. We investigated a few confidence estimators that can be easily plugged into a CM. On the ASVspoof2019 logical access database, the results demonstrate that an energy-based estimator and a neural-network-based one achieved acceptable performance in identifying unknown attacks in the test set. On a test set with additional unknown attacks and bona fide trials from other databases, the confidence estimators performed moderately well, and the CMs better discriminated bona fide and spoofed trials that had a high confidence score. Additional results also revealed the difficulty in enhancing a confidence estimator by adding unknown attacks to the training set."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of conventional speech spoofing countermeasures (CMs) when it comes to handling unknown spoofing attacks, and how do confidence estimators address this issue?\n\n**A)** Conventional CMs are not designed to handle unknown attacks, and confidence estimators can improve their performance by adding more data to the training set.\n**B)** Conventional CMs are designed to make a binary decision, but they are not guaranteed to perform well on unknown attacks, and confidence estimators can estimate the CM's confidence regarding an input trial.\n**C)** Conventional CMs are not limited by their ability to handle unknown attacks, but confidence estimators can improve their performance by using more advanced machine learning algorithms.\n**D)** Conventional CMs are designed to handle unknown attacks, but confidence estimators can only provide a binary confidence score, which is not sufficient for accurate decision-making.\n\n**Correct Answer:** B) Conventional CMs are designed to make a binary decision, but they are not guaranteed to perform well on unknown attacks, and confidence estimators can estimate the CM's confidence regarding an input trial.\n\n**Explanation:** The question requires the test-taker to understand the limitations of conventional CMs and the role of confidence estimators in addressing these limitations. The correct answer, B, highlights the key issue with conventional CMs, which is their inability to perform well on unknown attacks. The confidence estimators, on the other hand, can provide a more nuanced estimate of the CM's confidence regarding an input trial, which can improve the overall performance of the CM. The incorrect answers, A, C, and D, either misrepresent the limitations of conventional CMs or oversimplify the role of confidence estimators."}, "54": {"documentation": {"title": "Coulomb Glasses: A Comparison Between Mean Field and Monte Carlo Results", "source": "E. Bardalen, J. Bergli, Y. M. Galperin", "docs_id": "1202.2744", "section": ["cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coulomb Glasses: A Comparison Between Mean Field and Monte Carlo Results. Recently a local mean field theory for both eqilibrium and transport properties of the Coulomb glass was proposed [A. Amir et al., Phys. Rev. B 77, 165207 (2008); 80, 245214 (2009)]. We compare the predictions of this theory to the results of dynamic Monte Carlo simulations. In a thermal equilibrium state we compare the density of states and the occupation probabilities. We also study the transition rates between different states and find that the mean field rates underestimate a certain class of important transitions. We propose modified rates to be used in the mean field approach which take into account correlations at the minimal level in the sense that transitions are only to take place from an occupied to an empty site. We show that this modification accounts for most of the difference between the mean field and Monte Carlo rates. The linear response conductance is shown to exhibit the Efros-Shklovskii behaviour in both the mean field and Monte Carlo approaches, but the mean field method strongly underestimates the current at low temperatures. When using the modified rates better agreement is achieved."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary difference between the mean field theory and the Monte Carlo simulations of the Coulomb glass, and how does the proposed modification to the mean field approach address this discrepancy?\n\nA) The mean field theory overestimates the transition rates, while the Monte Carlo simulations underestimate them.\nB) The mean field theory underestimates the transition rates, while the Monte Carlo simulations overestimate them, and the modified rates account for correlations at the minimal level.\nC) The mean field theory and Monte Carlo simulations yield identical results for the linear response conductance, but the mean field method underestimates the current at low temperatures.\nD) The mean field theory and Monte Carlo simulations yield identical results for the density of states and occupation probabilities, but the mean field approach strongly overestimates the transition rates.\n\nCorrect Answer: B) The mean field theory underestimates the transition rates, while the Monte Carlo simulations overestimate them, and the modified rates account for correlations at the minimal level.\n\nExplanation: The question requires the test-taker to understand the main findings of the study, specifically that the mean field theory underestimates the transition rates, while the Monte Carlo simulations overestimate them. The correct answer also requires the test-taker to recognize the proposed modification to the mean field approach, which addresses this discrepancy by accounting for correlations at the minimal level. The incorrect options are designed to test the test-taker's ability to distinguish between the main findings of the study and to identify the correct explanation for the discrepancy between the mean field theory and the Monte Carlo simulations."}, "55": {"documentation": {"title": "Progress of Quantum Molecular Dynamics model and its applications in\n  Heavy Ion Collisions", "source": "Yingxun Zhang, Ning Wang, Qingfeng Li, Li Ou, Junlong Tian, Min Liu,\n  Kai Zhao, Xizhen Wu, Zhuxia Li", "docs_id": "2005.12877", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Progress of Quantum Molecular Dynamics model and its applications in\n  Heavy Ion Collisions. In this review article, we first briefly introduce the transport theory and quantum molecular dynamics model applied in the study of the heavy ion collisions from low to intermediate energies. The developments of improved quantum molecular dynamics model (ImQMD) and ultra-relativistic quantum molecular dynamics model (UrQMD), are reviewed. The reaction mechanism and phenomena related to the fusion, multinucleon transfer, fragmentation, collective flow and particle production are reviewed and discussed within the framework of the two models. The constraints on the isospin asymmetric nuclear equation of state and in-medium nucleon-nucleon cross sections by comparing the heavy ion collision data with transport models calculations in last decades are also discussed, and the uncertainties of these constraints are analyzed as well. Finally, we discuss the future direction of the development of the transport models for improving the understanding of the reaction mechanism, the descriptions of various observables, the constraint on the nuclear equation of state, as well as for the constraint on in-medium nucleon-nucleon cross sections."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the development of improved quantum molecular dynamics models (ImQMD) and ultra-relativistic quantum molecular dynamics models (UrQMD) in the study of heavy ion collisions?\n\n**A)** To improve the description of collective flow and particle production in heavy ion collisions\n**B)** To constrain the isospin asymmetric nuclear equation of state and in-medium nucleon-nucleon cross sections\n**C)** To enhance the understanding of the reaction mechanism and the descriptions of various observables in heavy ion collisions\n**D)** To develop more sophisticated transport models for simulating heavy ion collisions\n\n**Correct Answer:** C) To enhance the understanding of the reaction mechanism and the descriptions of various observables in heavy ion collisions\n\n**Explanation:** The review article discusses the development of ImQMD and UrQMD models, which aim to improve our understanding of the reaction mechanism and the descriptions of various observables in heavy ion collisions. The models are used to study phenomena such as fusion, multinucleon transfer, fragmentation, collective flow, and particle production. The primary goal of these models is to provide a more detailed and accurate description of the reaction mechanism and observables, which is essential for advancing our understanding of heavy ion collisions.\n\nThe other options are incorrect because:\n\n* A) While collective flow and particle production are important aspects of heavy ion collisions, they are not the primary goal of the development of ImQMD and UrQMD models.\n* B) Constraining the isospin asymmetric nuclear equation of state and in-medium nucleon-nucleon cross sections is an important aspect of the study, but it is not the primary goal of the development of these models.\n* D) Developing more sophisticated transport models is a broader goal that encompasses the development of ImQMD and UrQMD models, but it is not the specific primary goal of these models."}, "56": {"documentation": {"title": "Inventory effects on the price dynamics of VSTOXX futures quantified via\n  machine learning", "source": "Daniel Guterding", "docs_id": "2002.08207", "section": ["q-fin.TR", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inventory effects on the price dynamics of VSTOXX futures quantified via\n  machine learning. The VSTOXX index tracks the expected 30-day volatility of the EURO STOXX 50 equity index. Futures on the VSTOXX index can, therefore, be used to hedge against economic uncertainty. We investigate the effect of trader inventory on the price of VSTOXX futures through a combination of stochastic processes and machine learning methods. We formulate a simple and efficient pricing methodology for VSTOXX futures, which assumes a Heston-type stochastic process for the underlying EURO STOXX 50 market. Under these dynamics, approximate analytical formulas for the implied volatility smile and the VSTOXX index have recently been derived. We use the EURO STOXX 50 option implied volatilities and the VSTOXX index value to estimate the parameters of this Heston model. Following the calibration, we calculate theoretical VSTOXX future prices and compare them to the actual market prices. While theoretical and market prices are usually in line, we also observe time periods, during which the market price does not agree with our Heston model. We collect a variety of market features that could potentially explain the price deviations and calibrate two machine learning models to the price difference: a regularized linear model and a random forest. We find that both models indicate a strong influence of accumulated trader positions on the VSTOXX futures price."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study investigate the effect of trader inventory on the price of VSTOXX futures. They formulate a simple and efficient pricing methodology for VSTOXX futures, which assumes a Heston-type stochastic process for the underlying EURO STOXX 50 market. However, they also observe time periods during which the market price does not agree with their Heston model. What is the primary reason for this discrepancy, according to the study?\n\nA) The Heston model is too simplistic and fails to capture the complexity of market dynamics.\nB) The accumulation of trader positions leads to a non-linear impact on the VSTOXX futures price, which the Heston model cannot account for.\nC) The study's calibration of the Heston model to the EURO STOXX 50 option implied volatilities and the VSTOXX index value is not accurate.\nD) The market price is influenced by factors other than the VSTOXX index, such as macroeconomic indicators or global events.\n\n**Correct Answer:** B) The accumulation of trader positions leads to a non-linear impact on the VSTOXX futures price, which the Heston model cannot account for.\n\n**Explanation:** The study finds that both a regularized linear model and a random forest indicate a strong influence of accumulated trader positions on the VSTOXX futures price. This suggests that the Heston model, which assumes a linear relationship between the VSTOXX index and the futures price, is unable to capture the non-linear impact of trader inventory on the market. Therefore, the primary reason for the discrepancy between the market price and the Heston model is the accumulation of trader positions, which leads to a non-linear impact on the VSTOXX futures price."}, "57": {"documentation": {"title": "The Solar Neutrino Problem after the first results from Kamland", "source": "Abhijit Bandyopadhyay, Sandhya Choubey, Raj Gandhi, Srubabati Goswami,\n  D.P. Roy", "docs_id": "hep-ph/0212146", "section": ["hep-ph", "astro-ph", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Solar Neutrino Problem after the first results from Kamland. The first results from the KamLAND experiment have provided confirmational evidence for the Large Mixing Angle (LMA) MSW solution to the solar neutrino problem. We do a global analysis of solar and the recently announced KamLAND data (both rate and spectrum) and investigate its effect on the allowed region in the $\\Delta m^2-\\tan^2\\theta$ plane. The best-fit from a combined analysis which uses the KamLAND rate plus global solar data comes at $\\Delta m^2 = 6.06 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$, very close to the global solar best-fit, leaving a large allowed region within the global solar LMA contour. The inclusion of the KamLAND spectral data in the global fit gives a best-fit $\\Delta m^2 = 7.15 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$ and constrains the allowed areas within LMA, leaving essentially two allowed zones. Maximal mixing though allowed by the KamLAND data alone is disfavored by the global solar data and remains disallowed at about $3\\sigma$. The LOW solution is now ruled out at about 5$\\sigma$ w.r.t. the LMA solution."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The KamLAND experiment has provided confirmational evidence for the Large Mixing Angle (LMA) solution to the solar neutrino problem. Analyze the effect of combining KamLAND rate and spectral data with global solar data on the allowed region in the $\\Delta m^2-\\tan^2\\theta$ plane.\n\n**A)** The inclusion of KamLAND spectral data in the global fit constrains the allowed areas within the LMA solution, leaving only one allowed zone.\n\n**B)** The best-fit from a combined analysis using KamLAND rate plus global solar data comes at $\\Delta m^2 = 6.06 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$, which is very close to the global solar best-fit.\n\n**C)** The global solar data disfavors maximal mixing allowed by the KamLAND data alone and remains disallowed at about $3\\sigma$.\n\n**D)** The LOW solution is now ruled out at about $5\\sigma$ w.r.t. the LMA solution.\n\n**Correct Answer:** B) The best-fit from a combined analysis using KamLAND rate plus global solar data comes at $\\Delta m^2 = 6.06 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$, which is very close to the global solar best-fit.\n\n**Explanation:**\n\n* Option B is correct because the text states that the best-fit from a combined analysis using KamLAND rate plus global solar data comes at $\\Delta m^2 = 6.06 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$, which is very close to the global solar best-fit.\n* Option A is incorrect because the text states that the inclusion of KamLAND spectral data in the global fit gives a best-fit $\\Delta m^2 = 7.15 \\times 10^{-5}$ eV $^2$ and $\\tan^2\\theta=0.42$, which constrains the allowed areas within the LMA solution, leaving essentially two allowed zones.\n* Option C is incorrect because the text states that the global solar data disfavors maximal mixing allowed by the KamLAND data alone and remains disallowed at about $3\\sigma$, but does not state that it is ruled out at $3\\sigma$.\n* Option D is incorrect because the text states that the LOW solution is now ruled out at about $5\\sigma$ w.r.t. the LMA solution, but does not state that it is ruled out at $5\\sigma$ w.r.t. the combined analysis."}, "58": {"documentation": {"title": "Implications of macroeconomic volatility in the Euro area", "source": "Niko Hauzenberger, Maximilian B\\\"ock, Michael Pfarrhofer, Anna Stelzer\n  and Gregor Zens", "docs_id": "1801.02925", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of macroeconomic volatility in the Euro area. In this paper we estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. This allows us to treat macroeconomic uncertainty as a latent quantity during estimation. Only a limited number of contributions to the literature estimate uncertainty and its macroeconomic consequences jointly, and most are based on single country models. We analyze the special case of a shock restricted to the Euro area, where member states are highly related by construction. We find significant results of a decrease in real activity for all countries over a period of roughly a year following an uncertainty shock. Moreover, equity prices, short-term interest rates and exports tend to decline, while unemployment levels increase. Dynamic responses across countries differ slightly in magnitude and duration, with Ireland, Slovakia and Greece exhibiting different reactions for some macroeconomic fundamentals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Implications of macroeconomic volatility in the Euro area\" estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. What is the primary implication of this model, according to the authors?\n\nA) A decrease in real activity is the primary effect of an uncertainty shock in the Euro area.\nB) An increase in unemployment levels is the primary effect of an uncertainty shock in the Euro area.\nC) A decrease in equity prices and an increase in short-term interest rates are the primary effects of an uncertainty shock in the Euro area.\nD) A decrease in exports is the primary effect of an uncertainty shock in the Euro area.\n\n**Correct Answer:** A) A decrease in real activity is the primary effect of an uncertainty shock in the Euro area.\n\n**Explanation:** The authors of the paper estimate a Bayesian vector autoregressive model with factor stochastic volatility in the error term to assess the effects of an uncertainty shock in the Euro area. The model allows them to treat macroeconomic uncertainty as a latent quantity during estimation. According to the paper, the primary implication of this model is that a decrease in real activity is the primary effect of an uncertainty shock in the Euro area, with significant results observed for all countries over a period of roughly a year following the shock."}, "59": {"documentation": {"title": "Answer Generation through Unified Memories over Multiple Passages", "source": "Makoto Nakatsuji, Sohei Okui", "docs_id": "2004.13829", "section": ["cs.CL", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Answer Generation through Unified Memories over Multiple Passages. Machine reading comprehension methods that generate answers by referring to multiple passages for a question have gained much attention in AI and NLP communities. The current methods, however, do not investigate the relationships among multiple passages in the answer generation process, even though topics correlated among the passages may be answer candidates. Our method, called neural answer Generation through Unified Memories over Multiple Passages (GUM-MP), solves this problem as follows. First, it determines which tokens in the passages are matched to the question. In particular, it investigates matches between tokens in positive passages, which are assigned to the question, and those in negative passages, which are not related to the question. Next, it determines which tokens in the passage are matched to other passages assigned to the same question and at the same time it investigates the topics in which they are matched. Finally, it encodes the token sequences with the above two matching results into unified memories in the passage encoders and learns the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism. As a result, GUM-MP can generate answers by pointing to important tokens present across passages. Evaluations indicate that GUM-MP generates much more accurate results than the current models do."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the neural answer generation method GUM-MP, and how does it address the limitations of current methods?\n\nA) To generate answers by referring to a single passage for a question, and to investigate the relationships among multiple passages in the answer generation process.\nB) To generate answers by referring to multiple passages for a question, and to determine which tokens in the passages are matched to the question and to other passages assigned to the same question.\nC) To generate answers by pointing to important tokens present across passages, and to learn the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism.\nD) To generate answers by referring to a single passage for a question, and to investigate the topics in which tokens are matched to other passages assigned to the same question.\n\nCorrect Answer: C) To generate answers by pointing to important tokens present across passages, and to learn the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism.\n\nExplanation: The correct answer is C) because the method GUM-MP solves the problem of current methods by generating answers by pointing to important tokens present across passages, and by learning the answer sequence using an encoder-decoder with a multiple-pointer-generator mechanism. This is stated in the introduction of the paper: \"Our method, called neural answer Generation through Unified Memories over Multiple Passages (GUM-MP), solves this problem as follows. First, it determines which tokens in the passages are matched to the question. In particular, it investigates matches between tokens in positive passages, which are assigned to the question, and those in negative passages, which are not related to the question. Next, it determines which tokens in the passage are matched to other passages assigned to the same question and at the same time it investigates the topics in which they are matched. Finally, it encodes the token sequences with the above two matching results into unified memories in the passage encoders and learns the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism.\""}}