{"0": {"documentation": {"title": "Transit surveys for Earths in the habitable zones of white dwarfs", "source": "Eric Agol (University of Washington)", "docs_id": "1103.2791", "section": ["astro-ph.EP", "astro-ph.IM", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transit surveys for Earths in the habitable zones of white dwarfs. To date the search for habitable Earth-like planets has primarily focused on nuclear burning stars. I propose that this search should be expanded to cool white dwarf stars that have expended their nuclear fuel. I define the continuously habitable zone of white dwarfs, and show that it extends from ~0.005 to 0.02 AU for white dwarfs with masses from 0.4 to 0.9 solar masses, temperatures less than 10,000 K, and habitable durations of at least 3 Gyr. As they are similar in size to Earth, white dwarfs may be deeply eclipsed by terrestrial planets that orbit edge-on, which can easily be detected with ground-based telescopes. If planets can migrate inward or reform near white dwarfs, I show that a global robotic telescope network could carry out a transit survey of nearby white dwarfs placing interesting constraints on the presence of habitable Earths. If planets were detected, I show that the survey would favor detection of planets similar to Earth: similar size, temperature, rotation period, and host star temperatures similar to the Sun. The Large Synoptic Survey Telescope could place even tighter constraints on the frequency of habitable Earths around white dwarfs. The confirmation and characterization of these planets might be carried out with large ground and space telescopes."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: A transit survey for habitable Earth-like planets around white dwarfs is proposed. Which of the following statements is NOT correct regarding this proposal?\n\nA) The continuously habitable zone for white dwarfs extends from approximately 0.005 to 0.02 AU.\n\nB) The survey would be most effective for white dwarfs with masses between 0.4 and 0.9 solar masses and temperatures above 10,000 K.\n\nC) White dwarfs' similarity in size to Earth makes it easier to detect deep eclipses caused by terrestrial planets orbiting edge-on.\n\nD) The Large Synoptic Survey Telescope could potentially place tighter constraints on the frequency of habitable Earths around white dwarfs compared to a global robotic telescope network.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation states that the survey would be most effective for white dwarfs with temperatures less than 10,000 K, not above 10,000 K as stated in option B. \n\nOption A is correct according to the text, which defines the continuously habitable zone of white dwarfs as extending from ~0.005 to 0.02 AU.\n\nOption C is also correct, as the document mentions that white dwarfs may be deeply eclipsed by terrestrial planets due to their similar size to Earth.\n\nOption D is accurate, as the text states that the Large Synoptic Survey Telescope could place even tighter constraints on the frequency of habitable Earths around white dwarfs compared to a global robotic telescope network."}, "1": {"documentation": {"title": "Direct observation of the effects of spin dependent momentum of light in\n  optical tweezers", "source": "Debapriya Pal, Subhasish Dutta Gupta, Nirmalya Ghosh, and Ayan\n  Banerjee", "docs_id": "2004.04064", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct observation of the effects of spin dependent momentum of light in\n  optical tweezers. We demonstrate that tight focusing of a circularly polarized Gaussian beam in optical tweezers leads to spin-momentum locking - with the transverse spin angular momentum density being independent of helicity, while the transverse momentum (Poynting vector) becomes helicity dependent. Our theoretical calculations, numerical simulations, and experiments reveal that the presence of a stratified medium in the path of the trapping beam significantly enhances the magnitude of transverse momentum in the radial direction with respect to the beam axis, and likewise, also leads to high off-axial intensity. This overlap allows us to experimentally observe the circular motion of a birefringent particle, trapped off-axis, in response to an input circularly polarized fundamental Gaussian beam carrying no intrinsic orbital angular momentum. The circular motion is dependent on the helicity of the input beam, so that we can identify it to be the signature of the elusive Belinfante spin in propagating light beams obtained in our optical tweezers setup. Our work can be extended to higher-order beams carrying intrinsic orbital angular momentum leading to simple routes of achieving complex particle manipulation using optical tweezers."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In an optical tweezers experiment using a tightly focused circularly polarized Gaussian beam, what phenomenon is observed that demonstrates spin-momentum locking, and how does the presence of a stratified medium affect the beam properties?\n\nA) The transverse spin angular momentum density becomes helicity-dependent, while the transverse momentum remains constant regardless of helicity.\n\nB) The transverse momentum (Poynting vector) becomes helicity-dependent, while the transverse spin angular momentum density remains independent of helicity. The stratified medium enhances the magnitude of transverse momentum in the radial direction.\n\nC) Both the transverse spin angular momentum density and transverse momentum become helicity-dependent. The stratified medium reduces the off-axial intensity of the beam.\n\nD) The transverse spin angular momentum density and transverse momentum both remain independent of helicity. The stratified medium has no significant effect on the beam properties.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation states that \"tight focusing of a circularly polarized Gaussian beam in optical tweezers leads to spin-momentum locking - with the transverse spin angular momentum density being independent of helicity, while the transverse momentum (Poynting vector) becomes helicity dependent.\" Additionally, it mentions that \"the presence of a stratified medium in the path of the trapping beam significantly enhances the magnitude of transverse momentum in the radial direction with respect to the beam axis, and likewise, also leads to high off-axial intensity.\" This aligns with the information provided in option B.\n\nOption A is incorrect because it reverses the helicity dependence of the transverse spin angular momentum density and transverse momentum. Option C is wrong because it incorrectly states that both properties become helicity-dependent and misrepresents the effect of the stratified medium on off-axial intensity. Option D is incorrect as it fails to acknowledge the helicity dependence of the transverse momentum and the effects of the stratified medium on beam properties."}, "2": {"documentation": {"title": "$\\Theta^+$ baryon, $N^* (1685)$ resonance, and $\\pi N$ sigma term in the\n  context of the LEPS and DIANA experiments", "source": "Ghil-Seok Yang and Hyun-Chul Kim", "docs_id": "1204.5644", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$\\Theta^+$ baryon, $N^* (1685)$ resonance, and $\\pi N$ sigma term in the\n  context of the LEPS and DIANA experiments. We reexamine properties of the baryon antidecuplet $\\Theta^+$ and $N^*$, and the $\\pi N$ sigma term within the framework of a chiral soliton model, focusing on their dependence on the $\\Theta^+$ mass. It turns out that the measured value of the $N^*$ mass, $M_{N^*}=1686$ MeV, is consistent with that of the $\\Theta^+$ mass $M_{\\Theta^+}=1524$ MeV by the LEPS collaboration. The $N^*\\to N\\gamma$ magnetic transition moments are almost independent of the $\\Theta^+$ mass. The ratio of the radiative decay width $\\Gamma_{nn^*}$ to $\\Gamma_{pp^*}$ turns out to be around 5. The decay width for $\\Theta^+\\to NK$ is studied in the context of the LEPS and DIANA experiments. When the LEPS value of the $\\Theta^+$ mass is employed, we obtain $\\Gamma_{\\Theta NK} =(0.5\\pm0.1)$ MeV. The $\\pi N$ sigma term is found to be almost independent of the $\\Theta^+$ mass. In addition, we derive a new expression for the $\\pi N$ sigma term in terms of the isospin mass splittings of the hyperon octet as well as that of the antidecuplet $N^*$."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Based on the chiral soliton model analysis of the $\\Theta^+$ baryon and related particles, which of the following statements is correct?\n\nA) The $N^*\\to N\\gamma$ magnetic transition moments strongly depend on the $\\Theta^+$ mass.\n\nB) The ratio of the radiative decay width $\\Gamma_{nn^*}$ to $\\Gamma_{pp^*}$ is approximately 0.2.\n\nC) When using the LEPS value for the $\\Theta^+$ mass (1524 MeV), the decay width $\\Gamma_{\\Theta NK}$ is calculated to be $(0.5\\pm0.1)$ MeV.\n\nD) The $\\pi N$ sigma term shows significant variation with changes in the $\\Theta^+$ mass.\n\nCorrect Answer: C\n\nExplanation: \nOption A is incorrect because the text states that the $N^*\\to N\\gamma$ magnetic transition moments are almost independent of the $\\Theta^+$ mass.\n\nOption B is incorrect. The passage mentions that the ratio of $\\Gamma_{nn^*}$ to $\\Gamma_{pp^*}$ is around 5, not 0.2.\n\nOption C is correct. The document explicitly states that when the LEPS value of the $\\Theta^+$ mass is employed, they obtain $\\Gamma_{\\Theta NK} =(0.5\\pm0.1)$ MeV.\n\nOption D is incorrect. The passage indicates that the $\\pi N$ sigma term is found to be almost independent of the $\\Theta^+$ mass."}, "3": {"documentation": {"title": "Proposal for measuring out-of-time-ordered correlators at finite\n  temperature with coupled spin chains", "source": "Bhuvanesh Sundar, Andreas Elben, Lata Kh Joshi, Torsten V. Zache", "docs_id": "2107.02196", "section": ["quant-ph", "cond-mat.quant-gas", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Proposal for measuring out-of-time-ordered correlators at finite\n  temperature with coupled spin chains. Information scrambling, which is the spread of local information through a system's many-body degrees of freedom, is an intrinsic feature of many-body dynamics. In quantum systems, the out-of-time-ordered correlator (OTOC) quantifies information scrambling. Motivated by experiments that have measured the OTOC at infinite temperature and a theory proposal to measure the OTOC at finite temperature using the thermofield double state, we describe a protocol to measure the OTOC in a finite temperature spin chain that is realized approximately as one half of the ground state of two moderately-sized coupled spin chains. We consider a spin Hamiltonian with particle-hole symmetry, for which we show that the OTOC can be measured without needing sign-reversal of the Hamiltonian. We describe a protocol to mitigate errors in the estimated OTOC, arising from the finite approximation of the system to the thermofield double state. We show that our protocol is also robust to main sources of decoherence in experiments."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the context of measuring out-of-time-ordered correlators (OTOCs) at finite temperature using coupled spin chains, which of the following statements is NOT correct?\n\nA) The protocol involves using two moderately-sized coupled spin chains to approximate the thermofield double state.\n\nB) The proposed method requires sign-reversal of the Hamiltonian for all spin systems.\n\nC) The OTOC quantifies information scrambling in quantum systems.\n\nD) The protocol includes error mitigation strategies to address issues arising from the finite approximation of the thermofield double state.\n\nCorrect Answer: B\n\nExplanation:\nA is correct: The documentation states that the protocol uses \"one half of the ground state of two moderately-sized coupled spin chains\" to approximate the thermofield double state.\n\nB is incorrect: The document specifically mentions that for a spin Hamiltonian with particle-hole symmetry, \"the OTOC can be measured without needing sign-reversal of the Hamiltonian.\" This contradicts the statement in option B, making it the incorrect answer.\n\nC is correct: The document clearly states that \"the out-of-time-ordered correlator (OTOC) quantifies information scrambling\" in quantum systems.\n\nD is correct: The proposal includes \"a protocol to mitigate errors in the estimated OTOC, arising from the finite approximation of the system to the thermofield double state.\""}, "4": {"documentation": {"title": "Inflation from the internal volume in type IIB/F-theory compactification", "source": "Ignatios Antoniadis, Yifan Chen, George K. Leontaris", "docs_id": "1810.05060", "section": ["hep-th", "astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inflation from the internal volume in type IIB/F-theory compactification. We study cosmological inflation within a recently proposed framework of perturbative moduli stabilisation in type IIB/F theory compactifications on Calabi-Yau threefolds. The stabilisation mechanism utilises three stacks of magnetised 7-branes and relies on perturbative corrections to the K\\\"ahler potential that grow logarithmically in the transverse sizes of co-dimension two due to local tadpoles of closed string states in the bulk. The inflaton is the K\\\"ahler modulus associated with the internal compactification volume that starts rolling down the scalar potential from an initial condition around its maximum. Although the parameter space allows moduli stabilisation in de Sitter space, the resulting number of e-foldings is too low. An extra uplifting source of the vacuum energy is then required to achieve phenomenologically viable inflation and a positive (although tiny) vacuum energy at the minimum. Here we use, as an example, a new Fayet-Iliopoulos term proposed recently in supergravity that can be written for a non R-symmetry U(1) and is gauge invariant at the Lagrangian level; its possible origin though in string theory remains an open interesting problem."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the described type IIB/F-theory compactification model for cosmological inflation, which of the following statements is correct regarding the inflaton and the challenges in achieving viable inflation?\n\nA) The inflaton is a complex structure modulus that starts rolling from its minimum value in the scalar potential.\n\nB) The model naturally achieves a sufficient number of e-foldings without requiring any additional uplifting mechanisms.\n\nC) The inflaton is the K\u00e4hler modulus associated with the internal compactification volume, starting its roll from near the maximum of the scalar potential.\n\nD) The stabilization mechanism uses four stacks of magnetized 7-branes and non-perturbative corrections to the superpotential.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that \"The inflaton is the K\u00e4hler modulus associated with the internal compactification volume that starts rolling down the scalar potential from an initial condition around its maximum.\" This directly corresponds to option C.\n\nOption A is incorrect because the inflaton is described as a K\u00e4hler modulus, not a complex structure modulus, and it starts near the maximum, not the minimum.\n\nOption B is incorrect because the text mentions that the resulting number of e-foldings is too low and requires an extra uplifting source to achieve viable inflation.\n\nOption D is incorrect on two counts: the stabilization mechanism uses three (not four) stacks of magnetized 7-branes, and it relies on perturbative corrections to the K\u00e4hler potential, not non-perturbative corrections to the superpotential.\n\nThis question tests the understanding of the specific details of the inflation model described in the documentation, requiring careful reading and comprehension of the technical aspects presented."}, "5": {"documentation": {"title": "Zipf's law and criticality in multivariate data without fine-tuning", "source": "David J. Schwab, Ilya Nemenman, Pankaj Mehta", "docs_id": "1310.0448", "section": ["q-bio.NC", "cond-mat.stat-mech", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Zipf's law and criticality in multivariate data without fine-tuning. The joint probability distribution of many degrees of freedom in biological systems, such as firing patterns in neural networks or antibody sequence composition in zebrafish, often follow Zipf's law, where a power law is observed on a rank-frequency plot. This behavior has recently been shown to imply that these systems reside near to a unique critical point where the extensive parts of the entropy and energy are exactly equal. Here we show analytically, and via numerical simulations, that Zipf-like probability distributions arise naturally if there is an unobserved variable (or variables) that affects the system, e. g. for neural networks an input stimulus that causes individual neurons in the network to fire at time-varying rates. In statistics and machine learning, these models are called latent-variable or mixture models. Our model shows that no fine-tuning is required, i.e. Zipf's law arises generically without tuning parameters to a point, and gives insight into the ubiquity of Zipf's law in a wide range of systems."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of biological systems exhibiting Zipf's law behavior, which of the following statements most accurately describes the relationship between criticality and latent variable models?\n\nA) Systems following Zipf's law must be fine-tuned to a critical point where entropy and energy are exactly equal.\n\nB) Zipf-like distributions arise only in systems with observable variables that directly cause power law behavior.\n\nC) Latent variable models can produce Zipf-like distributions without the need for fine-tuning to a critical point.\n\nD) The presence of unobserved variables in a system necessarily prevents the emergence of Zipf's law behavior.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that Zipf-like probability distributions can arise naturally in systems with unobserved (latent) variables affecting the system, without requiring fine-tuning to a critical point. This is a key insight of the research, showing that Zipf's law can emerge generically in latent variable or mixture models.\n\nAnswer A is incorrect because the documentation challenges the idea that systems must be fine-tuned to a critical point to exhibit Zipf's law behavior.\n\nAnswer B is incorrect as it contradicts the main point of the research, which emphasizes the role of unobserved (latent) variables in producing Zipf-like distributions.\n\nAnswer D is incorrect because the research actually suggests that the presence of unobserved variables can lead to the emergence of Zipf's law behavior, rather than preventing it.\n\nThis question tests the student's understanding of the relationship between Zipf's law, criticality, and latent variable models in biological systems, as presented in the given documentation."}, "6": {"documentation": {"title": "Simple cubic random-site percolation thresholds for neighborhoods\n  containing fourth-nearest neighbors", "source": "K. Malarz", "docs_id": "1501.01586", "section": ["cond-mat.stat-mech", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simple cubic random-site percolation thresholds for neighborhoods\n  containing fourth-nearest neighbors. In the paper random-site percolation thresholds for simple cubic lattice with sites' neighborhoods containing next-next-next-nearest neighbors (4NN) are evaluated with Monte Carlo simulations. A recently proposed algorithm with low sampling for percolation thresholds estimation [Bastas et al., arXiv:1411.5834] is implemented for the studies of the top-bottom wrapping probability. The obtained percolation thresholds are $p_C(\\text{4NN})=0.31160(12)$, $p_C(\\text{4NN+NN})=0.15040(12)$, $p_C(\\text{4NN+2NN})=0.15950(12)$, $p_C(\\text{4NN+3NN})=0.20490(12)$, $p_C(\\text{4NN+2NN+NN})=0.11440(12)$, $p_C(\\text{4NN+3NN+NN})=0.11920(12)$, $p_C(\\text{4NN+3NN+2NN})=0.11330(12)$, $p_C(\\text{4NN+3NN+2NN+NN})=0.10000(12)$, where 3NN, 2NN, NN stands for next-next-nearest neighbors, next-nearest neighbors, and nearest neighbors, respectively. As an SC lattice with 4NN neighbors may be mapped onto two independent interpenetrated SC lattices but with two times larger lattice constant the percolation threshold $p_C$(4NN) is exactly equal to $p_C$(NN). The simplified Bastas et al. method allows for reaching uncertainty of the percolation threshold value $p_C$ similar to those obtained with classical method but ten times faster."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a simple cubic lattice with random-site percolation, which of the following statements is correct regarding the percolation thresholds (pc) for different neighborhood configurations?\n\nA) pc(4NN) < pc(4NN+NN) < pc(4NN+2NN) < pc(4NN+3NN)\nB) pc(4NN+3NN+2NN+NN) < pc(4NN+3NN+2NN) < pc(4NN+3NN+NN) < pc(4NN+2NN+NN)\nC) pc(4NN) = pc(NN), and this equality is due to the mapping of 4NN onto two independent interpenetrated SC lattices\nD) The Bastas et al. method used in this study provides percolation threshold estimates with higher precision but requires more computational time compared to classical methods\n\nCorrect Answer: C\n\nExplanation: Option C is correct because the paper explicitly states that \"As an SC lattice with 4NN neighbors may be mapped onto two independent interpenetrated SC lattices but with two times larger lattice constant the percolation threshold pc(4NN) is exactly equal to pc(NN).\" This equality and its explanation are directly provided in the text.\n\nOption A is incorrect because it doesn't reflect the actual order of the thresholds given in the paper. The correct order would be pc(4NN+NN) < pc(4NN+2NN) < pc(4NN+3NN) < pc(4NN).\n\nOption B is incorrect because while it does show a correct ascending order of some thresholds, it doesn't include all configurations and isn't the main point of the research.\n\nOption D is incorrect because the paper states that the Bastas et al. method allows for reaching similar uncertainty in the percolation threshold value but ten times faster than classical methods, not with higher precision or more computational time."}, "7": {"documentation": {"title": "Understanding the Great Recession Using Machine Learning Algorithms", "source": "Rickard Nyman, Paul Ormerod", "docs_id": "2001.02115", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the Great Recession Using Machine Learning Algorithms. Nyman and Ormerod (2017) show that the machine learning technique of random forests has the potential to give early warning of recessions. Applying the approach to a small set of financial variables and replicating as far as possible a genuine ex ante forecasting situation, over the period since 1990 the accuracy of the four-step ahead predictions is distinctly superior to those actually made by the professional forecasters. Here we extend the analysis by examining the contributions made to the Great Recession of the late 2000s by each of the explanatory variables. We disaggregate private sector debt into its household and non-financial corporate components. We find that both household and non-financial corporate debt were key determinants of the Great Recession. We find a considerable degree of non-linearity in the explanatory models. In contrast, the public sector debt to GDP ratio appears to have made very little contribution. It did rise sharply during the Great Recession, but this was as a consequence of the sharp fall in economic activity rather than it being a cause. We obtain similar results for both the United States and the United Kingdom."}, "answer": "Here's a multi-form question based on the provided information:\n\nQuestion: Which of the following statements best summarizes the key findings of Nyman and Ormerod's study on predicting recessions using machine learning techniques?\n\nA) Public sector debt was the primary contributor to the Great Recession, while private sector debt had minimal impact.\n\nB) Random forests outperformed professional forecasters in predicting recessions, and both household and non-financial corporate debt were key determinants of the Great Recession.\n\nC) The study found a linear relationship between economic variables and recession prediction, with public sector debt being the most significant factor.\n\nD) Machine learning techniques were less accurate than traditional forecasting methods, and only household debt was identified as a major cause of the Great Recession.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately reflects the main findings of the study as described in the passage. The study showed that the random forests machine learning technique provided more accurate predictions of recessions compared to professional forecasters. Additionally, the research found that both household and non-financial corporate debt were key determinants of the Great Recession. \n\nAnswer A is incorrect because the passage states that public sector debt made very little contribution to the recession. \n\nAnswer C is wrong on two counts: the study found a considerable degree of non-linearity in the explanatory models, not a linear relationship, and public sector debt was not identified as a significant factor.\n\nAnswer D is incorrect because the machine learning technique (random forests) was reported to be more accurate than professional forecasters, not less accurate. Also, while household debt was indeed identified as a major factor, the study found that non-financial corporate debt was also a key determinant, not just household debt."}, "8": {"documentation": {"title": "On the statistics of differences of zeta zeros starting from zero number\n  $10^{23}$", "source": "Jouni Takalo", "docs_id": "2001.11353", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the statistics of differences of zeta zeros starting from zero number\n  $10^{23}$. We study distributions of differences of unscaled Riemann zeta zeros, $\\gamma-\\gamma^{'}$, at large. We show, that independently of the location of the zeros, i.e., even for zeros as high as $10^{23}$, their differences have similar statistical properties. The distributions of differences are skewed usually towards the nearest zeta zero. We show, however, that this is not always the case, but depends upon the distance and number of nearby zeros on each side of the corresponding distribution. The skewness, however, always decreases when zeta zero is crossed from left to right, i.e., in increasing direction. Furthermore, we show that the variance of distributions has local maximum or, at least, a turning point at every zeta zero, i.e., local minimum of the second derivative of the variance. In addition, it seems that the higher the zeros the more compactly the distributions of the differences are located in the skewness-kurtosis -plane. Furthermore, we show that distributions can be fitted with Johnson probability density function, despite the value of skewness or kurtosis of the distribution."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements about the distribution of differences of unscaled Riemann zeta zeros (\u03b3 - \u03b3') at large heights (around 10^23) is NOT correct?\n\nA) The skewness of the distribution always decreases when crossing a zeta zero from left to right.\n\nB) The variance of the distributions has a local maximum or turning point at every zeta zero.\n\nC) The distributions of differences are always skewed towards the nearest zeta zero, regardless of the number and distance of nearby zeros.\n\nD) The distributions can be fitted with Johnson probability density function, regardless of their skewness or kurtosis.\n\nCorrect Answer: C\n\nExplanation: Statement C is incorrect and thus the correct answer to this question. The documentation states that the skewness of the distributions towards the nearest zeta zero \"is not always the case, but depends upon the distance and number of nearby zeros on each side of the corresponding distribution.\" This contradicts the absolute claim made in option C.\n\nOptions A, B, and D are all correct according to the given information:\nA) The documentation explicitly states that \"The skewness, however, always decreases when zeta zero is crossed from left to right, i.e., in increasing direction.\"\nB) It's mentioned that \"the variance of distributions has local maximum or, at least, a turning point at every zeta zero.\"\nD) The text states that \"distributions can be fitted with Johnson probability density function, despite the value of skewness or kurtosis of the distribution.\"\n\nThis question tests the reader's ability to carefully parse and understand nuanced statistical information, identifying which statement contradicts the given facts."}, "9": {"documentation": {"title": "Symmetry Breaking and Link Homologies II", "source": "Nitu Kitchloo", "docs_id": "1910.07444", "section": ["math.AT", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetry Breaking and Link Homologies II. In the first part of this paper, we constructed a filtered U(r)-equivariant stable homotopy type called the spectrum of strict broken symmetries sB(L) of links L given by closing a braid with r strands. We further showed that evaluating this spectrum on suitable U(r)-equivariant cohomology theories gives rise to a spectral sequence of link invariants that converges to the cohomology of the limiting spectrum. In this followup, we apply Borel equivariant singular cohomology to our construction. We show that the E_2-term is isomorphic to an unreduced, integral form of triply-graded link homology, whose value on the unknot is a free graded algebra on two generators in degrees 2 and 1 resp. More precisely, we show that the E_1-term of the spectral sequence is isomorphic to the Hochschild-homology complex of Soergel bimodules that was shown by M. Khovanov to compute triply-graded link homology. We also set up the theory that allows for twisting equivariant cohomologies by adjoint-equivariant local systems on U(r). This allows us to twist Borel equivariant cohomology by a power series p(x) with coefficients given by formal variables, and no constant term. Based on computations, we speculate that the specialization to the case p(x) = x^n, gives rise to a spectral sequence whose E_2-term is isomorphic to sl(n)-link homology. In particular, p(x) can can be viewed as the universal differential of a potential that has no linear term"}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of the spectrum of strict broken symmetries sB(L) for links L, which of the following statements is correct regarding the application of Borel equivariant singular cohomology and its relationship to triply-graded link homology?\n\nA) The E_1-term of the spectral sequence is isomorphic to the Hochschild-homology complex of Soergel bimodules, which computes doubly-graded link homology.\n\nB) The E_2-term of the spectral sequence is isomorphic to a reduced, rational form of triply-graded link homology, whose value on the unknot is a free graded algebra on three generators.\n\nC) The E_2-term of the spectral sequence is isomorphic to an unreduced, integral form of triply-graded link homology, whose value on the unknot is a free graded algebra on two generators in degrees 2 and 1 respectively.\n\nD) The E_3-term of the spectral sequence is isomorphic to the Hochschild-homology complex of Soergel bimodules, which computes triply-graded link homology.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that when applying Borel equivariant singular cohomology to the spectrum of strict broken symmetries sB(L), \"the E_2-term is isomorphic to an unreduced, integral form of triply-graded link homology, whose value on the unknot is a free graded algebra on two generators in degrees 2 and 1 resp.\" This directly corresponds to the statement in option C.\n\nOption A is incorrect because it mentions the E_1-term being isomorphic to the Hochschild-homology complex, which is true, but it incorrectly states that this computes doubly-graded link homology instead of triply-graded link homology.\n\nOption B is incorrect because it describes the E_2-term as a reduced, rational form with three generators, which contradicts the given information about it being an unreduced, integral form with two generators.\n\nOption D is incorrect because it attributes the isomorphism with the Hochschild-homology complex to the E_3-term, while the documentation clearly states this is a property of the E_1-term."}, "10": {"documentation": {"title": "The Compressed Baryonic Matter Experiment at FAIR: Progress with\n  feasibility studies and detector developments", "source": "Johann M. Heuser (for the CBM collaboration)", "docs_id": "0907.2136", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Compressed Baryonic Matter Experiment at FAIR: Progress with\n  feasibility studies and detector developments. The Compressed Baryonic Matter (CBM) experiment is being planned at the international research center FAIR, under realization next to the GSI laboratory in Darmstadt, Germany. Its physics programme addresses the QCD phase diagram in the region of highest net baryon densities. Of particular interest are the expected first order phase transition from partonic to hadronic matter, ending in a critical point, and modifications of hadron properties in the dense medium as a signal of chiral symmetry restoration. Laid out as a fixed-target experiment at the heavy-ion synchrotrons SIS-100/300, the detector will record both proton-nucleus and nucleus-nucleus collisions at beam energies between 10 and 45$A$ GeV. Hadronic, leptonic and photonic observables have to be measured with large acceptance. The interaction rates will reach 10 MHz to measure extremely rare probes like charm near threshold. Two versions of the experiment are being studied, optimized for either electron-hadron or muon identification, combined with silicon detector based charged-particle tracking and micro-vertex detection. The CBM physics requires the development of novel detector sytems, trigger and data acquisition concepts as well as innovative real-time reconstruction techniques. Progress with feasibility studies of the CBM experiment and the development of its detector systems are reported."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: The Compressed Baryonic Matter (CBM) experiment at FAIR aims to study the QCD phase diagram at high net baryon densities. Which of the following combinations of experimental features and physics goals is NOT correct for the CBM experiment?\n\nA) Beam energies between 10 and 45A GeV; investigation of the first order phase transition from partonic to hadronic matter\nB) Interaction rates up to 10 MHz; measurement of extremely rare probes like charm near threshold\nC) Fixed-target experiment design; study of modifications of hadron properties in dense medium\nD) Optimized for positron-hadron identification; exploration of the critical point in the QCD phase diagram\n\nCorrect Answer: D\n\nExplanation: \nA) is correct. The CBM experiment will use beam energies between 10 and 45A GeV and aims to study the first order phase transition from partonic to hadronic matter.\n\nB) is correct. The experiment is designed to handle interaction rates up to 10 MHz, allowing for the measurement of extremely rare probes like charm production near threshold.\n\nC) is correct. CBM is indeed designed as a fixed-target experiment and aims to study modifications of hadron properties in dense medium as a signal of chiral symmetry restoration.\n\nD) is incorrect. The document states that two versions of the experiment are being studied, optimized for either electron-hadron or muon identification, not positron-hadron identification. Additionally, while exploring the critical point is part of the experiment's goals, this option combines an incorrect experimental feature with a correct physics goal.\n\nThis question tests the student's ability to integrate multiple aspects of the experiment's design and physics goals, requiring careful reading and understanding of the provided information."}, "11": {"documentation": {"title": "Time is Money: The Equilibrium Trading Horizon and Optimal Arrival Price", "source": "Kevin Patrick Darby", "docs_id": "2104.05844", "section": ["q-fin.MF", "q-fin.PM", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time is Money: The Equilibrium Trading Horizon and Optimal Arrival Price. Executing even moderately large derivatives orders can be expensive and risky; it's hard to balance the uncertainty of working an order over time versus paying a liquidity premium for immediate execution. Here, we introduce the Time Is Money model, which calculates the Equilibrium Trading Horizon over which to execute an order within the adversarial forces of variance risk and liquidity premium. We construct a hypothetical at-the-money option within Arithmetic Brownian Motion and invert the Bachelier model to compute an inflection point between implied variance and liquidity cost as governed by a central limit order book, each in real time as they evolve. As a result, we demonstrate a novel, continuous-time Arrival Price framework. Further, we argue that traders should be indifferent to choosing between variance risk and liquidity cost, unless they have a predetermined bias or an exogenous position with a convex payoff. We, therefore, introduce half-life factor asymptotics to the model based on a convexity factor and compare results to existing models. We also describe a specialization of the model for trading a basket of correlated instruments, as exemplified by a futures calendar spread. Finally, we establish groundwork for microstructure optimizations as well as explore short term drift and conditional expected slippage within the Equilibrium Horizon framework."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the Time Is Money model for optimal order execution, which of the following best describes the Equilibrium Trading Horizon?\n\nA) The time period that maximizes trading profits regardless of market conditions\nB) The optimal duration for executing an order that balances variance risk and liquidity premium\nC) The timeframe determined solely by the liquidity available in the central limit order book\nD) The period required to execute an order with zero slippage\n\nCorrect Answer: B\n\nExplanation: The Equilibrium Trading Horizon in the Time Is Money model is the optimal duration for executing an order that balances variance risk and liquidity premium. This concept is central to the model's approach of finding an equilibrium between the uncertainty of working an order over time (variance risk) and the cost of immediate execution (liquidity premium). The model calculates this horizon by considering both the evolving implied variance and liquidity cost in real-time, using a hypothetical at-the-money option within Arithmetic Brownian Motion and inverting the Bachelier model. \n\nOption A is incorrect because the model doesn't focus solely on maximizing profits but on optimizing execution considering both risk and cost. \n\nOption C is incomplete, as it only considers liquidity and ignores the variance risk component. \n\nOption D is unrealistic, as the model acknowledges that executing large orders typically involves some level of slippage and seeks to optimize rather than eliminate it entirely."}, "12": {"documentation": {"title": "Signatures of a 4$\\pi$-periodic supercurrent in the voltage response of\n  capacitively shunted topological Josephson junctions", "source": "Jordi Pic\\'o-Cort\\'es, Fernando Dom\\'inguez and Gloria Platero", "docs_id": "1703.09100", "section": ["cond-mat.mes-hall", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signatures of a 4$\\pi$-periodic supercurrent in the voltage response of\n  capacitively shunted topological Josephson junctions. We investigate theoretical aspects of the detection of Majorana bound states in Josephson junctions using the semiclassical RCSJ model of junction dynamics. The influence of a 4$\\pi$-periodic supercurrent contribution can be detected through its effect on the width of the Shapiro steps and the Fourier spectrum of the voltage signal. We explain how the inclusion of a capacitance term results in a strong quenching of the odd steps when the junction is underdamped, and hence may be used to effectively detect Majorana bound states. Furthermore, in presence of capacitance the first and third steps are quenched to a different degree, as observed experimentally. We examine the emission spectrum of phase-locked solutions, showing that the presence of period-doubling may difficult the measurement of the 4$\\pi$-periodic contribution from the Fourier spectrum. Finally, we study the voltage response in the quasiperiodic regime and indicate how the Fourier spectra and the first-return maps in this regime reflect the change of periodicity in the supercurrent."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a capacitively shunted topological Josephson junction, which of the following statements is NOT true regarding the detection of Majorana bound states using the RCSJ model?\n\nA) The presence of a 4\u03c0-periodic supercurrent can be detected through its effect on the width of Shapiro steps.\n\nB) In an underdamped junction, the inclusion of capacitance results in strong quenching of even Shapiro steps.\n\nC) The first and third Shapiro steps are quenched to different degrees when capacitance is present.\n\nD) Period-doubling in the emission spectrum may complicate the measurement of the 4\u03c0-periodic contribution from the Fourier spectrum.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation states that the inclusion of a capacitance term results in strong quenching of the odd steps, not the even steps, when the junction is underdamped. This quenching of odd steps is described as a potential method to effectively detect Majorana bound states.\n\nOptions A, C, and D are all true according to the given information:\nA) The document explicitly states that the 4\u03c0-periodic supercurrent contribution can be detected through its effect on Shapiro step widths.\nC) The text mentions that in the presence of capacitance, the first and third steps are quenched to different degrees.\nD) The document indicates that period-doubling may make it difficult to measure the 4\u03c0-periodic contribution from the Fourier spectrum."}, "13": {"documentation": {"title": "Broadband Purcell effect: Radiative decay engineering with metamaterials", "source": "Zubin Jacob, Igor Smolyaninov and Evgenii Narimanov", "docs_id": "0910.3981", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Broadband Purcell effect: Radiative decay engineering with metamaterials. Engineering the photonic density of states (PDOS) using resonant microcavities or periodic dielectric media gives control over a plethora of classical and quantum phenomena associated with light. Here, we show that nanostructured metamaterials with hyperbolic dispersion, possess a broad bandwidth singularity in the PDOS, an effect not present in any other photonic system, which allows remarkable control over light-matter interactions. A spectacular manifestation of this non-resonant PDOS alteration is the broadband Purcell effect, an enhancement in the spontaneous emission of a light source, which ultimately leads to a device that can efficiently harness a single photon from an isolated emitter. Our approach differs from conventional resonant Purcell effect routes to single photon sources with a limitation in bandwidth, which places restrictions on the probable use of such methods for practical device applications, especially at room temperature. The proposed metadevice, useful for applications from quantum communications to biosensing also opens up the possibility of using metamaterials to probe the quantum electrodynamic properties of atoms and artificial atoms such as quantum dots."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the unique advantage of nanostructured metamaterials with hyperbolic dispersion over conventional resonant microcavities or periodic dielectric media for controlling light-matter interactions?\n\nA) They provide a narrower bandwidth for the Purcell effect\nB) They exhibit a broad bandwidth singularity in the photonic density of states (PDOS)\nC) They only work at extremely low temperatures\nD) They are limited to enhancing classical optical phenomena\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text explicitly states that \"nanostructured metamaterials with hyperbolic dispersion, possess a broad bandwidth singularity in the PDOS, an effect not present in any other photonic system.\" This broad bandwidth singularity is the key advantage that distinguishes these metamaterials from other photonic systems.\n\nAnswer A is incorrect because the text emphasizes a broadband effect, not a narrow bandwidth.\n\nAnswer C is incorrect because the text mentions the potential for room temperature applications, stating that the approach \"differs from conventional resonant Purcell effect routes to single photon sources with a limitation in bandwidth, which places restrictions on the probable use of such methods for practical device applications, especially at room temperature.\"\n\nAnswer D is incorrect because the text discusses both classical and quantum phenomena, mentioning applications \"from quantum communications to biosensing\" and the possibility of probing \"quantum electrodynamic properties of atoms and artificial atoms such as quantum dots.\""}, "14": {"documentation": {"title": "Deep HST-WFPC2 photometry of NGC 288. I. Binary Systems and Blue\n  Stragglers", "source": "M. Bellazzini (Oss. Astr. di Bologna), F. Fusi Pecci (Oss. Astr. di\n  Bologna / Oss. Astr. di Cagliari), M. Messineo (Oss. Astr. di Bologna /\n  Sterrenwach Leiden), L. Monaco (Bologna University), and R.T. Rood (Virginia\n  University)", "docs_id": "astro-ph/0112343", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep HST-WFPC2 photometry of NGC 288. I. Binary Systems and Blue\n  Stragglers. We present the first results of a deep WFPC2 photometric survey of the loose galactic globular cluster NGC 288. The fraction of binary systems is estimated from the color distribution of objects near the Main Sequence (MS) with a method analogous to that introduced by Rubenstein & Bailyn(1997). We have unequivocally detected a significant population of binary systems which has a radial distribution that has been significantly influenced by mass segregation. In the inner region of the cluster (r<1 r_h = 1.6 r_c) the binary fraction (f_b) lies in the range 0.08--0.38 regardless of the assumed distribution of mass ratios, F(q). The most probable f_b lies between 0.10 and 0.20 depending on the adopted F(q). On the other hand, in the outer region (r> 1 r_h), f_b must be less than 0.10, and the most likely value is 0.0, independently of the adopted F(q). The detected population of binaries is dominated by primordial systems. The specific frequency of Blue Straggler Stars (BSS) is exceptionally high, suggesting that the BSS production mechanism via binary evolution can be very efficient. A large population of BSS is possible even in low density environments if a sufficient reservoir of primordial binaries is available. The observed distribution of BSS in the Color Magnitude Diagram is not compatible with a rate of BSS production which has been constant in time, if it is assumed that all the BSS are formed by the merging of two stars."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Based on the deep WFPC2 photometric survey of NGC 288, which of the following statements best describes the binary fraction (f_b) distribution in the cluster?\n\nA) The binary fraction is uniformly distributed throughout the cluster, with f_b \u2248 0.10-0.20 in both inner and outer regions.\n\nB) The binary fraction is higher in the outer region (r > 1 r_h) compared to the inner region (r < 1 r_h).\n\nC) The binary fraction in the inner region (r < 1 r_h) is between 0.08-0.38, while in the outer region (r > 1 r_h) it is likely 0.0 and must be less than 0.10.\n\nD) The binary fraction is independent of radial distribution and is consistently around 0.38 throughout the cluster.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the findings reported in the Arxiv documentation. The study found that in the inner region of the cluster (r < 1 r_h = 1.6 r_c), the binary fraction (f_b) lies in the range 0.08-0.38, with the most probable f_b between 0.10 and 0.20. In contrast, for the outer region (r > 1 r_h), f_b must be less than 0.10, with the most likely value being 0.0. This radial distribution difference is attributed to mass segregation, which has significantly influenced the binary population in the cluster.\n\nOption A is incorrect because it suggests a uniform distribution, which contradicts the observed radial differences. Option B is wrong as it reverses the actual distribution, claiming a higher binary fraction in the outer region. Option D is incorrect as it states a constant binary fraction throughout the cluster, which does not align with the reported radial variation."}, "15": {"documentation": {"title": "The Correlated Electronic States of a few Polycyclic Aromatic\n  Hydrocarbons: A Computational Study", "source": "Geetanjali Giri, Anusooya Y. Pati, and S. Ramasesha", "docs_id": "1905.01839", "section": ["cond-mat.str-el", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Correlated Electronic States of a few Polycyclic Aromatic\n  Hydrocarbons: A Computational Study. In recent years Polycyclic Aromatic Hydrocarbons (PAHs) have been studied for their electronic properties as they are viewed as nanodots of graphene. They have also been of interest as functional molecules for applications such as light emitting diodes and solar cells. Since last few years varying structural and chemical properties corresponding to the size and geometry of these molecules have been studied both theoretically and experimentally. In this paper, we carry out a systematic study of the electronic states of several PAHs using the Pariser-Parr-Pople model which incorporates long-range electron correlations. In all the molecules studied by us, we find that the 2A state is below the 1B state and hence none of them will be fluorescent in the gaseous phase. The singlet-triplet gap is more than one-half of the singlet-singlet gap in all cases and hence none of these PAHs can be candidates for improved solar cell efficiencies in a singlet fission. We discuss in detail the properties of the electronic states which include bond orders and spin densities (in triplets) of these systems."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements is NOT consistent with the findings of the computational study on Polycyclic Aromatic Hydrocarbons (PAHs) described in the given text?\n\nA) The 2A electronic state is consistently found to be at a lower energy level than the 1B state in all studied PAH molecules.\n\nB) PAHs studied in this research are unlikely to exhibit fluorescence when in the gaseous phase.\n\nC) The singlet-triplet gap in the examined PAHs is always less than half of the singlet-singlet gap.\n\nD) The investigated PAHs are not suitable candidates for improving solar cell efficiencies through singlet fission.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it contradicts the information provided in the text. The passage states that \"The singlet-triplet gap is more than one-half of the singlet-singlet gap in all cases,\" which is the opposite of what option C claims.\n\nOptions A, B, and D are all consistent with the findings described in the text:\nA) The text explicitly states that \"the 2A state is below the 1B state\" for all studied molecules.\nB) The text indicates that none of the studied PAHs will be fluorescent in the gaseous phase due to the relative positions of the 2A and 1B states.\nD) The passage directly states that none of these PAHs can be candidates for improved solar cell efficiencies in singlet fission, due to the relationship between the singlet-triplet and singlet-singlet gaps."}, "16": {"documentation": {"title": "The Near and Mid-infrared photometric properties of known redshift\n  $z\\geq5$ Quasars", "source": "Nicholas P. Ross and Nicholas J. G. Cross", "docs_id": "1906.06974", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Near and Mid-infrared photometric properties of known redshift\n  $z\\geq5$ Quasars. We assemble a catalogue of 488 spectroscopically confirmed very high ($z\\geq5.00$) redshift quasars and report their near- ($ZYJHK_{s}/K$) and mid- (WISE W1234) infrared properties. 97\\% of the VH$z$Q sample is detected in one or more NIR ($ZYJHK/K_{s}$) band, with lack of coverage rather than lack of depth being the reason for the non-detections. 389 (80\\%) of the very high redshift quasars are detected at 3.4$\\mu$m in the W1 band from the unWISE catalog and all of the $z\\geq7$ quasars are detected in both unWISE W1 and W2. Using archival WFCAM/UKIRT and VIRCAM/VISTA data we check for photometric variability that might be expected from super-Eddington accretion. We find 28 of the quasars have sufficient NIR measurements and signal-to-noise ratio to look for variability. Weak variability was detected in multiple bands of SDSS J0959+0227, and very marginally in the $Y$-band of MMT J0215-0529. Only one quasar, SDSS J0349+0034, shows significant differences between WFCAM and VISTA magnitudes in one band. With supermassive black hole accretion likely to be redshift invariant up to very high-redshift, further monitoring of these sources is warranted. All the data, analysis codes and plots used and generated here can be found at: https://github.com/d80b2t/VHzQ}{\\tt github.com/d80b2t/VHzQ."}, "answer": "Here's a multi-form question based on the given information:\n\nQuestion: A study of very high redshift quasars (z\u22655) examined their near- and mid-infrared properties. Which of the following statements is true regarding the detection of these quasars in different wavelength bands?\n\nA) All z\u22657 quasars were detected in WISE W1 band only\nB) 80% of the quasars were detected in the near-infrared bands, but less than half in mid-infrared\nC) 97% of quasars were detected in at least one near-infrared band, and 80% in WISE W1\nD) All quasars were detected in both near-infrared and mid-infrared bands\n\nCorrect Answer: C\n\nExplanation: The passage states that \"97% of the VHzQ sample is detected in one or more NIR (ZYJHK/Ks) band\" and \"389 (80%) of the very high redshift quasars are detected at 3.4\u03bcm in the W1 band from the unWISE catalog.\" This directly corresponds to option C. \n\nOption A is incorrect because the passage mentions that all z\u22657 quasars were detected in both W1 and W2 bands, not just W1. \n\nOption B is incorrect as it underestimates the near-infrared detection rate (97% vs. stated 80%) and doesn't accurately represent the mid-infrared detection rate.\n\nOption D is too strong of a statement, as not all quasars were detected in both near- and mid-infrared bands according to the given information."}, "17": {"documentation": {"title": "Projective and Coarse Projective Integration for Problems with\n  Continuous Symmetries", "source": "Michail E. Kavousanakis, Radek Erban, Andreas G. Boudouvis, C. William\n  Gear and Ioannis G. Kevrekidis", "docs_id": "math/0608122", "section": ["math.DS", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Projective and Coarse Projective Integration for Problems with\n  Continuous Symmetries. Temporal integration of equations possessing continuous symmetries (e.g. systems with translational invariance associated with traveling solutions and scale invariance associated with self-similar solutions) in a ``co-evolving'' frame (i.e. a frame which is co-traveling, co-collapsing or co-exploding with the evolving solution) leads to improved accuracy because of the smaller time derivative in the new spatial frame. The slower time behavior permits the use of {\\it projective} and {\\it coarse projective} integration with longer projective steps in the computation of the time evolution of partial differential equations and multiscale systems, respectively. These methods are also demonstrated to be effective for systems which only approximately or asymptotically possess continuous symmetries. The ideas of projective integration in a co-evolving frame are illustrated on the one-dimensional, translationally invariant Nagumo partial differential equation (PDE). A corresponding kinetic Monte Carlo model, motivated from the Nagumo kinetics, is used to illustrate the coarse-grained method. A simple, one-dimensional diffusion problem is used to illustrate the scale invariant case. The efficiency of projective integration in the co-evolving frame for both the macroscopic diffusion PDE and for a random-walker particle based model is again demonstrated."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of projective and coarse projective integration for problems with continuous symmetries, which of the following statements is most accurate?\n\nA) Temporal integration in a co-evolving frame always results in slower time behavior, regardless of the type of continuous symmetry present in the system.\n\nB) Projective integration in a co-evolving frame is only effective for systems that strictly possess continuous symmetries and cannot be applied to systems with approximate or asymptotic symmetries.\n\nC) The improved accuracy in a co-evolving frame is primarily due to the larger time derivative in the new spatial frame, allowing for shorter projective steps.\n\nD) The method allows for longer projective steps in both partial differential equations and multiscale systems, by leveraging the slower time behavior in the co-evolving frame.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that temporal integration in a co-evolving frame leads to improved accuracy due to smaller time derivatives in the new spatial frame. This slower time behavior allows for the use of projective and coarse projective integration with longer projective steps in both partial differential equations and multiscale systems.\n\nOption A is incorrect because while the co-evolving frame often results in slower time behavior, the statement doesn't account for the specific types of symmetries mentioned (translational, scale).\n\nOption B is false because the document explicitly states that these methods are also effective for systems which only approximately or asymptotically possess continuous symmetries.\n\nOption C is incorrect because it mentions a larger time derivative, whereas the document states that the improved accuracy is due to a smaller time derivative in the new spatial frame."}, "18": {"documentation": {"title": "Josephson (001) tilt grain boundary junctions of high temperature\n  superconductors", "source": "Gerald B. Arnold, Richard A. Klemm", "docs_id": "cond-mat/0411069", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Josephson (001) tilt grain boundary junctions of high temperature\n  superconductors. We calculate the critical current $I_c$ across in-plane (001) tilt grain boundary junctions of high temperature superconductors. We solve for the electronic states corresponding to the electron-doped cuprates, two slightly different hole-doped cuprates, and an extremely underdoped hole-doped cuprate in each half-space, and weakly connect the two half-spaces by either specular or random quasiparticle tunneling. We treat symmetric, straight, and fully asymmetric junctions with s-, extended-s-, or d$_{x^2-y^2}$-wave order parameters. For symmetric junctions with random grain boundary tunneling, our results are generally in agreement with the Sigrist-Rice form for ideal junctions that has been used to interpret ``phase-sensitive'' experiments consisting of such in-plane grain boundary junctions. For specular grain boundary tunneling across symmetric juncitons, our results depend upon the Fermi surface topology, but are usually rather consistent with the random facet model of Tsuei {\\it et al.} [Phys. Rev. Lett. {\\bf 73}, 593 (1994)]. Our results for asymmetric junctions of electron-doped cuparates are in agreement with the Sigrist-Rice form. However, ou resutls for asymmetric junctions of hole-doped cuprates show that the details of the Fermi surface topology and of the tunneling processes are both very important, so that the ``phase-sensitive'' experiments based upon the in-plane Josephson junctions are less definitive than has generally been thought."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of Josephson (001) tilt grain boundary junctions of high temperature superconductors, which of the following statements is most accurate regarding the results for asymmetric junctions?\n\nA) Results for both electron-doped and hole-doped cuprates consistently agree with the Sigrist-Rice form.\n\nB) Results for electron-doped cuprates agree with the Sigrist-Rice form, while results for hole-doped cuprates are highly dependent on Fermi surface topology and tunneling processes.\n\nC) Results for hole-doped cuprates agree with the Sigrist-Rice form, while results for electron-doped cuprates are highly dependent on Fermi surface topology and tunneling processes.\n\nD) Results for both electron-doped and hole-doped cuprates show little dependence on Fermi surface topology and tunneling processes.\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the complex results presented in the document regarding asymmetric junctions. The correct answer is B because the document states that \"Our results for asymmetric junctions of electron-doped cuparates are in agreement with the Sigrist-Rice form. However, our results for asymmetric junctions of hole-doped cuprates show that the details of the Fermi surface topology and of the tunneling processes are both very important.\" This directly supports option B, highlighting the difference in behavior between electron-doped and hole-doped cuprates in asymmetric junctions."}, "19": {"documentation": {"title": "A Second Look at Post Crisis Pricing of Derivatives - Part I: A Note on\n  Money Accounts and Collateral", "source": "Hovik Tumasyan", "docs_id": "1806.09198", "section": ["q-fin.PR", "q-fin.MF", "q-fin.RM", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Second Look at Post Crisis Pricing of Derivatives - Part I: A Note on\n  Money Accounts and Collateral. The paper reviews origins of the approach to pricing derivatives post-crisis by following three papers that have received wide acceptance from practitioners as the theoretical foundations for it - [Piterbarg 2010], [Burgard and Kjaer 2010] and [Burgard and Kjaer 2013]. The review reveals several conceptual and technical inconsistencies with the approaches taken in these papers. In particular, a key component of the approach - prescription of cost components to a risk-free money account, generates derivative prices that are not cleared by the markets that trade the derivative and its underlying securities. It also introduces several risk-free positions (accounts) that accrue at persistently non-zero spreads with respect to each other and the risk free rate. In the case of derivatives with counterparty default risk [Burgard and Kjaer 2013] introduces an approach referred to as semi-replication, which through the choice of cost components in the money account results in derivative prices that carry arbitrage opportunities in the form of holding portfolio of counterparty's bonds versus a derivative position with it. This paper derives no-arbitrage expressions for default-risky derivative contracts with and without collateral, avoiding these inconsistencies."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: According to the paper's review of post-crisis derivative pricing approaches, which of the following statements is NOT a criticism of the methods proposed by Piterbarg (2010) and Burgard and Kjaer (2010, 2013)?\n\nA) The prescription of cost components to a risk-free money account leads to derivative prices that are not cleared by the markets trading the derivative and its underlying securities.\n\nB) The approaches introduce several risk-free positions (accounts) that accrue at persistently non-zero spreads with respect to each other and the risk-free rate.\n\nC) In the case of derivatives with counterparty default risk, the semi-replication approach results in derivative prices that carry arbitrage opportunities.\n\nD) The methods fail to account for the impact of collateralization on derivative pricing in post-crisis markets.\n\nCorrect Answer: D\n\nExplanation: The question asks for the statement that is NOT a criticism mentioned in the paper. Options A, B, and C are all explicitly stated criticisms in the provided text. Option A refers to the issue of market clearing, option B mentions the problem of persistent non-zero spreads between risk-free accounts, and option C describes the arbitrage opportunities arising from the semi-replication approach.\n\nOption D, however, is not mentioned as a criticism in the given text. In fact, the paper discusses derivatives with and without collateral, indicating that collateralization is considered in these approaches. Therefore, D is the correct answer as it is not a criticism raised by the paper against the reviewed pricing methods."}, "20": {"documentation": {"title": "Hydrodynamic Attractor in a Hubble Expansion", "source": "Zhiwei Du, Xu-Guang Huang and Hidetoshi Taya", "docs_id": "2104.12534", "section": ["nucl-th", "gr-qc", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hydrodynamic Attractor in a Hubble Expansion. We analytically investigate hydrodynamic attractor solutions in both M\\\"{u}ller-Israel-Stewart (MIS) and kinetic theories in a viscous fluid system undergoing a Hubble expansion with a fixed expansion rate. We show that the gradient expansion for the MIS theory and the Chapman-Enskog expansion for the Boltzmann equation within the relaxation time approximation are factorially divergent and obtain hydrodynamic attractor solutions by applying the Borel resummation technique to those asymptotic divergent series. In both theories, we find that the hydrodynamic attractor solutions are globally attractive and only the first-order non-hydrodynamic mode exists. We also find that the hydrodynamic attractor solutions in the two theories disagree with each other when gradients become large, and that the speed of the attraction is different. Similarities and differences from hydrodynamic attractors in the Bjorken and Gubser flows are also discussed. Our results push the idea of far-from-equilibrium hydrodynamics in systems undergoing a Hubble expansion."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of hydrodynamic attractor solutions for a viscous fluid system undergoing Hubble expansion, which of the following statements is correct?\n\nA) The gradient expansion for the M\u00fcller-Israel-Stewart (MIS) theory converges, while the Chapman-Enskog expansion for the Boltzmann equation diverges.\n\nB) The hydrodynamic attractor solutions in MIS and kinetic theories agree perfectly even when gradients become large.\n\nC) The Borel resummation technique is applied to obtain hydrodynamic attractor solutions from the factorially divergent series.\n\nD) Multiple non-hydrodynamic modes exist in the attractor solutions for both MIS and kinetic theories.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that the Borel resummation technique is applied to obtain hydrodynamic attractor solutions from the factorially divergent series of both the gradient expansion for MIS theory and the Chapman-Enskog expansion for the Boltzmann equation.\n\nOption A is incorrect because both expansions are described as factorially divergent, not just the Chapman-Enskog expansion.\n\nOption B is false because the documentation explicitly mentions that the hydrodynamic attractor solutions in the two theories disagree with each other when gradients become large.\n\nOption D is incorrect because the documentation states that only the first-order non-hydrodynamic mode exists in both theories, not multiple modes."}, "21": {"documentation": {"title": "Line Failure Localization of Power Networks Part II: Cut Set Outages", "source": "Linqi Guo, Chen Liang, Alessandro Zocca, Steven H. Low, Adam Wierman", "docs_id": "2005.11320", "section": ["eess.SY", "cs.SY", "math.DS", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Line Failure Localization of Power Networks Part II: Cut Set Outages. Transmission line failure in power systems prop-agate non-locally, making the control of the resulting outages extremely difficult. In Part II of this paper, we continue the study of line failure localizability in transmission networks and characterize the impact of cut set outages. We establish a Simple Path Criterion, showing that the propagation pattern due to bridge outages, a special case of cut set failures, are fully determined by the positions in the network of the buses that participate in load balancing. We then extend our results to general cut set outages. In contrast to non-cut outages discussed in Part I whose subsequent line failures are contained within the original blocks, cut set outages typically impact the whole network, affecting the power flows on all remaining lines. We corroborate our analytical results in both parts using the IEEE 118-bus test system, in which the failure propagation patterns exhibit a clear block-diagonal structure predicted by our theory, even when using full AC power flow equations."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of line failure localization in power networks, which of the following statements best describes the characteristics of cut set outages compared to non-cut outages?\n\nA) Cut set outages have localized impacts, affecting only the immediate vicinity of the failed line.\nB) Cut set outages typically impact the whole network, affecting power flows on all remaining lines.\nC) Cut set outages exhibit the same block-diagonal structure as non-cut outages.\nD) Cut set outages are easier to control and mitigate than non-cut outages.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the documentation, cut set outages typically impact the whole network, affecting the power flows on all remaining lines. This is in contrast to non-cut outages discussed in Part I of the paper, where subsequent line failures are contained within the original blocks. \n\nAnswer A is incorrect because it describes a localized impact, which is not characteristic of cut set outages. \n\nAnswer C is incorrect because the block-diagonal structure is associated with non-cut outages, not cut set outages. \n\nAnswer D is incorrect because the documentation suggests that transmission line failures propagate non-locally, making the control of resulting outages extremely difficult, especially for cut set outages that affect the whole network."}, "22": {"documentation": {"title": "Influence of defects on the critical behaviour at the \\boldmath{105} K\n  structural phase transition of SrTiO$_3$: I. The broad component", "source": "H. H\\\"unnefeld, T. Niem\\\"oller, J. R. Schneider, B. A. Kaufmann and F.\n  Schwabl", "docs_id": "cond-mat/0006127", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Influence of defects on the critical behaviour at the \\boldmath{105} K\n  structural phase transition of SrTiO$_3$: I. The broad component. The critical fluctuations in SrTiO$_3$ near its $105 K$ structural phase transition were studied with triple axis diffractometry using high energy ($\\ge 100 keV$) synchrotron radiation in different SrTiO$_3$ crystals with different oxygen vacancy concentrations. Due to the presence of oxygen vacancies the critical behaviour is changed compared to defect-free systems. In our experiments a smearing out of the squared order parameter and a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature is observed, with the crossover temperature strongly depending on the concentration of the defects. To understand the experimental findings, e.g. the unusual values for the critical exponents found near the critical temperature, the Ginzburg-Landau-Wilson functional for structural phase transitions in disordered systems was analyzed using renormalization group theory and the replica trick. Considering the effects of defects which locally increase the transition temperature leads to a qualitative understanding of the observed behaviour. The crossover behaviour of the critical exponents can be modeled and a quantitative analysis of the observed experimental data is presented."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of SrTiO3's structural phase transition at 105 K, what effect did oxygen vacancies have on the critical behavior, and how was this explained theoretically?\n\nA) Oxygen vacancies sharpened the phase transition, explained by increased lattice rigidity. This was modeled using classical thermodynamics.\n\nB) Oxygen vacancies had no significant effect on the critical behavior. The system behaved identically to defect-free SrTiO3.\n\nC) Oxygen vacancies caused a smearing of the squared order parameter and a crossover of critical exponents \u03bd and \u03b3. This was explained using the Ginzburg-Landau-Wilson functional with renormalization group theory and the replica trick.\n\nD) Oxygen vacancies lowered the transition temperature, explained by decreased lattice stability. This was modeled using molecular dynamics simulations.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation clearly states that oxygen vacancies led to a \"smearing out of the squared order parameter and a crossover of the critical exponents \u03bd and \u03b3 above the phase transition temperature.\" To explain this behavior, the researchers used \"the Ginzburg-Landau-Wilson functional for structural phase transitions in disordered systems\" and analyzed it \"using renormalization group theory and the replica trick.\" This theoretical approach allowed them to model the crossover behavior of the critical exponents and provide a quantitative analysis of the experimental data.\n\nOptions A and D are incorrect as they propose effects (sharpening of transition, lowering of transition temperature) and explanations (increased lattice rigidity, molecular dynamics) that are not mentioned in the given text. Option B is also incorrect as it contradicts the observed changes in critical behavior due to oxygen vacancies."}, "23": {"documentation": {"title": "Cosmic Axions", "source": "E. P. S. Shellard and R. A. Battye (University of Cambridge)", "docs_id": "astro-ph/9802216", "section": ["astro-ph", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cosmic Axions. The current cosmological constraints on a dark matter axion are reviewed. We describe the basic mechanisms by which axions are created in the early universe, both in the standard thermal scenario in which axion strings form and in inflationary models. In the thermal scenario, the dominant process for axion production is through the radiative decay of an axion string network, which implies a dark matter axion of mass m_a ~ 100 microeV with specified large uncertainties. An inflationary phase does not affect this string bound if the reheat temperature is high T_reh > f_a or, conversely, for T_reh < f_a, if the Hubble parameter during inflation is large H_1 > f_a; in both cases, strings form and we return to the standard picture with a m_a ~ 100 microeV dark matter axion. Inflationary models with f_a > H_1 > T_reh face strong CMBR constraints and require `anthropic misalignment' fine-tuning in order to produce a dark matter axion; in this case, some inflation models are essentially incompatible with a detectable axion, while others can be engineered to allow a dark matter axion anywhere in a huge mass range below m_a < 1 meV. We endeavour to clarify the sometimes confusing and contradictory literature on axion cosmology."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the standard thermal scenario for axion production in the early universe, what is the primary mechanism for axion creation and what does it imply about the mass of dark matter axions?\n\nA) Axion production through Primordial Black Hole evaporation, implying a dark matter axion mass of ~1 meV\nB) Radiative decay of an axion string network, implying a dark matter axion mass of ~100 \u03bceV\nC) Thermal production in the quark-gluon plasma, implying a dark matter axion mass of ~10 eV\nD) Misalignment mechanism during the QCD phase transition, implying a dark matter axion mass of ~1 \u03bceV\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the standard thermal scenario for axion production in the early universe. According to the provided information, in this scenario, \"the dominant process for axion production is through the radiative decay of an axion string network, which implies a dark matter axion of mass m_a ~ 100 microeV with specified large uncertainties.\" This directly corresponds to option B.\n\nOption A is incorrect as it mentions Primordial Black Hole evaporation, which is not discussed in the given text as a primary mechanism for axion production.\n\nOption C is incorrect because while thermal production in the quark-gluon plasma can contribute to axion production, it is not described as the dominant process in the standard thermal scenario according to the given information.\n\nOption D is incorrect as it refers to the misalignment mechanism, which is not mentioned as the primary production method in the standard thermal scenario. Additionally, the mass it implies (1 \u03bceV) is significantly lower than the ~100 \u03bceV mentioned in the text.\n\nThis question challenges students to identify the correct mechanism and its implications for axion mass in the context of the standard thermal scenario, distinguishing it from other potential production mechanisms."}, "24": {"documentation": {"title": "Filtering the intensity of public concern from social media count data\n  with jumps", "source": "Matteo Iacopini and Carlo R.M.A. Santagiustina", "docs_id": "2012.13267", "section": ["stat.AP", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Filtering the intensity of public concern from social media count data\n  with jumps. Count time series obtained from online social media data, such as Twitter, have drawn increasing interest among academics and market analysts over the past decade. Transforming Web activity records into counts yields time series with peculiar features, including the coexistence of smooth paths and sudden jumps, as well as cross-sectional and temporal dependence. Using Twitter posts about country risks for the United Kingdom and the United States, this paper proposes an innovative state space model for multivariate count data with jumps. We use the proposed model to assess the impact of public concerns in these countries on market systems. To do so, public concerns inferred from Twitter data are unpacked into country-specific persistent terms, risk social amplification events, and co-movements of the country series. The identified components are then used to investigate the existence and magnitude of country-risk spillovers and social amplification effects on the volatility of financial markets."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the innovative approach and primary goal of the research described in the Arxiv documentation?\n\nA) The study uses a traditional time series analysis to examine the impact of Twitter posts on stock market prices, focusing solely on smooth data patterns.\n\nB) The research proposes a new state space model for univariate count data without jumps, aiming to predict future social media trends in the UK and US.\n\nC) The paper introduces a novel state space model for multivariate count data with jumps, designed to assess the impact of public concerns on market systems by analyzing Twitter data about country risks.\n\nD) The study employs sentiment analysis on Twitter data to directly measure the emotional state of users in the UK and US, without considering its impact on financial markets.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key elements of the research described in the documentation. The study proposes an \"innovative state space model for multivariate count data with jumps\" and uses this model to analyze Twitter posts about country risks for the UK and US. The primary goal is to assess the impact of public concerns on market systems, which involves unpacking the data into persistent terms, risk social amplification events, and co-movements of country series. This approach allows for the investigation of country-risk spillovers and social amplification effects on financial market volatility.\n\nOption A is incorrect because it mentions traditional time series analysis and focuses only on smooth patterns, whereas the described approach specifically addresses jumps in the data.\n\nOption B is wrong because it refers to univariate data without jumps and focuses on predicting social media trends, which is not the main goal of the research.\n\nOption D is incorrect as it describes a simpler sentiment analysis approach and doesn't capture the complexity of the proposed model or its application to market systems."}, "25": {"documentation": {"title": "Synchronization in a semiclassical Kuramoto model", "source": "Ignacio Hermoso de Mendoza, Leonardo A. Pach\\'on, Jes\\'us\n  G\\'omez-Garde\\~nes, David Zueco", "docs_id": "1309.3972", "section": ["cond-mat.stat-mech", "nlin.AO", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronization in a semiclassical Kuramoto model. Synchronization is a ubiquitous phenomenon occurring in social, biological, and technological systems when the internal rhythms of their constituents are adapted to be in unison as a result of their coupling. This natural tendency towards dynamical consensus has spurred a large body of theoretical and experimental research in recent decades. The Kuramoto model constitutes the most studied and paradigmatic framework in which to study synchronization. In particular, it shows how synchronization appears as a phase transition from a dynamically disordered state at some critical value for the coupling strength between the interacting units. The critical properties of the synchronization transition of this model have been widely studied and many variants of its formulations have been considered to address different physical realizations. However, the Kuramoto model has been studied only within the domain of classical dynamics, thus neglecting its applications for the study of quantum synchronization phenomena. Based on a system-bath approach and within the Feynman path-integral formalism, we derive equations for the Kuramoto model by taking into account the first quantum fluctuations. We also analyze its critical properties, the main result being the derivation of the value for the synchronization onset. This critical coupling increases its value as quantumness increases, as a consequence of the possibility of tunneling that quantum fluctuations provide."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the semiclassical Kuramoto model described in the text, which of the following statements is correct regarding the effect of quantum fluctuations on synchronization?\n\nA) Quantum fluctuations decrease the critical coupling value for synchronization onset.\nB) The introduction of quantum effects eliminates the possibility of a synchronization phase transition.\nC) Quantum fluctuations increase the critical coupling value required for synchronization onset.\nD) The semiclassical Kuramoto model shows that quantum effects have no impact on the synchronization threshold.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states, \"This critical coupling increases its value as quantumness increases, as a consequence of the possibility of tunneling that quantum fluctuations provide.\" This directly indicates that the introduction of quantum fluctuations leads to an increase in the critical coupling value required for the onset of synchronization.\n\nAnswer A is incorrect because it contradicts the information provided in the text. The critical coupling value increases, not decreases, with quantum effects.\n\nAnswer B is incorrect because the text does not suggest that quantum effects eliminate the synchronization phase transition. Instead, it implies that the transition still occurs, but at a higher critical coupling value.\n\nAnswer D is incorrect because the passage clearly indicates that quantum effects do have an impact on the synchronization threshold, contrary to this statement.\n\nThis question tests the student's ability to carefully read and interpret complex scientific information, particularly focusing on the novel aspects introduced by considering quantum effects in the Kuramoto model."}, "26": {"documentation": {"title": "Single temperature for Monte Carlo optimization on complex landscapes", "source": "Denis Tolkunov, Alexandre V. Morozov", "docs_id": "1202.0340", "section": ["physics.comp-ph", "cond-mat.stat-mech", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single temperature for Monte Carlo optimization on complex landscapes. We propose a new strategy for Monte Carlo (MC) optimization on rugged multidimensional landscapes. The strategy is based on querying the statistical properties of the landscape in order to find the temperature at which the mean first passage time across the current region of the landscape is minimized. Thus, in contrast to other algorithms such as simulated annealing (SA), we explicitly match the temperature schedule to the statistics of landscape irregularities. In cases where this statistics is approximately the same over the entire landscape, or where non-local moves couple distant parts of the landscape, single-temperature MC will outperform any other MC algorithm with the same move set. We also find that in strongly anisotropic Coulomb spin glass and traveling salesman problems, the only relevant statistics (which we use to assign a single MC temperature) is that of irregularities in low-energy funnels. Our results may explain why protein folding in nature is efficient at room temperatures."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the proposed Monte Carlo optimization strategy for complex landscapes, what is the key difference from traditional simulated annealing (SA), and what potential advantage does this offer?\n\nA) It uses multiple temperatures simultaneously, allowing for faster convergence in multi-dimensional spaces.\nB) It queries the statistical properties of the landscape to find a single optimal temperature, potentially outperforming other MC algorithms with the same move set.\nC) It focuses solely on high-energy regions of the landscape, avoiding local minima traps more effectively.\nD) It dynamically adjusts the temperature based on the current solution quality, leading to more efficient exploration of the solution space.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The proposed strategy differs from simulated annealing by querying the statistical properties of the landscape to find a single temperature at which the mean first passage time across the current region is minimized. This approach explicitly matches the temperature to the statistics of landscape irregularities, unlike SA which typically uses a predefined cooling schedule.\n\nThis strategy can potentially outperform other Monte Carlo algorithms (including SA) that use the same move set, especially in cases where the landscape statistics are approximately uniform or when non-local moves couple distant parts of the landscape. The single-temperature approach is particularly effective in strongly anisotropic problems like Coulomb spin glass and traveling salesman problems, where the relevant statistics are those of irregularities in low-energy funnels.\n\nOption A is incorrect because the strategy uses a single optimal temperature, not multiple temperatures. Option C is incorrect as the method doesn't focus solely on high-energy regions but considers the overall landscape statistics. Option D describes a feature more typical of adaptive simulated annealing, not the proposed method which aims to find a single optimal temperature."}, "27": {"documentation": {"title": "Mining Urban Performance: Scale-Independent Classification of Cities\n  Based on Individual Economic Transactions", "source": "Stanislav Sobolevsky, Izabela Sitko, Sebastian Grauwin, Remi Tachet\n  des Combes, Bartosz Hawelka, Juan Murillo Arias, Carlo Ratti", "docs_id": "1405.4301", "section": ["physics.soc-ph", "cs.SI", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mining Urban Performance: Scale-Independent Classification of Cities\n  Based on Individual Economic Transactions. Intensive development of urban systems creates a number of challenges for urban planners and policy makers in order to maintain sustainable growth. Running efficient urban policies requires meaningful urban metrics, which could quantify important urban characteristics including various aspects of an actual human behavior. Since a city size is known to have a major, yet often nonlinear, impact on the human activity, it also becomes important to develop scale-free metrics that capture qualitative city properties, beyond the effects of scale. Recent availability of extensive datasets created by human activity involving digital technologies creates new opportunities in this area. In this paper we propose a novel approach of city scoring and classification based on quantitative scale-free metrics related to economic activity of city residents, as well as domestic and foreign visitors. It is demonstrated on the example of Spain, but the proposed methodology is of a general character. We employ a new source of large-scale ubiquitous data, which consists of anonymized countrywide records of bank card transactions collected by one of the largest Spanish banks. Different aspects of the classification reveal important properties of Spanish cities, which significantly complement the pattern that might be discovered with the official socioeconomic statistics."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the novel approach to city scoring and classification proposed in this research?\n\nA) It relies solely on official socioeconomic statistics to develop scale-free metrics for urban areas.\n\nB) It uses anonymized bank card transaction data to create quantitative scale-free metrics related to economic activity of city residents and visitors.\n\nC) It focuses exclusively on the impact of city size on human activity, ignoring other qualitative city properties.\n\nD) It develops urban metrics that are intentionally scale-dependent to account for the nonlinear impact of city size.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage states that the researchers \"propose a novel approach of city scoring and classification based on quantitative scale-free metrics related to economic activity of city residents, as well as domestic and foreign visitors.\" It also mentions that they \"employ a new source of large-scale ubiquitous data, which consists of anonymized countrywide records of bank card transactions collected by one of the largest Spanish banks.\"\n\nOption A is incorrect because the approach uses bank transaction data, not official socioeconomic statistics. The text actually states that this approach \"significantly complement(s) the pattern that might be discovered with the official socioeconomic statistics.\"\n\nOption C is incorrect because while city size is mentioned as an important factor, the approach aims to \"capture qualitative city properties, beyond the effects of scale,\" not focus exclusively on city size.\n\nOption D is incorrect because the metrics developed are described as \"scale-free,\" not scale-dependent."}, "28": {"documentation": {"title": "Principal component analysis for estimating parameters of the L1287\n  dense core by fitting model spectral maps into observed ones", "source": "L. E. Pirogov and P. M. Zemlyanukha", "docs_id": "2101.08219", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Principal component analysis for estimating parameters of the L1287\n  dense core by fitting model spectral maps into observed ones. An algorithm has been developed for finding the global minimum of a multidimensional error function by fitting model spectral maps into observed ones. Principal component analysis is applied to reduce the dimensionality of the model and the coupling degree between the parameters, and to determine the region of the minimum. The k-nearest neighbors method is used to calculate the optimal parameter values. The algorithm is used to estimate the physical parameters of the contracting dense star-forming core of L1287. Maps in the HCO+(1-0), H13CO+(1-0), HCN(1-0), and H13CN(1-0) lines, calculated within a 1D microturbulent model, are fitted into the observed ones. Estimates are obtained for the physical parameters of the core, including the radial profiles of density ($\\propto r^{-1.7}$), turbulent velocity ($\\propto r^{-0.4}$), and contraction velocity ($\\propto r^{-0.1}$). Confidence intervals are calculated for the parameter values. The power-law index of the contraction-velocity radial profile, considering the determination error, is lower in absolute terms than the expected one in the case of gas collapse onto the protostar in free fall. This result can serve as an argument in favor of a global contraction model for the L1287 core."}, "answer": "Question: Which of the following statements best describes the methodology and findings of the study on the L1287 dense core, as presented in the Arxiv documentation?\n\nA) The study used principal component analysis to increase the dimensionality of the model and strengthen the coupling between parameters, resulting in a density profile proportional to r^-2.\n\nB) The algorithm developed employs the k-nearest neighbors method to determine the region of the minimum, while principal component analysis is used to calculate the optimal parameter values.\n\nC) The study found that the power-law index of the contraction-velocity radial profile is higher than expected for free-fall collapse onto a protostar, supporting a global contraction model for L1287.\n\nD) Principal component analysis was used to reduce dimensionality and parameter coupling, while k-nearest neighbors method calculated optimal values, resulting in density, turbulent velocity, and contraction velocity profiles of r^-1.7, r^-0.4, and r^-0.1 respectively.\n\nCorrect Answer: D\n\nExplanation: Option D accurately summarizes the key aspects of the methodology and findings presented in the documentation. The study used principal component analysis to reduce dimensionality and parameter coupling, while the k-nearest neighbors method was employed to calculate optimal parameter values. The results include the radial profiles for density (\u221d r^-1.7), turbulent velocity (\u221d r^-0.4), and contraction velocity (\u221d r^-0.1).\n\nOption A is incorrect as it misrepresents the use of principal component analysis and provides an incorrect density profile.\n\nOption B incorrectly switches the roles of principal component analysis and k-nearest neighbors method.\n\nOption C is incorrect because the study actually found that the power-law index of the contraction-velocity profile was lower in absolute terms than expected for free-fall collapse, not higher."}, "29": {"documentation": {"title": "Sequences of Exact Analytical Solutions for Plane-Waves in Graded Media", "source": "Jean-Claude Krapez", "docs_id": "1704.08929", "section": ["physics.optics", "physics.app-ph", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sequences of Exact Analytical Solutions for Plane-Waves in Graded Media. We present a new method for building sequences of solvable profiles of the electromagnetic (EM) admittance in lossless isotropic materials with 1D graded permittivity and permeability (in particular profiles of the optical refractive-index). These solvable profiles lead to analytical closed-form expressions of the EM fields, for both TE and TM modes. The Property-and-Field Darboux Transformations method, initially developed for heat diffusion modelling, is here transposed to the Maxwell equations in the optical-depth space. Several examples are provided, all stemming from a constant seed-potential, which makes them based on elementary functions only. Solvable profiles of increasingly complex shape can be obtained by iterating the process or by assembling highly flexible canonical profiles. Their implementation for modelling optical devices like matching layers, rugate filters, Bragg gratings, chirped mirrors or 1D photonic crystals, offers an exact and cost-effective alternative to the classical approaches"}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the key innovation and potential application of the method presented in the paper?\n\nA) It uses the Property-and-Field Darboux Transformations to solve heat diffusion problems in optical materials.\n\nB) It provides a way to generate sequences of solvable profiles for EM admittance in 3D graded media, applicable to anisotropic materials.\n\nC) It offers an analytical method for solving Maxwell's equations in the time domain for lossy materials.\n\nD) It presents a technique for creating solvable profiles of EM admittance in 1D graded media, potentially useful for modeling various optical devices.\n\nCorrect Answer: D\n\nExplanation: \nOption D is correct because it accurately summarizes the key innovation and potential application of the method described in the paper. The method presents a way to build sequences of solvable profiles of electromagnetic admittance in lossless isotropic materials with 1D graded permittivity and permeability. These profiles lead to analytical closed-form expressions of EM fields, which can be used to model optical devices like matching layers, rugate filters, Bragg gratings, chirped mirrors, or 1D photonic crystals.\n\nOption A is incorrect because while the method does use Property-and-Field Darboux Transformations, it applies them to Maxwell's equations in the optical-depth space, not to heat diffusion problems.\n\nOption B is incorrect as the method is specifically for 1D graded media, not 3D, and it focuses on isotropic materials, not anisotropic ones.\n\nOption C is incorrect because the method deals with lossless materials and works in the optical-depth space, not the time domain."}, "30": {"documentation": {"title": "High Performance Financial Simulation Using Randomized Quasi-Monte Carlo\n  Methods", "source": "Linlin Xu and Giray \\\"Okten", "docs_id": "1408.5526", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High Performance Financial Simulation Using Randomized Quasi-Monte Carlo\n  Methods. GPU computing has become popular in computational finance and many financial institutions are moving their CPU based applications to the GPU platform. Since most Monte Carlo algorithms are embarrassingly parallel, they benefit greatly from parallel implementations, and consequently Monte Carlo has become a focal point in GPU computing. GPU speed-up examples reported in the literature often involve Monte Carlo algorithms, and there are software tools commercially available that help migrate Monte Carlo financial pricing models to GPU. We present a survey of Monte Carlo and randomized quasi-Monte Carlo methods, and discuss existing (quasi) Monte Carlo sequences in GPU libraries. We discuss specific features of GPU architecture relevant for developing efficient (quasi) Monte Carlo methods. We introduce a recent randomized quasi-Monte Carlo method, and compare it with some of the existing implementations on GPU, when they are used in pricing caplets in the LIBOR market model and mortgage backed securities."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of GPU computing for financial simulations, which of the following statements is most accurate regarding the implementation of Monte Carlo and quasi-Monte Carlo methods?\n\nA) GPU implementations of Monte Carlo methods always outperform CPU implementations, regardless of the specific financial model being used.\n\nB) Randomized quasi-Monte Carlo methods are inherently unsuitable for GPU implementation due to their sequential nature.\n\nC) The embarrassingly parallel nature of most Monte Carlo algorithms makes them particularly well-suited for GPU implementation, but the efficiency of quasi-Monte Carlo methods on GPUs depends on specific architectural considerations.\n\nD) GPU libraries universally contain optimized implementations of all known (quasi) Monte Carlo sequences, eliminating the need for custom implementations in financial simulations.\n\nCorrect Answer: C\n\nExplanation: Option C is the most accurate statement based on the given information. The document mentions that Monte Carlo algorithms are \"embarrassingly parallel,\" which makes them ideal for GPU implementation. However, it also discusses \"specific features of GPU architecture relevant for developing efficient (quasi) Monte Carlo methods,\" implying that the implementation of quasi-Monte Carlo methods on GPUs requires careful consideration of the GPU's architecture for optimal performance.\n\nOption A is incorrect because while GPU implementations often provide significant speedups, the statement is too absolute and doesn't account for variations in different financial models.\n\nOption B is incorrect because the document actually discusses implementing randomized quasi-Monte Carlo methods on GPUs, indicating they are not inherently unsuitable for GPU implementation.\n\nOption D is overly broad and incorrect. While the document mentions existing (quasi) Monte Carlo sequences in GPU libraries, it doesn't suggest that these libraries contain all known sequences or eliminate the need for custom implementations."}, "31": {"documentation": {"title": "New parton distributions in fixed flavour factorization scheme from\n  recent deep-inelastic-scattering data", "source": "H. Khanpour, Ali N. Khorramian, S. Atashbar Tehrani", "docs_id": "1205.5194", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New parton distributions in fixed flavour factorization scheme from\n  recent deep-inelastic-scattering data. We present our QCD analysis of the proton structure function $F_2^p(x,Q^2)$ to determine the parton distributions at the next-to-leading order (NLO). The heavy quark contributions to $F_2^i(x,Q^2)$, with $i$ = $c$, $b$ have been included in the framework of the `fixed flavour number scheme' (FFNS). The results obtained in the FFNS are compared with available results such as the general-mass variable-flavour-number scheme (GM-VFNS) and other prescriptions used in global fits of PDFs. In the present QCD analysis, we use a wide range of the inclusive neutral-current deep-inelastic-scattering (NC DIS) data, including the most recent data for charm $F_2^c$, bottom $F_2^b$, longitudinal $F_L$ structure functions and also the reduced DIS cross sections $\\sigma_{r,NC}^\\pm$ from HERA experiments. The most recent HERMES data for proton and deuteron structure functions are also added. We take into account ZEUS neutral current $e^ \\pm p$ DIS inclusive jet cross section data from HERA together with the recent Tevatron Run-II inclusive jet cross section data from CDF and D{\\O}. The impact of these recent DIS data on the PDFs extracted from the global fits are studied. We present two families of PDFs, {\\tt KKT12} and {\\tt KKT12C}, without and with HERA `combined' data sets on $e^{\\pm}p$ DIS. We find these are in good agreement with the available theoretical models."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the QCD analysis described, which of the following statements is NOT true regarding the data used and the methodology employed?\n\nA) The analysis incorporates HERA data on charm and bottom structure functions, as well as longitudinal structure functions.\n\nB) The study compares results obtained in the Fixed Flavour Number Scheme (FFNS) with those from the General-Mass Variable-Flavour-Number Scheme (GM-VFNS).\n\nC) The analysis exclusively uses deep-inelastic-scattering (DIS) data, omitting any hadron collider data to maintain consistency.\n\nD) Two sets of parton distribution functions (PDFs) are presented: KKT12 and KKT12C, differing in their inclusion of HERA 'combined' data sets.\n\nCorrect Answer: C\n\nExplanation: Option C is incorrect and thus the correct answer to the question asking which statement is NOT true. The text explicitly mentions that the analysis includes \"Tevatron Run-II inclusive jet cross section data from CDF and D0,\" which are hadron collider data. This contradicts the statement in option C that only DIS data were used.\n\nOptions A, B, and D are all true according to the given text:\nA) The text mentions using data for charm F2c, bottom F2b, and longitudinal FL structure functions from HERA.\nB) The passage states that results obtained in the FFNS are compared with those from GM-VFNS.\nD) The text explicitly mentions presenting two families of PDFs: KKT12 and KKT12C, with the latter including HERA 'combined' data sets."}, "32": {"documentation": {"title": "Symmetry energy systematics and its high density behavior", "source": "Lie-Wen Chen", "docs_id": "1506.09057", "section": ["nucl-th", "astro-ph.HE", "astro-ph.SR", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetry energy systematics and its high density behavior. We explore the systematics of the density dependence of nuclear matter symmetry energy in the ambit of microscopic calculations with various energy density functionals, and find that the symmetry energy from subsaturation density to supra-saturation density can be well determined by three characteristic parameters of the symmetry energy at saturation density $\\rho_0 $, i.e., the magnitude $E_{\\text{sym}}({\\rho_0 })$, the density slope $L$ and the density curvature $K_{\\text{sym}}$. This finding opens a new window to constrain the supra-saturation density behavior of the symmetry energy from its (sub-)saturation density behavior. In particular, we obtain $L=46.7 \\pm 12.8$ MeV and $K_{\\text{sym}}=-166.9 \\pm 168.3$ MeV as well as $E_{\\text{sym}}({2\\rho _{0}}) \\approx 40.2 \\pm 12.8$ MeV and $L({2\\rho _{0}}) \\approx 8.9 \\pm 108.7$ MeV based on the present knowledge of $E_{\\text{sym}}({\\rho_{0}}) = 32.5 \\pm 0.5$ MeV, $E_{\\text{sym}}({\\rho_c}) = 26.65 \\pm 0.2$ MeV and $L({\\rho_c}) = 46.0 \\pm 4.5$ MeV at $\\rho_{\\rm{c}}= 0.11$ fm$^{-3}$ extracted from nuclear mass and the neutron skin thickness of Sn isotopes. Our results indicate that the symmetry energy cannot be stiffer than a linear density dependence.In addition, we also discuss the quark matter symmetry energy since the deconfined quarks could be the right degree of freedom in dense matter at high baryon densities."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Based on the systematics of nuclear matter symmetry energy explored in this study, which of the following statements is correct regarding the symmetry energy at twice the saturation density (2\u03c1\u2080)?\n\nA) E_sym(2\u03c1\u2080) \u2248 40.2 \u00b1 12.8 MeV and L(2\u03c1\u2080) \u2248 8.9 \u00b1 108.7 MeV\nB) E_sym(2\u03c1\u2080) \u2248 65.0 \u00b1 1.0 MeV and L(2\u03c1\u2080) \u2248 93.4 \u00b1 20.5 MeV\nC) E_sym(2\u03c1\u2080) \u2248 32.5 \u00b1 0.5 MeV and L(2\u03c1\u2080) \u2248 46.7 \u00b1 12.8 MeV\nD) E_sym(2\u03c1\u2080) \u2248 26.65 \u00b1 0.2 MeV and L(2\u03c1\u2080) \u2248 46.0 \u00b1 4.5 MeV\n\nCorrect Answer: A\n\nExplanation: The correct answer is A, as stated directly in the text: \"we obtain ... E_sym(2\u03c1\u2080) \u2248 40.2 \u00b1 12.8 MeV and L(2\u03c1\u2080) \u2248 8.9 \u00b1 108.7 MeV\". \n\nOption B presents values that are not mentioned in the text and are significantly different from the correct values. \n\nOption C incorrectly uses the values for E_sym(\u03c1\u2080) and L at saturation density, not at twice the saturation density. \n\nOption D incorrectly uses the values for E_sym(\u03c1c) and L(\u03c1c) at \u03c1c = 0.11 fm\u207b\u00b3, which is below saturation density, not at twice the saturation density.\n\nThis question tests the student's ability to carefully read and extract specific numerical results from a dense scientific text, as well as to distinguish between values at different densities."}, "33": {"documentation": {"title": "General Compound Hawkes Processes for Mid-Price Prediction", "source": "Myles Sjogren (1) and Timothy DeLise (2) ((1) University of Calgary,\n  (2) Universit\\'e de Montr\\'eal)", "docs_id": "2110.07075", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "General Compound Hawkes Processes for Mid-Price Prediction. High frequency financial data is burdened by a level of randomness that is unavoidable and obfuscates the task of modelling. This idea is reflected in the intraday evolution of limit orders book data for many financial assets and suggests several justifications for the use of stochastic models. For instance, the arbitrary distribution of inter arrival times and the subsequent dependence structure between consecutive book events. This has lead to the development of many stochastic models for the dynamics of limit order books. In this paper we look to examine the adaptability of one family of such models, the General Compound Hawkes Process (GCHP) models, to new data and new tasks. We further focus on the prediction problem for the mid-price within a limit order book and the practical applications of these stochastic models, which is the main contribution of this paper. To this end we examine the use of the GCHP for predicting the direction and volatility of futures and stock data and discuss possible extensions of the model to help improve its predictive capabilities."}, "answer": "Question: Which of the following statements best describes the main focus and contribution of the paper discussed in the given text?\n\nA) Developing new stochastic models for limit order book dynamics\nB) Comparing different families of Hawkes Process models\nC) Examining the use of General Compound Hawkes Process (GCHP) models for mid-price prediction in limit order books\nD) Analyzing the arbitrary distribution of inter-arrival times in high-frequency financial data\n\nCorrect Answer: C\n\nExplanation: The main focus and contribution of the paper, as described in the text, is examining the use of General Compound Hawkes Process (GCHP) models for predicting the mid-price within a limit order book. This is explicitly stated in the passage: \"We further focus on the prediction problem for the mid-price within a limit order book and the practical applications of these stochastic models, which is the main contribution of this paper.\" The text also mentions that the paper examines \"the use of the GCHP for predicting the direction and volatility of futures and stock data.\"\n\nOption A is incorrect because while the text mentions the development of stochastic models, it's not the main focus of this particular paper. Option B is not supported by the text, as it doesn't mention comparing different families of Hawkes Process models. Option D, while mentioned in the text, is more of a justification for using stochastic models rather than the main focus of the paper."}, "34": {"documentation": {"title": "Compressed Sensing of Scanning Transmission Electron Microscopy (STEM)\n  on Non-Rectangular Scans", "source": "Xin Li, Ondrej Dyck, Sergei V. Kalinin, Stephen Jesse", "docs_id": "1805.04957", "section": ["physics.ins-det", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compressed Sensing of Scanning Transmission Electron Microscopy (STEM)\n  on Non-Rectangular Scans. Scanning Transmission Electron Microscopy (STEM) has become the main stay for materials characterization on atomic level, with applications ranging from visualization of localized and extended defects to mapping order parameter fields. In the last several years, attention was attracted by potential of STEM to explore beam induced chemical processes and especially manipulating atomic motion, enabling atom-by-atom fabrication. These applications, as well as traditional imaging of beam sensitive materials, necessitate increasing dynamic range of STEM between imaging and manipulation modes, and increasing absolute scanning/imaging speeds, that can be achieved by combining sparse sensing methods with non-rectangular scanning trajectories. Here we developed a general method for real-time reconstruction of sparsely sampled images from high-speed, non-invasive and diverse scanning pathways. This approach is demonstrated on both the synthetic data where ground truth is known and the experimental STEM data. This work lays the foundation for future tasks such as optimal design of dose efficient scanning strategies and real-time adaptive inference and control of e-beam induced atomic fabrication."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary motivation for developing compressed sensing techniques with non-rectangular scans in Scanning Transmission Electron Microscopy (STEM)?\n\nA) To improve the resolution of atomic-level imaging in traditional STEM applications\nB) To reduce the cost of STEM equipment and make it more accessible to researchers\nC) To increase the dynamic range between imaging and manipulation modes while improving scanning speeds\nD) To eliminate the need for sample preparation in STEM experiments\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that the goal is to increase the dynamic range between imaging and manipulation modes in STEM, while also increasing absolute scanning/imaging speeds. This is achieved by combining sparse sensing methods with non-rectangular scanning trajectories.\n\nAnswer A is incorrect because, while STEM is used for atomic-level imaging, the primary motivation described here is not about improving resolution but rather about enhancing the versatility and speed of the technique.\n\nAnswer B is not mentioned in the given text and is not the primary motivation for this development.\n\nAnswer D is also not mentioned and is not related to the described advances in STEM techniques.\n\nThe key point is that this development aims to make STEM more versatile, allowing it to switch between gentle imaging of beam-sensitive materials and more intense beam manipulation for tasks like atom-by-atom fabrication, all while increasing scanning speeds."}, "35": {"documentation": {"title": "Daily Deals: Prediction, Social Diffusion, and Reputational\n  Ramifications", "source": "John W. Byers, Michael Mitzenmacher, Georgios Zervas", "docs_id": "1109.1530", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Daily Deals: Prediction, Social Diffusion, and Reputational\n  Ramifications. Daily deal sites have become the latest Internet sensation, providing discounted offers to customers for restaurants, ticketed events, services, and other items. We begin by undertaking a study of the economics of daily deals on the web, based on a dataset we compiled by monitoring Groupon and LivingSocial sales in 20 large cities over several months. We use this dataset to characterize deal purchases; glean insights about operational strategies of these firms; and evaluate customers' sensitivity to factors such as price, deal scheduling, and limited inventory. We then marry our daily deals dataset with additional datasets we compiled from Facebook and Yelp users to study the interplay between social networks and daily deal sites. First, by studying user activity on Facebook while a deal is running, we provide evidence that daily deal sites benefit from significant word-of-mouth effects during sales events, consistent with results predicted by cascade models. Second, we consider the effects of daily deals on the longer-term reputation of merchants, based on their Yelp reviews before and after they run a daily deal. Our analysis shows that while the number of reviews increases significantly due to daily deals, average rating scores from reviewers who mention daily deals are 10% lower than scores of their peers on average."}, "answer": "Question: Based on the study of daily deal sites described in the documentation, which of the following statements is supported by the research findings?\n\nA) Daily deal sites have no significant impact on the number of reviews a merchant receives on Yelp.\n\nB) The average rating scores from reviewers who mention daily deals are generally higher than those who don't.\n\nC) Daily deal sites benefit from word-of-mouth effects during sales events, as evidenced by user activity on Facebook.\n\nD) Customers show little sensitivity to factors such as price, deal scheduling, and limited inventory when purchasing daily deals.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that by studying user activity on Facebook while a deal is running, the researchers found evidence that daily deal sites benefit from significant word-of-mouth effects during sales events. This is consistent with results predicted by cascade models.\n\nOption A is incorrect because the research shows that the number of reviews increases significantly due to daily deals.\n\nOption B is incorrect because the study found that average rating scores from reviewers who mention daily deals are actually 10% lower than scores of their peers on average, not higher.\n\nOption D is incorrect as the documentation mentions that the researchers evaluated customers' sensitivity to factors such as price, deal scheduling, and limited inventory, implying that these factors do influence customer behavior."}, "36": {"documentation": {"title": "Classical Topology and Quantum States", "source": "A.P. Balachandran", "docs_id": "quant-ph/0002055", "section": ["quant-ph", "gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Classical Topology and Quantum States. Any two infinite-dimensional (separable) Hilbert spaces are unitarily isomorphic. The sets of all their self-adjoint operators are also therefore unitarily equivalent. Thus if all self-adjoint operators can be observed, and if there is no further major axiom in quantum physics than those formulated for example in Dirac's `Quantum Mechanics', then a quantum physicist would not be able to tell a torus from a hole in the ground. We argue that there are indeed such axioms involving observables with smooth time evolution: they contain commutative subalgebras from which the spatial slice of spacetime with its topology (and with further refinements of the axiom, its $C^K-$ and $C^\\infty-$ structures) can be reconstructed using Gel'fand - Naimark theory and its extensions. Classical topology is an attribute of only certain quantum observables for these axioms, the spatial slice emergent from quantum physics getting progressively less differentiable with increasingly higher excitations of energy and eventually altogether ceasing to exist. After formulating these axioms, we apply them to show the possibility of topology change and to discuss quantized fuzzy topologies. Fundamental issues concerning the role of time in quantum physics are also addressed."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In quantum physics, what additional axiom is proposed to allow differentiation between spatial topologies (such as distinguishing a torus from a hole), and what implications does this have for the nature of spacetime at high energies?\n\nA) An axiom involving non-commutative algebras, implying that spacetime becomes more precisely defined at higher energies.\n\nB) An axiom involving observables with smooth time evolution and commutative subalgebras, suggesting that spacetime becomes less differentiable and may cease to exist at higher energies.\n\nC) An axiom based on the Gelfand-Naimark theory, indicating that spacetime topology is invariant across all energy levels.\n\nD) An axiom extending Dirac's formulation of quantum mechanics, implying that spatial topology is always well-defined regardless of energy.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation suggests that additional axioms involving observables with smooth time evolution are needed to distinguish between spatial topologies. These axioms contain commutative subalgebras from which the spatial slice of spacetime and its topology can be reconstructed using Gelfand-Naimark theory. Importantly, this approach implies that the spatial slice becomes progressively less differentiable with higher energy excitations and may eventually cease to exist altogether. This contrasts with the other options which either suggest the opposite (A), imply invariance (C), or maintain that spatial topology is always well-defined (D)."}, "37": {"documentation": {"title": "Domain Shift in Computer Vision models for MRI data analysis: An\n  Overview", "source": "Ekaterina Kondrateva, Marina Pominova, Elena Popova, Maxim Sharaev,\n  Alexander Bernstein, Evgeny Burnaev", "docs_id": "2010.07222", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Domain Shift in Computer Vision models for MRI data analysis: An\n  Overview. Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yetonly a few applications are now in clinical use and one of the reasons for that is poor transferability of themodels to data from different sources or acquisition domains. Development of new methods and algorithms forthe transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for thedevelopment of accurate models and their use in clinics. In present work, we overview methods used to tackle thedomain shift problem in machine learning and computer vision. The algorithms discussed in this survey includeadvanced data processing, model architecture enhancing and featured training, as well as predicting in domaininvariant latent space. The application of the autoencoding neural networks and their domain-invariant variationsare heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging(MRI) data analysis and conclude on their performance as well as propose directions for further research."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following best describes the primary challenge in applying computer vision models to MRI data analysis across different medical facilities, as highlighted in the passage?\n\nA) Lack of advanced data processing techniques\nB) Insufficient computational power for model training\nC) Poor transferability of models across different acquisition domains\nD) Limited availability of large-scale MRI datasets\n\nCorrect Answer: C\n\nExplanation: The passage explicitly states that \"poor transferability of the models to data from different sources or acquisition domains\" is one of the main reasons why few machine learning applications for medical image analysis are in clinical use. This directly corresponds to option C. While the other options might be relevant challenges in the field, they are not specifically highlighted as the primary issue in the given text. The passage emphasizes the importance of developing methods for domain adaptation and transfer learning to address this particular challenge in multi-modal medical imaging data analysis."}, "38": {"documentation": {"title": "Bayesian Multi-Scale Optimistic Optimization", "source": "Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas", "docs_id": "1402.7005", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Multi-Scale Optimistic Optimization. Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the primary innovation and advantage of the Bayesian Multi-Scale Optimistic Optimization technique as presented in the Arxiv documentation?\n\nA) It introduces a new acquisition function that is easier to optimize than traditional ones used in Bayesian optimization.\n\nB) It combines Gaussian process confidence bounds with treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions.\n\nC) It provides a method to exactly solve the NP-hard problem of optimizing acquisition functions in Bayesian optimization.\n\nD) It improves the convergence rate of Bayesian optimization by introducing a more efficient sampling strategy for the search space.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The key innovation described in the document is the combination of Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. This addresses a major shortcoming of traditional Bayesian optimization, which requires costly and difficult auxiliary optimization at each iteration.\n\nAnswer A is incorrect because the technique doesn't introduce a new acquisition function, but rather eliminates the need for optimizing acquisition functions altogether.\n\nAnswer C is incorrect because the technique doesn't claim to solve the NP-hard problem of optimizing acquisition functions. Instead, it avoids this problem by eliminating the need for such optimization.\n\nAnswer D is incorrect because while the technique does improve efficiency, it doesn't do so through a new sampling strategy. The improvement comes from eliminating the auxiliary optimization step.\n\nThe correct answer captures the essence of the innovation: combining two existing techniques to overcome a significant limitation of traditional Bayesian optimization, while also providing theoretical convergence guarantees without assuming exact optimization of acquisition functions."}, "39": {"documentation": {"title": "Holographic complexity and non-commutative gauge theory", "source": "Josiah Couch, Stefan Eccles, Willy Fischler, and Ming-Lei Xiao", "docs_id": "1710.07833", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Holographic complexity and non-commutative gauge theory. We study the holographic complexity of noncommutative field theories. The four-dimensional $\\mathcal{N}=4$ noncommutative super Yang-Mills theory with Moyal algebra along two of the spatial directions has a well known holographic dual as a type IIB supergravity theory with a stack of D3 branes and non-trivial NS-NS B fields. We start from this example and find that the late time holographic complexity growth rate, based on the \"complexity equals action\" conjecture, experiences an enhancement when the non-commutativity is turned on. This enhancement saturates a new limit which is exactly 1/4 larger than the commutative value. We then attempt to give a quantum mechanics explanation of the enhancement. Finite time behavior of the complexity growth rate is also studied. Inspired by the non-trivial result, we move on to more general setup in string theory where we have a stack of D$p$ branes and also turn on the B field. Multiple noncommutative directions are considered in higher $p$ cases."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of holographic complexity for the four-dimensional N=4 noncommutative super Yang-Mills theory with Moyal algebra along two spatial directions, what is observed regarding the late time holographic complexity growth rate when non-commutativity is introduced?\n\nA) The growth rate decreases by 1/4 compared to the commutative value\nB) The growth rate remains unchanged from the commutative value\nC) The growth rate experiences an enhancement that saturates at exactly 1/4 larger than the commutative value\nD) The growth rate becomes infinite due to non-commutativity\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of a key finding in the paper. The correct answer is C because the documentation explicitly states: \"We start from this example and find that the late time holographic complexity growth rate, based on the 'complexity equals action' conjecture, experiences an enhancement when the non-commutativity is turned on. This enhancement saturates a new limit which is exactly 1/4 larger than the commutative value.\"\n\nOption A is incorrect as it suggests a decrease rather than an increase. Option B is wrong because the growth rate does change. Option D is an extreme statement not supported by the text. Only C accurately reflects the described enhancement and its specific magnitude."}, "40": {"documentation": {"title": "Global sensitivity analysis using derivative-based sparse Poincar\\'e\n  chaos expansions", "source": "Nora L\\\"uthen, Olivier Roustant, Fabrice Gamboa, Bertrand Iooss,\n  Stefano Marelli, and Bruno Sudret", "docs_id": "2107.00394", "section": ["stat.CO", "cs.NA", "math.CA", "math.NA", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Global sensitivity analysis using derivative-based sparse Poincar\\'e\n  chaos expansions. Variance-based global sensitivity analysis, in particular Sobol' analysis, is widely used for determining the importance of input variables to a computational model. Sobol' indices can be computed cheaply based on spectral methods like polynomial chaos expansions (PCE). Another choice are the recently developed Poincar\\'e chaos expansions (PoinCE), whose orthonormal tensor-product basis is generated from the eigenfunctions of one-dimensional Poincar\\'e differential operators. In this paper, we show that the Poincar\\'e basis is the unique orthonormal basis with the property that partial derivatives of the basis form again an orthogonal basis with respect to the same measure as the original basis. This special property makes PoinCE ideally suited for incorporating derivative information into the surrogate modelling process. Assuming that partial derivative evaluations of the computational model are available, we compute spectral expansions in terms of Poincar\\'e basis functions or basis partial derivatives, respectively, by sparse regression. We show on two numerical examples that the derivative-based expansions provide accurate estimates for Sobol' indices, even outperforming PCE in terms of bias and variance. In addition, we derive an analytical expression based on the PoinCE coefficients for a second popular sensitivity index, the derivative-based sensitivity measure (DGSM), and explore its performance as upper bound to the corresponding total Sobol' indices."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: Which of the following statements about Poincar\u00e9 chaos expansions (PoinCE) is NOT correct?\n\nA) PoinCE basis functions are orthonormal and generated from eigenfunctions of one-dimensional Poincar\u00e9 differential operators.\n\nB) The partial derivatives of PoinCE basis functions form an orthogonal basis with respect to a different measure than the original basis.\n\nC) PoinCE can be used to compute Sobol' indices for global sensitivity analysis.\n\nD) PoinCE allows for the incorporation of derivative information in the surrogate modeling process.\n\nCorrect Answer: B\n\nExplanation:\nA is correct: The documentation states that PoinCE uses \"orthonormal tensor-product basis is generated from the eigenfunctions of one-dimensional Poincar\u00e9 differential operators.\"\n\nB is incorrect: The documentation specifically mentions that PoinCE is \"the unique orthonormal basis with the property that partial derivatives of the basis form again an orthogonal basis with respect to the same measure as the original basis.\" This is contrary to the statement in option B, which claims it's a different measure.\n\nC is correct: The document discusses using PoinCE for computing Sobol' indices, stating \"we show on two numerical examples that the derivative-based expansions provide accurate estimates for Sobol' indices.\"\n\nD is correct: The text mentions that PoinCE is \"ideally suited for incorporating derivative information into the surrogate modelling process.\"\n\nTherefore, B is the statement that is NOT correct about Poincar\u00e9 chaos expansions."}, "41": {"documentation": {"title": "Uniaxial linear resistivity of superconducting La(1.905)Ba(0.095)CuO(4)\n  induced by an external magnetic field", "source": "Jinsheng Wen, Qing Jie, Qiang Li, M. Huecker, M. v. Zimmermann, Su\n  Jung Han, Zhijun Xu, D. K. Singh, R. M. Konik, Liyuan Zhang, Genda Gu, J. M.\n  Tranquada", "docs_id": "1009.0031", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniaxial linear resistivity of superconducting La(1.905)Ba(0.095)CuO(4)\n  induced by an external magnetic field. We present an experimental study of the anisotropic resistivity of superconducting La(2-x)Ba(x)CuO(4) with x=0.095 and transition temperature Tc=32 K. In a magnetic field perpendicular to the CuO(2) layers, H(perp), we observe that the resistivity perpendicular to the layers, \\rho(perp), becomes finite at a temperature consistent with previous studies on very similar materials; however, the onset of finite parallel resistivity, \\rho(par), occurs at a much higher temperature. This behavior contradicts conventional theory, which predicts that \\rho(perp) and \\rho(par) should become finite at the same temperature. Voltage vs. current measurements near the threshold of voltage detectability indicate linear behavior perpendicular to the layers, becoming nonlinear at higher currents, while the behavior is nonlinear from the onset parallel to the layers. These results, in the presence of moderate H(perp), appear consistent with superconducting order parallel to the layers with voltage fluctuations between the layers due to thermal noise. In search of uncommon effects that might help to explain this behavior, we have performed diffraction measurements that provide evidence for H(perp)-induced charge and spin stripe order. The field-induced decoupling of superconducting layers is similar to the decoupled phase observed previously in La(2-x)Ba(x)CuO(4) with x=1/8 in zero field."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of La(1.905)Ba(0.095)CuO(4) under an external magnetic field perpendicular to the CuO(2) layers, which of the following observations contradicts conventional superconductivity theory and suggests a novel behavior?\n\nA) The resistivity perpendicular to the layers (\\rho(perp)) becomes finite at a temperature consistent with previous studies.\n\nB) The resistivity parallel to the layers (\\rho(par)) becomes finite at a much higher temperature than \\rho(perp).\n\nC) Voltage vs. current measurements show linear behavior perpendicular to the layers near the threshold of voltage detectability.\n\nD) The magnetic field induces charge and spin stripe order in the material.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the passage, the onset of finite parallel resistivity (\\rho(par)) occurs at a much higher temperature than the perpendicular resistivity (\\rho(perp)). This behavior contradicts conventional superconductivity theory, which predicts that both \\rho(perp) and \\rho(par) should become finite at the same temperature. This unexpected difference in onset temperatures suggests a novel behavior in the material's superconducting properties.\n\nOption A is incorrect because the behavior of \\rho(perp) is described as consistent with previous studies and doesn't contradict conventional theory.\n\nOption C, while describing an interesting observation, does not directly contradict conventional theory and is not presented as the main unconventional finding.\n\nOption D, although an interesting observation that might help explain the behavior, is not itself the contradiction to conventional theory that the question is asking about."}, "42": {"documentation": {"title": "A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM\n  Structures", "source": "Hossein Pourmeidani, Mehdi Habibi", "docs_id": "1907.04504", "section": ["cs.AR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM\n  Structures. Due to the small size of nanoscale devices, they are highly prone to process disturbances which results in manufacturing defects. Some of the defects are randomly distributed throughout the nanodevice layer. Other disturbances tend to be local and lead to cluster defects caused by factors such as layer misintegration and line width variations. In this paper, we propose a method for identifying cluster defects from random ones. The motivation is to repair the cluster defects using rectangular ranges in a range matching content-addressable memory (RM-CAM) and random defects using triple-modular redundancy (TMR). It is believed a combination of these two approaches is more effective for repairing defects at high error rate with less resource. With the proposed fault repairing technique, defect recovery results are examined for different fault distribution scenarios. Also the mapping circuit structure required for two conceptual 32*32 and 64*64 bit RAMs are presented and their speed, power and transistor count are reported."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of defect tolerance in NRAM structures, which combination of techniques is proposed to effectively repair both cluster and random defects at high error rates while minimizing resource usage?\n\nA) Triple-modular redundancy (TMR) for cluster defects and range matching content-addressable memory (RM-CAM) for random defects\nB) Range matching content-addressable memory (RM-CAM) for cluster defects and triple-modular redundancy (TMR) for random defects\nC) Only range matching content-addressable memory (RM-CAM) for both cluster and random defects\nD) Only triple-modular redundancy (TMR) for both cluster and random defects\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the proposed method aims to repair cluster defects using rectangular ranges in a range matching content-addressable memory (RM-CAM) and random defects using triple-modular redundancy (TMR). This combination is believed to be more effective for repairing defects at high error rates with less resource usage. \n\nOption A is incorrect because it reverses the techniques used for each type of defect. \n\nOptions C and D are incorrect because they suggest using only one technique for both types of defects, which does not align with the proposed combined approach described in the documentation."}, "43": {"documentation": {"title": "A dynamic theory of spatial externalities", "source": "Raouf Boucekkine, Giorgio Fabbri, Salvatore Federico, Fausto Gozzi", "docs_id": "2112.10584", "section": ["econ.TH", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A dynamic theory of spatial externalities. We characterize the shape of spatial externalities in a continuous time and space differential game with transboundary pollution. We posit a realistic spatiotemporal law of motion for pollution (diffusion and advection), and tackle spatiotemporal non-cooperative (and cooperative) differential games. Precisely, we consider a circle partitioned into several states where a local authority decides autonomously about its investment, production and depollution strategies over time knowing that investment/production generates pollution, and pollution is transboundary. The time horizon is infinite. We allow for a rich set of geographic heterogeneities across states. We solve analytically the induced non-cooperative differential game and characterize its long-term spatial distributions. In particular, we prove that there exist a Perfect Markov Equilibrium, unique among the class of the affine feedbacks. We further provide with a full exploration of the free riding problem and the associated border effect."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the spatial externalities model described, which of the following statements is most accurate regarding the Perfect Markov Equilibrium (PME) and the free-riding problem?\n\nA) The PME is unique among all possible equilibria and completely eliminates the free-riding problem across all states.\n\nB) Multiple PMEs exist, but only one is unique among the class of affine feedbacks, and it exacerbates the free-riding problem uniformly across all states.\n\nC) A unique PME exists among the class of affine feedbacks, and it likely results in varying degrees of free-riding across different states, potentially creating a border effect.\n\nD) The PME is non-existent in this model due to the complex interplay of geographic heterogeneities and transboundary pollution, making free-riding inevitable.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of key concepts in the spatial externalities model. Option C is correct because:\n\n1. The documentation states that they \"prove that there exist a Perfect Markov Equilibrium, unique among the class of the affine feedbacks.\"\n2. It mentions a \"full exploration of the free riding problem and the associated border effect,\" suggesting that free-riding varies across states and is influenced by geographical position.\n3. The model allows for \"a rich set of geographic heterogeneities across states,\" which would likely lead to varying degrees of free-riding rather than uniform effects.\n\nOption A is incorrect because the PME is only unique among affine feedbacks, not all equilibria, and it doesn't eliminate free-riding. Option B is wrong because only one PME is mentioned as unique among affine feedbacks, not multiple. Option D is incorrect because the model does prove the existence of a PME, contrary to this statement."}, "44": {"documentation": {"title": "Energy loss of heavy quarks and $B$ and $D$ meson spectra in PbPb\n  collisions at LHC energies", "source": "Kapil Saraswat, Prashant Shukla, Vineet Kumar, Venktesh Singh", "docs_id": "1702.05733", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy loss of heavy quarks and $B$ and $D$ meson spectra in PbPb\n  collisions at LHC energies. We study the production and evolution of charm and bottom quarks in hot partonic medium produced in heavy ion collisions. The heavy quarks loose energy in the medium which is reflected in the transverse momentum spectra of heavy mesons. The collisional energy loss of heavy quarks has been calculated using QCD calculations. The radiative energy loss is obtained using two models namely reaction operator formalism and generalized dead cone approach. The nuclear modification factors, $R_{AA}$ as a function of transverse momentum by including shadowing and energy loss are calculated for $D^{0}$ and $B^{+}$ mesons in PbPb collisions at $\\sqrt{s_{NN}}$ = 5.02 TeV and for $D^{0}$ mesons at $\\sqrt{s_{NN}}$ = 2.76 TeV and are compared with the recent measurements. The radiative energy loss from generalized dead cone approach alone is sufficient to produce measured $D^{0}$ meson $R_{AA}$ at both the LHC energies. The radiative energy loss from reaction operator formalism plus collisional energy loss gives good description of $D^{0}$ meson $R_{AA}$. For the case of $B^{+}$ meson, the radiative energy loss from generalized dead cone approach plus collisional energy loss gives good description of the CMS data. The radiative process is dominant for charm quarks while for the bottom, both the radiative process and the elastic collisions are important."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the study of heavy quark energy loss in PbPb collisions at LHC energies, which of the following statements is most accurate regarding the radiative energy loss models and their effectiveness in describing the nuclear modification factor (R_AA) for D^0 and B^+ mesons?\n\nA) The reaction operator formalism alone adequately describes the R_AA for both D^0 and B^+ mesons at all LHC energies studied.\n\nB) The generalized dead cone approach is sufficient to explain the R_AA for D^0 mesons, while a combination of radiative and collisional energy loss is needed for B^+ mesons.\n\nC) Collisional energy loss is the dominant factor in describing the R_AA for both D^0 and B^+ mesons at all transverse momenta.\n\nD) The reaction operator formalism combined with collisional energy loss is necessary to describe the R_AA for both D^0 and B^+ mesons at all LHC energies.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the text states that \"The radiative energy loss from generalized dead cone approach alone is sufficient to produce measured D^0 meson R_AA at both the LHC energies.\" For B^+ mesons, it mentions that \"the radiative energy loss from generalized dead cone approach plus collisional energy loss gives good description of the CMS data.\" This indicates that while the generalized dead cone approach is enough for D^0 mesons, a combination of radiative and collisional energy loss is needed for B^+ mesons. \n\nOption A is incorrect because the reaction operator formalism alone is not mentioned as being sufficient for both mesons. Option C is wrong because radiative energy loss is described as dominant for charm quarks, not collisional energy loss. Option D is incorrect because while the reaction operator formalism plus collisional energy loss works for D^0 mesons, it's not mentioned as necessary for B^+ mesons."}, "45": {"documentation": {"title": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition", "source": "Jos\\'e Henrique de Morais Goulart, Pierre Comon", "docs_id": "1712.05742", "section": ["math.NA", "cs.NA", "math.AG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition. Under the action of the general linear group with tensor structure, the ranks of matrices $A$ and $B$ forming an $m \\times n$ pencil $A + \\lambda B$ can change, but in a restricted manner. Specifically, with every pencil one can associate a pair of minimal ranks, which is unique up to a permutation. This notion can be defined for matrix pencils and, more generally, also for matrix polynomials of arbitrary degree. In this paper, we provide a formal definition of the minimal ranks, discuss its properties and the natural hierarchy it induces in a pencil space. Then, we show how the minimal ranks of a pencil can be determined from its Kronecker canonical form. For illustration, we classify the orbits according to their minimal ranks (under the action of the general linear group) in the case of real pencils with $m, n \\le 4$. Subsequently, we show that real regular $2k \\times 2k$ pencils having only complex-valued eigenvalues, which form an open positive-volume set, do not admit a best approximation (in the norm topology) on the set of real pencils whose minimal ranks are bounded by $2k-1$. Our results can be interpreted from a tensor viewpoint, where the minimal ranks of a degree-$(d-1)$ matrix polynomial characterize the minimal ranks of matrices constituting a block-term decomposition of an $m \\times n \\times d$ tensor into a sum of matrix-vector tensor products."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Consider a 4x4 real matrix pencil A + \u03bbB. Which of the following statements about its minimal ranks is most likely to be true?\n\nA) The minimal ranks are always (4,4) for any 4x4 real matrix pencil.\n\nB) If the pencil has only complex-valued eigenvalues, its minimal ranks can always be reduced to (3,3) through the action of the general linear group.\n\nC) The minimal ranks of the pencil can be determined from its Kronecker canonical form, but this information is not useful for understanding the pencil's properties under group action.\n\nD) For a regular 4x4 real pencil with only complex-valued eigenvalues, there may not exist a best approximation (in the norm topology) by pencils with minimal ranks bounded by 3.\n\nCorrect Answer: D\n\nExplanation: Option D is correct based on the information provided in the documentation. The text states that \"real regular 2k \u00d7 2k pencils having only complex-valued eigenvalues, which form an open positive-volume set, do not admit a best approximation (in the norm topology) on the set of real pencils whose minimal ranks are bounded by 2k-1.\" For a 4x4 pencil, k=2, so 2k-1 = 3, which matches the statement in option D.\n\nOption A is incorrect because the minimal ranks can vary and are not always (4,4) for every 4x4 pencil. \n\nOption B is false because while the ranks can change under group action, there's no guarantee that they can always be reduced to (3,3), especially for pencils with only complex-valued eigenvalues.\n\nOption C is partially true but ultimately incorrect. The Kronecker canonical form does indeed help determine the minimal ranks, but this information is actually very useful for understanding the pencil's properties under group action, contrary to what the option states."}, "46": {"documentation": {"title": "Interacting Swarm Sensing and Stabilization", "source": "Ira B. Schwartz, Victoria Edwards and Jason Hindes", "docs_id": "2106.01824", "section": ["nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interacting Swarm Sensing and Stabilization. Swarming behavior, where coherent motion emerges from the interactions of many mobile agents, is ubiquitous in physics and biology. Moreover, there are many efforts to replicate swarming dynamics in mobile robotic systems which take inspiration from natural swarms. In particular, understanding how swarms come apart, change their behavior, and interact with other swarms is a research direction of special interest to the robotics and defense communities. Here we develop a theoretical approach that can be used to predict the parameters under which colliding swarms form a stable milling state. Our analytical methods rely on the assumption that, upon collision, two swarms oscillate near a limit-cycle, where each swarm rotates around the other while maintaining an approximately constant density. Using our methods, we are able to predict the critical swarm-swarm interaction coupling (below which two colliding swarms merely scatter) for nearly aligned collisions as a function of physical swarm parameters. We show that the critical coupling corresponds to a saddle-node bifurcation of a limit-cycle in the constant-density approximation. Finally, we show preliminary results from experiments in which two swarms of micro UAVs collide and form a milling state, which is in general agreement with our theory."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of colliding swarms forming a stable milling state, what does the critical swarm-swarm interaction coupling correspond to, and how is it determined according to the theoretical approach described?\n\nA) It corresponds to a Hopf bifurcation and is determined by the swarm's initial velocity.\nB) It corresponds to a saddle-node bifurcation of a limit-cycle in the constant-density approximation and is predicted for nearly aligned collisions as a function of physical swarm parameters.\nC) It corresponds to a period-doubling bifurcation and is calculated based on the swarm's size and density.\nD) It corresponds to a transcritical bifurcation and is derived from the swarm's rotational speed around each other.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the researchers developed a theoretical approach to predict the critical swarm-swarm interaction coupling, below which colliding swarms merely scatter. This critical coupling is described as corresponding to \"a saddle-node bifurcation of a limit-cycle in the constant-density approximation.\" The approach predicts this critical coupling \"for nearly aligned collisions as a function of physical swarm parameters.\" This matches exactly with option B.\n\nOptions A, C, and D introduce different types of bifurcations (Hopf, period-doubling, and transcritical) which are not mentioned in the given text. They also suggest different methods for determining the critical coupling that are not supported by the documentation provided."}, "47": {"documentation": {"title": "Automated Model Selection for Time-Series Anomaly Detection", "source": "Yuanxiang Ying, Juanyong Duan, Chunlei Wang, Yujing Wang, Congrui\n  Huang, Bixiong Xu", "docs_id": "2009.04395", "section": ["cs.LG", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automated Model Selection for Time-Series Anomaly Detection. Time-series anomaly detection is a popular topic in both academia and industrial fields. Many companies need to monitor thousands of temporal signals for their applications and services and require instant feedback and alerts for potential incidents in time. The task is challenging because of the complex characteristics of time-series, which are messy, stochastic, and often without proper labels. This prohibits training supervised models because of lack of labels and a single model hardly fits different time series. In this paper, we propose a solution to address these issues. We present an automated model selection framework to automatically find the most suitable detection model with proper parameters for the incoming data. The model selection layer is extensible as it can be updated without too much effort when a new detector is available to the service. Finally, we incorporate a customized tuning algorithm to flexibly filter anomalies to meet customers' criteria. Experiments on real-world datasets show the effectiveness of our solution."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary challenge and proposed solution in the automated model selection framework for time-series anomaly detection, as presented in the Arxiv documentation?\n\nA) The challenge is the abundance of labeled data, and the solution is to train a single supervised model to fit all time series.\n\nB) The challenge is the complex nature of time-series data and lack of proper labels, and the solution is an automated model selection framework that finds the most suitable detection model for each incoming data stream.\n\nC) The challenge is the simplicity of time-series data, and the solution is to manually select models for each incoming data stream.\n\nD) The challenge is the lack of available detection models, and the solution is to create a fixed set of parameters that work for all time series.\n\nCorrect Answer: B\n\nExplanation: The question tests the reader's understanding of both the core challenge in time-series anomaly detection and the proposed solution. Option B correctly identifies the challenge as the complex nature of time-series data (described as \"messy, stochastic, and often without proper labels\" in the text) and the lack of labels, which prevents training supervised models. It also accurately describes the proposed solution: an automated model selection framework that finds the most suitable detection model for each incoming data stream.\n\nOption A is incorrect because it misrepresents the challenge (abundance of labeled data instead of lack of labels) and proposes a single model approach, which the text explicitly states is ineffective.\n\nOption C is incorrect as it mischaracterizes the nature of time-series data as simple (when it's actually complex) and suggests manual model selection, which is opposite to the automated approach described.\n\nOption D is incorrect because it misidentifies the challenge and proposes a fixed parameter set, which goes against the paper's emphasis on flexibility and customization for different time series."}, "48": {"documentation": {"title": "Quality of Service Guarantees for Physical Unclonable Functions", "source": "Onur G\\\"unl\\\"u, Rafael F. Schaefer, and H. Vincent Poor", "docs_id": "2107.05675", "section": ["eess.SP", "cs.CR", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quality of Service Guarantees for Physical Unclonable Functions. We consider a secret key agreement problem in which noisy physical unclonable function (PUF) outputs facilitate reliable, secure, and private key agreement with the help of public, noiseless, and authenticated storage. PUF outputs are highly correlated, so transform coding methods have been combined with scalar quantizers to extract uncorrelated bit sequences with reliability guarantees. For PUF circuits with continuous-valued outputs, the models for transformed outputs are made more realistic by replacing the fitted distributions with corresponding truncated ones. The state-of-the-art PUF methods that provide reliability guarantees to each extracted bit are shown to be inadequate to guarantee the same reliability level for all PUF outputs. Thus, a quality of service parameter is introduced to control the percentage of PUF outputs for which a target reliability level can be guaranteed. A public ring oscillator (RO) output dataset is used to illustrate that a truncated Gaussian distribution can be fitted to transformed RO outputs that are inputs to uniform scalar quantizers such that reliability guarantees can be provided for each bit extracted from any PUF device under additive Gaussian noise components by eliminating a small subset of PUF outputs. Furthermore, we conversely show that it is not possible to provide such reliability guarantees without eliminating any PUF output if no extra secrecy and privacy leakage is allowed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Physical Unclonable Functions (PUFs) and secret key agreement, which of the following statements is correct regarding the use of truncated distributions and quality of service parameters?\n\nA) Truncated distributions are used to model PUF outputs with discrete values, improving the accuracy of reliability guarantees.\n\nB) The introduction of a quality of service parameter allows for 100% of PUF outputs to meet a target reliability level without any trade-offs.\n\nC) Truncated Gaussian distributions can be fitted to transformed ring oscillator outputs, enabling reliability guarantees for each bit extracted from any PUF device under additive Gaussian noise, but only after eliminating a small subset of PUF outputs.\n\nD) State-of-the-art PUF methods that provide reliability guarantees for each extracted bit are sufficient to guarantee the same reliability level for all PUF outputs.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"a truncated Gaussian distribution can be fitted to transformed RO outputs that are inputs to uniform scalar quantizers such that reliability guarantees can be provided for each bit extracted from any PUF device under additive Gaussian noise components by eliminating a small subset of PUF outputs.\"\n\nOption A is incorrect because truncated distributions are used for continuous-valued outputs, not discrete values.\n\nOption B is incorrect because the quality of service parameter is introduced to control the percentage of PUF outputs for which a target reliability level can be guaranteed, implying that not all outputs can meet the target without trade-offs.\n\nOption D is incorrect as the documentation explicitly states that \"state-of-the-art PUF methods that provide reliability guarantees to each extracted bit are shown to be inadequate to guarantee the same reliability level for all PUF outputs.\""}, "49": {"documentation": {"title": "Optimal Learning for Structured Bandits", "source": "Bart P.G. Van Parys, Negin Golrezaei", "docs_id": "2007.07302", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Learning for Structured Bandits. We study structured multi-armed bandits, which is the problem of online decision-making under uncertainty in the presence of structural information. In this problem, the decision-maker needs to discover the best course of action despite observing only uncertain rewards over time. The decision-maker is aware of certain structural information regarding the reward distributions and would like to minimize their regret by exploiting this information, where the regret is its performance difference against a benchmark policy that knows the best action ahead of time. In the absence of structural information, the classical upper confidence bound (UCB) and Thomson sampling algorithms are well known to suffer only minimal regret. As recently pointed out, neither algorithms are, however, capable of exploiting structural information that is commonly available in practice. We propose a novel learning algorithm that we call DUSA whose worst-case regret matches the information-theoretic regret lower bound up to a constant factor and can handle a wide range of structural information. Our algorithm DUSA solves a dual counterpart of the regret lower bound at the empirical reward distribution and follows its suggested play. Our proposed algorithm is the first computationally viable learning policy for structured bandit problems that has asymptotic minimal regret."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of structured multi-armed bandits, which of the following statements is true about the DUSA algorithm?\n\nA) It performs worse than UCB and Thompson sampling algorithms in exploiting structural information.\nB) It achieves the information-theoretic regret lower bound exactly.\nC) It solves a primal problem of the regret upper bound at the true reward distribution.\nD) It is the first computationally viable algorithm for structured bandits with asymptotic minimal regret.\n\nCorrect Answer: D\n\nExplanation: \nA is incorrect because DUSA is specifically designed to exploit structural information, which UCB and Thompson sampling cannot do effectively.\nB is incorrect because DUSA matches the information-theoretic regret lower bound up to a constant factor, not exactly.\nC is incorrect because DUSA solves a dual counterpart of the regret lower bound at the empirical reward distribution, not a primal problem at the true reward distribution.\nD is correct because the text explicitly states that DUSA is \"the first computationally viable learning policy for structured bandit problems that has asymptotic minimal regret.\""}, "50": {"documentation": {"title": "Torelli groups, extended Johnson homomorphisms, and new cycles on the\n  moduli space of curves", "source": "S. Morita and R. C. Penner", "docs_id": "math/0602461", "section": ["math.GT", "math.AG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Torelli groups, extended Johnson homomorphisms, and new cycles on the\n  moduli space of curves. Infinite presentations are given for all of the higher Torelli groups of once-punctured surfaces. In the case of the classical Torelli group, a finite presentation of the corresponding groupoid is also given, and finite presentations of the classical Torelli groups acting trivially on homology modulo N are derived for all N. Furthermore, the first Johnson homomorphism, which is defined from the classical Torelli group to the third exterior power of the homology of the surface, is shown to lift to an explicit canonical 1-cocycle of the Teichmueller space. The main tool for these results is the known mapping class group invariant ideal cell decomposition of the Teichmueller space. This new 1-cocycle is mapping class group equivariant, so various contractions of its powers yield various combinatorial (co)cycles of the moduli space of curves, which are also new. Our combinatorial construction can be related to former works of Kawazumi and the first-named author with the consequence that the algebra generated by the cohomology classes represented by the new cocycles is precisely the tautological algebra of the moduli space. There is finally a discussion of prospects for similarly finding cocycle lifts of the higher Johnson homomorphisms."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements about the research described in the Arxiv documentation is NOT correct?\n\nA) The study provides infinite presentations for all higher Torelli groups of once-punctured surfaces.\n\nB) The research demonstrates that the first Johnson homomorphism can be lifted to an explicit canonical 1-cocycle of the Teichmueller space.\n\nC) The combinatorial construction in this work proves that the algebra generated by the cohomology classes represented by the new cocycles is entirely distinct from the tautological algebra of the moduli space.\n\nD) Finite presentations of the classical Torelli groups acting trivially on homology modulo N are derived for all N.\n\nCorrect Answer: C\n\nExplanation:\nA is correct: The documentation states that \"Infinite presentations are given for all of the higher Torelli groups of once-punctured surfaces.\"\n\nB is correct: The text mentions that \"the first Johnson homomorphism... is shown to lift to an explicit canonical 1-cocycle of the Teichmueller space.\"\n\nC is incorrect: The documentation actually states that \"the algebra generated by the cohomology classes represented by the new cocycles is precisely the tautological algebra of the moduli space,\" not that it's entirely distinct.\n\nD is correct: The text explicitly states that \"finite presentations of the classical Torelli groups acting trivially on homology modulo N are derived for all N.\"\n\nThe correct answer is C because it contradicts the information provided in the documentation, while the other options accurately reflect the research findings."}, "51": {"documentation": {"title": "Internal heating and thermal emission from old neutron stars:\n  Constraints on dense-matter and gravitational physics", "source": "Andreas Reisenegger, Rodrigo Fernandez, Paula Jofre", "docs_id": "astro-ph/0610955", "section": ["astro-ph", "gr-qc", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Internal heating and thermal emission from old neutron stars:\n  Constraints on dense-matter and gravitational physics. The equilibrium composition of neutron star matter is achieved through weak interactions (direct and inverse beta decays), which proceed on relatively long time scales. If the density of a matter element is perturbed, it will relax to the new chemical equilibrium through non-equilibrium reactions, which produce entropy that is partly released through neutrino emission, while a similar fraction heats the matter and is eventually radiated as thermal photons. We examined two possible mechanisms causing such density perturbations: 1) the reduction in centrifugal force caused by spin-down (particularly in millisecond pulsars), leading to \"rotochemical heating\", and 2) a hypothetical time-variation of the gravitational constant, as predicted by some theories of gravity and current cosmological models, leading to \"gravitochemical heating\". If only slow weak interactions are allowed in the neutron star (modified Urca reactions, with or without Cooper pairing), rotochemical heating can account for the observed ultraviolet emission from the closest millisecond pulsar, PSR J0437-4715, which also provides a constraint on |dG/dt| of the same order as the best available in the literature."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements most accurately describes the relationship between rotochemical heating and the observed ultraviolet emission from PSR J0437-4715?\n\nA) Rotochemical heating is incompatible with the observed ultraviolet emission from PSR J0437-4715, suggesting alternative heating mechanisms must be at play.\n\nB) Rotochemical heating can fully account for the observed ultraviolet emission from PSR J0437-4715, but only if fast weak interactions are allowed in the neutron star.\n\nC) Rotochemical heating can account for the observed ultraviolet emission from PSR J0437-4715 if only slow weak interactions (modified Urca reactions) are allowed in the neutron star.\n\nD) Rotochemical heating partially contributes to the observed ultraviolet emission from PSR J0437-4715, but gravitochemical heating is the primary source.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"If only slow weak interactions are allowed in the neutron star (modified Urca reactions, with or without Cooper pairing), rotochemical heating can account for the observed ultraviolet emission from the closest millisecond pulsar, PSR J0437-4715.\" This directly supports option C.\n\nOption A is incorrect because the documentation indicates that rotochemical heating is compatible with the observed emission. Option B is incorrect because it specifies fast weak interactions, while the document mentions slow weak interactions. Option D is incorrect because the document does not suggest that gravitochemical heating is the primary source of the observed emission."}, "52": {"documentation": {"title": "$B_s$ Mixing at SLD", "source": "Cheng-Ju Stephen Lin (representing the SLD Collaboration)", "docs_id": "hep-ex/0011040", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$B_s$ Mixing at SLD. We set a preliminary 95% C.L. exclusion on the oscillation frequency of $B_s^0 - \\bar{B_s^0}$ mixing using a sample of 400,000 hadronic $Z^0$ decays collected by the SLD experiment at the SLC during the 1996-98 run. Three analyses are presented in this paper. The first analysis partially reconstructs the $B_s^0$ by combining a fully reconstructed $D_s$ with the remaining charged B decay tracks. The second analysis selects a sample of events with a partially reconstructed charm vertex and a lepton track. The third analysis reconstructs b-hadrons topologically and exploits the $b \\to c$ cascade charge structure to determine the flavor of the b-hadron at decay. All three analyses take advantage of the large forward-backward asymmetry of the polarized $Z^0 \\to b \\bar{b}$ decays and information in the hemisphere opposite to the reconstructed B vertex to determine the b-hadron flavor at production. The results of the three analyses are combined to exclude the following values of the $B_s^0 - \\bar{B_s^0}$ oscillation frequency: $\\Delta m_s < 7.6 ps^{-1}$ and $11.8 < \\Delta m_s < 14.8 ps^{-1}$ at the 95% confidence level."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the methodology and results of the SLD experiment's investigation into B_s mixing?\n\nA) The experiment used a single analysis method involving fully reconstructed D_s mesons, and set a lower limit on the B_s^0 - B\u0304_s^0 oscillation frequency of \u0394m_s > 14.8 ps^-1 at 95% C.L.\n\nB) Three distinct analysis methods were employed, all utilizing the forward-backward asymmetry of polarized Z^0 \u2192 bb\u0304 decays, resulting in the exclusion of \u0394m_s < 7.6 ps^-1 and 11.8 < \u0394m_s < 14.8 ps^-1 at 95% C.L.\n\nC) Two analysis methods were used: one involving D_s reconstruction and another using charm vertex and lepton track information. The experiment set an upper limit on \u0394m_s of 11.8 ps^-1 at 95% C.L.\n\nD) The experiment combined four different analysis techniques, including full B_s^0 reconstruction, and excluded all values of \u0394m_s below 14.8 ps^-1 at 95% C.L.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately summarizes the key aspects of the experiment:\n\n1. It mentions the use of three distinct analysis methods, which is correct according to the passage.\n2. It correctly states that all methods utilized the forward-backward asymmetry of polarized Z^0 \u2192 bb\u0304 decays.\n3. It accurately reports the excluded ranges for \u0394m_s: < 7.6 ps^-1 and 11.8 < \u0394m_s < 14.8 ps^-1 at 95% C.L.\n\nOptions A, C, and D are incorrect because they either misstate the number of analysis methods used, incorrectly report the results, or provide inaccurate information about the experimental techniques employed."}, "53": {"documentation": {"title": "Automated Intracranial Artery Labeling using a Graph Neural Network and\n  Hierarchical Refinement", "source": "Li Chen, Thomas Hatsukami, Jenq-Neng Hwang, Chun Yuan", "docs_id": "2007.14472", "section": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automated Intracranial Artery Labeling using a Graph Neural Network and\n  Hierarchical Refinement. Automatically labeling intracranial arteries (ICA) with their anatomical names is beneficial for feature extraction and detailed analysis of intracranial vascular structures. There are significant variations in the ICA due to natural and pathological causes, making it challenging for automated labeling. However, the existing public dataset for evaluation of anatomical labeling is limited. We construct a comprehensive dataset with 729 Magnetic Resonance Angiography scans and propose a Graph Neural Network (GNN) method to label arteries by classifying types of nodes and edges in an attributed relational graph. In addition, a hierarchical refinement framework is developed for further improving the GNN outputs to incorporate structural and relational knowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%, and 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a testing set of 105 scans with both healthy and diseased subjects. This is a significant improvement over available state-of-the-art methods. Automatic artery labeling is promising to minimize manual effort in characterizing the complicated ICA networks and provides valuable information for the identification of geometric risk factors of vascular disease. Our code and dataset are available at https://github.com/clatfd/GNN-ARTLABEL."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the challenges and achievements of the automated intracranial artery (ICA) labeling method presented in the study?\n\nA) The method achieved 97.5% node labeling accuracy but struggled with diseased subjects, resulting in only 30% of scans being correctly labeled for all Circle of Willis nodes.\n\nB) The study utilized a small dataset of 200 MRA scans and achieved moderate success with a node labeling accuracy of 85% using traditional machine learning techniques.\n\nC) The proposed method combined a Graph Neural Network (GNN) with a hierarchical refinement framework, achieving 97.5% node labeling accuracy and correctly labeling 63.8% of scans for all Circle of Willis nodes in both healthy and diseased subjects.\n\nD) The research focused solely on healthy subjects and achieved perfect labeling accuracy for the Circle of Willis, but did not address the challenges posed by pathological variations in ICAs.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key aspects of the study's methodology and results. The research used a large dataset of 729 MRA scans and proposed a GNN method combined with a hierarchical refinement framework. This approach achieved a node labeling accuracy of 97.5% and correctly labeled 63.8% of scans for all Circle of Willis nodes in both healthy and diseased subjects. This information aligns with the details provided in the original text and represents the study's main achievements.\n\nOption A is incorrect because it misrepresents the performance on diseased subjects. The study actually included both healthy and diseased subjects in its 63.8% correct labeling rate for all Circle of Willis nodes.\n\nOption B is incorrect as it understates the size of the dataset (729 scans, not 200) and the accuracy achieved. It also incorrectly suggests the use of traditional machine learning techniques instead of the GNN and hierarchical refinement approach described.\n\nOption D is incorrect because the study did not focus solely on healthy subjects and did address the challenges of pathological variations, as evidenced by the inclusion of both healthy and diseased subjects in the testing set."}, "54": {"documentation": {"title": "Decomposition tables for experiments I. A chain of randomizations", "source": "C. J. Brien, R. A. Bailey", "docs_id": "0911.4027", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decomposition tables for experiments I. A chain of randomizations. One aspect of evaluating the design for an experiment is the discovery of the relationships between subspaces of the data space. Initially we establish the notation and methods for evaluating an experiment with a single randomization. Starting with two structures, or orthogonal decompositions of the data space, we describe how to combine them to form the overall decomposition for a single-randomization experiment that is ``structure balanced.'' The relationships between the two structures are characterized using efficiency factors. The decomposition is encapsulated in a decomposition table. Then, for experiments that involve multiple randomizations forming a chain, we take several structures that pairwise are structure balanced and combine them to establish the form of the orthogonal decomposition for the experiment. In particular, it is proven that the properties of the design for such an experiment are derived in a straightforward manner from those of the individual designs. We show how to formulate an extended decomposition table giving the sources of variation, their relationships and their degrees of freedom, so that competing designs can be evaluated."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of decomposition tables for experiments with a chain of randomizations, which of the following statements is NOT correct?\n\nA) Efficiency factors are used to characterize the relationships between two structures in a single-randomization experiment.\n\nB) The properties of a design for an experiment with multiple randomizations can be derived independently from the individual designs in the chain.\n\nC) An extended decomposition table includes sources of variation, their relationships, and their degrees of freedom.\n\nD) Structure balance is a key concept in combining orthogonal decompositions of the data space for single-randomization experiments.\n\nCorrect Answer: B\n\nExplanation: \nOption B is incorrect and thus the correct answer to this question asking for which statement is NOT correct. The documentation states that \"the properties of the design for such an experiment are derived in a straightforward manner from those of the individual designs.\" This implies that the properties are not derived independently, but rather are interconnected and derived from the individual designs in the chain.\n\nOptions A, C, and D are all correct statements based on the given information:\n\nA) The document mentions that \"The relationships between the two structures are characterized using efficiency factors.\"\n\nC) It's stated that the extended decomposition table gives \"the sources of variation, their relationships and their degrees of freedom.\"\n\nD) The text describes how to combine two structures \"to form the overall decomposition for a single-randomization experiment that is 'structure balanced.'\"\n\nThis question tests the understanding of key concepts in decomposition tables for multi-randomization experiments and requires careful reading to identify the subtle incorrect statement."}, "55": {"documentation": {"title": "Vector Nonlinear Schr\\\"odinger Equation on the half-line", "source": "V. Caudrelier and Q. C. Zhang", "docs_id": "1110.2990", "section": ["math-ph", "hep-th", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vector Nonlinear Schr\\\"odinger Equation on the half-line. We investigate the Manakov model or, more generally, the vector nonlinear Schr\\\"odinger equation on the half-line. Using a B\\\"acklund transformation method, two classes of integrable boundary conditions are derived: mixed Neumann/Dirichlet and Robin boundary conditions. Integrability is shown by constructing a generating function for the conserved quantities. We apply a nonlinear mirror image technique to construct the inverse scattering method with these boundary conditions. The important feature in the reconstruction formula for the fields is the symmetry property of the scattering data emerging from the presence of the boundary. Particular attention is paid to the discrete spectrum. An interesting phenomenon of transmission between the components of a vector soliton interacting with the boundary is demonstrated. This is specific to the vector nature of the model and is absent in the scalar case. For one-soliton solutions, we show that the boundary can be used to make certain components of the incoming soliton vanishingly small. This is reminiscent of the phenomenon of light polarization by reflection."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the vector nonlinear Schr\u00f6dinger equation on the half-line, which of the following statements is correct regarding the interaction of vector solitons with the boundary?\n\nA) The boundary interaction always preserves the amplitude ratios between components of a vector soliton.\n\nB) The phenomenon of transmission between components is also observed in scalar nonlinear Schr\u00f6dinger equations.\n\nC) The boundary can be used to significantly reduce or eliminate certain components of an incoming vector soliton.\n\nD) The discrete spectrum is unaffected by the presence of the boundary in the inverse scattering method.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that \"For one-soliton solutions, we show that the boundary can be used to make certain components of the incoming soliton vanishingly small.\" This demonstrates that the boundary can indeed be used to significantly reduce or eliminate certain components of an incoming vector soliton.\n\nOption A is incorrect because the documentation mentions a \"phenomenon of transmission between the components of a vector soliton interacting with the boundary,\" which implies that the amplitude ratios between components can change.\n\nOption B is false because the text specifically states that this transmission phenomenon is \"specific to the vector nature of the model and is absent in the scalar case.\"\n\nOption D is incorrect because the documentation mentions that \"Particular attention is paid to the discrete spectrum,\" suggesting that the discrete spectrum is affected by the boundary in the inverse scattering method.\n\nThis question tests the understanding of the unique properties of vector solitons interacting with boundaries in the vector nonlinear Schr\u00f6dinger equation, as opposed to scalar cases, and the ability to interpret the implications of the research findings described in the documentation."}, "56": {"documentation": {"title": "Synergistical Electroluminescent Manipulation for Efficient Blue\n  Perovskite Light-Emitting Diodes Through Interfacial Nucleation Seeding", "source": "Hai-Yan Wu, Yang Shen, Yan-Qing Li, Kong-Chao Shen, and Jian-Xin Tang", "docs_id": "2012.03172", "section": ["physics.app-ph", "cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synergistical Electroluminescent Manipulation for Efficient Blue\n  Perovskite Light-Emitting Diodes Through Interfacial Nucleation Seeding. The low efficiency of metal halide perovskite light-emitting diodes (PeLEDs) with blue emission block their potential applications in large-area full-color displays and solid-state lighting. A delicate control over the entire electroluminescence process is indispensable to overcome the efficiency limitations of blue PeLEDs. Here, we demonstrate an efficient device architecture to synergistically reduce the energetic losses during electron-photon conversion and boost the extraction of trapped light in the device. An interfacial nucleation seeding scheme is proposed to control the crystallization process of highly emissive perovskite nanocrystals and suppress the trap-mediated non-radiative recombination losses due to interfacial hydrogen bonding interactions. This manipulation results in a record external quantum efficiency (EQE) of 12.8% for blue PeLEDs emitting at 486 nm, along with the improved spectral stability and operation lifetime. Additionally, the maximum EQE reaches 16.8% after combining an internal outcoupling structure without spectral distortion, which can be further raised to 27.5% when using a lens-based structure on top of the device. We anticipate that our work provides an effective method for its application in high-performance PeLEDs."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which combination of techniques resulted in the highest reported external quantum efficiency (EQE) for blue perovskite light-emitting diodes (PeLEDs) in this study?\n\nA) Interfacial nucleation seeding alone\nB) Interfacial nucleation seeding with internal outcoupling structure\nC) Interfacial nucleation seeding with internal outcoupling structure and lens-based structure\nD) Interfacial nucleation seeding with lens-based structure only\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the multiple techniques used to improve the efficiency of blue PeLEDs and their combined effects. The correct answer is C because the document states that the interfacial nucleation seeding scheme resulted in a record EQE of 12.8% for blue PeLEDs emitting at 486 nm. This efficiency was further improved to 16.8% by combining an internal outcoupling structure. Finally, the maximum EQE reached 27.5% when using a lens-based structure on top of the device, in addition to the previous techniques. This combination of all three methods (interfacial nucleation seeding, internal outcoupling structure, and lens-based structure) produced the highest reported EQE in the study."}, "57": {"documentation": {"title": "Reconstruction of high-energy part of gamma-ray spectrum in thermal\n  neutron capture by $^{113}$Cd", "source": "V.A. Plujko, O.M. Gorbachenko, K.M. Solodovnyk, V.M. Petrenko", "docs_id": "2109.07963", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconstruction of high-energy part of gamma-ray spectrum in thermal\n  neutron capture by $^{113}$Cd. Average gamma-ray spectrum from $^{114}$Cd after thermal neutron capture in $^{113}$Cd was evaluated in units of mb/MeV. Two approaches are considered for estimation of average gamma-ray spectrum with normalization of the experimental data: mean spectra for all gamma-energies were found by averaging frequency polygon for experimental data histogram, and mean spectra were estimated as combination of theoretical values at low gamma-ray energies and averaging experimental data in high-energy range. The experimental spectra were evaluated from the gamma-intensities given by Mheemeed et al [A. Mheemeed et al., Nucl. Phys. A 412 (1984) 113] and Belgya et al [T. Belgya et al., EPJ Web Of Conf. 146 (2017) 05009]. They were normalized to average theoretical spectrum which were calculated by EMPIRE and TALYS codes with default input parameters. Procedure of normalization of high-energy part of the spectrum was described. As for now, the most reliable estimated $\\gamma$- spectrum for $^{113}$Cd(n,{x$\\gamma$}) reaction induced by thermal neutrons was presented."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the reconstruction of the high-energy part of the gamma-ray spectrum for thermal neutron capture by 113Cd, which of the following combinations best describes the methodology used for estimating the average gamma-ray spectrum?\n\nA) Averaging frequency polygon for experimental data histogram and using only EMPIRE code calculations\nB) Combining theoretical values at high gamma-ray energies with averaged experimental data in the low-energy range\nC) Using only TALYS code calculations and normalizing to experimental data from Mheemeed et al\nD) Combining theoretical values at low gamma-ray energies with averaged experimental data in the high-energy range\n\nCorrect Answer: D\n\nExplanation: The documentation states that two approaches were considered for estimating the average gamma-ray spectrum. The correct answer, D, accurately describes one of these approaches: \"mean spectra were estimated as combination of theoretical values at low gamma-ray energies and averaging experimental data in high-energy range.\" This method combines theoretical calculations for the low-energy part of the spectrum with experimental data for the high-energy part.\n\nOption A is partially correct as it mentions one of the approaches (averaging frequency polygon for experimental data histogram) but incorrectly states that only EMPIRE code was used.\n\nOption B is incorrect because it reverses the energy ranges for theoretical and experimental data.\n\nOption C is incorrect because it oversimplifies the methodology, ignoring the combination of theoretical and experimental data, and only mentions one of the experimental sources.\n\nThe correct approach involves a careful combination of theoretical calculations and experimental data, normalized using both EMPIRE and TALYS codes, to produce the most reliable estimated gamma spectrum for the 113Cd(n,x\u03b3) reaction induced by thermal neutrons."}, "58": {"documentation": {"title": "Universal quantum simulation with pre-threshold superconducting qubits:\n  Single-excitation subspace method", "source": "Michael R. Geller, John M. Martinis, Andrew T. Sornborger, Phillip C.\n  Stancil, Emily J. Pritchett, Andrei Galiautdinov", "docs_id": "1210.5260", "section": ["quant-ph", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Universal quantum simulation with pre-threshold superconducting qubits:\n  Single-excitation subspace method. We propose a method for general-purpose quantum computation and simulation that is well suited for today's pre-threshold-fidelity superconducting qubits. This approach makes use of the $n$-dimensional single-excitation subspace (SES) of a system of $n$ tunably coupled qubits. It can be viewed as a nonscalable special case of the standard gate-based quantum computing model, but allows many operations in the unitary group SU($n$) to be implemented by a single application of the Hamiltonian. Our approach bypasses the need to decompose the evolution operator into elementary gates, making large, nontrivial computations possible without error correction. The method is especially well suited for universal quantum simulation, specifically simulation of the Schr\\\"odinger equation with a real but otherwise arbitrary $n \\times n$ Hamiltonian. We argue that a 1000-qubit SES processor, which would require no known improvements in superconducting device technology and which could be built today, should be capable of achieving quantum speedup relative to a petaflop supercomputer. We speculate on the utility and practicality of such a universal quantum simulator."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following statements best describes the key advantage of the Single-Excitation Subspace (SES) method for quantum computation with pre-threshold superconducting qubits?\n\nA) It allows for perfect error correction in superconducting qubit systems.\nB) It enables the implementation of many SU(n) operations with a single Hamiltonian application, bypassing gate decomposition.\nC) It provides a fully scalable approach to quantum computing using current superconducting qubit technology.\nD) It eliminates the need for tunable coupling between qubits in superconducting systems.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The key advantage of the SES method described in the document is that it \"allows many operations in the unitary group SU(n) to be implemented by a single application of the Hamiltonian.\" This approach \"bypasses the need to decompose the evolution operator into elementary gates,\" which is a significant benefit for pre-threshold superconducting qubit systems.\n\nAnswer A is incorrect because the method does not claim to provide perfect error correction. In fact, the document states that this approach allows \"large, nontrivial computations possible without error correction.\"\n\nAnswer C is incorrect because the document explicitly states that this is a \"nonscalable special case of the standard gate-based quantum computing model.\"\n\nAnswer D is incorrect because the method actually relies on \"tunably coupled qubits,\" not eliminating the need for tunable coupling.\n\nThis question tests the reader's understanding of the core concept and advantages of the SES method as presented in the document, requiring careful comprehension of the technical details provided."}, "59": {"documentation": {"title": "Multipath-Enhanced Device-Free Localization in Wideband Wireless\n  Networks", "source": "Martin Schmidhammer, Christian Gentner, Stephan Sand, Uwe-Carsten\n  Fiebig", "docs_id": "2010.04531", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multipath-Enhanced Device-Free Localization in Wideband Wireless\n  Networks. State-of-the-art device-free localization systems infer presence and location of users based on received signal strength measurements of line-of-sight links in wireless networks. In this letter, we propose to enhance device-free localization systems by exploiting multipath propagation between the individual network nodes. Particularly indoors, wireless propagation channels are characterized by multipath propagation, i.e., received signals comprise multipath components due to reflection and scattering. Given prior information about the surrounding environment, e.g., a floor plan, the individual propagation paths of multipath components can be derived geometrically. Inherently, these propagation paths differ spatially from the line-of-sight propagation path and can be considered as additional links in the wireless network. This extended network determines the novel multipath-enhanced device-free localization system. Using theoretical performance bounds on the localization error, we show that including multipath components into device-free localization systems improves the overall localization performance and extends the effective observation area significantly."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the primary innovation and advantage of the multipath-enhanced device-free localization system proposed in this research?\n\nA) It eliminates the need for wireless network nodes, relying solely on environmental reflections for localization.\n\nB) It uses machine learning algorithms to predict user movements, improving accuracy over traditional systems.\n\nC) It incorporates multipath components as additional virtual links in the network, extending the effective observation area and improving localization performance.\n\nD) It requires users to carry specialized devices that interact with multipath signals for more precise localization.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation described in the document is the exploitation of multipath propagation in wireless networks for device-free localization. By considering multipath components (reflections and scattering) as additional virtual links in the network, the system extends beyond just line-of-sight measurements. This approach significantly extends the effective observation area and improves overall localization performance, as demonstrated by theoretical performance bounds on localization error.\n\nOption A is incorrect because the system still relies on wireless network nodes; it doesn't eliminate them.\nOption B is incorrect as the document doesn't mention machine learning algorithms or movement prediction.\nOption D is incorrect because the system is described as \"device-free,\" meaning it doesn't require users to carry any specialized equipment."}}