{"0": {"documentation": {"title": "Learning Context-Dependent Choice Functions", "source": "Karlson Pfannschmidt, Pritha Gupta, Bj\\\"orn Haddenhorst, Eyke\n  H\\\"ullermeier", "docs_id": "1901.10860", "section": ["cs.LG", "cs.NE", "econ.GN", "q-fin.EC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Context-Dependent Choice Functions. Choice functions accept a set of alternatives as input and produce a preferred subset of these alternatives as output. We study the problem of learning such functions under conditions of context-dependence of preferences, which means that the preference in favor of a certain choice alternative may depend on what other options are also available. In spite of its practical relevance, this kind of context-dependence has received little attention in preference learning so far. We propose a suitable model based on context-dependent (latent) utility functions, thereby reducing the problem to the task of learning such utility functions. Practically, this comes with a number of challenges. For example, the set of alternatives provided as input to a choice function can be of any size, and the output of the function should not depend on the order in which the alternatives are presented. To meet these requirements, we propose two general approaches based on two representations of context-dependent utility functions, as well as instantiations in the form of appropriate end-to-end trainable neural network architectures. Moreover, to demonstrate the performance of both networks, we present extensive empirical evaluations on both synthetic and real-world datasets."}, "answer": "Question: Which of the following statements best describes the key challenge addressed in the study of learning context-dependent choice functions?\n\nA) The difficulty of collecting large datasets for preference learning\nB) The need for choice functions to handle input sets of varying sizes and be order-independent\nC) The computational complexity of training neural networks for preference learning\nD) The challenge of visualizing high-dimensional preference data\n\nCorrect Answer: B\n\nExplanation: The key challenge addressed in the study is the need for choice functions to handle input sets of varying sizes and be order-independent. This is explicitly mentioned in the text: \"Practically, this comes with a number of challenges. For example, the set of alternatives provided as input to a choice function can be of any size, and the output of the function should not depend on the order in which the alternatives are presented.\"\n\nOption A is incorrect because while data collection can be challenging in preference learning, it's not specifically highlighted as the key challenge in this context.\n\nOption C is not accurate because although computational complexity might be a concern, it's not emphasized as the primary challenge in the given text.\n\nOption D is not mentioned at all in the provided information and is therefore incorrect.\n\nThe correct answer, B, directly addresses the practical challenges discussed in implementing context-dependent choice functions, which is a central focus of the research described."}, "1": {"documentation": {"title": "Responsible Scoring Mechanisms Through Function Sampling", "source": "Abolfazl Asudeh and H. V. Jagadish", "docs_id": "1911.10073", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Responsible Scoring Mechanisms Through Function Sampling. Human decision-makers often receive assistance from data-driven algorithmic systems that provide a score for evaluating objects, including individuals. The scores are generated by a function (mechanism) that takes a set of features as input and generates a score.The scoring functions are either machine-learned or human-designed and can be used for different decision purposes such as ranking or classification. Given the potential impact of these scoring mechanisms on individuals' lives and on society, it is important to make sure these scores are computed responsibly. Hence we need tools for responsible scoring mechanism design. In this paper, focusing on linear scoring functions, we highlight the importance of unbiased function sampling and perturbation in the function space for devising such tools. We provide unbiased samplers for the entire function space, as well as a $\\theta$-vicinity around a given function. We then illustrate the value of these samplers for designing effective algorithms in three diverse problem scenarios in the context of ranking. Finally, as a fundamental method for designing responsible scoring mechanisms, we propose a novel approach for approximating the construction of the arrangement of hyperplanes. Despite the exponential complexity of an arrangement in the number of dimensions, using function sampling, our algorithm is linear in the number of samples and hyperplanes, and independent of the number of dimensions."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the importance and application of unbiased function sampling in responsible scoring mechanism design, as discussed in the Arxiv paper?\n\nA) It allows for the creation of more complex, non-linear scoring functions that are inherently unbiased.\n\nB) It provides a method for sampling from the entire function space and a \u03b8-vicinity around a given function, enabling the development of tools for responsible scoring mechanism design.\n\nC) It eliminates the need for human oversight in algorithmic decision-making systems by ensuring perfect fairness.\n\nD) It reduces the computational complexity of scoring mechanisms to a constant time, regardless of the number of features or dimensions.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper emphasizes the importance of unbiased function sampling in responsible scoring mechanism design. It specifically mentions providing \"unbiased samplers for the entire function space, as well as a \u03b8-vicinity around a given function.\" This sampling approach is crucial for developing tools to ensure responsible scoring mechanisms.\n\nOption A is incorrect because the paper focuses on linear scoring functions, not complex non-linear ones.\n\nOption C is incorrect as the paper does not claim to eliminate the need for human oversight or ensure perfect fairness. It aims to provide tools for more responsible mechanism design, not to replace human judgment entirely.\n\nOption D is incorrect because while the paper mentions an algorithm that is linear in the number of samples and hyperplanes and independent of dimensions, it does not claim to reduce complexity to constant time for all scoring mechanisms."}, "2": {"documentation": {"title": "Robust Likelihood Ratio Tests for Incomplete Economic Models", "source": "Hiroaki Kaido, Yi Zhang", "docs_id": "1910.04610", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Likelihood Ratio Tests for Incomplete Economic Models. This study develops a framework for testing hypotheses on structural parameters in incomplete models. Such models make set-valued predictions and hence do not generally yield a unique likelihood function. The model structure, however, allows us to construct tests based on the least favorable pairs of likelihoods using the theory of Huber and Strassen (1973). We develop tests robust to model incompleteness that possess certain optimality properties. We also show that sharp identifying restrictions play a role in constructing such tests in a computationally tractable manner. A framework for analyzing the local asymptotic power of the tests is developed by embedding the least favorable pairs into a model that allows local approximations under the limits of experiments argument. Examples of the hypotheses we consider include those on the presence of strategic interaction effects in discrete games of complete information. Monte Carlo experiments demonstrate the robust performance of the proposed tests."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of testing hypotheses on structural parameters in incomplete economic models, which of the following statements is most accurate regarding the approach developed in this study?\n\nA) The framework relies on unique likelihood functions to construct robust tests.\n\nB) The tests are based on the most favorable pairs of likelihoods using Huber and Strassen's theory.\n\nC) Sharp identifying restrictions are irrelevant in constructing computationally tractable tests.\n\nD) The approach uses least favorable pairs of likelihoods and incorporates sharp identifying restrictions for tractable test construction.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The study develops a framework for testing hypotheses on structural parameters in incomplete models, which make set-valued predictions and don't generally yield a unique likelihood function. The approach uses least favorable pairs of likelihoods, based on Huber and Strassen's theory (1973), to construct tests robust to model incompleteness. Additionally, the documentation explicitly states that sharp identifying restrictions play a role in constructing such tests in a computationally tractable manner.\n\nOption A is incorrect because incomplete models do not yield unique likelihood functions, as stated in the text.\n\nOption B is wrong because the tests are based on the least favorable pairs, not the most favorable pairs of likelihoods.\n\nOption C is incorrect because the study emphasizes that sharp identifying restrictions are important for constructing computationally tractable tests."}, "3": {"documentation": {"title": "Mapping Organization Knowledge Network and Social Media Based Reputation\n  Management", "source": "Andry Alamsyah, Maribella Syawiluna", "docs_id": "2102.12337", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mapping Organization Knowledge Network and Social Media Based Reputation\n  Management. Knowledge management is an important aspect of an organization, especially in the ICT industry. Having more control of it is essentials for the organization to stay competitive in the business. One way to assess the organization's knowledge capital is by measuring employee knowledge networks and their personal reputation in social media. Using this measurement, we see how employees build relationships around their peer networks or clients virtually. We are also able to see how knowledge networks support organizational performance. The research objective is to map knowledge network and reputation formulation in order to fully understand how knowledge flow and whether employee reputation has a higher degree of influence in the organization's knowledge network. We particularly develop formulas to measure knowledge networks and personal reputation based on their social media activities. As a case study, we pick an Indonesian ICT company that actively build their business around their employee peer knowledge outside the company. For the knowledge network, we perform data collection by conducting interviews. For reputation management, we collect data from several popular social media. We base our work on Social Network Analysis (SNA) methodology. The result shows that employees' knowledge is directly proportional to their reputation, but there are different reputations level on different social media observed in this research."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the relationship between an employee's knowledge and reputation as found in the study, and what additional insight does it provide about reputation across different social media platforms?\n\nA) Employee knowledge is inversely proportional to their reputation, and reputation levels are consistent across all social media platforms.\n\nB) Employee knowledge is directly proportional to their reputation, and reputation levels are consistent across all social media platforms.\n\nC) Employee knowledge is directly proportional to their reputation, but reputation levels vary across different social media platforms.\n\nD) There is no clear relationship between employee knowledge and reputation, but reputation levels vary across different social media platforms.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text explicitly states, \"The result shows that employees' knowledge is directly proportional to their reputation, but there are different reputations level on different social media observed in this research.\" This indicates that while there is a positive correlation between an employee's knowledge and their overall reputation, the level of reputation can vary depending on the specific social media platform being examined. Options A and B are incorrect because they misstate the relationship between knowledge and reputation or the consistency of reputation across platforms. Option D is incorrect because it contradicts the stated relationship between knowledge and reputation in the text."}, "4": {"documentation": {"title": "Importance of initial and final state effects for azimuthal correlations\n  in p+Pb collisions", "source": "Moritz Greif, Carsten Greiner, Bj\\\"orn Schenke, S\\\"oren Schlichting,\n  Zhe Xu", "docs_id": "1903.00314", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Importance of initial and final state effects for azimuthal correlations\n  in p+Pb collisions. We study the influence and interplay of initial state and final state effects in the dynamics of small systems, focusing on azimuthal correlations at different multiplicities. To this end we introduce a new model, matching the classical Yang-Mills dynamics of pre-equilibrium gluon fields (IP-GLASMA) to a perturbative QCD based parton cascade for the final state evolution (BAMPS) on an event-by-event basis. Depending on multiplicity of the event, we see transverse momentum dependent signatures of the initial, but also the final state in azimuthal correlation observables, such as $v_2\\left\\lbrace 2PC\\right\\rbrace(p_T)$. In low-multiplicity events, initial state correlations dominate for transverse momenta $p_T>2~\\mathrm{GeV}$, whereas in high-multiplicity events and at low momenta final state interactions dominate and initial state correlations strongly affect $v_2\\left\\lbrace 2PC\\right\\rbrace(p_T)$ for $p_T>2~\\mathrm{GeV}$ as well as the $p_T$ integrated $v_2\\left\\lbrace 2PC\\right\\rbrace$. Nearly half of the final pT integrated $v_2\\left\\lbrace 2PC\\right\\rbrace$ is contributed by the initial state in low-multiplicity events, whereas in high-multiplicity the share is much less. Based on Ref. [1], we are now able to carry out a systematic multiplicity scan, probing the dynamics on the border of initial state dominated to final state dominated - but not yet hydrodynamic regime."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the study of azimuthal correlations in p+Pb collisions, how do initial and final state effects contribute to v\u2082{2PC}(pT) in different multiplicity events?\n\nA) In low-multiplicity events, final state interactions dominate for all pT ranges, while in high-multiplicity events, initial state correlations are dominant.\n\nB) Initial state correlations dominate for pT > 2 GeV in both low and high-multiplicity events, with final state interactions playing a minimal role.\n\nC) In low-multiplicity events, initial state correlations dominate for pT > 2 GeV, while in high-multiplicity events, final state interactions dominate at low pT and initial state correlations strongly affect v\u2082{2PC}(pT) for pT > 2 GeV.\n\nD) Final state interactions are equally important in both low and high-multiplicity events, with initial state correlations having negligible impact on v\u2082{2PC}(pT) across all pT ranges.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the findings described in the documentation. In low-multiplicity events, initial state correlations dominate for transverse momenta pT > 2 GeV. In high-multiplicity events, final state interactions dominate at low momenta, while initial state correlations strongly affect v\u2082{2PC}(pT) for pT > 2 GeV. This answer captures the nuanced interplay between initial and final state effects in different multiplicity regimes and pT ranges, as described in the given text."}, "5": {"documentation": {"title": "Measuring the Utilization of Public Open Spaces by Deep Learning: a\n  Benchmark Study at the Detroit Riverfront", "source": "Peng Sun, Rui Hou, Jerome Lynch", "docs_id": "2002.01461", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measuring the Utilization of Public Open Spaces by Deep Learning: a\n  Benchmark Study at the Detroit Riverfront. Physical activities and social interactions are essential activities that ensure a healthy lifestyle. Public open spaces (POS), such as parks, plazas and greenways, are key environments that encourage those activities. To evaluate a POS, there is a need to study how humans use the facilities within it. However, traditional approaches to studying use of POS are manual and therefore time and labor intensive. They also may only provide qualitative insights. It is appealing to make use of surveillance cameras and to extract user-related information through computer vision. This paper proposes a proof-of-concept deep learning computer vision framework for measuring human activities quantitatively in POS and demonstrates a case study of the proposed framework using the Detroit Riverfront Conservancy (DRFC) surveillance camera network. A custom image dataset is presented to train the framework; the dataset includes 7826 fully annotated images collected from 18 cameras across the DRFC park space under various illumination conditions. Dataset analysis is also provided as well as a baseline model for one-step user localization and activity recognition. The mAP results are 77.5\\% for {\\it pedestrian} detection and 81.6\\% for {\\it cyclist} detection. Behavioral maps are autonomously generated by the framework to locate different POS users and the average error for behavioral localization is within 10 cm."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: What combination of factors makes the proposed deep learning computer vision framework in this study particularly innovative for measuring human activities in public open spaces?\n\nA) It uses surveillance cameras, provides qualitative insights, and has a 77.5% mAP for pedestrian detection.\n\nB) It's manual, time-intensive, and generates behavioral maps with an average error within 10 cm.\n\nC) It employs deep learning, offers quantitative measurements, and autonomously generates behavioral maps with high accuracy.\n\nD) It's a proof-of-concept, uses 18 cameras, and has an 81.6% mAP for cyclist detection.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it captures the key innovative aspects of the framework described in the document. The framework employs deep learning techniques for computer vision, which is a departure from traditional manual methods. It offers quantitative measurements of human activities in public open spaces, which addresses the limitation of qualitative-only insights from traditional approaches. Finally, the framework autonomously generates behavioral maps with high accuracy (average error within 10 cm), which is a significant capability for studying space utilization.\n\nOption A is incorrect because while it mentions the use of surveillance cameras and the mAP for pedestrian detection, it incorrectly states that the framework provides qualitative insights, when in fact it offers quantitative measurements.\n\nOption B is incorrect because it describes the framework as manual and time-intensive, which are characteristics of traditional approaches that this new framework aims to improve upon.\n\nOption D, while mentioning some correct details about the study, does not capture the core innovative aspects of the framework that make it a significant improvement over traditional methods for studying public open space utilization."}, "6": {"documentation": {"title": "Reinforced Deep Markov Models With Applications in Automatic Trading", "source": "Tadeu A. Ferreira", "docs_id": "2011.04391", "section": ["q-fin.TR", "cs.LG", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reinforced Deep Markov Models With Applications in Automatic Trading. Inspired by the developments in deep generative models, we propose a model-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed to integrate desirable properties of a reinforcement learning algorithm acting as an automatic trading system. The network architecture allows for the possibility that market dynamics are partially visible and are potentially modified by the agent's actions. The RDMM filters incomplete and noisy data, to create better-behaved input data for RL planning. The policy search optimisation also properly accounts for state uncertainty. Due to the complexity of the RKDF model architecture, we performed ablation studies to understand the contributions of individual components of the approach better. To test the financial performance of the RDMM we implement policies using variants of Q-Learning, DynaQ-ARIMA and DynaQ-LSTM algorithms. The experiments show that the RDMM is data-efficient and provides financial gains compared to the benchmarks in the optimal execution problem. The performance improvement becomes more pronounced when price dynamics are more complex, and this has been demonstrated using real data sets from the limit order book of Facebook, Intel, Vodafone and Microsoft."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the key advantages of the Reinforced Deep Markov Model (RDMM) as presented in the research?\n\nA) It exclusively focuses on fully observable market dynamics and assumes the agent's actions have no impact on the market.\n\nB) It relies solely on raw market data without any filtering or preprocessing for reinforcement learning.\n\nC) It integrates deep generative models with reinforcement learning, accounts for partially observable market states, and considers the potential impact of the agent's actions on market dynamics.\n\nD) It is designed specifically for Q-Learning algorithms and cannot be used with other reinforcement learning approaches.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key features and advantages of the RDMM as described in the documentation. The RDMM integrates deep generative models with reinforcement learning, which is mentioned in the opening sentence. It also accounts for partially observable market states, as indicated by the statement \"The network architecture allows for the possibility that market dynamics are partially visible.\" Additionally, it considers the potential impact of the agent's actions on market dynamics, as evidenced by \"and are potentially modified by the agent's actions.\"\n\nAnswer A is incorrect because it contradicts the document's description of RDMM handling partially observable states and considering the agent's impact on the market. Answer B is wrong because the documentation explicitly states that RDMM filters incomplete and noisy data to create better-behaved input for RL planning. Answer D is too narrow and incorrect, as the document mentions that RDMM was tested with various algorithms including Q-Learning, DynaQ-ARIMA, and DynaQ-LSTM, not just Q-Learning."}, "7": {"documentation": {"title": "Carboneyane: A nodal line topological carbon with sp-sp2-sp3 chemical\n  bonds", "source": "Jing-Yang You, Xing-Yu Ma, Zhen Zhang, Kuan-Rong Hao, Qing-Bo Yan,\n  Xian-Lei Sheng, and Gang Su", "docs_id": "1812.11095", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Carboneyane: A nodal line topological carbon with sp-sp2-sp3 chemical\n  bonds. A structurally stable carbon allotrope with plentiful topological properties is predicted by means of first-principles calculations. This novel carbon allotrope possesses the simple space group C2/m, and contains simultaneously sp, sp2 and sp3 hybridized bonds in one structure, which is thus coined as carboneyane. The calculations on geometrical, vibrational, and electronic properties reveal that carboneyane, with good ductility and a much lower density 1.43 g/cm3, is a topological metal with a pair of nodal lines traversing the whole Brillouin zone, such that they can only be annihilated in a pair when symmetry is preserved. The symmetry and topological protections of the nodal lines as well as the associated surface states are discussed. By comparing its x-ray diffraction pattern with experimental results, we find that three peaks of carboneyane meet with the detonation soot. On account of the fluffy structure, carboneyane is shown to have potential applications in areas of storage, adsorption and electrode materials."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Carboneyane, a novel carbon allotrope, exhibits unique properties. Which of the following statements is NOT correct regarding carboneyane?\n\nA) It contains a combination of sp, sp2, and sp3 hybridized bonds in its structure.\nB) It has a density of 1.43 g/cm3 and is classified as a topological insulator.\nC) It possesses a pair of nodal lines traversing the entire Brillouin zone.\nD) Its x-ray diffraction pattern shows three peaks that match with detonation soot.\n\nCorrect Answer: B\n\nExplanation: \nA is correct: The passage explicitly states that carboneyane \"contains simultaneously sp, sp2 and sp3 hybridized bonds in one structure.\"\n\nB is incorrect: While the density of 1.43 g/cm3 is correct, carboneyane is described as a \"topological metal\" in the text, not a topological insulator. This makes B the incorrect statement.\n\nC is correct: The passage mentions that carboneyane has \"a pair of nodal lines traversing the whole Brillouin zone.\"\n\nD is correct: The text states, \"By comparing its x-ray diffraction pattern with experimental results, we find that three peaks of carboneyane meet with the detonation soot.\"\n\nThis question tests the student's ability to carefully read and interpret complex scientific information, distinguishing between correct and incorrect statements based on the given text."}, "8": {"documentation": {"title": "Active Importance Sampling for Variational Objectives Dominated by Rare\n  Events: Consequences for Optimization and Generalization", "source": "Grant M. Rotskoff and Andrew R. Mitchell and Eric Vanden-Eijnden", "docs_id": "2008.06334", "section": ["physics.data-an", "cond-mat.stat-mech", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active Importance Sampling for Variational Objectives Dominated by Rare\n  Events: Consequences for Optimization and Generalization. Deep neural networks, when optimized with sufficient data, provide accurate representations of high-dimensional functions; in contrast, function approximation techniques that have predominated in scientific computing do not scale well with dimensionality. As a result, many high-dimensional sampling and approximation problems once thought intractable are being revisited through the lens of machine learning. While the promise of unparalleled accuracy may suggest a renaissance for applications that require parameterizing representations of complex systems, in many applications gathering sufficient data to develop such a representation remains a significant challenge. Here we introduce an approach that combines rare events sampling techniques with neural network optimization to optimize objective functions that are dominated by rare events. We show that importance sampling reduces the asymptotic variance of the solution to a learning problem, suggesting benefits for generalization. We study our algorithm in the context of learning dynamical transition pathways between two states of a system, a problem with applications in statistical physics and implications in machine learning theory. Our numerical experiments demonstrate that we can successfully learn even with the compounding difficulties of high-dimension and rare data."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation and potential impact of the approach introduced in the paper?\n\nA) It combines deep neural networks with importance sampling to optimize objective functions dominated by common events, potentially revolutionizing low-dimensional sampling problems.\n\nB) It merges rare events sampling techniques with neural network optimization to tackle objective functions dominated by rare events, potentially enabling learning in high-dimensional spaces with limited data.\n\nC) It introduces a new type of neural network architecture specifically designed for rare event detection, without any connection to importance sampling or optimization techniques.\n\nD) It proposes a method to replace deep neural networks with traditional scientific computing techniques for high-dimensional function approximation, improving scalability.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper introduces an approach that combines rare events sampling techniques with neural network optimization to optimize objective functions dominated by rare events. This innovation addresses the challenge of learning in high-dimensional spaces where data, particularly for rare events, is limited. \n\nAnswer A is incorrect because it mentions common events instead of rare events, which is the focus of the paper. \n\nAnswer C is incorrect because while the approach does deal with rare events, it doesn't introduce a new neural network architecture. Instead, it combines existing techniques (rare events sampling and neural network optimization) in a novel way.\n\nAnswer D is incorrect because the paper actually leverages the strengths of deep neural networks for high-dimensional problems, rather than replacing them with traditional scientific computing techniques.\n\nThe correct answer (B) captures the essence of the paper's contribution: addressing the challenge of learning in high-dimensional spaces with limited data, particularly when dealing with rare events, by combining rare events sampling with neural network optimization."}, "9": {"documentation": {"title": "Automatic Aggregation by Joint Modeling of Aspects and Values", "source": "Christina Sauper, Regina Barzilay", "docs_id": "1401.6422", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automatic Aggregation by Joint Modeling of Aspects and Values. We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis."}, "answer": "Question: Which of the following best describes the primary innovation and capabilities of the model presented in the Arxiv documentation?\n\nA) It focuses solely on sentiment analysis of product reviews without considering aspects.\nB) It performs aspect identification on medical summaries but cannot handle sentiment analysis.\nC) It jointly models aspect identification and sentiment analysis for product reviews, enabling discovery of highly-rated or inconsistent aspects.\nD) It uses a non-generative approach that requires extensive manual labeling of training data.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation clearly states that the model \"simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect.\" This joint modeling of aspects and sentiments is the key innovation, allowing for the discovery of highly-rated or inconsistent aspects of a product.\n\nOption A is incorrect because the model doesn't focus solely on sentiment analysis; it combines this with aspect identification.\n\nOption B is partially correct in that the model can perform aspect identification on medical summaries, but it's not limited to this. The model can also handle sentiment analysis for product reviews, which this option incorrectly excludes.\n\nOption D is incorrect because the documentation explicitly states that it's a generative model, not a non-generative approach. Additionally, there's no mention of extensive manual labeling of training data."}, "10": {"documentation": {"title": "Site-specific Forest-assembly of Single-Wall Carbon Nanotubes on\n  Electron-beam Patterned SiOx/Si Substrates", "source": "Haoyan Wei, Sang Nyon Kim, Sejong Kim, Bryan D. Huey, Fotios\n  Papadimitrakopoulos, Harris L. Marcus", "docs_id": "1105.1598", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Site-specific Forest-assembly of Single-Wall Carbon Nanotubes on\n  Electron-beam Patterned SiOx/Si Substrates. Based on electron-beam direct writing on the SiOx/Si substrates, favorable absorption sites for ferric cations (Fe3+ ions) were created on the surface oxide layer. This allowed Fe3+-assisted self-assembled arrays of single-wall carbon nanotube (SWNT) probes to be produced. Auger investigation indicated that the incident energetic electrons depleted oxygen, creating more dangling bonds around Si atoms at the surface of the SiOx layer. This resulted in a distinct difference in the friction forces from unexposed regions as measured by lateral force microscopy (LFM). Atomic force microscopy (AFM) affirmed that the irradiated domains absorbed considerably more Fe3+ ions upon immersion into pH 2.2 aqueous FeCl3 solution. This rendered a greater yield of FeO(OH)/FeOCl precipitates, primarily FeO(OH), upon subsequent washing with lightly basic dimethylformamide (DMF) solution. Such selective metalfunctionalization established the basis for the subsequent patterned forest-assembly of SWNTs as demonstrated by resonance Raman spectroscopy."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which sequence of processes best describes the site-specific forest-assembly of single-wall carbon nanotubes (SWNTs) on electron-beam patterned SiOx/Si substrates?\n\nA) Electron-beam irradiation \u2192 Fe3+ ion absorption \u2192 DMF washing \u2192 SWNT assembly\nB) Fe3+ ion absorption \u2192 Electron-beam irradiation \u2192 SWNT assembly \u2192 DMF washing\nC) Electron-beam irradiation \u2192 Fe3+ ion absorption \u2192 SWNT assembly \u2192 DMF washing\nD) Electron-beam irradiation \u2192 DMF washing \u2192 Fe3+ ion absorption \u2192 SWNT assembly\n\nCorrect Answer: A\n\nExplanation: The correct sequence is A) Electron-beam irradiation \u2192 Fe3+ ion absorption \u2192 DMF washing \u2192 SWNT assembly. \n\nFirst, electron-beam direct writing is used to create favorable absorption sites for Fe3+ ions by depleting oxygen and creating dangling bonds around Si atoms on the SiOx surface. \n\nNext, the substrate is immersed in an aqueous FeCl3 solution, allowing the irradiated areas to absorb more Fe3+ ions. \n\nThen, the substrate is washed with a lightly basic DMF solution, which leads to the formation of FeO(OH)/FeOCl precipitates, primarily FeO(OH).\n\nFinally, this selective metal-functionalization establishes the basis for the subsequent patterned forest-assembly of SWNTs.\n\nOptions B, C, and D are incorrect because they do not follow the proper sequence of events described in the documentation. The DMF washing step, in particular, must occur after Fe3+ ion absorption and before SWNT assembly, which eliminates these options."}, "11": {"documentation": {"title": "A McKean-Vlasov game of commodity production, consumption and trading", "source": "Ren\\'e A\\\"id, Ofelia Bonesini, Giorgia Callegaro, Luciano Campi", "docs_id": "2111.04391", "section": ["math.OC", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A McKean-Vlasov game of commodity production, consumption and trading. We propose a model where a producer and a consumer can affect the price dynamics of some commodity controlling drift and volatility of, respectively, the production rate and the consumption rate. We assume that the producer has a short position in a forward contract on \\lambda units of the underlying at a fixed price F, while the consumer has the corresponding long position. Moreover, both players are risk-averse with respect to their financial position and their risk aversions are modelled through an integrated-variance penalization. We study the impact of risk aversion on the interaction between the producer and the consumer as well as on the derivative price. In mathematical terms, we are dealing with a two-player linear-quadratic McKean-Vlasov stochastic differential game. Using methods based on the martingale optimality principle and BSDEs, we find a Nash equilibrium and characterize the corresponding strategies and payoffs in semi-explicit form. Furthermore, we compute the two indifference prices (one for the producer and one for the consumer) induced by that equilibrium and we determine the quantity \\lambda such that the players agree on the price. Finally, we illustrate our results with some numerics. In particular, we focus on how the risk aversions and the volatility control costs of the players affect the derivative price."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the McKean-Vlasov game of commodity production, consumption and trading described, which of the following statements is NOT correct regarding the model and its analysis?\n\nA) The producer controls the drift of the production rate, while the consumer controls the volatility of the consumption rate.\n\nB) Both players are risk-averse, modeled through an integrated-variance penalization.\n\nC) The Nash equilibrium strategies and payoffs are characterized in semi-explicit form using martingale optimality principle and BSDEs.\n\nD) The model determines the quantity \u03bb such that the producer and consumer agree on the derivative price.\n\nCorrect Answer: A\n\nExplanation: The correct answer is A because it incorrectly states the controls of the players. According to the text, the producer controls the drift and volatility of the production rate, while the consumer controls the drift and volatility of the consumption rate. The other statements are correct:\n\nB is correct as the text explicitly states that both players are risk-averse and their risk aversions are modeled through an integrated-variance penalization.\n\nC is correct as the text mentions using methods based on the martingale optimality principle and BSDEs to find a Nash equilibrium and characterize the corresponding strategies and payoffs in semi-explicit form.\n\nD is correct as the text states that they determine the quantity \u03bb such that the players agree on the price.\n\nThis question tests the student's careful reading and understanding of the complex model described in the text, particularly the roles and controls of the players in the game."}, "12": {"documentation": {"title": "Joint System and Algorithm Design for Computationally Efficient Fan Beam\n  Coded Aperture X-ray Coherent Scatter Imaging", "source": "Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P.\n  MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani\n  Krishnamurthy, Anuj Kapadia, Lawrence Carin, and David J. Brady", "docs_id": "1603.06400", "section": ["cs.CV", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint System and Algorithm Design for Computationally Efficient Fan Beam\n  Coded Aperture X-ray Coherent Scatter Imaging. In x-ray coherent scatter tomography, tomographic measurements of the forward scatter distribution are used to infer scatter densities within a volume. A radiopaque 2D pattern placed between the object and the detector array enables the disambiguation between different scatter events. The use of a fan beam source illumination to speed up data acquisition relative to a pencil beam presents computational challenges. To facilitate the use of iterative algorithms based on a penalized Poisson log-likelihood function, efficient computational implementation of the forward and backward models are needed. Our proposed implementation exploits physical symmetries and structural properties of the system and suggests a joint system-algorithm design, where the system design choices are influenced by computational considerations, and in turn lead to reduced reconstruction time. Computational-time speedups of approximately 146 and 32 are achieved in the computation of the forward and backward models, respectively. Results validating the forward model and reconstruction algorithm are presented on simulated analytic and Monte Carlo data."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of fan beam coded aperture X-ray coherent scatter imaging, which of the following statements best describes the key innovation and its primary benefit as presented in the research?\n\nA) The use of a pencil beam source illumination to speed up data acquisition, resulting in reduced computational challenges.\n\nB) The implementation of a 3D radiopaque pattern between the object and detector array, enabling faster tomographic measurements.\n\nC) The development of a joint system-algorithm design that exploits physical symmetries and structural properties, leading to significant computational speedups.\n\nD) The application of non-iterative algorithms based on a modified Gaussian likelihood function, resulting in more efficient forward and backward models.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation presented in the research is the development of a joint system-algorithm design that exploits physical symmetries and structural properties of the system. This approach leads to significant computational speedups in both the forward and backward models, with reported speedups of approximately 146 and 32 times, respectively.\n\nAnswer A is incorrect because the research mentions using a fan beam source illumination to speed up data acquisition compared to a pencil beam, not the other way around.\n\nAnswer B is incorrect because the research refers to a 2D radiopaque pattern, not a 3D pattern. Additionally, while this pattern helps disambiguate scatter events, it's not the primary innovation leading to computational efficiency.\n\nAnswer D is incorrect because the research mentions using iterative algorithms based on a penalized Poisson log-likelihood function, not non-iterative algorithms or a Gaussian likelihood function.\n\nThe correct answer emphasizes the joint system-algorithm design approach, which is the core innovation that enables more efficient computational implementation and faster reconstruction times in fan beam coded aperture X-ray coherent scatter imaging."}, "13": {"documentation": {"title": "Gini Index based Initial Coin Offering Mechanism", "source": "Mingyu Guo, Zhenghui Wang, Yuko Sakurai", "docs_id": "2002.11387", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gini Index based Initial Coin Offering Mechanism. As a fundraising method, Initial Coin Offering (ICO) has raised billions of dollars for thousands of startups in the past two years. Existing ICO mechanisms place more emphasis on the short-term benefits of maximal fundraising while ignoring the problem of unbalanced token allocation, which negatively impacts subsequent fundraising and has bad effects on introducing new investors and resources. We propose a new ICO mechanism which uses the concept of Gini index for the very first time as a mechanism design constraint to control allocation inequality. Our mechanism maintains an elegant and straightforward structure. It allows the agents to modify their bids as a price discovery process, while limiting the bids of whales. We analyze the agents' equilibrium behaviors under our mechanism. Under natural technical assumptions, we show that most agents have simple dominant strategies and the equilibrium revenue approaches the optimal revenue asymptotically in the number of agents. We verify our mechanism using real ICO dataset we collected, and confirm that our mechanism performs well in terms of both allocation fairness and revenue."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary innovation and benefit of the Gini Index-based Initial Coin Offering (ICO) mechanism as presented in the Arxiv documentation?\n\nA) It maximizes short-term fundraising potential for startups by attracting large investors (\"whales\").\n\nB) It introduces a price discovery process that allows agents to modify their bids freely without any restrictions.\n\nC) It uses the Gini index as a design constraint to balance token allocation fairness while maintaining revenue optimization.\n\nD) It eliminates the need for subsequent fundraising rounds by ensuring optimal initial token distribution.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation of this ICO mechanism is the use of the Gini index as a design constraint to control allocation inequality. This approach aims to balance token allocation fairness with revenue optimization, addressing the problem of unbalanced token distribution that can negatively impact future fundraising and investor attraction.\n\nOption A is incorrect because the mechanism actually aims to limit the bids of \"whales\" (large investors) to prevent unbalanced allocation, rather than maximizing short-term fundraising.\n\nOption B is partially correct in mentioning the price discovery process, but it's inaccurate in stating that there are no restrictions. The mechanism does allow agents to modify bids but also imposes limits on large bids to maintain fairness.\n\nOption D is an overstatement. While the mechanism aims to improve initial token distribution, it doesn't eliminate the need for future fundraising entirely. The goal is to create a more balanced foundation for subsequent fundraising efforts and investor attraction."}, "14": {"documentation": {"title": "UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation", "source": "Pablo Martinez-Gonzalez, Sergiu Oprea, Alberto Garcia-Garcia, Alvaro\n  Jover-Alvarez, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "docs_id": "1810.06936", "section": ["cs.RO", "cs.CV", "cs.MM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation. Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. Those problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary advantage of UnrealROX over traditional synthetic data generation methods for robotics simulations?\n\nA) It provides faster data generation compared to real-world data collection.\nB) It offers perfect annotation accuracy for all generated data.\nC) It combines photorealistic environments with human-controlled robot interactions in virtual reality.\nD) It eliminates the need for any real-world data in robotic vision research.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. UnrealROX's key innovation is its combination of highly photorealistic environments with human-controlled robot interactions in a virtual reality setting. This approach addresses the \"reality gap\" that exists in many synthetic datasets by providing more realistic scenes and interactions.\n\nOption A is partially true but not the primary advantage; while synthetic data generation is generally faster than real-world data collection, this is true for most synthetic environments, not just UnrealROX.\n\nOption B is incorrect. While UnrealROX provides automatic annotation, the text doesn't claim perfect accuracy, and this isn't its main advantage over other synthetic data generation methods.\n\nOption D is incorrect and too extreme. While UnrealROX aims to provide very realistic synthetic data, it doesn't eliminate the need for all real-world data in robotic vision research.\n\nThe correct answer highlights UnrealROX's unique approach of using virtual reality to allow human operators to control robots in hyperrealistic environments, which provides a level of realism and interaction that surpasses traditional synthetic data generation methods."}, "15": {"documentation": {"title": "Long range correlations and folding angle in polymers with applications\n  to {\\alpha}-helical proteins", "source": "Andrey Krokhotin, Stam Nicolis and Antti J. Niemi", "docs_id": "1306.5335", "section": ["physics.bio-ph", "cond-mat.stat-mech", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Long range correlations and folding angle in polymers with applications\n  to {\\alpha}-helical proteins. The conformational complexity of linear polymers far exceeds that of point-like atoms and molecules. Polymers can bend, twist, even become knotted. Thus they may also display a much richer phase structure than point particles. But it is not very easy to characterize the phase of a polymer. Essentially, the only attribute is the radius of gyration. The way how it changes when the degree of polymerization becomes different, and how it evolves when the ambient temperature and solvent properties change, discloses the phase of the polymer. Moreover, in any finite length chain there are corrections to scaling, that complicate the detailed analysis of the phase structure. Here we introduce a quantity that we call the folding angle, a novel tool to identify and scrutinize the phases of polymers. We argue for a mean-field relationship between its values and those of the scaling exponent in the radius of gyration. But unlike in the case of the radius of gyration, the value of the folding angle can be evaluated from a single structure. As an example we estimate the value of the folding angle in the case of crystallographic {\\alpha}-helical protein structures in the Protein Data Bank (PDB). We also show how the value can be numerically computed using a theoretical model of {\\alpha}-helical chiral homopolymers."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: A researcher is studying the phase structure of polymers and wants to use a novel approach to characterize different polymer phases. Which of the following statements best describes the advantages and applications of the \"folding angle\" concept introduced in the text?\n\nA) The folding angle can only be calculated for \u03b1-helical proteins and requires multiple polymer structures for accurate measurement.\n\nB) The folding angle is directly proportional to the radius of gyration and provides the same information about polymer phases.\n\nC) The folding angle can be evaluated from a single structure and has a mean-field relationship with the scaling exponent of the radius of gyration.\n\nD) The folding angle is primarily useful for studying the knotting behavior of polymers and cannot be applied to protein structures.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text introduces the folding angle as a novel tool for identifying and scrutinizing polymer phases. It states that there is a mean-field relationship between the folding angle values and the scaling exponent in the radius of gyration. Importantly, the text emphasizes that \"unlike in the case of the radius of gyration, the value of the folding angle can be evaluated from a single structure.\" This makes it a powerful and efficient tool for polymer phase analysis. The text also mentions its application to \u03b1-helical protein structures in the Protein Data Bank, demonstrating its relevance to protein studies.\n\nOption A is incorrect because the folding angle is not limited to \u03b1-helical proteins and does not require multiple structures. Option B is wrong as the folding angle is not directly proportional to the radius of gyration but has a mean-field relationship with its scaling exponent. Option D is incorrect because the folding angle is not primarily for studying knotting behavior and can indeed be applied to protein structures."}, "16": {"documentation": {"title": "Measurements of $\\gamma \\gamma \\to \\mbox{Higgs}$ and $\\gamma \\gamma \\to\n  W^{+}W^{-}$ in $e^{+}e^{-}$ collisions at the Future Circular Collider", "source": "David d'Enterria, Patricia Rebello Teles, Daniel E. Martins", "docs_id": "1712.07023", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurements of $\\gamma \\gamma \\to \\mbox{Higgs}$ and $\\gamma \\gamma \\to\n  W^{+}W^{-}$ in $e^{+}e^{-}$ collisions at the Future Circular Collider. The measurements of the two-photon production of the Higgs boson and of $W^{\\pm}$ boson pairs in $e^{+}e^{-}$ collisions at the Future Circular Collider (FCC-ee) are investigated. The processes $e^{+}e^{-}\\xrightarrow{\\gamma \\gamma}e^+\\,{\\rm H}\\,e^-,e^+\\,{\\rm W^+W^-}\\,e^-$ are computed using the effective photon approximation for electron-positron beams, and studied in their ${\\rm H}\\to b\\bar{b}$ and ${\\rm W^+W^-}\\to 4j$ decay final-states including parton showering and hadronization, jet reconstruction, $e^\\pm$ forward tagging, and realistic experimental cuts. After selection criteria, up to 75 Higgs bosons and 6600 $\\rm W^{\\pm}$ pairs will be reconstructed on top of controllable continuum backgrounds at $\\sqrt{s} = $240 and 350 GeV for the total expected integrated luminosities, by tagging the scattered $e^\\pm$ with near-beam detectors. A 5$\\sigma$ observation of $\\gamma \\gamma \\to$H is thereby warranted, as well as high-statistics studies of triple $\\rm \\gamma WW$ and quartic $\\rm \\gamma\\gamma WW$ electroweak couplings, improving by at least factors of 2 and 10 the current limits on dimension-6 anomalous quartic gauge couplings."}, "answer": "Question: In the context of the FCC-ee experiments described, which of the following statements is correct regarding the expected outcomes and significance of the measurements?\n\nA) The study predicts the observation of approximately 750 Higgs bosons and 660 W\u00b1 pairs after applying selection criteria.\n\nB) The measurements are expected to improve the current limits on dimension-6 anomalous quartic gauge couplings by factors of 5 and 20 respectively.\n\nC) A 3\u03c3 observation of \u03b3\u03b3 \u2192 H is anticipated, along with moderate-statistics studies of triple \u03b3WW and quartic \u03b3\u03b3WW electroweak couplings.\n\nD) Up to 75 Higgs bosons and 6600 W\u00b1 pairs are expected to be reconstructed, with a 5\u03c3 observation of \u03b3\u03b3 \u2192 H warranted.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that \"After selection criteria, up to 75 Higgs bosons and 6600 W\u00b1 pairs will be reconstructed\" and \"A 5\u03c3 observation of \u03b3\u03b3 \u2192 H is thereby warranted\". This matches exactly with option D.\n\nOption A is incorrect as it significantly overestimates the number of Higgs bosons (750 vs. 75) and underestimates the number of W\u00b1 pairs (660 vs. 6600).\n\nOption B is incorrect because the improvement factors mentioned in the document are \"at least factors of 2 and 10\" for the current limits on dimension-6 anomalous quartic gauge couplings, not 5 and 20.\n\nOption C is incorrect on two counts: it states a 3\u03c3 observation of \u03b3\u03b3 \u2192 H (instead of the correct 5\u03c3) and describes the studies as \"moderate-statistics\" when the document suggests \"high-statistics studies\" of the electroweak couplings."}, "17": {"documentation": {"title": "Theory of polymer translocation through a flickering nanopore under an\n  alternating driving force", "source": "Jalal Sarabadani, Timo Ikonen and Tapio Ala-Nissila", "docs_id": "1505.04057", "section": ["cond-mat.stat-mech", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theory of polymer translocation through a flickering nanopore under an\n  alternating driving force. We develop a theory for polymer translocation driven by a time-dependent force through an oscillating nanopore. To this end, we extend the iso-flux tension propagation theory (IFTP) [Sarabadani \\textit{et al., J. Chem. Phys.}, 2014, \\textbf{141}, 214907] for such a setup. We assume that the external driving force in the pore has a component oscillating in time, and the flickering pore is similarly described by an oscillating term in the pore friction. In addition to numerically solving the model, we derive analytical approximations that are in good agreement with the numerical simulations. Our results show that by controlling either the force or pore oscillations, the translocation process can be either sped up or slowed down depending on the frequency of the oscillations and the characteristic time scale of the process. We also show that while in the low and high frequency limits the translocation time $\\tau$ follows the established scaling relation with respect to chain length $N_0$, in the intermediate frequency regime small periodic fluctuations can have drastic effects on the dynamical scaling. The results can be easily generalized for non-periodic oscillations and elucidate the role of time dependent forces and pore oscillations in driven polymer translocation."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of polymer translocation through a flickering nanopore under an alternating driving force, which of the following statements is true regarding the translocation time \u03c4 and its scaling relation with respect to chain length N0?\n\nA) The scaling relation between \u03c4 and N0 remains constant across all oscillation frequencies of the force and pore.\n\nB) The scaling relation is only affected in the high frequency limit of oscillations.\n\nC) Small periodic fluctuations in the intermediate frequency regime can significantly alter the dynamical scaling between \u03c4 and N0.\n\nD) The scaling relation is only affected in the low frequency limit of oscillations.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of how oscillations in force and pore affect the scaling relation between translocation time \u03c4 and chain length N0. The correct answer is C because the documentation explicitly states: \"in the intermediate frequency regime small periodic fluctuations can have drastic effects on the dynamical scaling.\" \n\nAnswer A is incorrect because the scaling relation does not remain constant across all frequencies. The document mentions that the scaling relation holds in the low and high frequency limits, but changes in the intermediate regime.\n\nAnswers B and D are incorrect because they only mention effects in the high and low frequency limits respectively, while the document specifically highlights the importance of the intermediate frequency regime for altering the scaling relation.\n\nThis question requires careful reading and understanding of the complex relationships between oscillation frequency, translocation time, and chain length described in the document."}, "18": {"documentation": {"title": "A Shell Bonded to an Elastic Foundation", "source": "Kavinda Jayawardana", "docs_id": "2012.12185", "section": ["math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Shell Bonded to an Elastic Foundation. In our analysis, we derive a mathematical model for a shell bonded to an elastic foundation, by modifying Koiter's linear shell equations. We prove the existence and the uniqueness of the solutions, and we explicitly derive the governing equations and the boundary conditions for the general case. Finally, with numerical modelling and asymptotic analyses, we show that there exist optimal values of the Young's modulus, the Poisson's ratio and the thickness of the shell (with respect to the elastic foundation), and the curvature of the contact region such that the planar solution derived by the shell model (where stretching effects are dominant) is a good approximation. It is often regarded in the field of stretchable and flexible electronics that the planar solution is mostly accurate when the stiffness of the thinner body (e.g. the shell) increases indefinitely. The significance of our work is that, as far as we are aware, this is the first analysis showing that indefinitely increasing the stiffness of the shell may not guarantee a more accurate solution."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In the analysis of a shell bonded to an elastic foundation, which of the following statements is most accurate regarding the relationship between the shell's stiffness and the accuracy of the planar solution?\n\nA) Increasing the shell's stiffness indefinitely always leads to a more accurate planar solution.\n\nB) The planar solution is most accurate when the shell's stiffness is minimized.\n\nC) There exist optimal values for the shell's properties (including Young's modulus) that maximize the accuracy of the planar solution.\n\nD) The accuracy of the planar solution is independent of the shell's stiffness.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of a key finding from the research. Option A represents the conventional wisdom in the field of stretchable and flexible electronics, which the research challenges. Option B is incorrect as it contradicts both conventional wisdom and the research findings. Option D is false because the research clearly indicates that the shell's properties, including stiffness, affect the accuracy of the planar solution.\n\nThe correct answer, C, aligns with the research's main conclusion. The study found that there are optimal values for the shell's properties, including Young's modulus (which relates to stiffness), Poisson's ratio, and thickness, that maximize the accuracy of the planar solution. This is a novel finding, as it contradicts the common belief that indefinitely increasing the shell's stiffness always improves the accuracy of the planar solution."}, "19": {"documentation": {"title": "Localization in the Kicked Ising Chain", "source": "Daniel Waltner, Petr Braun", "docs_id": "2101.10057", "section": ["cond-mat.stat-mech", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Localization in the Kicked Ising Chain. Determining the border between ergodic and localized behavior is of central interest for interacting many-body systems. We consider here the recently very popular spin-chain model that is periodically excited. A convenient description of such a many-body system is achieved by the dual operator that evolves the system in contrast to the time-evolution operator not in time but in particle direction. We identify in this paper the largest eigenvalue of a function based on the dual operator as a convenient tool to determine if the system shows ergodic or many-body localized features. By perturbation theory in the vicinity of the noninteracting system we explain analytically the eigenvalue structure and compare it with numerics in [P. Braun, D. Waltner, M. Akila, B. Gutkin, T. Guhr, Phys. Rev. E $\\bf{101}$, 052201 (2020)] for small times. Furthermore we identify a quantity that allows based on extensive large-time numerical computations of the spectral form factor to distinguish between localized and ergodic system features and to determine the Thouless time, i.e. the transition time between these regimes in the thermodynamic limit."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the study of the kicked Ising chain model, which of the following statements is most accurate regarding the method used to determine the border between ergodic and localized behavior?\n\nA) The time-evolution operator is used to directly identify the transition between ergodic and many-body localized features.\n\nB) The largest eigenvalue of a function based on the dual operator, which evolves the system in particle direction rather than time, is utilized as a tool to determine ergodic or many-body localized features.\n\nC) The spectral form factor alone is sufficient to distinguish between localized and ergodic system features without the need for additional analysis.\n\nD) Perturbation theory is applied to the interacting system to analytically explain the eigenvalue structure for large time scales.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation specifically states that \"We identify in this paper the largest eigenvalue of a function based on the dual operator as a convenient tool to determine if the system shows ergodic or many-body localized features.\" This dual operator evolves the system in particle direction rather than time, making it a unique approach to studying this many-body system.\n\nOption A is incorrect because the documentation emphasizes the use of the dual operator, not the time-evolution operator, for this analysis.\n\nOption C is partially true but incomplete. While the spectral form factor is used, it's not sufficient on its own. The documentation mentions that a specific quantity derived from \"extensive large-time numerical computations of the spectral form factor\" is used to distinguish between localized and ergodic features.\n\nOption D is incorrect because the perturbation theory is applied \"in the vicinity of the noninteracting system\" and is used to explain the eigenvalue structure for small times, not large time scales."}, "20": {"documentation": {"title": "Estimating covariance functions of multivariate skew-Gaussian random\n  fields on the sphere", "source": "Alfredo Alegr\\'ia, Sandra Caro, Moreno Bevilacqua, Emilio Porcu and\n  Jorge Clarke", "docs_id": "1612.03341", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating covariance functions of multivariate skew-Gaussian random\n  fields on the sphere. This paper considers a multivariate spatial random field, with each component having univariate marginal distributions of the skew-Gaussian type. We assume that the field is defined spatially on the unit sphere embedded in $\\mathbb{R}^3$, allowing for modeling data available over large portions of planet Earth. This model admits explicit expressions for the marginal and cross covariances. However, the $n$-dimensional distributions of the field are difficult to evaluate, because it requires the sum of $2^n$ terms involving the cumulative and probability density functions of a $n$-dimensional Gaussian distribution. Since in this case inference based on the full likelihood is computationally unfeasible, we propose a composite likelihood approach based on pairs of spatial observations. This last being possible thanks to the fact that we have a closed form expression for the bivariate distribution. We illustrate the effectiveness of the method through simulation experiments and the analysis of a real data set of minimum and maximum temperatures."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: A researcher is studying global temperature patterns using a multivariate spatial random field model on the surface of the Earth. Which of the following statements best describes the challenges and methodologies associated with this approach?\n\nA) The model uses a univariate Gaussian distribution for each component, making it computationally simple to calculate n-dimensional distributions of the field.\n\nB) The full likelihood inference is computationally feasible and is the preferred method for parameter estimation in this model.\n\nC) The model employs a multivariate skew-Gaussian random field on a sphere, with closed-form expressions for marginal and cross covariances, but uses composite likelihood for inference due to computational limitations in calculating n-dimensional distributions.\n\nD) The spatial random field is defined on a flat plane, simplifying calculations but limiting its applicability to small geographical areas.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key aspects of the model and methodology described in the documentation. The model uses a multivariate spatial random field with skew-Gaussian marginal distributions, defined on a sphere to represent the Earth's surface. While it has closed-form expressions for marginal and cross covariances, the n-dimensional distributions are computationally challenging to evaluate. As a result, the researchers propose using a composite likelihood approach based on pairs of spatial observations, leveraging the availability of closed-form bivariate distributions.\n\nOption A is incorrect because the model uses skew-Gaussian, not univariate Gaussian distributions, and the n-dimensional distributions are actually difficult to calculate.\n\nOption B is incorrect because the documentation explicitly states that full likelihood inference is computationally unfeasible for this model.\n\nOption D is incorrect because the model is defined on a sphere (to represent the Earth) rather than a flat plane, allowing it to model data over large portions of the planet."}, "21": {"documentation": {"title": "Uncovering disassortativity in large scale-free networks", "source": "Nelly Litvak and Remco van der Hofstad", "docs_id": "1204.0266", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uncovering disassortativity in large scale-free networks. Mixing patterns in large self-organizing networks, such as the Internet, the World Wide Web, social and biological networks are often characterized by degree-degree dependencies between neighbouring nodes. In this paper we propose a new way of measuring degree-degree dependencies. One of the problems with the commonly used assortativity coefficient is that in disassortative networks its magnitude decreases with the network size. We mathematically explain this phenomenon and validate the results on synthetic graphs and real-world network data. As an alternative, we suggest to use rank correlation measures such as Spearman's rho. Our experiments convincingly show that Spearman's rho produces consistent values in graphs of different sizes but similar structure, and it is able to reveal strong (positive or negative) dependencies in large graphs. In particular, we discover much stronger negative degree-degree dependencies} in Web graphs than was previously thought. {Rank correlations allow us to compare the assortativity of networks of different sizes, which is impossible with the assortativity coefficient due to its genuine dependence on the network size. We conclude that rank correlations provide a suitable and informative method for uncovering network mixing patterns."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In large scale-free networks, why is Spearman's rho suggested as a better measure of degree-degree dependencies compared to the assortativity coefficient?\n\nA) Spearman's rho is easier to calculate for large networks\nB) The assortativity coefficient increases with network size in disassortative networks\nC) Spearman's rho allows for comparison of networks of different sizes, while the assortativity coefficient is size-dependent\nD) Spearman's rho only works for assortative networks, not disassortative ones\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the limitations of the assortativity coefficient and the advantages of using Spearman's rho for measuring degree-degree dependencies in large scale-free networks.\n\nOption A is incorrect because ease of calculation is not mentioned as a reason for preferring Spearman's rho.\n\nOption B is incorrect and actually opposite to what the text states. The documentation mentions that in disassortative networks, the magnitude of the assortativity coefficient decreases (not increases) with network size.\n\nOption C is correct. The text explicitly states that \"Rank correlations allow us to compare the assortativity of networks of different sizes, which is impossible with the assortativity coefficient due to its genuine dependence on the network size.\" Spearman's rho is a rank correlation measure that produces consistent values for graphs of different sizes but similar structure.\n\nOption D is incorrect because Spearman's rho works for both assortative and disassortative networks. The text mentions that it can reveal strong positive or negative dependencies in large graphs.\n\nThis question requires careful reading and understanding of the key points in the documentation, making it suitable for an exam testing comprehension of network analysis concepts."}, "22": {"documentation": {"title": "Routing brain traffic through the von Neumann bottleneck: Efficient\n  cache usage in spiking neural network simulation code on general purpose\n  computers", "source": "Jari Pronold, Jakob Jordan, Brian J. N. Wylie, Itaru Kitayama, Markus\n  Diesmann, Susanne Kunkel", "docs_id": "2109.12855", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Routing brain traffic through the von Neumann bottleneck: Efficient\n  cache usage in spiking neural network simulation code on general purpose\n  computers. Simulation is a third pillar next to experiment and theory in the study of complex dynamic systems such as biological neural networks. Contemporary brain-scale networks correspond to directed graphs of a few million nodes, each with an in-degree and out-degree of several thousands of edges, where nodes and edges correspond to the fundamental biological units, neurons and synapses, respectively. When considering a random graph, each node's edges are distributed across thousands of parallel processes. The activity in neuronal networks is also sparse. Each neuron occasionally transmits a brief signal, called spike, via its outgoing synapses to the corresponding target neurons. This spatial and temporal sparsity represents an inherent bottleneck for simulations on conventional computers: Fundamentally irregular memory-access patterns cause poor cache utilization. Using an established neuronal network simulation code as a reference implementation, we investigate how common techniques to recover cache performance such as software-induced prefetching and software pipelining can benefit a real-world application. The algorithmic changes reduce simulation time by up to 50%. The study exemplifies that many-core systems assigned with an intrinsically parallel computational problem can overcome the von Neumann bottleneck of conventional computer architectures."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of simulating brain-scale neural networks on general purpose computers, which of the following statements best describes the primary challenge and its solution as discussed in the Arxiv documentation?\n\nA) The challenge is excessive power consumption, solved by using specialized neuromorphic hardware.\n\nB) The challenge is the von Neumann bottleneck caused by irregular memory-access patterns, addressed through software-induced prefetching and software pipelining.\n\nC) The challenge is insufficient parallel processing capabilities, resolved by increasing the number of CPU cores.\n\nD) The challenge is the complexity of neural network algorithms, mitigated by simplifying the network structure.\n\nCorrect Answer: B\n\nExplanation: The documentation discusses the challenge of simulating brain-scale neural networks on conventional computers, specifically mentioning the \"von Neumann bottleneck\" caused by irregular memory-access patterns due to the spatial and temporal sparsity of neuronal activity. This leads to poor cache utilization, which is a key performance issue.\n\nThe text explicitly states that common techniques to recover cache performance, such as \"software-induced prefetching and software pipelining,\" were investigated and applied to an established neuronal network simulation code. These algorithmic changes resulted in reducing simulation time by up to 50%, directly addressing the von Neumann bottleneck.\n\nOption A is incorrect as the document doesn't mention power consumption or neuromorphic hardware. Option C, while related to parallel processing, doesn't address the specific memory access issue highlighted. Option D misses the point about memory access patterns being the primary challenge, rather than algorithmic complexity."}, "23": {"documentation": {"title": "Energy-Enstrophy Stability of beta-plane Kolmogorov Flow with Drag", "source": "Yue-Kin Tsang, William R. Young", "docs_id": "0803.0558", "section": ["physics.flu-dyn", "nlin.CD", "physics.ao-ph", "physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy-Enstrophy Stability of beta-plane Kolmogorov Flow with Drag. We develop a new nonlinear stability method, the Energy-Enstrophy (EZ) method, that is specialized to two-dimensional hydrodynamics; the method is applied to a beta-plane flow driven by a sinusoidal body force, and retarded by drag with damping time-scale mu^{-1}. The standard energy method (Fukuta and Murakami, J. Phys. Soc. Japan, 64, 1995, pp 3725) shows that the laminar solution is monotonically and globally stable in a certain portion of the (mu,beta)-parameter space. The EZ method proves nonlinear stability in a larger portion of the (mu,beta)-parameter space. And by penalizing high wavenumbers, the EZ method identifies a most strongly amplifying disturbance that is more physically realistic than that delivered by the energy method. Linear instability calculations are used to determine the region of the (mu,beta)-parameter space where the flow is unstable to infinitesimal perturbations. There is only a small gap between the linearly unstable region and the nonlinearly stable region, and full numerical solutions show only small transient amplification in that gap."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the advantages of the Energy-Enstrophy (EZ) method over the standard energy method in analyzing the stability of beta-plane Kolmogorov flow with drag?\n\nA) The EZ method proves nonlinear stability in a smaller portion of the (mu,beta)-parameter space compared to the energy method.\n\nB) The EZ method identifies a most strongly amplifying disturbance that is less physically realistic than that delivered by the energy method.\n\nC) The EZ method proves nonlinear stability in a larger portion of the (mu,beta)-parameter space and identifies a more physically realistic most strongly amplifying disturbance.\n\nD) The EZ method only analyzes linear stability, while the energy method can prove global stability in certain regions of the parameter space.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"The EZ method proves nonlinear stability in a larger portion of the (mu,beta)-parameter space\" compared to the standard energy method. Additionally, it mentions that \"by penalizing high wavenumbers, the EZ method identifies a most strongly amplifying disturbance that is more physically realistic than that delivered by the energy method.\" These two key advantages are combined in option C.\n\nOption A is incorrect because it states the opposite of what the documentation says about the portion of the parameter space where stability is proven.\n\nOption B is incorrect because it contradicts the statement about the EZ method identifying a more physically realistic disturbance.\n\nOption D is incorrect because the EZ method is described as a nonlinear stability method, not just for linear stability analysis. Furthermore, the energy method's ability to prove global stability in certain regions is not unique, as the EZ method can do this for an even larger portion of the parameter space."}, "24": {"documentation": {"title": "Fused Density Estimation: Theory and Methods", "source": "Robert Bassett and James Sharpnack", "docs_id": "1805.03288", "section": ["stat.ME", "math.OC", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fused Density Estimation: Theory and Methods. In this paper we introduce a method for nonparametric density estimation on geometric networks. We define fused density estimators as solutions to a total variation regularized maximum-likelihood density estimation problem. We provide theoretical support for fused density estimation by proving that the squared Hellinger rate of convergence for the estimator achieves the minimax bound over univariate densities of log-bounded variation. We reduce the original variational formulation in order to transform it into a tractable, finite-dimensional quadratic program. Because random variables on geometric networks are simple generalizations of the univariate case, this method also provides a useful tool for univariate density estimation. Lastly, we apply this method and assess its performance on examples in the univariate and geometric network setting. We compare the performance of different optimization techniques to solve the problem, and use these results to inform recommendations for the computation of fused density estimators."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the theoretical significance and practical application of fused density estimation as presented in the paper?\n\nA) It achieves the minimax bound over multivariate densities and is primarily useful for parametric density estimation on geometric networks.\n\nB) It provides a method for parametric density estimation on geometric networks and shows superior performance to traditional methods in all scenarios.\n\nC) It achieves the minimax bound over univariate densities of log-bounded variation and offers a tractable solution for both univariate and geometric network density estimation.\n\nD) It introduces a new method for multivariate density estimation and proves its convergence rate for arbitrary probability distributions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key points from the paper. The fused density estimator achieves the minimax bound over univariate densities of log-bounded variation, as stated in the paper: \"We provide theoretical support for fused density estimation by proving that the squared Hellinger rate of convergence for the estimator achieves the minimax bound over univariate densities of log-bounded variation.\" Additionally, the method is applicable to both univariate and geometric network settings, as mentioned: \"Because random variables on geometric networks are simple generalizations of the univariate case, this method also provides a useful tool for univariate density estimation.\"\n\nOption A is incorrect because the method is nonparametric, not parametric, and the minimax bound is for univariate densities, not multivariate.\n\nOption B is wrong because the method is nonparametric, and the paper doesn't claim superior performance in all scenarios.\n\nOption D is incorrect because the method is primarily focused on univariate and geometric network density estimation, not multivariate density estimation in general, and the convergence rate is specific to densities of log-bounded variation, not arbitrary probability distributions."}, "25": {"documentation": {"title": "A Stock Prediction Model Based on DCNN", "source": "Qiao Zhou and Ningning Liu", "docs_id": "2009.03239", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Stock Prediction Model Based on DCNN. The prediction of a stock price has always been a challenging issue, as its volatility can be affected by many factors such as national policies, company financial reports, industry performance, and investor sentiment etc.. In this paper, we present a prediction model based on deep CNN and the candle charts, the continuous time stock information is processed. According to different information richness, prediction time interval and classification method, the original data is divided into multiple categories as the training set of CNN. In addition, the convolutional neural network is used to predict the stock market and analyze the difference in accuracy under different classification methods. The results show that the method has the best performance when the forecast time interval is 20 days. Moreover, the Moving Average Convergence Divergence and three kinds of moving average are added as input. This method can accurately predict the stock trend of the US NDAQ exchange for 92.2%. Meanwhile, this article distinguishes three conventional classification methods to provide guidance for future research."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A stock prediction model based on deep CNN and candle charts achieves 92.2% accuracy in predicting stock trends for the US NDAQ exchange. Which of the following combinations of factors most likely contributed to this high accuracy?\n\nA) 5-day forecast interval, Relative Strength Index (RSI), and simple moving average\nB) 20-day forecast interval, Moving Average Convergence Divergence (MACD), and three types of moving averages\nC) 30-day forecast interval, Bollinger Bands, and exponential moving average\nD) 10-day forecast interval, Stochastic Oscillator, and weighted moving average\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the key factors that contributed to the model's high accuracy. The correct answer is B because:\n\n1. The documentation explicitly states that \"the method has the best performance when the forecast time interval is 20 days.\"\n2. It mentions that \"Moving Average Convergence Divergence and three kinds of moving average are added as input.\"\n3. These additions resulted in the 92.2% accuracy for predicting stock trends on the US NDAQ exchange.\n\nOptions A, C, and D are incorrect because they include technical indicators or time intervals not mentioned in the given information, or they contradict the stated optimal forecast interval. This question requires careful reading and integration of multiple pieces of information from the text, making it challenging for exam takers."}, "26": {"documentation": {"title": "Integrable and Superintegrable Extensions of the Rational Calogero-Moser\n  Model in 3 Dimensions", "source": "Allan P. Fordy and Qing Huang", "docs_id": "2111.15659", "section": ["nlin.SI", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrable and Superintegrable Extensions of the Rational Calogero-Moser\n  Model in 3 Dimensions. We consider a class of Hamiltonian systems in 3 degrees of freedom, with a particular type of quadratic integral and which includes the rational Calogero-Moser system as a particular case. For the general class, we introduce separation coordinates to find the general separable (and therefore Liouville integrable) system, with two quadratic integrals. This gives a coupling of the Calogero-Moser system with a large class of potentials, generalising the series of potentials which are separable in parabolic coordinates. Particular cases are {\\em superintegrable}, including Kepler and a resonant oscillator. The initial calculations of the paper are concerned with the flat (Cartesian type) kinetic energy, but in Section \\ref{sec:conflat-general}, we introduce a {\\em conformal factor} $\\varphi$ to $H$ and extend the two quadratic integrals to this case. All the previous results are generalised to this case. We then introduce some 2 and 3 dimensional symmetry algebras of the Kinetic energy (Killing vectors), which restrict the conformal factor. This enables us to reduce our systems from 3 to 2 degrees of freedom, giving rise to many interesting systems, including both Kepler type and H\\'enon-Heiles type potentials on a Darboux-Koenigs $D_2$ background."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the approach and results of the study on extensions of the rational Calogero-Moser model in 3 dimensions?\n\nA) The study only focuses on flat Cartesian kinetic energy and does not consider any conformal factors or symmetry algebras.\n\nB) The research introduces separation coordinates to find general separable systems with three quadratic integrals, but does not address superintegrability.\n\nC) The study introduces a conformal factor to the Hamiltonian, extends quadratic integrals, and uses symmetry algebras to reduce the system from 3 to 2 degrees of freedom, resulting in various interesting systems including Kepler and H\u00e9non-Heiles type potentials.\n\nD) The research is limited to superintegrable systems and does not consider Liouville integrable systems or separable potentials.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key aspects and results of the study. The research introduces a conformal factor \u03c6 to the Hamiltonian H and extends the two quadratic integrals to this case. It then uses 2 and 3 dimensional symmetry algebras of the Kinetic energy (Killing vectors) to restrict the conformal factor, enabling the reduction of the systems from 3 to 2 degrees of freedom. This approach leads to interesting systems, including both Kepler type and H\u00e9non-Heiles type potentials on a Darboux-Koenigs D2 background.\n\nOption A is incorrect because the study does consider conformal factors and symmetry algebras. Option B is incorrect as it mentions three quadratic integrals instead of two and fails to address the inclusion of superintegrable cases. Option D is incorrect because the study considers both Liouville integrable systems and separable potentials, not just superintegrable systems."}, "27": {"documentation": {"title": "The network paradigm as a modeling tool in regional economy: the case of\n  interregional commuting in Greece", "source": "Dimitrios Tsiotas, Labros Sdrolias, Dimitrios Belias", "docs_id": "2001.09664", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The network paradigm as a modeling tool in regional economy: the case of\n  interregional commuting in Greece. Network Science is an emerging discipline using the network paradigm to model communication systems as pair-sets of interconnected nodes and their linkages (edges). This paper applies this paradigm to study an interacting system in regional economy consisting of daily road transportation flows for labor purposes, the so-called commuting phenomenon. In particular, the commuting system in Greece including 39 non-insular prefectures is modeled into a complex network and it is studied using measures and methods of complex network analysis and empirical techniques. The study aims to detect the structural characteristics of the Greek interregional commuting network (GCN) and to interpret how this network is related to the regional development. The analysis highlights the effect of the spatial constraints in the structure of the GCN, it provides insights about the major road transport projects constructed the last decade, and it outlines a populationcontrolled (gravity) pattern of commuting, illustrating that high-populated regions attract larger volumes of the commuting activity, which consequently affects their productivity. Overall, this paper highlights the effectiveness of complex network analysis in the modeling of systems of regional economy, such as the systems of spatial interaction and the transportation networks, and it promotes the use of the network paradigm to the regional research."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the relationship between the Greek interregional commuting network (GCN) and regional development, as suggested by the study?\n\nA) The GCN structure is primarily determined by political boundaries rather than spatial constraints.\n\nB) High-populated regions attract larger volumes of commuting activity, which negatively impacts their productivity.\n\nC) The GCN analysis revealed that major road transport projects had no significant effect on commuting patterns.\n\nD) The study found a population-controlled (gravity) pattern of commuting, with more populous regions attracting larger commuting volumes, potentially affecting their productivity.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that the analysis \"outlines a population-controlled (gravity) pattern of commuting, illustrating that high-populated regions attract larger volumes of the commuting activity, which consequently affects their productivity.\" This directly supports the statement in option D.\n\nOption A is incorrect because the study highlights \"the effect of spatial constraints in the structure of the GCN,\" not political boundaries.\n\nOption B is incorrect because while the study does mention that high-populated regions attract larger volumes of commuting activity, it suggests this affects productivity, but does not specify a negative impact.\n\nOption C is incorrect because the study actually \"provides insights about the major road transport projects constructed the last decade,\" implying that these projects did have an effect on commuting patterns."}, "28": {"documentation": {"title": "Globally nilpotent differential operators and the square Ising model", "source": "A. Bostan, S. Boukraa, S. Hassani, J.-M. Maillard, J.-A. Weil, N.\n  Zenine", "docs_id": "0812.4931", "section": ["math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Globally nilpotent differential operators and the square Ising model. We recall various multiple integrals related to the isotropic square Ising model, and corresponding, respectively, to the n-particle contributions of the magnetic susceptibility, to the (lattice) form factors, to the two-point correlation functions and to their lambda-extensions. These integrals are holonomic and even G-functions: they satisfy Fuchsian linear differential equations with polynomial coefficients and have some arithmetic properties. We recall the explicit forms, found in previous work, of these Fuchsian equations. These differential operators are very selected Fuchsian linear differential operators, and their remarkable properties have a deep geometrical origin: they are all globally nilpotent, or, sometimes, even have zero p-curvature. Focusing on the factorised parts of all these operators, we find out that the global nilpotence of the factors corresponds to a set of selected structures of algebraic geometry: elliptic curves, modular curves, and even a remarkable weight-1 modular form emerging in the three-particle contribution $ \\chi^{(3)}$ of the magnetic susceptibility of the square Ising model. In the case where we do not have G-functions, but Hamburger functions (one irregular singularity at 0 or $ \\infty$) that correspond to the confluence of singularities in the scaling limit, the p-curvature is also found to verify new structures associated with simple deformations of the nilpotent property."}, "answer": "Question: Which of the following statements best describes the nature and implications of the globally nilpotent differential operators associated with the square Ising model, as discussed in the given text?\n\nA) These operators exclusively relate to elliptic curves and have no connection to modular forms or other algebraic structures.\n\nB) The global nilpotence of these operators is a trivial property with no significant geometric or arithmetic implications.\n\nC) These operators are Fuchsian linear differential operators with polynomial coefficients, exhibiting global nilpotence that corresponds to selected structures in algebraic geometry, including elliptic curves, modular curves, and even a weight-1 modular form in the three-particle contribution to magnetic susceptibility.\n\nD) The p-curvature of these operators is always zero, with no variations or special cases mentioned for Hamburger functions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key points from the given text. The passage emphasizes that the differential operators related to the square Ising model are Fuchsian linear differential operators with polynomial coefficients. Their global nilpotence is highlighted as a remarkable property with deep geometric origins, corresponding to selected structures in algebraic geometry such as elliptic curves and modular curves. The text specifically mentions a weight-1 modular form emerging in the three-particle contribution \u03c7^(3) to the magnetic susceptibility.\n\nAnswer A is incorrect because it oversimplifies the connections, ignoring the mentioned links to modular forms and other algebraic structures. Answer B is wrong as it undermines the significance of the global nilpotence property, which the text presents as having important geometric and arithmetic implications. Answer D is incorrect because the text mentions that while some operators have zero p-curvature, this is not always the case, especially for Hamburger functions where new structures associated with simple deformations of the nilpotent property are observed."}, "29": {"documentation": {"title": "Time-Varying Coefficient Model Estimation Through Radial Basis Functions", "source": "Juan Sosa and Lina Buitrago", "docs_id": "2103.00315", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time-Varying Coefficient Model Estimation Through Radial Basis Functions. In this paper we estimate the dynamic parameters of a time-varying coefficient model through radial kernel functions in the context of a longitudinal study. Our proposal is based on a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points. In addition, we study different alternatives of estimation and inference including a Frequentist approach using weighted least squares along with bootstrap methods, and a Bayesian approach through both Markov chain Monte Carlo and variational methods. We compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study considering multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation. Our experiments show that the capabilities of our proposal based on radial kernel functions are indeed comparable with or even better than those obtained from regression splines. We illustrate our methodology by analyzing data from two AIDS clinical studies."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of estimating dynamic parameters for a time-varying coefficient model, which of the following statements is NOT true according to the paper?\n\nA) The proposed method uses a linear combination of weighted kernel functions centered around specific time points.\n\nB) The paper compares the radial kernel functions approach with regression splines through simulation studies.\n\nC) The Bayesian approach in the study only utilizes Markov chain Monte Carlo methods for estimation and inference.\n\nD) The study considers multiple scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation in its simulation experiments.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the paper states that the Bayesian approach uses both Markov chain Monte Carlo and variational methods, not just Markov chain Monte Carlo. \n\nOption A is true as the paper explicitly mentions using \"a linear combination of weighted kernel functions involving a bandwidth, centered around a given set of time points.\"\n\nOption B is correct as the document states they \"compare the estimation strategies mention above with each other, and our radial kernel functions proposal with an expansion based on regression spline, by means of an extensive simulation study.\"\n\nOption D is accurate because the paper mentions considering \"multiples scenarios in terms of sample size, number of repeated measurements, and subject-specific correlation\" in their experiments.\n\nTherefore, option C is the only statement that is not true according to the given information."}, "30": {"documentation": {"title": "Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality\n  Learning Framework for Land Cover and Land Use Classification", "source": "Danfeng Hong, Naoto Yokoya, Nan Ge, Jocelyn Chanussot, Xiao Xiang Zhu", "docs_id": "1901.02838", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality\n  Learning Framework for Land Cover and Land Use Classification. In this paper, we aim at tackling a general but interesting cross-modality feature learning question in remote sensing community --- can a limited amount of highly-discrimin-ative (e.g., hyperspectral) training data improve the performance of a classification task using a large amount of poorly-discriminative (e.g., multispectral) data? Traditional semi-supervised manifold alignment methods do not perform sufficiently well for such problems, since the hyperspectral data is very expensive to be largely collected in a trade-off between time and efficiency, compared to the multispectral data. To this end, we propose a novel semi-supervised cross-modality learning framework, called learnable manifold alignment (LeMA). LeMA learns a joint graph structure directly from the data instead of using a given fixed graph defined by a Gaussian kernel function. With the learned graph, we can further capture the data distribution by graph-based label propagation, which enables finding a more accurate decision boundary. Additionally, an optimization strategy based on the alternating direction method of multipliers (ADMM) is designed to solve the proposed model. Extensive experiments on two hyperspectral-multispectral datasets demonstrate the superiority and effectiveness of the proposed method in comparison with several state-of-the-art methods."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge addressed by the Learnable Manifold Alignment (LeMA) framework in the context of remote sensing, and how does it propose to overcome this challenge?\n\nA) It addresses the challenge of having too much hyperspectral data by using multispectral data to reduce computational complexity.\n\nB) It tackles the issue of having limited hyperspectral training data by leveraging a large amount of multispectral data through a novel graph learning approach.\n\nC) It solves the problem of noisy multispectral data by using hyperspectral data to clean and refine the dataset.\n\nD) It aims to replace hyperspectral data entirely with multispectral data for more efficient land cover and land use classification.\n\nCorrect Answer: B\n\nExplanation: The LeMA framework addresses the challenge of having limited highly-discriminative (hyperspectral) training data while having access to a large amount of poorly-discriminative (multispectral) data. Traditional semi-supervised manifold alignment methods struggle with this scenario due to the high cost and time-intensive nature of collecting hyperspectral data.\n\nLeMA proposes to overcome this challenge by learning a joint graph structure directly from the data, rather than using a fixed graph defined by a Gaussian kernel function. This learned graph allows for better capture of the data distribution through graph-based label propagation, enabling a more accurate decision boundary. The framework aims to improve the performance of land cover and land use classification tasks by effectively utilizing the limited hyperspectral data in conjunction with the abundant multispectral data."}, "31": {"documentation": {"title": "Online Adjoint Methods for Optimization of PDEs", "source": "Justin Sirignano and Konstantinos Spiliopoulos", "docs_id": "2101.09621", "section": ["math.OC", "math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online Adjoint Methods for Optimization of PDEs. We present and mathematically analyze an online adjoint algorithm for the optimization of partial differential equations (PDEs). Traditional adjoint algorithms would typically solve a new adjoint PDE at each optimization iteration, which can be computationally costly. In contrast, an online adjoint algorithm updates the design variables in continuous-time and thus constantly makes progress towards minimizing the objective function. The online adjoint algorithm we consider is similar in spirit to the pseudo-time-stepping, one-shot method which has been previously proposed. Motivated by the application of such methods to engineering problems, we mathematically study the convergence of the online adjoint algorithm. The online adjoint algorithm relies upon a time-relaxed adjoint PDE which provides an estimate of the direction of steepest descent. The algorithm updates this estimate continuously in time, and it asymptotically converges to the exact direction of steepest descent as $t \\rightarrow \\infty$. We rigorously prove that the online adjoint algorithm converges to a critical point of the objective function for optimizing the PDE. Under appropriate technical conditions, we also prove a convergence rate for the algorithm. A crucial step in the convergence proof is a multi-scale analysis of the coupled system for the forward PDE, adjoint PDE, and the gradient descent ODE for the design variables."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the key advantage of the online adjoint algorithm over traditional adjoint algorithms in PDE optimization?\n\nA) It solves the adjoint PDE faster than traditional methods\nB) It updates design variables in continuous-time, allowing for constant progress\nC) It eliminates the need for solving PDEs altogether\nD) It guarantees global optimization of the objective function\n\nCorrect Answer: B\n\nExplanation: The key advantage of the online adjoint algorithm is that it updates design variables in continuous-time, allowing for constant progress towards minimizing the objective function. This is in contrast to traditional adjoint algorithms, which typically solve a new adjoint PDE at each discrete optimization iteration.\n\nOption A is incorrect because the speed of solving the adjoint PDE is not specifically mentioned as an advantage. The online method uses a time-relaxed adjoint PDE, but its primary benefit is the continuous updating, not necessarily faster PDE solving.\n\nOption C is incorrect because the online adjoint method still involves PDEs. It uses a time-relaxed adjoint PDE and considers the forward PDE in its analysis.\n\nOption D is incorrect because the documentation only mentions convergence to a critical point of the objective function, not global optimization. In fact, global optimization is generally not guaranteed for most optimization algorithms dealing with complex PDEs.\n\nThe correct answer, B, captures the essence of the online adjoint algorithm's advantage: continuous-time updates that allow for constant progress in optimization, as opposed to discrete iterations in traditional methods."}, "32": {"documentation": {"title": "Maximum Likelihood de novo reconstruction of viral populations using\n  paired end sequencing data", "source": "Raunaq Malhotra, Manjari Mukhopadhyay Steven Wu, Allen Rodrigo, Mary\n  Poss, Raj Acharya", "docs_id": "1502.04239", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Maximum Likelihood de novo reconstruction of viral populations using\n  paired end sequencing data. We present MLEHaplo, a maximum likelihood de novo assembly algorithm for reconstructing viral haplotypes in a virus population from paired-end next generation sequencing (NGS) data. Using the pairing information of reads in our proposed Viral Path Reconstruction Algorithm (ViPRA), we generate a small subset of paths from a De Bruijn graph of reads that serve as candidate paths for true viral haplotypes. Our proposed method MLEHaplo then generates a maximum likelihood estimate of the viral population using the paths reconstructed by ViPRA. We evaluate and compare MLEHaplo on simulated datasets of 1200 base pairs at different sequence coverage, on HCV strains with sequencing errors, and on a lab mixture of five HIV-1 strains. MLEHaplo reconstructs full length viral haplotypes having a 100% sequence identity to the true viral haplotypes in most of the small genome simulated viral populations at 250x sequencing coverage. While reference based methods either under-estimate or over-estimate the viral haplotypes, MLEHaplo limits the over-estimation to 3 times the size of true viral haplotypes, reconstructs the full phylogeny in the HCV to greater than 99% sequencing identity and captures more sequencing variation for the HIV-1 strains dataset compared to their known consensus sequences."}, "answer": "Here's a multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the performance of MLEHaplo in reconstructing viral haplotypes?\n\nA) It consistently underestimates the number of viral haplotypes in a population.\n\nB) It achieves 100% sequence identity to true viral haplotypes only at extremely high sequencing coverage (>1000x).\n\nC) It reconstructs full-length viral haplotypes with 100% sequence identity in most small genome simulated viral populations at 250x sequencing coverage.\n\nD) It overestimates the number of viral haplotypes by at least 10 times the size of true viral haplotypes.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that \"MLEHaplo reconstructs full length viral haplotypes having a 100% sequence identity to the true viral haplotypes in most of the small genome simulated viral populations at 250x sequencing coverage.\" This directly corresponds to option C.\n\nOption A is incorrect because the passage mentions that reference-based methods might underestimate, but MLEHaplo does not have this issue.\n\nOption B is incorrect because MLEHaplo achieves 100% sequence identity at 250x coverage, not requiring extremely high coverage like >1000x.\n\nOption D is incorrect because while MLEHaplo may overestimate, the passage states it \"limits the over-estimation to 3 times the size of true viral haplotypes,\" not 10 times as suggested in this option."}, "33": {"documentation": {"title": "Field-theory calculation of the electric dipole moment of the neutron\n  and paramagnetic atoms", "source": "S.A. Blundell, J. Griffith, and J. Sapirstein", "docs_id": "1205.2341", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Field-theory calculation of the electric dipole moment of the neutron\n  and paramagnetic atoms. Electric dipole moments (edms) of bound states that arise from the constituents having edms are studied with field-theoretic techniques. The systems treated are the neutron and a set of paramagnetic atoms. In the latter case it is well known that the atomic edm differs greatly from the electron edm when the internal electric fields of the atom are taken into account. In the nonrelativistic limit these fields lead to a complete suppression, but for heavy atoms large enhancement factors are present. A general bound-state field theory approach applicable to both the neutron and paramagnetic atoms is set up. It is applied first to the neutron, treating the quarks as moving freely in a confining spherical well. It is shown that the effect of internal electric fields is small in this case. The atomic problem is then revisited using field-theory techniques in place of the usual Hamiltonian methods, and the atomic enhancement factor is shown to be consistent with previous calculations. Possible application of bound-state techniques to other sources of the neutron edm is discussed."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the field-theory calculation of electric dipole moments (EDMs) for bound states, how do the results for the neutron compare to those for heavy paramagnetic atoms?\n\nA) The neutron shows strong enhancement of its EDM due to internal electric fields, while heavy paramagnetic atoms show complete suppression.\n\nB) Both the neutron and heavy paramagnetic atoms show complete suppression of their EDMs in the non-relativistic limit.\n\nC) The neutron's EDM is largely unaffected by internal electric fields, while heavy paramagnetic atoms show significant enhancement factors.\n\nD) Both the neutron and heavy paramagnetic atoms show strong enhancement of their EDMs due to internal electric fields.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the different behaviors of EDMs in neutrons versus heavy paramagnetic atoms. The correct answer is C because the text states that for the neutron, \"the effect of internal electric fields is small in this case,\" while for heavy atoms, \"large enhancement factors are present.\" This contrasts with lighter atoms, where \"in the nonrelativistic limit these fields lead to a complete suppression.\" The question requires synthesizing information from different parts of the text and understanding the key differences between these systems."}, "34": {"documentation": {"title": "Identification and Estimation of Spillover Effects in Randomized\n  Experiments", "source": "Gonzalo Vazquez-Bare", "docs_id": "1711.02745", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification and Estimation of Spillover Effects in Randomized\n  Experiments. I study identification, estimation and inference for spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used reduced-form linear-in-means (RF-LIM) regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the RF-LIM coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of spillover effects in randomized experiments, which of the following statements is correct regarding the commonly-used reduced-form linear-in-means (RF-LIM) regression and the difference in means between treated and controls?\n\nA) The RF-LIM regression accurately identifies only positive spillover effects, while the difference in means identifies solely direct effects.\n\nB) Both the RF-LIM regression and the difference in means provide unbiased estimates of spillover effects without any sign issues.\n\nC) The RF-LIM regression identifies a weighted sum of spillover effects with some negative weights, and the difference in means identifies a combination of direct and spillover effects entering with different signs.\n\nD) The RF-LIM regression and the difference in means are equally effective in identifying nonlinear spillover effects in all experimental settings.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the documentation, the commonly-used reduced-form linear-in-means (RF-LIM) regression identifies a weighted sum of spillover effects with some negative weights. Additionally, the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. This highlights the potential issues with these traditional methods in accurately capturing spillover effects in randomized experiments.\n\nOption A is incorrect because it misrepresents the capabilities of both methods. The RF-LIM regression does not exclusively identify positive spillover effects, and the difference in means does not solely identify direct effects.\n\nOption B is incorrect as it suggests that both methods provide unbiased estimates without sign issues, which contradicts the information provided in the documentation.\n\nOption D is incorrect because it overstates the effectiveness of these methods in identifying nonlinear spillover effects. The documentation actually proposes new nonparametric estimators to overcome the limitations of these traditional approaches, especially in capturing nonlinear effects."}, "35": {"documentation": {"title": "Analogue simulation of gravitational waves in a 3+1 dimensional\n  Bose-Einstein condensate", "source": "Daniel Hartley, Tupac Bravo, Dennis R\\\"atzel, Richard Howl, Ivette\n  Fuentes", "docs_id": "1712.01140", "section": ["gr-qc", "cond-mat.quant-gas", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analogue simulation of gravitational waves in a 3+1 dimensional\n  Bose-Einstein condensate. The recent detections of gravitational waves (GWs) by the LIGO and Virgo collaborations have opened the field of GW astronomy, intensifying interest in GWs and other possible detectors sensitive in different frequency ranges. Although strong GW producing events are rare and currently unpredictable, GWs can in principle be simulated in analogue systems at will in the lab. Simulation of GWs in a manifestly quantum system would allow for the study of the interaction of quantum phenomena with GWs. Such predicted interaction is exploited in a recently proposed Bose-Einstein condensate (BEC) based GW detector. In this paper, we show how to manipulate a BEC to mimic the effect of a passing GW. By simultaneously varying the external potential applied to the BEC, and an external magnetic field near a Feshbach resonance, we show that the resulting change in speed of sound can directly reproduce a GW metric. We also show how to simulate a metric used in the recently proposed BEC based GW detector, to provide an environment for testing the proposed metrology scheme of the detector. Explicit expressions for simulations of various GW sources are given. This result is also useful to generally test the interaction of quantum phenomena with GWs in a curved spacetime analogue experiment."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which combination of manipulations in a Bose-Einstein condensate (BEC) system allows for the simulation of a gravitational wave (GW) metric according to the paper?\n\nA) Varying only the external potential applied to the BEC\nB) Changing only the external magnetic field near a Feshbach resonance\nC) Simultaneously varying the external potential and the external magnetic field near a Feshbach resonance\nD) Altering the temperature of the BEC while keeping the external potential constant\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The paper states that to mimic the effect of a passing gravitational wave in a Bose-Einstein condensate, one must \"simultaneously vary the external potential applied to the BEC, and an external magnetic field near a Feshbach resonance.\" This combination of manipulations results in a change in the speed of sound within the BEC, which can directly reproduce a gravitational wave metric.\n\nOption A is incomplete as it only mentions varying the external potential, which alone is not sufficient to simulate the GW metric. Option B is also incomplete, as it only mentions changing the magnetic field, which is just one part of the required manipulation. Option D introduces a variable (temperature) that is not mentioned in the given text as a method for simulating GWs in a BEC.\n\nThis question tests the reader's understanding of the specific experimental setup required to simulate gravitational waves in a BEC system, as described in the paper."}, "36": {"documentation": {"title": "Search for sub-solar mass ultracompact binaries in Advanced LIGO's\n  second observing run", "source": "The LIGO Scientific Collaboration and the Virgo Collaboration: B. P.\n  Abbott, R. Abbott, T. D. Abbott, S. Abraham, F. Acernese, K. Ackley, C.\n  Adams, R. X. Adhikari, V. B. Adya, C. Affeldt, M. Agathos, K. Agatsuma, N.\n  Aggarwal, O. D. Aguiar, L. Aiello, A. Ain, P. Ajith, G. Allen, A. Allocca, M.\n  A. Aloy, P. A. Altin, A. Amato, S. Anand, A. Ananyeva, S. B. Anderson, W. G.\n  Anderson, S. V. Angelova, S. Antier, S. Appert, K. Arai, M. C. Araya, J. S.\n  Areeda, M. Ar\\`ene, N. Arnaud, S. M. Aronson, K. G. Arun, S. Ascenzi, G.\n  Ashton, S. M. Aston, P. Astone, F. Aubin, P. Aufmuth, K. AultONeal, C.\n  Austin, V. Avendano, A. Avila-Alvarez, S. Babak, P. Bacon, F. Badaracco, M.\n  K. M. Bader, S. Bae, J. Baird, P. T. Baker, F. Baldaccini, G. Ballardin, S.\n  W. Ballmer, A. Bals, S. Banagiri, J. C. Barayoga, C. Barbieri, S. E. Barclay,\n  B. C. Barish, D. Barker, K. Barkett, S. Barnum, F. Barone, B. Barr, L.\n  Barsotti, M. Barsuglia, D. Barta, J. Bartlett, I. Bartos, R. Bassiri, A.\n  Basti, M. Bawaj, J. C. Bayley, M. Bazzan, B. B\\'ecsy, M. Bejger, I.\n  Belahcene, A. S. Bell, D. Beniwal, M. G. Benjamin, B. K. Berger, G. Bergmann,\n  S. Bernuzzi, C. P. L. Berry, D. Bersanetti, A. Bertolini, J. Betzwieser, R.\n  Bhandare, J. Bidler, E. Biggs, I. A. Bilenko, S. A. Bilgili, G. Billingsley,\n  R. Birney, O. Birnholtz, S. Biscans, M. Bischi, S. Biscoveanu, A. Bisht, M.\n  Bitossi, M. A. Bizouard, J. K. Blackburn, J. Blackman, C. D. Blair, D. G.\n  Blair, R. M. Blair, S. Bloemen, F. Bobba, N. Bode, M. Boer, Y. Boetzel, G.\n  Bogaert, F. Bondu, R. Bonnand, P. Booker, B. A. Boom, R. Bork, V. Boschi, S.\n  Bose, V. Bossilkov, J. Bosveld, Y. Bouffanais, A. Bozzi, C. Bradaschia, P. R.\n  Brady, A. Bramley, M. Branchesi, J. E. Brau, M. Breschi, T. Briant, J. H.\n  Briggs, F. Brighenti, A. Brillet, M. Brinkmann, P. Brockill, A. F. Brooks, J.\n  Brooks, D. D. Brown, S. Brunett, A. Buikema, T. Bulik, H. J. Bulten, A.\n  Buonanno, D. Buskulic, C. Buy, R. L. Byer, M. Cabero, L. Cadonati, G.\n  Cagnoli, C. Cahillane, J. Calder\\'on Bustillo, T. A. Callister, E. Calloni,\n  J. B. Camp, W. A. Campbell, K. C. Cannon, H. Cao, J. Cao, G. Carapella, F.\n  Carbognani, S. Caride, M. F. Carney, G. Carullo, J. Casanueva Diaz, C.\n  Casentini, S. Caudill, M. Cavagli\\`a, F. Cavalier, R. Cavalieri, G. Cella, P.\n  Cerd\\'a-Dur\\'an, E. Cesarini, O. Chaibi, K. Chakravarti, S. J. Chamberlin, M.\n  Chan, S. Chao, P. Charlton, E. A. Chase, E. Chassande-Mottin, D. Chatterjee,\n  M. Chaturvedi, K. Chatziioannou, B. D. Cheeseboro, H. Y. Chen, X. Chen, Y.\n  Chen, H.-P. Cheng, C. K. Cheong, H. Y. Chia, F. Chiadini, A. Chincarini, A.\n  Chiummo, G. Cho, H. S. Cho, M. Cho, N. Christensen, Q. Chu, S. Chua, K. W.\n  Chung, S. Chung, G. Ciani, M. Cie{\\'s}lar, A. A. Ciobanu, R. Ciolfi, F.\n  Cipriano, A. Cirone, F. Clara, J. A. Clark, P. Clearwater, F. Cleva, E.\n  Coccia, P.-F. Cohadon, D. Cohen, M. Colleoni, C. G. Collette, C. Collins, M.\n  Colpi, L. R. Cominsky, M. Constancio Jr., L. Conti, S. J. Cooper, P. Corban,\n  T. R. Corbitt, I. Cordero-Carri\\'on, S. Corezzi, K. R. Corley, N. Cornish, D.\n  Corre, A. Corsi, S. Cortese, C. A. Costa, R. Cotesta, M. W. Coughlin, S. B.\n  Coughlin, J.-P. Coulon, S. T. Countryman, P. Couvares, P. B. Covas, E. E.\n  Cowan, D. M. Coward, M. J. Cowart, D. C. Coyne, R. Coyne, J. D. E. Creighton,\n  T. D. Creighton, J. Cripe, M. Croquette, S. G. Crowder, T. J. Cullen, A.\n  Cumming, L. Cunningham, E. Cuoco, T. Dal Canton, G. D\\'alya, B. D'Angelo, S.\n  L. Danilishin, S. D'Antonio, K. Danzmann, A. Dasgupta, C. F. Da Silva Costa,\n  L. E. H. Datrier, V. Dattilo, I. Dave, M. Davier, D. Davis, E. J. Daw, D.\n  DeBra, M. Deenadayalan, J. Degallaix, M. De Laurentis, S. Del\\'eglise, W. Del\n  Pozzo, L. M. DeMarchi, N. Demos, T. Dent, R. De Pietri, R. De Rosa, C. De\n  Rossi, R. DeSalvo, O. de Varona, S. Dhurandhar, M. C. D\\'iaz, T. Dietrich, L.\n  Di Fiore, C. DiFronzo, C. Di Giorgio, F. Di Giovanni, M. Di Giovanni, T. Di\n  Girolamo, A. Di Lieto, B. Ding, S. Di Pace, I. Di Palma, F. Di Renzo, A. K.\n  Divakarla, A. Dmitriev, Z. Doctor, F. Donovan, K. L. Dooley, S. Doravari, I.\n  Dorrington, T. P. Downes, M. Drago, J. C. Driggers, Z. Du, J.-G. Ducoin, P.\n  Dupej, O. Durante, S. E. Dwyer, P. J. Easter, G. Eddolls, T. B. Edo, A.\n  Effler, P. Ehrens, J. Eichholz, S. S. Eikenberry, M. Eisenmann, R. A.\n  Eisenstein, L. Errico, R. C. Essick, H. Estelles, D. Estevez, Z. B. Etienne,\n  T. Etzel, M. Evans, T. M. Evans, V. Fafone, S. Fairhurst, X. Fan, S. Farinon,\n  B. Farr, W. M. Farr, E. J. Fauchon-Jones, M. Favata, M. Fays, M. Fazio, C.\n  Fee, J. Feicht, M. M. Fejer, F. Feng, A. Fernandez-Galiana, I. Ferrante, E.\n  C. Ferreira, T. A. Ferreira, F. Fidecaro, I. Fiori, D. Fiorucci, M. Fishbach,\n  R. P. Fisher, J. M. Fishner, R. Fittipaldi, M. Fitz-Axen, V. Fiumara, R.\n  Flaminio, M. Fletcher, E. Floden, E. Flynn, H. Fong, J. A. Font, P. W. F.\n  Forsyth, J.-D. Fournier, Francisco Hernandez Vivanco, S. Frasca, F. Frasconi,\n  Z. Frei, A. Freise, R. Frey, V. Frey, P. Fritschel, V. V. Frolov, G.\n  Fronz\\`e, P. Fulda, M. Fyffe, H. A. Gabbard, B. U. Gadre, S. M. Gaebel, J. R.\n  Gair, L. Gammaitoni, S. G. Gaonkar, C. Garc\\'ia-Quir\\'os, F. Garufi, B.\n  Gateley, S. Gaudio, G. Gaur, V. Gayathri, G. Gemme, E. Genin, A. Gennai, D.\n  George, J. George, L. Gergely, S. Ghonge, Abhirup Ghosh, Archisman Ghosh, S.\n  Ghosh, B. Giacomazzo, J. A. Giaime, K. D. Giardina, D. R. Gibson, K. Gill, L.\n  Glover, J. Gniesmer, P. Godwin, E. Goetz, R. Goetz, B. Goncharov, G.\n  Gonz\\'alez, J. M. Gonzalez Castro, A. Gopakumar, S. E. Gossan, M. Gosselin,\n  R. Gouaty, B. Grace, A. Grado, M. Granata, A. Grant, S. Gras, P. Grassia, C.\n  Gray, R. Gray, G. Greco, A. C. Green, R. Green, E. M. Gretarsson, A.\n  Grimaldi, S. J. Grimm, P. Groot, H. Grote, S. Grunewald, P. Gruning, G. M.\n  Guidi, H. K. Gulati, Y. Guo, A. Gupta, Anchal Gupta, P. Gupta, E. K.\n  Gustafson, R. Gustafson, L. Haegel, O. Halim, B. R. Hall, E. D. Hall, E. Z.\n  Hamilton, G. Hammond, M. Haney, M. M. Hanke, J. Hanks, C. Hanna, M. D.\n  Hannam, O. A. Hannuksela, T. J. Hansen, J. Hanson, T. Harder, T. Hardwick, K.\n  Haris, J. Harms, G. M. Harry, I. W. Harry, R. K. Hasskew, C. J. Haster, K.\n  Haughian, F. J. Hayes, J. Healy, A. Heidmann, M. C. Heintze, H. Heitmann, F.\n  Hellman, P. Hello, G. Hemming, M. Hendry, I. S. Heng, J. Hennig, M. Heurs, S.\n  Hild, T. Hinderer, S. Hochheim, D. Hofman, A. M. Holgado, N. A. Holland, K.\n  Holt, D. E. Holz, P. Hopkins, C. Horst, J. Hough, E. J. Howell, C. G. Hoy, Y.\n  Huang, M. T. H\\\"ubner, E. A. Huerta, D. Huet, B. Hughey, V. Hui, S. Husa, S.\n  H. Huttner, T. Huynh-Dinh, B. Idzkowski, A. Iess, H. Inchauspe, C. Ingram, R.\n  Inta, G. Intini, B. Irwin, H. N. Isa, J.-M. Isac, M. Isi, B. R. Iyer, T.\n  Jacqmin, S. J. Jadhav, K. Jani, N. N. Janthalur, P. Jaranowski, D. Jariwala,\n  A. C. Jenkins, J. Jiang, D. S. Johnson, A. W. Jones, D. I. Jones, J. D.\n  Jones, R. Jones, R. J. G. Jonker, L. Ju, J. Junker, C. V. Kalaghatgi, V.\n  Kalogera, B. Kamai, S. Kandhasamy, G. Kang, J. B. Kanner, S. J. Kapadia, S.\n  Karki, R. Kashyap, M. Kasprzack, S. Katsanevas, E. Katsavounidis, W. Katzman,\n  S. Kaufer, K. Kawabe, N. V. Keerthana, F. K\\'ef\\'elian, D. Keitel, R.\n  Kennedy, J. S. Key, F. Y. Khalili, I. Khan, S. Khan, E. A. Khazanov, N.\n  Khetan, M. Khursheed, N. Kijbunchoo, Chunglee Kim, J. C. Kim, K. Kim, W. Kim,\n  W. S. Kim, Y.-M. Kim, C. Kimball, P. J. King, M. Kinley-Hanlon, R. Kirchhoff,\n  J. S. Kissel, L. Kleybolte, J. H. Klika, S. Klimenko, T. D. Knowles, P. Koch,\n  S. M. Koehlenbeck, G. Koekoek, S. Koley, V. Kondrashov, A. Kontos, N. Koper,\n  M. Korobko, W. Z. Korth, M. Kovalam, D. B. Kozak, C. Kr\\\"amer, V. Kringel, N.\n  Krishnendu, A. Kr\\'olak, N. Krupinski, G. Kuehn, A. Kumar, P. Kumar, Rahul\n  Kumar, Rakesh Kumar, L. Kuo, A. Kutynia, S. Kwang, B. D. Lackey, D. Laghi, K.\n  H. Lai, T. L. Lam, M. Landry, B. B. Lane, R. N. Lang, J. Lange, B. Lantz, R.\n  K. Lanza, A. Lartaux-Vollard, P. D. Lasky, M. Laxen, A. Lazzarini, C.\n  Lazzaro, P. Leaci, S. Leavey, Y. K. Lecoeuche, C. H. Lee, H. K. Lee, H. M.\n  Lee, H. W. Lee, J. Lee, K. Lee, J. Lehmann, A. K. Lenon, N. Leroy, N.\n  Letendre, Y. Levin, A. Li, J. Li, K. J. L. Li, T. G. F. Li, X. Li, F. Lin, F.\n  Linde, S. D. Linker, T. B. Littenberg, J. Liu, X. Liu, M. Llorens-Monteagudo,\n  R. K. L. Lo, L. T. London, A. Longo, M. Lorenzini, V. Loriette, M. Lormand,\n  G. Losurdo, J. D. Lough, C. O. Lousto, G. Lovelace, M. E. Lower, H. L\\\"uck,\n  D. Lumaca, A. P. Lundgren, R. Lynch, Y. Ma, R. Macas, S. Macfoy, M. MacInnis,\n  D. M. Macleod, A. Macquet, I. Maga\\~na Hernandez, F. Maga\\~na-Sandoval, R. M.\n  Magee, E. Majorana, I. Maksimovic, A. Malik, N. Man, V. Mandic, V. Mangano,\n  G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, F. Marchesoni, F. Marion,\n  S. M\\'arka, Z. M\\'arka, C. Markakis, A. S. Markosyan, A. Markowitz, E. Maros,\n  A. Marquina, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, V. Martinez,\n  D. V. Martynov, H. Masalehdan, K. Mason, E. Massera, A. Masserot, T. J.\n  Massinger, M. Masso-Reid, S. Mastrogiovanni, A. Matas, F. Matichard, L.\n  Matone, N. Mavalvala, J. J. McCann, R. McCarthy, D. E. McClelland, P.\n  McClincy, S. McCormick, L. McCuller, S. C. McGuire, C. McIsaac, J. McIver, D.\n  J. McManus, T. McRae, S. T. McWilliams, D. Meacher, G. D. Meadors, M. Mehmet,\n  A. K. Mehta, J. Meidam, E. Mejuto Villa, A. Melatos, G. Mendell, R. A.\n  Mercer, L. Mereni, K. Merfeld, E. L. Merilh, M. Merzougui, S. Meshkov, C.\n  Messenger, C. Messick, F. Messina, R. Metzdorff, P. M. Meyers, F. Meylahn, A.\n  Miani, H. Miao, C. Michel, H. Middleton, L. Milano, A. L. Miller, M.\n  Millhouse, J. C. Mills, M. C. Milovich-Goff, O. Minazzoli, Y. Minenkov, A.\n  Mishkin, C. Mishra, T. Mistry, S. Mitra, V. P. Mitrofanov, G. Mitselmakher,\n  R. Mittleman, G. Mo, D. Moffa, K. Mogushi, S. R. P. Mohapatra, M.\n  Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, F. Morawski, G.\n  Moreno, S. Morisaki, B. Mours, C. M. Mow-Lowry, F. Muciaccia, Arunava\n  Mukherjee, D. Mukherjee, S. Mukherjee, Subroto Mukherjee, N. Mukund, A.\n  Mullavey, J. Munch, E. A. Mu\\~niz, M. Muratore, P. G. Murray, I. Nardecchia,\n  L. Naticchioni, R. K. Nayak, B. F. Neil, J. Neilson, G. Nelemans, T. J. N.\n  Nelson, M. Nery, A. Neunzert, L. Nevin, K. Y. Ng, S. Ng, C. Nguyen, P.\n  Nguyen, D. Nichols, S. A. Nichols, S. Nissanke, F. Nocera, C. North, L. K.\n  Nuttall, M. Obergaulinger, J. Oberling, B. D. O'Brien, G. Oganesyan, G. H.\n  Ogin, J. J. Oh, S. H. Oh, F. Ohme, H. Ohta, M. A. Okada, M. Oliver, P.\n  Oppermann, Richard J. Oram, B. O'Reilly, R. G. Ormiston, L. F. Ortega, R.\n  O'Shaughnessy, S. Ossokine, D. J. Ottaway, H. Overmier, B. J. Owen, A. E.\n  Pace, G. Pagano, M. A. Page, G. Pagliaroli, A. Pai, S. A. Pai, J. R. Palamos,\n  O. Palashov, C. Palomba, H. Pan, P. K. Panda, P. T. H. Pang, C. Pankow, F.\n  Pannarale, B. C. Pant, F. Paoletti, A. Paoli, A. Parida, W. Parker, D.\n  Pascucci, A. Pasqualetti, R. Passaquieti, D. Passuello, M. Patil, B.\n  Patricelli, E. Payne, B. L. Pearlstone, T. C. Pechsiri, A. J. Pedersen, M.\n  Pedraza, R. Pedurand, A. Pele, S. Penn, A. Perego, C. J. Perez, C.\n  P\\'erigois, A. Perreca, J. Petermann, H. P. Pfeiffer, M. Phelps, K. S.\n  Phukon, O. J. Piccinni, M. Pichot, F. Piergiovanni, V. Pierro, G. Pillant, L.\n  Pinard, I. M. Pinto, M. Pirello, M. Pitkin, W. Plastino, R. Poggiani, D. Y.\n  T. Pong, S. Ponrathnam, P. Popolizio, E. K. Porter, J. Powell, A. K.\n  Prajapati, J. Prasad, K. Prasai, R. Prasanna, G. Pratten, T. Prestegard, M.\n  Principe, G. A. Prodi, L. Prokhorov, M. Punturo, P. Puppo, M. P\\\"urrer, H.\n  Qi, V. Quetschke, P. J. Quinonez, F. J. Raab, G. Raaijmakers, H. Radkins, N.\n  Radulesco, P. Raffai, S. Raja, C. Rajan, B. Rajbhandari, M. Rakhmanov, K. E.\n  Ramirez, A. Ramos-Buades, Javed Rana, K. Rao, P. Rapagnani, V. Raymond, M.\n  Razzano, J. Read, T. Regimbau, L. Rei, S. Reid, D. H. Reitze, P. Rettegno, F.\n  Ricci, C. J. Richardson, J. W. Richardson, P. M. Ricker, G. Riemenschneider,\n  K. Riles, M. Rizzo, N. A. Robertson, F. Robinet, A. Rocchi, L. Rolland, J. G.\n  Rollins, V. J. Roma, M. Romanelli, R. Romano, C. L. Romel, J. H. Romie, C. A.\n  Rose, D. Rose, K. Rose, D. Rosi\\'nska, S. G. Rosofsky, M. P. Ross, S. Rowan,\n  A. R\\\"udiger, P. Ruggi, G. Rutins, K. Ryan, S. Sachdev, T. Sadecki, M.\n  Sakellariadou, O. S. Salafia, L. Salconi, M. Saleem, A. Samajdar, L. Sammut,\n  E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, K. A. Santiago,\n  E. Santos, N. Sarin, B. Sassolas, B. S. Sathyaprakash, O. Sauter, R. L.\n  Savage, P. Schale, M. Scheel, J. Scheuer, P. Schmidt, R. Schnabel, R. M. S.\n  Schofield, A. Sch\\\"onbeck, E. Schreiber, B. W. Schulte, B. F. Schutz, J.\n  Scott, S. M. Scott, E. Seidel, D. Sellers, A. S. Sengupta, N. Sennett, D.\n  Sentenac, V. Sequino, A. Sergeev, Y. Setyawati, D. A. Shaddock, T. Shaffer,\n  M. S. Shahriar, M. B. Shaner, A. Sharma, P. Sharma, P. Shawhan, H. Shen, R.\n  Shink, D. H. Shoemaker, D. M. Shoemaker, K. Shukla, S. ShyamSundar, K.\n  Siellez, M. Sieniawska, D. Sigg, L. P. Singer, D. Singh, N. Singh, A.\n  Singhal, A. M. Sintes, S. Sitmukhambetov, V. Skliris, B. J. J. Slagmolen, T.\n  J. Slaven-Blair, J. R. Smith, R. J. E. Smith, S. Somala, E. J. Son, S. Soni,\n  B. Sorazu, F. Sorrentino, T. Souradeep, E. Sowell, A. P. Spencer, M. Spera,\n  A. K. Srivastava, V. Srivastava, K. Staats, C. Stachie, M. Standke, D. A.\n  Steer, M. Steinke, J. Steinlechner, S. Steinlechner, D. Steinmeyer, S. P.\n  Stevenson, D. Stocks, R. Stone, D. J. Stops, K. A. Strain, G. Stratta, S. E.\n  Strigin, A. Strunk, R. Sturani, A. L. Stuver, V. Sudhir, T. Z. Summerscales,\n  L. Sun, S. Sunil, A. Sur, J. Suresh, P. J. Sutton, B. L. Swinkels, M. J.\n  Szczepa\\'nczyk, M. Tacca, S. C. Tait, C. Talbot, D. B. Tanner, D. Tao, M.\n  T\\'apai, A. Tapia, J. D. Tasson, R. Taylor, R. Tenorio, L. Terkowski, M.\n  Thomas, P. Thomas, S. R. Thondapu, K. A. Thorne, E. Thrane, Shubhanshu\n  Tiwari, Srishti Tiwari, V. Tiwari, K. Toland, M. Tonelli, Z. Tornasi, A.\n  Torres-Forn\\'e, C. I. Torrie, D. T\\\"oyr\\\"a, F. Travasso, G. Traylor, M. C.\n  Tringali, A. Tripathee, A. Trovato, L. Trozzo, K. W. Tsang, M. Tse, R. Tso,\n  L. Tsukada, D. Tsuna, T. Tsutsui, D. Tuyenbayev, K. Ueno, D. Ugolini, C. S.\n  Unnikrishnan, A. L. Urban, S. A. Usman, H. Vahlbruch, G. Vajente, G. Valdes,\n  M. Valentini, N. van Bakel, M. van Beuzekom, J. F. J. van den Brand, C. Van\n  Den Broeck, D. C. Vander-Hyde, L. van der Schaaf, J. V. VanHeijningen, A. A.\n  van Veggel, M. Vardaro, V. Varma, S. Vass, M. Vas\\'uth, A. Vecchio, G.\n  Vedovato, J. Veitch, P. J. Veitch, K. Venkateswara, G. Venugopalan, D.\n  Verkindt, F. Vetrano, A. Vicer\\'e, A. D. Viets, S. Vinciguerra, D. J. Vine,\n  J.-Y. Vinet, S. Vitale, T. Vo, H. Vocca, C. Vorvick, S. P. Vyatchanin, A. R.\n  Wade, L. E. Wade, M. Wade, R. Walet, M. Walker, L. Wallace, S. Walsh, H.\n  Wang, J. Z. Wang, S. Wang, W. H. Wang, Y. F. Wang, R. L. Ward, Z. A. Warden,\n  J. Warner, M. Was, J. Watchi, B. Weaver, L.-W. Wei, M. Weinert, A. J.\n  Weinstein, R. Weiss, F. Wellmann, L. Wen, E. K. Wessel, P. We{\\ss}els, J. W.\n  Westhouse, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, D. M. Wilken,\n  D. Williams, A. R. Williamson, J. L. Willis, B. Willke, W. Winkler, C. C.\n  Wipf, H. Wittel, G. Woan, J. Woehler, J. K. Wofford, J. L. Wright, D. S. Wu,\n  D. M. Wysocki, S. Xiao, R. Xu, H. Yamamoto, C. C. Yancey, L. Yang, Y. Yang,\n  Z. Yang, M. J. Yap, M. Yazback, D. W. Yeeles, Hang Yu, Haocun Yu, S. H. R.\n  Yuen, A. K. Zadro\\.zny, A. Zadro\\.zny, M. Zanolin, T. Zelenova, J.-P. Zendri,\n  M. Zevin, J. Zhang, L. Zhang, T. Zhang, C. Zhao, G. Zhao, M. Zhou, Z. Zhou,\n  X. J. Zhu, A. B. Zimmerman, M. E. Zucker, J. Zweizig, and S. Shandera", "docs_id": "1904.08976", "section": ["astro-ph.CO", "astro-ph.HE", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for sub-solar mass ultracompact binaries in Advanced LIGO's\n  second observing run. We present an Advanced LIGO and Advanced Virgo search for sub-solar mass ultracompact objects in data obtained during Advanced LIGO's second observing run. In contrast to a previous search of Advanced LIGO data from the first observing run, this search includes the effects of component spin on the gravitational waveform. We identify no viable gravitational wave candidates consistent with sub-solar mass ultracompact binaries with at least one component between 0.2 - 1.0 solar masses. We use the null result to constrain the binary merger rate of (0.2 solar mass, 0.2 solar mass) binaries to be less than 3.7 x 10^5 Gpc^-3 yr^-1 and the binary merger rate of (1.0 solar mass, 1.0 solar mass) binaries to be less than 5.2 x 10^3 Gpc^-3 yr^-1. Sub-solar mass ultracompact objects are not expected to form via known stellar evolution channels, though it has been suggested that primordial density fluctuations or particle dark matter with cooling mechanisms and/or nuclear interactions could form black holes with sub-solar masses. Assuming a particular primordial black hole formation model, we constrain a population of merging 0.2 solar mass black holes to account for less than 16% of the dark matter density and a population of merging 1.0 solar mass black holes to account for less than 2% of the dark matter density. We discuss how constraints on the merger rate and dark matter fraction may be extended to arbitrary black hole population models that predict sub-solar mass binaries."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Based on the Advanced LIGO and Advanced Virgo search for sub-solar mass ultracompact objects, which of the following statements is correct regarding the constraints on primordial black holes (PBHs) as dark matter candidates?\n\nA) The search conclusively ruled out PBHs as a significant component of dark matter.\n\nB) The results suggest that 0.2 solar mass PBHs could account for up to 16% of dark matter, while 1.0 solar mass PBHs could account for up to 2% of dark matter.\n\nC) The study found evidence for a significant population of sub-solar mass black hole binaries.\n\nD) The constraints on PBHs as dark matter candidates are independent of the assumed formation model.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text states that \"Assuming a particular primordial black hole formation model, we constrain a population of merging 0.2 solar mass black holes to account for less than 16% of the dark matter density and a population of merging 1.0 solar mass black holes to account for less than 2% of the dark matter density.\"\n\nAnswer A is incorrect because the search did not conclusively rule out PBHs as a significant component of dark matter; it only placed upper limits on their contribution.\n\nAnswer C is incorrect because the study did not find evidence for sub-solar mass black hole binaries. In fact, it states, \"We identify no viable gravitational wave candidates consistent with sub-solar mass ultracompact binaries.\"\n\nAnswer D is incorrect because the constraints are based on a particular primordial black hole formation model, as mentioned in the text. The results may vary for different formation models."}, "37": {"documentation": {"title": "Re-weighting of somatosensory inputs from the foot and the ankle for\n  controlling posture during quiet standing following trunk extensor muscles\n  fatigue", "source": "Nicolas Vuillerme (TIMC), Nicolas Pinsault (TIMC)", "docs_id": "0802.1907", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Re-weighting of somatosensory inputs from the foot and the ankle for\n  controlling posture during quiet standing following trunk extensor muscles\n  fatigue. The present study focused on the effects of trunk extensor muscles fatigue on postural control during quiet standing under different somatosensory conditions from the foot and the ankle. With this aim, 20 young healthy adults were asked to stand as immobile as possible in two conditions of No fatigue and Fatigue of trunk extensor muscles. In Experiment 1 (n = 10), somatosensation from the foot and the ankle was degraded by standing on a foam surface. In Experiment 2 (n = 10), somatosensation from the foot and ankle was facilitated through the increased cutaneous feedback at the foot and ankle provided by strips of athletic tape applied across both ankle joints. The centre of foot pressure displacements (CoP) were recorded using a force platform. The results showed that (1) trunk extensor muscles fatigue increased CoP displacements under normal somatosensatory conditions (Experiment 1 and Experiment 2), (2) this destabilizing effect was exacerbated when somatosensation from the foot and the ankle was degraded (Experiment 1), and (3) this destabilizing effect was mitigated when somatosensation from the foot and the ankle was facilitated (Experiment 2). Altogether, the present findings evidenced re-weighting of sensory cues for controlling posture during quiet standing following trunk extensor muscles fatigue by increasing the reliance on the somatosensory inputs from the foot and the ankle. This could have implications in clinical and rehabilitative areas."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements most accurately reflects the study's findings regarding the relationship between trunk extensor muscle fatigue and postural control during quiet standing?\n\nA) Trunk extensor muscle fatigue had no significant effect on postural control regardless of somatosensory conditions.\n\nB) The destabilizing effect of trunk extensor muscle fatigue was consistently mitigated by both degraded and facilitated somatosensation from the foot and ankle.\n\nC) Trunk extensor muscle fatigue increased CoP displacements under normal conditions, with this effect being exacerbated by degraded foot and ankle somatosensation and mitigated by facilitated somatosensation.\n\nD) The study found that trunk extensor muscle fatigue only affected postural control when somatosensation from the foot and ankle was artificially manipulated.\n\nCorrect Answer: C\n\nExplanation: Option C correctly summarizes the key findings of the study. The research showed that trunk extensor muscle fatigue increased Center of Pressure (CoP) displacements under normal conditions. This destabilizing effect was made worse when somatosensation from the foot and ankle was degraded (as in Experiment 1 with the foam surface), and it was reduced when somatosensation was facilitated (as in Experiment 2 with the athletic tape). This answer captures the nuanced relationship between muscle fatigue and different somatosensory conditions, which is central to the study's conclusions about sensory re-weighting in postural control."}, "38": {"documentation": {"title": "Stability of Compacton Solutions of Fifth-Order Nonlinear Dispersive\n  Equations", "source": "Bishwajyoti Dey and Avinash Khare", "docs_id": "patt-sol/9904007", "section": ["nlin.PS", "nlin.PS", "nlin.SI", "quant-ph", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability of Compacton Solutions of Fifth-Order Nonlinear Dispersive\n  Equations. We consider fifth-order nonlinear dispersive $K(m,n,p)$ type equations to study the effect of nonlinear dispersion. Using simple scaling arguments we show, how, instead of the conventional solitary waves like solitons, the interaction of the nonlinear dispersion with nonlinear convection generates compactons - the compact solitary waves free of exponential tails. This interaction also generates many other solitary wave structures like cuspons, peakons, tipons etc. which are otherwise unattainable with linear dispersion. Various self similar solutions of these higher order nonlinear dispersive equations are also obtained using similarity transformations. Further, it is shown that, like the third-order nonlinear $K(m,n)$ equations, the fifth-order nonlinear dispersive equations also have the same four conserved quantities and further even any arbitrary odd order nonlinear dispersive $K(m,n,p...)$ type equations also have the same three (and most likely the four) conserved quantities. Finally, the stability of the compacton solutions for the fifth-order nonlinear dispersive equations are studied using linear stability analysis. From the results of the linear stability analysis it follows that, unlike solitons, all the allowed compacton solutions are stable, since the stability conditions are satisfied for arbitrary values of the nonlinear parameters."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements about fifth-order nonlinear dispersive K(m,n,p) type equations is NOT correct?\n\nA) They generate compactons through the interaction of nonlinear dispersion with nonlinear convection.\n\nB) They possess the same four conserved quantities as third-order nonlinear K(m,n) equations.\n\nC) All allowed compacton solutions are unstable according to linear stability analysis.\n\nD) They can produce solitary wave structures like cuspons, peakons, and tipons.\n\nCorrect Answer: C\n\nExplanation: \nA is correct as the passage states that \"the interaction of the nonlinear dispersion with nonlinear convection generates compactons.\"\n\nB is correct according to the text: \"like the third-order nonlinear K(m,n) equations, the fifth-order nonlinear dispersive equations also have the same four conserved quantities.\"\n\nC is incorrect. The passage actually states the opposite: \"unlike solitons, all the allowed compacton solutions are stable, since the stability conditions are satisfied for arbitrary values of the nonlinear parameters.\"\n\nD is correct as the text mentions that these equations generate \"many other solitary wave structures like cuspons, peakons, tipons etc.\"\n\nTherefore, C is the statement that is NOT correct, making it the right answer for this question."}, "39": {"documentation": {"title": "Towards Bursting Filter Bubble via Contextual Risks and Uncertainties", "source": "Rikiya Takahashi and Shunan Zhang", "docs_id": "1706.09985", "section": ["stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Bursting Filter Bubble via Contextual Risks and Uncertainties. A rising topic in computational journalism is how to enhance the diversity in news served to subscribers to foster exploration behavior in news reading. Despite the success of preference learning in personalized news recommendation, their over-exploitation causes filter bubble that isolates readers from opposing viewpoints and hurts long-term user experiences with lack of serendipity. Since news providers can recommend neither opposite nor diversified opinions if unpopularity of these articles is surely predicted, they can only bet on the articles whose forecasts of click-through rate involve high variability (risks) or high estimation errors (uncertainties). We propose a novel Bayesian model of uncertainty-aware scoring and ranking for news articles. The Bayesian binary classifier models probability of success (defined as a news click) as a Beta-distributed random variable conditional on a vector of the context (user features, article features, and other contextual features). The posterior of the contextual coefficients can be computed efficiently using a low-rank version of Laplace's method via thin Singular Value Decomposition. Efficiencies in personalized targeting of exceptional articles, which are chosen by each subscriber in test period, are evaluated on real-world news datasets. The proposed estimator slightly outperformed existing training and scoring algorithms, in terms of efficiency in identifying successful outliers."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following best describes the primary goal and methodology of the approach discussed in the Arxiv document \"Towards Bursting Filter Bubble via Contextual Risks and Uncertainties\"?\n\nA) To increase user engagement by recommending only the most popular news articles using a deterministic algorithm\n\nB) To combat filter bubbles by recommending articles with high predicted click-through rates, regardless of content diversity\n\nC) To enhance news diversity by recommending articles with high variability in click-through rate predictions using a Bayesian uncertainty-aware model\n\nD) To improve the accuracy of click-through rate predictions for all news articles using a neural network approach\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document describes an approach to enhance diversity in news recommendations and combat filter bubbles. The key aspects of this approach are:\n\n1. It aims to recommend articles whose click-through rate predictions involve high variability (risks) or high estimation errors (uncertainties).\n2. It uses a Bayesian model of uncertainty-aware scoring and ranking for news articles.\n3. The model employs a Beta-distributed random variable conditional on contextual features to estimate the probability of a news click.\n4. The goal is to foster exploration behavior in news reading and avoid over-exploitation of user preferences, which can lead to filter bubbles.\n\nOption A is incorrect because the approach doesn't focus on recommending only popular articles, but rather on introducing diversity.\nOption B is incorrect as it contradicts the goal of combating filter bubbles by recommending diverse content.\nOption D is incorrect because while the approach does involve improving predictions, it specifically focuses on leveraging uncertainty and variability rather than improving overall accuracy for all articles."}, "40": {"documentation": {"title": "Surface appearance of dynamo-generated large-scale fields", "source": "J. Warnecke (1,2) and A. Brandenburg (1,2) ((1) Nordita (2), Stockholm\n  University)", "docs_id": "1002.3620", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Surface appearance of dynamo-generated large-scale fields. Twisted magnetic fields are frequently seen to emerge above the visible surface of the Sun. This emergence is usually associated with the rise of buoyant magnetic flux structures. Here we ask how magnetic fields from a turbulent large-scale dynamo appear above the surface if there is no magnetic buoyancy. The computational domain is split into two parts. In the lower part, which we refer to as the turbulence zone, the flow is driven by an assumed helical forcing function leading to dynamo action. Above this region, which we refer to as the exterior, a nearly force-free magnetic field is computed at each time step using the stress-and-relax method. Twisted arcade-like field structures are found to emerge in the exterior above the turbulence zone. Strong current sheets tend to form above the neutral line, where the vertical field component vanishes. Time series of the magnetic field structure show recurrent plasmoid ejections. The degree to which the exterior field is force free is estimated as the ratio of the dot product of current density and magnetic field strength to their respective rms values. This ratio reaches values of up to 95% in the exterior. A weak outward flow is driven by the residual Lorentz force."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the study of dynamo-generated large-scale fields, what is the primary mechanism responsible for the emergence of twisted magnetic fields above the Sun's visible surface in this particular simulation, and what unique feature distinguishes this model from conventional solar magnetic field models?\n\nA) Magnetic buoyancy, with the unique feature being the use of a stress-and-relax method in the exterior region\nB) Helical forcing function in the turbulence zone, with the unique feature being the absence of magnetic buoyancy\nC) Plasmoid ejections, with the unique feature being the formation of strong current sheets above the neutral line\nD) Force-free magnetic field computation, with the unique feature being the split computational domain\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The study specifically investigates how magnetic fields from a turbulent large-scale dynamo appear above the surface without magnetic buoyancy, which is typically associated with the emergence of twisted magnetic fields. The unique feature of this model is that it does not rely on magnetic buoyancy, contrary to conventional models of solar magnetic field emergence.\n\nThe helical forcing function in the turbulence zone drives the dynamo action, leading to the generation of large-scale fields. The absence of magnetic buoyancy is a deliberate choice in this simulation to explore alternative mechanisms for field emergence.\n\nOption A is incorrect because the study explicitly states that there is no magnetic buoyancy in this model. While the stress-and-relax method is used in the exterior region, it's not the primary mechanism for field emergence.\n\nOption C mentions plasmoid ejections and current sheets, which are observed phenomena in the simulation but are not the primary mechanism for the initial emergence of the fields.\n\nOption D describes aspects of the model's setup but doesn't capture the key distinction of this study, which is the investigation of field emergence without magnetic buoyancy."}, "41": {"documentation": {"title": "Design of 11.8 MHZ Buncher for Isac at Triumf", "source": "A.K. Mitra, R.L. Poirier, R.E. Laxdal", "docs_id": "physics/0008213", "section": ["physics.acc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Design of 11.8 MHZ Buncher for Isac at Triumf. The high energy beam transport (HEBT) line for the ISAC radioactive beam facility at TRIUMF requires an 11.8 MHz buncher. The main requirements of the buncher are to operate in cw mode with a velocity acceptance of 2.2% and an effective voltage of 100 kV, which for a three gap buncher gives a drift tube voltage of 30 kV. A lumped element circuit is more suitable than a distributed rf structure for this low frequency of operation. The resonant frequency of 11.8 MHz is obtained by an inductive coil in parallel with the capacitance of the drift tube. The coil is housed in a dust free box at atmospheric pressure whereas the drift tube is placed in a vacuum chamber and an rf feedthrough connects them. Two design of this feedthrough, one using disk and one using tubular ceramics, operating at 30 kV rf, are described in this paper. MAFIA and SUPERFISH codes are used to simulate the fields in the feedthroughs, particularly around the ceramic metal interfaces. Test results of the prototype feedthroughs are presented and the choice of the proposed final solution is outlined."}, "answer": "Question: What is the primary reason for choosing a lumped element circuit over a distributed RF structure for the 11.8 MHz buncher in the ISAC facility at TRIUMF?\n\nA) The lumped element circuit provides a higher effective voltage\nB) The low frequency of operation makes a lumped element circuit more suitable\nC) Lumped element circuits have a wider velocity acceptance\nD) Distributed RF structures are not compatible with the HEBT line\n\nCorrect Answer: B\n\nExplanation: The passage explicitly states that \"A lumped element circuit is more suitable than a distributed rf structure for this low frequency of operation.\" This directly points to the low frequency (11.8 MHz) as the primary reason for choosing a lumped element circuit. \n\nOption A is incorrect because the effective voltage requirement (100 kV) is a general specification and not a reason for choosing the circuit type.\n\nOption C is not supported by the information given. The velocity acceptance of 2.2% is mentioned as a requirement but not as a reason for choosing the circuit type.\n\nOption D is a false statement. There's no information suggesting that distributed RF structures are incompatible with the HEBT line. The choice is based on suitability, not compatibility."}, "42": {"documentation": {"title": "A mixing interpolation method to mimic pasta phases in compact star\n  matter", "source": "David Blaschke, David Alvarez-Castillo", "docs_id": "1807.03258", "section": ["nucl-th", "astro-ph.HE", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A mixing interpolation method to mimic pasta phases in compact star\n  matter. We present a new method to interpolate between two matter phases that allows for a description of mixed phases and can be used, e.g., for mimicking transitions between pasta structures occuring in the crust as well as in the inner core of compact stars. This interpolation method is based on assuming switch functions that are used to define a mixture of subphases while fulfilling constraints of thermodynamic stability. The width of the transition depends on a free parameter, the pressure increment relative to the critical pressure of a Maxwell construction. As an example we present a trigonometric function ansatz for the switch function together with a pressure increment during the transition. We note that the resulting mixed phase equation of state bears similarities with the appearance of substitutional compounds in neutron star crusts and with the sequence of transitions between different pasta phases in the hadron-to-quark matter transition. We apply this method to the case of a hadron-to-quark matter transition and test the robustness of the compact star mass twin phenomenon against the appearance of pasta phases modeled in this way."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of modeling pasta phases in compact star matter, which of the following statements is NOT true regarding the mixing interpolation method described?\n\nA) The method uses switch functions to define a mixture of subphases while maintaining thermodynamic stability.\n\nB) The width of the transition is solely determined by the critical pressure of a Maxwell construction.\n\nC) The method can be applied to transitions between pasta structures in both the crust and inner core of compact stars.\n\nD) The approach bears similarities to the appearance of substitutional compounds in neutron star crusts.\n\nCorrect Answer: B\n\nExplanation: \nThe correct answer is B because the width of the transition depends on a free parameter, which is the pressure increment relative to the critical pressure of a Maxwell construction, not solely on the critical pressure itself. \n\nOption A is true according to the text, which states that the method uses switch functions to define subphase mixtures while fulfilling thermodynamic stability constraints.\n\nOption C is correct as the text mentions that this method can be used for mimicking transitions between pasta structures occurring in both the crust and inner core of compact stars.\n\nOption D is also true, as the documentation explicitly states that the resulting mixed phase equation of state has similarities with the appearance of substitutional compounds in neutron star crusts.\n\nThis question tests the reader's understanding of the key aspects of the mixing interpolation method and requires careful attention to detail from the given information."}, "43": {"documentation": {"title": "Scale-dependent bias and bispectrum in neutrino separate universe\n  simulations", "source": "Chi-Ting Chiang, Wayne Hu, Yin Li, Marilena LoVerde", "docs_id": "1710.01310", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scale-dependent bias and bispectrum in neutrino separate universe\n  simulations. Cosmic background neutrinos have a large velocity dispersion, which causes the evolution of long-wavelength density perturbations to depend on scale. This scale-dependent growth leads to the well-known suppression in the linear theory matter power spectrum that is used to probe neutrino mass. In this paper, we study the impact of long-wavelength density perturbations on small-scale structure formation. By performing separate universe simulations where the long-wavelength mode is absorbed into the local expansion, we measure the responses of the cold dark matter (CDM) power spectrum and halo mass function, which correspond to the squeezed-limit bispectrum and halo bias. We find that the scale-dependent evolution of the long-wavelength modes causes these quantities to depend on scale and provide simple expressions to model them in terms of scale and the amount of massive neutrinos. Importantly, this scale-dependent bias reduces the suppression in the linear halo power spectrum due to massive neutrinos by 13 and 26% for objects of bias $\\bar{b}=2$ and $\\bar{b} \\gg1$, respectively. We demonstrate with high statistical significance that the scale-dependent halo bias ${\\it cannot}$ be modeled by the CDM and neutrino density transfer functions at the time when the halos are identified. This reinforces the importance of the temporal nonlocality of structure formation, especially when the growth is scale dependent."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: How does the scale-dependent bias caused by cosmic background neutrinos affect the suppression in the linear halo power spectrum for massive neutrinos, and what does this imply about structure formation?\n\nA) It increases the suppression by 13% for objects with bias b\u0304=2, indicating that neutrinos enhance small-scale structure formation.\n\nB) It reduces the suppression by 26% for objects with bias b\u0304\u226b1, suggesting that the scale-dependent growth strengthens large-scale structure formation.\n\nC) It reduces the suppression by 13% for objects with bias b\u0304=2 and 26% for objects with bias b\u0304\u226b1, emphasizing the importance of temporal nonlocality in structure formation.\n\nD) It has no significant effect on the linear halo power spectrum, implying that neutrino mass can be accurately probed using only the matter power spectrum.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that the scale-dependent bias reduces the suppression in the linear halo power spectrum due to massive neutrinos by 13% for objects of bias b\u0304=2 and 26% for objects of bias b\u0304\u226b1. This finding emphasizes the importance of temporal nonlocality in structure formation, especially when growth is scale-dependent. The question tests understanding of how neutrino-induced scale-dependent bias affects structure formation and the limitations of using only the matter power spectrum to probe neutrino mass. Options A and B are incorrect as they misrepresent the effect's direction or magnitude, while D is wrong because the effect is significant and cannot be ignored when probing neutrino mass."}, "44": {"documentation": {"title": "Syntactically Look-Ahead Attention Network for Sentence Compression", "source": "Hidetaka Kamigaito, Manabu Okumura", "docs_id": "2002.01145", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Syntactically Look-Ahead Attention Network for Sentence Compression. Sentence compression is the task of compressing a long sentence into a short one by deleting redundant words. In sequence-to-sequence (Seq2Seq) based models, the decoder unidirectionally decides to retain or delete words. Thus, it cannot usually explicitly capture the relationships between decoded words and unseen words that will be decoded in the future time steps. Therefore, to avoid generating ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. To solve this problem, we propose a novel Seq2Seq model, syntactically look-ahead attention network (SLAHAN), that can generate informative summaries by explicitly tracking both dependency parent and child words during decoding and capturing important words that will be decoded in the future. The results of the automatic evaluation on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLAHAN also improved the summarization performance on longer sentences. Furthermore, in the human evaluation, SLAHAN improved informativeness without losing readability."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following best describes the primary innovation of the Syntactically Look-Ahead Attention Network (SLAHAN) for sentence compression?\n\nA) It uses a bidirectional encoder to process input sentences more effectively.\nB) It incorporates a post-processing step to improve grammaticality of compressed sentences.\nC) It explicitly tracks both dependency parent and child words during decoding to capture important future words.\nD) It employs a reinforcement learning approach to optimize for ROUGE scores directly.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation of SLAHAN, as described in the documentation, is its ability to \"explicitly track both dependency parent and child words during decoding and capturing important words that will be decoded in the future.\" This approach allows the model to make more informed decisions about which words to keep or delete, addressing the problem of sometimes dropping important words that traditional sequence-to-sequence models face.\n\nOption A is incorrect because while many Seq2Seq models use bidirectional encoders, this is not mentioned as the primary innovation of SLAHAN.\n\nOption B is incorrect because the documentation doesn't mention any post-processing step. SLAHAN aims to improve compression quality during the decoding process itself.\n\nOption D is incorrect because the documentation doesn't mention using reinforcement learning. The improvements in ROUGE scores are a result of the model's architecture, not the training approach.\n\nThe correct answer highlights SLAHAN's unique ability to look ahead and consider syntactic relationships, which is crucial for maintaining informativeness and grammaticality in sentence compression."}, "45": {"documentation": {"title": "Breaking Symmetries of the Reservoir Equations in Echo State Networks", "source": "Joschka Herteux, Christoph R\\\"ath", "docs_id": "2010.07103", "section": ["physics.data-an", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Breaking Symmetries of the Reservoir Equations in Echo State Networks. Reservoir computing has repeatedly been shown to be extremely successful in the prediction of nonlinear time-series. However, there is no complete understanding of the proper design of a reservoir yet. We find that the simplest popular setup has a harmful symmetry, which leads to the prediction of what we call mirror-attractor. We prove this analytically. Similar problems can arise in a general context, and we use them to explain the success or failure of some designs. The symmetry is a direct consequence of the hyperbolic tangent activation function. Further, four ways to break the symmetry are compared numerically: A bias in the output, a shift in the input, a quadratic term in the readout, and a mixture of even and odd activation functions. Firstly, we test their susceptibility to the mirror-attractor. Secondly, we evaluate their performance on the task of predicting Lorenz data with the mean shifted to zero. The short-time prediction is measured with the forecast horizon while the largest Lyapunov exponent and the correlation dimension are used to represent the climate. Finally, the same analysis is repeated on a combined dataset of the Lorenz attractor and the Halvorsen attractor, which we designed to reveal potential problems with symmetry. We find that all methods except the output bias are able to fully break the symmetry with input shift and quadratic readout performing the best overall."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Echo State Networks and reservoir computing, which of the following statements is correct regarding the symmetry problem and its solutions?\n\nA) The symmetry problem is caused by the sigmoid activation function and can be fully resolved by adding a bias to the output layer.\n\nB) The mixture of even and odd activation functions is the most effective method for breaking the symmetry and improving performance on complex datasets.\n\nC) The input shift and quadratic readout methods performed best overall in breaking symmetry and predicting both short-term behavior and long-term climate of the test datasets.\n\nD) The hyperbolic tangent activation function introduces a beneficial symmetry that improves the prediction of nonlinear time-series in all cases.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the documentation, the study found that input shift and quadratic readout methods performed the best overall in breaking the harmful symmetry and improving predictions. These methods were effective in both short-term predictions (measured by forecast horizon) and long-term climate predictions (measured by largest Lyapunov exponent and correlation dimension) for the Lorenz data and the combined Lorenz-Halvorsen dataset.\n\nOption A is incorrect because while the symmetry problem is indeed caused by the hyperbolic tangent (not sigmoid) activation function, adding a bias to the output layer was not the most effective solution. The document states that \"all methods except the output bias are able to fully break the symmetry.\"\n\nOption B is incorrect because although the mixture of even and odd activation functions was one of the methods tested, it was not specifically mentioned as the most effective method.\n\nOption D is incorrect because the symmetry introduced by the hyperbolic tangent function is described as \"harmful\" in the document, leading to the prediction of a \"mirror-attractor,\" which is problematic for accurate predictions."}, "46": {"documentation": {"title": "Mental Health and Abortions among Young Women: Time-varying Unobserved\n  Heterogeneity, Health Behaviors, and Risky Decisions", "source": "Lena Janys and Bettina Siflinger", "docs_id": "2103.12159", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mental Health and Abortions among Young Women: Time-varying Unobserved\n  Heterogeneity, Health Behaviors, and Risky Decisions. In this paper, we provide causal evidence on abortions and risky health behaviors as determinants of mental health development among young women. Using administrative in- and outpatient records from Sweden, we apply a novel grouped fixed-effects estimator proposed by Bonhomme and Manresa (2015) to allow for time-varying unobserved heterogeneity. We show that the positive association obtained from standard estimators shrinks to zero once we control for grouped time-varying unobserved heterogeneity. We estimate the group-specific profiles of unobserved heterogeneity, which reflect differences in unobserved risk to be diagnosed with a mental health condition. We then analyze mental health development and risky health behaviors other than unwanted pregnancies across groups. Our results suggest that these are determined by the same type of unobserved heterogeneity, which we attribute to the same unobserved process of decision-making. We develop and estimate a theoretical model of risky choices and mental health, in which mental health disparity across groups is generated by different degrees of self-control problems. Our findings imply that mental health concerns cannot be used to justify restrictive abortion policies. Moreover, potential self-control problems should be targeted as early as possible to combat future mental health consequences."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: According to the study described, which of the following statements best reflects the researchers' findings regarding the relationship between abortions and mental health among young women?\n\nA) Abortions have a significant causal impact on deteriorating mental health outcomes.\n\nB) Standard estimators show a positive association between abortions and mental health issues, which remains consistent when controlling for time-varying unobserved heterogeneity.\n\nC) The study found no causal relationship between abortions and mental health outcomes once time-varying unobserved heterogeneity was controlled for.\n\nD) The research conclusively proves that abortions improve mental health outcomes in young women.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study found that the positive association between abortions and mental health issues obtained from standard estimators \"shrinks to zero\" once they controlled for grouped time-varying unobserved heterogeneity. This indicates that there is no causal relationship between abortions and mental health outcomes when accounting for these unobserved factors.\n\nAnswer A is incorrect because the study does not support a causal impact of abortions on deteriorating mental health.\n\nAnswer B is incorrect because while standard estimators did show a positive association, this association disappeared when controlling for time-varying unobserved heterogeneity.\n\nAnswer D is incorrect because the study does not claim that abortions improve mental health outcomes; rather, it found no causal relationship between the two.\n\nThe question tests understanding of the study's methodology and main findings, requiring careful reading and interpretation of the complex results presented in the documentation."}, "47": {"documentation": {"title": "Jet Sampling: Improving Event Reconstruction through Multiple\n  Interpretations", "source": "Dilani Kahawala, David Krohn, and Matthew D. Schwartz", "docs_id": "1304.2394", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jet Sampling: Improving Event Reconstruction through Multiple\n  Interpretations. The classification of events involving jets as signal-like or background-like can depend strongly on the jet algorithm used and its parameters. This is partly due to the fact that standard jet algorithms yield a single partition of the particles in an event into jets, even if no particular choice stands out from the others. As an alternative, we propose that one should consider multiple interpretations of each event, generalizing the Qjets procedure to event-level analysis. With multiple interpretations, an event is no longer restricted to either satisfy cuts or not satisfy them - it can be assigned a weight between 0 and 1 based on how well it satisfies the cuts. These cut-weights can then be used to improve the discrimination power of an analysis or reduce the uncertainty on mass or cross-section measurements. For example, using this approach on a Higgs plus Z boson sample, with h->bb we find an 28% improvement in significance can be realized at the 8 TeV LHC. Through a number of other examples, we show various ways in which having multiple interpretations can be useful on the event level."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: The Jet Sampling method proposed in the paper aims to improve event reconstruction in particle physics by:\n\nA) Increasing the energy resolution of individual jet measurements\nB) Applying machine learning algorithms to classify jets more accurately\nC) Considering multiple interpretations of each event and assigning weights based on cut satisfaction\nD) Modifying existing jet algorithms to produce more stable results\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The Jet Sampling method, as described in the document, proposes considering multiple interpretations of each event, rather than restricting an event to either satisfying cuts or not. This approach allows for assigning a weight between 0 and 1 to each event based on how well it satisfies the cuts, which can improve discrimination power and reduce uncertainty in measurements.\n\nAnswer A is incorrect because the method doesn't focus on improving the energy resolution of individual jets, but rather on event-level analysis.\n\nAnswer B is incorrect because while machine learning might be used in conjunction with this method, the core idea is about multiple interpretations, not specifically about applying machine learning algorithms.\n\nAnswer D is incorrect because the method doesn't modify existing jet algorithms, but instead proposes a new approach to using their results by considering multiple interpretations of the same event.\n\nThe key innovation of this method is that it moves away from the binary classification of events (signal-like or background-like) based on a single jet algorithm result, and instead considers the event from multiple perspectives, potentially improving the overall analysis."}, "48": {"documentation": {"title": "Non-Weyl Microwave Graphs", "source": "Micha{\\l} {\\L}awniczak, Ji\\v{r}\\'i Lipovsk\\'y, and Leszek Sirko", "docs_id": "1904.06905", "section": ["quant-ph", "math-ph", "math.MP", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-Weyl Microwave Graphs. One of the most important characteristics of a quantum graph is the average density of resonances, $\\rho = \\frac{\\mathcal{L}}{\\pi}$, where $\\mathcal{L}$ denotes the length of the graph. This is a very robust measure. It does not depend on the number of vertices in a graph and holds also for most of the boundary conditions at the vertices. Graphs obeying this characteristic are called Weyl graphs. Using microwave networks which simulate quantum graphs we show that there exist graphs which do not adhere to this characteristic. Such graphs will be called non-Weyl graphs. For standard coupling conditions we demonstrate that the transition from a Weyl graph to a non-Weyl graph occurs if we introduce a balanced vertex. A vertex of a graph is called balanced if the numbers of infinite leads and internal edges meeting at a vertex are the same. Our experimental results confirm the theoretical predictions of [E. B. Davies and A. Pushnitski, Analysis and PDE 4, 729 (2011)] and are in excellent agreement with the numerical calculations yielding the resonances of the networks."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: A quantum graph with total length L = 10\u03c0 has 5 vertices and exhibits non-Weyl behavior. Which of the following statements is most likely true about this graph?\n\nA) The average density of resonances is exactly 10\nB) The graph has no balanced vertices\nC) At least one vertex in the graph has an equal number of infinite leads and internal edges\nD) The graph follows standard coupling conditions but cannot have any infinite leads\n\nCorrect Answer: C\n\nExplanation: \nA) is incorrect because for a non-Weyl graph, the average density of resonances does not follow the standard formula \u03c1 = L/\u03c0. Even if it were a Weyl graph, \u03c1 would be 10, but this is not guaranteed for a non-Weyl graph.\n\nB) is incorrect because the presence of at least one balanced vertex is what typically causes a graph to exhibit non-Weyl behavior. The absence of balanced vertices would likely result in Weyl behavior.\n\nC) is correct. The passage states that \"the transition from a Weyl graph to a non-Weyl graph occurs if we introduce a balanced vertex.\" A balanced vertex is defined as one where \"the numbers of infinite leads and internal edges meeting at a vertex are the same.\" Since this graph exhibits non-Weyl behavior, it likely has at least one balanced vertex.\n\nD) is incorrect because while the graph may follow standard coupling conditions, the presence of infinite leads is not incompatible with non-Weyl behavior. In fact, infinite leads are part of what defines a balanced vertex."}, "49": {"documentation": {"title": "Could a Hexagonal Sunspot Have Been Observed During the Maunder Minimum?", "source": "V.M.S. Carrasco, J.M. Vaquero, M.C. Gallego", "docs_id": "1803.00358", "section": ["astro-ph.SR", "physics.hist-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Could a Hexagonal Sunspot Have Been Observed During the Maunder Minimum?. The Maunder Minimum was the period between 1645 and 1715 whose main characteristic was abnormally low and prolonged solar activity. However, some authors have doubted this low level of solar activity during that period by questioning the accuracy and objectivity of the observers. This work presents a particular case of a sunspot observed during the Maunder Minimum with an unusual shape of its umbra and penumbra: a hexagon. This sunspot was observed by Cassini in November 1676, just at the core of the Maunder Minimum. This historical observation is compared with a twin case that occurred recently in May 2016. The conclusion reached is that Cassini's record is another example of the good quality observations made during the Maunder Minimum, showing the meticulousness of the astronomers of that epoch. This sunspot observation made by Cassini does not support the conclusions of Zolotova and Ponyavin (Astrophys. J. 800, 42, 2015) that professional astronomers in the 17th century only registered round sunspots. Finally, a discussion is given of the importance of this kind of unusual sunspot record for a better assessment of the true level of solar activity in the Maunder Minimum."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best represents the significance of Cassini's hexagonal sunspot observation in 1676 with respect to our understanding of the Maunder Minimum?\n\nA) It proves that the Maunder Minimum was a period of normal solar activity, contrary to popular belief.\n\nB) It demonstrates that astronomers during the Maunder Minimum were incapable of accurately recording sunspot shapes.\n\nC) It provides evidence supporting the high quality of astronomical observations during the Maunder Minimum and challenges claims that only round sunspots were recorded in that era.\n\nD) It suggests that hexagonal sunspots were more common during the Maunder Minimum than in modern times.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because Cassini's observation of a hexagonal sunspot in 1676, during the core of the Maunder Minimum, serves as evidence of the high-quality observations made during that period. This observation challenges the claim made by some researchers (specifically Zolotova and Ponyavin) that professional astronomers in the 17th century only registered round sunspots. The hexagonal shape, being unusual, demonstrates the meticulousness of astronomers like Cassini during the Maunder Minimum. This observation is significant as it contributes to a more accurate assessment of solar activity during this period and counters doubts about the low level of solar activity reported during the Maunder Minimum.\n\nOption A is incorrect because the observation doesn't prove normal solar activity during the Maunder Minimum; it only provides insight into the quality of observations.\n\nOption B is incorrect as the observation actually demonstrates the opposite \u2013 that astronomers were capable of accurately recording complex sunspot shapes.\n\nOption D is incorrect because there's no evidence provided to suggest hexagonal sunspots were more common during the Maunder Minimum. The observation is noted for its unusualness, not its frequency."}, "50": {"documentation": {"title": "Estimating the Number of Essential Genes in Random Transposon\n  Mutagenesis Libraries", "source": "Oliver Will, Michael A Jacobs", "docs_id": "q-bio/0608005", "section": ["q-bio.OT", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating the Number of Essential Genes in Random Transposon\n  Mutagenesis Libraries. Biologists use random transposon mutagenesis to construct knockout libraries for bacteria. Random mutagenesis offers cost and efficiency benefits over the standard site directed mutagenesis, but one can no longer ensure that all the nonessential genes will appear in the library. In random libraries for haploid organisms, there is always a class of genes for which knockout clones have not been made, and the members of this class are either essential or nonessential. One requires statistical methods to estimate the number of essential genes. Two groups of researchers, Blades and Broman and Jacobs et al., independently and simultaneously developed methods to do this. Blades and Broman used a Gibbs sampler and Jacobs et al. used a parametric bootstrap. We compare the performance of these two methods and find that they both depend on having an accurate probabilistic model for transposon insertion or on having a library with a large number of clones. At this point, we do not have good enough probabilistic models so we must build libraries that have at least five clones per open reading frame to accurately estimate the number of essential genes."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In estimating the number of essential genes using random transposon mutagenesis libraries, which of the following statements is most accurate?\n\nA) The Gibbs sampler method developed by Blades and Broman is consistently more accurate than the parametric bootstrap method by Jacobs et al.\n\nB) Statistical methods are unnecessary for determining essential genes in random transposon mutagenesis libraries.\n\nC) Current probabilistic models for transposon insertion are sufficiently accurate to estimate essential genes with small libraries.\n\nD) To accurately estimate the number of essential genes, libraries should have at least five clones per open reading frame.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that \"we must build libraries that have at least five clones per open reading frame to accurately estimate the number of essential genes.\" This is because current probabilistic models for transposon insertion are not accurate enough to make reliable estimates with smaller libraries.\n\nAnswer A is incorrect because the document doesn't state that one method is consistently more accurate than the other. It mentions that both methods have similar limitations.\n\nAnswer B is incorrect because the document explicitly states that statistical methods are required to estimate the number of essential genes in random libraries.\n\nAnswer C is incorrect because the document clearly states that \"we do not have good enough probabilistic models\" for accurate estimation with small libraries."}, "51": {"documentation": {"title": "Discovery of 178 Giant Radio Galaxies in 1059 deg$^2$ of the Rapid ASKAP\n  Continuum Survey at 888 MHz", "source": "Heinz Andernach, Eric F. Jim\\'enez-Andrade, and Anthony G. Willis", "docs_id": "2111.08807", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discovery of 178 Giant Radio Galaxies in 1059 deg$^2$ of the Rapid ASKAP\n  Continuum Survey at 888 MHz. We report the results of a visual inspection of images of the Rapid ASKAP Continuum Survey (RACS) in search of extended radio galaxies (ERG) that reach or exceed linear sizes on the order of one Megaparsec. We searched a contiguous area of 1059deg$^2$ from RA$_{\\rm J}$=20$^h$20$^m$ to 06$^h$20$^m$, and $-50^{\\circ}<\\rm{Dec}_J<-40^{\\circ}$, which is covered by deep multi-band optical images of the Dark Energy Survey (DES), and in which previously only three ERGs larger than 1Mpc had been reported. For over 1800 radio galaxy candidates inspected, our search in optical and infrared images resulted in hosts for 1440 ERG, for which spectroscopic and photometric redshifts from various references were used to convert their largest angular size (LAS) to projected linear size (LLS). This resulted in 178 newly discovered giant radio sources (GRS) with LLS$>$1Mpc, of which 18 exceed 2Mpc and the largest one is 3.4Mpc. Their redshifts range from 0.02 to $\\sim$2.0, but only 10 of the 178 new GRS have spectroscopic redshifts. For the 146 host galaxies the median $r$-band magnitude and redshift are 20.9 and 0.64, while for the 32 quasars or candidates these are 19.7 and 0.75. Merging the six most recent large compilations of GRS results in 458 GRS larger than 1Mpc, so we were able to increase this number by $\\sim39\\%$ to now 636."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: A team of researchers conducted a visual inspection of images from the Rapid ASKAP Continuum Survey (RACS) to search for extended radio galaxies (ERG). Which of the following statements most accurately describes their findings and the impact on our understanding of giant radio sources (GRS)?\n\nA) They discovered 178 new GRS with linear sizes between 1-2 Mpc, increasing the total known GRS by approximately 25%.\n\nB) The search covered an area of 1059 deg\u00b2, resulting in 1440 ERG hosts identified, with 178 new GRS larger than 1 Mpc, including 18 exceeding 2 Mpc.\n\nC) The largest GRS discovered was 5 Mpc in size, with most new GRS having spectroscopic redshifts between 0.02 and 2.0.\n\nD) The study focused on a region previously known to contain dozens of GRS, confirming 178 of these and adding 18 new discoveries larger than 2 Mpc.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately summarizes the key findings of the study. The researchers searched an area of 1059 deg\u00b2 and identified hosts for 1440 ERG. They discovered 178 new giant radio sources (GRS) with linear sizes larger than 1 Mpc, of which 18 exceed 2 Mpc. \n\nAnswer A is incorrect because it understates the impact (the total known GRS increased by about 39%, not 25%) and doesn't mention the larger GRS discovered.\n\nAnswer C is incorrect because the largest GRS discovered was 3.4 Mpc, not 5 Mpc, and only 10 of the 178 new GRS have spectroscopic redshifts, not most of them.\n\nAnswer D is incorrect because the region was not previously known to contain dozens of GRS; in fact, only three ERGs larger than 1 Mpc had been reported in this area before the study.\n\nThis question tests the student's ability to accurately interpret and synthesize complex scientific findings from a research report."}, "52": {"documentation": {"title": "Longitudinal and transverse spin transfer to $\\Lambda$ and $\\bar\\Lambda$\n  hyperons in p+p collisions at STAR", "source": "Qing-Hua Xu (for the STAR Collaboration)", "docs_id": "1812.10621", "section": ["hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal and transverse spin transfer to $\\Lambda$ and $\\bar\\Lambda$\n  hyperons in p+p collisions at STAR. The longitudinal or transverse spin transfer to Lambda and anti-Lambda hyperons in polarized proton-proton collisions is expected to be sensitive to the helicity or transversity distributions of strange and anti-strange quarks of the proton, and to the corresponding polarized fragmentation function. We report the first measurement of the transverse spin transfer to $\\Lambda$ and $\\bar \\Lambda$ along the polarization direction of the fragmenting quark, $D_{TT}$, in transversely polarized proton-proton collisions at 200 GeV with the STAR experiment at RHIC. The data correspond to an integrated luminosity of 18 pb$^{-1}$, and cover a kinematic range of |$\\eta$|< 1.2 and transverse momentum $p_T$ up to 8 GeV/c. We also report an improved measurement of the longitudinal spin transfer $D_{LL}$ to $\\Lambda$ and $\\bar \\Lambda$ with $p_T$ up to 6 GeV/c, using data with about twelve times larger figure-of-merit than the previously published STAR results. The prospects of hyperon polarization measurements in the forward pseudo-rapidity region (2.5<$\\eta$<4) in p+p collision in the year of 2021 and beyond will also be discussed, which is based on the STAR forward detector upgrade plan including a forward tracking system and a forward calorimeter system."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements is NOT correct regarding the STAR experiment's measurements of spin transfer to \u039b and anti-\u039b hyperons in polarized proton-proton collisions?\n\nA) The experiment reported the first measurement of transverse spin transfer DTT to \u039b and anti-\u039b hyperons.\nB) The longitudinal spin transfer DLL measurement used data with approximately twelve times larger figure-of-merit than previous STAR results.\nC) The kinematic range covered in the experiment included pseudorapidity |\u03b7| < 1.2 and transverse momentum pT up to 10 GeV/c.\nD) The data for the transverse spin transfer measurement corresponded to an integrated luminosity of 18 pb^-1.\n\nCorrect Answer: C\n\nExplanation: Option C is incorrect and thus the correct answer to this question. The Arxiv documentation states that the transverse momentum pT was measured up to 8 GeV/c for DTT and up to 6 GeV/c for DLL, not 10 GeV/c as stated in option C. \n\nOption A is correct as the document explicitly states this is the first measurement of transverse spin transfer DTT to \u039b and anti-\u039b hyperons.\n\nOption B is correct as the documentation mentions an improved measurement of DLL using data with about twelve times larger figure-of-merit than previously published STAR results.\n\nOption D is correct as the documentation clearly states that the data correspond to an integrated luminosity of 18 pb^-1."}, "53": {"documentation": {"title": "Do Public Program Benefits Crowd Out Private Transfers in Developing\n  Countries? A Critical Review of Recent Evidence", "source": "Plamen Nikolov, Matthew Bonci", "docs_id": "2006.00737", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Do Public Program Benefits Crowd Out Private Transfers in Developing\n  Countries? A Critical Review of Recent Evidence. Precipitated by rapid globalization, rising inequality, population growth, and longevity gains, social protection programs have been on the rise in low- and middle-income countries (LMICs) in the last three decades. However, the introduction of public benefits could displace informal mechanisms for risk-protection, which are especially prevalent in LMICs. If the displacement of private transfers is considerably large, the expansion of social protection programs could even lead to social welfare loss. In this paper, we critically survey the recent empirical literature on crowd-out effects in response to public policies, specifically in the context of LMICs. We review and synthesize patterns from the behavioral response to various types of social protection programs. Furthermore, we specifically examine for heterogeneous treatment effects by important socioeconomic characteristics. We conclude by drawing on lessons from our synthesis of studies. If poverty reduction objectives are considered, along with careful program targeting that accounts for potential crowd-out effects, there may well be a net social gain."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the potential consequences of introducing public social protection programs in low- and middle-income countries (LMICs), according to the passage?\n\nA) The introduction of public benefits will always lead to social welfare loss due to the displacement of informal risk-protection mechanisms.\n\nB) Public social protection programs in LMICs have no impact on existing informal risk-protection mechanisms.\n\nC) The expansion of social protection programs could potentially lead to social welfare loss if the displacement of private transfers is considerably large.\n\nD) Public social protection programs in LMICs are guaranteed to improve social welfare regardless of their impact on informal risk-protection mechanisms.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that \"If the displacement of private transfers is considerably large, the expansion of social protection programs could even lead to social welfare loss.\" This indicates that there is a potential for social welfare loss if public programs significantly displace informal risk-protection mechanisms, but it's not guaranteed to happen in all cases.\n\nOption A is incorrect because it overstates the case, suggesting that social welfare loss will always occur, which is not supported by the passage.\n\nOption B is incorrect because the passage clearly indicates that public programs can impact informal risk-protection mechanisms, potentially displacing them.\n\nOption D is incorrect because it ignores the potential negative consequences of displacement mentioned in the passage and overstates the guaranteed benefits of public programs.\n\nThe question tests the reader's ability to understand the nuanced relationship between public social protection programs and existing informal mechanisms in LMICs, as well as the potential consequences of their interaction."}, "54": {"documentation": {"title": "Optimal retirement consumption with a stochastic force of mortality", "source": "Huaxiong Huang, Moshe A. Milevsky and Thomas S. Salisbury", "docs_id": "1205.2295", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal retirement consumption with a stochastic force of mortality. We extend the lifecycle model (LCM) of consumption over a random horizon (a.k.a. the Yaari model) to a world in which (i.) the force of mortality obeys a diffusion process as opposed to being deterministic, and (ii.) a consumer can adapt their consumption strategy to new information about their mortality rate (a.k.a. health status) as it becomes available. In particular, we derive the optimal consumption rate and focus on the impact of mortality rate uncertainty vs. simple lifetime uncertainty -- assuming the actuarial survival curves are initially identical -- in the retirement phase where this risk plays a greater role. In addition to deriving and numerically solving the PDE for the optimal consumption rate, our main general result is that when utility preferences are logarithmic the initial consumption rates are identical. But, in a CRRA framework in which the coefficient of relative risk aversion is greater (smaller) than one, the consumption rate is higher (lower) and a stochastic force of mortality does make a difference. That said, numerical experiments indicate that even for non-logarithmic preferences, the stochastic mortality effect is relatively minor from the individual's perspective. Our results should be relevant to researchers interested in calibrating the lifecycle model as well as those who provide normative guidance (a.k.a. financial advice) to retirees."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the extended lifecycle model (LCM) with a stochastic force of mortality, how does the optimal consumption rate compare to the traditional Yaari model when the coefficient of relative risk aversion (CRRA) is greater than one, assuming initially identical actuarial survival curves?\n\nA) The optimal consumption rate is lower than in the Yaari model\nB) The optimal consumption rate is identical to the Yaari model\nC) The optimal consumption rate is higher than in the Yaari model\nD) The optimal consumption rate is unpredictable and varies randomly\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of how the stochastic force of mortality affects optimal consumption in the extended LCM compared to the traditional Yaari model. The key information is provided in the passage: \"But, in a CRRA framework in which the coefficient of relative risk aversion is greater (smaller) than one, the consumption rate is higher (lower) and a stochastic force of mortality does make a difference.\"\n\nWhen the coefficient of relative risk aversion is greater than one, the model predicts a higher consumption rate compared to the Yaari model. This is counterintuitive, as one might expect more risk-averse individuals (CRRA > 1) to consume less when faced with uncertainty. However, the model suggests that the ability to adapt consumption to new mortality information leads to higher initial consumption rates for these individuals.\n\nOption A is incorrect because it states the opposite of the model's prediction. Option B is incorrect because it applies only to logarithmic utility preferences, not when CRRA > 1. Option D is incorrect because the model does provide a predictable direction for the consumption rate, not a random variation."}, "55": {"documentation": {"title": "Siamese Networks with Location Prior for Landmark Tracking in Liver\n  Ultrasound Sequences", "source": "Alvaro Gomariz, Weiye Li, Ece Ozkan, Christine Tanner, Orcun Goksel", "docs_id": "1901.08109", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Siamese Networks with Location Prior for Landmark Tracking in Liver\n  Ultrasound Sequences. Image-guided radiation therapy can benefit from accurate motion tracking by ultrasound imaging, in order to minimize treatment margins and radiate moving anatomical targets, e.g., due to breathing. One way to formulate this tracking problem is the automatic localization of given tracked anatomical landmarks throughout a temporal ultrasound sequence. For this, we herein propose a fully-convolutional Siamese network that learns the similarity between pairs of image regions containing the same landmark. Accordingly, it learns to localize and thus track arbitrary image features, not only predefined anatomical structures. We employ a temporal consistency model as a location prior, which we combine with the network-predicted location probability map to track a target iteratively in ultrasound sequences. We applied this method on the dataset of the Challenge on Liver Ultrasound Tracking (CLUST) with competitive results, where our work is the first to effectively apply CNNs on this tracking problem, thanks to our temporal regularization."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the innovative approach and key advantages of the proposed method for landmark tracking in liver ultrasound sequences?\n\nA) It uses a recurrent neural network to predict landmark positions based solely on temporal information without considering image features.\n\nB) It employs a fully-convolutional Siamese network to learn similarity between image regions, combined with a temporal consistency model as a location prior.\n\nC) It relies exclusively on predefined anatomical structures and cannot track arbitrary image features.\n\nD) It uses a traditional computer vision approach without deep learning, focusing on edge detection and feature matching algorithms.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately describes the key components and advantages of the proposed method. The approach uses a fully-convolutional Siamese network to learn similarities between image regions containing the same landmark, which allows it to track arbitrary image features rather than just predefined anatomical structures. Additionally, it incorporates a temporal consistency model as a location prior, which is combined with the network-predicted location probability map to track targets iteratively in ultrasound sequences.\n\nOption A is incorrect because the method doesn't solely rely on temporal information and does consider image features through the Siamese network.\n\nOption C is incorrect because the method can track arbitrary image features and is not limited to predefined anatomical structures.\n\nOption D is incorrect because the approach does use deep learning (specifically a CNN-based Siamese network) rather than traditional computer vision techniques.\n\nThis question tests understanding of the method's key components, its ability to track arbitrary features, and how it combines deep learning with temporal consistency for improved tracking in ultrasound sequences."}, "56": {"documentation": {"title": "Enabling Fast and Universal Audio Adversarial Attack Using Generative\n  Model", "source": "Yi Xie, Zhuohang Li, Cong Shi, Jian Liu, Yingying Chen, Bo Yuan", "docs_id": "2004.12261", "section": ["eess.AS", "cs.LG", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enabling Fast and Universal Audio Adversarial Attack Using Generative\n  Model. Recently, the vulnerability of DNN-based audio systems to adversarial attacks has obtained the increasing attention. However, the existing audio adversarial attacks allow the adversary to possess the entire user's audio input as well as granting sufficient time budget to generate the adversarial perturbations. These idealized assumptions, however, makes the existing audio adversarial attacks mostly impossible to be launched in a timely fashion in practice (e.g., playing unnoticeable adversarial perturbations along with user's streaming input). To overcome these limitations, in this paper we propose fast audio adversarial perturbation generator (FAPG), which uses generative model to generate adversarial perturbations for the audio input in a single forward pass, thereby drastically improving the perturbation generation speed. Built on the top of FAPG, we further propose universal audio adversarial perturbation generator (UAPG), a scheme crafting universal adversarial perturbation that can be imposed on arbitrary benign audio input to cause misclassification. Extensive experiments show that our proposed FAPG can achieve up to 167X speedup over the state-of-the-art audio adversarial attack methods. Also our proposed UAPG can generate universal adversarial perturbation that achieves much better attack performance than the state-of-the-art solutions."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the key innovation and advantage of the Fast Audio Adversarial Perturbation Generator (FAPG) compared to existing audio adversarial attacks?\n\nA) It allows the adversary to possess the entire user's audio input before generating perturbations.\nB) It generates adversarial perturbations in a single forward pass, significantly improving generation speed.\nC) It creates universal perturbations that work on any audio input without modification.\nD) It requires a substantial time budget to generate highly effective adversarial perturbations.\n\nCorrect Answer: B\n\nExplanation: The key innovation of FAPG is its ability to generate adversarial perturbations for audio input in a single forward pass, which drastically improves the perturbation generation speed. This is in contrast to existing methods that typically require more time and processing. Option A describes a limitation of existing methods that FAPG aims to overcome. Option C refers to UAPG, a separate technique built on top of FAPG, not FAPG itself. Option D contradicts the main advantage of FAPG, which is its speed in generating perturbations."}, "57": {"documentation": {"title": "High-throughput molecular imaging via deep learning enabled Raman\n  spectroscopy", "source": "Conor C. Horgan, Magnus Jensen, Anika Nagelkerke, Jean-Phillipe\n  St-Pierre, Tom Vercauteren, Molly M. Stevens, Mads S. Bergholt", "docs_id": "2009.13318", "section": ["eess.IV", "cs.CV", "physics.med-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-throughput molecular imaging via deep learning enabled Raman\n  spectroscopy. Raman spectroscopy enables non-destructive, label-free imaging with unprecedented molecular contrast but is limited by slow data acquisition, largely preventing high-throughput imaging applications. Here, we present a comprehensive framework for higher-throughput molecular imaging via deep learning enabled Raman spectroscopy, termed DeepeR, trained on a large dataset of hyperspectral Raman images, with over 1.5 million spectra (400 hours of acquisition) in total. We firstly perform denoising and reconstruction of low signal-to-noise ratio Raman molecular signatures via deep learning, with a 9x improvement in mean squared error over state-of-the-art Raman filtering methods. Next, we develop a neural network for robust 2-4x super-resolution of hyperspectral Raman images that preserves molecular cellular information. Combining these approaches, we achieve Raman imaging speed-ups of up to 160x, enabling high resolution, high signal-to-noise ratio cellular imaging in under one minute. Finally, transfer learning is applied to extend DeepeR from cell to tissue-scale imaging. DeepeR provides a foundation that will enable a host of higher-throughput Raman spectroscopy and molecular imaging applications across biomedicine."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following combinations best describes the key innovations and outcomes of the DeepeR framework for high-throughput Raman spectroscopy imaging?\n\nA) Denoising with 5x improvement, 3x super-resolution, and 100x speed-up in cellular imaging\nB) Denoising with 9x improvement, 2-4x super-resolution, and 160x speed-up in cellular imaging\nC) Denoising with 9x improvement, 5x super-resolution, and 100x speed-up in tissue imaging\nD) Denoising with 7x improvement, 3-5x super-resolution, and 140x speed-up in cellular imaging\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The DeepeR framework, as described in the text, achieves several key improvements:\n\n1. Denoising and reconstruction of Raman spectra with a 9x improvement in mean squared error compared to state-of-the-art methods.\n2. Development of a neural network for 2-4x super-resolution of hyperspectral Raman images.\n3. Combination of these approaches leads to Raman imaging speed-ups of up to 160x, enabling high-resolution, high signal-to-noise ratio cellular imaging in under one minute.\n\nOption A is incorrect because it understates the denoising improvement (5x instead of 9x) and the speed-up (100x instead of 160x).\nOption C is incorrect because it overstates the super-resolution (5x instead of 2-4x) and incorrectly applies the speed-up to tissue imaging rather than cellular imaging.\nOption D is incorrect because it understates the denoising improvement (7x instead of 9x) and uses an incorrect range for super-resolution (3-5x instead of 2-4x).\n\nThis question tests the student's ability to carefully read and synthesize multiple pieces of technical information from the text, making it suitable for an advanced exam on the topic."}, "58": {"documentation": {"title": "Maker-Breaker games on random geometric graphs", "source": "Andrew Beveridge, Andrzej Dudek, Alan Frieze, Tobias Muller, Milos\n  Stojakovic", "docs_id": "1309.5759", "section": ["math.CO", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Maker-Breaker games on random geometric graphs. In a Maker-Breaker game on a graph $G$, Breaker and Maker alternately claim edges of $G$. Maker wins if, after all edges have been claimed, the graph induced by his edges has some desired property. We consider four Maker-Breaker games played on random geometric graphs. For each of our four games we show that if we add edges between $n$ points chosen uniformly at random in the unit square by order of increasing edge-length then, with probability tending to one as $n\\to\\infty$, the graph becomes Maker-win the very moment it satisfies a simple necessary condition. In particular, with high probability, Maker wins the connectivity game as soon as the minimum degree is at least two; Maker wins the Hamilton cycle game as soon as the minimum degree is at least four; Maker wins the perfect matching game as soon as the minimum degree is at least two and every edge has at least three neighbouring vertices; and Maker wins the $H$-game as soon as there is a subgraph from a finite list of \"minimal graphs\". These results also allow us to give precise expressions for the limiting probability that $G(n,r)$ is Maker-win in each case, where $G(n,r)$ is the graph on $n$ points chosen uniformly at random on the unit square with an edge between two points if and only if their distance is at most $r$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a Maker-Breaker game on a random geometric graph G(n,r), where n points are chosen uniformly at random in the unit square and edges are added between points with distance at most r, which of the following statements is NOT correct regarding the conditions for Maker to win with high probability as n approaches infinity?\n\nA) Maker wins the connectivity game as soon as the minimum degree of the graph is at least two.\n\nB) Maker wins the Hamilton cycle game as soon as the minimum degree of the graph is at least four.\n\nC) Maker wins the perfect matching game as soon as the minimum degree is at least three and every edge has at least two neighboring vertices.\n\nD) Maker wins the H-game as soon as there is a subgraph from a finite list of \"minimal graphs\".\n\nCorrect Answer: C\n\nExplanation: \nOptions A, B, and D are correct statements based on the given information. However, option C is incorrect. The correct condition for Maker to win the perfect matching game with high probability is that the minimum degree should be at least two (not three) and every edge should have at least three (not two) neighboring vertices. This makes C the incorrect statement and thus the correct answer to the question asking which statement is NOT correct."}, "59": {"documentation": {"title": "Robust Lasso with missing and grossly corrupted observations", "source": "Nam H. Nguyen and Trac D. Tran", "docs_id": "1112.0391", "section": ["math.ST", "cs.IT", "math.IT", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Lasso with missing and grossly corrupted observations. This paper studies the problem of accurately recovering a sparse vector $\\beta^{\\star}$ from highly corrupted linear measurements $y = X \\beta^{\\star} + e^{\\star} + w$ where $e^{\\star}$ is a sparse error vector whose nonzero entries may be unbounded and $w$ is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both $\\beta^{\\star}$ and $e^{\\star}$. Our first result shows that the extended Lasso can faithfully recover both the regression as well as the corruption vector. Our analysis relies on the notion of extended restricted eigenvalue for the design matrix $X$. Our second set of results applies to a general class of Gaussian design matrix $X$ with i.i.d rows $\\oper N(0, \\Sigma)$, for which we can establish a surprising result: the extended Lasso can recover exact signed supports of both $\\beta^{\\star}$ and $e^{\\star}$ from only $\\Omega(k \\log p \\log n)$ observations, even when the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is indeed optimal."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the extended Lasso optimization for recovering a sparse vector \u03b2* from corrupted linear measurements, which of the following statements is correct regarding the number of observations required for exact signed support recovery in the case of a Gaussian design matrix X with i.i.d rows N(0, \u03a3)?\n\nA) \u03a9(k log p) observations are sufficient, where k is the sparsity of \u03b2* and p is the dimension of \u03b2*.\nB) \u03a9(k log n) observations are required, where k is the sparsity of \u03b2* and n is the number of observations.\nC) \u03a9(k log p log n) observations are necessary and sufficient, even when the fraction of corruption is arbitrarily close to one.\nD) \u03a9(n log p) observations are needed, where n is the number of observations and p is the dimension of \u03b2*.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that for a general class of Gaussian design matrix X with i.i.d rows N(0, \u03a3), the extended Lasso can recover exact signed supports of both \u03b2* and e* from only \u03a9(k log p log n) observations. This result holds even when the fraction of corruption is arbitrarily close to one. Furthermore, the documentation emphasizes that this amount of observations is optimal for achieving exact signed support recovery.\n\nOption A is incorrect because it doesn't include the log n factor and doesn't capture the full complexity of the required observations.\nOption B is incorrect as it omits the log p factor, which is crucial in determining the number of required observations.\nOption D is incorrect because it doesn't reflect the sparsity k and overestimates the required number of observations by using n instead of k."}}