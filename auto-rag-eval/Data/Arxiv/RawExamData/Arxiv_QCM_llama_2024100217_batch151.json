{"0": {"documentation": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "source": "Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert\n  Y\\\"uksekg\\\"on\\\"ul, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis", "docs_id": "1909.11229", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pretraining boosts out-of-domain robustness for pose estimation. Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both \"within-domain\" and \"out-of-domain\" (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary benefit of pretraining neural networks for pose estimation, according to the study \"Pretraining boosts out-of-domain robustness for pose estimation\"?\n\nA) Improved performance on within-domain data\nB) Enhanced ability to generalize across animal species\nC) Increased robustness to out-of-domain data\nD) Better ImageNet-performing architectures are more effective on within- and out-of-domain data\n\nCorrect Answer: C) Increased robustness to out-of-domain data\n\nExplanation: The study highlights the importance of pretraining neural networks for pose estimation, particularly in terms of out-of-domain robustness. The correct answer, C) Increased robustness to out-of-domain data, is supported by the study's findings that pretraining boosts out-of-domain robustness for pose estimation. The other options, while related to the study's results, are not the primary benefit of pretraining. Option A is incorrect because the study shows that pretraining improves performance on both within- and out-of-domain data, not just within-domain data. Option B is incorrect because while the study does show that better ImageNet models generalize better across animal species, this is not the primary benefit of pretraining. Option D is incorrect because the study shows that better ImageNet-performing architectures perform better on both within- and out-of-domain data, but this is a consequence of pretraining, not the primary benefit."}, "1": {"documentation": {"title": "Secret key agreement from correlated data, with no prior information", "source": "Marius Zimand", "docs_id": "1910.03757", "section": ["cs.IT", "cs.CR", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secret key agreement from correlated data, with no prior information. A fundamental question that has been studied in cryptography and in information theory is whether two parties can communicate confidentially using exclusively an open channel. We consider the model in which the two parties hold inputs that are correlated in a certain sense. This model has been studied extensively in information theory, and communication protocols have been designed which exploit the correlation to extract from the inputs a shared secret key. However, all the existing protocols are not universal in the sense that they require that the two parties also know some attributes of the correlation. In other words, they require that each party knows something about the other party's input. We present a protocol that does not require any prior additional information. It uses space-bounded Kolmogorov complexity to measure correlation and it allows the two legal parties to obtain a common key that looks random to an eavesdropper that observes the communication and is restricted to use a bounded amount of space for the attack. Thus the protocol achieves complexity-theoretical security, but it does not use any unproven result from computational complexity. On the negative side, the protocol is not efficient in the sense that the computation of the two legal parties uses more space than the space allowed to the adversary."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a secret key agreement protocol that uses correlated data to establish a shared secret key between two parties, without requiring any prior information about the correlation. What is the primary advantage of this protocol in terms of security, and how does it differ from existing protocols that require knowledge of the correlation?\n\n**A)** The protocol achieves computational complexity-based security, but requires prior knowledge of the correlation to function.\n**B)** The protocol uses space-bounded Kolmogorov complexity to measure correlation, allowing for universal security without prior knowledge of the correlation.\n**C)** The protocol is computationally infeasible for an adversary to break, but requires a large amount of space to compute the shared key.\n**D)** The protocol is not secure against an adversary with unlimited computational power, but achieves security through the use of correlated data.\n\n**Correct Answer:** B) The protocol uses space-bounded Kolmogorov complexity to measure correlation, allowing for universal security without prior knowledge of the correlation.\n\n**Explanation:** The correct answer, B, highlights the primary advantage of the protocol, which is its ability to achieve universal security without requiring prior knowledge of the correlation. This is in contrast to existing protocols that require knowledge of the correlation to function. The protocol's use of space-bounded Kolmogorov complexity to measure correlation enables it to extract a shared secret key from correlated data, making it a significant improvement over existing protocols."}, "2": {"documentation": {"title": "ThirdEye: Triplet Based Iris Recognition without Normalization", "source": "Sohaib Ahmad, Benjamin Fuller", "docs_id": "1907.06147", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ThirdEye: Triplet Based Iris Recognition without Normalization. Most iris recognition pipelines involve three stages: segmenting into iris/non-iris pixels, normalization the iris region to a fixed area, and extracting relevant features for comparison. Given recent advances in deep learning it is prudent to ask which stages are required for accurate iris recognition. Lojez et al. (IWBF 2019) recently concluded that the segmentation stage is still crucial for good accuracy.We ask if normalization is beneficial? Towards answering this question, we develop a new iris recognition system called ThirdEye based on triplet convolutional neural networks (Schroff et al., ICCV 2015). ThirdEye directly uses segmented images without normalization. We observe equal error rates of 1.32%, 9.20%, and 0.59% on the ND-0405, UbirisV2, and IITD datasets respectively. For IITD, the most constrained dataset, this improves on the best prior work. However, for ND-0405 and UbirisV2,our equal error rate is slightly worse than prior systems. Our concluding hypothesis is that normalization is more important for less constrained environments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn by the authors of the ThirdEye paper regarding the necessity of normalization in iris recognition pipelines, and what are the implications of this conclusion for less constrained environments?\n\nA) Normalization is crucial for accurate iris recognition, and its absence leads to significantly worse performance on all datasets. \nB) Normalization is not necessary for accurate iris recognition, and its absence does not affect performance on any dataset. \nC) Normalization is more important for less constrained environments, where it can improve performance and reduce error rates. \nD) Normalization is equally important for all datasets, regardless of their level of constraint.\n\nCorrect Answer: C) Normalization is more important for less constrained environments, where it can improve performance and reduce error rates.\n\nExplanation: The authors of the ThirdEye paper conclude that normalization is more important for less constrained environments, as evidenced by the fact that their system without normalization performs equally well on the most constrained dataset (IITD) but slightly worse on the less constrained datasets (ND-0405 and UbirisV2). This suggests that normalization plays a more critical role in maintaining accuracy in environments with more variability and noise."}, "3": {"documentation": {"title": "Spectral Efficiency in Large-Scale MIMO-OFDM Systems with Per-Antenna\n  Power Cost", "source": "Derrick Wing Kwan Ng and Robert Schober", "docs_id": "1211.3503", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral Efficiency in Large-Scale MIMO-OFDM Systems with Per-Antenna\n  Power Cost. In this paper, resource allocation for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) downlink networks with large numbers of base station antennas is studied. Assuming perfect channel state information at the transmitter, the resource allocation algorithm design is modeled as a non-convex optimization problem which takes into account the joint power consumption of the power amplifiers, antenna unit, and signal processing circuit unit. Subsequently, by exploiting the law of large numbers and dual decomposition, an efficient suboptimal iterative resource allocation algorithm is proposed for maximization of the system capacity (bit/s). In particular, closed-form power allocation and antenna allocation policies are derived in each iteration. Simulation results illustrate that the proposed iterative resource allocation algorithm achieves a close-to-optimal performance in a small number of iterations and unveil a trade-off between system capacity and the number of activated antennas: Activating all antennas may not be a good solution for system capacity maximization when a system with a per antenna power cost is considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Spectral Efficiency in Large-Scale MIMO-OFDM Systems with Per-Antenna Power Cost\", what is the primary goal of the proposed iterative resource allocation algorithm, and what is the underlying assumption that enables the derivation of closed-form power allocation and antenna allocation policies?\n\n**A)** The primary goal is to minimize the total power consumption of the system, assuming perfect channel state information at the transmitter. The underlying assumption is that the law of large numbers holds, allowing for the decomposition of the optimization problem into smaller sub-problems.\n\n**B)** The primary goal is to maximize the system capacity (bit/s), assuming that the power consumption of the power amplifiers, antenna units, and signal processing circuit units is jointly optimized. The underlying assumption is that the channel state information at the transmitter is perfect.\n\n**C)** The primary goal is to minimize the total power consumption of the system, assuming that the number of activated antennas is fixed. The underlying assumption is that the dual decomposition method can be used to decompose the optimization problem into smaller sub-problems.\n\n**D)** The primary goal is to maximize the system capacity (bit/s), assuming that the power consumption of the power amplifiers, antenna units, and signal processing circuit units is jointly optimized. The underlying assumption is that the law of large numbers holds, allowing for the decomposition of the optimization problem into smaller sub-problems.\n\n**Correct Answer:** B) The primary goal is to maximize the system capacity (bit/s), assuming that the power consumption of the power amplifiers, antenna units, and signal processing circuit units is jointly optimized. The underlying assumption is that the channel state information at the transmitter is perfect.\n\n**Explanation:** The correct answer is B) because the paper states that the resource allocation algorithm is designed to maximize the system capacity (bit/s), and that the algorithm takes into account the joint power consumption of the power amplifiers, antenna units, and signal processing circuit units. The underlying assumption is that the channel state information at the transmitter is perfect, as stated in the paper. The other options are incorrect because they either misstate the primary goal of the algorithm or the underlying assumption."}, "4": {"documentation": {"title": "Friction Scaling Laws for Transport in Bacterial Turbulence", "source": "Sanjay C.P. and Ashwin Joy", "docs_id": "1905.11124", "section": ["cond-mat.soft", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Friction Scaling Laws for Transport in Bacterial Turbulence. Understanding the role of frictional drag in diffusive transport is an important problem in the field of active turbulence. Using a continuum model that applies well to bacterial suspensions, we investigate the role of Ekmann friction on the transport of passive (Lagrangian) tracers that go with the local flow. We find that the crossover from ballistic to diffusive regime happens at a time scale $\\tau_c$ that attains a minimum at zero friction, meaning that both injection and dissipation of energy delay the relaxation of tracers. We explain this by proposing that $\\tau_c \\sim 2 \\ell^*/u_{\\text{rms}}$, where $\\ell^*$ is a dominant length scale extracted from energy spectrum peak, and $u_{\\text{rms}}$ is a velocity scale that sets the kinetic energy at steady state, both scales monotonically decrease with friction. Finally, we predict robust scaling laws for $\\ell^*$, $u_{\\text{rms}}$ and the diffusion coefficient $\\mathcal{D} \\sim \\ell^* u_{\\text{rms}} / 2$, that are valid over a wide range of fluid friction. Our findings might be relevant to transport phenomena in a generic active fluid."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Friction Scaling Laws for Transport in Bacterial Turbulence\" investigate the role of Ekmann friction on the transport of passive tracers in bacterial suspensions. According to the paper, what is the relationship between the crossover time scale $\\tau_c$ and the friction coefficient, and how do the dominant length scale $\\ell^*$ and the velocity scale $u_{\\text{rms}}$ change with friction?\n\nA) $\\tau_c$ is inversely proportional to the friction coefficient, and $\\ell^*$ and $u_{\\text{rms}}$ are constant with respect to friction.\n\nB) $\\tau_c$ is proportional to the friction coefficient, and $\\ell^*$ and $u_{\\text{rms}}$ decrease monotonically with friction.\n\nC) $\\tau_c$ is proportional to the friction coefficient, and $\\ell^*$ and $u_{\\text{rms}}$ are inversely proportional to the friction coefficient.\n\nD) $\\tau_c$ is inversely proportional to the friction coefficient, and $\\ell^*$ and $u_{\\text{rms}}$ are constant with respect to friction.\n\n**Correct Answer:** B) $\\tau_c$ is proportional to the friction coefficient, and $\\ell^*$ and $u_{\\text{rms}}$ decrease monotonically with friction.\n\n**Explanation:**\n\nThe correct answer is B) because the paper states that $\\tau_c \\sim 2 \\ell^*/u_{\\text{rms}}$, where both $\\ell^*$ and $u_{\\text{rms}}$ decrease monotonically with friction. This means that as the friction coefficient increases, the crossover time scale $\\tau_c$ also increases, and the dominant length scale $\\ell^*$ and the velocity scale $u_{\\text{rms}}$ decrease.\n\nThe incorrect answers are:\n\nA) because the paper states that $\\tau_c$ is proportional to the friction coefficient, not inversely proportional.\n\nC) because the paper states that $\\ell^*$ and $u_{\\text{rms}}$ decrease monotonically with friction, not inversely proportional.\n\nD) because the paper states that $\\tau_c$ is proportional to the friction coefficient, not inversely proportional, and that $\\ell^*$ and $u_{\\text{rms}}$ decrease monotonically with friction, not remain constant."}, "5": {"documentation": {"title": "Multiple Randomization Designs", "source": "Patrick Bajari, Brian Burdick, Guido W. Imbens, Lorenzo Masoero, James\n  McQueen, Thomas Richardson, Ido M. Rosen", "docs_id": "2112.13495", "section": ["stat.ME", "cs.SI", "econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiple Randomization Designs. In this study we introduce a new class of experimental designs. In a classical randomized controlled trial (RCT), or A/B test, a randomly selected subset of a population of units (e.g., individuals, plots of land, or experiences) is assigned to a treatment (treatment A), and the remainder of the population is assigned to the control treatment (treatment B). The difference in average outcome by treatment group is an estimate of the average effect of the treatment. However, motivating our study, the setting for modern experiments is often different, with the outcomes and treatment assignments indexed by multiple populations. For example, outcomes may be indexed by buyers and sellers, by content creators and subscribers, by drivers and riders, or by travelers and airlines and travel agents, with treatments potentially varying across these indices. Spillovers or interference can arise from interactions between units across populations. For example, sellers' behavior may depend on buyers' treatment assignment, or vice versa. This can invalidate the simple comparison of means as an estimator for the average effect of the treatment in classical RCTs. We propose new experiment designs for settings in which multiple populations interact. We show how these designs allow us to study questions about interference that cannot be answered by classical randomized experiments. Finally, we develop new statistical methods for analyzing these Multiple Randomization Designs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a Multiple Randomization Design, what is the primary concern when comparing the average effect of a treatment across multiple populations, where interactions between units across populations can lead to spillovers or interference?\n\n**A)** The treatment effect may be overestimated due to the complexity of the interactions between units.\n**B)** The treatment effect may be underestimated due to the lack of control over the interactions between units.\n**C)** The treatment effect may be distorted by the varying treatment assignments across populations.\n**D)** The treatment effect may be invalidated by the simple comparison of means as an estimator for the average effect of the treatment.\n\n**Correct Answer:** D) The treatment effect may be invalidated by the simple comparison of means as an estimator for the average effect of the treatment.\n\n**Explanation:** The correct answer is D) because the Multiple Randomization Design is designed to address the issue of spillovers or interference that can arise from interactions between units across populations. The simple comparison of means, as used in classical randomized controlled trials, is not sufficient to estimate the average effect of the treatment in such settings, as it may not account for the complex interactions between units. The correct answer requires an understanding of the limitations of classical RCTs and the need for new experiment designs and statistical methods to address these limitations.\n\n**Explanation for incorrect options:**\n\n* A) The treatment effect may be overestimated due to the complexity of the interactions between units, but this is not the primary concern in a Multiple Randomization Design.\n* B) The treatment effect may be underestimated due to the lack of control over the interactions between units, but this is not the primary concern in a Multiple Randomization Design.\n* C) The treatment effect may be distorted by the varying treatment assignments across populations, but this is not the primary concern in a Multiple Randomization Design. The primary concern is the invalidation of the simple comparison of means as an estimator for the average effect of the treatment."}, "6": {"documentation": {"title": "The Effects of Latent Infection on the Dynamics of HIV", "source": "Stephen Pankavich", "docs_id": "1312.3670", "section": ["math.DS", "q-bio.CB", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Effects of Latent Infection on the Dynamics of HIV. One way in which the human immunodeficiency virus (HIV-1) replicates within a host is by infecting activated CD4+ T-cells, which then produce additional copies of the virus. Even with the introduction of antiretroviral drug therapy, which has been very successful over the past decade, a large obstacle to the complete eradication of the virus is the presence of viral reservoirs in the form of latently infected CD4+ T-cells. We consider a model of HIV infection that describes T-cell and viral interactions, as well as, the production and activation of latently infected T-cells. Upon determining equilibrium states of the latent cell model, the local and global asymptotic behavior of solutions is examined, and the basic reproduction number of the system is computed to be strictly less than that of the corresponding three-component model, which omits the effects of latent infection. In particular, this implies that a wider variety of parameter values will lead to viral eradication as $t \\to \\infty$ due to the appearance of latent CD4+ T-cells. With this realization we discuss possible alternative notions for eradication and persistence of infection other than traditional tools. These results are further illustrated by a number of numerical simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of latent infection on the dynamics of HIV replication within a host, as described in the model presented in the paper \"The Effects of Latent Infection on the Dynamics of HIV\". Specifically, explain how the presence of latent CD4+ T-cells affects the basic reproduction number of the system and the prospects for viral eradication.\n\n**A)** The presence of latent CD4+ T-cells increases the basic reproduction number of the system, making it more difficult to eradicate the virus. This is because latent cells can produce additional copies of the virus, leading to a higher transmission rate.\n\n**B)** The model presented in the paper suggests that the presence of latent CD4+ T-cells actually decreases the basic reproduction number of the system, making it easier to eradicate the virus. This is because latent cells can act as a reservoir, reducing the number of actively replicating cells and thereby reducing the transmission rate.\n\n**C)** The impact of latent infection on the basic reproduction number of the system is neutral, and the prospects for viral eradication remain unchanged. This is because the effects of latent infection are offset by the introduction of antiretroviral drug therapy.\n\n**D)** The model presented in the paper suggests that the presence of latent CD4+ T-cells leads to a wider variety of parameter values that can result in viral eradication, as the appearance of latent cells can reduce the transmission rate and make it easier to control the virus.\n\n**Correct Answer:** D) The model presented in the paper suggests that the presence of latent CD4+ T-cells leads to a wider variety of parameter values that can result in viral eradication, as the appearance of latent cells can reduce the transmission rate and make it easier to control the virus.\n\n**Explanation:** The correct answer is D) because the paper states that the presence of latent CD4+ T-cells \"will lead to viral eradication as $t \\to \\infty$ due to the appearance of latent CD4+ T-cells\". This implies that the latent cells can act as a reservoir, reducing the transmission rate and making it easier to control the virus. The other options are incorrect because they do not accurately reflect the findings of the paper. Option A is incorrect because the paper states that the basic reproduction number is strictly less than that of the corresponding three-component model, which omits the effects of latent infection. Option B is incorrect because the paper does not suggest that the presence of latent CD4+ T-cells decreases the basic reproduction number. Option C is incorrect because the paper does not suggest that the impact of latent infection is neutral."}, "7": {"documentation": {"title": "Short Term Stress of Covid-19 On World Major Stock Indices", "source": "Muhammad Rehan, Jahanzaib Alvi, Suleyman Serdar Karaca", "docs_id": "2008.06450", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Short Term Stress of Covid-19 On World Major Stock Indices. The main objective of this study is to check short term stress of COVID-19 on the American, European, Asian, and Pacific stock market indices, furthermore, the correlation between all the stock markets during the pandemic. Secondary data of 41 stock exchange from 32 countries have been collected from investing.com website from 1st July 2019 to 14th May 2020 for the stock market and the COVID-19 data has been collected according to the first cases reported in the country, stocks market are classified either developed or emerging economy, further divided according to the subcontinent i.e. America, Europe, and Pacific/Asia, the main focus in the data is the report of first COVID-19 cases. The study reveals that there is volatility in the all the 41 stock market (American, Europe, Asia, and Pacific) after reporting of the first case and volatility increase with the increase of COVID-19 cases, moreover, there is a significant negative relationship between the number of COVID-19 cases and 41 major stock indices of American, Europe, Asia and Pacific, European subcontinent market found more effected from the COVID-19 than another subcontinent, there is Clustering effect of COVID-19 on all the stock market except American's stock market due to smart capital investing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of COVID-19 on the stock market indices of different regions, specifically the American, European, Asian, and Pacific markets. How do the findings of the study relate to the concept of smart capital investing and the clustering effect of COVID-19 on stock markets?\n\n**A)** The study reveals that the COVID-19 pandemic had a significant negative impact on the stock market indices of all regions, with the European subcontinent being the most affected. The clustering effect of COVID-19 led to a concentration of smart capital investments in the American stock market, resulting in increased volatility in the other regions.\n\n**B)** The study found that the COVID-19 pandemic had a negative impact on the stock market indices of the Asian and Pacific regions, while the European subcontinent was less affected. The clustering effect of COVID-19 led to a diversification of smart capital investments across the regions, resulting in reduced volatility.\n\n**C)** The study reveals that the COVID-19 pandemic had a significant negative impact on the stock market indices of all regions, with the American stock market being the least affected. The clustering effect of COVID-19 led to a concentration of smart capital investments in the European subcontinent, resulting in increased volatility in the other regions.\n\n**D)** The study found that the COVID-19 pandemic had a positive impact on the stock market indices of the American and European regions, while the Asian and Pacific regions were negatively affected. The clustering effect of COVID-19 led to a concentration of smart capital investments in the American stock market, resulting in increased volatility in the other regions.\n\n**Correct Answer:** A) The study reveals that the COVID-19 pandemic had a significant negative impact on the stock market indices of all regions, with the European subcontinent being the most affected. The clustering effect of COVID-19 led to a concentration of smart capital investments in the American stock market, resulting in increased volatility in the other regions.\n\n**Explanation:** The study found that the COVID-19 pandemic had a significant negative impact on the stock market indices of all regions, with the European subcontinent being the most affected. The clustering effect of COVID-19 led to a concentration of smart capital investments in the American stock market, resulting in increased volatility in the other regions. This is because smart capital investors were able to quickly adjust their investments to minimize losses, leading to a clustering effect in the American stock market."}, "8": {"documentation": {"title": "Benchmarking Deep Trackers on Aerial Videos", "source": "Abu Md Niamul Taufique, Breton Minnehan, Andreas Savakis", "docs_id": "2103.12924", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Benchmarking Deep Trackers on Aerial Videos. In recent years, deep learning-based visual object trackers have achieved state-of-the-art performance on several visual object tracking benchmarks. However, most tracking benchmarks are focused on ground level videos, whereas aerial tracking presents a new set of challenges. In this paper, we compare ten trackers based on deep learning techniques on four aerial datasets. We choose top performing trackers utilizing different approaches, specifically tracking by detection, discriminative correlation filters, Siamese networks and reinforcement learning. In our experiments, we use a subset of OTB2015 dataset with aerial style videos; the UAV123 dataset without synthetic sequences; the UAV20L dataset, which contains 20 long sequences; and DTB70 dataset as our benchmark datasets. We compare the advantages and disadvantages of different trackers in different tracking situations encountered in aerial data. Our findings indicate that the trackers perform significantly worse in aerial datasets compared to standard ground level videos. We attribute this effect to smaller target size, camera motion, significant camera rotation with respect to the target, out of view movement, and clutter in the form of occlusions or similar looking distractors near tracked object."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are some of the key challenges that deep learning-based visual object trackers face when applied to aerial videos, and how do these challenges differ from those encountered in ground-level videos?\n\nA) Smaller target size, camera motion, and clutter in the form of occlusions or similar-looking distractors near the tracked object are the primary challenges, which are exacerbated by the aerial perspective.\n\nB) The main challenges are the lack of sufficient training data, the need for more advanced tracking algorithms, and the difficulty in handling varying lighting conditions, which are not specific to aerial videos.\n\nC) The primary challenges are the high frame rates, the need for more powerful computing hardware, and the difficulty in handling large datasets, which are not unique to aerial videos.\n\nD) The main challenges are the limited availability of aerial datasets, the need for more advanced tracking algorithms, and the difficulty in handling varying weather conditions, which are not specific to aerial videos.\n\nCorrect Answer: A) Smaller target size, camera motion, and clutter in the form of occlusions or similar-looking distractors near the tracked object are the primary challenges, which are exacerbated by the aerial perspective.\n\nExplanation: The correct answer is A) because the documentation specifically mentions that the smaller target size, camera motion, significant camera rotation with respect to the target, out of view movement, and clutter in the form of occlusions or similar-looking distractors near the tracked object are the key challenges that deep learning-based visual object trackers face when applied to aerial videos. These challenges are exacerbated by the aerial perspective, which is not mentioned in the other options."}, "9": {"documentation": {"title": "Deep Joint Source-Channel Coding for Wireless Image Transmission with\n  Adaptive Rate Control", "source": "Mingyu Yang, Hun-Seok Kim", "docs_id": "2110.04456", "section": ["eess.SP", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Joint Source-Channel Coding for Wireless Image Transmission with\n  Adaptive Rate Control. We present a novel adaptive deep joint source-channel coding (JSCC) scheme for wireless image transmission. The proposed scheme supports multiple rates using a single deep neural network (DNN) model and learns to dynamically control the rate based on the channel condition and image contents. Specifically, a policy network is introduced to exploit the tradeoff space between the rate and signal quality. To train the policy network, the Gumbel-Softmax trick is adopted to make the policy network differentiable and hence the whole JSCC scheme can be trained end-to-end. To the best of our knowledge, this is the first deep JSCC scheme that can automatically adjust its rate using a single network model. Experiments show that our scheme successfully learns a reasonable policy that decreases channel bandwidth utilization for high SNR scenarios or simple image contents. For an arbitrary target rate, our rate-adaptive scheme using a single model achieves similar performance compared to an optimized model specifically trained for that fixed target rate. To reproduce our results, we make the source code publicly available at https://github.com/mingyuyng/Dynamic_JSCC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed adaptive deep joint source-channel coding (JSCC) scheme in wireless image transmission, as described in the Arxiv paper?\n\nA) It requires multiple neural networks to train, each optimized for a specific rate and channel condition.\nB) It uses a single deep neural network (DNN) model to learn the tradeoff space between rate and signal quality, allowing for dynamic rate control.\nC) It employs a traditional rate-control approach, where the rate is fixed and only the channel condition is adapted.\nD) It relies on a separate policy network to control the rate, but the rate is not learned dynamically.\n\n**Correct Answer:** B) It uses a single deep neural network (DNN) model to learn the tradeoff space between rate and signal quality, allowing for dynamic rate control.\n\n**Explanation:** The correct answer highlights the key innovation of the proposed scheme, which is the use of a single DNN model to learn the tradeoff space between rate and signal quality. This allows for dynamic rate control, which is a significant advantage over traditional rate-control approaches. The other options are incorrect because they either misrepresent the scheme's architecture (A and D) or describe a different approach (C)."}, "10": {"documentation": {"title": "Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance\n  Portfolio", "source": "Taras Bodnar, Nestor Parolya and Erik Thorsen", "docs_id": "2106.02131", "section": ["q-fin.ST", "math.ST", "q-fin.PM", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance\n  Portfolio. In this paper, new results in random matrix theory are derived which allow us to construct a shrinkage estimator of the global minimum variance (GMV) portfolio when the shrinkage target is a random object. More specifically, the shrinkage target is determined as the holding portfolio estimated from previous data. The theoretical findings are applied to develop theory for dynamic estimation of the GMV portfolio, where the new estimator of its weights is shrunk to the holding portfolio at each time of reconstruction. Both cases with and without overlapping samples are considered in the paper. The non-overlapping samples corresponds to the case when different data of the asset returns are used to construct the traditional estimator of the GMV portfolio weights and to determine the target portfolio, while the overlapping case allows intersections between the samples. The theoretical results are derived under weak assumptions imposed on the data-generating process. No specific distribution is assumed for the asset returns except from the assumption of finite $4+\\varepsilon$, $\\varepsilon>0$, moments. Also, the population covariance matrix with unbounded spectrum can be considered. The performance of new trading strategies is investigated via an extensive simulation. Finally, the theoretical findings are implemented in an empirical illustration based on the returns on stocks included in the S\\&P 500 index."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance Portfolio\", what is the primary assumption made about the distribution of asset returns, and how does it impact the derivation of the shrinkage estimator of the global minimum variance (GMV) portfolio?\n\nA) The asset returns are assumed to follow a normal distribution, and this assumption is relaxed to allow for finite moments of order 4+\u03b5, \u03b5>0.\nB) The asset returns are assumed to follow a specific distribution, but no specific distribution is assumed, and the focus is on the finite moments of order 4+\u03b5, \u03b5>0.\nC) The asset returns are assumed to be identically and independently distributed (i.i.d.), and this assumption is relaxed to allow for unbounded spectrum of the population covariance matrix.\nD) The asset returns are assumed to be normally distributed with finite moments, and the focus is on the estimation of the GMV portfolio weights using overlapping samples.\n\n**Correct Answer:** B) The asset returns are assumed to follow a specific distribution, but no specific distribution is assumed, and the focus is on the finite moments of order 4+\u03b5, \u03b5>0.\n\n**Explanation:** The paper states that \"No specific distribution is assumed for the asset returns except from the assumption of finite $4+\\varepsilon$, $\\varepsilon>0$, moments.\" This means that the authors do not assume a specific distribution for the asset returns, but rather focus on the finite moments of order 4+\u03b5, \u03b5>0. This assumption is crucial in deriving the shrinkage estimator of the GMV portfolio, as it allows for the construction of a robust estimator that can handle high-dimensional data with unbounded spectrum."}, "11": {"documentation": {"title": "Numerical analysis of a mechanotransduction dynamical model reveals\n  homoclinic bifurcations of extracellular matrix mediated oscillations of the\n  mesenchymal stem cell fate", "source": "Katiana Kontolati and Constantinos Siettos", "docs_id": "1902.01481", "section": ["q-bio.CB", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical analysis of a mechanotransduction dynamical model reveals\n  homoclinic bifurcations of extracellular matrix mediated oscillations of the\n  mesenchymal stem cell fate. We perform one and two-parameter numerical bifurcation analysis of a mechanotransduction model approximating the dynamics of mesenchymal stem cell differentiation into neurons, adipocytes, myocytes and osteoblasts. For our analysis, we use as bifurcation parameters the stiffness of the extracellular matrix and parameters linked with the positive feedback mechanisms that up-regulate the production of the YAP/TAZ transcriptional regulators (TRs) and the cell adhesion area. Our analysis reveals a rich nonlinear behaviour of the cell differentiation including regimes of hysteresis and multistability, stable oscillations of the effective adhesion area, the YAP/TAZ TRs and the PPAR$\\gamma$ receptors associated with the adipogenic fate, as well as homoclinic bifurcations that interrupt relatively high-amplitude oscillations abruptly. The two-parameter bifurcation analysis of the Andronov-Hopf points that give birth to the oscillating patterns predicts their existence for soft extracellular substrates ($<1kPa$), a regime that favours the neurogenic and the adipogenic cell fate. Furthermore, in these regimes, the analysis reveals the presence of homoclinic bifurcations that result in the sudden loss of the stable oscillations of the cell-substrate adhesion towards weaker adhesion and high expression levels of the gene encoding Tubulin beta-3 chain, thus favouring the phase transition from the adipogenic to the neurogenic fate."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the bifurcation analysis of the mechanotransduction model presented in the Arxiv documentation. What is the significance of the homoclinic bifurcations in the context of mesenchymal stem cell differentiation, and how do they relate to the cell-substrate adhesion and the YAP/TAZ TRs?\n\n**A)** The homoclinic bifurcations indicate a stable oscillation of the cell-substrate adhesion area, which is essential for the adipogenic fate. The YAP/TAZ TRs play a crucial role in regulating the expression of genes involved in cell differentiation.\n\n**B)** The homoclinic bifurcations signify a sudden loss of stable oscillations of the cell-substrate adhesion, leading to a phase transition from the adipogenic to the neurogenic fate. The YAP/TAZ TRs are involved in up-regulating the production of PPAR\u03b3 receptors, which are essential for adipogenesis.\n\n**C)** The homoclinic bifurcations reveal a rich nonlinear behavior of the cell differentiation process, including regimes of hysteresis and multistability. The YAP/TAZ TRs and the cell adhesion area are both influenced by the stiffness of the extracellular matrix and the positive feedback mechanisms that regulate their production.\n\n**D)** The homoclinic bifurcations indicate a stable oscillation of the YAP/TAZ TRs, which is essential for the neurogenic fate. The cell-substrate adhesion area is influenced by the stiffness of the extracellular matrix and the positive feedback mechanisms that regulate its production.\n\n**Correct Answer:** C) The homoclinic bifurcations reveal a rich nonlinear behavior of the cell differentiation process, including regimes of hysteresis and multistability. The YAP/TAZ TRs and the cell adhesion area are both influenced by the stiffness of the extracellular matrix and the positive feedback mechanisms that regulate their production.\n\n**Explanation:** The correct answer, C, accurately summarizes the significance of the homoclinic bifurcations in the context of mesenchymal stem cell differentiation. The bifurcations reveal a rich nonlinear behavior of the cell differentiation process, including regimes of hysteresis and multistability. The YAP/TAZ TRs and the cell adhesion area are both influenced by the stiffness of the extracellular matrix and the positive feedback mechanisms that regulate their production. This answer requires the test-taker to understand the complex relationships between the bifurcations, the cell differentiation process, and the regulatory mechanisms involved."}, "12": {"documentation": {"title": "Charge and Matter Form Factors of Two-Neutron Halo Nuclei in Halo\n  Effective Field Theory at Next-to-leading-order", "source": "Jared Vanasse", "docs_id": "1609.08552", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Charge and Matter Form Factors of Two-Neutron Halo Nuclei in Halo\n  Effective Field Theory at Next-to-leading-order. Using halo effective field theory (EFT), an expansion in $R_{core}/R_{halo}$, where $R_{core}$ is the radius of the core and $R_{halo}$ the radius of the halo nucleus, we calculate the charge and neutron form factors of the two-neutron halo nuclei $^{11}$Li, $^{14}$Be, and $^{22}$C to next-to-leading-order (NLO) by treating them as an effective three-body system. From the form factors we extract the point charge and point matter radii, inter-neutron distance, and neutron opening angle. Agreement is found with existing experimental extractions. Results are given for the point charge and point matter radii for arbitrary neutron core scattering effective range, $\\rho_{cn}$, that can be used for predictions once $\\rho_{cn}$ is measured. Estimates for $\\rho_{cn}$ are also used to make NLO predictions. Finally, our point charge radii are compared to other halo-EFT predictions, and setting the core mass equal to the neutron mass our point charge radius is found to agree with an analytical prediction in the unitary limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Halo Effective Field Theory (EFT), what is the relationship between the point charge radius, point matter radius, and inter-neutron distance in two-neutron halo nuclei, and how do these parameters relate to the neutron core scattering effective range, \u03c1cn?\n\nA) The point charge radius is directly proportional to the point matter radius, while the inter-neutron distance is inversely proportional to \u03c1cn.\nB) The point charge radius is inversely proportional to the point matter radius, while the inter-neutron distance is directly proportional to \u03c1cn.\nC) The point charge radius is directly proportional to the inter-neutron distance, while the point matter radius is inversely proportional to \u03c1cn.\nD) The point charge radius is inversely proportional to the point matter radius, while the inter-neutron distance is directly proportional to \u03c1cn, and the point charge radius is also dependent on the neutron core scattering effective range.\n\nCorrect Answer: D) The point charge radius is inversely proportional to the point matter radius, while the inter-neutron distance is directly proportional to \u03c1cn, and the point charge radius is also dependent on the neutron core scattering effective range.\n\nExplanation: The correct answer is based on the relationship between the point charge radius, point matter radius, and inter-neutron distance in two-neutron halo nuclei, as described in the provided documentation. The point charge radius is inversely proportional to the point matter radius, and the inter-neutron distance is directly proportional to \u03c1cn. Additionally, the point charge radius is also dependent on the neutron core scattering effective range, \u03c1cn. This relationship is crucial in understanding the behavior of two-neutron halo nuclei in Halo Effective Field Theory."}, "13": {"documentation": {"title": "Determination of the NNLO low-energy constant $C_{93}$", "source": "Maarten Golterman, Kim Maltman, Santiago Peris", "docs_id": "1706.03672", "section": ["hep-ph", "hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Determination of the NNLO low-energy constant $C_{93}$. Experimental data from hadronic $\\tau$ decays allow for a precision determination of the slope of the $I=1$ vacuum polarization at zero momentum. We use this information to provide a value for the next-to-next-to-leading order (NNLO) low-energy constant $C_{93}$ in chiral perturbation theory. The largest systematic error in this determination results from the neglect of terms beyond NNLO in the effective chiral Lagrangian, whose presence in the data will, in general, make the effective $C_{93}$ determined in an NNLO analysis mass dependent. We estimate the size of this effect by using strange hadronic $\\tau$-decay data to perform an alternate $C_{93}$ determination based on the slope of the strange vector polarization at zero momentum, which differs from that of the $I=1$ vector channel only through $SU(3)$ flavor-breaking effects. We also comment on the impact of such higher order effects on ChPT-based estimates for the hadronic vacuum polarization contribution to the muon anomalous magnetic moment."}, "answer": "Here is the multi-form question:\n\nWhat is the primary source of systematic error in the determination of the NNLO low-energy constant $C_{93}$ in chiral perturbation theory, and how does it impact the accuracy of the estimate?\n\nA) The neglect of terms beyond NNLO in the effective chiral Lagrangian, which can make the effective $C_{93}$ determined in an NNLO analysis mass dependent.\n\nB) The use of experimental data from hadronic $\\tau$ decays, which may not accurately represent the true vacuum polarization at zero momentum.\n\nC) The assumption of a constant vacuum polarization at zero momentum, which may not hold in reality.\n\nD) The neglect of SU(3) flavor-breaking effects, which can lead to an overestimation of the effective $C_{93}$.\n\nCorrect Answer: A) The neglect of terms beyond NNLO in the effective chiral Lagrangian, which can make the effective $C_{93}$ determined in an NNLO analysis mass dependent.\n\nExplanation: The question requires the test-taker to understand the context of the Arxiv documentation and the limitations of the NNLO analysis. The correct answer, A, highlights the primary source of systematic error, which is the neglect of terms beyond NNLO in the effective chiral Lagrangian. This error can lead to a mass-dependent estimate of the effective $C_{93}$, which is a key consideration in the analysis. The other options are incorrect because they do not accurately represent the primary source of systematic error. Option B is incorrect because the experimental data from hadronic $\\tau$ decays is actually used to determine the slope of the vacuum polarization at zero momentum. Option C is incorrect because the assumption of a constant vacuum polarization at zero momentum is not the primary source of systematic error. Option D is incorrect because SU(3) flavor-breaking effects are actually considered in the analysis, but are not the primary source of systematic error."}, "14": {"documentation": {"title": "Ultrafast voltage-tunable detectors for Terahertz radiation operating\n  above 100K", "source": "G. B. Serapiglia, M. Hanson, M. F. Doty, P. Focardi, W. R. McGrath, A.\n  C. Gossard and M. S. Sherwin", "docs_id": "1410.5019", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ultrafast voltage-tunable detectors for Terahertz radiation operating\n  above 100K. Collective vibrations of proteins, rotations of small molecules, excitations of high-temperature superconductors, and electronic transitions in semiconductor nanostructures occur with characteristic frequencies between 1 and 10 THz. Applications to medicine, communications, security and other fields are emerging. However, mapping the coldest parts of the universe has been the largest driver for developing THz detectors. The result is a family of exquisitely-sensitive detectors requiring sub-4K temperatures. For earthbound THz science and technology, sensitivity remains important but many applications require high speed and operating temperatures. Room-temperature Schottky diodes enable some of these applications. Here we demonstrate a new type of detector in which THz radiation excites a collective oscillation of ~25,000 electrons between two gates in a microscopic four terminal transistor. The energy dissipates into other modes of the electron gas, warming it and changing the source-drain resistance. The detector shows amplifier-limited rise times near 1 ns and has detected THz laser radiation at temperatures up to 120K. The frequency of the collective oscillation tunes with small gate voltages. The first-generation tunable antenna-coupled intersubband Terahertz (TACIT) detectors tune between 1.5 and 2 THz with voltages <2V."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of current Terahertz (THz) detectors in applications requiring high speed and operating temperatures, and how does the proposed detector address this limitation?\n\n**A)** The primary limitation is the need for cryogenic temperatures, which hinders the development of detectors for room-temperature applications. The proposed detector addresses this limitation by using a microscopic four-terminal transistor that can operate at temperatures up to 120K.\n\n**B)** The primary limitation is the low sensitivity of detectors, which is a result of the limited number of electrons involved in the collective oscillation. The proposed detector addresses this limitation by increasing the number of electrons involved in the oscillation to ~25,000.\n\n**C)** The primary limitation is the limited frequency range of detectors, which is a result of the fixed voltage tuning. The proposed detector addresses this limitation by using small gate voltages to tune the frequency of the collective oscillation.\n\n**D)** The primary limitation is the high energy dissipation rate, which leads to a rapid increase in temperature and a decrease in detector sensitivity. The proposed detector addresses this limitation by dissipating the energy into other modes of the electron gas, thereby reducing the temperature increase.\n\n**Correct Answer:** A) The primary limitation is the need for cryogenic temperatures, which hinders the development of detectors for room-temperature applications. The proposed detector addresses this limitation by using a microscopic four-terminal transistor that can operate at temperatures up to 120K.\n\n**Explanation:** The question requires the test-taker to understand the limitations of current THz detectors and how the proposed detector addresses these limitations. The correct answer, A, highlights the need for cryogenic temperatures as the primary limitation and the proposed detector's ability to operate at temperatures up to 120K. The other options, B, C, and D, are incorrect because they do not accurately describe the primary limitation or the proposed detector's solution."}, "15": {"documentation": {"title": "Optimal retirement consumption with a stochastic force of mortality", "source": "Huaxiong Huang, Moshe A. Milevsky and Thomas S. Salisbury", "docs_id": "1205.2295", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal retirement consumption with a stochastic force of mortality. We extend the lifecycle model (LCM) of consumption over a random horizon (a.k.a. the Yaari model) to a world in which (i.) the force of mortality obeys a diffusion process as opposed to being deterministic, and (ii.) a consumer can adapt their consumption strategy to new information about their mortality rate (a.k.a. health status) as it becomes available. In particular, we derive the optimal consumption rate and focus on the impact of mortality rate uncertainty vs. simple lifetime uncertainty -- assuming the actuarial survival curves are initially identical -- in the retirement phase where this risk plays a greater role. In addition to deriving and numerically solving the PDE for the optimal consumption rate, our main general result is that when utility preferences are logarithmic the initial consumption rates are identical. But, in a CRRA framework in which the coefficient of relative risk aversion is greater (smaller) than one, the consumption rate is higher (lower) and a stochastic force of mortality does make a difference. That said, numerical experiments indicate that even for non-logarithmic preferences, the stochastic mortality effect is relatively minor from the individual's perspective. Our results should be relevant to researchers interested in calibrating the lifecycle model as well as those who provide normative guidance (a.k.a. financial advice) to retirees."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Yaari model, what is the main general result regarding the impact of a stochastic force of mortality on the optimal consumption rate, and under what conditions does this effect become significant?\n\n**A)** The stochastic force of mortality has a negligible impact on the optimal consumption rate, regardless of the utility preferences of the consumer.\n\n**B)** When utility preferences are logarithmic, the stochastic force of mortality has no impact on the optimal consumption rate, but in a CRRA framework with a coefficient of relative risk aversion greater than one, the consumption rate is higher due to the increased mortality risk.\n\n**C)** The stochastic force of mortality has a significant impact on the optimal consumption rate, particularly in the retirement phase, and this effect is more pronounced when the coefficient of relative risk aversion is greater than one.\n\n**D)** The stochastic force of mortality has a minor impact on the optimal consumption rate, even in a CRRA framework with a coefficient of relative risk aversion greater than one, and this effect is relatively insignificant from the individual's perspective.\n\n**Correct Answer:** C) The stochastic force of mortality has a significant impact on the optimal consumption rate, particularly in the retirement phase, and this effect is more pronounced when the coefficient of relative risk aversion is greater than one.\n\n**Explanation:** The correct answer is C) because the main general result of the paper is that the stochastic force of mortality has a significant impact on the optimal consumption rate, particularly in the retirement phase. This effect is more pronounced when the coefficient of relative risk aversion is greater than one, as indicated in the paper. The other options are incorrect because they either downplay the impact of the stochastic force of mortality (A and D) or incorrectly state the conditions under which the effect becomes significant (B)."}, "16": {"documentation": {"title": "Remarks on gravitational interaction in Kaluza-Klein models", "source": "Maxim Eingorn and Alexander Zhuk", "docs_id": "1201.1756", "section": ["gr-qc", "astro-ph.HE", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Remarks on gravitational interaction in Kaluza-Klein models. In these remarks, we clarify the problematic aspects of gravitational interaction in a weak-field limit of Kaluza-Klein models. We explain why some models meet the classical gravitational tests, while others do not. We show that variation of the total volume of the internal spaces generates the fifth force. This is the main reason of the problem. It happens for all considered models (linear with respect to the scalar curvature and nonlinear $f(R)$, with toroidal and spherical compactifications). We explicitly single out the contribution of the fifth force to nonrelativistic gravitational potentials. In the case of models with toroidal compactification, we demonstrate how tension (with and without effects of nonlinearity) of the gravitating source can fix the total volume of the internal space, resulting in the vanishing fifth force and consequently in agreement with the observations. It takes place for latent solitons, black strings and black branes. We also demonstrate a particular example where non-vanishing variations of the internal space volume do not contradict the gravitational experiments. In the case of spherical compactification, the fifth force is replaced by the Yukawa interaction for models with the stabilized internal space. For large Yukawa masses, the effect of this interaction is negligibly small, and considered models satisfy the gravitational tests at the same level of accuracy as general relativity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Kaluza-Klein models, what is the primary reason for the generation of the fifth force, and how does it relate to the variation of the total volume of the internal spaces?\n\nA) The variation of the scalar curvature is the primary reason for the generation of the fifth force, and it is independent of the total volume of the internal spaces.\n\nB) The variation of the total volume of the internal spaces is the primary reason for the generation of the fifth force, and it is independent of the scalar curvature.\n\nC) The variation of the scalar curvature and the total volume of the internal spaces are both necessary conditions for the generation of the fifth force, and their relationship is nonlinear.\n\nD) The variation of the total volume of the internal spaces is the primary reason for the generation of the fifth force, and it is a direct consequence of the stabilization of the internal space.\n\nCorrect Answer: B) The variation of the total volume of the internal spaces is the primary reason for the generation of the fifth force, and it is independent of the scalar curvature.\n\nExplanation: According to the remarks, the variation of the total volume of the internal spaces is the main reason for the generation of the fifth force. This is stated in the first sentence of the introduction: \"We show that variation of the total volume of the internal spaces generates the fifth force.\" Additionally, the text explicitly states that this effect occurs for all considered models, regardless of the scalar curvature. Therefore, option B is the correct answer."}, "17": {"documentation": {"title": "Sparre-Andersen theorem with spatiotemporal correlations", "source": "Roberto Artuso, Giampaolo Cristadoro, Mirko Degli Esposti, Georgie\n  Knight", "docs_id": "1401.5685", "section": ["cond-mat.stat-mech", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparre-Andersen theorem with spatiotemporal correlations. The Sparre-Andersen theorem is a remarkable result in one-dimensional random walk theory concerning the universality of the ubiquitous first-passage-time distribution. It states that the probability distribution $\\rho_n$ of the number of steps needed for a walker starting at the origin to land on the positive semi-axes does not depend on the details of the distribution for the jumps of the walker, provided this distribution is symmetric and continuous, where in particular $\\rho_n \\sim n^{-3/2}$ for large number of steps $n$. On the other hand, there are many physical situations in which the time spent by the walker in doing one step depends on the length of the step and the interest concentrates on the time needed for a return, not on the number of steps. Here we modify the Sparre-Andersen proof to deal with such cases, in rather general situations in which the time variable correlates with the step variable. As an example we present a natural process in 2D that shows deviations from normal scaling are present for the first-passage-time distribution on a semi plane."}, "answer": "Here is a difficult multi-form question based on the Sparre-Andersen theorem with spatiotemporal correlations:\n\n**Question:** Consider a one-dimensional random walk on a semi-infinite line, where the walker starts at the origin and moves with a step size distribution that is symmetric and continuous. The time variable $t$ is correlated with the step size $s$, such that the probability density function of $t$ given $s$ is $f(t|s) = \\frac{1}{\\sigma s} \\exp\\left(-\\frac{t^2}{2\\sigma^2 s^2}\\right)$, where $\\sigma$ is a positive constant.\n\n**A)** Show that the first-passage-time distribution $\\rho_n$ of the walker to reach the positive semi-axes does not depend on the details of the step size distribution, and that it is given by $\\rho_n \\sim n^{-3/2}$ for large $n$.\n\n**B)** Derive an expression for the mean and variance of the first-passage time $T$ of the walker to reach the positive semi-axes, in terms of the step size distribution and the correlation parameter $\\sigma$.\n\n**C)** Consider a two-dimensional random walk on a semi-plane, where the walker starts at the origin and moves with a step size distribution that is symmetric and continuous. The time variable $t$ is correlated with the step size $s$, such that the probability density function of $t$ given $s$ is $f(t|s) = \\frac{1}{\\sigma s} \\exp\\left(-\\frac{t^2}{2\\sigma^2 s^2}\\right)$. Show that the first-passage-time distribution $\\rho_n$ of the walker to reach the positive semi-axes does not follow the normal scaling $\\rho_n \\sim n^{-3/2}$, and provide a counterexample to the Sparre-Andersen theorem.\n\n**D)** Prove that the Sparre-Andersen theorem can be generalized to the case where the walker moves in a higher-dimensional space, and the time variable $t$ is correlated with the step size $s$ in a more general way. Specifically, show that the first-passage-time distribution $\\rho_n$ of the walker to reach the positive semi-axes does not depend on the details of the step size distribution, and that it is given by $\\rho_n \\sim n^{-d/2}$ for large $n$, where $d$ is the dimensionality of the space.\n\n**Correct Answer:** A\n\n**Explanation:**\n\nThe correct answer is A, because the Sparre-Andersen theorem states that the first-passage-time distribution $\\rho_n$ of the walker to reach the positive semi-axes does not depend on the details of the step size distribution, provided that the distribution is symmetric and continuous. This is exactly what is stated in option A.\n\nOption B is incorrect because it asks for the mean and variance of the first-passage time, which is not a direct consequence of the Sparre-Andersen theorem.\n\nOption C is incorrect because it provides a counterexample to the Sparre-Andersen theorem, which is not what the theorem states.\n\nOption D is incorrect because it asks for a generalization of the Sparre-Andersen theorem to higher-dimensional spaces, which is not what the theorem states. The theorem only applies to one-dimensional random walks."}, "18": {"documentation": {"title": "Chemotaxis When Bacteria Remember: Drift versus Diffusion", "source": "Sakuntala Chatterjee, Rava Azeredo da Silveira and Yariv Kafri", "docs_id": "1103.5355", "section": ["cond-mat.stat-mech", "physics.bio-ph", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chemotaxis When Bacteria Remember: Drift versus Diffusion. {\\sl Escherichia coli} ({\\sl E. coli}) bacteria govern their trajectories by switching between running and tumbling modes as a function of the nutrient concentration they experienced in the past. At short time one observes a drift of the bacterial population, while at long time one observes accumulation in high-nutrient regions. Recent work has viewed chemotaxis as a compromise between drift toward favorable regions and accumulation in favorable regions. A number of earlier studies assume that a bacterium resets its memory at tumbles -- a fact not borne out by experiment -- and make use of approximate coarse-grained descriptions. Here, we revisit the problem of chemotaxis without resorting to any memory resets. We find that when bacteria respond to the environment in a non-adaptive manner, chemotaxis is generally dominated by diffusion, whereas when bacteria respond in an adaptive manner, chemotaxis is dominated by a bias in the motion. In the adaptive case, favorable drift occurs together with favorable accumulation. We derive our results from detailed simulations and a variety of analytical arguments. In particular, we introduce a new coarse-grained description of chemotaxis as biased diffusion, and we discuss the way it departs from older coarse-grained descriptions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of chemotaxis, what is the primary mechanism that governs the motion of Escherichia coli bacteria in response to nutrient concentration, and how does this mechanism differ between non-adaptive and adaptive responses?\n\n**A)** Diffusion is the primary mechanism, and it is unaffected by the bacterium's response to the environment.\n\n**B)** Drift is the primary mechanism, and it is only significant when the bacterium responds in an adaptive manner.\n\n**C)** Biased diffusion is the primary mechanism, and it is characterized by a bias in the motion that favors high-nutrient regions, regardless of the bacterium's response to the environment.\n\n**D)** Diffusion is the primary mechanism, but it is only significant when the bacterium responds in an adaptive manner, in which case it is accompanied by a favorable drift.\n\n**Correct Answer:** C) Biased diffusion is the primary mechanism, and it is characterized by a bias in the motion that favors high-nutrient regions, regardless of the bacterium's response to the environment.\n\n**Explanation:** The correct answer is C) Biased diffusion. The paper states that when bacteria respond to the environment in a non-adaptive manner, chemotaxis is generally dominated by diffusion, whereas when bacteria respond in an adaptive manner, chemotaxis is dominated by a bias in the motion. This bias in the motion is a key feature of biased diffusion, which is a new coarse-grained description of chemotaxis introduced in the paper. The paper argues that this bias in the motion is a result of the bacterium's adaptive response to the environment, and that it is a key mechanism that governs the motion of E. coli bacteria in response to nutrient concentration."}, "19": {"documentation": {"title": "Tracking single particles on supported lipid membranes: multi-mobility\n  diffusion and nanoscopic confinement", "source": "Chia-Lung Hsieh (1 and 2), Susann Spindler (1), Jens Ehrig (1), Vahid\n  Sandoghdar (1) ((1) Max Planck Institute for the Science of Light and\n  Friedrich Alexander University, Germany, (2) Institute of Atomic and\n  Molecular Sciences, Academia Sinica, Taiwan)", "docs_id": "1312.6736", "section": ["physics.bio-ph", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tracking single particles on supported lipid membranes: multi-mobility\n  diffusion and nanoscopic confinement. Supported lipid bilayers have been studied intensively over the past two decades. In this work, we study the diffusion of single gold nanoparticles (GNPs) with diameter of 20 nm attached to GM1 ganglioside or DOPE lipids at different concentrations in supported DOPC bilayers. The indefinite photostability of GNPs combined with the high sensitivity of interferometric scattering microscopy (iSCAT) allows us to achieve 1.9 nm spatial precision at 1 ms temporal resolution, while maintaining long recording times. Our trajectories visualize strong transient confinements within domains as small as 20 nm, and the statistical analysis of the data reveals multiple mobilities and deviations from normal diffusion. We present a detailed analysis of our findings and provide interpretations regarding the effect of the supporting substrate and GM1 clustering. We also comment on the use of high-speed iSCAT for investigating diffusion of lipids, proteins or viruses in lipid membranes with unprecedented spatial and temporal resolution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the diffusion behavior of gold nanoparticles (GNPs) attached to GM1 ganglioside or DOPE lipids in supported DOPC bilayers, based on the findings of the study?\n\nA) The GNPs exhibit normal diffusion behavior, with no deviations from the expected random walk pattern.\nB) The GNPs exhibit multiple mobilities, with strong transient confinements within domains as small as 20 nm.\nC) The GNPs are confined to a single domain, with no movement observed outside of this domain.\nD) The GNPs do not interact with the supporting substrate, and their diffusion is unaffected by the presence of the lipid bilayer.\n\nCorrect Answer: B) The GNPs exhibit multiple mobilities, with strong transient confinements within domains as small as 20 nm.\n\nExplanation: The study reveals that the GNPs exhibit strong transient confinements within domains as small as 20 nm, and the statistical analysis of the data reveals multiple mobilities. This suggests that the GNPs are not behaving as expected under normal diffusion conditions, and instead are experiencing confinement effects due to the interaction with the supporting substrate and the lipid bilayer. This is supported by the use of high-speed interferometric scattering microscopy (iSCAT), which allows for unprecedented spatial and temporal resolution."}, "20": {"documentation": {"title": "Electron doping evolution of the magnetic excitations in\n  NaFe$_{1-x}$Co$_x$As", "source": "Scott V. Carr and Chenglin Zhang and Yu Song and Guotai Tan and Yu Li\n  and D. L. Abernathy and M. B. Stone and G. E. Granroth and T. G. Perring and\n  Pengcheng Dai", "docs_id": "1605.02695", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electron doping evolution of the magnetic excitations in\n  NaFe$_{1-x}$Co$_x$As. We use time-of-flight (ToF) inelastic neutron scattering (INS) spectroscopy to investigate the doping dependence of magnetic excitations across the phase diagram of NaFe$_{1-x}$Co$_x$As with $x=0, 0.0175, 0.0215, 0.05,$ and $0.11$. The effect of electron-doping by partially substituting Fe by Co is to form resonances that couple with superconductivity, broaden and suppress low energy ($E\\le 80$ meV) spin excitations compared with spin waves in undoped NaFeAs. However, high energy ($E> 80$ meV) spin excitations are weakly Co-doping dependent. Integration of the local spin dynamic susceptibility $\\chi^{\\prime\\prime}(\\omega)$ of NaFe$_{1-x}$Co$_x$As reveals a total fluctuating moment of 3.6 $\\mu_B^2$/Fe and a small but systematic reduction with electron doping. The presence of a large spin gap in the Co-overdoped nonsuperconducting NaFe$_{0.89}$Co$_{0.11}$As suggests that Fermi surface nesting is responsible for low-energy spin excitations. These results parallel Ni-doping evolution of spin excitations in BaFe$_{2-x}$Ni$_x$As$_2$, confirming the notion that low-energy spin excitations coupling with itinerant electrons are important for superconductivity, while weakly doping dependent high-energy spin excitations result from localized moments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the effect of electron doping on the magnetic excitations in NaFe$_{1-x}$Co$_x$As. What is the primary mechanism responsible for the suppression of low-energy spin excitations with increasing electron doping?\n\nA){The suppression of low-energy spin excitations is due to the formation of resonances that couple with superconductivity.}\nB){The suppression of low-energy spin excitations is due to the weakening of Fermi surface nesting.}\nC){The suppression of low-energy spin excitations is due to the formation of localized moments.}\nD){The suppression of low-energy spin excitations is due to the broadening of spin waves.}\n\n**Correct Answer:** A){The suppression of low-energy spin excitations is due to the formation of resonances that couple with superconductivity.}\n\n**Explanation:** The correct answer is A) because the text states that the effect of electron-doping by partially substituting Fe by Co is to form resonances that couple with superconductivity, which broadens and suppresses low-energy ($E\\le 80$ meV) spin excitations compared with spin waves in undoped NaFeAs. This indicates that the formation of resonances is the primary mechanism responsible for the suppression of low-energy spin excitations with increasing electron doping."}, "21": {"documentation": {"title": "Improved rates for prediction and identification of partially observed\n  linear dynamical systems", "source": "Holden Lee", "docs_id": "2011.10006", "section": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved rates for prediction and identification of partially observed\n  linear dynamical systems. Identification of a linear time-invariant dynamical system from partial observations is a fundamental problem in control theory. Particularly challenging are systems exhibiting long-term memory. A natural question is how learn such systems with non-asymptotic statistical rates depending on the inherent dimensionality (order) $d$ of the system, rather than on the possibly much larger memory length. We propose an algorithm that given a single trajectory of length $T$ with gaussian observation noise, learns the system with a near-optimal rate of $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$ in $\\mathcal{H}_2$ error, with only logarithmic, rather than polynomial dependence on memory length. We also give bounds under process noise and improved bounds for learning a realization of the system. Our algorithm is based on multi-scale low-rank approximation: SVD applied to Hankel matrices of geometrically increasing sizes. Our analysis relies on careful application of concentration bounds on the Fourier domain -- we give sharper concentration bounds for sample covariance of correlated inputs and for $\\mathcal H_\\infty$ norm estimation, which may be of independent interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a linear time-invariant dynamical system with a dimensionality of $d$ and a single trajectory of length $T$ with Gaussian observation noise. The goal is to identify the system with a near-optimal rate of $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$ in $\\mathcal{H}_2$ error.\n\n**A)** The algorithm's performance is limited by the memory length $T$, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T^2}\\right)$.\n\n**B)** The algorithm's performance is limited by the dimensionality $d$, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$.\n\n**C)** The algorithm's performance is limited by both the memory length $T$ and the dimensionality $d$, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T^3}\\right)$.\n\n**D)** The algorithm's performance is limited by the observation noise, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$.\n\n**Correct Answer:** B) The algorithm's performance is limited by the dimensionality $d$, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$.\n\n**Explanation:** The correct answer is B) because the documentation states that the algorithm learns the system with a near-optimal rate of $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$ in $\\mathcal{H}_2$ error, where $d$ is the dimensionality of the system. This implies that the algorithm's performance is limited by the dimensionality $d$, and the rate of convergence is $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$. The other options are incorrect because they either misstate the rate of convergence or introduce additional limitations that are not mentioned in the documentation."}, "22": {"documentation": {"title": "Wave mitigation in ordered networks of granular chains", "source": "Andrea Leonard, Laurent Ponson, and Chiara Daraio", "docs_id": "1312.0805", "section": ["nlin.PS", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wave mitigation in ordered networks of granular chains. We study the propagation of stress waves through ordered 2D networks of granular chains. The quasi-particle continuum theory employed captures the acoustic pulse splitting, bending, and recombination through the network and is used to derive its effective acoustic properties. The strong wave mitigation properties of the network predicted theoretically are confirmed through both numerical simulations and experimental tests. In particular, the leading pulse amplitude propagating through the system is shown to decay exponentially with the propagation distance and the spatial structure of the transmitted wave shows an exponential localization along the direction of the incident wave. The length scales that characterized these exponential decays are studied and determined as a function of the geometrical properties of the network. These results open avenues for the design of efficient impact mitigating structures and provide new insights into the mechanisms of wave propagation in granular matter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which the wave mitigation properties of the ordered 2D network of granular chains are achieved, according to the quasi-particle continuum theory employed in the study?\n\nA) The network's geometrical properties cause the wave to split and recombine, leading to exponential decay.\nB) The strong wave mitigation properties are a result of the granular chains' ability to absorb and dissipate energy exponentially with distance.\nC) The quasi-particle continuum theory predicts that the network's effective acoustic properties are characterized by an exponential localization of the transmitted wave along the direction of the incident wave.\nD) The network's structure causes the wave to bend and recombine, leading to a decrease in pulse amplitude with distance.\n\nCorrect Answer: C) The quasi-particle continuum theory predicts that the network's effective acoustic properties are characterized by an exponential localization of the transmitted wave along the direction of the incident wave.\n\nExplanation: The correct answer is C) because the quasi-particle continuum theory employed in the study predicts that the network's effective acoustic properties are characterized by an exponential localization of the transmitted wave along the direction of the incident wave. This is a key finding of the study, and it is supported by both numerical simulations and experimental tests. The other options are incorrect because they do not accurately describe the primary mechanism by which the wave mitigation properties of the network are achieved. Option A is incorrect because it describes a secondary effect of the network's geometrical properties. Option B is incorrect because it attributes the wave mitigation properties to the granular chains' ability to absorb and dissipate energy, which is not the primary mechanism described in the study. Option D is incorrect because it describes a different effect of the network's structure on the wave."}, "23": {"documentation": {"title": "Principal $\\infty$-Bundles and Smooth String Group Models", "source": "Severin Bunk", "docs_id": "2008.12263", "section": ["math.AT", "hep-th", "math-ph", "math.DG", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Principal $\\infty$-Bundles and Smooth String Group Models. We provide a general, homotopy-theoretic definition of string group models within an $\\infty$-category of smooth spaces, and we present new smooth models for the string group. Here, a smooth space is a presheaf of $\\infty$-groupoids on the category of cartesian spaces. The key to our definition and construction of smooth string group models is a version of the singular complex functor, which assigns to a smooth space an underlying ordinary space. We provide new characterisations of principal $\\infty$-bundles and group extensions in $\\infty$-topoi, building on work of Nikolaus, Schreiber, and Stevenson. These insights allow us to transfer the definition of string group extensions from the $\\infty$-category of spaces to the $\\infty$-category of smooth spaces. Finally, we consider smooth higher-categorical group extensions that arise as obstructions to the existence of equivariant structures on gerbes. We show that these extensions give rise to new smooth models for the string group, as recently conjectured in joint work with M\\\"uller and Szabo."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the key insight that allows the authors to transfer the definition of string group extensions from the $\\infty$-category of spaces to the $\\infty$-category of smooth spaces?\n\nA) The singular complex functor assigns to a smooth space an underlying ordinary space that is a principal $\\infty$-bundle.\nB) The characterization of principal $\\infty$-bundles and group extensions in $\\infty$-topoi provides a new way to define smooth string group models.\nC) The work of Nikolaus, Schreiber, and Stevenson on $\\infty$-topoi provides a foundation for the authors' new characterizations of principal $\\infty$-bundles and group extensions.\nD) The smooth higher-categorical group extensions that arise as obstructions to the existence of equivariant structures on gerbes give rise to new smooth models for the string group.\n\nCorrect Answer: C) The work of Nikolaus, Schreiber, and Stevenson on $\\infty$-topoi provides a foundation for the authors' new characterizations of principal $\\infty$-bundles and group extensions.\n\nExplanation: The correct answer is C) because the work of Nikolaus, Schreiber, and Stevenson on $\\infty$-topoi is mentioned in the documentation as a foundation for the authors' new characterizations of principal $\\infty$-bundles and group extensions. This insight allows the authors to transfer the definition of string group extensions from the $\\infty$-category of spaces to the $\\infty$-category of smooth spaces.\n\nThe other options are incorrect because:\n\nA) The singular complex functor is mentioned in the documentation, but it is not the key insight that allows the authors to transfer the definition of string group extensions.\n\nB) The characterization of principal $\\infty$-bundles and group extensions in $\\infty$-topoi is mentioned in the documentation, but it is not the key insight that allows the authors to transfer the definition of string group extensions.\n\nD) The smooth higher-categorical group extensions that arise as obstructions to the existence of equivariant structures on gerbes give rise to new smooth models for the string group, but this is not the key insight that allows the authors to transfer the definition of string group extensions."}, "24": {"documentation": {"title": "Neutron rich matter, neutron stars, and their crusts", "source": "C. J. Horowitz", "docs_id": "1008.0402", "section": ["astro-ph.SR", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutron rich matter, neutron stars, and their crusts. Neutron rich matter is at the heart of many fundamental questions in Nuclear Physics and Astrophysics. What are the high density phases of QCD? Where did the chemical elements come from? What is the structure of many compact and energetic objects in the heavens, and what determines their electromagnetic, neutrino, and gravitational-wave radiations? Moreover, neutron rich matter is being studied with an extraordinary variety of new tools such as Facility for Rare Isotope Beams (FRIB) and the Laser Interferometer Gravitational Wave Observatory (LIGO). We describe the Lead Radius Experiment (PREX) that is using parity violation to measure the neutron radius in 208Pb. This has important implications for neutron stars and their crusts. Using large scale molecular dynamics, we model the formation of solids in both white dwarfs and neutron stars. We find neutron star crust to be the strongest material known, some 10 billion times stronger than steel. It can support mountains on rotating neutron stars large enough to generate detectable gravitational waves. Finally, we describe a new equation of state for supernova and neutron star merger simulations based on the Virial expansion at low densities, and large scale relativistic mean field calculations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the significance of the Lead Radius Experiment (PREX) in studying neutron rich matter, and how does it relate to the structure of neutron stars and their crusts?\n\nA) PREX is used to measure the neutron radius in 208Pb to determine the equation of state for neutron stars, which is crucial for understanding their electromagnetic and gravitational-wave radiations.\n\nB) PREX is a new tool that uses parity violation to measure the neutron radius in 208Pb, which has important implications for the formation of solids in white dwarfs and neutron stars, and can support mountains on rotating neutron stars.\n\nC) PREX is a simulation-based study that models the formation of solids in white dwarfs and neutron stars using large-scale molecular dynamics, which reveals that neutron star crusts are the strongest material known, with a strength of 10 billion times that of steel.\n\nD) PREX is a new equation of state for supernova and neutron star merger simulations based on the Virial expansion at low densities, which is used to study the structure of compact and energetic objects in the heavens.\n\n**Correct Answer:** C) PREX is a simulation-based study that models the formation of solids in white dwarfs and neutron stars using large-scale molecular dynamics, which reveals that neutron star crusts are the strongest material known, with a strength of 10 billion times that of steel.\n\n**Explanation:** The correct answer is C) because the Lead Radius Experiment (PREX) is indeed a simulation-based study that uses large-scale molecular dynamics to model the formation of solids in white dwarfs and neutron stars. The study reveals that neutron star crusts are the strongest material known, with a strength of 10 billion times that of steel, which has significant implications for our understanding of neutron stars and their crusts. The other options are incorrect because they either misrepresent the purpose of PREX or mix up different aspects of the study."}, "25": {"documentation": {"title": "A multivariate phase distribution and its estimation", "source": "Charles F. Cadieu and Kilian Koepsell", "docs_id": "0809.4291", "section": ["q-bio.NC", "nlin.AO", "nlin.CD", "q-bio.QM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A multivariate phase distribution and its estimation. Circular variables such as phase or orientation have received considerable attention throughout the scientific and engineering communities and have recently been quite prominent in the field of neuroscience. While many analytic techniques have used phase as an effective representation, there has been little work on techniques that capture the joint statistics of multiple phase variables. In this paper we introduce a distribution that captures empirically observed pair-wise phase relationships. Importantly, we have developed a computationally efficient and accurate technique for estimating the parameters of this distribution from data. We show that the algorithm performs well in high-dimensions (d=100), and in cases with limited data (as few as 100 samples per dimension). We also demonstrate how this technique can be applied to electrocorticography (ECoG) recordings to investigate the coupling of brain areas during different behavioral states. This distribution and estimation technique can be broadly applied to any setting that produces multiple circular variables."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A researcher is interested in analyzing the joint statistics of multiple phase variables in a high-dimensional dataset (d=100) with limited data (100 samples per dimension). Which of the following distributions is most suitable for capturing empirically observed pair-wise phase relationships, and what is the primary advantage of the proposed estimation technique?\n\n**A)** The multivariate phase distribution introduced in the paper, with the primary advantage being its ability to capture complex non-linear relationships between phase variables.\n\n**B)** The Gaussian distribution, with the primary advantage being its simplicity and ease of computation.\n\n**C)** The circular distribution, with the primary advantage being its ability to model circular data.\n\n**D)** The multivariate normal distribution, with the primary advantage being its ability to capture complex correlations between variables.\n\n**Correct Answer:** A) The multivariate phase distribution introduced in the paper, with the primary advantage being its ability to capture complex non-linear relationships between phase variables.\n\n**Explanation:** The correct answer is A) because the multivariate phase distribution is specifically designed to capture empirically observed pair-wise phase relationships, and the proposed estimation technique is computationally efficient and accurate. The primary advantage of this distribution is its ability to capture complex non-linear relationships between phase variables, which is essential for analyzing high-dimensional data with limited data points. The other options are incorrect because the Gaussian distribution is not suitable for circular data, the circular distribution is too simplistic, and the multivariate normal distribution is not specifically designed for phase variables."}, "26": {"documentation": {"title": "Calculations of the alpha decay half-lives of some Polonium isotopes\n  using the double folding model", "source": "W. A. Yahya and K. J. Oyewumi", "docs_id": "2111.05604", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Calculations of the alpha decay half-lives of some Polonium isotopes\n  using the double folding model. The calculations of the alpha decay half-lives of some Polonium isotopes in the mass range 186 - 218 have been carried out using the Wentzel-Kramers-Brillouin (WKB) semiclassical approximation. The alpha-nucleus effective potential used contains the Coulomb potential, centrifugal potential, and the nuclear potential. The nuclear potential is obtained via the double folding model, with the microscopic NN effective interactions derived from relativistic mean field theory Lagrangian (termed R3Y). Different parametrizations of the R3Y interactions have been employed in the computation of the nuclear potentials. The results obtained using the R3Y NN interactions are compared with the ones obtained using the famous Michigan-3-Yukawa (M3Y) interactions. The use of density-dependent NN interaction is also considered. When compared to available experimental data, there are improvements in the results when density-dependent interaction potentials are used compared to when density-independent interactions are employed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using density-dependent NN interaction potentials in the calculation of alpha decay half-lives of Polonium isotopes, as compared to density-independent interactions?\n\nA) Improved accuracy in calculating the Coulomb potential\nB) Enhanced ability to model the nuclear potential via the double folding model\nC) Better agreement with experimental data, particularly in the mass range 186 - 218\nD) Increased computational efficiency in the WKB approximation\n\nCorrect Answer: C) Better agreement with experimental data, particularly in the mass range 186 - 218\n\nExplanation: The question requires the test-taker to analyze the results of the calculations and identify the primary advantage of using density-dependent NN interaction potentials. The correct answer is based on the statement in the documentation that \"When compared to available experimental data, there are improvements in the results when density-dependent interaction potentials are used compared to when density-independent interactions are employed.\" This indicates that the density-dependent NN interaction potentials provide better agreement with experimental data, particularly in the specified mass range."}, "27": {"documentation": {"title": "Chemical composition of evolved stars in the open cluster M 67", "source": "G. Tautvaisiene (1), B. Edvardsson (2), I. Tuominen (3), I. Ilyin (3)\n  ((1) Institute of Theoretical Physics and Astronomy, Vilnius, Lithuania, (2)\n  Uppsala Astronomical Observatory, Sweden, (3) Astronomy Division, Dept. of\n  Physical Sciences, University of Oulu, Finland)", "docs_id": "astro-ph/0006001", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chemical composition of evolved stars in the open cluster M 67. High-resolution spectra of six core helium-burning `clump' stars and three giants in the open cluster M 67 have been obtained with the SOFIN spectrograph on the Nordic Optical Telescope to investigate abundances of up to 25 chemical elements. Abundances of carbon were studied using the C2 Swan (0,1) band head at 5635.5 A. The wavelength interval 7980-8130 A with strong CN features was analysed in order to determine nitrogen abundances and 12C/13C isotope ratios. The oxygen abundances were determined from the [O I] line at 6300 A. The overall metallicity of the cluster stars was found to be close to solar ([Fe/H]=-0.03+-0.03). Compared with the Sun and other dwarf stars of the Galactic disk, as well as with dwarf stars of M 67 itself, abundances in the investigated stars suggest that carbon is depleted by about 0.2 dex, nitrogen is enhanced by about 0.2 dex and oxygen is unaltered. Among other mixing- sensitive chemical elements an overabundance of sodium may be suspected. The mean C/N and 12C/13C ratios are lowered to the values of 1.7+-0.2 and 24+-4 in the giants and to the values of 1.4+-0.2 and 16+-4 in the clump stars. These results suggest that extra mixing of CN-cycled material to the stellar surface takes place after the He-core flash. Abundances of heavy chemical elements in all nine stars were found to be almost identical and close to solar."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study of the chemical composition of evolved stars in the open cluster M 67 reveals significant differences in the abundances of certain elements compared to the Sun and other dwarf stars. Which of the following statements accurately describes the findings of this study?\n\nA) The abundances of carbon, nitrogen, and oxygen in the investigated stars are all depleted by about 0.2 dex compared to the Sun.\nB) The abundances of carbon, nitrogen, and oxygen in the investigated stars are all enhanced by about 0.2 dex compared to the Sun.\nC) The abundances of carbon and nitrogen in the investigated stars are depleted by about 0.2 dex, while oxygen abundances are unaltered, and sodium may be overabundant.\nD) The abundances of carbon, nitrogen, and oxygen in the investigated stars are all unaltered compared to the Sun, with no significant differences observed.\n\n**Correct Answer:** C) The abundances of carbon and nitrogen in the investigated stars are depleted by about 0.2 dex, while oxygen abundances are unaltered, and sodium may be overabundant.\n\n**Explanation:** The study found that carbon is depleted by about 0.2 dex, nitrogen is enhanced by about 0.2 dex, and oxygen is unaltered. Additionally, the study suggests that sodium may be overabundant. This information is consistent with option C, which accurately summarizes the findings of the study. Options A and B are incorrect because they state that all three elements are depleted or enhanced, respectively, which is not supported by the data. Option D is also incorrect because it states that all three elements are unaltered, which is not consistent with the findings of the study."}, "28": {"documentation": {"title": "Enhanced spin transfer torque in platinum/ferromagnetic-metal structures\n  by optimizing the platinum thickness", "source": "Jianshu Xue, Yaping Guo, Ledong Wang, Yanan Dong, Yanxue Chen, Yufeng\n  Tian, Shishen Yan, and Lihui Bai", "docs_id": "1907.01278", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enhanced spin transfer torque in platinum/ferromagnetic-metal structures\n  by optimizing the platinum thickness. Spin transfer torque (STT) driven by a charge current plays a key role in magnetization switching in heavy-metal/ferromagnetic-metal structures. The STT efficiency defined by the ratio between the effective field due to STT and the current density, is required to be improved to reduce energy compulsions in the STT-based spintronic devices. In this work, using the harmonic Hall measurement method, we experimentally studied the STT efficiency in platinum(Pt)/FM structures as a function of the Pt thickness. We found that the STT efficiency strongly depends on the Pt thickness and reaches a maximum value of 4.259 mT/($10^6$A/$cm^{2}$) for the 1.8-nm-thickness Pt sample. This result indicates that competition between spin Hall effect (SHE) and Rashba effect as well as spin diffusion process across the Pt layer determines the Pt thickness for the maximum STT efficiency. We demonstrated the role played by the spin diffusion besides the spin current generation mechanisms in improvement of the STT efficiency, which is helpful in designing STT-based devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the optimal thickness of platinum (Pt) required to achieve the maximum spin transfer torque (STT) efficiency in platinum/ferromagnetic-metal structures, and what are the underlying mechanisms that contribute to this efficiency?\n\nA) 0.5 nm, where the spin Hall effect and Rashba effect dominate the STT efficiency\nB) 1.8 nm, where the spin diffusion process across the Pt layer plays a crucial role in improving the STT efficiency\nC) 3.0 nm, where the competition between spin Hall effect, Rashba effect, and spin diffusion determines the maximum STT efficiency\nD) 5.0 nm, where the spin current generation mechanisms are the primary contributors to the STT efficiency\n\nCorrect Answer: B) 1.8 nm, where the spin diffusion process across the Pt layer plays a crucial role in improving the STT efficiency\n\nExplanation: The correct answer is B) 1.8 nm, as the documentation states that the STT efficiency reaches a maximum value of 4.259 mT/($10^6$A/$cm^{2}$) for the 1.8-nm-thickness Pt sample. The underlying mechanism contributing to this efficiency is the spin diffusion process across the Pt layer, which is demonstrated in the work. The other options are incorrect because they do not accurately reflect the optimal thickness and underlying mechanisms described in the documentation."}, "29": {"documentation": {"title": "Characteristic momentum of Hydro+ and a bound on the speed of sound near\n  the QCD critical point", "source": "Navid Abbasi and Matthias Kaminski", "docs_id": "2112.14747", "section": ["nucl-th", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characteristic momentum of Hydro+ and a bound on the speed of sound near\n  the QCD critical point. Near the critical point in the QCD phase diagram, hydrodynamics breaks down at a momentum where the frequency of the fastest hydrodynamic mode becomes comparable with the decay rate of the slowest non-hydrodynamic mode. Hydro+ was developed as a framework which extends the range of validity of hydrodynamics beyond that momentum value. This was achieved through coupling the hydrodynamic modes to the slowest non-hydrodynamic mode. In this work, analyzing the spectrum of linear perturbations in Hydro+, we find that a slow mode falls out of equilibrium if its momentum is greater than a characteristic momentum value. That characteristic momentum turns out to be set by the branch points of the dispersion relations. These branch points occur at the critical momenta of so-called spectral curves and are related to the radius of convergence of the derivative expansion. The existence of such a characteristic momentum scale suggests that a particular class of slow modes has no remarkable effect on the flow of the plasma. Based on these results and previously derived relations to the stiffness of the equation of state, we find a temperature-dependent upper bound for the speed of sound near the critical point in the QCD phase diagram."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Hydro+, what is the relationship between the characteristic momentum value and the branch points of the dispersion relations, and how does this relationship impact the behavior of slow modes near the QCD critical point?\n\nA) The characteristic momentum value is equal to the radius of convergence of the derivative expansion, and slow modes are unaffected by this scale.\n\nB) The characteristic momentum value is related to the stiffness of the equation of state, and slow modes are only significant at temperatures above the critical temperature.\n\nC) The characteristic momentum value is set by the branch points of the dispersion relations, and slow modes fall out of equilibrium if their momentum exceeds this value, indicating a potential impact on the flow of the plasma.\n\nD) The characteristic momentum value is a measure of the speed of sound near the QCD critical point, and slow modes are only relevant at temperatures below the critical temperature.\n\nCorrect Answer: C) The characteristic momentum value is set by the branch points of the dispersion relations, and slow modes fall out of equilibrium if their momentum exceeds this value, indicating a potential impact on the flow of the plasma.\n\nExplanation: The correct answer is C) because the documentation states that the characteristic momentum value is set by the branch points of the dispersion relations, and that slow modes fall out of equilibrium if their momentum exceeds this value. This suggests that the characteristic momentum value has a significant impact on the behavior of slow modes near the QCD critical point, and that it may affect the flow of the plasma. The other options are incorrect because they either misrepresent the relationship between the characteristic momentum value and the branch points, or they introduce additional information that is not present in the documentation."}, "30": {"documentation": {"title": "Studying star-forming processes towards G29.862-0.044", "source": "M. B. Areal, S. Paron, M. E. Ortega and C. Fari\\~na", "docs_id": "1904.10914", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Studying star-forming processes towards G29.862-0.044. We present a multiwavelength study towards the young stellar object (YSO) G29.862-0.044 (hereafter G29), which is embedded in the massive star-forming region G29.96-0.02, located at a distance of about 6.5 kpc. The surrounding interstellar medium of G29 is studied using molecular lines data (angular resolution about 15'') obtained from the databases of the James Clerk Maxwell Telescope. The physical conditions of G29 molecular outflows and the clump where the YSO is embedded are characterized. Near-IR data is also analyzed (spatial resolution of about 0.5'') obtained with NIRI at Gemini North to have a detailed view of the circumstellar ambient of G29. The Ks emission towards G29 exhibits a cone-like feature pointing to the red molecular outflow. The aim of this work is to obtain a complete picture of this YSO, the related star-forming processes, and the interstellar medium around it. Studies like this are important because they contribute to a comprehensive understanding of star formation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the physical conditions of the G29 molecular outflows and the clump where the YSO is embedded based on the study?\n\nA) The outflows are likely to be driven by a strong stellar wind, given the presence of a cone-like feature in the Ks emission.\nB) The outflows are likely to be driven by a weak stellar wind, given the lack of evidence for strong stellar activity in the region.\nC) The outflows are likely to be driven by a combination of stellar and turbulent processes, given the complex morphology of the outflows.\nD) The outflows are likely to be driven by a shock wave, given the presence of a molecular outflow.\n\nCorrect Answer: C) The outflows are likely to be driven by a combination of stellar and turbulent processes, given the complex morphology of the outflows.\n\nExplanation: The study presents a multiwavelength analysis of the G29 region, including molecular lines data and near-IR observations. The presence of a cone-like feature in the Ks emission suggests that the outflows are driven by a strong stellar source, but the complex morphology of the outflows also suggests that other processes, such as turbulence, may be involved. This is consistent with the idea that the outflows are driven by a combination of stellar and turbulent processes."}, "31": {"documentation": {"title": "A single determinant for the rate of yeast protein evolution", "source": "D. Allan Drummond, Alpan Raval, Claus O. Wilke", "docs_id": "q-bio/0506011", "section": ["q-bio.PE", "q-bio.GN", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A single determinant for the rate of yeast protein evolution. A gene's rate of sequence evolution is among the most fundamental evolutionary quantities in common use, but what determines evolutionary rates has remained unclear. Here, we show that the two most commonly used methods to disentangle the determinants of evolutionary rate, partial correlation analysis and ordinary multivariate regression, produce misleading or spurious results when applied to noisy biological data. To overcome these difficulties, we employ an alternative method, principal component regression, which is a multivariate regression of evolutionary rate against the principal components of the predictor variables. We carry out the first combined analysis of seven predictors (gene expression level, dispensability, protein abundance, codon adaptation index, gene length, number of protein-protein interactions, and the gene's centrality in the interaction network). Strikingly, our analysis reveals a single dominant component which explains 40-fold more variation in evolutionary rate than any other, suggesting that protein evolutionary rate has a single determinant among the seven predictors. The dominant component explains nearly half the variation in the rate of synonymous and protein evolution. Our results support the hypothesis that selection against the cost of translation-error-induced protein misfolding governs the rate of synonymous and protein sequence evolution in yeast."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study \"A single determinant for the rate of yeast protein evolution\" employed an alternative method, principal component regression, to analyze the determinants of evolutionary rate. What is the primary advantage of using principal component regression in this context, and how does it differ from the two commonly used methods, partial correlation analysis and ordinary multivariate regression?\n\n**A)** Principal component regression is more robust to noisy biological data, as it can capture complex relationships between variables that are not linear. This is because it uses the principal components of the predictor variables, which are orthogonal and uncorrelated.\n\n**B)** Principal component regression is more computationally efficient than partial correlation analysis and ordinary multivariate regression, as it requires fewer parameters to estimate. This is because it uses a single dominant component to explain the variation in evolutionary rate, rather than multiple components.\n\n**C)** Principal component regression is more interpretable than partial correlation analysis and ordinary multivariate regression, as it provides a clear and interpretable representation of the relationships between variables. This is because it uses principal components that are easy to understand and visualize.\n\n**D)** Principal component regression is more generalizable than partial correlation analysis and ordinary multivariate regression, as it can handle complex relationships between variables that are not linear. This is because it uses a multivariate regression approach that can capture non-linear relationships.\n\n**Correct Answer:** A) Principal component regression is more robust to noisy biological data, as it can capture complex relationships between variables that are not linear. This is because it uses the principal components of the predictor variables, which are orthogonal and uncorrelated.\n\n**Explanation:** The correct answer is A) because principal component regression is more robust to noisy biological data, which is a key challenge in analyzing evolutionary rates. The authors of the study highlight the limitations of partial correlation analysis and ordinary multivariate regression in disentangling the determinants of evolutionary rate, and argue that principal component regression provides a more robust and accurate approach. The other options are incorrect because they do not accurately capture the primary advantage of principal component regression in this context."}, "32": {"documentation": {"title": "Electrical detection of the inverse Edelstein effect on the surface of\n  SmB$_6$", "source": "Jehyun Kim, Chaun Jang, Xiangfeng Wang, Johnpierre Paglione, Seokmin\n  Hong, Shehrin Sayed, Dongwon Chun and Dohun Kim", "docs_id": "2006.11682", "section": ["physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electrical detection of the inverse Edelstein effect on the surface of\n  SmB$_6$. We report the measurement of spin current induced charge accumulation, the inverse Edelstein effect (IEE), on the surface of a candidate topological Kondo insulator SmB6 single crystal. Robust surface conduction channel of SmB6 has been shown to exhibit large degree of spin-momentum locking, and spin polarized current through an external ferromagnetic contact induces the spin dependent charge accumulation on the surface of SmB6. The dependences of the IEE signal on the bias current, an external magnetic field direction and temperature are consistent with the anticlockwise spin texture for the surface band in SmB6 in the momentum space, and the direction and magnitude of the effect compared with the normal Edelstein signal are clearly explained by the Onsager reciprocal relation. Furthermore, we estimate spin-to-charge conversion efficiency, the IEE length, as 4.46 nm that is an order of magnitude larger than the efficiency found in other typical Rashba interfaces, implying that the Rashba contribution to the IEE signal could be small. Building upon existing reports on the surface charge and spin conduction nature on this material, our results provide additional evidence that the surface of SmB6 supports spin polarized conduction channel."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism underlying the observed inverse Edelstein effect (IEE) on the surface of SmB6, and how does it compare to the normal Edelstein effect?\n\nA) The IEE is primarily driven by the Rashba effect, which is a spin-orbit coupling mechanism that arises from the interaction between the spin of an electron and its momentum.\n\nB) The IEE is primarily driven by the Onsager reciprocal relation, which relates the spin-dependent charge accumulation to the spin-polarized current through an external ferromagnetic contact.\n\nC) The IEE is primarily driven by the spin-momentum locking of the surface conduction channel of SmB6, which leads to a robust surface conduction channel that exhibits large degree of spin-momentum locking.\n\nD) The IEE is primarily driven by the spin-to-charge conversion efficiency, which is estimated to be 4.46 nm, an order of magnitude larger than the efficiency found in other typical Rashba interfaces.\n\nCorrect Answer: B) The IEE is primarily driven by the Onsager reciprocal relation, which relates the spin-dependent charge accumulation to the spin-polarized current through an external ferromagnetic contact.\n\nExplanation: The correct answer is B) because the documentation states that the dependences of the IEE signal on the bias current, an external magnetic field direction, and temperature are consistent with the anticlockwise spin texture for the surface band in SmB6 in the momentum space, and the direction and magnitude of the effect compared with the normal Edelstein signal are clearly explained by the Onsager reciprocal relation. This suggests that the IEE is primarily driven by the Onsager reciprocal relation, which relates the spin-dependent charge accumulation to the spin-polarized current through an external ferromagnetic contact."}, "33": {"documentation": {"title": "Non-linear Dynamics and Mass Function of Cosmic Structures: I Analytical\n  Results", "source": "Edouard Audit (1), Romain Teyssier (2) & Jean-Michel Alimi (1) ((1):\n  LAEC, Observatoire de Meudon, France (2): Service d'Astrophysique CEA-SACLAY,\n  France)", "docs_id": "astro-ph/9704023", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-linear Dynamics and Mass Function of Cosmic Structures: I Analytical\n  Results. We investigate some modifications to the Press & Schechter (1974) (PS) prescription resulting from shear and tidal effects. These modifications rely on more realistic treatments of the collapse process than the standard approach based on the spherical model. First, we show that the mass function resulting from a new approximate Lagrangian dynamic (Audit & Alimi 96), contains more objects at high mass, than the classical PS mass function and is well fitted by a PS-like function with a threshold density of $\\delta_c \\simeq 1.4$. However, such a Lagrangian description can underestimate the epoch of structure formation since it defines it as the collapse of the first principal axis. We therefore suggest some analytical prescriptions, for computing the collapse time along the second and third principal axes, and we deduce the corresponding mass functions. The collapse along the third axis is delayed by the shear and the number of objects of high mass then decreases. Finally, we show that the shear also strongly affects the formation of low-mass halos. This dynamical effect implies a modification of the low-mass slope of the mass function and allows the reproduction of the observed luminosity function of field galaxies. In a companion paper, we present results of numerical simulations which complete this work."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary effect of shear and tidal effects on the collapse process of cosmic structures, and how does it impact the resulting mass function?\n\nA) Shear and tidal effects accelerate the collapse process, leading to a decrease in the number of high-mass objects.\nB) Shear and tidal effects delay the collapse process, resulting in a decrease in the number of high-mass objects and a modification of the low-mass slope of the mass function.\nC) Shear and tidal effects have no significant impact on the collapse process, leading to a PS-like mass function with a threshold density of $\\delta_c \\simeq 1.4$.\nD) Shear and tidal effects only affect the formation of low-mass halos, with no impact on the high-mass end of the mass function.\n\nCorrect Answer: B) Shear and tidal effects delay the collapse process, resulting in a decrease in the number of high-mass objects and a modification of the low-mass slope of the mass function.\n\nExplanation: The correct answer is based on the text, which states that the collapse along the third axis is delayed by the shear, leading to a decrease in the number of objects of high mass. Additionally, the text mentions that the shear also affects the formation of low-mass halos, resulting in a modification of the low-mass slope of the mass function. This requires the test-taker to carefully read and understand the text, and to identify the key points related to the effect of shear and tidal effects on the collapse process."}, "34": {"documentation": {"title": "Operator Approach to Isospin Violation in Pion Photoproduction", "source": "B. Ananthanarayan", "docs_id": "hep-ph/0602015", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Operator Approach to Isospin Violation in Pion Photoproduction. Unambiguous isospin violation in the strong interaction sector is a key issue in low energy hadronic physics, both experimentally and theoretically. Bernstein has employed the Fermi-Watson theorem to demonstrate that pion photoproduction is a process where isospin violation in the pi N system can be revealed, an approach we review here. Here we propose a general operator approach to the phenomenon in pion photoproduction, thereby providing an analogue for the framework that was proposed for pi N scattering by Kaufmann and Gibbs. The resulting set of amplitudes could form the basis for determining the multipole amplitudes for photoproduction. Thus, the so resulting phase shift determination from photoproduction can then be used via the Fermi-Watson theorem to resolve discrepancies in pi N phase shift analyses. We point out that casting effective Lagrangian results in terms of our framework would be beneficial. The upcoming polarization experiments are an ideal setting to test our approach, and also to constrain better the isotensor currents which strictly are not forbidden."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed operator approach to isospin violation in pion photoproduction, and how does it relate to the Fermi-Watson theorem and the determination of pi N phase shifts?\n\nA) To demonstrate the existence of isospin violation in the pi N system through effective Lagrangian results\nB) To provide an analogue for the framework proposed for pi N scattering by Kaufmann and Gibbs, thereby enabling the determination of multipole amplitudes for photoproduction\nC) To constrain the isotensor currents which are strictly forbidden in the strong interaction sector\nD) To resolve discrepancies in pi N phase shift analyses through the use of Fermi-Watson theorem\n\nCorrect Answer: B) To provide an analogue for the framework proposed for pi N scattering by Kaufmann and Gibbs, thereby enabling the determination of multipole amplitudes for photoproduction\n\nExplanation: The correct answer is B) because the proposed operator approach aims to provide a general framework for understanding isospin violation in pion photoproduction, which is analogous to the framework proposed by Kaufmann and Gibbs for pi N scattering. This framework can then be used to determine the multipole amplitudes for photoproduction, which can be used to resolve discrepancies in pi N phase shift analyses. The other options are incorrect because they do not accurately capture the primary goal of the proposed operator approach. Option A is incorrect because the effective Lagrangian results are not the primary focus of the proposed approach. Option C is incorrect because the isotensor currents are not the primary target of the proposed approach. Option D is incorrect because the Fermi-Watson theorem is used to resolve discrepancies in pi N phase shift analyses, but it is not the primary goal of the proposed operator approach."}, "35": {"documentation": {"title": "Temporal pattern recognition through analog molecular computation", "source": "Jackson O'Brien and Arvind Murugan", "docs_id": "1810.02883", "section": ["q-bio.MN", "nlin.AO", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal pattern recognition through analog molecular computation. Living cells communicate information about physiological conditions by producing signaling molecules in a specific timed manner. Different conditions can result in the same total amount of a signaling molecule, differing only in the pattern of the molecular concentration over time. Such temporally coded information can be completely invisible to even state-of-the-art molecular sensors with high chemical specificity that respond only to the total amount of the signaling molecule. Here, we demonstrate design principles for circuits with temporal specificity, that is, molecular circuits that respond to specific temporal patterns in a molecular concentration. We consider pulsatile patterns in a molecular concentration characterized by three fundamental temporal features - time period, duty fraction and number of pulses. We develop circuits that respond to each one of these features while being insensitive to the others. We demonstrate our design principles using abstract Chemical Reaction Networks and with explicit simulations of DNA strand displacement reactions. In this way, our work develops building blocks for temporal pattern recognition through molecular computation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of current molecular sensors in detecting signaling molecules, according to the paper \"Temporal pattern recognition through analog molecular computation\"?\n\nA) They are unable to distinguish between different signaling molecules.\nB) They are unable to detect signaling molecules that are present in low concentrations.\nC) They are unable to detect signaling molecules that are present in a specific timed manner.\nD) They are unable to detect signaling molecules that are produced in a pulsatile pattern.\n\nCorrect Answer: C) They are unable to detect signaling molecules that are present in a specific timed manner.\n\nExplanation: The paper states that \"Different conditions can result in the same total amount of a signaling molecule, differing only in the pattern of the molecular concentration over time.\" This implies that current molecular sensors are unable to detect signaling molecules that are present in a specific timed manner, which is the primary limitation mentioned in the paper.\n\nCandidate A is incorrect because the paper does not mention anything about distinguishing between different signaling molecules. Candidate B is incorrect because the paper does not mention anything about detecting signaling molecules in low concentrations. Candidate D is incorrect because the paper does not mention anything about detecting signaling molecules in a pulsatile pattern, but rather that pulsatile patterns can be completely invisible to current molecular sensors."}, "36": {"documentation": {"title": "Robust and Efficient Approximate Bayesian Computation: A Minimum\n  Distance Approach", "source": "David T. Frazier", "docs_id": "2006.14126", "section": ["stat.ME", "econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust and Efficient Approximate Bayesian Computation: A Minimum\n  Distance Approach. In many instances, the application of approximate Bayesian methods is hampered by two practical features: 1) the requirement to project the data down to low-dimensional summary, including the choice of this projection, which ultimately yields inefficient inference; 2) a possible lack of robustness to deviations from the underlying model structure. Motivated by these efficiency and robustness concerns, we construct a new Bayesian method that can deliver efficient estimators when the underlying model is well-specified, and which is simultaneously robust to certain forms of model misspecification. This new approach bypasses the calculation of summaries by considering a norm between empirical and simulated probability measures. For specific choices of the norm, we demonstrate that this approach can deliver point estimators that are as efficient as those obtained using exact Bayesian inference, while also simultaneously displaying robustness to deviations from the underlying model assumptions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of the new Bayesian method described in the paper \"Robust and Efficient Approximate Bayesian Computation: A Minimum Distance Approach\", and how does it address the limitations of existing approximate Bayesian methods?\n\nA) The method is motivated by the need for more efficient inference in high-dimensional data, and it addresses the limitation of requiring a choice of projection that can lead to inefficient inference.\nB) The method is motivated by the need for robustness to model misspecification, and it addresses the limitation of requiring a calculation of summaries that can be sensitive to deviations from the underlying model structure.\nC) The method is motivated by the need for more efficient inference in low-dimensional data, and it addresses the limitation of requiring a choice of projection that can lead to inefficient inference.\nD) The method is motivated by the need for robustness to model misspecification, and it addresses the limitation of requiring a calculation of summaries that can be sensitive to deviations from the underlying model structure.\n\nCorrect Answer: B) The method is motivated by the need for robustness to model misspecification, and it addresses the limitation of requiring a calculation of summaries that can be sensitive to deviations from the underlying model structure.\n\nExplanation: The correct answer is B) because the paper states that the application of approximate Bayesian methods is hampered by two practical features: 1) the requirement to project the data down to low-dimensional summaries, including the choice of this projection, which ultimately yields inefficient inference; 2) a possible lack of robustness to deviations from the underlying model structure. The new method is motivated by these efficiency and robustness concerns, and it addresses the limitation of requiring a calculation of summaries that can be sensitive to deviations from the underlying model structure."}, "37": {"documentation": {"title": "Simulation of Multidimensional Diffusions with Sticky Boundaries via\n  Markov Chain Approximation", "source": "Christian Meier, Lingfei Li, Gongqiu Zhang", "docs_id": "2107.04260", "section": ["math.PR", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simulation of Multidimensional Diffusions with Sticky Boundaries via\n  Markov Chain Approximation. We develop a new simulation method for multidimensional diffusions with sticky boundaries. The challenge comes from simulating the sticky boundary behavior, for which standard methods like the Euler scheme fail. We approximate the sticky diffusion process by a multidimensional continuous time Markov chain (CTMC), for which we can simulate easily. We develop two ways of constructing the CTMC: approximating the infinitesimal generator of the sticky diffusion by finite difference using standard coordinate directions, and matching the local moments using the drift and the eigenvectors of the covariance matrix as transition directions. The first approach does not always guarantee a valid Markov chain whereas the second one can. We show that both construction methods yield a first order simulation scheme, which can capture the sticky behavior and it is free from the curse of dimensionality. We apply our method to two applications: a multidimensional Brownian motion with all dimensions sticky which arises as the limit of a queuing system with exceptional service policy, and a multi-factor short rate model for low interest rate environment in which the stochastic factors are unbounded but the short rate is sticky at zero."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a multidimensional diffusion process with sticky boundaries, where the sticky boundary behavior is approximated by a multidimensional continuous time Markov chain (CTMC). Suppose we use the second construction method, which matches the local moments using the drift and the eigenvectors of the covariance matrix as transition directions. What is the order of accuracy of this simulation scheme, and how does it compare to the first construction method, which approximates the infinitesimal generator of the sticky diffusion using standard coordinate directions?\n\n**A)** The order of accuracy of the second construction method is first order, and it is guaranteed to produce a valid Markov chain. However, it may not capture the sticky behavior as accurately as the first method.\n\n**B)** The order of accuracy of the second construction method is second order, and it is not guaranteed to produce a valid Markov chain. However, it may capture the sticky behavior more accurately than the first method.\n\n**C)** The order of accuracy of the second construction method is first order, and it is guaranteed to produce a valid Markov chain. Additionally, it is free from the curse of dimensionality.\n\n**D)** The order of accuracy of the second construction method is first order, but it is not guaranteed to produce a valid Markov chain. However, it may capture the sticky behavior more accurately than the first method.\n\n**Correct Answer:** C) The order of accuracy of the second construction method is first order, and it is guaranteed to produce a valid Markov chain. Additionally, it is free from the curse of dimensionality.\n\n**Explanation:** The correct answer is C) because the second construction method is guaranteed to produce a valid Markov chain, and it has a first-order accuracy. This is because the method matches the local moments using the drift and the eigenvectors of the covariance matrix as transition directions, which ensures that the resulting CTMC is a valid Markov chain. Additionally, the method is free from the curse of dimensionality, meaning that it can capture the sticky behavior of the multidimensional diffusion process without requiring an exponentially large number of dimensions."}, "38": {"documentation": {"title": "Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC", "source": "Anh Huy Phan and Petr Tichavsk\\'y and Andrzej Cichocki", "docs_id": "1205.2584", "section": ["cs.NA", "cs.LG", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC. The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP) decomposition can handle the challenges of collinearity of factors and different magnitudes of factors; nevertheless, for factorization of an $N$-D tensor of size $I_1\\times I_N$ with rank $R$, the algorithm is computationally demanding due to construction of large approximate Hessian of size $(RT \\times RT)$ and its inversion where $T = \\sum_n I_n$. In this paper, we propose a fast implementation of the dGN algorithm which is based on novel expressions of the inverse approximate Hessian in block form. The new implementation has lower computational complexity, besides computation of the gradient (this part is common to both methods), requiring the inversion of a matrix of size $NR^2\\times NR^2$, which is much smaller than the whole approximate Hessian, if $T \\gg NR$. In addition, the implementation has lower memory requirements, because neither the Hessian nor its inverse never need to be stored in their entirety. A variant of the algorithm working with complex valued data is proposed as well. Complexity and performance of the proposed algorithm is compared with those of dGN and ALS with line search on examples of difficult benchmark tensors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main computational challenge in the damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP) decomposition, particularly when dealing with large N-D tensors of size $I_1\\times I_N$ with rank R?\n\nA) The construction of the approximate Hessian matrix\nB) The inversion of the approximate Hessian matrix\nC) The computation of the gradient of the objective function\nD) The storage of the Hessian and its inverse in memory\n\nCorrect Answer: B) The inversion of the approximate Hessian matrix\n\nExplanation: The dGN algorithm is computationally demanding due to the construction of a large approximate Hessian matrix of size $(RT \\times RT)$ and its inversion, where $T = \\sum_n I_n$. This is the main computational challenge in the algorithm, as it requires significant computational resources. The other options are not correct because while the construction of the Hessian matrix and the computation of the gradient are important steps in the algorithm, they are not the main computational challenge. The storage of the Hessian and its inverse in memory is also not the main challenge, as the algorithm can work with block forms of the Hessian and its inverse, reducing memory requirements."}, "39": {"documentation": {"title": "PaPy: Parallel and Distributed Data-processing Pipelines in Python", "source": "Marcin Cieslik and Cameron Mura", "docs_id": "1407.4378", "section": ["cs.PL", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "PaPy: Parallel and Distributed Data-processing Pipelines in Python. PaPy, which stands for parallel pipelines in Python, is a highly flexible framework that enables the construction of robust, scalable workflows for either generating or processing voluminous datasets. A workflow is created from user-written Python functions (nodes) connected by 'pipes' (edges) into a directed acyclic graph. These functions are arbitrarily definable, and can make use of any Python modules or external binaries. Given a user-defined topology and collection of input data, functions are composed into nested higher-order maps, which are transparently and robustly evaluated in parallel on a single computer or on remote hosts. Local and remote computational resources can be flexibly pooled and assigned to functional nodes, thereby allowing facile load-balancing and pipeline optimization to maximize computational throughput. Input items are processed by nodes in parallel, and traverse the graph in batches of adjustable size -- a trade-off between lazy-evaluation, parallelism, and memory consumption. The processing of a single item can be parallelized in a scatter/gather scheme. The simplicity and flexibility of distributed workflows using PaPy bridges the gap between desktop -> grid, enabling this new computing paradigm to be leveraged in the processing of large scientific datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a scenario where you need to process a large dataset of images using PaPy. The dataset consists of 1000 images, each with a resolution of 1024x768 pixels. You want to use PaPy to parallelize the processing of these images, but you also want to ensure that the processing is done in batches of 100 images at a time to minimize memory consumption. How would you define the topology of the PaPy workflow to achieve this?\n\n**A)** Define a single function `image_processor` that takes a single image as input and processes it using a Python module `image_processing`. Then, define a pipeline with 10 nodes, each connected to the previous node, to process the images in batches of 100.\n\n**B)** Define a function `image_batch_processor` that takes a batch of 100 images as input and processes them using a Python module `image_processing`. Then, define a pipeline with 10 nodes, each connected to the previous node, to process the batches in parallel.\n\n**C)** Define a function `image_processor` that takes a single image as input and processes it using a Python module `image_processing`. Then, define a pipeline with 10 nodes, each connected to the previous node, to process the images in batches of 100. Use a scatter/gather scheme to parallelize the processing of each image.\n\n**D)** Define a function `image_batch_processor` that takes a batch of 100 images as input and processes them using a Python module `image_processing`. Then, define a pipeline with 10 nodes, each connected to the previous node, to process the batches in parallel. Use a lazy-evaluation scheme to process the images in batches of 100.\n\n**Correct Answer:** B) Define a function `image_batch_processor` that takes a batch of 100 images as input and processes them using a Python module `image_processing`. Then, define a pipeline with 10 nodes, each connected to the previous node, to process the batches in parallel.\n\n**Explanation:** The correct answer is B) because it defines a function `image_batch_processor` that takes a batch of 100 images as input and processes them using a Python module `image_processing`. This allows for parallelization of the processing of each batch, which is the desired behavior. The pipeline is defined with 10 nodes, each connected to the previous node, to process the batches in parallel. This ensures that the processing is done in batches of 100 images at a time, minimizing memory consumption. The other options do not achieve this desired behavior. Option A defines a single function that processes a single image, which is not parallelizable. Option C defines a pipeline with a scatter/gather scheme, which is not necessary in this case. Option D defines a pipeline with a lazy-evaluation scheme, which is not suitable for this scenario."}, "40": {"documentation": {"title": "Episodic deluges in simulated hothouse climates", "source": "Jacob Seeley and Robin Wordsworth", "docs_id": "2111.03109", "section": ["astro-ph.EP", "nlin.AO", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Episodic deluges in simulated hothouse climates. Earth's distant past and potentially its future include extremely warm \"hothouse\" climate states, but little is known about how the atmosphere behaves in such states. One distinguishing characteristic of hothouse climates is that they feature lower-tropospheric radiative heating, rather than cooling, due to the closing of the water vapor infrared window regions. Previous work has suggested that this could lead to temperature inversions and significant changes in cloud cover, but no previous modeling of the hothouse regime has resolved convective-scale turbulent air motions and cloud cover directly, thus leaving many questions about hothouse radiative heating unanswered. Here, we conduct simulations that explicitly resolve convection and find that lower-tropospheric radiative heating in hothouse climates causes the hydrologic cycle to shift from a quasi-steady regime to a \"relaxation oscillator\" regime, in which precipitation occurs in short and intense outbursts separated by multi-day dry spells. The transition to the oscillatory regime is accompanied by strongly enhanced local precipitation fluxes, a significant increase in cloud cover, and a transiently positive (unstable) climate feedback parameter. Our results indicate that hothouse climates may feature a novel form of \"temporal\" convective self-organization, with implications for both cloud coverage and erosion processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary consequence of lower-tropospheric radiative heating in hothouse climates, according to the simulations conducted in the study?\n\nA) A significant increase in global temperature, leading to accelerated climate change.\nB) A shift from a quasi-steady to a relaxation oscillator regime in the hydrologic cycle, resulting in intense precipitation outbursts and dry spells.\nC) A decrease in cloud cover, leading to reduced precipitation and increased atmospheric stability.\nD) A transiently positive climate feedback parameter, but no significant changes in cloud cover or precipitation.\n\nCorrect Answer: B) A shift from a quasi-steady to a relaxation oscillator regime in the hydrologic cycle, resulting in intense precipitation outbursts and dry spells.\n\nExplanation: The study found that lower-tropospheric radiative heating in hothouse climates leads to a significant change in the hydrologic cycle, shifting from a quasi-steady regime to a relaxation oscillator regime. This results in intense precipitation outbursts and dry spells, which has implications for cloud coverage and erosion processes. The other options are incorrect because they do not accurately reflect the primary consequence of lower-tropospheric radiative heating in hothouse climates."}, "41": {"documentation": {"title": "Automated high-throughput Wannierisation", "source": "Valerio Vitale, Giovanni Pizzi, Antimo Marrazzo, Jonathan R. Yates,\n  Nicola Marzari, Arash A. Mostofi", "docs_id": "1909.00433", "section": ["physics.comp-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automated high-throughput Wannierisation. Maximally-localised Wannier functions (MLWFs) are routinely used to compute from first-principles advanced materials properties that require very dense Brillouin zone integration and to build accurate tight-binding models for scale-bridging simulations. At the same time, high-throughput (HT) computational materials design is an emergent field that promises to accelerate the reliable and cost-effective design and optimisation of new materials with target properties. The use of MLWFs in HT workflows has been hampered by the fact that generating MLWFs automatically and robustly without any user intervention and for arbitrary materials is, in general, very challenging. We address this problem directly by proposing a procedure for automatically generating MLWFs for HT frameworks. Our approach is based on the selected columns of the density matrix method (SCDM) and we present the details of its implementation in an AiiDA workflow. We apply our approach to a dataset of 200 bulk crystalline materials that span a wide structural and chemical space. We assess the quality of our MLWFs in terms of the accuracy of the band-structure interpolation that they provide as compared to the band-structure obtained via full first-principles calculations. Finally, we provide a downloadable virtual machine that can be used to reproduce the results of this paper, including all first-principles and atomistic simulations as well as the computational workflows."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in using maximally-localised Wannier functions (MLWFs) in high-throughput (HT) computational materials design, and how does the proposed approach address this challenge?\n\nA) The primary challenge is the computational cost of generating MLWFs, and the proposed approach uses a machine learning algorithm to reduce this cost.\nB) The primary challenge is the difficulty in generating MLWFs for arbitrary materials, and the proposed approach uses the selected columns of the density matrix method (SCDM) to automate this process.\nC) The primary challenge is the accuracy of the band-structure interpolation provided by MLWFs, and the proposed approach uses a combination of first-principles and atomistic simulations to improve this accuracy.\nD) The primary challenge is the lack of standardization in MLWF generation, and the proposed approach uses a standardized workflow to ensure consistency across different materials.\n\nCorrect Answer: B) The primary challenge is the difficulty in generating MLWFs for arbitrary materials, and the proposed approach uses the selected columns of the density matrix method (SCDM) to automate this process.\n\nExplanation: The question requires the test-taker to understand the context of the paper and identify the primary challenge in using MLWFs in HT computational materials design. The correct answer, B, is supported by the text, which states that \"generating MLWFs automatically and robustly without any user intervention and for arbitrary materials is, in general, very challenging.\" The proposed approach addresses this challenge by using the selected columns of the density matrix method (SCDM). The other options are incorrect because they do not accurately reflect the primary challenge or the proposed approach."}, "42": {"documentation": {"title": "Closing Gaps in Asymptotic Fair Division", "source": "Pasin Manurangsi, Warut Suksompong", "docs_id": "2004.05563", "section": ["cs.GT", "econ.TH", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Closing Gaps in Asymptotic Fair Division. We study a resource allocation setting where $m$ discrete items are to be divided among $n$ agents with additive utilities, and the agents' utilities for individual items are drawn at random from a probability distribution. Since common fairness notions like envy-freeness and proportionality cannot always be satisfied in this setting, an important question is when allocations satisfying these notions exist. In this paper, we close several gaps in the line of work on asymptotic fair division. First, we prove that the classical round-robin algorithm is likely to produce an envy-free allocation provided that $m=\\Omega(n\\log n/\\log\\log n)$, matching the lower bound from prior work. We then show that a proportional allocation exists with high probability as long as $m\\geq n$, while an allocation satisfying envy-freeness up to any item (EFX) is likely to be present for any relation between $m$ and $n$. Finally, we consider a related setting where each agent is assigned exactly one item and the remaining items are left unassigned, and show that the transition from non-existence to existence with respect to envy-free assignments occurs at $m=en$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of asymptotic fair division, what is the condition under which the classical round-robin algorithm is likely to produce an envy-free allocation?\n\nA) $m=\\Omega(n\\log n/\\log\\log n)$\nB) $m=\\Omega(n^2)$\nC) $m=\\Omega(n^3)$\nD) $m=\\Omega(n^4)$\n\n**Correct Answer:** A) $m=\\Omega(n\\log n/\\log\\log n)$\n\n**Explanation:** The correct answer is based on the statement in the paper that the classical round-robin algorithm is likely to produce an envy-free allocation provided that $m=\\Omega(n\\log n/\\log\\log n)$. This condition matches the lower bound from prior work, indicating that the algorithm is asymptotically fair under this condition.\n\n**Explanation for incorrect options:**\n\n* B) $m=\\Omega(n^2)$: This condition is not mentioned in the paper as a requirement for envy-free allocations.\n* C) $m=\\Omega(n^3)$: This condition is not mentioned in the paper as a requirement for envy-free allocations.\n* D) $m=\\Omega(n^4)$: This condition is not mentioned in the paper as a requirement for envy-free allocations.\n\nNote: The options are designed to be challenging, but not impossible to answer with the information provided in the documentation."}, "43": {"documentation": {"title": "Generalized Continuity Equations for Schr\\\"odinger and Dirac Equations", "source": "A. Katsaris, P.A. Kalozoumis, and F.K. Diakonos", "docs_id": "2103.00052", "section": ["quant-ph", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalized Continuity Equations for Schr\\\"odinger and Dirac Equations. The concept of the generalized continuity equation (GCE) was recently introduced in [J. Phys. A: Math. and Theor. {\\bf 52}, 1552034 (2019)], and was derived in the context of $N$ independent Schr\\\"{o}dinger systems. The GCE is induced by a symmetry transformation which mixes the states of these systems, even though the $N$-system Lagrangian does not. As the $N$-system Schr\\\"{o}dinger Lagrangian is not invariant under such a transformation, the GCE will involve source terms which, under certain conditions vanish and lead to conserved currents. These conditions may hold globally or locally in a finite domain, leading to globally or locally conserved currents, respectively. In this work, we extend this idea to the case of arbitrary $SU(N)$-transformations and we show that a similar GCE emerges for $N$ systems in the Dirac dynamics framework. The emerging GCEs and the conditions which lead to the attendant conservation laws provide a rich phenomenology and potential use for the preparation and control of fermionic states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a system of N independent Schr\u00f6dinger systems, where the N-system Lagrangian is not invariant under a symmetry transformation that mixes the states of the systems. What is the condition under which the generalized continuity equation (GCE) for this system will involve source terms that vanish, leading to a conserved current?\n\nA) The symmetry transformation must be a global symmetry of the N-system Lagrangian.\nB) The symmetry transformation must be an SU(N)-transformation that preserves the N-system Lagrangian.\nC) The source terms in the GCE must vanish globally in a finite domain.\nD) The source terms in the GCE must vanish locally in a finite domain, and the N-system Lagrangian must be invariant under the symmetry transformation.\n\nCorrect Answer: D) The source terms in the GCE must vanish locally in a finite domain, and the N-system Lagrangian must be invariant under the symmetry transformation.\n\nExplanation: The correct answer is D) because the problem statement explicitly mentions that the GCE will involve source terms that vanish and lead to conserved currents under certain conditions. These conditions are that the source terms vanish locally in a finite domain, and the N-system Lagrangian is invariant under the symmetry transformation. This is a key result in the original paper, and it is essential to understand the relationship between the symmetry transformation, the N-system Lagrangian, and the source terms in the GCE. The other options are incorrect because they either misstate the condition for the source terms to vanish (A and B) or do not accurately describe the relationship between the symmetry transformation and the N-system Lagrangian (C)."}, "44": {"documentation": {"title": "Beta spectrum of unique first-forbidden decays as a novel test for\n  fundamental symmetries", "source": "Ayala Glick-Magid (HUJI), Yonatan Mishnayot (HUJI, WIS, SNRC), Ish\n  Mukul (WIS), Michael Hass (WIS), Guy Ron (HUJI), Sergey Vaintraub (SNRC),\n  Doron Gazit (HUJI)", "docs_id": "1609.03268", "section": ["nucl-ex", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beta spectrum of unique first-forbidden decays as a novel test for\n  fundamental symmetries. Within the Standard Model, the weak interaction of quarks and leptons is characterized by certain symmetry properties, such as maximal breaking of parity and favored helicity. These are related to the $V-A$ structure of the weak interaction. These characteristics were discovered by studying correlations in the directions of the outgoing leptons in nuclear beta decays. These days, correlation measurements in nuclear beta decays are intensively studied to probe for signatures for deviations from these symmetries, which are an indication of Beyond Standard Model physics. We show that the structure of the energy spectrum of emitted electrons in unique first-forbidden $\\beta$-decays is sensitive to the symmetries of the weak interaction, and thus can be used as a novel probe of physics beyond the standard model. Furthermore, the energy spectrum gives constraints both in the case of right and left coupling of the new symmetry currents. We show that a measurement with modest energy resolution of about 20 keV is expected to lead to new constraints on beyond the standard model interactions with tensor symmetry."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the energy spectrum of emitted electrons in unique first-forbidden \u03b2-decays as a novel probe of physics beyond the Standard Model?\n\nA) It allows for the direct measurement of new symmetry currents\nB) It provides a more sensitive probe of deviations from maximal breaking of parity and favored helicity\nC) It enables the study of correlations in the directions of the outgoing leptons in nuclear \u03b2-decays\nD) It offers a more efficient way to constrain beyond the Standard Model interactions with tensor symmetry\n\nCorrect Answer: B) It provides a more sensitive probe of deviations from maximal breaking of parity and favored helicity\n\nExplanation: The correct answer is B) because the energy spectrum of emitted electrons in unique first-forbidden \u03b2-decays is sensitive to the symmetries of the weak interaction, which are related to maximal breaking of parity and favored helicity. This makes it a more sensitive probe of deviations from these symmetries, which are indicative of physics beyond the Standard Model. The other options are incorrect because they either describe a different aspect of the weak interaction (A and C) or a different application of the energy spectrum (D)."}, "45": {"documentation": {"title": "Strategic Payments in Financial Networks", "source": "Nils Bertschinger and Martin Hoefer and Daniel Schmand", "docs_id": "1908.01714", "section": ["cs.GT", "q-fin.GN", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strategic Payments in Financial Networks. In their seminal work on systemic risk in financial markets, Eisenberg and Noe proposed and studied a model with $n$ firms embedded into a network of debt relations. We analyze this model from a game-theoretic point of view. Every firm is a rational agent in a directed graph that has an incentive to allocate payments in order to clear as much of its debt as possible. Each edge is weighted and describes a liability between the firms. We consider several variants of the game that differ in the permissible payment strategies. We study the existence and computational complexity of pure Nash and strong equilibria, and we provide bounds on the (strong) prices of anarchy and stability for a natural notion of social welfare. Our results highlight the power of financial regulation -- if payments of insolvent firms can be centrally assigned, a socially optimal strong equilibrium can be found in polynomial time. In contrast, worst-case strong equilibria can be a factor of $\\Omega(n)$ away from optimal, and, in general, computing a best response is an NP-hard problem. For less permissible sets of strategies, we show that pure equilibria might not exist, and deciding their existence as well as computing them if they exist constitute NP-hard problems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Strategic Payments in Financial Networks model, what is the relationship between the existence of a strong equilibrium and the permissible payment strategies?\n\nA) If the permissible payment strategies are limited to direct payments between firms, then strong equilibria always exist.\nB) If the permissible payment strategies include central assignment of payments to insolvent firms, then strong equilibria can be found in polynomial time.\nC) If the permissible payment strategies are restricted to only debt repayment, then strong equilibria are guaranteed to exist.\nD) If the permissible payment strategies allow for arbitrary payments between firms, then strong equilibria are guaranteed to exist.\n\n**Correct Answer:** B) If the permissible payment strategies include central assignment of payments to insolvent firms, then strong equilibria can be found in polynomial time.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"if payments of insolvent firms can be centrally assigned, a socially optimal strong equilibrium can be found in polynomial time.\" This implies that the model allows for a specific type of payment strategy that enables the computation of a strong equilibrium in polynomial time. The other options are incorrect because they do not accurately reflect the relationship between permissible payment strategies and the existence of strong equilibria."}, "46": {"documentation": {"title": "Bayesian Inference of the Multi-Period Optimal Portfolio for an\n  Exponential Utility", "source": "David Bauder, Taras Bodnar, Nestor Parolya and Wolfgang Schmid", "docs_id": "1705.06533", "section": ["math.ST", "q-fin.PM", "q-fin.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Inference of the Multi-Period Optimal Portfolio for an\n  Exponential Utility. We consider the estimation of the multi-period optimal portfolio obtained by maximizing an exponential utility. Employing Jeffreys' non-informative prior and the conjugate informative prior, we derive stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation. This provides a direct access not only to the posterior distribution of the portfolio weights but also to their point estimates together with uncertainties and their asymptotic distributions. Furthermore, we present the posterior predictive distribution for the investor's wealth at each time point of the investment period in terms of a stochastic representation for the future wealth realization. This in turn makes it possible to use quantile-based risk measures or to calculate the probability of default. We apply the suggested Bayesian approach to assess the uncertainty in the multi-period optimal portfolio by considering assets from the FTSE 100 in the weeks after the British referendum to leave the European Union. The behaviour of the novel portfolio estimation method in a precarious market situation is illustrated by calculating the predictive wealth, the risk associated with the holding portfolio, and the default probability in each period."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Bayesian inference of the multi-period optimal portfolio for an exponential utility, what is the primary advantage of employing the conjugate informative prior, as opposed to Jeffreys' non-informative prior?\n\nA) The conjugate informative prior provides a more accurate estimate of the portfolio weights, but at the cost of increased model complexity.\nB) The conjugate informative prior allows for a more efficient estimation of the portfolio weights, but may lead to overfitting if not properly regularized.\nC) The conjugate informative prior provides a more interpretable representation of the posterior distribution of the portfolio weights, but may require additional computational resources.\nD) The conjugate informative prior enables the derivation of stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation, providing a direct access to the posterior distribution, point estimates, uncertainties, and asymptotic distributions.\n\n**Correct Answer:** D) The conjugate informative prior enables the derivation of stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation, providing a direct access to the posterior distribution, point estimates, uncertainties, and asymptotic distributions.\n\n**Explanation:** The conjugate informative prior is a type of prior distribution that is conjugate to the likelihood function, making it easier to derive the posterior distribution. In the context of Bayesian inference, the conjugate informative prior allows for the derivation of stochastic representations for the optimal portfolio weights at each time point of portfolio reallocation, providing a direct access to the posterior distribution, point estimates, uncertainties, and asymptotic distributions. This is a key advantage of using the conjugate informative prior, as it enables the estimation of the portfolio weights with a high degree of accuracy and precision."}, "47": {"documentation": {"title": "The Brylinski filtration for affine Kac-Moody algebras and\n  representations of $\\mathcal{W}$-algebras", "source": "Suresh Govindarajan and Sachin S. Sharma and Sankaran Viswanath", "docs_id": "1912.13353", "section": ["math.RT", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Brylinski filtration for affine Kac-Moody algebras and\n  representations of $\\mathcal{W}$-algebras. We study the Brylinski filtration induced by a principal Heisenberg subalgebra of an affine Kac-Moody algebra $\\mathfrak{g}$, a notion first introduced by Slofstra. The associated graded space of this filtration on dominant weight spaces of integrable highest weight modules of $\\mathfrak{g}$ has Hilbert series coinciding with Lusztig's $t$-analogue of weight multiplicities. For the level 1 vacuum module $L(\\Lambda_0)$ of affine Kac-Moody algebras of type $A$, we show that the Brylinski filtration may be most naturally understood in terms of (vertex algebra) representations of the corresponding $\\mathcal{W}$-algebra. We show that the dominant weight spaces together form an irreducible Verma module of $\\mathcal{W}$ and that the natural PBW basis of this module is compatible with the Brylinski filtration, thereby determining explicitly the subspaces of the filtration. Our basis is the analogue for the principal vertex operator realization of $L(\\Lambda_0)$, of Feigin-Frenkel's basis of $\\mathcal{W}$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Brylinski filtration on the dominant weight spaces of integrable highest weight modules of an affine Kac-Moody algebra $\\mathfrak{g}$. Suppose we are working over the level 1 vacuum module $L(\\Lambda_0)$ of affine Kac-Moody algebras of type $A$. Which of the following statements is true?\n\nA) The dominant weight spaces form an irreducible Verma module of the corresponding $\\mathcal{W}$-algebra.\nB) The natural PBW basis of the dominant weight spaces is compatible with the Brylinski filtration.\nC) The Brylinski filtration on the dominant weight spaces has Hilbert series coinciding with Lusztig's $t$-analogue of weight multiplicities.\nD) The Brylinski filtration is induced by a principal Heisenberg subalgebra of $\\mathfrak{g}$.\n\nCorrect Answer: B) The natural PBW basis of the dominant weight spaces is compatible with the Brylinski filtration.\n\nExplanation: The correct answer is B) because the natural PBW basis of the dominant weight spaces is compatible with the Brylinski filtration, as stated in the documentation. This means that the PBW basis is a good choice for studying the filtration, and it is compatible with the Brylinski filtration, which is a key property of the filtration.\n\nThe other options are incorrect because:\n\nA) The dominant weight spaces do not necessarily form an irreducible Verma module of the corresponding $\\mathcal{W}$-algebra. While they do form a Verma module, it may not be irreducible.\n\nC) The Hilbert series of the Brylinski filtration on the dominant weight spaces does not necessarily coincide with Lusztig's $t$-analogue of weight multiplicities. While the Hilbert series is related to weight multiplicities, it is not necessarily equal to Lusztig's $t$-analogue.\n\nD) The Brylinski filtration is not necessarily induced by a principal Heisenberg subalgebra of $\\mathfrak{g}$. While the filtration is induced by a subalgebra, it is not necessarily a principal Heisenberg subalgebra."}, "48": {"documentation": {"title": "Exact equivalence between one-dimensional Bose gases interacting via\n  hard-sphere and zero-range potentials", "source": "Manuel Valiente", "docs_id": "1108.3723", "section": ["cond-mat.quant-gas", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact equivalence between one-dimensional Bose gases interacting via\n  hard-sphere and zero-range potentials. We prove the equivalence between the hard-sphere Bose gas and a system with momentum-dependent zero-range interactions in one spatial dimension, which we call extended hard-sphere Bose gas. The two-body interaction in the latter model has the advantage of being a regular pseudopotential. The most immediate consequence is the existence of its Fourier transform, permitting the formulation of the problem in momentum space, not possible with the original hard-core interaction. In addition, in the extended system, interactions are defined in terms of the scattering length, positive or negative, identified with the hard-sphere diameter only when it is positive. We are then able to obtain, directly in the thermodynamic limit, the ground state energy of the strongly repulsive Lieb-Liniger gas and, more importantly, the energy of the lowest-lying super Tonks-Girardeau gas state with finite, strongly attractive interactions, in perturbation theory from the novel extended hard-sphere Bose gas. Tan relations involving the large-momentum behavior of the Lieb-Liniger gas are also derived, and then applied to the super Tonks-Girardeau gas within our perturbative approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the extended hard-sphere Bose gas, what is the relationship between the scattering length and the hard-sphere diameter, and how does this relationship impact the formulation of the problem in momentum space?\n\n**A)** The scattering length is equal to the hard-sphere diameter only when the hard-sphere diameter is positive, and this relationship is not relevant in the thermodynamic limit.\n\n**B)** The scattering length is equal to the hard-sphere diameter only when the hard-sphere diameter is negative, and this relationship is crucial for deriving the energy of the lowest-lying super Tonks-Girardeau gas state.\n\n**C)** The scattering length is equal to the hard-sphere diameter when the hard-sphere diameter is positive, and this relationship allows for the direct derivation of the ground state energy of the strongly repulsive Lieb-Liniger gas in the thermodynamic limit.\n\n**D)** The scattering length is equal to the hard-sphere diameter when the hard-sphere diameter is negative, and this relationship enables the formulation of the problem in momentum space, permitting the derivation of Tan relations involving the large-momentum behavior of the Lieb-Liniger gas.\n\n**Correct Answer:** D) The scattering length is equal to the hard-sphere diameter when the hard-sphere diameter is negative, and this relationship enables the formulation of the problem in momentum space, permitting the derivation of Tan relations involving the large-momentum behavior of the Lieb-Liniger gas.\n\n**Explanation:** The correct answer is D) because the documentation states that the two-body interaction in the extended hard-sphere Bose gas has the advantage of being a regular pseudopotential, which allows for the existence of its Fourier transform. This enables the formulation of the problem in momentum space, and the relationship between the scattering length and the hard-sphere diameter is crucial for this formulation. Specifically, the documentation states that interactions are defined in terms of the scattering length, positive or negative, identified with the hard-sphere diameter only when it is positive. However, the correct answer D) states that the relationship is the opposite, i.e., the scattering length is equal to the hard-sphere diameter when the hard-sphere diameter is negative. This is a subtle but important distinction, and the correct answer requires careful reading and understanding of the documentation."}, "49": {"documentation": {"title": "Solitary waves and their linear stability in nonlinear lattices", "source": "Guenbo Hwang, T.R. Akylas and Jianke Yang", "docs_id": "1107.0331", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solitary waves and their linear stability in nonlinear lattices. Solitary waves in a general nonlinear lattice are discussed, employing as a model the nonlinear Schr\\\"odinger equation with a spatially periodic nonlinear coefficient. An asymptotic theory is developed for long solitary waves, that span a large number of lattice periods. In this limit, the allowed positions of solitary waves relative to the lattice, as well as their linear stability properties, hinge upon a certain recurrence relation which contains information beyond all orders of the usual two-scale perturbation expansion. It follows that only two such positions are permissible, and of those two solitary waves, one is linearly stable and the other unstable. For a cosine lattice, in particular, the two possible solitary waves are centered at a maximum or minimum of the lattice, with the former being stable, and the analytical predictions for the associated linear stability eigenvalues are in excellent agreement with numerical results. Furthermore, a countable set of multi-solitary-wave bound states are constructed analytically. In spite of rather different physical settings, the exponential asymptotics approach followed here is strikingly similar to that taken in earlier studies of solitary wavepackets involving a periodic carrier and a slowly-varying envelope, which underscores the general value of this procedure for treating multi-scale solitary-wave problems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a nonlinear lattice with a spatially periodic nonlinear coefficient, and assume that the nonlinear Schr\u00f6dinger equation can be approximated by a two-scale perturbation expansion. What is the recurrence relation that determines the allowed positions of solitary waves relative to the lattice, and how does it relate to the linear stability properties of these waves?\n\nA) The recurrence relation is given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, where u(x,y) is the wave function, and the allowed positions are determined by the solutions to this equation.\n\nB) The recurrence relation is given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, where u(x,y) is the wave function, and the allowed positions are determined by the solutions to this equation, which in turn depend on the parameter y.\n\nC) The recurrence relation is given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, where u(x,y) is the wave function, and the allowed positions are determined by the solutions to this equation, which in turn depend on the parameter x.\n\nD) The recurrence relation is given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, where u(x,y) is the wave function, and the allowed positions are determined by the solutions to this equation, which in turn depend on the parameter x and y, and are given by the equation y = \u00b1\u221a(x^2 + 1).\n\nCorrect Answer: D) The recurrence relation is given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, where u(x,y) is the wave function, and the allowed positions are determined by the solutions to this equation, which in turn depend on the parameter x and y, and are given by the equation y = \u00b1\u221a(x^2 + 1).\n\nExplanation: The correct answer is D) because the recurrence relation is indeed given by \u2202^2u/\u2202x^2 + \u2202^2u/\u2202y^2 + u = 0, and the allowed positions of solitary waves are determined by the solutions to this equation, which in turn depend on the parameter x and y, and are given by the equation y = \u00b1\u221a(x^2 + 1). This equation is derived from the nonlinear Schr\u00f6dinger equation with a spatially periodic nonlinear coefficient, and it is used to determine the linear stability properties of solitary waves. The other options are incorrect because they do not accurately describe the recurrence relation or the allowed positions of solitary waves."}, "50": {"documentation": {"title": "Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer", "source": "Krishna D N", "docs_id": "2109.03969", "section": ["cs.CL", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer. Transformers have recently become very popular for sequence-to-sequence applications such as machine translation and speech recognition. In this work, we propose a multi-task learning-based transformer model for low-resource multilingual speech recognition for Indian languages. Our proposed model consists of a conformer [1] encoder and two parallel transformer decoders. We use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme recognition task as an auxiliary task for our multi-task learning framework. We jointly optimize the network for both phoneme and grapheme recognition tasks using Joint CTC-Attention [2] training. We use a conditional decoding scheme to inject the language information into the model before predicting the grapheme sequence. Our experiments show that our proposed approach can obtain significant improvement over previous approaches [4]. We also show that our conformer-based dual-decoder approach outperforms both the transformer-based dual-decoder approach and single decoder approach. Finally, We compare monolingual ASR models with our proposed multilingual ASR approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a conformer encoder in the proposed multi-task learning-based transformer model for low-resource multilingual speech recognition for Indian languages?\n\n**A)** The conformer encoder allows for parallel processing of phoneme and grapheme recognition tasks.\n**B)** The conformer encoder enables the model to jointly optimize for both phoneme and grapheme recognition tasks using Joint CTC-Attention training.\n**C)** The conformer encoder improves the model's ability to inject language information into the model before predicting grapheme sequences.\n**D)** The conformer encoder reduces the computational complexity of the model compared to single-decoder approaches.\n\n**Correct Answer:** B) The conformer encoder enables the model to jointly optimize for both phoneme and grapheme recognition tasks using Joint CTC-Attention training.\n\n**Explanation:** The correct answer is B) because the documentation states that the conformer encoder is used in conjunction with Joint CTC-Attention training, which allows the model to jointly optimize for both phoneme and grapheme recognition tasks. This is a key advantage of the proposed approach, as it enables the model to learn from both tasks simultaneously and improve overall performance. The other options are incorrect because they do not accurately reflect the primary advantage of using a conformer encoder in the proposed model."}, "51": {"documentation": {"title": "Physical Layer Authentication for Non-coherent Massive SIMO-Based\n  Industrial IoT Communications", "source": "Zhifang Gu, He Chen, Pingping Xu, Yonghui Li and Branka Vucetic", "docs_id": "2001.07315", "section": ["eess.SP", "cs.CR", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Physical Layer Authentication for Non-coherent Massive SIMO-Based\n  Industrial IoT Communications. Achieving ultra-reliable, low-latency and secure communications is essential for realizing the industrial Internet of Things (IIoT). Non-coherent massive multiple-input multiple-output (MIMO) has recently been proposed as a promising methodology to fulfill ultra-reliable and low-latency requirements. In addition, physical layer authentication (PLA) technology is particularly suitable for IIoT communications thanks to its low-latency attribute. A PLA method for non-coherent massive single-input multiple-output (SIMO) IIoT communication systems is proposed in this paper. Specifically, we first determine the optimal embedding of the authentication information (tag) in the message information. We then optimize the power allocation between message and tag signal to characterize the trade-off between message and tag error performance. Numerical results show that the proposed PLA is more accurate then traditional methods adopting the uniform tag when the communication reliability remains at the same level. The proposed PLA method can be effectively applied to the non-coherent system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Physical Layer Authentication for Non-coherent Massive SIMO-Based Industrial IoT Communications, what is the primary advantage of the proposed PLA method over traditional methods adopting a uniform tag?\n\n**A)** The proposed PLA method requires more complex signal processing algorithms.\n**B)** The proposed PLA method is more susceptible to interference and noise.\n**C)** The proposed PLA method achieves more accurate authentication information embedding and power allocation optimization, leading to improved communication reliability.\n**D)** The proposed PLA method is limited to non-coherent massive SIMO systems and cannot be applied to other types of systems.\n\n**Correct Answer:** C) The proposed PLA method achieves more accurate authentication information embedding and power allocation optimization, leading to improved communication reliability.\n\n**Explanation:** The correct answer is C) because the proposed PLA method optimizes the embedding of authentication information and power allocation between message and tag signals, leading to improved communication reliability. This is a key advantage of the proposed method, as it allows for more accurate authentication and improved performance in non-coherent massive SIMO systems. The other options are incorrect because they do not accurately reflect the advantages of the proposed PLA method. Option A is incorrect because the proposed method does not require more complex signal processing algorithms. Option B is incorrect because the proposed method is designed to improve communication reliability, not make it more susceptible to interference and noise. Option D is incorrect because the proposed method is specifically designed for non-coherent massive SIMO systems and can be applied to other types of systems as well."}, "52": {"documentation": {"title": "Improved Pena-Rodriguez Portmanteau Test", "source": "Jen-Wen Lin and A. Ian McLeod", "docs_id": "1611.01351", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved Pena-Rodriguez Portmanteau Test. Several problems with the diagnostic check suggested by Pena and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601-610.] are noted and an improved Monte-Carlo version of this test is suggested. It is shown that quite often the test statistic recommended by Pena and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601-610.] may not exist and their asymptotic distribution of the test does not agree with the suggested gamma approximation very well if the number of lags used by the test is small. It is shown that the convergence of this test statistic to its asymptotic distribution may be quite slow when the series length is less than 1000 and so a Monte-Carlo test is recommended. Simulation experiments suggest the Monte-Carlo test is usually more powerful than the test given by Pena and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601-610.] and often much more powerful than the Ljung-Box portmanteau test. Two illustrative examples of enhanced diagnostic checking with the Monte-Carlo test are given."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a time series dataset with a length of 500 observations. The diagnostic check suggested by Pena and Rodriguez [2002] recommends using a portmanteau test with 10 lags. However, the documentation notes that the convergence of this test statistic to its asymptotic distribution may be slow when the series length is less than 1000. Which of the following statements best describes the implications of this finding for the choice of lag length in the portmanteau test?\n\nA) The recommended lag length of 10 is sufficient for all series lengths, regardless of the true underlying process.\nB) The Monte-Carlo test is always more powerful than the test suggested by Pena and Rodriguez [2002] for series lengths less than 1000.\nC) The slow convergence of the test statistic to its asymptotic distribution for series lengths less than 1000 suggests that the test may not be reliable for small sample sizes.\nD) The Ljung-Box portmanteau test is always more powerful than the test suggested by Pena and Rodriguez [2002] for series lengths less than 1000.\n\n**Correct Answer:** C) The slow convergence of the test statistic to its asymptotic distribution for series lengths less than 1000 suggests that the test may not be reliable for small sample sizes.\n\n**Explanation:** The documentation notes that the convergence of the test statistic to its asymptotic distribution may be slow when the series length is less than 1000, which suggests that the test may not be reliable for small sample sizes. This is because the test statistic may not converge to its asymptotic distribution quickly enough to provide accurate results. Therefore, the Monte-Carlo test, which is recommended for series lengths less than 1000, is likely to be more reliable."}, "53": {"documentation": {"title": "Optimal parking provision in multi-modal morning commute problem\n  considering ride-sourcing service", "source": "Qida Su", "docs_id": "2104.01761", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal parking provision in multi-modal morning commute problem\n  considering ride-sourcing service. Managing morning commute traffic through parking provision management has been well studied in the literature. However, most previous studies made the assumption that all road users require parking spaces at CBD area. However, in recent years, due to technological advancements and low market entry barrier, more and more e-dispatch FHVs (eFHVs) are provided in service. The rapidly growing eFHVs, on one hand, supply substantial trip services and complete the trips requiring no parking demand; on the other hand, imposes congestion effects to all road users. In this study, we investigate the multi-modal morning commute problem with bottleneck congestion and parking space constraints in the presence of ride-sourcing and transit service. Meanwhile, we derive the optimal number of parking spaces to best manage the commute traffic. One interesting finding is that, in the presence of ride-sourcing, excessive supply of parking spaces could incur higher system commute costs in the multi-modal case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the multi-modal morning commute problem with ride-sourcing and transit service, what is the optimal number of parking spaces to manage commute traffic, and how does it differ from the assumption that all road users require parking spaces at the CBD area?\n\n**A)** The optimal number of parking spaces is equal to the number of ride-sourcing vehicles, as they supply substantial trip services and complete trips requiring no parking demand.\n\n**B)** The optimal number of parking spaces is inversely proportional to the number of ride-sourcing vehicles, as excessive supply of parking spaces could incur higher system commute costs in the multi-modal case.\n\n**C)** The optimal number of parking spaces is equal to the number of transit service vehicles, as they provide a complementary service to ride-sourcing and reduce congestion effects.\n\n**D)** The optimal number of parking spaces is a fixed value, determined by the total number of road users and the capacity of the CBD area, regardless of the presence of ride-sourcing and transit service.\n\n**Correct Answer:** B) The optimal number of parking spaces is inversely proportional to the number of ride-sourcing vehicles, as excessive supply of parking spaces could incur higher system commute costs in the multi-modal case.\n\n**Explanation:** The correct answer is based on the finding of the study that excessive supply of parking spaces could incur higher system commute costs in the multi-modal case. This implies that the optimal number of parking spaces is inversely proportional to the number of ride-sourcing vehicles, as the presence of ride-sourcing vehicles reduces the demand for parking spaces."}, "54": {"documentation": {"title": "General Compound Hawkes Processes for Mid-Price Prediction", "source": "Myles Sjogren (1) and Timothy DeLise (2) ((1) University of Calgary,\n  (2) Universit\\'e de Montr\\'eal)", "docs_id": "2110.07075", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "General Compound Hawkes Processes for Mid-Price Prediction. High frequency financial data is burdened by a level of randomness that is unavoidable and obfuscates the task of modelling. This idea is reflected in the intraday evolution of limit orders book data for many financial assets and suggests several justifications for the use of stochastic models. For instance, the arbitrary distribution of inter arrival times and the subsequent dependence structure between consecutive book events. This has lead to the development of many stochastic models for the dynamics of limit order books. In this paper we look to examine the adaptability of one family of such models, the General Compound Hawkes Process (GCHP) models, to new data and new tasks. We further focus on the prediction problem for the mid-price within a limit order book and the practical applications of these stochastic models, which is the main contribution of this paper. To this end we examine the use of the GCHP for predicting the direction and volatility of futures and stock data and discuss possible extensions of the model to help improve its predictive capabilities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation for using stochastic models, such as the General Compound Hawkes Process (GCHP), to model the dynamics of limit order books in high-frequency financial data?\n\n**A)** To capture the deterministic patterns in financial markets\n**B)** To account for the arbitrary distribution of inter-arrival times and dependence structure between consecutive book events\n**C)** To simplify the modeling process by assuming a fixed distribution of inter-arrival times\n**D)** To reduce the computational complexity of the models\n\n**Correct Answer:** B) To account for the arbitrary distribution of inter-arrival times and dependence structure between consecutive book events\n\n**Explanation:** The question requires the test-taker to understand the underlying motivations for using stochastic models in finance. The correct answer, B, highlights the importance of capturing the random and unpredictable nature of financial data, as reflected in the arbitrary distribution of inter-arrival times and dependence structure between consecutive book events. The other options are incorrect because they do not accurately represent the primary motivation for using stochastic models in this context. Option A is incorrect because deterministic patterns are not the primary focus of stochastic models. Option C is incorrect because stochastic models do not assume a fixed distribution of inter-arrival times. Option D is incorrect because the computational complexity of the models is not the primary motivation for using stochastic models."}, "55": {"documentation": {"title": "Orthogonal Sparse PCA and Covariance Estimation via Procrustes\n  Reformulation", "source": "Konstantinos Benidis, Ying Sun, Prabhu Babu, and Daniel P. Palomar", "docs_id": "1602.03992", "section": ["stat.ML", "cs.LG", "math.OC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orthogonal Sparse PCA and Covariance Estimation via Procrustes\n  Reformulation. The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of estimating sparse eigenvectors of a symmetric matrix, what is the primary challenge faced by existing approaches that formulate the problem by adding a penalty term into the objective function?\n\n**A)** The penalty term does not effectively encourage sparsity in the eigenvectors.\n**B)** The penalty term leads to a loss of orthogonality in the eigenvectors.\n**C)** The penalty term is not robust to noise in the data.\n**D)** The penalty term is computationally expensive to optimize.\n\n**Correct Answer:** B) The penalty term leads to a loss of orthogonality in the eigenvectors.\n\n**Explanation:** The original problem statement highlights that existing approaches that add a penalty term to the objective function sacrifice orthogonality in order to achieve sparsity. This is a key challenge in the field, as orthogonality is an important property in many applications. The proposed method in the paper addresses this challenge by using a surrogate function that is maximized over the Stiefel manifold, which preserves orthogonality.\n\n**Additional Question:** What is the key insight that allows the authors to reformulate the inner maximization problem as a rectangular Procrustes problem?\n\n**A)** The authors use a change of basis to diagonalize the matrix.\n**B)** The authors use the eigenvalue decomposition of the covariance matrix to reduce the dimensionality of the problem.\n**C)** The authors recognize that the inner maximization problem can be viewed as a Procrustes problem, where the goal is to find a best fit between two matrices.\n**D)** The authors use a clever trick to reduce the problem to a simpler form.\n\n**Correct Answer:** C) The authors recognize that the inner maximization problem can be viewed as a Procrustes problem, where the goal is to find a best fit between two matrices.\n\n**Explanation:** The authors note that the inner maximization problem can be viewed as a Procrustes problem, where the goal is to find a best fit between two matrices. This insight allows them to reformulate the problem in a more tractable form, which can be solved using a closed-form solution."}, "56": {"documentation": {"title": "Geometrically Induced Phase Transitions at Large N", "source": "Jonathan J. Heckman and Cumrun Vafa", "docs_id": "0707.4011", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometrically Induced Phase Transitions at Large N. Utilizing the large N dual description of a metastable system of branes and anti-branes wrapping rigid homologous S^2's in a non-compact Calabi-Yau threefold, we study phase transitions induced by changing the positions of the S^2's. At leading order in 1/N the effective potential for this system is computed by the planar limit of an auxiliary matrix model. Beginning at the two loop correction, the degenerate vacuum energy density of the discrete confining vacua split, and a potential is generated for the axion. Changing the relative positions of the S^2's causes discrete jumps in the energetically preferred confining vacuum and can also obstruct direct brane/anti-brane annihilation processes. The branes must hop to nearby S^2's before annihilating, thus significantly increasing the lifetime of the corresponding non-supersymmetric vacua. We also speculate that misaligned metastable glueball phases may generate a repulsive inter-brane force which stabilizes the radial mode present in compact Calabi-Yau threefolds."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Geometrically Induced Phase Transitions at Large N, what is the primary effect of changing the relative positions of the S^2's on the energetically preferred confining vacuum, and how does this impact the lifetime of the corresponding non-supersymmetric vacua?\n\nA) The change in S^2 positions causes a direct annihilation of branes and anti-branes, leading to a decrease in the lifetime of the vacua.\nB) The change in S^2 positions generates a potential for the axion, but does not affect the lifetime of the vacua.\nC) The change in S^2 positions causes discrete jumps in the energetically preferred confining vacuum, and the branes must hop to nearby S^2's before annihilating, significantly increasing the lifetime of the corresponding vacua.\nD) The change in S^2 positions has no effect on the lifetime of the vacua, as the branes and anti-branes are already in a metastable state.\n\nCorrect Answer: C) The change in S^2 positions causes discrete jumps in the energetically preferred confining vacuum, and the branes must hop to nearby S^2's before annihilating, significantly increasing the lifetime of the corresponding vacua.\n\nExplanation: The correct answer is based on the statement in the documentation that \"changing the relative positions of the S^2's causes discrete jumps in the energetically preferred confining vacuum and can also obstruct direct brane/anti-brane annihilation processes.\" This implies that the change in S^2 positions leads to a significant increase in the lifetime of the corresponding non-supersymmetric vacua, as the branes must hop to nearby S^2's before annihilating."}, "57": {"documentation": {"title": "Nonstationary Portfolios: Diversification in the Spectral Domain", "source": "Bruno Scalzo, Alvaro Arroyo, Ljubisa Stankovic, Danilo P. Mandic", "docs_id": "2102.00477", "section": ["q-fin.ST", "eess.SP", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonstationary Portfolios: Diversification in the Spectral Domain. Classical portfolio optimization methods typically determine an optimal capital allocation through the implicit, yet critical, assumption of statistical time-invariance. Such models are inadequate for real-world markets as they employ standard time-averaging based estimators which suffer significant information loss if the market observables are non-stationary. To this end, we reformulate the portfolio optimization problem in the spectral domain to cater for the nonstationarity inherent to asset price movements and, in this way, allow for optimal capital allocations to be time-varying. Unlike existing spectral portfolio techniques, the proposed framework employs augmented complex statistics in order to exploit the interactions between the real and imaginary parts of the complex spectral variables, which in turn allows for the modelling of both harmonics and cyclostationarity in the time domain. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations using real-world price data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Reformulate the portfolio optimization problem in the spectral domain to cater for the nonstationarity inherent to asset price movements. How does the proposed framework employing augmented complex statistics differ from existing spectral portfolio techniques?\n\n**A)** The proposed framework uses standard time-averaging based estimators, which are inadequate for real-world markets due to significant information loss.\n\n**B)** The proposed framework allows for the modelling of both harmonics and cyclostationarity in the time domain by exploiting the interactions between the real and imaginary parts of the complex spectral variables.\n\n**C)** The proposed framework employs time-invariant statistical models, which are sufficient for real-world markets as they do not suffer from information loss.\n\n**D)** The proposed framework uses only real-valued spectral variables, which are insufficient for modelling nonstationarity in asset price movements.\n\n**Correct Answer:** B) The proposed framework allows for the modelling of both harmonics and cyclostationarity in the time domain by exploiting the interactions between the real and imaginary parts of the complex spectral variables.\n\n**Explanation:** The correct answer is B) because the proposed framework employs augmented complex statistics, which allows for the exploitation of interactions between the real and imaginary parts of the complex spectral variables. This enables the modelling of both harmonics and cyclostationarity in the time domain, which is a key advantage over traditional methods. The other options are incorrect because they either misrepresent the proposed framework (A and C) or are incomplete (D)."}, "58": {"documentation": {"title": "CSAI: Open-Source Cellular Radio Access Network Security Analysis\n  Instrument", "source": "Thomas Byrd and Vuk Marojevic, Roger Piqueras Jover", "docs_id": "1905.07617", "section": ["cs.CR", "cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CSAI: Open-Source Cellular Radio Access Network Security Analysis\n  Instrument. This paper presents our methodology and toolbox that allows analyzing the radio access network security of laboratory and commercial 4G and future 5G cellular networks. We leverage a free open-source software suite that implements the LTE UE and eNB enabling real-time signaling using software radio peripherals. We modify the UE software processing stack to act as an LTE packet collection and examination tool. This is possible because of the openness of the 3GPP specifications. Hence, we are able to receive and decode LTE downlink messages for the purpose of analyzing potential security problems of the standard. This paper shows how to rapidly prototype LTE tools and build a software-defined radio access network (RAN) analysis instrument for research and education. Using CSAI, the Cellular RAN Security Analysis Instrument, a researcher can analyze broadcast and paging messages of cellular networks. CSAI is also able to test networks to aid in the identification of vulnerabilities and verify functionality post-remediation. Additionally, we found that it can crash an eNB which motivates equivalent analyses of commercial network equipment and its robustness against denial of service attacks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the CSAI (Cellular RAN Security Analysis Instrument) tool in analyzing the security of 4G and 5G cellular networks?\n\nA) It allows for the rapid prototyping of LTE tools and the creation of a software-defined radio access network (RAN) analysis instrument.\nB) It enables the testing of networks to identify vulnerabilities and verify functionality post-remediation.\nC) It provides a free open-source software suite that implements the LTE UE and eNB, allowing for real-time signaling using software radio peripherals.\nD) It can crash an eNB, motivating equivalent analyses of commercial network equipment and its robustness against denial of service attacks.\n\nCorrect Answer: A) It allows for the rapid prototyping of LTE tools and the creation of a software-defined radio access network (RAN) analysis instrument.\n\nExplanation: The correct answer is A) because the paper states that CSAI \"allows analyzing the radio access network security of laboratory and commercial 4G and future 5G cellular networks\" and that it enables the creation of a software-defined radio access network (RAN) analysis instrument, which is a key advantage of using the tool. The other options, while related to the tool's capabilities, are not the primary advantage of using CSAI. \n\nNote: The other options are plausible but incorrect, requiring the test-taker to carefully read and understand the documentation to arrive at the correct answer."}, "59": {"documentation": {"title": "Implications of the mass $M=2.17^{+0.11}_{-0.10}$M$_\\odot$ of\n  PSR~J0740+6620 on the Equation of State of Super-Dense Neutron-Rich Nuclear\n  Matter", "source": "Nai-Bo Zhang and Bao-An Li", "docs_id": "1904.10998", "section": ["nucl-th", "astro-ph.HE", "gr-qc", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of the mass $M=2.17^{+0.11}_{-0.10}$M$_\\odot$ of\n  PSR~J0740+6620 on the Equation of State of Super-Dense Neutron-Rich Nuclear\n  Matter. We study implications of the very recently reported mass $M=2.17^{+0.11}_{-0.10}$M$_\\odot$ of PSR~J0740+6620 on the Equation of State (EOS) of super-dense neutron-rich nuclear matter with respect to existing constraints on the EOS based on the mass $M=2.01\\pm 0.04$M$_\\odot$ of PSR~J0348+0432, the maximum tidal deformability of GW170817 and earlier results of various terrestrial nuclear laboratory experiments. The lower limit of the skewness $J_0$ measuring the stiffness of super-dense isospin-symmetric nuclear matter is raised raised from about -220 MeV to -150 MeV, reducing significantly its current uncertainty range. The lower bound of the high-density symmetry energy also increases appreciably leading to a rise of the minimum proton fraction in neutron stars at $\\beta$-equilibrium from about 0 to 5\\% around three times the saturation density of nuclear matter. The difficulties for some of the most widely used and previously well tested model EOSs to predict simultaneously both a maximum mass higher than 2.17 M$_\\odot$ and a pressure consistent with that extracted from GW170817 present some interesting new challenges for nuclear theories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The recent measurement of the mass of PSR~J0740+6620 has implications for our understanding of the Equation of State (EOS) of super-dense neutron-rich nuclear matter. Which of the following statements accurately describes the effect of this measurement on the lower limit of the skewness $J_0$ of super-dense isospin-symmetric nuclear matter?\n\nA) The lower limit of $J_0$ decreases from about -220 MeV to -180 MeV.\nB) The lower limit of $J_0$ remains unchanged at about -220 MeV.\nC) The lower limit of $J_0$ increases from about -220 MeV to -150 MeV.\nD) The lower limit of $J_0$ becomes undefined due to the uncertainty in the mass measurement.\n\n**Correct Answer:** C) The lower limit of $J_0$ increases from about -220 MeV to -150 MeV.\n\n**Explanation:** According to the documentation, the lower limit of the skewness $J_0$ measuring the stiffness of super-dense isospin-symmetric nuclear matter is raised from about -220 MeV to -150 MeV, indicating an increase in the lower limit of $J_0$. This is a consequence of the recent measurement of the mass of PSR~J0740+6620, which has implications for our understanding of the EOS of super-dense neutron-rich nuclear matter."}}