{"0": {"documentation": {"title": "A Social Network Analysis of Occupational Segregation", "source": "I. Sebastian Buhai and Marco J. van der Leij", "docs_id": "2004.09293", "section": ["econ.TH", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Social Network Analysis of Occupational Segregation. We propose an equilibrium interaction model of occupational segregation and labor market inequality between two social groups, generated exclusively through the documented tendency to refer informal job seekers of identical \"social color\". The expected social color homophily in job referrals strategically induces distinct career choices for individuals from different social groups, which further translates into stable partial occupational segregation equilibria with sustained wage and employment inequality -- in line with observed patterns of racial or gender labor market disparities. Supporting the qualitative analysis with a calibration and simulation exercise, we furthermore show that both first and second best utilitarian social optima entail segregation, any integration policy requiring explicit distributional concerns. Our framework highlights that the mere social interaction through homophilous contact networks can be a pivotal channel for the propagation and persistence of gender and racial labor market gaps, complementary to long studied mechanisms such as taste or statistical discrimination."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: According to the proposed equilibrium interaction model of occupational segregation, which of the following statements is NOT a direct consequence of the tendency to refer informal job seekers of identical \"social color\"?\n\nA) The emergence of stable partial occupational segregation equilibria\nB) Sustained wage and employment inequality between social groups\nC) Distinct career choices for individuals from different social groups\nD) The necessity for taste-based discrimination in hiring practices\n\nCorrect Answer: D\n\nExplanation:\nA, B, and C are direct consequences of the model described in the documentation. The tendency to refer job seekers of the same \"social color\" leads to distinct career choices (C), which results in stable partial occupational segregation (A) and sustained wage and employment inequality (B). However, the necessity for taste-based discrimination (D) is not mentioned as a direct consequence of this referral behavior. In fact, the model shows that segregation and inequality can persist even without explicit discrimination, making D the correct answer as it is NOT a direct consequence of the described referral tendency."}, "1": {"documentation": {"title": "Microlens Parallax Asymmetries Toward the LMC", "source": "Andrew Gould", "docs_id": "astro-ph/9802132", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microlens Parallax Asymmetries Toward the LMC. If the microlensing events now being detected toward the Large Magellanic Cloud (LMC) are due to lenses in the Milky Way halo, then the events should typically have asymmetries of order 1% due to parallax from the reflex motion of the Earth. By contrast, if the lenses are in the LMC, the parallax effects should be negligible. A ground-based search for such parallax asymmetries would therefore clarify the location of the lenses. A modest effort (2 hours per night on a 1 m telescope) could measure 15 parallax asymmetries over 5 years and so marginally discriminate between the halo and the LMC as the source of the lenses. A dedicated 1 m telescope would approximately double the number of measurements and would therefore clearly distinguish between the alternatives. However, compared to satellite parallaxes, the information extracted from ground-based parallaxes is substantially less useful for understanding the nature of the halo lenses (if that is what they are). The backgrounds of asymmetries due to binary-source and binary-lens events are estimated to be approximately 7% and 12% respectively. These complicate the interpretation of detected parallax asymmetries, but not critically."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A study is conducted to determine the location of lenses causing microlensing events toward the Large Magellanic Cloud (LMC). Which of the following statements best describes the expected parallax asymmetries and their implications?\n\nA) Parallax asymmetries of about 10% would strongly indicate that the lenses are in the Milky Way halo, while asymmetries of about 5% would suggest the lenses are in the LMC.\n\nB) Negligible parallax asymmetries would suggest the lenses are in the LMC, while asymmetries of about 1% would indicate the lenses are likely in the Milky Way halo.\n\nC) Parallax asymmetries of about 1% would strongly suggest the lenses are in the LMC, while negligible asymmetries would indicate the lenses are in the Milky Way halo.\n\nD) The presence or absence of parallax asymmetries is not a reliable indicator of lens location due to the high background of asymmetries caused by binary-source and binary-lens events.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that if the microlensing events are due to lenses in the Milky Way halo, they should typically have asymmetries of about 1% due to parallax from the Earth's reflex motion. In contrast, if the lenses are in the LMC, the parallax effects should be negligible. This directly corresponds to option B.\n\nOption A is incorrect because it suggests much larger asymmetries than mentioned in the document and reverses the implications of the asymmetries.\n\nOption C is incorrect because it inverts the relationship between the asymmetries and the lens locations compared to what's stated in the document.\n\nOption D is incorrect because, while the document does mention background asymmetries from binary events, it states that these \"complicate the interpretation of detected parallax asymmetries, but not critically.\" Therefore, parallax asymmetries are still considered a useful indicator of lens location."}, "2": {"documentation": {"title": "Convolutional neural networks for atomistic systems", "source": "Kevin Ryczko, Kyle Mills, Iryna Luchak, Christa Homenick, Isaac\n  Tamblyn", "docs_id": "1706.09496", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convolutional neural networks for atomistic systems. We introduce a new method, called CNNAS (convolutional neural networks for atomistic systems), for calculating the total energy of atomic systems which rivals the computational cost of empirical potentials while maintaining the accuracy of \\emph{ab initio} calculations. This method uses deep convolutional neural networks (CNNs), where the input to these networks are simple representations of the atomic structure. We use this approach to predict energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types. Using a dataset consisting of graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures, with and without defects, we trained a deep CNN that is capable of predicting DFT energies to an extremely high accuracy, with a mean absolute error (MAE) of 0.198 meV / atom (maximum absolute error of 16.1 meV / atom). To explore our new methodology, we investigate the ability of a deep neural network (DNN) in predicting a Lennard-Jones energy and separation distance for a dataset of dimer molecules in both two and three dimensions. In addition, we systematically investigate the flexibility of the deep learning models by performing interpolation and extrapolation tests."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A research team is using CNNAS (Convolutional Neural Networks for Atomistic Systems) to predict energies for various 2D hexagonal lattice structures. They achieve a mean absolute error (MAE) of 0.198 meV / atom when compared to DFT calculations. What can be inferred about the CNNAS method based on this information?\n\nA) CNNAS is less accurate than empirical potentials but faster than DFT calculations\nB) CNNAS achieves DFT-level accuracy with computational efficiency comparable to empirical potentials\nC) CNNAS is more accurate than DFT but requires significantly more computational resources\nD) CNNAS can only be applied to 2D hexagonal lattices and cannot be generalized to other atomic systems\n\nCorrect Answer: B\n\nExplanation: \nThe correct answer is B. The documentation states that CNNAS \"rivals the computational cost of empirical potentials while maintaining the accuracy of ab initio calculations.\" It also mentions that the method predicts DFT energies \"to an extremely high accuracy\" with the given MAE. This indicates that CNNAS achieves DFT-level accuracy (ab initio calculations) while being computationally efficient like empirical potentials.\n\nAnswer A is incorrect because CNNAS maintains the accuracy of ab initio methods (like DFT), not less accurate than empirical potentials.\n\nAnswer C is incorrect because while CNNAS achieves high accuracy, it's not stated to be more accurate than DFT. Moreover, it's described as computationally efficient, not requiring more resources.\n\nAnswer D is too limiting. While the example given uses 2D hexagonal lattices, the method is introduced for \"atomistic systems\" in general, suggesting broader applicability."}, "3": {"documentation": {"title": "PolyScientist: Automatic Loop Transformations Combined with Microkernels\n  for Optimization of Deep Learning Primitives", "source": "Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Gagandeep\n  Goyal, Ramakrishna Upadrasta, Bharat Kaul", "docs_id": "2002.02145", "section": ["cs.PL", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "PolyScientist: Automatic Loop Transformations Combined with Microkernels\n  for Optimization of Deep Learning Primitives. At the heart of deep learning training and inferencing are computationally intensive primitives such as convolutions which form the building blocks of deep neural networks. Researchers have taken two distinct approaches to creating high performance implementations of deep learning kernels, namely, 1) library development exemplified by Intel MKL-DNN for CPUs, 2) automatic compilation represented by the TensorFlow XLA compiler. The two approaches have their drawbacks: even though a custom built library can deliver very good performance, the cost and time of development of the library can be high. Automatic compilation of kernels is attractive but in practice, till date, automatically generated implementations lag expert coded kernels in performance by orders of magnitude. In this paper, we develop a hybrid solution to the development of deep learning kernels that achieves the best of both worlds: the expert coded microkernels are utilized for the innermost loops of kernels and we use the advanced polyhedral technology to automatically tune the outer loops for performance. We design a novel polyhedral model based data reuse algorithm to optimize the outer loops of the kernel. Through experimental evaluation on an important class of deep learning primitives namely convolutions, we demonstrate that the approach we develop attains the same levels of performance as Intel MKL-DNN, a hand coded deep learning library."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following best describes the hybrid approach proposed by PolyScientist for optimizing deep learning primitives?\n\nA) Combining automatic loop transformations with hand-coded microkernels\nB) Utilizing Intel MKL-DNN for CPUs along with TensorFlow XLA compiler\nC) Applying polyhedral technology to optimize only the innermost loops of kernels\nD) Developing a new compiler that automatically generates entire deep learning kernels\n\nCorrect Answer: A\n\nExplanation: \nThe correct answer is A) Combining automatic loop transformations with hand-coded microkernels. This hybrid approach is the core innovation described in the PolyScientist paper. It aims to achieve the best of both worlds by using expert-coded microkernels for the innermost loops and applying advanced polyhedral technology to automatically tune the outer loops for performance.\n\nOption B is incorrect because while it mentions two existing approaches (Intel MKL-DNN and TensorFlow XLA), it doesn't describe the novel hybrid method proposed in the paper.\n\nOption C is incorrect because the polyhedral technology is applied to optimize the outer loops, not the innermost loops. The innermost loops are handled by expert-coded microkernels.\n\nOption D is incorrect because PolyScientist does not develop an entirely new compiler. Instead, it combines existing approaches (hand-coded microkernels and automatic loop transformations) in a novel way.\n\nThis question tests the reader's understanding of the key innovation presented in the paper and their ability to differentiate it from existing approaches in the field of deep learning kernel optimization."}, "4": {"documentation": {"title": "Energy-efficient Rail Guided Vehicle Routing for Two-Sided\n  Loading/Unloading Automated Freight Handling System", "source": "Wuhua Hu, Jianfeng Mao, Keji Wei", "docs_id": "1502.01452", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy-efficient Rail Guided Vehicle Routing for Two-Sided\n  Loading/Unloading Automated Freight Handling System. Rail-guided vehicles (RGVs) are widely employed in automated freight handling system (AFHS) to transport surging air cargo. Energy-efficient routing of such vehicles is of great interest for both financial and environmental sustainability. Given a multi-capacity RGV working on a linear track in AFHS, we consider its optimal routing under two-sided loading/unloading (TSLU) operations, in which energy consumption is minimized under conflict-avoidance and time window constraints. The energy consumption takes account of routing-dependent gross weight and dynamics of the RGV, and the conflict-avoidance constraints ensure conflict-free transport service under TSLU operations. The problem is formulated as a mixed-integer linear program, and solved by incorporating valid inequalities that exploit structural properties of the problem. The static problem model and solution approach are then integrated with a rolling-horizon approach to solve the dynamic routing problem where air cargo enters and departs from the system dynamically in time. Simulation results suggest that the proposed strategy is able to route an RGV to transport air cargo with an energy cost that is considerably lower than one of the most commonly used heuristic methods implemented in current practice."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of energy-efficient Rail Guided Vehicle (RGV) routing for automated freight handling systems, which of the following statements is NOT true?\n\nA) The problem is formulated as a mixed-integer linear program and incorporates valid inequalities to exploit structural properties.\n\nB) The energy consumption model considers only the fixed weight of the RGV, ignoring the varying cargo load during routing.\n\nC) The system employs a rolling-horizon approach to address dynamic routing challenges as air cargo enters and departs over time.\n\nD) Two-sided loading/unloading operations are considered, with conflict-avoidance constraints ensuring conflict-free transport service.\n\nCorrect Answer: B\n\nExplanation: Option B is incorrect and thus the correct answer to this question asking for the statement that is NOT true. The documentation clearly states that the energy consumption takes into account \"routing-dependent gross weight and dynamics of the RGV,\" which means it considers the varying cargo load during routing, not just the fixed weight of the RGV.\n\nOptions A, C, and D are all correct statements based on the information provided:\nA) The problem is indeed formulated as a mixed-integer linear program with valid inequalities.\nC) A rolling-horizon approach is used for dynamic routing as cargo enters and leaves the system over time.\nD) Two-sided loading/unloading operations are considered with conflict-avoidance constraints."}, "5": {"documentation": {"title": "Experimental study of a low-order wavefront sensor for high-contrast\n  coronagraphic imagers: results in air and in vacuum", "source": "Julien Lozi, Ruslan Belikov, Sandrine J. Thomas, Eugene Pluzhnik,\n  Eduardo Bendek, Olivier Guyon, Glenn Schneider", "docs_id": "1407.4160", "section": ["astro-ph.IM", "astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental study of a low-order wavefront sensor for high-contrast\n  coronagraphic imagers: results in air and in vacuum. For the technology development of the mission EXCEDE (EXoplanetary Circumstellar Environments and Disk Explorer) - a 0.7 m telescope equipped with a Phase-Induced Amplitude Apodization Coronagraph (PIAA-C) and a 2000-element MEMS deformable mirror, capable of raw contrasts of 1e-6 at 1.2 lambda/D and 1e-7 above 2 lambda/D - we developed two test benches simulating its key components, one in air, the other in vacuum. To achieve this level of contrast, one of the main goals is to remove low-order aberrations, using a Low-Order WaveFront Sensor (LOWFS). We tested this key component, together with the coronagraph and the wavefront control, in air at NASA Ames Research Center and in vacuum at Lockheed Martin. The LOWFS, controlling tip/tilt modes in real time at 1~kHz, allowed us to reduce the disturbances in air to 1e-3 lambda/D rms, letting us achieve a contrast of 2.8e-7 between 1.2 and 2 lambda/D. Tests are currently being performed to achieve the same or a better level of correction in vacuum. With those results, and by comparing them to simulations, we are able to deduce its performances on different coronagraphs - different sizes of telescopes, inner working angles, contrasts, etc. - and therefore study its contribution beyond EXCEDE."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: The EXCEDE mission aims to achieve high-contrast imaging using several key technologies. Which combination of components and performance metrics accurately describes the EXCEDE system?\n\nA) 0.5 m telescope, PIAA-C coronagraph, 1000-element MEMS deformable mirror, raw contrast of 1e-5 at 1.5 lambda/D\nB) 0.7 m telescope, PIAA-C coronagraph, 2000-element MEMS deformable mirror, raw contrast of 1e-6 at 1.2 lambda/D and 1e-7 above 2 lambda/D\nC) 1.0 m telescope, Lyot coronagraph, 3000-element MEMS deformable mirror, raw contrast of 1e-8 at 2 lambda/D\nD) 0.7 m telescope, Vortex coronagraph, 1500-element MEMS deformable mirror, raw contrast of 1e-7 at 1.5 lambda/D\n\nCorrect Answer: B\n\nExplanation: The correct answer is B, which accurately describes the EXCEDE mission components and performance metrics as stated in the documentation. The EXCEDE mission uses a 0.7 m telescope equipped with a Phase-Induced Amplitude Apodization Coronagraph (PIAA-C) and a 2000-element MEMS deformable mirror. The system is capable of achieving raw contrasts of 1e-6 at 1.2 lambda/D and 1e-7 above 2 lambda/D. The other options contain incorrect combinations of telescope size, coronagraph type, deformable mirror specifications, or contrast performance, making them incorrect choices for this question."}, "6": {"documentation": {"title": "Data Consistent Artifact Reduction for Limited Angle Tomography with\n  Deep Learning Prior", "source": "Yixing Huang, Alexander Preuhs, Guenter Lauritsch, Michael Manhart,\n  Xiaolin Huang, and Andreas Maier", "docs_id": "1908.06792", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data Consistent Artifact Reduction for Limited Angle Tomography with\n  Deep Learning Prior. Robustness of deep learning methods for limited angle tomography is challenged by two major factors: a) due to insufficient training data the network may not generalize well to unseen data; b) deep learning methods are sensitive to noise. Thus, generating reconstructed images directly from a neural network appears inadequate. We propose to constrain the reconstructed images to be consistent with the measured projection data, while the unmeasured information is complemented by learning based methods. For this purpose, a data consistent artifact reduction (DCAR) method is introduced: First, a prior image is generated from an initial limited angle reconstruction via deep learning as a substitute for missing information. Afterwards, a conventional iterative reconstruction algorithm is applied, integrating the data consistency in the measured angular range and the prior information in the missing angular range. This ensures data integrity in the measured area, while inaccuracies incorporated by the deep learning prior lie only in areas where no information is acquired. The proposed DCAR method achieves significant image quality improvement: for 120-degree cone-beam limited angle tomography more than 10% RMSE reduction in noise-free case and more than 24% RMSE reduction in noisy case compared with a state-of-the-art U-Net based method."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following best describes the key innovation of the Data Consistent Artifact Reduction (DCAR) method for limited angle tomography, as presented in the research?\n\nA) It uses a larger training dataset to improve generalization of the neural network.\nB) It applies noise reduction techniques to make deep learning methods less sensitive to noise.\nC) It generates reconstructed images directly from a neural network without additional constraints.\nD) It combines deep learning prior with data consistency constraints in iterative reconstruction.\n\nCorrect Answer: D\n\nExplanation: The DCAR method introduces a novel approach by combining deep learning with data consistency constraints. It first generates a prior image using deep learning to substitute missing information, then applies a conventional iterative reconstruction algorithm that integrates data consistency in the measured angular range with the prior information in the missing angular range. This ensures data integrity in the measured area while using the deep learning prior only where no information is acquired. Options A and B address the challenges mentioned but are not the key innovations of DCAR. Option C is explicitly stated as inadequate in the text. Option D correctly captures the essence of the DCAR method's innovation."}, "7": {"documentation": {"title": "Multi-component Fermionic Dark Matter and IceCube PeV scale Neutrinos in\n  Left-Right Model with Gauge Unification", "source": "Debasish Borah, Arnab Dasgupta, Ujjal Kumar Dey, Sudhanwa Patra,\n  Gaurav Tomar", "docs_id": "1704.04138", "section": ["hep-ph", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-component Fermionic Dark Matter and IceCube PeV scale Neutrinos in\n  Left-Right Model with Gauge Unification. We consider a simple extension of the minimal left-right symmetric model (LRSM) in order to explain the PeV neutrino events seen at the IceCube experiment from a heavy decaying dark matter. The dark matter sector is composed of two fermions: one at PeV scale and the other at TeV scale such that the heavier one can decay into the lighter one and two neutrinos. The gauge annihilation cross sections of PeV dark matter are not large enough to generate its relic abundance within the observed limit. We include a pair of real scalar triplets $\\Omega_{L,R}$ which can bring the thermally overproduced PeV dark matter abundance into the observed range through late time decay and consequent entropy release thereby providing a consistent way to obtain the correct relic abundance without violating the unitarity bound on dark matter mass. Another scalar field, a bitriplet under left-right gauge group is added to assist the heavier dark matter decay. The presence of an approximate global $U(1)_X$ symmetry can naturally explain the origin of tiny couplings required for long-lived nature of these decaying particles. We also show, how such an extended LRSM can be incorporated within a non-supersymmetric $SO(10)$ model where the gauge coupling unification at a very high scale naturally accommodate a PeV scale intermediate symmetry, required to explain the PeV events at IceCube."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: In the described extension of the minimal left-right symmetric model (LRSM), which combination of components and mechanisms correctly explains both the PeV neutrino events at IceCube and the correct dark matter relic abundance?\n\nA) A single PeV-scale fermion dark matter particle that decays directly into neutrinos, with its relic abundance regulated by gauge annihilation processes.\n\nB) Two fermionic dark matter particles (PeV and TeV scale), with the heavier one decaying into the lighter one and two neutrinos, and real scalar triplets \u03a9_{L,R} regulating the PeV dark matter abundance through late-time decay and entropy release.\n\nC) A PeV-scale bosonic dark matter particle decaying into neutrinos, with its abundance controlled by a bitriplet scalar field under the left-right gauge group.\n\nD) Three fermionic dark matter particles of varying scales, with the heaviest decaying into the others and neutrinos, regulated by both scalar triplets and a global U(1)_X symmetry.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately represents the key components and mechanisms described in the given text. The model includes two fermionic dark matter particles, one at PeV scale and one at TeV scale, with the heavier one decaying into the lighter one and two neutrinos, explaining the IceCube PeV neutrino events. The text explicitly mentions the inclusion of real scalar triplets \u03a9_{L,R} to bring the thermally overproduced PeV dark matter abundance into the observed range through late-time decay and entropy release. This combination addresses both the IceCube observations and the dark matter relic abundance issue while respecting the unitarity bound on dark matter mass.\n\nOption A is incorrect because it doesn't account for the two-component dark matter system or the role of scalar triplets in regulating abundance. Option C is wrong as it describes a bosonic rather than fermionic dark matter and misattributes the role of the bitriplet scalar. Option D incorrectly states three fermionic dark matter particles and doesn't accurately represent the roles of the scalar fields and symmetries mentioned in the text."}, "8": {"documentation": {"title": "Characterization of the TRIGA Mark II reactor full-power steady state", "source": "Antonio Cammi, Matteo Zanetti, Davide Chiesa, Massimiliano Clemenza,\n  Stefano Pozzi, Ezio Previtali, Monica Sisti, Giovanni Magrotti, Michele\n  Prata, Andrea Salvini", "docs_id": "1503.00873", "section": ["physics.ins-det", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterization of the TRIGA Mark II reactor full-power steady state. In this work, the characterization of the full-power steady state of the TRIGA Mark II nuclear reactor of the University of Pavia is performed by coupling Monte Carlo (MC) simulation for neutronics with \"Multiphysics\" model for thermal-hydraulics. Neutronic analyses have been performed starting from a MC model of the entire reactor system, based on the MCNP5 code, that was already validated in fresh fuel and zero-power configuration (in which thermal effects are negligible) using the available experimental data as benchmark. In order to describe the full-power reactor configuration, the temperature distribution in the core is necessary. To evaluate it, a thermal-hydraulic model has been developed, using the power distribution results from MC simulation as input. The thermal-hydraulic model is focused on the core active region and takes into account sub-cooled boiling effects present at full reactor power. The obtained temperature distribution is then introduced in the MC model and a benchmark analysis is carried out to validate the model in fresh fuel and full-power configuration. The good agreement between experimental data and simulation results concerning full-power reactor criticality, proves the reliability of the adopted methodology of analysis, both from neutronics and thermal-hydraulics perspective."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the characterization of the TRIGA Mark II reactor full-power steady state, which of the following statements best describes the methodology and its validation?\n\nA) The thermal-hydraulic model was developed first, and its results were used as input for the Monte Carlo simulation of neutronics.\n\nB) The Monte Carlo model was validated only for full-power configuration, as thermal effects are significant in this state.\n\nC) The methodology involved coupling Monte Carlo simulation for neutronics with a Multiphysics model for thermal-hydraulics, and was validated through benchmark analysis of full-power reactor criticality.\n\nD) The thermal-hydraulic model ignored sub-cooled boiling effects to simplify calculations at full reactor power.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The methodology described in the document involves coupling Monte Carlo (MC) simulation for neutronics with a \"Multiphysics\" model for thermal-hydraulics. The MC model was first validated for fresh fuel and zero-power configuration. Then, a thermal-hydraulic model was developed using the power distribution results from the MC simulation as input. This thermal-hydraulic model included sub-cooled boiling effects at full reactor power. The resulting temperature distribution was then introduced back into the MC model, and a benchmark analysis was performed to validate the model in fresh fuel and full-power configuration. The good agreement between experimental data and simulation results for full-power reactor criticality proved the reliability of this coupled methodology.\n\nOption A is incorrect because the MC simulation for neutronics was performed first, not the thermal-hydraulic model.\n\nOption B is incorrect because the MC model was initially validated for fresh fuel and zero-power configuration, not just full-power.\n\nOption D is incorrect because the thermal-hydraulic model explicitly took into account sub-cooled boiling effects present at full reactor power, rather than ignoring them."}, "9": {"documentation": {"title": "A Brownian particle having a fluctuating mass", "source": "R. Lambiotte & M. Ausloos", "docs_id": "cond-mat/0508773", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Brownian particle having a fluctuating mass. We focus on the dynamics of a Brownian particle whose mass fluctuates. First we show that the behaviour is similar to that of a Brownian particle moving in a fluctuating medium, as studied by Beck [Phys. Rev. Lett. 87 (2001) 180601]. By performing numerical simulations of the Langevin equation, we check the theoretical predictions derived in the adiabatic limit, and study deviations outside this limit. We compare the mass velocity distribution with truncated Tsallis distributions [J. Stat. Phys. 52 (1988) 479] and find excellent agreement if the masses are chi- squared distributed. We also consider the diffusion of the Brownian particle by studying a Bernoulli random walk with fluctuating walk length in one dimension. We observe the time dependence of the position distribution kurtosis and find interesting behaviours. We point out a few physical cases where the mass fluctuation problem could be encountered as a first approximation for agglomeration- fracture non equilibrium processes."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A Brownian particle with fluctuating mass is being studied. Which of the following statements is most accurate regarding the behavior of this particle and its mass velocity distribution?\n\nA) The particle's behavior is fundamentally different from a Brownian particle moving in a fluctuating medium, and its mass velocity distribution follows a Gaussian distribution.\n\nB) The particle's behavior is similar to a Brownian particle in a fluctuating medium, and its mass velocity distribution is best described by a Lorentzian distribution.\n\nC) The particle's behavior resembles that of a Brownian particle in a fluctuating medium, and its mass velocity distribution shows excellent agreement with truncated Tsallis distributions when masses are chi-squared distributed.\n\nD) The particle's behavior is unpredictable and does not conform to any known distribution, requiring entirely new mathematical models for description.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that the behavior of a Brownian particle with fluctuating mass is similar to that of a Brownian particle moving in a fluctuating medium, as studied by Beck. Furthermore, it mentions that when comparing the mass velocity distribution with truncated Tsallis distributions, excellent agreement is found if the masses are chi-squared distributed. This directly corresponds to the statement in option C.\n\nOption A is incorrect because it contradicts the documentation's assertion about the similarity to a particle in a fluctuating medium and misrepresents the distribution.\n\nOption B is incorrect because while it correctly identifies the similarity in behavior, it incorrectly specifies a Lorentzian distribution instead of the truncated Tsallis distribution mentioned in the document.\n\nOption D is incorrect as it overstates the unpredictability of the particle's behavior and ignores the successful application of existing distributions (truncated Tsallis) in describing the mass velocity distribution."}, "10": {"documentation": {"title": "Linear Equilibria for Dynamic LQG Games with Asymmetric Information and\n  Dependent Types", "source": "Nasimeh Heydaribeni and Achilleas Anastasopoulos", "docs_id": "1909.04834", "section": ["econ.GN", "cs.SY", "eess.SY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Linear Equilibria for Dynamic LQG Games with Asymmetric Information and\n  Dependent Types. We consider a non-zero-sum linear quadratic Gaussian (LQG) dynamic game with asymmetric information. Each player observes privately a noisy version of a (hidden) state of the world $V$, resulting in dependent private observations. We study perfect Bayesian equilibria (PBE) for this game with equilibrium strategies that are linear in players' private estimates of $V$. The main difficulty arises from the fact that players need to construct estimates on other players' estimate on $V$, which in turn would imply that an infinite hierarchy of estimates on estimates needs to be constructed, rendering the problem unsolvable. We show that this is not the case: each player's estimate on other players' estimates on $V$ can be summarized into her own estimate on $V$ and some appropriately defined public information. Based on this finding we characterize the PBE through a backward/forward algorithm akin to dynamic programming for the standard LQG control problem. Unlike the standard LQG problem, however, Kalman filter covariance matrices, as well as some other required quantities, are observation-dependent and thus cannot be evaluated off-line through a forward recursion."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of a non-zero-sum linear quadratic Gaussian (LQG) dynamic game with asymmetric information, what is the key insight that allows for the characterization of perfect Bayesian equilibria (PBE) without constructing an infinite hierarchy of estimates?\n\nA) Players' estimates of other players' estimates on V can be directly observed, simplifying the problem.\n\nB) The game assumes symmetric information, eliminating the need for higher-order estimates.\n\nC) Each player's estimate on other players' estimates on V can be summarized using only the player's own estimate on V and some public information.\n\nD) The Kalman filter covariance matrices can be evaluated off-line, making higher-order estimates unnecessary.\n\nCorrect Answer: C\n\nExplanation: The key insight presented in the documentation is that \"each player's estimate on other players' estimates on V can be summarized into her own estimate on V and some appropriately defined public information.\" This approach avoids the need to construct an infinite hierarchy of estimates, which would have made the problem unsolvable.\n\nOption A is incorrect because players cannot directly observe others' estimates; they only have access to their own noisy observations.\n\nOption B is incorrect as the game explicitly involves asymmetric information, not symmetric.\n\nOption D is incorrect because the documentation states that \"Kalman filter covariance matrices, as well as some other required quantities, are observation-dependent and thus cannot be evaluated off-line through a forward recursion.\"\n\nThis question tests understanding of the core concept that enables the characterization of PBE in this complex game setting."}, "11": {"documentation": {"title": "Effect of reaction step-size noise on the switching dynamics of\n  stochastic populations", "source": "Shay Be'er, Metar Heller-Algazi and Michael Assaf", "docs_id": "1509.03820", "section": ["cond-mat.stat-mech", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effect of reaction step-size noise on the switching dynamics of\n  stochastic populations. In genetic circuits, when the mRNA lifetime is short compared to the cell cycle, proteins are produced in geometrically-distributed bursts, which greatly affects the cellular switching dynamics between different metastable phenotypic states. Motivated by this scenario, we study a general problem of switching or escape in stochastic populations, where influx of particles occurs in groups or bursts, sampled from an arbitrary distribution. The fact that the step size of the influx reaction is a-priori unknown, and in general, may fluctuate in time with a given correlation time and statistics, introduces an additional non-demographic step-size noise into the system. Employing the probability generating function technique in conjunction with Hamiltonian formulation, we are able to map the problem in the leading order onto solving a stationary Hamilton-Jacobi equation. We show that bursty influx exponentially decreases the mean escape time compared to the \"usual case\" of single-step influx. In particular, close to bifurcation we find a simple analytical expression for the mean escape time, which solely depends on the mean and variance of the burst-size distribution. Our results are demonstrated on several realistic distributions and compare well with numerical Monte-Carlo simulations."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of stochastic populations with bursty influx, which of the following statements is most accurate regarding the mean escape time from metastable states?\n\nA) The mean escape time is primarily determined by the correlation time of the step-size noise.\n\nB) Bursty influx always increases the mean escape time compared to single-step influx.\n\nC) Near bifurcation, the mean escape time depends solely on the mean and variance of the burst-size distribution.\n\nD) The probability generating function technique is insufficient to analyze the system's dynamics.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"close to bifurcation we find a simple analytical expression for the mean escape time, which solely depends on the mean and variance of the burst-size distribution.\" This directly supports option C.\n\nOption A is incorrect because while step-size noise is mentioned, the correlation time is not specifically identified as the primary determinant of mean escape time.\n\nOption B is incorrect and actually opposite to what the text states. The documentation mentions that \"bursty influx exponentially decreases the mean escape time compared to the 'usual case' of single-step influx.\"\n\nOption D is incorrect because the text indicates that the probability generating function technique, in conjunction with Hamiltonian formulation, is successfully used to map the problem onto solving a stationary Hamilton-Jacobi equation.\n\nThis question tests the student's ability to carefully read and interpret complex scientific information, particularly focusing on the key findings regarding mean escape time in stochastic populations with bursty influx."}, "12": {"documentation": {"title": "Orbital dynamics in the photogravitational restricted four-body problem:\n  Lagrange configuration", "source": "J. E. Osorio-Vargas, F. L. Dubeibe, Guillermo A. Gonz\\'alez", "docs_id": "1910.09757", "section": ["nlin.CD", "astro-ph.EP", "astro-ph.SR", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orbital dynamics in the photogravitational restricted four-body problem:\n  Lagrange configuration. We study the effect of the radiation parameter in the location, stability and orbital dynamics in the Lagrange configuration of the restricted four-body problem when one of the primaries is a radiating body. The equations of motion for the test particle are derived by assuming that the primaries revolve in the same plane with uniform angular velocity, and regardless of their mass distribution, they will always lie at the vertices of an equilateral triangle. The insertion of the radiation factor in the restricted four-body problem, let us model more realistically the dynamics of a test particle orbiting an astrophysical system with an active star. The dynamical mechanisms responsible for the smoothening on the basin structures of the configuration space is related to the decrease in the total number of fixed points with increasing values of the radiation parameter. In our model of the Sun-Jupiter-Trojan Asteroid system, it is found that despite the repulsive character of the solar radiation pressure, there exist two stable libration points roughly located at the position of L4 and L5 in the Sun-Jupiter system."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the photogravitational restricted four-body problem with Lagrange configuration, what is the primary effect of increasing the radiation parameter on the system's dynamics?\n\nA) It increases the total number of fixed points in the system\nB) It causes the primaries to deviate from their equilateral triangle formation\nC) It eliminates the stable libration points analogous to L4 and L5\nD) It reduces the total number of fixed points and smoothens the basin structures in the configuration space\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the complex dynamics in the photogravitational restricted four-body problem. The correct answer is D because the documentation states: \"The dynamical mechanisms responsible for the smoothening on the basin structures of the configuration space is related to the decrease in the total number of fixed points with increasing values of the radiation parameter.\"\n\nOption A is incorrect as it contradicts the document's statement about decreasing fixed points. \n\nOption B is wrong because the document specifies that \"regardless of their mass distribution, they will always lie at the vertices of an equilateral triangle.\"\n\nOption C is incorrect because the document mentions: \"there exist two stable libration points roughly located at the position of L4 and L5 in the Sun-Jupiter system,\" despite the repulsive character of solar radiation pressure.\n\nThis question requires synthesizing information from different parts of the text and understanding the relationship between the radiation parameter and system dynamics."}, "13": {"documentation": {"title": "Model-Rich Approaches to Eliciting Possibly Weak or Incomplete\n  Preferences: Evidence from a Multi-Valued Choice Experiment", "source": "Georgios Gerasimou", "docs_id": "2111.14431", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Model-Rich Approaches to Eliciting Possibly Weak or Incomplete\n  Preferences: Evidence from a Multi-Valued Choice Experiment. This paper contributes to the elicitation of a decision maker's strict preferences and their possible indifference or incomparability/indecisiveness. Every subject in both treatments of an incentivized lab experiment could choose multiple alternatives from each of the 50 distinct menus of popular gift-card pairs that they saw. Subjects in the non-forced-choice treatment could, in addition, avoid/delay making an active choice at those menus. Applying a non-parametric optimization method on data collected from 273 subjects, we find that nearly 60% of them are well-approximated by an indifference-permitting model of complete- or incomplete-preference maximization. Most recovered preferences are unique, have a non-trivial indifference part and, where relevant, a distinct indecisiveness part. The two kinds of distinctions between indifference and indecisiveness uncovered by this method are theory-guided and documented empirically for the first time. These findings suggest that accounting for possible indifferences and/or incomparabilities in the data-collection process and analysis can be useful in eliciting transitive weak preferences. Two aspects of the experimental design, finally, allow for interpreting an additional 10% of subjects as revealing a systematic preference for randomization or satisficing."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: According to the paper, which of the following statements best describes the findings regarding subjects' preferences in the multi-valued choice experiment?\n\nA) Approximately 60% of subjects exhibited preferences that were best modeled by a forced-choice paradigm without allowance for indifference or incomparability.\n\nB) The majority of subjects showed preferences that could be well-approximated by an indifference-permitting model of complete- or incomplete-preference maximization, with most recovered preferences being unique and having a non-trivial indifference part.\n\nC) The experiment found that subjects overwhelmingly preferred to avoid or delay making active choices when given the option in the non-forced-choice treatment.\n\nD) The study concluded that accounting for indifferences and incomparabilities in preference elicitation is unnecessary, as most subjects displayed clear-cut, complete preferences.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper states that \"nearly 60% of them [subjects] are well-approximated by an indifference-permitting model of complete- or incomplete-preference maximization. Most recovered preferences are unique, have a non-trivial indifference part and, where relevant, a distinct indecisiveness part.\" This directly supports the statement in option B.\n\nOption A is incorrect because it contradicts the findings, suggesting a forced-choice model without indifference, which is not what the paper reports.\n\nOption C is not supported by the information given. While the non-forced-choice treatment allowed for avoiding/delaying choices, the paper does not state that subjects overwhelmingly chose this option.\n\nOption D is incorrect as it directly contradicts the paper's conclusion about the importance of accounting for indifferences and incomparabilities in preference elicitation."}, "14": {"documentation": {"title": "Bayesian Inference for NMR Spectroscopy with Applications to Chemical\n  Quantification", "source": "Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin,\n  Mick D. Mantle, Lynn F. Gladden, Andrew Blake", "docs_id": "1402.3580", "section": ["stat.AP", "stat.ME", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Inference for NMR Spectroscopy with Applications to Chemical\n  Quantification. Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic properties of atomic nuclei to discover the structure, reaction state and chemical environment of molecules. We propose a probabilistic generative model and inference procedures for NMR spectroscopy. Specifically, we use a weighted sum of trigonometric functions undergoing exponential decay to model free induction decay (FID) signals. We discuss the challenges in estimating the components of this general model -- amplitudes, phase shifts, frequencies, decay rates, and noise variances -- and offer practical solutions. We compare with conventional Fourier transform spectroscopy for estimating the relative concentrations of chemicals in a mixture, using synthetic and experimentally acquired FID signals. We find the proposed model is particularly robust to low signal to noise ratios (SNR), and overlapping peaks in the Fourier transform of the FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) which are not possible with conventional spectroscopy (5% sensitivity)."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A researcher is analyzing a complex mixture using NMR spectroscopy and encounters significant peak overlap in the Fourier transform spectrum, along with a low signal-to-noise ratio. Which of the following approaches would likely provide the most accurate quantification of the relative concentrations of chemicals in this challenging sample?\n\nA) Conventional Fourier transform spectroscopy with peak deconvolution\nB) Bayesian inference using a probabilistic generative model of the FID signal\nC) Increasing the number of scans to improve signal-to-noise ratio\nD) Using two-dimensional NMR techniques to resolve overlapping peaks\n\nCorrect Answer: B\n\nExplanation: The correct answer is B) Bayesian inference using a probabilistic generative model of the FID signal. \n\nThe documentation describes a new approach using Bayesian inference and a probabilistic generative model for NMR spectroscopy. This method is specifically noted to be robust in situations with low signal-to-noise ratios and overlapping peaks in the Fourier transform spectrum, which matches the scenario described in the question. The text states that this approach enables \"accurate predictions (e.g., 1% sensitivity at low SNR) which are not possible with conventional spectroscopy (5% sensitivity).\"\n\nOption A, conventional Fourier transform spectroscopy, is less effective in this scenario, as the documentation implies it has poorer sensitivity (5%) compared to the Bayesian approach (1%) at low SNR.\n\nOption C, increasing the number of scans, could improve SNR but wouldn't directly address the peak overlap issue and may not be as effective as the Bayesian approach for quantification.\n\nOption D, using 2D NMR, could potentially help resolve overlapping peaks but wasn't mentioned in the given text and may not address the low SNR issue as effectively as the Bayesian method.\n\nThe Bayesian approach is described as particularly suitable for the challenging conditions presented in the question, making it the most likely to provide accurate quantification of the relative concentrations of chemicals in this complex mixture."}, "15": {"documentation": {"title": "A Multisection Broadband Impedance Transforming Branch-Line Hybrid", "source": "S. Kumar, C. Tannous and T. Danshin", "docs_id": "physics/0104032", "section": ["physics.comp-ph", "physics.gen-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Multisection Broadband Impedance Transforming Branch-Line Hybrid. Measurements and design equations for a two section impedance transforming hybrid suitable for MMIC applications and a new method of synthesis for multisection branch-line hybrids are reported. The synthesis method allows the response to be specified either of Butterworth or Chebyshev type. Both symmetric (with equal input and output impedances) and non-symmetric (impedance transforming) designs are feasible. Starting from a given number of sections, type of response, and impedance transformation ratio and for a specified midband coupling, power division ratio, isolation or directivity ripple bandwidth, the set of constants needed for the evaluation of the reflection coefficient response is first calculated. The latter is used to define a driving point impedance of the circuit, synthesize it and obtain the branch line immittances with the use of the concept of double length unit elements (DLUE). The experimental results obtained with microstrip hybrids constructed to test the validity of the brute force optimization and the synthesized designs show very close agreement with the computed responses."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A multisection branch-line hybrid is designed using the synthesis method described in the document. Which of the following statements is NOT true regarding this method?\n\nA) It allows for both Butterworth and Chebyshev type responses\nB) It can produce symmetric designs with equal input and output impedances\nC) It uses the concept of double length unit elements (DLUE) to obtain branch line immittances\nD) It requires specifying the exact number of sections to achieve the desired bandwidth\n\nCorrect Answer: D\n\nExplanation: \nA is correct: The document explicitly states that the synthesis method allows the response to be specified as either Butterworth or Chebyshev type.\n\nB is correct: The text mentions that both symmetric (with equal input and output impedances) and non-symmetric (impedance transforming) designs are feasible.\n\nC is correct: The document states that the branch line immittances are obtained \"with the use of the concept of double length unit elements (DLUE).\"\n\nD is incorrect: While the method starts with a given number of sections, it does not require specifying the exact number to achieve the desired bandwidth. Instead, it uses parameters like midband coupling, power division ratio, isolation or directivity ripple bandwidth to calculate the constants needed for the reflection coefficient response, which is then used to synthesize the circuit.\n\nThis question tests the student's understanding of the synthesis method's flexibility and input parameters, requiring careful reading and interpretation of the given information."}, "16": {"documentation": {"title": "Cancer Diagnosis with QUIRE: QUadratic Interactions among infoRmative\n  fEatures", "source": "Salim Chowdhury, Yanjun Qi, Alex Stewart, Rachel Ostroff, Renqiang Min", "docs_id": "1302.3855", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cancer Diagnosis with QUIRE: QUadratic Interactions among infoRmative\n  fEatures. Responsible for many complex human diseases including cancers, disrupted or abnormal gene interactions can be identified through their expression changes correlating with the progression of a disease. However, the examination of all possible combinatorial interactions between gene features in a genome-wide case-control study is computationally infeasible as the search space is exponential in nature. In this paper, we propose a novel computational approach, QUIRE, to identify discriminative complex interactions among informative gene features for cancer diagnosis. QUIRE works in two stages, where it first identifies functionally relevant feature groups for the disease and, then explores the search space capturing the combinatorial relationships among the genes from the selected informative groups. Using QUIRE, we explore the differential patterns and the interactions among informative gene features in three different types of cancers, Renal Cell Carcinoma(RCC), Ovarian Cancer(OVC) and Colorectal Cancer (CRC). Our experimental results show that QUIRE identifies gene-gene interactions that can better identify the different cancer stages of samples and can predict CRC recurrence and death from CRC more successfully, as compared to other state-of-the-art feature selection methods. A literature survey shows that many of the interactions identified by QUIRE play important roles in the development of cancer."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: QUIRE (QUadratic Interactions among infoRmative fEatures) is a novel computational approach for cancer diagnosis. Which of the following statements best describes the key advantage of QUIRE over traditional genome-wide case-control studies?\n\nA) It analyzes all possible gene interactions simultaneously, providing a comprehensive view of genetic factors in cancer.\n\nB) It focuses solely on individual gene expressions, ignoring complex interactions to simplify the analysis.\n\nC) It uses a two-stage approach to efficiently identify and explore discriminative complex interactions among informative gene features.\n\nD) It relies exclusively on pre-existing knowledge about gene interactions in cancer, reducing computational requirements.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. QUIRE uses a two-stage approach to efficiently identify and explore discriminative complex interactions among informative gene features. This is a key advantage because:\n\n1. Examining all possible combinatorial interactions in a genome-wide study is computationally infeasible due to the exponential nature of the search space.\n\n2. QUIRE first identifies functionally relevant feature groups for the disease, narrowing down the search space.\n\n3. It then explores the combinatorial relationships among genes from the selected informative groups, making the analysis more efficient and focused.\n\nAnswer A is incorrect because while comprehensive, analyzing all possible gene interactions simultaneously is computationally infeasible.\n\nAnswer B is incorrect because QUIRE specifically aims to identify complex interactions, not just individual gene expressions.\n\nAnswer D is incorrect because QUIRE doesn't rely exclusively on pre-existing knowledge. It uses a computational approach to identify new, potentially important interactions."}, "17": {"documentation": {"title": "Synt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition", "source": "Ting-Yao Hu, Mohammadreza Armandpour, Ashish Shrivastava, Jen-Hao Rick\n  Chang, Hema Koppula, Oncel Tuzel", "docs_id": "2110.11479", "section": ["eess.AS", "cs.LG", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition. With recent advances in speech synthesis, synthetic data is becoming a viable alternative to real data for training speech recognition models. However, machine learning with synthetic data is not trivial due to the gap between the synthetic and the real data distributions. Synthetic datasets may contain artifacts that do not exist in real data such as structured noise, content errors, or unrealistic speaking styles. Moreover, the synthesis process may introduce a bias due to uneven sampling of the data manifold. We propose two novel techniques during training to mitigate the problems due to the distribution gap: (i) a rejection sampling algorithm and (ii) using separate batch normalization statistics for the real and the synthetic samples. We show that these methods significantly improve the training of speech recognition models using synthetic data. We evaluate the proposed approach on keyword detection and Automatic Speech Recognition (ASR) tasks, and observe up to 18% and 13% relative error reduction, respectively, compared to naively using the synthetic data."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following best describes the primary challenge in using synthetic data for training speech recognition models, as highlighted in the Synt++ research?\n\nA) Synthetic data is too expensive to generate in large quantities\nB) The gap between synthetic and real data distributions\nC) Synthetic data cannot capture complex speech patterns\nD) Real data is always superior to synthetic data for model training\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation explicitly states that \"machine learning with synthetic data is not trivial due to the gap between the synthetic and the real data distributions.\" This distribution gap is identified as the primary challenge in utilizing synthetic data effectively.\n\nAnswer A is incorrect because the document doesn't mention cost as a factor.\n\nAnswer C is not accurate because while synthetic data may have limitations, the research focuses on addressing distribution gaps rather than claiming synthetic data can't capture complex patterns.\n\nAnswer D is incorrect because the research aims to improve the use of synthetic data, implying that it can be valuable for training when used correctly, not that real data is always superior.\n\nThe question tests understanding of the core problem addressed by the Synt++ research and requires careful reading of the provided information."}, "18": {"documentation": {"title": "Mathematical Modeling, Laboratory Experiments, and Sensitivity Analysis\n  of Bioplug Technology at Darcy Scale", "source": "David Landa-Marb\\'an, Gunhild B{\\o}dtker, Bartek Florczyk Vik, Per\n  Pettersson, Iuliu Sorin Pop, Kundan Kumar, Florin Adrian Radu", "docs_id": "2002.00090", "section": ["physics.app-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mathematical Modeling, Laboratory Experiments, and Sensitivity Analysis\n  of Bioplug Technology at Darcy Scale. In this paper we study a Darcy-scale mathematical model for biofilm formation in porous media. The pores in the core are divided into three phases: water, oil, and biofilm. The water and oil flow are modeled by an extended version of Darcy's law and the substrate is transported by diffusion and convection in the water phase. Initially there is biofilm on the pore walls. The biofilm consumes substrate for production of biomass and modifies the pore space which changes the rock permeability. The model includes detachment of biomass due to water flux and death of bacteria, and is implemented in MRST. We discuss the capability of the numerical simulator to capture results from laboratory experiments. We perform a novel sensitivity analysis based on sparse-grid interpolation and multi-wavelet expansion to identify the critical model parameters. Numerical experiments using diverse injection strategies are performed to study the impact of different porosity-permeability relations in a core saturated with water and oil."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the Darcy-scale mathematical model for biofilm formation in porous media described in the paper, which of the following statements is NOT correct?\n\nA) The model incorporates three phases in the pores: water, oil, and biofilm.\n\nB) The substrate is transported only by diffusion in the water phase.\n\nC) The model accounts for detachment of biomass due to water flux and bacterial death.\n\nD) The study includes a sensitivity analysis based on sparse-grid interpolation and multi-wavelet expansion.\n\nCorrect Answer: B\n\nExplanation: \nA is correct as the paper explicitly states that the pores are divided into three phases: water, oil, and biofilm.\n\nB is incorrect and thus the correct answer to this question. The paper states that \"the substrate is transported by diffusion and convection in the water phase,\" not just by diffusion.\n\nC is correct as the paper mentions that \"The model includes detachment of biomass due to water flux and death of bacteria.\"\n\nD is correct as the paper describes \"a novel sensitivity analysis based on sparse-grid interpolation and multi-wavelet expansion to identify the critical model parameters.\"\n\nThis question tests the reader's understanding of the model's key components and their ability to identify a subtle misstatement about the substrate transport mechanism."}, "19": {"documentation": {"title": "Dielectric screening in two-dimensional insulators: Implications for\n  excitonic and impurity states in graphane", "source": "Pierluigi Cudazzo, Ilya V. Tokatly and Angel Rubio", "docs_id": "1104.3346", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dielectric screening in two-dimensional insulators: Implications for\n  excitonic and impurity states in graphane. For atomic thin layer insulating materials we provide an exact analytic form of the two-dimensional screened potential. In contrast to three-dimensional systems where the macroscopic screening can be described by a static dielectric constant in 2D systems the macroscopic screening is non local (q-dependent) showing a logarithmic divergence for small distances and reaching the unscreened Coulomb potential for large distances. The cross-over of these two regimes is dictated by 2D layer polarizability that can be easily computed by standard first-principles techniques. The present results have strong implications for describing gap-impurity levels and also exciton binding energies. The simple model derived here captures the main physical effects and reproduces well, for the case of graphane, the full many-body GW plus Bethe-Salpeter calculations. As an additional outcome we show that the impurity hole-doping in graphane leads to strongly localized states, what hampers applications in electronic devices. In spite of the inefficient and nonlocal two-dimensional macroscopic screening we demonstrate that a simple $\\mathbf{k}\\cdot\\mathbf{p}$ approach is capable to describe the electronic and transport properties of confined 2D systems."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In two-dimensional insulators like graphane, how does the screened potential behave compared to three-dimensional systems, and what are its implications?\n\nA) The screened potential is constant and can be described by a static dielectric constant, similar to 3D systems.\n\nB) The screened potential is non-local and q-dependent, showing a logarithmic divergence for small distances and approaching the unscreened Coulomb potential for large distances.\n\nC) The screened potential is always equal to the unscreened Coulomb potential, regardless of distance.\n\nD) The screened potential exponentially decays with distance and is independent of the material's polarizability.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that for 2D systems, unlike 3D systems where screening can be described by a static dielectric constant, the macroscopic screening is non-local (q-dependent). It shows a logarithmic divergence for small distances and reaches the unscreened Coulomb potential for large distances. This behavior has significant implications for describing gap-impurity levels and exciton binding energies in 2D materials like graphane.\n\nOption A is incorrect because it describes the behavior of 3D systems, not 2D systems. Option C is incorrect because the screened potential is not always equal to the unscreened Coulomb potential; it only approaches this for large distances. Option D is incorrect as it doesn't match the described behavior and ignores the material's polarizability, which is crucial in determining the cross-over between the two regimes of the screened potential."}, "20": {"documentation": {"title": "Static response of deformable microchannels: A comparative modelling\n  study", "source": "Tanmay C. Shidhore and Ivan C. Christov", "docs_id": "1709.03002", "section": ["physics.flu-dyn", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Static response of deformable microchannels: A comparative modelling\n  study. We present a comparative modelling study of fluid-structure interactions in microchannels. Through a mathematical analysis based on plate theory and the lubrication approximation for low-Reynolds-number flow, we derive models for the flow rate-pressure drop relation for long shallow microchannels with both thin and thick deformable top walls. These relations are tested against full three-dimensional two-way-coupled fluid-structure interaction simulations. Three types of microchannels, representing different elasticity regimes and having been experimentally characterized previously, are chosen as benchmarks for our theory and simulations. Good agreement is found in most cases for the predicted, simulated and measured flow rate-pressure drop relationships. The numerical simulations performed allow us to also carefully examine the deformation profile of the top wall of the microchannel in any cross section, showing good agreement with the theory. Specifically, the prediction that span-wise displacement in a long shallow microchannel decouples from the flow-wise deformation is confirmed, and the predicted scaling of the maximum displacement with the hydrodynamic pressure and the various material and geometric parameters is validated."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In a long, shallow microchannel with a deformable top wall, which of the following statements is most accurate regarding the relationship between span-wise displacement and flow-wise deformation?\n\nA) Span-wise displacement is directly proportional to flow-wise deformation\nB) Span-wise displacement is inversely proportional to flow-wise deformation\nC) Span-wise displacement is independent of flow-wise deformation\nD) Span-wise displacement and flow-wise deformation are always equal\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"the prediction that span-wise displacement in a long shallow microchannel decouples from the flow-wise deformation is confirmed.\" This means that the span-wise displacement is independent of the flow-wise deformation.\n\nOption A is incorrect because there is no direct proportional relationship mentioned.\nOption B is incorrect as there is no inverse relationship described.\nOption D is incorrect as the displacement and deformation are not stated to be equal, but rather decoupled or independent.\n\nThis question tests the student's understanding of the complex fluid-structure interactions in microchannels and their ability to interpret the relationship between different types of deformation in these systems."}, "21": {"documentation": {"title": "Synchrotron x-ray scattering of UN and U2N3 epitaxial films", "source": "E. Lawrence Bright, R. Springell, D. G. Porter, S. P. Collins, and G.\n  H. Lander", "docs_id": "1907.11685", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchrotron x-ray scattering of UN and U2N3 epitaxial films. We examine the magnetic ordering of UN and of a closely related nitride, U2N3, by preparing thin epitaxial films and using synchrotron x-ray techniques. The magnetic configuration and subsequent coupling to the lattice are key features of the electronic structure. The well-known antiferromagnetic (AF) ordering of UN is confirmed, but the expected accompanying distortion at Tn is not observed. Instead, we propose that the strong magneto-elastic interaction at low temperature involves changes in the strain of the material. These strains vary as a function of the sample form. As a consequence, the accepted AF configuration of UN may be incorrect. In the case of cubic a-U2N3, no single crystals have been previously prepared, and we have determined the AF ordering wave-vector. The AF Tn is close to that previously reported. In addition, resonant diffraction methods have identified an aspherical quadrupolar charge contribution in U2N3 involving the 5f electrons; the first time this has been observed in an actinide compound."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about the magnetic ordering and structural properties of UN and U2N3 is NOT supported by the findings described in the Arxiv documentation?\n\nA) The antiferromagnetic ordering of UN was confirmed, but the expected lattice distortion at the N\u00e9el temperature was not observed.\n\nB) The study suggests that the strong magneto-elastic interaction in UN at low temperature involves changes in strain that vary depending on the sample form.\n\nC) The antiferromagnetic ordering wave-vector of cubic \u03b1-U2N3 was determined for the first time using epitaxial thin films.\n\nD) Resonant diffraction methods revealed an aspherical quadrupolar charge contribution in UN involving the 5f electrons.\n\nCorrect Answer: D\n\nExplanation: \nA is correct according to the documentation, which states that the AF ordering of UN was confirmed but the expected distortion at Tn was not observed.\n\nB is supported by the text, which proposes that the strong magneto-elastic interaction at low temperature involves changes in the strain of the material, varying as a function of the sample form.\n\nC is accurate, as the documentation mentions that no single crystals of cubic \u03b1-U2N3 had been previously prepared, and they determined the AF ordering wave-vector.\n\nD is incorrect. The documentation states that the aspherical quadrupolar charge contribution involving the 5f electrons was observed in U2N3, not in UN. This was noted as the first time such an observation was made in an actinide compound."}, "22": {"documentation": {"title": "Geometric Graph Properties of the Spatial Preferred Attachment model", "source": "Jeannette Janssen, Pawel Pralat and Rory Wilson", "docs_id": "1111.0508", "section": ["cs.SI", "math.CO", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometric Graph Properties of the Spatial Preferred Attachment model. The spatial preferred attachment (SPA) model is a model for networked information spaces such as domains of the World Wide Web, citation graphs, and on-line social networks. It uses a metric space to model the hidden attributes of the vertices. Thus, vertices are elements of a metric space, and link formation depends on the metric distance between vertices. We show, through theoretical analysis and simulation, that for graphs formed according to the SPA model it is possible to infer the metric distance between vertices from the link structure of the graph. Precisely, the estimate is based on the number of common neighbours of a pair of vertices, a measure known as {\\sl co-citation}. To be able to calculate this estimate, we derive a precise relation between the number of common neighbours and metric distance. We also analyze the distribution of {\\sl edge lengths}, where the length of an edge is the metric distance between its end points. We show that this distribution has three different regimes, and that the tail of this distribution follows a power law."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the Spatial Preferred Attachment (SPA) model for networked information spaces, which of the following statements is NOT true regarding the relationship between metric distance and graph structure?\n\nA) The model uses a metric space to represent hidden attributes of vertices.\n\nB) The number of common neighbors (co-citation) can be used to estimate the metric distance between vertices.\n\nC) The distribution of edge lengths follows a uniform distribution across all distance ranges.\n\nD) The formation of links between vertices depends on their metric distance.\n\nCorrect Answer: C\n\nExplanation:\nA is correct: The SPA model indeed uses a metric space to model hidden attributes of vertices.\nB is correct: The documentation states that it's possible to infer metric distance from the number of common neighbors (co-citation).\nC is incorrect: The distribution of edge lengths has three different regimes and the tail follows a power law, not a uniform distribution.\nD is correct: The documentation explicitly states that link formation depends on the metric distance between vertices.\n\nThe correct answer is C because it contradicts the information provided. The distribution of edge lengths in the SPA model is more complex, with three different regimes and a power-law tail, rather than being uniformly distributed."}, "23": {"documentation": {"title": "Mean Field Contest with Singularity", "source": "Marcel Nutz, Yuchong Zhang", "docs_id": "2103.04219", "section": ["math.OC", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mean Field Contest with Singularity. We formulate a mean field game where each player stops a privately observed Brownian motion with absorption. Players are ranked according to their level of stopping and rewarded as a function of their relative rank. There is a unique mean field equilibrium and it is shown to be the limit of associated $n$-player games. Conversely, the mean field strategy induces $n$-player $\\varepsilon$-Nash equilibria for any continuous reward function -- but not for discontinuous ones. In a second part, we study the problem of a principal who can choose how to distribute a reward budget over the ranks and aims to maximize the performance of the median player. The optimal reward design (contract) is found in closed form, complementing the merely partial results available in the $n$-player case. We then analyze the quality of the mean field design when used as a proxy for the optimizer in the $n$-player game. Surprisingly, the quality deteriorates dramatically as $n$ grows. We explain this with an asymptotic singularity in the induced $n$-player equilibrium distributions."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the mean field contest with singularity described, which of the following statements is correct regarding the relationship between the mean field equilibrium and n-player games?\n\nA) The mean field equilibrium strategy always induces n-player Nash equilibria for any reward function, including discontinuous ones.\n\nB) The mean field equilibrium is the limit of associated n-player games, but the mean field strategy only induces \u03b5-Nash equilibria for continuous reward functions.\n\nC) The mean field equilibrium cannot be related to n-player games due to the singularity in the absorption of Brownian motion.\n\nD) The mean field strategy induces exact Nash equilibria for n-player games with discontinuous reward functions, but only \u03b5-Nash equilibria for continuous ones.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation states that \"There is a unique mean field equilibrium and it is shown to be the limit of associated n-player games. Conversely, the mean field strategy induces n-player \u03b5-Nash equilibria for any continuous reward function -- but not for discontinuous ones.\" This directly corresponds to the statement in option B.\n\nOption A is incorrect because it overstates the relationship, claiming it works for all reward functions including discontinuous ones, which is explicitly contradicted in the text.\n\nOption C is incorrect because the text does establish a relationship between the mean field equilibrium and n-player games, contrary to this option's claim.\n\nOption D is incorrect because it reverses the relationship between continuous and discontinuous reward functions stated in the original text."}, "24": {"documentation": {"title": "Matrix product state of multi-time correlations", "source": "Katja Klobas, Matthieu Vanicat, Juan P. Garrahan, Toma\\v{z} Prosen", "docs_id": "1912.09742", "section": ["cond-mat.stat-mech", "nlin.CG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Matrix product state of multi-time correlations. For an interacting spatio-temporal lattice system we introduce a formal way of expressing multi-time correlation functions of local observables located at the same spatial point with a time state, i.e. a statistical distribution of configurations observed along a time lattice. Such a time state is defined with respect to a particular equilibrium state that is invariant under space and time translations. The concept is developed within the Rule 54 reversible cellular automaton, for which we explicitly construct a matrix product form of the time state, with matrices that act on the 3-dimensional auxiliary space. We use the matrix-product state to express equal-space time-dependent density-density correlation function, which, for special maximum-entropy values of equilibrium parameters, agrees with the previous results. Additionally, we obtain an explicit expression for the probabilities of observing all multi-time configurations, which enables us to study distributions of times between consecutive excitations and prove the absence of decoupling of timescales in the Rule 54 model."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of the Rule 54 reversible cellular automaton, which of the following statements about the matrix product form of the time state is correct?\n\nA) The matrices in the matrix product form act on a 2-dimensional auxiliary space.\n\nB) The time state is defined with respect to a non-equilibrium state that varies under space and time translations.\n\nC) The matrix product state can be used to express spatially-separated time-dependent correlation functions.\n\nD) The matrix product form allows for the calculation of probabilities for all multi-time configurations, enabling the study of distributions of times between consecutive excitations.\n\nCorrect Answer: D\n\nExplanation: \nOption A is incorrect because the documentation specifically states that the matrices act on a 3-dimensional auxiliary space, not 2-dimensional.\n\nOption B is wrong as the time state is defined with respect to a particular equilibrium state that is invariant under space and time translations, not a non-equilibrium state that varies.\n\nOption C is inaccurate because the documentation mentions using the matrix product state to express equal-space time-dependent density-density correlation function, not spatially-separated correlations.\n\nOption D is correct. The documentation explicitly states that they \"obtain an explicit expression for the probabilities of observing all multi-time configurations, which enables us to study distributions of times between consecutive excitations.\" This is precisely what the matrix product form allows, according to the given information."}, "25": {"documentation": {"title": "Solving Hodgkin-Huxley equations using the compact difference scheme\n  -tapering dendrite", "source": "Asha Gopinathan and Joseph Mathew", "docs_id": "1308.1788", "section": ["q-bio.NC", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Hodgkin-Huxley equations using the compact difference scheme\n  -tapering dendrite. Dendritic processing is now considered to be important in pre-processing of signals coming into a cell. Dendrites are involved in both propagation and backpropagation of signals. In a cylindrical dendrite, signals moving in either direction will be similar. However, if the dendrites taper, then this is not the case any more. The picture gets more complex if the ion channel distribution along the dendrite is also non-uniform. These equations have been solved using the Chebyshev pseudo-spectral method. Here we look at non-uniform dendritic voltage gated channels in both cylindrical and tapering dendrites. For back-propagating signals, the signal is accentuated in the case of tapering dendrites. We assume a Hodgkin-Huxley formulation of ion channels and solve these equations with the compact finite-difference scheme. The scheme gives spectral-like spatial resolution while being easier to solve than spectral methods. We show that the scheme is able to reproduce the results obtained from spectral methods. The compact difference scheme is widely used to study turbulence in airflow, however it is being used for the first time in our laboratory to solve the equations involving transmission of signals in the brain."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In a study comparing signal propagation in cylindrical and tapering dendrites using the Hodgkin-Huxley model and compact difference scheme, which of the following statements is most accurate?\n\nA) Back-propagating signals are attenuated in tapering dendrites compared to cylindrical dendrites.\n\nB) The compact difference scheme provides lower spatial resolution than spectral methods but is easier to implement.\n\nC) The study assumes uniform ion channel distribution along both cylindrical and tapering dendrites.\n\nD) The compact difference scheme reproduces results similar to those obtained from spectral methods while offering computational advantages.\n\nCorrect Answer: D\n\nExplanation: \nOption A is incorrect because the passage states that for back-propagating signals, \"the signal is accentuated in the case of tapering dendrites,\" not attenuated.\n\nOption B is incorrect as the passage mentions that the compact difference scheme gives \"spectral-like spatial resolution,\" implying it's comparable to spectral methods in resolution, not lower.\n\nOption C is incorrect because the study specifically looks at \"non-uniform dendritic voltage gated channels in both cylindrical and tapering dendrites.\"\n\nOption D is correct. The passage states that \"The scheme gives spectral-like spatial resolution while being easier to solve than spectral methods\" and \"We show that the scheme is able to reproduce the results obtained from spectral methods.\" This indicates that the compact difference scheme can achieve similar results to spectral methods while offering computational advantages."}, "26": {"documentation": {"title": "Systematic fluctuation expansion for neural network activity equations", "source": "Michael A. Buice, Jack D. Cowan, Carson C. Chow", "docs_id": "0902.3925", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Systematic fluctuation expansion for neural network activity equations. Population rate or activity equations are the foundation of a common approach to modeling for neural networks. These equations provide mean field dynamics for the firing rate or activity of neurons within a network given some connectivity. The shortcoming of these equations is that they take into account only the average firing rate while leaving out higher order statistics like correlations between firing. A stochastic theory of neural networks which includes statistics at all orders was recently formulated. We describe how this theory yields a systematic extension to population rate equations by introducing equations for correlations and appropriate coupling terms. Each level of the approximation yields closed equations, i.e. they depend only upon the mean and specific correlations of interest, without an {\\it ad hoc} criterion for doing so. We show in an example of an all-to-all connected network how our system of generalized activity equations captures phenomena missed by the mean fieldrate equations alone."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the main advantage of the systematic fluctuation expansion approach for neural network activity equations over traditional population rate equations?\n\nA) It provides a more accurate representation of individual neuron firing patterns.\nB) It introduces a stochastic element to neural network modeling, making it more realistic.\nC) It allows for the inclusion of higher-order statistics, such as correlations between firing, without ad hoc assumptions.\nD) It simplifies the computational complexity of neural network models.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The systematic fluctuation expansion approach described in the text offers a significant advantage over traditional population rate equations by allowing the inclusion of higher-order statistics, particularly correlations between firing, without relying on ad hoc assumptions.\n\nTraditional population rate equations focus only on average firing rates, neglecting higher-order statistics. The new approach systematically extends these equations by introducing equations for correlations and appropriate coupling terms. This method yields closed equations at each level of approximation, depending only on the mean and specific correlations of interest, without requiring arbitrary criteria for closure.\n\nOption A is incorrect because the approach focuses on population-level statistics rather than individual neuron patterns. Option B, while partially true (the approach does incorporate stochastic elements), does not capture the main advantage of the method. Option D is incorrect; the approach likely increases computational complexity rather than simplifying it, as it introduces additional equations for correlations."}, "27": {"documentation": {"title": "Learning Context-Dependent Choice Functions", "source": "Karlson Pfannschmidt, Pritha Gupta, Bj\\\"orn Haddenhorst, Eyke\n  H\\\"ullermeier", "docs_id": "1901.10860", "section": ["cs.LG", "cs.NE", "econ.GN", "q-fin.EC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Context-Dependent Choice Functions. Choice functions accept a set of alternatives as input and produce a preferred subset of these alternatives as output. We study the problem of learning such functions under conditions of context-dependence of preferences, which means that the preference in favor of a certain choice alternative may depend on what other options are also available. In spite of its practical relevance, this kind of context-dependence has received little attention in preference learning so far. We propose a suitable model based on context-dependent (latent) utility functions, thereby reducing the problem to the task of learning such utility functions. Practically, this comes with a number of challenges. For example, the set of alternatives provided as input to a choice function can be of any size, and the output of the function should not depend on the order in which the alternatives are presented. To meet these requirements, we propose two general approaches based on two representations of context-dependent utility functions, as well as instantiations in the form of appropriate end-to-end trainable neural network architectures. Moreover, to demonstrate the performance of both networks, we present extensive empirical evaluations on both synthetic and real-world datasets."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of learning choice functions with context-dependent preferences, which of the following statements is NOT a challenge addressed by the proposed model?\n\nA) Handling input sets of alternatives with varying sizes\nB) Ensuring the output is independent of the order of input alternatives\nC) Dealing with the computational complexity of large-scale preference datasets\nD) Representing context-dependent utility functions\n\nCorrect Answer: C\n\nExplanation: \nThe question tests understanding of the key challenges addressed in the research on learning context-dependent choice functions. \n\nOption A is incorrect because the documentation explicitly mentions that handling input sets of alternatives of any size is a challenge that the proposed model addresses.\n\nOption B is incorrect as the documentation states that ensuring the output doesn't depend on the order of presented alternatives is a requirement the model aims to meet.\n\nOption C is the correct answer (i.e., the statement that is NOT a challenge addressed by the proposed model) because the documentation does not mention dealing with computational complexity of large-scale datasets as a specific challenge addressed by their model.\n\nOption D is incorrect because the documentation clearly indicates that the model is based on representing context-dependent utility functions, which is a key aspect of their approach.\n\nThe correct answer demonstrates that while the proposed model addresses several challenges related to learning context-dependent choice functions, it does not specifically focus on computational complexity issues related to large-scale datasets, at least not as presented in the given documentation."}, "28": {"documentation": {"title": "Growth and Decay in Life-Like Cellular Automata", "source": "David Eppstein", "docs_id": "0911.2890", "section": ["nlin.CG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Growth and Decay in Life-Like Cellular Automata. We propose a four-way classification of two-dimensional semi-totalistic cellular automata that is different than Wolfram's, based on two questions with yes-or-no answers: do there exist patterns that eventually escape any finite bounding box placed around them? And do there exist patterns that die out completely? If both of these conditions are true, then a cellular automaton rule is likely to support spaceships, small patterns that move and that form the building blocks of many of the more complex patterns that are known for Life. If one or both of these conditions is not true, then there may still be phenomena of interest supported by the given cellular automaton rule, but we will have to look harder for them. Although our classification is very crude, we argue that it is more objective than Wolfram's (due to the greater ease of determining a rigorous answer to these questions), more predictive (as we can classify large groups of rules without observing them individually), and more accurate in focusing attention on rules likely to support patterns with complex behavior. We support these assertions by surveying a number of known cellular automaton rules."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: According to the proposed four-way classification system for two-dimensional semi-totalistic cellular automata, which combination of characteristics would most likely indicate a rule that supports spaceships?\n\nA) Patterns that eventually escape any finite bounding box, but no patterns that die out completely\nB) Patterns that die out completely, but none that escape any finite bounding box\nC) Both patterns that escape any finite bounding box and patterns that die out completely\nD) Neither patterns that escape any finite bounding box nor patterns that die out completely\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the proposed classification system described in the document. The correct answer is C because the document states: \"If both of these conditions are true, then a cellular automaton rule is likely to support spaceships.\" The two conditions referred to are:\n1) The existence of patterns that eventually escape any finite bounding box placed around them.\n2) The existence of patterns that die out completely.\n\nOption A is incorrect because it only satisfies one of the two required conditions. Option B is also incorrect for the same reason, satisfying only the second condition. Option D is incorrect because it satisfies neither condition, whereas the document indicates that both conditions need to be true for a rule to likely support spaceships.\n\nThis question challenges students to synthesize information from the text and understand the relationship between the proposed classification system and the likelihood of a rule supporting spaceships, which are described as \"small patterns that move and that form the building blocks of many of the more complex patterns.\""}, "29": {"documentation": {"title": "On the star-critical Ramsey number of a forest versus complete graphs", "source": "Azam Kamranian, Ghaffar Raeisi", "docs_id": "1912.00703", "section": ["math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the star-critical Ramsey number of a forest versus complete graphs. Let $G$ and $G_1, G_2, \\ldots , G_t$ be given graphs. By $G\\rightarrow (G_1, G_2, \\ldots , G_t)$ we mean if the edges of $G$ are arbitrarily colored by $t$ colors, then for some $i$, $1\\leq i\\leq t$, the spanning subgraph of $G$ whose edges are colored with the $i$-th color, contains a copy of $G_i$. The Ramsey number $R(G_1, G_2, \\ldots, G_t)$ is the smallest positive integer $n$ such that $K_n\\rightarrow (G_1, G_2, \\ldots , G_t)$ and the size Ramsey number $\\hat{R}(G_1, G_2, \\ldots , G_t)$ is defined as $\\min\\{|E(G)|:~G\\rightarrow (G_1, G_2, \\ldots , G_t)\\}$. Also, for given graphs $G_1, G_2, \\ldots , G_t$ with $r=R(G_1, G_2, \\ldots , G_t)$, the star-critical Ramsey number $R_*(G_1, G_2, \\ldots , G_t)$ is defined as $\\min\\{\\delta(G):~G\\subseteq K_r, ~G\\rightarrow (G_1, G_2, \\ldots , G_t)\\}$. In this paper, the Ramsey number and also the star-critical Ramsey number of a forest versus any number of complete graphs will be computed exactly in terms of the Ramsey number of complete graphs. As a result, the computed star-critical Ramsey number is used to give a tight bound for the size Ramsey number of a forest versus a complete graph."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Given a forest F with n vertices and no isolated vertices, and a complete graph Km, which of the following statements about R*(F, Km) is correct?\n\nA) R*(F, Km) = R(F, Km) - 1\nB) R*(F, Km) = R(Km-1, Km) - 1\nC) R*(F, Km) = R(Kn-1, Km) - 1\nD) R*(F, Km) = min{R(Kt, Km) - 1 : t \u2265 n-1}\n\nCorrect Answer: D\n\nExplanation: The star-critical Ramsey number R*(F, Km) for a forest F with n vertices (no isolated vertices) versus a complete graph Km is given by the formula:\n\nR*(F, Km) = min{R(Kt, Km) - 1 : t \u2265 n-1}\n\nThis result is derived from the fact that the Ramsey number of a forest versus a complete graph can be expressed in terms of the Ramsey number of complete graphs. The star-critical Ramsey number is always one less than the corresponding Ramsey number, and we need to consider all complete graphs Kt where t is at least n-1 (since the forest has n vertices).\n\nOption A is incorrect because R(F, Km) is not directly used in the formula for R*(F, Km).\nOption B is incorrect because it only considers Km-1, which may not be sufficient for all forests.\nOption C is incorrect because it only considers Kn-1, which may not be the minimum value for all cases.\nOption D correctly captures the minimum value over all relevant complete graphs."}, "30": {"documentation": {"title": "Constraining the nuclear symmetry energy and properties of neutron star\n  from GW170817 by Bayesian analysis", "source": "Yuxi Li, Houyuan Chen, Dehua Wen and Jing Zhang", "docs_id": "2008.02955", "section": ["nucl-th", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraining the nuclear symmetry energy and properties of neutron star\n  from GW170817 by Bayesian analysis. Based on the distribution of tidal deformabilities and component masses of binary neutron star merger GW170817, the parametric equation of states (EOS) are employed to probe the nuclear symmetry energy and the properties of neutron star. To obtain a proper distribution of the parameters of the EOS that is consistent with the observation, Bayesian analysis is used and the constraints of causality and maximum mass are considered. From this analysis, it is found that the symmetry energy at twice the saturation density of nuclear matter can be constrained within $E_{sym}(2{\\rho_{0}})$ = $34.5^{+20.5}_{-2.3}$ MeV at 90\\% credible level. Moreover, the constraints on the radii and dimensionless tidal deformabilities of canonical neutron stars are also demonstrated through this analysis, and the corresponding constraints are 10.80 km $< R_{1.4} <$ 13.20 km and $133 < \\Lambda_{1.4} < 686$ at 90\\% credible level, with the most probable value of $\\bar{R}_{1.4}$ = 12.60 km and $\\bar{\\Lambda}_{1.4}$ = 500, respectively. With respect to the prior, our result (posterior result) prefers a softer EOS, corresponding to a lower expected value of symmetry energy, a smaller radius and a smaller tidal deformability."}, "answer": "Here's a multi-form question based on the given information:\n\nQuestion: Based on the Bayesian analysis of GW170817 data, which of the following statements is NOT correct regarding the constraints on neutron star properties and nuclear symmetry energy?\n\nA) The symmetry energy at twice the saturation density of nuclear matter is constrained to be E_sym(2\u03c1_0) = 34.5^(+20.5)_(-2.3) MeV at 90% credible level.\n\nB) The radius of a canonical 1.4 solar mass neutron star is constrained to be between 10.80 km and 13.20 km at 90% credible level.\n\nC) The dimensionless tidal deformability of a canonical 1.4 solar mass neutron star is constrained to be between 133 and 686 at 90% credible level.\n\nD) The analysis strongly favors a stiffer equation of state, corresponding to higher expected values of symmetry energy, larger radii, and larger tidal deformabilities.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D because it contradicts the information given in the passage. The text states that \"With respect to the prior, our result (posterior result) prefers a softer EOS, corresponding to a lower expected value of symmetry energy, a smaller radius and a smaller tidal deformability.\" This is the opposite of what option D claims.\n\nOptions A, B, and C are all correct statements based on the information provided in the passage. They accurately reflect the constraints on symmetry energy, neutron star radius, and tidal deformability derived from the Bayesian analysis of GW170817 data."}, "31": {"documentation": {"title": "Dilaton dominance in the early Universe dilutes Dark Matter relic\n  abundances", "source": "A. B. Lahanas", "docs_id": "1102.4277", "section": ["hep-ph", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dilaton dominance in the early Universe dilutes Dark Matter relic\n  abundances. The role of the dilaton field and its coupling to matter may result to a dilution of Dark Matter (DM) relic densities. This is to be contrasted with quintessence scenarios in which relic densities are augmented, due to modification of the expansion rate, since Universe is not radiation dominated at DM decoupling. Dilaton field, besides this, affects relic densities through its coupling to dust which tends to decrease relic abundances. Thus two separate mechanisms compete each other resulting, in general, to a decrease of the relic density. This feature may be welcome and can rescue the situation if Direct Dark Matter experiments point towards small neutralino-nucleon cross sections, implying small neutralino annihilation rates and hence large relic densities, at least in the popular supersymmetric scenarios. In the presence of a diluting mechanism both experimental constraints can be met. The role of the dilaton for this mechanism has been studied in the context of the non-critical string theory but in this work we follow a rather general approach assuming that the dilaton dominates only at early eras long before Big Bang Nucleosynthesis."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of Dark Matter (DM) relic abundances, how does the presence of a dominant dilaton field in the early Universe differ from quintessence scenarios, and what implications does this have for supersymmetric DM models?\n\nA) The dilaton field increases DM relic densities due to its coupling with matter, while quintessence scenarios decrease them by modifying the expansion rate.\n\nB) Both dilaton dominance and quintessence scenarios increase DM relic densities, but through different mechanisms.\n\nC) Dilaton dominance generally decreases DM relic densities through two competing mechanisms, while quintessence scenarios tend to increase them.\n\nD) Dilaton dominance and quintessence scenarios both decrease DM relic densities, but dilaton coupling to dust is more effective in this reduction.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation states that dilaton dominance in the early Universe generally results in a decrease of DM relic densities through two competing mechanisms: 1) the dilaton's coupling to dust, which tends to decrease relic abundances, and 2) its effect on the expansion rate. This is in contrast to quintessence scenarios, which are said to augment relic densities due to modification of the expansion rate when the Universe is not radiation-dominated at DM decoupling. \n\nThe question also touches on the implications for supersymmetric DM models, as the text mentions that this dilution mechanism could help reconcile scenarios where direct DM experiments suggest small neutralino-nucleon cross sections (implying large relic densities) with observational constraints. The dilaton's effect could potentially \"rescue the situation\" by reducing these large relic densities to acceptable levels."}, "32": {"documentation": {"title": "Coexistence curve and molecule number density of AdS topological charged\n  black hole in massive gravity", "source": "Yi-Fei Wang, Ming Zhang, Wen-Biao Liu", "docs_id": "1711.04403", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coexistence curve and molecule number density of AdS topological charged\n  black hole in massive gravity. The coexistence curve and molecule number density of a 4-dimensional AdS topological charged black hole in massive gravity is investigated. We find that the analytic expression of the coexistence curve in the reduced parameter space is dependent on theory parameters. This is very different from the previous results obtained in other modified gravity such as $f(R)$ gravity and Gauss-Bonnet gravity. Besides, we derive the explicit expression of the physical quantity which describes the difference of the number densities of AdS topological charged black hole molecules between the small and large black hole. It is observed that the difference of the molecule number densities is also dependent on theory parameters. Both the expressions of the coexistence curve and the difference of the molecule number densities can be reduced into a form which is similar to a RN-AdS black hole if the mass of graviton $m$ is zero. Moreover, we find the shifted temperature under massive gravity. This can highlight the important role played by the mass of graviton and other parameters in the phase transitions of AdS black holes in massive gravity."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of a 4-dimensional AdS topological charged black hole in massive gravity, which of the following statements is correct?\n\nA) The coexistence curve in the reduced parameter space is independent of theory parameters, similar to f(R) gravity and Gauss-Bonnet gravity.\n\nB) The difference in molecule number densities between small and large black holes is only affected by the mass of the graviton.\n\nC) When the mass of the graviton (m) approaches zero, the expressions for the coexistence curve and the difference in molecule number densities become similar to those of a Reissner-Nordstr\u00f6m-AdS (RN-AdS) black hole.\n\nD) The shifted temperature in massive gravity is independent of the mass of the graviton and other theory parameters.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"Both the expressions of the coexistence curve and the difference of the molecule number densities can be reduced into a form which is similar to a RN-AdS black hole if the mass of graviton m is zero.\" This directly supports the statement in option C.\n\nOption A is incorrect because the documentation explicitly mentions that the coexistence curve is dependent on theory parameters, which is different from f(R) gravity and Gauss-Bonnet gravity.\n\nOption B is incorrect as the documentation indicates that the difference in molecule number densities is dependent on theory parameters, not just the mass of the graviton.\n\nOption D is incorrect because the documentation states that the shifted temperature is affected by the mass of the graviton and other parameters, highlighting their important role in phase transitions."}, "33": {"documentation": {"title": "Carbon enrichment of the evolved stars in the Sagittarius dwarf\n  spheroidal", "source": "Iain McDonald, Jennifer R. White, Albert A. Zijlstra, Lizette Guzman\n  Ramirez, Cezary Szyszka, Jacobus Th. van Loon, Eric Lagadec, Olivia C. Jones", "docs_id": "1209.2563", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Carbon enrichment of the evolved stars in the Sagittarius dwarf\n  spheroidal. We present spectra of 1142 colour-selected stars in the direction of the Sagittarius Dwarf Spheroidal (Sgr dSph) galaxy, of which 1058 were taken with VLT/FLAMES multi-object spectrograph and 84 were taken with the SAAO Radcliffe 1.9-m telescope grating spectrograph. Spectroscopic membership is confirmed (at >99% confidence) for 592 stars on the basis of their radial velocity, and spectral types are given. Very slow rotation is marginally detected around the galaxy's major axis. We identify five S stars and 23 carbon stars, of which all but four carbon stars are newly-determined and all but one (PQ Sgr) are likely Sgr dSph members. We examine the onset of carbon-richness in this metal-poor galaxy in the context of stellar models. We compare the stellar death rate (one star per 1000-1700 years) to known planetary nebula dynamical ages and find that the bulk population produce the observed (carbon-rich) planetary nebulae. We compute average lifetimes of S and carbon stars as 60-250 and 130-500 kyr, compared to a total thermal-pulsing asymptotic giant branch lifetime of 530-1330 kyr. We conclude by discussing the return of carbon-rich material to the ISM."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Based on the spectroscopic study of the Sagittarius Dwarf Spheroidal (Sgr dSph) galaxy, which of the following statements is most accurate regarding the carbon stars identified in the survey?\n\nA) The study identified 23 carbon stars, all of which were previously known members of Sgr dSph.\nB) Out of the 23 carbon stars identified, 19 are newly-determined and likely members of Sgr dSph.\nC) The study found that carbon stars in Sgr dSph have an average lifetime of 530-1330 kyr.\nD) All identified carbon stars, except PQ Sgr, are confirmed members of Sgr dSph with 100% certainty.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the study identified 23 carbon stars, of which \"all but four carbon stars are newly-determined and all but one (PQ Sgr) are likely Sgr dSph members.\" This means that 19 carbon stars are newly-determined and likely members of Sgr dSph.\n\nOption A is incorrect because it states that all 23 carbon stars were previously known, which contradicts the information provided.\n\nOption C is incorrect because the given range (530-1330 kyr) actually refers to the total thermal-pulsing asymptotic giant branch lifetime, not specifically to carbon stars. The document states that the average lifetime of carbon stars is 130-500 kyr.\n\nOption D is incorrect because it implies absolute certainty of membership for all carbon stars except PQ Sgr, which is not supported by the text. The document uses the term \"likely\" for Sgr dSph membership, indicating a high probability but not absolute certainty."}, "34": {"documentation": {"title": "Near Optimal Online Distortion Minimization for Energy Harvesting Nodes", "source": "Ahmed Arafa, Sennur Ulukus", "docs_id": "1705.10305", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near Optimal Online Distortion Minimization for Energy Harvesting Nodes. We consider online scheduling for an energy harvesting communication system where a sensor node collects samples from a Gaussian source and sends them to a destination node over a Gaussian channel. The sensor is equipped with a finite-sized battery that is recharged by an independent and identically distributed (i.i.d.) energy harvesting process over time. The goal is to minimize the long term average distortion of the source samples received at the destination. We study two problems: the first is when sampling is cost-free, and the second is when there is a sampling cost incurred whenever samples are collected. We show that fixed fraction policies [Shaviv-Ozgur], in which a fixed fraction of the battery state is consumed in each time slot, are near-optimal in the sense that they achieve a long term average distortion that lies within a constant additive gap from the optimal solution for all energy arrivals and battery sizes. For the problem with sampling costs, the transmission policy is bursty; the sensor can collect samples and transmit for only a portion of the time."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of online scheduling for an energy harvesting communication system with a Gaussian source and channel, which of the following statements is most accurate regarding the fixed fraction policies?\n\nA) They achieve the optimal long-term average distortion for all energy arrivals and battery sizes.\n\nB) They are only effective when there is no sampling cost involved.\n\nC) They consume a variable fraction of the battery state in each time slot.\n\nD) They achieve a long-term average distortion within a constant additive gap from the optimal solution.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the key characteristics and performance of fixed fraction policies in the described energy harvesting communication system. Option A is incorrect because the policies are described as \"near-optimal,\" not optimal. Option B is false because the document doesn't limit their effectiveness to cost-free sampling scenarios. Option C contradicts the definition of fixed fraction policies, which use a fixed (not variable) fraction of the battery state. \n\nOption D is correct because it accurately reflects the statement in the document: \"fixed fraction policies [Shaviv-Ozgur], in which a fixed fraction of the battery state is consumed in each time slot, are near-optimal in the sense that they achieve a long term average distortion that lies within a constant additive gap from the optimal solution for all energy arrivals and battery sizes.\"\n\nThis question requires careful reading and interpretation of the given information, making it suitable for a challenging exam question."}, "35": {"documentation": {"title": "Different asymptotic behaviors of thick branes in mimetic gravity", "source": "Tao-Tao Sui, Yu-Peng Zhang, Bao-Min Gu, Yu-Xiao Liu", "docs_id": "2005.08438", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Different asymptotic behaviors of thick branes in mimetic gravity. In this paper, thick branes generated by mimetic scalar field with Lagrange multiplier formulation are investigated. We give three typical thick brane background solutions with different asymptotic behaviors and show that all the solutions are stable under tensor perturbations. The effective potentials of the tensor perturbations exhibit as volcano potential, P\\\"{o}schl-Teller potential, and harmonic oscillator potential for the three background solutions, respectively. All the tensor zero modes (massless gravitons) of the three cases can be localized on the brane. We also calculate the corrections to the Newtonian potential. On a large scale, the corrections to the Newtonian potential can be ignored. While on a small scale, the correction from the volcano-like potential is more pronounced than the other two cases. Combining the latest results of short-range gravity experiments that the usual Newtonian potential $\\propto1/r$ holds down to a length scale at $52\\mu$m, we get the constraint on the scale parameter as $k\\gtrsim 10^{-4}$eV, and constraint on the corresponding five-dimensional fundamental scale as $bM_\\ast \\gtrsim10^5$TeV."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A study on thick branes in mimetic gravity investigates three typical background solutions with different asymptotic behaviors. Which of the following statements accurately describes the findings and implications of this research?\n\nA) The effective potentials of tensor perturbations for all three solutions exhibit only volcano-like potentials, and the corrections to the Newtonian potential are significant at all scales.\n\nB) The tensor zero modes can be localized on the brane for only two of the three background solutions, and the corrections to the Newtonian potential are negligible at all scales.\n\nC) The effective potentials of tensor perturbations exhibit as volcano, P\u00f6schl-Teller, and harmonic oscillator potentials for the three solutions respectively. The corrections to the Newtonian potential are more pronounced for the volcano-like potential at small scales, and the scale parameter k is constrained to be less than 10^-4 eV.\n\nD) All three background solutions are stable under tensor perturbations, the tensor zero modes can be localized on the brane for all cases, and the corrections to the Newtonian potential are negligible on large scales but more pronounced for the volcano-like potential at small scales. The scale parameter k is constrained to be greater than or equal to 10^-4 eV.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D because it accurately summarizes the key findings of the research:\n\n1. All three background solutions are reported to be stable under tensor perturbations.\n2. The effective potentials of the tensor perturbations exhibit as volcano, P\u00f6schl-Teller, and harmonic oscillator potentials for the three solutions.\n3. The tensor zero modes (massless gravitons) can be localized on the brane for all three cases.\n4. The corrections to the Newtonian potential are negligible on large scales.\n5. At small scales, the correction from the volcano-like potential is more pronounced than the other two cases.\n6. Based on short-range gravity experiments, the scale parameter k is constrained to be greater than or equal to 10^-4 eV (k \u2273 10^-4 eV).\n\nOptions A, B, and C contain inaccuracies or incomplete information when compared to the given documentation."}, "36": {"documentation": {"title": "Reconstructing the Freeze-out State in Pb+Pb Collisions at 158 AGeV/c", "source": "Boris Tomasik (Regensburg), Urs Achim Wiedemann (Columbia University),\n  Ulrich Heinz (CERN)", "docs_id": "nucl-th/9907096", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconstructing the Freeze-out State in Pb+Pb Collisions at 158 AGeV/c. For a class of analytical parametrizations of the freeze-out state of relativistic heavy ion collisions, we perform a simultaneous analysis of the single-particle m_t-spectra and two-particle Bose-Einstein correlations measured in central Pb+Pb collisions at the CERN SPS. The analysis includes a full model parameter scan with chi^2 confidence levels. A comparison of different transverse density profiles for the particle emission region allows for a quantitative discussion of possible model dependencies of the results. Our fit results suggest a low thermal freeze-out temperature T approximately 95 +- 15 MeV and a large average transverse flow velocity of about 0.55c +- 0.07c. Moreover, the fit favours a box-shaped transverse density profile over a Gaussian one. We discuss the origins and the consequences of these results in detail. In order to reproduce the measured pion multiplicity our model requires a positive pion chemical potential. A study of the pion phase-space density indicates \\mu_\\pi approximately 60 MeV for T = 100 MeV."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the analysis of Pb+Pb collisions at 158 AGeV/c, which combination of parameters best describes the freeze-out state according to the study, and what does this imply about the collision dynamics?\n\nA) T \u2248 120 \u00b1 15 MeV, average transverse flow velocity \u2248 0.45c \u00b1 0.07c, Gaussian transverse density profile; implying a hotter but less rapidly expanding system\n\nB) T \u2248 95 \u00b1 15 MeV, average transverse flow velocity \u2248 0.55c \u00b1 0.07c, box-shaped transverse density profile; suggesting a cooler but more rapidly expanding system with a sharp edge\n\nC) T \u2248 95 \u00b1 15 MeV, average transverse flow velocity \u2248 0.45c \u00b1 0.07c, Gaussian transverse density profile; indicating a cooler system with moderate expansion and smooth edges\n\nD) T \u2248 120 \u00b1 15 MeV, average transverse flow velocity \u2248 0.55c \u00b1 0.07c, box-shaped transverse density profile; pointing to a hotter, rapidly expanding system with a sharp edge\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the study reports a low thermal freeze-out temperature T \u2248 95 \u00b1 15 MeV and a large average transverse flow velocity of about 0.55c \u00b1 0.07c. Additionally, the fit favors a box-shaped transverse density profile over a Gaussian one. This combination suggests a cooler but more rapidly expanding system with a well-defined edge to the emission region. This result is significant as it provides insights into the dynamics of the collision, indicating that the system undergoes substantial collective expansion before freezing out at a relatively low temperature."}, "37": {"documentation": {"title": "Traveling fronts in self-replicating persistent random walks with\n  multiple internal states", "source": "Keisuke Ishihara, Ashish B. George, Ryan Cornelius, Kirill S. Korolev", "docs_id": "2004.08313", "section": ["cond-mat.stat-mech", "nlin.PS", "q-bio.PE", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Traveling fronts in self-replicating persistent random walks with\n  multiple internal states. Self-activation coupled to a transport mechanism results in traveling waves that describe polymerization reactions, forest fires, tumor growth, and even the spread of epidemics. Diffusion is a simple and commonly used model of particle transport. Many physical and biological systems are, however, better described by persistent random walks that switch between multiple states of ballistic motion. So far, traveling fronts in persistent random walk models have only been analyzed in special, simplified cases. Here, we formulate the general model of reaction-transport processes in such systems and show how to compute the expansion velocity for arbitrary number of states. For the two-state model, we obtain a closed-form expression for the velocity and report how it is affected by different transport and replication parameters. We also show that nonzero death rates result in a discontinuous transition from quiescence to propagation. We compare our results to a recent observation of a discontinuous onset of propagation in microtubule asters and comment on the universal nature of the underlying mechanism."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of self-replicating persistent random walks with multiple internal states, which of the following statements is NOT correct?\n\nA) The model can be applied to describe phenomena such as polymerization reactions, forest fires, and the spread of epidemics.\n\nB) Diffusion is always the most accurate model for describing particle transport in physical and biological systems.\n\nC) The general model allows for the computation of expansion velocity for an arbitrary number of states.\n\nD) For the two-state model, nonzero death rates can lead to a discontinuous transition from quiescence to propagation.\n\nCorrect Answer: B\n\nExplanation:\nA is correct as the documentation explicitly states that the model can describe \"polymerization reactions, forest fires, tumor growth, and even the spread of epidemics.\"\n\nB is incorrect and thus the correct answer to the question. The documentation states that \"Many physical and biological systems are, however, better described by persistent random walks that switch between multiple states of ballistic motion\" rather than simple diffusion.\n\nC is correct as the text mentions \"Here, we formulate the general model of reaction-transport processes in such systems and show how to compute the expansion velocity for arbitrary number of states.\"\n\nD is correct as the documentation clearly states \"We also show that nonzero death rates result in a discontinuous transition from quiescence to propagation.\""}, "38": {"documentation": {"title": "Ultrametricity and Memory in a Solvable Model of Self-Organized\n  Criticality", "source": "Stefan Boettcher and Maya Paczuski (University of Oklahoma and\n  Brookhaven National Laboratory)", "docs_id": "cond-mat/9603018", "section": ["cond-mat", "nlin.AO", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ultrametricity and Memory in a Solvable Model of Self-Organized\n  Criticality. Slowly driven dissipative systems may evolve to a critical state where long periods of apparent equilibrium are punctuated by intermittent avalanches of activity. We present a self-organized critical model of punctuated equilibrium behavior in the context of biological evolution, and solve it in the limit that the number of independent traits for each species diverges. We derive an exact equation of motion for the avalanche dynamics from the microscopic rules. In the continuum limit, avalanches propagate via a diffusion equation with a nonlocal, history-dependent potential representing memory. This nonlocal potential gives rise to a non-Gaussian (fat) tail for the subdiffusive spreading of activity. The probability for the activity to spread beyond a distance $r$ in time $s$ decays as $\\sqrt{24\\over\\pi}s^{-3/2}x^{1/3} \\exp{[-{3\\over 4}x^{1/3}]}$ for $x={r^4\\over s} \\gg 1$. The potential represents a hierarchy of time scales that is dynamically generated by the ultrametric structure of avalanches, which can be quantified in terms of ``backward'' avalanches. In addition, a number of other correlation functions characterizing the punctuated equilibrium dynamics are determined exactly."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the solvable model of self-organized criticality described, what is the key characteristic of the equation governing avalanche propagation in the continuum limit, and how does it affect the spreading of activity?\n\nA) It's a standard diffusion equation with local interactions, leading to Gaussian spreading of activity.\n\nB) It's a wave equation with nonlinear terms, resulting in shock-like propagation of avalanches.\n\nC) It's a diffusion equation with a nonlocal, history-dependent potential, causing non-Gaussian (fat) tail subdiffusive spreading of activity.\n\nD) It's a fractional diffusion equation with time-fractional derivatives, producing anomalous diffusion of avalanches.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"In the continuum limit, avalanches propagate via a diffusion equation with a nonlocal, history-dependent potential representing memory.\" This nonlocal potential is crucial as it \"gives rise to a non-Gaussian (fat) tail for the subdiffusive spreading of activity.\" The other options are incorrect:\n\nA is wrong because the equation is not a standard diffusion equation with local interactions, and the spreading is explicitly stated to be non-Gaussian.\n\nB is incorrect as there's no mention of a wave equation or shock-like propagation.\n\nD, while plausible for anomalous diffusion, is not supported by the given information. The document doesn't mention fractional diffusion equations or time-fractional derivatives.\n\nThe correct answer C accurately reflects the described equation's nature and its effect on avalanche propagation, highlighting the model's complex dynamics and the importance of memory effects in the system's evolution."}, "39": {"documentation": {"title": "A Multi-Agent-Based Rolling Optimization Method for Restoration\n  Scheduling of Electrical Distribution Systems with Distributed Generation", "source": "Donghan Feng, Fan Wu, Yun Zhou, Usama Rahman, Xiaojin Zhao, Chen Fang", "docs_id": "1812.11356", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Multi-Agent-Based Rolling Optimization Method for Restoration\n  Scheduling of Electrical Distribution Systems with Distributed Generation. Resilience against major disasters is the most essential characteristic of future electrical distribution systems (EDS). A multi-agent-based rolling optimization method for EDS restoration scheduling is proposed in this paper. When a blackout occurs, considering the risk of losing the centralized authority due to the failure of the common core communication network, the agents available after disasters or cyber-attacks identify the communication-connected parts (CCPs) in the EDS with distributed communication. A multi-time interval optimization model is formulated and solved by the agents for the restoration scheduling of a CCP. A rolling optimization process for the entire EDS restoration is proposed. During the scheduling/rescheduling in the rolling process, the CCPs in the EDS are reidentified and the restoration schedules for the CCPs are updated. Through decentralized decision-making and rolling optimization, EDS restoration scheduling can automatically start and periodically update itself, providing effective solutions for EDS restoration scheduling in a blackout event. A modified IEEE 123-bus EDS is utilized to demonstrate the effectiveness of the proposed method."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the multi-agent-based rolling optimization method for electrical distribution system (EDS) restoration scheduling, what is the primary reason for identifying communication-connected parts (CCPs) after a blackout occurs?\n\nA) To establish a centralized control system for the entire EDS\nB) To minimize the risk of cyber-attacks on the distribution network\nC) To enable decentralized decision-making in case of failure of the core communication network\nD) To optimize the placement of distributed generation units within the EDS\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that \"When a blackout occurs, considering the risk of losing the centralized authority due to the failure of the common core communication network, the agents available after disasters or cyber-attacks identify the communication-connected parts (CCPs) in the EDS with distributed communication.\" This indicates that the primary reason for identifying CCPs is to enable decentralized decision-making when the centralized communication network fails.\n\nOption A is incorrect because the method aims to work without centralized control in case of network failure. Option B, while related to system security, is not the main reason for identifying CCPs in this context. Option D, although relevant to EDS operation, is not specifically related to the identification of CCPs in the restoration scheduling process described."}, "40": {"documentation": {"title": "An Experimental Investigation of the Scaling of Columnar Joints", "source": "Lucas Goehring, Zhenquan Lin, and Stephen W. Morris", "docs_id": "cond-mat/0606221", "section": ["cond-mat.soft", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Experimental Investigation of the Scaling of Columnar Joints. Columnar jointing is a fracture pattern common in igneous rocks in which cracks self-organize into a roughly hexagonal arrangement, leaving behind an ordered colonnade. We report observations of columnar jointing in a laboratory analog system, desiccated corn starch slurries. Using measurements of moisture density, evaporation rates, and fracture advance rates as evidence, we suggest an advective-diffusive system is responsible for the rough scaling behavior of columnar joints. This theory explains the order of magnitude difference in scales between jointing in lavas and in starches. We investigated the scaling of average columnar cross-sectional areas due to the evaporation rate, the analog of the cooling rate of igneous columnar joints. We measured column areas in experiments where the evaporation rate depended on lamp height and time, in experiments where the evaporation rate was fixed using feedback methods, and in experiments where gelatin was added to vary the rheology of the starch. Our results suggest that the column area at a particular depth is related to both the current conditions, and hysteretically to the geometry of the pattern at previous depths. We argue that there exists a range of stable column scales allowed for any particular evaporation rate."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the relationship between columnar joint scaling and evaporation rate in the corn starch analog system, as suggested by the research?\n\nA) Column area is solely determined by the current evaporation rate, with no influence from previous pattern geometries.\n\nB) Column area is inversely proportional to evaporation rate, with faster evaporation always resulting in smaller column cross-sections.\n\nC) Column area is directly proportional to evaporation rate, with faster evaporation always resulting in larger column cross-sections.\n\nD) Column area is related to both current evaporation conditions and hysteretically to the geometry of the pattern at previous depths, with a range of stable column scales possible for any particular evaporation rate.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The research suggests that the column area at a particular depth is related to both the current conditions (such as evaporation rate) and hysteretically to the geometry of the pattern at previous depths. This means that the history of the pattern formation influences its current state. Additionally, the researchers argue that there exists a range of stable column scales allowed for any particular evaporation rate, rather than a single fixed relationship.\n\nAnswer A is incorrect because it ignores the hysteretic effect of previous pattern geometries. Answers B and C are incorrect because they suggest a simple proportional relationship between evaporation rate and column area, which is not supported by the research findings. The actual relationship is more complex, involving both current conditions and historical factors, with a range of possible stable scales."}, "41": {"documentation": {"title": "Robustness and modular structure in networks", "source": "James P. Bagrow and Sune Lehmann and Yong-Yeol Ahn", "docs_id": "1102.5085", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robustness and modular structure in networks. Complex networks have recently attracted much interest due to their prevalence in nature and our daily lives [1, 2]. A critical property of a network is its resilience to random breakdown and failure [3-6], typically studied as a percolation problem [7-9] or by modeling cascading failures [10-12]. Many complex systems, from power grids and the Internet to the brain and society [13-15], can be modeled using modular networks comprised of small, densely connected groups of nodes [16, 17]. These modules often overlap, with network elements belonging to multiple modules [18, 19]. Yet existing work on robustness has not considered the role of overlapping, modular structure. Here we study the robustness of these systems to the failure of elements. We show analytically and empirically that it is possible for the modules themselves to become uncoupled or non-overlapping well before the network disintegrates. If overlapping modular organization plays a role in overall functionality, networks may be far more vulnerable than predicted by conventional percolation theory."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: According to the passage, which of the following statements best describes the novel insight presented about network robustness in modular systems with overlapping structures?\n\nA) Modular networks are inherently more robust than non-modular networks due to their densely connected groups.\n\nB) Conventional percolation theory accurately predicts the vulnerability of overlapping modular networks.\n\nC) The uncoupling of modules can occur before the entire network disintegrates, potentially compromising functionality earlier than expected.\n\nD) Overlapping modules always increase the overall robustness of complex networks against random failures.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage introduces a new perspective on network robustness, specifically for modular networks with overlapping structures. The key insight is that \"it is possible for the modules themselves to become uncoupled or non-overlapping well before the network disintegrates.\" This suggests that the functionality of the network might be compromised earlier than what conventional percolation theory would predict, as stated in the last sentence: \"If overlapping modular organization plays a role in overall functionality, networks may be far more vulnerable than predicted by conventional percolation theory.\"\n\nOption A is incorrect because while the passage mentions that modular networks are comprised of densely connected groups, it doesn't claim they are inherently more robust.\n\nOption B is directly contradicted by the passage, which states that conventional percolation theory may underestimate the vulnerability of these networks.\n\nOption D is too absolute and not supported by the passage. The text actually suggests that overlapping modules might make networks more vulnerable in certain ways."}, "42": {"documentation": {"title": "Evolutionary food web model based on body masses gives realistic\n  networks with permanent species turnover", "source": "Korinna T. Allhoff, Daniel Ritterskamp, Bj\\\"orn C. Rall, Barbara\n  Drossel, Christian Guill", "docs_id": "1409.3373", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolutionary food web model based on body masses gives realistic\n  networks with permanent species turnover. The networks of predator-prey interactions in ecological systems are remarkably complex, but nevertheless surprisingly stable in terms of long term persistence of the system as a whole. In order to understand the mechanism driving the complexity and stability of such food webs, we developed an eco-evolutionary model in which new species emerge as modifications of existing ones and dynamic ecological interactions determine which species are viable. The food-web structure thereby emerges from the dynamical interplay between speciation and trophic interactions. The proposed model is less abstract than earlier evolutionary food web models in the sense that all three evolving traits have a clear biological meaning, namely the average body mass of the individuals, the preferred prey body mass, and the width of their potential prey body mass spectrum. We observed networks with a wide range of sizes and structures and high similarity to natural food webs. The model networks exhibit a continuous species turnover, but massive extinction waves that affect more than $50 \\%$ of the network are not observed."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the evolutionary food web model described, which of the following combinations best represents the three evolving traits with clear biological meaning?\n\nA) Body size, prey abundance, and habitat preference\nB) Average body mass, preferred prey body mass, and width of potential prey body mass spectrum\nC) Lifespan, hunting strategy, and metabolic rate\nD) Population density, trophic level, and reproductive rate\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the key evolving traits in the model. Option B is correct because the documentation explicitly states that the three evolving traits with clear biological meaning are \"the average body mass of the individuals, the preferred prey body mass, and the width of their potential prey body mass spectrum.\" \n\nOption A is incorrect as it includes habitat preference, which is not mentioned as an evolving trait. Option C is incorrect as lifespan, hunting strategy, and metabolic rate are not specified as evolving traits in this model. Option D is incorrect because population density, trophic level, and reproductive rate are not mentioned as the evolving traits in the given information.\n\nThis question challenges students to carefully read and comprehend the specific details of the model's evolving traits, distinguishing them from other plausible but incorrect ecological factors."}, "43": {"documentation": {"title": "Maker-Breaker games on random geometric graphs", "source": "Andrew Beveridge, Andrzej Dudek, Alan Frieze, Tobias Muller, Milos\n  Stojakovic", "docs_id": "1309.5759", "section": ["math.CO", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Maker-Breaker games on random geometric graphs. In a Maker-Breaker game on a graph $G$, Breaker and Maker alternately claim edges of $G$. Maker wins if, after all edges have been claimed, the graph induced by his edges has some desired property. We consider four Maker-Breaker games played on random geometric graphs. For each of our four games we show that if we add edges between $n$ points chosen uniformly at random in the unit square by order of increasing edge-length then, with probability tending to one as $n\\to\\infty$, the graph becomes Maker-win the very moment it satisfies a simple necessary condition. In particular, with high probability, Maker wins the connectivity game as soon as the minimum degree is at least two; Maker wins the Hamilton cycle game as soon as the minimum degree is at least four; Maker wins the perfect matching game as soon as the minimum degree is at least two and every edge has at least three neighbouring vertices; and Maker wins the $H$-game as soon as there is a subgraph from a finite list of \"minimal graphs\". These results also allow us to give precise expressions for the limiting probability that $G(n,r)$ is Maker-win in each case, where $G(n,r)$ is the graph on $n$ points chosen uniformly at random on the unit square with an edge between two points if and only if their distance is at most $r$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a Maker-Breaker game on a random geometric graph G(n,r), which of the following statements is correct regarding the connectivity game?\n\nA) Maker wins as soon as the minimum degree is at least three.\nB) Maker wins when the graph has a Hamiltonian cycle.\nC) Maker wins as soon as the minimum degree is at least two.\nD) Maker wins when every edge has at least three neighboring vertices.\n\nCorrect Answer: C\n\nExplanation: According to the documentation, for the connectivity game played on random geometric graphs, Maker wins as soon as the minimum degree is at least two. This is stated explicitly: \"Maker wins the connectivity game as soon as the minimum degree is at least two.\" \n\nOption A is incorrect because it states a higher minimum degree than necessary. \nOption B is incorrect because it confuses the connectivity game with the Hamilton cycle game. \nOption D is incorrect because it describes a condition for the perfect matching game, not the connectivity game.\n\nThe correct answer, C, accurately reflects the necessary condition for Maker to win the connectivity game on random geometric graphs as described in the documentation."}, "44": {"documentation": {"title": "A self-starting bi-chromatic LiNbO3 soliton microcomb", "source": "Yang HE, Qi-Fan Yang, Jingwei Ling, Rui Luo, Hanxiao Liang, Mingxiao\n  Li, Boqiang Shen, Heming Wang, Kerry Vahala, and Qiang Lin", "docs_id": "1812.09610", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A self-starting bi-chromatic LiNbO3 soliton microcomb. For its many useful properties, including second and third-order optical nonlinearity as well as electro-optic control, lithium niobate is considered an important potential microcomb material. Here, a soliton microcomb is demonstrated in a monolithic high-Q lithium niobate resonator. Besides the demonstration of soliton mode locking, the photorefractive effect enables mode locking to self-start and soliton switching to occur bi-directionally. Second-harmonic generation of the soliton spectrum is also observed, an essential step for comb self-referencing. The Raman shock time constant of lithium niobate is also determined by measurement of soliton self-frequency-shift. Besides the considerable technical simplification provided by a self-starting soliton system, these demonstrations, together with the electro-optic and piezoelectric properties of lithium niobate, open the door to a multi-functional microcomb providing f-2f generation and fast electrical control of optical frequency and repetition rate, all of which are critical in applications including time keeping, frequency synthesis/division, spectroscopy and signal generation."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which combination of properties and capabilities makes lithium niobate (LiNbO3) particularly promising for microcomb applications, as demonstrated in the described research?\n\nA) Second-order nonlinearity, electro-optic control, and ability to generate terahertz radiation\nB) Third-order nonlinearity, piezoelectric properties, and intrinsic superconductivity\nC) Second and third-order nonlinearity, electro-optic control, and self-starting soliton mode locking\nD) Photorefractive effect, high thermal conductivity, and natural occurrence of quantum entanglement\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation highlights several key properties and capabilities of lithium niobate that make it promising for microcomb applications:\n\n1. Second and third-order optical nonlinearity: This is explicitly mentioned in the text as one of the useful properties of lithium niobate.\n\n2. Electro-optic control: The text mentions this as another important property of lithium niobate.\n\n3. Self-starting soliton mode locking: The research demonstrates that the photorefractive effect in lithium niobate enables mode locking to self-start.\n\nOption A is incorrect because while it includes some correct elements (second-order nonlinearity and electro-optic control), the ability to generate terahertz radiation is not mentioned in the given text.\n\nOption B is incorrect because although it mentions third-order nonlinearity and piezoelectric properties (which are correct), it falsely includes intrinsic superconductivity, which is not mentioned or relevant to the described microcomb applications.\n\nOption D is incorrect because, while it correctly mentions the photorefractive effect, it incorrectly includes high thermal conductivity and natural occurrence of quantum entanglement, which are not discussed in the given text as relevant properties for microcomb applications in lithium niobate."}, "45": {"documentation": {"title": "Uniform Post Selection Inference for LAD Regression and Other\n  Z-estimation problems", "source": "Alexandre Belloni and Victor Chernozhukov and Kengo Kato", "docs_id": "1304.0282", "section": ["math.ST", "econ.EM", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniform Post Selection Inference for LAD Regression and Other\n  Z-estimation problems. We develop uniformly valid confidence regions for regression coefficients in a high-dimensional sparse median regression model with homoscedastic errors. Our methods are based on a moment equation that is immunized against non-regular estimation of the nuisance part of the median regression function by using Neyman's orthogonalization. We establish that the resulting instrumental median regression estimator of a target regression coefficient is asymptotically normally distributed uniformly with respect to the underlying sparse model and is semi-parametrically efficient. We also generalize our method to a general non-smooth Z-estimation framework with the number of target parameters $p_1$ being possibly much larger than the sample size $n$. We extend Huber's results on asymptotic normality to this setting, demonstrating uniform asymptotic normality of the proposed estimators over $p_1$-dimensional rectangles, constructing simultaneous confidence bands on all of the $p_1$ target parameters, and establishing asymptotic validity of the bands uniformly over underlying approximately sparse models. Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal Score test; Uniformly valid inference; Z-estimation."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of high-dimensional sparse median regression with homoscedastic errors, which of the following statements is NOT correct regarding the instrumental median regression estimator of a target regression coefficient?\n\nA) It is asymptotically normally distributed.\nB) Its asymptotic normality is uniform with respect to the underlying sparse model.\nC) It achieves semi-parametric efficiency.\nD) It requires regular estimation of the nuisance part of the median regression function.\n\nCorrect Answer: D\n\nExplanation:\nA, B, and C are correct statements based on the given information. The document states that the instrumental median regression estimator is \"asymptotically normally distributed uniformly with respect to the underlying sparse model and is semi-parametrically efficient.\"\n\nD is incorrect because the method specifically uses Neyman's orthogonalization to immunize against non-regular estimation of the nuisance part of the median regression function. The document mentions \"a moment equation that is immunized against non-regular estimation of the nuisance part of the median regression function by using Neyman's orthogonalization.\" This implies that regular estimation of the nuisance part is not required, making D the incorrect statement."}, "46": {"documentation": {"title": "An efficient method for sorting and selecting for social behaviour", "source": "Alex Szorkovszky, Alexander Kotrschal, James E. Herbert Read, David\n  J.T. Sumpter, Niclas Kolm, Kristiaan Pelckmans", "docs_id": "1602.05833", "section": ["q-bio.QM", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An efficient method for sorting and selecting for social behaviour. In this article we provide a systematic experimental method for sorting animals according to socially relevant traits, without assaying them or even tagging them individually. Instead, they are repeatedly subjected to behavioural assays in groups, between which the group memberships are rearranged, in order to test the effect of many different combinations of individuals on a group-level property or feature. We analyse this method using a general model for the group feature, and simulate a variety of specific cases to track how individuals are sorted in each case. We find that in the case where the members of a group contribute equally to the group feature, the sorting procedure increases the between-group behavioural variation well above what is expected for groups randomly sampled from a population. For a wide class of group feature models, the individual phenotypes are efficiently sorted across the groups and thus become available for further analysis on how individual properties affect group behaviour. We also show that the experimental data can be used to estimate the individual-level repeatability of the underlying traits."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the described method for sorting animals according to socially relevant traits, which of the following statements is most accurate regarding the experimental process and its outcomes?\n\nA) The method requires individual tagging and assaying of animals to track their social behaviors accurately.\n\nB) The sorting procedure decreases between-group behavioral variation compared to randomly sampled groups from a population.\n\nC) The method is effective only when group members contribute unequally to the group feature being measured.\n\nD) The experimental data can be used to estimate individual-level repeatability of underlying traits without directly measuring individuals.\n\nCorrect Answer: D\n\nExplanation: \nOption A is incorrect because the method specifically does not require individual tagging or assaying, as stated in the text: \"without assaying them or even tagging them individually.\"\n\nOption B is the opposite of what the document states. The text mentions that the sorting procedure \"increases the between-group behavioural variation well above what is expected for groups randomly sampled from a population.\"\n\nOption C is incorrect because the method is described as effective when \"members of a group contribute equally to the group feature,\" not unequally.\n\nOption D is correct and directly supported by the last sentence of the provided text: \"We also show that the experimental data can be used to estimate the individual-level repeatability of the underlying traits.\" This demonstrates that the method can infer individual traits from group-level data without direct individual measurements."}, "47": {"documentation": {"title": "Toxicity Detection can be Sensitive to the Conversational Context", "source": "Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon,\n  Jeffrey Sorensen and Leo Laugier", "docs_id": "2111.10223", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Toxicity Detection can be Sensitive to the Conversational Context. User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on existing datasets will also tend to disregard context, making the detection of context-sensitive toxicity harder when it does occur. We construct and publicly release a dataset of 10,000 posts with two kinds of toxicity labels: (i) annotators considered each post with the previous one as context; and (ii) annotators had no additional context. Based on this, we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. We then evaluate machine learning systems on this task, showing that classifiers of practical quality can be developed, and we show that data augmentation with knowledge distillation can improve the performance further. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which often may be unnecessary and may otherwise introduce significant additional cost."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the primary challenge and proposed solution in context-sensitive toxicity detection, as outlined in the Arxiv documentation?\n\nA) Existing toxicity detection datasets are too large, so we need to reduce their size to improve accuracy.\n\nB) Context-sensitive toxic posts are common in current datasets, but detectors fail to recognize them, necessitating the development of more sophisticated AI models.\n\nC) Current toxicity detection datasets lack context-sensitive examples, leading to detectors that disregard context. The solution involves creating a new dataset with context-aware labels and developing classifiers for context sensitivity estimation.\n\nD) Toxicity detection is inherently subjective, so we should rely solely on human moderators rather than automated systems.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the main problem and proposed solution described in the documentation. The key points are:\n\n1. Current toxicity detection datasets rarely include posts whose toxicity depends on conversational context.\n2. This leads to toxicity detectors that tend to disregard context.\n3. The researchers created a new dataset of 10,000 posts with two types of toxicity labels: one considering the previous post as context, and one without context.\n4. They introduced a new task called \"context sensitivity estimation\" to identify posts whose perceived toxicity changes when context is considered.\n5. They developed and evaluated machine learning systems for this task, showing that practical classifiers can be created.\n\nThis approach aims to enhance toxicity detection datasets with more context-dependent posts and improve the efficiency of content moderation by identifying when considering parent posts is necessary."}, "48": {"documentation": {"title": "Associative Recall in Non-Randomly Diluted Neuronal Networks", "source": "Luciano da Fontoura Costa and Dietrich Stauffer", "docs_id": "cond-mat/0302040", "section": ["cond-mat.stat-mech", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Associative Recall in Non-Randomly Diluted Neuronal Networks. The potential for associative recall of diluted neuronal networks is investigated with respect to several biologically relevant configurations, more specifically the position of the cells along the input space and the spatial distribution of their connections. First we put the asymmetric Hopfield model onto a scale-free Barabasi-Albert network. Then, a geometrical diluted architecture, which maps from L-bit input patterns into $N$-neurons networks, with R=N/L<1 (we adopt R=0.1, 0.2 and 0.3), is considered. The distribution of the connections between cells along the one-dimensional input space follows a normal distribution centered at each cell, in the sense that cells that are closer to each other have increased probability to interconnect. The models also explicitly consider the placement of the neuronal cells along the input space in such a way that denser regions of that space tend to become denser, therefore implementing a special case of the Barabasi-Albert connecting scheme. The obtained results indicate that, for the case of the considered stimuli and noise, the network performance increases with the spatial uniformity of cell distribution."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of associative recall in non-randomly diluted neuronal networks, which of the following statements is most accurate regarding the network's performance?\n\nA) The network performance improves as the spatial distribution of neuronal cells becomes more clustered in certain regions of the input space.\n\nB) The network performance is independent of the spatial distribution of neuronal cells along the input space.\n\nC) The network performance increases with the spatial uniformity of cell distribution along the input space.\n\nD) The network performance is optimal when the distribution of connections between cells follows a uniform distribution rather than a normal distribution.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that \"The obtained results indicate that, for the case of the considered stimuli and noise, the network performance increases with the spatial uniformity of cell distribution.\" This directly supports the statement in option C.\n\nOption A is incorrect because it contradicts the findings. The study suggests that uniformity, not clustering, improves performance.\n\nOption B is incorrect because the documentation clearly indicates that the spatial distribution of cells does affect network performance.\n\nOption D is incorrect because the study specifically mentions using a normal distribution for connections between cells, not a uniform distribution. The documentation states, \"The distribution of the connections between cells along the one-dimensional input space follows a normal distribution centered at each cell.\"\n\nThis question tests the student's ability to carefully read and interpret the research findings, distinguishing between the various factors that influence network performance in this specific study of associative recall in diluted neuronal networks."}, "49": {"documentation": {"title": "Wave control through soft microstructural curling: bandgap shifting,\n  reconfigurable anisotropy and switchable chirality", "source": "Paolo Celli, Stefano Gonella, Vahid Tajeddini, Anastasia Muliana, Saad\n  Ahmed, Zoubeida Ounaies", "docs_id": "1609.08404", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wave control through soft microstructural curling: bandgap shifting,\n  reconfigurable anisotropy and switchable chirality. In this work, we discuss and numerically validate a strategy to attain reversible macroscopic changes in the wave propagation characteristics of cellular metamaterials with soft microstructures. The proposed cellular architecture is characterized by unit cells featuring auxiliary populations of symmetrically-distributed smart cantilevers stemming from the nodal locations. Through an external stimulus (the application of an electric field), we induce extreme, localized, reversible curling deformation of the cantilevers---a shape modification which does not affect the overall shape, stiffness and load bearing capability of the structure. By carefully engineering the spatial pattern of straight (non activated) and curled (activated) cantilevers, we can induce several profound modifications of the phononic characteristics of the structure: generation and/or shifting of total and partial bandgaps, cell symmetry relaxation (which implies reconfigurable wave beaming), and chirality switching. While in this work we discuss the specific case of composite cantilevers with a PDMS core and active layers of electrostrictive terpolymer P(VDF-TrFE-CTFE), the strategy can be extended to other smart materials (such as dielectric elastomers or shape-memory polymers)."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the primary mechanism and outcome of the wave control strategy discussed in the Arxiv documentation?\n\nA) The strategy involves large-scale deformation of the entire cellular structure, resulting in changes to the overall stiffness and load-bearing capacity of the metamaterial.\n\nB) The approach uses thermally-activated shape memory alloys to induce global shape changes in the metamaterial, leading to bandgap generation.\n\nC) The method employs localized, reversible curling of microstructural cantilevers through electric field application, enabling bandgap shifting and reconfigurable anisotropy without altering the structure's overall properties.\n\nD) The technique relies on chemical reactions within the PDMS core to trigger irreversible changes in the cellular architecture, resulting in permanent wave propagation modifications.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately describes the key aspects of the wave control strategy presented in the documentation. The method uses \"extreme, localized, reversible curling deformation of the cantilevers\" activated by an electric field. This microstructural change allows for \"generation and/or shifting of total and partial bandgaps\" and \"cell symmetry relaxation (which implies reconfigurable wave beaming)\" without affecting \"the overall shape, stiffness and load bearing capability of the structure.\"\n\nAnswer A is incorrect because the strategy specifically avoids large-scale deformation and maintains the overall structural properties. Answer B is wrong as it mentions thermally-activated shape memory alloys, which are not discussed in the given text. The document actually mentions electrostrictive materials. Answer D is incorrect because the changes are reversible, not irreversible, and the PDMS core is not described as undergoing chemical reactions to induce the changes."}, "50": {"documentation": {"title": "Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction", "source": "Chenhan Yuan, Ryan Rossi, Andrew Katz, and Hoda Eldardiry", "docs_id": "2009.12683", "section": ["cs.LG", "cs.CL", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction. The models of n-ary cross sentence relation extraction based on distant supervision assume that consecutive sentences mentioning n entities describe the relation of these n entities. However, on one hand, this assumption introduces noisy labeled data and harms the models' performance. On the other hand, some non-consecutive sentences also describe one relation and these sentences cannot be labeled under this assumption. In this paper, we relax this strong assumption by a weaker distant supervision assumption to address the second issue and propose a novel sentence distribution estimator model to address the first problem. This estimator selects correctly labeled sentences to alleviate the effect of noisy data is a two-level agent reinforcement learning model. In addition, a novel universal relation extractor with a hybrid approach of attention mechanism and PCNN is proposed such that it can be deployed in any tasks, including consecutive and nonconsecutive sentences. Experiments demonstrate that the proposed model can reduce the impact of noisy data and achieve better performance on general n-ary cross sentence relation extraction task compared to baseline models."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following best describes the novel approach proposed in the paper to address the limitations of existing n-ary cross-sentence relation extraction models?\n\nA) A universal relation extractor using only PCNN without attention mechanism\nB) A sentence distribution estimator model based on three-level agent reinforcement learning\nC) A two-level agent reinforcement learning model combined with a universal relation extractor using attention mechanism and PCNN\nD) A model that strictly adheres to the assumption that only consecutive sentences mentioning n entities describe their relation\n\nCorrect Answer: C\n\nExplanation: The paper proposes a novel approach that combines two key elements:\n\n1. A sentence distribution estimator model based on two-level agent reinforcement learning to address the issue of noisy labeled data. This model selects correctly labeled sentences to alleviate the effect of noisy data.\n\n2. A universal relation extractor that uses a hybrid approach of attention mechanism and PCNN (Piecewise Convolutional Neural Networks). This extractor is designed to be deployable in any tasks, including both consecutive and non-consecutive sentences.\n\nOption A is incorrect because the universal relation extractor uses both attention mechanism and PCNN, not just PCNN.\n\nOption B is incorrect because the reinforcement learning model is two-level, not three-level.\n\nOption D is incorrect because the paper explicitly relaxes the strong assumption about consecutive sentences and proposes a weaker distant supervision assumption to address non-consecutive sentences that describe a relation.\n\nOption C correctly combines the two main novel elements proposed in the paper: the two-level agent reinforcement learning model for sentence distribution estimation and the universal relation extractor with attention mechanism and PCNN."}, "51": {"documentation": {"title": "Numerical study of shock formation in the dispersionless\n  Kadomtsev-Petviashvili equation and dispersive regularizations", "source": "Christian Klein, Kristelle Roidot", "docs_id": "1304.6513", "section": ["math.AP", "math.NA", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical study of shock formation in the dispersionless\n  Kadomtsev-Petviashvili equation and dispersive regularizations. The formation of singularities in solutions to the dispersionless Kadomtsev-Petviashvili (dKP) equation is studied numerically for different classes of initial data. The asymptotic behavior of the Fourier coefficients is used to quantitatively identify the critical time and location and the type of the singularity. The approach is first tested in detail in 1+1 dimensions for the known case of the Hopf equation, where it is shown that the break-up of the solution can be identified with prescribed accuracy. For dissipative regularizations of this shock formation as the Burgers' equation and for dispersive regularizations as the Korteweg-de Vries equation, the Fourier coefficients indicate as expected global regularity of the solutions. The Kadomtsev-Petviashvili (KP) equation can be seen as a dispersive regularization of the dKP equation. The behavior of KP solutions for small dispersion parameter $\\epsilon\\ll 1$ near a break-up of corresponding dKP solutions is studied. It is found that the difference between KP and dKP solutions for the same initial data at the critical point scales roughly as $\\epsilon^{2/7}$ as for the Korteweg-de Vries equation."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the numerical study of shock formation in the dispersionless Kadomtsev-Petviashvili (dKP) equation, which of the following statements is correct regarding the comparison between the Kadomtsev-Petviashvili (KP) equation and the dKP equation near the break-up point?\n\nA) The KP equation behaves identically to the dKP equation near the break-up point, regardless of the dispersion parameter \u03b5.\n\nB) The difference between KP and dKP solutions at the critical point scales approximately as \u03b5^(1/3), similar to the Burgers' equation.\n\nC) The KP equation always prevents shock formation, resulting in a completely different solution compared to the dKP equation.\n\nD) The difference between KP and dKP solutions for the same initial data at the critical point scales roughly as \u03b5^(2/7), analogous to the Korteweg-de Vries equation.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"The behavior of KP solutions for small dispersion parameter \u03b5\u226a1 near a break-up of corresponding dKP solutions is studied. It is found that the difference between KP and dKP solutions for the same initial data at the critical point scales roughly as \u03b5^(2/7) as for the Korteweg-de Vries equation.\" This directly supports option D and contradicts the other options.\n\nOption A is incorrect because the KP equation, being a dispersive regularization of the dKP equation, does not behave identically to the dKP equation near the break-up point.\n\nOption B is incorrect because the scaling relationship mentioned (\u03b5^(1/3)) is not supported by the given information and is not associated with the Burgers' equation in this context.\n\nOption C is too extreme. While the KP equation is a dispersive regularization of the dKP equation, it doesn't completely prevent shock formation or result in a completely different solution. The scaling relationship indicates that there is a difference, but it's related to the dispersion parameter \u03b5."}, "52": {"documentation": {"title": "Theory of antiferroelectric phase transitions", "source": "Pierre Tol\\'edano and Mael Guennou", "docs_id": "1601.05687", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theory of antiferroelectric phase transitions. At variance with structural ferroic phase transitions which give rise to macroscopic tensors coupled to macroscopic fields, criteria defining antiferroelectric (AFE) phase transitions are still under discussion due to the absence of specific symmetry properties characterizing their existence. They are recognized by the proximity of a ferroelectric (FE) phase induced under applied electric field, with a double hysteresis loop relating the induced polarization to the electric field and a typical anomaly of the dielectric permittivity. Here, we show that there exist indeed symmetry criteria defining AFE transitions. They relate the local symmetry of the polar crystallographic sites emerging at an AFE phase transition with the macroscopic symmetry of the AFE phase. The dielectric properties of AFE transitions are deduced from a Landau theoretical model in which ferroelectric and ferrielectric phases are shown to stabilize as the result of specific symmetry-allowed couplings of the AFE order- parameter with the field-induced polarization."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the key insight about antiferroelectric (AFE) phase transitions presented in this research?\n\nA) AFE transitions are characterized by the absence of any symmetry properties, making them fundamentally different from ferroic transitions.\n\nB) The proximity of a ferroelectric phase and a double hysteresis loop are sufficient criteria to fully define and identify AFE transitions.\n\nC) There exist symmetry criteria defining AFE transitions, relating local symmetry of polar sites to the macroscopic symmetry of the AFE phase.\n\nD) AFE transitions can only be understood through their dielectric properties, without any consideration of symmetry or structural changes.\n\nCorrect Answer: C\n\nExplanation: The key insight presented in this research is that there are indeed symmetry criteria that define antiferroelectric (AFE) phase transitions, contrary to previous beliefs. These criteria relate the local symmetry of the polar crystallographic sites that emerge during an AFE transition to the macroscopic symmetry of the AFE phase. This finding provides a more fundamental understanding of AFE transitions based on symmetry considerations, going beyond the previously recognized characteristics such as the proximity of a ferroelectric phase, double hysteresis loops, and dielectric anomalies.\n\nOption A is incorrect because the research actually identifies symmetry properties for AFE transitions. Option B is incomplete, as it only describes observable characteristics but not the underlying symmetry criteria. Option D is incorrect because the research emphasizes the importance of symmetry criteria, not just dielectric properties."}, "53": {"documentation": {"title": "Active Damping of Power Oscillations Following Frequency Changes in Low\n  Inertia Power Systems", "source": "Marios Zarifakis, William T. Coffey, Yuri P. Kalmykov, Serguey V.\n  Titov, Declan J. Byrne, and Stephen J. Carrig", "docs_id": "1908.04405", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active Damping of Power Oscillations Following Frequency Changes in Low\n  Inertia Power Systems. The absolute requirement to increase the amount of energy generation from renewable sources e.g. predominantly asynchronously connected wind turbines and photovoltaic installations, may in practice during transient events (where frequency changes are examined) excite oscillatory response of the power output of large grid connected synchronous-generators. The response of such generators must be controlled either by varying the applied torque of a turbine or by altering the electromagnetic torque in the airgap. Choosing the latter, the adequacy of a voltage regulator, particularly that of the embedded Power System Stabilizer (PSS) circuit, is investigated using the IEEE PSS1A model for the automatic voltage regulator of a synchronous generator driven by a gas turbine. The response is obtained via closed form analytic solutions for both small (linear) and large (nonlinear) scale transient events in the energy grid system. In tandem with the analytical study, the behavior simulated with a computer model from MatLab-SimPowerSystems is reviewed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In low-inertia power systems with increased renewable energy generation, what method is investigated to actively dampen power oscillations following frequency changes, and what specific model is used in the study?\n\nA) Varying the applied torque of wind turbines using the IEEE PSS2B model\nB) Altering the electromagnetic torque in the airgap using the IEEE PSS1A model for the automatic voltage regulator\nC) Adjusting the pitch angle of photovoltaic panels using the IEEE PSS3A model\nD) Modifying the fuel input of gas turbines using the IEEE PSS4B model\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of key concepts from the document. The correct answer is B because the study investigates altering the electromagnetic torque in the airgap as a method to control generator response, specifically using the IEEE PSS1A model for the automatic voltage regulator of a synchronous generator driven by a gas turbine. \n\nOption A is incorrect because while varying applied torque is mentioned, it's not the chosen method, and PSS2B is not the model used. \nOption C is incorrect as it introduces concepts not mentioned in the document (pitch angle of PV panels and PSS3A model). \nOption D is incorrect because although gas turbines are mentioned, modifying fuel input is not the focus, and PSS4B is not the referenced model.\n\nThis question requires synthesizing information from different parts of the text and identifying the specific methods and models used in the study."}, "54": {"documentation": {"title": "Reconstructing firm-level interactions: the Dutch input-output network", "source": "Leonardo Niccol\\`o Ialongo, Camille de Valk, Emiliano Marchese, Fabian\n  Jansen, Hicham Zmarrou, Tiziano Squartini, Diego Garlaschelli", "docs_id": "2111.15248", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconstructing firm-level interactions: the Dutch input-output network. Recent crises have shown that the knowledge of the structure of input-output networks at the firm level is crucial when studying economic resilience from the microscopic point of view of firms that rewire their connections under supply and demand shocks. Unfortunately, empirical inter-firm network data are rarely accessible and protected by confidentiality. The available methods of network reconstruction from partial information, which have been devised for financial exposures, are inadequate for inter-firm relationships because they treat all pairs of nodes as potentially interacting, thereby overestimating the rewiring capabilities of the system. Here we use two big data sets of transactions in the Netherlands to represent a large portion of the Dutch inter-firm network and document the properties of one of the few analysed networks of this kind. We, then, introduce a generalized maximum-entropy reconstruction method that preserves the production function of each firm in the data, i.e. the input and output flows of each node for each product type. We confirm that the new method becomes increasingly more reliable as a finer product resolution is considered and can therefore be used as a generative model of inter-firm networks with fine production constraints. The likelihood of the model, being related to the entropy, proxies the rewiring capability of the system for a fixed input-output configuration."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In the context of reconstructing firm-level interactions in the Dutch input-output network, which of the following statements best describes the advantages of the newly introduced generalized maximum-entropy reconstruction method over existing methods?\n\nA) It eliminates the need for empirical inter-firm network data entirely.\nB) It treats all pairs of nodes as potentially interacting, maximizing rewiring capabilities.\nC) It preserves the production function of each firm while becoming more reliable with finer product resolution.\nD) It focuses solely on financial exposures between firms, ignoring product-specific interactions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage introduces a new generalized maximum-entropy reconstruction method that preserves the production function of each firm in the data, including input and output flows for each product type. This method becomes increasingly reliable as finer product resolution is considered, making it suitable as a generative model for inter-firm networks with detailed production constraints.\n\nOption A is incorrect because the method still relies on some empirical data, it doesn't eliminate the need entirely.\n\nOption B is actually a criticism of existing methods mentioned in the passage, which overestimate rewiring capabilities by treating all node pairs as potentially interacting.\n\nOption D is incorrect because the new method explicitly considers product-specific interactions, not just financial exposures.\n\nThe key advantage of this new method is its ability to model more realistic inter-firm relationships by preserving specific production functions and improving with finer product resolution, which is captured in option C."}, "55": {"documentation": {"title": "Prompt charmonia production and polarization at LHC in the NRQCD with\n  $k_T$-factorization. Part III: $J/\\psi$ meson", "source": "S.P. Baranov, A.V. Lipatov", "docs_id": "1611.10141", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prompt charmonia production and polarization at LHC in the NRQCD with\n  $k_T$-factorization. Part III: $J/\\psi$ meson. In the framework of $k_T$-factorization approach, the production and polarization of prompt $J/\\psi$ mesons at the LHC energies is studied. Our consideration is based on the non-relativistic QCD formalism for bound states and off-shell amplitudes for hard partonic subprocesses. Both the direct production mechanism and feed-down contributions from $\\chi_c$ and $\\psi(2S)$ decays are taken into account. The transverse momentum dependent (or unintegrated) gluon densities in a proton were derived from Ciafaloni-Catani-Fiorani-Marchesini evolution equation or, alternatively, were chosen in accordance with Kimber-Martin-Ryskin prescription. The non-perturbative color-octet matrix elements were first deduced from the fits to the latest CMS data on $J/\\psi$ transverse momentum distributions and then applied to describe the ATLAS and LHCb data on $J/\\psi$ production and polarization at $\\sqrt s = 7$, $8$ and $13$ TeV. We perform an estimation of polarization parameters $\\lambda_\\theta$, $\\lambda_\\phi$ and $\\lambda_{\\theta \\phi}$ which determine $J/\\psi$ spin density matrix and demonstrate that treating the soft gluon emission as a series of explicit color-electric dipole transitions within NRQCD leads to unpolarized $J/\\psi$ production at high transverse momenta, that is in qualitative agreement with the LHC data."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements accurately describes the approach and findings of the study on prompt J/\u03c8 meson production and polarization at LHC energies?\n\nA) The study uses the collinear factorization approach and finds that J/\u03c8 mesons are highly polarized at high transverse momenta.\n\nB) The research employs the kT-factorization approach, considers only direct production mechanisms, and concludes that J/\u03c8 mesons are unpolarized at low transverse momenta.\n\nC) The study utilizes the kT-factorization approach, incorporates both direct and feed-down production mechanisms, and demonstrates that J/\u03c8 mesons are unpolarized at high transverse momenta.\n\nD) The research uses the NRQCD formalism without considering off-shell amplitudes and finds that J/\u03c8 mesons have strong transverse polarization at all momentum ranges.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key aspects of the study described in the documentation. The research uses the kT-factorization approach along with NRQCD formalism for bound states and off-shell amplitudes for hard partonic subprocesses. It considers both direct production and feed-down contributions from \u03c7c and \u03c8(2S) decays. Most importantly, the study demonstrates that treating soft gluon emission as a series of explicit color-electric dipole transitions within NRQCD leads to unpolarized J/\u03c8 production at high transverse momenta, which is in qualitative agreement with LHC data.\n\nOption A is incorrect because it mentions collinear factorization instead of kT-factorization and incorrectly states high polarization at high transverse momenta. Option B is wrong because it only considers direct production and incorrectly states unpolarization at low transverse momenta. Option D is incorrect as it doesn't mention the kT-factorization approach, ignores off-shell amplitudes, and wrongly claims strong transverse polarization at all momentum ranges."}, "56": {"documentation": {"title": "Fused Density Estimation: Theory and Methods", "source": "Robert Bassett and James Sharpnack", "docs_id": "1805.03288", "section": ["stat.ME", "math.OC", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fused Density Estimation: Theory and Methods. In this paper we introduce a method for nonparametric density estimation on geometric networks. We define fused density estimators as solutions to a total variation regularized maximum-likelihood density estimation problem. We provide theoretical support for fused density estimation by proving that the squared Hellinger rate of convergence for the estimator achieves the minimax bound over univariate densities of log-bounded variation. We reduce the original variational formulation in order to transform it into a tractable, finite-dimensional quadratic program. Because random variables on geometric networks are simple generalizations of the univariate case, this method also provides a useful tool for univariate density estimation. Lastly, we apply this method and assess its performance on examples in the univariate and geometric network setting. We compare the performance of different optimization techniques to solve the problem, and use these results to inform recommendations for the computation of fused density estimators."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the theoretical support and computational approach for fused density estimation as presented in the paper?\n\nA) It achieves the minimax bound over multivariate densities and is solved using a linear programming approach.\n\nB) It achieves the minimax bound over univariate densities of log-bounded variation and is solved using a finite-dimensional quadratic program.\n\nC) It achieves the minimax bound over geometric network densities and is solved using a stochastic gradient descent algorithm.\n\nD) It achieves the minimax bound over all possible density functions and is solved using a closed-form analytical solution.\n\nCorrect Answer: B\n\nExplanation: The paper states that the squared Hellinger rate of convergence for the fused density estimator achieves the minimax bound over univariate densities of log-bounded variation. This provides theoretical support for the method. \n\nFurthermore, the authors mention that they reduce the original variational formulation to transform it into a tractable, finite-dimensional quadratic program. This computational approach allows for practical implementation of the method.\n\nOption A is incorrect because the paper focuses on univariate densities, not multivariate, and uses quadratic programming, not linear programming.\n\nOption C is incorrect because while the method can be applied to geometric networks, the minimax bound is specifically mentioned for univariate densities. Additionally, the paper does not mention using stochastic gradient descent.\n\nOption D is incorrect because the minimax bound is not claimed for all possible density functions, and the problem is solved numerically through quadratic programming, not through a closed-form analytical solution."}, "57": {"documentation": {"title": "Frequentist Coverage Properties of Uncertainty Intervals for Weak\n  Poisson Signals in the Presence of Background", "source": "K. J. Coakley, J. D. Splett, D. S. Simons", "docs_id": "0804.4032", "section": ["physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Frequentist Coverage Properties of Uncertainty Intervals for Weak\n  Poisson Signals in the Presence of Background. We construct uncertainty intervals for weak Poisson signals in the presence of background. We consider the case where a primary experiment yields a realization of the signal plus background, and a second experiment yields a realization of the background. The data acquisitions times for the background-only experiment,T_bg, and the primary experiment,T, are selected so that their ratio varies from 1 to 25. The expected number of background counts in the primary experiment varies from 0.2 to 2. We construct 90 and 95 percent confidence intervals based on a propagation-of-errors method as well as two implementations of a Neyman procedure where acceptance regions are constructed based on a likelihood-ratio criterion that automatically determines whether the resulting confidence interval is one-sided or two-sided. The first Neyman procedure (due to Feldman and Cousins) neglects uncertainty in the background. In the other Neyman procedure, we account for uncertainty in the background with a parametric bootstrap method. We also construct minimum length Bayesian credibility intervals. For each method, we test for the presence of a signal based on the value of the lower endpoint of the uncertainty interval. When T_bg/T is 5 or more and the expected background is 2 or less, the Feldman Cousins method outperforms the other methods considered."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In a study of weak Poisson signals with background, researchers construct uncertainty intervals using various methods. Which of the following statements is correct regarding the performance of these methods?\n\nA) The propagation-of-errors method consistently outperforms all other methods across all background levels and T_bg/T ratios.\n\nB) The Bayesian credibility intervals provide the best coverage properties regardless of the background level and T_bg/T ratio.\n\nC) The Feldman-Cousins method, which neglects background uncertainty, performs best when T_bg/T is 5 or more and the expected background is 2 or less.\n\nD) The Neyman procedure with parametric bootstrap always provides superior results compared to the Feldman-Cousins method, regardless of experimental conditions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage explicitly states, \"When T_bg/T is 5 or more and the expected background is 2 or less, the Feldman Cousins method outperforms the other methods considered.\" This directly supports the statement in option C.\n\nOption A is incorrect because the passage doesn't indicate that the propagation-of-errors method consistently outperforms other methods.\n\nOption B is incorrect as the text doesn't suggest that Bayesian credibility intervals provide the best coverage properties in all scenarios.\n\nOption D is incorrect because the passage doesn't state that the Neyman procedure with parametric bootstrap always outperforms the Feldman-Cousins method. In fact, it suggests that under certain conditions (T_bg/T \u2265 5 and expected background \u2264 2), the Feldman-Cousins method performs better than other methods, including the Neyman procedure with parametric bootstrap."}, "58": {"documentation": {"title": "Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data", "source": "Jeya Maria Jose V., Rajeev Yasarla, Puyang Wang, Ilker Hacihaliloglu,\n  Vishal M. Patel", "docs_id": "1912.08364", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data. Automatic segmentation of anatomical landmarks from ultrasound (US) plays an important role in the management of preterm neonates with a very low birth weight due to the increased risk of developing intraventricular hemorrhage (IVH) or other complications. One major problem in developing an automatic segmentation method for this task is the limited availability of annotated data. To tackle this issue, we propose a novel image synthesis method using multi-scale self attention generator to synthesize US images from various segmentation masks. We show that our method can synthesize high-quality US images for every manipulated segmentation label with qualitative and quantitative improvements over the recent state-of-the-art synthesis methods. Furthermore, for the segmentation task, we propose a novel method, called Confidence-guided Brain Anatomy Segmentation (CBAS) network, where segmentation and corresponding confidence maps are estimated at different scales. In addition, we introduce a technique which guides CBAS to learn the weights based on the confidence measure about the estimate. Extensive experiments demonstrate that the proposed method for both synthesis and segmentation tasks achieve significant improvements over the recent state-of-the-art methods. In particular, we show that the new synthesis framework can be used to generate realistic US images which can be used to improve the performance of a segmentation algorithm."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following best describes the primary challenge and the proposed solution in developing an automatic segmentation method for brain anatomy in preterm neonates using 2D ultrasound?\n\nA) Challenge: Excessive data availability; Solution: Data reduction techniques\nB) Challenge: Limited annotated data; Solution: Novel image synthesis method using multi-scale self attention generator\nC) Challenge: Poor image quality; Solution: Advanced image enhancement algorithms\nD) Challenge: Lack of computing power; Solution: Cloud-based processing systems\n\nCorrect Answer: B\n\nExplanation: The primary challenge mentioned in the text is the limited availability of annotated data for developing an automatic segmentation method for brain anatomy in preterm neonates. To address this issue, the researchers propose a novel image synthesis method using a multi-scale self attention generator to synthesize ultrasound images from various segmentation masks. This approach allows them to generate more training data, which can improve the performance of segmentation algorithms. Options A, C, and D are not mentioned as primary challenges or solutions in the given text."}, "59": {"documentation": {"title": "Coulomb breakup reactions of $^{11}$Li in the coupled-channel\n  $^9$Li~+~$n$~+~$n$ three-body model", "source": "Yuma Kikuchi, Takayuki Myo, Kiyoshi Kato, Kiyomi Ikeda", "docs_id": "1302.3004", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coulomb breakup reactions of $^{11}$Li in the coupled-channel\n  $^9$Li~+~$n$~+~$n$ three-body model. We investigate the three-body Coulomb breakup of a two-neutron halo nucleus $^{11}$Li. We use the coupled-channel $^9$Li + $n$ + $n$ three-body model, which includes the coupling between last neutron states and the various $2p$-$2h$ configurations in $^9$Li due to the tensor and pairing correlations. The three-body scattering states of $^{11}$Li are described by using the combined methods of the complex scaling and the Lippmann-Schwinger equation. The calculated breakup cross section successfully reproduces the experiments. The large mixing of the s-state in the halo ground state of $^{11}$Li is shown to play an important role in explanation of shape and strength of the breakup cross section. In addition, we predict the invariant mass spectra for binary subsystems of $^{11}$Li. It is found that the two kinds of virtual s-states of $^9$Li-$n$ and $n$-$n$ systems in the final three-body states of $^{11}$Li largely contribute to make low-lying peaks in the invariant mass spectra. On the other hand, in the present analysis, it is suggested that the contributions of the p-wave resonances of $^{10}$Li is hardly confirmed in the spectra."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the coupled-channel $^9$Li + $n$ + $n$ three-body model of $^{11}$Li Coulomb breakup reactions, which of the following statements is correct regarding the contributions to the invariant mass spectra?\n\nA) The p-wave resonances of $^{10}$Li significantly contribute to the spectra\nB) The d-wave states of $^9$Li-$n$ system dominate the low-lying peaks\nC) Virtual s-states of both $^9$Li-$n$ and $n$-$n$ systems contribute to low-lying peaks\nD) The f-wave states of the three-body final state are responsible for the spectral shape\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"two kinds of virtual s-states of $^9$Li-$n$ and $n$-$n$ systems in the final three-body states of $^{11}$Li largely contribute to make low-lying peaks in the invariant mass spectra.\" \n\nOption A is incorrect because the text suggests that \"the contributions of the p-wave resonances of $^{10}$Li is hardly confirmed in the spectra.\"\n\nOptions B and D are distractors that mention wave states not discussed in the given text, making them incorrect.\n\nThis question tests the understanding of the complex nuclear physics concepts presented in the documentation, particularly the contributions to the invariant mass spectra in the three-body model of $^{11}$Li Coulomb breakup reactions."}}