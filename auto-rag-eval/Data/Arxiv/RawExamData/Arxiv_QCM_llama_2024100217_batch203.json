{"0": {"documentation": {"title": "Probabilistic Autoencoder using Fisher Information", "source": "Johannes Zacherl (1 and 2), Philipp Frank (1 and 2) and Torsten A.\n  En{\\ss}lin (1 and 2) ((1) Max-Planck Institut f\\\"ur Astrophysik (2)\n  Ludwig-Maximilians-Universit\\\"at M\\\"unchen)", "docs_id": "2110.14947", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Probabilistic Autoencoder using Fisher Information. Neural Networks play a growing role in many science disciplines, including physics. Variational Autoencoders (VAEs) are neural networks that are able to represent the essential information of a high dimensional data set in a low dimensional latent space, which have a probabilistic interpretation. In particular the so-called encoder network, the first part of the VAE, which maps its input onto a position in latent space, additionally provides uncertainty information in terms of a variance around this position. In this work, an extension to the Autoencoder architecture is introduced, the FisherNet. In this architecture, the latent space uncertainty is not generated using an additional information channel in the encoder, but derived from the decoder, by means of the Fisher information metric. This architecture has advantages from a theoretical point of view as it provides a direct uncertainty quantification derived from the model, and also accounts for uncertainty cross-correlations. We can show experimentally that the FisherNet produces more accurate data reconstructions than a comparable VAE and its learning performance also apparently scales better with the number of latent space dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the FisherNet architecture over traditional Variational Autoencoders (VAEs) in terms of uncertainty quantification?\n\n**A)** FisherNet provides a more complex encoder network that can capture non-linear relationships in the data.\n**B)** FisherNet uses an additional information channel in the encoder to generate uncertainty, which is not present in traditional VAEs.\n**C)** FisherNet derives uncertainty from the decoder using the Fisher information metric, providing a direct and theoretically sound approach to uncertainty quantification.\n**D)** FisherNet only works with high-dimensional data sets, while traditional VAEs can handle low-dimensional data.\n\n**Correct Answer:** C) FisherNet derives uncertainty from the decoder using the Fisher information metric, providing a direct and theoretically sound approach to uncertainty quantification.\n\n**Explanation:** The correct answer is C) because the documentation states that the FisherNet architecture derives uncertainty from the decoder using the Fisher information metric, which provides a direct and theoretically sound approach to uncertainty quantification. This is in contrast to traditional VAEs, which generate uncertainty using an additional information channel in the encoder. The other options are incorrect because they do not accurately reflect the advantages of the FisherNet architecture. Option A is incorrect because the documentation does not mention the complexity of the encoder network as an advantage of FisherNet. Option B is incorrect because the documentation states that FisherNet does not use an additional information channel in the encoder. Option D is incorrect because the documentation does not mention the dimensionality of the data as a requirement for FisherNet."}, "1": {"documentation": {"title": "Development of a Miniaturized Deformable Mirror Controller", "source": "Eduardo Bendek, Dana Lynch, Eugene Pluzhnik, Ruslan Belikov, Benjamin\n  Klamm, Elizabeth Hyde, and Katherine Mumm", "docs_id": "1703.07450", "section": ["astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Development of a Miniaturized Deformable Mirror Controller. High-Performance Adaptive Optics systems are rapidly spreading as useful applications in the fields of astronomy, ophthalmology, and telecommunications. This technology is critical to enable coronagraphic direct imaging of exoplanets utilized in ground-based telescopes and future space missions such as WFIRST, EXO-C, HabEx, and LUVOIR. We have developed a miniaturized Deformable Mirror controller to enable active optics on small space imaging mission. The system is based on the Boston Micromachines Corporation Kilo-DM, which is one of the most widespread DMs on the market. The system has three main components: The Deformable Mirror, the Driving Electronics, and the Mechanical and Heat management. The system is designed to be extremely compact and have low- power consumption to enable its use not only on exoplanet missions, but also in a wide-range of applications that require precision optical systems, such as direct line-of-sight laser communications, and guidance systems. The controller is capable of handling 1,024 actuators with 220V maximum dynamic range, 16bit resolution, and 14bit accuracy, and operating at up to 1kHz frequency. The system fits in a 10x10x5cm volume, weighs less than 0.5kg, and consumes less than 8W. We have developed a turnkey solution reducing the risk for currently planned as well as future missions, lowering their cost by significantly reducing volume, weight and power consumption of the wavefront control hardware."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary advantages of the miniaturized Deformable Mirror controller developed in this study, and how do they impact the feasibility of its use in various applications?\n\n**A)** The system's compact size and low power consumption enable its use in space missions, but its limited dynamic range and resolution make it unsuitable for high-precision applications.\n\n**B)** The controller's ability to handle 1,024 actuators with 220V maximum dynamic range, 16bit resolution, and 14bit accuracy, and operating at up to 1kHz frequency, makes it an ideal solution for a wide range of applications, including direct line-of-sight laser communications and guidance systems.\n\n**C)** The system's weight and volume are significantly reduced, but its high cost and complexity make it unsuitable for use in low-cost, low-precision applications.\n\n**D)** The controller's compact size and low power consumption are beneficial for space missions, but its limited frequency range and resolution make it unsuitable for applications requiring high-speed and high-precision control.\n\n**Correct Answer:** B) The controller's ability to handle 1,024 actuators with 220V maximum dynamic range, 16bit resolution, and 14bit accuracy, and operating at up to 1kHz frequency, makes it an ideal solution for a wide range of applications, including direct line-of-sight laser communications and guidance systems.\n\n**Explanation:** The correct answer highlights the key advantages of the miniaturized Deformable Mirror controller, including its ability to handle a large number of actuators, high dynamic range, and high resolution, as well as its low power consumption and compact size. These features make it an ideal solution for a wide range of applications, including direct line-of-sight laser communications and guidance systems. The other options are incorrect because they either downplay the controller's advantages (A and C) or misrepresent its capabilities (D)."}, "2": {"documentation": {"title": "Semi-supervised Anomaly Detection using AutoEncoders", "source": "Manpreet Singh Minhas, John Zelek", "docs_id": "2001.03674", "section": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semi-supervised Anomaly Detection using AutoEncoders. Anomaly detection refers to the task of finding unusual instances that stand out from the normal data. In several applications, these outliers or anomalous instances are of greater interest compared to the normal ones. Specifically in the case of industrial optical inspection and infrastructure asset management, finding these defects (anomalous regions) is of extreme importance. Traditionally and even today this process has been carried out manually. Humans rely on the saliency of the defects in comparison to the normal texture to detect the defects. However, manual inspection is slow, tedious, subjective and susceptible to human biases. Therefore, the automation of defect detection is desirable. But for defect detection lack of availability of a large number of anomalous instances and labelled data is a problem. In this paper, we present a convolutional auto-encoder architecture for anomaly detection that is trained only on the defect-free (normal) instances. For the test images, residual masks that are obtained by subtracting the original image from the auto-encoder output are thresholded to obtain the defect segmentation masks. The approach was tested on two data-sets and achieved an impressive average F1 score of 0.885. The network learnt to detect the actual shape of the defects even though no defected images were used during the training."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional anomaly detection methods in industrial optical inspection and infrastructure asset management, and how does the proposed convolutional auto-encoder architecture address this limitation?\n\n**A)** Traditional methods are limited by the availability of large amounts of labeled data, whereas the proposed architecture is limited by the need for manual inspection. \n**B)** Traditional methods are limited by the subjective nature of human inspection, whereas the proposed architecture is limited by the lack of availability of anomalous instances. \n**C)** Traditional methods are limited by the slow and tedious nature of manual inspection, whereas the proposed architecture is limited by the need for human expertise. \n**D)** Traditional methods are limited by the lack of automation, whereas the proposed architecture is limited by the need for manual thresholding. \n\n**Correct Answer:** B) Traditional methods are limited by the subjective nature of human inspection, whereas the proposed architecture is limited by the lack of availability of anomalous instances.\n\n**Explanation:** The question requires the test-taker to understand the limitations of traditional anomaly detection methods in industrial optical inspection and infrastructure asset management, as mentioned in the documentation. The correct answer, B, highlights the subjective nature of human inspection as a limitation of traditional methods, and the lack of availability of anomalous instances as a limitation of the proposed architecture. The test-taker needs to analyze the pros and cons of each option to arrive at the correct answer."}, "3": {"documentation": {"title": "Price Jump Prediction in Limit Order Book", "source": "Ban Zheng, Eric Moulines, Fr\\'ed\\'eric Abergel", "docs_id": "1204.1381", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Price Jump Prediction in Limit Order Book. A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we define price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of different features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we find that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Price Jump Prediction in Limit Order Book paper, what is the primary advantage of using LASSO logistic regression for predicting price jumps, and how does it contribute to the analysis of limit order book data?\n\n**A)** LASSO logistic regression helps to reduce overfitting by selecting the most relevant features, allowing for more accurate predictions of price jumps. However, it may also lead to underfitting if not enough features are selected.\n\n**B)** LASSO logistic regression is particularly useful for identifying the importance of individual features in predicting price jumps, enabling researchers to highlight the most influential variables. This approach also helps to avoid multicollinearity issues.\n\n**C)** LASSO logistic regression is a variant of logistic regression that can handle high-dimensional data, making it an ideal choice for analyzing the complex features of limit order book data. By selecting the most relevant features, LASSO logistic regression can improve the accuracy of price jump predictions.\n\n**D)** LASSO logistic regression is a type of regularization technique that can help to prevent overfitting by adding a penalty term to the loss function. This approach can lead to more stable and generalizable models, but may also require careful tuning of hyperparameters.\n\n**Correct Answer:** B) LASSO logistic regression is particularly useful for identifying the importance of individual features in predicting price jumps, enabling researchers to highlight the most influential variables. This approach also helps to avoid multicollinearity issues.\n\n**Explanation:** The correct answer highlights the key advantage of using LASSO logistic regression in the context of the Price Jump Prediction in Limit Order Book paper. By selecting the most relevant features, LASSO logistic regression can identify the importance of individual variables in predicting price jumps, allowing researchers to highlight the most influential factors. This approach also helps to avoid multicollinearity issues, which can arise when multiple features are highly correlated with each other."}, "4": {"documentation": {"title": "Minority games played by arbitrageurs on the energy market", "source": "Tim Ritmeester and Hildegard Meyer-Ortmanns", "docs_id": "2012.10475", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Minority games played by arbitrageurs on the energy market. Along with the energy transition, the energy markets change their organization toward more decentralized and self-organized structures, striving for locally optimal profits. These tendencies may endanger the physical grid stability. One realistic option is the exhaustion of reserve energy due to an abuse by arbitrageurs. We map the energy market to different versions of a minority game and determine the expected amount of arbitrage as well as its fluctuations as a function of the model parameters. Of particular interest are the impact of heterogeneous contributions of arbitrageurs, the interplay between external stochastic events and nonlinear price functions of reserve power, and the effect of risk aversion due to suspected penalties. The non-monotonic dependence of arbitrage on the control parameters reveals an underlying phase transition that is the counterpart to replica symmetry breaking in spin glasses. As conclusions from our results we propose economic and statutory measures to counteract a detrimental effect of arbitrage."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the potential impact of minority games played by arbitrageurs on the energy market, considering the effects of heterogeneous contributions, external stochastic events, and risk aversion. How do these factors influence the expected amount of arbitrage and its fluctuations, and what are the implications for physical grid stability?\n\n**A)** The expected amount of arbitrage is directly proportional to the number of arbitrageurs, while its fluctuations are independent of the model parameters. This is because the arbitrageurs' contributions are homogeneous and unaffected by external events.\n\n**B)** The non-monotonic dependence of arbitrage on the control parameters reveals a phase transition that is analogous to replica symmetry breaking in spin glasses. This phase transition is driven by the interplay between heterogeneous contributions and external stochastic events.\n\n**C)** The proposed economic and statutory measures to counteract the detrimental effect of arbitrage are ineffective, as they do not address the underlying causes of the problem. The measures should focus on regulating the behavior of arbitrageurs rather than their incentives.\n\n**D)** The energy market's transition towards decentralized and self-organized structures is a necessary condition for the emergence of minority games played by arbitrageurs. This transition is driven by the need for locally optimal profits, which in turn leads to the exhaustion of reserve energy due to arbitrage.\n\n**Correct Answer:** B) The non-monotonic dependence of arbitrage on the control parameters reveals a phase transition that is analogous to replica symmetry breaking in spin glasses. This phase transition is driven by the interplay between heterogeneous contributions and external stochastic events.\n\n**Explanation:** The correct answer is based on the analysis of the documentation, which highlights the importance of heterogeneous contributions, external stochastic events, and risk aversion in the emergence of minority games played by arbitrageurs. The non-monotonic dependence of arbitrage on the control parameters is a key feature of the model, and its interpretation as a phase transition analogous to replica symmetry breaking in spin glasses is a crucial insight into the underlying dynamics of the system."}, "5": {"documentation": {"title": "Signatures of chiral dynamics in the Nucleon to Delta transition", "source": "Tobias A. Gail and Thomas R. Hemmert", "docs_id": "nucl-th/0512082", "section": ["nucl-th", "hep-lat", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signatures of chiral dynamics in the Nucleon to Delta transition. Utilizing the methods of chiral effective field theory we present an analysis of the electromagnetic $N\\Delta$-transition current in the framework of the non-relativistic \"small scale expansion\" (SSE) to leading-one-loop order. We discuss the momentum dependence of the magnetic dipole, electric quadrupole and coulomb quadrupole transition form factors up to a momentum transfer of $Q^2<0.3$ GeV$^2$. Particular emphasis is put on the identification of the role of chiral dynamics in this transition. Our analysis indicates that there is indeed non-trivial momentum dependence in the two quadrupole form factors at small $Q^2<0.15$ GeV$^2$ arising from long distance pion physics, leading for example to negative radii in the (real part of the) quadrupole transition form factors. We compare our results with the EMR($Q^2$) and CMR($Q^2$) multipole-ratios from pion-electroproduction experiments and find a remarkable agreement up to four-momentum transfer of $Q^2\\approx 0.3$ GeV$^2$. Finally, we discuss the chiral extrapolation of the three transition form factors at $Q^2=0$, identifying rapid changes in the (real part of the) quark-mass dependence of the quadrupole transition moments for pion masses below 200 MeV, which arise again from long distance pion dynamics. Our findings indicate that dipole extrapolation methods currently used in lattice QCD analyses of baryon form factors are not applicable for the chiral extrapolation of $N\\Delta$ quadrupole transition form factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the chiral extrapolation of the N\u0394 quadrupole transition form factors, and how does it relate to the applicability of dipole extrapolation methods in lattice QCD analyses of baryon form factors?\n\nA) The chiral extrapolation of N\u0394 quadrupole transition form factors indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses below 200 MeV, which arises from long distance pion dynamics, and this change is not captured by dipole extrapolation methods.\n\nB) The chiral extrapolation of N\u0394 quadrupole transition form factors indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses above 200 MeV, which arises from long distance pion dynamics, and this change is captured by dipole extrapolation methods.\n\nC) The chiral extrapolation of N\u0394 quadrupole transition form factors indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses below 200 MeV, which arises from short distance quark dynamics, and this change is not captured by dipole extrapolation methods.\n\nD) The chiral extrapolation of N\u0394 quadrupole transition form factors indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses above 200 MeV, which arises from short distance quark dynamics, and this change is captured by dipole extrapolation methods.\n\nCorrect Answer: A) The chiral extrapolation of N\u0394 quadrupole transition form factors indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses below 200 MeV, which arises from long distance pion dynamics, and this change is not captured by dipole extrapolation methods.\n\nExplanation: The correct answer is A) because the documentation states that the analysis indicates a rapid change in the quark-mass dependence of the quadrupole transition moments for pion masses below 200 MeV, which arises from long distance pion dynamics. This change is not captured by dipole extrapolation methods, which is the primary conclusion regarding the chiral extrapolation of the N\u0394 quadrupole transition form factors."}, "6": {"documentation": {"title": "Connecting the dots across time: Reconstruction of single cell signaling\n  trajectories using time-stamped data", "source": "Sayak Mukherjee, David Stewart, William Stewart, Lewis L. Lanier,\n  Jayajit Das", "docs_id": "1609.08035", "section": ["q-bio.QM", "cond-mat.stat-mech", "cs.CG", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Connecting the dots across time: Reconstruction of single cell signaling\n  trajectories using time-stamped data. Single cell responses are shaped by the geometry of signaling kinetic trajectories carved in a multidimensional space spanned by signaling protein abundances. It is however challenging to assay large number (>3) of signaling species in live-cell imaging which makes it difficult to probe single cell signaling kinetic trajectories in large dimensions. Flow and mass cytometry techniques can measure a large number (4 - >40) of signaling species but are unable to track single cells. Thus cytometry experiments provide detailed time stamped snapshots of single cell signaling kinetics. Is it possible to use the time stamped cytometry data to reconstruct single cell signaling trajectories? Borrowing concepts of conserved and slow variables from non-equilibrium statistical physics we develop an approach to reconstruct signaling trajectories using snapshot data by creating new variables that remain invariant or vary slowly during the signaling kinetics. We apply this approach to reconstruct trajectories using snapshot data obtained from in silico simulations and live-cell imaging measurements. The use of invariants and slow variables to reconstruct trajectories provides a radically different way to track object using snapshot data. The approach is likely to have implications for solving matching problems in a wide range of disciplines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of flow and mass cytometry techniques in reconstructing single cell signaling trajectories, and how does the proposed approach address this limitation?\n\nA) Flow and mass cytometry techniques are unable to track single cells due to limitations in data processing algorithms, but the proposed approach uses machine learning to improve tracking accuracy.\nB) Flow and mass cytometry techniques are unable to track single cells because they require a large number of signaling species to be measured simultaneously, whereas the proposed approach uses invariants and slow variables to reduce the dimensionality of the data.\nC) Flow and mass cytometry techniques are unable to track single cells due to the need for high-speed imaging, whereas the proposed approach uses snapshot data to reconstruct trajectories.\nD) Flow and mass cytometry techniques are unable to track single cells because they are limited by the number of cells that can be measured simultaneously, whereas the proposed approach uses in silico simulations to generate large datasets.\n\n**Correct Answer:** B) Flow and mass cytometry techniques are unable to track single cells because they require a large number of signaling species to be measured simultaneously, whereas the proposed approach uses invariants and slow variables to reduce the dimensionality of the data.\n\n**Explanation:** The correct answer is B because the text states that flow and mass cytometry techniques can measure a large number of signaling species, but are unable to track single cells. The proposed approach addresses this limitation by creating new variables that remain invariant or vary slowly during the signaling kinetics, effectively reducing the dimensionality of the data and allowing for the reconstruction of single cell signaling trajectories."}, "7": {"documentation": {"title": "Automatic 4D Facial Expression Recognition via Collaborative\n  Cross-domain Dynamic Image Network", "source": "Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao", "docs_id": "1905.02319", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automatic 4D Facial Expression Recognition via Collaborative\n  Cross-domain Dynamic Image Network. This paper proposes a novel 4D Facial Expression Recognition (FER) method using Collaborative Cross-domain Dynamic Image Network (CCDN). Given a 4D data of face scans, we first compute its geometrical images, and then combine their correlated information in the proposed cross-domain image representations. The acquired set is then used to generate cross-domain dynamic images (CDI) via rank pooling that encapsulates facial deformations over time in terms of a single image. For the training phase, these CDIs are fed into an end-to-end deep learning model, and the resultant predictions collaborate over multi-views for performance gain in expression classification. Furthermore, we propose a 4D augmentation scheme that not only expands the training data scale but also introduces significant facial muscle movement patterns to improve the FER performance. Results from extensive experiments on the commonly used BU-4DFE dataset under widely adopted settings show that our proposed method outperforms the state-of-the-art 4D FER methods by achieving an accuracy of 96.5% indicating its effectiveness."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed Collaborative Cross-domain Dynamic Image Network (CCDN) method in the paper \"Automatic 4D Facial Expression Recognition via Collaborative Cross-domain Dynamic Image Network\"?\n\n**A)** To improve the accuracy of 2D facial expression recognition by combining multiple views.\n**B)** To generate cross-domain dynamic images (CDI) that encapsulate facial deformations over time in a single image.\n**C)** To develop a 4D augmentation scheme that expands the training data scale and introduces significant facial muscle movement patterns.\n**D)** To train an end-to-end deep learning model that collaborates over multi-views for performance gain in expression classification.\n\n**Correct Answer:** D) To train an end-to-end deep learning model that collaborates over multi-views for performance gain in expression classification.\n\n**Explanation:** The correct answer is D) because the paper proposes a novel 4D Facial Expression Recognition (FER) method using CCDN, which involves training an end-to-end deep learning model that collaborates over multi-views to achieve performance gain in expression classification. The other options are incorrect because they describe specific components or techniques used in the paper, but not the primary goal of the CCDN method."}, "8": {"documentation": {"title": "Prediction of Zoonosis Incidence in Human using Seasonal Auto Regressive\n  Integrated Moving Average (SARIMA)", "source": "Adhistya Erna Permanasari, Dayang Rohaya Awang Rambli, Dhanapal Durai\n  Dominic", "docs_id": "0910.0820", "section": ["cs.LG", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prediction of Zoonosis Incidence in Human using Seasonal Auto Regressive\n  Integrated Moving Average (SARIMA). Zoonosis refers to the transmission of infectious diseases from animal to human. The increasing number of zoonosis incidence makes the great losses to lives, including humans and animals, and also the impact in social economic. It motivates development of a system that can predict the future number of zoonosis occurrences in human. This paper analyses and presents the use of Seasonal Autoregressive Integrated Moving Average (SARIMA) method for developing a forecasting model that able to support and provide prediction number of zoonosis human incidence. The dataset for model development was collected on a time series data of human tuberculosis occurrences in United States which comprises of fourteen years of monthly data obtained from a study published by Centers for Disease Control and Prevention (CDC). Several trial models of SARIMA were compared to obtain the most appropriate model. Then, diagnostic tests were used to determine model validity. The result showed that the SARIMA(9,0,14)(12,1,24)12 is the fittest model. While in the measure of accuracy, the selected model achieved 0.062 of Theils U value. It implied that the model was highly accurate and a close fit. It was also indicated the capability of final model to closely represent and made prediction based on the tuberculosis historical dataset."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a system that can predict the future number of zoonosis occurrences in humans, and how does the SARIMA model address this challenge?\n\n**A)** The increasing number of zoonosis incidence makes great losses to lives, including humans and animals, and also impacts social and economic factors. The SARIMA model is used to analyze and present the use of SARIMA for developing a forecasting model that can support and provide predictions on the number of zoonosis human incidence.\n\n**B)** The Centers for Disease Control and Prevention (CDC) published a study on human tuberculosis occurrences in the United States, which comprises of fourteen years of monthly data. The SARIMA model is used to compare several trial models to obtain the most appropriate model.\n\n**C)** The SARIMA model is used to determine the validity of the model by performing diagnostic tests, and the result showed that the SARIMA(9,0,14)(12,1,24)12 is the fittest model.\n\n**D)** The Theils U value is used to measure the accuracy of the selected model, which achieved 0.062, indicating that the model was highly accurate and a close fit.\n\n**Correct Answer:** A) The increasing number of zoonosis incidence makes great losses to lives, including humans and animals, and also impacts social and economic factors. The SARIMA model is used to analyze and present the use of SARIMA for developing a forecasting model that can support and provide predictions on the number of zoonosis human incidence.\n\n**Explanation:** The question requires the candidate to understand the primary motivation behind the development of a system that can predict zoonosis occurrences in humans. The correct answer, A, highlights the significant impact of zoonosis incidence on human lives and the economy. The SARIMA model is used to address this challenge by providing a forecasting model that can support and provide predictions on the number of zoonosis human incidence. The other options, B, C, and D, are incorrect because they focus on specific aspects of the SARIMA model, such as the CDC study, diagnostic tests, or accuracy measures, rather than the primary motivation behind its development."}, "9": {"documentation": {"title": "In All Likelihood, Deep Belief Is Not Enough", "source": "Lucas Theis, Sebastian Gerwinn, Fabian Sinz and Matthias Bethge", "docs_id": "1011.6086", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "In All Likelihood, Deep Belief Is Not Enough. Statistical models of natural stimuli provide an important tool for researchers in the fields of machine learning and computational neuroscience. A canonical way to quantitatively assess and compare the performance of statistical models is given by the likelihood. One class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data are deep belief networks. Analyses of these models, however, have been typically limited to qualitative analyses based on samples due to the computationally intractable nature of the model likelihood. Motivated by these circumstances, the present article provides a consistent estimator for the likelihood that is both computationally tractable and simple to apply in practice. Using this estimator, a deep belief network which has been suggested for the modeling of natural image patches is quantitatively investigated and compared to other models of natural image patches. Contrary to earlier claims based on qualitative results, the results presented in this article provide evidence that the model under investigation is not a particularly good model for natural images"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the article \"In All Likelihood, Deep Belief Is Not Enough\" aim to provide a consistent estimator for the likelihood of deep belief networks. What is the primary motivation behind this effort, and what type of data are these models typically applied to?\n\n**A)** The authors aim to provide a consistent estimator for the likelihood to improve the interpretability of deep belief networks, and these models are typically applied to synthetic data.\n\n**B)** The authors aim to provide a consistent estimator for the likelihood to make deep belief networks more computationally efficient, and these models are typically applied to natural image patches.\n\n**C)** The authors aim to provide a consistent estimator for the likelihood to facilitate the use of deep belief networks in computer vision applications, and these models are typically applied to text data.\n\n**D)** The authors aim to provide a consistent estimator for the likelihood to simplify the training process of deep belief networks, and these models are typically applied to time series data.\n\n**Correct Answer:** B) The authors aim to provide a consistent estimator for the likelihood to make deep belief networks more computationally efficient, and these models are typically applied to natural image patches.\n\n**Explanation:** The correct answer is B because the article states that the authors aim to provide a consistent estimator for the likelihood to make deep belief networks more computationally tractable, which is a key motivation for their work. Additionally, the article mentions that deep belief networks have been applied to a variety of complex data, including natural image patches, which supports this answer."}, "10": {"documentation": {"title": "Tensor-Based Fusion of EEG and FMRI to Understand Neurological Changes\n  in Schizophrenia", "source": "Evrim Acar, Yuri Levin-Schwartz, Vince D. Calhoun and T\\\"ulay Adal{\\i}", "docs_id": "1612.02189", "section": ["stat.AP", "q-bio.NC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tensor-Based Fusion of EEG and FMRI to Understand Neurological Changes\n  in Schizophrenia. Neuroimaging modalities such as functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) provide information about neurological functions in complementary spatiotemporal resolutions; therefore, fusion of these modalities is expected to provide better understanding of brain activity. In this paper, we jointly analyze fMRI and multi-channel EEG signals collected during an auditory oddball task with the goal of capturing brain activity patterns that differ between patients with schizophrenia and healthy controls. Rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel EEG signals, we preserve the multi-way structure of EEG data and use a coupled matrix and tensor factorization (CMTF) model to jointly analyze fMRI and EEG signals. Our analysis reveals that (i) joint analysis of EEG and fMRI using a CMTF model can capture meaningful temporal and spatial signatures of patterns that behave differently in patients and controls, and (ii) these differences and the interpretability of the associated components increase by including multiple electrodes from frontal, motor and parietal areas, but not necessarily by including all electrodes in the analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a coupled matrix and tensor factorization (CMTF) model to jointly analyze fMRI and EEG signals, as proposed in the paper \"Tensor-Based Fusion of EEG and FMRI to Understand Neurological Changes in Schizophrenia\"?\n\n**A)** It allows for the selection of a single electrode to represent multi-channel EEG signals.\n**B)** It preserves the multi-way structure of EEG data and enables the capture of meaningful temporal and spatial signatures of patterns that behave differently in patients and controls.\n**C)** It increases the interpretability of the associated components by including multiple electrodes from frontal, motor, and parietal areas.\n**D)** It reduces the computational complexity of the analysis by reducing the dimensionality of the data.\n\n**Correct Answer:** B) It preserves the multi-way structure of EEG data and enables the capture of meaningful temporal and spatial signatures of patterns that behave differently in patients and controls.\n\n**Explanation:** The correct answer is B) because the paper states that \"Rather than selecting a single electrode or matricizing the third-order tensor that can be naturally used to represent multi-channel EEG signals, we preserve the multi-way structure of EEG data and use a coupled matrix and tensor factorization (CMTF) model to jointly analyze fMRI and EEG signals.\" This indicates that the primary advantage of using the CMTF model is that it preserves the multi-way structure of EEG data, allowing for the capture of meaningful temporal and spatial signatures of patterns that behave differently in patients and controls."}, "11": {"documentation": {"title": "A wavelet analysis of inter-dependence, contagion and long memory among\n  global equity markets", "source": "Avishek Bhandari", "docs_id": "2003.14110", "section": ["econ.EM", "nlin.CD", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A wavelet analysis of inter-dependence, contagion and long memory among\n  global equity markets. This study attempts to investigate into the structure and features of global equity markets from a time-frequency perspective. An analysis grounded on this framework allows one to capture information from a different dimension, as opposed to the traditional time domain analyses, where multiscale structures of financial markets are clearly extracted. In financial time series, multiscale features manifest themselves due to presence of multiple time horizons. The existence of multiple time horizons necessitates a careful investigation of each time horizon separately as market structures are not homogenous across different time horizons. The presence of multiple time horizons, with varying levels of complexity, requires one to investigate financial time series from a heterogeneous market perspective where market players are said to operate at different investment horizons. This thesis extends the application of time-frequency based wavelet techniques to: i) analyse the interdependence of global equity markets from a heterogeneous investor perspective with a special focus on the Indian stock market, ii) investigate the contagion effect, if any, of financial crises on Indian stock market, and iii) to study fractality and scaling properties of global equity markets and analyse the efficiency of Indian stock markets using wavelet based long memory methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the study, as described in the thesis, in analyzing global equity markets from a time-frequency perspective?\n\nA) To investigate the impact of economic indicators on stock prices\nB) To analyze the interdependence of global equity markets from a heterogeneous investor perspective\nC) To study the fractality and scaling properties of global equity markets\nD) To examine the efficiency of Indian stock markets using wavelet based long memory methods\n\n**A)** B) To analyze the interdependence of global equity markets from a heterogeneous investor perspective\n**B)** C) To study the fractality and scaling properties of global equity markets\n**C)** D) To examine the efficiency of Indian stock markets using wavelet based long memory methods\n**D)** A) To investigate the impact of economic indicators on stock prices\n\n**Correct Answer:** B) To study the fractality and scaling properties of global equity markets\n\n**Explanation:** The thesis aims to investigate the structure and features of global equity markets from a time-frequency perspective, with a focus on capturing information from a different dimension. The study extends the application of time-frequency based wavelet techniques to analyze the fractality and scaling properties of global equity markets, which is option C. Options A and D are related to specific aspects of the study, but not the primary objective. Option B is partially correct, as the study does analyze the interdependence of global equity markets, but it is not the primary objective."}, "12": {"documentation": {"title": "Dynamical ion transfer between coupled Coulomb crystals in a double well\n  potential", "source": "Andrea Klumpp, Alexandra Zampetaki and Peter Schmelcher", "docs_id": "1706.03995", "section": ["physics.atom-ph", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical ion transfer between coupled Coulomb crystals in a double well\n  potential. We investigate the non-equilibrium dynamics of coupled Coulomb crystals of different sizes trapped in a double well potential. The dynamics is induced by an instantaneous quench of the potential barrier separating the two crystals. Due to the intra- and inter-crystal Coulomb interactions and the asymmetric population of the potential wells we observe a complex reordering of ions within the two crystals as well as ion transfer processes from one well to the other. The study and analysis of the latter processes constitutes the main focus of this work. In particular we examine the dependence of the observed ion transfers on the quench amplitude performing an analysis for different crystalline configurations ranging from one-dimensional ion chains via two-dimensional zig-zag chains and ring structures to three-dimensional spherical structures. Such an analysis provides us with the means to extract the general principles governing the ion transfer dynamics and we gain some insight on the structural disorder caused by the quench of the barrier height."}, "answer": "Here is the difficult multi-form question:\n\n**Question:** Analyze the dynamics of coupled Coulomb crystals in a double well potential, as described in the given Arxiv documentation. What are the main factors that influence the ion transfer processes between the two crystals, and how do they depend on the crystalline configuration?\n\n**A)** The ion transfer processes are primarily influenced by the amplitude of the potential barrier quench, and the crystalline configuration has a negligible effect on the dynamics.\n\n**B)** The ion transfer processes are primarily influenced by the intra- and inter-crystal Coulomb interactions, and the crystalline configuration plays a crucial role in determining the dynamics.\n\n**C)** The ion transfer processes are primarily influenced by the asymmetric population of the potential wells, and the crystalline configuration has a significant effect on the dynamics.\n\n**D)** The ion transfer processes are primarily influenced by the structural disorder caused by the quench of the barrier height, and the crystalline configuration has a negligible effect on the dynamics.\n\n**Correct Answer:** B) The ion transfer processes are primarily influenced by the intra- and inter-crystal Coulomb interactions, and the crystalline configuration plays a crucial role in determining the dynamics.\n\n**Explanation:** The correct answer is B) because the documentation states that the dynamics is induced by an instantaneous quench of the potential barrier separating the two crystals, and that the intra- and inter-crystal Coulomb interactions play a crucial role in the observed ion transfers. Additionally, the analysis for different crystalline configurations ranging from one-dimensional ion chains to three-dimensional spherical structures suggests that the crystalline configuration has a significant effect on the dynamics."}, "13": {"documentation": {"title": "Decentralized Payment Clearing using Blockchain and Optimal Bidding", "source": "Hamed Amini, Maxim Bichuch and Zachary Feinstein", "docs_id": "2109.00446", "section": ["q-fin.RM", "econ.GN", "q-fin.EC", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decentralized Payment Clearing using Blockchain and Optimal Bidding. In this paper, we construct a decentralized clearing mechanism which endogenously and automatically provides a claims resolution procedure. This mechanism can be used to clear a network of obligations through blockchain. In particular, we investigate default contagion in a network of smart contracts cleared through blockchain. In so doing, we provide an algorithm which constructs the blockchain so as to guarantee the payments can be verified and the miners earn a fee. We, additionally, consider the special case in which the blocks have unbounded capacity to provide a simple equilibrium clearing condition for the terminal net worths; existence and uniqueness are proven for this system. Finally, we consider the optimal bidding strategies for each firm in the network so that all firms are utility maximizers with respect to their terminal wealths. We first look for a mixed Nash equilibrium bidding strategies, and then also consider Pareto optimal bidding strategies. The implications of these strategies, and more broadly blockchain, on systemic risk are considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the decentralized payment clearing mechanism using blockchain and optimal bidding, what is the primary goal of the algorithm that constructs the blockchain to guarantee payment verification and miner earnings?\n\nA) To minimize the total cost of clearing the network\nB) To maximize the total utility of the firms in the network\nC) To ensure that the payments can be verified and the miners earn a fee, while also guaranteeing the stability of the network\nD) To optimize the bidding strategies for each firm in the network\n\n**Correct Answer:** C) To ensure that the payments can be verified and the miners earn a fee, while also guaranteeing the stability of the network\n\n**Explanation:** The algorithm in the paper is designed to construct the blockchain in such a way that it guarantees the payments can be verified and the miners earn a fee. This is crucial for the stability of the network, as it ensures that the payments are secure and the miners are incentivized to participate in the clearing process. The algorithm's primary goal is to achieve this balance between payment verification and miner earnings, while also ensuring the stability of the network.\n\n**Candidate A (Incorrect):** While minimizing the total cost of clearing the network may be a desirable outcome, it is not the primary goal of the algorithm. The algorithm's focus is on ensuring payment verification and miner earnings, rather than solely minimizing costs.\n\n**Candidate B (Incorrect):** Maximizing the total utility of the firms in the network is not the primary goal of the algorithm. While the algorithm does consider the utility of the firms, its primary focus is on ensuring payment verification and miner earnings.\n\n**Candidate D (Incorrect):** Optimizing the bidding strategies for each firm in the network is an important aspect of the paper, but it is not the primary goal of the algorithm. The algorithm's primary goal is to ensure payment verification and miner earnings, while also guaranteeing the stability of the network."}, "14": {"documentation": {"title": "Bayesian data assimilation in shape registration", "source": "C.J. Cotter, S.L. Cotter, F.-X. Vialard", "docs_id": "1212.5088", "section": ["math.PR", "math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian data assimilation in shape registration. In this paper we apply a Bayesian framework to the problem of geodesic curve matching. Given a template curve, the geodesic equations provide a mapping from initial conditions for the conjugate momentum onto topologically equivalent shapes. Here, we aim to recover the well-defined posterior distribution on the initial momentum which gives rise to observed points on the target curve; this is achieved by explicitly including a reparameterisation in the formulation. Appropriate priors are chosen for the functions which together determine this field and the positions of the observation points, the initial momentum $p_0$ and the reparameterisation vector field $\\nu$, informed by regularity results about the forward model. Having done this, we illustrate how Maximum Likelihood Estimators (MLEs) can be used to find regions of high posterior density, but also how we can apply recently developed \\SLC{Markov chain Monte Carlo (MCMC)} methods on function spaces to characterise the whole of the posterior density. These illustrative examples also include scenarios where the posterior distribution is multimodal and irregular, leading us to the conclusion that knowledge of a state of global maximal posterior density does not always give us the whole picture, and full posterior sampling can give better quantification of likely states and the overall uncertainty inherent in the problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Bayesian data assimilation framework applied to geodesic curve matching, what is the primary goal of explicitly including a reparameterisation in the formulation, and how does this contribute to the recovery of the posterior distribution on the initial momentum?\n\n**A)** The primary goal is to regularise the forward model, and the reparameterisation allows for a more accurate estimation of the initial momentum by reducing the impact of noise in the observation points.\n\n**B)** The primary goal is to account for the non-linear effects of the reparameterisation vector field on the target curve, and the reparameterisation enables the estimation of the posterior distribution on the initial momentum by incorporating these effects into the likelihood function.\n\n**C)** The primary goal is to reduce the dimensionality of the parameter space, and the reparameterisation allows for a more efficient estimation of the posterior distribution on the initial momentum by transforming the high-dimensional space into a lower-dimensional space.\n\n**D)** The primary goal is to incorporate prior knowledge about the regularity of the forward model, and the reparameterisation enables the estimation of the posterior distribution on the initial momentum by incorporating this prior knowledge into the likelihood function.\n\n**Correct Answer:** B) The primary goal is to account for the non-linear effects of the reparameterisation vector field on the target curve, and the reparameterisation enables the estimation of the posterior distribution on the initial momentum by incorporating these effects into the likelihood function.\n\n**Explanation:** The correct answer is B) because the reparameterisation is explicitly included in the formulation to account for the non-linear effects of the reparameterisation vector field on the target curve. This allows for a more accurate estimation of the posterior distribution on the initial momentum by incorporating these effects into the likelihood function. The other options are incorrect because they do not accurately capture the primary goal of the reparameterisation in the context of Bayesian data assimilation for geodesic curve matching."}, "15": {"documentation": {"title": "Experimental Design under Network Interference", "source": "Davide Viviano", "docs_id": "2003.08421", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental Design under Network Interference. This paper discusses the problem of the design of a two-wave experiment under network interference. We consider (i) a possibly fully connected network, (ii) spillover effects occurring across neighbors, (iii) local dependence of unobservables characteristics. We allow for a class of estimands of interest which includes the average effect of treating the entire network, the average spillover effects, average direct effects, and interactions of the latter two. We propose a design mechanism where the experimenter optimizes over participants and treatment assignments to minimize the variance of the estimators of interest, using the first-wave experiment for estimation of the variance. We characterize conditions on the first and second wave experiments to guarantee unconfounded experimentation, we showcase tradeoffs in the choice of the pilot's size, and we formally characterize the pilot's size relative to the main experiment. We derive asymptotic properties of estimators of interest under the proposed design mechanism and regret guarantees of the proposed method. Finally we illustrate the advantage of the method over state-of-art methodologies on simulated and real-world networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Experimental Design under Network Interference paper, what is the primary goal of the proposed design mechanism, and how does it relate to the minimization of variance of estimators of interest?\n\n**A)** The primary goal is to maximize the sample size of the pilot experiment to reduce the variance of estimators of interest, while also ensuring unconfounded experimentation.\n\n**B)** The primary goal is to optimize treatment assignments and participant selection to minimize the variance of estimators of interest, using the first-wave experiment for estimation of the variance.\n\n**C)** The primary goal is to ensure that the pilot experiment is fully representative of the main experiment, while also minimizing the variance of estimators of interest.\n\n**D)** The primary goal is to guarantee unconfounded experimentation, while also minimizing the variance of estimators of interest through the use of a pilot experiment.\n\n**Correct Answer:** B) The primary goal is to optimize treatment assignments and participant selection to minimize the variance of estimators of interest, using the first-wave experiment for estimation of the variance.\n\n**Explanation:** The correct answer is B) because the paper proposes a design mechanism that optimizes over participants and treatment assignments to minimize the variance of estimators of interest, using the first-wave experiment for estimation of the variance. This is stated in the introduction of the paper: \"We propose a design mechanism where the experimenter optimizes over participants and treatment assignments to minimize the variance of the estimators of interest, using the first-wave experiment for estimation of the variance.\""}, "16": {"documentation": {"title": "Beta Spectrum Generator: High precision allowed $\\beta$ spectrum shapes", "source": "Leendert Hayen and Nathal Severijns", "docs_id": "1803.00525", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beta Spectrum Generator: High precision allowed $\\beta$ spectrum shapes. Several searches for Beyond Standard Model physics rely on an accurate and highly precise theoretical description of the allowed $\\beta$ spectrum. Following recent theoretical advances, a C++ implementation of an analytical description of the allowed beta spectrum shape was constructed. It implements all known corrections required to give a theoretical description accurate to a few parts in $10^4$. The remaining nuclear structure-sensitive input can optionally be calculated in an extreme single-particle approximation with a variety of nuclear potentials, or obtained through an interface with more state-of-the-art computations. Due to its relevance in modern neutrino physics, the corresponding (anti)neutrino spectra are readily available with appropriate radiative corrections. In the interest of user-friendliness, a graphical interface was developed in Python with a coupling to a variety of nuclear databases. We present several test cases and illustrate potential usage of the code. Our work can be used as the foundation for current and future high-precision experiments related to the beta decay process. Source code: https://github.com/leenderthayen/BSG Documentation: http://bsg.readthedocs.io"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary purpose of the Beta Spectrum Generator (BSG) code, and what are some of its key features?\n\nA) To calculate the energy spectrum of neutrinos in beta decay, with a focus on high-precision calculations and optional nuclear structure-sensitive input.\nB) To simulate the scattering of particles in nuclear reactions, with a graphical interface for visualizing results.\nC) To model the behavior of subatomic particles in high-energy collisions, with a focus on radiative corrections.\nD) To generate spectra for particle decays, with an emphasis on user-friendliness and coupling to nuclear databases.\n\nCorrect Answer: A) To calculate the energy spectrum of neutrinos in beta decay, with a focus on high-precision calculations and optional nuclear structure-sensitive input.\n\nExplanation: The correct answer is A) because the BSG code is specifically designed to calculate the energy spectrum of neutrinos in beta decay, with a focus on high-precision calculations. The code implements all known corrections required to achieve this accuracy, and allows for optional nuclear structure-sensitive input to be calculated using various nuclear potentials or interfaces with more advanced computations. The other options are incorrect because they describe features of the code that are not its primary purpose. Option B describes a feature of a different code, option C describes a different application of the code, and option D is too vague and does not accurately capture the code's purpose."}, "17": {"documentation": {"title": "The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based\n  Physiological Estimation", "source": "Ambareesh Revanur, Zhihua Li, Umur A. Ciftci, Lijun Yin, Laszlo A.\n  Jeni", "docs_id": "2109.10471", "section": ["cs.CY", "cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based\n  Physiological Estimation. Telehealth has the potential to offset the high demand for help during public health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating blood volume variations in the microvascular tissue from video - would be well suited for these situations. Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population. In this paper, we outline the evaluation protocol, the data used, and the results. V4V is to be held in conjunction with the 2021 International Conference on Computer Vision."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the 1st Vision for Vitals Challenge (V4V) dataset, and how does it relate to the potential benefits of telehealth during public health emergencies?\n\n**A)** To evaluate the performance of remote photoplethysmography (rPPG) methods in controlled laboratory settings, with the goal of improving diagnostic accuracy for cardiovascular diseases.\n**B)** To provide a standardized dataset for comparing the accuracy of various non-contact video-based physiological estimation methods, with the aim of improving remote healthcare services during public health emergencies.\n**C)** To develop a new algorithm for estimating heart rate from digital video, with the goal of improving patient monitoring in clinical settings.\n**D)** To investigate the effects of facial expressions and illumination changes on the accuracy of remote PPG methods, with the aim of improving the robustness of these methods in naturalistic conditions.\n\n**Correct Answer:** B) To provide a standardized dataset for comparing the accuracy of various non-contact video-based physiological estimation methods, with the aim of improving remote healthcare services during public health emergencies.\n\n**Explanation:** The correct answer is B) because the V4V Challenge aims to enable comparisons among alternative methods for non-contact video-based physiological estimation, which is crucial for improving remote healthcare services during public health emergencies. The challenge provides a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population, which allows for a comprehensive evaluation of the performance of different methods in naturalistic conditions."}, "18": {"documentation": {"title": "Hot dense magnetized ultrarelativistic spinor matter in a slab", "source": "Yu. A. Sitenko", "docs_id": "1606.08241", "section": ["hep-th", "astro-ph.HE", "cond-mat.mes-hall", "math-ph", "math.MP", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hot dense magnetized ultrarelativistic spinor matter in a slab. Properties of hot dense ultrarelativistic spinor matter in a slab of finite width, placed in a transverse uniform magnetic field, are studied. The admissible set of boundary conditions is determined by the requirement that spinor matter be confined inside the slab. In thermal equilibrium, the chiral separation effect in the slab is shown to depend on both temperature and chemical potential; this is distinct from the unrealistic case of the magnetic field filling the unbounded (infinite) medium, when the effect is temperature independent. In the realistic case of the slab, a stepwise behaviour of the axial current density at zero temperature is smoothed out as temperature increases, turning into a linear behaviour at infinitely large temperature. A choice of boundary conditions can facilitate either augmentation or attenuation of the chiral separation effect; in particular, the effect can persist even at zero chemical potential, if temperature is finite. Thus the boundary condition can serve as a source that is additional to the spinor matter density."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of hot dense magnetized ultrarelativistic spinor matter in a slab, what is the effect of increasing temperature on the chiral separation effect in the slab, and how does it differ from the unrealistic case of the magnetic field filling the unbounded medium?\n\n**A)** The chiral separation effect is temperature independent and remains unchanged at all temperatures.\n**B)** The chiral separation effect is temperature dependent and is smoothed out as temperature increases, turning into a linear behavior at infinitely large temperature.\n**C)** The chiral separation effect is temperature independent and is only affected by the chemical potential.\n**D)** The chiral separation effect is only present at zero temperature and disappears at all other temperatures.\n\n**Correct Answer:** B) The chiral separation effect is temperature dependent and is smoothed out as temperature increases, turning into a linear behavior at infinitely large temperature.\n\n**Explanation:** According to the documentation, the chiral separation effect in the slab is temperature dependent and is smoothed out as temperature increases, turning into a linear behavior at infinitely large temperature. This is in contrast to the unrealistic case of the magnetic field filling the unbounded medium, where the effect is temperature independent."}, "19": {"documentation": {"title": "Webs of (p,q) 5-branes, Five Dimensional Field Theories and Grid\n  Diagrams", "source": "Ofer Aharony, Amihay Hanany, Barak Kol", "docs_id": "hep-th/9710116", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Webs of (p,q) 5-branes, Five Dimensional Field Theories and Grid\n  Diagrams. We continue to study 5d N=1 supersymmetric field theories and their compactifications on a circle through brane configurations. We develop a model, which we call (p,q) Webs, which enables simple geometrical computations to reproduce the known results, and facilitates further study. The physical concepts of field theory are transparent in this picture, offering an interpretation for global symmetries, local symmetries, the effective (running) coupling, the Coulomb and Higgs branches, the monopole tensions, and the mass of BPS particles. A rule for the dimension of the Coulomb branch is found by introducing Grid Diagrams. Some known classifications of field theories are reproduced. In addition to the study of the vacuum manifold we develop methods to determine the BPS spectrum. Some states, such as quarks, correspond to instantons inside the 5-brane which we call strips. In general, these may not be identified with (p,q) strings. We describe how a strip can bend out of a 5-brane, becoming a string. A general BPS state corresponds to a Web of strings and strips. For special values of the string coupling a few strips can combine and leave the 5-brane as a string."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of (p,q) Webs, what is the relationship between the dimension of the Coulomb branch and the Grid Diagrams, and how does this relate to the physical concept of the effective (running) coupling?\n\nA) The dimension of the Coulomb branch is directly proportional to the number of strips in the Web, and the effective coupling is inversely proportional to the number of strips.\n\nB) The dimension of the Coulomb branch is determined by the number of Grid Diagrams, and the effective coupling is a function of the string coupling constant.\n\nC) The dimension of the Coulomb branch is related to the number of strips in the Web, but the effective coupling is a constant that depends on the specific (p,q) Web configuration.\n\nD) The dimension of the Coulomb branch is a measure of the number of Grid Diagrams, and the effective coupling is a function of the number of strips in the Web.\n\nCorrect Answer: B) The dimension of the Coulomb branch is determined by the number of Grid Diagrams, and the effective coupling is a function of the string coupling constant.\n\nExplanation: The correct answer is B) because the documentation states that a rule for the dimension of the Coulomb branch is found by introducing Grid Diagrams, and the effective coupling is a function of the string coupling constant. This relationship is a key concept in the (p,q) Webs framework, and understanding it is essential for analyzing the physical concepts of field theory in this context."}, "20": {"documentation": {"title": "A space of goals: the cognitive geometry of informationally bounded\n  agents", "source": "Karen Archer, Nicola Catenacci Volpi, Franziska Br\\\"oker, Daniel\n  Polani", "docs_id": "2111.03699", "section": ["cs.AI", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A space of goals: the cognitive geometry of informationally bounded\n  agents. Traditionally, Euclidean geometry is treated by scientists as a priori and objective. However, when we take the position of an agent, the problem of selecting a best route should also factor in the abilities of the agent, its embodiment and particularly its cognitive effort. In this paper we consider geometry in terms of travel between states within a world by incorporating information processing costs with the appropriate spatial distances. This induces a geometry that increasingly differs from the original geometry of the given world, as information costs become increasingly important. We visualize this \\textit{\"cognitive geometry\"} by projecting it onto 2- and 3-dimensional spaces showing distinct distortions reflecting the emergence of epistemic and information-saving strategies as well as pivot states. The analogies between traditional cost-based geometries and those induced by additional informational costs invite a generalization of the traditional notion of geodesics as cheapest routes towards the notion of \\textit{infodesics}. Crucially, the concept of infodesics approximates the usual geometric property that, travelling from a start to a goal along a geodesic, not only the goal, but all intermediate points are equally visited at optimal cost from the start."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Consider an agent navigating a complex world with multiple states, where the agent's cognitive effort and information processing costs play a crucial role in selecting the best route. How does the concept of \"infodesics\" relate to the traditional notion of geodesics, and what are the implications of incorporating information costs into the geometry of the world?\n\n**A)** Infodesics are equivalent to geodesics, as both represent the shortest path between two points, but infodesics take into account the agent's cognitive effort and information processing costs.\n\n**B)** Infodesics are a type of geodesic that is specifically designed to minimize the agent's cognitive effort and information processing costs, while still finding the shortest path between two points.\n\n**C)** Infodesics are a type of geodesic that is influenced by the agent's embodiment and cognitive geometry, but do not necessarily take into account the agent's information processing costs.\n\n**D)** Infodesics are a type of geodesic that is the opposite of traditional geodesics, as they prioritize the agent's cognitive effort and information processing costs over finding the shortest path.\n\n**Correct Answer:** B) Infodesics are a type of geodesic that is specifically designed to minimize the agent's cognitive effort and information processing costs, while still finding the shortest path between two points.\n\n**Explanation:** The concept of infodesics is introduced in the paper as a generalization of traditional geodesics, which prioritize the shortest path between two points. Infodesics, on the other hand, take into account the agent's cognitive effort and information processing costs, making them a more nuanced and realistic representation of the agent's navigation process. This requires the test-taker to understand the key concepts and implications of the paper, and to apply that understanding to a specific scenario."}, "21": {"documentation": {"title": "Flood Disasters and Health Among the Urban Poor", "source": "Michelle Escobar Carias, David Johnston, Rachel Knott, Rohan Sweeney", "docs_id": "2111.05455", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Flood Disasters and Health Among the Urban Poor. Billions of people live in urban poverty, with many forced to reside in disaster-prone areas. Research suggests that such disasters harm child nutrition and increase adult morbidity. However, little is known about impacts on mental health, particularly of people living in slums. In this paper we estimate the effects of flood disasters on the mental and physical health of poor adults and children in urban Indonesia. Our data come from the Indonesia Family Life Survey and new surveys of informal settlement residents. We find that urban poor populations experience increases in acute morbidities and depressive symptoms following floods, that the negative mental health effects last longer, and that the urban wealthy show no health effects from flood exposure. Further analysis suggests that worse economic outcomes may be partly responsible. Overall, the results provide a more nuanced understanding of the morbidities experienced by populations most vulnerable to increased disaster occurrence."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nFlood Disasters and Health Among the Urban Poor: A Critical Analysis\n\nThe study on flood disasters and health among the urban poor in Indonesia highlights the significant negative impacts of such disasters on the mental and physical health of poor adults and children. However, the study also suggests that the negative effects may be partly attributed to worse economic outcomes. Analyze the findings of the study and discuss the following:\n\nA) The study's conclusion that the negative mental health effects of flood disasters last longer in urban poor populations, and explain how this finding relates to the concept of \"trauma\" in the context of disaster exposure.\n\nB) The study's observation that urban poor populations experience increases in acute morbidities and depressive symptoms following floods, and discuss the potential mechanisms underlying these findings.\n\nC) The study's finding that the urban wealthy show no health effects from flood exposure, and explain the possible reasons for this disparity in health outcomes.\n\nD) The study's suggestion that worse economic outcomes may be partly responsible for the negative health effects of flood disasters, and discuss the potential pathways through which economic hardship exacerbates the negative impacts of disasters on health.\n\nCorrect Answer: D) The study's suggestion that worse economic outcomes may be partly responsible for the negative health effects of flood disasters, and discuss the potential pathways through which economic hardship exacerbates the negative impacts of disasters on health.\n\nExplanation: The correct answer is D) because the study suggests that worse economic outcomes may be partly responsible for the negative health effects of flood disasters. This is supported by the finding that urban poor populations experience worse economic outcomes, which in turn exacerbate the negative impacts of disasters on health. The study's analysis of the potential pathways through which economic hardship affects health outcomes is a key aspect of its findings, and D) requires the test-taker to demonstrate an understanding of this relationship. The other options, while related to the study's findings, do not capture the specific relationship between economic outcomes and health effects that is highlighted in the study."}, "22": {"documentation": {"title": "How to Implement A Priori Information: A Statistical Mechanics Approach", "source": "Joerg C. Lemm", "docs_id": "cond-mat/9808039", "section": ["cond-mat.dis-nn", "nlin.AO", "cond-mat.stat-mech", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How to Implement A Priori Information: A Statistical Mechanics Approach. A new general framework is presented for implementing complex a priori knowledge, having in mind especially situations where the number of available training data is small compared to the complexity of the learning task. A priori information is hereby decomposed into simple components represented by quadratic building blocks (quadratic concepts) which are then combined by conjunctions and disjunctions to built more complex, problem specific error functionals. While conjunction of quadratic concepts leads to classical quadratic regularization functionals, disjunctions, representing ambiguous priors, result in non--convex error functionals. These go beyond classical quadratic regularization approaches and correspond, in Bayesian interpretation, to non--gaussian processes. Numerical examples show that the resulting stationarity equations, despite being in general nonlinear, inhomogeneous (integro--)differential equations, are not necessarily difficult to solve. Appendix A relates the formalism of statistical mechanics to statistics and Appendix B describes the framework of Bayesian decision theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a scenario where a machine learning model is trained on a small dataset to predict a complex function, such as a non-linear relationship between two variables. The model is given a priori knowledge about the underlying process, which can be represented as a quadratic concept. However, the prior knowledge is ambiguous and can be interpreted in multiple ways, leading to a non-convex error functional.\n\nA) The model can be trained using a classical quadratic regularization functional, which is a linear combination of the quadratic concepts.\n\nB) The model can be trained using a non-convex error functional, which is a disjunction of the quadratic concepts, and this approach is suitable for situations where the prior knowledge is ambiguous.\n\nC) The model can be trained using a Bayesian approach, which incorporates the prior knowledge into the model's parameters, but this approach is only suitable for Gaussian processes.\n\nD) The model can be trained using a classical quadratic regularization functional, but the prior knowledge must be represented as a disjunction of quadratic concepts to account for ambiguity.\n\nCorrect Answer: B) The model can be trained using a non-convex error functional, which is a disjunction of the quadratic concepts, and this approach is suitable for situations where the prior knowledge is ambiguous.\n\nExplanation: The correct answer is B) because the problem statement mentions that the prior knowledge is ambiguous and can be represented as a disjunction of quadratic concepts, leading to a non-convex error functional. This approach is suitable for situations where the prior knowledge is ambiguous, as it allows for multiple interpretations of the prior knowledge. The other options are incorrect because they do not take into account the ambiguity of the prior knowledge. Option A is incorrect because it assumes that the prior knowledge can be represented as a linear combination of quadratic concepts, which is not the case when the prior knowledge is ambiguous. Option C is incorrect because it assumes that the prior knowledge is Gaussian, which is not necessarily the case. Option D is incorrect because it assumes that the prior knowledge must be represented as a disjunction of quadratic concepts, but it does not take into account the ambiguity of the prior knowledge."}, "23": {"documentation": {"title": "Hierarchical Associative Memory", "source": "Dmitry Krotov", "docs_id": "2107.06446", "section": ["cs.NE", "cond-mat.dis-nn", "cs.LG", "q-bio.NC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hierarchical Associative Memory. Dense Associative Memories or Modern Hopfield Networks have many appealing properties of associative memory. They can do pattern completion, store a large number of memories, and can be described using a recurrent neural network with a degree of biological plausibility and rich feedback between the neurons. At the same time, up until now all the models of this class have had only one hidden layer, and have only been formulated with densely connected network architectures, two aspects that hinder their machine learning applications. This paper tackles this gap and describes a fully recurrent model of associative memory with an arbitrary large number of layers, some of which can be locally connected (convolutional), and a corresponding energy function that decreases on the dynamical trajectory of the neurons' activations. The memories of the full network are dynamically \"assembled\" using primitives encoded in the synaptic weights of the lower layers, with the \"assembling rules\" encoded in the synaptic weights of the higher layers. In addition to the bottom-up propagation of information, typical of commonly used feedforward neural networks, the model described has rich top-down feedback from higher layers that help the lower-layer neurons to decide on their response to the input stimuli."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing models of associative memory, such as Dense Associative Memories and Modern Hopfield Networks, that hinders their machine learning applications?\n\n**A)** They can only store a limited number of memories.\n**B)** They can only perform pattern completion.\n**C)** They have only one hidden layer and are formulated with densely connected network architectures.\n**D)** They can only be used for classification tasks.\n\n**Correct Answer:** C) They have only one hidden layer and are formulated with densely connected network architectures.\n\n**Explanation:** The question requires the test-taker to understand the limitations of existing models of associative memory, as mentioned in the provided documentation. The correct answer, C, highlights the two specific aspects that hinder the machine learning applications of these models: having only one hidden layer and being formulated with densely connected network architectures. The other options are incorrect because they do not accurately represent the limitations of these models."}, "24": {"documentation": {"title": "Twofold and Fourfold Symmetric Anisotropic Magnetoresistance Effect in A\n  Model with Crystal Field", "source": "Satoshi Kokado and Masakiyo Tsunoda", "docs_id": "1507.05912", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Twofold and Fourfold Symmetric Anisotropic Magnetoresistance Effect in A\n  Model with Crystal Field. We theoretically study the twofold and fourfold symmetric anisotropic magnetoresistance (AMR) effects of ferromagnets. We here use the two-current model for a system consisting of a conduction state and localized d states. The localized d states are obtained from a Hamiltonian with a spin--orbit interaction, an exchange field, and a crystal field. From the model, we first derive general expressions for the coefficient of the twofold symmetric term ($C_2$) and that of the fourfold symmetric term ($C_4$) in the AMR ratio. In the case of a strong ferromagnet, the dominant term in $C_2$ is proportional to the difference in the partial densities of states (PDOSs) at the Fermi energy ($E_{\\rm F}$) between the $d\\varepsilon$ and $d\\gamma$ states, and that in $C_4$ is proportional to the difference in the PDOSs at $E_{\\rm F}$ among the $d\\varepsilon$ states. Using the dominant terms, we next analyze the experimental results for Fe$_4$N, in which $|C_2|$ and $|C_4|$ increase with decreasing temperature. The experimental results can be reproduced by assuming that the tetragonal distortion increases with decreasing temperature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the two-current model for a system consisting of a conduction state and localized d states, derive the general expressions for the coefficient of the twofold symmetric term ($C_2$) and that of the fourfold symmetric term ($C_4$) in the AMR ratio.\n\n**A)** $C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$ and $C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\n**B)** $C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$ and $C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE + \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\n**C)** $C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$ and $C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE - \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\n**D)** $C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$ and $C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE + \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE + \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\n**Correct Answer:** **B)** $C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$ and $C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE + \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\n**Explanation:** The correct answer is based on the fact that the twofold symmetric term ($C_2$) is proportional to the difference in the partial densities of states (PDOSs) at the Fermi energy ($E_{\\rm F}$) between the $d\\varepsilon$ and $d\\gamma$ states, and the fourfold symmetric term ($C_4$) is proportional to the difference in the PDOSs at $E_{\\rm F}$ among the $d\\varepsilon$ states. The correct expressions for $C_2$ and $C_4$ are therefore:\n\n$C_2 = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\gamma}(E) dE$\n\n$C_4 = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE + \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\rho_{d\\varepsilon}(E) \\rho_{d\\varepsilon}(E) dE$\n\nThe other options are incorrect because they do not accurately represent the relationship between $C_2$ and $C_4$ in the AMR ratio."}, "25": {"documentation": {"title": "Continuous Record Laplace-based Inference about the Break Date in\n  Structural Change Models", "source": "Alessandro Casini and Pierre Perron", "docs_id": "1804.00232", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Continuous Record Laplace-based Inference about the Break Date in\n  Structural Change Models. Building upon the continuous record asymptotic framework recently introduced by Casini and Perron (2018a) for inference in structural change models, we propose a Laplace-based (Quasi-Bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. It is defined by an integration rather than an optimization-based method. A transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the Quasi-posterior. For a given choice of a loss function, the Laplace-type estimator is the minimizer of the expected risk with the expectation taken under the Quasi-posterior. Besides providing an alternative estimate that is more precise|lower mean absolute error (MAE) and lower root-mean squared error (RMSE)|than the usual least-squares one, the Quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of Highest Density Region. The resulting Laplace-based inferential procedure is shown to have lower MAE and RMSE, and the confidence sets strike the best balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Continuous Record Laplace-based Inference about the Break Date in Structural Change Models, what is the primary advantage of using a Laplace-type estimator over the usual least-squares estimator?\n\n**A)** The Laplace-type estimator is more computationally intensive.\n**B)** The Laplace-type estimator provides a more precise estimate of the break date, with lower mean absolute error (MAE) and lower root-mean squared error (RMSE).\n**C)** The Laplace-type estimator is only applicable to small-scale structural change models.\n**D)** The Laplace-type estimator is not asymptotically valid.\n\n**Correct Answer:** B) The Laplace-type estimator provides a more precise estimate of the break date, with lower mean absolute error (MAE) and lower root-mean squared error (RMSE).\n\n**Explanation:** The correct answer is B) because the documentation states that the Laplace-type estimator provides an alternative estimate that is more precise, with lower MAE and RMSE, than the usual least-squares estimator. This is a key advantage of the proposed method, as it allows for more accurate inference about the break date in structural change models. The other options are incorrect because they do not accurately reflect the benefits of the Laplace-type estimator."}, "26": {"documentation": {"title": "Photon-phonon parametric oscillation induced by the quadratic coupling\n  in an optomechanical resonator", "source": "Lin Zhang, Fengzhou Ji, Xu Zhang, Weiping Zhang", "docs_id": "1602.02221", "section": ["nlin.CD", "physics.optics", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photon-phonon parametric oscillation induced by the quadratic coupling\n  in an optomechanical resonator. A direct photon-phonon parametric effect of the quadratic coupling on the mean-field dynamics of an optomechanical resonator in the large-scale-movement regime is found and investigated. Under a weak pumping power, the mechanical resonator damps to steady state with a nonlinear static response sensitively modified by the quadratic coupling. When the driving powerincreases beyond the static energy balance, the steady states lose their stabilities via Hopf bifurcations and the resonator produces stable self-sustained oscillation(limit-circle behavior) of discrete energies with step-like amplitudes due to the parametric effect of the quadratic coupling, which can be understood roughly by the power balance between gain and loss on the resonator. A further increase of the pumping power can induce chaotic dynamic of the resonator via a typical routine of period-doubling bifurcation but which can be stabilized by the parametric effect through an inversion bifurcation process back to limit-circle states. The bifurcation-to-inverse-bifurcation transitions are numerically verified by the maximal Lyapunov exponents of the dynamics and which indicate an efficient way to suppress the chaotic behavior of the optomechanical resonator by the quadratic coupling. Furthermore, the parametric effect of the quadratic coupling on the dynamic transitions of an optomechanical resonator can be conveniently detected or traced by the output power spectrum of the cavity field."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of photon-phonon parametric oscillation induced by the quadratic coupling in an optomechanical resonator, what is the outcome of the period-doubling bifurcation process when the pumping power is further increased?\n\n**A)** The resonator undergoes a stable limit-circle behavior with step-like amplitudes, which can be suppressed by the parametric effect of the quadratic coupling.\n\n**B)** The resonator exhibits chaotic dynamics, which can be stabilized by the parametric effect through an inversion bifurcation process back to limit-circle states.\n\n**C)** The resonator damps to steady state with a nonlinear static response, which is modified by the quadratic coupling, but the pumping power does not affect the dynamics.\n\n**D)** The resonator produces stable self-sustained oscillation of discrete energies, but the period-doubling bifurcation process is not relevant to the dynamics.\n\n**Correct Answer:** B) The resonator exhibits chaotic dynamics, which can be stabilized by the parametric effect through an inversion bifurcation process back to limit-circle states.\n\n**Explanation:** The correct answer is B) because the period-doubling bifurcation process is a route to chaos, but the parametric effect of the quadratic coupling can stabilize the dynamics by inducing an inversion bifurcation process, which leads back to the limit-circle states. This is supported by the fact that the maximal Lyapunov exponents of the dynamics indicate an efficient way to suppress the chaotic behavior of the optomechanical resonator by the quadratic coupling."}, "27": {"documentation": {"title": "Relativistic nuclear collisions: Establishing a non-critical baseline\n  for fluctuation measurements", "source": "Peter Braun-Munzinger, Bengt Friman, Krzysztof Redlich, Anar Rustamov,\n  Johanna Stachel", "docs_id": "2007.02463", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relativistic nuclear collisions: Establishing a non-critical baseline\n  for fluctuation measurements. We study the influence of global baryon number conservation on the non-critical baseline of net baryon cumulants in heavy-ion collisions in a given acceptance, accounting for the asymmetry between the mean-numbers of baryons and antibaryons. We derive the probability distribution of net baryon number in a restricted phase space from the canonical partition function that incorporates exact conservation of baryon number in the full system. Furthermore, we provide tools to compute cumulants of any order from the generating function of uncorrelated baryons constrained by exact baryon number conservation. The results are applied to quantify the non-critical baseline for cumulants of net proton number fluctuations obtained in heavy-ion collisions by the STAR collaboration at different RHIC energies and by the ALICE collaboration at the LHC. Furthermore, volume fluctuations are added by a Monte Carlo procedure based on the centrality dependence of charged particle production as measured experimentally. Compared to the predictions based on the hadron resonance gas model or Skellam distribution a clear suppression of fluctuations is observed due to exact baryon-number conservation. The suppression increases with the order of the cumulant and towards lower collision energies. Predictions for net proton cumulants up to the eight order in heavy-ion collisions are given for experimentally accessible collision energies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary effect of exact baryon-number conservation on the non-critical baseline of net baryon cumulants in heavy-ion collisions, as studied in the context of relativistic nuclear collisions?\n\nA) It leads to an increase in fluctuations due to the asymmetry between the mean-numbers of baryons and antibaryons.\nB) It results in a suppression of fluctuations, which increases with the order of the cumulant and towards lower collision energies.\nC) It has no significant impact on the non-critical baseline of net baryon cumulants.\nD) It causes a shift in the centrality dependence of charged particle production.\n\nCorrect Answer: B) It results in a suppression of fluctuations, which increases with the order of the cumulant and towards lower collision energies.\n\nExplanation: The correct answer is based on the statement in the documentation that \"Compared to the predictions based on the hadron resonance gas model or Skellam distribution a clear suppression of fluctuations is observed due to exact baryon-number conservation.\" This indicates that the primary effect of exact baryon-number conservation is a suppression of fluctuations, which increases with the order of the cumulant and towards lower collision energies."}, "28": {"documentation": {"title": "Nonperturbative Description of Deep Inelastic Structure Functions in\n  Light-Front QCD", "source": "A. Harindranath, Rajen Kundu, and Wei-Min Zhang", "docs_id": "hep-ph/9806220", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonperturbative Description of Deep Inelastic Structure Functions in\n  Light-Front QCD. We explore the deep inelastic structure functions of hadrons nonperturbatively in an inverse power expansion of the light-front energy of the probe in the framework of light-front QCD. We arrive at the general expressions for various structure functions as the Fourier transform of matrix elements of different components of bilocal vector and axial vector currents on the light-front in a straightforward manner. The complexities of the structure functions are mainly carried by the multi-parton wave functions of the hadrons, while, the bilocal currents have a dynamically dependent yet simple structure on the light-front in this description. We also present a novel analysis of the power corrections based on light-front power counting which resolves some ambiguities of the conventional twist analysis in deep inelastic processes. Further, the factorization theorem and the scale evolution of the structure functions are presented in this formalism by using old-fashioned light-front time-ordered perturbation theory with multi-parton wave functions. Nonperturbative QCD dynamics underlying the structure functions can be explored in the same framework. Once the nonperturbative multi-parton wave functions are known from low-energy light-front QCD, a complete description of deep inelastic structure functions can be realized."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the framework of light-front QCD, what is the relationship between the nonperturbative multi-parton wave functions of hadrons and the structure functions of deep inelastic scattering, as described in the paper \"Nonperturbative Description of Deep Inelastic Structure Functions in Light-Front QCD\"?\n\nA) The multi-parton wave functions are directly proportional to the structure functions.\nB) The multi-parton wave functions are inversely proportional to the structure functions.\nC) The multi-parton wave functions are dynamically dependent on the structure functions, but have a simple structure on the light-front.\nD) The multi-parton wave functions are independent of the structure functions, and can be used to describe the nonperturbative QCD dynamics underlying the structure functions.\n\nCorrect Answer: C) The multi-parton wave functions are dynamically dependent on the structure functions, but have a simple structure on the light-front.\n\nExplanation: According to the paper, the complexities of the structure functions are mainly carried by the multi-parton wave functions of the hadrons, while the bilocal currents have a dynamically dependent yet simple structure on the light-front. This suggests that the multi-parton wave functions are dynamically dependent on the structure functions, but have a simple structure on the light-front."}, "29": {"documentation": {"title": "Time is Money: The Equilibrium Trading Horizon and Optimal Arrival Price", "source": "Kevin Patrick Darby", "docs_id": "2104.05844", "section": ["q-fin.MF", "q-fin.PM", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time is Money: The Equilibrium Trading Horizon and Optimal Arrival Price. Executing even moderately large derivatives orders can be expensive and risky; it's hard to balance the uncertainty of working an order over time versus paying a liquidity premium for immediate execution. Here, we introduce the Time Is Money model, which calculates the Equilibrium Trading Horizon over which to execute an order within the adversarial forces of variance risk and liquidity premium. We construct a hypothetical at-the-money option within Arithmetic Brownian Motion and invert the Bachelier model to compute an inflection point between implied variance and liquidity cost as governed by a central limit order book, each in real time as they evolve. As a result, we demonstrate a novel, continuous-time Arrival Price framework. Further, we argue that traders should be indifferent to choosing between variance risk and liquidity cost, unless they have a predetermined bias or an exogenous position with a convex payoff. We, therefore, introduce half-life factor asymptotics to the model based on a convexity factor and compare results to existing models. We also describe a specialization of the model for trading a basket of correlated instruments, as exemplified by a futures calendar spread. Finally, we establish groundwork for microstructure optimizations as well as explore short term drift and conditional expected slippage within the Equilibrium Horizon framework."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Time Is Money model, what is the primary objective of the Equilibrium Trading Horizon, and how does it relate to the trade-off between variance risk and liquidity premium?\n\nA) To minimize the liquidity premium while maximizing the variance risk, in order to maximize the expected return on investment.\nB) To maximize the expected return on investment while minimizing the variance risk, in order to minimize the liquidity premium.\nC) To balance the uncertainty of working an order over time versus paying a liquidity premium for immediate execution, by finding an optimal inflection point between implied variance and liquidity cost.\nD) To determine the optimal trading strategy for a basket of correlated instruments, by considering the convexity factor and short-term drift.\n\n**Correct Answer:** C) To balance the uncertainty of working an order over time versus paying a liquidity premium for immediate execution, by finding an optimal inflection point between implied variance and liquidity cost.\n\n**Explanation:** The correct answer is C) because the Time Is Money model aims to find an optimal trading horizon that balances the uncertainty of working an order over time versus paying a liquidity premium for immediate execution. This is achieved by computing an inflection point between implied variance and liquidity cost, which is governed by a central limit order book. The model demonstrates a novel, continuous-time Arrival Price framework that takes into account the trade-off between variance risk and liquidity premium.\n\n**Candidate A:** This answer is incorrect because it suggests that the primary objective of the Equilibrium Trading Horizon is to minimize the liquidity premium while maximizing the variance risk, which is not the case. The model actually aims to balance the two risks, not prioritize one over the other.\n\n**Candidate B:** This answer is incorrect because it suggests that the primary objective of the Equilibrium Trading Horizon is to maximize the expected return on investment while minimizing the variance risk, which is not the case. The model does not explicitly mention maximizing the expected return on investment as its primary objective.\n\n**Candidate D:** This answer is incorrect because it suggests that the primary objective of the Equilibrium Trading Horizon is to determine the optimal trading strategy for a basket of correlated instruments, which is not the case. While the model does describe a specialization for trading a basket of correlated instruments, this is not its primary objective."}, "30": {"documentation": {"title": "Antisymmetric magnetoresistance and helical magnetic structure in\n  compensated Gd/Co multilayer", "source": "Surendra Singh, M. A. Basha, C. L. Prajapat, Harsh Bhatt, Yogesh\n  Kumar, M. Gupta, C. J. Kinane, J. Cooper, M. R. Gonal, S. Langridge and S.\n  Basu", "docs_id": "1904.11269", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Antisymmetric magnetoresistance and helical magnetic structure in\n  compensated Gd/Co multilayer. Using spin dependent specular and off-specular polarized neutron reflectivity (PNR), we report the observation of a twisted helical magnetic structure with planar 2{\\pi} domain wall (DW) and highly correlated magnetic domains in a Gd/Co multilayer. Specular PNR with polarization analysis reveals the formation of planar 2{\\pi}DWs below a compensation temperature (TComp), resulting to positive exchange bias in this system. Off-specular PNR with spin polarization showed development of magnetic inhomogenities (increase in magnetic roughness) for central part (thickness ~ 25-30 {\\AA}) of each Gd layer, where magnetization is aligned perpendicular (in-plane) to an applied field. These magnetic roughness are vertically correlated and results into Bragg sheet in spin flip channel of Off-specular PNR data, which is contributing towards an antisymmetric magnetoresistance at TComp in the system. The growth and tunability of highly correlated magnetic inhomogeneities (roughness) and domain structure around TComp in combination of twisted helical magnetic structure with planar 2{\\pi}DWs will be key for application in all-spin-based technology."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism contributing to the antisymmetric magnetoresistance observed in the Gd/Co multilayer at the compensation temperature (TComp)?\n\nA) The formation of planar 2\u03c0 domain walls (DWs) and highly correlated magnetic domains\nB) The development of magnetic inhomogeneities (increase in magnetic roughness) in the central part of each Gd layer\nC) The twisted helical magnetic structure and planar 2\u03c0 DWs, which result in a Bragg sheet in the spin flip channel of Off-specular PNR data\nD) The positive exchange bias resulting from the formation of planar 2\u03c0 DWs below TComp\n\nCorrect Answer: C) The twisted helical magnetic structure and planar 2\u03c0 DWs, which result in a Bragg sheet in the spin flip channel of Off-specular PNR data\n\nExplanation: The correct answer is C) because the development of magnetic inhomogeneities (increase in magnetic roughness) in the central part of each Gd layer, as observed through Off-specular PNR with spin polarization, results in a Bragg sheet in the spin flip channel. This Bragg sheet is a key contributor to the antisymmetric magnetoresistance observed at TComp. The other options are incorrect because while the twisted helical magnetic structure and planar 2\u03c0 DWs are present in the system, they are not the primary mechanism contributing to the antisymmetric magnetoresistance. The positive exchange bias is a result of the formation of planar 2\u03c0 DWs below TComp, but it is not the primary mechanism contributing to the antisymmetric magnetoresistance."}, "31": {"documentation": {"title": "Signal recovery from a few linear measurements of its high-order spectra", "source": "Tamir Bendory, Dan Edidin, Shay Kreymer", "docs_id": "2103.01551", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signal recovery from a few linear measurements of its high-order spectra. The $q$-th order spectrum is a polynomial of degree $q$ in the entries of a signal $x\\in\\mathbb{C}^N$, which is invariant under circular shifts of the signal. For $q\\geq 3$, this polynomial determines the signal uniquely, up to a circular shift, and is called a high-order spectrum. The high-order spectra, and in particular the bispectrum ($q=3$) and the trispectrum ($q=4$), play a prominent role in various statistical signal processing and imaging applications, such as phase retrieval and single-particle reconstruction. However, the dimension of the $q$-th order spectrum is $N^{q-1}$, far exceeding the dimension of $x$, leading to increased computational load and storage requirements. In this work, we show that it is unnecessary to store and process the full high-order spectra: a signal can be characterized uniquely, up to symmetries, from only $N+1$ linear measurements of its high-order spectra. The proof relies on tools from algebraic geometry and is corroborated by numerical experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a signal $x \\in \\mathbb{C}^N$ with a high-order spectrum of degree $q$. Suppose we are given $N+1$ linear measurements of the high-order spectra of $x$, and we want to recover the signal $x$ up to circular shifts. What is the minimum number of linear measurements required to uniquely determine the signal $x$, and what is the underlying mathematical concept that justifies this result?\n\n**A)** The minimum number of linear measurements required is $N+1$, and the underlying concept is that the high-order spectra are invariant under circular shifts, allowing for unique determination up to symmetries.\n\n**B)** The minimum number of linear measurements required is $N^2$, and the underlying concept is that the high-order spectra are uniquely determined by the signal's Fourier transform.\n\n**C)** The minimum number of linear measurements required is $N+1$, and the underlying concept is that the high-order spectra can be represented as a polynomial of degree $q$ in the entries of $x$, which can be uniquely determined by $N+1$ linear measurements.\n\n**D)** The minimum number of linear measurements required is $N^2$, and the underlying concept is that the high-order spectra are uniquely determined by the signal's autocorrelation function.\n\n**Correct Answer:** C) The minimum number of linear measurements required is $N+1$, and the underlying concept is that the high-order spectra can be represented as a polynomial of degree $q$ in the entries of $x$, which can be uniquely determined by $N+1$ linear measurements.\n\n**Explanation:** The correct answer is based on the fact that the high-order spectra of a signal $x \\in \\mathbb{C}^N$ can be represented as a polynomial of degree $q$ in the entries of $x$, which is invariant under circular shifts. The problem statement mentions that the dimension of the $q$-th order spectrum is $N^{q-1}$, but it does not require storing or processing the full high-order spectra. The correct answer relies on the fact that $N+1$ linear measurements of the high-order spectra can uniquely determine the signal $x$ up to symmetries, as justified by the underlying mathematical concept."}, "32": {"documentation": {"title": "Multidimensional integration through Markovian sampling under steered\n  function morphing: a physical guise from statistical mechanics", "source": "Mirco Zerbetto, Diego Frezzato", "docs_id": "1410.2810", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multidimensional integration through Markovian sampling under steered\n  function morphing: a physical guise from statistical mechanics. We present a computational strategy for the evaluation of multidimensional integrals on hyper-rectangles based on Markovian stochastic exploration of the integration domain while the integrand is being morphed by starting from an initial appropriate profile. Thanks to an abstract reformulation of Jarzynski's equality applied in stochastic thermodynamics to evaluate the free-energy profiles along selected reaction coordinates via non-equilibrium transformations, it is possible to cast the original integral into the exponential average of the distribution of the pseudo-work (that we may term \"computational work\") involved in doing the function morphing, which is straightforwardly solved. Several tests illustrate the basic implementation of the idea, and show its performance in terms of computational time, accuracy and precision. The formulation for integrand functions with zeros and possible sign changes is also presented. It will be stressed that our usage of Jarzynski's equality shares similarities with a practice already known in statistics as Annealed Importance Sampling (AIS), when applied to computation of the normalizing constants of distributions. In a sense, here we dress the AIS with its \"physical\" counterpart borrowed from statistical mechanics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Multidimensional integration through Markovian sampling under steered function morphing: a physical guise from statistical mechanics\", what is the primary advantage of using Jarzynski's equality in the evaluation of multidimensional integrals?\n\nA) It allows for the direct computation of the integral without any approximation.\nB) It enables the use of stochastic thermodynamics to evaluate the free-energy profiles along selected reaction coordinates.\nC) It provides a way to cast the original integral into the exponential average of the distribution of the pseudo-work involved in doing the function morphing.\nD) It facilitates the use of Annealed Importance Sampling (AIS) to compute the normalizing constants of distributions.\n\n**Correct Answer:** C) It provides a way to cast the original integral into the exponential average of the distribution of the pseudo-work involved in doing the function morphing.\n\n**Explanation:** The correct answer is C) because the paper states that \"Thanks to an abstract reformulation of Jarzynski's equality applied in stochastic thermodynamics to evaluate the free-energy profiles along selected reaction coordinates via non-equilibrium transformations, it is possible to cast the original integral into the exponential average of the distribution of the pseudo-work (that we may term \"computational work\") involved in doing the function morphing, which is straightforwardly solved.\" This shows that the primary advantage of using Jarzynski's equality is to transform the original integral into a more tractable form.\n\n**Candidate A is incorrect** because while the paper does mention that the original integral can be evaluated directly, this is not the primary advantage of using Jarzynski's equality.\n\n**Candidate B is incorrect** because while the paper does mention the use of stochastic thermodynamics to evaluate free-energy profiles, this is not the primary advantage of using Jarzynski's equality in the context of multidimensional integration.\n\n**Candidate D is incorrect** because while the paper does mention the connection to Annealed Importance Sampling (AIS), this is not the primary advantage of using Jarzynski's equality in the context of multidimensional integration."}, "33": {"documentation": {"title": "SOUP: Spatial-Temporal Demand Forecasting and Competitive Supply", "source": "Bolong Zheng, Qi Hu, Lingfeng Ming, Jilin Hu, Lu Chen, Kai Zheng,\n  Christian S. Jensen", "docs_id": "2009.12157", "section": ["cs.DB", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SOUP: Spatial-Temporal Demand Forecasting and Competitive Supply. We consider a setting with an evolving set of requests for transportation from an origin to a destination before a deadline and a set of agents capable of servicing the requests. In this setting, an assignment authority is to assign agents to requests such that the average idle time of the agents is minimized. An example is the scheduling of taxis (agents) to meet incoming requests for trips while ensuring that the taxis are empty as little as possible. In this paper, we study the problem of spatial-temporal demand forecasting and competitive supply (SOUP). We address the problem in two steps. First, we build a granular model that provides spatial-temporal predictions of requests. Specifically, we propose a Spatial-Temporal Graph Convolutional Sequential Learning (ST-GCSL) algorithm that predicts the service requests across locations and time slots. Second, we provide means of routing agents to request origins while avoiding competition among the agents. In particular, we develop a demand-aware route planning (DROP) algorithm that considers both the spatial-temporal predictions and the supplydemand state. We report on extensive experiments with realworld and synthetic data that offer insight into the performance of the solution and show that it is capable of outperforming the state-of-the-art proposals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Spatial-Temporal Demand Forecasting and Competitive Supply (SOUP) problem, what is the primary objective of the assignment authority in assigning agents to requests, and how does the proposed ST-GCSL algorithm contribute to achieving this objective?\n\n**A)** The assignment authority aims to minimize the average idle time of agents, and ST-GCSL predicts service requests to optimize agent utilization.\n\n**B)** The assignment authority seeks to maximize the average idle time of agents, and ST-GCSL predicts service requests to minimize agent utilization.\n\n**C)** The assignment authority aims to minimize the total travel time of agents, and ST-GCSL predicts service requests to optimize agent routing.\n\n**D)** The assignment authority seeks to maximize the total travel time of agents, and ST-GCSL predicts service requests to minimize agent routing.\n\n**Correct Answer:** A) The assignment authority aims to minimize the average idle time of agents, and ST-GCSL predicts service requests to optimize agent utilization.\n\n**Explanation:** The correct answer is A) because the assignment authority's primary objective is to minimize the average idle time of agents, which is achieved by assigning agents to requests in a way that minimizes the time agents spend idle. The ST-GCSL algorithm contributes to achieving this objective by predicting service requests across locations and time slots, thereby enabling the assignment authority to make informed decisions about agent allocation."}, "34": {"documentation": {"title": "Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation", "source": "Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata", "docs_id": "1910.02420", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation. Electromagnetic stimulation of the human brain is a key tool for the neurophysiological characterization and diagnosis of several neurological disorders. Transcranial magnetic stimulation (TMS) is one procedure that is commonly used clinically. However, personalized TMS requires a pipeline for accurate head model generation to provide target-specific stimulation. This process includes intensive segmentation of several head tissues based on magnetic resonance imaging (MRI), which has significant potential for segmentation error, especially for low-contrast tissues. Additionally, a uniform electrical conductivity is assigned to each tissue in the model, which is an unrealistic assumption based on conventional volume conductor modeling. This paper proposes a novel approach to the automatic estimation of electric conductivity in the human head for volume conductor models without anatomical segmentation. A convolutional neural network is designed to estimate personalized electrical conductivity values based on anatomical information obtained from T1- and T2-weighted MRI scans. This approach can avoid the time-consuming process of tissue segmentation and maximize the advantages of position-dependent conductivity assignment based on water content values estimated from MRI intensity values. The computational results of the proposed approach provide similar but smoother electric field results for the brain when compared to conventional approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of conventional volume conductor modeling in the context of personalized transcranial magnetic stimulation (TMS) for neurological disorders, and how does the proposed approach address this limitation?\n\n**A)** Conventional volume conductor modeling assumes a uniform electrical conductivity across all tissues, which can lead to inaccurate electric field results. The proposed approach addresses this limitation by estimating personalized electrical conductivity values based on anatomical information from MRI scans.\n\n**B)** Conventional volume conductor modeling relies heavily on tissue segmentation, which can be time-consuming and prone to errors. The proposed approach addresses this limitation by avoiding the need for tissue segmentation and instead using MRI intensity values to estimate water content values.\n\n**C)** Conventional volume conductor modeling assumes a fixed electrical conductivity for all tissues, which can lead to inaccurate electric field results. The proposed approach addresses this limitation by estimating personalized electrical conductivity values based on anatomical information from MRI scans.\n\n**D)** Conventional volume conductor modeling uses a fixed electrical conductivity for all tissues, which can lead to inaccurate electric field results. The proposed approach addresses this limitation by using a convolutional neural network to estimate personalized electrical conductivity values based on anatomical information from MRI scans.\n\n**Correct Answer:** A) Conventional volume conductor modeling assumes a uniform electrical conductivity across all tissues, which can lead to inaccurate electric field results. The proposed approach addresses this limitation by estimating personalized electrical conductivity values based on anatomical information from MRI scans.\n\n**Explanation:** The correct answer is A) because the documentation states that conventional volume conductor modeling assumes a uniform electrical conductivity for each tissue, which is an unrealistic assumption. The proposed approach addresses this limitation by estimating personalized electrical conductivity values based on anatomical information from MRI scans, which provides more accurate electric field results."}, "35": {"documentation": {"title": "A Class of Maximal-Rate, Low-PAPR, Non-square Complex Orthogonal Designs", "source": "Smarajit Das, B. Sundar Rajan", "docs_id": "0808.1400", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Class of Maximal-Rate, Low-PAPR, Non-square Complex Orthogonal Designs. Space-time block codes (STBCs) from non-square complex orthogonal designs are bandwidth efficient when compared with those from square real/complex orthogonal designs. Though there exists rate-1 ROD for any number of transmit antennas, rate-1 complex orthogonal designs (COD) does not exist for more than 2 transmit antennas. Liang (IEEE Trans. Inform. Theory, 2003) and Lu et al (IEEE Trans. Inform. Theory, 2005) have constructed a class of maximal rate non-square CODs where the rate is ${1/2}+\\frac{1}{n}$ if number of transmit antennas $n$ is even and ${1/2}+\\frac{1}{n+1}$ if $n$ is odd. In this paper, we present a simple construction for maximal rate non-square CODs obtained from square CODs which resembles the construction of rate-1 non-square RODs from square RODs. These designs are shown to be amenable for construction of a class of generalized CODs (called Coordinate-Interleaved Scaled CODs) with low peak-to-average power ratio (PAPR) having the same parameters as the maximal rate codes. Simulation results indicate that these codes perform better than the existing maximal rate codes under peak power constraint while performing the same under average power constraint."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a non-square complex orthogonal design (COD) constructed from a square COD. What is the rate of this non-square COD, and how does it compare to the rate of a maximal rate square COD?\n\n**A)** The rate of the non-square COD is ${1/2}+\\frac{1}{n}$, where $n$ is the number of transmit antennas, and it is greater than the rate of the maximal rate square COD.\n\n**B)** The rate of the non-square COD is ${1/2}+\\frac{1}{n}$, where $n$ is the number of transmit antennas, and it is less than the rate of the maximal rate square COD.\n\n**C)** The rate of the non-square COD is ${1/2}+\\frac{1}{n+1}$, where $n$ is the number of transmit antennas, and it is equal to the rate of the maximal rate square COD.\n\n**D)** The rate of the non-square COD is ${1/2}+\\frac{1}{n}$, where $n$ is the number of transmit antennas, and it is only applicable when $n$ is odd.\n\n**Correct Answer:** B) The rate of the non-square COD is ${1/2}+\\frac{1}{n}$, where $n$ is the number of transmit antennas, and it is less than the rate of the maximal rate square COD.\n\n**Explanation:** According to the provided documentation, the rate of the non-square COD is ${1/2}+\\frac{1}{n}$, where $n$ is the number of transmit antennas. This rate is less than the rate of the maximal rate square COD, which is ${1/2}+\\frac{1}{n}$ when $n$ is even and ${1/2}+\\frac{1}{n+1}$ when $n$ is odd. Therefore, option B is the correct answer."}, "36": {"documentation": {"title": "COGEVIS: A New Scale to Evaluate Cognition in Patients with Visual\n  Deficiency", "source": "Claire Meyniel, Dalila Samri (IM2A), Farah Stefano (CH St Joseph),\n  Joel Crevoisier, Florence Bont\\'e (SAPPH), Raffaella Migliaccio (ICM, IM2A,\n  UPMC), Laure Delaby (IM2A), Anne Bertrand (ARAMIS, UPMC, ICM), Marie Odile\n  Habert (CATI), Bruno Dubois (UPMC, ICM, IM2A), Bahram Bodaghi, St\\'ephane\n  Epelbaum (IM2A, ARAMIS, UPMC, ICM)", "docs_id": "1812.07328", "section": ["q-bio.NC", "cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "COGEVIS: A New Scale to Evaluate Cognition in Patients with Visual\n  Deficiency. We evaluated the cognitive status of visually impaired patients referred to low vision rehabilitation (LVR) based on a standard cognitive battery and a new evaluation tool, named the COGEVIS, which can be used to assess patients with severe visual deficits. We studied patients aged 60 and above, referred to the LVR Hospital in Paris. Neurological and cognitive evaluations were performed in an expert memory center. Thirty-eight individuals, 17 women and 21 men with a mean age of 70.3 $\\pm$ 1.3 years and a mean visual acuity of 0.12 $\\pm$ 0.02, were recruited over a one-year period. Sixty-three percent of participants had normal cognitive status. Cognitive impairment was diagnosed in 37.5% of participants. The COGEVIS score cutoff point to screen for cognitive impairment was 24 (maximum score of 30) with a sensitivity of 66.7% and a specificity of 95%. Evaluation following 4 months of visual rehabilitation showed an improvement of Instrumental Activities of Daily Living (p = 0 004), National Eye Institute Visual Functioning Questionnaire (p = 0 035), and Montgomery-{\\AA}sberg Depression Rating Scale (p = 0 037). This study introduces a new short test to screen for cognitive impairment in visually impaired patients."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A 75-year-old woman with severe visual impairment is referred to a low vision rehabilitation center. She has a mean visual acuity of 0.12 and is experiencing cognitive impairment, as indicated by a COGEVIS score of 22. After 4 months of visual rehabilitation, her Instrumental Activities of Daily Living (IADL) scores have improved significantly. However, her National Eye Institute Visual Functioning Questionnaire (NEI-VFQ) scores have not shown a significant improvement. Which of the following statements best describes the patient's cognitive status and visual rehabilitation outcomes?\n\nA) The patient's cognitive impairment is likely due to her severe visual impairment, and her visual rehabilitation has been successful in improving her IADL scores.\nB) The patient's cognitive impairment is likely due to factors other than her visual impairment, and her visual rehabilitation has been successful in improving her NEI-VFQ scores.\nC) The patient's cognitive impairment is likely due to her age and other comorbidities, and her visual rehabilitation has been successful in improving her IADL scores, but not her NEI-VFQ scores.\nD) The patient's cognitive impairment is likely due to her visual impairment, and her visual rehabilitation has been unsuccessful in improving her NEI-VFQ scores.\n\nCorrect Answer: C) The patient's cognitive impairment is likely due to her age and other comorbidities, and her visual rehabilitation has been successful in improving her IADL scores, but not her NEI-VFQ scores.\n\nExplanation: The patient's COGEVIS score of 22 indicates cognitive impairment, but the fact that her IADL scores have improved significantly after 4 months of visual rehabilitation suggests that her visual rehabilitation has been successful in improving her functional abilities. However, the lack of improvement in her NEI-VFQ scores suggests that her visual rehabilitation has not addressed her visual function, which may be due to her age and other comorbidities. This answer choice takes into account the complexities of the patient's situation and the limitations of the COGEVIS and NEI-VFQ tools."}, "37": {"documentation": {"title": "Latent fingerprint minutia extraction using fully convolutional network", "source": "Yao Tang, Fei Gao, Jufu Feng", "docs_id": "1609.09850", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Latent fingerprint minutia extraction using fully convolutional network. Minutiae play a major role in fingerprint identification. Extracting reliable minutiae is difficult for latent fingerprints which are usually of poor quality. As the limitation of traditional handcrafted features, a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises. Raw fingerprints are mapped to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given threshold. Then small regions centering at these minutia points are entered into a convolutional neural network (CNN) to reclassify these minutiae and calculate their orientations. The CNN shares convolutional layers with the fully convolutional network to speed up. 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database, we achieve 53\\% recall rate and 53\\% precise rate that outperform many other algorithms. Our trained model is also visualized to show that we have successfully extracted features preserving ridge information of a latent fingerprint."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional handcrafted features in fingerprint identification, and how does the proposed fully convolutional network (FCN) address this limitation?\n\n**A)** Traditional handcrafted features are limited by their inability to learn complex patterns, which is addressed by the FCN's ability to learn features directly from data.\n\n**B)** Traditional handcrafted features are limited by their inability to handle poor quality images, which is addressed by the FCN's use of convolutional layers to extract features from raw fingerprints.\n\n**C)** Traditional handcrafted features are limited by their inability to preserve ridge information, which is addressed by the FCN's use of convolutional layers to reclassify minutiae and calculate their orientations.\n\n**D)** Traditional handcrafted features are limited by their inability to handle complex background noises, which is addressed by the FCN's use of convolutional layers to learn features directly from data and map raw fingerprints to a minutia-score map.\n\n**Correct Answer:** D) Traditional handcrafted features are limited by their inability to handle complex background noises, which is addressed by the FCN's use of convolutional layers to learn features directly from data and map raw fingerprints to a minutia-score map.\n\n**Explanation:** The correct answer is D) because the documentation states that traditional handcrafted features are limited by their inability to handle complex background noises, and the proposed FCN addresses this limitation by using convolutional layers to learn features directly from data and map raw fingerprints to a minutia-score map. This allows for the extraction of a large number of minutiae, even in poor quality images."}, "38": {"documentation": {"title": "Spin-orbit coupling rule in bound fermions systems", "source": "J.-P. Ebran, E. Khan, A. Mutschler, D. Vretenar", "docs_id": "1506.00911", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin-orbit coupling rule in bound fermions systems. Spin-orbit coupling characterizes quantum systems such as atoms, nuclei, hypernuclei, quarkonia, etc., and is essential for understanding their spectroscopic properties. Depending on the system, the effect of spin-orbit coupling on shell structure is large in nuclei, small in quarkonia, perturbative in atoms. In the standard non-relativistic reduction of the single-particle Dirac equation, we derive a universal rule for the relative magnitude of the spin-orbit effect that applies to very different quantum systems, regardless of whether the spin-orbit coupling originates from the strong or electromagnetic interaction. It is shown that in nuclei the near equality of the mass of the nucleon and the difference between the large repulsive and attractive potentials explains the fact that spin-orbit splittings are comparable to the energy spacing between major shells. For a specific ratio between the particle mass and the effective potential whose gradient determines the spin-orbit force, we predict the occurrence of giant spin-orbit energy splittings that dominate the single-particle excitation spectrum."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary reason for the large spin-orbit effect in nuclei, according to the standard non-relativistic reduction of the single-particle Dirac equation?\n\nA) The strong interaction between nucleons dominates the spin-orbit coupling.\nB) The near equality of the nucleon mass and the difference between the large repulsive and attractive potentials explains the large spin-orbit effect.\nC) The electromagnetic interaction between nucleons is responsible for the spin-orbit coupling.\nD) The spin-orbit coupling is negligible in nuclei due to the small mass of the nucleon.\n\n**Correct Answer:** B) The near equality of the nucleon mass and the difference between the large repulsive and attractive potentials explains the large spin-orbit effect.\n\n**Explanation:** According to the documentation, the near equality of the nucleon mass and the difference between the large repulsive and attractive potentials in nuclei explains the large spin-orbit effect. This is because the spin-orbit coupling is proportional to the ratio of the particle mass to the effective potential, and in nuclei, this ratio is close to unity, resulting in a large spin-orbit effect."}, "39": {"documentation": {"title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival\n  Prediction using Patch-based Graph Convolutional Networks", "source": "Richard J. Chen, Ming Y. Lu, Muhammad Shaban, Chengkuan Chen, Tiffany\n  Y. Chen, Drew F. K. Williamson, Faisal Mahmood", "docs_id": "2107.13048", "section": ["eess.IV", "cs.CV", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival\n  Prediction using Patch-based Graph Convolutional Networks. Cancer prognostication is a challenging task in computational pathology that requires context-aware representations of histology features to adequately infer patient survival. Despite the advancements made in weakly-supervised deep learning, many approaches are not context-aware and are unable to model important morphological feature interactions between cell identities and tissue types that are prognostic for patient survival. In this work, we present Patch-GCN, a context-aware, spatially-resolved patch-based graph convolutional network that hierarchically aggregates instance-level histology features to model local- and global-level topological structures in the tumor microenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five different cancer types from the Cancer Genome Atlas (TCGA), and demonstrate that Patch-GCN outperforms all prior weakly-supervised approaches by 3.58-9.46%. Our code and corresponding models are publicly available at https://github.com/mahmoodlab/Patch-GCN."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of existing weakly-supervised deep learning approaches to cancer prognostication, according to the authors of the \"Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks\" paper?\n\nA) They are unable to model the interactions between cell identities and tissue types.\nB) They are limited to only considering local-level topological structures.\nC) They are not context-aware and do not account for the tumor microenvironment.\nD) They are computationally expensive and require large amounts of data.\n\nCorrect Answer: C) They are not context-aware and do not account for the tumor microenvironment.\n\nExplanation: The authors of the paper highlight that existing weakly-supervised deep learning approaches are not context-aware and are unable to model important morphological feature interactions between cell identities and tissue types that are prognostic for patient survival. This suggests that the primary limitation of these approaches is their lack of context-awareness and inability to account for the tumor microenvironment."}, "40": {"documentation": {"title": "The Gap Between Model-Based and Model-Free Methods on the Linear\n  Quadratic Regulator: An Asymptotic Viewpoint", "source": "Stephen Tu and Benjamin Recht", "docs_id": "1812.03565", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Gap Between Model-Based and Model-Free Methods on the Linear\n  Quadratic Regulator: An Asymptotic Viewpoint. The effectiveness of model-based versus model-free methods is a long-standing question in reinforcement learning (RL). Motivated by recent empirical success of RL on continuous control tasks, we study the sample complexity of popular model-based and model-free algorithms on the Linear Quadratic Regulator (LQR). We show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension. For policy evaluation, we study a simple family of problem instances and show that nominal (certainty equivalence principle) control also requires several factors of state and input dimension fewer samples than the policy gradient method to reach the same level of control performance on these instances. Furthermore, the gap persists even when employing commonly used baselines. To the best of our knowledge, this is the first theoretical result which demonstrates a separation in the sample complexity between model-based and model-free methods on a continuous control task."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main finding regarding the sample complexity of model-based and model-free methods on the Linear Quadratic Regulator (LQR) task, as reported in the study \"The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint\"?\n\nA) Model-based methods require significantly more samples than model-free methods to achieve the same level of control performance.\nB) The sample complexity gap between model-based and model-free methods can be at least a factor of state dimension, and model-based methods require asymptotically fewer samples for policy evaluation.\nC) The study found that model-free methods are more effective than model-based methods for continuous control tasks.\nD) The sample complexity of model-based and model-free methods is the same, and the choice of method does not affect the number of samples required.\n\n**Correct Answer:** B) The sample complexity gap between model-based and model-free methods can be at least a factor of state dimension, and model-based methods require asymptotically fewer samples for policy evaluation.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"we show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension.\" This indicates that model-based methods can be more efficient than model-free methods in terms of sample complexity, especially for large state dimensions."}, "41": {"documentation": {"title": "Zipf's law and criticality in multivariate data without fine-tuning", "source": "David J. Schwab, Ilya Nemenman, Pankaj Mehta", "docs_id": "1310.0448", "section": ["q-bio.NC", "cond-mat.stat-mech", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Zipf's law and criticality in multivariate data without fine-tuning. The joint probability distribution of many degrees of freedom in biological systems, such as firing patterns in neural networks or antibody sequence composition in zebrafish, often follow Zipf's law, where a power law is observed on a rank-frequency plot. This behavior has recently been shown to imply that these systems reside near to a unique critical point where the extensive parts of the entropy and energy are exactly equal. Here we show analytically, and via numerical simulations, that Zipf-like probability distributions arise naturally if there is an unobserved variable (or variables) that affects the system, e. g. for neural networks an input stimulus that causes individual neurons in the network to fire at time-varying rates. In statistics and machine learning, these models are called latent-variable or mixture models. Our model shows that no fine-tuning is required, i.e. Zipf's law arises generically without tuning parameters to a point, and gives insight into the ubiquity of Zipf's law in a wide range of systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Zipf's law and criticality in multivariate data, what is the primary implication of the existence of an unobserved variable (or variables) that affects the system, as demonstrated in the provided Arxiv documentation?\n\n**A)** The system is likely to exhibit a uniform distribution of probabilities, rather than a power law.\n**B)** The system is likely to reside near a unique critical point where the extensive parts of the entropy and energy are exactly equal.\n**C)** The system is likely to require fine-tuning of parameters to achieve Zipf's law.\n**D)** The system is likely to exhibit a Gaussian distribution of probabilities.\n\n**Correct Answer:** B) The system is likely to reside near a unique critical point where the extensive parts of the entropy and energy are exactly equal.\n\n**Explanation:** The provided documentation states that the existence of an unobserved variable (or variables) that affects the system leads to the emergence of Zipf-like probability distributions, which in turn implies that the system resides near a unique critical point where the extensive parts of the entropy and energy are exactly equal. This is a key finding of the study, and option B accurately reflects this conclusion. Options A, C, and D are incorrect because they do not align with the implications of the study's findings."}, "42": {"documentation": {"title": "Quantitative in-situ monitoring of germinating barley seeds using X-ray\n  dark-field radiography", "source": "Mikkel Schou Nielsen, Kasper Borg Damkj{\\ae}r, Robert Feidenhans'l", "docs_id": "1701.03756", "section": ["physics.bio-ph", "physics.med-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantitative in-situ monitoring of germinating barley seeds using X-ray\n  dark-field radiography. During production of malt from barley seeds, cell walls and starch granules in the endosperm are degraded. Although this modification process is important for malt quality, the modification patterns of individual barley seeds have yet to be reported. The use of destructive microscopy methods have previously limited the investigations to ensemble averages. X-ray dark-field radiography is a recent non-destructive imaging method which is sensitive to microstructural variations. In this study, the method was applied for quantitative in-situ monitoring of barley seeds. Microstructural changes relating to water uptake and modification were monitored over a 43-55 h period. Sub-resolution stress cracks as well as a dark-field signal believed to originate from starch granules were detected. The evolution of the dark-field signal followed the known modification pattern in barley seeds. Through image analysis, quantitative parameters describing the movement of the front of the observed pattern were obtained. Based on these findings, X-ray dark-field radiography presents a possible novel approach to monitor the modification of germinating barley seeds."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using X-ray dark-field radiography in monitoring the modification of germinating barley seeds, as described in the study?\n\nA) It allows for the detection of sub-resolution stress cracks, which can lead to changes in malt quality.\nB) It provides a non-destructive imaging method, enabling the monitoring of individual barley seeds without damaging them.\nC) It enables the quantitative analysis of microstructural changes, allowing for the prediction of modification patterns.\nD) It is limited to detecting only starch granules, and not other microstructural variations.\n\nCorrect Answer: B) It provides a non-destructive imaging method, enabling the monitoring of individual barley seeds without damaging them.\n\nExplanation: The study highlights the use of X-ray dark-field radiography as a non-destructive method for monitoring the modification of germinating barley seeds. This is a significant advantage, as it allows for the investigation of individual seeds without damaging them, which would be impossible with traditional destructive microscopy methods. The correct answer, B, reflects this key benefit of the method. The other options, while related to the topic, are not the primary advantage of using X-ray dark-field radiography."}, "43": {"documentation": {"title": "Finite-time Lyapunov dimension and hidden attractor of the Rabinovich\n  system", "source": "N.V. Kuznetsov, G.A. Leonov, T.N. Mokaev, A. Prasad, M.D. Shrimali", "docs_id": "1504.04723", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finite-time Lyapunov dimension and hidden attractor of the Rabinovich\n  system. The Rabinovich system, describing the process of interaction between waves in plasma, is considered. It is shown that the Rabinovich system can exhibit a {hidden attractor} in the case of multistability as well as a classical {self-excited attractor}. The hidden attractor in this system can be localized by analytical-numerical methods based on the {continuation} and {perpetual points}. For numerical study of the attractors' dimension the concept of {finite-time Lyapunov dimension} is developed. A conjecture on the Lyapunov dimension of self-excited attractors and the notion of {exact Lyapunov dimension} are discussed. A comparative survey on the computation of the finite-time Lyapunov exponents by different algorithms is presented and an approach for a reliable numerical estimation of the finite-time Lyapunov dimension is suggested. Various estimates of the finite-time Lyapunov dimension for the hidden attractor and hidden transient chaotic set in the case of multistability are given."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main contribution of the Rabinovich system in the context of plasma dynamics, and how does it relate to the concepts of hidden attractors and finite-time Lyapunov dimension?\n\nA) The Rabinovich system is a classical self-excited attractor that can be localized using continuation and perpetual points methods, and its finite-time Lyapunov dimension is a measure of its chaotic behavior.\n\nB) The Rabinovich system exhibits multistability, and its hidden attractor can be localized using numerical methods, while the finite-time Lyapunov dimension is a measure of the system's sensitivity to initial conditions.\n\nC) The Rabinovich system is a hidden attractor that can be localized using analytical-numerical methods, and its finite-time Lyapunov dimension is a measure of its stability, which is related to the concept of exact Lyapunov dimension.\n\nD) The Rabinovich system is a classical self-excited attractor that can be localized using numerical methods, and its finite-time Lyapunov dimension is a measure of its chaotic behavior, which is related to the concept of perpetual points.\n\nCorrect Answer: B) The Rabinovich system exhibits multistability, and its hidden attractor can be localized using numerical methods, while the finite-time Lyapunov dimension is a measure of the system's sensitivity to initial conditions.\n\nExplanation: The correct answer is B) because the documentation states that the Rabinovich system can exhibit a hidden attractor in the case of multistability, and that numerical methods can be used to localize this attractor. Additionally, the finite-time Lyapunov dimension is mentioned as a concept related to the system's sensitivity to initial conditions, which is a key aspect of multistability. The other options are incorrect because they either misrepresent the Rabinovich system's behavior or confuse the concepts of hidden attractors and finite-time Lyapunov dimension."}, "44": {"documentation": {"title": "Controlling volatility of wind-solar power", "source": "Hans Lustfeld", "docs_id": "2102.00587", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Controlling volatility of wind-solar power. The main advantage of wind and solar power plants is the power production free of CO2. Their main disadvantage is the volatility of the generated power. According to the estimates of H.-W. Sinn[1], suppressing this volatility requires pumped-storage plants with a huge capacity, several orders of magnitude larger than the present available capacity in Germany[2]. Sinn concluded that wind-solar power can be used only together with conventional power plants as backups. However, based on German power data[3] of 2019 we show that the required storage capacity can significantly be reduced, provided i) a surplus of wind-solar power plants is supplied, ii) smart meters are installed, iii) partly a different kind of wind turbines and solar panels are used in Germany. Our calculations suggest that all the electric energy, presently produced in Germany, can be obtained from wind-solar power alone. And our results let us predict that wind-solar power can be used to produce in addition the energy for transportation, warm water, space heating and in part for process heating, meaning an increase of the present electric energy production by a factor of about 5[1]. Of course, to put such a prediction on firm ground the present calculations have to be confirmed for a period of many years. And it should be kept in mind, that in any case a huge number of wind turbines and solar panels is required."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the study on controlling volatility of wind-solar power, what is the main advantage of wind and solar power plants, and what is the main disadvantage?\n\n**A)** Wind and solar power plants have a huge capacity and produce power free of CO2, but their main disadvantage is that they are not reliable.\n**B)** Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for long-term energy production.\n**C)** Wind and solar power plants have a huge capacity and produce power free of CO2, but their main disadvantage is that they are not economically viable.\n**D)** Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for energy production.\n\n**Correct Answer:** D) Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for energy production.\n\n**Explanation:** The correct answer is D) because the study states that the main advantage of wind and solar power plants is that they produce power free of CO2, and the main disadvantage is that they are not suitable for energy production due to their volatility. The other options are incorrect because they either misstate the main advantage or the main disadvantage of wind and solar power plants."}, "45": {"documentation": {"title": "RIS-assisted UAV Communications for IoT with Wireless Power Transfer\n  Using Deep Reinforcement Learning", "source": "Khoi Khac Nguyen and Antonino Masaracchia and Tan Do-Duy and H.\n  Vincent Poor and Trung Q. Duong", "docs_id": "2108.02889", "section": ["eess.SP", "cs.GT", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RIS-assisted UAV Communications for IoT with Wireless Power Transfer\n  Using Deep Reinforcement Learning. Many of the devices used in Internet-of-Things (IoT) applications are energy-limited, and thus supplying energy while maintaining seamless connectivity for IoT devices is of considerable importance. In this context, we propose a simultaneous wireless power transfer and information transmission scheme for IoT devices with support from reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a first phase, IoT devices harvest energy from the UAV through wireless power transfer; and then in a second phase, the UAV collects data from the IoT devices through information transmission. To characterise the agility of the UAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at maximizing the total network sum-rate, we jointly optimize the trajectory of the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS. We formulate a Markov decision process and propose two deep reinforcement learning algorithms to solve the optimization problem of maximizing the total network sum-rate. Numerical results illustrate the effectiveness of the UAV's flying path optimization and the network's throughput of our proposed techniques compared with other benchmark schemes. Given the strict requirements of the RIS and UAV, the significant improvement in processing time and throughput performance demonstrates that our proposed scheme is well applicable for practical IoT applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed RIS-assisted UAV Communications for IoT with Wireless Power Transfer using Deep Reinforcement Learning, what is the primary objective of the Markov decision process, and how does it relate to the optimization of the UAV's flying path, energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS?\n\n**A)** The primary objective is to minimize the energy consumption of the UAV while maximizing the network sum-rate, and the Markov decision process optimizes the phaseshift matrix of the RIS to achieve this goal.\n\n**B)** The primary objective is to maximize the total network sum-rate, and the Markov decision process jointly optimizes the trajectory of the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS to achieve this goal.\n\n**C)** The primary objective is to minimize the latency of the network, and the Markov decision process optimizes the phaseshift matrix of the RIS to achieve this goal, while the UAV's flying path and energy harvesting scheduling are optimized separately.\n\n**D)** The primary objective is to maximize the throughput of the network, and the Markov decision process optimizes the trajectory of the UAV to achieve this goal, while the energy harvesting scheduling of IoT devices and the phaseshift matrix of the RIS are optimized separately.\n\n**Correct Answer:** B) The primary objective is to maximize the total network sum-rate, and the Markov decision process jointly optimizes the trajectory of the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS to achieve this goal.\n\n**Explanation:** The correct answer is B) because the problem statement explicitly mentions that the goal is to maximize the total network sum-rate, and the Markov decision process is formulated to jointly optimize the trajectory of the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift matrix of the RIS to achieve this goal. The other options are incorrect because they either misstate the primary objective or incorrectly separate the optimization of the UAV's flying path, energy harvesting scheduling, and phaseshift matrix."}, "46": {"documentation": {"title": "Prompt and non-prompt $J/\\psi$ elliptic flow in Pb+Pb collisions at\n  $\\sqrt{s_{_\\text{NN}}} = 5.02$ TeV with the ATLAS detector", "source": "ATLAS Collaboration", "docs_id": "1807.05198", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prompt and non-prompt $J/\\psi$ elliptic flow in Pb+Pb collisions at\n  $\\sqrt{s_{_\\text{NN}}} = 5.02$ TeV with the ATLAS detector. The elliptic flow of prompt and non-prompt $J/\\psi$ was measured in the dimuon decay channel in Pb+Pb collisions at $\\sqrt{s_{_\\text{NN}}}=5.02$ TeV with an integrated luminosity of $0.42~\\mathrm{nb}^{-1}$ with the ATLAS detector at the LHC. The prompt and non-prompt signals are separated using a two-dimensional simultaneous fit of the invariant mass and pseudo-proper decay time of the dimuon system from the $J/\\psi$ decay. The measurement is performed in the kinematic range of dimuon transverse momentum and rapidity $9<p_\\mathrm{T}<30$ GeV, $|y|<2$, and 0-60% collision centrality. The elliptic flow coefficient, $v_2$, is evaluated relative to the event plane and the results are presented as a function of transverse momentum, rapidity and centrality. It is found that prompt and non-prompt $J/\\psi$ mesons have non-zero elliptic flow. Prompt $J/\\psi$ $v_2$ decreases as a function of $p_\\mathrm{T}$, while non-prompt $J/\\psi$ it is, with limited statistical significance, consistent with a flat behaviour over the studied kinematic region. There is no observed dependence on rapidity or centrality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the results of the ATLAS experiment on the elliptic flow of prompt and non-prompt $J/\\psi$ mesons in Pb+Pb collisions at $\\sqrt{s_{_\\text{NN}}} = 5.02$ TeV. What can be inferred about the behavior of the prompt and non-prompt $J/\\psi$ elliptic flow coefficients, $v_2$, as a function of transverse momentum, rapidity, and centrality?\n\nA) The prompt $J/\\psi$ $v_2$ is consistent with a flat behavior over the entire kinematic region, while the non-prompt $J/\\psi$ $v_2$ decreases with increasing transverse momentum.\n\nB) The prompt $J/\\psi$ $v_2$ decreases with increasing transverse momentum, while the non-prompt $J/\\psi$ $v_2$ is consistent with a flat behavior over the entire kinematic region.\n\nC) The prompt $J/\\psi$ $v_2$ is consistent with a flat behavior over the entire kinematic region, while the non-prompt $J/\\psi$ $v_2$ decreases with increasing rapidity.\n\nD) The prompt $J/\\psi$ $v_2$ decreases with increasing centrality, while the non-prompt $J/\\psi$ $v_2$ is consistent with a flat behavior over the entire kinematic region.\n\nCorrect Answer: B) The prompt $J/\\psi$ $v_2$ decreases with increasing transverse momentum, while the non-prompt $J/\\psi$ $v_2$ is consistent with a flat behavior over the entire kinematic region.\n\nExplanation: The correct answer is based on the statement in the documentation that \"prompt $J/\\psi$ $v_2$ decreases as a function of $p_\\mathrm{T}$, while non-prompt $J/\\psi$ $v_2$ is, with limited statistical significance, consistent with a flat behaviour over the studied kinematic region.\" This indicates that the prompt $J/\\psi$ elliptic flow coefficient decreases with increasing transverse momentum, while the non-prompt $J/\\psi$ elliptic flow coefficient is consistent with a flat behavior over the entire kinematic region."}, "47": {"documentation": {"title": "Optimal Linear Instrumental Variables Approximations", "source": "Juan Carlos Escanciano and Wei Li", "docs_id": "1805.03275", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Linear Instrumental Variables Approximations. This paper studies the identification and estimation of the optimal linear approximation of a structural regression function. The parameter in the linear approximation is called the Optimal Linear Instrumental Variables Approximation (OLIVA). This paper shows that a necessary condition for standard inference on the OLIVA is also sufficient for the existence of an IV estimand in a linear model. The instrument in the IV estimand is unknown and may not be identified. A Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed, which can be implemented by standard regression routines. We establish the asymptotic normality of the TSIV estimator assuming neither completeness nor identification of the instrument. As an important application of our analysis, we robustify the classical Hausman test for exogeneity against misspecification of the linear structural model. We also discuss extensions to weighted least squares criteria. Monte Carlo simulations suggest an excellent finite sample performance for the proposed inferences. Finally, in an empirical application estimating the elasticity of intertemporal substitution (EIS) with US data, we obtain TSIV estimates that are much larger than their standard IV counterparts, with our robust Hausman test failing to reject the null hypothesis of exogeneity of real interest rates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a structural regression model with a linear approximation of the structural regression function, where the parameter of interest is the Optimal Linear Instrumental Variables Approximation (OLIVA). Suppose we have a Two-Step IV (TSIV) estimator based on Tikhonov regularization, which can be implemented by standard regression routines. Under what conditions on the instrument and the structural model can we establish the asymptotic normality of the TSIV estimator, assuming neither completeness nor identification of the instrument?\n\n**A)** The TSIV estimator is asymptotically normal if the instrument is identified and the structural model is identified.\n**B)** The TSIV estimator is asymptotically normal if the instrument is not identified and the structural model is not identified.\n**C)** The TSIV estimator is asymptotically normal if the instrument is identified and the structural model is not identified.\n**D)** The TSIV estimator is asymptotically normal if the instrument is not identified and the structural model is identified.\n\n**Correct Answer:** C) The TSIV estimator is asymptotically normal if the instrument is identified and the structural model is not identified.\n\n**Explanation:** According to the paper, the TSIV estimator is asymptotically normal under the assumption that neither completeness nor identification of the instrument holds. This means that the instrument is not identified if it is not possible to determine its effect on the endogenous variable, and completeness refers to the assumption that the instrument is not a function of the endogenous variable. The paper states that the TSIV estimator is asymptotically normal if the instrument is identified, which means that it is possible to determine its effect on the endogenous variable. Additionally, the paper states that the TSIV estimator is asymptotically normal if the structural model is not identified, which means that the model is not fully specified and the instrument is not a function of the endogenous variable. Therefore, the correct answer is C) The TSIV estimator is asymptotically normal if the instrument is identified and the structural model is not identified."}, "48": {"documentation": {"title": "Discovering Language of the Stocks", "source": "Marko Po\\v{z}enel and Dejan Lavbi\\v{c}", "docs_id": "1902.08684", "section": ["cs.CE", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discovering Language of the Stocks. Stock prediction has always been attractive area for researchers and investors since the financial gains can be substantial. However, stock prediction can be a challenging task since stocks are influenced by a multitude of factors whose influence vary rapidly through time. This paper proposes a novel approach (Word2Vec) for stock trend prediction combining NLP and Japanese candlesticks. First, we create a simple language of Japanese candlesticks from the source OHLC data. Then, sentences of words are used to train the NLP Word2Vec model where training data classification also takes into account trading commissions. Finally, the model is used to predict trading actions. The proposed approach was compared to three trading models Buy & Hold, MA and MACD according to the yield achieved. We first evaluated Word2Vec on three shares of Apple, Microsoft and Coca-Cola where it outperformed the comparative models. Next we evaluated Word2Vec on stocks from Russell Top 50 Index where our Word2Vec method was also very successful in test phase and only fall behind the Buy & Hold method in validation phase. Word2Vec achieved positive results in all scenarios while the average yields of MA and MACD were still lower compared to Word2Vec."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach proposed in the paper \"Discovering Language of the Stocks\" for stock trend prediction, and how does it differ from traditional trading models?\n\n**A)** The paper proposes a traditional machine learning approach using historical stock prices, which is compared to three trading models: Buy & Hold, MA, and MACD.\n\n**B)** The paper proposes a novel approach combining NLP (Natural Language Processing) and Japanese candlesticks to predict stock trends, which is compared to three traditional trading models.\n\n**C)** The paper proposes a traditional rule-based approach using technical indicators, such as moving averages and relative strength index, to predict stock trends.\n\n**D)** The paper proposes a hybrid approach combining machine learning and traditional technical analysis to predict stock trends.\n\n**Correct Answer:** B) The paper proposes a novel approach combining NLP and Japanese candlesticks to predict stock trends, which is compared to three traditional trading models.\n\n**Explanation:** The correct answer is B) because the paper specifically mentions that it proposes a novel approach combining NLP and Japanese candlesticks to predict stock trends, which is a key aspect of the paper's methodology. The other options are incorrect because they do not accurately reflect the approach proposed in the paper. Option A is incorrect because it mentions traditional machine learning approaches, which are not mentioned in the paper. Option C is incorrect because it mentions traditional technical indicators, which are not the focus of the paper. Option D is incorrect because it mentions a hybrid approach, but the paper does not explicitly mention combining machine learning and traditional technical analysis."}, "49": {"documentation": {"title": "Top-quark pair production cross sections at NNLO+NNLL in pPb collisions\n  at $\\sqrt{s_{NN}}$ = 8.16 TeV", "source": "David d'Enterria", "docs_id": "1706.09521", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Top-quark pair production cross sections at NNLO+NNLL in pPb collisions\n  at $\\sqrt{s_{NN}}$ = 8.16 TeV. Total and fiducial top pair ($t\\bar{t}$) production cross sections in proton-lead (pPb) collisions at $\\sqrt{s_{NN}}$ = 8.16 TeV are computed at next-to-next-to-leading-order (NNLO) accuracy including next-to-next-to-leading-log (NNLL) gluon resummation, using the CT14 and CT10 proton parton distribution functions (PDF), and the EPPS16 and EPS09 nuclear PDF parametrizations for the lead ion. The total cross sections amount to $\\sigma(pPb\\to t\\bar{t}+X) = 59.0 \\pm 5.3$(CT14+EPPS16)$\\,^{+1.6}_{-2.1}$(scale) nb, and $57.5 \\pm \\,^{+4.3}_{-3.3}$(CT10+EPS09)$\\,^{+1.5}_{-2.0}$(scale) nb, with small modifications with respect to the result computed using the free proton PDF alone. The normalized ratio of pPb to pp cross sections (nuclear modification factor) is $R_{pPb} = 1.04 \\,^{\\pm 0.07(EPPS16)}_{\\pm0.03(EPS09)}$. In the lepton+jets decay mode, $t\\bar{t} \\to b\\bar{b} W(\\ell\\nu) W(q\\bar{q}')$, one expects $\\sim$600 $t\\bar{t}$ events in the 180 nb$^{-1}$ integrated luminosity collected in pPb collisions at the LHC so far, after typical acceptance and efficiency losses. Differential cross sections at NLO accuracy are presented as a function of transverse momentum and rapidity of the top quarks, and of their decay b-jets and isolated leptons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of top-quark pair production in pPb collisions at $\\sqrt{s_{NN}}$ = 8.16 TeV, what is the expected number of $t\\bar{t}$ events in the lepton+jets decay mode, assuming a luminosity of 180 nb$^{-1}$ and typical acceptance and efficiency losses?\n\nA){The expected number of $t\\bar{t}$ events is approximately 300.}\nB){The expected number of $t\\bar{t}$ events is approximately 600.}\nC){The expected number of $t\\bar{t}$ events is approximately 900.}\nD){The expected number of $t\\bar{t}$ events is approximately 1200.}\n\nCorrect Answer: B){The expected number of $t\\bar{t}$ events is approximately 600.}\n\nExplanation: The question is based on the statement in the documentation that \"In the lepton+jets decay mode, $t\\bar{t} \\to b\\bar{b} W(\\ell\\nu) W(q\\bar{q}')$, one expects $\\sim$600 $t\\bar{t}$ events in the 180 nb$^{-1}$ integrated luminosity collected in pPb collisions at the LHC so far, after typical acceptance and efficiency losses.\""}, "50": {"documentation": {"title": "High-dimensional Adaptive Minimax Sparse Estimation with Interactions", "source": "Chenglong Ye and Yuhong Yang", "docs_id": "1804.02482", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-dimensional Adaptive Minimax Sparse Estimation with Interactions. High-dimensional linear regression with interaction effects is broadly applied in research fields such as bioinformatics and social science. In this paper, we first investigate the minimax rate of convergence for regression estimation in high-dimensional sparse linear models with two-way interactions. We derive matching upper and lower bounds under three types of heredity conditions: strong heredity, weak heredity and no heredity. From the results: (i) A stronger heredity condition may or may not drastically improve the minimax rate of convergence. In fact, in some situations, the minimax rates of convergence are the same under all three heredity conditions; (ii) The minimax rate of convergence is determined by the maximum of the total price of estimating the main effects and that of estimating the interaction effects, which goes beyond purely comparing the order of the number of non-zero main effects $r_1$ and non-zero interaction effects $r_2$; (iii) Under any of the three heredity conditions, the estimation of the interaction terms may be the dominant part in determining the rate of convergence for two different reasons: 1) there exist more interaction terms than main effect terms or 2) a large ambient dimension makes it more challenging to estimate even a small number of interaction terms. Second, we construct an adaptive estimator that achieves the minimax rate of convergence regardless of the true heredity condition and the sparsity indices $r_1, r_2$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of high-dimensional adaptive minimax sparse estimation with interactions, what is the primary factor that determines the minimax rate of convergence, according to the results of the paper?\n\n**A)** The number of non-zero main effects $r_1$ and non-zero interaction effects $r_2$.\n**B)** The strength of the heredity condition.\n**C)** The total price of estimating the main effects and the interaction effects.\n**D)** The ambient dimension of the data.\n\n**Correct Answer:** C) The total price of estimating the main effects and the interaction effects.\n\n**Explanation:** The paper states that the minimax rate of convergence is determined by the maximum of the total price of estimating the main effects and that of estimating the interaction effects. This is a key finding that goes beyond simply comparing the order of the number of non-zero main effects and non-zero interaction effects. The correct answer requires an understanding of the paper's results and the ability to identify the primary factor that determines the minimax rate of convergence."}, "51": {"documentation": {"title": "The Catchment Area of Jets", "source": "Matteo Cacciari, Gavin P. Salam and Gregory Soyez", "docs_id": "0802.1188", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Catchment Area of Jets. The area of a jet is a measure of its susceptibility to radiation, like pileup or underlying event (UE), that on average, in the jet's neighbourhood, is uniform in rapidity and azimuth. In this article we establish a theoretical grounding for the discussion of jet areas, introducing two main definitions, passive and active areas, which respectively characterise the sensitivity to pointlike or diffuse pileup and UE radiation. We investigate the properties of jet areas for three standard jet algorithms, k_t, Cambridge/Aachen and SISCone. Passive areas for single-particle jets are equal to the naive geometrical expectation \\pi R^2, but acquire an anomalous dimension at higher orders in the coupling, calculated here at leading order. The more physically relevant active areas differ from \\pi R^2 even for single-particle jets, substantially so in the case of the cone algorithms like SISCone with a Tevatron Run-II split--merge procedure. We compare our results with direct measures of areas in parton-shower Monte Carlo simulations and find good agreement with the main features of the analytical predictions. We furthermore justify the use of jet areas to subtract the contamination from pileup."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the main difference between the passive and active areas of a jet, and how do they relate to the sensitivity to pileup and underlying event (UE) radiation?\n\nA) Passive areas are more sensitive to UE radiation, while active areas are more sensitive to pointlike radiation.\nB) Passive areas are less sensitive to UE radiation, while active areas are more sensitive to pointlike radiation.\nC) Passive areas are more sensitive to pointlike radiation, while active areas are more sensitive to UE radiation.\nD) Passive areas are less sensitive to pointlike radiation, while active areas are more sensitive to UE radiation.\n\nCorrect Answer: B) Passive areas are less sensitive to UE radiation, while active areas are more sensitive to pointlike radiation.\n\nExplanation: According to the article, passive areas for single-particle jets are equal to the naive geometrical expectation \u03c0R^2, but acquire an anomalous dimension at higher orders in the coupling. This means that passive areas are less sensitive to UE radiation. On the other hand, active areas differ from \u03c0R^2 even for single-particle jets, and are more sensitive to pointlike radiation. This is particularly true for cone algorithms like SISCone, which have a Tevatron Run-II split-merge procedure."}, "52": {"documentation": {"title": "K-theory and phase transitions at high energies", "source": "T.V. Obikhod", "docs_id": "1604.05447", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "K-theory and phase transitions at high energies. The duality between $E_8\\times E_8$ heteritic string on manifold $K3\\times T^2$ and Type IIA string compactified on a Calabi-Yau manifold induces a correspondence between vector bundles on $K3\\times T^2$ and Calabi-Yau manifolds. Vector bundles over compact base space $K3\\times T^2$ form the set of isomorphism classes, which is a semi-ring under the operation of Whitney sum and tensor product. The construction of semi-ring $Vect\\ X$ of isomorphism classes of complex vector bundles over X leads to the ring $KX=K(Vect\\ X)$, called Grothendieck group. As K3 has no isometries and no non-trivial one-cycles, so vector bundle winding modes arise from the $T^2$ compactification. Since we have focused on supergravity in d=11, there exist solutions in d=10 for which space-time is Minkowski space and extra dimensions are $K3\\times T^2$. The complete set of soliton solutions of supergravity theory is characterized by RR charges, identified by K-theory. Toric presentation of Calabi-Yau through Batyrev's toric approximation enables us to connect transitions between Calabi-Yau manifolds, classified by enhanced symmetry group, with K-theory classification."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a Type IIA string compactified on a Calabi-Yau manifold $X$ with a $K3\\times T^2$ factor. The vector bundle $E$ over $K3\\times T^2$ is given by the Whitney sum of two line bundles $L_1$ and $L_2$ over $K3$, and a tensor product of $L_1$ and $L_2$ over $T^2$. What is the corresponding element in the Grothendieck group $KX$?\n\nA) $[L_1] + [L_2]$\nB) $[L_1] \\cdot [L_2]$\nC) $[L_1] + [L_2] \\cdot [L_1]$\nD) $[L_1] \\cdot [L_2] + [L_1] \\cdot [L_2]$\n\nCorrect Answer: C) $[L_1] + [L_2] \\cdot [L_1]$\n\nExplanation: The Grothendieck group $KX$ is generated by isomorphism classes of complex vector bundles over $X$. In this case, we have a vector bundle $E$ over $K3\\times T^2$ given by the Whitney sum of two line bundles $L_1$ and $L_2$ over $K3$, and a tensor product of $L_1$ and $L_2$ over $T^2$. The element in $KX$ corresponding to $E$ is given by the sum of the elements corresponding to $L_1$ and $L_2$, with the latter element being multiplied by itself due to the tensor product over $T^2$. This is because the tensor product over $T^2$ introduces a new factor of $L_1$ in the bundle, which is accounted for by multiplying the element corresponding to $L_2$ by itself."}, "53": {"documentation": {"title": "A new graph-based two-sample test for multivariate and object data", "source": "Hao Chen and Jerome H. Friedman", "docs_id": "1307.6294", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new graph-based two-sample test for multivariate and object data. Two-sample tests for multivariate data and especially for non-Euclidean data are not well explored. This paper presents a novel test statistic based on a similarity graph constructed on the pooled observations from the two samples. It can be applied to multivariate data and non-Euclidean data as long as a dissimilarity measure on the sample space can be defined, which can usually be provided by domain experts. Existing tests based on a similarity graph lack power either for location or for scale alternatives. The new test utilizes a common pattern that was overlooked previously, and works for both types of alternatives. The test exhibits substantial power gains in simulation studies. Its asymptotic permutation null distribution is derived and shown to work well under finite samples, facilitating its application to large data sets. The new test is illustrated on two applications: The assessment of covariate balance in a matched observational study, and the comparison of network data under different conditions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A new graph-based two-sample test for multivariate and object data is proposed in the paper. What is the primary limitation of existing tests based on a similarity graph, and how does the new test address this limitation?\n\n**A)** Existing tests lack power for location alternatives, but the new test is limited to scale alternatives.\n**B)** Existing tests lack power for scale alternatives, but the new test is limited to location alternatives.\n**C)** Existing tests lack power for both location and scale alternatives, but the new test is limited to only one of these alternatives.\n**D)** Existing tests lack power for both location and scale alternatives, and the new test addresses this limitation by utilizing a common pattern that was overlooked previously.\n\n**Correct Answer:** D) Existing tests lack power for both location and scale alternatives, and the new test addresses this limitation by utilizing a common pattern that was overlooked previously.\n\n**Explanation:** The correct answer is D) because the paper states that existing tests based on a similarity graph \"lack power either for location or for scale alternatives\". This indicates that both location and scale alternatives are a limitation of existing tests. The new test, on the other hand, \"utilizes a common pattern that was overlooked previously, and works for both types of alternatives\", which means it addresses both limitations."}, "54": {"documentation": {"title": "Generating the Observed Baryon Asymmetry from the Inflaton Field", "source": "Mark P. Hertzberg, Johanna Karouby", "docs_id": "1309.0010", "section": ["hep-ph", "astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generating the Observed Baryon Asymmetry from the Inflaton Field. We propose a mechanism by which the inflaton can generate baryogenesis, by taking the inflaton to be a complex scalar field with a weakly broken global symmetry and present a new version of the Affleck-Dine mechanism. The smallness of the breaking is motivated both by technical naturalness and a requirement for inflation. We study inflation driven by a quadratic potential for simplicity and discuss generalizations to other potentials. We compute the inflationary dynamics and find that a conserved particle number is obtained towards the end of inflation. We then explain in detail the later decay to baryons. We present two promising embeddings in particle physics: (i) using high dimension operators for a gauge singlet; we find this leads to the observed asymmetry for decay controlled by the ~ grand unified theory scale and this is precisely the regime where the effective field theory applies. (ii) using a colored inflaton, which requires small couplings. We also point out two observational consequences: a possible large scale dipole in the baryon density, and a striking prediction of isocurvature fluctuations whose amplitude is found to be just below current limits and potentially detectable in future data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism proposed in the paper \"Generating the Observed Baryon Asymmetry from the Inflaton Field\" for generating the observed baryon asymmetry in the universe, and what are the two promising embeddings in particle physics that are presented as a result of this mechanism?\n\nA) Affleck-Dine mechanism with a quadratic potential, embedding in the Standard Model via a gauge singlet\nB) Affleck-Dine mechanism with a quadratic potential, embedding in the Standard Model via a colored inflaton\nC) Affleck-Dine mechanism with a non-quadratic potential, embedding in the Standard Model via a gauge singlet\nD) Affleck-Dine mechanism with a non-quadratic potential, embedding in the Standard Model via a colored inflaton\n\nCorrect Answer: B) Affleck-Dine mechanism with a quadratic potential, embedding in the Standard Model via a colored inflaton\n\nExplanation: The paper proposes a new version of the Affleck-Dine mechanism, which involves a complex scalar field with a weakly broken global symmetry. The mechanism is driven by a quadratic potential and is presented as a result of this potential. The two promising embeddings in particle physics that are presented are (i) using high dimension operators for a gauge singlet, which leads to the observed asymmetry for decay controlled by the ~ grand unified theory scale, and (ii) using a colored inflaton, which requires small couplings. Option B is the correct answer because it accurately describes the Affleck-Dine mechanism with a quadratic potential and the embedding in the Standard Model via a colored inflaton."}, "55": {"documentation": {"title": "The Erd\\H{o}s-Ko-Rado property for some permutation groups", "source": "Bahman Ahmadi and Karen Meagher", "docs_id": "1311.7060", "section": ["math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Erd\\H{o}s-Ko-Rado property for some permutation groups. A subset in a group $G \\leq Sym(n)$ is intersecting if for any pair of permutations $\\pi,\\sigma$ in the subset there is an $i \\in \\{1,2,\\dots,n\\}$ such that $\\pi(i) = \\sigma(i)$. If the stabilizer of a point is the largest intersecting set in a group, we say that the group has the Erd\\H{o}s-Ko-Rado (EKR) property. Moreover, the group has the strict EKR property if every intersecting set of maximum size in the group is either the stabilizer of a point or the coset of the stabilizer of a point. In this paper we look at several families of permutation groups and determine if the groups have either the EKR property or the strict EKR property. First, we prove that all cyclic groups have the strict EKR property. Next we show that all dihedral and Frobenius groups have the EKR property and we characterize which ones have the strict EKR property. Further, we show that if all the groups in an external direct sum or an internal direct sum have the EKR (or strict EKR) property, then the product does as well. Finally, we show that the wreath product of two groups with EKR property also has the EKR property."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $G$ be a permutation group on $n$ elements, and let $H$ be a subgroup of $G$ that has the Erd\u0151s-Ko-Rado (EKR) property. Suppose that $H$ is a direct summand of $G$, and let $K$ be a subgroup of $G$ such that $H \\cap K$ is a subgroup of $H$. Which of the following statements is true?\n\nA) If $H$ has the strict EKR property, then $K$ also has the strict EKR property.\nB) If $H$ has the strict EKR property, then $K$ has the EKR property.\nC) If $H$ has the EKR property, then $K$ has the strict EKR property.\nD) If $H$ has the EKR property, then $K$ has the EKR property.\n\nCorrect Answer: B) If $H$ has the strict EKR property, then $K$ has the EKR property.\n\nExplanation: This question requires the student to understand the properties of permutation groups and their subgroups. The correct answer is based on the fact that if $H$ has the strict EKR property, then every subgroup of $H$ also has the EKR property. This is because the strict EKR property implies that every intersecting set of maximum size in $H$ is either the stabilizer of a point or the coset of the stabilizer of a point. Since $H$ is a direct summand of $G$, every subgroup of $H$ is also a subgroup of $G$, and therefore has the EKR property."}, "56": {"documentation": {"title": "Every Corporation Owns Its Image: Corporate Credit Ratings via\n  Convolutional Neural Networks", "source": "Bojing Feng, Wenfang Xue, Bindang Xue, Zeyu Liu", "docs_id": "2012.03744", "section": ["q-fin.RM", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Every Corporation Owns Its Image: Corporate Credit Ratings via\n  Convolutional Neural Networks. Credit rating is an analysis of the credit risks associated with a corporation, which reflect the level of the riskiness and reliability in investing. There have emerged many studies that implement machine learning techniques to deal with corporate credit rating. However, the ability of these models is limited by enormous amounts of data from financial statement reports. In this work, we analyze the performance of traditional machine learning models in predicting corporate credit rating. For utilizing the powerful convolutional neural networks and enormous financial data, we propose a novel end-to-end method, Corporate Credit Ratings via Convolutional Neural Networks, CCR-CNN for brevity. In the proposed model, each corporation is transformed into an image. Based on this image, CNN can capture complex feature interactions of data, which are difficult to be revealed by previous machine learning models. Extensive experiments conducted on the Chinese public-listed corporate rating dataset which we build, prove that CCR-CNN outperforms the state-of-the-art methods consistently."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of traditional machine learning models in predicting corporate credit rating, according to the study \"Every Corporation Owns Its Image: Corporate Credit Ratings via Convolutional Neural Networks\"?\n\nA) The models are not able to capture complex feature interactions in financial data.\nB) The models require enormous amounts of data from financial statement reports.\nC) The models are not able to handle non-linear relationships between financial data and credit risk.\nD) The models are not able to generalize well to new, unseen data.\n\nCorrect Answer: B) The models require enormous amounts of data from financial statement reports.\n\nExplanation: The study states that \"there have emerged many studies that implement machine learning techniques to deal with corporate credit rating. However, the ability of these models is limited by enormous amounts of data from financial statement reports.\" This suggests that the primary limitation of traditional machine learning models is indeed the requirement of large amounts of data from financial statement reports."}, "57": {"documentation": {"title": "High order algorithm for the time-tempered fractional Feynman-Kac\n  equation", "source": "Minghua Chen and Weihua Deng", "docs_id": "1607.05929", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High order algorithm for the time-tempered fractional Feynman-Kac\n  equation. We provide and analyze the high order algorithms for the model describing the functional distributions of particles performing anomalous motion with power-law jump length and tempered power-law waiting time. The model is derived in [Wu, Deng, and Barkai, Phys. Rev. E., 84 (2016), 032151], being called the time-tempered fractional Feynman-Kac equation. The key step of designing the algorithms is to discretize the time tempered fractional substantial derivative, being defined as $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t)\\!=\\!D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t)\\!-\\!\\lambda^\\gamma G(x,p,t) ~{\\rm with}~\\widetilde{\\lambda}=\\lambda+ pU(x),\\, p=\\rho+J\\eta,\\, J=\\sqrt{-1},$$ where $$D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{-\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz,$$ and $\\lambda \\ge 0$, $0<\\gamma<1$, $\\rho>0$, and $\\eta$ is a real number. The designed schemes are unconditionally stable and have the global truncation error $\\mathcal{O}(\\tau^2+h^2)$, being theoretically proved and numerically verified in {\\em complex} space. Moreover, some simulations for the distributions of the first passage time are performed, and the second order convergence is also obtained for solving the `physical' equation (without artificial source term)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider the time-tempered fractional Feynman-Kac equation, which models anomalous motion with power-law jump length and tempered power-law waiting time. The key step in designing high-order algorithms for this equation is discretizing the time-tempered fractional substantial derivative. What is the expression for the time-tempered fractional substantial derivative, denoted as $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t)$$, in terms of the given parameters $\\lambda$, $\\gamma$, $\\widetilde{\\lambda}$, $G(x,p,t)$, and $p$?\n\nA) $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{-\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz$$\nB) $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz$$\nC) $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz$$\nD) $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{-\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz$$\n\n**Correct Answer:** D) $${^S\\!}D_t^{\\gamma,\\widetilde{\\lambda}} G(x,p,t) =\\frac{1}{\\Gamma(1-\\gamma)} \\left[\\frac{\\partial}{\\partial t}+\\widetilde{\\lambda} \\right] \\int_{0}^t{\\left(t-z\\right)^{-\\gamma}}e^{-\\widetilde{\\lambda}\\cdot(t-z)}{G(x,p,z)}dz$$\n\n**Explanation:** The correct answer is D) because it matches the expression given in the documentation for the time-tempered fractional substantial derivative. The other options are incorrect because they either have a different exponent for the integral term or a different expression for the derivative term."}, "58": {"documentation": {"title": "Switching between phenotypes and population extinction", "source": "Ingo Lohmar and Baruch Meerson", "docs_id": "1107.5192", "section": ["q-bio.PE", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Switching between phenotypes and population extinction. Many types of bacteria can survive under stress by switching stochastically between two different phenotypes: the \"normals\" who multiply fast, but are vulnerable to stress, and the \"persisters\" who hardly multiply, but are resilient to stress. Previous theoretical studies of such bacterial populations have focused on the \\emph{fitness}: the asymptotic rate of unbounded growth of the population. Yet for an isolated population of established (and not very large) size, a more relevant measure may be the population \\emph{extinction risk} due to the interplay of adverse extrinsic variations and intrinsic noise of birth, death and switching processes. Applying a WKB approximation to the pertinent master equation of such a two-population system, we quantify the extinction risk, and find the most likely path to extinction under both favorable and adverse conditions. Analytical results are obtained both in the biologically relevant regime when the switching is rare compared with the birth and death processes, and in the opposite regime of frequent switching. We show that rare switches are most beneficial in reducing the extinction risk."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of bacterial populations that switch between phenotypes to survive stress, what is the most likely path to extinction under adverse conditions, and how does the frequency of switching between phenotypes affect the population's extinction risk?\n\n**A)** The most likely path to extinction under adverse conditions is the rapid multiplication of the \"persisters\" phenotype, which allows the population to adapt to changing environmental conditions. However, frequent switching between phenotypes increases the population's extinction risk.\n\n**B)** The most likely path to extinction under adverse conditions is the slow multiplication of the \"normals\" phenotype, which is vulnerable to stress. However, rare switches between phenotypes reduce the population's extinction risk by allowing the population to adapt to changing environmental conditions.\n\n**C)** The most likely path to extinction under adverse conditions is the simultaneous multiplication of both the \"normals\" and \"persisters\" phenotypes, which leads to a rapid decline in population size. However, the frequency of switching between phenotypes has no effect on the population's extinction risk.\n\n**D)** The most likely path to extinction under adverse conditions is the extinction of the \"persisters\" phenotype, which is resilient to stress. However, frequent switching between phenotypes increases the population's extinction risk by allowing the \"normals\" phenotype to dominate the population.\n\n**Correct Answer:** B) The most likely path to extinction under adverse conditions is the slow multiplication of the \"normals\" phenotype, which is vulnerable to stress. However, rare switches between phenotypes reduce the population's extinction risk by allowing the population to adapt to changing environmental conditions.\n\n**Explanation:** The correct answer is based on the analytical results obtained in the biologically relevant regime, where the switching is rare compared with the birth and death processes. In this regime, the population's extinction risk is reduced by rare switches between phenotypes, which allow the population to adapt to changing environmental conditions. The incorrect answers are based on incorrect assumptions about the relationship between the frequency of switching and the population's extinction risk."}, "59": {"documentation": {"title": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data", "source": "Katsuya Ito, Kei Nakagawa", "docs_id": "2002.00724", "section": ["q-fin.ST", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data. In time-series analysis, the term \"lead-lag effect\" is used to describe a delayed effect on a given time series caused by another time series. lead-lag effects are ubiquitous in practice and are specifically critical in formulating investment strategies in high-frequency trading. At present, there are three major challenges in analyzing the lead-lag effects. First, in practical applications, not all time series are observed synchronously. Second, the size of the relevant dataset and rate of change of the environment is increasingly faster, and it is becoming more difficult to complete the computation within a particular time limit. Third, some lead-lag effects are time-varying and only last for a short period, and their delay lengths are often affected by external factors. In this paper, we propose NAPLES (Negative And Positive lead-lag EStimator), a new statistical measure that resolves all these problems. Through experiments on artificial and real datasets, we demonstrate that NAPLES has a strong correlation with the actual lead-lag effects, including those triggered by significant macroeconomic announcements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of time-series analysis, what are the three major challenges in analyzing lead-lag effects, as identified in the paper \"NAPLES\"?\n\n**A)** Insufficient dataset size, computational complexity, and non-stationarity of the time series.\n**B)** Non-synchronous observation of time series, high-frequency data, and limited computational resources.\n**C)** Presence of outliers, non-linear relationships, and time-varying lead-lag effects.\n**D)** Inability to account for external factors, non-stationarity of the lead-lag effects, and limited dataset size.\n\n**Correct Answer:** B) Non-synchronous observation of time series, high-frequency data, and limited computational resources.\n\n**Explanation:** The paper highlights three major challenges in analyzing lead-lag effects: (1) non-synchronous observation of time series, (2) high-frequency data, and (3) limited computational resources. These challenges are critical in formulating investment strategies in high-frequency trading. The correct answer requires the candidate to identify the correct challenges mentioned in the paper.\n\n**Additional Marking Scheme:**\n\n* 1 mark for correctly identifying the three challenges (non-synchronous observation, high-frequency data, and limited computational resources)\n* 0.5 marks for correctly identifying at least two of the challenges\n* 0 marks for incorrectly identifying any of the challenges or providing an unrelated answer."}}