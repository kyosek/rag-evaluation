{"0": {"documentation": {"title": "Spatial parking planning design with mixed conventional and autonomous\n  vehicles", "source": "Qida Su, David Z.W. Wang", "docs_id": "2104.01773", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatial parking planning design with mixed conventional and autonomous\n  vehicles. Travellers in autonomous vehicles (AVs) need not to walk to the destination any more after parking like those in conventional human-driven vehicles (HVs). Instead, they can drop off directly at the destination and AVs can cruise for parking autonomously. It is a revolutionary change that such parking autonomy of AVs may increase the potential parking span substantially and affect the spatial parking equilibrium. Given this, from urban planners' perspective, it is of great necessity to reconsider the planning of parking supply along the city. To this end, this paper is the first to examine the spatial parking equilibrium considering the mix of AVs and HVs with parking cruising effect. It is found that the equilibrium solution of travellers' parking location choices can be biased due to the ignorance of cruising effects. On top of that, the optimal parking span of AVs at given parking supply should be no less than that at equilibrium. Besides, the optimal parking planning to minimize the total parking cost is also explored in a bi-level parking planning design problem (PPDP). While the optimal differentiated pricing allows the system to achieve optimal parking distribution, this study suggests that it is beneficial to encourage AVs to cruise further to park by reserving less than enough parking areas for AVs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of spatial parking planning design with mixed conventional and autonomous vehicles, what is the primary concern of urban planners regarding the parking autonomy of autonomous vehicles (AVs)?\n\nA) The potential increase in parking congestion due to AVs cruising for parking autonomously.\nB) The need to reserve more parking areas for AVs to accommodate their increased demand.\nC) The potential bias in travellers' parking location choices due to the ignorance of cruising effects.\nD) The impact of differentiated pricing on the optimal parking distribution.\n\n**Correct Answer:** C) The potential bias in travellers' parking location choices due to the ignorance of cruising effects.\n\n**Explanation:** The correct answer is C) The potential bias in travellers' parking location choices due to the ignorance of cruising effects. The paper highlights that the equilibrium solution of travellers' parking location choices can be biased due to the ignorance of cruising effects, which is a key concern for urban planners. The other options are incorrect because they do not accurately reflect the primary concern of urban planners regarding the parking autonomy of AVs. Option A is incorrect because the paper does not mention parking congestion as a concern. Option B is incorrect because the paper actually suggests reserving less than enough parking areas for AVs to encourage them to cruise further. Option D is incorrect because the paper does not discuss the impact of differentiated pricing on the optimal parking distribution."}, "1": {"documentation": {"title": "Electronic Hybridization of Large-Area Stacked Graphene Films", "source": "Jeremy T. Robinson, Scott W. Schmucker, C. Bogdan Diaconescu, James P.\n  Long, James C. Culbertson, Taisuke Ohta, Adam L. Friedman, Thomas E. Beechem", "docs_id": "1301.0246", "section": ["cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electronic Hybridization of Large-Area Stacked Graphene Films. Direct, tunable coupling between individually assembled graphene layers is a next step towards designer two-dimensional (2D) crystal systems, with relevance for fundamental studies and technological applications. Here we describe the fabrication and characterization of large-area (> cm^2), coupled bilayer graphene on SiO2/Si substrates. Stacking two graphene films leads to direct electronic interactions between layers, where the resulting film properties are determined by the local twist angle. Polycrystalline bilayer films have a \"stained-glass window\" appearance explained by the emergence of a narrow absorption band in the visible spectrum that depends on twist angle. Direct measurement of layer orientation via electron diffraction, together with Raman and optical spectroscopy, confirms the persistence of clean interfaces over large areas. Finally, we demonstrate that interlayer coupling can be reversibly turned off through chemical modification, enabling optical-based chemical detection schemes. Together, these results suggest that individual 2D crystals can be individually assembled to form electronically coupled systems suitable for large-scale applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of fabricating large-area, coupled bilayer graphene films on SiO2/Si substrates, as described in the study \"Electronic Hybridization of Large-Area Stacked Graphene Films\"?\n\nA) The ability to create a uniform, polycrystalline film structure\nB) The emergence of a narrow absorption band in the visible spectrum that depends on twist angle\nC) The potential for reversible interlayer coupling through chemical modification\nD) The creation of a uniform, transparent film with a \"stained-glass window\" appearance\n\nCorrect Answer: B) The emergence of a narrow absorption band in the visible spectrum that depends on twist angle\n\nExplanation: The study describes the fabrication and characterization of large-area, coupled bilayer graphene films, and highlights the emergence of a narrow absorption band in the visible spectrum that depends on the twist angle between the layers. This is a key finding that suggests the existence of a \"stained-glass window\" appearance in the films. While the other options are related to the study, they are not the primary advantage of fabricating these films. Option A is incorrect because the film structure is polycrystalline, not uniform. Option C is incorrect because while the study does demonstrate reversible interlayer coupling, this is not the primary advantage of fabricating these films. Option D is incorrect because the \"stained-glass window\" appearance is a result of the emergence of the narrow absorption band, not the primary advantage of fabricating these films."}, "2": {"documentation": {"title": "Group theoretical study of nonstrange and strange mixed symmetric baryon\n  states $[N_c-1,1]$ in the $1/N_c$ expansion", "source": "N. Matagne and Fl. Stancu", "docs_id": "1001.2214", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Group theoretical study of nonstrange and strange mixed symmetric baryon\n  states $[N_c-1,1]$ in the $1/N_c$ expansion. Using group theory arguments we extend and complete our previous work by deriving all SU(6) exact wave functions associated to the spectrum of mixed symmetric baryon states $[N_c-1,1]$ in the $1/N_c$ expansion. The extension to SU(6) enables us to study the mass spectra of both strange and nonstrange baryons, while previous work was restricted to nonstrange baryons described by SU(4). The wave functions are specially written in a form to allow a comparison with the approximate, customarily used wave functions, where the system is separated into a ground state core and an excited quark. We show that the matrix elements of the flavor operator calculated with the exact wave functions acquire the same asymptotic form at large $N_c$, irrespective of the spin-flavor multiplet contained in $[N_c-1,1]$, while with the approximate wave function one cannot obtain a similar behaviour. The isoscalar factors of the permutation group of $N_c$ particles derived here can be used in any problem where a given fermion system is described by the partition $[N_c-1,1]$, and one fermion has to be separated from the rest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the $1/N_c$ expansion, what is the key advantage of using the exact SU(6) wave functions for mixed symmetric baryon states $[N_c-1,1]$ over the approximate wave functions, and how does this relate to the behavior of the matrix elements of the flavor operator at large $N_c$?\n\n**A)** The exact wave functions allow for a more accurate separation of the system into a ground state core and an excited quark, leading to a better understanding of the mass spectra of baryons.\n\n**B)** The exact wave functions enable the derivation of isoscalar factors of the permutation group of $N_c$ particles, which can be used to study the behavior of fermion systems in various contexts.\n\n**C)** The exact wave functions acquire the same asymptotic form at large $N_c$ as the matrix elements of the flavor operator, regardless of the spin-flavor multiplet contained in $[N_c-1,1]$, whereas the approximate wave functions do not exhibit this behavior.\n\n**D)** The exact wave functions provide a more efficient method for calculating the matrix elements of the flavor operator, allowing for a faster computation of the mass spectra of baryons.\n\n**Correct Answer:** C) The exact wave functions acquire the same asymptotic form at large $N_c$ as the matrix elements of the flavor operator, regardless of the spin-flavor multiplet contained in $[N_c-1,1]$, whereas the approximate wave functions do not exhibit this behavior.\n\n**Explanation:** The correct answer, C, highlights the key advantage of using the exact SU(6) wave functions for mixed symmetric baryon states $[N_c-1,1]$. The exact wave functions allow for a more accurate understanding of the mass spectra of baryons, as they acquire the same asymptotic form at large $N_c$ as the matrix elements of the flavor operator, regardless of the spin-flavor multiplet contained in $[N_c-1,1]$. This is in contrast to the approximate wave functions, which do not exhibit this behavior. This understanding is crucial for studying the behavior of fermion systems in various contexts, particularly in the context of the $1/N_c$ expansion."}, "3": {"documentation": {"title": "Stock market microstructure inference via multi-agent reinforcement\n  learning", "source": "J. Lussange, I. Lazarevich, S. Bourgeois-Gironde, S. Palminteri, B.\n  Gutkin", "docs_id": "1909.07748", "section": ["q-fin.TR", "q-fin.CP", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stock market microstructure inference via multi-agent reinforcement\n  learning. Quantitative finance has had a long tradition of a bottom-up approach to complex systems inference via multi-agent systems (MAS). These statistical tools are based on modelling agents trading via a centralised order book, in order to emulate complex and diverse market phenomena. These past financial models have all relied on so-called zero-intelligence agents, so that the crucial issues of agent information and learning, central to price formation and hence to all market activity, could not be properly assessed. In order to address this, we designed a next-generation MAS stock market simulator, in which each agent learns to trade autonomously via model-free reinforcement learning. We calibrate the model to real market data from the London Stock Exchange over the years $2007$ to $2018$, and show that it can faithfully reproduce key market microstructure metrics, such as various price autocorrelation scalars over multiple time intervals. Agent learning thus enables model emulation of the microstructure with greater realism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a next-generation MAS stock market simulator, as described in the paper \"Stock market microstructure inference via multi-agent reinforcement learning\"?\n\nA) To improve the accuracy of financial models by incorporating agent information and learning\nB) To demonstrate the effectiveness of model-free reinforcement learning in simulating complex financial systems\nC) To address the limitations of traditional financial models that rely on zero-intelligence agents\nD) To develop a new framework for predicting stock prices using machine learning algorithms\n\n**Correct Answer:** C) To address the limitations of traditional financial models that rely on zero-intelligence agents\n\n**Explanation:** The question requires the test-taker to understand the context and motivation behind the development of the next-generation MAS stock market simulator. The correct answer, C, highlights the primary motivation as addressing the limitations of traditional financial models that rely on zero-intelligence agents, which cannot properly assess agent information and learning. The other options, while related to the topic, are not the primary motivation behind the development of the simulator. \n\n**Additional question:** What is the primary benefit of using model-free reinforcement learning in the MAS stock market simulator, as described in the paper?\n\nA) Improved accuracy of financial models\nB) Increased computational efficiency\nC) Ability to emulate complex market phenomena with greater realism\nD) Enhanced ability to predict stock prices\n\n**Correct Answer:** C) Ability to emulate complex market phenomena with greater realism\n\n**Explanation:** The correct answer, C, highlights the primary benefit of using model-free reinforcement learning in the MAS stock market simulator, which is the ability to emulate complex market phenomena with greater realism. This is because the simulator can learn to trade autonomously, allowing for a more realistic representation of market microstructure. The other options, while related to the topic, are not the primary benefit of using model-free reinforcement learning."}, "4": {"documentation": {"title": "Synchronisation in networks of delay-coupled type-I excitable systems", "source": "Andrew Keane and Thomas Dahms and Judith Lehnert and Sachin\n  Aralasurali Suryanarayana and Philipp H\\\"ovel and Eckehard Sch\\\"oll", "docs_id": "1209.1950", "section": ["cond-mat.dis-nn", "nlin.AO", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronisation in networks of delay-coupled type-I excitable systems. We use a generic model for type-I excitability (known as the SNIPER or SNIC model) to describe the local dynamics of nodes within a network in the presence of non-zero coupling delays. Utilising the method of the Master Stability Function, we investigate the stability of the zero-lag synchronised dynamics of the network nodes and its dependence on the two coupling parameters, namely the coupling strength and delay time. Unlike in the FitzHugh-Nagumo model (a model for type-II excitability), there are parameter ranges where the stability of synchronisation depends on the coupling strength and delay time. One important implication of these results is that there exist complex networks for which the adding of inhibitory links in a small-world fashion may not only lead to a loss of stable synchronisation, but may also restabilise synchronisation or introduce multiple transitions between synchronisation and desynchronisation. To underline the scope of our results, we show using the Stuart-Landau model that such multiple transitions do not only occur in excitable systems, but also in oscillatory ones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Investigate the stability of synchronisation in networks of delay-coupled type-I excitable systems, specifically the dependence of stability on the coupling strength and delay time. How do the results of this study relate to the FitzHugh-Nagumo model and the Stuart-Landau model, and what implications do these findings have for the design of complex networks?\n\n**A)** The stability of synchronisation in delay-coupled type-I excitable systems is independent of the coupling strength and delay time, and is only dependent on the initial conditions of the system. This is in contrast to the FitzHugh-Nagumo model, where synchronisation is stable for all values of the coupling strength and delay time.\n\n**B)** The study found that the stability of synchronisation in delay-coupled type-I excitable systems is dependent on both the coupling strength and delay time, and that there are parameter ranges where synchronisation is stable, unstable, or exhibits multiple transitions between synchronisation and desynchronisation. This is in contrast to the Stuart-Landau model, where synchronisation is stable for all values of the coupling strength and delay time.\n\n**C)** The results of this study suggest that the addition of inhibitory links in a small-world fashion may lead to a loss of stable synchronisation in complex networks, but may also restabilise synchronisation or introduce multiple transitions between synchronisation and desynchronisation. This is consistent with the findings of the FitzHugh-Nagumo model, which also showed that synchronisation can be unstable for certain values of the coupling strength and delay time.\n\n**D)** The study found that the stability of synchronisation in delay-coupled type-I excitable systems is only dependent on the initial conditions of the system, and that the coupling strength and delay time have no effect on the stability of synchronisation. This is in contrast to the Stuart-Landau model, which showed that synchronisation is stable for all values of the coupling strength and delay time.\n\n**Correct Answer:** B) The study found that the stability of synchronisation in delay-coupled type-I excitable systems is dependent on both the coupling strength and delay time, and that there are parameter ranges where synchronisation is stable, unstable, or exhibits multiple transitions between synchronisation and desynchronisation. This is in contrast to the Stuart-Landau model, where synchronisation is stable for all values of the coupling strength and delay time.\n\n**Explanation:** The correct answer is B) because the study specifically states that the stability of synchronisation in delay-coupled type-I excitable systems depends on both the coupling strength and delay time, and that there are parameter ranges where synchronisation is stable, unstable, or exhibits multiple transitions between synchronisation and desynchronisation. This is in contrast to the Stuart-Landau model, which showed that synchronisation is stable for all values of the coupling strength and delay time. The other options are incorrect because they either misrepresent the findings of the study or contradict the results of the Stuart-Landau model."}, "5": {"documentation": {"title": "SABCEMM-A Simulator for Agent-Based Computational Economic Market Models", "source": "Torsten Trimborn, Philipp Otte, Simon Cramer, Max Beikirch, Emma\n  Pabich, Martin Frank", "docs_id": "1801.01811", "section": ["q-fin.CP", "econ.EM", "q-fin.GN", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SABCEMM-A Simulator for Agent-Based Computational Economic Market Models. We introduce the simulation tool SABCEMM (Simulator for Agent-Based Computational Economic Market Models) for agent-based computational economic market (ABCEM) models. Our simulation tool is implemented in C++ and we can easily run ABCEM models with several million agents. The object-oriented software design enables the isolated implementation of building blocks for ABCEM models, such as agent types and market mechanisms. The user can design and compare ABCEM models in a unified environment by recombining existing building blocks using the XML-based SABCEMM configuration file. We introduce an abstract ABCEM model class which our simulation tool is built upon. Furthermore, we present the software architecture as well as computational aspects of SABCEMM. Here, we focus on the efficiency of SABCEMM with respect to the run time of our simulations. We show the great impact of different random number generators on the run time of ABCEM models. The code and documentation is published on GitHub at https://github.com/SABCEMM/SABCEMM, such that all results can be reproduced by the reader."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the SABCEMM simulator for agent-based computational economic market (ABCEM) models, and how does it enable the design and comparison of ABCEM models?\n\n**A)** SABCEMM allows for the easy implementation of complex market mechanisms, making it ideal for large-scale simulations.\n**B)** The object-oriented software design of SABCEMM enables the isolated implementation of building blocks for ABCEM models, such as agent types and market mechanisms, facilitating the design and comparison of ABCEM models.\n**C)** SABCEMM's use of XML-based configuration files allows for the easy modification and extension of ABCEM models, making it a flexible tool for researchers.\n**D)** The simulator's ability to run ABCEM models with several million agents makes it suitable for large-scale economic simulations.\n\n**Correct Answer:** B) The object-oriented software design of SABCEMM enables the isolated implementation of building blocks for ABCEM models, such as agent types and market mechanisms, facilitating the design and comparison of ABCEM models.\n\n**Explanation:** The correct answer is B) because the documentation states that the object-oriented software design of SABCEMM enables the isolated implementation of building blocks for ABCEM models, such as agent types and market mechanisms, which facilitates the design and comparison of ABCEM models. This allows researchers to design and compare different ABCEM models by recombining existing building blocks using the XML-based SABCEMM configuration file."}, "6": {"documentation": {"title": "Nucleon Mass with Highly Improved Staggered Quarks", "source": "Yin Lin, Aaron S. Meyer, Ciaran Hughes, Andreas S. Kronfeld, James N.\n  Simone, Alexei Strelchenko", "docs_id": "1911.12256", "section": ["hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nucleon Mass with Highly Improved Staggered Quarks. We present the first computation in a program of lattice-QCD baryon physics using staggered fermions for sea and valence quarks. For this initial study, we present a calculation of the nucleon mass, obtaining $964\\pm16$ MeV with all sources of statistical and systematic errors controlled and accounted for. This result is the most precise determination to date of the nucleon mass from first principles. We use the highly-improved staggered quark action, which is computationally efficient. Three gluon ensembles are employed, which have approximate lattice spacings $a=0.09$ fm, $0.12$ fm, and $0.15$ fm, each with equal-mass $u$/$d$, $s$, and $c$ quarks in the sea. Further, all ensembles have the light valence and sea $u$/$d$ quarks tuned to reproduce the physical pion mass, avoiding complications from chiral extrapolations or nonunitarity. Our work opens a new avenue for precise calculations of baryon properties, which are both feasible and relevant to experiments in particle and nuclear physics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key advantages of using the highly-improved staggered quark action in the calculation of the nucleon mass, and how do the lattice spacings of the employed gluon ensembles impact the results?\n\nA) The highly-improved staggered quark action allows for more precise calculations of baryon properties, while the lattice spacings of 0.09 fm, 0.12 fm, and 0.15 fm enable the study of different physical regimes.\n\nB) The highly-improved staggered quark action is computationally efficient, but the lattice spacings of 0.09 fm, 0.12 fm, and 0.15 fm lead to significant differences in the results due to non-unitarity.\n\nC) The highly-improved staggered quark action is more accurate than other quark actions, but the lattice spacings of 0.09 fm, 0.12 fm, and 0.15 fm are not sufficient to capture the full range of baryon properties.\n\nD) The highly-improved staggered quark action is only suitable for calculations with light quarks, while the lattice spacings of 0.09 fm, 0.12 fm, and 0.15 fm are necessary to study the effects of heavy quarks.\n\nCorrect Answer: A) The highly-improved staggered quark action allows for more precise calculations of baryon properties, while the lattice spacings of 0.09 fm, 0.12 fm, and 0.15 fm enable the study of different physical regimes.\n\nExplanation: The highly-improved staggered quark action is a computationally efficient and accurate method for calculating baryon properties. The use of three different lattice spacings (0.09 fm, 0.12 fm, and 0.15 fm) allows for the study of different physical regimes, including the continuum limit. This is a key advantage of the highly-improved staggered quark action, as it enables researchers to gain a more comprehensive understanding of baryon properties."}, "7": {"documentation": {"title": "Theory of Weak Identification in Semiparametric Models", "source": "Tetsuya Kaji", "docs_id": "1908.10478", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theory of Weak Identification in Semiparametric Models. We provide general formulation of weak identification in semiparametric models and an efficiency concept. Weak identification occurs when a parameter is weakly regular, i.e., when it is locally homogeneous of degree zero. When this happens, consistent or equivariant estimation is shown to be impossible. We then show that there exists an underlying regular parameter that fully characterizes the weakly regular parameter. While this parameter is not unique, concepts of sufficiency and minimality help pin down a desirable one. If estimation of minimal sufficient underlying parameters is inefficient, it introduces noise in the corresponding estimation of weakly regular parameters, whence we can improve the estimators by local asymptotic Rao-Blackwellization. We call an estimator weakly efficient if it does not admit such improvement. New weakly efficient estimators are presented in linear IV and nonlinear regression models. Simulation of a linear IV model demonstrates how 2SLS and optimal IV estimators are improved."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of weak identification in semiparametric models, what is the primary concern when a parameter is weakly regular, and how does it impact the estimation of weakly regular parameters?\n\n**A)** When a parameter is weakly regular, it implies that the model is misspecified, leading to inconsistent estimation. Therefore, the primary concern is to identify the correct specification of the model.\n\n**B)** When a parameter is weakly regular, it implies that the parameter is locally homogeneous of degree zero, making consistent or equivariant estimation impossible. The primary concern is to find an underlying regular parameter that fully characterizes the weakly regular parameter.\n\n**C)** When a parameter is weakly regular, it implies that the model is overparameterized, leading to inefficient estimation. The primary concern is to reduce the dimensionality of the parameter space to improve estimation efficiency.\n\n**D)** When a parameter is weakly regular, it implies that the model is underparameterized, leading to inconsistent estimation. The primary concern is to identify the correct set of parameters that can fully characterize the weakly regular parameter.\n\n**Correct Answer:** B) When a parameter is weakly regular, it implies that the parameter is locally homogeneous of degree zero, making consistent or equivariant estimation impossible. The primary concern is to find an underlying regular parameter that fully characterizes the weakly regular parameter.\n\n**Explanation:** The correct answer is based on the definition of weak identification in semiparametric models, which states that weak identification occurs when a parameter is weakly regular, i.e., when it is locally homogeneous of degree zero. This implies that consistent or equivariant estimation is impossible, and the primary concern is to find an underlying regular parameter that fully characterizes the weakly regular parameter. The other options are incorrect because they do not accurately capture the essence of weak identification in semiparametric models."}, "8": {"documentation": {"title": "Energy barriers in three-dimensional micromagnetic models and the\n  physics of thermo-viscous magnetization in multidomain particles", "source": "Karl Fabian and Valera P. Shcherbakov", "docs_id": "1702.00070", "section": ["physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy barriers in three-dimensional micromagnetic models and the\n  physics of thermo-viscous magnetization in multidomain particles. A first principle micromagnetic and statistical calculation of viscous remanent magnetization (VRM) in an ensemble of cubic magnetite pseudo single-domain particles is presented. This is achieved by developing a fast relaxation algorithm for finding optimal transition paths between micromagnetic local energy minima. It combines a nudged elastic band technique with action minimization. Initial paths are obtained by repetitive minimizations of modified energy functions. For a cubic pseudo-single domain particle, 60 different local energy minima are identified and all optimal energy barriers between them are numerically calculated for zero external field. The results allow to estimate also the energy barriers in in weak external fields which are necessary to construct the time dependent transition matrices which describe the continuous homogeneous Markov processes of VRM acquisition and decay. By spherical averaging the remanence acquisition in an isotropic PSD ensemble was calculated over all time scales. The modelled particle ensemble shows a physically meaningful overshooting during VRM acquisition. The results also explain why VRM acquisition in PSD particles can occur much faster than VRM decay and therefore can explain for findings of extremely stable VRM in some paleomagnetic studies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Arxiv paper \"Energy barriers in three-dimensional micromagnetic models and the physics of thermo-viscous magnetization in multidomain particles\", what is the primary advantage of using a nudged elastic band technique combined with action minimization to find optimal transition paths between micromagnetic local energy minima?\n\n**A)** It allows for a more accurate calculation of energy barriers in weak external fields.\n**B)** It enables the identification of 60 different local energy minima in a cubic pseudo-single domain particle.\n**C)** It facilitates the construction of time-dependent transition matrices that describe the continuous homogeneous Markov processes of VRM acquisition and decay.\n**D)** It provides a physically meaningful overshooting during VRM acquisition in an isotropic PSD ensemble.\n\n**Correct Answer:** C) It facilitates the construction of time-dependent transition matrices that describe the continuous homogeneous Markov processes of VRM acquisition and decay.\n\n**Explanation:** The correct answer is C) because the paper states that the modelled particle ensemble shows a physically meaningful overshooting during VRM acquisition, and that the results also explain why VRM acquisition in PSD particles can occur much faster than VRM decay. This implies that the nudged elastic band technique combined with action minimization is used to construct time-dependent transition matrices that describe the continuous homogeneous Markov processes of VRM acquisition and decay. The other options are incorrect because they do not accurately summarize the primary advantage of the technique used in the paper. Option A is incorrect because the paper does not specifically mention the accuracy of energy barrier calculations in weak external fields as the primary advantage of the technique. Option B is incorrect because the paper does mention the identification of 60 different local energy minima, but this is not the primary advantage of the technique. Option D is incorrect because the paper does mention the overshooting during VRM acquisition, but this is a consequence of the technique, not the primary advantage."}, "9": {"documentation": {"title": "Synthesis and Properties of Non-Curing Graphene Thermal Interface\n  Materials", "source": "Sahar Naghibi, Fariborz Kargar, Dylan Wright, Chun Yu Tammy Huang,\n  Amirmahdi Mohammadzadeh, Zahra Barani, Ruben Salgado and Alexander Balandin", "docs_id": "1911.10383", "section": ["physics.app-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synthesis and Properties of Non-Curing Graphene Thermal Interface\n  Materials. Development of the next generation thermal interface materials with high thermal conductivity is important for thermal management and packaging of electronic devices. We report on the synthesis and thermal conductivity measurements of non-curing thermal paste, i.e. grease, based on mineral oil with the mixture of graphene and few-layer graphene flakes as the fillers. It was found that graphene thermal paste exhibits a distinctive thermal percolation threshold with the thermal conductivity revealing a sublinear dependence on the filler loading. This behavior contrasts with the thermal conductivity of curing graphene thermal interface materials, based on epoxy, where super-linear dependence on the filler loading is observed. The performance of graphene thermal paste was benchmarked against top-of-the-line commercial thermal pastes. The obtained results show that non-curing graphene thermal interface materials outperforms the best commercial pastes in terms of thermal conductivity, at substantially lower filler concentration of ~ 27 vol%. The obtained results shed light on thermal percolation mechanism in non-curing polymeric matrices laden with quasi-two-dimensional fillers. Considering recent progress in graphene production via liquid phase exfoliation and oxide reduction, we argue that our results open a pathway for large-scale industrial application of graphene in thermal management of electronics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of non-curing graphene thermal interface materials over commercial thermal pastes, and how does the thermal conductivity of graphene thermal paste compare to that of curing graphene thermal interface materials?\n\nA) Non-curing graphene thermal interface materials exhibit a super-linear dependence on filler loading, whereas commercial thermal pastes have a sublinear dependence, resulting in higher thermal conductivity.\nB) Non-curing graphene thermal interface materials outperform commercial thermal pastes in terms of thermal conductivity, with a thermal conductivity that reveals a sublinear dependence on filler loading, contrasting with the super-linear dependence observed in curing graphene thermal interface materials.\nC) The thermal conductivity of graphene thermal paste is independent of filler loading, whereas commercial thermal pastes have a thermal conductivity that increases with filler loading, resulting in lower thermal conductivity.\nD) Non-curing graphene thermal interface materials have a lower thermal conductivity than commercial thermal pastes, with a thermal conductivity that decreases with filler loading, whereas curing graphene thermal interface materials have a higher thermal conductivity that increases with filler loading.\n\nCorrect Answer: B) Non-curing graphene thermal interface materials outperform commercial thermal pastes in terms of thermal conductivity, with a thermal conductivity that reveals a sublinear dependence on filler loading, contrasting with the super-linear dependence observed in curing graphene thermal interface materials.\n\nExplanation: The correct answer is B) because the documentation states that non-curing graphene thermal interface materials outperform commercial thermal pastes in terms of thermal conductivity, with a thermal conductivity that reveals a sublinear dependence on filler loading, contrasting with the super-linear dependence observed in curing graphene thermal interface materials. This is a key finding of the study, and the correct answer accurately summarizes the main advantage of non-curing graphene thermal interface materials."}, "10": {"documentation": {"title": "Semi-Supervised Histology Classification using Deep Multiple Instance\n  Learning and Contrastive Predictive Coding", "source": "Ming Y. Lu, Richard J. Chen, Jingwen Wang, Debora Dillon and Faisal\n  Mahmood", "docs_id": "1910.10825", "section": ["cs.CV", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semi-Supervised Histology Classification using Deep Multiple Instance\n  Learning and Contrastive Predictive Coding. Convolutional neural networks can be trained to perform histology slide classification using weak annotations with multiple instance learning (MIL). However, given the paucity of labeled histology data, direct application of MIL can easily suffer from overfitting and the network is unable to learn rich feature representations due to the weak supervisory signal. We propose to overcome such limitations with a two-stage semi-supervised approach that combines the power of data-efficient self-supervised feature learning via contrastive predictive coding (CPC) and the interpretability and flexibility of regularized attention-based MIL. We apply our two-stage CPC + MIL semi-supervised pipeline to the binary classification of breast cancer histology images. Across five random splits, we report state-of-the-art performance with a mean validation accuracy of 95% and an area under the ROC curve of 0.968. We further evaluate the quality of features learned via CPC relative to simple transfer learning and show that strong classification performance using CPC features can be efficiently leveraged under the MIL framework even with the feature encoder frozen."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of using multiple instance learning (MIL) for histology slide classification, and how does the proposed two-stage semi-supervised approach address this limitation?\n\n**A)** MIL can only learn rich feature representations when the supervisory signal is strong, but the proposed approach uses a self-supervised feature learning method to overcome this limitation.\n\n**B)** MIL can suffer from overfitting when the number of labeled instances is limited, but the proposed approach uses a regularization technique to prevent overfitting.\n\n**C)** MIL can only be applied to binary classification problems, but the proposed approach can be extended to multi-class classification problems using a multi-label MIL framework.\n\n**D)** MIL can learn rich feature representations when the number of labeled instances is limited, but the proposed approach uses a data-efficient self-supervised feature learning method to further improve performance.\n\n**Correct Answer:** D) MIL can learn rich feature representations when the number of labeled instances is limited, but the proposed approach uses a data-efficient self-supervised feature learning method to further improve performance.\n\n**Explanation:** The correct answer is D) because the documentation states that direct application of MIL can suffer from overfitting and the network is unable to learn rich feature representations due to the weak supervisory signal. The proposed two-stage semi-supervised approach addresses this limitation by using a data-efficient self-supervised feature learning method, specifically contrastive predictive coding (CPC), to learn rich feature representations."}, "11": {"documentation": {"title": "Recovering hidden signals of statistical anisotropy from a masked or\n  partial CMB sky", "source": "Pavan K. Aluri, Nidhi Pant, Aditya Rotti, and Tarun Souradeep", "docs_id": "1510.02454", "section": ["astro-ph.CO", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recovering hidden signals of statistical anisotropy from a masked or\n  partial CMB sky. Any isotropy violating phenomena on cosmic microwave background (CMB) induces off-diagonal correlations in the two-point function. These correlations themselves can be used to estimate the underlying anisotropic signals. Masking due to residual foregrounds, or availability of partial sky due to survey limitation, are unavoidable circumstances in CMB studies. But, masking induces additional correlations, and thus complicates the recovery of such signals. In this work, we discuss a procedure based on bipolar spherical harmonic (BipoSH) formalism to comprehensively addresses any spurious correlations induced by masking and successfully recover hidden signals of anisotropy in observed CMB maps. This method is generic, and can be applied to recover a variety of isotropy violating phenomena. Here, we illustrate the procedure by recovering the subtle Doppler boost signal from simulated boosted CMB skies, which has become possible with the unprecedented full-sky sensitivity of PLANCK probe."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in recovering hidden signals of statistical anisotropy from a masked or partial CMB sky, and how does the proposed BipoSH formalism address this challenge?\n\nA) Masking induces additional correlations that complicate the recovery of anisotropic signals, and the BipoSH formalism provides a method to remove these correlations.\nB) Masking does not induce additional correlations, and the BipoSH formalism is a generic method that can be applied to recover a variety of isotropy violating phenomena.\nC) The BipoSH formalism is a method to estimate the underlying anisotropic signals from off-diagonal correlations in the two-point function, but it does not address the challenge of masking.\nD) The BipoSH formalism is a method to remove masking effects from CMB maps, but it does not provide a way to recover hidden signals of anisotropy.\n\nCorrect Answer: A) Masking induces additional correlations that complicate the recovery of anisotropic signals, and the BipoSH formalism provides a method to remove these correlations.\n\nExplanation: The question requires the test-taker to understand the challenge of recovering hidden signals of statistical anisotropy from a masked or partial CMB sky, as described in the documentation. The correct answer, A, highlights the primary challenge of masking inducing additional correlations that complicate the recovery of anisotropic signals. The BipoSH formalism is then described as a method to remove these correlations, making it a comprehensive approach to addressing this challenge. The other options are incorrect because they either misrepresent the challenge or the BipoSH formalism's role in addressing it."}, "12": {"documentation": {"title": "Fusing numerical relativity and deep learning to detect higher-order\n  multipole waveforms from eccentric binary black hole mergers", "source": "Adam Rebei, E. A. Huerta, Sibo Wang, Sarah Habib, Roland Haas, Daniel\n  Johnson, Daniel George", "docs_id": "1807.09787", "section": ["gr-qc", "astro-ph.HE", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fusing numerical relativity and deep learning to detect higher-order\n  multipole waveforms from eccentric binary black hole mergers. We determine the mass-ratio, eccentricity and binary inclination angles that maximize the contribution of the higher-order waveform multipoles $(\\ell, \\, |m|)= \\{(2,\\,2),\\, (2,\\,1),\\, (3,\\,3),\\, (3,\\,2), \\, (3,\\,1),\\, (4,\\,4),\\, (4,\\,3),\\, (4,\\,2),\\,(4,\\,1)\\}$ for the gravitational wave detection of eccentric binary black hole mergers. We carry out this study using numerical relativity waveforms that describe non-spinning black hole binaries with mass-ratios $1\\leq q \\leq 10$, and orbital eccentricities as high as $e_0=0.18$ fifteen cycles before merger. For stellar-mass, asymmetric mass-ratio, binary black hole mergers, and assuming LIGO's Zero Detuned High Power configuration, we find that in regions of parameter space where black hole mergers modeled with $\\ell=|m|=2$ waveforms have vanishing signal-to-noise ratios, the inclusion of $(\\ell, \\, |m|)$ modes enables the observation of these sources with signal-to-noise ratios that range between 30\\% to 45\\% the signal-to-noise ratio of optimally oriented binary black hole mergers modeled with $\\ell=|m|=2$ numerical relativity waveforms. Having determined the parameter space where $(\\ell, \\, |m|)$ modes are important for gravitational wave detection, we construct waveform signals that describe these astrophysically motivate scenarios, and demonstrate that these topologically complex signals can be detected and characterized in real LIGO noise with deep learning algorithms."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of gravitational wave detection from eccentric binary black hole mergers, what is the significance of including higher-order multipole waveforms in the analysis, and how do the results of this study impact our understanding of the detection of stellar-mass, asymmetric mass-ratio, binary black hole mergers?\n\n**A)** The inclusion of higher-order multipole waveforms is crucial for detecting binary black hole mergers with high signal-to-noise ratios, particularly for sources with high orbital eccentricities. However, this study suggests that the detection of these sources is still limited by the signal-to-noise ratio, which remains a significant challenge.\n\n**B)** The study demonstrates that the inclusion of higher-order multipole waveforms can enable the detection of binary black hole mergers with signal-to-noise ratios that are 30% to 45% higher than those modeled with standard $\\ell=|m|=2$ waveforms. This suggests that the detection of these sources is significantly improved, but the results are still limited by the complexity of the waveform signals.\n\n**C)** The results of this study indicate that the detection of stellar-mass, asymmetric mass-ratio, binary black hole mergers is still a significant challenge, and that the inclusion of higher-order multipole waveforms does not necessarily improve the signal-to-noise ratio. The study highlights the need for further research to improve the detection of these sources.\n\n**D)** The study shows that the inclusion of higher-order multipole waveforms can enable the detection of binary black hole mergers with signal-to-noise ratios that are comparable to those modeled with standard $\\ell=|m|=2$ waveforms. This suggests that the detection of these sources is significantly improved, and that the results of this study have important implications for the field of gravitational wave astronomy.\n\n**Correct Answer:** B) The study demonstrates that the inclusion of higher-order multipole waveforms can enable the detection of binary black hole mergers with signal-to-noise ratios that are 30% to 45% higher than those modeled with standard $\\ell=|m|=2$ waveforms. This suggests that the detection of these sources is significantly improved, but the results are still limited by the complexity of the waveform signals.\n\n**Explanation:** The correct answer, B, is supported by the study's finding that the inclusion of higher-order multipole waveforms enables the detection of binary black hole mergers with signal-to-noise ratios that are 30% to 45% higher than those modeled with standard $\\ell=|m|=2$ waveforms. However, the study also notes that the results are still limited by the complexity of the waveform signals, which suggests that the detection of these sources is not yet fully resolved."}, "13": {"documentation": {"title": "Using Nudges to Prevent Student Dropouts in the Pandemic", "source": "Guilherme Lichand and Julien Christen", "docs_id": "2009.04767", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using Nudges to Prevent Student Dropouts in the Pandemic. The impacts of COVID-19 reach far beyond the hundreds of lives lost to the disease; in particular, the pre-existing learning crisis is expected to be magnified during school shutdown. Despite efforts to put distance learning strategies in place, the threat of student dropouts, especially among adolescents, looms as a major concern. Are interventions to motivate adolescents to stay in school effective amidst the pandemic? Here we show that, in Brazil, nudges via text messages to high-school students, to motivate them to stay engaged with school activities, substantially reduced dropouts during school shutdown, and greatly increased their motivation to go back to school when classes resume. While such nudges had been shown to decrease dropouts during normal times, it is surprising that those impacts replicate in the absence of regular classes because their effects are typically mediated by teachers (whose effort in the classroom changes in response to the nudges). Results show that insights from the science of adolescent psychology can be leveraged to shift developmental trajectories at a critical juncture. They also qualify those insights: effects increase with exposure and gradually fade out once communication stops, providing novel evidence that motivational interventions work by redirecting adolescents' attention."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on using nudges to prevent student dropouts in the pandemic highlights the effectiveness of motivational interventions in reducing dropouts. However, the results also suggest that the effects of these interventions are context-dependent and may fade out once communication stops. What are the implications of these findings for the development of effective interventions to prevent student dropouts, and how do they relate to the science of adolescent psychology?\n\n**A)** The findings suggest that motivational interventions are only effective in the presence of regular classes, and that their effects can be replicated by simply re-establishing regular classes after the pandemic.\n\n**B)** The results indicate that the effects of motivational interventions are mediated by teachers, and that the interventions work by redirecting adolescents' attention away from dropout behaviors.\n\n**C)** The study's findings suggest that the effects of motivational interventions increase with exposure and gradually fade out once communication stops, providing novel evidence that these interventions can be used to shift developmental trajectories at a critical juncture.\n\n**D)** The results indicate that the effectiveness of motivational interventions is limited to adolescents who are already at risk of dropping out, and that these interventions are not effective for students who are not at risk.\n\n**Correct Answer:** C) The study's findings suggest that the effects of motivational interventions increase with exposure and gradually fade out once communication stops, providing novel evidence that these interventions can be used to shift developmental trajectories at a critical juncture.\n\n**Explanation:** This answer choice accurately summarizes the main finding of the study, which is that the effects of motivational interventions increase with exposure and gradually fade out once communication stops. This finding has implications for the development of effective interventions to prevent student dropouts, as it suggests that these interventions can be used to shift developmental trajectories at a critical juncture."}, "14": {"documentation": {"title": "A closed-form solution for optimal mean-reverting trading strategies", "source": "Alexander Lipton and Marcos Lopez de Prado", "docs_id": "2003.10502", "section": ["q-fin.TR", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A closed-form solution for optimal mean-reverting trading strategies. When prices reflect all available information, they oscillate around an equilibrium level. This oscillation is the result of the temporary market impact caused by waves of buyers and sellers. This price behavior can be approximated through an Ornstein-Uhlenbeck (O-U) process. Market makers provide liquidity in an attempt to monetize this oscillation. They enter a long position when a security is priced below its estimated equilibrium level, and they enter a short position when a security is priced above its estimated equilibrium level. They hold that position until one of three outcomes occur: (1) they achieve the targeted profit; (2) they experience a maximum tolerated loss; (3) the position is held beyond a maximum tolerated horizon. All market makers are confronted with the problem of defining profit-taking and stop-out levels. More generally, all execution traders acting on behalf of a client must determine at what levels an order must be fulfilled. Those optimal levels can be determined by maximizing the trader's Sharpe ratio in the context of O-U processes via Monte Carlo experiments. This paper develops an analytical framework and derives those optimal levels by using the method of heat potentials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A market maker is using an Ornstein-Uhlenbeck (O-U) process to model the price behavior of a security. The market maker's goal is to maximize the Sharpe ratio of their trading strategy. However, they are also constrained by a maximum tolerated loss and a maximum tolerated horizon. Using the method of heat potentials, derive the optimal profit-taking and stop-out levels for the market maker.\n\n**A)** The optimal profit-taking level is the equilibrium level of the O-U process, and the optimal stop-out level is the maximum tolerated loss.\n\n**B)** The optimal profit-taking level is the maximum tolerated loss, and the optimal stop-out level is the equilibrium level of the O-U process.\n\n**C)** The optimal profit-taking level is the maximum tolerated horizon, and the optimal stop-out level is the equilibrium level of the O-U process.\n\n**D)** The optimal profit-taking level is the equilibrium level of the O-U process, and the optimal stop-out level is the maximum tolerated horizon.\n\n**Correct Answer:** B) The optimal profit-taking level is the maximum tolerated loss, and the optimal stop-out level is the equilibrium level of the O-U process.\n\n**Explanation:** The correct answer is B) because the market maker's goal is to maximize the Sharpe ratio, which is a measure of risk-adjusted return. The maximum tolerated loss is a risk constraint that limits the market maker's potential losses, and the equilibrium level of the O-U process is a risk-free level that the market maker can aim to reach. By setting the optimal profit-taking level as the maximum tolerated loss and the optimal stop-out level as the equilibrium level, the market maker can maximize their Sharpe ratio while minimizing their risk exposure.\n\nNote: The other options are incorrect because:\n\n* A) The equilibrium level is not necessarily the optimal profit-taking level, and the maximum tolerated loss is not necessarily the optimal stop-out level.\n* C) The maximum tolerated horizon is not directly related to the optimal profit-taking and stop-out levels.\n* D) The equilibrium level is not necessarily the optimal stop-out level, and the maximum tolerated horizon is not necessarily the optimal profit-taking level."}, "15": {"documentation": {"title": "Site-dependent hydrogenation on graphdiyne", "source": "P. A. S. Autreto, J. M. de Sousa and D. S. Galvao", "docs_id": "1408.2705", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Site-dependent hydrogenation on graphdiyne. Graphene is one of the most important materials in science today due to its unique and remarkable electronic, thermal and mechanical properties. However in its pristine state, graphene is a gapless semiconductor, what limits its use in transistor electronics. In part due to the revolution created by graphene in materials science, there is a renewed interest in other possible graphene-like two-dimensional structures. Examples of these structures are graphynes and graphdiynes, which are two-dimensional structures, composed of carbon atoms in sp2 and sp-hybridized states. Graphdiynes (benzenoid rings connecting two acetylenic groups) were recently synthesized and some of them are intrinsically nonzero gap systems. These systems can be easily hydrogenated and the relative level of hydrogenation can be used to tune the band gap values. We have investigated, using fully reactive molecular dynamics (ReaxFF), the structural and dynamics aspects of the hydrogenation mechanisms of graphdiyne membranes. Our results showed that the hydrogen bindings have different atom incorporation rates and that the hydrogenation patterns change in time in a very complex way. The formation of correlated domains reported to hydrogenated graphene is no longer observed in graphdiyne cases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of graphene's use in transistor electronics, and how do graphdiynes address this issue?\n\nA) Graphene's high thermal conductivity and mechanical strength are the primary limitations, and graphdiynes' ability to be easily hydrogenated is not relevant to this issue.\nB) Graphene's gapless semiconductor nature is the primary limitation, and graphdiynes' ability to be hydrogenated and tune their band gap values is not relevant to this issue.\nC) Graphene's high surface area and reactivity are the primary limitations, and graphdiynes' ability to be easily hydrogenated is not relevant to this issue.\nD) Graphene's high electrical conductivity and optical transparency are the primary limitations, and graphdiynes' ability to be easily hydrogenated is not relevant to this issue.\n\n**Correct Answer:** B) Graphene's gapless semiconductor nature is the primary limitation, and graphdiynes' ability to be hydrogenated and tune their band gap values is not relevant to this issue.\n\n**Explanation:** The question requires the test-taker to understand the limitations of graphene's use in transistor electronics and how graphdiynes address this issue. The correct answer, B, states that graphene's gapless semiconductor nature is the primary limitation, which is supported by the documentation. The other options are incorrect because they either misrepresent the limitations of graphene or incorrectly attribute the ability to be hydrogenated to graphdiynes' relevance to this issue."}, "16": {"documentation": {"title": "Three-dimensional pattern formation, multiple homogeneous soft modes,\n  and nonlinear dielectric electroconvection", "source": "Axel G. Rossberg", "docs_id": "nlin/0001065", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Three-dimensional pattern formation, multiple homogeneous soft modes,\n  and nonlinear dielectric electroconvection. Patterns forming spontaneously in extended, three-dimensional, dissipative systems are likely to excite several homogeneous soft modes ($\\approx$ hydrodynamic modes) of the underlying physical system, much more than quasi one- and two-dimensional patterns are. The reason is the lack of damping boundaries. This paper compares two analytic techniques to derive the patten dynamics from hydrodynamics, which are usually equivalent but lead to different results when applied to multiple homogeneous soft modes. Dielectric electroconvection in nematic liquid crystals is introduced as a model for three-dimensional pattern formation. The 3D pattern dynamics including soft modes are derived. For slabs of large but finite thickness the description is reduced further to a two-dimensional one. It is argued that the range of validity of 2D descriptions is limited to a very small region above threshold. The transition from 2D to 3D pattern dynamics is discussed. Experimentally testable predictions for the stable range of ideal patterns and the electric Nusselt numbers are made. For most results analytic approximations in terms of material parameters are given."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare and contrast the two analytic techniques used to derive pattern dynamics from hydrodynamics in the context of three-dimensional pattern formation, multiple homogeneous soft modes, and nonlinear dielectric electroconvection.\n\n**A)** The two techniques, which are usually equivalent, differ in their treatment of boundary conditions, with one technique assuming a fixed boundary and the other assuming a free boundary.\n\n**B)** The two techniques, which are usually equivalent, differ in their treatment of soft modes, with one technique assuming a single soft mode and the other assuming multiple soft modes.\n\n**C)** The two techniques, which are usually equivalent, differ in their mathematical formulation, with one technique using a linearized hydrodynamic model and the other using a nonlinear hydrodynamic model.\n\n**D)** The two techniques, which are usually equivalent, differ in their application to the specific case of dielectric electroconvection in nematic liquid crystals, with one technique being more suitable for the 2D case and the other being more suitable for the 3D case.\n\n**Correct Answer:** B) The two techniques, which are usually equivalent, differ in their treatment of soft modes, with one technique assuming a single soft mode and the other assuming multiple soft modes.\n\n**Explanation:** The correct answer is B) because the documentation states that the two analytic techniques used to derive pattern dynamics from hydrodynamics \"lead to different results when applied to multiple homogeneous soft modes\". This suggests that the techniques differ in their treatment of soft modes, with one technique assuming a single soft mode and the other assuming multiple soft modes."}, "17": {"documentation": {"title": "Spatial holeburning effects in the amplified spontaneous emission\n  spectra of the non-lasing supermode in semiconductor laser arrays", "source": "Holger F. Hofmann and Ortwin Hess", "docs_id": "physics/9804002", "section": ["physics.optics", "nlin.AO", "nlin.AO", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatial holeburning effects in the amplified spontaneous emission\n  spectra of the non-lasing supermode in semiconductor laser arrays. The amplified spontaneous emission spectrum of the light field in the non-lasing supermode of two coupled semiconductor lasers is analyzed using linearized Langevin equations. It is shown that the interference betweeen the laser mode and the fluctuating light field in the non-lasing mode causes spatial holeburning. This effect introduces a phase sensitive coupling between the laser field and the fluctuations in the non-lasing mode. For high laser fields, this coupling splits the spectrum of the non-lasing mode into a triplet consisting of two relaxation oscillation sidebands which are in phase with the laser light and a center line at the lasing frequency with a phase shift of pi half relative to the laser light. As the laser intensity is increased close to threshold, the spectrum shows a continuous transition from the single amplified spontaneous emission line at the frequency of the laser mode to the triplet structure. An analytical expression for this transition is derived and typical features are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the spatial holeburning effects in the amplified spontaneous emission spectra of the non-lasing supermode in semiconductor laser arrays. What is the phase shift of the center line at the lasing frequency relative to the laser light, and how does it change as the laser intensity is increased close to threshold?\n\n**A)** The phase shift is \u03c0/2, and it remains constant as the laser intensity increases.\n**B)** The phase shift is \u03c0/2, and it increases as the laser intensity increases close to threshold.\n**C)** The phase shift is \u03c0, and it decreases as the laser intensity increases close to threshold.\n**D)** The phase shift is \u03c0/2, and it remains constant as the laser intensity increases close to threshold.\n\n**Correct Answer:** C) The phase shift is \u03c0, and it decreases as the laser intensity increases close to threshold.\n\n**Explanation:** According to the documentation, the phase shift of the center line at the lasing frequency relative to the laser light is \u03c0/2. However, as the laser intensity is increased close to threshold, the spectrum shows a continuous transition from the single amplified spontaneous emission line at the frequency of the laser mode to the triplet structure. This transition is characterized by a decrease in the phase shift, which is consistent with option C."}, "18": {"documentation": {"title": "Level set Cox processes", "source": "Anders Hildeman, David Bolin, Jonas Wallin and Janine B. Illian", "docs_id": "1708.06982", "section": ["stat.ME", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Level set Cox processes. The log-Gaussian Cox process (LGCP) is a popular point process for modeling non-interacting spatial point patterns. This paper extends the LGCP model to handle data exhibiting fundamentally different behaviors in different subregions of the spatial domain. The aim of the analyst might be either to identify and classify these regions, to perform kriging, or to derive some properties of the parameters driving the random field in one or several of the subregions. The extension is based on replacing the latent Gaussian random field in the LGCP by a latent spatial mixture model. The mixture model is specified using a latent, categorically valued, random field induced by level set operations on a Gaussian random field. Conditional on the classification, the intensity surface for each class is modeled by a set of independent Gaussian random fields. This allows for standard stationary covariance structures, such as the Mat\\'{e}rn family, to be used to model Gaussian random fields with some degree of general smoothness but also occasional and structured sharp discontinuities. A computationally efficient MCMC method is proposed for Bayesian inference and we show consistency of finite dimensional approximations of the model. Finally, the model is fitted to point pattern data derived from a tropical rainforest on Barro Colorado island, Panama. We show that the proposed model is able to capture behavior for which inference based on the standard LGCP is biased."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper propose a level set Cox process model to handle spatial point patterns exhibiting different behaviors in different subregions of the spatial domain. What is the primary advantage of using a latent spatial mixture model in this context, and how does it enable the modeling of Gaussian random fields with both general smoothness and structured sharp discontinuities?\n\nA) The latent spatial mixture model allows for the incorporation of prior knowledge about the underlying spatial structure, enabling the modeling of both smooth and discontinuous patterns.\nB) The latent spatial mixture model enables the modeling of Gaussian random fields with both general smoothness and structured sharp discontinuities by allowing for the use of independent Gaussian random fields for each class.\nC) The latent spatial mixture model provides a more flexible and computationally efficient alternative to the standard LGCP model, enabling the modeling of complex spatial patterns.\nD) The latent spatial mixture model allows for the estimation of the parameters driving the random field in one or several of the subregions, enabling the derivation of properties of the parameters.\n\n**Correct Answer:** B) The latent spatial mixture model enables the modeling of Gaussian random fields with both general smoothness and structured sharp discontinuities by allowing for the use of independent Gaussian random fields for each class.\n\n**Explanation:** The correct answer highlights the key advantage of using a latent spatial mixture model in this context. By allowing for the use of independent Gaussian random fields for each class, the model enables the modeling of Gaussian random fields with both general smoothness and structured sharp discontinuities. This is a critical aspect of the model, as it allows for the capture of complex spatial patterns that may exhibit both smooth and discontinuous behavior. The other options do not accurately capture the primary advantage of the latent spatial mixture model."}, "19": {"documentation": {"title": "Asymptotics for the Fredholm Determinant of the Sine Kernel on a Union\n  of Intervals", "source": "Harold Widom", "docs_id": "hep-th/9405010", "section": ["hep-th", "math.CA", "nlin.SI", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotics for the Fredholm Determinant of the Sine Kernel on a Union\n  of Intervals. In the bulk scaling limit of the Gaussian Unitary Ensemble of Hermitian matrices the probability that an interval of length $s$ contains no eigenvalues is the Fredholm determinant of the sine kernel $\\sin(x-y)\\over\\pi(x-y)$ over this interval. A formal asymptotic expansion for the determinant as $s$ tends to infinity was obtained by Dyson. In this paper we replace a single interval of length $s$ by $sJ$ where $J$ is a union of $m$ intervals and present a proof of the asymptotics up to second order. The logarithmic derivative with respect to $s$ of the determinant equals a constant (expressible in terms of hyperelliptic integrals) times $s$, plus a bounded oscillatory function of $s$ (zero of $m=1$, periodic if $m=2$, and in general expressible in terms of the solution of a Jacobi inversion problem), plus $o(1)$. Also determined are the asymptotics of the trace of the resolvent operator, which is the ratio in the same model of the probability that the set contains exactly one eigenvalue to the probability that it contains none. The proofs use ideas from orthogonal polynomial theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Gaussian Unitary Ensemble of Hermitian matrices and the sine kernel on a union of intervals. Let $J$ be a union of $m$ intervals of length $s$, and let $\\det(sJ)$ denote the Fredholm determinant of the sine kernel over $J$. Prove that the logarithmic derivative with respect to $s$ of $\\det(sJ)$ satisfies the following asymptotic expansion:\n\n$$\\frac{d}{ds}\\log\\det(sJ) = c(s) + O\\left(\\frac{1}{s}\\right)$$\n\nwhere $c(s)$ is a function expressible in terms of hyperelliptic integrals, and $O\\left(\\frac{1}{s}\\right)$ is a bounded oscillatory function of $s$.\n\nA) The function $c(s)$ is given by the integral $\\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx$.\n\nB) The function $c(s)$ is given by the integral $\\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log x$.\n\nC) The function $c(s)$ is given by the integral $\\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log x + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log \\left(\\frac{1}{x}\\right)$.\n\nD) The function $c(s)$ is given by the integral $\\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log x + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log \\left(\\frac{1}{x}\\right) + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log \\left(\\frac{x}{1}\\right)$.\n\nCorrect Answer: B) The function $c(s)$ is given by the integral $\\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx + \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{x^2 + 1} dx \\log x$."}, "20": {"documentation": {"title": "Approximate Survey Propagation for Statistical Inference", "source": "Fabrizio Antenucci, Florent Krzakala, Pierfrancesco Urbani and Lenka\n  Zdeborov\\'a", "docs_id": "1807.01296", "section": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approximate Survey Propagation for Statistical Inference. Approximate message passing algorithm enjoyed considerable attention in the last decade. In this paper we introduce a variant of the AMP algorithm that takes into account glassy nature of the system under consideration. We coin this algorithm as the approximate survey propagation (ASP) and derive it for a class of low-rank matrix estimation problems. We derive the state evolution for the ASP algorithm and prove that it reproduces the one-step replica symmetry breaking (1RSB) fixed-point equations, well-known in physics of disordered systems. Our derivation thus gives a concrete algorithmic meaning to the 1RSB equations that is of independent interest. We characterize the performance of ASP in terms of convergence and mean-squared error as a function of the free Parisi parameter s. We conclude that when there is a model mismatch between the true generative model and the inference model, the performance of AMP rapidly degrades both in terms of MSE and of convergence, while ASP converges in a larger regime and can reach lower errors. Among other results, our analysis leads us to a striking hypothesis that whenever s (or other parameters) can be set in such a way that the Nishimori condition $M=Q>0$ is restored, then the corresponding algorithm is able to reach mean-squared error as low as the Bayes-optimal error obtained when the model and its parameters are known and exactly matched in the inference procedure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the approximate survey propagation (ASP) algorithm, what is the significance of the Nishimori condition $M=Q>0$ in terms of the convergence and performance of the algorithm?\n\n**A)** The Nishimori condition is a necessary condition for the convergence of the ASP algorithm, but it does not guarantee optimal performance. In fact, when $M=Q=0$, the algorithm may converge to a suboptimal solution.\n\n**B)** The Nishimori condition is a sufficient condition for the ASP algorithm to reach the Bayes-optimal error, but it does not guarantee convergence. In fact, the algorithm may converge to a non-optimal solution when $M=Q=0$.\n\n**C)** The Nishimori condition is a necessary and sufficient condition for the ASP algorithm to converge to the Bayes-optimal error, and it is also a necessary condition for the algorithm to converge to a suboptimal solution. When $M=Q>0$, the algorithm can reach mean-squared error as low as the Bayes-optimal error.\n\n**D)** The Nishimori condition is a condition that is only relevant for the AMP algorithm, and it is not applicable to the ASP algorithm. In fact, the ASP algorithm can converge to the Bayes-optimal error even when $M=Q=0$.\n\n**Correct Answer:** C) The Nishimori condition is a necessary and sufficient condition for the ASP algorithm to converge to the Bayes-optimal error, and it is also a necessary condition for the algorithm to converge to a suboptimal solution. When $M=Q>0$, the algorithm can reach mean-squared error as low as the Bayes-optimal error.\n\n**Explanation:** The Nishimori condition $M=Q>0$ is a condition that is known to be related to the replica symmetry breaking (RSB) phase transition in physics. In the context of the ASP algorithm, this condition is significant because it is a necessary and sufficient condition for the algorithm to converge to the Bayes-optimal error. When $M=Q>0$, the algorithm can reach mean-squared error as low as the Bayes-optimal error, which is a desirable property. On the other hand, when $M=Q=0$, the algorithm may converge to a suboptimal solution. Therefore, the correct answer is C."}, "21": {"documentation": {"title": "Subsampling Generative Adversarial Networks: Density Ratio Estimation in\n  Feature Space with Softplus Loss", "source": "Xin Ding, Z. Jane Wang, William J. Welch", "docs_id": "1909.10670", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Subsampling Generative Adversarial Networks: Density Ratio Estimation in\n  Feature Space with Softplus Loss. Filtering out unrealistic images from trained generative adversarial networks (GANs) has attracted considerable attention recently. Two density ratio based subsampling methods---Discriminator Rejection Sampling (DRS) and Metropolis-Hastings GAN (MH-GAN)---were recently proposed, and their effectiveness in improving GANs was demonstrated on multiple datasets. However, DRS and MH-GAN are based on discriminator based density ratio estimation (DRE) methods, so they may not work well if the discriminator in the trained GAN is far from optimal. Moreover, they do not apply to some GANs (e.g., MMD-GAN). In this paper, we propose a novel Softplus (SP) loss for DRE. Based on it, we develop a sample-based DRE method in a feature space learned by a specially designed and pre-trained ResNet-34 (DRE-F-SP). We derive the rate of convergence of a density ratio model trained under the SP loss. Then, we propose three different density ratio subsampling methods (DRE-F-SP+RS, DRE-F-SP+MH, and DRE-F-SP+SIR) for GANs based on DRE-F-SP. Our subsampling methods do not rely on the optimality of the discriminator and are suitable for all types of GANs. We empirically show our subsampling approach can substantially outperform DRS and MH-GAN on a synthetic dataset and the CIFAR-10 dataset, using multiple GANs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the Discriminator Rejection Sampling (DRS) and Metropolis-Hastings GAN (MH-GAN) methods proposed in recent studies, and how does the proposed Softplus (SP) loss for Density Ratio Estimation (DRE) address this limitation?\n\n**A)** DRS and MH-GAN methods are only suitable for GANs with a well-trained discriminator, and the SP loss does not improve their performance on datasets with poor discriminator quality.\n\n**B)** DRS and MH-GAN methods are based on discriminator-based density ratio estimation (DRE) methods, which may not work well if the discriminator in the trained GAN is far from optimal. The SP loss does not address this limitation.\n\n**C)** DRS and MH-GAN methods are not suitable for GANs with a small number of training samples, and the SP loss does not improve their performance on such datasets. The SP loss is specifically designed to work with a specially designed and pre-trained ResNet-34.\n\n**D)** DRS and MH-GAN methods are based on a different density ratio estimation approach that is not affected by the optimality of the discriminator, and the SP loss does not improve their performance on datasets with poor discriminator quality.\n\n**Correct Answer:** B) DRS and MH-GAN methods are based on discriminator-based density ratio estimation (DRE) methods, which may not work well if the discriminator in the trained GAN is far from optimal. The SP loss does not address this limitation.\n\n**Explanation:** The correct answer is B) because the documentation states that DRS and MH-GAN are based on discriminator-based density ratio estimation (DRE) methods, which may not work well if the discriminator in the trained GAN is far from optimal. The SP loss is proposed to address this limitation, but it does not improve the performance of DRS and MH-GAN on datasets with poor discriminator quality."}, "22": {"documentation": {"title": "Elegent -- an elastic event generator", "source": "Jan Ka\\v{s}par", "docs_id": "1310.2199", "section": ["physics.comp-ph", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Elegent -- an elastic event generator. Although elastic scattering of nucleons may look like a simple process, it presents a long-lasting challenge for theory. Due to missing hard energy scale, the perturbative QCD can not be applied. Instead, many phenomenological/theoretical models have emerged. In this paper we present a unified implementation of some of the most prominent models in a C++ library, moreover extended to account for effects of the electromagnetic interaction. The library is complemented with a number of utilities. For instance, programs to sample many distributions of interest in four-momentum transfer squared, t, impact parameter, b, and collision energy sqrt(s). These distributions at ISR, SppS, RHIC, Tevatron and LHC energies are available for download from the project web site. Both in the form of ROOT files and PDF figures providing comparisons among the models. The package includes also a tool for Monte-Carlo generation of elastic scattering events, which can easily be embedded in any other program framework."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of perturbative QCD in modeling elastic scattering of nucleons, and how does the Elegent library address this challenge?\n\nA) The lack of a hard energy scale makes it difficult to apply perturbative QCD, but the library provides a simplified implementation of the Glauber model.\nB) The absence of a hard energy scale prevents the use of perturbative QCD, but the library offers a unified implementation of several prominent models, including the Glauber, Glauber-Griphorsson, and Ruprecht models, as well as an extension to account for electromagnetic interactions.\nC) The difficulty in applying perturbative QCD is due to the complexity of the nucleon-nucleon interaction, but the library provides a tool for Monte-Carlo generation of elastic scattering events that can be easily embedded in other programs.\nD) The primary limitation of perturbative QCD is the need for a more sophisticated treatment of the nucleon-nucleon interaction, but the library provides only a simplified implementation of the Glauber model.\n\nCorrect Answer: B) The absence of a hard energy scale prevents the use of perturbative QCD, but the library offers a unified implementation of several prominent models, including the Glauber, Glauber-Griphorsson, and Ruprecht models, as well as an extension to account for electromagnetic interactions.\n\nExplanation: The question requires the test-taker to understand the limitations of perturbative QCD in modeling elastic scattering of nucleons and how the Elegent library addresses this challenge. The correct answer highlights the library's unified implementation of several prominent models, including the Glauber, Glauber-Griphorsson, and Ruprecht models, as well as its extension to account for electromagnetic interactions. This demonstrates a deeper understanding of the subject matter and the ability to analyze complex information."}, "23": {"documentation": {"title": "A massive variable flavour number scheme for the Drell-Yan process", "source": "R. Gauld", "docs_id": "2107.01226", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A massive variable flavour number scheme for the Drell-Yan process. The prediction of differential cross-sections in hadron-hadron scattering processes is typically performed in a scheme where the heavy-flavour quarks ($c, b, t$) are treated either as massless or massive partons. In this work, a method to describe the production of colour-singlet processes which combines these two approaches is presented. The core idea is that the contribution from power corrections involving the heavy-quark mass can be numerically isolated from the rest of the massive computation. These power corrections can then be combined with a massless computation (where they are absent), enabling the construction of differential cross-section predictions in a massive variable flavour number scheme. As an example, the procedure is applied to the low-mass Drell-Yan process within the LHCb fiducial region, where predictions for the rapidity and transverse-momentum distributions of the lepton pair are provided. To validate the procedure, it is shown how the $n_f$-dependent coefficient of a massless computation can be recovered from the massless limit of the massive one. This feature is also used to differentially extract the massless $\\text{N}^3\\text{LO}$ coefficient of the Drell-Yan process in the gluon-fusion channel."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the massive variable flavour number scheme for the Drell-Yan process, what is the key feature of the proposed method that allows for the numerical isolation of power corrections involving heavy-quark mass?\n\nA){The method uses a different renormalization scheme for the heavy-quark mass.}\nB){The method employs a numerical technique to separate the heavy-quark mass contributions from the rest of the computation.}\nC){The method relies on a specific choice of heavy-quark mass parameter.}\nD){The method uses a different approximation for the heavy-quark mass.}\n\nCorrect Answer: B){The method employs a numerical technique to separate the heavy-quark mass contributions from the rest of the computation.}\n\nExplanation: The correct answer is B) because the documentation states that the core idea of the proposed method is to numerically isolate the power corrections involving the heavy-quark mass from the rest of the massive computation. This is achieved through a numerical technique, which allows for the separation of these contributions. The other options are incorrect because they do not accurately describe the key feature of the proposed method."}, "24": {"documentation": {"title": "AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment", "source": "Huifeng Guan, Yuan Gao, Min Zhao, Yong Yang, Fuqin Deng, Tin Lun Lam", "docs_id": "2110.00760", "section": ["cs.RO", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment. Multi-agent path finding in dynamic crowded environments is of great academic and practical value for multi-robot systems in the real world. To improve the effectiveness and efficiency of communication and learning process during path planning in dynamic crowded environments, we introduce an algorithm called Attention and BicNet based Multi-agent path planning with effective reinforcement (AB-Mapper)under the actor-critic reinforcement learning framework. In this framework, on the one hand, we utilize the BicNet with communication function in the actor-network to achieve intra team coordination. On the other hand, we propose a centralized critic network that can selectively allocate attention weights to surrounding agents. This attention mechanism allows an individual agent to automatically learn a better evaluation of actions by also considering the behaviours of its surrounding agents. Compared with the state-of-the-art method Mapper,our AB-Mapper is more effective (85.86% vs. 81.56% in terms of success rate) in solving the general path finding problems with dynamic obstacles. In addition, in crowded scenarios, our method outperforms the Mapper method by a large margin,reaching a stunning gap of more than 40% for each experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the Attention and BicNet based Multi-agent Path Finding algorithm (AB-Mapper) in the context of multi-robot systems in dynamic crowded environments?\n\n**A)** AB-Mapper introduces a new communication protocol to improve intra-team coordination among agents.\n**B)** AB-Mapper proposes a centralized critic network that selectively allocates attention weights to surrounding agents, enabling individual agents to learn a better evaluation of actions.\n**C)** AB-Mapper replaces the state-of-the-art method Mapper with a new algorithm that achieves higher success rates in solving general path finding problems with dynamic obstacles.\n**D)** AB-Mapper introduces a new reinforcement learning framework that enables agents to learn from their environment and adapt to changing conditions.\n\n**Correct Answer:** B) AB-Mapper proposes a centralized critic network that selectively allocates attention weights to surrounding agents, enabling individual agents to learn a better evaluation of actions.\n\n**Explanation:** The correct answer is B) because the documentation states that the attention mechanism allows an individual agent to automatically learn a better evaluation of actions by also considering the behaviors of its surrounding agents. This is achieved through the centralized critic network that selectively allocates attention weights to surrounding agents. The other options are incorrect because they do not accurately summarize the primary contribution of AB-Mapper. Option A is incorrect because the documentation does not mention a new communication protocol. Option C is incorrect because while AB-Mapper does achieve higher success rates, this is not its primary contribution. Option D is incorrect because the documentation does not mention a new reinforcement learning framework."}, "25": {"documentation": {"title": "OpenRBC: A Fast Simulator of Red Blood Cells at Protein Resolution", "source": "Yu-Hang Tang, Lu Lu, He Li, Constantinos Evangelinos, Leopold\n  Grinberg, Vipin Sachdeva, George Em Karniadakis", "docs_id": "1701.02059", "section": ["physics.bio-ph", "cond-mat.mes-hall", "cs.CE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "OpenRBC: A Fast Simulator of Red Blood Cells at Protein Resolution. We present OpenRBC, a coarse-grained molecular dynamics code, which is capable of performing an unprecedented in silico experiment --- simulating an entire mammal red blood cell lipid bilayer and cytoskeleton as modeled by 4 million mesoscopic particles --- using a single shared memory commodity workstation. To achieve this, we invented an adaptive spatial-searching algorithm to accelerate the computation of short-range pairwise interactions in an extremely sparse 3D space. The algorithm is based on a Voronoi partitioning of the point cloud of coarse-grained particles, and is continuously updated over the course of the simulation. The algorithm enables the construction of the key spatial searching data structure in our code, i.e. a lattice-free cell list, with a time and space cost linearly proportional to the number of particles in the system. The position and shape of the cells also adapt automatically to the local density and curvature. The code implements OpenMP parallelization and scales to hundreds of hardware threads. It outperforms a legacy simulator by almost an order of magnitude in time-to-solution and more than 40 times in problem size, thus providing a new platform for probing the biomechanics of red blood cells."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary innovation of the adaptive spatial-searching algorithm used in OpenRBC, and how does it enable the efficient simulation of red blood cell lipid bilayer and cytoskeleton?\n\nA) The algorithm uses a hierarchical data structure to partition the point cloud of coarse-grained particles, allowing for faster computation of pairwise interactions.\nB) The algorithm employs a Voronoi partitioning of the point cloud, enabling the construction of a lattice-free cell list with a time and space cost linearly proportional to the number of particles in the system.\nC) The algorithm uses a machine learning approach to predict the position and shape of cells based on local density and curvature, allowing for more accurate simulations.\nD) The algorithm relies on a parallelization technique to distribute the computation of pairwise interactions across multiple hardware threads.\n\n**Correct Answer:** B) The algorithm employs a Voronoi partitioning of the point cloud, enabling the construction of a lattice-free cell list with a time and space cost linearly proportional to the number of particles in the system.\n\n**Explanation:** The adaptive spatial-searching algorithm in OpenRBC uses Voronoi partitioning to divide the point cloud of coarse-grained particles into cells. This allows for the efficient computation of pairwise interactions between particles, as the algorithm can quickly identify which particles are within a certain distance of each other. The lattice-free cell list constructed by this algorithm enables the simulation of large systems, such as the entire mammal red blood cell lipid bilayer and cytoskeleton, with a time and space cost linearly proportional to the number of particles in the system. This innovation is the primary contribution of the algorithm, enabling the simulation of complex biological systems with unprecedented accuracy and efficiency."}, "26": {"documentation": {"title": "TRI$\\mu$P - A New Facility to Investigate Fundamental Interactions with\n  Optically Trapped Radioactive Atoms", "source": "Klaus Jungmann (1) ((1) KVI Groningen)", "docs_id": "nucl-ex/0210023", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TRI$\\mu$P - A New Facility to Investigate Fundamental Interactions with\n  Optically Trapped Radioactive Atoms. At the Kernfysisch Versneller Instituut (KVI) in Groningen, NL, a new facility (TRI$\\mu$P) is under development. It aims for producing, slowing down and trapping of radioactive isotopes in order to perform accurate measurements on fundamental symmetries and interactions. A spectrum of radioactive nuclids will be produced in direct, inverse kinematics of fragmentation reactions using heavy ion beams from the superconducting AGOR cyclotron. The research programme pursued by the KVI group includes precision studies of nuclear $\\beta$-decays through $\\beta$--neutrino (recoil nucleus) momentum correlations in weak decays and searches for permanent electric dipole moments in heavy atomic systems. This offers a large potential for discovering new physics or to limit parameters in models beyond standard theory significantly. The scientific approach chosen in TRI$\\mu$P can be regarded as complementary to such high energy physics. The facility in Groningen will be open for use by the worldwide community of scientists."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the TRI$\\mu$P facility in Groningen, NL, and how does it differ from high-energy physics approaches?\n\nA) To study the properties of radioactive isotopes in high-energy collisions and search for new physics beyond standard theory.\nB) To investigate fundamental symmetries and interactions through the production, slowing down, and trapping of radioactive isotopes.\nC) To perform precision studies of nuclear $\\beta$-decays and searches for permanent electric dipole moments in heavy atomic systems using high-energy beams.\nD) To develop new methods for fragmenting heavy ions and studying their properties in high-energy collisions.\n\nCorrect Answer: B) To investigate fundamental symmetries and interactions through the production, slowing down, and trapping of radioactive isotopes.\n\nExplanation: The TRI$\\mu$P facility aims to produce, slow down, and trap radioactive isotopes to perform accurate measurements on fundamental symmetries and interactions. This approach is complementary to high-energy physics and offers a unique opportunity to discover new physics or limit parameters in models beyond standard theory. The correct answer highlights the primary goal of the facility, which is to investigate fundamental interactions through the study of radioactive isotopes."}, "27": {"documentation": {"title": "Ensemble Dynamics and Bred Vectors", "source": "Nusret Balci, Anna L. Mazzucato, Juan M. Restrepo, and George R. Sell", "docs_id": "1108.4918", "section": ["physics.ao-ph", "math.DS", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ensemble Dynamics and Bred Vectors. We introduce the new concept of an EBV to assess the sensitivity of model outputs to changes in initial conditions for weather forecasting. The new algorithm, which we call the \"Ensemble Bred Vector\" or EBV, is based on collective dynamics in essential ways. By construction, the EBV algorithm produces one or more dominant vectors. We investigate the performance of EBV, comparing it to the BV algorithm as well as the finite-time Lyapunov Vectors. We give a theoretical justification to the observed fact that the vectors produced by BV, EBV, and the finite-time Lyapunov vectors are similar for small amplitudes. Numerical comparisons of BV and EBV for the 3-equation Lorenz model and for a forced, dissipative partial differential equation of Cahn-Hilliard type that arises in modeling the thermohaline circulation, demonstrate that the EBV yields a size-ordered description of the perturbation field, and is more robust than the BV in the higher nonlinear regime. The EBV yields insight into the fractal structure of the Lorenz attractor, and of the inertial manifold for the Cahn-Hilliard-type partial differential equation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Ensemble Bred Vector (EBV) algorithm over the Bred Vector (BV) algorithm, and how does it relate to the fractal structure of the Lorenz attractor?\n\nA) EBV is more computationally intensive than BV, but it provides a more detailed description of the perturbation field.\nB) EBV is more robust than BV in the higher nonlinear regime, and it yields insight into the fractal structure of the Lorenz attractor.\nC) EBV is based on collective dynamics, but it produces a more simplified description of the perturbation field compared to BV.\nD) EBV is only applicable to dissipative partial differential equations, while BV can be used for both dissipative and non-dissipative systems.\n\nCorrect Answer: B) EBV is more robust than BV in the higher nonlinear regime, and it yields insight into the fractal structure of the Lorenz attractor.\n\nExplanation: The correct answer is B) because the documentation states that the EBV algorithm is more robust than the BV algorithm in the higher nonlinear regime, and it yields insight into the fractal structure of the Lorenz attractor. This is a key advantage of the EBV algorithm, as it provides a more detailed and accurate description of the perturbation field. The other options are incorrect because they either misrepresent the relationship between EBV and BV, or they limit the applicability of EBV to specific types of systems."}, "28": {"documentation": {"title": "Kohn-Sham theory with paramagnetic currents: compatibility and\n  functional differentiability", "source": "Andre Laestadius, Erik I. Tellgren, Markus Penz, Michael Ruggenthaler,\n  Simen Kvaal, and Trygve Helgaker", "docs_id": "1902.09086", "section": ["physics.chem-ph", "cond-mat.other", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kohn-Sham theory with paramagnetic currents: compatibility and\n  functional differentiability. Recent work has established Moreau-Yosida regularization as a mathematical tool to achieve rigorous functional differentiability in density-functional theory. In this article, we extend this tool to paramagnetic current-density-functional theory, the most common density-functional framework for magnetic field effects. The extension includes a well-defined Kohn-Sham iteration scheme with a partial convergence result. To this end, we rely on a formulation of Moreau-Yosida regularization for reflexive and strictly convex function spaces. The optimal $L^p$-characterization of the paramagnetic current density $L^1\\cap L^{3/2}$ is derived from the $N$-representability conditions. A crucial prerequisite for the convex formulation of paramagnetic current-density-functional theory, termed compatibility between function spaces for the particle density and the current density, is pointed out and analyzed. Several results about compatible function spaces are given, including their recursive construction. The regularized, exact functionals are calculated numerically for a Kohn-Sham iteration on a quantum ring, illustrating their performance for different regularization parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of paramagnetic current-density-functional theory, what is the optimal $L^p$-characterization of the paramagnetic current density, and how does it relate to the $N$-representability conditions?\n\nA) The paramagnetic current density is characterized by $L^p$-spaces as $L^1\\cap L^{3/2}$, and this characterization is derived from the $N$-representability conditions through a recursive construction of compatible function spaces.\n\nB) The paramagnetic current density is characterized by $L^p$-spaces as $L^2\\cap L^{3/2}$, and this characterization is derived from the $N$-representability conditions through a well-defined Kohn-Sham iteration scheme.\n\nC) The paramagnetic current density is characterized by $L^p$-spaces as $L^1\\cap L^{1/2}$, and this characterization is derived from the $N$-representability conditions through a formulation of Moreau-Yosida regularization for reflexive and strictly convex function spaces.\n\nD) The paramagnetic current density is characterized by $L^p$-spaces as $L^2\\cap L^{2/3}$, and this characterization is derived from the $N$-representability conditions through a partial convergence result.\n\nCorrect Answer: A) The paramagnetic current density is characterized by $L^p$-spaces as $L^1\\cap L^{3/2}$, and this characterization is derived from the $N$-representability conditions through a recursive construction of compatible function spaces.\n\nExplanation: The correct answer is A) because the documentation states that the optimal $L^p$-characterization of the paramagnetic current density is derived from the $N$-representability conditions, and it is characterized by $L^p$-spaces as $L^1\\cap L^{3/2}$. Additionally, the recursive construction of compatible function spaces is mentioned in the documentation as a crucial aspect of the theory. The other options are incorrect because they either mischaracterize the $L^p$-spaces or misstate the relationship between the $N$-representability conditions and the characterization of the paramagnetic current density."}, "29": {"documentation": {"title": "A very accurate method to approximate discontinuous functions with a\n  finite number of discontinuities", "source": "E. Stella, C. L. Ladera and G. Donoso", "docs_id": "1601.05132", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A very accurate method to approximate discontinuous functions with a\n  finite number of discontinuities. A simple and very accurate method to approximate a function with a finite number of discontinuities is presented. This method relies on hyperbolic tangent functions of rational arguments as connecting functions at the discontinuities, each argument being the reciprocal of Newton binomials that depend on the abscissae that define the domain of the discontinuous function and upon the abscissae of discontinuities. Our approximants take the form of linear combinations of such hyperbolic tangent functions with coefficients that are obtained by solving a linear system of inhomogeneous equations whose righthand sides are the partition functions that define the given discontinuous function. These approximants are analytic, and being free from the Gibbs phenomenon certainly converge at the discontinuity points much better than other known approximants to discontinuous functions, typical relative errors being of the order of 10-14 even when as close as 10-12 to the discontinuity points. Moreover, they can be readily scaled to larger intervals. Our method is here illustrated with a representative set of discontinuous mathematical physics functions, and by studying the dynamics of an oscillator subjected to a discontinuous force, but it can be applied to important cases of discontinuous functions in physics, mathematics, engineering and physical chemistry."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a discontinuous function f(x) defined on the interval [0, 1] with discontinuities at x = 0.5 and x = 0.8. The function can be approximated using the method described in the Arxiv documentation. What is the form of the approximant for f(x) in terms of hyperbolic tangent functions of rational arguments?\n\nA) \u2211[tanh(1/(2^x)) + tanh(1/(3^x))]\nB) \u2211[tanh(1/(x^2)) + tanh(1/(x^3))]\nC) \u2211[tanh(1/(x^2)) + tanh(1/(2^x))]\nD) \u2211[tanh(1/(x^3)) + tanh(1/(2^x))]\n\nCorrect Answer: C) \u2211[tanh(1/(x^2)) + tanh(1/(2^x))]"}, "30": {"documentation": {"title": "Towards testing the magnetic moment of the tau at one part per million", "source": "Andreas Crivellin, Martin Hoferichter and J. Michael Roney", "docs_id": "2111.10378", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards testing the magnetic moment of the tau at one part per million. Scaling the current tension in the anomalous magnetic moment of the muon with the square of the lepton mass ratio, as predicted by minimal flavor violation, suggests that the analog quantity for the $\\tau$ lepton, $a_\\tau$, could be affected by physics beyond the Standard Model (BSM) at the level of $10^{-6}$, and even larger effects are possible in generic BSM scenarios such as leptoquarks. While present limits are too weak to even probe the Schwinger term, the situation can improve significantly with $e^+e^-\\to\\tau^+\\tau^-$ data from Belle II assuming the SuperKEKB $e^+e^-$ collider is upgraded with a polarized $e^-$ beam. In this Letter, we study what it would take to become sensitive to realistic BSM contributions, one prerequisite being two-loop accuracy in the Pauli form factor $F_2$. We conclude that the most promising avenue would proceed via a combined analysis of transverse and longitudinal asymmetries in $e^+e^-\\to\\tau^+\\tau^-$, requiring and further motivating a SuperKEKB upgrade with polarized electrons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary requirement for the Belle II experiment to become sensitive to realistic Beyond the Standard Model (BSM) contributions in the study of the tau lepton's magnetic moment, and what type of data would significantly improve the current limits?\n\nA) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with unpolarized electron beams.\nB) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with polarized electron beams.\nC) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with unpolarized positron beams.\nD) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with unpolarized tau beams.\n\nCorrect Answer: B) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with polarized electron beams.\n\nExplanation: The correct answer is B) A two-loop accuracy in the Pauli form factor F2 and data from e+e- \u2192 \u03c4+\u03c4- with polarized electron beams. This is because the study concludes that a combined analysis of transverse and longitudinal asymmetries in e+e- \u2192 \u03c4+\u03c4- is required to become sensitive to realistic BSM contributions, and a SuperKEKB upgrade with a polarized e- beam is necessary to achieve this."}, "31": {"documentation": {"title": "N=4 Superconformal Characters and Partition Functions", "source": "M. Bianchi, F.A. Dolan, P.J. Heslop, H. Osborn", "docs_id": "hep-th/0609179", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "N=4 Superconformal Characters and Partition Functions. Character formulae for positive energy unitary representations of the N=4 superconformal group are obtained through use of reduced Verma modules and Weyl group symmetry. Expansions of these are given which determine the particular representations present and results such as dimensions of superconformal multiplets. By restriction of variables various `blind' characters are also obtained. Limits, corresponding to reduction to particular subgroups, in the characters isolate contributions from particular subsets of multiplets and in many cases simplify the results considerably. As a special case, the index counting short and semi-short multiplets which do not form long multiplets found recently is shown to be related to particular cases of reduced characters. Partition functions of N=4 super Yang Mills are investigated. Through analysis of these, exact formulae are obtained for counting half and some quarter BPS operators in the free case. Similarly, partial results for the counting of semi-short operators are given. It is also shown in particular examples how certain short operators which one might combine to form long multiplets due to group theoretic considerations may be protected dynamically."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of N=4 superconformal field theories, what is the relationship between the index counting of short and semi-short multiplets and the reduced characters of the N=4 superconformal group?\n\nA) The index counting is directly related to the number of reduced characters present in the partition function.\nB) The index counting is related to the Weyl group symmetry of the reduced characters, but not directly to the number of reduced characters.\nC) The index counting is a consequence of the reduced characters being blind, meaning they do not contribute to the partition function.\nD) The index counting is a measure of the number of semi-short operators that can be formed from short operators using group theoretic considerations.\n\nCorrect Answer: B) The index counting is related to the Weyl group symmetry of the reduced characters, but not directly to the number of reduced characters.\n\nExplanation: The correct answer is B) because the index counting of short and semi-short multiplets is related to the Weyl group symmetry of the reduced characters, but not directly to the number of reduced characters. This is because the Weyl group symmetry of the reduced characters determines the number of possible combinations of short operators that can form semi-short operators, which in turn determines the index counting. The number of reduced characters present in the partition function is not directly related to the index counting."}, "32": {"documentation": {"title": "Secure Wireless Communications via Cooperation", "source": "Lun Dong, Zhu Han, Athina P. Petropulu, H. Vincent Poor", "docs_id": "0809.4807", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secure Wireless Communications via Cooperation. The feasibility of physical-layer-based security approaches for wireless communications in the presence of one or more eavesdroppers is hampered by channel conditions. In this paper, cooperation is investigated as an approach to overcome this problem and improve the performance of secure communications. In particular, a decode-and-forward (DF) based cooperative protocol is considered, and the objective is to design the system for secrecy capacity maximization or transmit power minimization. System design for the DF-based cooperative protocol is first studied by assuming the availability of global channel state information (CSI). For the case of one eavesdropper, an iterative scheme is proposed to obtain the optimal solution for the problem of transmit power minimization. For the case of multiple eavesdroppers, the problem of secrecy capacity maximization or transmit power minimization is in general intractable. Suboptimal system design is proposed by adding an additional constraint, i.e., the complete nulling of signals at all eavesdroppers, which yields simple closed-form solutions for the aforementioned two problems. Then, the impact of imperfect CSI of eavesdroppers on system design is studied, in which the ergodic secrecy capacity is of interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Secure Wireless Communications via Cooperation\", what is the primary objective of the system design for the decode-and-forward (DF) based cooperative protocol, and how does it differ in the presence of one eavesdropper versus multiple eavesdroppers?\n\n**A)** The primary objective is to maximize the secrecy capacity, and the system design differs in that the problem becomes intractable for multiple eavesdroppers, requiring a suboptimal solution.\n\n**B)** The primary objective is to minimize the transmit power, and the system design differs in that the problem becomes intractable for multiple eavesdroppers, requiring a suboptimal solution, but can be solved iteratively for the case of one eavesdropper.\n\n**C)** The primary objective is to maximize the secrecy capacity or minimize the transmit power, and the system design differs in that the problem becomes intractable for multiple eavesdroppers, but can be solved using a simple closed-form solution by adding an additional constraint.\n\n**D)** The primary objective is to maximize the secrecy capacity or minimize the transmit power, and the system design differs in that the problem becomes intractable for multiple eavesdroppers, but can be solved using a simple closed-form solution by assuming the availability of global channel state information (CSI).\n\n**Correct Answer:** B) The primary objective is to minimize the transmit power, and the system design differs in that the problem becomes intractable for multiple eavesdroppers, requiring a suboptimal solution, but can be solved iteratively for the case of one eavesdropper.\n\n**Explanation:** The correct answer is B) because the primary objective of the system design is to minimize the transmit power, as stated in the paper. The system design differs in that the problem becomes intractable for multiple eavesdroppers, requiring a suboptimal solution, but can be solved iteratively for the case of one eavesdropper. This is evident from the paper, which states that \"For the case of one eavesdropper, an iterative scheme is proposed to obtain the optimal solution for the problem of transmit power minimization.\""}, "33": {"documentation": {"title": "Recent Neutrino Data and Type III Seesaw with Discrete Symmetry", "source": "Y. H. Ahn, C. S. Kim and Sechul Oh", "docs_id": "1103.0657", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recent Neutrino Data and Type III Seesaw with Discrete Symmetry. In light of the recent neutrino experiment results from Daya Bay and RENO Collaborations, we study phenomenology of neutrino mixing angles in the Type III seesaw model with an discrete $A_4 \\times Z_2$ symmetry, whose spontaneously breaking scale is much higher than the electroweak scale. At tree level, the tri-bimaximal (TBM) form of the lepton mixing matrix can be obtained from leptonic Yukawa interactions in a natural way. We introduce all possible effective dimension-5 operators, invariant under the Standard Model gauge group and $A_4 \\times Z_2$, and explicitly show that they induce a deviation of the lepton mixing from the TBM mixing matrix, which can explain a large mixing angle $\\theta_{13}$ together with small deviations of the solar and atmospheric mixing angles from the TBM. Two possible scenarios are investigated, by taking into account either negligible or sizable contributions from the light charged lepton sector to the lepton mixing matrix. Especially it is found in the latter scenario that all the neutrino experimental data, including the recent best-fit value of $\\theta_{13} = 8.68^{\\circ}$, can be accommodated. The leptonic CP violation characterized by the Jarlskog invariant $J_{CP}$ has a non-vanishing value, indicating a signal of maximal CP violation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Type III seesaw model with discrete $A_4 \\times Z_2$ symmetry, what is the implication of the non-vanishing Jarlskog invariant $J_{CP}$ on the CP violation in neutrino mixing, and how does this relate to the phenomenology of neutrino mixing angles?\n\nA) The non-vanishing $J_{CP}$ implies that the neutrino mixing angles are CP-conserving, which is inconsistent with the observed large mixing angle $\\theta_{13}$.\n\nB) The non-vanishing $J_{CP}$ indicates that the neutrino mixing angles exhibit maximal CP violation, which can be accommodated by the effective dimension-5 operators introduced in the model, allowing for a large mixing angle $\\theta_{13}$ together with small deviations of the solar and atmospheric mixing angles from the TBM.\n\nC) The non-vanishing $J_{CP}$ is a consequence of the electroweak scale being much lower than the spontaneously breaking scale of the $A_4 \\times Z_2$ symmetry, leading to a CP-violating lepton mixing matrix.\n\nD) The non-vanishing $J_{CP}$ is a prediction of the Standard Model, and its presence in the neutrino mixing matrix is a sign of new physics beyond the Standard Model.\n\nCorrect Answer: B) The non-vanishing $J_{CP}$ indicates that the neutrino mixing angles exhibit maximal CP violation, which can be accommodated by the effective dimension-5 operators introduced in the model, allowing for a large mixing angle $\\theta_{13}$ together with small deviations of the solar and atmospheric mixing angles from the TBM."}, "34": {"documentation": {"title": "Coulomb corrections to bremsstrahlung in electric field of heavy atom at\n  high energies", "source": "R.N.Lee, A.I.Milstein, V.M.Strakhovenko, and O.Ya. Schwarz", "docs_id": "hep-ph/0404224", "section": ["hep-ph", "nucl-th", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coulomb corrections to bremsstrahlung in electric field of heavy atom at\n  high energies. The differential and partially integrated cross sections are considered for bremsstrahlung from high-energy electrons in atomic field with the exact account of this field. The consideration exploits the quasiclassical electron Green's function and wave functions in an external electric field. It is shown that the Coulomb corrections to the differential cross section are very susceptible to screening. Nevertheless, the Coulomb corrections to the cross section summed up over the final-electron states are independent of screening in the leading approximation over a small parameter $1/mr_{scr}$ ($r_{scr}$ is a screening radius, $m$ is the electron mass, $\\hbar=c=1$). Bremsstrahlung from an electron beam of the finite size on heavy nucleus is considered as well. Again, the Coulomb corrections to the differential probability are very susceptible to the beam shape, while those to the probability integrated over momentum transfer are independent of it, apart from the trivial factor, which is the electron-beam density at zero impact parameter. For the Coulomb corrections to the bremsstrahlung spectrum, the next-to-leading terms with respect to the parameters $m/\\epsilon$ ($\\epsilon$ is the electron energy) and $1/mr_{scr}$ are obtained."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Coulomb corrections to bremsstrahlung in an electric field of a heavy atom at high energies, what is the relationship between the Coulomb corrections to the differential cross section and the screening radius $r_{scr}$?\n\nA) The Coulomb corrections to the differential cross section are independent of the screening radius $r_{scr}$.\nB) The Coulomb corrections to the differential cross section are proportional to the screening radius $r_{scr}$.\nC) The Coulomb corrections to the differential cross section are very susceptible to screening, but the corrections summed up over the final-electron states are independent of screening in the leading approximation over a small parameter $1/mr_{scr}$.\nD) The Coulomb corrections to the differential cross section are zero when the screening radius $r_{scr}$ is zero.\n\n**Correct Answer:** C) The Coulomb corrections to the differential cross section are very susceptible to screening, but the corrections summed up over the final-electron states are independent of screening in the leading approximation over a small parameter $1/mr_{scr}$.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the Coulomb corrections to the differential cross section are very susceptible to screening, Nevertheless, the Coulomb corrections to the cross section summed up over the final-electron states are independent of screening in the leading approximation over a small parameter $1/mr_{scr}$\". This indicates that the relationship between the Coulomb corrections and the screening radius is complex, and the correct answer requires an understanding of the nuances of the relationship."}, "35": {"documentation": {"title": "Tests for Sivers, Boer-Mulders and transversity distributions in\n  difference cross sections in SIDIS", "source": "Ekaterina Christova and Elliot Leader", "docs_id": "1512.01404", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tests for Sivers, Boer-Mulders and transversity distributions in\n  difference cross sections in SIDIS. A major experimental program is presently underway to determine the Sivers, Boer-Mulders and transversity distributions, vital for understanding the internal structure of the nucleon. To this end we consider the Sivers, Boer-Mulders and transversity azimuthal asymmetries of the difference cross sections of hadrons with opposite charges in SIDIS reactions with unpolarized and transversely polarized target l+N\\to l'+h+X, h=\\pi^\\pm, K^\\pm, h^\\pm. We show that on deuteron target these asymmetries are particularly simple and determine the sum of the valence-quark Q_V=u_V+d_V transverse momentum dependent distributions without any contributions from the strange or other sea-quark functions. At present, data on these asymmetries are presented for the integrated asymmetries i.e. the x_B- and z_h-dependent asymmetries. If data are available in small bins in Q^2, so that Q^2-dependence can be neglected, these expressions simplify dramatically leading to remarkably simple and powerful tests of the simplifying assumptions used in extracting these functions from the data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of SIDIS reactions, what is the primary goal of the experimental program to determine the Sivers, Boer-Mulders, and transversity distributions, and how do these distributions relate to the internal structure of the nucleon?\n\n**A)** To measure the total cross section of hadrons in SIDIS reactions and determine the average momentum of the produced hadrons.\n**B)** To extract the sum of the valence-quark Q_V=u_V+d_V transverse momentum dependent distributions, which are essential for understanding the internal structure of the nucleon.\n**C)** To test the simplifying assumptions used in extracting the Sivers, Boer-Mulders, and transversity distributions from the data, particularly in the case of deuteron targets.\n**D)** To determine the x_B- and z_h-dependent asymmetries of the difference cross sections of hadrons with opposite charges in SIDIS reactions.\n\n**Correct answer:** B) To extract the sum of the valence-quark Q_V=u_V+d_V transverse momentum dependent distributions, which are essential for understanding the internal structure of the nucleon.\n\n**Explanation:** The correct answer is B) because the experimental program aims to determine the Sivers, Boer-Mulders, and transversity distributions, which are crucial for understanding the internal structure of the nucleon. The distributions are related to the valence-quark Q_V=u_V+d_V transverse momentum dependent distributions, which are essential for this understanding. The other options are incorrect because they do not accurately describe the primary goal of the experimental program. Option A is incorrect because the total cross section is not the primary focus of the program. Option C is incorrect because while the program does test the simplifying assumptions, this is not the primary goal. Option D is incorrect because the x_B- and z_h-dependent asymmetries are a related but distinct aspect of the program."}, "36": {"documentation": {"title": "Tensor renormalization group and the volume independence in 2D U($N$)\n  and SU($N$) gauge theories", "source": "Mitsuaki Hirasawa, Akira Matsumoto, Jun Nishimura, Atis Yosprakob", "docs_id": "2110.05800", "section": ["hep-lat", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tensor renormalization group and the volume independence in 2D U($N$)\n  and SU($N$) gauge theories. The tensor renormalization group method is a promising approach to lattice field theories, which is free from the sign problem unlike standard Monte Carlo methods. One of the remaining issues is the application to gauge theories, which is so far limited to U(1) and SU(2) gauge groups. In the case of higher rank, it becomes highly nontrivial to restrict the number of representations in the character expansion to be used in constructing the fundamental tensor. We propose a practical strategy to accomplish this and demonstrate it in 2D U($N$) and SU($N$) gauge theories, which are exactly solvable. Using this strategy, we obtain the singular-value spectrum of the fundamental tensor, which turns out to have a definite profile in the large-$N$ limit. For the U($N$) case, in particular, we show that the large-$N$ behavior of the singular-value spectrum changes qualitatively at the critical coupling of the Gross-Witten-Wadia phase transition. As an interesting consequence, we find a new type of volume independence in the large-$N$ limit of the 2D U($N$) gauge theory with the $\\theta$ term in the strong coupling phase, which goes beyond the Eguchi-Kawai reduction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the tensor renormalization group method, what is the significance of the singular-value spectrum of the fundamental tensor in the large-$N$ limit, and how does it relate to the critical coupling of the Gross-Witten-Wadia phase transition in 2D U($N$) gauge theories?\n\nA) The singular-value spectrum is a measure of the volume independence of the theory, and its large-$N$ behavior is a direct consequence of the Eguchi-Kawai reduction.\n\nB) The singular-value spectrum is a proxy for the critical coupling of the Gross-Witten-Wadia phase transition, and its large-$N$ behavior is a signature of the phase transition.\n\nC) The singular-value spectrum is a tool for constructing the fundamental tensor, and its large-$N$ behavior is a consequence of the tensor renormalization group method's ability to restrict the number of representations.\n\nD) The singular-value spectrum is a measure of the theory's non-perturbative behavior, and its large-$N$ behavior is a consequence of the theory's asymptotic freedom.\n\nCorrect Answer: B) The singular-value spectrum is a proxy for the critical coupling of the Gross-Witten-Wadia phase transition, and its large-$N$ behavior is a signature of the phase transition.\n\nExplanation: The correct answer is B) because the singular-value spectrum of the fundamental tensor is shown to have a definite profile in the large-$N$ limit, and its behavior is used to demonstrate the existence of a new type of volume independence in the large-$N$ limit of the 2D U($N$) gauge theory with the $\\theta$ term in the strong coupling phase. The large-$N$ behavior of the singular-value spectrum is a signature of the phase transition, and it is used to study the critical coupling of the Gross-Witten-Wadia phase transition. The other options are incorrect because they do not accurately describe the significance of the singular-value spectrum in the context of the tensor renormalization group method."}, "37": {"documentation": {"title": "Propagation of pop ups in kirigami shells", "source": "Ahmad Rafsanjani, Lishuai Jin, Bolei Deng, Katia Bertoldi", "docs_id": "1905.00187", "section": ["cond-mat.soft", "nlin.PS", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Propagation of pop ups in kirigami shells. Kirigami-inspired metamaterials are attracting increasing interest because of their ability to achieve extremely large strains and shape changes via out-of-plane buckling. While in flat kirigami sheets the ligaments buckle simultaneously as Euler columns leading to a continuous phase transition, here we demonstrate that kirigami shells can also support discontinuous phase transitions. Specifically, we show via a combination of experiments, numerical simulations and theoretical analysis that in cylindrical kirigami shells the snapping-induced curvature inversion of the initially bent ligaments results in a pop-up process that first localizes near an imperfection and then, as the deformation is increased, progressively spreads through the structure. Notably, we find that the width of the transition zone as well as the stress at which propagation of the instability is triggered can be controlled by carefully selecting the geometry of the cuts and the curvature of the shell. Our study significantly expands the ability of existing kirigami metamaterials and opens avenues for the design of the next generation of responsive surfaces, as demonstrated by the design of a smart skin that significantly enhance the crawling efficiency of a simple linear actuator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which the propagation of pop-ups in kirigami shells is triggered, and how does it differ from the phase transition observed in flat kirigami sheets?\n\n**A)** The snapping-induced curvature inversion of the initially bent ligaments results in a continuous phase transition, similar to that observed in flat kirigami sheets.\n\n**B)** The propagation of pop-ups in kirigami shells is triggered by the simultaneous buckling of all ligaments, leading to a continuous phase transition.\n\n**C)** The snapping-induced curvature inversion of the initially bent ligaments results in a discontinuous phase transition, where the instability first localizes near an imperfection and then spreads through the structure.\n\n**D)** The propagation of pop-ups in kirigami shells is triggered by the curvature of the shell itself, rather than any defects or imperfections.\n\n**Correct Answer:** C) The snapping-induced curvature inversion of the initially bent ligaments results in a discontinuous phase transition, where the instability first localizes near an imperfection and then spreads through the structure.\n\n**Explanation:** The correct answer is C) because the documentation states that the propagation of pop-ups in kirigami shells is triggered by the snapping-induced curvature inversion of the initially bent ligaments, which results in a discontinuous phase transition. This is in contrast to the continuous phase transition observed in flat kirigami sheets, where all ligaments buckle simultaneously. The correct answer requires an understanding of the specific mechanism by which pop-ups propagate in kirigami shells, as well as the ability to distinguish it from the phase transition observed in flat kirigami sheets."}, "38": {"documentation": {"title": "Joint Long-Term Cache Allocation and Short-Term Content Delivery in\n  Green Cloud Small Cell Networks", "source": "Xiongwei Wu and Qiang Li and Xiuhua Li and Victor C. M. Leung and P.\n  C. Ching", "docs_id": "1904.10882", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint Long-Term Cache Allocation and Short-Term Content Delivery in\n  Green Cloud Small Cell Networks. Recent years have witnessed an exponential growth of mobile data traffic, which may lead to a serious traffic burn on the wireless networks and considerable power consumption. Network densification and edge caching are effective approaches to addressing these challenges. In this study, we investigate joint long-term cache allocation and short-term content delivery in cloud small cell networks (C-SCNs), where multiple smallcell BSs (SBSs) are connected to the central processor via fronthaul and can store popular contents so as to reduce the duplicated transmissions in networks. Accordingly, a long-term power minimization problem is formulated by jointly optimizing multicast beamforming, BS clustering, and cache allocation under quality of service (QoS) and storage constraints. The resultant mixed timescale design problem is an anticausal problem because the optimal cache allocation depends on the future file requests. To handle it, a two-stage optimization scheme is proposed by utilizing historical knowledge of users' requests and channel state information. Specifically, the online content delivery design is tackled with a penalty-based approach, and the periodic cache updating is optimized with a distributed alternating method. Simulation results indicate that the proposed scheme significantly outperforms conventional schemes and performs extremely close to a genie-aided lower bound in the low caching region."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of joint long-term cache allocation and short-term content delivery in cloud small cell networks (C-SCNs), what is the primary motivation behind the proposed two-stage optimization scheme?\n\nA) To minimize the overhead of cache updating and reduce the complexity of the optimization problem\nB) To optimize multicast beamforming and BS clustering while considering the QoS and storage constraints\nC) To utilize historical knowledge of users' requests and channel state information to improve the accuracy of cache allocation\nD) To reduce the duplicated transmissions in networks by storing popular contents in the small cell BSs\n\n**Correct Answer:** C) To utilize historical knowledge of users' requests and channel state information to improve the accuracy of cache allocation\n\n**Explanation:** The correct answer is C) because the two-stage optimization scheme is proposed to utilize historical knowledge of users' requests and channel state information to improve the accuracy of cache allocation. This is evident from the fact that the online content delivery design is tackled with a penalty-based approach, and the periodic cache updating is optimized with a distributed alternating method. The scheme aims to leverage the historical knowledge to make informed decisions about cache allocation, which is a key aspect of the proposed optimization scheme.\n\nThe other options are incorrect because:\n\nA) While minimizing cache updating overhead is an important consideration, it is not the primary motivation behind the proposed scheme.\n\nB) Multicast beamforming and BS clustering are important aspects of the optimization problem, but they are not the primary motivation behind the two-stage optimization scheme.\n\nD) Reducing duplicated transmissions is a benefit of the proposed scheme, but it is not the primary motivation behind the two-stage optimization scheme."}, "39": {"documentation": {"title": "Comprehending Isospin breaking effects of $X(3872)$ in a\n  Friedrichs-model-like scheme", "source": "Zhi-Yong Zhou, Zhiguang Xiao", "docs_id": "1711.01930", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Comprehending Isospin breaking effects of $X(3872)$ in a\n  Friedrichs-model-like scheme. Recently, we have shown that the $X(3872)$ state can be naturally generated as a bound state by incorporating the hadron interactions into the Godfrey-Isgur quark model using the Friedrichs-like model combined with the QPC model, in which the wave function for the $X(3872)$ as a combination of the bare $c\\bar c$ state and the continuum states can also be obtained. Under this scheme, we now investigate the isospin breaking effect of $X(3872)$ in its decays to $J/\\psi\\pi^+\\pi^-$ and $J/\\psi\\pi^+\\pi^-\\pi^0$. By Considering its dominant continuum parts coupling to $J/\\psi\\rho$ and $J/\\psi\\omega$ through the quark rearrangement process, one could obtain the reasonable ratio of $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq (0.58\\sim 0.92)$. It is also shown that the $\\bar D D^*$ invariant mass distributions in the $B\\rightarrow \\bar D D^* K$ decays could be understood qualitatively at the same time. This scheme may provide more insight to understand the enigmatic nature of the $X(3872)$ state."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Godfrey-Isgur quark model combined with the Friedrichs-like model and the QPC model, what is the approximate ratio of the branching fractions for the decays of the $X(3872)$ state to $J/\\psi\\pi^+\\pi^-$ and $J/\\psi\\pi^+\\pi^-\\pi^0$, respectively?\n\nA) $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq 0.5$\nB) $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq 0.58\\sim 0.92$\nC) $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq 0.92$\nD) $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq 0.5$\n\nCorrect Answer: B) $\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-\\pi^0)/\\mathcal{B}(X(3872)\\rightarrow J/\\psi\\pi^+\\pi^-)\\simeq 0.58\\sim 0.92$\n\nExplanation: The correct answer is based on the provided documentation, which states that the approximate ratio of the branching fractions for the decays of the $X(3872)$ state to $J/\\psi\\pi^+\\pi^-$ and $J/\\psi\\pi^+\\pi^-\\pi^0$ is $\\simeq (0.58\\sim 0.92)$. This ratio is given in option B, making it the correct answer."}, "40": {"documentation": {"title": "Measurement of event-by-event transverse momentum and multiplicity\n  fluctuations using strongly intensive measures $\\Delta[P_T, N]$ and\n  $\\Sigma[P_T, N]$ in nucleus-nucleus collisions at the CERN Super Proton\n  Synchrotron", "source": "NA49 Collaboration: T. Anticic, B. Baatar, J. Bartke, H. Beck, L.\n  Betev, H. Bialkowska, C. Blume, B. Boimska, J. Book, M. Botje, P. Buncic, P.\n  Christakoglou, P. Chung, O. Chvala, J. Cramer, V. Eckardt, Z. Fodor, P. Foka,\n  V. Friese, M. Gazdzicki, K. Grebieszkow, C.Hohne, K. Kadija, A. Karev, V.\n  Kolesnikov, M. Kowalski, D. Kresan, A. Laszlo, R. Lacey, M. van Leeuwen, M.\n  Mackowiak-Pawlowska, M. Makariev, A. Malakhov, G. Melkumov, M. Mitrovski, S.\n  Mrowczynski, G. Palla, A. Panagiotou, J. Pluta, D. Prindle, F. Puhlhofer, R.\n  Renfordt, C. Roland, G. Roland, M. Rybczynski, A. Rybicki, A. Sandoval, A.\n  Rustamov, N. Schmitz, T. Schuster, P. Seyboth, F. Sikler, E. Skrzypczak, M.\n  Slodkowski, G. Stefanek, R. Stock, H. Strobele, T. Susa, M. Szuba, D. Varga,\n  M. Vassiliou, G. Veres, G. Vesztergombi, D. Vranic, Z. Wlodarczyk, A.\n  Wojtaszek-Szwarc", "docs_id": "1509.04633", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of event-by-event transverse momentum and multiplicity\n  fluctuations using strongly intensive measures $\\Delta[P_T, N]$ and\n  $\\Sigma[P_T, N]$ in nucleus-nucleus collisions at the CERN Super Proton\n  Synchrotron. Results from the NA49 experiment at the CERN SPS are presented on event-by-event transverse momentum and multiplicity fluctuations of charged particles, produced at forward rapidities in central Pb+Pb interactions at beam momenta 20$A$, 30$A$, 40$A$, 80$A$, and 158$A$ GeV/c, as well as in systems of different size ($p+p$, C+C, Si+Si, and Pb+Pb) at 158$A$ GeV/c. This publication extends the previous NA49 measurements of the strongly intensive measure $\\Phi_{p_T}$ by a study of the recently proposed strongly intensive measures of fluctuations $\\Delta[P_T, N]$ and $\\Sigma[P_T, N]$. In the explored kinematic region transverse momentum and multiplicity fluctuations show no significant energy dependence in the SPS energy range. However, a remarkable system size dependence is observed for both $\\Delta[P_T, N]$ and $\\Sigma[P_T, N]$, with the largest values measured in peripheral Pb+Pb interactions. The results are compared with NA61/SHINE measurements in $p+p$ collisions, as well as with predictions of the UrQMD and EPOS models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the results of the NA49 experiment on event-by-event transverse momentum and multiplicity fluctuations in nucleus-nucleus collisions at the CERN Super Proton Synchrotron. What can be inferred about the energy dependence of these fluctuations in the SPS energy range, and how do the results compare to predictions from the UrQMD and EPOS models?\n\nA) The energy dependence of transverse momentum and multiplicity fluctuations shows a significant increase in the SPS energy range, with the largest values measured at the highest energies.\n\nB) The energy dependence of transverse momentum and multiplicity fluctuations shows no significant variation in the SPS energy range, with the results being consistent across all energies.\n\nC) The energy dependence of transverse momentum and multiplicity fluctuations shows a remarkable system size dependence, with the largest values measured in peripheral Pb+Pb interactions, but no significant energy dependence.\n\nD) The energy dependence of transverse momentum and multiplicity fluctuations shows a significant decrease in the SPS energy range, with the smallest values measured at the lowest energies.\n\nCorrect Answer: B) The energy dependence of transverse momentum and multiplicity fluctuations shows no significant variation in the SPS energy range, with the results being consistent across all energies.\n\nExplanation: The correct answer is B) because the documentation states that \"transverse momentum and multiplicity fluctuations show no significant energy dependence in the SPS energy range\". This means that the results of the NA49 experiment are consistent across all energies measured (20A, 30A, 40A, 80A, and 158A GeV/c), with no significant variation in the energy dependence of these fluctuations."}, "41": {"documentation": {"title": "Charge Conjugation and Pairing in a model Cu$_{5}$O$_{4}$ Cluster", "source": "Michele Cini, Adalberto Balzarotti, and Gianluca Stefanucci", "docs_id": "cond-mat/9811116", "section": ["cond-mat.supr-con", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Charge Conjugation and Pairing in a model Cu$_{5}$O$_{4}$ Cluster. Highly-symmetric three-band Hubbard Cu-O clusters have peculiar properties when the hole number is such that they admit W=0 hole pairs. These are two-hole eigenstates of the on-site Hubbard repulsion with eigenvalue 0, get bound by correlation effects when dressed by the interaction with the background, and cause superconducting flux quantization. We study the Cu$_{5}$O$_{4}$ cluster by exact diagonalization and show that bound electron pairs of $^{1}$% B$_{2}$ symmetry are obtained at an appropriate filling, and quantize flux like the hole pairs. The basic mechanism for pairing in this model is the second-order exchange diagram, and an approximate charge conjugation symmetry holds between electron and hole pairs. Further, the flux quantization property requires that the W=0 pairs of $d$ symmetry have $s$ symmetry couterparts, still with W=0; the former are due to a spin fluctuation, while the latter arise from a charge fluctuation mechanism. The simultaneous existence of both is an essential property of our model and is required for any model of superconducting $d$ pairs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Cu$_{5}$O$_{4}$ cluster model, what is the fundamental mechanism underlying the pairing of electrons and holes, and how does it relate to the quantization of superconducting flux?\n\n**A)** The pairing mechanism is based on a first-order exchange diagram, which leads to the formation of $d$-symmetry electron pairs and $s$-symmetry hole pairs, both with W=0.\n\n**B)** The pairing mechanism is based on a second-order exchange diagram, which leads to the formation of $^{1}$% B$_{2}$ symmetry electron pairs and $s$-symmetry hole pairs, both with W=0.\n\n**C)** The pairing mechanism is based on a spin fluctuation, which leads to the formation of $d$-symmetry electron pairs and $s$-symmetry hole pairs, both with W=0.\n\n**D)** The pairing mechanism is based on a charge fluctuation mechanism, which leads to the formation of $^{1}$% B$_{2}$ symmetry electron pairs and $s$-symmetry hole pairs, both with W=0.\n\n**Correct Answer:** B) The pairing mechanism is based on a second-order exchange diagram, which leads to the formation of $^{1}$% B$_{2}$ symmetry electron pairs and $s$-symmetry hole pairs, both with W=0.\n\n**Explanation:** The correct answer is B) because the documentation states that the basic mechanism for pairing in the Cu$_{5}$O$_{4}$ cluster model is the second-order exchange diagram. This diagram leads to the formation of $^{1}$% B$_{2}$ symmetry electron pairs and $s$-symmetry hole pairs, both with W=0. The other options are incorrect because they either misstate the pairing mechanism (A and C) or incorrectly describe the symmetry of the electron and hole pairs (D)."}, "42": {"documentation": {"title": "Lightweight self-conjugate nucleus $^{80}$Zr", "source": "A. Hamaker (1,2,3), E. Leistenschneider (1,2,6), R. Jain (1,2,3), G.\n  Bollen (1,2,3), S.A. Giuliani (1,4,5), K. Lund (1,2), W. Nazarewicz (1,3), L.\n  Neufcourt (1), C. Nicoloff (1,2,3), D. Puentes (1,2,3), R. Ringle (1,2), C.S.\n  Sumithrarachchi (1,2), I.T. Yandow (1,2,3) ((1) Facility for Rare Isotope\n  Beams, Michigan State University, East Lansing, Michigan, USA, (2) National\n  Superconducting Cyclotron Laboratory, Michigan State University, East\n  Lansing, Michigan, USA, (3) Department of Physics and Astronomy, Michigan\n  State University, East Lansing, Michigan, USA, (4) European Centre for\n  Theoretical Studies in Nuclear Physics and Related Areas (ECT*-FBK), Trento,\n  Italy, (5) Department of Physics, Faculty of Engineering and Physical\n  Sciences, University of Surrey, Guildford, Surrey, United Kingdom, (6) CERN,\n  Geneva, Switzerland)", "docs_id": "2108.13419", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lightweight self-conjugate nucleus $^{80}$Zr. Protons and neutrons in the atomic nucleus move in shells analogous to the electronic shell structures of atoms. Nuclear shell structure varies across the nuclear landscape due to changes of the nuclear mean field with the number of neutrons $N$ and protons $Z$. These variations can be probed with mass differences. The $N=Z=40$ self-conjugate nucleus $^{80}$Zr is of particular interest as its proton and neutron shell structures are expected to be very similar, and its ground state is highly deformed. In this work, we provide evidence for the existence of a deformed double shell closure in $^{80}$Zr through high precision Penning trap mass measurements of $^{80-83}$Zr. Our new mass values show that $^{80}$Zr is significantly lighter, and thus more bound than previously determined. This can be attributed to the deformed shell closure at $N=Z=40$ and the large Wigner energy. Our statistical Bayesian model mixing analysis employing several global nuclear mass models demonstrates difficulties with reproducing the observed mass anomaly using current theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the observed mass anomaly in the $^{80}$Zr nucleus, as evidenced by high precision Penning trap mass measurements?\n\nA) The large Wigner energy is not sufficient to explain the observed mass anomaly.\nB) The deformed shell closure at $N=Z=40$ is not a significant contributor to the mass anomaly.\nC) The deformed shell closure at $N=Z=40$ and the large Wigner energy are both significant contributors to the mass anomaly, but the former is the primary reason.\nD) The mass anomaly cannot be explained by any known nuclear physics phenomenon.\n\nCorrect Answer: C) The deformed shell closure at $N=Z=40$ and the large Wigner energy are both significant contributors to the mass anomaly, but the former is the primary reason.\n\nExplanation: The text states that the mass anomaly can be attributed to the deformed shell closure at $N=Z=40$ and the large Wigner energy. However, it also notes that the deformed shell closure is the primary reason for the anomaly, as evidenced by the fact that the mass values show that $^{80}$Zr is significantly lighter than previously determined. This suggests that the deformed shell closure is the primary contributor to the mass anomaly, but the large Wigner energy is also an important factor."}, "43": {"documentation": {"title": "Geometric Correlation between Dirac Equation and Yang-mills Equation/\n  Maxwell Equation", "source": "Xuegang Yu", "docs_id": "1103.4219", "section": ["physics.gen-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometric Correlation between Dirac Equation and Yang-mills Equation/\n  Maxwell Equation. The problem about geometric correspondence of Dirac particle and contain quality item of Yang-Mills equation has always not been solved.This paper introduced the hyperbolic imaginary unit in Minkowski space, established a classes of Dirac wave equations with t'Hooft matrices.In lightlike region of Minkowski space,we can discuss the hermitian conjugate transformation of Dirac positive particle and antiparticle, find the space-time corresponding points of Dirac particle,and draw Feynman clip-art though the geometrical relation between timelike region and lightlike region.The coupling of motion equation of Dirac positive particle and antiparticle can get Klein-Gordon equation, when it reach classical approximate we can get Schrodinger equation,and this illustrated that p meson or m meson may be composite particle. Using the relation of timelike region and lightlike region in Minkowski momentum space to renormalize the rest mass of particles,we can describe the geometric relation between rest mass and electromagnetic mass of particles. Then, we can elicit the Yang-Mills equation with electromagnetic mass through four Dirac wave equations with the hermitian conjugate transformation relation, and further launch the common forms of Maxwell equations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the geometric correspondence between the Dirac equation and Yang-Mills equation/Maxwell equation, what is the relationship between the rest mass and electromagnetic mass of particles, as described in the paper?\n\n**A)** The rest mass and electromagnetic mass are equivalent, and can be described by a single equation.\n\n**B)** The rest mass and electromagnetic mass are related by a scaling factor, and can be described by a modified Dirac equation.\n\n**C)** The rest mass and electromagnetic mass are distinct physical quantities, and can be described by separate equations, with the electromagnetic mass being a consequence of the Yang-Mills equation.\n\n**D)** The rest mass and electromagnetic mass are interchangeable, and can be described by a single equation with a specific transformation.\n\n**Correct Answer:** C) The rest mass and electromagnetic mass are distinct physical quantities, and can be described by separate equations, with the electromagnetic mass being a consequence of the Yang-Mills equation.\n\n**Explanation:** The paper describes how the coupling of motion equations of Dirac positive particles and antiparticles can lead to the Klein-Gordon equation, which can be approximated to the Schr\u00f6dinger equation in the classical limit. This illustrates that p mesons or m mesons may be composite particles. Furthermore, the paper discusses the renormalization of the rest mass of particles using the relation between timelike and lightlike regions in Minkowski momentum space, and shows how this leads to the Yang-Mills equation with electromagnetic mass. This demonstrates that the rest mass and electromagnetic mass are distinct physical quantities, related by separate equations."}, "44": {"documentation": {"title": "Giant optical anisotropy in transition metal dichalcogenides for\n  next-generation photonics", "source": "G. A. Ermolaev, D. V. Grudinin, Y. V. Stebunov, K. V. Voronin, V. G.\n  Kravets, J. Duan, A. B. Mazitov, G. I. Tselikov, A. Bylinkin, D. I.\n  Yakubovsky, S. M. Novikov, D. G. Baranov, A. Y. Nikitin, I. A. Kruglov, T.\n  Shegai, P. Alonso-Gonz\\'alez, A. N. Grigorenko, A. V. Arsenin, K. S.\n  Novoselov, V. S. Volkov", "docs_id": "2006.00884", "section": ["physics.app-ph", "cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Giant optical anisotropy in transition metal dichalcogenides for\n  next-generation photonics. Large optical anisotropy observed in a broad spectral range is of paramount importance for efficient light manipulation in countless devices. Although a giant anisotropy was recently observed in the mid-infrared wavelength range, for visible and near-infrared spectral intervals, the problem remains acute with the highest reported birefringence values of 0.8 in BaTiS3 and h-BN crystals. This inspired an intensive search for giant optical anisotropy among natural and artificial materials. Here, we demonstrate that layered transition metal dichalcogenides (TMDCs) provide an answer to this quest owing to their fundamental differences between intralayer strong covalent bonding and weak interlayer van der Walls interaction. To do this, we carried out a correlative far- and near-field characterization validated by first-principle calculations that reveals an unprecedented birefringence of 1.5 in the infrared and 3 in the visible light for MoS2. Our findings demonstrate that this outstanding anisotropy allows for tackling the diffraction limit enabling an avenue for on-chip next-generation photonics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the giant optical anisotropy observed in transition metal dichalcogenides (TMDCs), and how does it enable the tackling of the diffraction limit in next-generation photonics?\n\nA) The strong covalent bonding between intralayer atoms leads to a high degree of polarization, resulting in anisotropic optical properties.\nB) The weak interlayer van der Waals interaction between layers allows for the creation of a birefringent material with high optical anisotropy.\nC) The unique electronic structure of TMDCs, with a large bandgap and high carrier mobility, enables the manipulation of light at the nanoscale.\nD) The layered structure of TMDCs, with a strong intralayer covalent bond and weak interlayer van der Waals interaction, leads to a high degree of polarization and anisotropic optical properties.\n\nCorrect Answer: D) The layered structure of TMDCs, with a strong intralayer covalent bond and weak interlayer van der Waals interaction, leads to a high degree of polarization and anisotropic optical properties.\n\nExplanation: The correct answer is D) because the layered structure of TMDCs, with a strong intralayer covalent bond and weak interlayer van der Waals interaction, is the primary reason for the giant optical anisotropy observed in these materials. The strong covalent bonding between intralayer atoms leads to a high degree of polarization, while the weak interlayer van der Waals interaction allows for the creation of a birefringent material with high optical anisotropy. This anisotropy enables the tackling of the diffraction limit in next-generation photonics, as demonstrated by the high birefringence values observed in MoS2."}, "45": {"documentation": {"title": "Robust Group Subspace Recovery: A New Approach for Multi-Modality Data\n  Fusion", "source": "Sally Ghanem, Ashkan Panahi, Hamid Krim, and Ryan A. Kerekes", "docs_id": "2006.10657", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Group Subspace Recovery: A New Approach for Multi-Modality Data\n  Fusion. Robust Subspace Recovery (RoSuRe) algorithm was recently introduced as a principled and numerically efficient algorithm that unfolds underlying Unions of Subspaces (UoS) structure, present in the data. The union of Subspaces (UoS) is capable of identifying more complex trends in data sets than simple linear models. We build on and extend RoSuRe to prospect the structure of different data modalities individually. We propose a novel multi-modal data fusion approach based on group sparsity which we refer to as Robust Group Subspace Recovery (RoGSuRe). Relying on a bi-sparsity pursuit paradigm and non-smooth optimization techniques, the introduced framework learns a new joint representation of the time series from different data modalities, respecting an underlying UoS model. We subsequently integrate the obtained structures to form a unified subspace structure. The proposed approach exploits the structural dependencies between the different modalities data to cluster the associated target objects. The resulting fusion of the unlabeled sensors' data from experiments on audio and magnetic data has shown that our method is competitive with other state of the art subspace clustering methods. The resulting UoS structure is employed to classify newly observed data points, highlighting the abstraction capacity of the proposed method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed Robust Group Subspace Recovery (RoGSuRe) algorithm, and how does it differ from the original Robust Subspace Recovery (RoSuRe) algorithm?\n\n**A)** RoGSuRe aims to identify the underlying subspace structure of individual data modalities, whereas RoSuRe focuses on identifying the union of subspaces (UoS) structure present in the data.\n\n**B)** RoGSuRe seeks to learn a new joint representation of time series data from different modalities, whereas RoSuRe relies on a bi-sparsity pursuit paradigm and non-smooth optimization techniques.\n\n**C)** RoGSuRe aims to cluster target objects based on structural dependencies between different modalities, whereas RoSuRe focuses on identifying complex trends in data sets.\n\n**D)** RoGSuRe employs a bi-sparsity pursuit paradigm and non-smooth optimization techniques, similar to RoSuRe, whereas RoSuRe relies on a different optimization approach.\n\n**Correct Answer:** B) RoSuRe seeks to learn a new joint representation of time series data from different modalities, whereas RoSuRe relies on a bi-sparsity pursuit paradigm and non-smooth optimization techniques.\n\n**Explanation:** The correct answer is B) because the question asks about the primary goal of RoGSuRe, which is to learn a new joint representation of time series data from different modalities. The original RoSuRe algorithm, on the other hand, relies on a bi-sparsity pursuit paradigm and non-smooth optimization techniques to unfold the underlying UoS structure present in the data. This difference in approach is a key aspect of the proposed RoGSuRe algorithm."}, "46": {"documentation": {"title": "Optimal Reduction of Public Debt under Partial Observation of the\n  Economic Growth", "source": "Giorgia Callegaro, Claudia Ceci, Giorgio Ferrari", "docs_id": "1901.08356", "section": ["math.OC", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Reduction of Public Debt under Partial Observation of the\n  Economic Growth. We consider a government that aims at reducing the debt-to-gross domestic product (GDP) ratio of a country. The government observes the level of the debt-to-GDP ratio and an indicator of the state of the economy, but does not directly observe the development of the underlying macroeconomic conditions. The government's criterion is to minimize the sum of the total expected costs of holding debt and of debt's reduction policies. We model this problem as a singular stochastic control problem under partial observation. The contribution of the paper is twofold. Firstly, we provide a general formulation of the model in which the level of debt-to-GDP ratio and the value of the macroeconomic indicator evolve as a diffusion and a jump-diffusion, respectively, with coefficients depending on the regimes of the economy. These are described through a finite-state continuous-time Markov chain. We reduce via filtering techniques the original problem to an equivalent one with full information (the so-called separated problem), and we provide a general verification result in terms of a related optimal stopping problem under full information. Secondly, we specialize to a case study in which the economy faces only two regimes, and the macroeconomic indicator has a suitable diffusive dynamics. In this setting we provide the optimal debt reduction policy. This is given in terms of the continuous free boundary arising in an auxiliary fully two-dimensional optimal stopping problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a government that aims to minimize the sum of the total expected costs of holding debt and of debt's reduction policies, given that it observes the level of the debt-to-GDP ratio and an indicator of the state of the economy, but does not directly observe the development of the underlying macroeconomic conditions. The government's criterion is to minimize the sum of the total expected costs of holding debt and of debt's reduction policies.\n\n**A)** The optimal debt reduction policy can be obtained by solving a singular stochastic control problem under partial observation, where the level of debt-to-GDP ratio and the value of the macroeconomic indicator evolve as a diffusion and a jump-diffusion, respectively.\n\n**B)** The problem can be reduced to an equivalent one with full information (the so-called separated problem) using filtering techniques, and the optimal debt reduction policy can be obtained by solving an optimal stopping problem under full information.\n\n**C)** The optimal debt reduction policy is given in terms of the continuous free boundary arising in an auxiliary fully two-dimensional optimal stopping problem, and the economy faces only two regimes.\n\n**D)** The problem can be solved by using dynamic programming and the principle of optimality, without considering the partial observation of the economic growth.\n\n**Correct Answer:** B) The problem can be reduced to an equivalent one with full information (the so-called separated problem) using filtering techniques, and the optimal debt reduction policy can be obtained by solving an optimal stopping problem under full information.\n\n**Explanation:** The correct answer is B) because the paper provides a general formulation of the model using filtering techniques to reduce the original problem to an equivalent one with full information. The optimal debt reduction policy can then be obtained by solving an optimal stopping problem under full information, as stated in the paper. The other options are incorrect because they do not accurately summarize the main contribution of the paper. Option A is incorrect because it only mentions the singular stochastic control problem under partial observation, without mentioning the filtering techniques. Option C is incorrect because it only mentions the continuous free boundary, without mentioning the optimal stopping problem under full information. Option D is incorrect because it does not consider the partial observation of the economic growth, which is a key aspect of the problem."}, "47": {"documentation": {"title": "Uniform Convergence, Adversarial Spheres and a Simple Remedy", "source": "Gregor Bachmann, Seyed-Mohsen Moosavi-Dezfooli, Thomas Hofmann", "docs_id": "2105.03491", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniform Convergence, Adversarial Spheres and a Simple Remedy. Previous work has cast doubt on the general framework of uniform convergence and its ability to explain generalization in neural networks. By considering a specific dataset, it was observed that a neural network completely misclassifies a projection of the training data (adversarial set), rendering any existing generalization bound based on uniform convergence vacuous. We provide an extensive theoretical investigation of the previously studied data setting through the lens of infinitely-wide models. We prove that the Neural Tangent Kernel (NTK) also suffers from the same phenomenon and we uncover its origin. We highlight the important role of the output bias and show theoretically as well as empirically how a sensible choice completely mitigates the problem. We identify sharp phase transitions in the accuracy on the adversarial set and study its dependency on the training sample size. As a result, we are able to characterize critical sample sizes beyond which the effect disappears. Moreover, we study decompositions of a neural network into a clean and noisy part by considering its canonical decomposition into its different eigenfunctions and show empirically that for too small bias the adversarial phenomenon still persists."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the Neural Tangent Kernel (NTK) framework in explaining generalization in neural networks, as identified in the study \"Uniform Convergence, Adversarial Spheres and a Simple Remedy\"?\n\nA) NTK is too complex and difficult to interpret\nB) NTK fails to account for the role of output bias in neural networks\nC) NTK is unable to capture the phenomenon of adversarial examples\nD) NTK is too sensitive to the choice of hyperparameters\n\n**Correct Answer:** C) NTK is unable to capture the phenomenon of adversarial examples\n\n**Explanation:** The study highlights that the NTK framework suffers from the same phenomenon as previously studied data settings, where a neural network completely misclassifies a projection of the training data (adversarial set), rendering any existing generalization bound based on uniform convergence vacuous. This is because the NTK framework fails to account for the phenomenon of adversarial examples, which are a key challenge in understanding generalization in neural networks.\n\n**Additional explanations:**\n\n* Candidate A is incorrect because while NTK can be complex and difficult to interpret, this is not the primary limitation identified in the study.\n* Candidate B is incorrect because while output bias is an important factor in neural networks, it is not the primary limitation of NTK.\n* Candidate D is incorrect because the choice of hyperparameters is not the primary limitation of NTK in this context."}, "48": {"documentation": {"title": "A totally Eulerian Finite Volume solver for multi-material fluid flows:\n  Enhanced Natural Interface Positioning (ENIP)", "source": "Rapha\\\"el Loub\\`ere (IMT), Jean-Philippe Braeunig (CEA DIF, INRIA\n  Lorraine / IECN / LSIIT / IRMA), Jean-Michel Ghidaglia (CMLA, LRC MESO)", "docs_id": "1010.4208", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A totally Eulerian Finite Volume solver for multi-material fluid flows:\n  Enhanced Natural Interface Positioning (ENIP). This work concerns the simulation of compressible multi-material fluid flows and follows the method FVCF-NIP described in the former paper Braeunig et al (Eur. J. Mech. B/Fluids, 2009). This Cell-centered Finite Volume method is totally Eulerian since the mesh is not moving and a sharp interface, separating two materials, evolves through the grid. A sliding boundary condition is enforced at the interface and mass, momentum and total energy are conserved. Although this former method performs well on 1D test cases, the interface reconstruction suffers of poor accuracy in conserving shapes for instance in linear advection. This situation leads to spurious instabilities of the interface. The method Enhanced-NIP presented in the present paper cures an inconsistency in the former NIP method that improves strikingly the results. It takes advantage of a more consistent description of the interface in the numerical scheme. Results for linear advection and compressible Euler equations for inviscid fluids are presented to assess the benefits of this new method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Enhanced Natural Interface Positioning (ENIP) method over the FVCF-NIP method described in Braeunig et al (Eur. J. Mech. B/Fluids, 2009)?\n\nA) Improved interface reconstruction for 1D test cases\nB) Enhanced accuracy in conserving shapes for linear advection\nC) Improved consistency in the numerical scheme for multi-material fluid flows\nD) Increased computational efficiency for compressible Euler equations\n\nCorrect Answer: C) Improved consistency in the numerical scheme for multi-material fluid flows\n\nExplanation: The question requires the test-taker to understand the context of the paper and the limitations of the FVCF-NIP method. The correct answer, C, highlights the key improvement of the ENIP method, which is the enhanced consistency in the numerical scheme. This allows for more accurate simulations of compressible multi-material fluid flows. The other options are incorrect because they either focus on 1D test cases (A), which is not the primary advantage of ENIP, or are not directly related to the improvement of the ENIP method (B and D)."}, "49": {"documentation": {"title": "Input-output description of microwave radiation in the dynamical Coulomb\n  blockade", "source": "Juha Lepp\\\"akangas, G\\\"oran Johansson, Michael Marthaler, Mikael\n  Fogelstr\\\"om", "docs_id": "1309.3646", "section": ["cond-mat.mes-hall", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Input-output description of microwave radiation in the dynamical Coulomb\n  blockade. We study microwave radiation emitted by a small voltage-biased Josephson junction connected to a superconducting transmission line. An input-output formalism for the radiation field is established, using a perturbation expansion in the junction's critical current. Using output field operators solved up to the second order, we estimate the spectral density and the second-order coherence of the emitted field. For typical transmission line impedances and at frequencies below the main emission peak at the Josephson frequency, radiation occurs predominantly due to two-photon emission. This emission is characterized by a high degree of photon bunching if detected symmetrically around half of the Josephson frequency. Strong phase fluctuations in the transmission line make related nonclassical phase-dependent amplitude correlations short lived, and there is no steady-state two-mode squeezing. However, the radiation is shown to violate the classical Cauchy-Schwarz inequality of intensity cross-correlations, demonstrating the nonclassicality of the photon pair production in this region."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of microwave radiation emitted by a small voltage-biased Josephson junction, what is the primary mechanism of radiation that occurs predominantly at frequencies below the main emission peak at the Josephson frequency?\n\n**A)** One-photon emission due to strong phase fluctuations in the transmission line\n**B)** Two-photon emission characterized by a high degree of photon bunching\n**C)** Four-photon emission due to the Josephson effect\n**D)** Three-photon emission due to the superconducting transmission line\n\n**Correct Answer:** B) Two-photon emission characterized by a high degree of photon bunching\n\n**Explanation:** According to the documentation, the primary mechanism of radiation that occurs predominantly at frequencies below the main emission peak at the Josephson frequency is two-photon emission. This emission is characterized by a high degree of photon bunching if detected symmetrically around half of the Josephson frequency.\n\n**Explanation for incorrect options:**\n\n* A) One-photon emission due to strong phase fluctuations in the transmission line is not the primary mechanism of radiation at frequencies below the main emission peak.\n* C) Four-photon emission due to the Josephson effect is not mentioned in the documentation as a primary mechanism of radiation.\n* D) Three-photon emission due to the superconducting transmission line is not mentioned in the documentation as a primary mechanism of radiation."}, "50": {"documentation": {"title": "An Extensible and Personalizable Multi-Modal Trip Planner", "source": "Xudong Liu, Christian Fritz, Matthew Klenk", "docs_id": "1909.11604", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Extensible and Personalizable Multi-Modal Trip Planner. Despite a tremendous amount of work in the literature and in the commercial sectors, current approaches to multi-modal trip planning still fail to consistently generate plans that users deem optimal in practice. We believe that this is due to the fact that current planners fail to capture the true preferences of users, e.g., their preferences depend on aspects that are not modeled. An example of this could be a preference not to walk through an unsafe area at night. We present a novel multi-modal trip planner that allows users to upload auxiliary geographic data (e.g., crime rates) and to specify temporal constraints and preferences over these data in combination with typical metrics such as time and cost. Concretely, our planner supports the modes walking, biking, driving, public transit, and taxi, uses linear temporal logic to capture temporal constraints, and preferential cost functions to represent preferences. We show by examples that this allows the expression of very interesting preferences and constraints that, naturally, lead to quite diverse optimal plans."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A user wants to plan a trip from their home to a nearby city center, with the goal of minimizing travel time while avoiding areas with high crime rates at night. The user has uploaded a map of the city with crime rates marked, and has specified that they prefer to walk or bike during the day and take public transit or a taxi at night. The planner also needs to consider the cost of fuel for driving and the cost of parking for public transit. Which of the following multi-modal trip plans would best meet the user's preferences?\n\nA) Take a taxi from home to the city center, with a stop at a high-crime area along the way.\nB) Walk from home to a nearby park, then take public transit to the city center, with a transfer at a high-crime area.\nC) Bike from home to the city center, with a stop at a high-crime area along the way.\nD) Drive from home to the city center, with a stop at a high-crime area along the way and a parking fee.\n\nCorrect Answer: B) Walk from home to a nearby park, then take public transit to the city center, with a transfer at a high-crime area.\n\nExplanation: The correct answer is B) because it meets the user's preferences for minimizing travel time while avoiding areas with high crime rates at night. The plan involves walking during the day, which is a preferred mode of transportation, and taking public transit at night, which is also a preferred mode of transportation. The transfer at a high-crime area is necessary to avoid walking through an unsafe area at night. The other options do not meet the user's preferences, as they involve driving or biking at night, which are not preferred modes of transportation, or taking a taxi, which is not a preferred mode of transportation."}, "51": {"documentation": {"title": "Variable-lag Granger Causality for Time Series Analysis", "source": "Chainarong Amornbunchornvej, Elena Zheleva, and Tanya Y. Berger-Wolf", "docs_id": "1912.10829", "section": ["cs.LG", "econ.EM", "q-bio.QM", "stat.ME", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variable-lag Granger Causality for Time Series Analysis. Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences. Typical operationalizations of Granger causality make a strong assumption that every time point of the effect time series is influenced by a combination of other time series with a fixed time delay. However, the assumption of the fixed time delay does not hold in many applications, such as collective behavior, financial markets, and many natural phenomena. To address this issue, we develop variable-lag Granger causality, a generalization of Granger causality that relaxes the assumption of the fixed time delay and allows causes to influence effects with arbitrary time delays. In addition, we propose a method for inferring variable-lag Granger causality relations. We demonstrate our approach on an application for studying coordinated collective behavior and show that it performs better than several existing methods in both simulated and real-world datasets. Our approach can be applied in any domain of time series analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional Granger causality methods in time series analysis, and how does the proposed variable-lag Granger causality approach address this limitation?\n\n**A)** Traditional Granger causality methods assume that all time points of the effect time series are influenced by a combination of other time series with a fixed time delay, which is a reasonable assumption in many applications. However, this assumption does not hold in cases where causes can influence effects with arbitrary time delays.\n\n**B)** Traditional Granger causality methods assume that the relationship between two time series is causal, but do not account for the possibility of non-causal relationships. The proposed variable-lag Granger causality approach addresses this limitation by relaxing the assumption of a fixed time delay.\n\n**C)** Traditional Granger causality methods are limited by their inability to handle non-linear relationships between time series. The proposed variable-lag Granger causality approach addresses this limitation by allowing causes to influence effects with arbitrary time delays.\n\n**D)** Traditional Granger causality methods are limited by their reliance on a fixed time delay, which can lead to false positives in causal inference. The proposed variable-lag Granger causality approach addresses this limitation by relaxing the assumption of a fixed time delay.\n\n**Correct Answer:** D) Traditional Granger causality methods are limited by their reliance on a fixed time delay, which can lead to false positives in causal inference. The proposed variable-lag Granger causality approach addresses this limitation by relaxing the assumption of a fixed time delay.\n\n**Explanation:** The correct answer is D) because traditional Granger causality methods rely on a fixed time delay, which can lead to false positives in causal inference. The proposed variable-lag Granger causality approach addresses this limitation by relaxing the assumption of a fixed time delay, allowing causes to influence effects with arbitrary time delays. This approach is more flexible and can handle a wider range of causal relationships, making it more accurate in certain applications."}, "52": {"documentation": {"title": "Compound surface-plasmon-polariton waves guided by a thin metal layer\n  sandwiched between a homogeneous isotropic dielectric material and a\n  periodically multilayered isotropic dielectric material", "source": "Francesco Chiadini, Vincenzo Fiumara, Antonio Scaglione, and Akhlesh\n  Lakhtakia", "docs_id": "1506.08753", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compound surface-plasmon-polariton waves guided by a thin metal layer\n  sandwiched between a homogeneous isotropic dielectric material and a\n  periodically multilayered isotropic dielectric material. Multiple p- and s-polarized compound surface plasmon-polariton (SPP) waves at a fixed frequency can be guided by a structure consisting of a metal layer sandwiched between a homogeneous isotropic dielectric (HID) material and a periodic multilayered isotropic dielectric (PMLID) material. For any thickness of the metal layer, at least one compound SPP wave must exist. It possesses the p-polarization state, is strongly bound to the metal/HID interface when the metal thickness is large but to both metal/dielectric interfaces when the metal thickness is small. When the metal layer vanishes, this compound SPP wave transmutes into a Tamm wave. Additional compound SPP waves exist, depending on the thickness of the metal layer, the relative permittivity of the HID material, and the period and the composition of the PMLID material. Some of these are p polarized, the others being s polarized. All of them differ in phase speed, attenuation rate, and field profile, even though all are excitable at the same frequency. The multiplicity and the dependence of the number of compound SPP waves on the relative permittivity of the HID material when the metal layer is thin could be useful for optical sensing applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the fundamental property of compound surface-plasmon-polariton (SPP) waves that allows them to exist in a structure consisting of a metal layer sandwiched between a homogeneous isotropic dielectric (HID) material and a periodically multilayered isotropic dielectric (PMLID) material?\n\nA) They are only bound to the metal/dielectric interface when the metal thickness is small.\nB) They are strongly bound to the metal/HID interface when the metal thickness is large, but to both metal/dielectric interfaces when the metal thickness is small.\nC) They are only bound to the metal/dielectric interface when the metal thickness is large.\nD) They are not bound to any interface and can propagate freely.\n\n**Correct Answer:** B) They are strongly bound to the metal/HID interface when the metal thickness is large, but to both metal/dielectric interfaces when the metal thickness is small.\n\n**Explanation:** According to the documentation, compound SPP waves possess the p-polarization state and are strongly bound to the metal/HID interface when the metal thickness is large. However, when the metal thickness is small, they are bound to both metal/dielectric interfaces. This property is crucial for understanding the behavior of compound SPP waves in different metal layer thicknesses."}, "53": {"documentation": {"title": "Frequency-difference-dependent stochastic resonance in neural systems", "source": "Daqing Guo, Matjaz Perc, Yangsong Zhang, Peng Xu, Dezhong Yao", "docs_id": "1708.02554", "section": ["q-bio.NC", "nlin.AO", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Frequency-difference-dependent stochastic resonance in neural systems. Biological neurons receive multiple noisy oscillatory signals, and their dynamical response to the superposition of these signals is of fundamental importance for information processing in the brain. Here we study the response of neural systems to the weak envelope modulation signal, which is superimposed by two periodic signals with different frequencies. We show that stochastic resonance occurs at the beat frequency in neural systems at the single-neuron as well as the population level. The performance of this frequency-difference-dependent stochastic resonance is influenced by both the beat frequency and the two forcing frequencies. Compared to a single neuron, a population of neurons is more efficient in detecting the information carried by the weak envelope modulation signal at the beat frequency. Furthermore, an appropriate fine-tuning of the excitation-inhibition balance can further optimize the response of a neural ensemble to the superimposed signal. Our results thus introduce and provide insights into the generation and modulation mechanism of the frequency-difference-dependent stochastic resonance in neural systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which neural systems process information when receiving multiple noisy oscillatory signals with different frequencies?\n\nA) Stochastic resonance occurs at the mean frequency of the signals, enhancing the detection of the weak envelope modulation signal.\nB) The beat frequency between the two periodic signals is the key factor in determining the response of neural systems to the superimposed signal.\nC) The population level response is more efficient in detecting the information carried by the weak envelope modulation signal at the beat frequency, whereas single neurons are less effective.\nD) The excitation-inhibition balance in neural ensembles plays a crucial role in optimizing the response to the superimposed signal, but not directly related to the beat frequency.\n\n**Correct Answer:** B) The beat frequency between the two periodic signals is the key factor in determining the response of neural systems to the superimposed signal.\n\n**Explanation:** The correct answer is B) The beat frequency between the two periodic signals is the key factor in determining the response of neural systems to the superimposed signal. This is because the study shows that stochastic resonance occurs at the beat frequency in neural systems, both at the single-neuron and population levels. The beat frequency is the difference between the two forcing frequencies, and it is this frequency that is responsible for the enhanced detection of the weak envelope modulation signal."}, "54": {"documentation": {"title": "Non-CSS color codes on 2D lattices : Models and Topological Properties", "source": "Pramod Padmanabhan, Abhishek Chowdhury, Fumihiko Sugino", "docs_id": "2112.13617", "section": ["quant-ph", "cond-mat.str-el", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-CSS color codes on 2D lattices : Models and Topological Properties. The two-dimensional color code is an alternative to the toric code that encodes more logical qubits while maintaining crucial features of the $\\mathbb{Z}_2\\times\\mathbb{Z}_2$ toric code in the long wavelength limit. However its short range physics include single qubit Pauli operations that violate either three or six stabilisers as opposed to the toric code where single qubit Pauli operations violate two or four stabilisers. Exploiting this fact we construct several non-CSS versions of the two-dimensional color code falling into two families - those where either three, four or five stabilisers are violated and those which violate exactly four stabilisers for all the three types of single qubit Pauli operations. These models are not equivalent to the original color code by a local unitary transformation. While several such models are possible, we identify those models that have the same long range properties as the original color code. As a consequence of the non-CSS structure, the logical operators are of a mixed type which in some cases include all the three Pauli operators making them potentially useful for protection against biased Pauli noise."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the non-CSS versions of the two-dimensional color code over the original color code, and how does this relate to the long-range properties of the models?\n\n**A)** The non-CSS versions of the two-dimensional color code can encode more logical qubits while maintaining the same long-range properties as the original color code, but with the added benefit of being more robust against biased Pauli noise.\n\n**B)** The non-CSS versions of the two-dimensional color code can encode fewer logical qubits than the original color code, but with the added benefit of being more robust against biased Pauli noise.\n\n**C)** The non-CSS versions of the two-dimensional color code have the same long-range properties as the original color code, but with the added benefit of being more robust against biased Pauli noise.\n\n**D)** The non-CSS versions of the two-dimensional color code can encode more logical qubits than the original color code, but with the added benefit of being less robust against biased Pauli noise.\n\n**Correct Answer:** C) The non-CSS versions of the two-dimensional color code have the same long-range properties as the original color code, but with the added benefit of being more robust against biased Pauli noise.\n\n**Explanation:** The correct answer is C) because the non-CSS versions of the two-dimensional color code are constructed to have the same long-range properties as the original color code, but with the added benefit of being more robust against biased Pauli noise. This is stated in the introduction of the paper, which says that the non-CSS versions of the two-dimensional color code \"have the same long range properties as the original color code\". Additionally, the paper mentions that the logical operators in the non-CSS versions of the two-dimensional color code are of a mixed type, which in some cases include all three Pauli operators, making them potentially useful for protection against biased Pauli noise."}, "55": {"documentation": {"title": "An authenticated and secure accounting system for international\n  emissions trading", "source": "Chenxing Li, Yang Yu, Andrew Chi-Chih Yao, Da Zhang, Xiliang Zhang", "docs_id": "2011.13954", "section": ["econ.GN", "cs.CR", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An authenticated and secure accounting system for international\n  emissions trading. Expanding multi-country emissions trading system is considered as crucial to fill the existing mitigation gap for the 2\\degree C climate target. Trustworthy emissions accounting is the cornerstone of such a system encompassing different jurisdictions. However, traditional emissions measuring, reporting, and verification practices that support data authenticity might not be applicable as detailed data from large utilities and production facilities to be covered in the multi-country emissions trading system are usually highly sensitive and of severe national security concern. In this study, we propose a cryptographic framework for an authenticated and secure emissions accounting system that can resolve this data dilemma. We demonstrate that integrating a sequence of cryptographic protocols can preserve data authenticity and security for a stylized multi-country emissions trading system. We call for more research to promote applications of modern cryptography in future international climate governance to build trust and strengthen collaboration."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary concern of traditional emissions measuring, reporting, and verification practices in the context of a multi-country emissions trading system, and how does the proposed cryptographic framework address this concern?\n\nA) The primary concern is that traditional practices are not scalable to accommodate large utilities and production facilities, and the proposed framework addresses this by using a hierarchical structure to manage data.\nB) The primary concern is that traditional practices are not secure enough to protect sensitive data, and the proposed framework addresses this by using a sequence of cryptographic protocols to preserve data authenticity and security.\nC) The primary concern is that traditional practices are not transparent enough to ensure accountability, and the proposed framework addresses this by using a blockchain-based system to record all transactions.\nD) The primary concern is that traditional practices are not cost-effective, and the proposed framework addresses this by using a cloud-based system to reduce operational costs.\n\nCorrect Answer: B) The primary concern is that traditional practices are not secure enough to protect sensitive data, and the proposed framework addresses this by using a sequence of cryptographic protocols to preserve data authenticity and security.\n\nExplanation: The question requires the test-taker to understand the context of the study and the challenges posed by traditional emissions measuring, reporting, and verification practices. The correct answer, B, highlights the primary concern of protecting sensitive data, which is a key aspect of the proposed cryptographic framework. The other options are incorrect because they do not accurately reflect the primary concern or the solution proposed in the study."}, "56": {"documentation": {"title": "Exploring Maps with Greedy Navigators", "source": "Sang Hoon Lee, Petter Holme", "docs_id": "1111.2125", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring Maps with Greedy Navigators. During the last decade of network research focusing on structural and dynamical properties of networks, the role of network users has been more or less underestimated from the bird's-eye view of global perspective. In this era of global positioning system equipped smartphones, however, a user's ability to access local geometric information and find efficient pathways on networks plays a crucial role, rather than the globally optimal pathways. We present a simple greedy spatial navigation strategy as a probe to explore spatial networks. These greedy navigators use directional information in every move they take, without being trapped in a dead end based on their memory about previous routes. We suggest that the centralities measures have to be modified to incorporate the navigators' behavior, and present the intriguing effect of navigators' greediness where removing some edges may actually enhance the routing efficiency, which is reminiscent of Braess's paradox. In addition, using samples of road structures in large cities around the world, it is shown that the navigability measure we define reflects unique structural properties, which are not easy to predict from other topological characteristics. In this respect, we believe that our routing scheme significantly moves the routing problem on networks one step closer to reality, incorporating the inevitable incompleteness of navigators' information."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the greedy spatial navigation strategy presented in the paper \"Exploring Maps with Greedy Navigators\", and how does it relate to the role of network users in modern network research?\n\n**A)** The primary motivation is to optimize global pathways, as network users are often assumed to have access to complete and accurate information about the network. This assumption is challenged by the increasing availability of local geometric information through smartphones.\n\n**B)** The primary motivation is to incorporate the navigators' behavior into centrality measures, as the traditional measures are no longer sufficient to capture the complexities of spatial networks.\n\n**C)** The primary motivation is to demonstrate the effect of navigators' greediness on routing efficiency, and to show that removing some edges can actually enhance the routing efficiency, reminiscent of Braess's paradox.\n\n**D)** The primary motivation is to develop a routing scheme that reflects the unique structural properties of spatial networks, which are not easily predicted by traditional topological characteristics.\n\n**Correct Answer:** C) The primary motivation is to demonstrate the effect of navigators' greediness on routing efficiency, and to show that removing some edges can actually enhance the routing efficiency, reminiscent of Braess's paradox.\n\n**Explanation:** The correct answer is C) because the paper explicitly states that the greedy spatial navigation strategy is used to explore spatial networks and to demonstrate the effect of navigators' greediness on routing efficiency. The authors also mention Braess's paradox, which is a well-known phenomenon in network theory where removing some edges can actually decrease the overall efficiency of the network. This motivation is central to the paper's discussion of the navigators' behavior and its impact on routing efficiency."}, "57": {"documentation": {"title": "Long-range two-hybrid-qubit gates mediated by a microwave cavity with\n  red sidebands", "source": "J. C. Abadillo-Uriel, Cameron King, S. N. Coppersmith, Mark Friesen", "docs_id": "2106.10555", "section": ["cond-mat.mes-hall", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Long-range two-hybrid-qubit gates mediated by a microwave cavity with\n  red sidebands. Implementing two-qubit gates via strong coupling between quantum-dot qubits and a superconducting microwave cavity requires achieving coupling rates that are much faster than decoherence rates. Typically, this involves tuning the qubit either to a sweet spot, where it is relatively insensitive to charge noise, or to a point where it is resonant with the microwave cavity. Unfortunately, such operating points seldom coincide. Here, we theoretically investigate several schemes for performing gates between two quantum-dot hybrid qubits, mediated by a microwave cavity. The rich physics of the quantum dot hybrid qubit gives rise to two types of sweet spots, which can occur at operating points with strong charge dipole moments. Such strong interactions provide new opportunities for off-resonant gating, thereby removing one of the main obstacles for long-distance two-qubit gates. Our results suggest that the numerous tuning knobs of quantum dot hybrid qubits make them good candidates for strong coupling. In particular, we show that off-resonant red-sideband-mediated two-qubit gates can exhibit fidelities $>$95\\% for realistic operating parameters, and we describe improvements that could potentially yield gate fidelities $>$99\\%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in implementing two-qubit gates via strong coupling between quantum-dot qubits and a superconducting microwave cavity, and how do the authors of the study address this challenge?\n\nA) The primary challenge is achieving a balance between coupling rates and decoherence rates, and the authors address this by tuning the qubit to a sweet spot where it is relatively insensitive to charge noise.\n\nB) The primary challenge is achieving a balance between coupling rates and decoherence rates, and the authors address this by tuning the qubit to a point where it is resonant with the microwave cavity.\n\nC) The primary challenge is achieving a balance between coupling rates and decoherence rates, and the authors address this by using a microwave cavity with red sidebands to mediate the gates.\n\nD) The primary challenge is achieving a balance between coupling rates and decoherence rates, and the authors address this by using a different type of qubit altogether.\n\nCorrect Answer: C) The primary challenge is achieving a balance between coupling rates and decoherence rates, and the authors address this by using a microwave cavity with red sidebands to mediate the gates.\n\nExplanation: The correct answer is C) because the study specifically mentions that the authors investigate schemes for performing gates between two quantum-dot hybrid qubits mediated by a microwave cavity with red sidebands. This is the key innovation that allows for off-resonant gating and overcomes the primary challenge of achieving coupling rates that are much faster than decoherence rates. The other options are incorrect because they either mention tuning the qubit to a sweet spot or a resonant point, which are not the primary challenges addressed by the study."}, "58": {"documentation": {"title": "Frank-Wolfe with Subsampling Oracle", "source": "Thomas Kerdreux, Fabian Pedregosa and Alexandre d'Aspremont", "docs_id": "1803.07348", "section": ["math.OC", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Frank-Wolfe with Subsampling Oracle. We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \\emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of the algorithms on regression problems, involving both $\\ell_1$ and latent group lasso penalties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the convergence rates of the proposed randomized variants of the Frank-Wolfe algorithm and the Away-step FW algorithm. How do the subsampling oracles affect the convergence rates, and what are the implications for computational efficiency?\n\n**A)** The proposed randomized variants of the Frank-Wolfe algorithm achieve a linear convergence rate, while the Away-step FW algorithm achieves a sublinear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions.\n\n**B)** The proposed randomized variants of the Frank-Wolfe algorithm achieve a sublinear convergence rate, while the Away-step FW algorithm achieves a linear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions.\n\n**C)** The proposed randomized variants of the Frank-Wolfe algorithm achieve a linear convergence rate, while the Away-step FW algorithm achieves a sublinear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions.\n\n**D)** The proposed randomized variants of the Frank-Wolfe algorithm achieve a sublinear convergence rate, while the Away-step FW algorithm achieves a linear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions.\n\n**Correct Answer:** B) The proposed randomized variants of the Frank-Wolfe algorithm achieve a sublinear convergence rate, while the Away-step FW algorithm achieves a linear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions.\n\n**Explanation:** The correct answer is B) because the proposed randomized variants of the Frank-Wolfe algorithm achieve a sublinear convergence rate of $\\mathcal{O}(1/t)$, while the Away-step FW algorithm achieves a linear convergence rate. The subsampling oracles reduce the convergence rate by a constant factor, but the linear minimization step can be a fraction of the cost of the deterministic versions, especially when the data is streamed."}, "59": {"documentation": {"title": "Symmetric invariant manifolds in the Fermi-Pasta-Ulam lattice", "source": "B. Rink", "docs_id": "nlin/0209054", "section": ["nlin.SI", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetric invariant manifolds in the Fermi-Pasta-Ulam lattice. The Fermi-Pasta-Ulam (FPU) lattice with periodic boundary conditions and $n$ particles admits a large group of discrete symmetries. The fixed point sets of these symmetries naturally form invariant symplectic manifolds that are investigated in this short note. For each $k$ dividing $n$ we find $k$ degree of freedom invariant manifolds. They represent short wavelength solutions composed of $k$ Fourier-modes and can be interpreted as embedded lattices with periodic boundary conditions and only $k$ particles. Inside these invariant manifolds other invariant structures and exact solutions are found which represent for instance periodic and quasi-periodic solutions and standing and traveling waves. Some of these results have been found previously by other authors via a study of mode coupling coefficients and recently also by investigating `bushes of normal modes'. The method of this paper is similar to the latter method and much more systematic than the former. We arrive at previously unknown results without any difficult computations. It is shown moreover that similar invariant manifolds exist also in the Klein-Gordon lattice and in the thermodynamic and continuum limits."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Fermi-Pasta-Ulam (FPU) lattice with periodic boundary conditions and n particles. What is the relationship between the number of particles (n) and the number of degree of freedom invariant manifolds found in the paper?\n\nA) The number of degree of freedom invariant manifolds is equal to the number of particles (n).\nB) The number of degree of freedom invariant manifolds is equal to the number of particles (n) divided by 2.\nC) The number of degree of freedom invariant manifolds is equal to the number of particles (n) divided by k, where k is a divisor of n.\nD) The number of degree of freedom invariant manifolds is equal to the number of particles (n) minus the number of particles that are not part of any invariant manifold.\n\nCorrect Answer: C) The number of degree of freedom invariant manifolds is equal to the number of particles (n) divided by k, where k is a divisor of n.\n\nExplanation: The paper states that for each k dividing n, we find k degree of freedom invariant manifolds. This means that the number of invariant manifolds is related to the number of particles (n) and the divisors of n. The correct answer, C, reflects this relationship. The other options are incorrect because they do not accurately represent the relationship between n and the number of invariant manifolds."}}