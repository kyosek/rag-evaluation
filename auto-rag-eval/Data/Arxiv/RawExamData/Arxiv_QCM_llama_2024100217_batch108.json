{"0": {"documentation": {"title": "Information thermodynamics of financial markets: the Glosten-Milgrom\n  model", "source": "L\\'eo Touzo, Matteo Marsili, Don Zagier", "docs_id": "2010.01905", "section": ["cond-mat.stat-mech", "econ.TH", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information thermodynamics of financial markets: the Glosten-Milgrom\n  model. The Glosten-Milgrom model describes a single asset market, where informed traders interact with a market maker, in the presence of noise traders. We derive an analogy between this financial model and a Szil\\'ard information engine by {\\em i)} showing that the optimal work extraction protocol in the latter coincides with the pricing strategy of the market maker in the former and {\\em ii)} defining a market analogue of the physical temperature from the analysis of the distribution of market orders. Then we show that the expected gain of informed traders is bounded above by the product of this market temperature with the amount of information that informed traders have, in exact analogy with the corresponding formula for the maximal expected amount of work that can be extracted from a cycle of the information engine. This suggests that recent ideas from information thermodynamics may shed light on financial markets, and lead to generalised inequalities, in the spirit of the extended second law of thermodynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Glosten-Milgrom model and the Szil\\'ard information engine, what is the relationship between the market temperature and the expected gain of informed traders, and how does this relationship compare to the corresponding relationship in the information engine?\n\n**A)** The market temperature is directly proportional to the expected gain of informed traders, and this relationship is identical to the one in the information engine.\n\n**B)** The market temperature is inversely proportional to the expected gain of informed traders, and this relationship is identical to the one in the information engine.\n\n**C)** The market temperature is proportional to the expected gain of informed traders, but the relationship is not identical to the one in the information engine, which is given by the product of the market temperature and the amount of information that informed traders have.\n\n**D)** The market temperature is a measure of the noise traders' activity, and the expected gain of informed traders is bounded above by the product of the market temperature and the amount of information that informed traders have, but this relationship is not identical to the one in the information engine.\n\n**Correct Answer:** C) The market temperature is proportional to the expected gain of informed traders, but the relationship is not identical to the one in the information engine, which is given by the product of the market temperature and the amount of information that informed traders have.\n\n**Explanation:** The correct answer is C) because the documentation states that the expected gain of informed traders is bounded above by the product of the market temperature and the amount of information that informed traders have, but this relationship is not identical to the one in the information engine, which is given by the product of the market temperature and the amount of information that informed traders have. The market temperature is defined as a market analogue of the physical temperature, and it is proportional to the expected gain of informed traders, but the relationship is not identical to the one in the information engine."}, "1": {"documentation": {"title": "Backward-in-Time Selection of the Order of Dynamic Regression Prediction\n  Model", "source": "Ioannis Vlachos and Dimitris Kugiumtzis", "docs_id": "1301.2410", "section": ["stat.AP", "stat.ME", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Backward-in-Time Selection of the Order of Dynamic Regression Prediction\n  Model. We investigate the optimal structure of dynamic regression models used in multivariate time series prediction and propose a scheme to form the lagged variable structure called Backward-in-Time Selection (BTS) that takes into account feedback and multi-collinearity, often present in multivariate time series. We compare BTS to other known methods, also in conjunction with regularization techniques used for the estimation of model parameters, namely principal components, partial least squares and ridge regression estimation. The predictive efficiency of the different models is assessed by means of Monte Carlo simulations for different settings of feedback and multi-collinearity. The results show that BTS has consistently good prediction performance while other popular methods have varying and often inferior performance. The prediction performance of BTS was also found the best when tested on human electroencephalograms of an epileptic seizure, and to the prediction of returns of indices of world financial markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Backward-in-Time Selection of the Order of Dynamic Regression Prediction Model\" propose a method called Backward-in-Time Selection (BTS) to select the optimal structure of dynamic regression models for multivariate time series prediction. What are the key advantages of BTS over other methods, such as principal components, partial least squares, and ridge regression estimation, according to the authors' findings?\n\n**A)** BTS is more computationally efficient than other methods, but may not capture complex relationships between variables.\n**B)** BTS is more robust to multicollinearity and feedback effects, resulting in better predictive performance.\n**C)** BTS is more interpretable than other methods, but may not be suitable for large datasets.\n**D)** BTS is less prone to overfitting than other methods, but may not capture underlying patterns in the data.\n\n**Correct Answer:** B) BTS is more robust to multicollinearity and feedback effects, resulting in better predictive performance.\n\n**Explanation:** According to the paper, BTS takes into account feedback and multi-collinearity, which are often present in multivariate time series. The authors find that BTS has consistently good prediction performance, even when compared to other popular methods, such as principal components, partial least squares, and ridge regression estimation. This suggests that BTS is more robust to these issues, resulting in better predictive performance."}, "2": {"documentation": {"title": "Survival and Extinction in Cyclic and Neutral Three--Species Systems", "source": "Margarita Ifti and Birger Bergersen", "docs_id": "nlin/0208023", "section": ["nlin.AO", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Survival and Extinction in Cyclic and Neutral Three--Species Systems. We study the ABC model (A + B --> 2B, B + C --> 2C, C + A --> 2A), and its counterpart: the three--component neutral drift model (A + B --> 2A or 2B, B + C --> 2B or 2C, C + A --> 2C or 2A.) In the former case, the mean field approximation exhibits cyclic behaviour with an amplitude determined by the initial condition. When stochastic phenomena are taken into account the amplitude of oscillations will drift and eventually one and then two of the three species will become extinct. The second model remains stationary for all initial conditions in the mean field approximation, and drifts when stochastic phenomena are considered. We analyzed the distribution of first extinction times of both models by simulations and from the point of view of the Fokker-Planck equation. Survival probability vs. time plots suggest an exponential decay. For the neutral model the extinction rate is inversely proportional to the system size, while the cyclic model exhibits anomalous behaviour for small system sizes. In the large system size limit the extinction times for both models will be the same. This result is compatible with the smallest eigenvalue obtained from the numerical solution of the Fokker-Planck equation. We also studied the long--time behaviour of the probability distribution. The exponential decay is found to be robust against certain changes, such as the three reactions having different rates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the long-term behavior of the probability distribution in the ABC model and its neutral counterpart. How do the extinction rates of these two models compare, and what is the implication of the Fokker-Planck equation solution on the large system size limit?\n\n**A)** The neutral model exhibits anomalous behavior in the large system size limit, while the ABC model's extinction rate is inversely proportional to the system size. The Fokker-Planck equation solution suggests that the extinction times for both models will be the same in the large system size limit.\n\n**B)** The ABC model's long-term behavior is characterized by an exponential decay, while the neutral model's probability distribution remains stationary for all initial conditions. The Fokker-Planck equation solution indicates that the extinction rate of the neutral model is inversely proportional to the system size.\n\n**C)** The ABC model's extinction rate is directly proportional to the system size, while the neutral model's extinction rate is inversely proportional to the system size. The Fokker-Planck equation solution suggests that the large system size limit is characterized by a non-exponential decay.\n\n**D)** The neutral model's long-term behavior is characterized by an exponential decay, while the ABC model's probability distribution remains stationary for all initial conditions. The Fokker-Planck equation solution indicates that the extinction times for both models will be the same in the large system size limit, and the extinction rate of the ABC model is inversely proportional to the system size.\n\n**Correct Answer:** A) The neutral model exhibits anomalous behavior in the large system size limit, while the ABC model's extinction rate is inversely proportional to the system size. The Fokker-Planck equation solution suggests that the extinction times for both models will be the same in the large system size limit.\n\n**Explanation:**\n\nThe correct answer is A) because the documentation states that the neutral model exhibits anomalous behavior in the large system size limit, while the ABC model's extinction rate is inversely proportional to the system size. Additionally, the Fokker-Planck equation solution suggests that the extinction times for both models will be the same in the large system size limit, which is consistent with the smallest eigenvalue obtained from the numerical solution.\n\nCandidate B is incorrect because the documentation states that the neutral model's probability distribution remains stationary for all initial conditions, not that it remains stationary in the long term.\n\nCandidate C is incorrect because the documentation states that the ABC model's extinction rate is inversely proportional to the system size, not directly proportional.\n\nCandidate D is incorrect because the documentation states that the extinction times for both models will be the same in the large system size limit, but it does not state that the extinction rate of the ABC model is inversely proportional to the system size."}, "3": {"documentation": {"title": "On LASSO for Predictive Regression", "source": "Ji Hyung Lee, Zhentao Shi, Zhan Gao", "docs_id": "1810.03140", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On LASSO for Predictive Regression. Explanatory variables in a predictive regression typically exhibit low signal strength and various degrees of persistence. Variable selection in such a context is of great importance. In this paper, we explore the pitfalls and possibilities of the LASSO methods in this predictive regression framework. In the presence of stationary, local unit root, and cointegrated predictors, we show that the adaptive LASSO cannot asymptotically eliminate all cointegrating variables with zero regression coefficients. This new finding motivates a novel post-selection adaptive LASSO, which we call the twin adaptive LASSO (TAlasso), to restore variable selection consistency. Accommodating the system of heterogeneous regressors, TAlasso achieves the well-known oracle property. In contrast, conventional LASSO fails to attain coefficient estimation consistency and variable screening in all components simultaneously. We apply these LASSO methods to evaluate the short- and long-horizon predictability of S\\&P 500 excess returns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of predictive regression, what is the primary challenge that the adaptive LASSO method faces when dealing with cointegrated predictors, and how does the twin adaptive LASSO (TAlasso) address this issue?\n\n**A)** The adaptive LASSO method struggles to eliminate cointegrating variables with zero regression coefficients due to the presence of stationary, local unit root, and cointegrated predictors, and TAlasso overcomes this by using a novel post-selection approach.\n\n**B)** The adaptive LASSO method achieves asymptotic consistency in eliminating cointegrating variables with zero regression coefficients, but TAlasso is unnecessary as it does not provide any additional benefits.\n\n**C)** The adaptive LASSO method is unable to accommodate heterogeneous regressors, whereas TAlasso is able to achieve the oracle property, but this is not the primary challenge faced by the adaptive LASSO method.\n\n**D)** The adaptive LASSO method is unable to restore variable selection consistency in the presence of cointegrated predictors, and TAlasso is not a viable solution to this problem.\n\n**Correct Answer:** A) The adaptive LASSO method struggles to eliminate cointegrating variables with zero regression coefficients due to the presence of stationary, local unit root, and cointegrated predictors, and TAlasso overcomes this by using a novel post-selection approach.\n\n**Explanation:** The correct answer is A) because the documentation states that the adaptive LASSO method \"cannot asymptotically eliminate all cointegrating variables with zero regression coefficients\" in the presence of stationary, local unit root, and cointegrated predictors. This is a key finding that motivates the development of the twin adaptive LASSO (TAlasso) method. The other options are incorrect because they either misrepresent the adaptive LASSO method's limitations or overstate the benefits of TAlasso."}, "4": {"documentation": {"title": "Kinematics of Two Eruptive Prominences observed by EUVI/STEREO", "source": "Anand D. Joshi and Nandita Srivastava", "docs_id": "1101.4543", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kinematics of Two Eruptive Prominences observed by EUVI/STEREO. Two large northern polar crown prominences that erupted on 2010 April 13 and 2010 August 1 were analysed using images obtained from the Extreme UltraViolet Imager on the twin Solar Terrestrial Relations Observatory spacecraft. Several features along the prominence legs were reconstructed using a stereoscopic reconstruction technique developed by us. The three-dimensional changes exhibited by the prominences can be explained as an interplay between two different motions, namely helical twist in the prominence spine, and overall non-radial equatorward motion of the entire prominence structure. The sense of twist in both the prominences is determined from the changes in latitudes and longitudes of the reconstructed features. The prominences are observed starting from a few hours before the eruption. Increase in height before and during the eruption allowed us to study kinematics of the prominences in the two phases of eruption, the slow rise and the fast eruptive phase. A constant value of acceleration was found for each reconstructed feature in each phase, but it showed significant change from one leg to the other in both the prominences. The magnitude of acceleration during the eruptive phase is found to be commensurate with the net effect of the two motions stated above."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the kinematics of the two eruptive prominences observed by EUVI/STEREO on 2010 April 13 and 2010 August 1. What is the significance of the constant value of acceleration found for each reconstructed feature in each phase of eruption, and how does it relate to the overall motion of the prominences?\n\n**A)** The constant value of acceleration indicates that the prominences were moving at a constant speed throughout the eruption, with no significant changes in their motion.\n\n**B)** The constant value of acceleration suggests that the prominences were experiencing a constant force, which is consistent with the helical twist in the prominence spine and the non-radial equatorward motion of the entire prominence structure.\n\n**C)** The constant value of acceleration implies that the prominences were undergoing a uniform acceleration, which is not supported by the data, as the magnitude of acceleration during the eruptive phase is found to be commensurate with the net effect of the two motions.\n\n**D)** The constant value of acceleration indicates that the prominences were moving in a straight line, with no significant changes in their motion or direction.\n\n**Correct Answer:** B) The constant value of acceleration suggests that the prominences were experiencing a constant force, which is consistent with the helical twist in the prominence spine and the non-radial equatorward motion of the entire prominence structure.\n\n**Explanation:** The correct answer, B, is supported by the text, which states that \"A constant value of acceleration was found for each reconstructed feature in each phase, but it showed significant change from one leg to the other in both the prominences.\" This suggests that the acceleration is not constant throughout the eruption, but rather varies from one leg to the other. The text also states that the magnitude of acceleration during the eruptive phase is found to be commensurate with the net effect of the two motions, namely helical twist in the prominence spine and overall non-radial equatorward motion of the entire prominence structure. This supports the idea that the constant value of acceleration is related to the overall motion of the prominences, rather than a constant speed or direction."}, "5": {"documentation": {"title": "Scale-dependent bias and bispectrum in neutrino separate universe\n  simulations", "source": "Chi-Ting Chiang, Wayne Hu, Yin Li, Marilena LoVerde", "docs_id": "1710.01310", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scale-dependent bias and bispectrum in neutrino separate universe\n  simulations. Cosmic background neutrinos have a large velocity dispersion, which causes the evolution of long-wavelength density perturbations to depend on scale. This scale-dependent growth leads to the well-known suppression in the linear theory matter power spectrum that is used to probe neutrino mass. In this paper, we study the impact of long-wavelength density perturbations on small-scale structure formation. By performing separate universe simulations where the long-wavelength mode is absorbed into the local expansion, we measure the responses of the cold dark matter (CDM) power spectrum and halo mass function, which correspond to the squeezed-limit bispectrum and halo bias. We find that the scale-dependent evolution of the long-wavelength modes causes these quantities to depend on scale and provide simple expressions to model them in terms of scale and the amount of massive neutrinos. Importantly, this scale-dependent bias reduces the suppression in the linear halo power spectrum due to massive neutrinos by 13 and 26% for objects of bias $\\bar{b}=2$ and $\\bar{b} \\gg1$, respectively. We demonstrate with high statistical significance that the scale-dependent halo bias ${\\it cannot}$ be modeled by the CDM and neutrino density transfer functions at the time when the halos are identified. This reinforces the importance of the temporal nonlocality of structure formation, especially when the growth is scale dependent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of separate universe simulations, what is the primary effect of scale-dependent growth of long-wavelength density perturbations on the halo mass function, and how does it relate to the suppression of the linear halo power spectrum due to massive neutrinos?\n\nA) The scale-dependent growth leads to an increase in the halo mass function, which in turn reduces the suppression of the linear halo power spectrum due to massive neutrinos by 13%.\n\nB) The scale-dependent growth leads to a decrease in the halo mass function, which in turn reduces the suppression of the linear halo power spectrum due to massive neutrinos by 26%.\n\nC) The scale-dependent growth has no effect on the halo mass function, and the suppression of the linear halo power spectrum due to massive neutrinos remains unchanged.\n\nD) The scale-dependent growth leads to an increase in the halo mass function, which in turn increases the suppression of the linear halo power spectrum due to massive neutrinos by 13%.\n\nCorrect Answer: B) The scale-dependent growth leads to a decrease in the halo mass function, which in turn reduces the suppression of the linear halo power spectrum due to massive neutrinos by 26%.\n\nExplanation: The correct answer is B) because the documentation states that the scale-dependent growth of long-wavelength density perturbations leads to a decrease in the halo mass function, which in turn reduces the suppression of the linear halo power spectrum due to massive neutrinos by 13% for objects of bias $\\bar{b}=2$, and by 26% for objects of bias $\\bar{b} \\gg1$. This is a consequence of the temporal nonlocality of structure formation, which is reinforced by the fact that the scale-dependent halo bias cannot be modeled by the CDM and neutrino density transfer functions at the time when the halos are identified."}, "6": {"documentation": {"title": "Urban Housing Prices and Migration's Fertility Intentions: Based on the\n  2018 China Migrants' Dynamic Survey", "source": "Jingwen Tan and Shixi Kang", "docs_id": "2112.07273", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Urban Housing Prices and Migration's Fertility Intentions: Based on the\n  2018 China Migrants' Dynamic Survey. While the size of China's mobile population continues to expand, the fertility rate is significantly lower than the stable generation replacement level of the population, and the structural imbalance of human resource supply has attracted widespread attention. This paper uses LPM and Probit models to estimate the impact of house prices on the fertility intentions of the mobile population based on data from the 2018 National Mobile Population Dynamics Monitoring Survey. The lagged land sales price is used as an instrumental variable of house price to mitigate the potential endogeneity problem. The results show that for every 100\\% increase in the ratio of house price to household income of mobile population, the fertility intention of the female mobile population of working age at the inflow location will decrease by 4.42\\%, and the marginal effect of relative house price on labor force fertility intention is EXP(-0.222); the sensitivity of mobile population fertility intention to house price is affected by the moderating effect of infrastructure construction at the inflow location. The willingness to have children in the inflow area is higher for female migrants of working age with lower age, smaller family size and higher education. Based on the above findings, the study attempts to provide a new practical perspective for the mainline institutional change and balanced economic development in China's economic transition phase."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between house prices and fertility intentions among the mobile population in China, as estimated by the study. What is the marginal effect of relative house price on labor force fertility intention, and how does it interact with infrastructure construction at the inflow location?\n\n**A)** The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), and infrastructure construction at the inflow location has a moderating effect, increasing the fertility intention by 2.5% when the relative house price is high.\n\n**B)** The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), and infrastructure construction at the inflow location has a moderating effect, decreasing the fertility intention by 1.8% when the relative house price is low.\n\n**C)** The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), and infrastructure construction at the inflow location has a moderating effect, with no significant interaction between the two variables.\n\n**D)** The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), and infrastructure construction at the inflow location has a moderating effect, increasing the fertility intention by 5.6% when the relative house price is high.\n\n**Correct Answer:** C) The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), and infrastructure construction at the inflow location has a moderating effect, with no significant interaction between the two variables.\n\n**Explanation:** The study estimates that for every 100% increase in the ratio of house price to household income, the fertility intention of the female mobile population of working age at the inflow location decreases by 4.42%. The marginal effect of relative house price on labor force fertility intention is EXP(-0.222), which is approximately -0.795. However, the study also finds that infrastructure construction at the inflow location has a moderating effect on the relationship between house prices and fertility intentions, but this effect is not significant. Therefore, the correct answer is C."}, "7": {"documentation": {"title": "Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients", "source": "Panos Toulis and Edoardo M. Airoldi", "docs_id": "1408.2923", "section": ["stat.ME", "stat.CO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients. Stochastic gradient descent procedures have gained popularity for parameter estimation from large data sets. However, their statistical properties are not well understood, in theory. And in practice, avoiding numerical instability requires careful tuning of key parameters. Here, we introduce implicit stochastic gradient descent procedures, which involve parameter updates that are implicitly defined. Intuitively, implicit updates shrink standard stochastic gradient descent updates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed; thus, implicit procedures increase stability without increasing the computational burden. Our theoretical analysis provides the first full characterization of the asymptotic behavior of both standard and implicit stochastic gradient descent-based estimators, including finite-sample error bounds. Importantly, analytical expressions for the variances of these stochastic gradient-based estimators reveal their exact loss of efficiency. We also develop new algorithms to compute implicit stochastic gradient descent-based estimators for generalized linear models, Cox proportional hazards, M-estimators, in practice, and perform extensive experiments. Our results suggest that implicit stochastic gradient descent procedures are poised to become a workhorse for approximate inference from large data sets"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of implicit stochastic gradient descent procedures over standard stochastic gradient descent procedures, and how do they address the issue of numerical instability?\n\nA) Implicit procedures require explicit computation of the Fisher information matrix, whereas standard procedures do not. This leads to increased computational burden and reduced stability.\nB) Implicit procedures involve parameter updates that are implicitly defined, which shrink standard stochastic gradient descent updates and increase stability without increasing the computational burden.\nC) Implicit procedures are only applicable to generalized linear models, whereas standard procedures can be applied to a wider range of models.\nD) Implicit procedures are more efficient than standard procedures in terms of computational complexity, but do not address the issue of numerical instability.\n\nCorrect Answer: B) Implicit procedures involve parameter updates that are implicitly defined, which shrink standard stochastic gradient descent updates and increase stability without increasing the computational burden.\n\nExplanation: The correct answer is B) because the documentation states that implicit stochastic gradient descent procedures \"involve parameter updates that are implicitly defined, which intuitively shrink standard stochastic gradient descent updates.\" This implies that implicit procedures address the issue of numerical instability by shrinking the updates, making them more stable. The other options are incorrect because they either misrepresent the relationship between implicit and standard procedures (A and C), or do not address the issue of numerical instability (D)."}, "8": {"documentation": {"title": "Learning what they think vs. learning what they do: The\n  micro-foundations of vicarious learning", "source": "Sanghyun Park and Phanish Puranam", "docs_id": "2007.15264", "section": ["econ.TH", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning what they think vs. learning what they do: The\n  micro-foundations of vicarious learning. Vicarious learning is a vital component of organizational learning. We theorize and model two fundamental processes underlying vicarious learning: observation of actions (learning what they do) vs. belief sharing (learning what they think). The analysis of our model points to three key insights. First, vicarious learning through either process is beneficial even when no agent in a system of vicarious learners begins with a knowledge advantage. Second, vicarious learning through belief sharing is not universally better than mutual observation of actions and outcomes. Specifically, enabling mutual observability of actions and outcomes is superior to sharing of beliefs when the task environment features few alternatives with large differences in their value and there are no time pressures. Third, symmetry in vicarious learning in fact adversely affects belief sharing but improves observational learning. All three results are shown to be the consequence of how vicarious learning affects self-confirming biased beliefs."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of vicarious learning, what is the primary difference between the two fundamental processes underlying vicarious learning, and how do these differences impact the effectiveness of vicarious learning in various task environments?\n\n**A)** Vicarious learning through observation of actions is more beneficial than belief sharing when the task environment features many alternatives with similar values, while belief sharing is superior when the task environment features few alternatives with large differences in their value.\n\n**B)** Vicarious learning through observation of actions is universally better than belief sharing, regardless of the task environment, due to the benefits of mutual observation of actions and outcomes.\n\n**C)** Vicarious learning through belief sharing is not universally better than mutual observation of actions and outcomes, but rather is superior when the task environment features few alternatives with large differences in their value, and vice versa.\n\n**D)** Vicarious learning through observation of actions is adversely affected by symmetry, while belief sharing is improved by symmetry, leading to a trade-off between the two processes.\n\n**Correct Answer:** C) Vicarious learning through belief sharing is not universally better than mutual observation of actions and outcomes, but rather is superior when the task environment features few alternatives with large differences in their value, and vice versa.\n\n**Explanation:**\n\nThis question requires the test-taker to carefully read and understand the key insights from the Arxiv documentation. The correct answer, C, highlights the nuanced relationship between vicarious learning through observation of actions and belief sharing, and how these processes interact in different task environments. The incorrect answers, A, B, and D, oversimplify or misrepresent the findings from the documentation, demonstrating a lack of understanding of the complex relationships between vicarious learning and task environments.\n\nTo answer this question correctly, the test-taker must be able to:\n\n1. Identify the two fundamental processes underlying vicarious learning (observation of actions and belief sharing).\n2. Understand the conditions under which each process is beneficial or superior.\n3. Recognize the trade-offs between the two processes in different task environments.\n4. Analyze the relationships between vicarious learning, task environments, and the impact of symmetry on these relationships.\n\nThis question requires a deep understanding of the subject matter and the ability to think critically about the complex relationships between vicarious learning, task environments, and symmetry."}, "9": {"documentation": {"title": "Network Coded Gossip with Correlated Data", "source": "Bernhard Haeupler, Asaf Cohen, Chen Avin, Muriel M\\'edard", "docs_id": "1202.1801", "section": ["cs.IT", "cs.DC", "cs.DS", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Network Coded Gossip with Correlated Data. We design and analyze gossip algorithms for networks with correlated data. In these networks, either the data to be distributed, the data already available at the nodes, or both, are correlated. This model is applicable for a variety of modern networks, such as sensor, peer-to-peer and content distribution networks. Although coding schemes for correlated data have been studied extensively, the focus has been on characterizing the rate region in static memory-free networks. In a gossip-based scheme, however, nodes communicate among each other by continuously exchanging packets according to some underlying communication model. The main figure of merit in this setting is the stopping time -- the time required until nodes can successfully decode. While Gossip schemes are practical, distributed and scalable, they have only been studied for uncorrelated data. We wish to close this gap by providing techniques to analyze network coded gossip in (dynamic) networks with correlated data. We give a clean framework for oblivious network models that applies to a multitude of network and communication scenarios, specify a general setting for distributed correlated data, and give tight bounds on the stopping times of network coded protocols in this wide range of scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Network Coded Gossip with Correlated Data, what is the primary focus of the research, and what is the main limitation of existing gossip schemes for correlated data?\n\n**A)** The primary focus is on characterizing the rate region in static memory-free networks, and the main limitation is that existing gossip schemes are only applicable to uncorrelated data.\n\n**B)** The primary focus is on providing techniques to analyze network coded gossip in dynamic networks with correlated data, and the main limitation is that existing gossip schemes are not scalable.\n\n**C)** The primary focus is on specifying a general setting for distributed correlated data, and the main limitation is that existing gossip schemes are not practical for real-world networks.\n\n**D)** The primary focus is on giving tight bounds on the stopping times of network coded protocols in various network and communication scenarios, and the main limitation is that existing gossip schemes are not distributed.\n\n**Correct Answer:** A) The primary focus is on characterizing the rate region in static memory-free networks, and the main limitation is that existing gossip schemes are only applicable to uncorrelated data.\n\n**Explanation:**\n\nThe correct answer is A) because the documentation states that existing research on coding schemes for correlated data has focused on characterizing the rate region in static memory-free networks, and that the main limitation of existing gossip schemes is that they have only been studied for uncorrelated data. This highlights the gap in existing research that the new work aims to address.\n\nThe other options are incorrect because:\n\n* B) is incorrect because while the new work does aim to analyze network coded gossip in dynamic networks with correlated data, the main limitation of existing gossip schemes is not that they are not scalable.\n* C) is incorrect because specifying a general setting for distributed correlated data is actually one of the contributions of the new work, not a limitation.\n* D) is incorrect because giving tight bounds on the stopping times of network coded protocols is a key contribution of the new work, not a limitation of existing gossip schemes."}, "10": {"documentation": {"title": "Limits to green growth and the dynamics of innovation", "source": "Salvador Pueyo", "docs_id": "1904.09586", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Limits to green growth and the dynamics of innovation. Central to the official \"green growth\" discourse is the conjecture that absolute decoupling can be achieved with certain market instruments. This paper evaluates this claim focusing on the role of technology, while changes in GDP composition are treated elsewhere. Some fundamental difficulties for absolute decoupling, referring specifically to thermodynamic costs, are identified through a stylized model based on empirical knowledge on innovation and learning. Normally, monetary costs decrease more slowly than production grows, and this is unlikely to change should monetary costs align with thermodynamic costs, except, potentially, in the transition after the price reform. Furthermore, thermodynamic efficiency must eventually saturate for physical reasons. While this model, as usual, introduces technological innovation just as a source of efficiency, innovation also creates challenges: therefore, attempts to sustain growth by ever-accelerating innovation collide also with the limited reaction capacity of people and institutions. Information technology could disrupt innovation dynamics in the future, permitting quicker gains in eco-efficiency, but only up to saturation and exacerbating the downsides of innovation. These observations suggest that long-term sustainability requires much deeper transformations than the green growth discourse presumes, exposing the need to rethink scales, tempos and institutions, in line with ecological economics and the degrowth literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the claim that absolute decoupling can be achieved through market instruments, focusing on the role of technology, and explain why the paper suggests that long-term sustainability requires deeper transformations than the green growth discourse presumes.\n\n**A)** The paper suggests that absolute decoupling can be achieved through market instruments because technological innovation can lead to more efficient use of resources, thereby reducing thermodynamic costs. However, the paper also argues that the limited reaction capacity of people and institutions can hinder the sustainability of such growth, making deeper transformations necessary.\n\n**B)** The paper rejects the idea that absolute decoupling can be achieved through market instruments, as monetary costs decrease more slowly than production grows, and thermodynamic efficiency must eventually saturate. Furthermore, the paper suggests that attempts to sustain growth through ever-accelerating innovation can collide with the limitations of human and institutional capacity.\n\n**C)** The paper proposes that absolute decoupling can be achieved through market instruments, but only if information technology can disrupt innovation dynamics and permit quicker gains in eco-efficiency. However, the paper also warns that this could exacerbate the downsides of innovation, highlighting the need for deeper transformations.\n\n**D)** The paper suggests that absolute decoupling can be achieved through market instruments, but only if the price reform leads to a significant increase in thermodynamic costs, which would then align with monetary costs. However, the paper also argues that thermodynamic efficiency must eventually saturate, making deeper transformations necessary.\n\n**Correct Answer:** B) The paper rejects the idea that absolute decoupling can be achieved through market instruments, as monetary costs decrease more slowly than production grows, and thermodynamic efficiency must eventually saturate. Furthermore, the paper suggests that attempts to sustain growth through ever-accelerating innovation can collide with the limitations of human and institutional capacity.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the main argument of the paper. The paper argues that absolute decoupling is unlikely to be achieved through market instruments, as monetary costs decrease more slowly than production grows, and thermodynamic efficiency must eventually saturate. Additionally, the paper suggests that attempts to sustain growth through ever-accelerating innovation can collide with the limitations of human and institutional capacity, highlighting the need for deeper transformations."}, "11": {"documentation": {"title": "The approach to thermal equilibrium and \"thermodynamic normality\" --- An\n  observation based on the works by Goldstein, Lebowitz, Mastrodonato, Tumulka,\n  and Zanghi in 2009, and by von Neumann in 1929", "source": "Hal Tasaki", "docs_id": "1003.5424", "section": ["quant-ph", "cond-mat.stat-mech", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The approach to thermal equilibrium and \"thermodynamic normality\" --- An\n  observation based on the works by Goldstein, Lebowitz, Mastrodonato, Tumulka,\n  and Zanghi in 2009, and by von Neumann in 1929. We treat the problem of the approach to thermal equilibrium by only resorting to quantum dynamics of an isolated macroscopic system. Inspired by the two important works in 2009 and in 1929, we have noted that a condition we call \"thermodynamic normality\" for a macroscopic observable guarantees the approach to equilibrium (in the sense that a measurement of the observable at time $t$ almost certainly yields a result close to the corresponding microcanonical average for a sufficiently long and typical $t$). A crucial point is that we make no assumptions on the initial state of the system, except that its energy is distributed close to a certain macroscopic value. We also present three (rather artificial) models in which the thermodynamic normality can be established, thus providing concrete examples in which the approach to equilibrium is rigorously justified. Note that this kind of results which hold for ANY initial state are never possible in classical systems. We are thus dealing with a mechanism which is peculiar to quantum systems. The present note is written in a self-contained (and hopefully readable) manner. It only requires basic knowledge in quantum physics and equilibrium statistical mechanics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key assumption made about the initial state of the system in the approach to thermal equilibrium, as described in the 2009 works by Goldstein, Lebowitz, Mastrodonato, Tumulka, and Zanghi, and by von Neumann in 1929?\n\nA) The initial state is a pure quantum state.\nB) The initial state is a mixed quantum state with energy distributed close to a certain macroscopic value.\nC) The initial state is a classical system with no quantum properties.\nD) The initial state is a thermal equilibrium state.\n\n**Correct Answer:** B) The initial state is a mixed quantum state with energy distributed close to a certain macroscopic value.\n\n**Explanation:** The correct answer is B) because the documentation states that \"We make no assumptions on the initial state of the system, except that its energy is distributed close to a certain macroscopic value.\" This means that the initial state can be any mixed quantum state, as long as its energy is close to a certain macroscopic value. This assumption is crucial in establishing the thermodynamic normality and the approach to equilibrium.\n\n**Candidate A (Incorrect):** A) The initial state is a pure quantum state. This is not the correct assumption, as the documentation does not require the initial state to be a pure quantum state.\n\n**Candidate B (Incorrect):** B) The initial state is a mixed quantum state with energy distributed close to a certain macroscopic value. This is the correct assumption, but it is not the only possible answer. The documentation states that the initial state can be any mixed quantum state with energy close to a macroscopic value.\n\n**Candidate C (Incorrect):** C) The initial state is a classical system with no quantum properties. This is not relevant to the approach to thermal equilibrium, as the documentation deals with quantum systems.\n\n**Candidate D (Incorrect):** D) The initial state is a thermal equilibrium state. This is not the correct assumption, as the documentation does not require the initial state to be a thermal equilibrium state. In fact, the initial state can be any mixed quantum state with energy close to a macroscopic value."}, "12": {"documentation": {"title": "Acoustic Scene Classification Based on a Large-margin Factorized CNN", "source": "Janghoon Cho, Sungrack Yun, Hyoungwoo Park, Jungyun Eum and Kyuwoong\n  Hwang", "docs_id": "1910.06784", "section": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Acoustic Scene Classification Based on a Large-margin Factorized CNN. In this paper, we present an acoustic scene classification framework based on a large-margin factorized convolutional neural network (CNN). We adopt the factorized CNN to learn the patterns in the time-frequency domain by factorizing the 2D kernel into two separate 1D kernels. The factorized kernel leads to learn the main component of two patterns: the long-term ambient and short-term event sounds which are the key patterns of the audio scene classification. In training our model, we consider the loss function based on the triplet sampling such that the same audio scene samples from different environments are minimized, and simultaneously the different audio scene samples are maximized. With this loss function, the samples from the same audio scene are clustered independently of the environment, and thus we can get the classifier with better generalization ability in an unseen environment. We evaluated our audio scene classification framework using the dataset of the DCASE challenge 2019 task1A. Experimental results show that the proposed algorithm improves the performance of the baseline network and reduces the number of parameters to one third. Furthermore, the performance gain is higher on unseen data, and it shows that the proposed algorithm has better generalization ability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a large-margin factorized convolutional neural network (CNN) for acoustic scene classification, as proposed in the paper \"Acoustic Scene Classification Based on a Large-margin Factorized CNN\"?\n\n**A)** The factorized CNN allows for the learning of multiple patterns simultaneously, improving the model's ability to generalize to new environments.\n\n**B)** The use of a large-margin loss function enables the model to distinguish between audio scenes from different environments, leading to improved performance on unseen data.\n\n**C)** The factorized CNN reduces the number of parameters required for the model, making it more computationally efficient.\n\n**D)** The proposed algorithm improves the performance of the baseline network by learning more complex patterns in the time-frequency domain.\n\n**Correct Answer:** B) The use of a large-margin loss function enables the model to distinguish between audio scenes from different environments, leading to improved performance on unseen data.\n\n**Explanation:** The correct answer is B) because the paper states that the loss function based on triplet sampling \"minimizes the same audio scene samples from different environments and maximizes the different audio scene samples\". This implies that the large-margin loss function enables the model to distinguish between audio scenes from different environments, leading to improved performance on unseen data. The other options are incorrect because they do not accurately summarize the primary advantage of the proposed algorithm."}, "13": {"documentation": {"title": "Using numerical plant models and phenotypic correlation space to design\n  achievable ideotypes", "source": "Victor Picheny and Pierre Casadebaig and Ronan Tr\\'epos and Robert\n  Faivre and David Da Silva and Patrick Vincourt and Evelyne Costes", "docs_id": "1603.03238", "section": ["q-bio.QM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using numerical plant models and phenotypic correlation space to design\n  achievable ideotypes. Numerical plant models can predict the outcome of plant traits modifications resulting from genetic variations, on plant performance, by simulating physiological processes and their interaction with the environment. Optimization methods complement those models to design ideotypes, i.e. ideal values of a set of plant traits resulting in optimal adaptation for given combinations of environment and management, mainly through the maximization of a performance criteria (e.g. yield, light interception). As use of simulation models gains momentum in plant breeding, numerical experiments must be carefully engineered to provide accurate and attainable results, rooting them in biological reality. Here, we propose a multi-objective optimization formulation that includes a metric of performance, returned by the numerical model, and a metric of feasibility, accounting for correlations between traits based on field observations. We applied this approach to two contrasting models: a process-based crop model of sunflower and a functional-structural plant model of apple trees. In both cases, the method successfully characterized key plant traits and identified a continuum of optimal solutions, ranging from the most feasible to the most efficient. The present study thus provides successful proof of concept for this enhanced modeling approach, which identified paths for desirable trait modification, including direction and intensity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A plant breeding program aims to design ideotypes for sunflower crops that maximize yield while minimizing water usage under drought conditions. The program uses a process-based crop model to simulate the effects of genetic variations on plant traits. However, the model's predictions are influenced by correlations between traits, which are not accounted for in the optimization process.\n\n**A)** The program should use a single-objective optimization formulation that maximizes yield, ignoring the correlations between traits.\n\n**B)** The program should use a multi-objective optimization formulation that includes both yield and water usage as objectives, but neglects the correlations between traits.\n\n**C)** The program should use a multi-objective optimization formulation that includes both yield and water usage as objectives, and incorporates a metric of feasibility that accounts for correlations between traits.\n\n**D)** The program should use a single-objective optimization formulation that minimizes water usage, assuming that yield is not a limiting factor.\n\n**Correct Answer:** C) The program should use a multi-objective optimization formulation that includes both yield and water usage as objectives, and incorporates a metric of feasibility that accounts for correlations between traits.\n\n**Explanation:** The correct answer, C, is the only option that takes into account the correlations between traits, which are a key aspect of the problem. The other options either ignore the correlations (A and D) or neglect them in favor of a single objective (B). The correct approach is to use a multi-objective optimization formulation that incorporates both yield and water usage as objectives, as well as a metric of feasibility that accounts for correlations between traits, as described in the original documentation."}, "14": {"documentation": {"title": "A maximum entropy approach to H-theory: Statistical mechanics of\n  hierarchical systems", "source": "Giovani L. Vasconcelos, Domingos S. P. Salazar, and A. M. S. Mac\\^edo", "docs_id": "1706.09963", "section": ["cond-mat.stat-mech", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A maximum entropy approach to H-theory: Statistical mechanics of\n  hierarchical systems. A novel formalism, called H-theory, is applied to the problem of statistical equilibrium of a hierarchical complex system with multiple time and length scales. In this approach, the system is formally treated as being composed of a small subsystem---representing the region where the measurements are made---in contact with a set of `nested heat reservoirs' corresponding to the hierarchical structure of the system. The probability distribution function (pdf) of the fluctuating temperatures at each reservoir, conditioned on the temperature of the reservoir above it, is determined from a maximum entropy principle subject to appropriate constraints that describe the thermal equilibrium properties of the system. The marginal temperature distribution of the innermost reservoir is obtained by integrating over the conditional distributions of all larger scales, and the resulting pdf is written in analytical form in terms of certain special transcendental functions, known as the Fox $H$-functions. The distribution of states of the small subsystem is then computed by averaging the quasi-equilibrium Boltzmann distribution over the temperature of the innermost reservoir. This distribution can also be written in terms of $H$-functions. The general family of distributions reported here recovers, as particular cases, the stationary distributions recently obtained by Mac\\^edo {\\it et al.} [Phys.~Rev.~E {\\bf 95}, 032315 (2017)] from a stochastic dynamical approach to the problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the maximum entropy approach to H-theory, what is the expression for the marginal temperature distribution of the innermost reservoir, and how is it related to the Fox $H$-functions?\n\nA) $\\frac{1}{Z} \\exp\\left(-\\frac{E}{kT}\\right) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{kT_H}\\right)$, where $Z$ is the partition function and $T_H$ is the temperature of the innermost reservoir.\n\nB) $\\frac{1}{Z} \\exp\\left(-\\frac{E}{kT_H}\\right) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{kT}\\right) \\int_0^\\infty \\frac{1}{x} \\exp\\left(-\\frac{x}{x_H}\\right) dx$, where $Z$ is the partition function, $T_H$ is the temperature of the innermost reservoir, and $x_H$ is a parameter related to the Fox $H$-functions.\n\nC) $\\frac{1}{Z} \\exp\\left(-\\frac{E}{kT_H}\\right) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{kT}\\right) \\int_0^\\infty \\frac{1}{x} \\exp\\left(-\\frac{x}{x_H}\\right) dx$, where $Z$ is the partition function, $T_H$ is the temperature of the innermost reservoir, and $x_H$ is a parameter related to the Fox $H$-functions.\n\nD) $\\frac{1}{Z} \\exp\\left(-\\frac{E}{kT_H}\\right) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{kT}\\right) \\int_0^\\infty \\frac{1}{x} \\exp\\left(-\\frac{x}{x_H}\\right) dx$, where $Z$ is the partition function, $T_H$ is the temperature of the innermost reservoir, and $x_H$ is a parameter related to the Fox $H$-functions.\n\n**Correct Answer:** B) $\\frac{1}{Z} \\exp\\left(-\\frac{E}{kT_H}\\right) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{kT}\\right) \\int_0^\\infty \\frac{1}{x} \\exp\\left(-\\frac{x}{x_H}\\right) dx$, where $Z$ is the partition function, $T_H$ is the temperature of the innermost reservoir, and $x_H$ is a parameter related to the Fox $H$-functions.\n\n**Explanation:** The marginal temperature distribution of the innermost reservoir is obtained by integrating over the conditional distributions of all larger scales. This is done using the Fox $H$-functions, which are special transcendental functions that arise in the solution of the maximum entropy problem. The correct expression involves integrating over the conditional distributions, which is represented by the integral term, and the resulting expression is written in terms of the Fox $H$-functions."}, "15": {"documentation": {"title": "Scanning tunneling spectroscopy reveals a silicon dangling bond charge\n  state transition", "source": "Hatem Labidi, Marco Taucer, Mohammad Rashidi, Mohammad Koleini, Lucian\n  Livadaru, Jason Pitters, Martin Cloutier, Mark Salomons, Robert A. Wolkow", "docs_id": "1503.00646", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scanning tunneling spectroscopy reveals a silicon dangling bond charge\n  state transition. We report the study of single dangling bonds (DB) on the hydrogen terminated silicon (100) surface using a low temperature scanning tunneling microscope (LT-STM). By investigating samples prepared with different annealing temperatures, we establish the critical role of subsurface arsenic dopants on the DB electronic properties. We show that when the near surface concentration of dopants is depleted as a result of $1250{\\deg}C$ flash anneals, a single DB exhibits a sharp conduction step in its I(V) spectroscopy that is not due to a density of states effect but rather corresponds to a DB charge state transition. The voltage position of this transition is perfectly correlated with bias dependent changes in STM images of the DB at different charge states. Density functional theory (DFT) calculations further highlight the role of subsurface dopants on DB properties by showing the influence of the DB-dopant distance on the DB state. We discuss possible theoretical models of electronic transport through the DB that could account for our experimental observations."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism underlying the observed conduction step in the I(V) spectroscopy of a single dangling bond on a hydrogen-terminated silicon (100) surface, as reported in the study?\n\nA) Density of states effect due to the presence of surface defects\nB) Charge state transition of the dangling bond due to subsurface arsenic dopants\nC) Tunneling current oscillations caused by the scanning tunneling microscope's tip\nD) Electronic transport through the dangling bond via a quantum tunneling effect\n\nCorrect Answer: B) Charge state transition of the dangling bond due to subsurface arsenic dopants\n\nExplanation: The study reports that the conduction step in the I(V) spectroscopy is not due to a density of states effect, but rather corresponds to a sharp transition in the charge state of the single dangling bond. This transition is perfectly correlated with bias-dependent changes in STM images of the DB at different charge states. The density functional theory (DFT) calculations also highlight the role of subsurface dopants on DB properties, showing the influence of the DB-dopant distance on the DB state. Therefore, the primary mechanism underlying the observed conduction step is the charge state transition of the dangling bond due to subsurface arsenic dopants."}, "16": {"documentation": {"title": "Chaotic cyclotron and Hall trajectories due to spin-orbit coupling", "source": "E.V. Kirichenko, V. A. Stephanovich, and E. Ya. Sherman", "docs_id": "2005.04468", "section": ["nlin.CD", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chaotic cyclotron and Hall trajectories due to spin-orbit coupling. We demonstrate that the synergistic effect of a gauge field, Rashba spin-orbit coupling (SOC), and Zeeman splitting can generate chaotic cyclotron and Hall trajectories of particles. The physical origin of the chaotic behavior is that the SOC produces a spin-dependent (so-called anomalous) contribution to the particle velocity and the presence of Zeeman field reduces the number of integrals of motion. By using analytical and numerical arguments, we study the conditions of chaos emergence and report the dynamics both in the regular and chaotic regimes. {We observe the critical dependence of the dynamic patterns (such as the chaotic regime onset) on small variations in the initial conditions and problem parameters, that is the SOC and/or Zeeman constants. The transition to chaotic regime is further verified by the analysis of phase portraits as well as Lyapunov exponents spectrum.} The considered chaotic behavior can occur in solid state systems, weakly-relativistic plasmas, and cold atomic gases with synthetic gauge fields and spin-related couplings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the physical origin of the chaotic behavior observed in the cyclotron and Hall trajectories of particles in the presence of Rashba spin-orbit coupling (SOC) and Zeeman splitting?\n\nA) The SOC produces a spin-independent contribution to the particle velocity, while the Zeeman field reduces the number of integrals of motion.\nB) The SOC produces a spin-dependent (anomalous) contribution to the particle velocity, and the Zeeman field reduces the number of integrals of motion.\nC) The SOC produces a spin-independent contribution to the particle velocity, and the Zeeman field produces a spin-dependent contribution to the particle velocity.\nD) The SOC and Zeeman field have no effect on the particle velocity, and the chaotic behavior is solely due to the initial conditions.\n\n**Correct Answer:** B) The SOC produces a spin-dependent (anomalous) contribution to the particle velocity, and the Zeeman field reduces the number of integrals of motion.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"The physical origin of the chaotic behavior is that the SOC produces a spin-dependent (so-called anomalous) contribution to the particle velocity and the presence of Zeeman field reduces the number of integrals of motion.\" This indicates that the SOC is responsible for the spin-dependent contribution to the particle velocity, and the Zeeman field is responsible for reducing the number of integrals of motion, leading to chaotic behavior."}, "17": {"documentation": {"title": "Integrating electricity markets: Impacts of increasing trade on prices\n  and emissions in the western United States", "source": "Steven Dahlke", "docs_id": "1810.04759", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrating electricity markets: Impacts of increasing trade on prices\n  and emissions in the western United States. This paper presents empirically-estimated average hourly relationships between regional electricity trade in the United States and prices, emissions, and generation from 2015 through 2018. Consistent with economic theory, the analysis finds a negative relationship between electricity prices in California and regional trade, conditional on local demand. Each 1 gigawatt-hour increase in California electricity imports is associated with an average $0.15 per megawatt-hour decrease in the California Independent System Operator's wholesale electricity price. There is a net-negative short term relationship between carbon dioxide emissions in California and electricity imports that is partially offset by positive emissions from exporting neighbors. Specifically, each 1 GWh increase in regional trade is associated with a net 70-ton average decrease in CO2 emissions across the western U.S., conditional on demand levels. The results provide evidence that electricity imports mostly displace natural gas generation on the margin in the California electricity market. A small positive relationship is observed between short-run SO2 and NOx emissions in neighboring regions and California electricity imports. The magnitude of the SO2 and NOx results suggest an average increase of 0.1 MWh from neighboring coal plants is associated with a 1 MWh increase in imports to California."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the relationship between regional electricity trade in the United States and its impacts on electricity prices, emissions, and generation in the western United States, specifically in California, from 2015 to 2018.\n\n**A)** The analysis found a positive relationship between electricity prices in California and regional trade, indicating that increased imports lead to higher prices. Additionally, the study discovered a significant positive correlation between short-run SO2 and NOx emissions in neighboring regions and California electricity imports.\n\n**B)** The study revealed a negative relationship between electricity prices in California and regional trade, conditional on local demand, with each 1 gigawatt-hour increase in imports associated with a $0.15 per megawatt-hour decrease in the California Independent System Operator's wholesale electricity price. However, the analysis also found a small positive relationship between short-run SO2 and NOx emissions in neighboring regions and California electricity imports.\n\n**C)** The results of the analysis indicate that electricity imports mostly displace natural gas generation on the margin in the California electricity market, with a net-negative short-term relationship between carbon dioxide emissions in California and electricity imports. The study also found a positive relationship between short-run SO2 and NOx emissions in neighboring regions and California electricity imports.\n\n**D)** The analysis found a positive relationship between regional electricity trade in the United States and its impacts on electricity prices, emissions, and generation in the western United States, specifically in California, from 2015 to 2018. The study revealed that each 1 gigawatt-hour increase in imports was associated with a 70-ton average decrease in CO2 emissions across the western U.S.\n\n**Correct Answer:** C) The results of the analysis indicate that electricity imports mostly displace natural gas generation on the margin in the California electricity market, with a net-negative short-term relationship between carbon dioxide emissions in California and electricity imports. The study also found a positive relationship between short-run SO2 and NOx emissions in neighboring regions and California electricity imports.\n\n**Explanation:**\n\nThe correct answer, C, accurately summarizes the main findings of the study. The analysis indeed found that electricity imports mostly displace natural gas generation on the margin in the California electricity market. Additionally, the study revealed a net-negative short-term relationship between carbon dioxide emissions in California and electricity imports, indicating that increased imports lead to a decrease in CO2 emissions. The correct answer also correctly states the positive relationship between short-run SO2 and NOx emissions in neighboring regions and California electricity imports.\n\nThe incorrect answers, A and B, contain errors in the analysis. Answer A incorrectly states a positive relationship between electricity prices in California and regional trade, whereas the study actually found a negative relationship. Answer B incorrectly states a small positive relationship between short-run SO2 and NOx emissions in neighboring regions and California electricity imports, whereas the study actually found a positive relationship. Answer D incorrectly states that each 1 gigawatt-hour increase in imports was associated with a 70-ton average decrease in CO2 emissions across the western U.S., whereas the study actually found a net-negative short-term relationship between carbon dioxide emissions in California and electricity imports."}, "18": {"documentation": {"title": "Pair Formation within Multi-Agent Populations", "source": "David M.D. Smith and Neil F. Johnson", "docs_id": "physics/0604142", "section": ["physics.soc-ph", "cond-mat.stat-mech", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pair Formation within Multi-Agent Populations. We present a simple model for the formation of pairs in multi-agent populations of type A and B which move freely on a spatial network. Each agent of population A (and B) is labeled as Ai (and Bj) with i=1,.. NA (and j=1,..NB) and carries its own individual list of characteristics or 'phenotype'. When agents from opposite populations encounter one another on the network, they can form a relationship if not already engaged in one. The length of time for which any given pair stays together depends on the compatibility of the two constituent agents. Possible applications include the human dating scenario, and the commercial domain where two types of businesses A and B have members of each type looking for a business partner, i.e. Ai+Bj-->Rij. The pair Rij then survives for some finite time before dissociating Rij-->Ai+Bj. There are many possible generalizations of this basic setup. Here we content ourselves with some initial numerical results for the simplest of network topologies, together with some accompanying analytic analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Pair Formation within Multi-Agent Populations model, what is the primary factor that determines the length of time a pair stays together, and how does this factor relate to the compatibility of the two constituent agents?\n\n**A)** The length of time a pair stays together is determined by the strength of the spatial network, and compatibility is a secondary factor that influences the formation of new pairs.\n\n**B)** The length of time a pair stays together is determined by the compatibility of the two constituent agents, and the strength of the spatial network is a secondary factor that influences the formation of new pairs.\n\n**C)** The length of time a pair stays together is determined by the individual characteristics or 'phenotype' of the agents, and compatibility is a factor that influences the formation of new pairs.\n\n**D)** The length of time a pair stays together is determined by the type of spatial network, and compatibility is a factor that influences the formation of new pairs.\n\n**Correct Answer:** B) The length of time a pair stays together is determined by the compatibility of the two constituent agents, and the strength of the spatial network is a secondary factor that influences the formation of new pairs.\n\n**Explanation:** According to the documentation, the length of time a pair stays together depends on the compatibility of the two constituent agents. The strength of the spatial network is mentioned as a factor that influences the formation of new pairs, but it is not the primary factor that determines the length of time a pair stays together. Therefore, option B is the correct answer."}, "19": {"documentation": {"title": "Statistical and Economic Evaluation of Time Series Models for\n  Forecasting Arrivals at Call Centers", "source": "Andrea Bastianin, Marzio Galeotti, Matteo Manera", "docs_id": "1804.08315", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical and Economic Evaluation of Time Series Models for\n  Forecasting Arrivals at Call Centers. Call centers' managers are interested in obtaining accurate point and distributional forecasts of call arrivals in order to achieve an optimal balance between service quality and operating costs. We present a strategy for selecting forecast models of call arrivals which is based on three pillars: (i) flexibility of the loss function; (ii) statistical evaluation of forecast accuracy; (iii) economic evaluation of forecast performance using money metrics. We implement fourteen time series models and seven forecast combination schemes on three series of daily call arrivals. Although we focus mainly on point forecasts, we also analyze density forecast evaluation. We show that second moments modeling is important both for point and density forecasting and that the simple Seasonal Random Walk model is always outperformed by more general specifications. Our results suggest that call center managers should invest in the use of forecast models which describe both first and second moments of call arrivals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of forecasting daily call arrivals at call centers, what is the primary advantage of using a forecast model that describes both first and second moments of call arrivals, as suggested by the study?\n\n**A)** It allows for more accurate point forecasts, which are essential for service quality.\n**B)** It enables the incorporation of economic metrics, such as operating costs, into the evaluation of forecast performance.\n**C)** It provides a more robust and flexible framework for modeling the variability of call arrivals, which is critical for achieving an optimal balance between service quality and operating costs.\n**D)** It facilitates the use of more complex statistical models, which can capture the nuances of call arrival patterns.\n\n**Correct Answer:** C) It provides a more robust and flexible framework for modeling the variability of call arrivals, which is critical for achieving an optimal balance between service quality and operating costs.\n\n**Explanation:** The study highlights the importance of second moments modeling in both point and density forecasting. By describing both first and second moments of call arrivals, forecast models can capture the variability and uncertainty of call arrival patterns, which is essential for achieving an optimal balance between service quality and operating costs. This is in line with the three pillars of the strategy presented in the study, which emphasizes the importance of flexibility, statistical evaluation, and economic evaluation in selecting forecast models."}, "20": {"documentation": {"title": "Fostering Project Scheduling and Controlling Risk Management", "source": "Abdul Razaque, Christian Bach, Nyembo salama, Aziz Alotaibi", "docs_id": "1210.2021", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fostering Project Scheduling and Controlling Risk Management. Deployment of emerging technologies and rapid change in industries has created a lot of risk for initiating the new projects. Many techniques and suggestions have been introduced but still lack the gap from various prospective. This paper proposes a reliable project scheduling approach. The objectives of project scheduling approach are to focus on critical chain schedule and risk management. Several risks and reservations exist in projects. These critical reservations may not only foil the projects to be finished within time limit and budget, but also degrades the quality, and operational process. In the proposed approach, the potential risks of project are critically analyzed. To overcome these potential risks, fuzzy failure mode and effect analysis (FMEA) is introduced. In addition, several affects of each risk against each activity are evaluated. We use Monte Carlo simulation that helps to calculate the total time of project. Our approach helps to control risk mitigation that is determined using event tree analysis and fault tree analysis. We also implement distribute critical chain schedule for reliable scheduling that makes the project to be implemented within defined plan and schedule. Finally, adaptive procedure with density (APD) is deployed to get reasonable feeding buffer time and project buffer time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A project manager is tasked with deploying a new technology in a rapidly changing industry. The project has several critical activities, but there are concerns about potential risks that could impact the project's timeline, budget, and quality. The project manager has been introduced to a new approach that incorporates fuzzy failure mode and effect analysis (FMEA), Monte Carlo simulation, event tree analysis, fault tree analysis, and distributed critical chain scheduling. Which of the following is the primary objective of this project scheduling approach?\n\nA) To minimize project costs and maximize efficiency\nB) To focus on critical chain schedule and risk management\nC) To optimize project resource allocation and minimize waste\nD) To implement a traditional project management methodology\n\n**Correct Answer:** B) To focus on critical chain schedule and risk management\n\n**Explanation:** The correct answer is B) To focus on critical chain schedule and risk management. The project scheduling approach proposed in the paper aims to address the critical reservations and risks associated with project initiation, and to mitigate these risks through the use of FMEA, Monte Carlo simulation, event tree analysis, fault tree analysis, and distributed critical chain scheduling. The approach is designed to ensure that the project is completed within the defined plan and schedule, and that the quality and operational process are maintained.\n\n**Candidate A (Incorrect):** While minimizing project costs and maximizing efficiency are important considerations in project management, they are not the primary objective of this project scheduling approach.\n\n**Candidate B (Incorrect):** Optimizing project resource allocation and minimizing waste are important aspects of project management, but they are not the primary focus of this approach.\n\n**Candidate C (Incorrect):** Implementing a traditional project management methodology is not the primary objective of this approach, which incorporates new and innovative techniques to address the challenges of project initiation in rapidly changing industries."}, "21": {"documentation": {"title": "Reinforcement and inference in cross-situational word learning", "source": "Paulo F. C. Tilles and Jose F. Fontanari", "docs_id": "1307.3941", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reinforcement and inference in cross-situational word learning. Cross-situational word learning is based on the notion that a learner can determine the referent of a word by finding something in common across many observed uses of that word. Here we propose an adaptive learning algorithm that contains a parameter that controls the strength of the reinforcement applied to associations between concurrent words and referents, and a parameter that regulates inference, which includes built-in biases, such as mutual exclusivity, and information of past learning events. By adjusting these parameters so that the model predictions agree with data from representative experiments on cross-situational word learning, we were able to explain the learning strategies adopted by the participants of those experiments in terms of a trade-off between reinforcement and inference. These strategies can vary wildly depending on the conditions of the experiments. For instance, for fast mapping experiments (i.e., the correct referent could, in principle, be inferred in a single observation) inference is prevalent, whereas for segregated contextual diversity experiments (i.e., the referents are separated in groups and are exhibited with members of their groups only) reinforcement is predominant. Other experiments are explained with more balanced doses of reinforcement and inference."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of cross-situational word learning, what is the primary trade-off between reinforcement and inference that participants in experiments on this topic adopt, and how does this trade-off vary depending on the conditions of the experiments?\n\n**A)** The primary trade-off is between the strength of reinforcement and the amount of prior knowledge, with participants relying more on prior knowledge in experiments with segregated contextual diversity.\n\n**B)** The primary trade-off is between the strength of reinforcement and the presence of built-in biases, such as mutual exclusivity, with participants relying more on reinforcement in experiments with fast mapping.\n\n**C)** The primary trade-off is between the amount of reinforcement and the complexity of the referents, with participants relying more on reinforcement in experiments with simple referents and more on inference in experiments with complex referents.\n\n**D)** The primary trade-off is between the strength of reinforcement and the number of observed uses of a word, with participants relying more on reinforcement in experiments with few observed uses and more on inference in experiments with many observed uses.\n\n**Correct Answer:** B) The primary trade-off is between the strength of reinforcement and the presence of built-in biases, such as mutual exclusivity, with participants relying more on reinforcement in experiments with fast mapping.\n\n**Explanation:** According to the text, the correct answer is B) because the text states that \"For fast mapping experiments (i.e., the correct referent could, in principle, be inferred in a single observation) inference is prevalent, whereas for segregated contextual diversity experiments (i.e., the referents are separated in groups and are exhibited with members of their groups only) reinforcement is predominant.\" This suggests that in fast mapping experiments, inference is more important, and in segregated contextual diversity experiments, reinforcement is more important."}, "22": {"documentation": {"title": "Sparse Recovery from Extreme Eigenvalues Deviation Inequalities", "source": "Sandrine Dallaporta and Yohann De Castro", "docs_id": "1604.01171", "section": ["math.ST", "cs.IT", "math.IT", "math.PR", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparse Recovery from Extreme Eigenvalues Deviation Inequalities. This article provides a new toolbox to derive sparse recovery guarantees from small deviations on extreme singular values or extreme eigenvalues obtained in Random Matrix Theory. This work is based on Restricted Isometry Constants (RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional Statistics as these constants finely assess how a linear operator is conditioned on the set of sparse vectors and hence how it performs in SRSR. While it is an open problem to construct deterministic matrices with apposite RICs, one can prove that such matrices exist using random matrices models. In this paper, we show upper bounds on RICs for Gaussian and Rademacher matrices using state-of-the-art small deviation estimates on their extreme eigenvalues. This allows us to derive a lower bound on the probability of getting SRSR. One benefit of this paper is a direct and explicit derivation of upper bounds on RICs and lower bounds on SRSR from small deviations on the extreme eigenvalues given by Random Matrix theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Sparse Recovery from Extreme Eigenvalues Deviation Inequalities, what is the significance of Restricted Isometry Constants (RICs) in deriving sparse recovery guarantees?\n\nA) RICs are a measure of the linear operator's ability to distinguish between sparse and non-sparse vectors.\nB) RICs are a measure of the linear operator's ability to recover sparse signals from noisy data.\nC) RICs are a measure of the linear operator's conditioning on the set of sparse vectors, which is crucial for sparse recovery guarantees.\nD) RICs are a measure of the linear operator's ability to compress sparse signals.\n\n**Correct Answer:** C) RICs are a measure of the linear operator's conditioning on the set of sparse vectors, which is crucial for sparse recovery guarantees.\n\n**Explanation:** The correct answer is C) RICs are a measure of the linear operator's conditioning on the set of sparse vectors, which is crucial for sparse recovery guarantees. This is because RICs assess how a linear operator performs on sparse vectors, and this information is essential for deriving sparse recovery guarantees. The other options are incorrect because they either partially describe the role of RICs or introduce unrelated concepts."}, "23": {"documentation": {"title": "A VCSEL Array Transmission System with Novel Beam Activation Mechanisms", "source": "Zhihong Zeng, Mohammad Dehghani Soltani, Majid Safari and Harald Haas", "docs_id": "2108.06086", "section": ["cs.IT", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A VCSEL Array Transmission System with Novel Beam Activation Mechanisms. Optical wireless communication (OWC) is considered to be a promising technology which will alleviate traffic burden caused by the increasing number of mobile devices. In this study, a novel vertical-cavity surface-emitting laser (VCSEL) array is proposed for indoor OWC systems. To activate the best beam for a mobile user, two beam activation methods are proposed for the system. The method based on a corner-cube retroreflector (CCR) provides very low latency and allows real-time activation for high-speed users. The other method uses the omnidirectional transmitter (ODTx). The ODTx can serve the purpose of uplink transmission and beam activation simultaneously. Moreover, systems with ODTx are very robust to the random orientation of a user equipment (UE). System level analyses are carried out for the proposed VCSEL array system. For a single user scenario, the probability density function (PDF) of the signal-to-noise ratio (SNR) for the central beam of the VCSEL array system can be approximated as a uniform distribution. In addition, the average data rate of the central beam and its upper bound are given analytically and verified by Monte-Carlo simulations. For a multi-user scenario, an analytical upper bound for the average data rate is given. The effects of the cell size and the full width at half maximum (FWHM) angle on the system performance are studied. The results show that the system with a FWHM angle of $4^\\circ$ outperforms the others."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed VCSEL array transmission system, what is the primary advantage of using the omnidirectional transmitter (ODTx) for beam activation, and how does it impact the system's robustness to random user equipment (UE) orientation?\n\n**A)** ODTx provides higher latency and is less suitable for high-speed users.\n**B)** ODTx allows for real-time activation and is robust to random UE orientation, making it ideal for indoor OWC systems.\n**C)** ODTx is less effective in activating the best beam for a mobile user, leading to lower system performance.\n**D)** ODTx is only suitable for uplink transmission and does not support beam activation.\n\n**Correct Answer:** B) ODTx allows for real-time activation and is robust to random UE orientation, making it ideal for indoor OWC systems.\n\n**Explanation:** The correct answer is B) ODTx allows for real-time activation and is robust to random UE orientation, making it ideal for indoor OWC systems. This is because the ODTx can serve the purpose of uplink transmission and beam activation simultaneously, and systems with ODTx are very robust to the random orientation of a user equipment (UE). This makes it a suitable choice for indoor OWC systems, where user equipment can be randomly oriented."}, "24": {"documentation": {"title": "Robust Hedging of Options on a Leveraged Exchange Traded Fund", "source": "Alexander M. G. Cox and Sam M. Kinsley", "docs_id": "1702.07169", "section": ["q-fin.PR", "math.PR", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Hedging of Options on a Leveraged Exchange Traded Fund. A leveraged exchange traded fund (LETF) is an exchange traded fund that uses financial derivatives to amplify the price changes of a basket of goods. In this paper, we consider the robust hedging of European options on a LETF, finding model-free bounds on the price of these options. To obtain an upper bound, we establish a new optimal solution to the Skorokhod embedding problem (SEP) using methods introduced in Beiglb\\\"ock-Cox-Huesmann. This stopping time can be represented as the hitting time of some region by a Brownian motion, but unlike other solutions of e.g. Root, this region is not unique. Much of this paper is dedicated to characterising the choice of the embedding region that gives the required optimality property. Notably, this appears to be the first solution to the SEP where the solution is not uniquely characterised by its geometric structure, and an additional condition is needed on the stopping region to guarantee that it is the optimiser. An important part of determining the optimal region is identifying the correct form of the dual solution, which has a financial interpretation as a model-independent superhedging strategy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Robust Hedging of Options on a Leveraged Exchange Traded Fund\", what is the key characteristic of the stopping region in the Skorokhod embedding problem (SEP) that guarantees optimality, and what is its financial interpretation?\n\n**A)** The stopping region is uniquely characterised by its geometric structure, and it represents the maximum possible return of the underlying asset.\n\n**B)** The stopping region is not uniquely characterised by its geometric structure, and it represents a model-independent superhedging strategy that guarantees the optimal hedging of options.\n\n**C)** The stopping region is uniquely characterised by its geometric structure, and it represents the minimum possible risk of the underlying asset.\n\n**D)** The stopping region is uniquely characterised by its geometric structure, and it represents the expected return of the underlying asset.\n\n**Correct Answer:** B) The stopping region is not uniquely characterised by its geometric structure, and it represents a model-independent superhedging strategy that guarantees the optimal hedging of options.\n\n**Explanation:** The correct answer is B) because the paper states that the stopping region is not uniquely characterised by its geometric structure, and that an additional condition is needed on the stopping region to guarantee that it is the optimiser. Additionally, the dual solution has a financial interpretation as a model-independent superhedging strategy, which is a key concept in the paper."}, "25": {"documentation": {"title": "Empirical regularities of opening call auction in Chinese stock market", "source": "Gao-Feng Gu, Fei Ren, Xiao-Hui Ni, Wei Chen, Wei-Xing Zhou", "docs_id": "0905.0582", "section": ["q-fin.ST", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Empirical regularities of opening call auction in Chinese stock market. We study the statistical regularities of opening call auction using the ultra-high-frequency data of 22 liquid stocks traded on the Shenzhen Stock Exchange in 2003. The distribution of the relative price, defined as the relative difference between the order price in opening call auction and the closing price of last trading day, is asymmetric and that the distribution displays a sharp peak at zero relative price and a relatively wide peak at negative relative price. The detrended fluctuation analysis (DFA) method is adopted to investigate the long-term memory of relative order prices. We further study the statistical regularities of order sizes in opening call auction, and observe a phenomenon of number preference, known as order size clustering. The probability density function (PDF) of order sizes could be well fitted by a $q$-Gamma function, and the long-term memory also exists in order sizes. In addition, both the average volume and the average number of orders decrease exponentially with the price level away from the best bid or ask price level in the limit-order book (LOB) established immediately after the opening call auction, and a price clustering phenomenon is observed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What can be inferred about the distribution of relative prices in opening call auctions on the Shenzhen Stock Exchange from the empirical regularities observed in the study?\n\nA) The distribution is symmetric around zero relative price.\nB) The distribution is skewed to the right, with a long tail of high relative prices.\nC) The distribution is asymmetric, with a sharp peak at zero relative price and a relatively wide peak at negative relative price.\nD) The distribution is bimodal, with two distinct peaks at positive and negative relative prices.\n\n**Correct Answer:** C) The distribution is asymmetric, with a sharp peak at zero relative price and a relatively wide peak at negative relative price.\n\n**Explanation:** The study observes that the distribution of relative prices is asymmetric, with a sharp peak at zero relative price (i.e., when the order price is equal to the closing price of the last trading day) and a relatively wide peak at negative relative price (i.e., when the order price is lower than the closing price). This suggests that there is a strong tendency for orders to be placed at or near the closing price, and that prices tend to move in the opposite direction of the order price."}, "26": {"documentation": {"title": "Estimating Trade-Related Adjustment Costs in the Agricultural Sector in\n  Iran", "source": "Omid Karami and Mina Mahmoudi", "docs_id": "1806.04238", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating Trade-Related Adjustment Costs in the Agricultural Sector in\n  Iran. Tariff liberalization and its impact on tax revenue is an important consideration for developing countries, because they are increasingly facing the difficult task of implementing and harmonizing regional and international trade commitments. The tariff reform and its costs for Iranian government is one of the issues that are examined in this study. Another goal of this paper is, estimating the cost of trade liberalization. On this regard, imports value of agricultural sector in Iran in 2010 was analyzed according to two scenarios. For reforming nuisance tariff, a VAT policy is used in both scenarios. In this study, TRIST method is used. In the first scenario, imports' value decreased to a level equal to the second scenario and higher tariff revenue will be created. The results show that reducing the average tariff rate does not always result in the loss of tariff revenue. This paper is a witness that different forms of tariff can generate different amount of income when they have same level of liberalization and equal effect on producers. Therefore, using a good tariff regime can help a government to generate income when increases social welfare by liberalization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is a key finding of the study on Estimating Trade-Related Adjustment Costs in the Agricultural Sector in Iran, and how does it relate to the use of tariff regimes in generating revenue for governments?\n\nA) The study found that reducing the average tariff rate always results in a loss of tariff revenue, and therefore, governments should maintain high tariffs to maximize revenue.\nB) The study found that reducing the average tariff rate can result in higher tariff revenue, and therefore, governments should consider using a good tariff regime to generate income while increasing social welfare.\nC) The study found that the type of tariff regime used has no impact on the revenue generated, and therefore, governments should not consider tariff reform when implementing trade liberalization.\nD) The study found that the imports value of the agricultural sector in Iran in 2010 was unaffected by tariff reform, and therefore, governments should not consider tariff reform when implementing trade liberalization.\n\nCorrect Answer: B) The study found that reducing the average tariff rate can result in higher tariff revenue, and therefore, governments should consider using a good tariff regime to generate income while increasing social welfare."}, "27": {"documentation": {"title": "Searching for minicharged particles via birefringence, dichroism and\n  Raman spectroscopy of the vacuum polarized by a high-intensity laser wave", "source": "S. Villalba-Ch\\'avez and C. M\\\"uller", "docs_id": "1306.6456", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Searching for minicharged particles via birefringence, dichroism and\n  Raman spectroscopy of the vacuum polarized by a high-intensity laser wave. Absorption and dispersion of probe photons in the field of a high-intensity circularly polarized laser wave are investigated. The optical theorem is applied for determining the absorption coefficients in terms of the imaginary part of the vacuum polarization tensor. Compact expressions for the vacuum refraction indices and the photon absorption coefficients are obtained in various asymptotic regimes of interest. The outcomes of this analysis reveal that, far from the region relatively close to the threshold of the two-photon reaction, the birefringence and dichroism of the vacuum are small and, in some cases, strongly suppressed. On the contrary, in a vicinity of the region in which the photo-production of a pair occurs, these optical properties are manifest with lasers of moderate intensities. We take advantage of such a property in the search of minicharged particles by considering high-precision polarimetric experiments. In addition, Raman-like electromagnetic waves resulting from the inelastic part of the vacuum polarization tensor are suggested as an alternative form for finding exclusion limits on these hypothetical charge carriers. The envisaged parameters of upcoming high-intensity laser facilities are used for establishing upper bounds on the minicharged particles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which minicharged particles can be searched for in high-intensity laser experiments, and what optical properties are manifest in the vicinity of the region where photo-production of a pair occurs?\n\n**A)** The search for minicharged particles relies on the detection of Raman-like electromagnetic waves resulting from the inelastic part of the vacuum polarization tensor, and the optical properties manifest as strong birefringence and dichroism.\n\n**B)** The search for minicharged particles involves the use of high-precision polarimetric experiments to measure the absorption and dispersion of probe photons in the field of a high-intensity circularly polarized laser wave, and the optical properties manifest as small birefringence and dichroism.\n\n**C)** The search for minicharged particles can be achieved by exploiting the photo-production of a pair in the vicinity of the region where the optical properties of the vacuum are manifest, and the optical properties manifest as strong Raman-like electromagnetic waves.\n\n**D)** The search for minicharged particles relies on the application of the optical theorem to determine the absorption coefficients in terms of the imaginary part of the vacuum polarization tensor, and the optical properties manifest as strong birefringence and dichroism.\n\n**Correct Answer:** B) The search for minicharged particles involves the use of high-precision polarimetric experiments to measure the absorption and dispersion of probe photons in the field of a high-intensity circularly polarized laser wave, and the optical properties manifest as small birefringence and dichroism.\n\n**Explanation:** The correct answer is B) because the documentation states that high-precision polarimetric experiments are used to search for minicharged particles, and that the optical properties of the vacuum are manifest as small birefringence and dichroism in the vicinity of the region where photo-production of a pair occurs. The other options are incorrect because they either misrepresent the primary mechanism of the search or the manifestation of the optical properties."}, "28": {"documentation": {"title": "Efficient volatility estimation in a two-factor model", "source": "Olivier F\\'eron and Pierre Gruet and Marc Hoffmann", "docs_id": "1811.10241", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient volatility estimation in a two-factor model. We statistically analyse a multivariate HJM diffusion model with stochastic volatility. The volatility process of the first factor is left totally unspecified while the volatility of the second factor is the product of an unknown process and an exponential function of time to maturity. This exponential term includes some real parameter measuring the rate of increase of the second factor as time goes to maturity. From historical data, we efficiently estimate the time to maturity parameter in the sense of constructing an estimator that achieves an optimal information bound in a semiparametric setting. We also identify nonparametrically the paths of the volatility processes and achieve minimax bounds. We address the problem of degeneracy that occurs when the dimension of the process is greater than two, and give in particular optimal limit theorems under suitable regularity assumptions on the drift process. We consistently analyse the numerical behaviour of our estimators on simulated and real datasets of prices of forward contracts on electricity markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the multivariate HJM diffusion model with stochastic volatility described in the paper \"Efficient volatility estimation in a two-factor model\", what is the primary goal of the authors in estimating the time to maturity parameter in the volatility process of the second factor?\n\nA) To minimize the risk of model misspecification\nB) To construct an estimator that achieves an optimal information bound in a semiparametric setting\nC) To identify the underlying drivers of market volatility\nD) To estimate the drift process of the underlying asset\n\n**Correct Answer:** B) To construct an estimator that achieves an optimal information bound in a semiparametric setting\n\n**Explanation:** The authors aim to construct an estimator that achieves an optimal information bound in a semiparametric setting, which means that they want to estimate the time to maturity parameter in a way that minimizes the risk of overfitting or underfitting. This requires a careful analysis of the statistical properties of the data and the model, as well as the development of efficient estimation methods.\n\n**Candidate A:** This answer is incorrect because while minimizing the risk of model misspecification is an important consideration in statistical modeling, it is not the primary goal of the authors in this context.\n\n**Candidate B:** This is the correct answer. The authors explicitly state that they aim to construct an estimator that achieves an optimal information bound in a semiparametric setting.\n\n**Candidate C:** This answer is incorrect because while identifying the underlying drivers of market volatility is an important research question in finance, it is not the primary goal of the authors in this specific paper.\n\n**Candidate D:** This answer is incorrect because estimating the drift process of the underlying asset is not the primary goal of the authors in this context. The focus is on estimating the time to maturity parameter in the volatility process of the second factor."}, "29": {"documentation": {"title": "Perspectives on Reproducibility and Sustainability of Open-Source\n  Scientific Software from Seven Years of the Dedalus Project", "source": "Jeffrey S. Oishi and Benjamin P. Brown and Keaton J. Burns and Daniel\n  Lecoanet and Geoffrey M. Vasil", "docs_id": "1801.08200", "section": ["astro-ph.IM", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Perspectives on Reproducibility and Sustainability of Open-Source\n  Scientific Software from Seven Years of the Dedalus Project. As the Science Mission Directorate contemplates establishing an open code policy, we consider it timely to share our experiences as the developers of the open-source partial differential equation solver Dedalus. Dedalus is a flexible framework for solving partial differential equations. Its development team primarily uses it for studying stellar and planetary astrophysics. Dedalus was developed originally for astrophysical fluid dynamics (AFD), though it has found a much broader user base, including applied mathematicians, plasma physicists, and oceanographers. Here, we will focus on issues related to open-source software from the perspective of AFD. We use the term AFD with the understanding that astrophysics simulations are inherently multi-physics: fluid dynamics coupled with some combination of gravitational dynamics, radiation transfer, relativity, and magnetic fields. In practice, a few well-known open-source simulation packages represent a large fraction of published work in the field. However, we will argue that an open-code policy should encompass not just these large simulation codes, but also the input files and analysis scripts. It is our interest that NASA adopt an open-code policy because without it, reproducibility in computational science is needlessly hampered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the Dedalus project emphasize the importance of open-source software in the field of astrophysical fluid dynamics (AFD). However, they also argue that an open-code policy should encompass not just the simulation codes, but also the input files and analysis scripts. What are the primary reasons why the authors believe that an open-code policy is necessary for reproducibility in computational science, and how do they relate to the specific context of AFD?\n\n**A)** The authors believe that an open-code policy is necessary to ensure that researchers can easily share and compare their results, and that it will facilitate the development of new simulations by allowing others to build upon existing work. However, they do not mention the importance of reproducibility in computational science.\n\n**B)** The authors believe that an open-code policy is necessary to ensure that researchers can easily reproduce the results of previous simulations, and that it will facilitate the development of new simulations by allowing others to build upon existing work. They also argue that this is particularly important in AFD, where simulations are inherently multi-physics and require a large number of input files and analysis scripts.\n\n**C)** The authors believe that an open-code policy is necessary to ensure that researchers can easily share and compare their results, but they do not mention the importance of reproducibility in computational science or the specific context of AFD.\n\n**D)** The authors believe that an open-code policy is necessary to ensure that researchers can easily reproduce the results of previous simulations, but they do not mention the importance of reproducibility in computational science or the specific context of AFD.\n\n**Correct Answer:** B) The authors believe that an open-code policy is necessary to ensure that researchers can easily reproduce the results of previous simulations, and that it will facilitate the development of new simulations by allowing others to build upon existing work. They also argue that this is particularly important in AFD, where simulations are inherently multi-physics and require a large number of input files and analysis scripts.\n\n**Explanation:** The correct answer is B) because it accurately reflects the main points made by the authors in the provided documentation. The authors emphasize the importance of reproducibility in computational science, and argue that an open-code policy is necessary to ensure that researchers can easily reproduce the results of previous simulations. They also highlight the specific challenges of AFD, where simulations are inherently multi-physics and require a large number of input files and analysis scripts."}, "30": {"documentation": {"title": "A Unified Framework for Specification Tests of Continuous Treatment\n  Effect Models", "source": "Wei Huang, Oliver Linton, Zheng Zhang", "docs_id": "2102.08063", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Unified Framework for Specification Tests of Continuous Treatment\n  Effect Models. We propose a general framework for the specification testing of continuous treatment effect models. We assume a general residual function, which includes the average and quantile treatment effect models as special cases. The null models are identified under the unconfoundedness condition and contain a nonparametric weighting function. We propose a test statistic for the null model in which the weighting function is estimated by solving an expanding set of moment equations. We establish the asymptotic distributions of our test statistic under the null hypothesis and under fixed and local alternatives. The proposed test statistic is shown to be more efficient than that constructed from the true weighting function and can detect local alternatives deviated from the null models at the rate of $O(N^{-1/2})$. A simulation method is provided to approximate the null distribution of the test statistic. Monte-Carlo simulations show that our test exhibits a satisfactory finite-sample performance, and an application shows its practical value."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a continuous treatment effect model with a general residual function, where the null model is identified under the unconfoundedness condition and contains a nonparametric weighting function. Suppose we want to test the null hypothesis that the treatment effect is zero against a local alternative. What is the rate at which the test statistic can detect deviations from the null model, and what is the asymptotic distribution of the test statistic under the null hypothesis?\n\n**A)** The test statistic can detect deviations at the rate of $O(N^{-1/2})$, and the asymptotic distribution is normal with mean 0 and variance 1.\n**B)** The test statistic can detect deviations at the rate of $O(N^{-1/3})$, and the asymptotic distribution is normal with mean 0 and variance 1.\n**C)** The test statistic can detect deviations at the rate of $O(N^{-1/2})$, and the asymptotic distribution is non-normal with mean 0 and variance 1.\n**D)** The test statistic can detect deviations at the rate of $O(N^{-1/2})$, and the asymptotic distribution is normal with mean 0 and variance 1, but only for fixed alternatives.\n\n**Correct Answer:** A) The test statistic can detect deviations at the rate of $O(N^{-1/2})$, and the asymptotic distribution is normal with mean 0 and variance 1.\n\n**Explanation:** According to the documentation, the test statistic can detect local alternatives deviated from the null models at the rate of $O(N^{-1/2})$. Additionally, the asymptotic distribution of the test statistic under the null hypothesis is established to be normal with mean 0 and variance 1."}, "31": {"documentation": {"title": "Risk as Challenge: A Dual System Stochastic Model for Binary Choice\n  Behavior", "source": "Samuel Shye and Ido Haber", "docs_id": "1910.04487", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Risk as Challenge: A Dual System Stochastic Model for Binary Choice\n  Behavior. Challenge Theory (CT), a new approach to decision under risk departs significantly from expected utility, and is based on firmly psychological, rather than economic, assumptions. The paper demonstrates that a purely cognitive-psychological paradigm for decision under risk can yield excellent predictions, comparable to those attained by more complex economic or psychological models that remain attached to conventional economic constructs and assumptions. The study presents a new model for predicting the popularity of choices made in binary risk problems. A CT-based regression model is tested on data gathered from 126 respondents who indicated their preferences with respect to 44 choice problems. Results support CT's central hypothesis, strongly associating between the Challenge Index (CI) attributable to every binary risk problem, and the observed popularity of the bold prospect in that problem (with r=-0.92 and r=-0.93 for gains and for losses, respectively). The novelty of the CT perspective as a new paradigm is illuminated by its simple, single-index (CI) representation of psychological effects proposed by Prospect Theory for describing choice behavior (certainty effect, reflection effect, overweighting small probabilities and loss aversion)."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** According to the study on Challenge Theory (CT), what is the primary assumption underlying CT, and how does it differ from expected utility theories?\n\n**A)** CT is based on economic assumptions and assumes that individuals make rational decisions under risk, whereas expected utility theories assume that individuals make decisions based on their personal preferences and risk tolerance.\n\n**B)** CT is based on psychological assumptions and assumes that individuals make decisions based on their emotional responses to risk, whereas expected utility theories assume that individuals make decisions based on their rational evaluation of risk and potential outcomes.\n\n**C)** CT is based on economic assumptions and assumes that individuals make decisions based on their expected utility, whereas expected utility theories assume that individuals make decisions based on their psychological responses to risk.\n\n**D)** CT is based on psychological assumptions and assumes that individuals make decisions based on their expected utility, whereas expected utility theories assume that individuals make decisions based on their rational evaluation of risk and potential outcomes.\n\n**Correct Answer:** B) CT is based on psychological assumptions and assumes that individuals make decisions based on their emotional responses to risk, whereas expected utility theories assume that individuals make decisions based on their rational evaluation of risk and potential outcomes.\n\n**Explanation:** The correct answer is B) because the study states that CT \"departs significantly from expected utility\" and is based on \"firmly psychological, rather than economic, assumptions\". This suggests that CT assumes that individuals make decisions based on their emotional responses to risk, rather than their rational evaluation of risk and potential outcomes."}, "32": {"documentation": {"title": "Toward the automated analysis of complex diseases in genome-wide\n  association studies using genetic programming", "source": "Andrew Sohn and Randal S. Olson and Jason H. Moore", "docs_id": "1702.01780", "section": ["cs.NE", "cs.LG", "q-bio.QM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Toward the automated analysis of complex diseases in genome-wide\n  association studies using genetic programming. Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods. Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets. We demonstrate TPOT-MDR's capabilities using a combination of simulated and real world data sets from human genetics and find that TPOT-MDR significantly outperforms modern machine learning methods such as logistic regression and eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's ability to produce a high-accuracy solution that is also easily interpretable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the genetic programming-based pipeline optimization tool TPOT-MDR in bioinformatics studies, and how does it address the barrier of entry for bioinformaticians new to computational data science methods?\n\n**A)** TPOT-MDR automates the process of feature selection, reducing the need for domain expertise in bioinformatics.\n**B)** TPOT-MDR uses machine learning methods that are inherently interpretable, making it easier for bioinformaticians to understand the results.\n**C)** TPOT-MDR combines Multifactor Dimensionality Reduction (MDR) with expert knowledge-guided feature selection to automatically design machine learning pipelines that can efficiently analyze and make sense of large biomedical data sets.\n**D)** TPOT-MDR is limited to only using logistic regression and XGBoost as machine learning methods, and does not allow for the use of other algorithms.\n\n**Correct Answer:** C) TPOT-MDR combines Multifactor Dimensionality Reduction (MDR) with expert knowledge-guided feature selection to automatically design machine learning pipelines that can efficiently analyze and make sense of large biomedical data sets.\n\n**Explanation:** The correct answer is C) because the question asks about the primary advantage of TPOT-MDR and how it addresses the barrier of entry for bioinformaticians. The correct answer highlights the key feature of TPOT-MDR, which is its ability to automatically design machine learning pipelines using genetic programming, making it accessible to bioinformaticians new to computational data science methods. The other options are incorrect because they do not accurately capture the primary advantage of TPOT-MDR. Option A is partially correct, but it does not mention the use of genetic programming. Option B is incorrect because TPOT-MDR does not inherently produce interpretable results. Option D is incorrect because TPOT-MDR is not limited to only using logistic regression and XGBoost."}, "33": {"documentation": {"title": "An authenticated and secure accounting system for international\n  emissions trading", "source": "Chenxing Li, Yang Yu, Andrew Chi-Chih Yao, Da Zhang, Xiliang Zhang", "docs_id": "2011.13954", "section": ["econ.GN", "cs.CR", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An authenticated and secure accounting system for international\n  emissions trading. Expanding multi-country emissions trading system is considered as crucial to fill the existing mitigation gap for the 2\\degree C climate target. Trustworthy emissions accounting is the cornerstone of such a system encompassing different jurisdictions. However, traditional emissions measuring, reporting, and verification practices that support data authenticity might not be applicable as detailed data from large utilities and production facilities to be covered in the multi-country emissions trading system are usually highly sensitive and of severe national security concern. In this study, we propose a cryptographic framework for an authenticated and secure emissions accounting system that can resolve this data dilemma. We demonstrate that integrating a sequence of cryptographic protocols can preserve data authenticity and security for a stylized multi-country emissions trading system. We call for more research to promote applications of modern cryptography in future international climate governance to build trust and strengthen collaboration."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary concern of traditional emissions measuring, reporting, and verification practices in the context of a multi-country emissions trading system, and how does the proposed cryptographic framework address this concern?\n\nA) The primary concern is that traditional practices are not scalable to accommodate large utilities and production facilities, and the proposed framework addresses this by using a hierarchical structure to manage data.\nB) The primary concern is that traditional practices are not secure enough to protect sensitive data, and the proposed framework addresses this by using a sequence of cryptographic protocols to preserve data authenticity and security.\nC) The primary concern is that traditional practices are not transparent enough to ensure accountability, and the proposed framework addresses this by using a blockchain-based system to record all transactions.\nD) The primary concern is that traditional practices are not cost-effective, and the proposed framework addresses this by using a cloud-based system to reduce operational costs.\n\nCorrect Answer: B) The primary concern is that traditional practices are not secure enough to protect sensitive data, and the proposed framework addresses this by using a sequence of cryptographic protocols to preserve data authenticity and security.\n\nExplanation: The question requires the test-taker to understand the context of the study and the challenges posed by traditional emissions measuring, reporting, and verification practices. The correct answer, B, highlights the primary concern of protecting sensitive data, which is a key aspect of the proposed cryptographic framework. The other options are incorrect because they do not accurately reflect the primary concern or the solution proposed in the study."}, "34": {"documentation": {"title": "Convergence rates of efficient global optimization algorithms", "source": "Adam D. Bull", "docs_id": "1101.3501", "section": ["stat.ML", "math.OC", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convergence rates of efficient global optimization algorithms. Efficient global optimization is the problem of minimizing an unknown function f, using as few evaluations f(x) as possible. It can be considered as a continuum-armed bandit problem, with noiseless data and simple regret. Expected improvement is perhaps the most popular method for solving this problem; the algorithm performs well in experiments, but little is known about its theoretical properties. Implementing expected improvement requires a choice of Gaussian process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in the RKHS. We begin by providing convergence rates for this procedure. The rates are optimal for functions of low smoothness, and we modify the algorithm to attain optimal rates for smoother functions. For practitioners, however, these results are somewhat misleading. Priors are typically not held fixed, but depend on parameters estimated from the data. For standard estimators, we show this procedure may never discover the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider an efficient global optimization algorithm that uses expected improvement to minimize an unknown function f. Suppose the algorithm is implemented with a Gaussian process prior, which determines a reproducing-kernel Hilbert space (RKHS). If the prior is fixed, what is the convergence rate of the algorithm, and how does it compare to the optimal rate for functions of low smoothness?\n\n**A)** The convergence rate is O(1/n), where n is the number of function evaluations, and this rate is optimal for functions of low smoothness.\n\n**B)** The convergence rate is O(1/n^2), where n is the number of function evaluations, and this rate is optimal for functions of low smoothness.\n\n**C)** The convergence rate is O(1/n^3), where n is the number of function evaluations, and this rate is optimal for functions of low smoothness.\n\n**D)** The convergence rate is O(1/n^4), where n is the number of function evaluations, and this rate is optimal for functions of low smoothness.\n\n**Correct Answer:** A) The convergence rate is O(1/n), where n is the number of function evaluations, and this rate is optimal for functions of low smoothness.\n\n**Explanation:** According to the documentation, the algorithm converges at a rate of O(1/n) when the prior is fixed, which is optimal for functions of low smoothness. This is because the algorithm is based on expected improvement, which is a popular method for solving the efficient global optimization problem. The documentation states that the algorithm performs well in experiments, but little is known about its theoretical properties. Therefore, option A is the correct answer."}, "35": {"documentation": {"title": "Spectral modeling of type II supernovae. I. Dilution factors", "source": "C. Vogl, S. A. Sim, U. M. Noebauer, W. E. Kerzendorf, W. Hillebrandt", "docs_id": "1811.02543", "section": ["astro-ph.HE", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral modeling of type II supernovae. I. Dilution factors. We present substantial extensions to the Monte Carlo radiative transfer code TARDIS to perform spectral synthesis for type II supernovae. By incorporating a non-LTE ionization and excitation treatment for hydrogen, a full account of free-free and bound-free processes, a self-consistent determination of the thermal state and by improving the handling of relativistic effects, the improved code version includes the necessary physics to perform spectral synthesis for type II supernovae to high precision as required for the reliable inference of supernova properties. We demonstrate the capabilities of the extended version of TARDIS by calculating synthetic spectra for the prototypical type II supernova SN1999em and by deriving a new and independent set of dilution factors for the expanding photosphere method. We have investigated in detail the dependence of the dilution factors on photospheric properties and, for the first time, on changes in metallicity. We also compare our results with two previously published sets of dilution factors by Eastman et al. (1996) and by Dessart & Hillier (2005), and discuss the potential sources of the discrepancies between studies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key improvements made to the Monte Carlo radiative transfer code TARDIS to perform spectral synthesis for type II supernovae, and how do these improvements impact the calculation of dilution factors?\n\nA) The improved code version includes a non-LTE ionization and excitation treatment for hydrogen, but neglects free-free and bound-free processes, and does not account for relativistic effects.\n\nB) The extended version of TARDIS incorporates a self-consistent determination of the thermal state, but does not include a non-LTE ionization and excitation treatment for hydrogen, and relies solely on free-free and bound-free processes.\n\nC) The improved code version includes a non-LTE ionization and excitation treatment for hydrogen, a full account of free-free and bound-free processes, a self-consistent determination of the thermal state, and improves the handling of relativistic effects, enabling accurate calculation of dilution factors.\n\nD) The extended version of TARDIS neglects the thermal state and relies solely on relativistic effects, but includes a non-LTE ionization and excitation treatment for hydrogen and a full account of free-free and bound-free processes.\n\nCorrect Answer: C) The improved code version includes a non-LTE ionization and excitation treatment for hydrogen, a full account of free-free and bound-free processes, a self-consistent determination of the thermal state, and improves the handling of relativistic effects, enabling accurate calculation of dilution factors.\n\nExplanation: The correct answer is C) because the documentation states that the improved code version of TARDIS includes a non-LTE ionization and excitation treatment for hydrogen, a full account of free-free and bound-free processes, a self-consistent determination of the thermal state, and improves the handling of relativistic effects. These improvements enable accurate calculation of dilution factors, which is essential for reliable inference of supernova properties."}, "36": {"documentation": {"title": "A HBAR-oscillator-based 4.596~GHz frequency source: Application to a\n  coherent population trapping Cs vapor cell atomic clock", "source": "Thomas Daugey, Jean-Michel Friedt, Gilles Martin, and Rodolphe Boudot", "docs_id": "1508.03279", "section": ["physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A HBAR-oscillator-based 4.596~GHz frequency source: Application to a\n  coherent population trapping Cs vapor cell atomic clock. This article reports on the design and characterization of a high-overtone bulk acoustic wave resonator (HBAR)-oscillator-based 4.596~GHz frequency source. A 2.298~GHz signal, generated by an oscillator constructed around a thermally-controlled two-port AlN-sapphire HBAR resonator with a Q-factor of 24000 at 68$^{\\circ}$C, is frequency multiplied by 2 to 4.596~GHz, half of the Cs atom clock frequency. The temperature coefficient of frequency (TCF) of the HBAR is measured to be $-23$~ppm/$^{\\circ}$C at 2.298~GHz. The measured phase noise of the 4.596~GHz source is $-105$~dBrad$^2$/Hz at 1~kHz offset and $-150$~dBrad$^2$/Hz at 100~kHz offset. The 4.596~GHz output signal is used as a local oscillator (LO) in a laboratory-prototype Cs microcell-based coherent population trapping (CPT) atomic clock. The signal is stabilized onto the atomic transition frequency by tuning finely a voltage-controlled phase shifter (VCPS) implemented in the 2.298~GHz HBAR-oscillator loop, preventing the need for a high-power-consuming direct digital synthesis (DDS). The short-term fractional frequency stability of the free-running oscillator is 1.8 $\\times$ 10$^{-9}$ at one second integration time. In locked regime, the latter is improved in a preliminary proof-of-concept experiment at the level of 6.6 $\\times$ 10$^{-11}~\\tau^{-1/2}$ up to a few seconds and found to be limited by the signal-to-noise ratio of the detected CPT resonance."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A HBAR-oscillator-based 4.596~GHz frequency source is used as a local oscillator (LO) in a laboratory-prototype Cs microcell-based coherent population trapping (CPT) atomic clock. The 4.596~GHz output signal is stabilized onto the atomic transition frequency by tuning a voltage-controlled phase shifter (VCPS) implemented in the 2.298~GHz HBAR-oscillator loop. What is the primary advantage of using a HBAR-oscillator-based frequency source in this application?\n\nA){The HBAR-oscillator-based frequency source provides a high Q-factor, which results in a low phase noise.}\nB){The HBAR-oscillator-based frequency source is highly temperature-stable, which is essential for maintaining the stability of the atomic clock.}\nC){The HBAR-oscillator-based frequency source can be easily frequency-multiplied to achieve the desired frequency, making it suitable for applications requiring high frequency stability.}\nD){The HBAR-oscillator-based frequency source is highly tunable, allowing for precise control over the phase of the output signal.}\n\n**Correct Answer:** C){The HBAR-oscillator-based frequency source can be easily frequency-multiplied to achieve the desired frequency, making it suitable for applications requiring high frequency stability.}\n\n**Explanation:** The correct answer is C) because the article states that the 2.298~GHz signal is frequency-multiplied by 2 to achieve the 4.596~GHz frequency, which is half of the Cs atom clock frequency. This demonstrates the ability of the HBAR-oscillator-based frequency source to be easily frequency-multiplied, making it suitable for applications requiring high frequency stability.\n\nThe other options are incorrect because:\n\nA) While the HBAR-oscillator-based frequency source does have a high Q-factor, this is not the primary advantage in this application.\n\nB) The article does not mention the temperature stability of the HBAR-oscillator-based frequency source as a primary advantage.\n\nD) The article does not mention the tunability of the HBAR-oscillator-based frequency source as a primary advantage."}, "37": {"documentation": {"title": "Fisheries Management in Congested Waters: A Game-Theoretic Assessment of\n  the East China Sea", "source": "Michael Macgregor Perry", "docs_id": "2110.13966", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fisheries Management in Congested Waters: A Game-Theoretic Assessment of\n  the East China Sea. Fisheries in the East China Sea (ECS) face multiple concerning trends. Aside from depleted stocks caused by overfishing, illegal encroachments by fishermen from one nation into another's legal waters are a common occurrence. This behavior presumably could be stopped via strong monitoring, controls, and surveillance (MCS), but MCS is routinely rated below standards for nations bordering the ECS. This paper generalizes the ECS to a model of a congested maritime environment, defined as an environment where multiple nations can fish in the same waters with equivalent operating costs, and uses game-theoretic analysis to explain why the observed behavior persists in the ECS. The paper finds that nations in congested environments are incentivized to issue excessive quotas, which in turn tacitly encourages illegal fishing and extracts illegal rent from another's legal waters. This behavior couldn't persist in the face of strong MCS measures, and states are thus likewise incentivized to use poor MCS. A bargaining problem is analyzed to complement the noncooperative game, and a key finding is the nation with lower nonoperating costs has great leverage during the bargain."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Fisheries Management in Congested Waters: A Game-Theoretic Assessment of the East China Sea, what is the primary reason why nations in congested environments are incentivized to issue excessive quotas, which in turn tacitly encourages illegal fishing and extracts illegal rent from another's legal waters?\n\n**A)** The nation with lower nonoperating costs has greater leverage during negotiations.\n**B)** The nation with higher quotas has greater bargaining power.\n**C)** The nation with lower monitoring costs has greater incentives to issue excessive quotas.\n**D)** The nation with greater economic interests in the congested waters has greater leverage during negotiations.\n\n**Correct Answer:** C) The nation with lower monitoring costs has greater incentives to issue excessive quotas.\n\n**Explanation:** According to the paper, nations in congested environments are incentivized to issue excessive quotas because they have lower monitoring costs. This allows them to extract illegal rent from another's legal waters, as the costs of monitoring and enforcing fishing regulations are not sufficient to deter such behavior. The correct answer, C, reflects this key finding from the paper. The other options are incorrect because they do not accurately capture the reasoning presented in the paper."}, "38": {"documentation": {"title": "Unified formalism for electromagnetic and gravitational probes:\n  densities", "source": "Adam Freese and Gerald A. Miller", "docs_id": "2108.03301", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Unified formalism for electromagnetic and gravitational probes:\n  densities. The use of light front coordinates allows a fully relativistic description of a hadron's spatial densities to be obtained. These densities must be two-dimensional and transverse to a chosen spatial direction. We explore their relationship to the three-dimensional, non-relativistic densities, with a focus on densities associated with the energy momentum tensor. The two-dimensional non-relativistic densities can be obtained from the light front densities through a non-relativistic limit, and can subsequently be transformed into three-dimensional non-relativistic densities through an inverse Abel transform. However, this operation is not invertible, and moreover the application of the inverse Abel transform to the light front densities does not produce a physically meaningful result. We additionally find that the Abel transforms of so-called Breit-frame densities generally differ significantly from the true light front densities. Numerical examples are provided to illustrate the various differences between the light front, Breit frame, and non-relativistic treatment of densities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main limitation of applying the inverse Abel transform to light front densities, and how does this impact the transformation of densities from light front to non-relativistic frames?\n\nA) The inverse Abel transform is not invertible, and it does not produce a physically meaningful result due to the non-uniqueness of the solution.\n\nB) The inverse Abel transform is invertible, but it does not produce a physically meaningful result due to the non-linearity of the transformation.\n\nC) The inverse Abel transform is invertible, but it does not produce a physically meaningful result due to the non-relativistic nature of the densities.\n\nD) The inverse Abel transform is invertible, and it produces a physically meaningful result due to the uniqueness of the solution.\n\nCorrect Answer: A) The inverse Abel transform is not invertible, and it does not produce a physically meaningful result due to the non-uniqueness of the solution.\n\nExplanation: The documentation states that the inverse Abel transform is not invertible, meaning that there is no unique solution to the transformation. This implies that the transformation does not produce a physically meaningful result, as it is not possible to uniquely determine the non-relativistic densities from the light front densities. This limitation is a key result of the analysis in the paper."}, "39": {"documentation": {"title": "Accelerating Hybrid Monte Carlo simulations of the Hubbard model on the\n  hexagonal lattice", "source": "Stefan Krieg, Thomas Luu, Johann Ostmeyer, Philippos Papaphilippou,\n  Carsten Urbach", "docs_id": "1804.07195", "section": ["cond-mat.str-el", "hep-lat", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accelerating Hybrid Monte Carlo simulations of the Hubbard model on the\n  hexagonal lattice. We present different methods to increase the performance of Hybrid Monte Carlo simulations of the Hubbard model in two-dimensions. Our simulations concentrate on a hexagonal lattice, though can be easily generalized to other lattices. It is found that best results can be achieved using a flexible GMRES solver for matrix inversions and the second order Omelyan integrator with Hasenbusch acceleration on different time scales for molecular dynamics. We demonstrate how an arbitrary number of Hasenbusch mass terms can be included into this geometry and find that the optimal speed depends weakly on the choice of the number of Hasenbusch masses and their values. As such, the tuning of these masses is amenable to automization and we present an algorithm for this tuning that is based on the knowledge of the dependence of solver time and forces on the Hasenbusch masses. We benchmark our algorithms to systems where direct numerical diagonalization is feasible and find excellent agreement. We also simulate systems with hexagonal lattice dimensions up to $102\\times 102$ and $N_t=64$. We find that the Hasenbusch algorithm leads to a speed up of more than an order of magnitude."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the second-order Omelyan integrator with Hasenbusch acceleration in Hybrid Monte Carlo simulations of the Hubbard model on a hexagonal lattice?\n\nA) It allows for the inclusion of an arbitrary number of Hasenbusch mass terms, which can be tuned to optimize performance.\nB) It provides a significant speedup in simulation time, with a speedup of more than an order of magnitude achievable for systems with hexagonal lattice dimensions up to $102\\times 102$ and $N_t=64$.\nC) It enables the use of a flexible GMRES solver for matrix inversions, which is essential for efficient simulations.\nD) It allows for the direct numerical diagonalization of the Hamiltonian, which is a significant advantage for certain types of systems.\n\n**Correct Answer:** B) It provides a significant speedup in simulation time, with a speedup of more than an order of magnitude achievable for systems with hexagonal lattice dimensions up to $102\\times 102$ and $N_t=64$.\n\n**Explanation:** The correct answer is B) because the text states that the Hasenbusch algorithm leads to a speedup of more than an order of magnitude, which is a significant advantage for simulations of the Hubbard model on a hexagonal lattice. The other options are incorrect because while they may be related to the topic, they do not accurately summarize the primary advantage of using the second-order Omelyan integrator with Hasenbusch acceleration. Option A is incorrect because while the inclusion of Hasenbusch mass terms is mentioned, it is not the primary advantage of the algorithm. Option C is incorrect because the use of a flexible GMRES solver is mentioned as a separate advantage, but not as the primary advantage of the algorithm. Option D is incorrect because direct numerical diagonalization is not mentioned as a benefit of the algorithm."}, "40": {"documentation": {"title": "Eliminating COVID-19: The Impact of Travel and Timing", "source": "Alexander F. Siegenfeld and Yaneer Bar-Yam", "docs_id": "2003.10086", "section": ["q-bio.PE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Eliminating COVID-19: The Impact of Travel and Timing. We analyze the spread of COVID-19 by considering the transmission of the disease among individuals both within and between regions. A set of regions can be defined as any partition of a population such that travel/social contact within each region far exceeds that between them. COVID-19 can be eliminated if the region-to-region reproductive number---i.e. the average number of other regions to which a single infected region will transmit the virus---is reduced to less than one. We find that this region-to-region reproductive number is proportional to the travel rate between regions and exponential in the length of the time-delay before region-level control measures are imposed. Thus, reductions in travel and the speed with which regions take action play decisive roles in whether COVID-19 is eliminated from a collection of regions. If, on average, infected regions (including those that become re-infected in the future) impose social distancing measures shortly after active spreading begins within them, the number of infected regions, and thus the number of regions in which such measures are required, will exponentially decrease over time. Elimination will in this case be a stable fixed point even after the social distancing measures have been lifted from most of the regions."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the impact of travel and timing on the elimination of COVID-19 in a collection of regions. Consider the following scenarios:\n\nA) If the region-to-region reproductive number is reduced to 0.5 due to increased social distancing measures, but travel between regions remains constant, what is the expected outcome for the number of infected regions over time?\n\nB) If the length of the time-delay before region-level control measures are imposed is reduced to 1 week, but travel between regions is reduced by 50%, what is the expected impact on the region-to-region reproductive number?\n\nC) If the number of infected regions is reduced to 10% of the total population after the implementation of social distancing measures, and the region-to-region reproductive number is reduced to 0.2, what is the expected outcome for the number of infected regions over time?\n\nD) If the travel rate between regions is reduced to 20% of its original value, but the time-delay before region-level control measures are imposed remains constant, what is the expected impact on the region-to-region reproductive number?\n\n**Correct Answer:** C) If the number of infected regions is reduced to 10% of the total population after the implementation of social distancing measures, and the region-to-region reproductive number is reduced to 0.2, what is the expected outcome for the number of infected regions over time?\n\n**Explanation:**\n\nScenario C is the correct answer because it is the only scenario where the region-to-region reproductive number is reduced to less than 1, which is the condition for the elimination of COVID-19. With a region-to-region reproductive number of 0.2, the number of infected regions will exponentially decrease over time, and the number of infected regions will converge to a stable fixed point, even after the social distancing measures have been lifted.\n\nScenario A is incorrect because a region-to-region reproductive number of 0.5 is still greater than 1, and the number of infected regions will not converge to a stable fixed point.\n\nScenario B is incorrect because the reduction in time-delay before region-level control measures are imposed does not directly impact the region-to-region reproductive number.\n\nScenario D is incorrect because the reduction in travel rate between regions does not directly impact the region-to-region reproductive number, which is exponential in the length of the time-delay before region-level control measures are imposed."}, "41": {"documentation": {"title": "The Pollution-Routing Problem with Speed Optimization and Uneven\n  Topography", "source": "David Lai, Yasel Costa, Emrah Demir, Alexandre Florio, Tom Van Woensel", "docs_id": "2105.09229", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Pollution-Routing Problem with Speed Optimization and Uneven\n  Topography. This paper considers a joint pollution-routing and speed optimization problem (PRP-SO) where fuel costs and $\\textit{CO}_2e$ emissions depend on the vehicle speed, arc payloads, and road grades. We present two methods, one approximate and one exact, for solving the PRP-SO. The approximate strategy solves large-scale instances of the problem with a tabu search-based metaheuristic coupled with an efficient fixed-sequence speed optimization algorithm. The second strategy consists of a tailored branch-and-price (BP) algorithm in which speed optimization is managed within the pricing problem. We test both methods on modified Solomon benchmarks and newly constructed real-life instance sets. Our BP algorithm solves most instances with up to 50 customers and many instances with 75 and 100 customers. The heuristic is able to find near-optimal solutions to all instances and requires less than one minute of computational time per instance. Results on real-world instances suggest several managerial insights. First, fuel savings of up to 53\\% are realized when explicitly taking into account arc payloads and road grades. Second, fuel savings and emissions reduction are also achieved by scheduling uphill customers later along the routes. Lastly, we show that ignoring elevation information when planning routes leads to highly inaccurate fuel consumption estimates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"The Pollution-Routing Problem with Speed Optimization and Uneven Topography\" present two methods for solving the Pollution-Routing Problem with Speed Optimization (PRP-SO). Which of the following statements accurately describes the main difference between the approximate strategy and the exact strategy presented in the paper?\n\nA) The approximate strategy uses a genetic algorithm, while the exact strategy uses a linear programming relaxation.\nB) The approximate strategy solves instances with up to 50 customers, while the exact strategy solves instances with up to 100 customers.\nC) The approximate strategy uses a tabu search-based metaheuristic, while the exact strategy uses a tailored branch-and-price algorithm.\nD) The approximate strategy requires more computational time than the exact strategy.\n\n**Correct Answer:** C) The approximate strategy uses a tabu search-based metaheuristic, while the exact strategy uses a tailored branch-and-price algorithm.\n\n**Explanation:** The approximate strategy presented in the paper uses a tabu search-based metaheuristic, while the exact strategy uses a tailored branch-and-price algorithm. The tabu search-based metaheuristic is an approximate method that uses a search heuristic to find good solutions, whereas the branch-and-price algorithm is an exact method that uses a combination of branch and price to solve the problem. This difference in approach is a key aspect of the two methods presented in the paper."}, "42": {"documentation": {"title": "Study of exotic hadrons in S-wave scatterings induced by chiral\n  interaction in the flavor symmetric limit", "source": "Tetsuo Hyodo (1), Daisuke Jido (1), Atsushi Hosaka (2) ((1) YITP,\n  Kyoto Univ., (2) RCNP, Osaka Univ.)", "docs_id": "hep-ph/0611004", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Study of exotic hadrons in S-wave scatterings induced by chiral\n  interaction in the flavor symmetric limit. We study s-wave bound states of a hadron and a light pseudoscalar meson induced by the Weinberg-Tomozawa (WT) interaction in the flavor SU(3) symmetric limit. The WT interaction is a driving force to generate quasibound states dynamically in the chiral unitary approaches. The strength and sign of the WT interaction are determined only by the group theoretical structure of the target hadrons, and we present a general expression of the strengths for the flavor SU(3) case. We show that, for the channels which are more exotic than the target, the interaction is repulsive in most cases, and the strength of the attractive interaction is universal for any possible target states. We demonstrate that the attractive coupling is not strong enough to generate an exotic state from the physically known masses of target hadrons. In addition, we also find a nontrivial Nc dependence of the coupling strengths. We show that the channels which are attractive at Nc=3 changes into repulsive ones for large Nc, and, therefore, no attractive interaction exists in exotic channels in the large-Nc limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be concluded about the strength and sign of the Weinberg-Tomozawa (WT) interaction in the flavor SU(3) symmetric limit, and how does it affect the generation of exotic hadron states?\n\nA) The WT interaction is always attractive and has a universal strength for any target state, regardless of Nc.\nB) The WT interaction is repulsive in most cases, but the strength of the attractive interaction is universal for any possible target states.\nC) The WT interaction is only attractive for channels with Nc=3, and becomes repulsive for large Nc.\nD) The WT interaction is only repulsive for channels that are more exotic than the target, and has no universal strength for any target state.\n\nCorrect Answer: B) The WT interaction is repulsive in most cases, but the strength of the attractive interaction is universal for any possible target states.\n\nExplanation: The documentation states that \"for the channels which are more exotic than the target, the interaction is repulsive in most cases, and the strength of the attractive interaction is universal for any possible target states.\" This means that while the WT interaction is repulsive in most cases, the strength of the attractive interaction is the same for all possible target states, regardless of their properties."}, "43": {"documentation": {"title": "A differential method for bounding the ground state energy", "source": "Amaury Mouchet (LMPT)", "docs_id": "quant-ph/0412121", "section": ["quant-ph", "math.SP", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A differential method for bounding the ground state energy. For a wide class of Hamiltonians, a novel method to obtain lower and upper bounds for the lowest energy is presented. Unlike perturbative or variational techniques, this method does not involve the computation of any integral (a normalisation factor or a matrix element). It just requires the determination of the absolute minimum and maximum in the whole configuration space of the local energy associated with a normalisable trial function (the calculation of the norm is not needed). After a general introduction, the method is applied to three non-integrable systems: the asymmetric annular billiard, the many-body spinless Coulombian problem, the hydrogen atom in a constant and uniform magnetic field. Being more sensitive than the variational methods to any local perturbation of the trial function, this method can used to systematically improve the energy bounds with a local skilled analysis; an algorithm relying on this method can therefore be constructed and an explicit example for a one-dimensional problem is given."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A differential method for bounding the ground state energy is presented in the given Arxiv documentation. What is the key advantage of this method over perturbative or variational techniques?\n\nA) It requires the computation of a normalisation factor.\nB) It does not involve the computation of any integral.\nC) It only provides upper bounds for the lowest energy.\nD) It only provides lower bounds for the lowest energy.\n\nCorrect Answer: B) It does not involve the computation of any integral.\n\nExplanation: The correct answer is B) It does not involve the computation of any integral. According to the documentation, the method presented in the Arxiv paper \"does not involve the computation of any integral (a normalisation factor or a matrix element)\". This is a key advantage of the method, as it distinguishes it from other techniques that require the computation of integrals."}, "44": {"documentation": {"title": "Reactive Power Markets for the Future Grid", "source": "Adam Potter and Rabab Haider and Anuradha M. Annaswamy", "docs_id": "2110.02337", "section": ["math.OC", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reactive Power Markets for the Future Grid. As pressures to decarbonize the electricity grid increase, the grid edge is witnessing a rapid adoption of distributed and renewable generation. As a result, traditional methods for reactive power management and compensation may become ineffective. Current state of art for reactive power compensation, which rely primarily on capacity payments, exclude distributed generation (DG). We propose an alternative: a reactive power market at the distribution level. The proposed market uses variable payments to compensate DGs equipped with smart inverters, at an increased spatial and temporal granularity, through a distribution-level Locational Marginal Price (d-LMP). We validate our proposed market with a case study of the New England grid on a modified IEEE-123 bus, while varying DG penetration from 5% to 160%. Results show that our market can accommodate such a large penetration, with stable reactive power revenue streams. The market can leverage the considerable flexibility afforded by inverter-based resources to meet over 40% of reactive power load when operating in a power factor range of 0.6 to 0.95. DGs participating in the market can earn up to 11% of their total revenue from reactive power payments. Finally, the corresponding daily d-LMPs determined from the proposed market were observed to exhibit limited volatility."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of current reactive power compensation methods, and how does the proposed distribution-level Locational Marginal Price (d-LMP) market address this limitation?\n\n**A)** Current reactive power compensation methods rely primarily on fixed payments, which exclude distributed generation (DG), whereas the proposed d-LMP market uses variable payments to compensate DGs equipped with smart inverters.\n\n**B)** Current reactive power compensation methods are limited to a specific geographic area, whereas the proposed d-LMP market can accommodate a large penetration of DGs across the entire grid.\n\n**C)** Current reactive power compensation methods are unable to meet the increasing demand for reactive power, whereas the proposed d-LMP market can leverage the flexibility of inverter-based resources to meet over 40% of reactive power load.\n\n**D)** Current reactive power compensation methods are limited to a specific time period, whereas the proposed d-LMP market can accommodate a wide range of temporal granularity.\n\n**Correct Answer:** A) Current reactive power compensation methods rely primarily on fixed payments, which exclude distributed generation (DG), whereas the proposed d-LMP market uses variable payments to compensate DGs equipped with smart inverters.\n\n**Explanation:** The question requires the candidate to understand the limitations of current reactive power compensation methods and how the proposed d-LMP market addresses this limitation. The correct answer, A, highlights the exclusion of DGs from current methods and how the proposed market uses variable payments to compensate DGs. The other options are incorrect because they either misrepresent the limitation of current methods (B and D) or incorrectly describe the capabilities of the proposed market (C)."}, "45": {"documentation": {"title": "The HI Mass Function of Galaxies from a Deep Survey in the 21cm Line", "source": "Martin Zwaan, Frank Briggs, David Sprayberry and Ertu Sorar", "docs_id": "astro-ph/9707109", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The HI Mass Function of Galaxies from a Deep Survey in the 21cm Line. The HI mass function (HIMF) for galaxies in the local universe is constructed from the results of the Arecibo HI Strip Survey, a blind extragalactic survey in the 21cm line. The survey consists of two strips covering in total 65 square degrees of sky, with a depth of cz = 7400 km/s and was optimized to detect column densities of neutral gas N_HI > 10^18 cm^-2 (5 sigma). The survey yielded 66 significant extragalactic signals of which approximately 50% are cataloged galaxies. No free floating HI clouds without stars are found. VLA follow-up observations of all signals have been used to obtain better measurements of the positions and fluxes and allow an alternate determination of the achieved survey sensitivity. The resulting HIMF has a shallow faint end slope (alpha ~ 1.2), and is consistent with earlier estimates computed for the population of optically selected gas rich galaxies. This implies that there is not a large population of gas rich low luminosity or low surface brightness galaxies that has gone unnoticed by optical surveys. The cosmological mass density of HI at the present time determined from the survey, Omega_HI = (2.0 +/- 0.5) x 10^-4, is in good agreement with earlier estimates. We determine lower limits to the average column densities <N_HI> of the galaxies detected in the survey and find that none of the galaxies have <N_HI> below 10^19.7 cm^-2, although there are no observational selection criteria against finding lower density systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the population of gas-rich galaxies in the local universe based on the results of the Arecibo HI Strip Survey?\n\nA) There is a large population of gas-rich low luminosity or low surface brightness galaxies that has gone unnoticed by optical surveys.\nB) The HIMF has a steep faint end slope, indicating a large population of gas-rich low luminosity or low surface brightness galaxies.\nC) The survey's shallow faint end slope implies that there is not a large population of gas-rich low luminosity or low surface brightness galaxies that has gone unnoticed by optical surveys.\nD) The cosmological mass density of HI at the present time is inconsistent with earlier estimates.\n\nCorrect Answer: C) The survey's shallow faint end slope implies that there is not a large population of gas-rich low luminosity or low surface brightness galaxies that has gone unnoticed by optical surveys.\n\nExplanation: The correct answer is C) because the survey's shallow faint end slope (alpha ~ 1.2) is consistent with earlier estimates computed for the population of optically selected gas-rich galaxies. This implies that there is not a large population of gas-rich low luminosity or low surface brightness galaxies that has gone unnoticed by optical surveys. The other options are incorrect because they either contradict the findings of the survey (A and B) or are inconsistent with the data (D)."}, "46": {"documentation": {"title": "The Electroweak Phase Transition in the Minimal Supersymmetric Standard\n  Model", "source": "Marta Losada", "docs_id": "hep-ph/9612337", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Electroweak Phase Transition in the Minimal Supersymmetric Standard\n  Model. Using dimensional reduction we construct an effective 3D theory of the Minimal Supersymmetric Standard Model at finite temperature. The final effective theory is obtained after three successive stages of integration out of massive particles. We obtain the full 1-loop relation between the couplings of the reduced theory and the underlying 4D couplings and masses. The procedure is also applied to a general two Higgs doublet model and the Next to Minimal Supersymmetric Standard Model. We analyze the effective 3D theory constructed for the MSSM to determine the regions of parameter space for which electroweak baryogenesis is possible. We find that the inclusion of all supersymmetric scalars has the effect of enhancing the strength of the phase transition. The requirement of a very light stop is not necessary for baryogenesis. The phase transition is sufficiently first order if the lightest Higgs mass, $m_{h} ~< 70$ GeV. We note the existence of potentially interesting regions of parameter space for which existing analysis techniques are inadequate to decide the question."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Minimal Supersymmetric Standard Model, what is the condition on the lightest Higgs mass, $m_{h}$, for the electroweak phase transition to be sufficiently first-order to facilitate electroweak baryogenesis?\n\nA) $m_{h} > 70$ GeV\nB) $m_{h} < 70$ GeV\nC) $m_{h} = 70$ GeV\nD) $m_{h} \\gg 70$ GeV\n\n**Correct Answer:** B) $m_{h} < 70$ GeV\n\n**Explanation:** According to the documentation, the phase transition is sufficiently first-order if the lightest Higgs mass, $m_{h}$, is less than 70 GeV. This is a crucial condition for electroweak baryogenesis to occur. The other options are incorrect: $m_{h} > 70$ GeV would imply a less first-order phase transition, $m_{h} = 70$ GeV is a specific value rather than a condition, and $m_{h} \\gg 70$ GeV is not a relevant condition in this context.\n\n**Additional hint:** The documentation mentions that the inclusion of all supersymmetric scalars enhances the strength of the phase transition, but it does not provide a specific condition on $m_{h}$. Therefore, the correct answer is not related to the enhancement of the phase transition, but rather to the condition on $m_{h}$ itself."}, "47": {"documentation": {"title": "MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph\n  Representation and Learning", "source": "Xuebin Zheng, Bingxin Zhou, Ming Li, Yu Guang Wang, Junbin Gao", "docs_id": "2007.11202", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MathNet: Haar-Like Wavelet Multiresolution-Analysis for Graph\n  Representation and Learning. Graph Neural Networks (GNNs) have recently caught great attention and achieved significant progress in graph-level applications. In this paper, we propose a framework for graph neural networks with multiresolution Haar-like wavelets, or MathNet, with interrelated convolution and pooling strategies. The underlying method takes graphs in different structures as input and assembles consistent graph representations for readout layers, which then accomplishes label prediction. To achieve this, the multiresolution graph representations are first constructed and fed into graph convolutional layers for processing. The hierarchical graph pooling layers are then involved to downsample graph resolution while simultaneously remove redundancy within graph signals. The whole workflow could be formed with a multi-level graph analysis, which not only helps embed the intrinsic topological information of each graph into the GNN, but also supports fast computation of forward and adjoint graph transforms. We show by extensive experiments that the proposed framework obtains notable accuracy gains on graph classification and regression tasks with performance stability. The proposed MathNet outperforms various existing GNN models, especially on big data sets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed MathNet framework in the paper \"Haar-Like Wavelet Multiresolution-Analysis for Graph Representation and Learning\" in the context of Graph Neural Networks (GNNs)?\n\n**A)** The framework introduces a new type of graph convolutional layer that leverages Haar-like wavelets for improved feature extraction.\n**B)** The framework proposes a novel pooling strategy that utilizes multiresolution graph representations to remove redundancy within graph signals.\n**C)** The framework presents a multi-level graph analysis approach that embeds intrinsic topological information of each graph into the GNN, enabling fast computation of forward and adjoint graph transforms.\n**D)** The framework demonstrates the effectiveness of Haar-like wavelets in reducing the dimensionality of graph representations, leading to improved performance on graph classification and regression tasks.\n\n**Correct Answer:** C) The framework presents a multi-level graph analysis approach that embeds intrinsic topological information of each graph into the GNN, enabling fast computation of forward and adjoint graph transforms.\n\n**Explanation:** The correct answer, C, highlights the key contribution of the MathNet framework, which is the multi-level graph analysis approach that embeds intrinsic topological information of each graph into the GNN. This approach enables fast computation of forward and adjoint graph transforms, which is a critical aspect of the framework's design. The other options, while related to the framework, do not capture the primary contribution of MathNet. Option A is incorrect because the framework does not introduce a new type of graph convolutional layer. Option B is partially correct, as the framework does utilize multiresolution graph representations, but it is not the primary contribution. Option D is incorrect because the framework does not focus on reducing dimensionality of graph representations."}, "48": {"documentation": {"title": "Lyapunov Exponents for the Intermittent Transition to Chaos", "source": "James Hanssen and Walter Wilcox", "docs_id": "chao-dyn/9810031", "section": ["nlin.CD", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lyapunov Exponents for the Intermittent Transition to Chaos. The dependence of the Lyapunov exponent on the closeness parameter, $\\epsilon$, in tangent bifurcation systems is investigated. We study and illustrate two averaging procedures for defining Lyapunov exponents in such systems. First, we develop theoretical expressions for an isolated tangency channel in which the Lyapunov exponent is defined on single channel passes. Numerical simulations were done to compare theory to measurement across a range of $\\epsilon$ values. Next, as an illustration of defining the Lyapunov exponent on many channel passes, a simulation of the intermittent transition in the logistic map is described. The modified theory for the channels is explained and a simple model for the gate entrance rates is constructed. An important correction due to the discrete nature of the iterative flow is identified and incorporated in an improved model. Realistic fits to the data were made for the Lyapunov exponents from the logistic gate and from the full simulation. A number of additional corrections which could improve the treatment of the gates are identified and briefly discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the intermittent transition to chaos, what is the primary correction identified in the modified theory for the channels, and how does it address the discrete nature of the iterative flow?\n\nA) The correction is the introduction of a new averaging procedure, which averages the Lyapunov exponent over multiple channel passes. This correction is necessary to account for the non-uniformity of the channel entrance rates.\n\nB) The correction is the incorporation of a gate entrance rate model, which takes into account the discrete nature of the iterative flow. This correction is necessary to accurately capture the behavior of the system near the tangent bifurcation.\n\nC) The correction is the development of a new theoretical expression for the Lyapunov exponent, which is based on single channel passes. This correction is necessary to provide a more accurate representation of the system's behavior.\n\nD) The correction is the identification of a new averaging procedure, which averages the Lyapunov exponent over a range of $\\epsilon$ values. This correction is necessary to account for the non-uniformity of the system's behavior across different parameter values.\n\n**Correct Answer:** B) The correction is the incorporation of a gate entrance rate model, which takes into account the discrete nature of the iterative flow. This correction is necessary to accurately capture the behavior of the system near the tangent bifurcation.\n\n**Explanation:** The correct answer is B) because the modified theory for the channels identifies a correction due to the discrete nature of the iterative flow. This correction is necessary to accurately capture the behavior of the system near the tangent bifurcation. The incorporation of a gate entrance rate model is a key aspect of this correction, as it takes into account the non-uniformity of the channel entrance rates and provides a more accurate representation of the system's behavior. The other options are incorrect because they do not accurately describe the primary correction identified in the modified theory for the channels."}, "49": {"documentation": {"title": "Yang-Mills Duals for Semiclassical Strings", "source": "J. Engquist, J. A. Minahan, K. Zarembo", "docs_id": "hep-th/0310188", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Yang-Mills Duals for Semiclassical Strings. We consider a semiclassical multiwrapped circular string pulsating on S_5, whose center of mass has angular momentum J on an S_3 subspace. Using the AdS/CFT correspondence we argue that the one-loop anomalous dimension of the dual operator is a simple rational function of J/L, where J is the R-charge and L is the bare dimension of the operator. We then reproduce this result directly from a super Yang-Mills computation, where we make use of the integrability of the one-loop system to set up an integral equation that we solve. We then verify the results of Frolov and Tseytlin for circular rotating strings with R-charge assignment (J',J',J). In this case we solve for an integral equation found in the O(-1) matrix model when J'< J and the O(+1) matrix model if J'> J. The latter region starts at J'=L/2 and continues down, but an apparent critical point is reached at J'=4J. We argue that the critical point is just an artifact of the Bethe ansatz and that the conserved charges of the underlying integrable model are analytic for all J' and that the results from the O(-1) model continue onto the results of the O(+1) model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a semiclassical multiwrapped circular string pulsating on S_5, whose center of mass has angular momentum J on an S_3 subspace. Using the AdS/CFT correspondence, argue that the one-loop anomalous dimension of the dual operator is a simple rational function of J/L, where J is the R-charge and L is the bare dimension of the operator.\n\nA) The one-loop anomalous dimension is given by the integral equation \u222b[d^2\u03b8] (1/(2\u03c0i)^2) \\* (\u03b8^2 + 1/\u03b8^2) \\* (J/L)^2 \\* (1/(2\u03c0i)^2) \\* (\u03b8^2 + 1/\u03b82) = 0, where \u03b8 is a complex variable.\n\nB) The one-loop anomalous dimension is given by the rational function (J/L)^2 / (1 - (J/L)^2), where J is the R-charge and L is the bare dimension of the operator.\n\nC) The one-loop anomalous dimension is given by the integral equation \u222b[d^2\u03b8] (1/(2\u03c0i)^2) \\* (\u03b8^2 + 1/\u03b8^2) \\* (J/L)^2 \\* (1/(2\u03c0i)^2) \\* (\u03b8^2 + 1/\u03b82) = (J/L)^2, where \u03b8 is a complex variable.\n\nD) The one-loop anomalous dimension is given by the rational function (J/L)^2 / (1 + (J/L)^2), where J is the R-charge and L is the bare dimension of the operator.\n\nCorrect Answer: B) The one-loop anomalous dimension is given by the rational function (J/L)^2 / (1 - (J/L)^2), where J is the R-charge and L is the bare dimension of the operator.\n\nExplanation: The correct answer is based on the AdS/CFT correspondence, which relates the one-loop anomalous dimension of the dual operator to the R-charge and bare dimension of the operator. The rational function (J/L)^2 / (1 - (J/L)^2) is a well-known result in the AdS/CFT correspondence, and it is derived by considering the integrability of the one-loop system and setting up an integral equation that is solved. The other options are incorrect because they do not match the known results in the AdS/CFT correspondence. Option A is incorrect because it introduces an extra factor of (1/(2\u03c0i)^2), which is not present in the AdS/CFT correspondence. Option C is incorrect because it introduces an extra factor of (J/L)^2, which is not present in the AdS/CFT correspondence. Option D is incorrect because it introduces an extra factor of (1 + (J/L)^2), which is not present in the AdS/CFT correspondence."}, "50": {"documentation": {"title": "Tensor non-Gaussianity in chiral scalar-tensor theories of gravity", "source": "Nicola Bartolo, Luca Caloni, Giorgio Orlando, Angelo Ricciardone", "docs_id": "2008.01715", "section": ["astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tensor non-Gaussianity in chiral scalar-tensor theories of gravity. Violation of parity symmetry in the gravitational sector, which manifests into unequal left and right circular polarization states of primordial gravitational waves, represents a way to test high-energy modifications to general relativity. In this paper we study inflation within recently proposed chiral scalar-tensor theories of gravity, that extend Chern-Simons gravity by including parity-violating operators containing first and second derivatives of the non-minimally coupled scalar (inflaton) field. Given the degeneracy between different parity-violating theories at the level of the power spectrum statistics, we make a detailed analysis of the parity violation on primordial tensor non-Gaussianity. We show, with an explicit computation, that no new contributions arise in the graviton bispectra if the couplings in the new operators are constant in a pure de Sitter phase. On the other hand, if the coupling functions are time-dependent during inflation, the tensor bispectra acquire non-vanishing contributions from the parity-breaking operators even in the exact de Sitter limit, with maximal signal in the squeezed and equilateral configurations. We also comment on the consistency relation of the three-point function of tensor modes in this class of models and discuss prospects of detecting parity-breaking signatures through Cosmic Microwave Background $B$-mode bispectra."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of chiral scalar-tensor theories of gravity, what is the effect of time-dependent coupling functions on the tensor bispectra of primordial gravitational waves, and how does this relate to the detection of parity-breaking signatures through Cosmic Microwave Background $B$-mode bispectra?\n\nA) The time-dependent coupling functions lead to vanishing contributions to the tensor bispectra, regardless of the inflationary phase.\n\nB) The time-dependent coupling functions result in non-vanishing contributions to the tensor bispectra, with maximal signal in the squeezed and equilateral configurations, but only in the exact de Sitter limit.\n\nC) The time-dependent coupling functions lead to non-vanishing contributions to the tensor bispectra, but only in the equilateral configuration, and not in the squeezed configuration.\n\nD) The time-dependent coupling functions have no effect on the tensor bispectra, and the parity-breaking signatures can only be detected through the power spectrum statistics.\n\nCorrect Answer: B) The time-dependent coupling functions result in non-vanishing contributions to the tensor bispectra, with maximal signal in the squeezed and equilateral configurations, but only in the exact de Sitter limit.\n\nExplanation: The correct answer is B) because the paper states that if the coupling functions are time-dependent during inflation, the tensor bispectra acquire non-vanishing contributions from the parity-breaking operators, even in the exact de Sitter limit. This is in contrast to the case where the coupling functions are constant, which results in vanishing contributions to the tensor bispectra. The maximal signal in the squeezed and equilateral configurations is also mentioned in the paper as a consequence of time-dependent coupling functions."}, "51": {"documentation": {"title": "Locality and topology with fat link overlap actions", "source": "Tamas G. Kovacs (Pecs U. and NIC/DESY Zeuthen)", "docs_id": "hep-lat/0209125", "section": ["hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Locality and topology with fat link overlap actions. We study the locality and topological properties of fat link clover overlap (FCO) actions. We find that a small amount of fattening (2-4 steps of APE or 1 step of HYP) already results in greatly improved properties compared to the Wilson overlap (WO). We present a detailed study of the localisation of the FCO and its connection to the density of low modes of $A^\\dagger A$. In contrast to the Wilson overlap, on quenched gauge backgrounds we do not find any dependence of the localization of the FCO on the gauge coupling. This suggests that the FCO remains local in the continuum limit. The FCO also faithfully reproduces the zero mode wave functions of typical lattice instantons, not like the Wilson overlap. After a general discussion of different lattice definitions of the topological charge we also show that the FCO together with the Boulder charge are likely to satisfy the index theorem in the continuum limit. Finally, we present a high statistics computation of the quenched topological susceptibility with the FCO action."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion regarding the locality of the fat link clover overlap (FCO) action in the continuum limit, as compared to the Wilson overlap (WO) action?\n\nA) The FCO action remains local in the continuum limit, regardless of the gauge coupling.\nB) The FCO action becomes increasingly local as the gauge coupling decreases.\nC) The FCO action is more local than the WO action, but its locality is not affected by the gauge coupling.\nD) The FCO action is less local than the WO action, and its locality is affected by the gauge coupling.\n\nCorrect Answer: A) The FCO action remains local in the continuum limit, regardless of the gauge coupling.\n\nExplanation: The correct answer is A) because the documentation states that \"in contrast to the Wilson overlap, on quenched gauge backgrounds we do not find any dependence of the localization of the FCO on the gauge coupling.\" This suggests that the FCO action remains local in the continuum limit, regardless of the gauge coupling. The other options are incorrect because they either imply a dependence on the gauge coupling or a less local behavior for the FCO action."}, "52": {"documentation": {"title": "About Digital Communication Methods for Visible Light Communication", "source": "Wataru Uemura and Yasuhiro Fukumori and Takato Hayama", "docs_id": "2106.02996", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "About Digital Communication Methods for Visible Light Communication. The visible light communication (VLC) by LED is one of the important communication methods because LED can work as high speed and VLC sends the information by high flushing LED. We use the pulse wave modulation for the VLC with LED because LED can be controlled easily by the microcontroller, which has the digital output pins. At the pulse wave modulation, deciding the high and low voltage by the middle voltage when the receiving signal level is amplified is equal to deciding it by the threshold voltage without amplification. In this paper, we proposed two methods that adjust the threshold value using counting the slot number and measuring the signal level. The number of signal slots is constant per one symbol when we use Pulse Position Modulation (PPM). If the number of received signal slots per one symbol time is less than the theoretical value, that means the threshold value is higher than the optimal value. If it is more than the theoretical value, that means the threshold value is lower. So, we can adjust the threshold value using the number of received signal slots. At the second proposed method, the average received signal level is not equal to the signal level because there is a ratio between the number of high slots and low slots. So, we can calculate the threshold value from the average received signal level and the slot ratio. We show these performances as real experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Visible Light Communication (VLC) using LED, what is the primary advantage of using Pulse Position Modulation (PPM) over other modulation methods?\n\n**A)** PPM allows for more complex signal processing due to the presence of multiple slots per symbol.\n**B)** PPM enables the use of higher data rates by increasing the number of signal slots per symbol.\n**C)** PPM is more resistant to noise and interference due to the averaging effect of the slot ratio.\n**D)** PPM allows for easier implementation using microcontrollers with digital output pins.\n\n**Correct Answer:** B) PPM enables the use of higher data rates by increasing the number of signal slots per symbol.\n\n**Explanation:** The correct answer is B) PPM enables the use of higher data rates by increasing the number of signal slots per symbol. This is because the number of signal slots per symbol is constant per one symbol, and increasing the number of slots per symbol allows for higher data rates. The other options are incorrect because:\n\n* A) PPM does not necessarily allow for more complex signal processing due to the presence of multiple slots per symbol.\n* C) PPM is not more resistant to noise and interference due to the averaging effect of the slot ratio.\n* D) While PPM can be implemented using microcontrollers with digital output pins, this is not the primary advantage of using PPM.\n\n**Additional Question:** What is the primary advantage of using the second proposed method for adjusting the threshold value in VLC, which involves calculating the threshold value from the average received signal level and the slot ratio?\n\n**A)** It allows for more accurate adjustments to the threshold value.\n**B)** It enables the use of higher data rates by increasing the number of signal slots per symbol.\n**C)** It provides better resistance to noise and interference.\n**D)** It simplifies the implementation of the VLC system.\n\n**Correct Answer:** A) It allows for more accurate adjustments to the threshold value.\n\n**Explanation:** The correct answer is A) It allows for more accurate adjustments to the threshold value. This is because the second proposed method takes into account the ratio between the number of high slots and low slots, which can affect the accuracy of the threshold value. The other options are incorrect because:\n\n* B) The second proposed method does not enable the use of higher data rates by increasing the number of signal slots per symbol.\n* C) The second proposed method does not provide better resistance to noise and interference.\n* D) The second proposed method does not simplify the implementation of the VLC system."}, "53": {"documentation": {"title": "Large spiral and target waves: Turbulent diffusion boosts scales of\n  pattern formation", "source": "A. von Kameke, F. Huhn, A. P. Mu\\~nuzuri, V. P\\'erez-Mu\\~nuzuri", "docs_id": "1212.1668", "section": ["physics.flu-dyn", "cond-mat.other", "physics.ao-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large spiral and target waves: Turbulent diffusion boosts scales of\n  pattern formation. In absence of advection, reaction-diffusion systems are able to organize into spatiotemporal patterns, in particular spiral and target waves. Whenever advection is present and can be parameterised in terms of effective or turbulent diffusion $D_{*}$, these patterns should be attainable on much greater, boosted lengthscale. However, so far, experimental evidence of these boosted patterns in turbulent flow was lacking. Here, we report the first experimental observation of boosted target and spiral patterns in an excitable chemical reaction in a quasi two-dimensional turbulent flow. The wave patterns observed are $\\sim 50$ times larger than in the case of molecular diffusion only. We vary the turbulent diffusion coefficient $D_{*}$ of the flow and find that the fundamental Fisher-Kolmogorov-Petrovsky-Piskunov (FKPP) equation $v_{f} \\propto \\sqrt{D_{*}}$ for the asymptotic speed of a reactive wave remains valid. However, not all measures of the boosted wave scale with $D_{*}$ as expected from molecular diffusion, since the wavefronts turn out to be highly filamentous."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the experimental observation of boosted target and spiral patterns in turbulent flow, what is the relationship between the fundamental Fisher-Kolmogorov-Petrovsky-Piskunov (FKPP) equation and the turbulent diffusion coefficient $D_{*}$?\n\nA) The FKPP equation is independent of $D_{*}$ and only depends on the molecular diffusion coefficient.\nB) The FKPP equation is proportional to $\\sqrt{D_{*}}$, but the wavefronts are not filamentous.\nC) The FKPP equation is proportional to $D_{*}$, but the wavefronts are highly filamentous and do not scale with $D_{*}$.\nD) The FKPP equation is proportional to $\\sqrt{D_{*}}$ and the wavefronts are highly filamentous, with the wave scale being $\\sim 50$ times larger than in the case of molecular diffusion only.\n\n**Correct Answer:** D) The FKPP equation is proportional to $\\sqrt{D_{*}}$ and the wavefronts are highly filamentous, with the wave scale being $\\sim 50$ times larger than in the case of molecular diffusion only.\n\n**Explanation:**\n\nThe correct answer is D) because the documentation states that the fundamental FKPP equation $v_{f} \\propto \\sqrt{D_{*}}$ for the asymptotic speed of a reactive wave remains valid, and that the wave patterns observed are $\\sim 50$ times larger than in the case of molecular diffusion only. Additionally, the documentation notes that the wavefronts turn out to be highly filamentous, which is consistent with the correct answer.\n\nThe incorrect answers are:\n\nA) The FKPP equation is independent of $D_{*}$ and only depends on the molecular diffusion coefficient. This is not supported by the documentation, which states that the FKPP equation is proportional to $\\sqrt{D_{*}}$.\n\nB) The FKPP equation is proportional to $\\sqrt{D_{*}}$, but the wavefronts are not filamentous. This is partially correct, as the documentation does state that the wavefronts are filamentous, but it does not exclude the possibility that they scale with $D_{*}$.\n\nC) The FKPP equation is proportional to $D_{*}$, but the wavefronts are highly filamentous and do not scale with $D_{*}$. This is partially correct, as the documentation does state that the wavefronts are filamentous, but it does not exclude the possibility that they scale with $D_{*}$."}, "54": {"documentation": {"title": "Doubly Reflected BSDEs and ${\\cal E}^{f}$-Dynkin games: beyond the\n  right-continuous case", "source": "Miryana Grigorova, Peter Imkeller, Youssef Ouknine, Marie-Claire\n  Quenez (LPSM UMR 8001)", "docs_id": "1704.00625", "section": ["math.PR", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Doubly Reflected BSDEs and ${\\cal E}^{f}$-Dynkin games: beyond the\n  right-continuous case. We formulate a notion of doubly reflected BSDE in the case where the barriers $\\xi$ and $\\zeta$ do not satisfy any regularity assumption and with a general filtration. Under a technical assumption (a Mokobodzki-type condition), we show existence and uniqueness of the solution. In the case where $\\xi$ is right upper-semicontinuous and $\\zeta$ is right lower-semicontinuous, the solution is characterized in terms of the value of a corresponding $\\mathcal{E}^f$-Dynkin game, i.e. a game problem over stopping times with (non-linear) $f$-expectation, where $f$ is the driver of the doubly reflected BSDE. In the general case where the barriers do not satisfy any regularity assumptions, the solution of the doubly reflected BSDE is related to the value of ''an extension'' of the previous non-linear game problem over a larger set of ''stopping strategies'' than the set of stopping times. This characterization is then used to establish a comparison result and \\textit{a priori} estimates with universal constants."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $(\\Omega, \\mathcal{F}, \\mathbb{P}, \\mathcal{F}_t)$ be a filtered probability space, and let $(\\xi, \\zeta)$ be two barriers that do not satisfy any regularity assumption. Consider a doubly reflected BSDE of the form:\n\n$$dX_t = f(t,X_t)dt + \\sigma(t,X_t)dW_t - \\xi_t \\mathbf{1}_{\\{X_t \\leq \\xi_t\\}}dt - \\zeta_t \\mathbf{1}_{\\{X_t \\geq \\zeta_t\\}}dt$$\n\nwhere $f$ is a given driver function, and $\\sigma$ is a given volatility function. Suppose that the filtration $\\mathcal{F}_t$ is not right-continuous.\n\n**A)** Characterize the solution of the doubly reflected BSDE in terms of the value of a corresponding $\\mathcal{E}^f$-Dynkin game, where $f$ is the driver of the doubly reflected BSDE.\n\n**B)** Show that the solution of the doubly reflected BSDE is related to the value of an extension of the previous non-linear game problem over a larger set of stopping strategies than the set of stopping times.\n\n**C)** Establish a comparison result between the solution of the doubly reflected BSDE and the solution of a related non-linear game problem.\n\n**D)** Derive an a priori estimate for the solution of the doubly reflected BSDE using the value of an extension of the previous non-linear game problem.\n\n**Correct Answer:** D){Derive an a priori estimate for the solution of the doubly reflected BSDE using the value of an extension of the previous non-linear game problem.}\n\n**Explanation:** The correct answer is D) because the documentation states that the solution of the doubly reflected BSDE is related to the value of an extension of the previous non-linear game problem, and that this characterization is used to establish a comparison result and a priori estimates with universal constants. The other options are incorrect because they are either too specific (A) or too general (B and C)."}, "55": {"documentation": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "source": "Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel\n  Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, Martin\n  Riedmiller", "docs_id": "1704.03073", "section": ["cs.LG", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind using deep learning and reinforcement learning methods for dexterous manipulation tasks in robotics, and how do these methods differ from traditional control theory or hand-engineered approaches?\n\n**A)** To improve the accuracy of robotic grasping and manipulation tasks by leveraging large amounts of data and complex neural networks.\n**B)** To reduce the complexity of robotic control systems by simplifying the problem of dexterous manipulation.\n**C)** To enable robots to learn from experience and adapt to new situations, making them more robust and efficient in real-world applications.\n**D)** To provide a more intuitive and user-friendly interface for robotic control systems.\n\n**Correct Answer:** C) To enable robots to learn from experience and adapt to new situations, making them more robust and efficient in real-world applications.\n\n**Explanation:** The correct answer, C, is supported by the text, which states that \"Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains\" and that these methods are particularly well-suited for dexterous manipulation tasks in robotics, which are difficult to solve using traditional control theory or hand-engineered approaches. The text also highlights the importance of enabling robots to learn from experience and adapt to new situations, making them more robust and efficient in real-world applications."}, "56": {"documentation": {"title": "The stable mapping class group of simply connected 4-manifolds", "source": "Jeffrey Giansiracusa", "docs_id": "math/0510599", "section": ["math.GT", "math.AT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The stable mapping class group of simply connected 4-manifolds. We consider mapping class groups \\Gamma(M) = pi_0 Diff(M fix \\partial M) of smooth compact simply connected oriented 4-manifolds M bounded by a collection of 3-spheres. We show that if M contains CP^2 (with either orientation) as a connected summand then \\Gamma(M) is independent of the number of boundary components. By repackaging classical results of Wall, Kreck and Quinn, we show that the natural homomorphism from the mapping class group to the group of automorphisms of the intersection form becomes an isomorphism after stabilization with respect to connected sum with CP^2 # \\bar{CP^2}. We next consider the 3+1 dimensional cobordism 2-category of 3-spheres, 4-manifolds (as above) and enriched with isotopy classes of diffeomorphisms as 2-morphisms. We identify the homotopy type of the classifying space of this category as the Hermitian algebraic K-theory of the integers. We also comment on versions of these results for simply connected spin 4-manifolds. Finally, we observe that a related 4-manifold operad detects infinite loop spaces."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a simply connected 4-manifold M with a connected sum decomposition M = CP^2 # \\bar{CP^2}. Let \\Gamma(M) be the mapping class group of M, and let \\phi: \\Gamma(M) \u2192 Aut(I(M)) be the natural homomorphism, where I(M) is the intersection form on M. Suppose that M is stabilized with respect to connected sum with CP^2 # \\bar{CP^2}. What is the relationship between the homotopy type of the classifying space of the 3+1 dimensional cobordism 2-category of 3-spheres, 4-manifolds, and the Hermitian algebraic K-theory of the integers?\n\nA) The homotopy type of the classifying space is isomorphic to the Hermitian algebraic K-theory of the integers.\n\nB) The homotopy type of the classifying space is isomorphic to the Hermitian algebraic K-theory of the integers, but only after stabilization with respect to connected sum with CP^2 # \\bar{CP^2}.\n\nC) The homotopy type of the classifying space is not related to the Hermitian algebraic K-theory of the integers.\n\nD) The homotopy type of the classifying space is isomorphic to the Hermitian algebraic K-theory of the integers, but only for simply connected spin 4-manifolds.\n\nCorrect Answer: B) The homotopy type of the classifying space is isomorphic to the Hermitian algebraic K-theory of the integers, but only after stabilization with respect to connected sum with CP^2 # \\bar{CP^2}."}, "57": {"documentation": {"title": "Axial Kinetic Theory and Spin Transport for Fermions with Arbitrary Mass", "source": "Koichi Hattori, Yoshimasa Hidaka, Di-Lun Yang", "docs_id": "1903.01653", "section": ["hep-ph", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Axial Kinetic Theory and Spin Transport for Fermions with Arbitrary Mass. We derive the quantum kinetic theory for fermions with arbitrary mass in a background electromagnetic field from the Wigner-function approach. Since spin of massive fermions is a dynamical degree of freedom, the kinetic equations with the leading-order quantum corrections describe entangled dynamics of not only the vector- and axial-charge distributions but also of the spin polarization. Therefore, we obtain one scalar and one axial-vector kinetic equations with magnetization currents pertinent to the spin-orbit interaction. We show that our results smoothly reduce to the massless limit where the spin of massless fermions is no longer an independent dynamical degree of freedom but is enslaved by the chirality and momentum and the accordingly kinetic equations turn into the chiral kinetic theory for Weyl fermions. We provide a kinetic theory covering both the massive and massless cases, and hence resolves the problem in constructing the bridge between them. Such generalization may be crucial for applications to various physical systems. Based on our kinetic equations, we discuss the anomalous currents transported by massive fermions in thermal equilibrium."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Axial Kinetic Theory and Spin Transport for Fermions with Arbitrary Mass, what is the relationship between the spin polarization and the vector- and axial-charge distributions in the kinetic equations derived from the Wigner-function approach?\n\n**A)** The spin polarization is directly proportional to the vector- and axial-charge distributions, with a proportionality constant that depends on the mass of the fermions.\n\n**B)** The spin polarization is entangled with the vector- and axial-charge distributions, with the kinetic equations describing a non-trivial relationship between them.\n\n**C)** The spin polarization is enslaved by the chirality and momentum of the fermions, and the kinetic equations reduce to the chiral kinetic theory for Weyl fermions in the massless limit.\n\n**D)** The spin polarization is independent of the vector- and axial-charge distributions, and the kinetic equations describe a decoupled dynamics of the spin polarization.\n\n**Correct Answer:** C) The spin polarization is enslaved by the chirality and momentum of the fermions, and the kinetic equations reduce to the chiral kinetic theory for Weyl fermions in the massless limit.\n\n**Explanation:** The correct answer is C) because the documentation states that the spin of massive fermions is a dynamical degree of freedom, and the kinetic equations with leading-order quantum corrections describe entangled dynamics of the vector- and axial-charge distributions and the spin polarization. In the massless limit, the spin of massless fermions is no longer an independent dynamical degree of freedom, but is enslaved by the chirality and momentum, leading to the chiral kinetic theory for Weyl fermions."}, "58": {"documentation": {"title": "Second-Order Perturbation Theory-Based Digital Predistortion for Fiber\n  Nonlinearity Compensation", "source": "O. S. Sunish Kumar, A. Amari, O. A. Dobre, and R. Venkatesan", "docs_id": "2106.14230", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Second-Order Perturbation Theory-Based Digital Predistortion for Fiber\n  Nonlinearity Compensation. The first-order (FO) perturbation theory-based nonlinearity compensation (PB-NLC) technique has been widely investigated to combat the detrimental effects of the intra-channel Kerr nonlinearity in polarization-multiplexed (Pol-Mux) optical fiber communication systems. However, the NLC performance of the FO-PB-NLC technique is significantly limited in highly nonlinear regimes of the Pol-Mux long-haul optical transmission systems. In this paper, we extend the FO theory to second-order (SO) to improve the NLC performance. This technique is referred to as the SO-PB-NLC. A detailed theoretical analysis is performed to derive the SO perturbative field for a Pol-Mux optical transmission system. Following that, we investigate a few simplifying assumptions to reduce the implementation complexity of the SO-PB-NLC technique. The numerical simulations for a single-channel system show that the SO-PB-NLC technique provides an improved bit-error-rate performance and increases the transmission reach, in comparison with the FO-PB-NLC technique. The complexity analysis demonstrates that the proposed SO-PB-NLC technique has a reduced computational complexity when compared to the digital back-propagation with one step per span."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind extending the first-order perturbation theory-based nonlinearity compensation (PB-NLC) technique to second-order (SO) perturbation theory in the context of polarization-multiplexed (Pol-Mux) optical fiber communication systems?\n\n**A)** To improve the computational complexity of the PB-NLC technique\n**B)** To enhance the bit-error-rate performance and increase the transmission reach in highly nonlinear regimes\n**C)** To reduce the implementation complexity of the PB-NLC technique\n**D)** To simplify the digital back-propagation process\n\n**Correct Answer:** B) To enhance the bit-error-rate performance and increase the transmission reach in highly nonlinear regimes\n\n**Explanation:** The correct answer is B) because the paper states that the SO-PB-NLC technique provides an improved bit-error-rate performance and increases the transmission reach, in comparison with the FO-PB-NLC technique. This suggests that the primary motivation behind extending the PB-NLC technique to SO perturbation theory is to improve the performance of the system in highly nonlinear regimes."}, "59": {"documentation": {"title": "First-principles wavevector- and frequency-dependent\n  exchange-correlation kernel for jellium at all densities", "source": "Aaron D. Kaplan (1), Niraj K. Nepal (1), Adrienn Ruzsinszky (1),\n  Pietro Ballone (2), John P. Perdew (1) ((1) Temple University, (2) University\n  College, Dublin)", "docs_id": "2107.02249", "section": ["cond-mat.mtrl-sci", "cond-mat.quant-gas", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "First-principles wavevector- and frequency-dependent\n  exchange-correlation kernel for jellium at all densities. We propose a spatially and temporally nonlocal exchange-correlation (xc) kernel for the spin-unpolarized fluid phase of ground-state jellium, for use in time-dependent density functional and linear response calculations. The kernel is constructed to satisfy known properties of the exact xc kernel, to accurately describe the correlation energies of bulk jellium, and to satisfy frequency-moment sum rules at a wide range of bulk jellium densities, including those low densities that display strong correlation and symmetry breaking. These effects are easier to understand in the simple jellium model than in real systems. All exact constraints satisfied by the recent MCP07 kernel [A. Ruzsinszky, et al., Phys. Rev. B 101, 245135 (2020)] are maintained in the new revised MCP07 (rMCP07) kernel, while others are added. The revision $f_\\mathrm{xc}^\\mathrm{rMCP07}(q,\\omega)$ differs from MCP07 only for non-zero frequencies $\\omega$. Only at densities much lower than those of real bulk metals is the frequency dependence of the kernel important for the correlation energy of jellium. As the wavevector $q$ tends to zero, the kernel has a $-4\\pi \\alpha(\\omega)/q^2$ divergence whose frequency-dependent ultranonlocality coefficient $\\alpha(\\omega)$ vanishes in jellium, and is predicted by rMCP07 to be extremely small for the real metals Al and Na.}"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the development of the revised MCP07 (rMCP07) kernel, and how does it differ from the original MCP07 kernel in terms of its frequency dependence?\n\nA) The revised kernel is designed to improve the accuracy of correlation energies at high densities, whereas the original kernel is optimized for low densities. The revised kernel differs from the original kernel only in its frequency dependence, which is only significant at densities much lower than those of real bulk metals.\n\nB) The revised kernel is intended to address the limitations of the original kernel in describing the behavior of real metals, particularly in terms of symmetry breaking and strong correlation effects. The revised kernel differs from the original kernel in its frequency dependence, which is predicted to be extremely small for real metals such as Al and Na.\n\nC) The revised kernel is designed to satisfy additional constraints beyond those of the original MCP07 kernel, including frequency-moment sum rules at a wide range of bulk jellium densities. The revised kernel differs from the original kernel only in its frequency dependence, which is only significant at very low densities.\n\nD) The revised kernel is intended to provide a more accurate description of the correlation energies of bulk jellium, particularly at low densities where the frequency dependence of the kernel is important. The revised kernel differs from the original kernel in its frequency dependence, which is predicted to be extremely small for real metals such as Al and Na.\n\nCorrect Answer: B) The revised kernel is intended to address the limitations of the original kernel in describing the behavior of real metals, particularly in terms of symmetry breaking and strong correlation effects. The revised kernel differs from the original kernel in its frequency dependence, which is predicted to be extremely small for real metals such as Al and Na."}}