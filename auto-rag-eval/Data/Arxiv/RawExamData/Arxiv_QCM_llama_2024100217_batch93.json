{"0": {"documentation": {"title": "Correlational latent heat by nonlocal quantum kinetic theory", "source": "K. Morawetz", "docs_id": "1805.09683", "section": ["cond-mat.str-el", "nucl-th", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlational latent heat by nonlocal quantum kinetic theory. The kinetic equation of nonlocal and non-instantaneous character unifies the achievements of the transport in dense quantum gases with the Landau theory of quasiclassical transport in Fermi systems. Large cancellations in the off-shell motion appear which are hidden usually in non-Markovian behaviors. The remaining corrections are expressed in terms of shifts in space and time that characterize the non-locality of the scattering process. In this way quantum transport is possible to recast into a quasi-classical picture. The balance equations for the density, momentum, energy and entropy include besides quasiparticle also the correlated two-particle contributions beyond the Landau theory. The medium effects on binary collisions are shown to mediate the latent heat, i.e., an energy conversion between correlation and thermal energy. For Maxwellian particles with time-dependent s-wave scattering, the correlated parts of the observables are calculated and a sign change of the latent heat is reported at a universal ratio of scattering length to the thermal De Broglie wavelength. This is interpreted as a change from correlational heating to cooling."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Correlational latent heat by nonlocal quantum kinetic theory, what is the significance of the sign change of the latent heat at a universal ratio of scattering length to the thermal De Broglie wavelength?\n\n**A)** This indicates a transition from quantum to classical behavior in the system.\n**B)** It signifies the emergence of non-equilibrium effects in the system, leading to correlational heating.\n**C)** The sign change is a consequence of the Landau theory of quasiclassical transport in Fermi systems, and it marks a change from correlational heating to cooling.\n**D)** This phenomenon is a direct result of the non-Markovian behavior of the system, which leads to the mediation of latent heat by medium effects on binary collisions.\n\n**Correct Answer:** C) It signifies the emergence of non-equilibrium effects in the system, leading to correlational heating.\n\n**Explanation:** The correct answer, C, is supported by the text, which states that the sign change of the latent heat is reported at a universal ratio of scattering length to the thermal De Broglie wavelength. This indicates a change from correlational heating to cooling, as mentioned in the text. The other options are incorrect because they either misinterpret the significance of the sign change (A and D) or incorrectly attribute the phenomenon to the Landau theory (B)."}, "1": {"documentation": {"title": "Graph-based Transforms for Video Coding", "source": "Hilmi E. Egilmez, Yung-Hsuan Chao, Antonio Ortega", "docs_id": "1909.00952", "section": ["eess.IV", "cs.LG", "cs.MM", "cs.SY", "eess.SY", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graph-based Transforms for Video Coding. In many state-of-the-art compression systems, signal transformation is an integral part of the encoding and decoding process, where transforms provide compact representations for the signals of interest. This paper introduces a class of transforms called graph-based transforms (GBTs) for video compression, and proposes two different techniques to design GBTs. In the first technique, we formulate an optimization problem to learn graphs from data and provide solutions for optimal separable and nonseparable GBT designs, called GL-GBTs. The optimality of the proposed GL-GBTs is also theoretically analyzed based on Gaussian-Markov random field (GMRF) models for intra and inter predicted block signals. The second technique develops edge-adaptive GBTs (EA-GBTs) in order to flexibly adapt transforms to block signals with image edges (discontinuities). The advantages of EA-GBTs are both theoretically and empirically demonstrated. Our experimental results demonstrate that the proposed transforms can significantly outperform the traditional Karhunen-Loeve transform (KLT)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of graph-based transforms (GBTs) for video compression, as proposed in the paper \"Graph-based Transforms for Video Coding\"?\n\nA) To improve the efficiency of traditional transform coding techniques, such as the Karhunen-Loeve transform (KLT).\nB) To enable the modeling of complex video signals using compact representations, which can be efficiently compressed and decompressed.\nC) To develop new techniques for image denoising and deblurring.\nD) To provide a framework for machine learning-based video compression, leveraging graph-based methods for signal transformation.\n\n**Correct Answer:** B) To enable the modeling of complex video signals using compact representations, which can be efficiently compressed and decompressed.\n\n**Explanation:** The paper introduces GBTs as a class of transforms for video compression, aiming to provide compact representations for video signals. The primary motivation is to enable efficient compression and decompression of complex video signals, which is a key challenge in modern video coding systems. The proposed GBTs, including GL-GBTs and EA-GBTs, are designed to address this challenge by providing flexible and efficient ways to transform video signals.\n\n**Candidate A:** While traditional transform coding techniques, such as the KLT, are widely used in video compression, the primary motivation behind GBTs is not to improve the efficiency of these techniques, but rather to develop new methods for signal transformation.\n\n**Candidate B:** This is the correct answer, as the paper aims to enable efficient compression and decompression of complex video signals using compact representations.\n\n**Candidate C:** Image denoising and deblurring are not the primary focus of the paper, which is focused on video compression.\n\n**Candidate D:** While machine learning-based video compression is an active area of research, the paper does not aim to provide a framework for this specific application, but rather to develop new transform techniques for video compression."}, "2": {"documentation": {"title": "Allosteric collaboration between elongation factor G and the ribosomal\n  L1 stalk directs tRNA movements during translation", "source": "Jingyi Fei, Jonathan E. Bronson, Jake M. Hofman, Rathi L. Srinivas,\n  Chris H. Wiggins and Ruben L. Gonzalez, Jr", "docs_id": "0909.0466", "section": ["q-bio.BM", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Allosteric collaboration between elongation factor G and the ribosomal\n  L1 stalk directs tRNA movements during translation. Determining the mechanism by which transfer RNAs (tRNAs) rapidly and precisely transit through the ribosomal A, P and E sites during translation remains a major goal in the study of protein synthesis. Here, we report the real-time dynamics of the L1 stalk, a structural element of the large ribosomal subunit that is implicated in directing tRNA movements during translation. Within pre-translocation ribosomal complexes, the L1 stalk exists in a dynamic equilibrium between open and closed conformations. Binding of elongation factor G (EF-G) shifts this equilibrium towards the closed conformation through one of at least two distinct kinetic mechanisms, where the identity of the P-site tRNA dictates the kinetic route that is taken. Within post-translocation complexes, L1 stalk dynamics are dependent on the presence and identity of the E-site tRNA. Collectively, our data demonstrate that EF-G and the L1 stalk allosterically collaborate to direct tRNA translocation from the P to the E sites, and suggest a model for the release of E-site tRNA."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary function of the L1 stalk in the large ribosomal subunit, and how does its dynamic equilibrium between open and closed conformations influence tRNA translocation during translation?\n\nA) The L1 stalk is responsible for binding to elongation factor G (EF-G) and shifting the equilibrium towards the closed conformation, which facilitates the release of E-site tRNA.\nB) The L1 stalk is a structural element that directs tRNA movements during translation by existing in a dynamic equilibrium between open and closed conformations, with the identity of the P-site tRNA dictating the kinetic route taken.\nC) The L1 stalk is involved in the recognition of the E-site tRNA and its release from the ribosome, with its dynamic equilibrium influencing the post-translocation complexes.\nD) The L1 stalk is a static element that remains unchanged throughout the translation process, and its function is solely dependent on the binding of EF-G.\n\nCorrect Answer: B) The L1 stalk is a structural element that directs tRNA movements during translation by existing in a dynamic equilibrium between open and closed conformations, with the identity of the P-site tRNA dictating the kinetic route taken.\n\nExplanation: The correct answer is B) because the L1 stalk's dynamic equilibrium between open and closed conformations is crucial for directing tRNA movements during translation. The identity of the P-site tRNA dictates the kinetic route taken, which is a key aspect of the allosteric collaboration between EF-G and the L1 stalk. The other options are incorrect because they either oversimplify the function of the L1 stalk or attribute its function to other ribosomal components."}, "3": {"documentation": {"title": "Random horizon principal-agent problem", "source": "Yiqing Lin, Zhenjie Ren, Nizar Touzi, Junjian Yang", "docs_id": "2002.10982", "section": ["math.OC", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Random horizon principal-agent problem. We consider a general formulation of the random horizon Principal-Agent problem with a continuous payment and a lump-sum payment at termination. In the European version of the problem, the random horizon is chosen solely by the principal with no other possible action from the agent than exerting effort on the dynamics of the output process. We also consider the American version of the contract, which covers the seminal Sannikov's model, where the agent can also quit by optimally choosing the termination time of the contract. Our main result reduces such non-zero-sum stochastic differential games to appropriate stochastic control problems which may be solved by standard methods of stochastic control theory. This reduction is obtained by following Sannikov's approach, further developed by Cvitanic, Possamai, and Touzi. We first introduce an appropriate class of contracts for which the agent's optimal effort is immediately characterized by the standard verification argument in stochastic control theory. We then show that this class of contracts is dense in an appropriate sense so that the optimization over this restricted family of contracts represents no loss of generality. The result is obtained by using the recent well-posedness result of random horizon second-order backward SDE."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Random Horizon Principal-Agent problem, what is the main result of the paper, and how does it relate to the reduction of non-zero-sum stochastic differential games to stochastic control problems?\n\nA) The main result is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by using the recent well-posedness result of random horizon second-order backward SDE.\n\nB) The main result is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by following Sannikov's approach, further developed by Cvitanic, Possamai, and Touzi.\n\nC) The main result is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by using the recent well-posedness result of random horizon second-order backward SDE, which is a special case of the general approach.\n\nD) The main result is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by following Sannikov's approach, further developed by Cvitanic, Possamai, and Touzi, which is a general approach that covers both the European and American versions of the contract.\n\n**Correct Answer:** B) The main result is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by following Sannikov's approach, further developed by Cvitanic, Possamai, and Touzi.\n\n**Explanation:** The correct answer is B) because the main result of the paper is that the agent's optimal effort can be characterized by a standard verification argument in stochastic control theory, and this result is obtained by following Sannikov's approach, further developed by Cvitanic, Possamai, and Touzi. This approach is a general one that covers both the European and American versions of the contract, and it is the main contribution of the paper. The other options are incorrect because they either omit the general approach or the recent well-posedness result of random horizon second-order backward SDE."}, "4": {"documentation": {"title": "Efficiency of the Price Formation Process in Presence of High Frequency\n  Participants: a Mean Field Game analysis", "source": "Aim\\'e Lachapelle, Jean-Michel Lasry, Charles-Albert Lehalle,\n  Pierre-Louis Lions", "docs_id": "1305.6323", "section": ["q-fin.TR", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficiency of the Price Formation Process in Presence of High Frequency\n  Participants: a Mean Field Game analysis. This paper deals with a stochastic order-driven market model with waiting costs, for order books with heterogenous traders. Offer and demand of liquidity drives price formation and traders anticipate future evolutions of the order book. The natural framework we use is mean field game theory, a class of stochastic differential games with a continuum of anonymous players. Several sources of heterogeneity are considered including the mean size of orders. Thus we are able to consider the coexistence of Institutional Investors and High Frequency Traders (HFT). We provide both analytical solutions and numerical experiments. Implications on classical quantities are explored: order book size, prices, and effective bid/ask spread. According to the model, in markets with Institutional Investors only we show the existence of inefficient liquidity imbalances in equilibrium, with two symmetrical situations corresponding to what we call liquidity calls for liquidity. During these situations the transaction price significantly moves away from the fair price. However this macro phenomenon disappears in markets with both Institutional Investors and HFT, although a more precise study shows that the benefits of the new situation go to HFT only, leaving Institutional Investors even with higher trading costs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Mean Field Game analysis of the price formation process in a stochastic order-driven market model with waiting costs, what is the implication of the coexistence of Institutional Investors and High Frequency Traders (HFT) on the existence of inefficient liquidity imbalances in equilibrium?\n\n**A)** The coexistence of Institutional Investors and HFT leads to the existence of inefficient liquidity imbalances in equilibrium, with Institutional Investors bearing the costs.\n\n**B)** The coexistence of Institutional Investors and HFT eliminates the existence of inefficient liquidity imbalances in equilibrium, with HFT benefiting from the new situation.\n\n**C)** The coexistence of Institutional Investors and HFT leads to the emergence of two symmetrical situations, known as liquidity calls for liquidity, where the transaction price significantly moves away from the fair price.\n\n**D)** The coexistence of Institutional Investors and HFT has no impact on the existence of inefficient liquidity imbalances in equilibrium.\n\n**Correct Answer:** B) The coexistence of Institutional Investors and HFT eliminates the existence of inefficient liquidity imbalances in equilibrium, with HFT benefiting from the new situation.\n\n**Explanation:** According to the paper, in markets with both Institutional Investors and HFT, the macro phenomenon of inefficient liquidity imbalances in equilibrium disappears. However, a more precise study shows that the benefits of this new situation go to HFT, leaving Institutional Investors with even higher trading costs. This implies that the coexistence of Institutional Investors and HFT eliminates the existence of inefficient liquidity imbalances in equilibrium, with HFT benefiting from the new situation."}, "5": {"documentation": {"title": "Mechanical bound states in the continuum for macroscopic optomechanics", "source": "Mengdi Zhao and Kejie Fang", "docs_id": "1902.00614", "section": ["physics.optics", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mechanical bound states in the continuum for macroscopic optomechanics. Bound states in the continuum (BICs), an emerging type of long-lived resonances different from the cavity-based ones, have been explored in several classical systems, including photonic crystals and surface acoustic waves. Here, we reveal symmetry-protected mechanical BICs in the structure of slab-on-substrate optomechanical crystals. Using a group theory approach, we identified all the mechanical BICs at the $\\Gamma$ point in optomechanical crystals with $C_{4v}$ and $C_{6v}$ symmetries as examples, and analyzed their coupling with the co-localized optical BICs and guided resonances due to both moving boundary and photo-elastic effects. We verified the theoretical analysis with numerical simulations of specific optomechanical crystals which support substantial optomechanical interactions between the mechanical BICs and optical resonances. Due to the unique features of high-$Q$, large-size mechanical BICs and substrate-enabled thermal dissipation, this architecture of slab-on-substrate optomechanical crystals might be useful for exploring macroscopic quantum mechanical physics and enabling new applications such as high-throughput sensing and free-space beam steering."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the slab-on-substrate optomechanical crystal architecture in enabling macroscopic quantum mechanical physics, as suggested by the study on mechanical bound states in the continuum?\n\nA) High optical absorption due to moving boundary effects\nB) Large-size mechanical BICs with high-Q factor and substrate-enabled thermal dissipation\nC) Enhanced optomechanical interactions between mechanical BICs and optical resonances due to photo-elastic effects\nD) Increased mechanical stiffness due to the presence of the substrate\n\n**Correct Answer:** B) Large-size mechanical BICs with high-Q factor and substrate-enabled thermal dissipation\n\n**Explanation:** The study highlights the unique features of the slab-on-substrate optomechanical crystal architecture, including the presence of high-Q, large-size mechanical BICs and substrate-enabled thermal dissipation. These features are crucial for enabling macroscopic quantum mechanical physics and opening up new applications such as high-throughput sensing and free-space beam steering. The correct answer, B, is the primary advantage of this architecture, as it allows for the exploration of macroscopic quantum mechanical physics.\n\n**Candidate A:** High optical absorption due to moving boundary effects is not the primary advantage of the slab-on-substrate optomechanical crystal architecture. While moving boundary effects are mentioned in the study, they are not the primary benefit.\n\n**Candidate B:** Large-size mechanical BICs with high-Q factor and substrate-enabled thermal dissipation is the correct answer, as explained above.\n\n**Candidate C:** Enhanced optomechanical interactions between mechanical BICs and optical resonances due to photo-elastic effects is not the primary advantage of the slab-on-substrate optomechanical crystal architecture. While photo-elastic effects are mentioned in the study, they are not the primary benefit.\n\n**Candidate D:** Increased mechanical stiffness due to the presence of the substrate is not the primary advantage of the slab-on-substrate optomechanical crystal architecture. The study does not mention increased mechanical stiffness as a benefit of this architecture."}, "6": {"documentation": {"title": "Green thermoelectrics: Observation and analysis of plant thermoelectric\n  response", "source": "C. Goupil, H. Ouerdane, A. Khamsing, Y. Apertet, F. Bouteau, S.\n  Mancuso, R. Patino, and Ph. Lecoeur", "docs_id": "1502.04613", "section": ["physics.bio-ph", "cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Green thermoelectrics: Observation and analysis of plant thermoelectric\n  response. Plants are sensitive to thermal and electrical effects; yet the coupling of both, known as thermoelectricity, and its quantitative measurement in vegetal systems never were reported. We recorded the thermoelectric response of bean sprouts under various thermal conditions and stress. The obtained experimental data unambiguously demonstrate that a temperature difference between the roots and the leaves of a bean sprout induces a thermoelectric voltage between these two points. Basing our analysis of the data on the force-flux formalism of linear response theory, we found that the strength of the vegetal equivalent to the thermoelectric coupling is one order of magnitude larger than that in the best thermoelectric materials. Experimental data also show the importance of the thermal stress variation rate in the plant's electrophysiological response. Therefore, thermoelectric effects are sufficiently important to partake in the complex and intertwined processes of energy and matter transport within plants."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the thermoelectric response of bean sprouts under various thermal conditions and stress, as reported in the study \"Green thermoelectrics: Observation and analysis of plant thermoelectric response\". What is the key finding regarding the strength of the thermoelectric coupling in plant systems, and how does it compare to that in the best thermoelectric materials?\n\n**A)** The study found that the thermoelectric coupling in plant systems is significantly weaker than that in the best thermoelectric materials, and that the strength of the coupling is inversely proportional to the thermal stress variation rate.\n\n**B)** The study found that the thermoelectric coupling in plant systems is one order of magnitude stronger than that in the best thermoelectric materials, and that the strength of the coupling is directly proportional to the thermal stress variation rate.\n\n**C)** The study found that the thermoelectric coupling in plant systems is comparable to that in the best thermoelectric materials, and that the strength of the coupling is independent of the thermal stress variation rate.\n\n**D)** The study found that the thermoelectric coupling in plant systems is significantly weaker than that in the best thermoelectric materials, and that the strength of the coupling is directly proportional to the thermal stress variation rate.\n\n**Correct Answer:** B) The study found that the thermoelectric coupling in plant systems is one order of magnitude stronger than that in the best thermoelectric materials, and that the strength of the coupling is directly proportional to the thermal stress variation rate.\n\n**Explanation:** The correct answer is based on the statement in the documentation: \"Basing our analysis of the data on the force-flux formalism of linear response theory, we found that the strength of the vegetal equivalent to the thermoelectric coupling is one order of magnitude larger than that in the best thermoelectric materials.\" This indicates that the thermoelectric coupling in plant systems is stronger than that in the best thermoelectric materials. Additionally, the documentation states that \"Experimental data also show the importance of the thermal stress variation rate in the plant's electrophysiological response\", which suggests that the strength of the coupling is directly proportional to the thermal stress variation rate."}, "7": {"documentation": {"title": "Optimal execution with dynamic risk adjustment", "source": "Xue Cheng, Marina Di Giacinto, and Tai-Ho Wang", "docs_id": "1901.00617", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal execution with dynamic risk adjustment. This paper considers the problem of optimal liquidation of a position in a risky security in a financial market, where price evolution are risky and trades have an impact on price as well as uncertainty in the filling orders. The problem is formulated as a continuous time stochastic optimal control problem aiming at maximizing a generalized risk-adjusted profit and loss function. The expression of the risk adjustment is derived from the general theory of dynamic risk measures and is selected in the class of $g$-conditional risk measures. The resulting theoretical framework is nonclassical since the target function depends on backward components. We show that, under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed form solution and an explicit expression of the optimal liquidation policies. In this way it is immediate to quantify the impact of risk-adjustment on the profit and loss and on the expression of the optimal liquidation policies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a financial market with a risky security, where the price evolution is governed by a stochastic differential equation. The liquidation of a position in this security is subject to uncertainty in the filling of orders and has an impact on the price. The goal is to formulate the problem as a continuous-time stochastic optimal control problem, aiming to maximize a generalized risk-adjusted profit and loss function.\n\n**A)** Formulate the problem as a stochastic optimal control problem, where the state variable is the current price of the security, the control variable is the amount of the security to be liquidated, and the objective function is to maximize the expected value of the risk-adjusted profit and loss.\n\n**B)** Derive the expression of the risk adjustment using the general theory of dynamic risk measures and select it from the class of $g$-conditional risk measures. Explain the implications of this selection on the resulting theoretical framework.\n\n**C)** Show that, under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed-form solution and an explicit expression of the optimal liquidation policies. Explain the significance of this result in terms of quantifying the impact of risk-adjustment on the profit and loss and on the expression of the optimal liquidation policies.\n\n**D)** Discuss the nonclassical nature of the resulting theoretical framework, where the target function depends on backward components. Explain how this nonclassical nature affects the formulation and solution of the problem.\n\n**Correct Answer:** C) Show that, under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed-form solution and an explicit expression of the optimal liquidation policies. Explain the significance of this result in terms of quantifying the impact of risk-adjustment on the profit and loss and on the expression of the optimal liquidation policies.\n\n**Explanation:**\n\nThe correct answer is C) because the paper specifically states that under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed-form solution and an explicit expression of the optimal liquidation policies. This result is significant because it allows for the quantification of the impact of risk-adjustment on the profit and loss and on the expression of the optimal liquidation policies, which is a key aspect of the problem.\n\nThe other options are incorrect because:\n\n* A) Formulating the problem as a stochastic optimal control problem is a necessary step, but it is not the main result of the paper.\n* B) Deriving the expression of the risk adjustment is important, but it is not the main focus of the paper.\n* D) Discussing the nonclassical nature of the resulting theoretical framework is relevant, but it is not the main result of the paper."}, "8": {"documentation": {"title": "Enumerating all maximal biclusters in numerical datasets", "source": "Rosana Veroneze, Arindam Banerjee and Fernando J. Von Zuben", "docs_id": "1403.3562", "section": ["cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enumerating all maximal biclusters in numerical datasets. Biclustering has proved to be a powerful data analysis technique due to its wide success in various application domains. However, the existing literature presents efficient solutions only for enumerating maximal biclusters with constant values, or heuristic-based approaches which can not find all biclusters or even support the maximality of the obtained biclusters. Here, we present a general family of biclustering algorithms for enumerating all maximal biclusters with (i) constant values on rows, (ii) constant values on columns, or (iii) coherent values. Versions for perfect and for perturbed biclusters are provided. Our algorithms have four key properties (just the algorithm for perturbed biclusters with coherent values fails to exhibit the first property): they are (1) efficient (take polynomial time per pattern), (2) complete (find all maximal biclusters), (3) correct (all biclusters attend the user-defined measure of similarity), and (4) non-redundant (all the obtained biclusters are maximal and the same bicluster is not enumerated twice). They are based on a generalization of an efficient formal concept analysis algorithm called In-Close2. Experimental results point to the necessity of having efficient enumerative biclustering algorithms and provide a valuable insight into the scalability of our family of algorithms and its sensitivity to user-defined parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the four key properties of the proposed biclustering algorithms for enumerating all maximal biclusters, as stated in the paper \"Enumerating all maximal biclusters in numerical datasets\"?\n\nA) Efficient, complete, correct, and redundant\nB) Efficient, complete, correct, and non-redundant\nC) Efficient, complete, incorrect, and redundant\nD) Efficient, incomplete, correct, and non-redundant\n\n**Correct Answer:** B) Efficient, complete, correct, and non-redundant\n\n**Explanation:** The paper states that the proposed algorithms have four key properties: they are efficient (take polynomial time per pattern), complete (find all maximal biclusters), correct (all biclusters attend the user-defined measure of similarity), and non-redundant (all the obtained biclusters are maximal and the same bicluster is not enumerated twice). Option B correctly lists these four properties. Options A, C, and D are incorrect because they either omit one of the properties or incorrectly state the properties."}, "9": {"documentation": {"title": "Numerical approximation for fractional diffusion equation forced by a\n  tempered fractional Gaussian noise", "source": "Xing Liu, Weihua Deng", "docs_id": "1912.06990", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical approximation for fractional diffusion equation forced by a\n  tempered fractional Gaussian noise. This paper discusses the fractional diffusion equation forced by a tempered fractional Gaussian noise. The fractional diffusion equation governs the probability density function of the subordinated killed Brownian motion. The tempered fractional Gaussian noise plays the role of fluctuating external source with the property of localization. We first establish the regularity of the infinite dimensional stochastic integration of the tempered fractional Brownian motion and then build the regularity of the mild solution of the fractional stochastic diffusion equation. The spectral Galerkin method is used for space approximation; after that the system is transformed into an equivalent form having better regularity than the original one in time. Then we use the semi-implicit Euler scheme to discretize the time derivative. In terms of the temporal-spatial error splitting technique, we obtain the error estimates of the fully discrete scheme in the sense of mean-squared $L^2$-norm. Extensive numerical experiments confirm the theoretical estimates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the fractional diffusion equation forced by a tempered fractional Gaussian noise, what is the primary goal of the spectral Galerkin method used for space approximation, and how does it contribute to the overall regularity of the mild solution of the fractional stochastic diffusion equation?\n\nA) To reduce the dimensionality of the infinite-dimensional stochastic integration, thereby improving the stability of the numerical scheme.\nB) To introduce a regularization technique that enhances the accuracy of the numerical solution, particularly in the spatial domain.\nC) To transform the original equation into an equivalent form that exhibits better regularity properties, facilitating the application of numerical methods.\nD) To employ a spectral decomposition technique that allows for the efficient computation of the tempered fractional Gaussian noise.\n\n**Correct Answer:** C) To transform the original equation into an equivalent form that exhibits better regularity properties, facilitating the application of numerical methods.\n\n**Explanation:** The spectral Galerkin method is used for space approximation to transform the original equation into an equivalent form that exhibits better regularity properties. This is achieved by decomposing the tempered fractional Gaussian noise into its spectral components, which allows for the application of numerical methods that can handle the resulting equation with improved regularity. This transformation is crucial in establishing the regularity of the mild solution of the fractional stochastic diffusion equation, as it enables the use of numerical schemes that can accurately capture the behavior of the solution."}, "10": {"documentation": {"title": "Influence of defects on the critical behaviour at the \\boldmath{105} K\n  structural phase transition of SrTiO$_3$: I. The broad component", "source": "H. H\\\"unnefeld, T. Niem\\\"oller, J. R. Schneider, B. A. Kaufmann and F.\n  Schwabl", "docs_id": "cond-mat/0006127", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Influence of defects on the critical behaviour at the \\boldmath{105} K\n  structural phase transition of SrTiO$_3$: I. The broad component. The critical fluctuations in SrTiO$_3$ near its $105 K$ structural phase transition were studied with triple axis diffractometry using high energy ($\\ge 100 keV$) synchrotron radiation in different SrTiO$_3$ crystals with different oxygen vacancy concentrations. Due to the presence of oxygen vacancies the critical behaviour is changed compared to defect-free systems. In our experiments a smearing out of the squared order parameter and a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature is observed, with the crossover temperature strongly depending on the concentration of the defects. To understand the experimental findings, e.g. the unusual values for the critical exponents found near the critical temperature, the Ginzburg-Landau-Wilson functional for structural phase transitions in disordered systems was analyzed using renormalization group theory and the replica trick. Considering the effects of defects which locally increase the transition temperature leads to a qualitative understanding of the observed behaviour. The crossover behaviour of the critical exponents can be modeled and a quantitative analysis of the observed experimental data is presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the influence of oxygen vacancies on the critical behavior of SrTiO$_3$ near its 105 K structural phase transition, using the Ginzburg-Landau-Wilson functional and renormalization group theory. How do the results of this analysis account for the observed smearing out of the squared order parameter and crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature?\n\n**A)** The presence of oxygen vacancies leads to a qualitative change in the critical behavior of SrTiO$_3$, resulting in a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature. This crossover is a direct consequence of the increased transition temperature caused by the defects.\n\n**B)** The Ginzburg-Landau-Wilson functional and renormalization group theory provide a quantitative analysis of the observed experimental data, which shows that the critical behavior of SrTiO$_3$ near its 105 K structural phase transition is influenced by the concentration of oxygen vacancies. The smearing out of the squared order parameter is a result of the defects' effect on the critical exponents.\n\n**C)** The crossover behavior of the critical exponents can be modeled using the replica trick, which allows for a qualitative understanding of the observed behavior. However, the quantitative analysis of the experimental data requires a more detailed understanding of the effects of oxygen vacancies on the critical behavior of SrTiO$_3$.\n\n**D)** The results of the analysis suggest that the critical behavior of SrTiO$_3$ near its 105 K structural phase transition is influenced by the concentration of oxygen vacancies, leading to a smearing out of the squared order parameter and a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature. This is a direct consequence of the increased transition temperature caused by the defects.\n\n**Correct Answer:** D) The results of the analysis suggest that the critical behavior of SrTiO$_3$ near its 105 K structural phase transition is influenced by the concentration of oxygen vacancies, leading to a smearing out of the squared order parameter and a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature. This is a direct consequence of the increased transition temperature caused by the defects.\n\n**Explanation:** The correct answer is D) because it accurately summarizes the main points of the analysis. The presence of oxygen vacancies leads to a qualitative change in the critical behavior of SrTiO$_3$, resulting in a crossover of the critical exponents $\\nu$ and $\\gamma$ above the phase transition temperature. This crossover is a direct consequence of the increased transition temperature caused by the defects. The other options are incorrect because they either provide a qualitative understanding of the behavior (A and C) or a quantitative analysis of the data (B), but do not accurately summarize the main points of the analysis."}, "11": {"documentation": {"title": "Theoretical Guarantees for Model Auditing with Finite Adversaries", "source": "Mario Diaz and Peter Kairouz and Jiachun Liao and Lalitha Sankar", "docs_id": "1911.03405", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical Guarantees for Model Auditing with Finite Adversaries. Privacy concerns have led to the development of privacy-preserving approaches for learning models from sensitive data. Yet, in practice, even models learned with privacy guarantees can inadvertently memorize unique training examples or leak sensitive features. To identify such privacy violations, existing model auditing techniques use finite adversaries defined as machine learning models with (a) access to some finite side information (e.g., a small auditing dataset), and (b) finite capacity (e.g., a fixed neural network architecture). Our work investigates the requirements under which an unsuccessful attempt to identify privacy violations by a finite adversary implies that no stronger adversary can succeed at such a task. We do so via parameters that quantify the capabilities of the finite adversary, including the size of the neural network employed by such an adversary and the amount of side information it has access to as well as the regularity of the (perhaps privacy-guaranteeing) audited model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the key parameters that quantify the capabilities of a finite adversary in model auditing, and how do they relate to the success of the adversary in identifying privacy violations?\n\n**A)** The size of the neural network employed by the adversary, the amount of side information it has access to, and the strength of the regularization applied to the audited model.\n\n**B)** The capacity of the finite adversary, the amount of side information it has access to, and the type of neural network architecture used by the audited model.\n\n**C)** The size of the auditing dataset, the amount of side information the adversary has access to, and the type of regularization applied to the audited model.\n\n**D)** The capacity of the finite adversary, the amount of side information it has access to, and the type of neural network architecture used by the audited model.\n\n**Correct Answer:** D) The capacity of the finite adversary, the amount of side information it has access to, and the type of neural network architecture used by the audited model.\n\n**Explanation:** The correct answer is D) because the parameters that quantify the capabilities of the finite adversary are indeed the capacity of the finite adversary, the amount of side information it has access to, and the type of neural network architecture used by the audited model. These parameters are mentioned in the provided documentation as the key factors that determine the success of the finite adversary in identifying privacy violations. The other options are incorrect because they either omit one or more of these parameters or introduce irrelevant factors."}, "12": {"documentation": {"title": "Stochastic model of financial markets reproducing scaling and memory in\n  volatility return intervals", "source": "Vygintas Gontis, Shlomo Havlin, Aleksejus Kononovicius, Boris\n  Podobnik, H. Eugene Stanley", "docs_id": "1507.05203", "section": ["q-fin.GN", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stochastic model of financial markets reproducing scaling and memory in\n  volatility return intervals. We investigate the volatility return intervals in the NYSE and FOREX markets. We explain previous empirical findings using a model based on the interacting agent hypothesis instead of the widely-used efficient market hypothesis. We derive macroscopic equations based on the microscopic herding interactions of agents and find that they are able to reproduce various stylized facts of different markets and different assets with the same set of model parameters. We show that the power-law properties and the scaling of return intervals and other financial variables have a similar origin and could be a result of a general class of non-linear stochastic differential equations derived from a master equation of an agent system that is coupled by herding interactions. Specifically, we find that this approach enables us to recover the volatility return interval statistics as well as volatility probability and spectral densities for the NYSE and FOREX markets, for different assets, and for different time-scales. We find also that the historical S\\&P500 monthly series exhibits the same volatility return interval properties recovered by our proposed model. Our statistical results suggest that human herding is so strong that it persists even when other evolving fluctuations perturbate the financial system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Stochastic model of financial markets reproducing scaling and memory in volatility return intervals\" propose a model based on the interacting agent hypothesis to explain the stylized facts of financial markets. What is the primary assumption underlying this model, and how does it differ from the efficient market hypothesis?\n\nA) The model assumes that agents are rational and make decisions based on expected returns, whereas the efficient market hypothesis assumes that agents are risk-averse and make decisions based on expected utility. The proposed model differs from the efficient market hypothesis in that it assumes that agents are driven by herding behavior, rather than rational decision-making.\n\nB) The model assumes that agents are driven by herding behavior, whereas the efficient market hypothesis assumes that agents are driven by risk aversion. The proposed model differs from the efficient market hypothesis in that it assumes that agents are driven by herding behavior, rather than risk aversion.\n\nC) The model assumes that agents are driven by risk aversion, whereas the efficient market hypothesis assumes that agents are driven by herding behavior. The proposed model differs from the efficient market hypothesis in that it assumes that agents are driven by risk aversion, rather than herding behavior.\n\nD) The model assumes that agents are driven by herding behavior, whereas the efficient market hypothesis assumes that agents are driven by a combination of risk aversion and herding behavior. The proposed model differs from the efficient market hypothesis in that it assumes that agents are driven solely by herding behavior, rather than a combination of factors.\n\n**Correct Answer:** A) The model assumes that agents are rational and make decisions based on expected returns, whereas the efficient market hypothesis assumes that agents are risk-averse and make decisions based on expected utility. The proposed model differs from the efficient market hypothesis in that it assumes that agents are driven by herding behavior, rather than rational decision-making.\n\n**Explanation:** The proposed model assumes that agents are driven by herding behavior, rather than rational decision-making. This is in contrast to the efficient market hypothesis, which assumes that agents are rational and make decisions based on expected returns. The authors of the paper argue that the proposed model is able to reproduce the stylized facts of financial markets, including the power-law properties and scaling of return intervals, by incorporating herding behavior into the model."}, "13": {"documentation": {"title": "From Light Nuclei to Nuclear Matter. The Role of Relativity?", "source": "F. Coester", "docs_id": "nucl-th/0111025", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "From Light Nuclei to Nuclear Matter. The Role of Relativity?. The success of non-relativistic quantum dynamics in accounting for the binding energies and spectra of light nuclei with masses up to A=10 raises the question whether the same dynamics applied to infinite nuclear matter agrees with the empirical saturation properties of large nuclei.The simple unambiguous relation between few-nucleon and many-nucleon Hamiltonians is directly related to the Galilean covariance of nonrelativistic dynamics. Relations between the irreducible unitary representations of the Galilei and Poincare groups indicate thatthe ``nonrelativistic'' nuclear Hamiltonians may provide sufficiently accurate approximations to Poincare invariant mass operators. In relativistic nuclear dynamics based on suitable Lagrangeans the intrinsic nucleon parity is an explicit, dynamically relevant, degree of freedom and the emphasis is on properties of nuclear matter. The success of this approach suggests the question how it might account for the spectral properties of light nuclei."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary concern regarding the application of non-relativistic quantum dynamics to infinite nuclear matter, and how does it relate to the empirical saturation properties of large nuclei?\n\nA) The non-relativistic dynamics may not account for the relativistic effects that become significant at high densities, leading to a breakdown in the simple unambiguous relation between few-nucleon and many-nucleon Hamiltonians.\n\nB) The non-relativistic dynamics may not be able to capture the intrinsic nucleon parity, which is an explicit, dynamically relevant degree of freedom in relativistic nuclear dynamics.\n\nC) The non-relativistic dynamics may not be able to account for the spectral properties of light nuclei, which are influenced by the relativistic effects that become significant at high densities.\n\nD) The non-relativistic dynamics may not be able to capture the Poincare invariance of the nuclear Hamiltonians, which is a key feature of relativistic nuclear dynamics.\n\nCorrect Answer: C) The non-relativistic dynamics may not be able to account for the spectral properties of light nuclei, which are influenced by the relativistic effects that become significant at high densities.\n\nExplanation: The question is based on the idea that the success of non-relativistic quantum dynamics in accounting for the binding energies and spectra of light nuclei with masses up to A=10 raises the question whether the same dynamics applied to infinite nuclear matter agrees with the empirical saturation properties of large nuclei. The correct answer, C, is based on the idea that the non-relativistic dynamics may not be able to account for the spectral properties of light nuclei, which are influenced by the relativistic effects that become significant at high densities. This is a key concern in the context of the provided documentation, which discusses the role of relativity in nuclear dynamics."}, "14": {"documentation": {"title": "Multi-modality fusion using canonical correlation analysis methods:\n  Application in breast cancer survival prediction from histology and genomics", "source": "Vaishnavi Subramanian, Tanveer Syeda-Mahmood, and Minh N. Do", "docs_id": "2111.13987", "section": ["cs.LG", "eess.SP", "q-bio.QM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-modality fusion using canonical correlation analysis methods:\n  Application in breast cancer survival prediction from histology and genomics. The availability of multi-modality datasets provides a unique opportunity to characterize the same object of interest using multiple viewpoints more comprehensively. In this work, we investigate the use of canonical correlation analysis (CCA) and penalized variants of CCA (pCCA) for the fusion of two modalities. We study a simple graphical model for the generation of two-modality data. We analytically show that, with known model parameters, posterior mean estimators that jointly use both modalities outperform arbitrary linear mixing of single modality posterior estimators in latent variable prediction. Penalized extensions of CCA (pCCA) that incorporate domain knowledge can discover correlations with high-dimensional, low-sample data, whereas traditional CCA is inapplicable. To facilitate the generation of multi-dimensional embeddings with pCCA, we propose two matrix deflation schemes that enforce desirable properties exhibited by CCA. We propose a two-stage prediction pipeline using pCCA embeddings generated with deflation for latent variable prediction by combining all the above. On simulated data, our proposed model drastically reduces the mean-squared error in latent variable prediction. When applied to publicly available histopathology data and RNA-sequencing data from The Cancer Genome Atlas (TCGA) breast cancer patients, our model can outperform principal components analysis (PCA) embeddings of the same dimension in survival prediction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of multi-modality fusion using canonical correlation analysis (CCA) methods, what is the primary advantage of using penalized variants of CCA (pCCA) over traditional CCA, and how do the proposed matrix deflation schemes facilitate the generation of desirable properties exhibited by CCA?\n\n**A)** pCCA is more computationally efficient than traditional CCA, and the matrix deflation schemes are unnecessary.\n**B)** pCCA can discover correlations with high-dimensional, low-sample data, and the matrix deflation schemes enforce desirable properties exhibited by CCA, such as linear independence and orthogonality.\n**C)** pCCA is more prone to overfitting than traditional CCA, and the matrix deflation schemes are used to reduce the dimensionality of the data.\n**D)** pCCA is only applicable to specific types of data, and the matrix deflation schemes are used to improve the interpretability of the results.\n\n**Correct Answer:** B) pCCA can discover correlations with high-dimensional, low-sample data, and the matrix deflation schemes enforce desirable properties exhibited by CCA, such as linear independence and orthogonality.\n\n**Explanation:** The correct answer is B) because the documentation states that pCCA can discover correlations with high-dimensional, low-sample data, and that the proposed matrix deflation schemes enforce desirable properties exhibited by CCA, such as linear independence and orthogonality. This is a key advantage of using pCCA over traditional CCA, as it allows for the discovery of correlations in high-dimensional data and the generation of embeddings with desirable properties."}, "15": {"documentation": {"title": "Collective strategy condensation: When envy splits societies", "source": "Claudius Gros", "docs_id": "2101.10824", "section": ["physics.soc-ph", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Collective strategy condensation: When envy splits societies. Human societies are characterized, besides others, by three constituent features. (A) Options, as for jobs and societal positions, differ with respect to their associated monetary and non-monetary payoffs. (B) Competition leads to reduced payoffs when individuals compete for the same option with others. (C) People care how they are doing relatively to others. The latter trait, the propensity to compare one's own success with that of others, expresses itself as envy. It is shown that the combination of (A)-(C) leads to spontaneous class stratification. Societies of agents split endogenously into two social classes, an upper and a lower class, when envy becomes relevant. A comprehensive analysis of the Nash equilibria characterizing a basic reference game is presented. Class separation is due to the condensation of the strategies of lower-class agents, which play an identical mixed strategy. Upper class agents do not condense, following individualist pure strategies. Model and results are size-consistent, holding for arbitrary large numbers of agents and options. Analytic results are confirmed by extensive numerical simulations. An analogy to interacting confined classical particles is discussed."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the Collective Strategy Condensation model, what is the primary mechanism driving the emergence of class stratification in human societies?\n\nA) The propensity for individuals to engage in cooperative behavior, leading to the formation of a single, homogeneous class.\nB) The tendency for individuals to compare their own success with that of others, resulting in the condensation of strategies among lower-class agents.\nC) The influence of external factors, such as economic inequality, on the formation of social classes.\nD) The role of social norms and cultural values in shaping individual behavior and class affiliation.\n\n**Correct Answer:** B) The tendency for individuals to compare their own success with that of others, resulting in the condensation of strategies among lower-class agents.\n\n**Explanation:** The correct answer is based on the idea that envy, or the propensity for individuals to compare their own success with that of others, is a key driver of class stratification in the Collective Strategy Condensation model. According to the model, when individuals compare their own success with that of others, they may feel envious of those who are doing better, leading to a condensation of strategies among lower-class agents. This condensation results in the emergence of a single, homogeneous class, while upper-class agents continue to follow individualist pure strategies. The other options are incorrect because they do not accurately capture the primary mechanism driving class stratification in the model."}, "16": {"documentation": {"title": "Positively-homogeneous Konus-Divisia indices and their applications to\n  demand analysis and forecasting", "source": "Nikolay Klemashev, Alexander Shananin", "docs_id": "1501.05771", "section": ["math.OC", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Positively-homogeneous Konus-Divisia indices and their applications to\n  demand analysis and forecasting. This paper is devoted to revealed preference theory and its applications to testing economic data for consistency with utility maximization hypothesis, construction of index numbers, and forecasting. The quantitative measures of inconsistency of economic data with utility maximization behavior are also discussed. The structure of the paper is based on comparison between the two tests of revealed preference theory - generalized axiom of revealed preference (GARP) and homothetic axiom of revealed prefernce (HARP). We do this comparison both theoretically and empirically. In particular we assess empirically the power of these tests for consistency with maximization behavior and the size of forecasting sets based on them. For the forecasting problem we show that when using HARP there is an effective way of building the forecasting set since this set is given by the solution of the system of linear inequalities. The paper also touches upon the question of testing a set of Engel curves rather than finite set of observations for consistency with utility maximization behavior and shows that this question has effective solution when we require the rationalizing utility function to be positively homogeneous."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of revealed preference theory, compare the power of the Generalized Axiom of Revealed Preference (GARP) and the Homothetic Axiom of Revealed Preference (HARP) tests for consistency with utility maximization behavior, and discuss the implications of these tests for forecasting demand analysis.\n\n**A)** GARP has a higher power than HARP for testing consistency with utility maximization behavior, but HARP provides a more effective way of building forecasting sets due to its linear inequality structure.\n\n**B)** HARP has a higher power than GARP for testing consistency with utility maximization behavior, but GARP is more suitable for forecasting demand analysis due to its ability to handle non-linear relationships.\n\n**C)** Both GARP and HARP have the same power for testing consistency with utility maximization behavior, but HARP is more suitable for forecasting demand analysis due to its linear inequality structure.\n\n**D)** GARP is more suitable for testing consistency with utility maximization behavior, but HARP is more suitable for forecasting demand analysis due to its ability to handle non-linear relationships.\n\n**Correct Answer:** A) GARP has a higher power than HARP for testing consistency with utility maximization behavior, but HARP provides a more effective way of building forecasting sets due to its linear inequality structure.\n\n**Explanation:** The correct answer is A) because the paper states that GARP has a higher power than HARP for testing consistency with utility maximization behavior, but HARP provides an effective way of building forecasting sets due to its linear inequality structure. This is a key finding of the paper, and the correct answer requires the test-taker to carefully read and understand the comparison between GARP and HARP."}, "17": {"documentation": {"title": "Semi-Poisson statistics in quantum chaos", "source": "Antonio M. Garcia-Garcia, Jiao Wang", "docs_id": "cond-mat/0511171", "section": ["cond-mat.dis-nn", "cond-mat.mes-hall", "hep-th", "nlin.CD", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semi-Poisson statistics in quantum chaos. We investigate the quantum properties of a non-random Hamiltonian with a step-like singularity. It is shown that the eigenfunctions are multifractals and, in a certain range of parameters, the level statistics is described exactly by Semi-Poisson statistics (SP) typical of pseudo-integrable systems. It is also shown that our results are universal; namely, they depend exclusively on the presence of the step-like singularity and are not modified by smooth perturbations of the potential or the addition of a magnetic flux. Although the quantum properties of our system are similar to those of a disordered conductor at the Anderson transition, we report important quantitative differences in both the level statistics and the multifractal dimensions controlling the transition. Finally the study of quantum transport properties suggests that the classical singularity induces quantum anomalous diffusion. We discuss how these findings may be experimentally corroborated by using ultra cold atoms techniques."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key implications of the study on the quantum properties of a non-random Hamiltonian with a step-like singularity, and how do they differ from those of a disordered conductor at the Anderson transition?\n\nA) The study suggests that the quantum properties of the system are identical to those of a disordered conductor at the Anderson transition, and that the presence of a step-like singularity has no impact on the level statistics and multifractal dimensions.\n\nB) The study reveals that the quantum properties of the system are similar to those of a disordered conductor at the Anderson transition, but that the presence of a step-like singularity induces quantum anomalous diffusion, leading to important quantitative differences in the level statistics and multifractal dimensions controlling the transition.\n\nC) The study demonstrates that the quantum properties of the system are universal and depend exclusively on the presence of the step-like singularity, and that smooth perturbations of the potential or the addition of a magnetic flux do not modify the results.\n\nD) The study suggests that the quantum properties of the system are identical to those of a pseudo-integrable system, and that the presence of a step-like singularity leads to a transition to a new regime of quantum behavior characterized by Semi-Poisson statistics.\n\nCorrect Answer: B) The study reveals that the quantum properties of the system are similar to those of a disordered conductor at the Anderson transition, but that the presence of a step-like singularity induces quantum anomalous diffusion, leading to important quantitative differences in the level statistics and multifractal dimensions controlling the transition.\n\nExplanation: The correct answer is B) because the study highlights the similarities between the quantum properties of the system and those of a disordered conductor at the Anderson transition, but also notes the presence of quantum anomalous diffusion induced by the step-like singularity, leading to differences in the level statistics and multifractal dimensions. This requires the test-taker to carefully read and understand the nuances of the study's findings."}, "18": {"documentation": {"title": "Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline\n  Optimization", "source": "Davis M. Vigneault, Weidi Xie, David A. Bluemke, and J. Alison Noble", "docs_id": "1704.03660", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline\n  Optimization. Feature tracking Cardiac Magnetic Resonance (CMR) has recently emerged as an area of interest for quantification of regional cardiac function from balanced, steady state free precession (SSFP) cine sequences. However, currently available techniques lack full automation, limiting reproducibility. We propose a fully automated technique whereby a CMR image sequence is first segmented with a deep, fully convolutional neural network (CNN) architecture, and quadratic basis splines are fitted simultaneously across all cardiac frames using least squares optimization. Experiments are performed using data from 42 patients with hypertrophic cardiomyopathy (HCM) and 21 healthy control subjects. In terms of segmentation, we compared state-of-the-art CNN frameworks, U-Net and dilated convolution architectures, with and without temporal context, using cross validation with three folds. Performance relative to expert manual segmentation was similar across all networks: pixel accuracy was ~97%, intersection-over-union (IoU) across all classes was ~87%, and IoU across foreground classes only was ~85%. Endocardial left ventricular circumferential strain calculated from the proposed pipeline was significantly different in control and disease subjects (-25.3% vs -29.1%, p = 0.006), in agreement with the current clinical literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of current feature tracking Cardiac Magnetic Resonance (CMR) techniques, and how does the proposed technique address this limitation?\n\n**A)** The primary limitation is the lack of automation, which leads to variability in segmentation results. The proposed technique addresses this limitation by using a fully automated deep learning approach for segmentation and quadratic basis splines for feature tracking.\n\n**B)** The primary limitation is the inability to capture complex cardiac motion patterns. The proposed technique addresses this limitation by incorporating temporal context into the deep learning architecture and using quadratic basis splines to model cardiac motion.\n\n**C)** The primary limitation is the reliance on manual expert segmentation. The proposed technique addresses this limitation by using a fully automated deep learning approach for segmentation and quadratic basis splines for feature tracking.\n\n**D)** The primary limitation is the lack of consideration for cardiac function. The proposed technique addresses this limitation by incorporating endocardial left ventricular circumferential strain calculation into the feature tracking pipeline.\n\n**Correct Answer:** A) The primary limitation is the lack of automation, which leads to variability in segmentation results. The proposed technique addresses this limitation by using a fully automated deep learning approach for segmentation and quadratic basis splines for feature tracking.\n\n**Explanation:** The question requires the test-taker to understand the limitations of current feature tracking CMR techniques and how the proposed technique addresses these limitations. The correct answer, A, highlights the primary limitation of current techniques (lack of automation) and how the proposed technique addresses this limitation (fully automated deep learning approach for segmentation and quadratic basis splines for feature tracking). The other options are incorrect because they either focus on secondary limitations (B) or misrepresent the proposed technique's approach (C and D)."}, "19": {"documentation": {"title": "Pollution permits, Strategic Trading and Dynamic Technology Adoption", "source": "Santiago Moreno-Bromberg and Luca Taschini", "docs_id": "1103.2914", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pollution permits, Strategic Trading and Dynamic Technology Adoption. This paper analyzes the dynamic incentives for technology adoption under a transferable permits system, which allows for strategic trading on the permit market. Initially, firms can invest both in low-emitting production technologies and trade permits. In the model, technology adoption and allowance price are generated endogenously and are inter-dependent. It is shown that the non-cooperative permit trading game possesses a pure-strategy Nash equilibrium, where the allowance value reflects the level of uncovered pollution (demand), the level of unused allowances (supply), and the technological status. These conditions are also satisfied when a price support instrument, which is contingent on the adoption of the new technology, is introduced. Numerical investigation confirms that this policy generates a floating price floor for the allowances, and it restores the dynamic incentives to invest. Given that this policy comes at a cost, a criterion for the selection of a self-financing policy (based on convex risk measures) is proposed and implemented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Pollution permits, Strategic Trading and Dynamic Technology Adoption\", what is the primary effect of introducing a price support instrument on the allowance price, and how does it impact the dynamic incentives for technology adoption?\n\n**A)** The price support instrument increases the allowance price, reducing the incentives for technology adoption, and leads to a non-existent floating price floor.\n\n**B)** The price support instrument decreases the allowance price, increasing the incentives for technology adoption, and restores a floating price floor for the allowances.\n\n**C)** The price support instrument has no effect on the allowance price, and the dynamic incentives for technology adoption remain unchanged.\n\n**D)** The price support instrument increases the allowance price, but only for firms that have already adopted the new technology, and leads to a non-existent floating price floor.\n\n**Correct Answer:** B) The price support instrument decreases the allowance price, increasing the incentives for technology adoption, and restores a floating price floor for the allowances.\n\n**Explanation:** According to the paper, the introduction of a price support instrument leads to a floating price floor for the allowances, which restores the dynamic incentives to invest in technology adoption. This is because the price support instrument decreases the allowance price, making it more attractive for firms to adopt the new technology. Additionally, the paper shows that the allowance value reflects the level of uncovered pollution (demand), the level of unused allowances (supply), and the technological status, which is also satisfied when the price support instrument is introduced."}, "20": {"documentation": {"title": "Resonance fluorescence of GaAs quantum dots with near-unity photon\n  indistinguishability", "source": "Eva Sch\\\"oll, Lukas Hanschke, Lucas Schweickert, Katharina D. Zeuner,\n  Marcus Reindl, Saimon Filipe Covre da Silva, Thomas Lettner, Rinaldo Trotta,\n  Jonathan J. Finley, Kai M\\\"uller, Armando Rastelli, Val Zwiller, and Klaus D.\n  J\\\"ons", "docs_id": "1901.09721", "section": ["cond-mat.mes-hall", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resonance fluorescence of GaAs quantum dots with near-unity photon\n  indistinguishability. Photonic quantum technologies call for scalable quantum light sources that can be integrated, while providing the end user with single and entangled photons on-demand. One promising candidate are strain free GaAs/AlGaAs quantum dots obtained by droplet etching. Such quantum dots exhibit ultra low multi-photon probability and an unprecedented degree of photon pair entanglement. However, different to commonly studied InGaAs/GaAs quantum dots obtained by the Stranski-Krastanow mode, photons with a near-unity indistinguishability from these quantum emitters have proven to be elusive so far. Here, we show on-demand generation of near-unity indistinguishable photons from these quantum emitters by exploring pulsed resonance fluorescence. Given the short intrinsic lifetime of excitons confined in the GaAs quantum dots, we show single photon indistinguishability with a raw visibility of $V_{raw}=(94.2\\pm5.2)\\,\\%$, without the need for Purcell enhancement. Our results represent a milestone in the advance of GaAs quantum dots by demonstrating the final missing property standing in the way of using these emitters as a key component in quantum communication applications, e.g. as an entangled source for quantum repeater architectures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in using strain-free GaAs/AlGaAs quantum dots as a key component in quantum communication applications, and how do the authors' results address this challenge?\n\nA) The short intrinsic lifetime of excitons confined in the GaAs quantum dots hinders the generation of entangled photons, which is a fundamental requirement for quantum communication applications.\nB) The need for Purcell enhancement to increase the photon indistinguishability is a significant obstacle in using these quantum emitters as a reliable source of entangled photons.\nC) The strain-free GaAs/AlGaAs quantum dots exhibit ultra-low multi-photon probability, but the authors' results demonstrate the generation of near-unity indistinguishable photons without the need for Purcell enhancement.\nD) The authors' results show that the single photon indistinguishability of the GaAs quantum dots is limited by the short intrinsic lifetime of excitons, which must be overcome to achieve reliable entangled photon generation.\n\nCorrect Answer: C) The strain-free GaAs/AlGaAs quantum dots exhibit ultra-low multi-photon probability, but the authors' results demonstrate the generation of near-unity indistinguishable photons without the need for Purcell enhancement.\n\nExplanation: The correct answer is C) because the authors' results show that the strain-free GaAs/AlGaAs quantum dots exhibit ultra-low multi-photon probability, which is a desirable property for quantum communication applications. However, the authors also demonstrate the generation of near-unity indistinguishable photons without the need for Purcell enhancement, which addresses the primary challenge of achieving reliable entangled photon generation. The other options are incorrect because option A is not mentioned in the text, option B is partially correct but not the primary challenge, and option D is incorrect because the authors' results show that the single photon indistinguishability is not limited by the short intrinsic lifetime of excitons."}, "21": {"documentation": {"title": "Education Projects for Sustainable Development: Evidence from Ural\n  Federal University", "source": "Marina Volkova, Jol Stoffers, Dmitry Kochetkov", "docs_id": "1909.03429", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Education Projects for Sustainable Development: Evidence from Ural\n  Federal University. Sustainable development is a worldwide recognized social and political goal, discussed in both academic and political discourse and with much research on the topic related to sustainable development in higher education. Since mental models are formed more effectively at school age, we propose a new way of thinking that will help achieve this goal. This paper was written in the context of Russia, where the topic of sustainable development in education is poorly developed. The authors used the classical methodology of the case analysis. The analysis and interpretation of the results were conducted in the framework of the institutional theory. Presented is the case of Ural Federal University, which has been working for several years on the creation of a device for the purification of industrial sewer water in the framework of an initiative student group. Schoolchildren recently joined the program, and such projects have been called university-to-school projects. Successful solutions of inventive tasks contribute to the formation of mental models. This case has been analyzed in terms of institutionalism, and the authors argue for the primacy of mental institutions over normative ones during sustainable society construction. This case study is the first to analyze a partnership between a Federal University and local schools regarding sustainable education and proposes a new way of thinking."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary focus of the authors' new way of thinking in the context of sustainable development in education, as proposed in the paper \"Education Projects for Sustainable Development: Evidence from Ural Federal University\"?\n\nA) The development of normative institutions to govern sustainable practices\nB) The formation of mental models through inventive tasks and university-to-school projects\nC) The implementation of classical methodologies in case analysis\nD) The promotion of institutional theory as a framework for analysis\n\n**Correct Answer:** B) The formation of mental models through inventive tasks and university-to-school projects\n\n**Explanation:** The authors propose a new way of thinking that aims to help achieve the goal of sustainable development in education. They argue that mental models are formed more effectively at school age, and that successful solutions to inventive tasks contribute to the formation of mental models. The authors' focus is on the partnership between a Federal University and local schools, and the implementation of university-to-school projects, which they believe can help shape the mental models of schoolchildren. This approach is distinct from the classical methodology of case analysis and the institutional theory, which are used as frameworks for analysis in the paper."}, "22": {"documentation": {"title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models", "source": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar,\n  Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao,\n  Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "docs_id": "1712.01769", "section": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What structural and optimization improvements were introduced to the Listen, Attend, and Spell (LAS) model to improve its performance on challenging tasks such as voice search, and what were the results of these improvements?\n\nA) Word piece models were replaced with graphemes, and a single-head attention architecture was used.\nB) A multi-head attention architecture was introduced, and word piece models were used instead of graphemes, with synchronous training and scheduled sampling being used for optimization.\nC) A unidirectional LSTM encoder was used for streaming recognition, and label smoothing was used for optimization, with word piece models being used instead of graphemes.\nD) Minimum word error rate optimization was used, and a single-head attention architecture was used, with synchronous training and scheduled sampling being used for optimization.\n\nCorrect Answer: B) A multi-head attention architecture was introduced, and word piece models were used instead of graphemes, with synchronous training and scheduled sampling being used for optimization.\n\nExplanation: The correct answer is B) because the documentation states that a multi-head attention architecture was introduced, and word piece models were used instead of graphemes, with synchronous training and scheduled sampling being used for optimization. These improvements were shown to significantly improve the performance of the LAS model on challenging tasks such as voice search."}, "23": {"documentation": {"title": "Acoustic emission source localization in thin metallic plates: a\n  single-sensor approach based on multimodal edge reflections", "source": "Arvin Ebrahimkhanlou and Salvatore Salamone", "docs_id": "1707.00370", "section": ["physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Acoustic emission source localization in thin metallic plates: a\n  single-sensor approach based on multimodal edge reflections. This paper presents a new acoustic emission (AE) source localization for isotropic plates with reflecting boundaries. This approach that has no blind spot leverages multimodal edge reflections to identify AE sources with only a single sensor. The implementation of the proposed approach involves three main steps. First, the continuous wavelet transform (CWT) and the dispersion curves of the fundamental Lamb wave modes are utilized to estimate the distance between an AE source and a sensor. This step uses a modal acoustic emission approach. Then, an analytical model is proposed that uses the estimated distances to simulate the edge-reflected waves. Finally, the correlation between the experimental and the simulated waveforms is used to estimate the location of AE sources. Hsu-Nielson pencil lead break (PLB) tests were performed on an aluminum plate to validate this algorithm and promising results were achieved. Based on these results, the paper reports the statistics of the localization errors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed acoustic emission source localization approach in thin metallic plates, as described in the paper?\n\nA) It uses multiple sensors to reduce localization errors.\nB) It employs a modal acoustic emission approach to estimate distances between AE sources and sensors.\nC) It leverages multimodal edge reflections to identify AE sources with only a single sensor.\nD) It relies solely on the dispersion curves of the fundamental Lamb wave modes.\n\n**Correct Answer:** C) It leverages multimodal edge reflections to identify AE sources with only a single sensor.\n\n**Explanation:** The correct answer is C) because the paper states that the proposed approach \"has no blind spot\" and \"leverages multimodal edge reflections to identify AE sources with only a single sensor.\" This indicates that the approach's primary advantage is its ability to localize AE sources using a single sensor, which is a significant improvement over existing methods. The other options are incorrect because they either describe a secondary aspect of the approach (A and B) or a characteristic of the approach that is not its primary advantage (D)."}, "24": {"documentation": {"title": "N-terminal domain Increases Activation of Elephant Shark Glucocorticoid\n  and Mineralocorticoid Receptors", "source": "Yoshinao Katsu, Islam MD Shariful, Xiaozhi Lin, Wataru Takagi, Hiroshi\n  Urushitani, Satomi Kohno, Susumu Hyodo, Michael E. Baker", "docs_id": "1911.03517", "section": ["q-bio.MN", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "N-terminal domain Increases Activation of Elephant Shark Glucocorticoid\n  and Mineralocorticoid Receptors. Cortisol, corticosterone and aldosterone activate full-length glucocorticoid receptor (GR) from elephant shark, a cartilaginous fish belonging to the oldest group of jawed vertebrates. Activation by aldosterone a mineralocorticoid, indicates partial divergence of elephant shark GR from the MR. Progesterone activates elephant shark MR, but not elephant shark GR. Progesterone inhibits steroid binding to elephant shark GR, but not to human GR. Deletion of the N-terminal domain (NTD) from elephant shark GR (Truncated GR) reduced the response to corticosteroids, while truncated and full-length elephant shark MR had similar responses to corticosteroids. Chimeras of elephant shark GR NTD fused to MR DBD+LBD had increased activation by corticosteroids and progesterone compared to full-length elephant shark MR. Elephant shark MR NTD fused to GR DBD+LBD had similar activation as full-length elephant shark MR, indicating that activation of human GR by the NTD evolved early in GR divergence from the MR."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Compare the activation of glucocorticoid and mineralocorticoid receptors in elephant shark species with those in human species, focusing on the role of the N-terminal domain (NTD) in receptor activation.\n\n**A)** The NTD of the elephant shark glucocorticoid receptor (GR) is essential for its activation by corticosteroids, whereas the NTD of the mineralocorticoid receptor (MR) is not crucial for its activation by aldosterone.\n\n**B)** The NTD of the elephant shark GR is responsible for its partial divergence from the MR, resulting in reduced activation by corticosteroids in the truncated GR compared to the full-length GR.\n\n**C)** Chimeras of the elephant shark GR NTD fused to the MR DNA-binding domain (DBD) and ligand-binding domain (LBD) exhibit increased activation by corticosteroids and progesterone compared to the full-length elephant shark MR, indicating that the NTD evolved early in GR divergence from the MR.\n\n**D)** The NTD of the elephant shark MR is necessary for its activation by corticosteroids, whereas the NTD of the human GR is not required for its activation by corticosteroids.\n\n**Correct Answer:** C) Chimeras of the elephant shark GR NTD fused to the MR DBD+LBD had increased activation by corticosteroids and progesterone compared to full-length elephant shark MR, indicating that activation of human GR by the NTD evolved early in GR divergence from the MR.\n\n**Explanation:** This question requires the test-taker to analyze the provided documentation and understand the role of the NTD in receptor activation. The correct answer, C, is supported by the fact that chimeras of the elephant shark GR NTD fused to the MR DBD+LBD exhibit increased activation by corticosteroids and progesterone, indicating that the NTD evolved early in GR divergence from the MR. The other options are incorrect because they either misrepresent the role of the NTD or incorrectly state the relationship between the NTD and receptor activation."}, "25": {"documentation": {"title": "A Conditional Generative Model for Predicting Material Microstructures\n  from Processing Methods", "source": "Akshay Iyer, Biswadip Dey, Arindam Dasgupta, Wei Chen, Amit\n  Chakraborty", "docs_id": "1910.02133", "section": ["eess.IV", "cond-mat.mtrl-sci", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Conditional Generative Model for Predicting Material Microstructures\n  from Processing Methods. Microstructures of a material form the bridge linking processing conditions - which can be controlled, to the material property - which is the primary interest in engineering applications. Thus a critical task in material design is establishing the processing-structure relationship, which requires domain expertise and techniques that can model the high-dimensional material microstructure. This work proposes a deep learning based approach that models the processing-structure relationship as a conditional image synthesis problem. In particular, we develop an auxiliary classifier Wasserstein GAN with gradient penalty (ACWGAN-GP) to synthesize microstructures under a given processing condition. This approach is free of feature engineering, requires modest domain knowledge and is applicable to a wide range of material systems. We demonstrate this approach using the ultra high carbon steel (UHCS) database, where each microstructure is annotated with a label describing the cooling method it was subjected to. Our results show that ACWGAN-GP can synthesize high-quality multiphase microstructures for a given cooling method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed deep learning-based approach in the paper \"A Conditional Generative Model for Predicting Material Microstructures\"?\n\nA) To predict material properties from microstructures\nB) To synthesize microstructures under a given processing condition\nC) To classify microstructures based on their cooling methods\nD) To model the processing-structure relationship as a conditional image synthesis problem\n\n**Correct Answer:** B) To synthesize microstructures under a given processing condition\n\n**Explanation:** The paper proposes a deep learning-based approach to model the processing-structure relationship as a conditional image synthesis problem. The primary goal of this approach is to synthesize microstructures under a given processing condition, which is achieved through the development of an auxiliary classifier Wasserstein GAN with gradient penalty (ACWGAN-GP). This approach is free of feature engineering and requires modest domain knowledge, making it applicable to a wide range of material systems.\n\n**Candidate A (Incorrect):** While the approach does involve synthesizing microstructures, its primary goal is not to predict material properties from microstructures.\n\n**Candidate B (Correct):** This is the correct answer, as the approach is specifically designed to synthesize microstructures under a given processing condition.\n\n**Candidate C (Incorrect):** The approach does not involve classifying microstructures based on their cooling methods, but rather synthesizing microstructures under a given processing condition.\n\n**Candidate D (Incorrect):** While the approach does involve modeling the processing-structure relationship, its primary goal is not to model this relationship as a conditional image synthesis problem, but rather to synthesize microstructures under a given processing condition."}, "26": {"documentation": {"title": "Universal Statistical Properties of Inertial-particle Trajectories in\n  Three-dimensional, Homogeneous, Isotropic, Fluid Turbulence", "source": "Akshay Bhatnagar, Anupam Gupta, Dhrubaditya Mitra, Prasad Perlekar,\n  and Rahul Pandit", "docs_id": "1412.2686", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Universal Statistical Properties of Inertial-particle Trajectories in\n  Three-dimensional, Homogeneous, Isotropic, Fluid Turbulence. We uncover universal statistical properties of the trajectories of heavy inertial particles in three-dimensional, statistically steady, homogeneous, and isotropic turbulent flows by extensive direct numerical simulations. We show that the probability distribution functions (PDFs) $P(\\phi)$, of the angle $\\phi$ between the Eulerian velocity ${\\bf u}$ and the particle velocity ${\\bf v}$, at this point and time, shows a power-law region in which $P(\\phi) \\sim \\phi^{-\\gamma}$, with a new universal exponent $\\gamma \\simeq 4$. Furthermore, the PDFs of the trajectory curvature $\\kappa$ and modulus $\\theta$ of the torsion $\\vartheta$ have power-law tails that scale, respectively, as $P(\\kappa) \\sim \\kappa^{-h_\\kappa}$, as $\\kappa \\to \\infty$, and $P(\\theta) \\sim \\theta^{-h_\\theta}$, as $\\theta \\to \\infty$, with exponents $h_\\kappa \\simeq 2.5$ and $h_\\theta \\simeq 3$ that are universal to the extent that they do not depend on the Stokes number ${\\rm St}$ (given our error bars). We also show that $\\gamma$, $h_\\kappa$ and $h_\\theta$ can be obtained by using simple stochastic models. We characterize the complexity of heavy-particle trajectories by the number $N_{\\rm I}(t,{\\rm St})$ of points (up until time $t$) at which $\\vartheta$ changes sign. We show that $n_{\\rm I}({\\rm St}) \\equiv \\lim_{t\\to\\infty} \\frac{N_{\\rm I}(t,{\\rm St})}{t} \\sim {\\rm St}^{-\\Delta}$, with $\\Delta \\simeq 0.4$ a universal exponent."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of inertial-particle trajectories in three-dimensional, homogeneous, isotropic, fluid turbulence, what is the relationship between the probability distribution function (PDF) of the angle $\\phi$ between the Eulerian velocity ${\\bf u}$ and the particle velocity ${\\bf v}$, and the universal exponent $\\gamma$?\n\nA) $P(\\phi) \\sim \\phi^{-\\gamma}$, with $\\gamma$ being a function of the Stokes number ${\\rm St}$.\nB) $P(\\phi) \\sim \\phi^{-\\gamma}$, with $\\gamma$ being a universal constant independent of ${\\rm St}$.\nC) $P(\\phi) \\sim \\phi^{\\gamma}$, with $\\gamma$ being a function of the trajectory curvature $\\kappa$.\nD) $P(\\phi) \\sim \\phi^{-\\gamma}$, with $\\gamma$ being a function of the number of points $N_{\\rm I}(t,{\\rm St})$ at which $\\vartheta$ changes sign.\n\nCorrect Answer: B) $P(\\phi) \\sim \\phi^{-\\gamma}$, with $\\gamma$ being a universal constant independent of ${\\rm St}$.\n\nExplanation: The correct answer is based on the statement in the Arxiv documentation that the probability distribution function (PDF) $P(\\phi)$ of the angle $\\phi$ between the Eulerian velocity ${\\bf u}$ and the particle velocity ${\\bf v}$ shows a power-law region in which $P(\\phi) \\sim \\phi^{-\\gamma}$, with a new universal exponent $\\gamma \\simeq 4$. This indicates that $\\gamma$ is a universal constant independent of the Stokes number ${\\rm St}$."}, "27": {"documentation": {"title": "Experimental investigation of the elastic enhancement factor in a\n  transient region between regular and chaotic dynamics", "source": "Micha{\\l} {\\L}awniczak, Ma{\\l}gorzata Bia{\\l}ous, Vitalii Yunko,\n  Szymon Bauch, and Leszek Sirko (Polish Academy of Sciences)", "docs_id": "1503.08049", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental investigation of the elastic enhancement factor in a\n  transient region between regular and chaotic dynamics. We present the results of an experimental study of the elastic enhancement factor W for a microwave rectangular cavity simulating a two-dimensional quantum billiard in a transient region between regular and chaotic dynamics. The cavity was coupled to a vector network analyzer via two microwave antennas. The departure of the system from the integrable one due to presence of antennas acting as scatterers is characterised by the parameter of chaoticity k = 2.8. The experimental results for the rectangular cavity are compared with the ones obtained for a microwave rough cavity simulating a chaotic quantum billiard. The experimental results were obtained for the frequency range v = 16 - 18.5 GHz and moderate absorption strength y = 5.2 - 7.4. We show that the elastic enhancement factor for the rectangular cavity lies below the theoretical value W = 3 predicted for integrable systems and it is significantly higher than the one obtained for the rough cavity. The results obtained for the microwave rough cavity are smaller than the ones obtained within the framework of Random Matrix Theory and lie between them and the ones predicted within a recently introduced model of the two-channel coupling (V. Sokolov and O. Zhirov, arXiv:1411.6211v2[nucl-th], 12 Dec 2014)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the elastic enhancement factor W in the transient region between regular and chaotic dynamics in the microwave rectangular cavity, and how does it compare to the theoretical value predicted for integrable systems?\n\nA) The elastic enhancement factor W is significantly lower than the theoretical value W = 3 predicted for integrable systems, and its value is comparable to the one obtained for the rough cavity.\n\nB) The elastic enhancement factor W is significantly higher than the theoretical value W = 3 predicted for integrable systems, and its value is comparable to the one obtained for the rough cavity.\n\nC) The elastic enhancement factor W is significantly lower than the theoretical value W = 3 predicted for integrable systems, and its value is lower than the one obtained for the rough cavity.\n\nD) The elastic enhancement factor W is significantly higher than the theoretical value W = 3 predicted for integrable systems, and its value is higher than the one obtained for the rough cavity.\n\nCorrect Answer: B) The elastic enhancement factor W is significantly higher than the theoretical value W = 3 predicted for integrable systems, and its value is comparable to the one obtained for the rough cavity.\n\nExplanation: The correct answer is B) because the experimental results show that the elastic enhancement factor W for the rectangular cavity lies below the theoretical value W = 3 predicted for integrable systems, but it is significantly higher than the one obtained for the rough cavity. This suggests that the presence of antennas acting as scatterers in the rectangular cavity leads to a significant enhancement of the elastic enhancement factor, which is not observed in the rough cavity."}, "28": {"documentation": {"title": "Ambient heat and human sleep", "source": "Kelton Minor, Andreas Bjerre-Nielsen, Sigga Svala Jonasdottir, Sune\n  Lehmann, Nick Obradovich", "docs_id": "2011.07161", "section": ["cs.CY", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ambient heat and human sleep. Ambient temperatures are rising globally, with the greatest increases recorded at night. Concurrently, the prevalence of insufficient sleep is increasing in many populations, with substantial costs to human health and well-being. Even though nearly a third of the human lifespan is spent asleep, it remains unknown whether temperature and weather impact objective measures of sleep in real-world settings, globally. Here we link billions of sleep measurements from wearable devices comprising over 7 million nighttime sleep records across 68 countries to local daily meteorological data from 2015 to 2017. Rising nighttime temperatures shorten within-person sleep duration primarily through delayed onset, increasing the probability of insufficient sleep. The effect of temperature on sleep loss is substantially larger for residents from lower income countries and older adults, and females are affected more than are males. Nighttime temperature increases inflict the greatest sleep loss during summer and fall months, and we do not find evidence of short-term acclimatization. Coupling historical behavioral measurements with output from climate models, we project that climate change will further erode human sleep, producing substantial geographic inequalities. Our findings have significant implications for adaptation planning and illuminate a pathway through which rising temperatures may globally impact public health."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which rising nighttime temperatures affect human sleep duration, and how does this impact individuals from different socioeconomic backgrounds and age groups?\n\nA) Rising nighttime temperatures shorten sleep duration primarily through increased sleep fragmentation, which disproportionately affects individuals from lower income countries and older adults.\nB) Rising nighttime temperatures shorten sleep duration primarily through delayed onset, which increases the probability of insufficient sleep, and this effect is more pronounced in females than in males.\nC) Rising nighttime temperatures shorten sleep duration primarily through increased sleep fragmentation, which disproportionately affects individuals from higher income countries and younger adults.\nD) Rising nighttime temperatures shorten sleep duration primarily through increased sleep fragmentation, which disproportionately affects individuals from lower income countries and older adults, but this effect is mitigated by short-term acclimatization.\n\nCorrect Answer: B) Rising nighttime temperatures shorten sleep duration primarily through delayed onset, which increases the probability of insufficient sleep, and this effect is more pronounced in females than in males.\n\nExplanation: The correct answer is based on the statement in the documentation that \"Rising nighttime temperatures shorten within-person sleep duration primarily through delayed onset, increasing the probability of insufficient sleep.\" Additionally, the documentation states that \"The effect of temperature on sleep loss is substantially larger for residents from lower income countries and older adults, and females are affected more than are males.\" Therefore, option B accurately reflects the primary mechanism by which rising nighttime temperatures affect human sleep duration and the demographic groups most affected."}, "29": {"documentation": {"title": "Fairness-Aware PAC Learning from Corrupted Data", "source": "Nikola Konstantinov, Christoph H. Lampert", "docs_id": "2102.06004", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fairness-Aware PAC Learning from Corrupted Data. Addressing fairness concerns about machine learning models is a crucial step towards their long-term adoption in real-world automated systems. While many approaches have been developed for training fair models from data, little is known about the robustness of these methods to data corruption. In this work we consider fairness-aware learning under worst-case data manipulations. We show that an adversary can in some situations force any learner to return an overly biased classifier, regardless of the sample size and with or without degrading accuracy, and that the strength of the excess bias increases for learning problems with underrepresented protected groups in the data. We also prove that our hardness results are tight up to constant factors. To this end, we study two natural learning algorithms that optimize for both accuracy and fairness and show that these algorithms enjoy guarantees that are order-optimal in terms of the corruption ratio and the protected groups frequencies in the large data limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a fairness-aware learning algorithm that aims to minimize both accuracy and bias in a machine learning model. Suppose an adversary corrupts the data by adding noise to the labels, which can lead to an overly biased classifier. How does the strength of the excess bias increase in this scenario, and what are the implications for the learning algorithm's performance?\n\n**A)** The strength of the excess bias increases as the sample size decreases, and the learning algorithm's performance degrades significantly.\n\n**B)** The strength of the excess bias increases as the corruption ratio increases, and the learning algorithm's performance remains unaffected.\n\n**C)** The strength of the excess bias increases as the frequencies of underrepresented protected groups in the data increase, and the learning algorithm's performance improves.\n\n**D)** The strength of the excess bias increases as the frequencies of underrepresented protected groups in the data decrease, and the learning algorithm's performance remains unaffected.\n\n**Correct Answer:** D) The strength of the excess bias increases as the frequencies of underrepresented protected groups in the data decrease, and the learning algorithm's performance remains unaffected.\n\n**Explanation:** According to the documentation, the strength of the excess bias increases for learning problems with underrepresented protected groups in the data. This means that if the frequencies of underrepresented protected groups decrease, the excess bias will increase. Additionally, the learning algorithm's performance remains unaffected, as the corruption ratio and the frequencies of underrepresented protected groups in the data are not directly related to the algorithm's performance."}, "30": {"documentation": {"title": "Antineutrino flux and spectrum calculation for spent nuclear fuel for\n  the Daya Bay antineutrino experiment", "source": "X.B. Ma, Y.F. Zhao, Y.X. Chen, W.L. Zhong, F.P. An", "docs_id": "1512.07353", "section": ["physics.ins-det", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Antineutrino flux and spectrum calculation for spent nuclear fuel for\n  the Daya Bay antineutrino experiment. Spent nuclear fuel (SNF) antineutrino flux is an important source of uncertainties for a reactor neutrino flux prediction. However, if one want to determine the contribution of spent fuel, many data are needed, such as the amount of spent fuel in the pool, the time after discharged from the reactor core, the burnup of each assembly, and the antineutrino spectrum of the isotopes in the spend fuel. A method to calculate the contribution of SNF is proposed in this study. In this method, reactor simulation code verified by experiment have been used to simulate the fuel depletion by taking into account more than 2000 isotopes and fission products, the quantity of SNF in each six spend fuel pool, and the antineutrino spectrum of SNF varying with time after SNF discharged from core. Results show that the contribution of SNF to the total antineutrino flux is about 0.26%~0.34%, and the shutdown impact is about 20%. The SNF spectrum would distort the softer part of antineutrino spectra, and the maximum contribution from SNF is about 3.0%, but there is 18\\% difference between line evaluate method and under evaluate method. In addition, non-equilibrium effects are also discussed, and the results are compatible with theirs considering the uncertainties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary source of uncertainty in predicting reactor neutrino flux, and how does the proposed method in the study address this uncertainty?\n\nA) The amount of spent fuel in the pool is the primary source of uncertainty, and the proposed method uses reactor simulation code to simulate fuel depletion.\nB) The burnup of each assembly is the primary source of uncertainty, and the proposed method uses the antineutrino spectrum of isotopes in spent fuel to calculate the contribution of spent fuel.\nC) The time after discharged from the reactor core is the primary source of uncertainty, and the proposed method uses the quantity of SNF in each six spent fuel pool to calculate the contribution of spent fuel.\nD) The antineutrino spectrum of the isotopes in spent fuel is the primary source of uncertainty, and the proposed method uses reactor simulation code to simulate fuel depletion and calculate the contribution of spent fuel.\n\n**Correct Answer:** D) The antineutrino spectrum of the isotopes in spent fuel is the primary source of uncertainty, and the proposed method uses reactor simulation code to simulate fuel depletion and calculate the contribution of spent fuel.\n\n**Explanation:** The study highlights that the antineutrino spectrum of isotopes in spent fuel is an important source of uncertainty in predicting reactor neutrino flux. The proposed method addresses this uncertainty by using reactor simulation code to simulate fuel depletion and calculate the contribution of spent fuel, taking into account the antineutrino spectrum of isotopes in spent fuel varying with time after SNF discharged from core."}, "31": {"documentation": {"title": "The OLYMPUS Experiment", "source": "R. Milner, D.K. Hasell, M. Kohl, U. Schneekloth, N. Akopov, R.\n  Alarcon, V.A. Andreev, O. Ates, A. Avetisyan, D. Bayadilov, R. Beck, S.\n  Belostotski, J.C. Bernauer, J. Bessuille, F. Brinker, B. Buck, J.R. Calarco,\n  V. Carassiti, E. Cisbani, G. Ciullo, M. Contalbrigo, N. D'Ascenzo, R. De Leo,\n  J. Diefenbach, T.W. Donnelly, K. Dow, G. Elbakian, D. Eversheim, S. Frullani,\n  Ch. Funke, G. Gavrilov, B. Gl\\\"aser, N. G\\\"orrissen, J. Hauschildt, B.S.\n  Henderson, Ph. Hoffmeister, Y. Holler, L.D. Ice, A. Izotov, R. Kaiser, G.\n  Karyan, J. Kelsey, D. Khaneft, P. Klassen, A. Kiselev, A. Krivshich, I.\n  Lehmann, P. Lenisa, D. Lenz, S. Lumsden, Y. Ma, F. Maas, H. Marukyan, O.\n  Miklukho, A. Movsisyan, M. Murray, Y. Naryshkin, C. O'Connor, R. Perez\n  Benito, R. Perrino, R.P. Redwine, D. Rodr\\'iguez Pi\\~neiro, G. Rosner, R.L.\n  Russell, A. Schmidt, B. Seitz, M. Statera, A. Thiel, H. Vardanyan, D.\n  Veretennikov, C. Vidal, A. Winnebeck, V. Yeganov", "docs_id": "1312.1730", "section": ["physics.ins-det", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The OLYMPUS Experiment. The OLYMPUS experiment was designed to measure the ratio between the positron-proton and electron-proton elastic scattering cross sections, with the goal of determining the contribution of two-photon exchange to the elastic cross section. Two-photon exchange might resolve the discrepancy between measurements of the proton form factor ratio, $\\mu_p G^p_E/G^p_M$, made using polarization techniques and those made in unpolarized experiments. OLYMPUS operated on the DORIS storage ring at DESY, alternating between 2.01~GeV electron and positron beams incident on an internal hydrogen gas target. The experiment used a toroidal magnetic spectrometer instrumented with drift chambers and time-of-flight detectors to measure rates for elastic scattering over the polar angular range of approximately $25^\\circ$--$75^\\circ$. Symmetric M{\\o}ller/Bhabha calorimeters at $1.29^\\circ$ and telescopes of GEM and MWPC detectors at $12^\\circ$ served as luminosity monitors. A total luminosity of approximately 4.5~fb$^{-1}$ was collected over two running periods in 2012. This paper provides details on the accelerator, target, detectors, and operation of the experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat was the primary goal of the OLYMPUS experiment, and how did it relate to the discrepancy between measurements of the proton form factor ratio?\n\nA) To measure the proton form factor ratio and resolve the discrepancy between measurements made using polarization techniques and those made in unpolarized experiments.\nB) To determine the contribution of two-photon exchange to the elastic cross section and resolve the discrepancy between measurements of the proton form factor ratio.\nC) To measure the ratio between the positron-proton and electron-proton elastic scattering cross sections and compare it to the proton form factor ratio.\nD) To study the properties of the hydrogen gas target and its interaction with the electron and positron beams.\n\nCorrect Answer: B) To determine the contribution of two-photon exchange to the elastic cross section and resolve the discrepancy between measurements of the proton form factor ratio.\n\nExplanation: The OLYMPUS experiment was designed to measure the ratio between the positron-proton and electron-proton elastic scattering cross sections, with the goal of determining the contribution of two-photon exchange to the elastic cross section. This was motivated by the discrepancy between measurements of the proton form factor ratio made using polarization techniques and those made in unpolarized experiments. Therefore, option B is the correct answer."}, "32": {"documentation": {"title": "Scaling and Universality of the Complexity of Analog Computation", "source": "Yaniv Avizrats, Joshua Feinberg & Shmuel Fishman", "docs_id": "cond-mat/0511354", "section": ["cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling and Universality of the Complexity of Analog Computation. We apply a probabilistic approach to study the computational complexity of analog computers which solve linear programming problems. We analyze numerically various ensembles of linear programming problems and obtain, for each of these ensembles, the probability distribution functions of certain quantities which measure the computational complexity, known as the convergence rate, the barrier and the computation time. We find that in the limit of very large problems these probability distributions are universal scaling functions. In other words, the probability distribution function for each of these three quantities becomes, in the limit of large problem size, a function of a single scaling variable, which is a certain composition of the quantity in question and the size of the system. Moreover, various ensembles studied seem to lead essentially to the same scaling functions, which depend only on the variance of the ensemble. These results extend analytical and numerical results obtained recently for the Gaussian ensemble, and support the conjecture that these scaling functions are universal."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the probabilistic approach to studying the computational complexity of analog computers solving linear programming problems, as described in the paper \"Scaling and Universality of the Complexity of Analog Computation\". What is the main finding regarding the probability distribution functions of the convergence rate, barrier, and computation time in the limit of very large problems?\n\nA) These functions become deterministic and no longer depend on the system size.\nB) The probability distribution functions converge to a single universal scaling function that depends only on the variance of the ensemble.\nC) The convergence rate, barrier, and computation time are independent of each other and do not exhibit any scaling behavior.\nD) The probability distribution functions become increasingly complex and difficult to analyze as the problem size increases.\n\nCorrect Answer: B) The probability distribution functions converge to a single universal scaling function that depends only on the variance of the ensemble.\n\nExplanation: The paper states that in the limit of very large problems, the probability distribution functions of the convergence rate, barrier, and computation time become universal scaling functions, which depend only on a single scaling variable that is a composition of the quantity in question and the size of the system. This means that the probability distribution functions converge to a single universal function that is determined by the variance of the ensemble, regardless of the specific problem being solved."}, "33": {"documentation": {"title": "Designing Direct Matching Mechanism for India with Comprehensive\n  Affirmative Action", "source": "Orhan Ayg\\\"un and Bertan Turhan", "docs_id": "2004.13264", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Designing Direct Matching Mechanism for India with Comprehensive\n  Affirmative Action. Since 1950, India has been implementing the most comprehensive affirmative action program in the world. Vertical reservations are provided to members of historically discriminated Scheduled Castes (SC), Scheduled Tribes (ST), and Other Backward Classes (OBC). Horizontal reservations are provided for other disadvantaged groups, such as women and disabled people, within each vertical category. There is no well-defined procedure to implement horizontal reservations jointly with vertical reservation and OBC de-reservations. Sequential processes currently in use for OBC de-reservations and meritorious reserve candidates lead to severe shortcomings. Most importantly, indirect mechanisms currently used in practice do not allow reserve category applicants to fully express their preferences. To overcome these and other related issues, we design several different choice rules for institutions that take meritocracy, vertical and horizontal reservations, and OBC de-reservations into account. We propose a centralized mechanism to satisfactorily clear matching markets in India."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Design a direct matching mechanism for India's affirmative action program that takes into account vertical reservations, horizontal reservations, and OBC de-reservations, while ensuring that reserve category applicants can fully express their preferences. Consider the following constraints:\n\n* The mechanism should prioritize meritocracy while accommodating vertical and horizontal reservations.\n* The mechanism should address the issue of sequential processes currently used for OBC de-reservations and meritorious reserve candidates.\n* The mechanism should allow reserve category applicants to fully express their preferences.\n\n**Candidate A:** Implement a centralized auction mechanism that takes into account the number of seats available in each vertical and horizontal category, as well as the preferences of reserve category applicants.\n\n**Candidate B:** Design a weighted random assignment mechanism that assigns applicants to institutions based on their merit scores, while giving preference to applicants from historically discriminated groups.\n\n**Candidate C:** Develop a hybrid mechanism that combines elements of centralized auctions and weighted random assignment, allowing reserve category applicants to express their preferences while ensuring meritocracy.\n\n**Candidate D:** Propose a mechanism that uses machine learning algorithms to analyze applicant data and preferences, and then uses this information to optimize the allocation of seats in each category.\n\n**Correct Answer:** C) Develop a hybrid mechanism that combines elements of centralized auctions and weighted random assignment, allowing reserve category applicants to express their preferences while ensuring meritocracy.\n\n**Explanation:** The correct answer, C, is the most suitable solution because it addresses the need to balance meritocracy with the requirements of vertical and horizontal reservations, while also allowing reserve category applicants to fully express their preferences. The hybrid mechanism can be designed to incorporate elements of centralized auctions and weighted random assignment, allowing for a more nuanced and efficient allocation of seats. This approach can help to overcome the shortcomings of sequential processes currently in use and ensure that the affirmative action program is implemented in a fair and effective manner."}, "34": {"documentation": {"title": "Soliton-phonon scattering problem in 1D nonlinear Schr\\\"odinger systems\n  with general nonlinearity", "source": "Daisuke A. Takahashi", "docs_id": "1201.2138", "section": ["cond-mat.quant-gas", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Soliton-phonon scattering problem in 1D nonlinear Schr\\\"odinger systems\n  with general nonlinearity. A scattering problem (or more precisely, a transmission-reflection problem) of linearized excitations in the presence of a dark soliton is considered in a one-dimensional nonlinear Schr\\\"odinger system with a general nonlinearity: $ \\mathrm{i}\\partial_t \\phi = -\\partial_x^2 \\phi + F(|\\phi|^2)\\phi $. If the system is interpreted as a Bose-Einstein condensate, the linearized excitation is a Bogoliubov phonon, and the linearized equation is the Bogoliubov equation. We exactly prove that the perfect transmission of the zero-energy phonon is suppressed at a critical state determined by Barashenkov's stability criterion [Phys. Rev. Lett. 77, (1996) 1193.], and near the critical state, the energy-dependence of the reflection coefficient shows a saddle-node type scaling law. The analytical results are well supported by numerical calculation for cubic-quintic nonlinearity. Our result gives an exact example of scaling laws of saddle-node bifurcation in time-reversible Hamiltonian systems. As a by-product of the proof, we also give all exact zero-energy solutions of the Bogoliubov equation and their finite energy extension."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the soliton-phonon scattering problem in 1D nonlinear Schr\u00f6dinger systems with general nonlinearity, what is the relationship between the Barashenkov's stability criterion and the energy-dependence of the reflection coefficient for the zero-energy phonon?\n\nA) The Barashenkov's stability criterion determines the energy-dependence of the reflection coefficient, which exhibits a saddle-node type scaling law near the critical state.\n\nB) The Barashenkov's stability criterion is a necessary condition for the perfect transmission of the zero-energy phonon, but it does not determine the energy-dependence of the reflection coefficient.\n\nC) The Barashenkov's stability criterion is a sufficient condition for the suppression of perfect transmission of the zero-energy phonon, and it is related to the energy-dependence of the reflection coefficient through a scaling law.\n\nD) The Barashenkov's stability criterion is a by-product of the proof, and it does not have any direct relationship with the energy-dependence of the reflection coefficient.\n\n**Correct Answer:** C) The Barashenkov's stability criterion is a sufficient condition for the suppression of perfect transmission of the zero-energy phonon, and it is related to the energy-dependence of the reflection coefficient through a scaling law.\n\n**Explanation:** The correct answer is C) because the Barashenkov's stability criterion is indeed related to the suppression of perfect transmission of the zero-energy phonon, and it is also related to the energy-dependence of the reflection coefficient through a scaling law. The correct answer can be inferred from the statement in the documentation that \"the energy-dependence of the reflection coefficient shows a saddle-node type scaling law near the critical state determined by Barashenkov's stability criterion\"."}, "35": {"documentation": {"title": "TLIO: Tight Learned Inertial Odometry", "source": "Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I. Mourikis,\n  Kostas Daniilidis, Vijay Kumar, Jakob Engel", "docs_id": "2007.01867", "section": ["cs.RO", "cs.CV", "cs.LG", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TLIO: Tight Learned Inertial Odometry. In this work we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This paper demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in integrating strap-down IMU measurements into a state estimation framework, and how does the proposed TLIO approach address this issue?\n\nA) The integration of IMU measurements is sensitive to sensor bias and noise, causing significant drift within seconds, and the TLIO approach uses a neural network to learn the kinematic motion model.\n\nB) The integration of IMU measurements is sensitive to sensor bias and noise, causing significant drift within seconds, and the TLIO approach uses a traditional Kalman filter to fuse the measurements.\n\nC) The integration of IMU measurements is insensitive to sensor bias and noise, causing significant drift within seconds, and the TLIO approach uses a neural network to learn the kinematic motion model.\n\nD) The integration of IMU measurements is sensitive to sensor bias and noise, causing significant drift within seconds, and the TLIO approach uses a traditional Kalman filter to fuse the measurements, but only for the velocity component.\n\n**Correct Answer:** A) The integration of IMU measurements is sensitive to sensor bias and noise, causing significant drift within seconds, and the TLIO approach uses a neural network to learn the kinematic motion model.\n\n**Explanation:** The TLIO approach addresses the challenge of integrating IMU measurements by using a neural network to learn the kinematic motion model, which is sensitive to sensor bias and noise. This allows for more accurate and robust state estimation, including pose, velocity, and sensor biases. The correct answer highlights the key innovation of the TLIO approach, which is the use of a neural network to learn the kinematic motion model."}, "36": {"documentation": {"title": "A natural $ S_4 \\times SO(10) $ model of flavour", "source": "Fredrik Bj\\\"orkeroth, Francisco J. de Anda, Stephen F. King, Elena\n  Perdomo", "docs_id": "1705.01555", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A natural $ S_4 \\times SO(10) $ model of flavour. We propose a natural $ S_4 \\times SO(10) $ supersymmetric grand unified theory of flavour with an auxiliary $\\mathbb{Z}_4^2 \\times \\mathbb{Z}_4^R$ symmetry, based on small Higgs representations (nothing larger than an adjoint) and hence a type-I seesaw mechanism. The Yukawa structure of all fermions is determined by the hierarchical vacuum expectation values of three $ S_4 $ triplet flavons, with CSD3 vacuum alignments, where up-type quarks and neutrinos couple to one Higgs $\\mathbf{10}$, and the down-type quarks and charged leptons couple to a second Higgs $\\mathbf{10}$. The Yukawa matrices are obtained from sums of low-rank matrices, where each matrix in the sum naturally accounts for the mass of a particular family, as in sequential dominance in the neutrino sector, which predicts a normal neutrino mass hierarchy. The model accurately fits all available quark and lepton data, with predictions for the leptonic $CP$ phase in 95$\\%$ credible intervals given by $ 281^\\circ < \\delta^\\ell < 308^\\circ $ and $ 225^\\circ < \\delta^\\ell < 253^\\circ $. The model reduces to the MSSM, with the two Higgs doublets emerging from the two Higgs $\\mathbf{10}$s without mixing, and we demonstrate how a $\\mu$ term of $\\mathcal{O}$(TeV) can be realised, as well as doublet-triplet splitting, with Planck scale operators controlled by symmetry, leading to acceptable proton decay."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed $S_4 \\times SO(10)$ supersymmetric grand unified theory of flavour, what is the predicted range for the leptonic $CP$ phase, $\\delta^\\ell$, and how does this relate to the model's ability to accommodate a normal neutrino mass hierarchy?\n\n**A)** The predicted range for $\\delta^\\ell$ is $45^\\circ < \\delta^\\ell < 135^\\circ$, which is inconsistent with the model's ability to accommodate a normal neutrino mass hierarchy.\n\n**B)** The predicted range for $\\delta^\\ell$ is $281^\\circ < \\delta^\\ell < 308^\\circ$ and $225^\\circ < \\delta^\\ell < 253^\\circ$, which is consistent with the model's ability to accommodate a normal neutrino mass hierarchy.\n\n**C)** The predicted range for $\\delta^\\ell$ is $135^\\circ < \\delta^\\ell < 225^\\circ$, which is inconsistent with the model's ability to accommodate a normal neutrino mass hierarchy.\n\n**D)** The predicted range for $\\delta^\\ell$ is $45^\\circ < \\delta^\\ell < 135^\\circ$, which is consistent with the model's ability to accommodate a normal neutrino mass hierarchy.\n\n**Correct Answer:** B) The predicted range for $\\delta^\\ell$ is $281^\\circ < \\delta^\\ell < 308^\\circ$ and $225^\\circ < \\delta^\\ell < 253^\\circ$, which is consistent with the model's ability to accommodate a normal neutrino mass hierarchy.\n\n**Explanation:** The correct answer is B) because the predicted range for $\\delta^\\ell$ is given in the documentation as $281^\\circ < \\delta^\\ell < 308^\\circ$ and $225^\\circ < \\delta^\\ell < 253^\\circ$, which indicates that the model can accommodate a normal neutrino mass hierarchy. The other options are incorrect because they do not match the predicted range for $\\delta^\\ell$ provided in the documentation."}, "37": {"documentation": {"title": "On the Simpson index for the Moran process with random selection and\n  immigration", "source": "Arnaud Personne (UCA), Arnaud Guillin (LMBP), Franck Jabot (UR LISC),\n  Arnaud Guillin", "docs_id": "1809.08890", "section": ["math.PR", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Simpson index for the Moran process with random selection and\n  immigration. Moran or Wright-Fisher processes are probably the most well known model to study the evolution of a population under various effects. Our object of study will be the Simpson index which measures the level of diversity of the population, one of the key parameter for ecologists who study for example forest dynamics. Following ecological motivations, we will consider here the case where there are various species with fitness and immigration parameters being random processes (and thus time evolving). To measure biodiversity, ecologists generally use the Simpson index, who has no closed formula, except in the neutral (no selection) case via a backward approach, and which is difficult to evaluate even numerically when the population size is large. Our approach relies on the large population limit in the \"weak\" selection case, and thus to give a procedure which enable us to approximate, with controlled rate, the expectation of the Simpson index at fixed time. Our approach will be forward and valid for all time, which is the main difference with the historical approach of Kingman, or Krone-Neuhauser. We will also study the long time behaviour of the Wright-Fisher process in a simplified setting, allowing us to get a full picture for the approximation of the expectation of the Simpson index."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Moran process with random selection and immigration, where the population size is large and the selection and immigration parameters are random processes. Suppose we want to approximate the expectation of the Simpson index at a fixed time using the \"weak\" selection case. What is the main difference between our approach and the historical approach of Kingman, or Krone-Neuhauser?\n\n**A)** Our approach is backward, while theirs is forward.\n**B)** Our approach is valid for all time, while theirs is only valid for the neutral case.\n**C)** Our approach uses a closed formula, while theirs does not.\n**D)** Our approach is based on the large population limit, while theirs is not.\n\n**Correct Answer:** B) Our approach is valid for all time, while theirs is only valid for the neutral case.\n\n**Explanation:** The correct answer is B) because the documentation states that our approach is \"forward and valid for all time\", which is a key difference with the historical approach of Kingman, or Krone-Neuhauser, which is only valid for the neutral case. The other options are incorrect because our approach is actually backward (not forward), we do not use a closed formula, and our approach is indeed based on the large population limit."}, "38": {"documentation": {"title": "Frequency-Domain Group-based Shrinkage Estimators for UWB Systems", "source": "Sheng Li, Rodrigo C. de Lamare and Martin Haardt", "docs_id": "1304.5817", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Frequency-Domain Group-based Shrinkage Estimators for UWB Systems. In this work, we propose low-complexity adaptive biased estimation algorithms, called group-based shrinkage estimators (GSEs), for parameter estimation and interference suppression scenarios with mechanisms to automatically adjust the shrinkage factors. The proposed estimation algorithms divide the target parameter vector into a number of groups and adaptively calculate one shrinkage factor for each group. GSE schemes improve the performance of the conventional least squares (LS) estimator in terms of the mean-squared error (MSE), while requiring a very modest increase in complexity. An MSE analysis is presented which indicates the lower bounds of the GSE schemes with different group sizes. We prove that our proposed schemes outperform the biased estimation with only one shrinkage factor and the best performance of GSE can be obtained with the maximum number of groups. Then, we consider an application of the proposed algorithms to single-carrier frequency-domain equalization (SC-FDE) of direct-sequence ultra-wideband (DS-UWB) systems, in which the structured channel estimation (SCE) algorithm and the frequency domain receiver employ the GSE. The simulation results show that the proposed algorithms significantly outperform the conventional unbiased estimator in the analyzed scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed group-based shrinkage estimators (GSEs) over the conventional least squares (LS) estimator in terms of parameter estimation and interference suppression scenarios?\n\n**A)** GSEs require a significant increase in computational complexity compared to LS estimators.\n**B)** GSEs improve the performance of LS estimators in terms of the mean-squared error (MSE) with a very modest increase in complexity.\n**C)** GSEs are only effective for single-carrier frequency-domain equalization (SC-FDE) of direct-sequence ultra-wideband (DS-UWB) systems.\n**D)** GSEs are limited to only one shrinkage factor, which is not adaptive.\n\n**Correct Answer:** B) GSEs improve the performance of LS estimators in terms of the mean-squared error (MSE) with a very modest increase in complexity.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed GSEs \"improve the performance of the conventional least squares (LS) estimator in terms of the mean-squared error (MSE), while requiring a very modest increase in complexity.\" This indicates that GSEs have a significant advantage over LS estimators in terms of MSE, while only requiring a small increase in computational complexity."}, "39": {"documentation": {"title": "Semiparametric Estimation of Treatment Effects in Randomized Experiments", "source": "Susan Athey, Peter J. Bickel, Aiyou Chen, Guido W. Imbens, Michael\n  Pollmann", "docs_id": "2109.02603", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiparametric Estimation of Treatment Effects in Randomized Experiments. We develop new semiparametric methods for estimating treatment effects. We focus on a setting where the outcome distributions may be thick tailed, where treatment effects are small, where sample sizes are large and where assignment is completely random. This setting is of particular interest in recent experimentation in tech companies. We propose using parametric models for the treatment effects, as opposed to parametric models for the full outcome distributions. This leads to semiparametric models for the outcome distributions. We derive the semiparametric efficiency bound for this setting, and propose efficient estimators. In the case with a constant treatment effect one of the proposed estimators has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes. Our analysis also results in an extension of Huber's model and trimmed mean to include asymmetry and a simplified condition on linear combinations of order statistics, which may be of independent interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a randomized experiment with a large sample size, where the treatment effect is small and the outcome distribution is thick-tailed. The assignment mechanism is completely random, and the treatment effect is assumed to be constant across all units.\n\n**Part A:** Derive the semiparametric efficiency bound for estimating the treatment effect in this setting. Assume that the outcome distribution is modeled using a semiparametric model, where the treatment effect is modeled using a parametric model.\n\n**Part B:** Suppose that the outcome distribution is modeled using a semiparametric model, where the treatment effect is modeled using a parametric model. Derive the asymptotic variance of the estimator for the treatment effect.\n\n**Part C:** Consider the case where the treatment effect is constant across all units. Show that one of the proposed estimators for the treatment effect has an interesting interpretation as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes.\n\n**Part D:** Discuss the implications of the analysis for Huber's model and trimmed mean, and how the results can be extended to include asymmetry and a simplified condition on linear combinations of order statistics.\n\n**A)** The semiparametric efficiency bound is given by the inverse of the Fisher information matrix, which is proportional to the inverse of the variance of the treatment effect estimator.\n\n**B)** The asymptotic variance of the estimator for the treatment effect is given by the inverse of the Fisher information matrix, which is proportional to the inverse of the variance of the treatment effect estimator.\n\n**C)** The estimator for the treatment effect can be interpreted as a weighted average of quantile treatment effects, with the weights proportional to (minus) the second derivative of the log of the density of the potential outcomes.\n\n**D)** The analysis results in an extension of Huber's model and trimmed mean to include asymmetry and a simplified condition on linear combinations of order statistics, which may be of independent interest. The results also provide a new perspective on the estimation of treatment effects in randomized experiments.\n\nCorrect Answer: C"}, "40": {"documentation": {"title": "EU Economic Modelling System", "source": "Olga Ivanova, d'Artis Kancs, Mark Thissen", "docs_id": "1912.07115", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "EU Economic Modelling System. This is the first study that attempts to assess the regional economic impacts of the European Institute of Innovation and Technology (EIT) investments in a spatially explicit macroeconomic model, which allows us to take into account all key direct, indirect and spatial spillover effects of EIT investments via inter-regional trade and investment linkages and a spatial diffusion of technology via an endogenously determined global knowledge frontier with endogenous growth engines driven by investments in knowledge and human capital. Our simulation results of highly detailed EIT expenditure data suggest that, besides sizable direct effects in those regions that receive the EIT investment support, there are also significant spatial spillover effects to other (non-supported) EU regions. Taking into account all key indirect and spatial spillover effects is a particular strength of the adopted spatial general equilibrium methodology; our results suggest that they are important indeed and need to be taken into account when assessing the impacts of EIT investment policies on regional economies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The European Institute of Innovation and Technology (EIT) investments have significant regional economic impacts, both direct and indirect, which can be assessed using a spatially explicit macroeconomic model. However, the model's ability to capture these impacts is limited by its reliance on inter-regional trade and investment linkages and a global knowledge frontier. Which of the following statements best describes the main strength of the adopted spatial general equilibrium methodology in this context?\n\nA) It allows for the estimation of direct effects of EIT investments on regional economies.\nB) It enables the capture of indirect effects of EIT investments on regional economies through inter-regional trade and investment linkages.\nC) It provides a detailed analysis of the spatial diffusion of technology across EU regions.\nD) It accounts for the endogenous growth engines driven by investments in knowledge and human capital.\n\n**Correct Answer:** B) It enables the capture of indirect effects of EIT investments on regional economies through inter-regional trade and investment linkages.\n\n**Explanation:** The correct answer is B) because the question specifically highlights the model's ability to capture indirect effects of EIT investments on regional economies through inter-regional trade and investment linkages. The other options are incorrect because they either focus on direct effects (A), spatial diffusion of technology (C), or endogenous growth engines (D), which are not the main strength of the adopted spatial general equilibrium methodology in this context."}, "41": {"documentation": {"title": "Dark matter filtering-out effect during a first-order phase transition", "source": "Dongjin Chway, Tae Hyun Jung, and Chang Sub Shin", "docs_id": "1912.04238", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dark matter filtering-out effect during a first-order phase transition. If the mass of dark matter is generated from a cosmological phase transition involving the nucleation of bubbles, the corresponding bubble walls can filter out dark matter particles during the phase transition. Only particles with sufficient momentum to overcome their mass inside the bubbles can pass through the walls. As a result, the dark matter number density after the phase transition has a suppression factor $\\exp(-M_\\chi/2\\tilde \\gamma T)$, where $M_\\chi$ is the dark matter mass, and $\\tilde \\gamma$ and $T$ are the Lorentz factor and temperature of the incoming fluid in the bubble wall rest frame, respectively. Under certain assumptions, we show that the filtering-out process can naturally provide a large suppression consistent with the observed dark matter density for a wide range of dark matter masses up to the Planck scale. Since the first-order phase transition is the decisive ingredient in our mechanism, a new connection is made between heavy dark matter scenarios and gravitational wave observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a first-order phase transition in the early universe where dark matter is generated through the nucleation of bubbles. Assuming the dark matter mass is $M_\\chi$, the Lorentz factor is $\\tilde \\gamma$, and the temperature of the incoming fluid is $T$, derive an expression for the suppression factor of the dark matter number density after the phase transition.\n\nA) $\\exp(-M_\\chi/2\\tilde \\gamma T)$\nB) $\\exp(-M_\\chi/4\\tilde \\gamma T)$\nC) $\\exp(-M_\\chi/8\\tilde \\gamma T)$\nD) $\\exp(-M_\\chi/16\\tilde \\gamma T)$\n\nCorrect Answer: A) $\\exp(-M_\\chi/2\\tilde \\gamma T)$\n\nExplanation: The suppression factor of the dark matter number density after the phase transition is given by the expression $\\exp(-M_\\chi/2\\tilde \\gamma T)$. This is derived from the fact that particles with sufficient momentum to overcome their mass inside the bubbles can pass through the walls, while those with insufficient momentum are filtered out. The correct expression takes into account the Lorentz factor $\\tilde \\gamma$ and temperature $T$ of the incoming fluid in the bubble wall rest frame."}, "42": {"documentation": {"title": "Informal Labour in India", "source": "Vinay Reddy Venumuddala", "docs_id": "2005.06795", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Informal Labour in India. India like many other developing countries is characterized by huge proportion of informal labour in its total workforce. The percentage of Informal Workforce is close to 92% of total as computed from NSSO 68th round on Employment and Unemployment, 2011-12. There are many traditional and geographical factors which might have been responsible for this staggering proportion of Informality in our country. As a part of this study, we focus mainly on finding out how Informality varies with Region, Sector, Gender, Social Group, and Working Age Groups. Further we look at how Total Inequality is contributed by Formal and Informal Labour, and how much do occupations/industries contribute to inequality within each of formal and informal labour groups separately. For the purposes of our study we use NSSO rounds 61 (2004-05) and 68 (2011-12) on employment and unemployment. The study intends to look at an overall picture of Informality, and based on the data highlight any inferences which are visible from the data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of regional, sectoral, and social factors on the proportion of informal labor in India, as per the NSSO rounds 61 (2004-05) and 68 (2011-12). How do these factors contribute to total inequality between formal and informal labor?\n\n**A)** The proportion of informal labor in India is highest in the rural sector, which is characterized by traditional and geographical factors such as lack of infrastructure and limited access to education.\n\n**B)** The sectoral distribution of informal labor in India is influenced by the presence of informal industries such as agriculture and small-scale manufacturing, which are often characterized by low wages and poor working conditions.\n\n**C)** The social factors that contribute to informal labor in India include the presence of marginalized groups such as Scheduled Castes and Scheduled Tribes, who are often forced into informal employment due to lack of access to education and job opportunities.\n\n**D)** The regional distribution of informal labor in India is influenced by the presence of informal labor in the unorganized sector, which is highest in the states of Uttar Pradesh and Bihar, where the lack of regulatory frameworks and enforcement mechanisms allows for the proliferation of informal employment.\n\n**Correct Answer:** B) The sectoral distribution of informal labor in India is influenced by the presence of informal industries such as agriculture and small-scale manufacturing, which are often characterized by low wages and poor working conditions.\n\n**Explanation:** The correct answer is based on the fact that the study focuses on the variation of informal labor with region, sector, gender, social group, and working age groups. The NSSO rounds 61 (2004-05) and 68 (2011-12) provide data on the sectoral distribution of informal labor in India, which shows that informal labor is highest in the unorganized sector, particularly in industries such as agriculture and small-scale manufacturing. These industries are often characterized by low wages and poor working conditions, which contribute to the high proportion of informal labor in India."}, "43": {"documentation": {"title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning", "source": "Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, Kun Zhang", "docs_id": "2107.02729", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning. One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments. In this paper, we propose a principled framework for adaptive RL, called \\textit{AdaRL}, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments. Specifically, we leverage a parsimonious graphical representation that characterizes structural relationships over variables in the RL system. Such graphical representations provide a compact way to encode what and where the changes across domains are, and furthermore inform us with a minimal set of changes that one has to consider for the purpose of policy adaptation. We show that by explicitly leveraging this compact representation to encode changes, we can efficiently adapt the policy to the target domain, in which only a few samples are needed and further policy optimization is avoided. We illustrate the efficacy of AdaRL through a series of experiments that vary factors in the observation, transition, and reward functions for Cartpole and Atari games."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a graphical representation in the proposed AdaRL framework for adaptive reinforcement learning, and how does it enable efficient policy adaptation to new environments?\n\n**A)** The graphical representation allows for the explicit encoding of changes across domains, enabling the adaptation of policies to new environments with minimal samples.\n\n**B)** The graphical representation provides a compact way to encode structural relationships over variables in the RL system, but does not necessarily enable efficient policy adaptation.\n\n**C)** The graphical representation is a necessary component of the AdaRL framework, but its primary function is to provide a visual representation of the RL system, rather than facilitating policy adaptation.\n\n**D)** The graphical representation is not used in the AdaRL framework, and policy adaptation is achieved through traditional reinforcement learning methods.\n\n**Correct Answer:** A) The graphical representation allows for the explicit encoding of changes across domains, enabling the adaptation of policies to new environments with minimal samples.\n\n**Explanation:** The correct answer is A) because the AdaRL framework leverages a graphical representation to encode structural relationships over variables in the RL system. This compact representation enables the explicit encoding of changes across domains, allowing for efficient policy adaptation to new environments with minimal samples. The other options are incorrect because they either misrepresent the role of the graphical representation or imply that it is not used in the AdaRL framework."}, "44": {"documentation": {"title": "Extension of the Lagrange multiplier test for error cross-section\n  independence to large panels with non normal errors", "source": "Zhaoyuan Li and Jianfeng Yao", "docs_id": "2103.06075", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Extension of the Lagrange multiplier test for error cross-section\n  independence to large panels with non normal errors. This paper reexamines the seminal Lagrange multiplier test for cross-section independence in a large panel model where both the number of cross-sectional units n and the number of time series observations T can be large. The first contribution of the paper is an enlargement of the test with two extensions: firstly the new asymptotic normality is derived in a simultaneous limiting scheme where the two dimensions (n, T) tend to infinity with comparable magnitudes; second, the result is valid for general error distribution (not necessarily normal). The second contribution of the paper is a new test statistic based on the sum of the fourth powers of cross-section correlations from OLS residuals, instead of their squares used in the Lagrange multiplier statistic. This new test is generally more powerful, and the improvement is particularly visible against alternatives with weak or sparse cross-section dependence. Both simulation study and real data analysis are proposed to demonstrate the advantages of the enlarged Lagrange multiplier test and the power enhanced test in comparison with the existing procedures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Reexamine the seminal Lagrange multiplier test for cross-section independence in a large panel model, considering the following:\n\nA) The test is only valid for normal errors and large panels with comparable magnitudes of n and T.\nB) The new test statistic based on the sum of the fourth powers of cross-section correlations from OLS residuals is more powerful than the traditional Lagrange multiplier statistic.\nC) The enlarged Lagrange multiplier test is only applicable to general error distributions, not necessarily normal.\nD) The test is generally more powerful against alternatives with weak or sparse cross-section dependence.\n\n**Correct Answer:** B) The new test statistic based on the sum of the fourth powers of cross-section correlations from OLS residuals is more powerful than the traditional Lagrange multiplier statistic.\n\n**Explanation:**\n\nThe correct answer is B) because the new test statistic proposed in the paper is based on the sum of the fourth powers of cross-section correlations from OLS residuals, which is generally more powerful than the traditional Lagrange multiplier statistic. This is stated in the paper as \"This new test is generally more powerful, and the improvement is particularly visible against alternatives with weak or sparse cross-section dependence.\"\n\nThe other options are incorrect because:\n\nA) The paper does not limit the test to normal errors, but rather extends it to general error distributions.\n\nC) The enlarged Lagrange multiplier test is actually valid for general error distributions, not just normal ones.\n\nD) While the new test is more powerful against alternatives with weak or sparse cross-section dependence, this is not the only scenario in which it is more powerful."}, "45": {"documentation": {"title": "$uvby-H_\\beta$ CCD photometry and membership segregation of the open\n  cluster NGC2682 (M67)", "source": "L. Balaguer-N\\'u\\~nez (1,2,3), D. Galad\\'i-Enr\\'iquez (4), C. Jordi\n  (1,5) ((1)UB Spain, (2)SHAO China,(3) IoA Cambridge UK,(4) IAA Spain,(5) IEEC\n  Spain)", "docs_id": "0704.2887", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$uvby-H_\\beta$ CCD photometry and membership segregation of the open\n  cluster NGC2682 (M67). Following deep astrometric and photometric study of the cluster NGC2682 (M67), we are able to accurately determine its fundamental parameters. Being an old and rich cluster, M67 is a relevant object for the analysis of the Galactic disk evolution. M67 is well studied but the lack of a wide and deep Stromgren photometric study makes our results worthwhile. The brightest stars of the open cluster M67 were used as uvby-Hbeta standard stars in our studies of NGC1817 and NGC2548, and the extension of the field covered, as well as the amount of observations, allowed to obtain the best set of Stromgren data ever published for this cluster. We discuss the results of our CCD uvby-Hbeta intermediate-band photometry, covering an area of about 50'x50' down to V 19. Moreover, a complete membership segregation based on astrometric and photometric criteria is obtained. The photometric analysis of a selected sample of stars yields a reddening value of E(b-y)= 0.03\\pm0.03, a distance modulus of V_0-M_V= 9.7\\pm0.2 and [Fe/H]= 0.01\\pm0.14. Through isochrone fitting we found an age of log t= 9.6\\pm0.1 (4.2\\pm0.2 Gyr). A clump of approximately 60 stars around V= 16, (b-y)= 0.4 could be interpreted as a population of pre-cataclysmic variable stars (if members), or as a stream of field G-type stars placed at twice the distance of the cluster (if non-members)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the estimated age of the open cluster M67, based on the isochrone fitting analysis?\n\nA) 4.2 \u00b1 0.2 Gyr\nB) 5.0 \u00b1 0.5 Gyr\nC) 6.0 \u00b1 0.6 Gyr\nD) 7.0 \u00b1 0.7 Gyr\n\nCorrect Answer: A) 4.2 \u00b1 0.2 Gyr\n\nExplanation: The text states that the age of M67 was found to be log t = 9.6 \u00b1 0.1, which corresponds to an age of 4.2 \u00b1 0.2 Gyr. This is the correct answer. The other options are incorrect, as they do not match the estimated age provided in the text."}, "46": {"documentation": {"title": "A Deep Learning Approach for Dynamic Balance Sheet Stress Testing", "source": "Anastasios Petropoulos, Vassilis Siakoulis, Konstantinos P. Panousis,\n  Theodoros Christophides, and Sotirios Chatzis", "docs_id": "2009.11075", "section": ["q-fin.CP", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Deep Learning Approach for Dynamic Balance Sheet Stress Testing. In the aftermath of the financial crisis, supervisory authorities have considerably improved their approaches in performing financial stress testing. However, they have received significant criticism by the market participants due to the methodological assumptions and simplifications employed, which are considered as not accurately reflecting real conditions. First and foremost, current stress testing methodologies attempt to simulate the risks underlying a financial institution's balance sheet by using several satellite models, making their integration a really challenging task with significant estimation errors. Secondly, they still suffer from not employing advanced statistical techniques, like machine learning, which capture better the nonlinear nature of adverse shocks. Finally, the static balance sheet assumption, that is often employed, implies that the management of a bank passively monitors the realization of the adverse scenario, but does nothing to mitigate its impact. To address the above mentioned criticism, we introduce in this study a novel approach utilizing deep learning approach for dynamic balance sheet stress testing. Experimental results give strong evidence that deep learning applied in big financial/supervisory datasets create a state of the art paradigm, which is capable of simulating real world scenarios in a more efficient way."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary limitations of current stress testing methodologies in simulating the risks underlying a financial institution's balance sheet, and how does the proposed deep learning approach address these limitations?\n\n**A)** Current stress testing methodologies are limited by their inability to capture the nonlinear nature of adverse shocks, and the proposed approach addresses this by employing advanced statistical techniques, such as machine learning. However, the approach does not account for the management's passive monitoring of adverse scenarios.\n\n**B)** Current stress testing methodologies are limited by their reliance on satellite models, which are difficult to integrate, and the proposed approach addresses this by utilizing a deep learning approach that can simulate real-world scenarios in a more efficient way. However, the approach does not employ advanced statistical techniques to capture nonlinear relationships.\n\n**C)** Current stress testing methodologies are limited by their static balance sheet assumption, which implies that management passively monitors adverse scenarios, and the proposed approach addresses this by introducing a dynamic balance sheet stress testing approach that captures the management's proactive response to mitigate the impact of adverse scenarios. However, the approach does not employ advanced statistical techniques to capture nonlinear relationships.\n\n**D)** Current stress testing methodologies are limited by their inability to capture the nonlinear nature of adverse shocks and their reliance on satellite models, which are difficult to integrate, and the proposed approach addresses these limitations by employing a deep learning approach that can simulate real-world scenarios in a more efficient way and employing advanced statistical techniques to capture nonlinear relationships.\n\n**Correct Answer:** D) Current stress testing methodologies are limited by their inability to capture the nonlinear nature of adverse shocks and their reliance on satellite models, which are difficult to integrate, and the proposed approach addresses these limitations by employing a deep learning approach that can simulate real-world scenarios in a more efficient way and employing advanced statistical techniques to capture nonlinear relationships.\n\n**Explanation:** The correct answer, D, accurately summarizes the primary limitations of current stress testing methodologies and how the proposed deep learning approach addresses these limitations. The approach is capable of simulating real-world scenarios in a more efficient way and employing advanced statistical techniques to capture nonlinear relationships, which are not adequately addressed by current methodologies."}, "47": {"documentation": {"title": "Phenomenological implications of asymmetric $AdS_5$ shockwave collision\n  studies for heavy ion physics", "source": "Berndt M\\\"uller, Andreas Rabenstein, Andreas Sch\\\"afer, Sebastian\n  Waeber, Laurence G. Yaffe", "docs_id": "2001.07161", "section": ["hep-ph", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phenomenological implications of asymmetric $AdS_5$ shockwave collision\n  studies for heavy ion physics. This paper discusses possible phenomenological implications for p+A and A+A collisions of the results of recent numerical AdS/CFT calculations examining asymmetric collisions of planar shocks. In view of the extreme Lorentz contraction, we model highly relativistic heavy ion collisions (HICs) as a superposition of collisions between many near-independent transverse \"pixels\" with differing incident longitudinal momenta. It was found that also for asymmetric collisions the hydrodynamization time is in good approximation a proper time, just like for symmetric collisions, depending on the geometric mean of the longitudinally integrated energy densities of the incident projectiles. For realistic collisions with fluctuations in the initial energy densities, these results imply a substantial increase in the hydrodynamization time for highly asymmetric pixels. However, even in this case the local hydrodynamization time still is significantly smaller than perturbative results for the thermalization time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the phenomenological implications of asymmetric $AdS_5$ shockwave collision, what is the implication of the hydrodynamization time being in good approximation a proper time, just like for symmetric collisions, depending on the geometric mean of the longitudinally integrated energy densities of the incident projectiles?\n\n**A)** The hydrodynamization time is independent of the energy densities of the incident projectiles.\n**B)** The hydrodynamization time is significantly smaller than perturbative results for the thermalization time for highly asymmetric collisions.\n**C)** The hydrodynamization time is in good approximation a proper time, just like for symmetric collisions, depending on the geometric mean of the longitudinally integrated energy densities of the incident projectiles.\n**D)** The hydrodynamization time is always larger than perturbative results for the thermalization time for highly asymmetric collisions.\n\n**Correct Answer:** C) The hydrodynamization time is in good approximation a proper time, just like for symmetric collisions, depending on the geometric mean of the longitudinally integrated energy densities of the incident projectiles.\n\n**Explanation:** The correct answer is C) because the paper states that the hydrodynamization time is in good approximation a proper time, just like for symmetric collisions, depending on the geometric mean of the longitudinally integrated energy densities of the incident projectiles. This means that the hydrodynamization time is not independent of the energy densities of the incident projectiles, but rather depends on them in a specific way. The other options are incorrect because they either misstate the relationship between the hydrodynamization time and the energy densities (A), or make an incorrect generalization about the relationship between the hydrodynamization time and the thermalization time (B and D)."}, "48": {"documentation": {"title": "Seebeck effect at the atomic scale", "source": "Eui-Sup Lee, Sanghee Cho, Ho-Ki Lyeo, and Yong-Hyun Kim", "docs_id": "1307.3742", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Seebeck effect at the atomic scale. The atomic variations of electronic wavefunctions at the surface and electron scattering near a defect have been detected unprecedentedly by tracing thermoelectric voltages given a temperature bias [Cho et al., Nature Mater. 12, 913 (2013)]. Because thermoelectricity, or Seebeck effect, is associated with heat-induced electron diffusion, how the thermoelectric signal is related to the atomic-scale wavefunctions and what the role of the temperature is at such a length scale remain very unclear. Here we show that coherent electron and heat transport through a point-like contact produces an atomic Seebeck effect, which is described by mesoscopic Seebeck coefficient multiplied with an effective temperature drop at the interface. The mesoscopic Seebeck coefficient is approximately proportional to the logarithmic energy derivative of local density of states at the Fermi energy. We deduced that the effective temperature drop at the tip-sample junction could vary at a sub-angstrom scale depending on atom-to-atom interaction at the interface. A computer-based simulation method of thermoelectric images is proposed, and a point defect in graphene was identified by comparing experiment and the simulation of thermoelectric imaging."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the mesoscopic Seebeck coefficient and the local density of states at the Fermi energy, as described in the study on the atomic Seebeck effect?\n\nA) The mesoscopic Seebeck coefficient is directly proportional to the local density of states at the Fermi energy.\nB) The mesoscopic Seebeck coefficient is approximately proportional to the logarithmic energy derivative of the local density of states at the Fermi energy.\nC) The mesoscopic Seebeck coefficient is inversely proportional to the local density of states at the Fermi energy.\nD) The mesoscopic Seebeck coefficient is independent of the local density of states at the Fermi energy.\n\nCorrect Answer: B) The mesoscopic Seebeck coefficient is approximately proportional to the logarithmic energy derivative of the local density of states at the Fermi energy.\n\nExplanation: The study describes the mesoscopic Seebeck coefficient as being approximately proportional to the logarithmic energy derivative of the local density of states at the Fermi energy. This relationship is crucial in understanding the atomic-scale Seebeck effect and its connection to the thermoelectric signal. The correct answer requires the test-taker to accurately recall and apply the mathematical relationship described in the study."}, "49": {"documentation": {"title": "Tangling clustering of inertial particles in stably stratified\n  turbulence", "source": "A. Eidelman, T. Elperin, N. Kleeorin, B. Melnik, I. Rogachevskii", "docs_id": "0911.4814", "section": ["physics.flu-dyn", "astro-ph.EP", "nlin.CD", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tangling clustering of inertial particles in stably stratified\n  turbulence. We have predicted theoretically and detected in laboratory experiments a new type of particle clustering (tangling clustering of inertial particles) in a stably stratified turbulence with imposed mean vertical temperature gradient. In this stratified turbulence a spatial distribution of the mean particle number density is nonuniform due to the phenomenon of turbulent thermal diffusion, that results in formation of a gradient of the mean particle number density, \\nabla N, and generation of fluctuations of the particle number density by tangling of the gradient, \\nabla N, by velocity fluctuations. The mean temperature gradient, \\nabla T, produces the temperature fluctuations by tangling of the gradient, \\nabla T, by velocity fluctuations. These fluctuations increase the rate of formation of the particle clusters in small scales. In the laboratory stratified turbulence this tangling clustering is much more effective than a pure inertial clustering that has been observed in isothermal turbulence. In particular, in our experiments in oscillating grid isothermal turbulence in air without imposed mean temperature gradient, the inertial clustering is very weak for solid particles with the diameter 10 microns and Reynolds numbers Re =250. Our theoretical predictions are in a good agreement with the obtained experimental results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of tangling clustering of inertial particles in stably stratified turbulence, what is the primary mechanism by which the mean particle number density gradient, \u2207N, is generated?\n\nA) Tangling of the temperature gradient, \u2207T, by velocity fluctuations\nB) Tangling of the particle number density gradient, \u2207N, by thermal diffusion\nC) Tangling of the mean temperature gradient, \u2207T, by particle collisions\nD) Tangling of the particle number density gradient, \u2207N, by turbulent eddies\n\n**Correct Answer:** A) Tangling of the temperature gradient, \u2207T, by velocity fluctuations\n\n**Explanation:** According to the documentation, the mean temperature gradient, \u2207T, produces temperature fluctuations by tangling of the gradient, \u2207T, by velocity fluctuations. These temperature fluctuations, in turn, contribute to the generation of particle clusters by tangling the particle number density gradient, \u2207N, by velocity fluctuations. Therefore, option A is the correct answer.\n\n**Additional explanation:** The other options are incorrect because:\n\n* Option B is incorrect because thermal diffusion is responsible for the formation of the particle number density gradient, \u2207N, but not its generation.\n* Option C is incorrect because particle collisions are not mentioned as a mechanism for generating the mean particle number density gradient, \u2207N.\n* Option D is incorrect because turbulent eddies are responsible for generating fluctuations in the particle number density, but not the mean particle number density gradient, \u2207N."}, "50": {"documentation": {"title": "Is there contextuality in behavioral and social systems?", "source": "Ehtibar Dzhafarov, Ru Zhang, and Janne Kujala", "docs_id": "1504.07422", "section": ["q-bio.NC", "math.PR", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Is there contextuality in behavioral and social systems?. Most behavioral and social experiments aimed at revealing contextuality are confined to cyclic systems with binary outcomes. In quantum physics, this broad class of systems includes as special cases Klyachko-Can-Binicioglu-Shumovsky-type, Einstein-Podolsky-Rosen-Bell-type, and Suppes-Zanotti-Leggett-Garg-type systems. The theory of contextuality known as Contextuality-by-Default allows one to define and measure contextuality in all such system, even if there are context-dependent errors in measurements, or if something in the contexts directly interacts with the measurements. This makes the theory especially suitable for behavioral and social systems, where direct interactions of \"everything with everything\" are ubiquitous. For cyclic systems with binary outcomes the theory provides necessary and sufficient conditions for noncontextuality, and these conditions are known to be breached in certain quantum systems. We review several behavioral and social data sets (from polls of public opinion to visual illusions to conjoint choices to word combinations to psychophysical matching), and none of these data provides any evidence for contextuality. Our working hypothesis is that this may be a broadly applicable rule: behavioral and social systems are noncontextual, i.e., all \"contextual effects\" in them result from the ubiquitous dependence of response distributions on the elements of contexts other than the ones to which the response is presumably or normatively directed."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: What is the primary conclusion drawn by the authors regarding the nature of contextuality in behavioral and social systems, as evidenced by their review of various data sets?\n\nA) Behavioral and social systems exhibit contextuality, and this is a fundamental property of their functioning.\nB) Behavioral and social systems are noncontextual, and all apparent contextuality can be attributed to factors outside the context of interest.\nC) The authors are unable to determine whether behavioral and social systems are contextually or noncontextually, and more research is needed.\nD) The review of data sets provides no conclusive evidence for or against contextuality in behavioral and social systems.\n\nCorrect Answer: B) Behavioral and social systems are noncontextual, and all apparent contextuality can be attributed to factors outside the context of interest.\n\nExplanation: The authors' working hypothesis is that behavioral and social systems are noncontextual, and that any apparent contextuality can be explained by factors outside the context of interest. This is supported by their review of various data sets, which found no evidence of contextuality. The authors suggest that contextuality may be a broadly applicable rule, and that behavioral and social systems may be more accurately described as noncontextual."}, "51": {"documentation": {"title": "The Two-user Causal Cognitive Interference Channel: Novel Outer Bounds\n  and Constant Gap Result for the Symmetric Gaussian Noise Channel in Weak\n  Interference", "source": "Martina Cardone, Daniela Tuninetti, Raymond Knopp", "docs_id": "1503.07372", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Two-user Causal Cognitive Interference Channel: Novel Outer Bounds\n  and Constant Gap Result for the Symmetric Gaussian Noise Channel in Weak\n  Interference. This paper studies the two-user Causal Cognitive Interference Channel (CCIC), where two transmitters aim to communicate independent messages to two different receivers via a common channel. One source, referred to as the cognitive, is capable of overhearing the other source, referred to as the primary, through a noisy in-band link and thus can assist in sending the primary's data. Two novel outer bounds of the type $2R_p+R_c$ and $R_p+2R_c$ are derived for the class of injective semi-deterministic CCICs where the noises at the different source-destination pairs are independent. An achievable rate region is derived based on Gelfand-Pinsker binning, superposition coding and simultaneous decoding at the receivers. The lower and outer bounds are then specialized to the practically relevant Gaussian noise case. The authors of this paper recently characterized to within a constant gap the capacity of the symmetric Gaussian CCIC in (a) the strong interference regime, and (b) for a subset of the weak interference regime when the cooperation link is larger than a given threshold. This work characterizes to within a constant gap the capacity for the symmetric Gaussian CCIC in the regime that was still open. In particular, it is shown that the novel outer bounds are necessary to characterize the capacity to within a constant gap when the cooperation link is weaker than the direct links, that is, in this regime unilateral cooperation leaves some system resources underutilized."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Two-user Causal Cognitive Interference Channel (CCIC), what is the significance of the novel outer bounds derived for the class of injective semi-deterministic CCICs, and how do they relate to the capacity characterization in the weak interference regime?\n\n**A)** The novel outer bounds are sufficient to characterize the capacity of the symmetric Gaussian CCIC in the weak interference regime, but they are not necessary.\n\n**B)** The novel outer bounds are necessary to characterize the capacity of the symmetric Gaussian CCIC in the weak interference regime, but they are not sufficient.\n\n**C)** The novel outer bounds are both necessary and sufficient to characterize the capacity of the symmetric Gaussian CCIC in the weak interference regime, and they are also sufficient for the strong interference regime.\n\n**D)** The novel outer bounds are neither necessary nor sufficient to characterize the capacity of the symmetric Gaussian CCIC in the weak interference regime, and unilateral cooperation leaves some system resources underutilized.\n\n**Correct Answer:** B) The novel outer bounds are necessary to characterize the capacity of the symmetric Gaussian CCIC in the weak interference regime, but they are not sufficient.\n\n**Explanation:**\n\nThe correct answer is B) because the paper states that the novel outer bounds are necessary to characterize the capacity to within a constant gap when the cooperation link is weaker than the direct links, but they are not sufficient to achieve the capacity in this regime. This means that the bounds provide a lower bound on the capacity, but there may be other strategies that can achieve the capacity.\n\nThe incorrect answers are:\n\nA) The paper does not state that the bounds are sufficient to characterize the capacity in the weak interference regime.\n\nC) The paper does not state that the bounds are sufficient for the strong interference regime.\n\nD) The paper actually states that the novel outer bounds are necessary to characterize the capacity in the weak interference regime, and that unilateral cooperation leaves some system resources underutilized, but this is not the same as saying that the bounds are neither necessary nor sufficient."}, "52": {"documentation": {"title": "CP Decomposition for Tensors via Alternating Least Squares with QR\n  Decomposition", "source": "Rachel Minster, Irina Viviano, Xiaotian Liu, Grey Ballard", "docs_id": "2112.10855", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CP Decomposition for Tensors via Alternating Least Squares with QR\n  Decomposition. The CP tensor decomposition is used in applications such as machine learning and signal processing to discover latent low-rank structure in multidimensional data. Computing a CP decomposition via an alternating least squares (ALS) method reduces the problem to several linear least squares problems. The standard way to solve these linear least squares subproblems is to use the normal equations, which inherit special tensor structure that can be exploited for computational efficiency. However, the normal equations are sensitive to numerical ill-conditioning, which can compromise the results of the decomposition. In this paper, we develop versions of the CP-ALS algorithm using the QR decomposition and the singular value decomposition (SVD), which are more numerically stable than the normal equations, to solve the linear least squares problems. Our algorithms utilize the tensor structure of the CP-ALS subproblems efficiently, have the same complexity as the standard CP-ALS algorithm when the rank is small, and are shown via examples to produce more stable results when ill-conditioning is present. Our MATLAB implementation achieves the same running time as the standard algorithm for small ranks, and we show that the new methods can obtain lower approximation error and more reliably recover low-rank signals from data with known ground truth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the QR decomposition and SVD in the CP-ALS algorithm, as opposed to the normal equations, in terms of numerical stability?\n\n**A)** The QR decomposition and SVD are more computationally intensive than the normal equations.\n**B)** The QR decomposition and SVD are more sensitive to numerical ill-conditioning than the normal equations.\n**C)** The QR decomposition and SVD are more numerically stable and can produce more accurate results when dealing with ill-conditioned linear least squares problems.\n**D)** The QR decomposition and SVD are less efficient than the normal equations for solving linear least squares problems.\n\n**Correct Answer:** C) The QR decomposition and SVD are more numerically stable and can produce more accurate results when dealing with ill-conditioned linear least squares problems.\n\n**Explanation:** The correct answer is C) because the QR decomposition and SVD are more numerically stable than the normal equations, which can be sensitive to numerical ill-conditioning. This is a key advantage of the proposed algorithms, as it allows for more accurate results to be obtained even in cases where the linear least squares problems are ill-conditioned. The other options are incorrect because they either misstate the advantage of the QR decomposition and SVD (A and B) or incorrectly state that they are less efficient than the normal equations (D)."}, "53": {"documentation": {"title": "Spectro-astrometry of V1515 Cyg", "source": "V. Agra-Amboage and P. J. V. Garcia", "docs_id": "1403.8112", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectro-astrometry of V1515 Cyg. FU Orionis objects are a class of young stars with important bursts in luminosity and which show evidence of accretion and ejection activity. It is generally accepted that they are surrounded by a Keplerian circumstellar disk and an infalling envelope. The outburst would occurs because of a sudden increase in the accretion rate. We aim at studying the regions closer to the central star in order to observe the signs of the accretion/ejection activity. We present optical observations of the Halpha line using the Integral Field Spectrograph OASIS, at the William Herschel Telescope, combined with Adaptive Optics. Since this technique gives the spectral information for both spatial directions, we carried out a two-dimensional spectro-astrometric study of the signal. We measured a clear spectro-astrometric signal in the North-South direction. The cross-correlation between the spectra showed a spatial distribution in velocity suggestive of scattering by a disk surrounding the star. This would be one of the few spatial inferences of a disk observed in a FU Orionis object. However, in order to fully understand the observed structure, higher angular and spectral resolution observations are required. V1515 Cyg appears now as an important object to be observed with a new generation of instruments to increase our knowledge about the disk and outflows structure in FU Orionis objects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the circumstellar disk surrounding V1515 Cyg based on the spectro-astrometric study of the Halpha line?\n\nA) The disk is a Keplerian disk with a constant angular momentum\nB) The disk is a flat disk with no rotation\nC) The disk is surrounded by a shell of gas and dust, with a velocity distribution suggestive of scattering by the disk\nD) The disk is a thin disk with no accretion activity\n\nCorrect Answer: C) The disk is surrounded by a shell of gas and dust, with a velocity distribution suggestive of scattering by the disk\n\nExplanation: The spectro-astrometric study of the Halpha line showed a clear spectro-astrometric signal in the North-South direction, indicating a spatial distribution in velocity. This is suggestive of scattering by a disk surrounding the star, as the velocity distribution would be affected by the disk's presence. The correct answer is C) The disk is surrounded by a shell of gas and dust, with a velocity distribution suggestive of scattering by the disk. The other options are incorrect because they do not accurately describe the inferred structure of the circumstellar disk. Option A is incorrect because the study did not provide information about the angular momentum of the disk. Option B is incorrect because the study showed a spatial distribution in velocity, indicating that the disk is not flat. Option D is incorrect because the study did not provide information about the accretion activity of the disk."}, "54": {"documentation": {"title": "Real-time estimation of the short-run impact of COVID-19 on economic\n  activity using electricity market data", "source": "Carlo Fezzi, Valeria Fanghella", "docs_id": "2007.03477", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Real-time estimation of the short-run impact of COVID-19 on economic\n  activity using electricity market data. The COVID-19 pandemic has caused more than 8 million confirmed cases and 500,000 death to date. In response to this emergency, many countries have introduced a series of social-distancing measures including lockdowns and businesses' temporary shutdowns, in an attempt to curb the spread of the infection. Accordingly, the pandemic has been generating unprecedent disruption on practically every aspect of society. This paper demonstrates that high-frequency electricity market data can be used to estimate the causal, short-run impact of COVID-19 on the economy. In the current uncertain economic conditions, timeliness is essential. Unlike official statistics, which are published with a delay of a few months, with our approach one can monitor virtually every day the impact of the containment policies, the extent of the recession and measure whether the monetary and fiscal stimuli introduced to address the crisis are being effective. We illustrate our methodology on daily data for the Italian day-ahead power market. Not surprisingly, we find that the containment measures caused a significant reduction in economic activities and that the GDP at the end of in May 2020 is still about 11% lower that what it would have been without the outbreak."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper suggest that high-frequency electricity market data can be used to estimate the causal, short-run impact of COVID-19 on the economy. What are the advantages of using high-frequency electricity market data over official statistics in monitoring the impact of containment policies during the pandemic?\n\n**A)** High-frequency data allows for more accurate estimation of economic activities, while official statistics are prone to errors due to sampling biases.\n**B)** High-frequency data provides real-time information, enabling policymakers to make timely decisions, whereas official statistics are published with a delay of several months.\n**C)** High-frequency data is more reliable than official statistics due to its ability to capture subtle changes in economic activity, whereas official statistics are often based on aggregate data.\n**D)** High-frequency data is less expensive to collect than official statistics, making it a more cost-effective option for monitoring economic activity.\n\n**Correct Answer:** B) High-frequency data provides real-time information, enabling policymakers to make timely decisions, whereas official statistics are published with a delay of several months.\n\n**Explanation:** The correct answer is B) because the paper highlights the importance of timeliness in monitoring the impact of containment policies during the pandemic. The authors argue that using high-frequency electricity market data allows for real-time monitoring, enabling policymakers to make timely decisions, whereas official statistics are published with a delay of several months. This is a key advantage of using high-frequency data, as it enables policymakers to respond quickly to changes in economic activity."}, "55": {"documentation": {"title": "Renewable Energy Targets and Unintended Storage Cycling: Implications\n  for Energy Modeling", "source": "Martin Kittel, Wolf-Peter Schill", "docs_id": "2107.13380", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Renewable Energy Targets and Unintended Storage Cycling: Implications\n  for Energy Modeling. To decarbonize the economy, many governments have set targets for the use of renewable energy sources. These are often formulated as relative shares of electricity demand or supply. Implementing respective constraints in energy models is a surprisingly delicate issue. They may cause a modeling artifact of excessive electricity storage use. We introduce this phenomenon as 'unintended storage cycling', which can be detected in case of simultaneous storage charging and discharging. In this paper, we provide an analytical representation of different approaches for implementing minimum renewable share constraints in models, and show how these may lead to unintended storage cycling. Using a parsimonious optimization model, we quantify related distortions of optimal dispatch and investment decisions as well as market prices, and identify important drivers of the phenomenon. Finally, we provide recommendations on how to avoid the distorting effects of unintended storage cycling in energy modeling."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the term used to describe the phenomenon where simultaneous storage charging and discharging in energy models leads to excessive electricity storage use, and how can it be detected?\n\n**A)** Renewable Energy Targets and Unintended Storage Cycling: Implications for Energy Modeling\n**B)** Storage Cycling and Market Distortions\n**C)** Energy Modeling Artifact and Storage Cycling\n**D)** Renewable Energy Targets and Unintended Storage Cycling: Implications for Energy Modeling\n\n**Correct Answer:** D) Renewable Energy Targets and Unintended Storage Cycling: Implications for Energy Modeling\n\n**Explanation:** The correct answer is D) Renewable Energy Targets and Unintended Storage Cycling: Implications for Energy Modeling. The term \"unintended storage cycling\" is introduced in the paper to describe the phenomenon of excessive electricity storage use, and it is explicitly mentioned in the title of the paper. The other options are incorrect because they either repeat the title or are not directly related to the concept of unintended storage cycling.\n\n**Explanation for each candidate:**\n\nA) This option is incorrect because it is the title of the paper, but it does not specifically describe the phenomenon of unintended storage cycling.\n\nB) This option is incorrect because it is a plausible-sounding term, but it is not the actual term used in the paper to describe the phenomenon.\n\nC) This option is incorrect because it is a vague phrase that does not accurately describe the phenomenon of unintended storage cycling.\n\nD) This option is correct because it is the actual term used in the paper to describe the phenomenon, and it is explicitly mentioned in the title."}, "56": {"documentation": {"title": "A Variational Inequality Approach to Bayesian Regression Games", "source": "Wenshuo Guo, Michael I. Jordan, Tianyi Lin", "docs_id": "2103.13509", "section": ["cs.LG", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Variational Inequality Approach to Bayesian Regression Games. Bayesian regression games are a special class of two-player general-sum Bayesian games in which the learner is partially informed about the adversary's objective through a Bayesian prior. This formulation captures the uncertainty in regard to the adversary, and is useful in problems where the learner and adversary may have conflicting, but not necessarily perfectly antagonistic objectives. Although the Bayesian approach is a more general alternative to the standard minimax formulation, the applications of Bayesian regression games have been limited due to computational difficulties, and the existence and uniqueness of a Bayesian equilibrium are only known for quadratic cost functions. First, we prove the existence and uniqueness of a Bayesian equilibrium for a class of convex and smooth Bayesian games by regarding it as a solution of an infinite-dimensional variational inequality (VI) in Hilbert space. We consider two special cases in which the infinite-dimensional VI reduces to a high-dimensional VI or a nonconvex stochastic optimization, and provide two simple algorithms of solving them with strong convergence guarantees. Numerical results on real datasets demonstrate the promise of this approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Bayesian regression games, what is the primary limitation of the Bayesian approach, and how does it differ from the standard minimax formulation?\n\n**A)** The Bayesian approach is limited by computational difficulties due to the need to approximate the posterior distribution, whereas the minimax formulation is more computationally efficient. However, the minimax approach assumes a perfectly antagonistic relationship between the learner and adversary, whereas the Bayesian approach captures uncertainty in the adversary's objective.\n\n**B)** The Bayesian approach is limited by the requirement that the cost functions be quadratic, whereas the minimax formulation can handle more general cost functions. However, the Bayesian approach provides a more general alternative to the minimax formulation, allowing for conflicting but not necessarily perfectly antagonistic objectives.\n\n**C)** The Bayesian approach is limited by the need to specify a prior distribution over the adversary's objective, whereas the minimax formulation does not require this assumption. However, the Bayesian approach provides a more nuanced representation of uncertainty in the adversary's objective, whereas the minimax approach assumes a more simplistic model.\n\n**D)** The Bayesian approach is limited by the requirement that the games be convex and smooth, whereas the minimax formulation can handle more general game structures. However, the Bayesian approach provides a more general framework for analyzing Bayesian games, whereas the minimax approach is more restrictive in its assumptions.\n\n**Correct Answer:** B) The Bayesian approach is limited by the requirement that the cost functions be quadratic, whereas the minimax formulation can handle more general cost functions. However, the Bayesian approach provides a more general alternative to the minimax formulation, allowing for conflicting but not necessarily perfectly antagonistic objectives."}, "57": {"documentation": {"title": "Hydrodynamics of galactic dark matter", "source": "Luis G. Cabral-Rosetti, Tonatiuh Matos, Dario Nu\\~nez and Roberto A.\n  Sussman", "docs_id": "gr-qc/0112044", "section": ["gr-qc", "astro-ph", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hydrodynamics of galactic dark matter. We consider simple hydrodynamical models of galactic dark matter in which the galactic halo is a self-gravitating and self-interacting gas that dominates the dynamics of the galaxy. Modeling this halo as a sphericaly symmetric and static perfect fluid satisfying the field equations of General Relativity, visible barionic matter can be treated as ``test particles'' in the geometry of this field. We show that the assumption of an empirical ``universal rotation curve'' that fits a wide variety of galaxies is compatible, under suitable approximations, with state variables characteristic of a non-relativistic Maxwell-Boltzmann gas that becomes an isothermal sphere in the Newtonian limit. Consistency criteria lead to a minimal bound for particle masses in the range $30 \\hbox{eV} \\leq m \\leq 60 \\hbox{eV}$ and to a constraint between the central temperature and the particles mass. The allowed mass range includes popular supersymmetric particle candidates, such as the neutralino, axino and gravitino, as well as lighter particles ($m\\approx$ keV) proposed by numerical N-body simulations associated with self-interactive CDM and WDM structure formation theories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of hydrodynamics of galactic dark matter, what is the minimal bound on the mass of particles that can satisfy the consistency criteria, and how does this bound relate to the central temperature of the dark matter halo?\n\n**A)** The minimal bound on particle mass is $10 \\hbox{keV}$, and the central temperature is inversely proportional to the particle mass.\n\n**B)** The minimal bound on particle mass is $30 \\hbox{eV}$, and the central temperature is directly proportional to the particle mass.\n\n**C)** The minimal bound on particle mass is $60 \\hbox{eV}$, and the central temperature is inversely proportional to the square of the particle mass.\n\n**D)** The minimal bound on particle mass is $30 \\hbox{eV}$, and the central temperature is inversely proportional to the particle mass, with a constraint that the central temperature must be greater than $10 \\hbox{keV}$.\n\n**Correct Answer:** D) The minimal bound on particle mass is $30 \\hbox{eV}$, and the central temperature is inversely proportional to the particle mass, with a constraint that the central temperature must be greater than $10 \\hbox{keV}$.\n\n**Explanation:** The correct answer is D) because the documentation states that the minimal bound on particle mass is indeed $30 \\hbox{eV}$, and the central temperature is inversely proportional to the particle mass, with a constraint that the central temperature must be greater than $10 \\hbox{keV}$. This constraint is mentioned in the original text as a result of the consistency criteria."}, "58": {"documentation": {"title": "Turbulence without Richardson-Kolmogorov cascade", "source": "Nicolas Mazellier and Christos Vassilicos", "docs_id": "0911.0841", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Turbulence without Richardson-Kolmogorov cascade. We present an experimental investigation of intense turbulence generated by a class of low-blockage space-filling fractal square grids. We confirm the existence of a protacted production region followed by a decaying region, as first reported by Hurst & Vassilicos (Physics of Fluids, 2007). We show that the centerline streamwise variation of most of the statistical properties of the turbulent flow can be scaled by a wake interaction length-scale $x_*$. We also confirm the finding of Seoud and Vassilicos (Physics of Fluids, 2007) that the ratio of the integral length-scale $L_u$ to the Taylor micro-scale $\\lambda$ remains constant in the decaying region whereas the Reynolds number $Re_\\lambda$ strongly decreases. As a result the scaling $L_{u}/\\lambda \\sim Re_{\\lambda}$ which follows from the $u'^{3}/L_u$ scaling of the dissipation rate in boundary-free shear flows and in usual grid-generated turbulence does not hold here. However, we show that the ratio $L_u/\\lambda$ is an increasing function of the inlet Reynolds number $Re_0$. This extraordinary decoupling is consistent with a self-preserving single length-scale decaying homogeneous turbulence proposed by George & Wang (Physics of Fluids, 2009) with which our results are compared."}, "answer": "Here is the multi-form question:\n\nWhat is the primary conclusion regarding the scaling of the integral length-scale $L_u$ to the Taylor micro-scale $\\lambda$ in the decaying region of the turbulent flow generated by the fractal square grid?\n\nA) The ratio $L_u/\\lambda$ remains constant and is independent of the inlet Reynolds number $Re_0$.\nB) The ratio $L_u/\\lambda$ is an increasing function of the inlet Reynolds number $Re_0$.\nC) The ratio $L_u/\\lambda$ is a decreasing function of the inlet Reynolds number $Re_0$.\nD) The ratio $L_u/\\lambda$ is a constant and does not depend on the inlet Reynolds number $Re_0$.\n\nCorrect Answer: B) The ratio $L_u/\\lambda$ is an increasing function of the inlet Reynolds number $Re_0$.\n\nExplanation: The correct answer is B) The ratio $L_u/\\lambda$ is an increasing function of the inlet Reynolds number $Re_0$. This is because the experimental investigation confirms that the ratio $L_u/\\lambda$ is an increasing function of the inlet Reynolds number $Re_0$, which is a key finding in the study. The other options are incorrect because they do not accurately reflect the results of the study. Option A is incorrect because the ratio $L_u/\\lambda$ does not remain constant in the decaying region. Option C is incorrect because the ratio $L_u/\\lambda$ is not a decreasing function of the inlet Reynolds number $Re_0$. Option D is incorrect because the ratio $L_u/\\lambda$ is not a constant and does depend on the inlet Reynolds number $Re_0$."}, "59": {"documentation": {"title": "RXTE and XMM observations of intermediate polar candidates", "source": "O. W. Butters, A. J. Norton, K. Mukai, J. A. Tomsick", "docs_id": "1011.5762", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RXTE and XMM observations of intermediate polar candidates. Aims. To determine the credentials of nine candidate intermediate polars in order to confirm whether or not they are magnetic cataclysmic variables. Methods. Frequency analysis of RXTE and XMM data was used to search for temporal variations which could be associated with the spin period of the magnetic white dwarf. X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target. Results. The hard X-ray light curve of V2069 Cyg shows a pulse period of 743.2 s, and its spectrum is fit by an absorbed bremsstrahlung model with an iron line, confirming this to be a genuine intermediate polar. The hard X-ray light curve of the previously confirmed intermediate polar IGR J00234+6141 is shown to be consistent with the previous low energy X-ray detection of a 563.5 s pulse period. The likely polar IGR J14536-5522 shows no coherent modulation at the previously identified period of 3.1 hr, but does exhibit a clear signal at periods likely to be harmonically related to it. Whilst our RXTE observations of RX J0153.3+7447, Swift J061223.0+701243.9, V436 Car and DD Cir are largely too faint to give any definitive results, the observation of IGR J16167-4957 and V2487 Oph show some characteristics of intermediate polars and these objects remain good candidates. Conclusions. We confirmed one new hard X-ray selected intermediate polar from our sample, V2069 Cyg."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary method used to determine the credentials of intermediate polar candidates in the study, and what type of data analysis was carried out to confirm their magnetic cataclysmic variable status?\n\nA) Spectral analysis of optical data was used to search for temporal variations, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target.\n\nB) Frequency analysis of XMM data was used to search for temporal variations, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target.\n\nC) Time-domain analysis of optical data was used to search for temporal variations, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target.\n\nD) Frequency analysis of RXTE data was used to search for temporal variations, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target.\n\nCorrect Answer: B) Frequency analysis of XMM data was used to search for temporal variations, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target.\n\nExplanation: The study used frequency analysis of XMM data to search for temporal variations that could be associated with the spin period of the magnetic white dwarf, and X-ray spectral analysis was carried out to characterise the emission and absorption properties of each target. This is evident from the methods section of the documentation, which states \"Frequency analysis of RXTE and XMM data was used to search for temporal variations...\"."}}