{"0": {"documentation": {"title": "Strong geometry dependence of the Casimir force between interpenetrated\n  rectangular gratings", "source": "Mingkang Wang, L. Tang, C. Y. Ng, Riccardo Messina, Brahim Guizal, J.\n  A. Crosse, Mauro Antezza, C. T. Chan, and H. B. Chan", "docs_id": "2009.02187", "section": ["quant-ph", "cond-mat.mes-hall", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strong geometry dependence of the Casimir force between interpenetrated\n  rectangular gratings. Quantum fluctuations give rise to Casimir forces between two parallel conducting plates, the magnitude of which increases monotonically as the separation decreases. By introducing nanoscale gratings to the surfaces, recent advances have opened opportunities for controlling the Casimir force in complex geometries. Here, we measure the Casimir force between two rectangular gratings in regimes not accessible before. Using an on-chip detection platform, we achieve accurate alignment between the two gratings so that they interpenetrate as the separation is reduced. Just before interpenetration occurs, the measured Casimir force is found to have a geometry dependence that is much stronger than previous experiments, with deviations from the proximity force approximation reaching a factor of ~500. After the gratings interpenetrate each other, the Casimir force becomes non-zero and independent of displacement. This work shows that the presence of gratings can strongly modify the Casimir force to control the interaction between nanomechanical components."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of introducing nanoscale gratings to the surfaces of two parallel conducting plates on the Casimir force between them?\n\nA) The Casimir force becomes zero and independent of displacement.\nB) The Casimir force becomes non-zero and independent of displacement, but with a geometry dependence that is weaker than previous experiments.\nC) The Casimir force becomes non-zero and independent of displacement, with a geometry dependence that is much stronger than previous experiments, and deviations from the proximity force approximation reaching a factor of ~500.\nD) The Casimir force becomes zero and dependent on displacement, with a geometry dependence that is weaker than previous experiments.\n\nCorrect Answer: C) The Casimir force becomes non-zero and independent of displacement, with a geometry dependence that is much stronger than previous experiments, and deviations from the proximity force approximation reaching a factor of ~500.\n\nExplanation: The correct answer is based on the text, which states that \"After the gratings interpenetrate each other, the Casimir force becomes non-zero and independent of displacement.\" This indicates that the presence of gratings leads to a non-zero and displacement-independent Casimir force. Additionally, the text mentions that the geometry dependence of the Casimir force is much stronger than previous experiments, with deviations from the proximity force approximation reaching a factor of ~500. This supports the correct answer."}, "1": {"documentation": {"title": "Self-learning projective quantum Monte Carlo simulations guided by\n  restricted Boltzmann machines", "source": "S. Pilati, E. M. Inack, P. Pieri", "docs_id": "1907.00907", "section": ["physics.comp-ph", "cond-mat.other", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Self-learning projective quantum Monte Carlo simulations guided by\n  restricted Boltzmann machines. The projective quantum Monte Carlo (PQMC) algorithms are among the most powerful computational techniques to simulate the ground state properties of quantum many-body systems. However, they are efficient only if a sufficiently accurate trial wave function is used to guide the simulation. In the standard approach, this guiding wave function is obtained in a separate simulation that performs a variational minimization. Here we show how to perform PQMC simulations guided by an adaptive wave function based on a restricted Boltzmann machine. This adaptive wave function is optimized along the PQMC simulation via unsupervised machine learning, avoiding the need of a separate variational optimization. As a byproduct, this technique provides an accurate ansatz for the ground state wave function, which is obtained by minimizing the Kullback-Leibler divergence with respect to the PQMC samples, rather than by minimizing the energy expectation value as in standard variational optimizations. The high accuracy of this self-learning PQMC technique is demonstrated for a paradigmatic sign-problem-free model, namely, the ferromagnetic quantum Ising chain, showing very precise agreement with the predictions of the Jordan-Wigner theory and of loop quantum Monte Carlo simulations performed in the low-temperature limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of using a self-learning projective quantum Monte Carlo (PQMC) simulation guided by an adaptive wave function based on a restricted Boltzmann machine, as described in the Arxiv documentation?\n\nA) It eliminates the need for a separate variational optimization, allowing for faster simulation times.\nB) It provides an accurate ansatz for the ground state wave function, which is obtained by minimizing the Kullback-Leibler divergence with respect to the PQMC samples.\nC) It enables the simulation of quantum many-body systems with arbitrary interactions, without the need for a specific trial wave function.\nD) It allows for the simulation of quantum systems in the high-temperature limit, where the standard variational approach is less effective.\n\nCorrect Answer: B) It provides an accurate ansatz for the ground state wave function, which is obtained by minimizing the Kullback-Leibler divergence with respect to the PQMC samples.\n\nExplanation: The correct answer is B) because the self-learning PQMC technique is described as providing an accurate ansatz for the ground state wave function, which is obtained by minimizing the Kullback-Leibler divergence with respect to the PQMC samples. This is a key advantage of the technique, as it allows for the accurate simulation of quantum many-body systems without the need for a separate variational optimization. The other options are incorrect because they do not accurately summarize the primary advantage of the self-learning PQMC technique. Option A is incorrect because while the technique may eliminate the need for a separate variational optimization, this is not its primary advantage. Option C is incorrect because the technique is not designed to simulate quantum systems with arbitrary interactions. Option D is incorrect because the technique is not specifically designed for the high-temperature limit."}, "2": {"documentation": {"title": "The Essential Role of String-Derived Symmetries in Ensuring\n  Proton-Stability and Light Neutrino Masses", "source": "Jogesh C. Pati (University of Maryland)", "docs_id": "hep-ph/9607446", "section": ["hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Essential Role of String-Derived Symmetries in Ensuring\n  Proton-Stability and Light Neutrino Masses. The paper addresses the problem of suppressing naturally the unsafe d=4 as well as the color-triplet mediated and/or gravity-linked d=5 proton-decay operators, which generically arise in SUSY-unification. It also attempts to give light masses to the neutrinos, of the type suggested by current experiments. It is noted that neither the symmetries in $SO(10)$, nor those in $E_6$, suffice for the purpose -- especially in the matter of suppressing naturally the d=5 proton-decay operators. By contrast, it is shown that a certain {\\it string-derived symmetry}, which cannot arise within conventional grand unification, but which does arise within a class of three-generation string-solutions, suffices, in conjuction with $B-L$, to safeguard proton-stability from all potential dangers, including those which may arise through higher dimensional operators and the color-triplets in the infinite tower of states. At the same time, the symmetry in question permits neutrinos to acquire appropriate masses. This shows that {\\it string theory plays an essential role in ensuring natural consistency of SUSY-unification with two low-energy observations -- proton-stability and light masses for the neutrinos}. The correlation between the masses of the extra $Z'$-boson (or bosons), which arise in these models, and proton-decay rate is noted."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of string-derived symmetries and SUSY-unification, what is the primary role of the symmetry in question, which arises within a class of three-generation string-solutions, in ensuring proton-stability and light neutrino masses?\n\nA) To suppress the d=4 proton-decay operators and provide a mechanism for neutrino mass generation through the Higgs mechanism.\nB) To provide a framework for the unification of fundamental forces, including gravity, and to generate the masses of the extra Z'-bosons.\nC) To safeguard proton-stability from all potential dangers, including those which may arise through higher dimensional operators and the color-triplets in the infinite tower of states, and to permit neutrinos to acquire appropriate masses.\nD) To introduce a new gauge boson, the Z'-boson, which is correlated with the proton-decay rate, and to provide a mechanism for the generation of neutrino masses through the seesaw mechanism.\n\n**Correct Answer:** C) To safeguard proton-stability from all potential dangers, including those which may arise through higher dimensional operators and the color-triplets in the infinite tower of states, and to permit neutrinos to acquire appropriate masses.\n\n**Explanation:** The correct answer is C) because the paper states that the symmetry in question \"suffices, in conjunction with B-L, to safeguard proton-stability from all potential dangers, including those which may arise through higher dimensional operators and the color-triplets in the infinite tower of states.\" Additionally, the paper notes that this symmetry \"permits neutrinos to acquire appropriate masses.\" The other options are incorrect because they either focus on the suppression of d=4 proton-decay operators (A), the unification of fundamental forces (B), or the introduction of a new gauge boson (D), which are not the primary roles of the symmetry in question."}, "3": {"documentation": {"title": "Electronic structures and optical properties of realistic transition\n  metal dichalcogenide heterostructures from first principles", "source": "Hannu-Pekka Komsa and Arkady V. Krasheninnikov", "docs_id": "1308.5061", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electronic structures and optical properties of realistic transition\n  metal dichalcogenide heterostructures from first principles. We calculate from first principles the electronic structure and optical properties of a number of transition metal dichalcogenide (TMD) bilayer heterostructures consisting of MoS2 layers sandwiched with WS2, MoSe2, MoTe2, BN, or graphene sheets. Contrary to previous works, the systems are constructed in such a way that the unstrained lattice constants of the constituent incommensurate monolayers are retained. We find strong interaction between the \\Gamma-point states in all TMD/TMD heterostructures, which can lead to an indirect gap. On the other hand, states near the K-point remain as in the monolayers. When TMDs are paired with BN or graphene layers, the interaction around \\Gamma-point is negligible, and the electronic structure resembles that of two independent monolayers. Calculations of optical properties of the MoS2/WS2 system show that even when the valence and conduction band edges are located in different layers, the mixing of optical transitions is minimal, and the optical characteristics of the monolayers are largely retained in these heterostructures. The intensity of interlayer transitions is found to be negligibly small, a discouraging result for engineering the optical gap of TMDs by heterostructuring."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary consequence of the strong interaction between the \u0393-point states in transition metal dichalcogenide (TMD) bilayer heterostructures, as calculated in the study?\n\nA) The indirect gap is eliminated, and the optical gap is preserved.\nB) The interaction around the \u0393-point is negligible, and the electronic structure resembles that of two independent monolayers.\nC) The mixing of optical transitions is significant, and the optical characteristics of the monolayers are largely retained.\nD) The intensity of interlayer transitions is found to be negligible, and the optical gap is engineered by heterostructuring.\n\nCorrect Answer: B) The interaction around the \u0393-point is negligible, and the electronic structure resembles that of two independent monolayers.\n\nExplanation: The study finds that the strong interaction between the \u0393-point states in all TMD/TMD heterostructures can lead to an indirect gap. However, when TMDs are paired with BN or graphene layers, the interaction around the \u0393-point is negligible, and the electronic structure resembles that of two independent monolayers. This suggests that the interaction around the \u0393-point is not significant, and the electronic structure is largely preserved."}, "4": {"documentation": {"title": "Online Adjoint Methods for Optimization of PDEs", "source": "Justin Sirignano and Konstantinos Spiliopoulos", "docs_id": "2101.09621", "section": ["math.OC", "math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online Adjoint Methods for Optimization of PDEs. We present and mathematically analyze an online adjoint algorithm for the optimization of partial differential equations (PDEs). Traditional adjoint algorithms would typically solve a new adjoint PDE at each optimization iteration, which can be computationally costly. In contrast, an online adjoint algorithm updates the design variables in continuous-time and thus constantly makes progress towards minimizing the objective function. The online adjoint algorithm we consider is similar in spirit to the pseudo-time-stepping, one-shot method which has been previously proposed. Motivated by the application of such methods to engineering problems, we mathematically study the convergence of the online adjoint algorithm. The online adjoint algorithm relies upon a time-relaxed adjoint PDE which provides an estimate of the direction of steepest descent. The algorithm updates this estimate continuously in time, and it asymptotically converges to the exact direction of steepest descent as $t \\rightarrow \\infty$. We rigorously prove that the online adjoint algorithm converges to a critical point of the objective function for optimizing the PDE. Under appropriate technical conditions, we also prove a convergence rate for the algorithm. A crucial step in the convergence proof is a multi-scale analysis of the coupled system for the forward PDE, adjoint PDE, and the gradient descent ODE for the design variables."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider an online adjoint algorithm for the optimization of a partial differential equation (PDE) of the form:\n\n\u2202u/\u2202t + \u2207\u22c5(Au) = f(t,x)\n\nwhere u = u(t,x) is the state variable, A is a matrix-valued function, and f is a source term. The online adjoint algorithm updates the design variables in continuous-time and relies on a time-relaxed adjoint PDE to provide an estimate of the direction of steepest descent.\n\nUnder the assumption that the matrix A is Lipschitz continuous with respect to the design variables, and that the source term f is smooth and bounded, prove that the online adjoint algorithm converges to a critical point of the objective function for optimizing the PDE.\n\nA) The online adjoint algorithm converges to a critical point of the objective function if and only if the matrix A is invertible.\n\nB) The online adjoint algorithm converges to a critical point of the objective function if and only if the source term f is identically zero.\n\nC) The online adjoint algorithm converges to a critical point of the objective function if and only if the time-relaxed adjoint PDE is well-posed.\n\nD) The online adjoint algorithm converges to a critical point of the objective function if and only if the Lipschitz continuity of the matrix A with respect to the design variables is sufficient for convergence.\n\nCorrect Answer: D) The online adjoint algorithm converges to a critical point of the objective function if and only if the Lipschitz continuity of the matrix A with respect to the design variables is sufficient for convergence.\n\nExplanation: The correct answer is D) because the Lipschitz continuity of the matrix A with respect to the design variables is a sufficient condition for the convergence of the online adjoint algorithm. This is because the Lipschitz continuity ensures that the adjoint PDE is well-posed, and the algorithm can update the design variables continuously in time to minimize the objective function. The other options are incorrect because they do not provide sufficient conditions for convergence. Option A is incorrect because the invertibility of the matrix A is not a necessary condition for convergence. Option B is incorrect because the source term f being identically zero is not a necessary condition for convergence. Option C is incorrect because the well-posedness of the time-relaxed adjoint PDE is not a sufficient condition for convergence."}, "5": {"documentation": {"title": "Upconverting nanodots of nayf4yb3er3 synthesis characterization and uv\n  visible luminescence study through ti sapphire 140 femtosecond laser pulses", "source": "Monami Das Modak, Ganesh Damarla, K Santhosh Kumar, Somedutta Maity,\n  Anil K Chaudhury and Pradip Paik", "docs_id": "2008.06783", "section": ["physics.app-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Upconverting nanodots of nayf4yb3er3 synthesis characterization and uv\n  visible luminescence study through ti sapphire 140 femtosecond laser pulses. In this work, dot-sized upconversion nanocrystals (UCN-dots) with diameter c.a. 3.4-0.15 nm have been synthesized. These UCN-dots exhibit visible emission (at 497, 527 and 545 nm) under the excitation with 980 nm CW-NIR laser. Further, these UCN-dots exhibit high energy upconversion emission (UV region, 206 to 231 nm) with Ti-Sapphire Femtosecond laser of 140-femtoseconds duration at 80 MHz repetition rate at different excitation, which has never been reported. This is interesting to report that the generation of high energy UV-Vis emission and their shifting from 206 to 231 nm for the UCN-dots by tuning the excitation wavelength ranging from 950 nm to 980 nm irradiated from Ti: sapphire Femtosecond laser observed. We have demonstrated the generation of high energy upconversions with change in energy band gaps as well as number of absorbed photons per photon emitted under the Femtosecond-laser excitation power. Additionally, we report the photo luminescence of UCN-dots in visible range with 450 nm excitation wavelength exhibiting blue and red emission (visible to visible). The generation of high energy up-conversion in UV-Vis region could be useful for designing optoelectronic and biomedical devices for therapeutic application."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism behind the high-energy upconversion emission observed in the UCN-dots under Ti-Sapphire Femtosecond laser excitation, and how does the excitation wavelength influence this phenomenon?\n\nA) The high-energy upconversion emission is primarily due to the thermalization of excited electrons, which is enhanced by the Femtosecond laser pulse duration. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands.\n\nB) The high-energy upconversion emission is primarily due to the non-radiative recombination of excited electrons, which is influenced by the number of absorbed photons per photon emitted. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands.\n\nC) The high-energy upconversion emission is primarily due to the quantum confinement effect, which is enhanced by the small size of the UCN-dots. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands.\n\nD) The high-energy upconversion emission is primarily due to the photoinduced electron-hole pairs, which are generated by the Femtosecond laser pulse. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands.\n\nCorrect Answer: D) The high-energy upconversion emission is primarily due to the photoinduced electron-hole pairs, which are generated by the Femtosecond laser pulse. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands.\n\nExplanation: The correct answer is D) because the documentation states that the high-energy upconversion emission is observed under Ti-Sapphire Femtosecond laser excitation, which generates photoinduced electron-hole pairs. The excitation wavelength influences this process by altering the energy gap between the conduction and valence bands, as evidenced by the observation of shifting emission peaks from 206 to 231 nm as the excitation wavelength is varied from 950 nm to 980 nm."}, "6": {"documentation": {"title": "Joint Neural Network Equalizer and Decoder", "source": "Weihong Xu (1 and 2 and 3), Zhiwei Zhong (1 and 2 and 3), Yair Be'ery\n  (4), Xiaohu You (1 and 2 and 3), Chuan Zhang (1 and 2 and 3) ((1) Lab of\n  Efficient Architectures for Digital-communication and Signal-processing\n  (LEADS), (2) National Mobile Communications Research Laboratory, (3) Quantum\n  Information Center, Southeast University, China, (4) School of Electrical\n  Engineering, Tel-Aviv University, Israel)", "docs_id": "1807.02040", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Joint Neural Network Equalizer and Decoder. Recently, deep learning methods have shown significant improvements in communication systems. In this paper, we study the equalization problem over the nonlinear channel using neural networks. The joint equalizer and decoder based on neural networks are proposed to realize blind equalization and decoding process without the knowledge of channel state information (CSI). Different from previous methods, we use two neural networks instead of one. First, convolutional neural network (CNN) is used to adaptively recover the transmitted signal from channel impairment and nonlinear distortions. Then the deep neural network decoder (NND) decodes the detected signal from CNN equalizer. Under various channel conditions, the experiment results demonstrate that the proposed CNN equalizer achieves better performance than other solutions based on machine learning methods. The proposed model reduces about $2/3$ of the parameters compared to state-of-the-art counterparts. Besides, our model can be easily applied to long sequence with $\\mathcal{O}(n)$ complexity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Joint Neural Network Equalizer and Decoder\", what is the primary advantage of using a convolutional neural network (CNN) as the equalizer, compared to other machine learning methods?\n\n**A)** The CNN equalizer achieves better performance in noisy channels.\n**B)** The CNN equalizer reduces the number of parameters required for equalization by approximately 2/3 compared to state-of-the-art counterparts.\n**C)** The CNN equalizer can be easily parallelized for faster computation.\n**D)** The CNN equalizer is more robust to nonlinear channel distortions.\n\n**Correct Answer:** B) The CNN equalizer reduces the number of parameters required for equalization by approximately 2/3 compared to state-of-the-art counterparts.\n\n**Explanation:** The question requires the test-taker to understand the key contribution of the paper, which is the use of a CNN equalizer that reduces the number of parameters required for equalization. The correct answer is supported by the text, which states that the proposed model reduces about 2/3 of the parameters compared to state-of-the-art counterparts. The other options are incorrect because they do not accurately reflect the primary advantage of the CNN equalizer."}, "7": {"documentation": {"title": "Recent results in nuclear astrophysics", "source": "Alain Coc, Fairouz Hammache and Juergen Kiener", "docs_id": "1605.07810", "section": ["nucl-ex", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recent results in nuclear astrophysics. In this review, we emphasize the interplay between astrophysical observations, modeling, and nuclear physics laboratory experiments. Several important nuclear cross sections for astrophysics have long been identified e.g. 12C(alpha,gamma)16O for stellar evolution, or 13C(alpha,n)16O and 22Ne(alpha,n)25Mg as neutron sources for the s-process. More recently, observations of lithium abundances in the oldest stars, or of nuclear gamma-ray lines from space, have required new laboratory experiments. New evaluation of thermonuclear reaction rates now includes the associated rate uncertainties that are used in astrophysical models to i) estimate final uncertainties on nucleosynthesis yields and ii) identify those reactions that require further experimental investigation. Sometimes direct cross section measurements are possible, but more generally the use of indirect methods is compulsory in view of the very low cross sections. Non-thermal processes are often overlooked but are also important for nuclear astrophysics, e.g. in gamma-ray emission from solar flares or in the interaction of cosmic rays with matter, and also motivate laboratory experiments. Finally, we show that beyond the historical motivations of nuclear astrophysics, understanding i) the energy sources that drive stellar evolution and ii) the origin of the elements can also be used to give new insights into physics beyond the standard model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation for conducting new laboratory experiments in nuclear astrophysics, and how do these experiments contribute to our understanding of stellar evolution and the origin of elements?\n\nA) To directly measure the cross sections of thermonuclear reactions, which are essential for estimating final uncertainties on nucleosynthesis yields.\nB) To investigate the role of non-thermal processes in gamma-ray emission from solar flares and the interaction of cosmic rays with matter.\nC) To evaluate the associated rate uncertainties that are used in astrophysical models to estimate final uncertainties on nucleosynthesis yields and identify reactions that require further experimental investigation.\nD) To understand the energy sources that drive stellar evolution and the origin of the elements, and to provide new insights into physics beyond the standard model.\n\nCorrect Answer: C) To evaluate the associated rate uncertainties that are used in astrophysical models to estimate final uncertainties on nucleosynthesis yields and identify reactions that require further experimental investigation.\n\nExplanation: The correct answer is C) because the review emphasizes the interplay between astrophysical observations, modeling, and nuclear physics laboratory experiments. The new evaluation of thermonuclear reaction rates now includes the associated rate uncertainties that are used in astrophysical models to estimate final uncertainties on nucleosynthesis yields and identify reactions that require further experimental investigation. This is the primary motivation for conducting new laboratory experiments in nuclear astrophysics."}, "8": {"documentation": {"title": "A realistic assessment of the CTA sensitivity to dark matter\n  annihilation", "source": "Hamish Silverwood, Christoph Weniger, Pat Scott and Gianfranco Bertone", "docs_id": "1408.4131", "section": ["astro-ph.HE", "astro-ph.CO", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A realistic assessment of the CTA sensitivity to dark matter\n  annihilation. We estimate the sensitivity of the upcoming CTA gamma-ray telescope to DM annihilation at the Galactic centre, improving on previous analyses in a number of significant ways. First, we perform a detailed analyses of all backgrounds, including diffuse astrophysical emission for the first time in a study of this type. Second, we present a statistical framework for including systematic errors and estimate the consequent degradation in sensitivity. These errors may come from e.g. event reconstruction, Monte Carlo determination of the effective area or uncertainty in atmospheric conditions. Third, we show that performing the analysis on a set of suitably optimised regions of interest makes it possible to partially compensate for the degradation in sensitivity caused by systematics and diffuse emission. To probe dark matter with the canonical thermal annihilation cross-section, CTA systematics like non-uniform variations in acceptance over a single field of view must be kept below the 0.3% level, unless the dark matter density rises more steeply in the centre of the Galaxy than predicted by a typical Navarro-Frenk-White or Einasto profile. For a contracted $r^{-1.3}$ profile, and systematics at the 1% level, CTA can probe annihilation to $b\\bar{b}$ at the canonical thermal level for dark matter masses between 100 GeV and 10 TeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the estimated range of dark matter masses that the CTA gamma-ray telescope can probe for annihilation to the $b\\bar{b}$ channel, assuming a contracted $r^{-1.3}$ profile and systematics at the 1% level?\n\nA) 10 GeV to 100 GeV\nB) 100 GeV to 10 TeV\nC) 1 TeV to 100 TeV\nD) 100 GeV to 1000 GeV\n\nCorrect Answer: B) 100 GeV to 10 TeV\n\nExplanation: According to the documentation, for a contracted $r^{-1.3}$ profile and systematics at the 1% level, CTA can probe annihilation to $b\\bar{b}$ at the canonical thermal level for dark matter masses between 100 GeV and 10 TeV."}, "9": {"documentation": {"title": "Deep differentiable forest with sparse attention for the tabular data", "source": "Yingshi Chen", "docs_id": "2003.00223", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep differentiable forest with sparse attention for the tabular data. We present a general architecture of deep differentiable forest and its sparse attention mechanism. The differentiable forest has the advantages of both trees and neural networks. Its structure is a simple binary tree, easy to use and understand. It has full differentiability and all variables are learnable parameters. We would train it by the gradient-based optimization method, which shows great power in the training of deep CNN. We find and analyze the attention mechanism in the differentiable forest. That is, each decision depends on only a few important features, and others are irrelevant. The attention is always sparse. Based on this observation, we improve its sparsity by data-aware initialization. We use the attribute importance to initialize the attention weight. Then the learned weight is much sparse than that from random initialization. Our experiment on some large tabular dataset shows differentiable forest has higher accuracy than GBDT, which is the state of art algorithm for tabular datasets. The source codes are available at https://github.com/closest-git/QuantumForest"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the differentiable forest architecture over traditional decision trees, and how does it leverage the strengths of both trees and neural networks?\n\n**A)** Differentiable forest has a more complex structure than traditional trees, allowing for more accurate predictions.\n**B)** Differentiable forest has full differentiability, enabling the training of the model using gradient-based optimization methods, which is a key advantage over traditional trees.\n**C)** Differentiable forest is more interpretable than neural networks, making it easier to understand and explain the model's decisions.\n**D)** Differentiable forest is more computationally efficient than neural networks, requiring less training data and computational resources.\n\n**Correct Answer:** B) Differentiable forest has full differentiability, enabling the training of the model using gradient-based optimization methods, which is a key advantage over traditional trees.\n\n**Explanation:** The documentation states that the differentiable forest architecture has \"full differentiability\" and can be trained using gradient-based optimization methods, which is a key advantage over traditional trees. This allows for more accurate and efficient training of the model. The other options are incorrect because they do not accurately reflect the advantages of the differentiable forest architecture."}, "10": {"documentation": {"title": "An iterative method for classification of binary data", "source": "Denali Molitor and Deanna Needell", "docs_id": "1809.03041", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An iterative method for classification of binary data. In today's data driven world, storing, processing, and gleaning insights from large-scale data are major challenges. Data compression is often required in order to store large amounts of high-dimensional data, and thus, efficient inference methods for analyzing compressed data are necessary. Building on a recently designed simple framework for classification using binary data, we demonstrate that one can improve classification accuracy of this approach through iterative applications whose output serves as input to the next application. As a side consequence, we show that the original framework can be used as a data preprocessing step to improve the performance of other methods, such as support vector machines. For several simple settings, we showcase the ability to obtain theoretical guarantees for the accuracy of the iterative classification method. The simplicity of the underlying classification framework makes it amenable to theoretical analysis and studying this approach will hopefully serve as a step toward developing theory for more sophisticated deep learning technologies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of an iterative method for classification of binary data, and how does it relate to the challenges of storing, processing, and gaining insights from large-scale data?\n\n**A)** The method is designed to improve the accuracy of classification for high-dimensional data, but it does not address the challenge of data compression.\n**B)** The method is intended to reduce the complexity of classification models, but it does not provide a solution for storing and processing large-scale data.\n**C)** The method is developed to address the need for efficient inference methods for analyzing compressed data, which is a major challenge in the data-driven world.\n**D)** The method is designed to improve the performance of support vector machines, but it does not provide a solution for data compression.\n\n**Correct Answer:** C) The method is developed to address the need for efficient inference methods for analyzing compressed data, which is a major challenge in the data-driven world.\n\n**Explanation:** The question requires the test-taker to understand the context and motivation behind the development of the iterative method for classification of binary data. The correct answer, C, highlights the primary motivation as addressing the need for efficient inference methods for analyzing compressed data, which is a major challenge in the data-driven world. The other options are incorrect because they either misrepresent the motivation (A and B) or focus on a secondary aspect (D)."}, "11": {"documentation": {"title": "Neutrino Quasielastic Scattering on Nuclear Targets: Parametrizing\n  Transverse Enhancement (Meson Exchange Currents)", "source": "A. Bodek, H. S. Budd and E. Christy", "docs_id": "1106.0340", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutrino Quasielastic Scattering on Nuclear Targets: Parametrizing\n  Transverse Enhancement (Meson Exchange Currents). We present a parametrization of the observed enhancement in the transverse electron quasielastic (QE) response function for nucleons bound in carbon as a function of the square of the four momentum transfer ($Q^2$) in terms of a correction to the magnetic form factors of bound nucleons. The parametrization should also be applicable to the transverse cross section in neutrino scattering. If the transverse enhancement originates from meson exchange currents (MEC), then it is theoretically expected that any enhancement in the longitudinal or axial contributions is small. We present the predictions of the \"Transverse Enhancement\" model (which is based on electron scattering data only) for the $\\nu_\\mu, \\bar{\\nu}_\\mu$ differential and total QE cross sections for nucleons bound in carbon. The $Q^2$ dependence of the transverse enhancement is observed to resolve much of the long standing discrepancy in the QE total cross sections and differential distributions between low energy and high energy neutrino experiments on nuclear targets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism expected to explain the observed transverse enhancement in the electron quasielastic response function for nucleons bound in carbon, and how does it relate to the theoretical expectations for neutrino scattering?\n\nA) Meson exchange currents (MEC) are expected to explain the transverse enhancement, and any enhancement in the longitudinal or axial contributions is expected to be small.\n\nB) The transverse enhancement is expected to arise from the difference in magnetic form factors between bound nucleons and free nucleons.\n\nC) The transverse enhancement is expected to be a result of the difference in the QE response function between low energy and high energy neutrino experiments on nuclear targets.\n\nD) The transverse enhancement is expected to be a result of the difference in the QE response function between electron scattering data and neutrino scattering data.\n\nCorrect Answer: A) Meson exchange currents (MEC) are expected to explain the transverse enhancement, and any enhancement in the longitudinal or axial contributions is expected to be small.\n\nExplanation: The correct answer is A) Meson exchange currents (MEC) are expected to explain the transverse enhancement, and any enhancement in the longitudinal or axial contributions is expected to be small. This is because the documentation states that if the transverse enhancement originates from meson exchange currents (MEC), then it is theoretically expected that any enhancement in the longitudinal or axial contributions is small. This requires the test-taker to understand the relationship between the transverse enhancement and the theoretical expectations for neutrino scattering."}, "12": {"documentation": {"title": "Denise: Deep Robust Principal Component Analysis for Positive\n  Semidefinite Matrices", "source": "Calypso Herrera, Florian Krach, Anastasis Kratsios, Pierre Ruyssen,\n  Josef Teichmann", "docs_id": "2004.13612", "section": ["stat.ML", "cs.LG", "math.OC", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Denise: Deep Robust Principal Component Analysis for Positive\n  Semidefinite Matrices. The robust PCA of covariance matrices plays an essential role when isolating key explanatory features. The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix. Since these algorithms are computationally expensive, it is preferable to learn and store a function that instantaneously performs this decomposition when evaluated. Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally of symmetric positive semidefinite matrices, which learns precisely such a function. Theoretical guarantees for Denise are provided. These include a novel universal approximation theorem adapted to our geometric deep learning problem, convergence to an optimal solution of the learning problem and convergence of the training scheme. Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately 2000x faster than the state-of-the-art, PCP, and 200x faster than the current speed optimized method, fast PCP."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of Denise, a deep learning-based algorithm for robust Principal Component Analysis (PCA) of covariance matrices?\n\nA) To improve the interpretability of PCA results\nB) To reduce the computational cost of PCA\nC) To develop a method that can be applied to any symmetric positive semidefinite matrix, without requiring re-running algorithms for every new matrix\nD) To enhance the stability of PCA results in the presence of noise\n\n**Correct Answer:** C) To develop a method that can be applied to any symmetric positive semidefinite matrix, without requiring re-running algorithms for every new matrix\n\n**Explanation:** The question requires the test-taker to understand the main motivation behind the development of Denise. The correct answer, C, highlights the key advantage of Denise, which is its ability to learn a function that can perform robust PCA of covariance matrices (or symmetric positive semidefinite matrices) without requiring re-running algorithms for every new matrix. This is a critical aspect of the algorithm, as it allows for efficient and scalable computation.\n\nThe incorrect options, A, B, and D, are plausible but not the primary motivation behind Denise. Option A is related to the interpretability of PCA results, but it is not the main driving force behind Denise. Option B is incorrect because while Denise is faster than some existing methods, its primary motivation is not to reduce computational cost. Option D is also incorrect because stability is not explicitly mentioned as a motivation for developing Denise."}, "13": {"documentation": {"title": "Location-Relative Attention Mechanisms For Robust Long-Form Speech\n  Synthesis", "source": "Eric Battenberg, RJ Skerry-Ryan, Soroosh Mariooryad, Daisy Stanton,\n  David Kao, Matt Shannon, Tom Bagby", "docs_id": "1910.10288", "section": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Location-Relative Attention Mechanisms For Robust Long-Form Speech\n  Synthesis. Despite the ability to produce human-level speech for in-domain text, attention-based end-to-end text-to-speech (TTS) systems suffer from text alignment failures that increase in frequency for out-of-domain text. We show that these failures can be addressed using simple location-relative attention mechanisms that do away with content-based query/key comparisons. We compare two families of attention mechanisms: location-relative GMM-based mechanisms and additive energy-based mechanisms. We suggest simple modifications to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dynamic Convolution Attention (DCA). We compare the various mechanisms in terms of alignment speed and consistency during training, naturalness, and ability to generalize to long utterances, and conclude that GMM attention and DCA can generalize to very long utterances, while preserving naturalness for shorter, in-domain utterances."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of attention-based end-to-end text-to-speech (TTS) systems, and how do location-relative attention mechanisms address this limitation?\n\n**A)** Attention-based TTS systems suffer from content-based query/key comparisons, which lead to text alignment failures. Location-relative attention mechanisms address this limitation by using location-based queries and keys.\n\n**B)** Attention-based TTS systems are unable to generalize to out-of-domain text due to their reliance on content-based query/key comparisons. Location-relative attention mechanisms address this limitation by using location-based queries and keys.\n\n**C)** Attention-based TTS systems are limited by their inability to produce natural-sounding speech for long utterances. Location-relative attention mechanisms address this limitation by using location-based queries and keys to improve alignment speed and consistency.\n\n**D)** Attention-based TTS systems are unable to produce high-quality speech due to their reliance on complex mathematical models. Location-relative attention mechanisms address this limitation by using simple modifications to GMM-based attention and introducing a new location-relative attention mechanism, Dynamic Convolution Attention (DCA).\n\n**Correct Answer:** D) Attention-based TTS systems are unable to produce high-quality speech due to their reliance on complex mathematical models. Location-relative attention mechanisms address this limitation by using simple modifications to GMM-based attention and introducing a new location-relative attention mechanism, Dynamic Convolution Attention (DCA).\n\n**Explanation:** The correct answer is D) because the documentation states that attention-based TTS systems suffer from text alignment failures that increase in frequency for out-of-domain text, and that location-relative attention mechanisms address this limitation by using simple modifications to GMM-based attention and introducing a new location-relative attention mechanism, Dynamic Convolution Attention (DCA). The other options are incorrect because they do not accurately summarize the primary limitation of attention-based TTS systems and how location-relative attention mechanisms address this limitation."}, "14": {"documentation": {"title": "Multi-objective Optimal Control of Dynamic Integrated Model of Climate\n  and Economy: Evolution in Action", "source": "Mostapha Kalami Heris and Shahryar Rahnamayan", "docs_id": "2007.00449", "section": ["econ.GN", "cs.NE", "cs.SY", "eess.SY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-objective Optimal Control of Dynamic Integrated Model of Climate\n  and Economy: Evolution in Action. One of the widely used models for studying economics of climate change is the Dynamic Integrated model of Climate and Economy (DICE), which has been developed by Professor William Nordhaus, one of the laureates of the 2018 Nobel Memorial Prize in Economic Sciences. Originally a single-objective optimal control problem has been defined on DICE dynamics, which is aimed to maximize the social welfare. In this paper, a bi-objective optimal control problem defined on DICE model, objectives of which are maximizing social welfare and minimizing the temperature deviation of atmosphere. This multi-objective optimal control problem solved using Non-Dominated Sorting Genetic Algorithm II (NSGA-II) also it is compared to previous works on single-objective version of the problem. The resulting Pareto front rediscovers the previous results and generalizes to a wide range of non-dominant solutions to minimize the global temperature deviation while optimizing the economic welfare. The previously used single-objective approach is unable to create such a variety of possibilities, hence, its offered solution is limited in vision and reachable performance. Beside this, resulting Pareto-optimal set reveals the fact that temperature deviation cannot go below a certain lower limit, unless we have significant technology advancement or positive change in global conditions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Dynamic Integrated Model of Climate and Economy (DICE), what is the primary limitation of the single-objective optimal control approach, as opposed to the multi-objective approach presented in the paper?\n\n**A)** The single-objective approach is unable to capture the complexity of the climate system, while the multi-objective approach is limited by its reliance on computational power.\n\n**B)** The single-objective approach is unable to balance the trade-off between economic welfare and temperature deviation, while the multi-objective approach is limited by its inability to account for external factors.\n\n**C)** The single-objective approach is unable to generate a wide range of Pareto-optimal solutions, while the multi-objective approach is limited by its reliance on a specific optimization algorithm.\n\n**D)** The single-objective approach is unable to consider the long-term implications of climate change, while the multi-objective approach is limited by its focus on short-term economic gains.\n\n**Correct Answer:** C) The single-objective approach is unable to generate a wide range of Pareto-optimal solutions, while the multi-objective approach is limited by its reliance on a specific optimization algorithm.\n\n**Explanation:** The correct answer is C) because the paper states that the single-objective approach is unable to create a variety of possibilities, hence its offered solution is limited in vision and reachable performance. This is in contrast to the multi-objective approach, which is able to generate a wide range of Pareto-optimal solutions. The other options are incorrect because they do not accurately reflect the limitations of the single-objective approach as described in the paper."}, "15": {"documentation": {"title": "Eigenvalue spectrum for single particle in a spheroidal cavity: A\n  Semiclassical approach", "source": "Sham S. Malik, A. K. Jain and S. R. Jain", "docs_id": "nucl-th/0209056", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Eigenvalue spectrum for single particle in a spheroidal cavity: A\n  Semiclassical approach. Following the semiclassical formalism of Strutinsky et al., we have obtained the complete eigenvalue spectrum for a particle enclosed in an infinitely high spheroidal cavity. Our spheroidal trace formula also reproduces the results of a spherical billiard in the limit $\\eta\\to1.0$. Inclusion of repetition of each family of the orbits with reference to the largest one significantly improves the eigenvalues of sphere and an exact comparison with the quantum mechanical results is observed upto the second decimal place for $kR_{0}\\geq{7}$. The contributions of the equatorial, the planar (in the axis of symmetry plane) and the non-planar(3-Dimensional) orbits are obtained from the same trace formula by using the appropriate conditions. The resulting eigenvalues compare very well with the quantum mechanical eigenvalues at normal deformation. It is interesting that the partial sum of equatorial orbits leads to eigenvalues with maximum angular momentum projection, while the summing of planar orbits leads to eigenvalues with $L_z=0$ except for L=1. The remaining quantum mechanical eigenvalues are observed to arise from the 3-dimensional(3D) orbits. Very few spurious eigenvalues arise in these partial sums. This result establishes the important role of 3D orbits even at normal deformations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the significance of the partial sum of equatorial orbits in the eigenvalue spectrum of a particle in a spheroidal cavity, as described in the seminal work of Strutinsky et al., and how does it relate to the quantum mechanical eigenvalues?\n\nA) The partial sum of equatorial orbits leads to eigenvalues with maximum angular momentum projection, which is a characteristic of the quantum mechanical eigenvalues.\n\nB) The partial sum of equatorial orbits leads to eigenvalues with $L_z=0$ except for L=1, which is a characteristic of the quantum mechanical eigenvalues.\n\nC) The partial sum of equatorial orbits leads to eigenvalues with maximum angular momentum projection, which is a characteristic of the quantum mechanical eigenvalues, and this result establishes the important role of 3D orbits even at normal deformations.\n\nD) The partial sum of equatorial orbits leads to eigenvalues with $L_z=0$ except for L=1, which is a characteristic of the quantum mechanical eigenvalues, and this result establishes the important role of 3D orbits even at normal deformations.\n\nCorrect Answer: C) The partial sum of equatorial orbits leads to eigenvalues with maximum angular momentum projection, which is a characteristic of the quantum mechanical eigenvalues, and this result establishes the important role of 3D orbits even at normal deformations.\n\nExplanation: The correct answer is C) because it accurately summarizes the significance of the partial sum of equatorial orbits in the eigenvalue spectrum of a particle in a spheroidal cavity. The partial sum of equatorial orbits leads to eigenvalues with maximum angular momentum projection, which is a characteristic of the quantum mechanical eigenvalues. This result establishes the important role of 3D orbits even at normal deformations, as mentioned in the original documentation."}, "16": {"documentation": {"title": "Improved Cryptanalysis of Rank Metric Schemes Based on Gabidulin Codes", "source": "Ayoub Otmani and Herv\\'e Tal\\'e Kalachi and S\\'elestin Ndjeya", "docs_id": "1602.08549", "section": ["cs.CR", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved Cryptanalysis of Rank Metric Schemes Based on Gabidulin Codes. We prove that any variant of the GPT cryptosystem which uses a right column scrambler over the extension field as advocated by the works of Gabidulin et al. with the goal to resist to Overbeck's structural attack are actually still vulnerable to that attack. We show that by applying the Frobenius operator appropriately on the public key, it is possible to build a Gabidulin code having the same dimension as the original secret Gabidulin code but with a lower length. In particular, the code obtained by this way correct less errors than the secret one but its error correction capabilities are beyond the number of errors added by a sender, and consequently an attacker is able to decrypt any ciphertext with this degraded Gabidulin code. We also considered the case where an isometric transformation is applied in conjunction with a right column scrambler which has its entries in the extension field. We proved that this protection is useless both in terms of performance and security. Consequently, our results show that all the existing techniques aiming to hide the inherent algebraic structure of Gabidulin codes have failed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion of the study on the cryptanalysis of Rank Metric Schemes Based on Gabidulin Codes, and how does it impact the security of the GPT cryptosystem?\n\nA) The study proves that Gabidulin codes are inherently secure against all types of attacks, and therefore the GPT cryptosystem is secure.\nB) The study shows that the use of a right column scrambler over the extension field is effective in resisting Overbeck's structural attack, and therefore the GPT cryptosystem is secure.\nC) The study demonstrates that the GPT cryptosystem is vulnerable to Overbeck's structural attack, and that the use of a right column scrambler over the extension field is actually ineffective in resisting this attack.\nD) The study concludes that the GPT cryptosystem is secure as long as the secret Gabidulin code has a certain property, but the property is not specified.\n\n**Correct Answer:** C) The study demonstrates that the GPT cryptosystem is vulnerable to Overbeck's structural attack, and that the use of a right column scrambler over the extension field is actually ineffective in resisting this attack.\n\n**Explanation:** The study proves that any variant of the GPT cryptosystem that uses a right column scrambler over the extension field is actually vulnerable to Overbeck's structural attack. This means that the use of this protection method is ineffective in resisting the attack, and therefore the GPT cryptosystem is not secure. The correct answer requires the test-taker to understand the implications of the study's findings and to identify the correct conclusion."}, "17": {"documentation": {"title": "Taming the B --> X(s) gamma spectrum by Dressed Gluon Exponentiation", "source": "Jeppe R. Andersen and Einan Gardi (Cambridge)", "docs_id": "hep-ph/0502159", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Taming the B --> X(s) gamma spectrum by Dressed Gluon Exponentiation. We show that the B \\to X(s) gamma photon energy (E_gamma) spectrum can be reliably computed by resummed perturbation theory. Our calculation is based on Dressed Gluon Exponentiation (DGE) incorporating Sudakov and renormalon resummation. It is shown that the resummed spectrum does not have the perturbative support properties: it smoothly extends to the non-perturbative region E_gamma > m/2, where m is the quark pole mass, and tends to zero near the physical endpoint. The calculation of the Sudakov factor, which determines the shape of the spectrum in the peak region, as well as that of the pole mass, which sets the energy scale, are performed using Principal-Value Borel summation. By using the same prescription in both, the cancellation of the leading renormalon ambiguity is respected. Furthermore, in computing the Sudakov exponent we go beyond the formal next-to-next-to-leading logarithmic accuracy using the large-order asymptotic behavior of the series, which is accurately determined from the relation with the pole mass. Upon matching the resummed result with the next-to-leading order expression we compute the spectrum, obtain its moments as a function of a minimum photon energy cut, analyze sources of uncertainty and show that our predictions are in good agreement with Belle data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the B \u2192 X(s) \u03b3 spectrum calculation using Dressed Gluon Exponentiation (DGE), what is the primary advantage of employing Principal-Value Borel summation in both the Sudakov factor and the pole mass calculation?\n\nA) It allows for the cancellation of higher-order renormalon ambiguities.\nB) It enables the computation of the Sudakov exponent with next-to-next-to-leading logarithmic accuracy.\nC) It facilitates the matching of the resummed result with the next-to-leading order expression.\nD) It provides a more accurate determination of the large-order asymptotic behavior of the series.\n\nCorrect Answer: B) It enables the computation of the Sudakov exponent with next-to-next-to-leading logarithmic accuracy.\n\nExplanation: The correct answer is B) because the documentation states that the computation of the Sudakov exponent goes beyond the formal next-to-next-to-leading logarithmic accuracy using the large-order asymptotic behavior of the series, which is accurately determined from the relation with the pole mass. This is a key aspect of the DGE approach, as it allows for a more accurate description of the B \u2192 X(s) \u03b3 spectrum in the non-perturbative region. The other options are incorrect because they do not accurately capture the primary advantage of employing Principal-Value Borel summation in this context."}, "18": {"documentation": {"title": "Preference Estimation in Deferred Acceptance with Partial School\n  Rankings", "source": "Shanjukta Nath", "docs_id": "2010.15960", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Preference Estimation in Deferred Acceptance with Partial School\n  Rankings. The Deferred Acceptance algorithm is a popular school allocation mechanism thanks to its strategy proofness. However, with application costs, strategy proofness fails, leading to an identification problem. In this paper, I address this identification problem by developing a new Threshold Rank setting that models the entire rank order list as a one-step utility maximization problem. I apply this framework to study student assignments in Chile. There are three critical contributions of the paper. I develop a recursive algorithm to compute the likelihood of my one-step decision model. Partial identification is addressed by incorporating the outside value and the expected probability of admission into a linear cost framework. The empirical application reveals that although school proximity is a vital variable in school choice, student ability is critical for ranking high academic score schools. The results suggest that policy interventions such as tutoring aimed at improving student ability can help increase the representation of low-income low-ability students in better quality schools in Chile."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Deferred Acceptance algorithm, what is the primary limitation of strategy proofness when application costs are considered, and how does the proposed Threshold Rank setting address this limitation?\n\n**A)** Strategy proofness fails due to the complexity of the rank order list, and the Threshold Rank setting simplifies the problem by assuming a linear cost framework.\n\n**B)** Strategy proofness is maintained when application costs are considered, but the Threshold Rank setting introduces a new identification problem by incorporating outside values and expected probabilities.\n\n**C)** The Threshold Rank setting addresses the identification problem by modeling the entire rank order list as a one-step utility maximization problem, which allows for a recursive algorithm to compute the likelihood of the decision model.\n\n**D)** The Threshold Rank setting is a variant of the Deferred Acceptance algorithm that eliminates the need for strategy proofness, but it introduces a new assumption about the relationship between school proximity and student ability.\n\n**Correct Answer:** C) The Threshold Rank setting addresses the identification problem by modeling the entire rank order list as a one-step utility maximization problem, which allows for a recursive algorithm to compute the likelihood of the decision model.\n\n**Explanation:** The correct answer is C) because the Threshold Rank setting is a key contribution of the paper, which addresses the identification problem by modeling the entire rank order list as a one-step utility maximization problem. This allows for a recursive algorithm to compute the likelihood of the decision model, which is a critical step in understanding the behavior of the Deferred Acceptance algorithm in the presence of application costs. The other options are incorrect because they either misrepresent the limitation of strategy proofness (A), introduce a new identification problem (B), or incorrectly describe the Threshold Rank setting (D)."}, "19": {"documentation": {"title": "Solving Heterogeneous General Equilibrium Economic Models with Deep\n  Reinforcement Learning", "source": "Edward Hill, Marco Bardoscia and Arthur Turrell", "docs_id": "2103.16977", "section": ["econ.GN", "cs.LG", "cs.MA", "q-fin.EC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Heterogeneous General Equilibrium Economic Models with Deep\n  Reinforcement Learning. General equilibrium macroeconomic models are a core tool used by policymakers to understand a nation's economy. They represent the economy as a collection of forward-looking actors whose behaviours combine, possibly with stochastic effects, to determine global variables (such as prices) in a dynamic equilibrium. However, standard semi-analytical techniques for solving these models make it difficult to include the important effects of heterogeneous economic actors. The COVID-19 pandemic has further highlighted the importance of heterogeneity, for example in age and sector of employment, in macroeconomic outcomes and the need for models that can more easily incorporate it. We use techniques from reinforcement learning to solve such models incorporating heterogeneous agents in a way that is simple, extensible, and computationally efficient. We demonstrate the method's accuracy and stability on a toy problem for which there is a known analytical solution, its versatility by solving a general equilibrium problem that includes global stochasticity, and its flexibility by solving a combined macroeconomic and epidemiological model to explore the economic and health implications of a pandemic. The latter successfully captures plausible economic behaviours induced by differential health risks by age."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of heterogeneous general equilibrium economic models, what is the primary challenge posed by standard semi-analytical techniques in capturing the effects of heterogeneous economic actors, and how do reinforcement learning techniques address this challenge?\n\n**A)** Standard semi-analytical techniques struggle to capture the effects of heterogeneous economic actors due to the complexity of their interactions, and reinforcement learning techniques address this challenge by incorporating stochastic effects and global variables in a more straightforward manner.\n\n**B)** Standard semi-analytical techniques are unable to capture the effects of heterogeneous economic actors because they rely on simplifying assumptions, whereas reinforcement learning techniques address this challenge by incorporating more realistic and nuanced representations of economic behaviors.\n\n**C)** Standard semi-analytical techniques are limited in their ability to capture the effects of heterogeneous economic actors due to computational constraints, whereas reinforcement learning techniques address this challenge by employing more efficient algorithms and parallel processing techniques.\n\n**D)** Standard semi-analytical techniques are unable to capture the effects of heterogeneous economic actors because they lack the ability to model complex interactions between agents, whereas reinforcement learning techniques address this challenge by incorporating techniques from machine learning and optimization.\n\n**Correct Answer:** B) Standard semi-analytical techniques are unable to capture the effects of heterogeneous economic actors because they rely on simplifying assumptions, whereas reinforcement learning techniques address this challenge by incorporating more realistic and nuanced representations of economic behaviors.\n\n**Explanation:** The correct answer, B, highlights the primary challenge posed by standard semi-analytical techniques in capturing the effects of heterogeneous economic actors. These techniques rely on simplifying assumptions, which can lead to inaccurate representations of complex economic behaviors. In contrast, reinforcement learning techniques address this challenge by incorporating more realistic and nuanced representations of economic behaviors, allowing for a more accurate capture of the effects of heterogeneous economic actors."}, "20": {"documentation": {"title": "Cosmological Origin of the Stellar Velocity Dispersions in Massive\n  Early-Type Galaxies", "source": "Abraham Loeb and P.J.E. Peebles", "docs_id": "astro-ph/0211465", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cosmological Origin of the Stellar Velocity Dispersions in Massive\n  Early-Type Galaxies. We show that the observed upper bound on the line-of-sight velocity dispersion of the stars in an early-type galaxy, sigma<400km/s, may have a simple dynamical origin within the LCDM cosmological model, under two main hypotheses. The first is that most of the stars now in the luminous parts of a giant elliptical formed at redshift z>6. Subsequently, the stars behaved dynamically just as an additional component of the dark matter. The second hypothesis is that the mass distribution characteristic of a newly formed dark matter halo forgets such details of the initial conditions as the stellar \"collisionless matter\" that was added to the dense parts of earlier generations of halos. We also assume that the stellar velocity dispersion does not evolve much at z<6, because a massive host halo grows mainly by the addition of material at large radii well away from the stellar core of the galaxy. These assumptions lead to a predicted number density of ellipticals as a function of stellar velocity dispersion that is in promising agreement with the Sloan Digital Sky Survey data."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the primary assumption underlying the authors' hypothesis that the observed upper bound on the line-of-sight velocity dispersion of the stars in an early-type galaxy (\u03c3 < 400 km/s) may have a simple dynamical origin within the LCDM cosmological model?\n\nA) The stellar velocity dispersion evolves significantly at z < 6 due to interactions with the dark matter halo.\nB) The mass distribution of a newly formed dark matter halo forgets the details of the initial conditions, including the stellar \"collisionless matter\" added to the dense parts of earlier generations of halos.\nC) The majority of stars in the luminous parts of a giant elliptical formed at redshift z > 6, and subsequently behaved dynamically as an additional component of the dark matter.\nD) The stellar velocity dispersion is solely determined by the gravitational potential of the dark matter halo, with no role for dynamical processes.\n\nCorrect Answer: C) The majority of stars in the luminous parts of a giant elliptical formed at redshift z > 6, and subsequently behaved dynamically as an additional component of the dark matter.\n\nExplanation: The correct answer is C) because the authors propose that most of the stars in the luminous parts of a giant elliptical formed at redshift z > 6, and subsequently behaved dynamically just as an additional component of the dark matter. This assumption is a key part of the authors' hypothesis, and is supported by the assumption that the stellar velocity dispersion does not evolve much at z < 6."}, "21": {"documentation": {"title": "Analytical properties of horizontal visibility graphs in the Feigenbaum\n  scenario", "source": "Bartolo Luque, Lucas Lacasa, Fernando J. Ballesteros, Alberto Robledo", "docs_id": "1201.2514", "section": ["physics.data-an", "math.DS", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analytical properties of horizontal visibility graphs in the Feigenbaum\n  scenario. Time series are proficiently converted into graphs via the horizontal visibility (HV) algorithm, which prompts interest in its capability for capturing the nature of different classes of series in a network context. We have recently shown [1] that dynamical systems can be studied from a novel perspective via the use of this method. Specifically, the period-doubling and band-splitting attractor cascades that characterize unimodal maps transform into families of graphs that turn out to be independent of map nonlinearity or other particulars. Here we provide an in depth description of the HV treatment of the Feigenbaum scenario, together with analytical derivations that relate to the degree distributions, mean distances, clustering coefficients, etc., associated to the bifurcation cascades and their accumulation points. We describe how the resultant families of graphs can be framed into a renormalization group scheme in which fixed-point graphs reveal their scaling properties. These fixed points are then re-derived from an entropy optimization process defined for the graph sets, confirming a suggested connection between renormalization group and entropy optimization. Finally, we provide analytical and numerical results for the graph entropy and show that it emulates the Lyapunov exponent of the map independently of its sign."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between the horizontal visibility (HV) algorithm and the Feigenbaum scenario, specifically in terms of the emergence of graph properties such as degree distributions, mean distances, and clustering coefficients. How do the resultant families of graphs relate to the bifurcation cascades and their accumulation points, and what insights do these relationships provide into the underlying dynamics of the system?\n\n**A)** The HV algorithm is a direct mapping of the Feigenbaum scenario, preserving the underlying dynamical structure of the system. The resulting graphs exhibit a clear scaling property, with fixed-point graphs revealing their renormalization group behavior.\n\n**B)** The HV algorithm is a heuristic method that captures the essence of the Feigenbaum scenario, but does not preserve the underlying dynamical structure of the system. The resulting graphs exhibit a complex, non-scaling behavior, with no clear fixed-point graphs.\n\n**C)** The HV algorithm is a tool for analyzing the Feigenbaum scenario, but does not provide any insights into the underlying dynamics of the system. The resulting graphs are simply a representation of the data, without any deeper meaning.\n\n**D)** The HV algorithm is a method for generating graphs that are independent of the Feigenbaum scenario, and do not capture any of the underlying dynamical structure of the system. The resulting graphs exhibit a uniform, random behavior, with no fixed-point graphs or scaling properties.\n\n**Correct Answer:** A) The HV algorithm is a direct mapping of the Feigenbaum scenario, preserving the underlying dynamical structure of the system. The resulting graphs exhibit a clear scaling property, with fixed-point graphs revealing their renormalization group behavior.\n\n**Explanation:** The HV algorithm is a method for converting time series into graphs, which has been shown to preserve the underlying dynamical structure of the system in the Feigenbaum scenario. The resulting graphs exhibit a clear scaling property, with fixed-point graphs revealing their renormalization group behavior. This is because the HV algorithm captures the essential features of the Feigenbaum scenario, including the period-doubling and band-splitting attractor cascades, and preserves these features in the resulting graphs. The fixed-point graphs in particular provide valuable insights into the underlying dynamics of the system, revealing the scaling properties of the system and the emergence of complex behavior."}, "22": {"documentation": {"title": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos", "source": "Pia Bideau, Erik Learned-Miller", "docs_id": "1604.00136", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary contribution of the proposed likelihood function in the paper \"It's Moving! A Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos\"?\n\nA) It uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects, but does not improve the initialization of the motion segmentation algorithm.\n\nB) It uses a combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects, but does not improve the initialization of the motion segmentation algorithm and is not a novel combination.\n\nC) It uses a combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects, improves the initialization of the motion segmentation algorithm, and is a novel combination that beats current state-of-the-art methods by a large margin.\n\nD) It uses a combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects, but does not improve the initialization of the motion segmentation algorithm and is not a novel combination.\n\nCorrect Answer: C) It uses a combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects, improves the initialization of the motion segmentation algorithm, and is a novel combination that beats current state-of-the-art methods by a large margin.\n\nExplanation: The correct answer is C) because the paper states that the proposed likelihood function uses a \"novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects\". Additionally, the paper claims that the algorithm developed using this likelihood function \"beats current state-of-the-art methods by a large margin\", indicating that it improves the initialization of the motion segmentation algorithm."}, "23": {"documentation": {"title": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games", "source": "Rubens O. Moraes and Levi H. S. Lelis", "docs_id": "1711.08101", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games. Action abstractions restrict the number of legal actions available during search in multi-unit real-time adversarial games, thus allowing algorithms to focus their search on a set of promising actions. Optimal strategies derived from un-abstracted spaces are guaranteed to be no worse than optimal strategies derived from action-abstracted spaces. In practice, however, due to real-time constraints and the state space size, one is only able to derive good strategies in un-abstracted spaces in small-scale games. In this paper we introduce search algorithms that use an action abstraction scheme we call asymmetric abstraction. Asymmetric abstractions retain the un-abstracted spaces' theoretical advantage over regularly abstracted spaces while still allowing the search algorithms to derive effective strategies, even in large-scale games. Empirical results on combat scenarios that arise in a real-time strategy game show that our search algorithms are able to substantially outperform state-of-the-art approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using asymmetric action abstractions in multi-unit control in adversarial real-time games, and how does it differ from regularly abstracted spaces?\n\nA) Asymmetric abstractions allow for a larger state space, enabling more comprehensive search.\nB) Asymmetric abstractions retain the theoretical advantage of un-abstracted spaces, while still allowing effective strategies to be derived in large-scale games.\nC) Asymmetric abstractions reduce the number of legal actions available, making the search space more manageable.\nD) Asymmetric abstractions are only applicable to small-scale games, whereas regularly abstracted spaces are suitable for large-scale games.\n\nCorrect Answer: B) Asymmetric abstractions retain the theoretical advantage of un-abstracted spaces, while still allowing effective strategies to be derived in large-scale games.\n\nExplanation: The correct answer highlights the key benefit of asymmetric action abstractions, which is to retain the theoretical advantage of un-abstracted spaces (i.e., optimal strategies derived from un-abstracted spaces are guaranteed to be no worse than those derived from abstracted spaces) while still enabling effective strategies to be derived in large-scale games. This is in contrast to regularly abstracted spaces, which may sacrifice some of the theoretical advantage in order to make the search space more manageable."}, "24": {"documentation": {"title": "Low-Power Wireless Wearable ECG Monitoring Chestbelt Based on\n  Ferroelectric Microprocessor", "source": "Zhendong Ai, Zihan Wang, Wei Cui", "docs_id": "2012.02290", "section": ["eess.SP", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low-Power Wireless Wearable ECG Monitoring Chestbelt Based on\n  Ferroelectric Microprocessor. Since cadiovascular disease (CVD) posts a heavy threat to people's health, long-term electrocardiogram (ECG) monitoring is of great value for the improvement of treatment. To realize remote long-term ECG monitoring, a low-power wireless wearable ECG monitoring device is proposed in this paper. The ECG monitoring device, abbreviated as ECGM, is designed based on ferroelectric microprocessor which provides ultra-low power consumption and contains four parts-MCU, BLE, Sensors and Power. The MCU part means circuit of MSP430FR2433, the core of ECGM. The BLE part is the CC2640R2F module applied for wireless transmission of the collected bio-signal data. And the sensors part includes several sensors like BMD101 used for monitoring bio-signals and motion of the wearer, while the Power part consists of battery circuit, charging circuit and 3.3V/1.8V/4.4V power supply circuit. The ECGM first collects ECG signals from the fabric electrodes adhered to wearers' chest, preprocesses the signals to eliminate the injected noise, and then transmit the output data to wearers' hand-held mobile phones through Bluetooth low energy (BLE). The wearers are enabled to acquire ECGs and other physiological parameters on their phones as well as some corresponding suggestions. The novelty of the system lies in the combination of low-power ECG sensor chip with ferroelectric microprocessor, thus achieving ultra-low power consumption and high signal quality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a ferroelectric microprocessor in the design of the low-power wireless wearable ECG monitoring device, ECGM?\n\nA) Reduced power consumption for the MCU part\nB) Improved signal quality for the BLE part\nC) Enhanced sensor capabilities for the Sensors part\nD) Ultra-low power consumption and high signal quality for the entire system\n\n**Correct Answer:** D) Ultra-low power consumption and high signal quality for the entire system\n\n**Explanation:** The question requires the candidate to understand the overall design and functionality of the ECGM device. The correct answer, D, highlights the primary advantage of using a ferroelectric microprocessor, which is the combination of ultra-low power consumption and high signal quality for the entire system. The other options, A, B, and C, are incorrect because they focus on specific components or aspects of the device, rather than the overall benefit of using a ferroelectric microprocessor.\n\n**Additional question:** What is the primary function of the BLE part in the ECGM device?\n\nA) To collect ECG signals from the fabric electrodes\nB) To preprocess the ECG signals to eliminate injected noise\nC) To transmit the output data to the wearer's hand-held mobile phone\nD) To provide power supply to the device\n\n**Correct Answer:** C) To transmit the output data to the wearer's hand-held mobile phone\n\n**Explanation:** This question requires the candidate to understand the specific function of the BLE part in the ECGM device. The correct answer, C, highlights the primary function of the BLE part, which is to transmit the output data to the wearer's hand-held mobile phone. The other options, A, B, and D, are incorrect because they describe other functions or components of the device."}, "25": {"documentation": {"title": "Optimal Non-Coherent Detector for Ambient Backscatter Communication\n  System", "source": "Sudarshan Guruacharya, Xiao Lu, and Ekram Hossain", "docs_id": "1911.10105", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Non-Coherent Detector for Ambient Backscatter Communication\n  System. The probability density function (pdf) of the received signal of an ambient backscatter communication system is derived, assuming that on-off keying (OOK) is performed at the tag, and that the ambient radio frequency (RF) signal is white Gaussian. The pdf of the received signal is then utilized to design two different types of non-coherent detectors. The first detector directly uses the received signal to perform a hypothesis test. The second detector first estimates the channel based on the observed signal and then performs the hypothesis test. Test statistics and optimal decision threshold of the detectors are derived. The energy detector is shown to be an approximation of the second detector. For cases where the reader is able to avoid or cancel the direct interference from the RF source (e.g., through successive interference cancellation), a third detector is given as a special case of the first detector. Numerical results show that both the first and the second detectors have the same bit error rate (BER) performance, making the second detector preferable over the first detector due to its computational simplicity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of ambient backscatter communication, what is the primary advantage of using the second detector over the first detector, and how does it relate to the energy detector?\n\n**A)** The second detector is more computationally intensive than the first detector, resulting in a higher BER performance. \n**B)** The second detector is a special case of the first detector, where the channel estimation step can be skipped. \n**C)** The second detector has the same BER performance as the first detector, but is preferable due to its computational simplicity, making it more suitable for real-time applications. \n**D)** The second detector is only applicable when the direct interference from the RF source can be avoided or cancelled, and is not a general-purpose detector.\n\n**Correct Answer:** C) The second detector has the same BER performance as the first detector, but is preferable due to its computational simplicity, making it more suitable for real-time applications.\n\n**Explanation:** The correct answer is C) because the documentation states that both the first and second detectors have the same BER performance, but the second detector is preferable due to its computational simplicity, making it more suitable for real-time applications. This requires the test-taker to carefully read and understand the trade-offs between the two detectors, and to identify the key advantage of the second detector."}, "26": {"documentation": {"title": "Few-body approach to structure of $\\bar{K}$-nuclear quasi-bound states", "source": "Shota Ohnishi, Wataru Horiuchi, Tsubasa Hoshino, Kenta Miyahara,\n  Tetsuo Hyodo", "docs_id": "1701.07589", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Few-body approach to structure of $\\bar{K}$-nuclear quasi-bound states. Structure of light antikaon-nuclear quasi-bound states, which consist of an antikaon $(\\bar{K}=K^-,~\\bar{K}^0)$ and a few nucleons $(N=p,~n)$ such as $\\bar{K}NN$, $\\bar{K}NNN$, $\\bar{K}NNNN$ and $\\bar{K}NNNNNN$ systems, is studied with full three- to seven-body calculations. Employing a realistic $\\bar{K}N$ potential based on the chiral SU(3) effective field theory with the SIDDHARTA constraint, we show that the central nucleon densities of these systems increases when the antikaon is injected, by about factor of two at maximum. The $\\bar{K}NNNN$ system shows the largest central density, about 0.74 fm$^{-3}$ even with the phenomenological $\\bar{K}N$ potential, which are not as high as those suggested in previous studies with approximate treatments of the few-body systems. We find the spin of the ground state of the $\\bar{K}NNNNNN$ system depends on the strength of the $\\bar{K}N$ attraction. Thus, the quantum number of the ground state can be another constraint on the $\\bar{K}N$ interaction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the central nucleon density of $\\bar{K}$-nuclear quasi-bound states, and how does it compare to previous studies?\n\nA) The central nucleon density decreases by a factor of two when an antikaon is injected, and previous studies overestimated the density.\nB) The central nucleon density increases by a factor of two when an antikaon is injected, and previous studies underestimated the density.\nC) The central nucleon density remains unchanged when an antikaon is injected, and previous studies were correct in their estimates.\nD) The central nucleon density decreases by a factor of two when an antikaon is injected, and previous studies were incorrect in their assumptions.\n\nCorrect Answer: B) The central nucleon density increases by a factor of two when an antikaon is injected, and previous studies underestimated the density.\n\nExplanation: The correct answer is B) because the documentation states that the central nucleon density \"increases when the antikaon is injected, by about a factor of two at maximum\". This contradicts option A, which states that the density decreases. Option C is incorrect because the documentation states that the density increases, not remains unchanged. Option D is also incorrect because the documentation states that the density increases, not decreases."}, "27": {"documentation": {"title": "Embodied Self-supervised Learning by Coordinated Sampling and Training", "source": "Yifan Sun and Xihong Wu", "docs_id": "2006.13350", "section": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Embodied Self-supervised Learning by Coordinated Sampling and Training. Self-supervised learning can significantly improve the performance of downstream tasks, however, the dimensions of learned representations normally lack explicit physical meanings. In this work, we propose a novel self-supervised approach to solve inverse problems by employing the corresponding physical forward process so that the learned representations can have explicit physical meanings. The proposed approach works in an analysis-by-synthesis manner to learn an inference network by iteratively sampling and training. At the sampling step, given observed data, the inference network is used to approximate the intractable posterior, from which we sample input parameters and feed them to a physical process to generate data in the observational space; At the training step, the same network is optimized with the sampled paired data. We prove the feasibility of the proposed method by tackling the acoustic-to-articulatory inversion problem to infer articulatory information from speech. Given an articulatory synthesizer, an inference model can be trained completely from scratch with random initialization. Our experiments demonstrate that the proposed method can converge steadily and the network learns to control the articulatory synthesizer to speak like a human. We also demonstrate that trained models can generalize well to unseen speakers or even new languages, and performance can be further improved through self-adaptation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the proposed self-supervised learning approach in \"Embodied Self-supervised Learning by Coordinated Sampling and Training\", and how does it differ from traditional self-supervised learning methods?\n\nA) To learn explicit physical meanings from learned representations by employing the corresponding physical forward process, and to improve the performance of downstream tasks by leveraging the learned representations.\n\nB) To learn implicit physical meanings from learned representations by ignoring the physical forward process, and to improve the performance of downstream tasks by leveraging the learned representations.\n\nC) To learn explicit physical meanings from learned representations by employing the corresponding physical forward process, and to improve the performance of downstream tasks by leveraging the learned representations, but only for tasks that require physical interactions.\n\nD) To learn implicit physical meanings from learned representations by ignoring the physical forward process, and to improve the performance of downstream tasks by leveraging the learned representations, but only for tasks that do not require physical interactions.\n\nCorrect Answer: A) To learn explicit physical meanings from learned representations by employing the corresponding physical forward process, and to improve the performance of downstream tasks by leveraging the learned representations.\n\nExplanation: The correct answer is A) because the proposed approach aims to learn explicit physical meanings from learned representations by employing the corresponding physical forward process, which is a key aspect of the method. The other options are incorrect because they either ignore the physical forward process (options B and D) or limit the application of the method to specific tasks (option C)."}, "28": {"documentation": {"title": "New Perspectives to Reduce Stress through Digital Humor", "source": "Misnal Munir, Amaliyah, Moses Glorino Rumambo Pandin", "docs_id": "2012.03144", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New Perspectives to Reduce Stress through Digital Humor. This study aimed to find new perspectives on the use of humor through digital media. A qualitative approach was used to conduct this study, where data were collected through a literature review. Stress is caused by the inability of a person to adapt between desires and reality. All forms of stress are basically caused by a lack of understanding of human's own limitations. Inability to fight limitations that will cause frustration, conflict, anxiety, and guilt. Too much stress can threaten a person's ability to deal with the environment. As a result, employees develop various kinds of stress symptoms that can interfere with their work performance. Thus, the management of work stress is important to do, one of which uses humor. However, in the digital age, the spread of humor can be easily facilitated. The results of this review article find new perspectives to reduce stress through digital humor, namely interactive humor, funny photos, manipulations, phanimation, celebrity soundboards, and PowerPoint humor. The research shows that the use of humor as a coping strategy is able to predict positive affect and well-being work-related. Moreover, digital humor which has various forms as well as easy, fast, and wide spread, then the effect is felt increasingly significant"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary limitations that lead to stress, according to the study, and how can the use of digital humor help mitigate these limitations?\n\nA) Unrealistic expectations and inability to cope with the consequences of one's actions lead to stress, and digital humor can help by providing a healthy distraction and improving mood.\n\nB) Lack of understanding of one's own limitations and inability to adapt to changing circumstances lead to stress, and digital humor can help by providing a sense of control and empowerment.\n\nC) Overemphasis on work-related tasks and inability to disconnect from work lead to stress, and digital humor can help by providing a much-needed break and improving work-life balance.\n\nD) Unrealistic expectations and inability to cope with the consequences of one's actions lead to stress, and digital humor can help by providing a healthy distraction and improving mood, but only for employees who are already experiencing high levels of stress.\n\nCorrect Answer: B) Lack of understanding of one's own limitations and inability to adapt to changing circumstances lead to stress, and digital humor can help by providing a sense of control and empowerment."}, "29": {"documentation": {"title": "Strong Clustering of Faint Galaxies at Small Angular Scales", "source": "L. Infante (P. Univ. Catolica de Chile), D.F. de Mello (Observatorio\n  Nacional-DAN, Brazil) and F. Menanteau (P. Univ. Catolica de Chile)", "docs_id": "astro-ph/9608037", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strong Clustering of Faint Galaxies at Small Angular Scales. The 2-point angular correlation function of galaxies, \\wt, has been computed on equatorial fields observed with the CTIO 4m prime focus, within a total area of 2.31 deg$^2$. In the magnitude range $19\\le m_R \\le 21.5$, corresponding to $<z>\\approx 0.35$, we find an excess of power in \\wt at scales $2''\\le\\theta \\le6''$ over what would be expected from an extrapolation of \\wt measured at larger $\\theta$. The significance of this excess is $\\approx 5\\sigma$. At larger scales, $6''< \\theta \\le 24''$, the amplitude of \\wt is 1.6 times smaller than the standard no evolutionary model. At these scales there is remarkable agreement between the present data and Infante \\& Pritchet (1995). At large angular scales ($6''< \\theta \\le 24''$) the data is best described by a model where clustering evolution in $\\xi(r,z)$ has taken place. Strong luminosity evolution cannot be ruled out with the present data. At smaller scales, $2''\\le \\theta \\le 6''$, our data are formally fit by models where $\\epsilon=-2.4 (\\Omega=0.2, r_o=5.1h^{-1}$Mpc) or $r_o = 7.3h^{-1}$Mpc $(\\Omega=0.2, \\epsilon=0)$. If the mean redshift of our sample is 0.35 then our data show a clear detection of the scale ($\\approx 19h^{-1}kpc$) where the clustering evolution approaches a highly non linear regime, i.e., $\\epsilon \\le 0$. The rate at which galaxies merge has been computed. If this rate is proportional to $(1+z)^m$, then $m=2.2 \\pm 0.5$."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the 2-point angular correlation function of galaxies, \\wt, in the context of galaxy clustering evolution. What can be inferred about the scale at which clustering evolution approaches a highly non-linear regime, and what implications does this have for our understanding of galaxy mergers?\n\nA) The scale at which clustering evolution approaches a highly non-linear regime is approximately 10h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^3.\n\nB) The scale at which clustering evolution approaches a highly non-linear regime is approximately 19h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^2.5.\n\nC) The scale at which clustering evolution approaches a highly non-linear regime is approximately 30h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^4.\n\nD) The scale at which clustering evolution approaches a highly non-linear regime is approximately 5h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^1.\n\n**Correct Answer:** B) The scale at which clustering evolution approaches a highly non-linear regime is approximately 19h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^2.5.\n\n**Explanation:** The correct answer is based on the text, which states that the scale at which clustering evolution approaches a highly non-linear regime is approximately 19h^{-1}kpc. Additionally, the text mentions that the merger rate is proportional to (1+z)^m, and estimates this value to be m = 2.2 \u00b1 0.5. Therefore, the correct answer is B) The scale at which clustering evolution approaches a highly non-linear regime is approximately 19h^{-1}kpc, and this corresponds to a merger rate that is proportional to (1+z)^2.5."}, "30": {"documentation": {"title": "On the usefulness of finding charts Or the runaway carbon stars of the\n  Blanco & McCarthy field 37", "source": "C. Loup (IAP, CNRS), N. Delmotte, D. Egret, M.-R. Cioni, F. Genova", "docs_id": "astro-ph/0302385", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the usefulness of finding charts Or the runaway carbon stars of the\n  Blanco & McCarthy field 37. We have been recently faced with the problem of cross--identifying stars recorded in historical catalogues with those extracted from recent fully digitized surveys (such as DENIS and 2MASS). Positions mentioned in the old catalogues are frequently of poor precision, but are generally accompanied by finding charts where the interesting objects are flagged. Those finding charts are sometimes our only link with the accumulated knowledge of past literature. While checking the identification of some of these objects in several catalogues, we had the surprise to discover a number of discrepancies in recent works.The main reason for these discrepancies was generally the blind application of the smallest difference in position as the criterion to identify sources from one historical catalogue to those in more recent surveys. In this paper we give examples of such misidentifications, and show how we were able to find and correct them.We present modern procedures to discover and solve cross--identification problems, such as loading digitized images of the sky through the Aladin service at CDS, and overlaying entries from historical catalogues and modern surveys. We conclude that the use of good finding charts still remains the ultimate (though time--consuming) tool to ascertain cross--identifications in difficult cases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for discrepancies in cross-identifying stars from historical catalogues with those extracted from recent fully digitized surveys, such as DENIS and 2MASS?\n\nA) The lack of finding charts in historical catalogues\nB) The blind application of the smallest difference in position as the criterion to identify sources\nC) The poor precision of positions mentioned in old catalogues\nD) The limited resolution of modern surveys\n\nCorrect Answer: B) The blind application of the smallest difference in position as the criterion to identify sources\n\nExplanation: The passage states that the main reason for discrepancies was the blind application of the smallest difference in position as the criterion to identify sources from one historical catalogue to those in more recent surveys. This highlights the importance of considering other factors beyond just positional differences when cross-identifying stars."}, "31": {"documentation": {"title": "Information Based Data-Driven Characterization of Stability and\n  Influence in Power Systems", "source": "Subhrajit Sinha, Pranav Sharma, Venkataramana Ajjarapu, Umesh Vaidya", "docs_id": "1910.11379", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information Based Data-Driven Characterization of Stability and\n  Influence in Power Systems. Stability analysis of a power network and its characterization (voltage or angle) is an important problem in the power system community. However, these problems are mostly studied using linearized models and participation factor analysis. In this paper, we provide a purely data-driven technique for small-signal stability classification (voltage or angle stability) and influence characterization for a power network. In particular, we use Koopman operator framework for data-driven discovery of the underlying power system dynamics and then leverage the newly developed concept of information transfer for discovering the causal structure. We further use it to not only identify the influential states (subspaces) in a power network, but also to clearly characterize and classify angle and voltage instabilities. We demonstrate the efficacy of the proposed framework on two different systems, namely the 3-bus system, where we reproduce the already known results regarding the types of instabilities, and the IEEE 9-bus system where we identify the influential generators and also the generator (and its states) which contribute to the system instability, thus identifying the type of instability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Information Based Data-Driven Characterization of Stability and Influence in Power Systems\", what is the primary approach used to analyze the stability and influence of a power network, and what is the key concept developed in this paper that enables the discovery of the causal structure of the power system dynamics?\n\n**A)** The authors use a linearized model to analyze the stability of the power network, and the key concept is the participation factor analysis.\n\n**B)** The authors employ a data-driven technique using the Koopman operator framework to discover the underlying power system dynamics, and the key concept is the information transfer.\n\n**C)** The authors use a hybrid approach combining linearized models and machine learning techniques to analyze the stability of the power network, and the key concept is the concept of influential states.\n\n**D)** The authors rely on traditional methods of stability analysis, such as the Routh-Hurwitz criterion, and the key concept is the identification of generator states contributing to system instability.\n\n**Correct Answer:** B) The authors employ a data-driven technique using the Koopman operator framework to discover the underlying power system dynamics, and the key concept is the information transfer.\n\n**Explanation:** The correct answer is B) because the paper states that the authors use the Koopman operator framework for data-driven discovery of the underlying power system dynamics, and then leverage the newly developed concept of information transfer to discover the causal structure of the power system. This is the primary approach used to analyze the stability and influence of a power network, as described in the paper."}, "32": {"documentation": {"title": "Transition state theory for wave packet dynamics. I. Thermal decay in\n  metastable Schr\\\"odinger systems", "source": "Andrej Junginger, J\\\"org Main, G\\\"unter Wunner, and Markus Dorwarth", "docs_id": "1202.2758", "section": ["nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transition state theory for wave packet dynamics. I. Thermal decay in\n  metastable Schr\\\"odinger systems. We demonstrate the application of transition state theory to wave packet dynamics in metastable Schr\\\"odinger systems which are approached by means of a variational ansatz for the wave function and whose dynamics is described within the framework of a time-dependent variational principle. The application of classical transition state theory, which requires knowledge of a classical Hamilton function, is made possible by mapping the variational parameters to classical phase space coordinates and constructing an appropriate Hamiltonian in action variables. This mapping, which is performed by a normal form expansion of the equations of motion and an additional adaptation to the energy functional, as well as the requirements to the variational ansatz are discussed in detail. The applicability of the procedure is demonstrated for a cubic model potential for which we calculate thermal decay rates of a frozen Gaussian wave function. The decay rate obtained with a narrow trial wave function agrees perfectly with the results using the classical normal form of the corresponding point particle. The results with a broader trial wave function go even beyond the classical approach, i.e., they agree with those using the quantum normal form. The method presented here will be applied to Bose-Einstein condensates in the following paper [A. Junginger, M. Dorwarth, J. Main, and G. Wunner, submitted to J. Phys. A]."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the key advantage of mapping the variational parameters to classical phase space coordinates in the application of transition state theory to wave packet dynamics in metastable Schr\u00f6dinger systems?\n\nA) It allows for the use of a classical Hamilton function without requiring knowledge of the wave function.\nB) It enables the construction of an appropriate Hamiltonian in action variables, which is essential for the application of classical transition state theory.\nC) It facilitates the use of a narrow trial wave function to calculate thermal decay rates.\nD) It provides a way to adapt the energy functional to the variational ansatz, which is necessary for the correct application of transition state theory.\n\nCorrect Answer: B) It enables the construction of an appropriate Hamiltonian in action variables, which is essential for the application of classical transition state theory.\n\nExplanation: The correct answer is B) because the mapping of variational parameters to classical phase space coordinates is crucial for constructing an appropriate Hamiltonian in action variables, which is a key requirement for the application of classical transition state theory. This mapping allows for the use of classical transition state theory, even though the system is described within the framework of a time-dependent variational principle. The other options are incorrect because they do not accurately describe the key advantage of this mapping. Option A is incorrect because the use of a classical Hamilton function is not the primary advantage of this mapping. Option C is incorrect because the use of a narrow trial wave function is not directly related to this mapping. Option D is incorrect because the adaptation of the energy functional is not the primary advantage of this mapping."}, "33": {"documentation": {"title": "Coverage in mmWave Cellular Networks with Base station Cooperation", "source": "Diana Maamari, Natasha Devroye, Daniela Tuninetti", "docs_id": "1503.05269", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coverage in mmWave Cellular Networks with Base station Cooperation. The presence of signal outage, due to shadowing and blockage, is expected to be the main bottleneck in millimeter wave (mmWave) networks. Moreover, with the anticipated vision that mmWave networks would have a dense deployment of base stations, interference from strong line-of-sight base stations increases too, thus further increasing the probability of outage. To address the issue of reducing outage, this paper explores the possibility of base station cooperation in the downlink of a mmWave heterogenous network. The main focus of this work is showing that, in a stochastic geometry framework, cooperation from randomly located base stations decreases outage probability. With the presumed vision that less severe fading will be experienced due to highly directional transmissions, one might expect that cooperation would increase the coverage probability; our numerical examples suggest that is in fact the case. Coverage probabilities are derived accounting for: different fading distributions, antenna directionality and blockage. Numerical results suggest that coverage with base station cooperation in dense mmWave systems and with no small scale fading considerably exceeds coverage with no cooperation. In contrast, an insignificant increase is reported when mmWave networks are less dense with a high probability of signal blockage and with Rayleigh fading."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a mmWave heterogeneous network, what is the primary benefit of base station cooperation in reducing outage probability, according to the paper \"Coverage in mmWave Cellular Networks with Base station Cooperation\"?\n\n**A)** Increased interference from strong line-of-sight base stations\n**B)** Decreased coverage probability due to highly directional transmissions\n**C)** Reduced outage probability through cooperation from randomly located base stations\n**D)** Increased signal blockage in dense mmWave systems\n\n**Correct Answer:** C) Reduced outage probability through cooperation from randomly located base stations\n\n**Explanation:** The paper suggests that cooperation from randomly located base stations decreases outage probability, which is the primary benefit of base station cooperation in reducing outage probability. This is stated in the introduction of the paper: \"To address the issue of reducing outage, this paper explores the possibility of base station cooperation in the downlink of a mmWave heterogenous network.\"\n\n**Note:** The other options are incorrect because:\n\n* A) Increased interference from strong line-of-sight base stations is actually a problem that base station cooperation aims to mitigate, not a benefit.\n* B) Decreased coverage probability due to highly directional transmissions is the opposite of what the paper suggests, which is that cooperation increases coverage probability.\n* D) Increased signal blockage in dense mmWave systems is not mentioned as a benefit of base station cooperation in the paper."}, "34": {"documentation": {"title": "Shrinkage for Categorical Regressors", "source": "Phillip Heiler, Jana Mareckova", "docs_id": "1901.01898", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shrinkage for Categorical Regressors. This paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors, (quasi-)experimental data or panel data models. The loss function is penalized by adding weighted squared l2-norm differences between group location parameters and informative first-stage estimates. Under quadratic loss, the penalized estimation problem has a simple interpretable closed-form solution that nests methods established in the literature on ridge regression, discretized support smoothing kernels and model averaging methods. We derive risk-optimal penalty parameters and propose a plug-in approach for estimation. The large sample properties are analyzed in an asymptotic local to zero framework by introducing a class of sequences for close and distant systems of locations that is sufficient for describing a large range of data generating processes. We provide the asymptotic distributions of the shrinkage estimators under different penalization schemes. The proposed plug-in estimator uniformly dominates the ordinary least squares in terms of asymptotic risk if the number of groups is larger than three. Monte Carlo simulations reveal robust improvements over standard methods in finite samples. Real data examples of estimating time trends in a panel and a difference-in-differences study illustrate potential applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a panel data model with a categorical regressor, where the group means are estimated using a quasi-experimental design. The goal is to estimate the time trend in the panel data. Which of the following regularization approaches is most suitable for reducing the point estimation risk of the group means, and why?\n\nA) Ridge regression with a fixed penalty parameter\nB) Discretized support smoothing kernel with a data-driven penalty parameter\nC) Shrinkage for categorical regressors with a plug-in approach using the sample mean as the penalty parameter\nD) Model averaging with a fixed weight matrix\n\n**Correct Answer:** C) Shrinkage for categorical regressors with a plug-in approach using the sample mean as the penalty parameter\n\n**Explanation:** The correct answer is C) Shrinkage for categorical regressors with a plug-in approach using the sample mean as the penalty parameter. This is because the paper introduces a flexible regularization approach that reduces the point estimation risk of group means, and the plug-in approach using the sample mean as the penalty parameter is a key feature of this method. The sample mean is a data-driven penalty parameter that is used to penalize the differences between group location parameters and informative first-stage estimates, which is a key aspect of the shrinkage approach.\n\nThe other options are incorrect because:\n\nA) Ridge regression with a fixed penalty parameter is a traditional regularization approach that does not take into account the specific characteristics of the categorical regressor.\n\nB) Discretized support smoothing kernel with a data-driven penalty parameter is a different regularization approach that is not specifically designed for categorical regressors.\n\nD) Model averaging with a fixed weight matrix is a method that combines multiple models, but it does not address the issue of reducing the point estimation risk of group means in the context of categorical regressors."}, "35": {"documentation": {"title": "On The Inverse Relaxation Approach To Supercapacitors Characterization", "source": "Mikhail Evgenievich Kompan and Vladislav Gennadievich Malyshkin", "docs_id": "1908.02559", "section": ["physics.app-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On The Inverse Relaxation Approach To Supercapacitors Characterization. A novel inverse relaxation technique for supercapacitor characterization is developed, modeled numerically, and experimentally tested on a number of commercial supercapacitors. It consists in shorting a supercapacitor for a short time $\\tau$, then switching to the open circuit regime and measuring an initial rebound and long-time relaxation. The results obtained are: the ratio of \"easy\" and \"hard\" to access capacitance and the dependence $C(\\tau)$, that determines what the capacitance the system responds at time-scale $\\tau$; it can be viewed as an alternative to used by some manufacturers approach to characterize a supercapacitor by fixed capacitance and time-scale dependent internal resistance. Among the advantages of proposed technique is that it does not require a source of fixed current, what simplifies the setup and allows a high discharge current regime. The approach can be used as a replacement of low-frequency impedance measurements and the ones of IEC 62391 type, it can be effectively applied to characterization of supercapacitors and other relaxation type systems with porous internal structure. The technique can be completely automated by a microcontroller to measure, analyze, and output the results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed inverse relaxation technique for supercapacitor characterization, and how does it differ from traditional methods?\n\nA) It requires a source of fixed current to measure the capacitance, which simplifies the setup and allows for high discharge current regimes. This approach can be used as a replacement for low-frequency impedance measurements and IEC 62391 type measurements, but it does not provide a complete picture of the supercapacitor's internal resistance.\n\nB) It does not require a source of fixed current, which simplifies the setup and allows for high discharge current regimes. This approach can be used as a replacement for low-frequency impedance measurements and IEC 62391 type measurements, and it provides a complete picture of the supercapacitor's internal resistance by measuring the ratio of \"easy\" and \"hard\" to access capacitance.\n\nC) It is limited to characterizing supercapacitors with a fixed capacitance and time-scale dependent internal resistance, and it does not provide a complete picture of the supercapacitor's internal resistance.\n\nD) It is a replacement for traditional methods of supercapacitor characterization, but it does not provide a complete picture of the supercapacitor's internal resistance.\n\n**Correct Answer:** B) It does not require a source of fixed current, which simplifies the setup and allows for high discharge current regimes. This approach can be used as a replacement for low-frequency impedance measurements and IEC 62391 type measurements, and it provides a complete picture of the supercapacitor's internal resistance by measuring the ratio of \"easy\" and \"hard\" to access capacitance.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the advantages of the proposed inverse relaxation technique. The technique does not require a source of fixed current, which simplifies the setup and allows for high discharge current regimes. Additionally, it provides a complete picture of the supercapacitor's internal resistance by measuring the ratio of \"easy\" and \"hard\" to access capacitance, which can be viewed as an alternative to traditional methods of characterizing supercapacitors."}, "36": {"documentation": {"title": "${\\bar D}D$ meson pair production in antiproton-nucleus collisions", "source": "R. Shyam and K. Tsushima", "docs_id": "1610.01473", "section": ["nucl-th", "hep-ex", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "${\\bar D}D$ meson pair production in antiproton-nucleus collisions. We study the $\\bar D D$ (${\\bar D}^0 D^0$ and $D^-D^+$) charm meson pair production in antiproton (${\\bar p}$) induced reactions on nuclei at beam energies ranging from threshold to several GeV. Our model is based on an effective Lagrangian approach that has only the baryon-meson degrees of freedom and involves the physical hadron masses. The reaction proceeds via the $t$-channel exchanges of $\\Lambda_c^+$, $\\Sigma_c^+$, and $\\Sigma_c^{++}$ baryons in the initial collision of the antiproton with one of the protons of the target nucleus. The medium effects on the exchanged baryons are included by incorporating in the corresponding propagators, the effective charm baryon masses calculated within a quark-meson coupling (QMC) model. The wave functions of the bound proton have been determined within the QMC model as well as in a phenomenological model where they are obtained by solving the Dirac equation with appropriate scalar and vector potentials. The initial- and final-state distortion effects have been approximated by using an eikonal approximation-based procedure. Detailed numerical results are presented for total and double differential cross sections for the ${\\bar D}^0 D^0$ and $D^-D^+$ production reactions on $^{16}$O and $^{90}$Zr targets. It is noticed that at ${\\bar p}$ beam momenta of interest to the ${\\bar P}ANDA$ experiment, medium effects lead to noticeable enhancements in the charm meson production cross sections."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which medium effects enhance the charm meson production cross sections in antiproton-nucleus collisions, according to the study on ${\\bar D}D$ meson pair production?\n\nA){The medium effects lead to an increase in the number of available baryon states, resulting in a higher probability of charm meson production.}\nB){The medium effects cause a shift in the masses of the exchanged baryons, leading to a change in the production cross sections.}\nC){The medium effects result in a modification of the wave functions of the bound protons, affecting the probability of charm meson production.}\nD){The medium effects lead to an increase in the coupling strength between the antiproton and the target nucleus, resulting in a higher probability of charm meson production.}\n\nCorrect Answer: C){The medium effects result in a modification of the wave functions of the bound protons, affecting the probability of charm meson production.}\n\nExplanation: The study mentions that the medium effects on the exchanged baryons are included by incorporating in the corresponding propagators, the effective charm baryon masses calculated within a quark-meson coupling (QMC) model. Additionally, the wave functions of the bound proton have been determined within the QMC model as well as in a phenomenological model. This suggests that the medium effects lead to a modification of the wave functions of the bound protons, which in turn affects the probability of charm meson production."}, "37": {"documentation": {"title": "Topological optimization of hybrid quantum key distribution networks", "source": "Ya-Xing Wang, Qiong Li, Hao-Kun Mao, Qi Han, Fu-Rong Huang, Hong-Wei\n  Xu", "docs_id": "2003.14100", "section": ["quant-ph", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological optimization of hybrid quantum key distribution networks. With the growing complexity of quantum key distribution (QKD) network structures, aforehand topology design is of great significance to support a large-number of nodes over a large-spatial area. However, the exclusivity of quantum channels, the limitation of key generation capabilities, the variety of QKD protocols and the necessity of untrusted-relay selection, make the optimal topology design a very complicated task. In this research, a hybrid QKD network is studied for the first time from the perspective of topology, by analyzing the topological differences of various QKD protocols. In addition, to make full use of hybrid networking, an analytical model for optimal topology calculation is proposed, to reach the goal of best secure communication service by optimizing the deployment of various QKD devices and the selection of untrusted-relays under a given cost limit. Plentiful simulation results show that hybrid networking and untrusted-relay selection can bring great performance advantages, and then the universality and effectiveness of the proposed analytical model are verified."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in designing an optimal topology for a hybrid quantum key distribution (QKD) network, and how does the proposed analytical model address this challenge?\n\nA) The primary challenge is the limited range of quantum channels, and the proposed model optimizes the deployment of QKD devices to maximize the network's range.\nB) The primary challenge is the exclusivity of quantum channels, the limitation of key generation capabilities, and the variety of QKD protocols, and the proposed model analyzes the topological differences of various QKD protocols to optimize the network's performance.\nC) The primary challenge is the necessity of untrusted-relay selection, and the proposed model proposes an analytical model for optimal topology calculation to select the most effective untrusted-relays.\nD) The primary challenge is the cost limit, and the proposed model optimizes the deployment of QKD devices and the selection of untrusted-relays under a given cost limit.\n\n**Correct Answer:** B) The primary challenge is the exclusivity of quantum channels, the limitation of key generation capabilities, and the variety of QKD protocols, and the proposed model analyzes the topological differences of various QKD protocols to optimize the network's performance.\n\n**Explanation:** The question requires the test-taker to understand the challenges in designing an optimal topology for a hybrid QKD network, as mentioned in the documentation. The correct answer, B, highlights the primary challenge as the exclusivity of quantum channels, the limitation of key generation capabilities, and the variety of QKD protocols. The proposed model addresses this challenge by analyzing the topological differences of various QKD protocols to optimize the network's performance. The other options are incorrect because they either focus on a single aspect of the challenge (A and C) or a secondary aspect (D)."}, "38": {"documentation": {"title": "Analysis of Noisy Evolutionary Optimization When Sampling Fails", "source": "Chao Qian, Chao Bian, Yang Yu, Ke Tang, Xin Yao", "docs_id": "1810.05045", "section": ["cs.NE", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of Noisy Evolutionary Optimization When Sampling Fails. In noisy evolutionary optimization, sampling is a common strategy to deal with noise. By the sampling strategy, the fitness of a solution is evaluated multiple times (called \\emph{sample size}) independently, and its true fitness is then approximated by the average of these evaluations. Previous studies on sampling are mainly empirical. In this paper, we first investigate the effect of sample size from a theoretical perspective. By analyzing the (1+1)-EA on the noisy LeadingOnes problem, we show that as the sample size increases, the running time can reduce from exponential to polynomial, but then return to exponential. This suggests that a proper sample size is crucial in practice. Then, we investigate what strategies can work when sampling with any fixed sample size fails. By two illustrative examples, we prove that using parent or offspring populations can be better. Finally, we construct an artificial noisy example to show that when using neither sampling nor populations is effective, adaptive sampling (i.e., sampling with an adaptive sample size) can work. This, for the first time, provides a theoretical support for the use of adaptive sampling."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of noisy evolutionary optimization, what is the theoretical implication of increasing the sample size in the (1+1)-EA on the noisy LeadingOnes problem?\n\n**A)** As the sample size increases, the running time of the algorithm remains constant.\n**B)** As the sample size increases, the running time of the algorithm decreases from exponential to polynomial, but then returns to exponential.\n**C)** As the sample size increases, the running time of the algorithm increases exponentially.\n**D)** As the sample size increases, the running time of the algorithm remains polynomial.\n\n**Correct Answer:** B) As the sample size increases, the running time of the algorithm decreases from exponential to polynomial, but then returns to exponential.\n\n**Explanation:** According to the paper, increasing the sample size in the (1+1)-EA on the noisy LeadingOnes problem leads to a decrease in running time from exponential to polynomial, but then the running time returns to exponential. This suggests that a proper sample size is crucial in practice to achieve efficient optimization."}, "39": {"documentation": {"title": "Entanglement generation in periodically driven integrable systems:\n  dynamical phase transitions and steady state", "source": "Arnab Sen, Sourav Nandy, and K. Sengupta", "docs_id": "1511.03668", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entanglement generation in periodically driven integrable systems:\n  dynamical phase transitions and steady state. We study a class of periodically driven $d-$dimensional integrable models and show that after $n$ drive cycles with frequency $\\omega$, pure states with non-area-law entanglement entropy $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ are generated, where $l$ is the linear dimension of the subsystem, and $d-1 \\le \\alpha(n,\\omega) \\le d$. We identify and analyze the crossover phenomenon from an area ($S \\sim l^{ d-1}$ for $d\\geq1$) to a volume ($S \\sim l^{d}$) law and provide a criterion for their occurrence which constitutes a generalization of Hastings' theorem to driven integrable systems in one dimension. We also find that $S_n$ generically decays to $S_{\\infty}$ as $(\\omega/n)^{(d+2)/2}$ for fast and $(\\omega/n)^{d/2}$ for slow periodic drives; these two dynamical phases are separated by a topological transition in the eigensprectrum of the Floquet Hamiltonian. This dynamical transition manifests itself in the temporal behavior of all local correlation functions and does not require a critical point crossing during the drive. We find that these dynamical phases show a rich re-entrant behavior as a function of $\\omega$ for $d=1$ models, and also discuss the dynamical transition for $d>1$ models. Finally, we study entanglement properties of the steady state and show that singular features (cusps and kinks in $d=1$) appear in $S_{\\infty}$ as a function of $\\omega$ whenever there is a crossing of the Floquet bands. We discuss experiments which can test our theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of periodically driven integrable systems, what is the general form of the entanglement entropy $S_n(l)$ for a subsystem of linear dimension $l$ after $n$ drive cycles with frequency $\\omega$, and what are the conditions under which the area law ($S \\sim l^{d-1}$) and volume law ($S \\sim l^{d}$) are observed?\n\nA) $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, with area law for $d \\geq 1$ and volume law for $d < 1$.\nB) $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, with area law for $d \\geq 1$ and volume law for $d \\geq 1$.\nC) $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, with area law for $d < 1$ and volume law for $d \\geq 1$.\nD) $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, with area law for $d \\geq 1$ and volume law for $d \\geq 1$, and the transition between the two laws occurs at $\\omega = n$.\n\nCorrect Answer: D) $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, with area law for $d \\geq 1$ and volume law for $d \\geq 1$, and the transition between the two laws occurs at $\\omega = n$.\n\nExplanation: The correct answer is D) because the documentation states that the entanglement entropy $S_n(l)$ has a general form of $S_n(l) \\sim l^{\\alpha(n,\\omega)}$ for $d-1 \\le \\alpha(n,\\omega) \\le d$, and that the area law ($S \\sim l^{d-1}$) and volume law ($S \\sim l^{d}$) are observed for $d \\geq 1$. Additionally, the documentation mentions that the transition between the two laws occurs at $\\omega = n$, which is consistent with option D."}, "40": {"documentation": {"title": "A Putative Early-Type Host Galaxy for GRB 060502B: Implications for the\n  Progenitors of Short-Duration Hard-Spectrum Bursts", "source": "J. S. Bloom, D. A. Perley (UC Berkeley), H.-W. Chen (U Chicago), N.\n  Butler (UC Berkeley), J. X. Prochaska (UCO Lick), D. Kocevski (UC Berkeley),\n  C. H. Blake, A. Szentgyorgyi, E. E. Falco (Harvard/CfA)", "docs_id": "astro-ph/0607223", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Putative Early-Type Host Galaxy for GRB 060502B: Implications for the\n  Progenitors of Short-Duration Hard-Spectrum Bursts. Starting with the first detection of an afterglow from a short-duration hard-spectrum gamma-ray burst (SHB) by Swift last year, a growing body of evidence has suggested that SHBs are associated with an older and lower-redshift galactic population than long-soft GRBs and, in a few cases, with large (>~ 10 kpc) projected offsets from the centers of their putative host galaxies. Here we present observations of the field of GRB 060502B, a SHB detected by Swift and localized by the X-ray Telescope (XRT). We find a massive red galaxy at a redshift of z=0.287 at an angular distance of 17.1 arcsec from our revised XRT position. Using associative and probabilistic arguments we suggest that this galaxy hosted the progenitor of GRB 060502B. If true, this offset would correspond to a physical displacement of 73 +- 19 kpc in projection, about twice the largest offset inferred for any SHB to date and almost an order of magnitude larger than a typical long-soft burst offset. Spectra and modeling of the star-formation history of this possible host show it to have undergone a large ancient starburst. If the progenitor of GRB 060502B was formed in this starburst episode, the time of the GRB explosion since birth is tau = 1.3 +- 0.2 Gyr and the minimum kick velocity of the SHB progenitor is v_kick = 55 +- 15 km/s."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the possible host galaxy of GRB 060502B, and what implications does this have for our understanding of the progenitors of short-duration hard-spectrum bursts?\n\nA) The galaxy is likely to be a low-mass, low-redshift galaxy, which would contradict the observed association of SHBs with an older and lower-redshift galactic population.\n\nB) The galaxy's star-formation history suggests that it underwent a large ancient starburst, which would provide a possible explanation for the large kick velocity of the SHB progenitor.\n\nC) The offset of the galaxy from the center of its host galaxy is likely to be due to a large-scale structure in the universe, rather than a physical displacement of the galaxy itself.\n\nD) The galaxy's redshift of z=0.287 implies that it is a distant galaxy, which would make it unlikely to be the host galaxy of a GRB that occurred in the local universe.\n\nCorrect Answer: B) The galaxy's star-formation history suggests that it underwent a large ancient starburst, which would provide a possible explanation for the large kick velocity of the SHB progenitor.\n\nExplanation: The correct answer is B because the text states that the galaxy's star-formation history shows it to have undergone a large ancient starburst, which would provide a possible explanation for the large kick velocity of the SHB progenitor. This is a key finding of the study, and it has implications for our understanding of the progenitors of short-duration hard-spectrum bursts. The other options are incorrect because they do not accurately reflect the information presented in the text. Option A is incorrect because the text states that the galaxy is associated with an older and lower-redshift galactic population, not a low-mass, low-redshift galaxy. Option C is incorrect because the text does not suggest that the offset is due to a large-scale structure in the universe. Option D is incorrect because the text states that the galaxy is at a redshift of z=0.287, which implies that it is not a distant galaxy, but rather a galaxy in the local universe."}, "41": {"documentation": {"title": "The Sloan Lens ACS Survey. III - The Structure and Formation of\n  Early-type Galaxies and their Evolution since z~1", "source": "L.V.E. Koopmans (Kapteyn Astronomical Institute), T.Treu (UCSB), A. S.\n  Bolton (CfA), S. Burles (MIT), L. A. Moustakas (JPL)", "docs_id": "astro-ph/0601628", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Sloan Lens ACS Survey. III - The Structure and Formation of\n  Early-type Galaxies and their Evolution since z~1. (Abridged) We present a joint gravitational lensing and stellar dynamical analysis of fifteen massive field early-type galaxies, selected from the Sloan Lens (SLACS) Survey. The following numerical results are found: (i) A joint-likelihood gives an average logarithmic density slope for the total mass density of 2.01 (+0.02/-0.03) (68 perecnt C.L). inside the Einstein radius. (ii) The average position-angle difference between the light distribution and the total mass distribution is found to be 0+-3 degrees, setting an upper limit of <= 0.035 on the average external shear. (iii) The average projected dark-matter mass fraction is inferred to be 0.25+-0.06 inside R_E, using the stellar mass-to-light ratios derived from the Fundamental Plane as priors. (iv) Combined with results from the LSD Survey, we find no significant evolution of the total density slope inside one effective radius: a linear fit gives d\\gamma'/dz = 0.23+-0.16 (1-sigma) for the range z=0.08-1.01. The small scatter and absence of significant evolution in the inner density slopes suggest a collisional scenario where gas and dark matter strongly couple during galaxy formation, leading to a total mass distribution that rapidly converge to dynamical isothermality."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the evolution of the total density slope inside one effective radius of early-type galaxies, based on the Sloan Lens ACS Survey results?\n\nA) The total density slope evolves significantly with redshift, indicating a major role for dark matter in galaxy formation.\nB) The total density slope remains constant with redshift, suggesting that early-type galaxies have always had a similar mass distribution.\nC) The total density slope shows a small scatter, but no significant evolution with redshift, indicating a collisional scenario where gas and dark matter strongly couple during galaxy formation.\nD) The total density slope evolves linearly with redshift, indicating a gradual increase in the importance of dark matter in galaxy formation.\n\nCorrect Answer: C) The total density slope shows a small scatter, but no significant evolution with redshift, indicating a collisional scenario where gas and dark matter strongly couple during galaxy formation.\n\nExplanation: The correct answer is based on the result (iv) from the Arxiv documentation, which states that a linear fit to the data gives d\u03b3'/dz = 0.23 \u00b1 0.16 (1-\u03c3) for the range z = 0.08-1.01. This indicates that the total density slope shows a small scatter, but no significant evolution with redshift, suggesting a collisional scenario where gas and dark matter strongly couple during galaxy formation."}, "42": {"documentation": {"title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays", "source": "Datong P. Zhou, Claire J. Tomlin", "docs_id": "1711.05928", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Budget-Constrained Multi-Armed Bandits with Multiple Plays. We study the multi-armed bandit problem with multiple plays and a budget constraint for both the stochastic and the adversarial setting. At each round, exactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$). In addition to observing the individual rewards for each arm played, the player also learns a vector of costs which has to be covered with an a-priori defined budget $B$. The game ends when the sum of current costs associated with the played arms exceeds the remaining budget. Firstly, we analyze this setting for the stochastic case, for which we assume each arm to have an underlying cost and reward distribution with support $[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound (UCB) algorithm which achieves $O(NK^4 \\log B)$ regret. Secondly, for the adversarial case in which the entire sequence of rewards and costs is fixed in advance, we derive an upper bound on the regret of order $O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known $\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high probability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Consider a multi-armed bandit problem with multiple plays and a budget constraint. The player has a budget of $B$ and can play at most $K$ arms at each round, where $1\\leq K \\leq N$. The rewards for each arm are drawn from a distribution with support $[0, 1]$, and the costs for each arm are drawn from a distribution with support $[c_{\\min}, 1]$. The game ends when the sum of current costs exceeds the remaining budget.\n\n**A)** The optimal strategy for this problem is to play the arm with the highest expected reward at each round, regardless of the costs.\n\n**B)** The optimal strategy for this problem is to play the arms with the lowest costs at each round, regardless of the rewards.\n\n**C)** The optimal strategy for this problem is to use an Upper Confidence Bound (UCB) algorithm, which balances the trade-off between exploring new arms and exploiting known arms.\n\n**D)** The optimal strategy for this problem is to use an extension of the $\\texttt{Exp3}$ algorithm, which is known to achieve a regret bound of $O(\\sqrt{NB\\log(N/K)})$ in the adversarial setting.\n\n**Correct Answer:** C) The optimal strategy for this problem is to use an Upper Confidence Bound (UCB) algorithm, which balances the trade-off between exploring new arms and exploiting known arms.\n\n**Explanation:** The correct answer is C) because the UCB algorithm is a well-known approach for multi-armed bandit problems with multiple plays and a budget constraint. The algorithm balances the trade-off between exploring new arms and exploiting known arms, which is optimal in this setting. The other options are incorrect because they do not take into account the budget constraint and the multiple plays.\n\n**Explanation of incorrect options:**\n\n* A) This option is incorrect because playing the arm with the highest expected reward at each round does not take into account the costs, which can lead to exceeding the budget.\n* B) This option is incorrect because playing the arms with the lowest costs at each round does not take into account the rewards, which can lead to missing out on high-reward arms.\n* D) This option is incorrect because while the $\\texttt{Exp3}$ algorithm is known to achieve a regret bound in the adversarial setting, it is not the optimal strategy for this specific problem with a budget constraint and multiple plays."}, "43": {"documentation": {"title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks", "source": "Sangeeta Srivastava, Yun Wang, Andros Tjandra, Anurag Kumar, Chunxi\n  Liu, Kritika Singh, Yatharth Saraf", "docs_id": "2110.07313", "section": ["cs.SD", "cs.LG", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks. Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of combining the wav2vec 2.0 framework with conformer architectures in the proposed self-supervised audio representation learning method?\n\nA) The conformer architectures can handle large amounts of unlabeled data more efficiently than wav2vec 2.0.\nB) The conformer architectures can reduce the need for labeled data by two-thirds, as mentioned in the paper.\nC) The conformer architectures can improve the performance of downstream tasks by reducing overfitting.\nD) The conformer architectures can handle non-speech audio tasks more effectively than wav2vec 2.0.\n\nCorrect Answer: B) The conformer architectures can reduce the need for labeled data by two-thirds, as mentioned in the paper.\n\nExplanation: The question requires the test-taker to understand the main contribution of the proposed method, which is the reduction of the need for labeled data by two-thirds. The correct answer is supported by the text, which states that \"Our self-supervised pre-training can reduce the need for labeled data by two-thirds.\" The other options are incorrect because they do not accurately reflect the main advantage of the proposed method. Option A is incorrect because the text does not mention the conformer architectures' ability to handle large amounts of unlabeled data. Option C is incorrect because the text does not mention the conformer architectures' ability to reduce overfitting. Option D is incorrect because the text does not mention the conformer architectures' ability to handle non-speech audio tasks more effectively than wav2vec 2.0."}, "44": {"documentation": {"title": "Self-organized quantization and oscillations on continuous fixed-energy\n  sandpiles", "source": "Jakob Niehues (1), Gorm Gruner Jensen (1) and Jan O. Haerter (1, 2, 3)\n  ((1) Niels Bohr Institute, (2) Leibniz Centre for Tropical Marine Research,\n  (3) Jacobs University Bremen)", "docs_id": "2111.04470", "section": ["cond-mat.stat-mech", "nlin.AO", "nlin.PS", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Self-organized quantization and oscillations on continuous fixed-energy\n  sandpiles. Atmospheric self-organization and activator-inhibitor dynamics in biology provide examples of checkerboard-like spatio-temporal organization. We study a simple model for local activation-inhibition processes. Our model, first introduced in the context of atmospheric moisture dynamics, is a continuous-energy and non-Abelian version of the fixed-energy sandpile model. Each lattice site is populated by a non-negative real number, its energy. Upon each timestep all sites with energy exceeding a unit threshold re-distribute their energy at equal parts to their nearest neighbors. The limit cycle dynamics gives rise to a complex phase diagram in dependence on the mean energy $\\mu$: For low $\\mu$, all dynamics ceases after few re-distribution events. For large $\\mu$, the dynamics is well-described as a diffusion process, where the order parameter, spatial variance $\\sigma$, is removed. States at intermediate $\\mu$ are dominated by checkerboard-like period-two phases which are however interspersed by much more complex phases of far longer periods. Phases are separated by discontinuous jumps in $\\sigma$ or $\\partial_{\\mu}\\sigma$ - akin to first and higher-order phase transitions. Overall, the energy landscape is dominated by few energy levels which occur as sharp spikes in the single-site density of states and are robust to noise."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the phase diagram of the continuous-energy and non-Abelian fixed-energy sandpile model. How do the dynamics change as the mean energy \u03bc increases, and what are the characteristics of the phases that emerge at intermediate \u03bc?\n\n**A)** The dynamics ceases after a few re-distribution events for all values of \u03bc, with no phase transitions occurring.\n\n**B)** For low \u03bc, the dynamics is well-described as a diffusion process, where the order parameter, spatial variance \u03c3, is removed, and for large \u03bc, the dynamics is chaotic and unpredictable.\n\n**C)** The phase diagram is dominated by few energy levels that occur as sharp spikes in the single-site density of states, and these energy levels are robust to noise, but the dynamics is well-described as a diffusion process for low \u03bc and chaotic for large \u03bc.\n\n**D)** The phase diagram exhibits a complex interplay between checkerboard-like period-two phases and more complex phases of far longer periods, with discontinuous jumps in \u03c3 or \u2202\u03bc\u03c3 separating these phases.\n\n**Correct Answer:** D) The phase diagram exhibits a complex interplay between checkerboard-like period-two phases and more complex phases of far longer periods, with discontinuous jumps in \u03c3 or \u2202\u03bc\u03c3 separating these phases.\n\n**Explanation:** The correct answer is based on the text, which states that for intermediate \u03bc, the dynamics is dominated by checkerboard-like period-two phases, but these phases are interspersed by more complex phases of far longer periods. Additionally, the text mentions that phases are separated by discontinuous jumps in \u03c3 or \u2202\u03bc\u03c3, which is consistent with option D. Options A and B are incorrect because they oversimplify the dynamics and do not capture the complex phase transitions described in the text. Option C is close, but the text does not mention that the energy levels are robust to noise, and the dynamics is not well-described as a diffusion process for all values of \u03bc."}, "45": {"documentation": {"title": "Entropy production from chaoticity in Yang-Mills field theory with use\n  of the Husimi function", "source": "Hidekazu Tsukiji, Hideaki Iida, Teiji Kunihiro, Akira Ohnishi and Toru\n  T. Takahashi", "docs_id": "1603.04622", "section": ["hep-ph", "cond-mat.stat-mech", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entropy production from chaoticity in Yang-Mills field theory with use\n  of the Husimi function. We investigate possible entropy production in Yang-Mills (YM) field theory by using a quantum distribution function called Husimi function $f_{\\rm H}(A, E, t)$ for YM field, which is given by a coarse graining of Wigner function and non-negative. We calculate the Husimi-Wehrl (HW) entropy $S_{\\rm HW}(t)=-{\\rm Tr}f_H \\log f_H$ defined as an integral over the phase-space, for which two adaptations of the test-particle method are used combined with Monte-Carlo method. We utilize the semiclassical approximation to obtain the time evolution of the distribution functions of the YM field, which is known to show a chaotic behavior in the classical limit. We also make a simplification of the multi-dimensional phase-space integrals by making a product ansatz for the Husimi function, which is found to give a 10-20 per cent over estimate of the HW entropy for a quantum system with a few degrees of freedom. We show that the quantum YM theory does exhibit the entropy production, and that the entropy production rate agrees with the sum of positive Lyapunov exponents or the Kolmogorov-Sinai entropy, suggesting that the chaoticity of the classical YM field causes the entropy production in the quantum YM theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Yang-Mills field theory, what is the relationship between the Husimi-Wehrl (HW) entropy and the Lyapunov exponents, and how does the chaoticity of the classical YM field affect the entropy production in the quantum YM theory?\n\nA) The HW entropy is directly proportional to the sum of positive Lyapunov exponents, and the chaoticity of the classical YM field causes a significant increase in entropy production.\n\nB) The HW entropy is inversely proportional to the sum of positive Lyapunov exponents, and the chaoticity of the classical YM field leads to a decrease in entropy production.\n\nC) The HW entropy is equal to the Kolmogorov-Sinai entropy, and the chaoticity of the classical YM field causes the HW entropy to increase at a rate consistent with the sum of positive Lyapunov exponents.\n\nD) The HW entropy is independent of the Lyapunov exponents, and the chaoticity of the classical YM field has no effect on entropy production in the quantum YM theory.\n\nCorrect Answer: C) The HW entropy is equal to the Kolmogorov-Sinai entropy, and the chaoticity of the classical YM field causes the HW entropy to increase at a rate consistent with the sum of positive Lyapunov exponents.\n\nExplanation: The correct answer is based on the statement in the documentation that \"the entropy production rate agrees with the sum of positive Lyapunov exponents or the Kolmogorov-Sinai entropy\". This implies that the HW entropy is equal to the Kolmogorov-Sinai entropy, and the chaoticity of the classical YM field causes the HW entropy to increase at a rate consistent with the sum of positive Lyapunov exponents."}, "46": {"documentation": {"title": "Environment Assisted Quantum Transport in Organic Molecules", "source": "Gabor Vattay and Istvan Csabai", "docs_id": "1503.00178", "section": ["cond-mat.mes-hall", "physics.chem-ph", "q-bio.BM", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Environment Assisted Quantum Transport in Organic Molecules. One of the new discoveries in quantum biology is the role of Environment Assisted Quantum Transport (ENAQT) in excitonic transport processes. In disordered quantum systems transport is most efficient when the environment just destroys quantum interferences responsible for localization, but the coupling does not drive the system to fully classical thermal diffusion yet. This poised realm between the pure quantum and the semi-classical domains has not been considered in other biological transport processes, such as charge transport through organic molecules. Binding in receptor-ligand complexes is assumed to be static as electrons are assumed to be not able to cross the ligand molecule. We show that ENAQT makes cross ligand transport possible and efficient between certain atoms opening the way for the reorganization of the charge distribution on the receptor when the ligand molecule docks. This new effect can potentially change our understanding how receptors work. We demonstrate room temperature ENAQT on the caffeine molecule."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary role of Environment Assisted Quantum Transport (ENAQT) in excitonic transport processes, and how does it differ from classical thermal diffusion?\n\nA) ENAQT enhances quantum interferences responsible for localization, leading to more efficient transport in disordered quantum systems.\nB) ENAQT destroys quantum interferences responsible for localization, allowing for efficient transport in disordered quantum systems without driving the system to fully classical thermal diffusion.\nC) ENAQT is a mechanism by which electrons can cross the ligand molecule, enabling the reorganization of charge distribution on the receptor during ligand binding.\nD) ENAQT is a phenomenon that only occurs at low temperatures and is not relevant to biological transport processes.\n\nCorrect Answer: B) ENAQT destroys quantum interferences responsible for localization, allowing for efficient transport in disordered quantum systems without driving the system to fully classical thermal diffusion.\n\nExplanation: The correct answer is based on the text, which states that \"transport is most efficient when the environment just destroys quantum interferences responsible for localization, but the coupling does not drive the system to fully classical thermal diffusion yet.\" This indicates that ENAQT plays a crucial role in destroying quantum interferences, allowing for efficient transport in disordered quantum systems without driving the system to fully classical thermal diffusion."}, "47": {"documentation": {"title": "Approaching the Transient Stability Boundary of a Power System: Theory\n  and Applications", "source": "Peng Yang, Feng Liu, Wei Wei, Zhaojian Wang", "docs_id": "2109.12514", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approaching the Transient Stability Boundary of a Power System: Theory\n  and Applications. Estimating the stability boundary is a fundamental and challenging problem in transient stability studies. It is known that a proper level set of a Lyapunov function or an energy function can provide an inner approximation of the stability boundary, and the estimation can be expanded by trajectory reversing methods. In this paper, we streamline the theoretical foundation of the expansion methodology, and generalize it by relaxing the request that the initial guess should be a subset of the stability region. We investigate topological characteristics of the expanded boundary, showing how an initial guess can approach the exact stability boundary locally or globally. We apply the theory to transient stability assessment, and propose expansion algorithms to improve the well-known Potential Energy Boundary Surface (PEBS) and Boundary of stability region based Controlling Unstable equilibrium point (BCU) methods. Case studies on the IEEE 39-bus system well verify our results and demonstrate that estimations of the stability boundary and the critical clearing time can be significantly improved with modest computational cost."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the expansion methodology proposed in the paper \"Approaching the Transient Stability Boundary of a Power System: Theory and Applications\"?\n\nA) To improve the accuracy of the Potential Energy Boundary Surface (PEBS) method\nB) To develop a new algorithm for estimating the stability boundary of a power system\nC) To relax the requirement that the initial guess should be a subset of the stability region\nD) To investigate the topological characteristics of the expanded boundary\n\n**Correct Answer:** C) To relax the requirement that the initial guess should be a subset of the stability region\n\n**Explanation:** The paper proposes a new expansion methodology that generalizes the traditional approach by relaxing the requirement that the initial guess should be a subset of the stability region. This allows for a more flexible and accurate estimation of the stability boundary. The correct answer, C, highlights this key innovation.\n\n**Candidate A (PEBS method)**: While the paper does apply the expansion methodology to the PEBS method, this is not the primary goal of the expansion methodology itself. The PEBS method is a specific application of the expansion methodology, not the goal.\n\n**Candidate B (new algorithm)**: The paper does propose new expansion algorithms, but this is not the primary goal of the expansion methodology. The expansion methodology is a theoretical framework that can be applied to various methods, including the PEBS method.\n\n**Candidate D (topological characteristics)**: While the paper does investigate the topological characteristics of the expanded boundary, this is not the primary goal of the expansion methodology. The expansion methodology is primarily focused on relaxing the requirement for the initial guess, not on investigating the topological characteristics of the boundary."}, "48": {"documentation": {"title": "Identifying Network Ties from Panel Data: Theory and an Application to\n  Tax Competition", "source": "Aureo de Paula, Imran Rasul, Pedro Souza", "docs_id": "1910.07452", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identifying Network Ties from Panel Data: Theory and an Application to\n  Tax Competition. Social interactions determine many economic behaviors, but information on social ties does not exist in most publicly available and widely used datasets. We present results on the identification of social networks from observational panel data that contains no information on social ties between agents. In the context of a canonical social interactions model, we provide sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified. While this result is relevant across different estimation strategies, we then describe how high-dimensional estimation techniques can be used to estimate the interactions model based on the Adaptive Elastic Net GMM method. We employ the method to study tax competition across US states. We find the identified social interactions matrix implies tax competition differs markedly from the common assumption of competition between geographically neighboring states, providing further insights for the long-standing debate on the relative roles of factor mobility and yardstick competition in driving tax setting behavior across states. Most broadly, our identification and application show the analysis of social interactions can be extended to economic realms where no network data exists."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition\", what is the primary assumption that the authors aim to relax in order to identify social interactions from observational panel data?\n\nA) The assumption of perfect information on social ties\nB) The assumption of no social interactions between agents\nC) The assumption of global identification of social interactions parameters\nD) The assumption of no endogenous social effect parameters\n\n**Correct Answer:** C) The assumption of global identification of social interactions parameters\n\n**Explanation:** The authors aim to identify social interactions from observational panel data that contains no information on social ties between agents. To achieve this, they need to relax the assumption of global identification of social interactions parameters, which is a key assumption in the canonical social interactions model. By providing sufficient conditions under which the social interactions matrix, endogenous and exogenous social effect parameters are all globally identified, the authors can estimate the interactions model from the panel data.\n\n**Candidate A:** This option is incorrect because the authors do not aim to relax the assumption of perfect information on social ties. In fact, the paper highlights the lack of information on social ties in the panel data.\n\n**Candidate B:** This option is incorrect because the authors aim to identify social interactions from the panel data, not relax the assumption of no social interactions between agents.\n\n**Candidate C:** This option is correct because the authors aim to relax the assumption of global identification of social interactions parameters in order to identify social interactions from observational panel data.\n\n**Candidate D:** This option is incorrect because the authors do not aim to relax the assumption of no endogenous social effect parameters. In fact, the paper highlights the importance of identifying endogenous social effect parameters in the context of social interactions."}, "49": {"documentation": {"title": "Higher Grading Conformal Affine Toda Teory and (Generalized)\n  Sine-Gordon/Massive Thirring Duality", "source": "Harold Blas", "docs_id": "hep-th/0306171", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Higher Grading Conformal Affine Toda Teory and (Generalized)\n  Sine-Gordon/Massive Thirring Duality. Some properties of the higher grading integrable generalizations of the conformal affine Toda systems are studied. The fields associated to the non-zero grade generators are Dirac spinors. The effective action is written in terms of the Wess-Zumino-Novikov-Witten (WZNW) action associated to an affine Lie algebra, and an off-critical theory is obtained as the result of the spontaneous breakdown of the conformal symmetry. Moreover, the off-critical theory presents a remarkable equivalence between the Noether and topological currents of the model. Related to the off-critical model we define a real and local Lagrangian provided some reality conditions are imposed on the fields of the model. This real action model is expected to describe the soliton sector of the original model, and turns out to be the master action from which we uncover the weak-strong phases described by (generalized) massive Thirring and sine-Gordon type models, respectively. The case of any (untwisted) affine Lie algebra furnished with the principal gradation is studied in some detail. The example of $\\hat{sl}(n) (n=2,3)$ is presented explicitly."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the higher grading conformal affine Toda theory and its generalization to the sine-Gordon/Massive Thirring duality. What is the relationship between the Noether currents and the topological currents in the off-critical theory, and how does this relationship manifest in the real and local Lagrangian formulation of the model?\n\nA) The Noether currents are proportional to the topological currents, but with a phase shift of \u03c0/2.\n\nB) The Noether currents are equal to the topological currents, but with a factor of 2.\n\nC) The Noether currents are orthogonal to the topological currents, and the Lagrangian formulation does not capture this relationship.\n\nD) The Noether currents are related to the topological currents through a non-trivial gauge transformation, which is not captured by the Lagrangian formulation.\n\nCorrect Answer: A) The Noether currents are proportional to the topological currents, but with a phase shift of \u03c0/2.\n\nExplanation: The off-critical theory exhibits a remarkable equivalence between the Noether and topological currents, which is a consequence of the spontaneous breakdown of conformal symmetry. This equivalence is reflected in the real and local Lagrangian formulation of the model, where the Noether currents are proportional to the topological currents, but with a phase shift of \u03c0/2. This phase shift is a result of the non-trivial gauge transformation that arises from the spontaneous symmetry breaking."}, "50": {"documentation": {"title": "Night-to-Night Variability of Sleep Electroencephalography-Based Brain\n  Age Measurements", "source": "Jacob Hogan, Haoqi Sun, Luis Paixao, Mike Westmeijer, Pooja Sikka,\n  Jing Jin, Ryan Tesh, Madalena Cardoso, Sydney S. Cash, Oluwaseun Akeju,\n  Robert Thomas, M. Brandon Westover", "docs_id": "2003.01248", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Night-to-Night Variability of Sleep Electroencephalography-Based Brain\n  Age Measurements. Objective Brain Age Index (BAI), calculated from sleep electroencephalography (EEG), has been proposed as a biomarker of brain health. This study quantifies night-to-night variability of BAI and establishes probability thresholds for inferring underlying brain pathology based on a patient's BAI. Methods 86 patients with multiple nights of consecutive EEG recordings were selected from Epilepsy Monitoring Unit patients whose EEGs reported as being within normal limits. BAI was calculated for each 12-hour segment of patient data using a previously described algorithm, and night-to-night variability in BAI was measured. Results The within-patient night-to-night standard deviation in BAI was 7.5 years. Estimates of BAI derived by averaging over 2, 3, and 4 nights had standard deviations of 4.7, 3.7, and 3.0 years, respectively. Conclusions Averaging BAI over n nights reduces night-to-night variability of BAI by a factor of the square root of n, rendering BAI more suitable as a biomarker of brain health at the individual level. Significance With increasing ease of EEG acquisition including wearable technology, BAI has the potential to track brain health and detect deviations from normal physiologic function. In a clinical setting, BAI could be used to identify patients who should undergo further investigation or monitoring."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The study on Night-to-Night Variability of Sleep Electroencephalography-Based Brain Age Index (BAI) suggests that averaging BAI over multiple nights reduces night-to-night variability. However, the study also reports that the standard deviation of BAI estimates decreases as the number of nights averaged increases. What is the underlying assumption that allows the researchers to conclude that averaging BAI over n nights reduces night-to-night variability?\n\nA) The standard deviation of BAI estimates is directly proportional to the number of nights averaged.\nB) The standard deviation of BAI estimates is inversely proportional to the square root of the number of nights averaged.\nC) The standard deviation of BAI estimates is independent of the number of nights averaged.\nD) The standard deviation of BAI estimates is directly proportional to the number of nights averaged, but the effect is only significant for small values of n.\n\n**Correct Answer:** B) The standard deviation of BAI estimates is inversely proportional to the square root of the number of nights averaged.\n\n**Explanation:** The correct answer is based on the fact that the study reports that the standard deviation of BAI estimates decreases as the number of nights averaged increases. This suggests that the standard deviation is inversely proportional to the square root of the number of nights averaged, which is a common result in statistical analysis. The correct answer can be inferred by analyzing the relationship between the standard deviation and the number of nights averaged, and recognizing that the study's findings are consistent with this relationship."}, "51": {"documentation": {"title": "Nuclear-atomic state degeneracy in neutrinoless double-electron capture:\n  A unique test for a Majorana-neutrino", "source": "D. Frekers", "docs_id": "hep-ex/0506002", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nuclear-atomic state degeneracy in neutrinoless double-electron capture:\n  A unique test for a Majorana-neutrino. There is a general consensus that detection of a double-beta decay without any neutrino involved would mark physics beyond the Standard Model. This is because in such decay modes lepton number conservation would be violated and the neutrino would reveal itself as being its own antiparticle, thereby of Majorana type. So far, the experimental focus has mostly been on the double beta minus decay variant, where one attempts to measure the spectrum of the two emitted electrons. A discrete line at the endpoint energy marks the unique signature for a Majorana neutrino. Little attention has been given to alternative decay modes in double-beta decay. In this note we show that there is at least one case in the periodic table, where the parent in the neutrinoless double-electron capture process is nearly degenerate with an excited state in the daughter, leading to a possible enhancement of the decay rate by several orders of magnitude. It is the nucleus 74-Se, which has this unique property. Furthermore, there is an easy to detect 2 gamma-ray decay cascade in 74-Ge, which follows the zero-neutrino double electron capture, and which by its mere detection provides unique signature of the Majorana neutrino."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the unique signature of a Majorana neutrino that can be detected through the zero-neutrino double electron capture process in the nucleus 74-Se, and how does it relate to the decay of 74-Ge?\n\nA) A discrete line at the endpoint energy marks the unique signature for a Majorana neutrino, which can be detected through the measurement of the spectrum of the two emitted electrons in the double beta minus decay variant.\nB) A 2 gamma-ray decay cascade in 74-Ge provides a unique signature of the Majorana neutrino, which can be detected through the observation of the decay rate enhancement in the zero-neutrino double electron capture process.\nC) The decay rate enhancement in the zero-neutrino double electron capture process is a result of the parent nucleus being nearly degenerate with an excited state in the daughter, leading to a possible enhancement of the decay rate by several orders of magnitude.\nD) The detection of a discrete line at the endpoint energy marks the unique signature for a Majorana neutrino, which can be detected through the observation of the 2 gamma-ray decay cascade in 74-Ge.\n\nCorrect Answer: B) A 2 gamma-ray decay cascade in 74-Ge provides a unique signature of the Majorana neutrino, which can be detected through the observation of the decay rate enhancement in the zero-neutrino double electron capture process.\n\nExplanation: The correct answer is B) A 2 gamma-ray decay cascade in 74-Ge provides a unique signature of the Majorana neutrino, which can be detected through the observation of the decay rate enhancement in the zero-neutrino double electron capture process. This is because the decay of 74-Ge follows the zero-neutrino double electron capture process, and its detection provides a unique signature of the Majorana neutrino. The other options are incorrect because they either describe the wrong signature (A and D) or only partially describe the process (C)."}, "52": {"documentation": {"title": "A Physical Model for Self-Similar Seashells", "source": "Paul A. Reiser", "docs_id": "1904.05238", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Physical Model for Self-Similar Seashells. This paper presents a simple physical model for self-similar (gnomonic, or first-order) seashell growth which is expressed in coordinate-free terms. The shell is expressed as the solution of a differential equation which expresses the growth dynamics, and may be used to investigate shell growth from both the local viewpoint of the organism building it and moving with the shell opening (aperture), as well as that of a researcher making global measurements upon a complete motionless shell. Coordinate systems needed to express the global and local descriptions of the shell are chosen. The parameters of growth, or their information equivalent, remain constant in the local system, and are used by the organism to build the shell, and are likely mirrored in the DNA of the organism building it. The transformations between local and global representations are provided. The global model of Cortie, which is very similar to the present model, is expressed in terms of the present model, and the global parameters provided by Cortie for various species of mollusk may be used to calculate the equivalent local parameters.Mathematica code is provided to implement these transformations, as well as to plot the shells using both global and local parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question**\n\nA physical model for self-similar seashell growth is presented in the paper \"A Physical Model for Self-Similar Seashells\". The model is expressed in coordinate-free terms and describes the growth dynamics of the shell as the solution of a differential equation. The parameters of growth remain constant in the local system and are used by the organism to build the shell.\n\n**Part A**\n\nThe global model of Cortie is expressed in terms of the present model. What is the relationship between the global parameters provided by Cortie for various species of mollusk and the equivalent local parameters?\n\nA) The global parameters are directly proportional to the local parameters.\nB) The global parameters are inversely proportional to the local parameters.\nC) The global parameters are equal to the local parameters.\nD) The global parameters are a function of the local parameters.\n\n**Part B**\n\nThe transformations between local and global representations of the shell are provided. What is the purpose of these transformations?\n\nA) To calculate the global parameters from the local parameters.\nB) To calculate the local parameters from the global parameters.\nC) To plot the shells using both global and local parameters.\nD) To investigate shell growth from both the local viewpoint of the organism building it and moving with the shell opening (aperture).\n\n**Part C**\n\nThe Mathematica code provided implements the transformations between local and global representations of the shell. What is the primary function of this code?\n\nA) To plot the shells using both global and local parameters.\nB) To calculate the global parameters from the local parameters.\nC) To calculate the local parameters from the global parameters.\nD) To investigate shell growth from both the local viewpoint of the organism building it and moving with the shell opening (aperture).\n\n**Part D**\n\nThe shell is expressed as the solution of a differential equation which expresses the growth dynamics. What is the significance of this differential equation in the context of the model?\n\nA) It describes the movement of the shell opening (aperture).\nB) It describes the growth of the shell from the local viewpoint of the organism building it.\nC) It describes the growth of the shell from the global viewpoint of a researcher making measurements upon a complete motionless shell.\nD) It describes the movement of the organism building the shell.\n\n**Correct Answer**\n\nA){candidate A}\n\nExplanation:\n\nPart A: The correct answer is A) The global parameters are directly proportional to the local parameters. This is because the global model of Cortie is expressed in terms of the present model, and the parameters of growth remain constant in the local system.\n\nPart B: The correct answer is D) To investigate shell growth from both the local viewpoint of the organism building it and moving with the shell opening (aperture). This is because the transformations between local and global representations of the shell are provided to facilitate both local and global investigations of shell growth.\n\nPart C: The correct answer is C) To calculate the local parameters from the global parameters. This is because the Mathematica code provided implements the transformations between local and global representations of the shell, allowing researchers to calculate local parameters from global parameters.\n\nPart D: The correct answer is C) It describes the growth of the shell from the global viewpoint of a researcher making measurements upon a complete motionless shell. This is because the differential equation expresses the growth dynamics of the shell, and the global viewpoint of a researcher making measurements upon a complete motionless shell is one of the two viewpoints described in the model."}, "53": {"documentation": {"title": "Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation\n  of Facial Gestures in Dyadic Settings", "source": "Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow", "docs_id": "2006.09888", "section": ["cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.AS", "eess.IV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation\n  of Facial Gestures in Dyadic Settings. To enable more natural face-to-face interactions, conversational agents need to adapt their behavior to their interlocutors. One key aspect of this is generation of appropriate non-verbal behavior for the agent, for example facial gestures, here defined as facial expressions and head movements. Most existing gesture-generating systems do not utilize multi-modal cues from the interlocutor when synthesizing non-verbal behavior. Those that do, typically use deterministic methods that risk producing repetitive and non-vivid motions. In this paper, we introduce a probabilistic method to synthesize interlocutor-aware facial gestures - represented by highly expressive FLAME parameters - in dyadic conversations. Our contributions are: a) a method for feature extraction from multi-party video and speech recordings, resulting in a representation that allows for independent control and manipulation of expression and speech articulation in a 3D avatar; b) an extension to MoGlow, a recent motion-synthesis method based on normalizing flows, to also take multi-modal signals from the interlocutor as input and subsequently output interlocutor-aware facial gestures; and c) a subjective evaluation assessing the use and relative importance of the input modalities. The results show that the model successfully leverages the input from the interlocutor to generate more appropriate behavior. Videos, data, and code available at: https://jonepatr.github.io/lets_face_it."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed probabilistic method for synthesizing interlocutor-aware facial gestures in dyadic conversations, as described in the paper \"Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation of Facial Gestures in Dyadic Settings\"?\n\n**A)** The method introduces a new deterministic approach to generating facial gestures that is more vivid and expressive than existing systems.\n\n**B)** The method extends the MoGlow motion-synthesis method to incorporate multi-modal signals from the interlocutor, allowing for more natural and adaptive facial gestures.\n\n**C)** The method proposes a novel feature extraction technique that enables independent control and manipulation of expression and speech articulation in a 3D avatar.\n\n**D)** The method introduces a subjective evaluation framework to assess the relative importance of input modalities in generating interlocutor-aware facial gestures.\n\n**Correct Answer:** C) The method proposes a novel feature extraction technique that enables independent control and manipulation of expression and speech articulation in a 3D avatar.\n\n**Explanation:** The correct answer is C) because the paper describes a method for feature extraction from multi-party video and speech recordings, which enables independent control and manipulation of expression and speech articulation in a 3D avatar. This is a key contribution of the paper, as it allows for more nuanced and adaptive facial gestures in dyadic conversations. The other options are incorrect because they either describe a different aspect of the paper (A and B) or a different contribution (D)."}, "54": {"documentation": {"title": "The Correlated Electronic States of a few Polycyclic Aromatic\n  Hydrocarbons: A Computational Study", "source": "Geetanjali Giri, Anusooya Y. Pati, and S. Ramasesha", "docs_id": "1905.01839", "section": ["cond-mat.str-el", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Correlated Electronic States of a few Polycyclic Aromatic\n  Hydrocarbons: A Computational Study. In recent years Polycyclic Aromatic Hydrocarbons (PAHs) have been studied for their electronic properties as they are viewed as nanodots of graphene. They have also been of interest as functional molecules for applications such as light emitting diodes and solar cells. Since last few years varying structural and chemical properties corresponding to the size and geometry of these molecules have been studied both theoretically and experimentally. In this paper, we carry out a systematic study of the electronic states of several PAHs using the Pariser-Parr-Pople model which incorporates long-range electron correlations. In all the molecules studied by us, we find that the 2A state is below the 1B state and hence none of them will be fluorescent in the gaseous phase. The singlet-triplet gap is more than one-half of the singlet-singlet gap in all cases and hence none of these PAHs can be candidates for improved solar cell efficiencies in a singlet fission. We discuss in detail the properties of the electronic states which include bond orders and spin densities (in triplets) of these systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be concluded about the electronic states of Polycyclic Aromatic Hydrocarbons (PAHs) based on the study described in the paper?\n\nA) The 2A state is above the 1B state, making them fluorescent in the gaseous phase.\nB) The singlet-triplet gap is less than one-half of the singlet-singlet gap, making them suitable for improved solar cell efficiencies.\nC) The 2A state is below the 1B state, and the singlet-triplet gap is more than one-half of the singlet-singlet gap, making them non-fluorescent and unsuitable for solar cell efficiencies.\nD) The study only examined the electronic states of PAHs with a specific molecular structure.\n\nCorrect Answer: C) The 2A state is below the 1B state, and the singlet-triplet gap is more than one-half of the singlet-singlet gap, making them non-fluorescent and unsuitable for solar cell efficiencies.\n\nExplanation: The correct answer is based on the information provided in the paper, which states that the 2A state is below the 1B state and that the singlet-triplet gap is more than one-half of the singlet-singlet gap. This implies that the PAHs studied are non-fluorescent and unsuitable for improved solar cell efficiencies. The other options are incorrect because they contradict the information provided in the paper."}, "55": {"documentation": {"title": "InSe: a two-dimensional material with strong interlayer coupling", "source": "Yuanhui Sun, Shulin Luo, Xin-Gang Zhao, Koushik Biswas, Song-Lin Li,\n  and Lijun Zhang", "docs_id": "1803.09919", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "InSe: a two-dimensional material with strong interlayer coupling. Atomically thin, two-dimensional (2D) indium selenide (InSe) has attracted considerable attention due to large tunability in the band gap (from 1.4 to 2.6 eV) and high carrier mobility. The intriguingly high dependence of band gap on layer thickness may lead to novel device applications, although its origin remains poorly understood, and generally attributed to quantum confinement effect. In this work, we demonstrate via first-principles calculations that strong interlayer coupling may be mainly responsible for this phenomenon, especially in the fewer-layer region, and it could also be an essential factor influencing other material properties of {\\beta}-InSe and {\\gamma}-InSe. Existence of strong interlayer coupling manifests itself in three aspects: (i) indirect-to-direct band gap transitions with increasing layer thickness; (ii) fan-like frequency diagrams of the shear and breathing modes of few-layer flakes; (iii) strong layer-dependent carrier mobilities. Our results indicate that multiple-layer InSe may be deserving of attention from FET-based technologies and also an ideal system to study interlayer coupling, possibly inherent in other 2D materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor responsible for the high dependence of the band gap of InSe on layer thickness, according to the authors' first-principles calculations?\n\nA) Quantum confinement effect\nB) Strong interlayer coupling\nC) Strain-induced lattice distortions\nD) Surface defects and impurities\n\nCorrect Answer: B) Strong interlayer coupling\n\nExplanation: The authors suggest that strong interlayer coupling is the primary factor responsible for the high dependence of the band gap of InSe on layer thickness, especially in the fewer-layer region. This is supported by the existence of indirect-to-direct band gap transitions, fan-like frequency diagrams of the shear and breathing modes, and strong layer-dependent carrier mobilities, all of which are manifestations of strong interlayer coupling."}, "56": {"documentation": {"title": "Tracking Triadic Cardinality Distributions for Burst Detection in Social\n  Activity Streams", "source": "Junzhou Zhao, John C.S. Lui, Don Towsley, Pinghui Wang, Xiaohong Guan", "docs_id": "1411.3808", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tracking Triadic Cardinality Distributions for Burst Detection in Social\n  Activity Streams. In everyday life, we often observe unusually frequent interactions among people before or during important events, e.g., we receive/send more greetings from/to our friends on Christmas Day, than usual. We also observe that some videos suddenly go viral through people's sharing in online social networks (OSNs). Do these seemingly different phenomena share a common structure? All these phenomena are associated with sudden surges of user activities in networks, which we call \"bursts\" in this work. We find that the emergence of a burst is accompanied with the formation of triangles in networks. This finding motivates us to propose a new method to detect bursts in OSNs. We first introduce a new measure, \"triadic cardinality distribution\", corresponding to the fractions of nodes with different numbers of triangles, i.e., triadic cardinalities, within a network. We demonstrate that this distribution changes when a burst occurs, and is naturally immunized against spamming social-bot attacks. Hence, by tracking triadic cardinality distributions, we can reliably detect bursts in OSNs. To avoid handling massive activity data generated by OSN users, we design an efficient sample-estimate solution to estimate the triadic cardinality distribution from sampled data. Extensive experiments conducted on real data demonstrate the usefulness of this triadic cardinality distribution and the effectiveness of our sample-estimate solution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the proposed method for detecting bursts in online social networks (OSNs), and how does it relate to the emergence of triangles in networks?\n\n**A)** The method is motivated by the need to detect spamming social-bot attacks, and it uses the triadic cardinality distribution to identify unusual patterns in network activity.\n\n**B)** The method is motivated by the observation that bursts in OSNs are often accompanied by the formation of triangles in networks, and it uses the triadic cardinality distribution to track changes in this pattern.\n\n**C)** The method is motivated by the need to analyze the structure of networks, and it uses the triadic cardinality distribution to identify clusters of nodes with similar triangle counts.\n\n**D)** The method is motivated by the need to detect anomalies in network activity, and it uses the triadic cardinality distribution to identify unusual patterns in the distribution of triangles.\n\n**Correct Answer:** B) The method is motivated by the observation that bursts in OSNs are often accompanied by the formation of triangles in networks, and it uses the triadic cardinality distribution to track changes in this pattern.\n\n**Explanation:** The correct answer is B) because the documentation states that the emergence of a burst is accompanied by the formation of triangles in networks, and this finding motivates the proposal of a new method to detect bursts in OSNs. The method uses the triadic cardinality distribution to track changes in this pattern, which is a key aspect of the proposed approach. The other options are incorrect because they do not accurately reflect the motivation behind the proposed method or the role of the triadic cardinality distribution in detecting bursts."}, "57": {"documentation": {"title": "Modelling and Analysis of Biochemical Signalling Pathway Cross-talk", "source": "Robin Donaldson (University of Glasgow), Muffy Calder (University of\n  Glasgow)", "docs_id": "1002.4062", "section": ["cs.CE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modelling and Analysis of Biochemical Signalling Pathway Cross-talk. Signalling pathways are abstractions that help life scientists structure the coordination of cellular activity. Cross-talk between pathways accounts for many of the complex behaviours exhibited by signalling pathways and is often critical in producing the correct signal-response relationship. Formal models of signalling pathways and cross-talk in particular can aid understanding and drive experimentation. We define an approach to modelling based on the concept that a pathway is the (synchronising) parallel composition of instances of generic modules (with internal and external labels). Pathways are then composed by (synchronising) parallel composition and renaming; different types of cross-talk result from different combinations of synchronisation and renaming. We define a number of generic modules in PRISM and five types of cross-talk: signal flow, substrate availability, receptor function, gene expression and intracellular communication. We show that Continuous Stochastic Logic properties can both detect and distinguish the types of cross-talk. The approach is illustrated with small examples and an analysis of the cross-talk between the TGF-b/BMP, WNT and MAPK pathways."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of modelling and analysis of biochemical signalling pathway cross-talk, what is the primary benefit of using Continuous Stochastic Logic properties to detect and distinguish the types of cross-talk between signalling pathways?\n\n**A)** Continuous Stochastic Logic properties provide a more accurate representation of the complex behaviours exhibited by signalling pathways, allowing for a better understanding of the underlying mechanisms.\n\n**B)** The use of Continuous Stochastic Logic properties enables the identification of specific types of cross-talk, such as signal flow and substrate availability, which are critical in producing the correct signal-response relationship.\n\n**C)** Continuous Stochastic Logic properties provide a more efficient method for modelling and analysing signalling pathways, allowing for faster and more accurate predictions of the outcomes of different cross-talk scenarios.\n\n**D)** The application of Continuous Stochastic Logic properties to signalling pathways enables the identification of potential bottlenecks and areas of cross-talk that may be critical in understanding the complex behaviours exhibited by these pathways.\n\n**Correct Answer:** B) The use of Continuous Stochastic Logic properties enables the identification of specific types of cross-talk, such as signal flow and substrate availability, which are critical in producing the correct signal-response relationship.\n\n**Explanation:** The correct answer is B) because the documentation states that Continuous Stochastic Logic properties can both detect and distinguish the types of cross-talk, and that different types of cross-talk result from different combinations of synchronisation and renaming. This suggests that the primary benefit of using Continuous Stochastic Logic properties is to identify specific types of cross-talk, such as signal flow and substrate availability, which are critical in producing the correct signal-response relationship."}, "58": {"documentation": {"title": "Scaling and Linear Response in the GOY Turbulence model", "source": "Leo Kadanoff, Detlef Lohse, Norbert Schorghofer (The James Franck\n  Institute, The University of Chicago)", "docs_id": "chao-dyn/9603011", "section": ["nlin.CD", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling and Linear Response in the GOY Turbulence model. The GOY model is a model for turbulence in which two conserved quantities cascade up and down a linear array of shells. When the viscosity parameter, $\\nu$, is small the model has a qualitative behavior which is similar to the Kolmogorov theories of turbulence. Here a static solution to the model is examined, and a linear stability analysis is performed to obtain response eigenvalues and eigenfunctions. Both the static behavior and the linear response show an inertial range with a relatively simple scaling structure. Our main results are: (i) The response frequencies cover a wide range of scales, with ratios which can be understood in terms of the frequency scaling properties of the model. (ii) Even small viscosities play a crucial role in determining the model's eigenvalue spectrum. (iii) As a parameter within the model is varied, it shows a ``phase transition'' in which there is an abrupt change in many eigenvalues from stable to unstable values. (iv) The abrupt change is determined by the model's conservation laws and symmetries. This work is thus intended to add to our knowledge of the linear response of a stiff dynamical systems and at the same time to help illuminate scaling within a class of turbulence models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the GOY turbulence model's response eigenvalues and eigenfunctions in the context of the given documentation. What is the significance of the \"phase transition\" observed in the model, and how does it relate to the conservation laws and symmetries of the model?\n\nA) The phase transition is a result of the model's viscosity parameter, \u03bd, being too small, leading to an unstable response eigenvalue spectrum. This is evident from the fact that small viscosities play a crucial role in determining the model's eigenvalue spectrum.\n\nB) The phase transition is a consequence of the model's conservation laws and symmetries, which lead to an abrupt change in the eigenvalue spectrum from stable to unstable values. This is supported by the fact that the model exhibits a \"phase transition\" in which many eigenvalues change stability.\n\nC) The phase transition is a result of the model's frequency scaling properties, which are determined by the ratio of response frequencies across different scales. This is evident from the fact that the response frequencies cover a wide range of scales.\n\nD) The phase transition is a result of the model's inertial range, which is characterized by a relatively simple scaling structure. This is supported by the fact that the model exhibits an inertial range with a simple scaling structure.\n\nCorrect Answer: B) The phase transition is a consequence of the model's conservation laws and symmetries, which lead to an abrupt change in the eigenvalue spectrum from stable to unstable values. This is supported by the fact that the model exhibits a \"phase transition\" in which many eigenvalues change stability.\n\nExplanation: The correct answer is B) because the documentation explicitly states that the \"phase transition\" is a result of the model's conservation laws and symmetries, which lead to an abrupt change in the eigenvalue spectrum from stable to unstable values. This is supported by the fact that the model exhibits a \"phase transition\" in which many eigenvalues change stability. The other options are incorrect because they do not accurately reflect the significance of the phase transition in the context of the GOY turbulence model. Option A is incorrect because the documentation does not suggest that the phase transition is a result of the model's viscosity parameter being too small. Option C is incorrect because the frequency scaling properties of the model are not directly related to the phase transition. Option D is incorrect because the inertial range is a characteristic of the model's behavior, but it is not the primary cause of the phase transition."}, "59": {"documentation": {"title": "A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data", "source": "Davide La Vecchia, Alban Moor, Olivier Scaillet", "docs_id": "2001.04867", "section": ["stat.ME", "econ.EM", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data. We develop and implement a novel fast bootstrap for dependent data. Our scheme is based on the i.i.d. resampling of the smoothed moment indicators. We characterize the class of parametric and semi-parametric estimation problems for which the method is valid. We show the asymptotic refinements of the proposed procedure, proving that it is higher-order correct under mild assumptions on the time series, the estimating functions, and the smoothing kernel. We illustrate the applicability and the advantages of our procedure for Generalized Empirical Likelihood estimation. As a by-product, our fast bootstrap provides higher-order correct asymptotic confidence distributions. Monte Carlo simulations on an autoregressive conditional duration model provide numerical evidence that the novel bootstrap yields higher-order accurate confidence intervals. A real-data application on dynamics of trading volume of stocks illustrates the advantage of our method over the routinely-applied first-order asymptotic theory, when the underlying distribution of the test statistic is skewed or fat-tailed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption underlying the proposed fast bootstrap method for dependent data, and how does it relate to the validity of the method for parametric and semi-parametric estimation problems?\n\nA) The method assumes i.i.d. resampling of the smoothed moment indicators, which is a necessary condition for higher-order correct asymptotic theory.\nB) The method assumes that the time series is stationary, which is a sufficient condition for the validity of the method.\nC) The method assumes that the estimating functions are linear, which is a necessary condition for the validity of the method.\nD) The method assumes that the smoothing kernel is symmetric, which is a sufficient condition for the validity of the method.\n\n**Correct Answer:** A) The method assumes i.i.d. resampling of the smoothed moment indicators, which is a necessary condition for higher-order correct asymptotic theory.\n\n**Explanation:** The correct answer is A) because the documentation states that the proposed fast bootstrap method is based on the i.i.d. resampling of the smoothed moment indicators, which is a necessary condition for higher-order correct asymptotic theory. The other options are incorrect because they do not accurately reflect the assumptions underlying the method. Option B is incorrect because the documentation does not mention stationarity as a necessary condition for the validity of the method. Option C is incorrect because the documentation does not mention linearity of the estimating functions as a necessary condition for the validity of the method. Option D is incorrect because the documentation does not mention symmetry of the smoothing kernel as a necessary condition for the validity of the method."}}