{"0": {"documentation": {"title": "Neocortical Dynamics at Multiple Scales: EEG Standing Waves, Statistical\n  Mechanics, and Physical Analogs", "source": "Lester Ingber and Paul L. Nunez", "docs_id": "1004.4322", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neocortical Dynamics at Multiple Scales: EEG Standing Waves, Statistical\n  Mechanics, and Physical Analogs. The dynamic behavior of scalp potentials (EEG) is apparently due to some combination of global and local processes with important top-down and bottom-up interactions across spatial scales. In treating global mechanisms, we stress the importance of myelinated axon propagation delays and periodic boundary conditions in the cortical-white matter system, which is topologically close to a spherical shell. By contrast, the proposed local mechanisms are multiscale interactions between cortical columns via short-ranged non-myelinated fibers. A mechanical model consisting of a stretched string with attached nonlinear springs demonstrates the general idea. The string produces standing waves analogous to large-scale coherence EEG observed in some brain states. The attached springs are analogous to the smaller (mesoscopic) scale columnar dynamics. Generally, we expect string displacement and EEG at all scales to result from both global and local phenomena. A statistical mechanics of neocortical interactions (SMNI) calculates oscillatory behavior consistent with typical EEG, within columns, between neighboring columns via short-ranged non-myelinated fibers, across cortical regions via myelinated fibers, and also derive a string equation consistent with the global EEG model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Neocortical Dynamics at Multiple Scales: EEG Standing Waves, Statistical Mechanics, and Physical Analogs\" propose a mechanical model to describe the dynamic behavior of scalp potentials (EEG). What is the primary mechanism that the authors suggest is responsible for the global mechanisms of EEG, and how does it relate to the cortical-white matter system?\n\n**A)** The global mechanisms of EEG are primarily due to the periodic boundary conditions in the cortical-white matter system, which is topologically close to a spherical shell. This is analogous to a stretched string with attached nonlinear springs, where the string produces standing waves.\n\n**B)** The global mechanisms of EEG are primarily due to the myelinated axon propagation delays in the cortical-white matter system, which are analogous to the attachment of nonlinear springs to a stretched string.\n\n**C)** The global mechanisms of EEG are primarily due to the short-ranged non-myelinated fibers between cortical columns, which are analogous to the attachment of nonlinear springs to a stretched string.\n\n**D)** The global mechanisms of EEG are primarily due to the myelinated fibers across cortical regions, which are analogous to the attachment of nonlinear springs to a stretched string.\n\n**Correct Answer:** A) The global mechanisms of EEG are primarily due to the periodic boundary conditions in the cortical-white matter system, which is topologically close to a spherical shell. This is analogous to a stretched string with attached nonlinear springs, where the string produces standing waves.\n\n**Explanation:** The authors of the paper propose a mechanical model to describe the dynamic behavior of scalp potentials (EEG). The global mechanisms of EEG are primarily due to the periodic boundary conditions in the cortical-white matter system, which is topologically close to a spherical shell. This is analogous to a stretched string with attached nonlinear springs, where the string produces standing waves. The attachment of nonlinear springs to the stretched string represents the myelinated axon propagation delays in the cortical-white matter system, which are important for the global mechanisms of EEG."}, "1": {"documentation": {"title": "Cellular Controlled Cooperative Unmanned Aerial Vehicle Networks with\n  Sense-and-Send Protocol", "source": "Shuhang Zhang, Hongliang Zhang, Boya Di, and Lingyang Song", "docs_id": "1805.11779", "section": ["cs.SY", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cellular Controlled Cooperative Unmanned Aerial Vehicle Networks with\n  Sense-and-Send Protocol. In this paper, we consider a cellular controlled unmanned aerial vehicle (UAV) sensing network in which multiple UAVs cooperatively complete each sensing task. We first propose a sense-and-send protocol where the UAVs collect sensory data of the tasks and transmit the collected data to the base station. We then formulate a joint trajectory, sensing location, and UAV scheduling optimization problem that minimizes the completion time for all the sensing tasks in the network. To solve this NP-hard problem efficiently, we decouple it into three sub-problems: trajectory optimization, sensing location optimization, and UAV scheduling. An iterative trajectory, sensing, and scheduling optimization (ITSSO) algorithm is proposed to solve these sub-problems jointly. The convergence and complexity of the ITSSO algorithm, together with the system performance are analysed. Simulation results show that the proposed ITSSO algorithm saves the task completion time by 15% compared to the non-cooperative scheme."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Consider a cellular controlled UAV sensing network with multiple UAVs cooperatively completing sensing tasks. The sense-and-send protocol is used, where UAVs collect sensory data and transmit it to the base station. Formulate the joint trajectory, sensing location, and UAV scheduling optimization problem that minimizes the completion time for all sensing tasks in the network.\n\n**A)** Minimize the sum of the completion times for each sensing task, subject to the constraints that the UAVs' trajectories do not intersect and the sensing locations are within the coverage area of the base station.\n\n**B)** Minimize the maximum completion time for any sensing task, subject to the constraints that the UAVs' trajectories are convex and the sensing locations are within the coverage area of the base station.\n\n**C)** Minimize the total energy consumption of the UAVs, subject to the constraints that the sensing locations are within the coverage area of the base station and the UAVs' trajectories do not intersect.\n\n**D)** Minimize the sum of the squared completion times for each sensing task, subject to the constraints that the UAVs' trajectories are convex and the sensing locations are within the coverage area of the base station.\n\n**Correct Answer:** A) Minimize the sum of the completion times for each sensing task, subject to the constraints that the UAVs' trajectories do not intersect and the sensing locations are within the coverage area of the base station.\n\n**Explanation:** The correct answer is A) because the problem statement explicitly mentions that the goal is to minimize the completion time for all sensing tasks in the network. The constraints that the UAVs' trajectories do not intersect and the sensing locations are within the coverage area of the base station are necessary to ensure that the UAVs can complete the sensing tasks efficiently and effectively. The other options (B, C, and D) are incorrect because they do not align with the problem statement or the constraints mentioned in the problem."}, "2": {"documentation": {"title": "System-Level Predictive Maintenance: Review of Research Literature and\n  Gap Analysis", "source": "Kyle Miller and Artur Dubrawski", "docs_id": "2005.05239", "section": ["cs.AI", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "System-Level Predictive Maintenance: Review of Research Literature and\n  Gap Analysis. This paper reviews current literature in the field of predictive maintenance from the system point of view. We differentiate the existing capabilities of condition estimation and failure risk forecasting as currently applied to simple components, from the capabilities needed to solve the same tasks for complex assets. System-level analysis faces more complex latent degradation states, it has to comprehensively account for active maintenance programs at each component level and consider coupling between different maintenance actions, while reflecting increased monetary and safety costs for system failures. As a result, methods that are effective for forecasting risk and informing maintenance decisions regarding individual components do not readily scale to provide reliable sub-system or system level insights. A novel holistic modeling approach is needed to incorporate available structural and physical knowledge and naturally handle the complexities of actively fielded and maintained assets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is a key challenge in applying current predictive maintenance methods to complex assets, and what type of modeling approach is needed to address this challenge?\n\n**A)** The challenge lies in the lack of data on complex assets, and a novel approach is needed to incorporate machine learning algorithms to handle the complexities of these assets.\n\n**B)** The challenge is that current methods are effective for simple components but do not scale to provide reliable sub-system or system level insights, and a holistic modeling approach is needed to account for coupling between different maintenance actions.\n\n**C)** The challenge is that predictive maintenance methods are too expensive to implement, and a novel approach is needed to reduce costs while maintaining system reliability.\n\n**D)** The challenge is that current methods are not able to handle the complexities of latent degradation states, and a novel approach is needed to incorporate structural and physical knowledge into the modeling process.\n\n**Correct Answer:** B) The challenge is that current methods are effective for simple components but do not scale to provide reliable sub-system or system level insights, and a holistic modeling approach is needed to account for coupling between different maintenance actions.\n\n**Explanation:** The question requires the test-taker to understand the key challenge in applying current predictive maintenance methods to complex assets, as identified in the provided documentation. The correct answer, B, highlights the challenge of scaling current methods to provide reliable sub-system or system level insights, and the need for a holistic modeling approach to account for coupling between different maintenance actions. The other options are incorrect because they either misrepresent the challenge or propose a solution that is not supported by the documentation."}, "3": {"documentation": {"title": "Insights on Rigidity and flexibility at the Global and Local Levels of\n  Protein Structures and their Roles in Homologous Psychrophilic, Mesophilic\n  and Thermophilic Proteins: A Computational Study", "source": "Srikanta Sen and Munna Sarkar", "docs_id": "2106.03768", "section": ["physics.bio-ph", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Insights on Rigidity and flexibility at the Global and Local Levels of\n  Protein Structures and their Roles in Homologous Psychrophilic, Mesophilic\n  and Thermophilic Proteins: A Computational Study. The rigidity and flexibility of homologous psychrophilic(P), mesophilic(M) and thermophilic(T) proteins have been investigated at the global and local levels in terms of packing factor and atomic fluctuations obtained from B-factors. For comparison of atomic fluctuations, correction of errors by considering errors in B-factors from all sources in a consolidated manner and conversion of the fluctuations to the same temperature have been suggested and validated. Results indicate no differences in the global values like average packing factor among the three classes of protein homologs but at local levels there are differences. Comparison of homologous proteins triplets show that the average atomic fluctuations at a given temperature obey the order P>M>T. Packing factors and the atomic fluctuations are anti-correlated suggesting that altering the rigidity of the active site might be a potential strategy to make tailor made psychrophilic or thermophilic proteins from their mesophilic homologs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion drawn from the study regarding the rigidity and flexibility of homologous psychrophilic, mesophilic, and thermophilic proteins, and how do these findings relate to the potential design of tailor-made psychrophilic or thermophilic proteins from their mesophilic homologs?\n\n**A)** The study suggests that the rigidity and flexibility of homologous proteins are identical across all three temperature classes, and that altering the rigidity of the active site is not a viable strategy for designing thermophilic proteins.\n\n**B)** The study indicates that the average atomic fluctuations at a given temperature obey the order P>M>T, and that packing factors and atomic fluctuations are anti-correlated, suggesting that altering the rigidity of the active site could be a potential strategy for designing tailor-made psychrophilic or thermophilic proteins from their mesophilic homologs.\n\n**C)** The study reveals that the global values of packing factor and atomic fluctuations are identical across all three temperature classes, but that local-level differences in rigidity and flexibility are significant, and that these differences may be exploited to design thermophilic proteins from their mesophilic homologs.\n\n**D)** The study concludes that the rigidity and flexibility of homologous proteins are inversely correlated with temperature, and that the active site of mesophilic proteins can be modified to increase their rigidity and reduce their flexibility, making them more suitable for thermophilic conditions.\n\n**Correct Answer:** B) The study indicates that the average atomic fluctuations at a given temperature obey the order P>M>T, and that packing factors and atomic fluctuations are anti-correlated, suggesting that altering the rigidity of the active site could be a potential strategy for designing tailor-made psychrophilic or thermophilic proteins from their mesophilic homologs.\n\n**Explanation:** The correct answer is based on the conclusion drawn from the study that the average atomic fluctuations at a given temperature obey the order P>M>T, and that packing factors and atomic fluctuations are anti-correlated. This suggests that altering the rigidity of the active site could be a potential strategy for designing tailor-made psychrophilic or thermophilic proteins from their mesophilic homologs. The other options are incorrect because they either misinterpret the findings of the study or contradict the conclusions drawn from the data."}, "4": {"documentation": {"title": "Regression Discontinuity Design with Multivalued Treatments", "source": "Carolina Caetano and Gregorio Caetano and Juan Carlos Escanciano", "docs_id": "2007.00185", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Regression Discontinuity Design with Multivalued Treatments. We study identification and estimation in the Regression Discontinuity Design (RDD) with a multivalued treatment variable. We also allow for the inclusion of covariates. We show that without additional information, treatment effects are not identified. We give necessary and sufficient conditions that lead to identification of LATEs as well as of weighted averages of the conditional LATEs. We show that if the first stage discontinuities of the multiple treatments conditional on covariates are linearly independent, then it is possible to identify multivariate weighted averages of the treatment effects with convenient identifiable weights. If, moreover, treatment effects do not vary with some covariates or a flexible parametric structure can be assumed, it is possible to identify (in fact, over-identify) all the treatment effects. The over-identification can be used to test these assumptions. We propose a simple estimator, which can be programmed in packaged software as a Two-Stage Least Squares regression, and packaged standard errors and tests can also be used. Finally, we implement our approach to identify the effects of different types of insurance coverage on health care utilization, as in Card, Dobkin and Maestas (2008)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Regression Discontinuity Design (RDD) with a multivalued treatment variable, what are the necessary and sufficient conditions for identifying the Local Average Treatment Effect (LATE) and weighted averages of the conditional LATE, assuming the first stage discontinuities of the multiple treatments conditional on covariates are linearly independent?\n\n**A)** The treatment effects must be constant across all covariates, and the discontinuities must be linearly independent.\n**B)** The treatment effects must be constant across all covariates, and the discontinuities must be linearly independent, and the treatment effects must not vary with some covariates.\n**C)** The treatment effects must be constant across all covariates, and the discontinuities must be linearly independent, and the treatment effects must not vary with some covariates, and a flexible parametric structure can be assumed.\n**D)** The treatment effects must be constant across all covariates, and the discontinuities must be linearly independent, and the treatment effects must not vary with some covariates, and a flexible parametric structure can be assumed, and the first stage discontinuities must be linearly independent.\n\n**Correct Answer:** C) The treatment effects must be constant across all covariates, and the discontinuities must be linearly independent, and the treatment effects must not vary with some covariates, and a flexible parametric structure can be assumed.\n\n**Explanation:** The correct answer requires an understanding of the necessary and sufficient conditions for identifying the LATE and weighted averages of the conditional LATE in the RDD with a multivalued treatment variable. The correct answer states that these conditions are met when the treatment effects are constant across all covariates, the discontinuities are linearly independent, the treatment effects do not vary with some covariates, and a flexible parametric structure can be assumed. This requires a deep understanding of the RDD framework and the assumptions required for identification."}, "5": {"documentation": {"title": "Motor-driven Dynamics of Cytoskeletal FIlaments in Motility Assays", "source": "Shiladitya Banerjee, M. Cristina Marchetti and Kristian\n  M\\\"uller-Nedebock", "docs_id": "1104.3360", "section": ["cond-mat.stat-mech", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Motor-driven Dynamics of Cytoskeletal FIlaments in Motility Assays. We model analytically the dynamics of a cytoskeletal filament in a motility assay. The filament is described as rigid rod free to slide in two dimensions. The motor proteins consist of polymeric tails tethered to the plane and modeled as linear springs and motor heads that bind to the filament. As in related models of rigid and soft two-state motors, the binding/unbinding dynamics of the motor heads and the dependence of the transition rates on the load exerted by the motor tails play a crucial role in controlling the filament's dynamics. Our work shows that the filament effectively behaves as a self-propelled rod at long times, but with non-Markovian noise sources arising from the coupling to the motor binding/unbinding dynamics. The effective propulsion force of the filament and the active renormalization of the various friction and diffusion constants are calculated in terms of microscopic motor and filament parameters. These quantities could be probed by optical force microscopy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the motor-driven dynamics of cytoskeletal filaments in motility assays, what is the primary mechanism by which the filament's dynamics are controlled, and how does it differ from Markovian noise sources?\n\n**A)** The filament's dynamics are controlled by the binding/unbinding dynamics of the motor heads, which is a Markovian process.\n\n**B)** The filament's dynamics are controlled by the load exerted by the motor tails, which is a non-Markovian process.\n\n**C)** The filament's dynamics are controlled by the coupling to the motor binding/unbinding dynamics, which is a Markovian process.\n\n**D)** The filament's dynamics are controlled by the active renormalization of the various friction and diffusion constants, which is a non-Markovian process.\n\n**Correct Answer:** C) The filament's dynamics are controlled by the coupling to the motor binding/unbinding dynamics, which is a Markovian process.\n\n**Explanation:** The correct answer is C) because the documentation states that the binding/unbinding dynamics of the motor heads and the dependence of the transition rates on the load exerted by the motor tails play a crucial role in controlling the filament's dynamics. This is a Markovian process, as it involves a random and probabilistic nature of the motor heads binding/unbinding. The coupling to the motor binding/unbinding dynamics is what gives rise to the non-Markovian noise sources, but the primary mechanism of control is still a Markovian process.\n\n**Note:** The other options are incorrect because:\n\n* A) The binding/unbinding dynamics of the motor heads is a Markovian process, but it is not the primary mechanism of control.\n* B) The load exerted by the motor tails is a non-Markovian process, but it is not the primary mechanism of control.\n* D) The active renormalization of the various friction and diffusion constants is a non-Markovian process, but it is not the primary mechanism of control."}, "6": {"documentation": {"title": "On modular Galois representations modulo prime powers", "source": "Imin Chen, Ian Kiming, Gabor Wiese", "docs_id": "1105.1918", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On modular Galois representations modulo prime powers. We study modular Galois representations mod $p^m$. We show that there are three progressively weaker notions of modularity for a Galois representation mod $p^m$: we have named these `strongly', `weakly', and `dc-weakly' modular. Here, `dc' stands for `divided congruence' in the sense of Katz and Hida. These notions of modularity are relative to a fixed level $M$. Using results of Hida we display a `stripping-of-powers of $p$ away from the level' type of result: A mod $p^m$ strongly modular representation of some level $Np^r$ is always dc-weakly modular of level $N$ (here, $N$ is a natural number not divisible by $p$). We also study eigenforms mod $p^m$ corresponding to the above three notions. Assuming residual irreducibility, we utilize a theorem of Carayol to show that one can attach a Galois representation mod $p^m$ to any `dc-weak' eigenform, and hence to any eigenform mod $p^m$ in any of the three senses. We show that the three notions of modularity coincide when $m=1$ (as well as in other, particular cases), but not in general."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $N$ be a level not divisible by $p$, and let $M$ be a fixed level such that $M \\geq N$. Suppose we have a modular Galois representation $\\rho$ mod $p^m$ of level $Np^r$ for some $r \\geq 0$. Which of the following statements is true?\n\nA) $\\rho$ is strongly modular mod $p^m$ if and only if $\\rho$ is weakly modular mod $p^m$.\nB) $\\rho$ is strongly modular mod $p^m$ if and only if $\\rho$ is dc-weakly modular mod $p^m$.\nC) $\\rho$ is strongly modular mod $p^m$ if and only if $\\rho$ is dc-weakly modular mod $p^m$ and also weakly modular mod $p^m$.\nD) $\\rho$ is strongly modular mod $p^m$ if and only if $\\rho$ is dc-weakly modular mod $p^m$ and also weakly modular mod $p^m$, but not necessarily the other way around.\n\nCorrect Answer: B) $\\rho$ is strongly modular mod $p^m$ if and only if $\\rho$ is dc-weakly modular mod $p^m$.\n\nExplanation: The text states that a mod $p^m$ strongly modular representation of some level $Np^r$ is always dc-weakly modular of level $N$. This implies that strong modularity implies dc-weak modularity, but not necessarily the other way around. Therefore, option B is the correct answer. Options A and C are incorrect because they imply that strong modularity and dc-weak modularity are equivalent, which is not necessarily the case. Option D is also incorrect because it implies that strong modularity implies dc-weak modularity and weak modularity, but not necessarily the other way around."}, "7": {"documentation": {"title": "A Tight and Unified Analysis of Gradient-Based Methods for a Whole\n  Spectrum of Games", "source": "Wa\\\"iss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, Gauthier\n  Gidel", "docs_id": "1906.05945", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Tight and Unified Analysis of Gradient-Based Methods for a Whole\n  Spectrum of Games. We consider differentiable games where the goal is to find a Nash equilibrium. The machine learning community has recently started using variants of the gradient method (GD). Prime examples are extragradient (EG), the optimistic gradient method (OG) and consensus optimization (CO), which enjoy linear convergence in cases like bilinear games, where the standard GD fails. The full benefits of theses relatively new methods are not known as there is no unified analysis for both strongly monotone and bilinear games. We provide new analyses of the EG's local and global convergence properties and use is to get a tighter global convergence rate for OG and CO. Our analysis covers the whole range of settings between bilinear and strongly monotone games. It reveals that these methods converge via different mechanisms at these extremes; in between, it exploits the most favorable mechanism for the given problem. We then prove that EG achieves the optimal rate for a wide class of algorithms with any number of extrapolations. Our tight analysis of EG's convergence rate in games shows that, unlike in convex minimization, EG may be much faster than GD."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a differentiable game where the goal is to find a Nash equilibrium. Suppose we are using the optimistic gradient method (OG) to solve this problem. What is the convergence rate of OG in the case of bilinear games, and how does it compare to the convergence rate of the standard gradient descent (GD) method?\n\nA) OG converges at a rate of O(1/t) in bilinear games, which is faster than GD.\nB) OG converges at a rate of O(1/t) in bilinear games, which is slower than GD.\nC) OG converges at a rate of O(1/t^2) in bilinear games, which is comparable to GD.\nD) OG converges at a rate of O(1/t^2) in bilinear games, which is slower than GD.\n\nCorrect Answer: A) OG converges at a rate of O(1/t) in bilinear games, which is faster than GD.\n\nExplanation: According to the provided documentation, the optimistic gradient method (OG) converges at a rate of O(1/t) in bilinear games, which is faster than the standard gradient descent (GD) method. This is because OG exploits the most favorable mechanism for the given problem, which in the case of bilinear games is a faster convergence rate."}, "8": {"documentation": {"title": "Forecasting financial crashes with quantum computing", "source": "Roman Orus, Samuel Mugel, Enrique Lizaso", "docs_id": "1810.07690", "section": ["q-fin.GN", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Forecasting financial crashes with quantum computing. A key problem in financial mathematics is the forecasting of financial crashes: if we perturb asset prices, will financial institutions fail on a massive scale? This was recently shown to be a computationally intractable (NP-hard) problem. Financial crashes are inherently difficult to predict, even for a regulator which has complete information about the financial system. In this paper we show how this problem can be handled by quantum annealers. More specifically, we map the equilibrium condition of a toy-model financial network to the ground-state problem of a spin-1/2 quantum Hamiltonian with 2-body interactions, i.e., a quadratic unconstrained binary optimization (QUBO) problem. The equilibrium market values of institutions after a sudden shock to the network can then be calculated via adiabatic quantum computation and, more generically, by quantum annealers. Our procedure could be implemented on near-term quantum processors, thus providing a potentially more efficient way to assess financial equilibrium and predict financial crashes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Forecasting financial crashes with quantum computing involves mapping a financial network to a quantum Hamiltonian. What type of optimization problem is this mapping equivalent to, and how does it relate to the financial crashes problem?\n\n**A)** Linear programming problem\n**B)** Quadratic unconstrained binary optimization (QUBO) problem\n**C)** Mixed-integer linear programming problem\n**D)** Nonlinear programming problem\n\n**Correct Answer:** B) Quadratic unconstrained binary optimization (QUBO) problem\n\n**Explanation:** The mapping of the financial network to a quantum Hamiltonian is equivalent to a QUBO problem, which is a type of optimization problem that involves finding the minimum or maximum of a quadratic function subject to binary constraints. In this context, the QUBO problem is used to model the equilibrium condition of the financial network, and the solution to this problem can be used to predict financial crashes. The correct answer, B) QUBO problem, is the only option that accurately reflects this relationship.\n\nNote: The other options are incorrect because linear programming problems typically involve linear constraints, mixed-integer linear programming problems involve both linear and integer constraints, and nonlinear programming problems involve non-linear functions, none of which are relevant to the QUBO problem described in the question."}, "9": {"documentation": {"title": "Fe alloy slurry and a compacting cumulate pile across Earth's inner-core\n  boundary", "source": "Youjun Zhang, Peter Nelson, Nick Dygert, Jung-Fu Lin", "docs_id": "1903.12574", "section": ["physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fe alloy slurry and a compacting cumulate pile across Earth's inner-core\n  boundary. Seismic observations show a reduced compressional-wave gradient at the base of the outer core relative to the preliminary reference Earth model and seismic wave asymmetry between the east-west hemispheres at the top of the inner core. Here, we propose a model for the inner core boundary (ICB), where a slurry layer forms through fractional crystallization of an Fe alloy at the base of the outer core (F layer) above a compacting cumulate pile at the top of the inner core (F' layer). Using recent mineral physics data, we show that fractional crystallization of an Fe alloy (e.g., Fe-Si-O) with light element partitioning can explain the observed reduced velocity gradient in the F layer, in cases with a solid fraction of ~15(5)% in liquid with a compositional gradient due to preferential light element partitioning into liquid. The compacting cumulate pile in the F' layer may exhibit lateral variations in thickness between the east-west hemispheres due to lateral variations of large-scale heat flow in the outer core, which may explain the east-west asymmetry observed in the seismic velocity. Our interpretations suggest that the inner core with solid Fe alloy has a high shear viscosity of ~10^23 Pa s."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the most plausible explanation for the observed east-west asymmetry in seismic velocity at the top of the inner core, and what implications does this have for the rheology of the inner core?\n\nA) The east-west asymmetry is due to lateral variations in the thickness of the compacting cumulate pile in the F' layer, which are caused by differences in large-scale heat flow between the hemispheres.\n\nB) The east-west asymmetry is due to the presence of a liquid layer with a compositional gradient in the F layer, which causes preferential light element partitioning into the liquid and leads to a non-uniform velocity gradient.\n\nC) The east-west asymmetry is due to the presence of a solid Fe alloy with a high shear viscosity in the inner core, which causes a non-uniform velocity gradient due to the anisotropic nature of the material.\n\nD) The east-west asymmetry is due to the presence of a slurry layer in the F layer, which is caused by the fractional crystallization of an Fe alloy and leads to a non-uniform velocity gradient due to the varying solid fraction and compositional gradient.\n\nCorrect Answer: D) The east-west asymmetry is due to the presence of a slurry layer in the F layer, which is caused by the fractional crystallization of an Fe alloy and leads to a non-uniform velocity gradient due to the varying solid fraction and compositional gradient.\n\nExplanation: The correct answer is D) because the presence of a slurry layer in the F layer, caused by the fractional crystallization of an Fe alloy, can explain the observed east-west asymmetry in seismic velocity at the top of the inner core. The varying solid fraction and compositional gradient in the slurry layer can lead to a non-uniform velocity gradient, which is consistent with the observed seismic data. The other options are incorrect because they do not provide a plausible explanation for the observed asymmetry. Option A is incorrect because the lateral variations in the thickness of the compacting cumulate pile are not supported by the data. Option B is incorrect because the presence of a liquid layer with a compositional gradient is not necessary to explain the observed asymmetry. Option C is incorrect because the high shear viscosity of the solid Fe alloy is not directly related to the observed asymmetry."}, "10": {"documentation": {"title": "Stability Analysis of Gradient-Based Distributed Formation Control with\n  Heterogeneous Sensing Mechanism: Two and Three Robot Case", "source": "Nelson P.K. Chan and Bayu Jayawardhana and Hector Garcia de Marina", "docs_id": "2010.10559", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability Analysis of Gradient-Based Distributed Formation Control with\n  Heterogeneous Sensing Mechanism: Two and Three Robot Case. This paper focuses on the stability analysis of a formation shape displayed by a team of mobile robots that uses heterogeneous sensing mechanism. Depending on the convenience and reliability of the local information, each robot utilizes the popular gradient-based control law which, in this paper, is either the distance-based or the bearing-only formation control. For the two and three robot case, we show that the use of heterogeneous gradient-based control laws can give rise to an undesired invariant set where a distorted formation shape is moving at a constant velocity. The (in)stability of such an invariant set is dependent on the specified distance and bearing constraints. For the two robot case, we prove almost global stability of the desired equilibrium set while for the three robot case, we guarantee local asymptotic stability for the correct formation shape. We also derive conditions for the three robot case in which the undesired invariant set is locally attractive. Numerical simulations are presented for illustrating the theoretical results in the three robot case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the stability of the formation shape displayed by a team of mobile robots using heterogeneous sensing mechanisms. What are the conditions under which the undesired invariant set is locally attractive, and how do the results differ between the two-robot and three-robot cases?\n\n**A)** The undesired invariant set is locally attractive when the distance constraint is relaxed, and the bearing-only formation control is used. In the two-robot case, the system is almost globally stable, while in the three-robot case, it is locally asymptotically stable.\n\n**B)** The undesired invariant set is locally attractive when the bearing constraint is relaxed, and the distance-based formation control is used. In the two-robot case, the system is locally asymptotically stable, while in the three-robot case, it is almost globally stable.\n\n**C)** The undesired invariant set is locally attractive when the distance constraint is relaxed, and the bearing-only formation control is used. In the two-robot case, the system is locally asymptotically stable, while in the three-robot case, it is locally attractive.\n\n**D)** The undesired invariant set is locally attractive when the bearing constraint is relaxed, and the distance-based formation control is used. In the two-robot case, the system is almost globally stable, while in the three-robot case, it is locally attractive.\n\n**Correct Answer:** B) The undesired invariant set is locally attractive when the bearing constraint is relaxed, and the distance-based formation control is used. In the two-robot case, the system is locally asymptotically stable, while in the three-robot case, it is almost globally stable.\n\n**Explanation:** The correct answer is B) because the paper states that the undesired invariant set is locally attractive when the bearing constraint is relaxed, and the distance-based formation control is used. Additionally, the paper shows that in the two-robot case, the system is locally asymptotically stable, while in the three-robot case, it is almost globally stable. The other options are incorrect because they either misstate the conditions for local attractiveness or the stability results for the two-robot and three-robot cases."}, "11": {"documentation": {"title": "Extreme-value statistics from Lagrangian convex hull analysis for\n  homogeneous turbulent Boussinesq convection and MHD convection", "source": "J. Pratt, A. Busse, W.-C. M\\\"uller, N.W. Watkins, S.C. Chapman", "docs_id": "1605.05983", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Extreme-value statistics from Lagrangian convex hull analysis for\n  homogeneous turbulent Boussinesq convection and MHD convection. We investigate the utility of the convex hull of many Lagrangian tracers to analyze transport properties of turbulent flows with different anisotropy. In direct numerical simulations of statistically homogeneous and stationary Navier-Stokes turbulence, neutral fluid Boussinesq convection, and MHD Boussinesq convection a comparison with Lagrangian pair dispersion shows that convex hull statistics capture the asymptotic dispersive behavior of a large group of passive tracer particles. Moreover, convex hull analysis provides additional information on the sub-ensemble of tracers that on average disperse most efficiently in the form of extreme value statistics and flow anisotropy via the geometric properties of the convex hulls. We use the convex hull surface geometry to examine the anisotropy that occurs in turbulent convection. Applying extreme value theory, we show that the maximal square extensions of convex hull vertices are well described by a classic extreme value distribution, the Gumbel distribution. During turbulent convection, intermittent convective plumes grow and accelerate the dispersion of Lagrangian tracers. Convex hull analysis yields information that supplements standard Lagrangian analysis of coherent turbulent structures and their influence on the global statistics of the flow."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using convex hull analysis in the study of turbulent flows, as demonstrated in the provided Arxiv documentation?\n\nA) Convex hull analysis provides a more accurate representation of the flow's spatial structure compared to Lagrangian pair dispersion.\nB) Convex hull analysis is more computationally efficient than Lagrangian pair dispersion for large-scale simulations.\nC) Convex hull analysis captures the asymptotic dispersive behavior of passive tracer particles in turbulent flows, providing additional information on extreme value statistics and flow anisotropy.\nD) Convex hull analysis is limited to studying only statistically homogeneous and stationary Navier-Stokes turbulence.\n\nCorrect Answer: C) Convex hull analysis captures the asymptotic dispersive behavior of passive tracer particles in turbulent flows, providing additional information on extreme value statistics and flow anisotropy.\n\nExplanation: The correct answer is C) because the provided documentation highlights the ability of convex hull analysis to capture the asymptotic dispersive behavior of passive tracer particles in turbulent flows, which is not a characteristic of Lagrangian pair dispersion. Additionally, convex hull analysis provides information on extreme value statistics and flow anisotropy, which is not mentioned as a limitation of Lagrangian pair dispersion."}, "12": {"documentation": {"title": "Modeling of the Greek road transportation network using complex network\n  analysis", "source": "Dimitrios Tsiotas", "docs_id": "2003.08091", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling of the Greek road transportation network using complex network\n  analysis. This article studies the interregional Greek road network (GRN) by applying complex network analysis (CNA) and an empirical approach. The study aims to extract the socioeconomic information immanent to the GRN's topology and to interpret the way in which this road network serves and promotes the regional development. The analysis shows that the topology of the GRN is submitted to spatial constraints, having lattice-like characteristics. Also, the GRN's structure is described by a gravity pattern, where places of higher population enjoy greater functionality, and its interpretation in regional terms illustrates the elementary pattern expressed by regional development through road construction. The study also reveals some interesting contradictions between the metropolitan and non-metropolitan (excluding Attica and Thessaloniki) comparison. Overall, the article highlights the effectiveness of using complex network analysis in the modeling of spatial networks and in particular of transportation systems and promotes the use of the network paradigm in the spatial and regional research."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the Greek road transportation network (GRN) using complex network analysis (CNA) and discuss the implications of the study's results on regional development.\n\n**A)** The study's findings suggest that the GRN's lattice-like characteristics are a result of the country's historical development, with major cities forming the core of the network.\n\n**B)** The gravity pattern observed in the GRN's structure implies that the network is optimized for efficient transportation, with high-population areas serving as hubs for regional connectivity.\n\n**C)** The study's results indicate that the GRN's topology is influenced by socioeconomic factors, such as population density and economic activity, which in turn affect regional development.\n\n**D)** The analysis reveals that the GRN's structure is characterized by a hierarchical pattern, with metropolitan areas dominating the network and non-metropolitan areas lagging behind.\n\n**Correct Answer:** C) The study's results indicate that the GRN's topology is influenced by socioeconomic factors, such as population density and economic activity, which in turn affect regional development.\n\n**Explanation:** The study's findings suggest that the GRN's lattice-like characteristics and gravity pattern are shaped by socioeconomic factors, such as population density and economic activity. This implies that the network's topology is influenced by the underlying socioeconomic structure of the country, which in turn affects regional development. This is evident in the study's interpretation of the GRN's structure as a reflection of regional development through road construction. The correct answer requires the test-taker to understand the study's methodology and results, and to analyze the implications of the findings on regional development."}, "13": {"documentation": {"title": "UAV Coverage Path Planning under Varying Power Constraints using Deep\n  Reinforcement Learning", "source": "Mirco Theile, Harald Bayerlein, Richard Nai, David Gesbert and Marco\n  Caccamo", "docs_id": "2003.02609", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UAV Coverage Path Planning under Varying Power Constraints using Deep\n  Reinforcement Learning. Coverage path planning (CPP) is the task of designing a trajectory that enables a mobile agent to travel over every point of an area of interest. We propose a new method to control an unmanned aerial vehicle (UAV) carrying a camera on a CPP mission with random start positions and multiple options for landing positions in an environment containing no-fly zones. While numerous approaches have been proposed to solve similar CPP problems, we leverage end-to-end reinforcement learning (RL) to learn a control policy that generalizes over varying power constraints for the UAV. Despite recent improvements in battery technology, the maximum flying range of small UAVs is still a severe constraint, which is exacerbated by variations in the UAV's power consumption that are hard to predict. By using map-like input channels to feed spatial information through convolutional network layers to the agent, we are able to train a double deep Q-network (DDQN) to make control decisions for the UAV, balancing limited power budget and coverage goal. The proposed method can be applied to a wide variety of environments and harmonizes complex goal structures with system constraints."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of UAV Coverage Path Planning under Varying Power Constraints using Deep Reinforcement Learning, what is the primary advantage of using a double deep Q-network (DDQN) to control the UAV?\n\n**A)** It allows for more efficient use of the UAV's power budget by learning to prioritize coverage of high-value areas.\n**B)** It enables the UAV to learn a control policy that generalizes over varying power constraints and balances limited power budget and coverage goal.\n**C)** It provides a more accurate estimate of the UAV's power consumption by incorporating additional sensor data.\n**D)** It enables the UAV to learn a control policy that optimizes coverage of the entire environment, regardless of power constraints.\n\n**Correct Answer:** B) It enables the UAV to learn a control policy that generalizes over varying power constraints and balances limited power budget and coverage goal.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed method uses a DDQN to learn a control policy that generalizes over varying power constraints and balances limited power budget and coverage goal. This is the primary advantage of using a DDQN in this context. The other options are incorrect because they do not accurately reflect the primary advantage of using a DDQN in this context. Option A is incorrect because while the DDQN may learn to prioritize coverage of high-value areas, this is not its primary advantage. Option C is incorrect because the DDQN does not incorporate additional sensor data to estimate power consumption. Option D is incorrect because the DDQN is designed to balance limited power budget and coverage goal, not optimize coverage of the entire environment."}, "14": {"documentation": {"title": "Reduction in the complexity of 1D 1H-NMR spectra by the use of Frequency\n  to Information Transformation", "source": "Homayoun Valafar, Faramarz Valafar", "docs_id": "2012.09267", "section": ["eess.SP", "cs.CV", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reduction in the complexity of 1D 1H-NMR spectra by the use of Frequency\n  to Information Transformation. Analysis of 1H-NMR spectra is often hindered by large variations that occur during the collection of these spectra. Large solvent and standard peaks, base line drift and negative peaks (due to improper phasing) are among some of these variations. Furthermore, some instrument dependent alterations, such as incorrect shimming, are also embedded in the recorded spectrum. The unpredictable nature of these alterations of the signal has rendered the automated and instrument independent computer analysis of these spectra unreliable. In this paper, a novel method of extracting the information content of a signal (in this paper, frequency domain 1H-NMR spectrum), called the frequency-information transformation (FIT), is presented and compared to a previously used method (SPUTNIK). FIT can successfully extract the relevant information to a pattern matching task present in a signal, while discarding the remainder of a signal by transforming a Fourier transformed signal into an information spectrum (IS). This technique exhibits the ability of decreasing the inter-class correlation coefficients while increasing the intra-class correlation coefficients. Different spectra of the same molecule, in other words, will resemble more to each other while the spectra of different molecules will look more different from each other. This feature allows easier automated identification and analysis of molecules based on their spectral signatures using computer algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Frequency-to-Information Transformation (FIT) method presented in the paper, and how does it differ from the previously used method (SPUTNIK)?\n\nA) FIT aims to increase the inter-class correlation coefficients while decreasing the intra-class correlation coefficients, whereas SPUTNIK does the opposite. FIT is used for automated identification and analysis of molecules based on their spectral signatures.\n\nB) FIT aims to decrease the inter-class correlation coefficients while increasing the intra-class correlation coefficients, whereas SPUTNIK does the opposite. FIT is used for automated identification and analysis of molecules based on their spectral signatures.\n\nC) FIT aims to increase the intra-class correlation coefficients while decreasing the inter-class correlation coefficients, whereas SPUTNIK does the opposite. FIT is used for automated identification and analysis of molecules based on their spectral signatures.\n\nD) FIT aims to decrease the intra-class correlation coefficients while increasing the inter-class correlation coefficients, whereas SPUTNIK does the opposite. FIT is used for automated identification and analysis of molecules based on their spectral signatures.\n\n**Correct Answer:** B) FIT aims to decrease the inter-class correlation coefficients while increasing the intra-class correlation coefficients, whereas SPUTNIK does the opposite. FIT is used for automated identification and analysis of molecules based on their spectral signatures.\n\n**Explanation:** The correct answer is B) because the paper states that FIT \"exhibits the ability of decreasing the inter-class correlation coefficients while increasing the intra-class correlation coefficients\". This means that FIT is designed to make similar spectra of the same molecule more similar to each other, while making spectra of different molecules more dissimilar. This is in contrast to SPUTNIK, which is not mentioned as having a specific goal in terms of correlation coefficients. Additionally, the paper states that FIT is used for automated identification and analysis of molecules based on their spectral signatures, which is consistent with option B."}, "15": {"documentation": {"title": "Ultracold neutron production and up-scattering in superfluid helium\n  between 1.1 K and 2.4 K", "source": "K. K. H. Leung, S. Ivanov, F. M. Piegsa, M. Simson and O. Zimmer", "docs_id": "1507.07475", "section": ["physics.ins-det", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ultracold neutron production and up-scattering in superfluid helium\n  between 1.1 K and 2.4 K. Ultracold neutrons (UCNs) were produced in a 4 liter volume of superfluid helium using the PF1B cold neutron beam facility at the Institut Laue-Langevin and then extracted to a detector at room temperature. With a converter temperature of 1.08 K the number of accumulated UCNs was counted to be $91,\\!700 \\pm 300$. From this, we derive a volumetric UCN production rate of $(6.9 \\pm 1.7)\\,\\mathrm{cm^{-3}\\,s^{-1}}$, which includes a correction for losses in the converter during UCN extraction caused by a short storage time, but not accounting for UCN transport and detection efficiencies. The up-scattering rate of UCNs due to excitations in the superfluid was studied by scanning the temperature between 1.2-2.4 K. Using the temperature-dependent UCN production rate calculated from inelastic neutron scattering data in the analysis, the only UCN up-scattering process found to be present was from two-phonon scattering. Our analysis rules out contributions from the other scattering processes to $\\lesssim 10\\%$ of their predicted levels."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary mechanism of ultracold neutron (UCN) up-scattering in superfluid helium, as determined by the experiment described in the Arxiv documentation?\n\nA) Two-phonon scattering\nB) Three-phonon scattering\nC) Electron-phonon scattering\nD) Nuclear scattering\n\n**Correct Answer:** A) Two-phonon scattering\n\n**Explanation:** The experiment described in the Arxiv documentation used a temperature-dependent UCN production rate calculated from inelastic neutron scattering data to study the up-scattering rate of UCNs due to excitations in the superfluid. The analysis found that the only UCN up-scattering process present was from two-phonon scattering, ruling out contributions from other scattering processes to $\\lesssim 10\\%$ of their predicted levels.\n\n**Rationale for incorrect options:**\n\n* B) Three-phonon scattering: While three-phonon scattering is a possible mechanism for UCN up-scattering, the experiment found no evidence for this process.\n* C) Electron-phonon scattering: Electron-phonon scattering is not a relevant mechanism for UCN up-scattering in this experiment.\n* D) Nuclear scattering: Nuclear scattering is not a relevant mechanism for UCN up-scattering in this experiment, as the UCNs are produced in a superfluid helium environment."}, "16": {"documentation": {"title": "Optimal Portfolio Using Factor Graphical Lasso", "source": "Tae-Hwy Lee and Ekaterina Seregina", "docs_id": "2011.00435", "section": ["econ.EM", "q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Portfolio Using Factor Graphical Lasso. Graphical models are a powerful tool to estimate a high-dimensional inverse covariance (precision) matrix, which has been applied for a portfolio allocation problem. The assumption made by these models is a sparsity of the precision matrix. However, when stock returns are driven by common factors, such assumption does not hold. We address this limitation and develop a framework, Factor Graphical Lasso (FGL), which integrates graphical models with the factor structure in the context of portfolio allocation by decomposing a precision matrix into low-rank and sparse components. Our theoretical results and simulations show that FGL consistently estimates the portfolio weights and risk exposure and also that FGL is robust to heavy-tailed distributions which makes our method suitable for financial applications. FGL-based portfolios are shown to exhibit superior performance over several prominent competitors including equal-weighted and Index portfolios in the empirical application for the S&P500 constituents."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of portfolio allocation, what is the primary assumption made by traditional graphical models, and how does the proposed Factor Graphical Lasso (FGL) framework address this limitation?\n\n**A)** Traditional graphical models assume that the precision matrix is sparse, but FGL does not address this assumption. Instead, it assumes that the precision matrix is low-rank and sparse.\n**B)** Traditional graphical models assume that the precision matrix is low-rank and sparse, but FGL does not address this assumption. Instead, it assumes that the precision matrix is high-rank and dense.\n**C)** Traditional graphical models assume that the precision matrix is sparse, but FGL addresses this limitation by decomposing the precision matrix into low-rank and sparse components.\n**D)** Traditional graphical models assume that the precision matrix is low-rank and sparse, and FGL addresses this limitation by assuming that the precision matrix is high-rank and dense.\n\n**Correct Answer:** C) Traditional graphical models assume that the precision matrix is sparse, but FGL addresses this limitation by decomposing the precision matrix into low-rank and sparse components.\n\n**Explanation:** The correct answer is C) because the documentation states that traditional graphical models assume sparsity of the precision matrix, but this assumption does not hold when stock returns are driven by common factors. The Factor Graphical Lasso (FGL) framework addresses this limitation by decomposing the precision matrix into low-rank and sparse components, which allows it to estimate portfolio weights and risk exposure more accurately."}, "17": {"documentation": {"title": "Global Optimization of Offshore Wind Farm Collection Systems", "source": "Juan-Andr\\'es P\\'erez-R\\'ua, Kaushik Das, Mathias Stolpe, and Nicolaos\n  A. Cutululis", "docs_id": "1907.02371", "section": ["math.OC", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Global Optimization of Offshore Wind Farm Collection Systems. A mathematical program for global optimization of the cable layout of Offshore Wind Farms (OWFs) is presented. The model consists on a Mixed Integer Linear Program (MILP). Modern branch-and-cut solvers are able to solve large-scale instances, defined by more than hundred Wind Turbines (WTs), and a reasonable number of Offshore Substations (OSSs). In addition to the MILP model to optimize total cable length or initial investment, a pre-processing strategy is proposed in order to incorporate total electrical power losses into the objective function. High fidelity models are adapted to calculate cables current capacities, spatial currents. The MILP model is embedded in an iterative algorithmic framework, consisting in solving a sequence of problems with increasing size of the search space. The search space is defined as a set of underlying candidate arcs. The applicability of the method is illustrated through 10 case studies of real-world large-scale wind farms. Results show that: (i) feasible points can quickly be obtained in seconds, (ii) points near the global optimum with an imposed maximum tolerance, are calculable in reasonable computational time in the order of hours, and (iii) the proposed method compares favorably against a state-of-the art method available in literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A Mixed Integer Linear Program (MILP) is used to optimize the cable layout of Offshore Wind Farms (OWFs). The model is able to solve large-scale instances with more than 100 Wind Turbines (WTs) and a reasonable number of Offshore Substations (OSSs). However, the model requires a high-fidelity model to calculate cables' current capacities and spatial currents. What is the primary advantage of using a pre-processing strategy to incorporate total electrical power losses into the objective function?\n\nA) Reduced computational time for solving the MILP model\nB) Improved accuracy of cable length optimization\nC) Enhanced ability to handle large-scale instances with complex cable layouts\nD) Increased efficiency in calculating cables' current capacities and spatial currents\n\n**Correct Answer:** B) Improved accuracy of cable length optimization\n\n**Explanation:** The pre-processing strategy is proposed to incorporate total electrical power losses into the objective function, which improves the accuracy of cable length optimization. This is because the strategy takes into account the losses that occur in the electrical power transmission, allowing for a more realistic optimization of the cable layout. The other options are incorrect because they do not accurately reflect the primary advantage of using the pre-processing strategy. Option A is incorrect because the computational time for solving the MILP model is not directly affected by the pre-processing strategy. Option C is incorrect because the ability to handle large-scale instances is not the primary advantage of the pre-processing strategy. Option D is incorrect because the efficiency in calculating cables' current capacities and spatial currents is not directly related to the pre-processing strategy."}, "18": {"documentation": {"title": "Debiased Off-Policy Evaluation for Recommendation Systems", "source": "Yusuke Narita, Shota Yasui, Kohei Yata", "docs_id": "2002.08536", "section": ["cs.LG", "cs.AI", "econ.EM", "stat.ME", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Debiased Off-Policy Evaluation for Recommendation Systems. Efficient methods to evaluate new algorithms are critical for improving interactive bandit and reinforcement learning systems such as recommendation systems. A/B tests are reliable, but are time- and money-consuming, and entail a risk of failure. In this paper, we develop an alternative method, which predicts the performance of algorithms given historical data that may have been generated by a different algorithm. Our estimator has the property that its prediction converges in probability to the true performance of a counterfactual algorithm at a rate of $\\sqrt{N}$, as the sample size $N$ increases. We also show a correct way to estimate the variance of our prediction, thus allowing the analyst to quantify the uncertainty in the prediction. These properties hold even when the analyst does not know which among a large number of potentially important state variables are actually important. We validate our method by a simulation experiment about reinforcement learning. We finally apply it to improve advertisement design by a major advertisement company. We find that our method produces smaller mean squared errors than state-of-the-art methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Debiased Off-Policy Evaluation for Recommendation Systems\n\n**A)** The proposed method in the paper \"Debiased Off-Policy Evaluation for Recommendation Systems\" is an A/B testing approach, which is reliable but time- and money-consuming, and entails a risk of failure.\n\n**B)** The estimator developed in the paper has the property that its prediction converges in probability to the true performance of a counterfactual algorithm at a rate of $\\sqrt{N}$, as the sample size $N$ increases.\n\n**C)** The authors of the paper validate their method by a simulation experiment about reinforcement learning, but do not provide any real-world application of their method.\n\n**D)** The proposed method in the paper is only applicable to recommendation systems that use a single algorithm, and does not account for the impact of multiple algorithms on the system's performance.\n\n**Correct Answer:** B) The estimator developed in the paper has the property that its prediction converges in probability to the true performance of a counterfactual algorithm at a rate of $\\sqrt{N}$, as the sample size $N$ increases.\n\n**Explanation:** This question requires the test-taker to understand the key contributions of the paper, specifically the estimator's convergence rate and its ability to estimate the variance of the prediction. The correct answer is based on the statement in the paper that the estimator's prediction converges in probability to the true performance of a counterfactual algorithm at a rate of $\\sqrt{N}$, as the sample size $N$ increases. The other options are incorrect because they either misrepresent the paper's approach (A), do not mention the estimator's convergence rate (C), or are too narrow in scope (D)."}, "19": {"documentation": {"title": "Evaluation of Radiation Hardness of High-Voltage Silicon Vertical JFETs", "source": "Gabriele Giacomini, Marco Bomben, Wei Chen, David Lynn", "docs_id": "2007.10892", "section": ["physics.ins-det", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of Radiation Hardness of High-Voltage Silicon Vertical JFETs. In the future ATLAS Inner Tracker, each silicon strip module will be equipped with a switch able to separate the high voltage supply from the sensor in case the latter becomes faulty. The switch, placed in between the HV supply and the sensor, needs to sustain a high voltage in its OFF state, to offer a low resistance path for the sensor leakage current in the ON state, and be radiation hard up to 1.2e15 neq/cm2 along with other requirements. While GaN JFETs have been selected as suitable rad-hard switch, a silicon vertical HV-JFET was developed by Brookhaven National Laboratory as an alternative option. Pre-irradiation results showed the functionality of the device and proved that the silicon HV-JFET satisfied the pre-irradiation requirements for the switch. To assess its suitability after irradiation, a few p-type HV-JFETs have been neutron irradiated at Jozef Stefan Institute (JSI, Ljubljana, Slovenia). This paper reports the static characterization of these irradiated devices and the TCAD numerical simulations used to get an insight of the physics governing the post-irradiation behaviour."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the suitability of silicon vertical HV-JFETs as a radiation-hard switch for the ATLAS Inner Tracker, considering their performance after neutron irradiation.\n\n**A)** The silicon vertical HV-JFETs' ability to sustain a high voltage in their OFF state and offer a low resistance path for sensor leakage current in the ON state is compromised after neutron irradiation, making them unsuitable for the ATLAS Inner Tracker.\n\n**B)** The pre-irradiation results and TCAD numerical simulations suggest that the silicon vertical HV-JFETs' radiation hardness is not a significant concern, and they can be considered as a viable alternative to GaN JFETs for the ATLAS Inner Tracker switch.\n\n**C)** The irradiated silicon vertical HV-JFETs' performance is limited by the radiation-induced changes in their electrical properties, which can lead to a degradation in their switching characteristics and overall functionality.\n\n**D)** The TCAD numerical simulations indicate that the silicon vertical HV-JFETs' radiation hardness is comparable to that of GaN JFETs, and their suitability for the ATLAS Inner Tracker switch is not significantly affected by neutron irradiation.\n\n**Correct Answer:** D) The TCAD numerical simulations indicate that the silicon vertical HV-JFETs' radiation hardness is comparable to that of GaN JFETs, and their suitability for the ATLAS Inner Tracker switch is not significantly affected by neutron irradiation.\n\n**Explanation:** The correct answer is D) because the TCAD numerical simulations are used to assess the physics governing the post-irradiation behavior of the silicon vertical HV-JFETs. The simulations indicate that the radiation hardness of the silicon vertical HV-JFETs is comparable to that of GaN JFETs, and their suitability for the ATLAS Inner Tracker switch is not significantly affected by neutron irradiation. This is supported by the fact that the pre-irradiation results showed the functionality of the device and proved that the silicon HV-JFET satisfied the pre-irradiation requirements for the switch."}, "20": {"documentation": {"title": "Probing chiral interactions up to next-to-next-to-next-to-leading order\n  in medium-mass nuclei", "source": "J. Hoppe, C. Drischler, K. Hebeler, A. Schwenk, J. Simonis", "docs_id": "1904.12611", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Probing chiral interactions up to next-to-next-to-next-to-leading order\n  in medium-mass nuclei. We study ground-state energies and charge radii of closed-shell medium-mass nuclei based on novel chiral nucleon-nucleon (NN) and three-nucleon (3N) interactions, with a focus on exploring the connections between finite nuclei and nuclear matter. To this end, we perform in-medium similarity renormalization group (IM-SRG) calculations based on chiral interactions at next-to-leading order (NLO), N$^2$LO, and N$^3$LO, where the 3N interactions at N$^2$LO and N$^3$LO are fit to the empirical saturation point of nuclear matter and to the triton binding energy. Our results for energies and radii at N$^2$LO and N$^3$LO overlap within uncertainties, and the cutoff variation of the interactions is within the EFT uncertainty band. We find underbound ground-state energies, as expected from the comparison to the empirical saturation point. The radii are systematically too large, but the agreement with experiment is better. We further explore variations of the 3N couplings to test their sensitivity in nuclei. While nuclear matter at saturation density is quite sensitive to the 3N couplings, we find a considerably weaker dependence in medium-mass nuclei. In addition, we explore a consistent momentum-space SRG evolution of these NN and 3N interactions, exhibiting improved many-body convergence. For the SRG-evolved interactions, the sensitivity to the 3N couplings is found to be stronger in medium-mass nuclei."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion regarding the sensitivity of nuclear matter and medium-mass nuclei to the three-nucleon (3N) interactions, as inferred from the study of ground-state energies and charge radii of closed-shell medium-mass nuclei?\n\nA) The 3N interactions have a stronger dependence on the nuclear matter at saturation density, leading to a better agreement with empirical data.\nB) The 3N interactions have a weaker dependence on the nuclear matter at saturation density, but a stronger dependence on medium-mass nuclei.\nC) The 3N interactions have a similar dependence on both nuclear matter and medium-mass nuclei, with no significant differences.\nD) The 3N interactions have a negligible dependence on both nuclear matter and medium-mass nuclei, leading to a poor agreement with empirical data.\n\nCorrect Answer: B) The 3N interactions have a weaker dependence on the nuclear matter at saturation density, but a stronger dependence on medium-mass nuclei.\n\nExplanation: The study found that while nuclear matter at saturation density is quite sensitive to the 3N couplings, the sensitivity is considerably weaker in medium-mass nuclei. This suggests that the 3N interactions have a weaker dependence on the nuclear matter at saturation density, but a stronger dependence on medium-mass nuclei."}, "21": {"documentation": {"title": "Nonlinear optical response of a two-dimensional quantum dot\n  supercrystal: Emerging multistability, periodic/aperiodic self-oscillations,\n  and hyperchaos", "source": "Pablo Alvarez Zapatero, Ramil F. Malikov, Igor V. Ryzhov, Andrey V.\n  Malyshev, Victor A. Malyshev", "docs_id": "1806.00387", "section": ["physics.optics", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear optical response of a two-dimensional quantum dot\n  supercrystal: Emerging multistability, periodic/aperiodic self-oscillations,\n  and hyperchaos. We study theoretically the nonlinear optical response of a two-dimensional semiconductor quantum dot supercrystal under a resonant continuous wave excitation. A single quantum dot is modeled as a three-level ladder-like system with the ground, one-exciton, and bi-exction states. We propose an exact linear parametric method of solving the nonlinear steady-state problem. It is demonstrate that the system may exhibit multistability, periodic and aperiodic self-oscillations, and hyperchaotic behavior, depending on the system's parameters and frequency of excitation. The effects originate from the retarded dipole-dipole interaction of quantum dots. The latter provides a positive feedback which, in combination with the nonlinearity of SQDs, leads to an exotic nonlinear dynamics of the system indicated above. We discuss relevance of the underlined effects for nanosized all-optical devices. In particular, a quantum dot supercrystal may serve as a nanosized all-optical switch, a tunable generator of trains of THz pulses (in self-oscillating regime), as well as a noise generator (in chaotic regime) at the nanoscale. We show also that the supercrystal can operate as a bistable mirror. All this suggests various nanophotonic applications of such type of materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What type of nonlinear dynamics can a two-dimensional semiconductor quantum dot supercrystal exhibit under resonant continuous wave excitation, and what are some potential nanophotonic applications of such a system?\n\n**A)** Multistability, periodic self-oscillations, and chaos, with potential applications as a nanosized all-optical switch, tunable generator of THz pulses, and noise generator.\n\n**B)** Hyperchaos, periodic self-oscillations, and multistability, with potential applications as a nanosized amplifier, tunable filter, and optical memory device.\n\n**C)** Multistability, periodic self-oscillations, and hyperchaos, with potential applications as a nanosized all-optical switch, tunable generator of THz pulses, and bistable mirror.\n\n**D)** Hyperchaos, multistability, and periodic self-oscillations, with potential applications as a nanosized optical modulator, tunable filter, and optical communication device.\n\n**Correct Answer:** C) Multistability, periodic self-oscillations, and hyperchaos, with potential applications as a nanosized all-optical switch, tunable generator of THz pulses, and bistable mirror.\n\n**Explanation:** The correct answer is C) because the documentation states that the system can exhibit multistability, periodic self-oscillations, and hyperchaotic behavior, and that these effects have potential nanophotonic applications, including as a nanosized all-optical switch, tunable generator of THz pulses, and bistable mirror. The other options are incorrect because they either omit one or more of the mentioned effects or propose applications that are not mentioned in the documentation."}, "22": {"documentation": {"title": "Efficient and Flexible Crowdsourcing of Specialized Tasks with\n  Precedence Constraints", "source": "Avhishek Chatterjee, Michael Borokhovich, Lav R. Varshney, Sriram\n  Vishwanath", "docs_id": "1601.04094", "section": ["cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient and Flexible Crowdsourcing of Specialized Tasks with\n  Precedence Constraints. Many companies now use crowdsourcing to leverage external (as well as internal) crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents, due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill-sets and availabilities of different types of agents with different skill-sets. Considering these constraints together necessitates the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized and offer customers (task requesters) the freedom to choose agents. In this work we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a non-profit company and show significant improvements over current practice."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A crowdsourcing system has a task with three steps, each requiring different skills and having varying levels of dependency among parts. The steps have precedence constraints, meaning that Step 2 cannot be performed until Step 1 is completed, and Step 3 cannot be performed until Step 2 is completed. The system has three types of agents with different skill-sets, and the task loads vary across different types of agents. Design an efficient allocation scheme for this crowdsourcing system that takes into account the precedence constraints, skill-sets of agents, and task loads.\n\n**A)** Use a greedy algorithm to allocate tasks to agents based on their availability and skill-sets, without considering the precedence constraints.\n\n**B)** Implement a distributed algorithm that uses a centralized scheduler to allocate tasks to agents, taking into account the precedence constraints and task loads.\n\n**C)** Design a heuristic algorithm that uses machine learning to predict the optimal allocation of tasks to agents based on historical data and task characteristics.\n\n**D)** Propose a novel scheme that uses a combination of graph theory and constraint programming to allocate tasks to agents, taking into account the precedence constraints, skill-sets of agents, and task loads.\n\n**Correct Answer:** D) Propose a novel scheme that uses a combination of graph theory and constraint programming to allocate tasks to agents, taking into account the precedence constraints, skill-sets of agents, and task loads.\n\n**Explanation:** The correct answer requires an understanding of the complexities of crowdsourcing systems with precedence constraints, skill-sets of agents, and task loads. The proposed scheme should be able to handle the dependencies between steps, the varying skill-sets of agents, and the task loads, while also ensuring that the allocation scheme is efficient, simple, fast, decentralized, and offers customers the freedom to choose agents. The use of graph theory and constraint programming provides a powerful framework for modeling and solving the allocation problem, and can lead to optimal or near-optimal solutions."}, "23": {"documentation": {"title": "Active Gel Model of Amoeboid Cell Motility", "source": "A. C. Callan-Jones and R. Voituriez", "docs_id": "1301.7562", "section": ["physics.bio-ph", "cond-mat.soft", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active Gel Model of Amoeboid Cell Motility. We develop a model of amoeboid cell motility based on active gel theory. Modeling the motile apparatus of a eukaryotic cell as a confined layer of finite length of poroelastic active gel permeated by a solvent, we first show that, due to active stress and gel turnover, an initially static and homogeneous layer can undergo a contractile-type instability to a polarized moving state in which the rear is enriched in gel polymer. This agrees qualitatively with motile cells containing an actomyosin-rich uropod at their rear. We find that the gel layer settles into a steadily moving, inhomogeneous state at long times, sustained by a balance between contractility and filament turnover. In addition, our model predicts an optimal value of the gel-susbstrate adhesion leading to maximum layer speed, in agreement with cell motility assays. The model may be relevant to motility of cells translocating in complex, confining environments that can be mimicked experimentally by cell migration through microchannels."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which the active gel model of amoeboid cell motility predicts the rear of the cell to become enriched in gel polymer, and what is the underlying physical process that drives this phenomenon?\n\n**A)** The active stress generated by the motile apparatus causes the gel layer to contract and become more dense at the rear, leading to an accumulation of gel polymer.\n\n**B)** The solvent permeation through the gel layer creates a concentration gradient that drives the gel polymer to accumulate at the rear of the cell.\n\n**C)** The actomyosin-rich uropod at the rear of the cell generates a localized contraction that causes the gel layer to become more dense and enriched in gel polymer.\n\n**D)** The gel turnover process, where the gel polymer is constantly being replaced, leads to a concentration of gel polymer at the rear of the cell due to the preferential flow of solvent.\n\n**Correct Answer:** D) The gel turnover process, where the gel polymer is constantly being replaced, leads to a concentration of gel polymer at the rear of the cell due to the preferential flow of solvent.\n\n**Explanation:** The correct answer is D) because the gel turnover process is a key mechanism in the active gel model of amoeboid cell motility. The model predicts that the gel layer settles into a steadily moving, inhomogeneous state at long times, sustained by a balance between contractility and filament turnover. The gel turnover process is responsible for the accumulation of gel polymer at the rear of the cell, which is driven by the preferential flow of solvent. This process is essential for the cell's ability to move and maintain its polarized state.\n\nThe other options are incorrect because:\n\n* A) The active stress generated by the motile apparatus is not the primary mechanism driving the accumulation of gel polymer at the rear of the cell.\n* B) The solvent permeation through the gel layer is not the primary mechanism driving the accumulation of gel polymer at the rear of the cell.\n* C) The actomyosin-rich uropod at the rear of the cell is not the primary mechanism driving the accumulation of gel polymer at the rear of the cell, although it may play a role in the overall motility of the cell."}, "24": {"documentation": {"title": "Larmor Frequency Depends on Structural Anisotropy in Magnetically\n  Heterogeneous Media", "source": "Alexander Ruh and Valerij G. Kiselev", "docs_id": "1808.10171", "section": ["cond-mat.soft", "physics.bio-ph", "physics.class-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Larmor Frequency Depends on Structural Anisotropy in Magnetically\n  Heterogeneous Media. Purpose: To investigate the effect of anisotropic magnetic microstructure on the measurable Larmor frequency offset in media with heterogeneous magnetic susceptibility. Specific objectives were (i) validation of recently developed theory for the case of fast diffusion and (ii) investigation of the transition between the regimes of fast and slow diffusion. Methods: Monte Carlo simulations in synthetic media. Results: Simulations demonstrate a perfect agreement with the previously developed theory for fast diffusion. Beyond this regime, the frequency offset shows a pronounced dependence on the medium microarchitecture and the diffusivity of NMR-reporting spins in relation to the magnitude of the susceptibility-induced magnetic field. Conclusion: While the effect of myelin in brain white matter is commonly treated assuming efficient diffusion narrowing, this regime does not hold for larger cells or higher magnetic susceptibility. In such a case, the effect essentially deviates from the prediction based on the assumption of diffusion narrowing."}, "answer": "Here is the difficult multi-form question:\n\n**Question:** In the context of the Larmor Frequency Depends on Structural Anisotropy in Magnetically Heterogeneous Media paper, what is the primary conclusion regarding the effect of myelin in brain white matter on the measurable Larmor frequency offset?\n\nA) The effect of myelin is negligible and can be treated as efficient diffusion narrowing.\nB) The effect of myelin is significant and deviates from the prediction based on the assumption of diffusion narrowing.\nC) The effect of myelin is only relevant in media with heterogeneous magnetic susceptibility.\nD) The effect of myelin is only relevant in media with fast diffusion.\n\n**Correct Answer:** B) The effect of myelin is significant and deviates from the prediction based on the assumption of diffusion narrowing.\n\n**Explanation:** The paper states that while the effect of myelin in brain white matter is commonly treated assuming efficient diffusion narrowing, the regime of fast diffusion does not hold for larger cells or higher magnetic susceptibility. This implies that the effect of myelin deviates from the prediction based on the assumption of diffusion narrowing. Therefore, option B is the correct answer."}, "25": {"documentation": {"title": "Study of scalar meson a_0(1450) from B \\to a_0(1450)K^* Decays", "source": "Zhi-Qing Zhang", "docs_id": "1106.0368", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Study of scalar meson a_0(1450) from B \\to a_0(1450)K^* Decays. In the two-quark model supposition for the meson $a_0(1450)$, which can be viewed as either the first excited state (scenario I) or the lowest lying state (scenario II), the branching ratios and the direct CP-violating asymmetries for decays $B^-\\to a^{0}_0(1450)K^{*-}, a^{-}_0(1450)K^{*0}$ and $\\bar B^0\\to a^{+}_0(1450)K^{*-}, a^{0}_0(1450)\\bar K^{*0}$ are studied by employing the perturbative QCD factorization approach. We find the following results: (a) For the decays $B^-\\to a^{-}_0(1450)K^{*0}, \\bar B^0\\to a^{+}_0(1450)K^{*-}, a^{0}_0(1450)\\bar K^{*0}$, their branching ratios in scenario II are larger than those in scenario I about one order. So it is easy for the experiments to differentiate between the scenario I and II for the meson $a_0(1450)$. (b)For the decay $B^-\\to a^{0}_0(1450)K^{*-}$, due to not receiving the enhancement from the $K^*-$emission factorizable diagrams, its penguin operator contributions are the smallest in scenario II, which makes its branching ratio drop into the order of $10^{-6}$. Even so, its branching ratio in scenario II is still larger than that in scenario I about 2.5 times. (c) Even though our predictions are much larger than those from the QCD factorization results, they are still consistent with each other within the large theoretical errors from the annihilation diagrams. (d) We predict the direct CP- violating asymmetry of the decay $B^-\\to a^{-}_0(1450)K^{*0}$ is small and only a few percent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the two-quark model, the meson $a_0(1450)$ can be viewed as either the first excited state (scenario I) or the lowest lying state (scenario II). Using the perturbative QCD factorization approach, the branching ratios and direct CP-violating asymmetries for decays $B^-\\to a^{0}_0(1450)K^{*-}, a^{-}_0(1450)K^{*0}$ and $\\bar B^0\\to a^{+}_0(1450)K^{*-}, a^{0}_0(1450)\\bar K^{*0}$ are studied. What can be concluded about the relative sizes of the branching ratios in scenario II compared to scenario I?\n\nA) The branching ratios in scenario II are smaller than those in scenario I.\nB) The branching ratios in scenario II are larger than those in scenario I.\nC) The branching ratios in scenario II are equal to those in scenario I.\nD) The branching ratios in scenario II cannot be compared to those in scenario I.\n\nCorrect Answer: B) The branching ratios in scenario II are larger than those in scenario I.\n\nExplanation: According to the text, the branching ratios in scenario II are larger than those in scenario I by about one order, making it easier for experiments to differentiate between the two scenarios."}, "26": {"documentation": {"title": "Experimental observation of topological Fermi arcs in type-II Weyl\n  semimetal MoTe2", "source": "Ke Deng, Guoliang Wan, Peng Deng, Kenan Zhang, Shijie Ding, Eryin\n  Wang, Mingzhe Yan, Huaqing Huang, Hongyun Zhang, Zhilin Xu, Jonathan\n  Denlinger, Alexei Fedorov, Haitao Yang, Wenhui Duan, Hong Yao, Yang Wu,\n  Shoushan Fan, Haijun Zhang, Xi Chen, and Shuyun Zhou", "docs_id": "1603.08508", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental observation of topological Fermi arcs in type-II Weyl\n  semimetal MoTe2. Weyl semimetal is a new quantum state of matter [1-12] hosting the condensed matter physics counterpart of relativisticWeyl fermion [13] originally introduced in high energy physics. The Weyl semimetal realized in the TaAs class features multiple Fermi arcs arising from topological surface states [10, 11, 14-16] and exhibits novel quantum phenomena, e.g., chiral anomaly induced negative mag-netoresistance [17-19] and possibly emergent supersymmetry [20]. Recently it was proposed theoretically that a new type (type-II) of Weyl fermion [21], which does not have counterpart in high energy physics due to the breaking of Lorentz invariance, can emerge as topologically-protected touching between electron and hole pockets. Here, we report direct spectroscopic evidence of topological Fermi arcs in the predicted type-II Weyl semimetal MoTe2 [22-24]. The topological surface states are confirmed by directly observing the surface states using bulk-and surface-sensitive angle-resolved photoemission spectroscopy (ARPES), and the quasi-particle interference (QPI) pattern between the two putative Fermi arcs in scanning tunneling microscopy (STM). Our work establishes MoTe2 as the first experimental realization of type-II Weyl semimetal, and opens up new opportunities for probing novel phenomena such as exotic magneto-transport [21] in type-II Weyl semimetals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary experimental evidence for the existence of topological Fermi arcs in the type-II Weyl semimetal MoTe2, and how does it relate to the theoretical proposal of a new type of Weyl fermion?\n\nA) The observation of chiral anomaly-induced negative magnetoresistance in MoTe2, which is a novel quantum phenomenon that can be used to probe the topological surface states.\n\nB) The direct spectroscopic evidence of topological Fermi arcs in MoTe2, which was obtained using bulk-and surface-sensitive angle-resolved photoemission spectroscopy (ARPES) and scanning tunneling microscopy (STM).\n\nC) The theoretical proposal of a new type of Weyl fermion, which does not have a counterpart in high-energy physics due to the breaking of Lorentz invariance, and its predicted emergence as topologically-protected touching between electron and hole pockets.\n\nD) The observation of quasi-particle interference (QPI) patterns between the two putative Fermi arcs in MoTe2, which provides further evidence for the existence of topological surface states.\n\nCorrect Answer: B) The direct spectroscopic evidence of topological Fermi arcs in MoTe2, which was obtained using bulk-and surface-sensitive angle-resolved photoemission spectroscopy (ARPES) and scanning tunneling microscopy (STM).\n\nExplanation: The correct answer is B) because the documentation specifically states that the topological surface states were confirmed by directly observing the surface states using bulk-and surface-sensitive angle-resolved photoemission spectroscopy (ARPES) and scanning tunneling microscopy (STM). This provides direct spectroscopic evidence for the existence of topological Fermi arcs in MoTe2. The other options are incorrect because they either describe a related phenomenon (chiral anomaly-induced negative magnetoresistance) or a theoretical proposal (new type of Weyl fermion), but do not provide direct evidence for the existence of topological Fermi arcs."}, "27": {"documentation": {"title": "Correlated local bending of DNA double helix and its effect on the\n  cyclization of short DNA fragments", "source": "Xinliang Xu, Beng Joo Reginald, Jianshu Cao", "docs_id": "1309.7515", "section": ["cond-mat.soft", "physics.bio-ph", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlated local bending of DNA double helix and its effect on the\n  cyclization of short DNA fragments. We report a theoretical study of DNA flexibility and quantitatively predict the ring closure probability as a function of DNA contour length. Recent experimental studies show that the flexibility of short DNA fragments (as compared to the persistence length of DNA l_P~150 base pairs) cannot be described by the traditional worm-like chain (WLC) model, e.g., the observed ring closure probability is much higher than predicted. To explain these observations, DNA flexibility is investigated with explicit considerations of a new length scale l_D~10 base pairs, over which DNA local bend angles are correlated. In this correlated worm-like chain (C-WLC) model, a finite length correction term is analytically derived and the persistence length is found to be contour length dependent. While our model reduces to the traditional worm-like chain model when treating long DNA at length scales much larger than l_P, it predicts that DNA becomes much more flexible at shorter sizes, which helps explain recent cyclization measurements of short DNA fragments around 100 base pairs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main difference between the traditional worm-like chain (WLC) model and the correlated worm-like chain (C-WLC) model proposed in the study on DNA flexibility?\n\nA) The C-WLC model assumes a fixed persistence length, while the WLC model assumes a variable persistence length.\nB) The C-WLC model uses a finite length correction term to account for correlated local bend angles, while the WLC model does not.\nC) The C-WLC model is only applicable to short DNA fragments, while the WLC model is applicable to long DNA.\nD) The C-WLC model predicts a constant ring closure probability, while the WLC model predicts a variable ring closure probability.\n\n**Correct Answer:** B) The C-WLC model uses a finite length correction term to account for correlated local bend angles, while the WLC model does not.\n\n**Explanation:** The study proposes a new length scale, l_D~10 base pairs, to investigate DNA flexibility, which is not accounted for in the traditional WLC model. The C-WLC model introduces a finite length correction term to capture the correlated local bend angles, leading to a contour length-dependent persistence length. This difference is crucial in explaining the observed higher ring closure probability in short DNA fragments compared to the predictions of the WLC model."}, "28": {"documentation": {"title": "The Next Geminga: Deep Multiwavelength Observations of a Neutron Star\n  Identified with 3EG J1835+5918", "source": "J. P. Halpern, E. V. Gotthelf, N. Mirabal, F. Camilo", "docs_id": "astro-ph/0205442", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Next Geminga: Deep Multiwavelength Observations of a Neutron Star\n  Identified with 3EG J1835+5918. We describe Chandra, HST, and radio observations that reveal a radio-quiet but magnetospherically active neutron star in the error circle of the high-energy gamma-ray source 3EG J1835+5918, the brightest of the unidentified EGRET sources at high Galactic latitude. A Chandra ACIS-S spectrum of the ultrasoft X-ray source RX J1836.2+5925, suggested by Mirabal & Halpern as the neutron star counterpart of 3EG J1835+5918, requires two components: a blackbody of T~3x10^5 K and a hard tail that can be parameterized as a power law of photon index Gamma~2. An upper limit of d < 800 pc can be derived from the blackbody fit under an assumption of R = 10 km. Deep optical imaging with the HST STIS CCD failed to detect this source to a limit of V > 28.5, thus f_X/f_V > 6000 and d > 250 pc assuming the X-ray fitted temperature for the full surface. Repeated observations with the 76 m Lovell telescope at Jodrell Bank place an upper limit of < 0.1 mJy on the flux density at 1400 MHz for a pulsar with P > 0.1 s, and < 0.25 mJy for a ~10 ms pulsar at the location of RX J1836.2+5925. All of this evidence points to an older, possibly more distant version of the highly efficient gamma-ray pulsar Geminga, as the origin of the gamma-rays from 3EG J1835+5918."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the distance of the neutron star RX J1836.2+5925 from the observations of its X-ray spectrum and optical imaging?\n\nA) The distance is less than 250 pc, as the X-ray spectrum requires a temperature of T ~ 3x10^5 K and the optical imaging fails to detect the source to a limit of V > 28.5.\n\nB) The distance is greater than 800 pc, as the X-ray spectrum requires a blackbody fit with a radius of R = 10 km and the optical imaging fails to detect the source to a limit of V > 28.5.\n\nC) The distance is approximately 250-800 pc, as the X-ray spectrum requires a temperature of T ~ 3x10^5 K and the optical imaging fails to detect the source to a limit of V > 28.5, but the exact distance cannot be determined.\n\nD) The distance is less than 250 pc, as the X-ray spectrum requires a temperature of T ~ 3x10^5 K and the optical imaging fails to detect the source to a limit of V > 28.5, and the flux density at 1400 MHz is less than 0.1 mJy.\n\nCorrect Answer: C) The distance is approximately 250-800 pc, as the X-ray spectrum requires a temperature of T ~ 3x10^5 K and the optical imaging fails to detect the source to a limit of V > 28.5, but the exact distance cannot be determined.\n\nExplanation: The X-ray spectrum requires a temperature of T ~ 3x10^5 K, which corresponds to a radius of R = 10 km. However, the optical imaging fails to detect the source to a limit of V > 28.5, which implies that the source is not visible at a distance of less than 250 pc. Additionally, the flux density at 1400 MHz is less than 0.1 mJy, which suggests that the source is not too luminous. Combining these constraints, we can infer that the distance of the neutron star RX J1836.2+5925 is approximately 250-800 pc, but the exact distance cannot be determined."}, "29": {"documentation": {"title": "Rheology of Lamellar Liquid Crystals in Two and Three Dimensions: A\n  Simulation Study", "source": "O. Henrich, K. Stratford, D. Marenduzzo, P. V. Coveney, M. E. Cates", "docs_id": "1112.3251", "section": ["cond-mat.soft", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "nlin.CG", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rheology of Lamellar Liquid Crystals in Two and Three Dimensions: A\n  Simulation Study. We present large scale computer simulations of the nonlinear bulk rheology of lamellar phases (smectic liquid crystals) at moderate to large values of the shear rate (Peclet numbers 10-100), in both two and three dimensions. In two dimensions we find that modest shear rates align the system and stabilise an almost regular lamellar phase, but high shear rates induce the nucleation and proliferation of defects, which in steady state is balanced by the annihilation of defects of opposite sign. The critical shear rate at onset of this second regime is controlled by thermodynamic and kinetic parameters; we offer a scaling analysis that relates the critical shear rate to a critical \"capillary number\" involving those variables. Within the defect proliferation regime, the defects may be partially annealed by slowly decreasing the applied shear rate; this causes marked memory effects, and history-dependent rheology. Simulations in three dimensions show instead shear-induced ordering even at the highest shear rates studied here. This suggests that the critical shear rate shifts markedly upward on increasing dimensionality. This may in part reflect the reduced constraints on defect motion, allowing them to find and annihilate each other more easily. Residual edge defects in the 3D aligned state mostly point along the flow velocity, an orientation impossible in two dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary effect of increasing the dimensionality on the critical shear rate at onset of the defect proliferation regime in lamellar liquid crystals?\n\nA) The critical shear rate remains unchanged\nB) The critical shear rate decreases\nC) The critical shear rate shifts markedly upward\nD) The critical shear rate becomes independent of dimensionality\n\n**Correct Answer:** C) The critical shear rate shifts markedly upward\n\n**Explanation:** According to the text, simulations in three dimensions show shear-induced ordering even at the highest shear rates studied, suggesting that the critical shear rate shifts markedly upward on increasing dimensionality. This is because the reduced constraints on defect motion allow them to find and annihilate each other more easily.\n\n**Candidate A (Incorrect)**: The text does not suggest that the critical shear rate remains unchanged with increasing dimensionality.\n\n**Candidate B (Incorrect)**: The text does not suggest that the critical shear rate decreases with increasing dimensionality.\n\n**Candidate D (Incorrect)**: The text actually suggests that the critical shear rate becomes more dependent on dimensionality, rather than becoming independent of it."}, "30": {"documentation": {"title": "Everlasting Secrecy by Exploiting Non-Idealities of the Eavesdropper's\n  Receiver", "source": "Azadeh Sheikholeslami, Dennis Goeckel and Hossein Pishro-Nik", "docs_id": "1210.1790", "section": ["cs.CR", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Everlasting Secrecy by Exploiting Non-Idealities of the Eavesdropper's\n  Receiver. Secure communication over a memoryless wiretap channel in the presence of a passive eavesdropper is considered. Traditional information-theoretic security methods require an advantage for the main channel over the eavesdropper channel to achieve a positive secrecy rate, which in general cannot be guaranteed in wireless systems. Here, we exploit the non-linear conversion operation in the eavesdropper's receiver to obtain the desired advantage - even when the eavesdropper has perfect access to the transmitted signal at the input to their receiver. The basic idea is to employ an ephemeral cryptographic key to force the eavesdropper to conduct two operations, at least one of which is non-linear, in a different order than the desired recipient. Since non-linear operations are not necessarily commutative, the desired advantage can be obtained and information-theoretic secrecy achieved even if the eavesdropper is given the cryptographic key immediately upon transmission completion. In essence, the lack of knowledge of the key during the short transmission time inhibits the recording of the signal in such a way that the secret information can never be extracted from it. The achievable secrecy rates for different countermeasures that the eavesdropper might employ are evaluated. It is shown that even in the case of an eavesdropper with uniformly better conditions (channel and receiver quality) than the intended recipient, a positive secure rate can be achieved."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Everlasting Secrecy by Exploiting Non-Idealities of the Eavesdropper's Receiver\", what is the primary advantage of employing an ephemeral cryptographic key in the eavesdropper's receiver, and how does it enable the achievement of information-theoretic secrecy?\n\n**A)** The ephemeral key allows the eavesdropper to conduct non-linear operations in a different order than the intended recipient, thereby ensuring that the secret information is not extracted from the signal.\n\n**B)** The ephemeral key enables the eavesdropper to conduct linear operations in a different order than the intended recipient, thereby ensuring that the secret information is not extracted from the signal.\n\n**C)** The ephemeral key allows the eavesdropper to conduct non-linear operations in the same order as the intended recipient, thereby ensuring that the secret information is not extracted from the signal.\n\n**D)** The ephemeral key has no effect on the eavesdropper's ability to extract secret information from the signal, and therefore does not enable the achievement of information-theoretic secrecy.\n\n**Correct Answer:** A) The ephemeral key allows the eavesdropper to conduct non-linear operations in a different order than the intended recipient, thereby ensuring that the secret information is not extracted from the signal.\n\n**Explanation:** The correct answer is A) because the ephemeral key enables the eavesdropper to conduct non-linear operations in a different order than the intended recipient, which is the primary advantage of employing an ephemeral cryptographic key in the eavesdropper's receiver. This allows the eavesdropper to be unable to extract the secret information from the signal, even if they have perfect access to the transmitted signal at the input to their receiver. The other options are incorrect because they do not accurately describe the advantage of employing an ephemeral key in the eavesdropper's receiver."}, "31": {"documentation": {"title": "Pretzel: Email encryption and provider-supplied functions are compatible", "source": "Trinabh Gupta, Henrique Fingler, Lorenzo Alvisi, Michael Walfish", "docs_id": "1612.04265", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pretzel: Email encryption and provider-supplied functions are compatible. Emails today are often encrypted, but only between mail servers---the vast majority of emails are exposed in plaintext to the mail servers that handle them. While better than no encryption, this arrangement leaves open the possibility of attacks, privacy violations, and other disclosures. Publicly, email providers have stated that default end-to-end encryption would conflict with essential functions (spam filtering, etc.), because the latter requires analyzing email text. The goal of this paper is to demonstrate that there is no conflict. We do so by designing, implementing, and evaluating Pretzel. Starting from a cryptographic protocol that enables two parties to jointly perform a classification task without revealing their inputs to each other, Pretzel refines and adapts this protocol to the email context. Our experimental evaluation of a prototype demonstrates that email can be encrypted end-to-end \\emph{and} providers can compute over it, at tolerable cost: clients must devote some storage and processing, and provider overhead is roughly 5 times versus the status quo."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Pretzel protocol, and how does it address the concerns of email providers regarding default end-to-end encryption?\n\nA) To enable email providers to analyze email text without compromising user privacy, while maintaining the security of email encryption.\nB) To demonstrate that email providers can compute over encrypted emails without compromising user privacy, while maintaining the security of email encryption.\nC) To enable email providers to perform spam filtering and other essential functions without revealing user inputs to each other, while maintaining the security of email encryption.\nD) To enable email providers to analyze email text without compromising user privacy, while maintaining the security of email encryption and enabling providers to compute over encrypted emails.\n\n**Correct Answer:** B) To demonstrate that email providers can compute over encrypted emails without compromising user privacy, while maintaining the security of email encryption.\n\n**Explanation:** The correct answer is B) because the Pretzel protocol refines and adapts a cryptographic protocol to enable email providers to compute over encrypted emails without revealing user inputs to each other. This addresses the concerns of email providers regarding default end-to-end encryption, which they claimed would conflict with essential functions such as spam filtering. The experimental evaluation of the prototype demonstrates that email can be encrypted end-to-end and providers can compute over it at tolerable cost."}, "32": {"documentation": {"title": "Multidimensional Contrast Limited Adaptive Histogram Equalization", "source": "Vincent Stimper, Stefan Bauer, Ralph Ernstorfer, Bernhard Sch\\\"olkopf,\n  R. Patrick Xian", "docs_id": "1906.11355", "section": ["eess.IV", "eess.SP", "physics.data-an", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multidimensional Contrast Limited Adaptive Histogram Equalization. Contrast enhancement is an important preprocessing technique for improving the performance of downstream tasks in image processing and computer vision. Among the existing approaches based on nonlinear histogram transformations, contrast limited adaptive histogram equalization (CLAHE) is a popular choice for dealing with 2D images obtained in natural and scientific settings. The recent hardware upgrade in data acquisition systems results in significant increase in data complexity, including their sizes and dimensions. Measurements of densely sampled data higher than three dimensions, usually composed of 3D data as a function of external parameters, are becoming commonplace in various applications in the natural sciences and engineering. The initial understanding of these complex multidimensional datasets often requires human intervention through visual examination, which may be hampered by the varying levels of contrast permeating through the dimensions. We show both qualitatively and quantitatively that using our multidimensional extension of CLAHE (MCLAHE) simultaneously on all dimensions of the datasets allows better visualization and discernment of multidimensional image features, as demonstrated using cases from 4D photoemission spectroscopy and fluorescence microscopy. Our implementation of multidimensional CLAHE in Tensorflow is publicly accessible and supports parallelization with multiple CPUs and various other hardware accelerators, including GPUs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the Multidimensional Contrast Limited Adaptive Histogram Equalization (MCLAHE) technique in image processing and computer vision, particularly in dealing with complex multidimensional datasets?\n\n**A)** MCLAHE enhances the contrast of individual dimensions, allowing for better visualization of features in each dimension separately.\n**B)** MCLAHE simultaneously processes all dimensions of a dataset, enabling the detection of subtle patterns and features that may be obscured by varying levels of contrast across dimensions.\n**C)** MCLAHE is a linear histogram transformation technique that does not require any additional processing steps, making it a simple and efficient approach for image enhancement.\n**D)** MCLAHE is only suitable for 2D images and does not have any benefits for 3D or higher-dimensional datasets.\n\n**Correct Answer:** B) MCLAHE simultaneously processes all dimensions of a dataset, enabling the detection of subtle patterns and features that may be obscured by varying levels of contrast across dimensions.\n\n**Explanation:** The correct answer, B, highlights the key benefit of MCLAHE in dealing with complex multidimensional datasets. By simultaneously processing all dimensions of a dataset, MCLAHE allows for the detection of subtle patterns and features that may be obscured by varying levels of contrast across dimensions. This is particularly important in applications such as 4D photoemission spectroscopy and fluorescence microscopy, where human intervention through visual examination may be hampered by the varying levels of contrast permeating through the dimensions."}, "33": {"documentation": {"title": "Practical Channel Estimation and Phase Shift Design for Intelligent\n  Reflecting Surface Empowered MIMO Systems", "source": "Sucheol Kim, Hyeongtaek Lee, Jihoon Cha, Sung-Jin Kim, Jaeyong Park,\n  and Junil Choi", "docs_id": "2104.14161", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Practical Channel Estimation and Phase Shift Design for Intelligent\n  Reflecting Surface Empowered MIMO Systems. In this paper, channel estimation techniques and phase shift design for intelligent reflecting surface (IRS)-empowered single-user multiple-input multiple-output (SU-MIMO) systems are proposed. Among four channel estimation techniques developed in the paper, the two novel ones, single-path approximated channel (SPAC) and selective emphasis on rank-one matrices (SEROM), have low training overhead to enable practical IRS-empowered SU-MIMO systems. SPAC is mainly based on parameter estimation by approximating IRS-related channels as dominant single-path channels. SEROM exploits IRS phase shifts as well as training signals for channel estimation and easily adjusts its training overhead. A closed-form solution for IRS phase shift design is also developed to maximize spectral efficiency where the solution only requires basic linear operations. Numerical results show that SPAC and SEROM combined with the proposed IRS phase shift design achieve high spectral efficiency even with low training overhead compared to existing methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of intelligent reflecting surface (IRS)-empowered single-user multiple-input multiple-output (SU-MIMO) systems, what is the primary advantage of the selective emphasis on rank-one matrices (SEROM) channel estimation technique over the single-path approximated channel (SPAC) technique?\n\n**A)** SEROM has a higher computational complexity than SPAC.\n**B)** SEROM requires more training signals than SPAC.\n**C)** SEROM can effectively exploit IRS phase shifts and training signals to achieve more accurate channel estimation, while SPAC relies on approximating IRS-related channels as dominant single-path channels.\n**D)** SEROM is more susceptible to interference from external sources than SPAC.\n\n**Correct Answer:** C) SEROM can effectively exploit IRS phase shifts and training signals to achieve more accurate channel estimation, while SPAC relies on approximating IRS-related channels as dominant single-path channels.\n\n**Explanation:** The correct answer, C, highlights the key advantage of SEROM over SPAC. SEROM's ability to exploit IRS phase shifts and training signals enables more accurate channel estimation, whereas SPAC relies on a simpler approximation of IRS-related channels. This difference in approach is a critical aspect of the paper's contribution to the field of IRS-empowered SU-MIMO systems."}, "34": {"documentation": {"title": "Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation", "source": "Jie Chen, Yu Zeng (Corresponding author)", "docs_id": "1808.09856", "section": ["stat.ML", "cs.LG", "physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation. With recent progress in algorithms and the availability of massive amounts of computation power, application of machine learning techniques is becoming a hot topic in the oil and gas industry. One of the most promising aspects to apply machine learning to the upstream field is the rock facies classification in reservoir characterization, which is crucial in determining the net pay thickness of reservoirs, thus a definitive factor in drilling decision making process. For complex machine learning tasks like facies classification, feature engineering is often critical. This paper shows the inclusion of physics-motivated feature interaction in feature augmentation can further improve the capability of machine learning in rock facies classification. We demonstrate this approach with the SEG 2016 machine learning contest dataset and the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$ better than current existing best F-1 score, where F-1 is an evaluation metric used to quantify average prediction accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the application of physics-motivated feature augmentation in rock facies classification, and how does it improve the performance of machine learning algorithms in this task?\n\n**A)** The primary motivation is to reduce the dimensionality of the feature space, and the augmentation improves performance by increasing the diversity of the training data. However, this approach may not be robust to changes in the underlying physics of the rock formations.\n\n**B)** The primary motivation is to incorporate domain knowledge into the feature engineering process, and the augmentation improves performance by capturing the complex interactions between physical properties of the rocks. This approach has been shown to be robust and can lead to significant improvements in F-1 score, with an average increase of $\\sim5\\%$ compared to existing best algorithms.\n\n**C)** The primary motivation is to improve the interpretability of the machine learning models, and the augmentation improves performance by reducing the impact of noise in the feature space. However, this approach may not be effective for complex rock formations with multiple facies.\n\n**D)** The primary motivation is to reduce the computational cost of training machine learning models, and the augmentation improves performance by using a smaller subset of the training data. However, this approach may not be effective for large datasets with many features.\n\n**Correct Answer:** B) The primary motivation is to incorporate domain knowledge into the feature engineering process, and the augmentation improves performance by capturing the complex interactions between physical properties of the rocks. This approach has been shown to be robust and can lead to significant improvements in F-1 score, with an average increase of $\\sim5\\%$ compared to existing best algorithms."}, "35": {"documentation": {"title": "beamModelTester: software framework for testing radio telescope beams", "source": "Ois\\'in Creaner, Tobia D. Carozzi", "docs_id": "1908.02141", "section": ["astro-ph.IM", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "beamModelTester: software framework for testing radio telescope beams. The flux, polarimetric and spectral response of phased array radio telescopes with no moving parts such as LOFAR is known to vary considerably with orientation of the source to the receivers. Calibration models exist for this dependency such as those that are used in the LOFAR pipeline. Presented here is a system for comparing the predicted outputs from any given model with the results of an observation. In this paper, a sample observation of a bright source, Cassiopeia A, is used to demonstrate the software in operation, by providing an observation and a model of that observation which can be compared with one another. The package presented here is flexible to allow it to be used with other models and sources. The system operates by first calculating the predictions of the model and the results of an observation of linear fluxes and Stokes parameters separately. The model and observed values are then joined using the variables common to both, time and frequency. Normalisation and RFI excision are carried out and the differences between the prediction and the observation are calculated. A wide selection of 2-, 3- and 4-dimensional plots are generated to illustrate the dependence of the model and the observation as well as the difference between them on independent parameters time, frequency, altitude and azimuth. Thus, beamModelTester provides a framework by which it is possible to calibrate and propose refinements to models and to compare models with one another."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the flux, polarimetric, and spectral response of phased array radio telescopes with no moving parts, such as LOFAR, to the predicted outputs from the beamModelTester software framework.\n\n**A)** The flux, polarimetric, and spectral response of phased array radio telescopes with no moving parts vary significantly with the orientation of the source to the receivers, and the beamModelTester software framework is not suitable for calibrating models in this scenario.\n\n**B)** The beamModelTester software framework is flexible and can be used to compare the predicted outputs from any given model with the results of an observation, including phased array radio telescopes with no moving parts.\n\n**C)** The beamModelTester software framework is primarily designed for calibrating models of bright sources, such as Cassiopeia A, and is not suitable for calibrating models of faint sources.\n\n**D)** The beamModelTester software framework generates 2-, 3-, and 4-dimensional plots to illustrate the dependence of the model and the observation on independent parameters, including time, frequency, altitude, and azimuth.\n\n**Correct Answer:** B) The beamModelTester software framework is flexible and can be used to compare the predicted outputs from any given model with the results of an observation, including phased array radio telescopes with no moving parts.\n\n**Explanation:** The correct answer is B) because the documentation states that the beamModelTester software framework is \"flexible to allow it to be used with other models and sources\", including phased array radio telescopes with no moving parts. The framework is designed to compare predicted outputs from any given model with the results of an observation, making it suitable for calibrating models of phased array radio telescopes. The other options are incorrect because they either misrepresent the flexibility of the framework (A), are too narrow in scope (C), or focus on a specific aspect of the framework's functionality (D)."}, "36": {"documentation": {"title": "Self-Enhanced GNN: Improving Graph Neural Networks Using Model Outputs", "source": "Han Yang, Xiao Yan, Xinyan Dai, Yongqiang Chen, James Cheng", "docs_id": "2002.07518", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Self-Enhanced GNN: Improving Graph Neural Networks Using Model Outputs. Graph neural networks (GNNs) have received much attention recently because of their excellent performance on graph-based tasks. However, existing research on GNNs focuses on designing more effective models without considering much about the quality of the input data. In this paper, we propose self-enhanced GNN (SEG), which improves the quality of the input data using the outputs of existing GNN models for better performance on semi-supervised node classification. As graph data consist of both topology and node labels, we improve input data quality from both perspectives. For topology, we observe that higher classification accuracy can be achieved when the ratio of inter-class edges (connecting nodes from different classes) is low and propose topology update to remove inter-class edges and add intra-class edges. For node labels, we propose training node augmentation, which enlarges the training set using the labels predicted by existing GNN models. SEG is a general framework that can be easily combined with existing GNN models. Experimental results validate that SEG consistently improves the performance of well-known GNN models such as GCN, GAT and SGC across different datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the Self-Enhanced GNN (SEG) framework, and how does it address the limitations of existing graph neural networks (GNNs)?\n\nA) SEG aims to reduce the complexity of GNNs by removing unnecessary layers, and it improves the quality of input data by adding intra-class edges and removing inter-class edges.\n\nB) SEG focuses on designing more effective GNN models by incorporating additional layers and increasing the number of parameters, and it enhances the quality of input data by augmenting node labels with predicted labels from existing GNN models.\n\nC) SEG seeks to improve the performance of GNNs on semi-supervised node classification tasks by enhancing the quality of input data from both topology and node label perspectives, specifically by reducing the ratio of inter-class edges and augmenting node labels with predicted labels.\n\nD) SEG aims to simplify the training process of GNNs by using pre-trained models and reducing the need for manual feature engineering, and it improves the quality of input data by adding noise to the training data.\n\nCorrect Answer: C) SEG seeks to improve the performance of GNNs on semi-supervised node classification tasks by enhancing the quality of input data from both topology and node label perspectives, specifically by reducing the ratio of inter-class edges and augmenting node labels with predicted labels.\n\nExplanation: The correct answer (C) accurately summarizes the primary goal of SEG, which is to improve the quality of input data for GNNs. SEG achieves this by addressing two aspects of input data quality: topology and node labels. By reducing the ratio of inter-class edges and adding intra-class edges, SEG improves the topology of the graph. Additionally, by augmenting node labels with predicted labels from existing GNN models, SEG enhances the quality of node labels. This approach is in line with the paper's statement that SEG improves input data quality from both perspectives."}, "37": {"documentation": {"title": "Microfluidic multipoles: theory and applications", "source": "Pierre-Alexandre Goyette, \\'Etienne Boulais, Fr\\'ed\\'eric Normandeau,\n  Gabriel Laberge, David Juncker, Thomas Gervais", "docs_id": "1810.01578", "section": ["physics.flu-dyn", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microfluidic multipoles: theory and applications. Microfluidic multipoles (MFMs) have been realized experimentally and hold promise for \"open-space\" biological and chemical surface processing. Whereas convective flow can readily be predicted using hydraulic-electrical analogies, the design of advanced MFMs is constrained by the lack of simple, accurate models to predict mass transport within them. In this work, we introduce the first exact solutions to mass transport in multipolar microfluidics based on the iterative conformal mapping of 2D advection-diffusion around a simple edge into dipoles and multipolar geometries, revealing a rich landscape of transport modes. The models were validated experimentally with a library of 3D printed MFM devices and found in excellent agreement. Following a theory-guided design approach, we further ideated and fabricated two new classes of spatiotemporally reconfigurable MFM devices that are used for processing surfaces with time-varying reagent streams, and to realize a multistep automated immunoassay. Overall, the results set the foundations for exploring, developing, and applying open-space MFMs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation in designing advanced Microfluidic Multipoles (MFMs) according to the study, and how do the authors address this limitation?\n\nA) The lack of simple, accurate models to predict mass transport within MFMs is due to the complexity of the microfluidic environment, and the authors address this by using numerical simulations.\nB) The authors introduce the first exact solutions to mass transport in MFMs based on the iterative conformal mapping of 2D advection-diffusion around a simple edge into dipoles and multipolar geometries.\nC) The primary limitation is the difficulty in predicting convective flow using hydraulic-electrical analogies, and the authors address this by using experimental validation.\nD) The authors use a theory-guided design approach to develop new classes of spatiotemporally reconfigurable MFM devices, but do not address the limitation of mass transport models.\n\n**Correct Answer:** B) The authors introduce the first exact solutions to mass transport in MFMs based on the iterative conformal mapping of 2D advection-diffusion around a simple edge into dipoles and multipolar geometries.\n\n**Explanation:** The question requires the test-taker to understand the primary limitation in designing advanced MFMs, which is the lack of simple, accurate models to predict mass transport within them. The correct answer highlights the authors' innovative approach to addressing this limitation by introducing exact solutions to mass transport in MFMs. The other options are incorrect because they either misrepresent the limitation or the authors' approach. Option A is incorrect because the complexity of the microfluidic environment is not the primary limitation, but rather the lack of accurate models. Option C is incorrect because convective flow is not the primary limitation, but rather mass transport. Option D is incorrect because the theory-guided design approach is a separate aspect of the study, not a solution to the limitation of mass transport models."}, "38": {"documentation": {"title": "Dynamic Power Allocation and User Scheduling for Power-Efficient and\n  Low-Latency Communications", "source": "Minseok Choi, Joongheon Kim and Jaekyun Moon", "docs_id": "1807.00682", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Power Allocation and User Scheduling for Power-Efficient and\n  Low-Latency Communications. In this paper, we propose a joint dynamic power control and user pairing algorithm for power-efficient and low-latency hybrid multiple access systems. In a hybrid multiple access system, user pairing determines whether the transmitter should serve a certain user by orthogonal multiple access (OMA) or non-orthogonal multiple access (NOMA). The proposed optimization framework minimizes the long-term time-average transmit power expenditure while reducing the queueing delay and satisfying time-average data rate requirements. The proposed technique observes channel and queue state information and adjusts queue backlogs to avoid an excessive queueing delay by appropriate user pairing and power allocation. Further, user scheduling for determining the activation of a given user link as well as flexible use of resources are captured in the proposed algorithm. Data-intensive simulation results show that the proposed scheme guarantees an end-to-end delay smaller than 1 ms with high power-efficiency and high reliability, based on the short frame structure designed for ultra-reliable low-latency communications (URLLC)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the proposed joint dynamic power control and user pairing algorithm for power-efficient and low-latency communications, what is the primary objective of the optimization framework?\n\nA) To minimize the long-term time-average queueing delay while satisfying time-average data rate requirements\nB) To maximize the long-term time-average transmit power expenditure while reducing queueing delay\nC) To minimize the long-term time-average transmit power expenditure while reducing queueing delay and satisfying time-average data rate requirements\nD) To guarantee an end-to-end delay smaller than 1 ms with high power-efficiency and high reliability\n\n**Correct Answer:** C) To minimize the long-term time-average transmit power expenditure while reducing queueing delay and satisfying time-average data rate requirements\n\n**Explanation:** The correct answer is C) To minimize the long-term time-average transmit power expenditure while reducing queueing delay and satisfying time-average data rate requirements. This is because the proposed algorithm aims to minimize the long-term time-average transmit power expenditure, while also reducing queueing delay and satisfying time-average data rate requirements. The other options are incorrect because they either focus on minimizing queueing delay (A and D), maximizing transmit power expenditure (B), or guaranteeing a specific delay (D), which is not the primary objective of the optimization framework.\n\n**Additional explanations:**\n\n* Option A is incorrect because while minimizing queueing delay is an important aspect of the algorithm, it is not the primary objective.\n* Option B is incorrect because maximizing transmit power expenditure is actually the opposite of what the algorithm aims to achieve.\n* Option D is incorrect because guaranteeing an end-to-end delay smaller than 1 ms is a specific performance metric, but not the primary objective of the optimization framework."}, "39": {"documentation": {"title": "Signal and noise in regime systems: a hypothesis on the predictability\n  of the North Atlantic Oscillation", "source": "Kristian Strommen, Tim N. Palmer", "docs_id": "1904.13322", "section": ["physics.ao-ph", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Signal and noise in regime systems: a hypothesis on the predictability\n  of the North Atlantic Oscillation. Studies conducted by the UK Met Office reported significant skill at predicting the winter NAO index with their seasonal prediction system. At the same time, a very low signal-to-noise ratio was observed, as measured using the `ratio of predictable components' (RPC) metric. We analyse both the skill and signal-to-noise ratio using a new statistical toy-model which assumes NAO predictability is driven by regime dynamics. It is shown that if the system is approximately bimodal in nature, with the model consistently underestimating the level of regime persistence each season, then both the high skill and high RPC value of the Met Office hindcasts can easily be reproduced. Underestimation of regime persistence could be attributable to any number of sources of model error, including imperfect regime structure or errors in the propagation of teleconnections. In particular, a high RPC value for a seasonal mean prediction may be expected even if the models internal level of noise is realistic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The UK Met Office's seasonal prediction system for the North Atlantic Oscillation (NAO) exhibits significant skill in predicting the winter NAO index, but also shows a very low signal-to-noise ratio, as measured by the ratio of predictable components (RPC) metric. Analyze the possible reasons for this apparent contradiction, considering the following statements:\n\nA) The Met Office's model is too simplistic and fails to capture the complex regime dynamics driving the NAO.\nB) The RPC metric is flawed and does not accurately measure the signal-to-noise ratio in the prediction system.\nC) The Met Office's model consistently underestimates the level of regime persistence each season, leading to an overestimation of the skill and an underestimation of the signal-to-noise ratio.\nD) The RPC metric is sensitive to the specific seasonal mean prediction used, and the Met Office's model is actually performing well when using a different prediction method.\n\n**Correct Answer:** C) The Met Office's model consistently underestimates the level of regime persistence each season, leading to an overestimation of the skill and an underestimation of the signal-to-noise ratio.\n\n**Explanation:** The correct answer is C) because the text states that the Met Office's model consistently underestimates the level of regime persistence each season, which can lead to an overestimation of the skill (i.e., the ability to predict the NAO index) and an underestimation of the signal-to-noise ratio (i.e., the ratio of predictable components). This underestimation of regime persistence could be due to various sources of model error, including imperfect regime structure or errors in the propagation of teleconnections."}, "40": {"documentation": {"title": "Secure Massive MIMO Transmission with an Active Eavesdropper", "source": "Yongpeng Wu, Robert Schober, Derrick Wing Kwan Ng, Chengshan Xiao, and\n  Giuseppe Caire", "docs_id": "1507.00789", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secure Massive MIMO Transmission with an Active Eavesdropper. In this paper, we investigate secure and reliable transmission strategies for multi-cell multi-user massive multiple-input multiple-output (MIMO) systems with a multi-antenna active eavesdropper. We consider a time-division duplex system where uplink training is required and an active eavesdropper can attack the training phase to cause pilot contamination at the transmitter. This forces the precoder used in the subsequent downlink transmission phase to implicitly beamform towards the eavesdropper, thus increasing its received signal power. Assuming matched filter precoding and artificial noise (AN) generation at the transmitter, we derive an asymptotic achievable secrecy rate when the number of transmit antennas approaches infinity. For the case of a single-antenna active eavesdropper, we obtain a closed-form expression for the optimal power allocation policy for the transmit signal and the AN, and find the minimum transmit power required to ensure reliable secure communication. Furthermore, we show that the transmit antenna correlation diversity of the intended users and the eavesdropper can be exploited in order to improve the secrecy rate. In fact, under certain orthogonality conditions of the channel covariance matrices, the secrecy rate loss introduced by the eavesdropper can be completely mitigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Secure Massive MIMO Transmission with an Active Eavesdropper, what is the optimal power allocation policy for the transmit signal and artificial noise (AN) generation at the transmitter, and what is the minimum transmit power required to ensure reliable secure communication when the number of transmit antennas approaches infinity?\n\n**A)** The optimal power allocation policy is to allocate the maximum power to the transmit signal and minimize the power of AN, resulting in a minimum transmit power of 0 dBm.\n\n**B)** The optimal power allocation policy is to allocate the minimum power to the transmit signal and maximize the power of AN, resulting in a minimum transmit power of 10 dBm.\n\n**C)** The optimal power allocation policy is to allocate the power to the transmit signal and AN in a way that satisfies the orthogonality conditions of the channel covariance matrices, resulting in a minimum transmit power that can be mitigated by exploiting the transmit antenna correlation diversity of the intended users and the eavesdropper.\n\n**D)** The optimal power allocation policy is to allocate the power to the transmit signal and AN in a way that maximizes the secrecy rate, resulting in a minimum transmit power that can be achieved by using a matched filter precoder.\n\n**Correct Answer:** C) The optimal power allocation policy is to allocate the power to the transmit signal and AN in a way that satisfies the orthogonality conditions of the channel covariance matrices, resulting in a minimum transmit power that can be mitigated by exploiting the transmit antenna correlation diversity of the intended users and the eavesdropper.\n\n**Explanation:** The correct answer is C) because the paper states that under certain orthogonality conditions of the channel covariance matrices, the secrecy rate loss introduced by the eavesdropper can be completely mitigated. This implies that the optimal power allocation policy is to allocate the power to the transmit signal and AN in a way that satisfies these conditions, resulting in a minimum transmit power that can be mitigated by exploiting the transmit antenna correlation diversity of the intended users and the eavesdropper."}, "41": {"documentation": {"title": "GLocal-K: Global and Local Kernels for Recommender Systems", "source": "Soyeon Caren Han, Taejun Lim, Siqu Long, Bernd Burgstaller, Josiah\n  Poon", "docs_id": "2108.12184", "section": ["cs.IR", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "GLocal-K: Global and Local Kernels for Recommender Systems. Recommender systems typically operate on high-dimensional sparse user-item matrices. Matrix completion is a very challenging task to predict one's interest based on millions of other users having each seen a small subset of thousands of items. We propose a Global-Local Kernel-based matrix completion framework, named GLocal-K, that aims to generalise and represent a high-dimensional sparse user-item matrix entry into a low dimensional space with a small number of important features. Our GLocal-K can be divided into two major stages. First, we pre-train an auto encoder with the local kernelised weight matrix, which transforms the data from one space into the feature space by using a 2d-RBF kernel. Then, the pre-trained auto encoder is fine-tuned with the rating matrix, produced by a convolution-based global kernel, which captures the characteristics of each item. We apply our GLocal-K model under the extreme low-resource setting, which includes only a user-item rating matrix, with no side information. Our model outperforms the state-of-the-art baselines on three collaborative filtering benchmarks: ML-100K, ML-1M, and Douban."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the Global-Local Kernel-based matrix completion framework, GLocal-K, proposed in the paper \"Global and Local Kernels for Recommender Systems\"?\n\n**A)** To reduce the dimensionality of the user-item matrix while preserving the global patterns of user behavior.\n**B)** To improve the interpretability of the autoencoder by incorporating a 2D-RBF kernel.\n**C)** To enhance the performance of collaborative filtering algorithms by leveraging both local and global item characteristics.\n**D)** To develop a novel method for matrix factorization that can handle high-dimensional sparse user-item matrices.\n\n**Correct Answer:** C) To enhance the performance of collaborative filtering algorithms by leveraging both local and global item characteristics.\n\n**Explanation:** The paper proposes a Global-Local Kernel-based matrix completion framework, GLocal-K, which aims to generalize and represent a high-dimensional sparse user-item matrix entry into a low-dimensional space with a small number of important features. The framework consists of two major stages: pre-training an autoencoder with a local kernelized weight matrix and fine-tuning it with a rating matrix produced by a convolution-based global kernel. The primary objective of GLocal-K is to enhance the performance of collaborative filtering algorithms by leveraging both local and global item characteristics, making option C the correct answer."}, "42": {"documentation": {"title": "CRIX an index for cryptocurrencies", "source": "Simon Trimborn, Wolfgang Karl H\\\"ardle", "docs_id": "2009.09782", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CRIX an index for cryptocurrencies. The cryptocurrency market is unique on many levels: Very volatile, frequently changing market structure, emerging and vanishing of cryptocurrencies on a daily level. Following its development became a difficult task with the success of cryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the IMF offers the index SDR and, prior to the EUR, the ECU existed, which was an index representing the development of European currencies. Index providers decide on a fixed number of index constituents which will represent the market segment. It is a challenge to fix a number and develop rules for the constituents in view of the market changes. In the frequently changing CC market, this challenge is even more severe. A method relying on the AIC is proposed to quickly react to market changes and therefore enable us to create an index, referred to as CRIX, for the cryptocurrency market. CRIX is chosen by model selection such that it represents the market well to enable each interested party studying economic questions in this market and to invest into the market. The diversified nature of the CC market makes the inclusion of altcoins in the index product critical to improve tracking performance. We have shown that assigning optimal weights to altcoins helps to reduce the tracking errors of a CC portfolio, despite the fact that their market cap is much smaller relative to Bitcoin. The codes used here are available via www.quantlet.de."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main challenge in creating an index for the cryptocurrency market, and how does it differ from traditional fiat currency markets?\n\nA) The cryptocurrency market is too small to be tracked, and the index should only include Bitcoin.\nB) The cryptocurrency market is too volatile, and the index should only include stablecoins.\nC) The cryptocurrency market is too diverse, and the index should include a fixed number of constituents to represent the market well.\nD) The cryptocurrency market is too complex, and the index should rely on machine learning algorithms to track its movements.\n\n**Correct Answer:** C) The cryptocurrency market is too diverse, and the index should include a fixed number of constituents to represent the market well.\n\n**Explanation:** The correct answer is C) The cryptocurrency market is too diverse, and the index should include a fixed number of constituents to represent the market well. This is because the cryptocurrency market is characterized by its high volatility and frequent changes in market structure, making it challenging to determine the optimal number of constituents for an index. The proposed method using the AIC (Akaike Information Criterion) aims to quickly react to market changes and create an index that represents the market well, despite its diversity."}, "43": {"documentation": {"title": "Improving Robustness using Joint Attention Network For Detecting Retinal\n  Degeneration From Optical Coherence Tomography Images", "source": "Sharif Amit Kamran, Alireza Tavakkoli, Stewart Lee Zuckerbrod", "docs_id": "2005.08094", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving Robustness using Joint Attention Network For Detecting Retinal\n  Degeneration From Optical Coherence Tomography Images. Noisy data and the similarity in the ocular appearances caused by different ophthalmic pathologies pose significant challenges for an automated expert system to accurately detect retinal diseases. In addition, the lack of knowledge transferability and the need for unreasonably large datasets limit clinical application of current machine learning systems. To increase robustness, a better understanding of how the retinal subspace deformations lead to various levels of disease severity needs to be utilized for prioritizing disease-specific model details. In this paper we propose the use of disease-specific feature representation as a novel architecture comprised of two joint networks -- one for supervised encoding of disease model and the other for producing attention maps in an unsupervised manner to retain disease specific spatial information. Our experimental results on publicly available datasets show the proposed joint-network significantly improves the accuracy and robustness of state-of-the-art retinal disease classification networks on unseen datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge faced by automated expert systems in detecting retinal diseases from optical coherence tomography images, and how does the proposed joint-network architecture address this challenge?\n\n**A)** The primary challenge is the lack of knowledge transferability between different ophthalmic pathologies, and the proposed joint-network addresses this by using disease-specific feature representation.\n\n**B)** The primary challenge is the similarity in ocular appearances caused by different ophthalmic pathologies, and the proposed joint-network addresses this by producing attention maps in an unsupervised manner to retain disease-specific spatial information.\n\n**C)** The primary challenge is the need for unreasonably large datasets, and the proposed joint-network addresses this by using supervised encoding of disease models.\n\n**D)** The primary challenge is the lack of robustness of current machine learning systems, and the proposed joint-network addresses this by utilizing retinal subspace deformations to prioritize disease-specific model details.\n\n**Correct Answer:** B) The primary challenge is the similarity in ocular appearances caused by different ophthalmic pathologies, and the proposed joint-network addresses this by producing attention maps in an unsupervised manner to retain disease-specific spatial information.\n\n**Explanation:** The correct answer is B) because the documentation states that \"noisy data and the similarity in the ocular appearances caused by different ophthalmic pathologies pose significant challenges for an automated expert system to accurately detect retinal diseases.\" The proposed joint-network architecture addresses this challenge by producing attention maps in an unsupervised manner to retain disease-specific spatial information, as mentioned in the documentation."}, "44": {"documentation": {"title": "Towards Axiomatic Explanations for Neural Ranking Models", "source": "Michael V\\\"olske, Alexander Bondarenko, Maik Fr\\\"obe, Matthias Hagen,\n  Benno Stein, Jaspreet Singh, Avishek Anand", "docs_id": "2106.08019", "section": ["cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Axiomatic Explanations for Neural Ranking Models. Recently, neural networks have been successfully employed to improve upon state-of-the-art performance in ad-hoc retrieval tasks via machine-learned ranking functions. While neural retrieval models grow in complexity and impact, little is understood about their correspondence with well-studied IR principles. Recent work on interpretability in machine learning has provided tools and techniques to understand neural models in general, yet there has been little progress towards explaining ranking models. We investigate whether one can explain the behavior of neural ranking models in terms of their congruence with well understood principles of document ranking by using established theories from axiomatic IR. Axiomatic analysis of information retrieval models has formalized a set of constraints on ranking decisions that reasonable retrieval models should fulfill. We operationalize this axiomatic thinking to reproduce rankings based on combinations of elementary constraints. This allows us to investigate to what extent the ranking decisions of neural rankers can be explained in terms of retrieval axioms, and which axioms apply in which situations. Our experimental study considers a comprehensive set of axioms over several representative neural rankers. While the existing axioms can already explain the particularly confident ranking decisions rather well, future work should extend the axiom set to also cover the other still \"unexplainable\" neural IR rank decisions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Towards Axiomatic Explanations for Neural Ranking Models\" aim to investigate whether neural ranking models can be explained in terms of their congruence with well-understood principles of document ranking. According to the paper, what is the primary goal of the authors' axiomatic analysis of information retrieval models?\n\nA) To develop new neural ranking models that outperform existing ones\nB) To formalize a set of constraints on ranking decisions that reasonable retrieval models should fulfill\nC) To investigate the relationship between neural ranking models and human judgment in document ranking\nD) To compare the performance of different neural ranking models on a specific dataset\n\n**Correct Answer:** B) To formalize a set of constraints on ranking decisions that reasonable retrieval models should fulfill\n\n**Explanation:** The correct answer is B) because the paper states that the authors aim to \"investigate whether one can explain the behavior of neural ranking models in terms of their congruence with well understood principles of document ranking by using established theories from axiomatic IR\". This implies that the primary goal of the authors' axiomatic analysis is to formalize a set of constraints on ranking decisions that reasonable retrieval models should fulfill."}, "45": {"documentation": {"title": "Ramsey interferometry with atoms and molecules: two-body versus\n  many-body phenomena", "source": "Krzysztof Goral, Thorsten Koehler, Keith Burnett", "docs_id": "cond-mat/0407627", "section": ["cond-mat.other", "physics.atom-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ramsey interferometry with atoms and molecules: two-body versus\n  many-body phenomena. We discuss the frequency and visibility of atom-molecule Ramsey fringes observed in recent experiments by Claussen et al.[Phys. Rev. A 67, 060701 (2003)]. In these experiments a 85Rb Bose-Einstein condensate was exposed to a sequence of magnetic field pulses on the high field side of the 155 G Feshbach resonance. The observed oscillation frequencies largely agree with the theoretically predicted magnetic field dependence of the binding energy of the highest excited diatomic vibrational state, except for a small region very close to the singularity of the scattering length. Our analytic treatment of the experiment, as well as our dynamical simulations, follow the magnitude of the measured oscillation frequencies as well as the visibilities of the Ramsey fringes. We show that significant deviations from a purely binary dynamics, with an associated binding frequency, occur when the spatial extent of the molecular wave function becomes comparable with the mean distance between the atoms in the dilute gas. The experiments thus clearly identify the conditions under which diatomic molecules may be identified as a separate entity of the gas or, conversely, when the concept of binary physics in a many-body environment is bound to break down."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Ramsey interferometry with atoms and molecules, what is the primary limitation of the binary dynamics approach when dealing with diatomic molecules in a dilute gas, as identified by the experiments of Claussen et al.?\n\nA) The binary dynamics approach is limited by the finite lifetime of the molecular state.\nB) The binary dynamics approach is limited by the spatial extent of the molecular wave function becoming comparable with the mean distance between the atoms in the gas.\nC) The binary dynamics approach is limited by the high field side of the Feshbach resonance.\nD) The binary dynamics approach is limited by the low field side of the Feshbach resonance.\n\nCorrect Answer: B) The binary dynamics approach is limited by the spatial extent of the molecular wave function becoming comparable with the mean distance between the atoms in the gas.\n\nExplanation: The correct answer is B) because the text states that \"significant deviations from a purely binary dynamics, with an associated binding frequency, occur when the spatial extent of the molecular wave function becomes comparable with the mean distance between the atoms in the dilute gas.\" This indicates that the primary limitation of the binary dynamics approach is when the molecular wave function becomes comparable in size to the mean distance between the atoms, leading to a breakdown in the concept of binary physics in a many-body environment."}, "46": {"documentation": {"title": "Detecting unseen visual relations using analogies", "source": "Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic", "docs_id": "1812.05736", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detecting unseen visual relations using analogies. We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as \"person riding dog\", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in detecting visual relations in images, and how does the proposed approach address this challenge?\n\nA) The primary challenge is collecting sufficient training data for all possible visual relations, and the proposed approach addresses this by learning a representation of visual relations that combines individual embeddings and visual phrase embeddings.\n\nB) The primary challenge is understanding the meaning of individual entities, and the proposed approach addresses this by learning individual embeddings for subject, object, and predicate.\n\nC) The primary challenge is transferring visual phrase embeddings from existing training triplets to unseen test triplets, and the proposed approach addresses this by using analogies between relations that involve similar objects.\n\nD) The primary challenge is retrieving unseen triplets with out-of-vocabulary predicates, and the proposed approach addresses this by demonstrating the benefits of the approach on three challenging datasets.\n\nCorrect Answer: A) The primary challenge is collecting sufficient training data for all possible visual relations, and the proposed approach addresses this by learning a representation of visual relations that combines individual embeddings and visual phrase embeddings.\n\nExplanation: The question requires the test-taker to understand the primary challenge in detecting visual relations in images, which is the combinatorial nature of visual relations and the difficulty of collecting sufficient training data for all possible triplets. The correct answer, A, highlights the proposed approach's ability to address this challenge by learning a representation of visual relations that combines individual embeddings and visual phrase embeddings. The other options are incorrect because they focus on specific aspects of the approach (B) or a specific application of the approach (C and D), rather than the primary challenge it addresses."}, "47": {"documentation": {"title": "The interplay of inhibitory and electrical synapses results in complex\n  persistent activity", "source": "R. Janaki and A. S. Vytheeswaran", "docs_id": "2104.02794", "section": ["q-bio.NC", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The interplay of inhibitory and electrical synapses results in complex\n  persistent activity. Inhibitory neurons play a crucial role in maintaining persistent neuronal activity. Although connected extensively through electrical synapses (gap-junctions), these neurons also exhibit interactions through chemical synapses in certain regions of the brain. When the coupling is sufficiently strong, the effects of these two synaptic modalities combine in a nonlinear way. Hence, in this work, we focus on the strong inhibition regime and identify the parametric conditions that result in the emergence of self-sustained oscillations in systems of coupled excitable neurons, in the presence of a brief sub-threshold stimulus. Our investigation on the dynamics in a minimal network of two neurons reveals a rich set of dynamical behaviors viz., periodic and various complex oscillations including period-n (n=2,4,8...) dynamics and chaos. We further extend our study by considering a system of inhibitory neurons arranged in a one-dimensional ring topology and determine the optimal conditions for sustained activity. Our work highlights the nonlinear dynamical behavior arising due to the combined effects of gap-junctions and strong synaptic inhibition, which can have potential implications in maintaining robust memory patterns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary role of inhibitory neurons in maintaining persistent neuronal activity, and how do they interact with electrical synapses in the brain?\n\n**A)** Inhibitory neurons play a crucial role in generating new neuronal activity, while electrical synapses facilitate the transmission of signals between neurons.\n\n**B)** Inhibitory neurons regulate the strength of electrical synapses, allowing for the fine-tuning of neuronal activity, while electrical synapses enable the synchronized firing of neurons.\n\n**C)** Inhibitory neurons maintain persistent neuronal activity by suppressing the activity of other neurons, while electrical synapses enable the transmission of signals between neurons through gap-junctions.\n\n**D)** Inhibitory neurons are responsible for generating new neuronal activity, while electrical synapses facilitate the transmission of signals between neurons through chemical synapses.\n\n**Correct Answer:** C) Inhibitory neurons maintain persistent neuronal activity by suppressing the activity of other neurons, while electrical synapses enable the transmission of signals between neurons through gap-junctions.\n\n**Explanation:** The correct answer is C) because inhibitory neurons play a crucial role in maintaining persistent neuronal activity by suppressing the activity of other neurons, while electrical synapses enable the transmission of signals between neurons through gap-junctions. This is evident from the provided documentation, which states that inhibitory neurons \"play a crucial role in maintaining persistent neuronal activity\" and that the effects of inhibitory and electrical synapses combine in a nonlinear way to result in complex persistent activity."}, "48": {"documentation": {"title": "Exact solutions of semiclassical non-characteristic Cauchy problems for\n  the sine-Gordon equation", "source": "Robert Buckingham Peter D. Miller", "docs_id": "0705.3159", "section": ["nlin.SI", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact solutions of semiclassical non-characteristic Cauchy problems for\n  the sine-Gordon equation. The use of the sine-Gordon equation as a model of magnetic flux propagation in Josephson junctions motivates studying the initial-value problem for this equation in the semiclassical limit in which the dispersion parameter $\\e$ tends to zero. Assuming natural initial data having the profile of a moving $-2\\pi$ kink at time zero, we analytically calculate the scattering data of this completely integrable Cauchy problem for all $\\e>0$ sufficiently small, and further we invert the scattering transform to calculate the solution for a sequence of arbitrarily small $\\e$. This sequence of exact solutions is analogous to that of the well-known $N$-soliton (or higher-order soliton) solutions of the focusing nonlinear Schr\\\"odinger equation. Plots of exact solutions for small $\\e$ reveal certain features that emerge in the semiclassical limit. For example, in the limit $\\epsilon\\to 0$ one observes the appearance of nonlinear caustics. In the appendices we give a self contained account of the Cauchy problem from the perspectives of both inverse scattering and classical analysis (Picard iteration). Specifically, Appendix A contains a complete formulation of the inverse-scattering method for generic $L^1$-Sobolev initial data, and Appendix B establishes the well-posedness for $L^p$-Sobolev initial data (which in particular completely justifies the inverse-scattering analysis in Appendix A)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the sine-Gordon equation in the semiclassical limit, where the dispersion parameter $\\epsilon$ tends to zero. What is the physical significance of the appearance of nonlinear caustics in the exact solutions for small $\\epsilon$, and how do these caustics relate to the well-posedness of the Cauchy problem for the sine-Gordon equation?\n\nA) Nonlinear caustics represent the breakdown of the semiclassical approximation, leading to a loss of integrability. This is a consequence of the ill-posedness of the Cauchy problem for the sine-Gordon equation.\n\nB) Nonlinear caustics are a manifestation of the emergence of solitons in the semiclassical limit, which in turn implies the well-posedness of the Cauchy problem for the sine-Gordon equation.\n\nC) Nonlinear caustics are a result of the interaction between the initial kink and the semiclassical approximation, leading to a breakdown of the solution's regularity. This is a consequence of the ill-posedness of the Cauchy problem for the sine-Gordon equation.\n\nD) Nonlinear caustics represent the formation of a stable soliton in the semiclassical limit, which in turn implies the well-posedness of the Cauchy problem for the sine-Gordon equation.\n\nCorrect Answer: C) Nonlinear caustics are a result of the interaction between the initial kink and the semiclassical approximation, leading to a breakdown of the solution's regularity. This is a consequence of the ill-posedness of the Cauchy problem for the sine-Gordon equation.\n\nExplanation: The appearance of nonlinear caustics in the exact solutions for small $\\epsilon$ is a consequence of the interaction between the initial kink and the semiclassical approximation. This interaction leads to a breakdown of the solution's regularity, which in turn implies the ill-posedness of the Cauchy problem for the sine-Gordon equation. The correct answer, C, reflects this understanding. The other options are incorrect because they either misinterpret the physical significance of nonlinear caustics or imply the well-posedness of the Cauchy problem, which is not supported by the documentation."}, "49": {"documentation": {"title": "GeoChemFoam: Operator Splitting based time-stepping for efficient\n  Volume-Of-Fluid simulation of capillary-dominated two-phase flow", "source": "Julien Maes and Hannah P. Menke", "docs_id": "2105.10576", "section": ["physics.flu-dyn", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "GeoChemFoam: Operator Splitting based time-stepping for efficient\n  Volume-Of-Fluid simulation of capillary-dominated two-phase flow. We present a novel time-stepping method, called Operator Splitting with Capillary Relaxation (OSCAR), for efficient Volume-Of-Fluid simulations of capillary-dominated two-phase flow. OSCAR uses operator splitting methods to separate the viscous drag and the surface tension forces. Different time-steps are used for the viscous drag steps, controlled by the injection velocity, and for the capillary relaxation steps, controlled by the velocity of capillary waves. Although OSCAR induces an additional numerical error of order 0 in time resulting from the splitting, it is well suited for simulations at low capillary number. First, the splitting error decreases with the capillary number and at low capillary number, the relaxation steps converge before reaching their last iteration, resulting in a large speed-up (here up to 250x) compared to standard time-stepping methods. The method is implemented in GeoChemFoam, our OpenFOAM-based CFD solver. Convergence, accuracy and efficiency are demonstrated on three benchmark cases: (1) the steady motion of an air bubble in a straight 2D microchannel, (2) injection of supercritical CO2 in a 3D constricted channel leading to a snap-off, and (3) water drainage in a 2D oil-wet micromodel representing a porous media."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Operator Splitting with Capillary Relaxation (OSCAR) method in Volume-Of-Fluid simulations of capillary-dominated two-phase flow, and how does it compare to standard time-stepping methods in terms of efficiency?\n\nA) OSCAR reduces the numerical error of order 0 in time, but at the cost of increased computational complexity.\nB) OSCAR induces an additional numerical error of order 0 in time, but it is well-suited for simulations at low capillary number, resulting in a significant speed-up compared to standard time-stepping methods.\nC) OSCAR is only suitable for simulations at high capillary number, and it does not provide any efficiency benefits compared to standard time-stepping methods.\nD) OSCAR is a deterministic method that always converges to the exact solution, but it is not suitable for simulations with high uncertainty.\n\nCorrect Answer: B) OSCAR induces an additional numerical error of order 0 in time, but it is well-suited for simulations at low capillary number, resulting in a significant speed-up compared to standard time-stepping methods.\n\nExplanation: The correct answer is B) OSCAR induces an additional numerical error of order 0 in time, but it is well-suited for simulations at low capillary number, resulting in a significant speed-up compared to standard time-stepping methods. This is because the splitting error decreases with the capillary number, and at low capillary number, the relaxation steps converge before reaching their last iteration, resulting in a large speed-up (up to 250x) compared to standard time-stepping methods."}, "50": {"documentation": {"title": "Estimating Redundancy in Clinical Text", "source": "Thomas Searle, Zina Ibrahim, James Teo, Richard JB Dobson", "docs_id": "2105.11832", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating Redundancy in Clinical Text. The current mode of use of Electronic Health Record (EHR) elicits text redundancy. Clinicians often populate new documents by duplicating existing notes, then updating accordingly. Data duplication can lead to a propagation of errors, inconsistencies and misreporting of care. Therefore, quantifying information redundancy can play an essential role in evaluating innovations that operate on clinical narratives. This work is a quantitative examination of information redundancy in EHR notes. We present and evaluate two strategies to measure redundancy: an information-theoretic approach and a lexicosyntactic and semantic model. We evaluate the measures by training large Transformer-based language models using clinical text from a large openly available US-based ICU dataset and a large multi-site UK based Trust. By comparing the information-theoretic content of the trained models with open-domain language models, the language models trained using clinical text have shown ~1.5x to ~3x less efficient than open-domain corpora. Manual evaluation shows a high correlation with lexicosyntactic and semantic redundancy, with averages ~43 to ~65%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind quantifying information redundancy in Electronic Health Record (EHR) notes, and how does it relate to the evaluation of innovations that operate on clinical narratives?\n\n**A)** To improve the accuracy of clinical decision-making by reducing the propagation of errors and inconsistencies in EHR notes.\n**B)** To enhance the efficiency of natural language processing (NLP) models in clinical text analysis by identifying redundant information.\n**C)** To facilitate the development of more effective language models that can learn from clinical text data and improve patient outcomes.\n**D)** To provide a more comprehensive understanding of the impact of data duplication on the quality of clinical care.\n\n**Correct Answer:** A) To improve the accuracy of clinical decision-making by reducing the propagation of errors and inconsistencies in EHR notes.\n\n**Explanation:** The question requires the test-taker to understand the context of the paper and the importance of quantifying information redundancy in EHR notes. The correct answer, A, highlights the primary motivation behind this effort, which is to improve the accuracy of clinical decision-making by reducing errors and inconsistencies in EHR notes. This is a critical aspect of clinical care, as errors and inconsistencies can lead to misdiagnosis, mismanagement of patient care, and ultimately, harm to patients.\n\nThe incorrect options, B, C, and D, are plausible but not directly related to the primary motivation behind quantifying information redundancy. Option B is related to the evaluation of NLP models, but it is not the primary motivation. Option C is related to the development of language models, but it is not directly connected to the motivation behind quantifying information redundancy. Option D is a tangential aspect of the paper, but it is not the primary motivation.\n\nTo answer this question correctly, the test-taker needs to demonstrate a deep understanding of the paper's context, motivations, and implications, as well as the ability to analyze and evaluate the information presented. This requires a high level of critical thinking, analytical skills, and the ability to identify the most relevant information.\n**Additional marks for explanation:** 10"}, "51": {"documentation": {"title": "Measuring growth index in a universe with massive neutrinos: A revisit\n  of the general relativity test with the latest observations", "source": "Ming-Ming Zhao, Jing-Fei Zhang, Xin Zhang", "docs_id": "1710.02391", "section": ["astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measuring growth index in a universe with massive neutrinos: A revisit\n  of the general relativity test with the latest observations. We make a consistency test for the general relativity (GR) through measuring the growth index $\\gamma$ in a universe with massive (sterile/active) neutrinos. We employ the redshift space distortion measurements to do the analysis. To constrain other cosmological parameters, we also use other cosmological measurements, including the Planck 2015 cosmic microwave background temperature and polarization data, the baryon acoustic oscillation data, the type Ia supernova JLA data, the weak lensing galaxy shear data, and the Planck 2015 lensing data. In a universe with massive sterile neutrinos, we obtain $\\gamma=0.624^{+0.055}_{-0.050}$, with the tension with the GR prediction $\\gamma=0.55$ at the 1.48$\\sigma$ level, showing that the consideration of sterile neutrinos still cannot make the true measurement of $\\gamma$ be well consistent with the GR prediction. In a universe with massive active neutrinos, we obtain $\\gamma=0.663\\pm0.045$ for the normal hierarchy case, $\\gamma=0.661^{+0.044}_{-0.050}$ for the degenerate hierarchy case, and $\\gamma=0.668^{+0.045}_{-0.051}$ for the inverted hierarchy case, with the tensions with GR all at beyond the 2$\\sigma$ level. We find that the consideration of massive active neutrinos (no matter what mass hierarchy is considered) almost does not influence the measurement of the growth index $\\gamma$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Measuring growth index in a universe with massive neutrinos: A revisit of the general relativity test with the latest observations\" find that the growth index \u03b3 is constrained by various cosmological measurements. However, they also observe tensions between their measurements and the predictions of general relativity (GR). Which of the following statements best summarizes the authors' findings regarding the impact of massive sterile neutrinos on the measurement of \u03b3?\n\nA) The inclusion of massive sterile neutrinos leads to a significant improvement in the consistency between \u03b3 and the GR prediction.\nB) The consideration of massive sterile neutrinos results in a 1.48\u03c3 tension with the GR prediction, indicating that the true measurement of \u03b3 is still not well consistent with the GR prediction.\nC) The authors find that the inclusion of massive sterile neutrinos has a negligible impact on the measurement of \u03b3, and the tension with the GR prediction is beyond 2\u03c3.\nD) The authors conclude that the growth index \u03b3 is well consistent with the GR prediction, regardless of the presence of massive sterile neutrinos.\n\n**Correct Answer:** B) The consideration of massive sterile neutrinos results in a 1.48\u03c3 tension with the GR prediction, indicating that the true measurement of \u03b3 is still not well consistent with the GR prediction.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that the consideration of massive sterile neutrinos still cannot make the true measurement of \u03b3 be well consistent with the GR prediction, with a tension at the 1.48\u03c3 level. This indicates that the inclusion of massive sterile neutrinos does not resolve the tension between the measurement of \u03b3 and the GR prediction."}, "52": {"documentation": {"title": "Efficient Multi-objective Neural Architecture Search via Lamarckian\n  Evolution", "source": "Thomas Elsken, Jan Hendrik Metzen, Frank Hutter", "docs_id": "1804.09081", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Multi-objective Neural Architecture Search via Lamarckian\n  Evolution. Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed LEMONADE algorithm in the paper \"Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution\", and how does it address the limitations of existing architecture search methods?\n\nA) To optimize neural architectures for high predictive performance while minimizing resource consumption, and to reduce the computational resources required for architecture search.\nB) To find the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method, and to warmstart children networks with the predictive performance of their trained parents.\nC) To design neural architectures that are competitive with human experts, while ensuring that the search process is efficient and scalable.\nD) To develop a Lamarckian inheritance mechanism for generating children networks that inherit the predictive performance of their parents, while allowing for exploration of the Pareto-front.\n\nCorrect Answer: B) To find the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method, and to warmstart children networks with the predictive performance of their trained parents.\n\nExplanation: The question requires the test-taker to understand the main contribution of the LEMONADE algorithm, which is to address the limitations of existing architecture search methods by finding the entire Pareto-front of architectures in a single run, and to warmstart children networks with the predictive performance of their parents. This requires an understanding of the multi-objective optimization problem and the proposed solution, as well as the ability to identify the key benefits of the LEMONADE algorithm."}, "53": {"documentation": {"title": "On Optimal Harvesting in Stochastic Environments: Optimal Policies in a\n  Relaxed Model", "source": "Richard H. Stockbridge and Chao Zhu", "docs_id": "1106.2773", "section": ["math.OC", "cs.SY", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Optimal Harvesting in Stochastic Environments: Optimal Policies in a\n  Relaxed Model. This paper examines the objective of optimally harvesting a single species in a stochastic environment. This problem has previously been analyzed in Alvarez (2000) using dynamic programming techniques and, due to the natural payoff structure of the price rate function (the price decreases as the population increases), no optimal harvesting policy exists. This paper establishes a relaxed formulation of the harvesting model in such a manner that existence of an optimal relaxed harvesting policy can not only be proven but also identified. The analysis embeds the harvesting problem in an infinite-dimensional linear program over a space of occupation measures in which the initial position enters as a parameter and then analyzes an auxiliary problem having fewer constraints. In this manner upper bounds are determined for the optimal value (with the given initial position); these bounds depend on the relation of the initial population size to a specific target size. The more interesting case occurs when the initial population exceeds this target size; a new argument is required to obtain a sharp upper bound. Though the initial population size only enters as a parameter, the value is determined in a closed-form functional expression of this parameter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of optimal harvesting in stochastic environments, what is the key insight that allows the authors to establish the existence of an optimal relaxed harvesting policy, despite the absence of an optimal harvesting policy in the original problem formulation?\n\n**A)** The authors assume a deterministic environment, which simplifies the problem.\n**B)** The authors use a different formulation of the harvesting model that embeds the problem in an infinite-dimensional linear program over a space of occupation measures.\n**C)** The authors apply dynamic programming techniques to the original problem formulation.\n**D)** The authors assume a fixed initial population size, which eliminates the need for a relaxed formulation.\n\n**Correct Answer:** B) The authors use a different formulation of the harvesting model that embeds the problem in an infinite-dimensional linear program over a space of occupation measures.\n\n**Explanation:** The correct answer is B) because the authors' key insight is to embed the harvesting problem in an infinite-dimensional linear program over a space of occupation measures, which allows them to establish the existence of an optimal relaxed harvesting policy. This formulation is a crucial step in the authors' analysis, as it enables them to analyze an auxiliary problem with fewer constraints and determine upper bounds for the optimal value. The other options are incorrect because they do not accurately reflect the authors' approach or the key insight that allows them to establish the existence of an optimal relaxed harvesting policy."}, "54": {"documentation": {"title": "Applications of the Morava $K$-theory to algebraic groups", "source": "Pavel Sechin, Nikita Semenov", "docs_id": "1805.09059", "section": ["math.AG", "math.KT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Applications of the Morava $K$-theory to algebraic groups. In the present article we discuss an approach to cohomological invariants of algebraic groups over fields of characteristic zero based on the Morava $K$-theories, which are generalized oriented cohomology theories in the sense of Levine--Morel. We show that the second Morava $K$-theory detects the triviality of the Rost invariant and, more generally, relate the triviality of cohomological invariants and the splitting of Morava motives. We describe the Morava $K$-theory of generalized Rost motives, compute the Morava $K$-theory of some affine varieties, and characterize the powers of the fundamental ideal of the Witt ring with the help of the Morava $K$-theory. Besides, we obtain new estimates on torsion in Chow groups of codimensions up to $2^n$ of quadrics from the $(n+2)$-nd power of the fundamental ideal of the Witt ring. We compute torsion in Chow groups of $K(n)$-split varieties with respect to a prime $p$ in all codimensions up to $\\frac{p^n-1}{p-1}$ and provide a combinatorial tool to estimate torsion up to codimension $p^n$. An important role in the proof is played by the gamma filtration on Morava $K$-theories, which gives a conceptual explanation of the nature of the torsion. Furthermore, we show that under some conditions the $K(n)$-motive of a smooth projective variety splits if and only if its $K(m)$-motive splits for all $m\\le n$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $X$ be a smooth projective variety over a field of characteristic zero, and let $K(n)$ denote the $n$-th Morava $K$-theory. Suppose that the $K(n)$-motive of $X$ splits if and only if its $K(m)$-motive splits for all $m\\le n$. Which of the following statements is true?\n\nA) The $K(n)$-motive of $X$ splits if and only if the Rost invariant of $X$ is trivial.\nB) The $K(n)$-motive of $X$ splits if and only if the fundamental ideal of the Witt ring of $X$ has a certain property.\nC) The $K(n)$-motive of $X$ splits if and only if the torsion in the Chow groups of $X$ is zero in all codimensions up to $\\frac{n^2-1}{n-1}$.\nD) The $K(n)$-motive of $X$ splits if and only if the gamma filtration on the $K(n)$-theory of $X$ is trivial.\n\nCorrect Answer: D) The $K(n)$-motive of $X$ splits if and only if the gamma filtration on the $K(n)$-theory of $X$ is trivial.\n\nExplanation: The correct answer is D) because the documentation states that the gamma filtration on Morava $K$-theories plays an important role in the proof, and it provides a conceptual explanation of the nature of the torsion. The other options are not supported by the documentation, and are therefore incorrect."}, "55": {"documentation": {"title": "A Joint Quantile and Expected Shortfall Regression Framework", "source": "Timo Dimitriadis and Sebastian Bayer", "docs_id": "1704.02213", "section": ["math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Joint Quantile and Expected Shortfall Regression Framework. We introduce a novel regression framework which simultaneously models the quantile and the Expected Shortfall (ES) of a response variable given a set of covariates. This regression is based on a strictly consistent loss function for the pair quantile and ES, which allows for M- and Z-estimation of the joint regression parameters. We show consistency and asymptotic normality for both estimators under weak regularity conditions. The underlying loss function depends on two specification functions, whose choice affects the properties of the resulting estimators. We find that the Z-estimator is numerically unstable and thus, we rely on M-estimation of the model parameters. Extensive simulations verify the asymptotic properties and analyze the small sample behavior of the M-estimator for different specification functions. This joint regression framework allows for various applications including estimating, forecasting, and backtesting ES, which is particularly relevant in light of the recent introduction of ES into the Basel Accords."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a regression framework that jointly models the quantile and Expected Shortfall (ES) of a response variable given a set of covariates. Suppose the underlying loss function depends on two specification functions, and the M-estimator is used to estimate the model parameters. What is the primary reason for relying on M-estimation over Z-estimation in this framework?\n\n**A)** The M-estimator is more numerically stable than the Z-estimator.\n**B)** The M-estimator is more robust to outliers than the Z-estimator.\n**C)** The M-estimator is more consistent than the Z-estimator under weak regularity conditions.\n**D)** The M-estimator is more efficient than the Z-estimator for estimating the joint regression parameters.\n\n**Correct Answer:** D) The M-estimator is more efficient than the Z-estimator for estimating the joint regression parameters.\n\n**Explanation:** The correct answer is D) because the text states that the Z-estimator is \"numerically unstable\" and thus, the M-estimator is relied upon. This implies that the M-estimator is preferred due to its numerical stability, but the primary reason is that it is more efficient for estimating the joint regression parameters. The other options are incorrect because they do not accurately reflect the reason for preferring the M-estimator over the Z-estimator."}, "56": {"documentation": {"title": "Pushing for weighted tree automata", "source": "Thomas Hanneforth and Andreas Maletti and Daniel Quernheim", "docs_id": "1702.00304", "section": ["cs.FL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pushing for weighted tree automata. A weight normalization procedure, commonly called pushing, is introduced for weighted tree automata (wta) over commutative semifields. The normalization preserves the recognized weighted tree language even for nondeterministic wta, but it is most useful for bottom-up deterministic wta, where it can be used for minimization and equivalence testing. In both applications a careful selection of the weights to be redistributed followed by normalization allows a reduction of the general problem to the corresponding problem for bottom-up deterministic unweighted tree automata. This approach was already successfully used by Mohri and Eisner for the minimization of deterministic weighted string automata. Moreover, the new equivalence test for two wta $M$ and $M'$ runs in time $\\mathcal O((\\lvert M \\rvert + \\lvert M'\\rvert) \\cdot \\log {(\\lvert Q\\rvert + \\lvert Q'\\rvert)})$, where $Q$ and $Q'$ are the states of $M$ and $M'$, respectively, which improves the previously best run-time $\\mathcal O(\\lvert M \\rvert \\cdot \\lvert M'\\rvert)$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the \"pushing\" weight normalization procedure for weighted tree automata (wta) over commutative semifields, and how does it improve the equivalence testing time for two wta models?\n\nA) The procedure preserves the recognized weighted tree language even for nondeterministic wta, but it is most useful for bottom-up deterministic wta, where it can be used for minimization and equivalence testing. This approach allows a reduction of the general problem to the corresponding problem for bottom-up deterministic unweighted tree automata.\n\nB) The procedure reduces the general problem of minimizing wta to the corresponding problem for bottom-up deterministic unweighted tree automata, but it does not preserve the recognized weighted tree language.\n\nC) The procedure improves the previously best run-time for equivalence testing of two wta models from O(|M| * |M'|) to O((|M| + |M'|) * log (|Q| + |Q'|)), where Q and Q' are the states of M and M', respectively.\n\nD) The procedure is only useful for bottom-up deterministic wta, and it does not preserve the recognized weighted tree language even for nondeterministic wta.\n\nCorrect Answer: A) The procedure preserves the recognized weighted tree language even for nondeterministic wta, but it is most useful for bottom-up deterministic wta, where it can be used for minimization and equivalence testing. This approach allows a reduction of the general problem to the corresponding problem for bottom-up deterministic unweighted tree automata.\n\nExplanation: The correct answer is A) because it accurately summarizes the primary advantage of the \"pushing\" weight normalization procedure. The procedure preserves the recognized weighted tree language even for nondeterministic wta, but it is most useful for bottom-up deterministic wta, where it can be used for minimization and equivalence testing. This approach allows a reduction of the general problem to the corresponding problem for bottom-up deterministic unweighted tree automata. The other options are incorrect because they either omit important information (B), provide an incomplete or inaccurate summary (C), or provide a misleading statement (D)."}, "57": {"documentation": {"title": "Synthetic learner: model-free inference on treatments over time", "source": "Davide Viviano and Jelena Bradic", "docs_id": "1904.01490", "section": ["stat.ME", "cs.LG", "econ.EM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synthetic learner: model-free inference on treatments over time. Understanding of the effect of a particular treatment or a policy pertains to many areas of interest -- ranging from political economics, marketing to health-care and personalized treatment studies. In this paper, we develop a non-parametric, model-free test for detecting the effects of treatment over time that extends widely used Synthetic Control tests. The test is built on counterfactual predictions arising from many learning algorithms. In the Neyman-Rubin potential outcome framework with possible carry-over effects, we show that the proposed test is asymptotically consistent for stationary, beta mixing processes. We do not assume that class of learners captures the correct model necessarily. We also discuss estimates of the average treatment effect, and we provide regret bounds on the predictive performance. To the best of our knowledge, this is the first set of results that allow for example any Random Forest to be useful for provably valid statistical inference in the Synthetic Control setting. In experiments, we show that our Synthetic Learner is substantially more powerful than classical methods based on Synthetic Control or Difference-in-Differences, especially in the presence of non-linear outcome models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Synthetic Learner paper, what is the primary assumption made about the class of learners used to build the counterfactual predictions, and how does this assumption relate to the provable validity of statistical inference in the Synthetic Control setting?\n\n**A)** The class of learners is assumed to capture the correct model, and the test is designed to account for any potential errors in this assumption.\n\n**B)** The class of learners is not assumed to capture the correct model, and the test is designed to provide regret bounds on the predictive performance.\n\n**C)** The class of learners is assumed to be a parametric model, and the test is designed to account for any potential biases in this assumption.\n\n**D)** The class of learners is assumed to be a non-parametric model, and the test is designed to provide asymptotically consistent estimates of the average treatment effect.\n\n**Correct Answer:** B) The class of learners is not assumed to capture the correct model, and the test is designed to provide regret bounds on the predictive performance.\n\n**Explanation:** The paper states that \"We do not assume that class of learners captures the correct model necessarily.\" This means that the class of learners is not assumed to be the true model, and the test is designed to provide regret bounds on the predictive performance, which is a measure of the test's performance in terms of its ability to make accurate predictions. This assumption is important because it allows the test to be provably valid in the Synthetic Control setting, even if the class of learners used to build the counterfactual predictions is not the true model."}, "58": {"documentation": {"title": "Large scale anisotropies on halo infall", "source": "Laura Ceccarelli, Dante J. Paz, Nelson Padilla and Diego G. Lambas", "docs_id": "1011.2494", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large scale anisotropies on halo infall. We perform a statistical analysis of the peculiar velocity field around dark matter haloes in numerical simulations. We examine different properties of the infall of material onto haloes and its relation to central halo shapes and the shape of the large scale surrounding regions (LSSR). We find that the amplitude of the infall velocity field along the halo shape minor axis is larger than that along the major axis. This is consistent for general triaxial haloes, and for both prolate and oblate systems. We also report a strong anisotropy of the velocity field along the principal axes of the LSSR. The infall velocity field around dark matter haloes reaches a maximum value along the direction of the minor axis of the LSSR, whereas along the direction of its major axis, it exhibits the smallest velocities. We also analyse the dependence of the matter velocity field on the local environment. The amplitude of the infall velocity at high local density regions is larger than at low local density regions. The velocity field tends to be more laminar along the direction towards the minor axis of the LSSR, where the mean ratio between flow velocity and velocity dispersion is of order unity and nearly constant up to scales of 15 Mpc/h. We also detect anisotropies in the outflowing component of the velocity field, showing a maximum amplitude along the surrounding LSSR major axis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the anisotropies in the infall velocity field around dark matter haloes in numerical simulations. How do the results of this study relate to the shape of the halo and the surrounding large-scale structure (LSSR), and what implications do these findings have for our understanding of the distribution of matter in the universe?\n\n**A)** The study suggests that the infall velocity field is more anisotropic along the minor axis of the halo, which is consistent with the idea that dark matter haloes are prolate spheroids. However, the results do not provide any information about the relationship between the infall velocity field and the LSSR.\n\n**B)** The study reveals a strong anisotropy in the infall velocity field along the principal axes of the LSSR, with the maximum velocity along the minor axis of the LSSR and the minimum velocity along the major axis. This suggests that the distribution of matter in the universe is more complex than previously thought, with regions of high and low density playing a crucial role in shaping the large-scale structure.\n\n**C)** The study's findings indicate that the amplitude of the infall velocity field along the halo shape minor axis is larger than that along the major axis, which is consistent with the idea that dark matter haloes are oblate spheroids. However, the results do not provide any information about the relationship between the infall velocity field and the LSSR.\n\n**D)** The study's results suggest that the infall velocity field is more laminar along the direction towards the minor axis of the LSSR, where the mean ratio between flow velocity and velocity dispersion is of order unity and nearly constant up to scales of 15 Mpc/h. This implies that the distribution of matter in the universe is more organized and structured than previously thought, with regions of high and low density playing a crucial role in shaping the large-scale structure.\n\n**Correct Answer:** B) The study reveals a strong anisotropy in the infall velocity field along the principal axes of the LSSR, with the maximum velocity along the minor axis of the LSSR and the minimum velocity along the major axis. This suggests that the distribution of matter in the universe is more complex than previously thought, with regions of high and low density playing a crucial role in shaping the large-scale structure.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the main findings of the study. The study reveals a strong anisotropy in the infall velocity field along the principal axes of the LSSR, with the maximum velocity along the minor axis of the LSSR and the minimum velocity along the major axis. This suggests that the distribution of matter in the universe is more complex than previously thought, with regions of high and low density playing a crucial role in shaping the large-scale structure. The other options are incorrect because they either provide incomplete or inaccurate summaries of the study's findings."}, "59": {"documentation": {"title": "Speckle interferometry at SOAR in 2018", "source": "Andrei Tokovinin, Brian D. Mason, Rene A. Mendez, Elliott P. Horch,\n  Cesar Briceno", "docs_id": "1905.10436", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Speckle interferometry at SOAR in 2018. The results of speckle interferometric observations at the 4.1 m Southern Astrophysical Research Telescope (SOAR) in 2018 are given, totaling 3097 measurements of 2427 resolved pairs with separations from 11 mas to 5.9\" (median 0.15\", magnitude difference up to 7 mag) and non-resolutions of 624 targets. This work continues our long-term speckle program. Its main goal is to monitor orbital motion of close binaries, including members of high-order hierarchies and Hipparcos pairs in the solar neighborhood. Also, pre-main-sequence stars in the Orion OB1 association were surveyed, resolving 26 out of 118 targets. In addition, we report discovery of 35 new companions among field visual multiples (some of which are likely optical) and first-time resolutions of another 31 pairs. By combining the measurements given here with the published ones, we computed 76 orbits for the first time and updated orbital elements of 34 visual binaries. Their periods range from 0.65 to 1100 years, and their quality varies from first tentative solutions of grade 5 to accurate elements of grades 1 and 2. Finally, a list of 53 spurious pairs discovered by various techniques and unresolved at SOAR"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the speckle interferometry data collected at the SOAR telescope in 2018. What can be inferred about the orbital periods of the resolved binary pairs, and how do the quality of the orbital elements vary?\n\n**A)** The orbital periods of the resolved binary pairs range from 0.65 to 1100 years, with the majority of pairs having periods between 10 and 100 years. The quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 1 and 2.\n\n**B)** The orbital periods of the resolved binary pairs range from 10 to 100 years, with the majority of pairs having periods between 1 and 10 years. The quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 2 and 3.\n\n**C)** The orbital periods of the resolved binary pairs range from 0.65 to 1100 years, with the majority of pairs having periods between 100 and 1100 years. The quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 1 and 2.\n\n**D)** The orbital periods of the resolved binary pairs range from 10 to 100 years, with the majority of pairs having periods between 0.65 and 10 years. The quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 1 and 2.\n\n**Correct Answer:** A) The orbital periods of the resolved binary pairs range from 0.65 to 1100 years, with the majority of pairs having periods between 10 and 100 years. The quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 1 and 2.\n\n**Explanation:** The correct answer can be inferred from the text, which states that the periods of the resolved binary pairs range from 0.65 to 1100 years, and that the quality of the orbital elements varies from first tentative solutions to accurate elements, with most pairs having elements of grade 1 and 2. This information is presented in the following sentence: \"Their periods range from 0.65 to 1100 years, and their quality varies from first tentative solutions of grade 5 to accurate elements of grades 1 and 2.\""}}