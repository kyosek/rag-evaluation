{"0": {"documentation": {"title": "Reduction of systemic risk by means of Pigouvian taxation", "source": "Vinko Zlati\\'c, Giampaolo Gabbi, Hrvoje Abraham", "docs_id": "1406.5817", "section": ["q-fin.RM", "physics.soc-ph", "q-fin.GN", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reduction of systemic risk by means of Pigouvian taxation. We analyze the possibility of reduction of systemic risk in financial markets through Pigouvian taxation of financial institutions which is used to support the rescue fund. We introduce the concept of the cascade risk with a clear operational definition as a subclass and a network related measure of the systemic risk. Using financial networks constructed from real Italian money market data and using realistic parameters, we show that the cascade risk can be substantially reduced by a small rate of taxation and by means of a simple strategy of the money transfer from the rescue fund to interbanking market subjects. Furthermore, we show that while negative effects on the return on investment ($ROI$) are direct and certain, an overall positive effect on risk adjusted return on investments ($ROI^{RA}$) is visible. Please note that \\emph{the taxation} is introduced as a monetary/regulatory, not as a fiscal measure, as the term could suggest. \\emph{The rescue fund} is implemented in a form of a common reserve fund."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A study on reducing systemic risk in financial markets through Pigouvian taxation proposes the following, EXCEPT:\n\nA) The implementation of a small tax rate on financial institutions to support a rescue fund\nB) The use of the rescue fund to transfer money to interbanking market subjects\nC) The introduction of cascade risk as a network-related measure of systemic risk\nD) The application of the tax as a fiscal measure to increase government revenue\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the key concepts presented in the documentation about reducing systemic risk through Pigouvian taxation. \n\nOption A is correct as the study explicitly mentions using a small rate of taxation on financial institutions to support the rescue fund.\n\nOption B is accurate, as the documentation states that a simple strategy of money transfer from the rescue fund to interbanking market subjects can reduce cascade risk.\n\nOption C is also correct, as the study introduces the concept of cascade risk as a subclass and network-related measure of systemic risk.\n\nOption D is the exception and therefore the correct answer to this question. The documentation specifically notes that \"the taxation is introduced as a monetary/regulatory, not as a fiscal measure.\" This means the tax is not intended to increase government revenue, but rather to support the rescue fund for reducing systemic risk in financial markets."}, "1": {"documentation": {"title": "ALICE TPC upgrade for High-Rate operations", "source": "Saikat Biswas (for the ALICE Collaboration)", "docs_id": "1511.04988", "section": ["physics.ins-det", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ALICE TPC upgrade for High-Rate operations. A new type of Time Projection Chamber (TPC) has been proposed for the upgrade of the ALICE (A Large Ion Collider Experiment at CERN) so as to cater to the high luminosity environment expected at the Large Hadron Collider (LHC) facility in future. This device will rely on the intrinsic ion back flow (IBF) suppression of Micro-Pattern Gas Detectors (MPGD) based technology in particular the Gas Electron Multiplier (GEM). GEM is to minimise the space charge effect in the main drift volume and thus will not require the standard gating grid and the resulting intrinsic dead time. It will thus be possible to read all minimum bias Pb--Pb events that the Large Hadron Collider (LHC) will deliver at the anticipated peak interaction rate of 50 kHz for the high luminosity heavy-ion era in Run 3. New read-out electronics will send the continuous data stream to a new online farm at rates up to 1~TByte/s. The new read-out chambers will consist of stacks of 4 GEM foils combining different hole pitches. In addition to a low ion back flow ($<$ 1\\%) other important requirements are good energy resolution (better than 12\\% (sigma) for $^{55}$Fe X-rays) and operational stability."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: The upgrade of the ALICE Time Projection Chamber (TPC) for high-rate operations involves several key innovations. Which of the following combinations accurately describes the main features and benefits of this upgrade?\n\nA) Use of Micro-Pattern Gas Detectors (MPGD), elimination of gating grid, continuous data stream at 1 GB/s, and energy resolution better than 20% for 55Fe X-rays\n\nB) Implementation of Gas Electron Multiplier (GEM) technology, retention of gating grid, data streaming at 1 TB/s, and ion back flow < 0.1%\n\nC) Utilization of GEM technology, removal of gating grid, continuous data stream at 1 TB/s, and ion back flow < 1%\n\nD) Adoption of MPGD technology, faster gating grid, data rate of 100 GB/s, and energy resolution better than 5% for 55Fe X-rays\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately combines several key features of the ALICE TPC upgrade:\n\n1. The upgrade uses Gas Electron Multiplier (GEM) technology, which is a type of Micro-Pattern Gas Detector (MPGD).\n2. The new design eliminates the need for a gating grid, reducing intrinsic dead time.\n3. The new readout electronics will send a continuous data stream at rates up to 1 TB/s.\n4. The ion back flow is required to be less than 1%.\n\nOption A is incorrect because it mentions a data rate of 1 GB/s (should be 1 TB/s) and the energy resolution is not accurate (should be better than 12% for 55Fe X-rays).\n\nOption B is incorrect because it mentions retaining the gating grid, which is actually being eliminated in the upgrade.\n\nOption D is incorrect because it mentions a faster gating grid (which is being removed entirely) and provides an incorrect data rate and energy resolution."}, "2": {"documentation": {"title": "Using Neutron Star Observations to Determine Crust Thicknesses, Moments\n  of Inertia, and Tidal Deformabilities", "source": "Andrew W. Steiner, Stefano Gandolfi, Farrukh J. Fattoyev, William G.\n  Newton", "docs_id": "1403.7546", "section": ["nucl-th", "astro-ph.HE", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using Neutron Star Observations to Determine Crust Thicknesses, Moments\n  of Inertia, and Tidal Deformabilities. We perform a systematic assessment of models for the equation of state (EOS) of dense matter in the context of recent neutron star mass and radius measurements to obtain a broad picture of the structure of neutron stars. We demonstrate that currently available neutron star mass and radius measurements provide strong constraints on moments of inertia, tidal deformabilities, and crust thicknesses. A measurement of the moment of inertia of PSR J0737-3039A with 10% error, without any other information from observations, will constrain the EOS over a range of densities to within 50%$-$60%. We find tidal deformabilities between 0.6 and $6\\times 10^{36}$ g cm$^{2}$ s$^{2}$ (to 95% confidence) for $M=1.4~\\mathrm{M}_{\\odot}$, and any measurement which constrains this range will provide an important constraint on dense matter. The crustal fraction of the moment of inertia can be as large as 10% for $M=1.4~\\mathrm{M}_{\\odot}$ permitting crusts to have a large enough moment of inertia reservoir to explain glitches in the Vela pulsar even with a large amount of superfluid entrainment. Finally, due to the uncertainty in the equation of state, there is at least a 40% variation in the thickness of the crust for a fixed mass and radius, which implies that future simulations of the cooling of a neutron star crust which has been heated by accretion will need to take this variation into account."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: According to the study, which of the following statements is true regarding the tidal deformabilities of neutron stars with a mass of 1.4 solar masses?\n\nA) They are constrained between 0.6 and 6 \u00d7 10^34 g cm^2 s^2 with 95% confidence\nB) They are constrained between 0.6 and 6 \u00d7 10^36 g cm^2 s^2 with 95% confidence\nC) They are constrained between 0.6 and 6 \u00d7 10^38 g cm^2 s^2 with 99% confidence\nD) They are constrained between 0.6 and 6 \u00d7 10^40 g cm^2 s^2 with 90% confidence\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The study states that \"We find tidal deformabilities between 0.6 and 6 \u00d7 10^36 g cm^2 s^2 (to 95% confidence) for M=1.4 M_\u2609\". This matches exactly with option B, which correctly presents both the range and the confidence level. Options A, C, and D are incorrect as they either present the wrong range of values or the wrong confidence level."}, "3": {"documentation": {"title": "Skyrmion Quantization and the Decay of the Delta", "source": "Nicholas Dorey, James Hughes and Michael Mattis", "docs_id": "hep-ph/9404274", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Skyrmion Quantization and the Decay of the Delta. We present the complete solution to the so-called ``Yukawa problem'' of the Skyrme model. This refers to the perceived difficulty of reproducing---purely from soliton physics---the usual pseudovector pion-nucleon coupling, echoed by pion coupling to the higher spin/isospin baryons $(I=J=3/2 , 5/2 , \\cdots , N_c/2 )$ in a manner fixed by large-$N_c$ group theory. The solution involves surprisingly elegant interplay between the classical and quantum properties of a new configuration, the ``new improved skyrmion''. This is the near-hedgehog obtained by minimizing the usual skyrmion mass functional augmented by an all-important isorotational kinetic term. The numerics are pleasing: a $\\Delta$ decay width within a few MeV of its measured value, and furthermore, the higher-spin baryons $(I=J \\ge 5/2 )$ with widths so large ($\\Gamma > 800 MeV$) that these undesirable large-$N_c$ artifacts effectively drop out of the spectrum, and pose no phenomenological problem. Beyond these specific results, we ground the Skyrme model in the Feynman Path Integral, and set up a transparent collective coordinate formalism that makes maximal use of the $1/N_c$ expansion. This approach elucidates the connection between skyrmions on the one hand, and Feynman diagrams in an effective field theory on the other."}, "answer": "Question: The \"Yukawa problem\" in the Skyrme model is resolved through:\n\nA) The introduction of a new configuration called the \"new improved skyrmion\"\nB) Purely classical analysis of soliton physics\nC) Abandoning the concept of pseudovector pion-nucleon coupling\nD) Increasing the number of colors (Nc) to infinity\n\nCorrect Answer: A\n\nExplanation: The \"Yukawa problem\" in the Skyrme model refers to the difficulty in reproducing the pseudovector pion-nucleon coupling and its extension to higher spin/isospin baryons using only soliton physics. The passage states that the solution involves \"surprisingly elegant interplay between the classical and quantum properties of a new configuration, the 'new improved skyrmion'.\" This new configuration is described as a near-hedgehog obtained by minimizing the usual skyrmion mass functional with an added isorotational kinetic term.\n\nOption B is incorrect because the solution involves both classical and quantum properties, not purely classical analysis. Option C is wrong because the goal is to reproduce the pseudovector coupling, not abandon it. Option D is incorrect because while large-Nc concepts are used, increasing Nc to infinity is not the solution to the problem.\n\nThe correct answer, A, directly addresses the key innovation described in the passage that solves the Yukawa problem."}, "4": {"documentation": {"title": "\\textit{Ab initio} study of Bi-based half Heusler alloys as potential\n  thermoelectric prospects", "source": "Sapna Singh, Mohd Zeeshan, Jeroen van den Brink, and Hem C. Kandpal", "docs_id": "1904.02488", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "\\textit{Ab initio} study of Bi-based half Heusler alloys as potential\n  thermoelectric prospects. We investigated six heavy element bismuth-based 18-VEC half-Heusler alloys CoTiBi, CoZrBi, CoHfBi, FeVBi, FeNbBi, and FeTaBi by first principles approach, in search of better thermoelectric prospects. The motivation is driven by expected lower thermal conductivity and the recent discovery of CoZrBi-based materials. Significantly, our calculated power factor values of all the systems show an increment of $\\sim$40\\% in comparison to the reported \\textit{p}-type CoTiSb. We propose that doping at Bi-site, on account of electronic features, will be helpful in achieving the proposed power factor values. Interestingly, the thermal conductivity of CoTiBi and CoZrBi was found to be lower and that of CoHfBi was almost parallel, in comparison to the reported CoTiSb. We also provide conservative estimates of the figure of merit, exceeding the reported CoTiSb and comparable to FeNbSb. Overall, our results suggest potential new candidates of bismuth-based ternary compounds for high thermoelectric performance."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the findings and implications of the study on Bi-based half Heusler alloys?\n\nA) The study found that all investigated alloys have lower thermal conductivity than CoTiSb, making them superior thermoelectric materials.\n\nB) The research suggests that doping at the Ti-site in CoTiBi will likely achieve the highest power factor values among the studied alloys.\n\nC) The calculated power factor values for the investigated alloys show an increase of approximately 40% compared to p-type CoTiSb, with Bi-site doping proposed as a method to achieve these values.\n\nD) The figure of merit estimates for all studied alloys significantly exceeded both CoTiSb and FeNbSb, indicating their immediate superiority as thermoelectric materials.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the key findings of the study. The text states that the calculated power factor values of all the systems show an increment of ~40% compared to the reported p-type CoTiSb. Additionally, the researchers propose that doping at the Bi-site will be helpful in achieving the proposed power factor values.\n\nAnswer A is incorrect because the study found that only CoTiBi and CoZrBi had lower thermal conductivity than CoTiSb, while CoHfBi was almost parallel. It doesn't apply to all investigated alloys.\n\nAnswer B is incorrect because the study suggests doping at the Bi-site, not the Ti-site, to achieve the proposed power factor values.\n\nAnswer D is incorrect because the text describes the figure of merit estimates as \"conservative\" and \"exceeding the reported CoTiSb and comparable to FeNbSb,\" not significantly exceeding both."}, "5": {"documentation": {"title": "Single-peaked domains with designer uncertainty", "source": "Aroon Narayanan", "docs_id": "2108.11268", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single-peaked domains with designer uncertainty. This paper studies single-peaked domains where the designer is uncertain about the underlying alignment according to which the domain is single-peaked. The underlying alignment is common knowledge amongst agents, but preferences are private knowledge. Thus, the state of the world has both a public and private element, with the designer uninformed of both. I first posit a relevant solution concept called implementation in mixed information equilibria, which requires Nash implementation in the public information and dominant strategy implementation in the private information given the public information. I then identify necessary and sufficient conditions for social rules to be implementable. The characterization is used to identify unanimous and anonymous implementable social rules for various belief structures of the designer, which basically boils down to picking the right rules from the large class of median rules identified by Moulin (1980), and hence this result can be seen as identifying which median rules are robust to designer uncertainty."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of single-peaked domains with designer uncertainty, which of the following statements is true regarding the implementation of social rules?\n\nA) The designer must have complete knowledge of both the underlying alignment and agents' preferences for successful implementation.\n\nB) Implementation in mixed information equilibria requires only Nash implementation in the public information, regardless of private information.\n\nC) The characterization of implementable social rules is independent of the designer's belief structure.\n\nD) Implementable unanimous and anonymous social rules are a subset of Moulin's median rules, selected based on the designer's belief structure.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that the characterization of implementable social rules \"is used to identify unanimous and anonymous implementable social rules for various belief structures of the designer, which basically boils down to picking the right rules from the large class of median rules identified by Moulin (1980).\" This indicates that the implementable social rules are indeed a subset of Moulin's median rules, and their selection depends on the designer's belief structure.\n\nOption A is incorrect because the designer is explicitly stated to be uncertain about the underlying alignment and agents' preferences.\n\nOption B is incorrect because the implementation in mixed information equilibria requires both Nash implementation in public information and dominant strategy implementation in private information.\n\nOption C is incorrect because the characterization of implementable social rules is said to depend on the designer's belief structure, as evidenced by the statement about identifying rules \"for various belief structures of the designer.\""}, "6": {"documentation": {"title": "A real-time thermal field theoretical analysis of Kubo-type shear\n  viscosity : Numerical understanding with simple examples", "source": "Sabyasachi Ghosh", "docs_id": "1404.4788", "section": ["nucl-th", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A real-time thermal field theoretical analysis of Kubo-type shear\n  viscosity : Numerical understanding with simple examples. A real-time thermal field theoretical calculation of shear viscosity has been described in the Kubo formalism for bosonic and fermionic medium. The two point function of viscous stress tensor in the lowest order provides one-loop skeleton diagram of boson or fermion field for bosonic or fermionic matter. According to the traditional diagrammatic technique of transport coefficients, the finite thermal width of boson or fermion is introduced in their internal lines during the evaluation of boson-boson or fermion-fermion loop diagram. These thermal widths of $\\phi$ boson and $\\psi$ fermion are respectively obtained from the imaginary part of self-energy for $\\phi\\Phi$ and $\\psi\\Phi$ loops, where interactions of higher mass $\\Phi$ boson with $\\phi$ and $\\psi$ are governed by the simple $\\phi\\phi\\Phi$ and ${\\ov\\psi}\\psi\\Phi$ interaction Lagrangian densities. A two-loop diagram, having same power of coupling constant as in the one-loop diagram, is deduced and its contribution appears much lower than the one-loop values of shear viscosity. Therefore the one-loop results of Kubo-type shear viscosity may be considered as leading order results for this simple $\\phi\\phi\\Phi$ and ${\\ov\\psi}\\psi\\Phi$ interactions. This approximation is valid for any values of coupling constant and at the temperatures greater than the mass of constituent particles of the medium."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the real-time thermal field theoretical calculation of shear viscosity using the Kubo formalism, which of the following statements is correct regarding the comparison between one-loop and two-loop diagrams?\n\nA) The two-loop diagram contributes significantly more to the shear viscosity than the one-loop diagram.\n\nB) The one-loop and two-loop diagrams contribute equally to the shear viscosity.\n\nC) The two-loop diagram's contribution is much lower than the one-loop diagram's, despite having the same power of coupling constant.\n\nD) The two-loop diagram is ignored entirely in the calculation of shear viscosity.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the relative contributions of one-loop and two-loop diagrams in the calculation of shear viscosity using the Kubo formalism. The correct answer is C because the documentation explicitly states: \"A two-loop diagram, having same power of coupling constant as in the one-loop diagram, is deduced and its contribution appears much lower than the one-loop values of shear viscosity.\" This indicates that despite having the same power of coupling constant, the two-loop diagram's contribution is significantly less than that of the one-loop diagram.\n\nOption A is incorrect because it contradicts the information provided, which states that the two-loop contribution is lower, not higher.\n\nOption B is also incorrect as the document clearly indicates an unequal contribution between one-loop and two-loop diagrams.\n\nOption D is incorrect because while the two-loop diagram's contribution is lower, it is not entirely ignored in the calculation.\n\nThis question challenges the student's ability to comprehend and interpret complex theoretical physics concepts and their relative importance in calculations."}, "7": {"documentation": {"title": "Interpretable Deep Learning-Based Forensic Iris Segmentation and\n  Recognition", "source": "Andrey Kuehlkamp, Aidan Boyd, Adam Czajka, Kevin Bowyer, Patrick\n  Flynn, Dennis Chute, Eric Benjamin", "docs_id": "2112.00849", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interpretable Deep Learning-Based Forensic Iris Segmentation and\n  Recognition. Iris recognition of living individuals is a mature biometric modality that has been adopted globally from governmental ID programs, border crossing, voter registration and de-duplication, to unlocking mobile phones. On the other hand, the possibility of recognizing deceased subjects with their iris patterns has emerged recently. In this paper, we present an end-to-end deep learning-based method for postmortem iris segmentation and recognition with a special visualization technique intended to support forensic human examiners in their efforts. The proposed postmortem iris segmentation approach outperforms the state of the art and in addition to iris annulus, as in case of classical iris segmentation methods - detects abnormal regions caused by eye decomposition processes, such as furrows or irregular specular highlights present on the drying and wrinkling cornea. The method was trained and validated with data acquired from 171 cadavers, kept in mortuary conditions, and tested on subject-disjoint data acquired from 259 deceased subjects. To our knowledge, this is the largest corpus of data used in postmortem iris recognition research to date. The source code of the proposed method are offered with the paper. The test data will be available through the National Archive of Criminal Justice Data (NACJD) archives."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the novel contribution of the proposed deep learning-based method for postmortem iris recognition, as presented in the Arxiv paper?\n\nA) It can recognize living individuals using iris patterns with higher accuracy than existing methods.\nB) It detects abnormal regions caused by eye decomposition processes in addition to the iris annulus.\nC) It is the first method to use deep learning for iris recognition in forensic applications.\nD) It can unlock mobile phones using postmortem iris scans.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper describes a novel deep learning-based method for postmortem iris segmentation and recognition. The key contribution, as stated in the text, is that \"in addition to iris annulus, as in case of classical iris segmentation methods - detects abnormal regions caused by eye decomposition processes, such as furrows or irregular specular highlights present on the drying and wrinkling cornea.\"\n\nOption A is incorrect because the paper focuses on postmortem iris recognition, not recognition of living individuals.\n\nOption C is incorrect because while the method uses deep learning, the text doesn't claim it's the first to do so in forensic applications.\n\nOption D is incorrect as the paper doesn't mention using postmortem iris scans for unlocking mobile phones. It only mentions phone unlocking as an application of iris recognition for living individuals.\n\nThis question tests the reader's ability to identify the key innovation in the proposed method and distinguish it from existing techniques and applications of iris recognition."}, "8": {"documentation": {"title": "Studies of J/$\\psi$ production at forward rapidity in Pb-Pb collisions\n  at $\\sqrt{s_{\\rm{NN}}}$ = 5.02 TeV", "source": "ALICE Collaboration", "docs_id": "1909.03158", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Studies of J/$\\psi$ production at forward rapidity in Pb-Pb collisions\n  at $\\sqrt{s_{\\rm{NN}}}$ = 5.02 TeV. The inclusive J/$\\psi$ production in Pb-Pb collisions at the center-of-mass energy per nucleon pair $\\sqrt{s_{\\rm{NN}}}$ = 5.02 TeV, measured with the ALICE detector at the CERN LHC, is reported. The J/$\\psi$ meson is reconstructed via the dimuon decay channel at forward rapidity ($2.5<y<4$) down to zero transverse momentum. The suppression of the J/$\\psi$ yield in Pb-Pb collisions with respect to binary-scaled pp collisions is quantified by the nuclear modification factor ($R_{\\rm{AA}}$). The $R_{\\rm{AA}}$ at $\\sqrt{s_{\\rm{NN}}}$ = 5.02 TeV is presented and compared with previous measurements at $\\sqrt{s_{\\rm{NN}}}$ = 2.76 TeV as a function of the centrality of the collision, and of the J/$\\psi$ transverse momentum and rapidity. The inclusive J/$\\psi$ $R_{\\rm{AA}}$ shows a suppression increasing toward higher $p_{\\rm{T}}$, with a steeper dependence for central collisions. The modification of the J/$\\psi$ average $p_{\\rm{T}}$ and $p_{\\rm{T}}^{2}$ is also studied. Comparisons with the results of models based on a transport equation and on statistical hadronization are also carried out."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In the ALICE experiment at CERN LHC, the J/\u03c8 meson production in Pb-Pb collisions at \u221as_NN = 5.02 TeV was studied. Which of the following statements accurately describes the findings regarding the nuclear modification factor (R_AA) for J/\u03c8 production?\n\nA) R_AA shows an increased suppression at lower transverse momentum (p_T) values, with a more gradual dependence in peripheral collisions.\n\nB) R_AA exhibits constant suppression across all p_T ranges, regardless of collision centrality.\n\nC) R_AA demonstrates increased suppression at higher p_T values, with a steeper dependence observed in central collisions.\n\nD) R_AA indicates enhanced J/\u03c8 production at higher p_T values, particularly in the most central collisions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states: \"The inclusive J/\u03c8 R_AA shows a suppression increasing toward higher p_T, with a steeper dependence for central collisions.\" This directly corresponds to the statement in option C.\n\nOption A is incorrect because it describes the opposite trend for both p_T dependence and centrality effects.\n\nOption B is incorrect as it suggests constant suppression, which contradicts the observed p_T and centrality dependence.\n\nOption D is incorrect because it describes enhanced production at higher p_T, which is the opposite of the suppression reported in the study.\n\nThis question tests the student's ability to accurately interpret and recall specific details from complex experimental results in particle physics."}, "9": {"documentation": {"title": "Sorting out quenched jets", "source": "Jasmine Brewer, Jos\\'e Guilherme Milhano, Jesse Thaler", "docs_id": "1812.05111", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sorting out quenched jets. We introduce a new 'quantile' analysis strategy to study the modification of jets as they traverse through a droplet of quark-gluon plasma. To date, most jet modification studies have been based on comparing the jet properties measured in heavy-ion collisions to a proton-proton baseline at the same reconstructed jet transverse momentum ($p_T$). It is well known, however, that the quenching of jets from their interaction with the medium leads to a migration of jets from higher to lower $p_T$, making it challenging to directly infer the degree and mechanism of jet energy loss. Our proposed quantile matching procedure is inspired by (but not reliant on) the approximate monotonicity of energy loss in the jet $p_T$. In this strategy, jets in heavy-ion collisions ordered by $p_T$ are viewed as modified versions of the same number of highest-energy jets in proton-proton collisions, and the fractional energy loss as a function of jet $p_T$ is a natural observable ($Q_{\\rm AA}$). Furthermore, despite non-monotonic fluctuations in the energy loss, we use an event generator to validate the strong correlation between the $p_T$ of the parton that initiates a heavy-ion jet and the $p_T$ of the vacuum jet which corresponds to it via the quantile procedure ($p_T^{\\rm quant}$). We demonstrate that this strategy both provides a complementary way to study jet modification and mitigates the effect of $p_T$ migration in heavy-ion collisions."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the 'quantile' analysis strategy for studying jet modification in quark-gluon plasma, which of the following statements is most accurate?\n\nA) The strategy compares jet properties in heavy-ion collisions to proton-proton baselines at the same reconstructed jet transverse momentum (p_T).\n\nB) Q_AA represents the ratio of jet p_T in heavy-ion collisions to that in proton-proton collisions for jets at the same quantile.\n\nC) The method assumes strict monotonicity of energy loss with respect to jet p_T.\n\nD) p_T^quant refers to the transverse momentum of the parton that initiates a jet in heavy-ion collisions.\n\nCorrect Answer: B\n\nExplanation: \nA is incorrect because the quantile strategy specifically avoids comparing jets at the same reconstructed p_T due to the issue of p_T migration.\n\nB is correct. Q_AA represents the fractional energy loss as a function of jet p_T, which can be interpreted as the ratio described in this option when using the quantile matching procedure.\n\nC is incorrect. While the method is inspired by the approximate monotonicity of energy loss in jet p_T, it does not rely on strict monotonicity and acknowledges non-monotonic fluctuations.\n\nD is incorrect. p_T^quant refers to the p_T of the vacuum (proton-proton) jet that corresponds to a heavy-ion jet via the quantile procedure, not the initiating parton in heavy-ion collisions.\n\nThis question tests understanding of the key concepts and terminology introduced in the quantile analysis strategy, requiring careful distinction between related but different concepts."}, "10": {"documentation": {"title": "Spatial structure and temporal evolution of energetic particle\n  injections in the inner magnetosphere during the 14 July 2013 substorm event", "source": "Matina Gkioulidou, S. Ohtani, D. G. Mitchell, A. Y. Ukhorskiy, G. D.\n  Reeves, D. L. Turner, J. W. Gjerloev, M. Nos\\'e, K. Koga, J. V. Rodriguez,\n  and L. J. Lanzerotti", "docs_id": "1606.02656", "section": ["physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatial structure and temporal evolution of energetic particle\n  injections in the inner magnetosphere during the 14 July 2013 substorm event. Recent results by the Van Allen Probes mission showed that the occurrence of energetic ion injections inside geosynchronous orbit could be very frequent throughout the main phase of a geomagnetic storm. Understanding, therefore, the formation and evolution of energetic particle injections is critical in order to quantify their effect in the inner magnetosphere. We present a case study of a substorm event that occurred during a weak storm $\\textit{ Dst }$ $\\sim$ -40nT on 14 July 2013. Van Allen Probe B, inside geosynchronous orbit, observed two energetic proton injections within 10min, with different dipolarization signatures and duration. The first one is a dispersionless, short-timescale injection pulse accompanied by a sharp dipolarization signature, while the second one is a dispersed, longer-timescale injection pulse accompanied by a gradual dipolarization signature. We combined ground magnetometer data from various stations and in situ particle and magnetic field data from multiple satellites in the inner magnetosphere and near-Earth plasma sheet to determine the spatial extent of these injections, their temporal evolution, and their effects in the inner magnetosphere. Our results indicate that there are different spatial and temporal scales at which injections can occur in the inner magnetosphere and depict the necessity of multipoint observations of both particle and magnetic field data in order to determine these scales."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: During the 14 July 2013 substorm event, Van Allen Probe B observed two distinct energetic proton injections inside geosynchronous orbit. Which of the following correctly describes the characteristics of these injections and their associated dipolarization signatures?\n\nA) The first injection was dispersed with a gradual dipolarization, while the second was dispersionless with a sharp dipolarization.\n\nB) Both injections were dispersionless with sharp dipolarization signatures.\n\nC) The first injection was dispersionless with a sharp dipolarization, while the second was dispersed with a gradual dipolarization.\n\nD) Both injections were dispersed with gradual dipolarization signatures.\n\nCorrect Answer: C\n\nExplanation: The question tests the understanding of the specific characteristics of the two energetic proton injections observed during the 14 July 2013 substorm event. The correct answer is C because the documentation clearly states that \"The first one is a dispersionless, short-timescale injection pulse accompanied by a sharp dipolarization signature, while the second one is a dispersed, longer-timescale injection pulse accompanied by a gradual dipolarization signature.\"\n\nOption A is incorrect as it reverses the characteristics of the two injections. Option B is wrong because it incorrectly states that both injections were dispersionless with sharp dipolarization signatures. Option D is incorrect as it states that both injections were dispersed with gradual dipolarization signatures, which is not consistent with the information provided in the document."}, "11": {"documentation": {"title": "DeepZip: Lossless Data Compression using Recurrent Neural Networks", "source": "Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, Idoia Ochoa", "docs_id": "1811.08162", "section": ["cs.CL", "eess.SP", "q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DeepZip: Lossless Data Compression using Recurrent Neural Networks. Sequential data is being generated at an unprecedented pace in various forms, including text and genomic data. This creates the need for efficient compression mechanisms to enable better storage, transmission and processing of such data. To solve this problem, many of the existing compressors attempt to learn models for the data and perform prediction-based compression. Since neural networks are known as universal function approximators with the capability to learn arbitrarily complex mappings, and in practice show excellent performance in prediction tasks, we explore and devise methods to compress sequential data using neural network predictors. We combine recurrent neural network predictors with an arithmetic coder and losslessly compress a variety of synthetic, text and genomic datasets. The proposed compressor outperforms Gzip on the real datasets and achieves near-optimal compression for the synthetic datasets. The results also help understand why and where neural networks are good alternatives for traditional finite context models"}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary innovation and advantage of the DeepZip compression method as presented in the Arxiv documentation?\n\nA) It uses a combination of Huffman coding and neural networks to achieve lossless compression of sequential data.\n\nB) It employs recurrent neural networks as predictors in conjunction with an arithmetic coder to losslessly compress various types of sequential data.\n\nC) It utilizes convolutional neural networks to identify patterns in data, enabling more efficient compression than traditional methods.\n\nD) It implements a hybrid approach combining both lossy and lossless compression techniques using deep learning models.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation clearly states that DeepZip combines recurrent neural network predictors with an arithmetic coder to achieve lossless compression of sequential data. This approach leverages the power of neural networks as universal function approximators to learn complex patterns in the data, potentially outperforming traditional compression methods.\n\nAnswer A is incorrect because while it mentions neural networks, it incorrectly includes Huffman coding, which is not mentioned in the documentation.\n\nAnswer C is incorrect because it mentions convolutional neural networks, which are not discussed in the given text. The documentation specifically refers to recurrent neural networks.\n\nAnswer D is incorrect because the documentation explicitly states that DeepZip is a lossless compression method. There is no mention of lossy compression or a hybrid approach."}, "12": {"documentation": {"title": "MHD simulations of coronal supra-arcade downflows including anisotropic\n  thermal conduction", "source": "E. Zurbriggen, A. Costa, A. Esquivel, M. Schneiter and M. C\\'ecere", "docs_id": "1703.05802", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MHD simulations of coronal supra-arcade downflows including anisotropic\n  thermal conduction. Coronal supra-arcade downflows (SADs) are observed as dark trails descending towards hot turbulent fan shaped regions. Due to the large temperature values, and gradients in these fan regions the thermal conduction should be very efficient. While several models have been proposed to explain the triggering and the evolution of SADs, none of these scenarios address a systematic consideration of thermal conduction. Thus, we accomplish this task numerically simulating the evolution of SADs within this framework. That is, SADs are conceived as voided (subdense) cavities formed by non-linear waves triggered by downflowing bursty localized reconnection events in a perturbed hot fan. We generate a properly turbulent fan, obtained by a stirring force that permits control of the energy and vorticity input in the medium where SADs develop. We include anisotropic thermal conduction and consider plasma properties consistent with observations. Our aim is to study if it is possible to prevent SADs to vanish by thermal diffusion. We find that this will be the case, depending on the turbulence parameters. In particular, if the magnetic field lines are able to envelope the voided cavities, thermally isolating them from the hot environment. Velocity shear perturbations that are able to generate instabilities of the Kelvin-Helmholtz type help to produce magnetic islands, extending the life-time of SADs."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the role of anisotropic thermal conduction in the evolution of coronal supra-arcade downflows (SADs) according to the MHD simulations discussed in the document?\n\nA) Thermal conduction always causes SADs to vanish rapidly due to high temperatures in the fan regions.\n\nB) Anisotropic thermal conduction has no significant impact on the evolution of SADs.\n\nC) The persistence of SADs depends on the turbulence parameters and magnetic field configuration, which can thermally isolate the voided cavities.\n\nD) Thermal conduction exclusively promotes the formation of SADs by enhancing temperature gradients in the fan regions.\n\nCorrect Answer: C\n\nExplanation: The document states that the study aims to determine if it's possible to prevent SADs from vanishing due to thermal diffusion. The researchers found that this depends on turbulence parameters, particularly if magnetic field lines can envelope the voided cavities, thermally isolating them from the hot environment. This is consistent with option C, which emphasizes the importance of turbulence parameters and magnetic field configuration in the persistence of SADs against thermal conduction effects.\n\nOption A is incorrect because the study shows that SADs can persist under certain conditions. Option B is wrong as the study specifically investigates the impact of anisotropic thermal conduction. Option D misrepresents the role of thermal conduction, as the document doesn't suggest it promotes SAD formation, but rather explores how SADs can survive despite its effects."}, "13": {"documentation": {"title": "A Penalized Multi-trait Mixed Model for Association Mapping in\n  Pedigree-based GWAS", "source": "Jin Liu and Can Yang and Xingjie Shi and Cong Li and Jian Huang and\n  Hongyu Zhao and Shuangge Ma", "docs_id": "1305.4413", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Penalized Multi-trait Mixed Model for Association Mapping in\n  Pedigree-based GWAS. In genome-wide association studies (GWAS), penalization is an important approach for identifying genetic markers associated with trait while mixed model is successful in accounting for a complicated dependence structure among samples. Therefore, penalized linear mixed model is a tool that combines the advantages of penalization approach and linear mixed model. In this study, a GWAS with multiple highly correlated traits is analyzed. For GWAS with multiple quantitative traits that are highly correlated, the analysis using traits marginally inevitably lose some essential information among multiple traits. We propose a penalized-MTMM, a penalized multivariate linear mixed model that allows both the within-trait and between-trait variance components simultaneously for multiple traits. The proposed penalized-MTMM estimates variance components using an AI-REML method and conducts variable selection and point estimation simultaneously using group MCP and sparse group MCP. Best linear unbiased predictor (BLUP) is used to find predictive values and the Pearson's correlations between predictive values and their corresponding observations are used to evaluate prediction performance. Both prediction and selection performance of the proposed approach and its comparison with the uni-trait penalized-LMM are evaluated through simulation studies. We apply the proposed approach to a GWAS data from Genetic Analysis Workshop (GAW) 18."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a genome-wide association study (GWAS) with multiple highly correlated traits, which of the following approaches would be most appropriate to capture both within-trait and between-trait variance components while also performing variable selection?\n\nA) Standard linear mixed model\nB) Penalized linear mixed model (Penalized-LMM)\nC) Penalized multi-trait mixed model (Penalized-MTMM)\nD) Best linear unbiased predictor (BLUP)\n\nCorrect Answer: C\n\nExplanation: \nThe penalized multi-trait mixed model (Penalized-MTMM) is the most appropriate approach for this scenario. Here's why:\n\nA) Standard linear mixed model: While this can account for complicated dependence structures among samples, it doesn't incorporate penalization for variable selection or handle multiple traits simultaneously.\n\nB) Penalized linear mixed model (Penalized-LMM): This combines the advantages of penalization and mixed models, but it's designed for single-trait analysis. It wouldn't capture the essential information among multiple highly correlated traits.\n\nC) Penalized multi-trait mixed model (Penalized-MTMM): This is the correct answer. It's specifically designed to handle multiple highly correlated traits in GWAS. It allows for both within-trait and between-trait variance components simultaneously, while also incorporating penalization for variable selection.\n\nD) Best linear unbiased predictor (BLUP): While BLUP is used in the proposed approach to find predictive values, it's not a complete method for handling multiple traits and variable selection in GWAS.\n\nThe Penalized-MTMM combines the strengths of penalization (for variable selection) and mixed models (for handling complex sample dependencies) while explicitly accounting for the relationships between multiple correlated traits. This makes it the most suitable approach for the described scenario."}, "14": {"documentation": {"title": "Accounting for Skill in Trend, Variability, and Autocorrelation\n  Facilitates Better Multi-Model Projections: Application to the AMOC and\n  Temperature Time Series", "source": "Roman Olson, Soon-Il An, Yanan Fan and Jason P. Evans", "docs_id": "1811.03192", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accounting for Skill in Trend, Variability, and Autocorrelation\n  Facilitates Better Multi-Model Projections: Application to the AMOC and\n  Temperature Time Series. We present a novel quasi-Bayesian method to weight multiple dynamical models by their skill at capturing both potentially non-linear trends and first-order autocorrelated variability of the underlying process, and to make weighted probabilistic projections. We validate the method using a suite of one-at-a-time cross-validation experiments involving Atlantic meridional overturning circulation (AMOC), its temperature-based index, as well as Korean summer mean maximum temperature. In these experiments the method tends to exhibit superior skill over a trend-only Bayesian model averaging weighting method in terms of weight assignment and probabilistic forecasts. Specifically, mean credible interval width, and mean absolute error of the projections tend to improve. We apply the method to a problem of projecting summer mean maximum temperature change over Korea by the end of the 21st century using a multi-model ensemble. Compared to the trend-only method, the new method appreciably sharpens the probability distribution function (pdf) and increases future most likely, median, and mean warming in Korea. The method is flexible, with a potential to improve forecasts in geosciences and other fields."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements best describes the key advantages of the novel quasi-Bayesian method presented in the paper for multi-model projections?\n\nA) It only considers linear trends in the underlying process and ignores variability.\nB) It weights models solely based on their ability to capture first-order autocorrelated variability.\nC) It accounts for both potentially non-linear trends and first-order autocorrelated variability, leading to improved weight assignment and probabilistic forecasts.\nD) It exclusively focuses on improving the mean absolute error of projections without considering credible interval width.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The paper describes a novel quasi-Bayesian method that weights multiple dynamical models based on their skill at capturing both potentially non-linear trends and first-order autocorrelated variability of the underlying process. This comprehensive approach leads to improved weight assignment and probabilistic forecasts, as evidenced by the superior performance in cross-validation experiments.\n\nOption A is incorrect because the method considers potentially non-linear trends, not just linear ones, and also accounts for variability.\n\nOption B is incomplete, as the method considers both trends and variability, not just autocorrelated variability.\n\nOption D is incorrect because the method improves both the mean credible interval width and the mean absolute error of projections, not just the latter.\n\nThe correct answer (C) accurately summarizes the key advantages of the presented method, highlighting its comprehensive approach to model weighting and its resulting improvements in projections."}, "15": {"documentation": {"title": "Non-Equilibrium Large N Yukawa Dynamics: marching through the Landau\n  pole", "source": "Daniel Boyanovsky, Hector J. de Vega, Richard Holman, Matthew R.\n  Martin", "docs_id": "hep-ph/0108113", "section": ["hep-ph", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-Equilibrium Large N Yukawa Dynamics: marching through the Landau\n  pole. The non-equilibrium dynamics of a Yukawa theory with N fermions coupled to a scalar field is studied in the large N limit with the goal of comparing the dynamics predicted from the renormalization group improved effective potential to that obtained including the fermionic backreaction. The effective potential is of the Coleman-Weinberg type. Its renormalization group improvement is unbounded from below and features a Landau pole. When viewed self-consistently, the initial time singularity does not arise. The different regimes of the dynamics of the fully renormalized theory are studied both analytically and numerically. Despite the existence of a Landau pole in the model, the dynamics of the mean field is smooth as it passes the location of the pole. This is a consequence of a remarkable cancellation between the effective potential and the dynamical chiral condensate. The asymptotic evolution is effectively described by a quartic upright effective potential. In all regimes, profuse particle production results in the formation of a dense fermionic plasma with occupation numbers nearly saturated up to a scale of the order of the mean field. This can be interpreted as a chemical potential. We discuss the implications of these results for cosmological preheating."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the study of non-equilibrium dynamics of a Yukawa theory with N fermions coupled to a scalar field in the large N limit, what phenomenon occurs as the mean field passes the location of the Landau pole?\n\nA) The dynamics become discontinuous and exhibit a singularity\nB) The effective potential dominates, causing rapid field oscillations\nC) The chiral condensate suddenly vanishes, leading to a phase transition\nD) A cancellation occurs between the effective potential and the dynamical chiral condensate, resulting in smooth dynamics\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states, \"Despite the existence of a Landau pole in the model, the dynamics of the mean field is smooth as it passes the location of the pole. This is a consequence of a remarkable cancellation between the effective potential and the dynamical chiral condensate.\"\n\nOption A is incorrect because the dynamics remain smooth, not discontinuous.\nOption B is incorrect as there's no mention of rapid field oscillations dominating at the Landau pole.\nOption C is incorrect because the chiral condensate doesn't vanish; instead, it participates in a cancellation effect.\n\nThis question tests understanding of the complex interplay between different components in the non-equilibrium dynamics of the Yukawa theory, particularly the behavior around the Landau pole."}, "16": {"documentation": {"title": "Superradiant instability of the Kerr-like black hole in\n  Einstein-bumblebee gravity", "source": "Rui Jiang, Rui-Hui Lin and Xiang-Hua Zhai", "docs_id": "2108.04702", "section": ["gr-qc", "astro-ph.HE", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Superradiant instability of the Kerr-like black hole in\n  Einstein-bumblebee gravity. An exact Kerr-like solution has been obtained recently in Einstein-bumblebee gravity model where Lorentz symmetry is spontaneously broken. In this paper, we investigate the superradiant instability of the Kerr-like black hole under the perturbation of a massive scalar field. We find the Lorentz breaking parameter $L$ does not affect the superradiance regime or the regime of the bound states. However, since $L$ appears in the metric and its effect cannot be erased by redefining the rotation parameter $\\tilde{a}=\\sqrt{1+L}a$, it indeed affects the bound state spectrum and the superradiance. We calculate the bound state spectrum via the continued-fraction method and show the influence of $L$ on the maximum binding energy and the damping rate. The superradiant instability could occur since the superradiance condition and the bound state condition could be both satisfied. Compared with Kerr black hole, the nature of the superradiant instability of this black hole depends non-monotonously not only on the rotation parameter of the black hole $\\tilde{a}$ and the product of the black hole mass $M$ and the field mass $\\mu$, but also on the Lorentz breaking parameter $L$. Through the Monte Carlo method, we find that for $l=m=1$ state the most unstable mode occurs at $L=-0.79637$, $\\tilde{a}/M=0.99884$ and $M\\mu=0.43920$, with the maximum growth rate of the field $\\omega_{I}M=1.676\\times10^{-6}$, which is about 10 times of that in Kerr black hole."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the study of superradiant instability of the Kerr-like black hole in Einstein-bumblebee gravity, which of the following statements is correct regarding the Lorentz breaking parameter L?\n\nA) L affects both the superradiance regime and the regime of bound states.\nB) L has no effect on the bound state spectrum or superradiance.\nC) L appears in the metric but its effect can be erased by redefining the rotation parameter.\nD) L influences the bound state spectrum and superradiance without affecting their regimes.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the complex relationship between the Lorentz breaking parameter L and various aspects of the Kerr-like black hole in Einstein-bumblebee gravity. \n\nOption A is incorrect because the documentation explicitly states that L does not affect the superradiance regime or the regime of bound states.\n\nOption B is wrong as the text clearly indicates that L does affect the bound state spectrum and superradiance.\n\nOption C is incorrect because while L does appear in the metric, the documentation specifically mentions that its effect cannot be erased by redefining the rotation parameter.\n\nOption D is the correct answer. The text states that L does not affect the superradiance regime or the regime of bound states, but it does influence the bound state spectrum and superradiance. This is because L appears in the metric and its effect cannot be eliminated by redefining the rotation parameter.\n\nThis question requires careful reading and understanding of the nuanced effects of the Lorentz breaking parameter in this complex physical system."}, "17": {"documentation": {"title": "Metastatic Cancer Image Classification Based On Deep Learning Method", "source": "Guanwen Qiu, Xiaobing Yu, Baolin Sun, Yunpeng Wang, Lipei Zhang", "docs_id": "2011.06984", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Metastatic Cancer Image Classification Based On Deep Learning Method. Using histopathological images to automatically classify cancer is a difficult task for accurately detecting cancer, especially to identify metastatic cancer in small image patches obtained from larger digital pathology scans. Computer diagnosis technology has attracted wide attention from researchers. In this paper, we propose a noval method which combines the deep learning algorithm in image classification, the DenseNet169 framework and Rectified Adam optimization algorithm. The connectivity pattern of DenseNet is direct connections from any layer to all consecutive layers, which can effectively improve the information flow between different layers. With the fact that RAdam is not easy to fall into a local optimal solution, and it can converge quickly in model training. The experimental results shows that our model achieves superior performance over the other classical convolutional neural networks approaches, such as Vgg19, Resnet34, Resnet50. In particular, the Auc-Roc score of our DenseNet169 model is 1.77% higher than Vgg19 model, and the Accuracy score is 1.50% higher. Moreover, we also study the relationship between loss value and batches processed during the training stage and validation stage, and obtain some important and interesting findings."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the advantages of the proposed method for metastatic cancer image classification?\n\nA) It combines DenseNet169 with traditional statistical analysis for improved accuracy.\nB) It uses a novel optimization algorithm that avoids local optima and converges quickly.\nC) It employs transfer learning from pre-trained models on large natural image datasets.\nD) It introduces a new CNN architecture specifically designed for histopathological images.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage states that the proposed method \"combines the deep learning algorithm in image classification, the DenseNet169 framework and Rectified Adam optimization algorithm.\" It specifically mentions that \"RAdam is not easy to fall into a local optimal solution, and it can converge quickly in model training.\" This directly aligns with the statement in option B.\n\nOption A is incorrect because while the method does use DenseNet169, it doesn't mention combining it with traditional statistical analysis.\n\nOption C is incorrect as there's no mention of transfer learning or pre-trained models on natural image datasets in the given text.\n\nOption D is incorrect because the method uses the existing DenseNet169 architecture rather than introducing a new CNN architecture specifically for histopathological images.\n\nThis question tests the student's ability to comprehend the key aspects of the proposed method and identify its main advantages as described in the text."}, "18": {"documentation": {"title": "Lower bound the T-count via unitary stabilizer nullity", "source": "Jiaqing Jiang, Xin Wang", "docs_id": "2103.09999", "section": ["quant-ph", "cs.CC", "cs.IT", "hep-th", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lower bound the T-count via unitary stabilizer nullity. We introduce magic measures for multi-qubit quantum gates and establish lower bounds on the non-Clifford resources for fault-tolerant quantum computation. First, we introduce the stabilizer nullity of an arbitrary multi-qubit unitary, which is based on the subgroup of the quotient Pauli group associated with the unitary. This unitary stabilizer nullity extends the state stabilizer nullity by Beverland et al. to a dynamic version. We in particular show this magic measure has desirable properties such as sub-additivity under composition and additivity under tensor product. Second, we prove that a given unitary's stabilizer nullity is a lower bound for the T-count, utilizing the above properties in gate synthesis. Third, we compare the state and the unitary stabilizer nullity, proving that the lower bounds for the T-count obtained by the unitary stabilizer nullity are never less than the state stabilizer nullity. Moreover, we show an explicit $n$-qubit unitary family of unitary stabilizer nullity $2n$, which implies that its T-count is at least $2n$. This gives an example where the bounds derived by the unitary stabilizer nullity strictly outperform the state stabilizer nullity by a factor of $2$. We further connect the unitary stabilizer nullity and the state stabilizer nullity with auxiliary systems, showing that adding auxiliary systems and choosing proper stabilizer states can strictly improving the lower bound obtained by the state stabilizer nullity."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about the unitary stabilizer nullity is NOT correct?\n\nA) It is sub-additive under composition of quantum gates\nB) It is additive under tensor product of quantum gates\nC) It always provides a tighter lower bound on the T-count compared to the state stabilizer nullity\nD) It can be used to establish lower bounds on non-Clifford resources for fault-tolerant quantum computation\n\nCorrect Answer: C\n\nExplanation: \nA is correct: The documentation states that the unitary stabilizer nullity has \"desirable properties such as sub-additivity under composition.\"\n\nB is correct: The text mentions that the unitary stabilizer nullity has \"additivity under tensor product.\"\n\nC is incorrect: While the unitary stabilizer nullity can provide better lower bounds than the state stabilizer nullity, it doesn't always do so. The document states that \"the lower bounds for the T-count obtained by the unitary stabilizer nullity are never less than the state stabilizer nullity,\" but this doesn't mean it's always tighter. In fact, an example is given where it outperforms the state stabilizer nullity by a factor of 2, implying that in some cases they might be equal.\n\nD is correct: The introduction explicitly states that the unitary stabilizer nullity is used to \"establish lower bounds on the non-Clifford resources for fault-tolerant quantum computation.\"\n\nTherefore, C is the statement that is NOT correct, making it the right answer for this question."}, "19": {"documentation": {"title": "Efficient expulsion of magnetic flux in superconducting RF cavities for\n  high $Q_0$ applications", "source": "S. Posen, A. Grassellino, A. Romanenko, O. Melnychuk, D. A.\n  Sergatskov, M. Martinello, M. Checchin, and A. C. Crawford", "docs_id": "1509.03957", "section": ["physics.acc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient expulsion of magnetic flux in superconducting RF cavities for\n  high $Q_0$ applications. Even when cooled through its transition temperature in the presence of an external magnetic field, a superconductor can expel nearly all external magnetic flux. This Letter presents an experimental study to identify the parameters that most strongly influence flux trapping in high purity niobium during cooldown. This is critical to the operation of superconducting radiofrequency cavities, in which trapped flux degrades the quality factor and therefore cryogenic efficiency. Flux expulsion was measured on a large survey of 1.3 GHz cavities prepared in various ways. It is shown that both spatial thermal gradient and high temperature treatment are critical to expelling external magnetic fields, while surface treatment has minimal effect. For the first time, it is shown that a cavity can be converted from poor expulsion behavior to strong expulsion behavior after furnace treatment, resulting in a substantial improvement in quality factor. Future plans are described to build on this result in order to optimize treatment for future cavities."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: In the context of superconducting RF cavities, which combination of factors most significantly influences the expulsion of magnetic flux, leading to improved quality factors?\n\nA) Surface treatment and cooling rate\nB) Spatial thermal gradient and low temperature annealing\nC) Spatial thermal gradient and high temperature treatment\nD) External magnetic field strength and cavity material purity\n\nCorrect Answer: C\n\nExplanation: The text specifically states that \"both spatial thermal gradient and high temperature treatment are critical to expelling external magnetic fields.\" It also mentions that surface treatment has minimal effect on flux expulsion. The cooling rate is not explicitly discussed as a major factor. While material purity and external magnetic field strength are relevant to superconductivity, they are not identified as the most critical factors for flux expulsion in this study. The correct answer highlights the two factors that the research found to be most influential in improving flux expulsion, which in turn leads to better quality factors in superconducting RF cavities."}, "20": {"documentation": {"title": "SGV 3.0 - a fast detector simulation", "source": "Mikael Berggren (DESY)", "docs_id": "1203.0217", "section": ["physics.ins-det", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SGV 3.0 - a fast detector simulation. The need for fast simulation programs is emphasised, both in terms of the need for \"rapid response\" to new results - in particular from the LHC - and new theoretical ideas, and in terms of how to cope with multi-billion simulated event samples. The latter would arise both from the need to be able to simulate significantly more events than expected in the real data, also for high cross-section processes, and the need to scan multi-parameter theories. The {\\it Simulation \\`a Grande Vitesse}, SGV, is presented, and is shown to be able to address these issues. The tracking performance of SGV is shown to reproduce very closely that of the full simulation and reconstruction of the ILD concept. Preliminary results on how to also closely emulate the calorimetric performance from full simulation is presented. The procedure is parametric, with no the need to simulate the detailed shower development, and promises to be many orders of magnitude faster than such approaches. Contrary to what is often the case with fast simulation programs, the procedure gives a somewhat {\\it pessimistic} result, compared to the full simulation and reconstruction."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the key advantages and characteristics of the SGV 3.0 fast detector simulation program?\n\nA) It provides optimistic results compared to full simulation and reconstruction, making it ideal for best-case scenario planning.\n\nB) It uses detailed shower development simulations to achieve high accuracy in calorimetric performance emulation.\n\nC) It closely reproduces tracking performance of full simulation, uses a parametric approach for calorimetry, and is significantly faster than traditional methods.\n\nD) It is primarily designed for small-scale simulations and is not suitable for multi-billion event samples.\n\nCorrect Answer: C\n\nExplanation: \nOption C is correct because it accurately summarizes the key features of SGV 3.0 as described in the text. The document states that SGV's tracking performance \"reproduce[s] very closely that of the full simulation and reconstruction.\" It also mentions a \"parametric\" approach for calorimetric performance that doesn't require detailed shower development simulation, making it \"many orders of magnitude faster.\" Lastly, it's explicitly stated that SGV can handle \"multi-billion simulated event samples.\"\n\nOption A is incorrect because the text specifically mentions that SGV gives \"somewhat pessimistic\" results compared to full simulation, not optimistic ones.\n\nOption B is wrong because SGV uses a parametric approach that doesn't require simulating detailed shower development.\n\nOption D is incorrect as the text emphasizes SGV's ability to handle large-scale simulations, including \"multi-billion simulated event samples.\""}, "21": {"documentation": {"title": "Scale-Invariant Structures of Spiral Waves", "source": "Daniel Sohn, Konstantinos N. Aronis, Hiroshi Ashikaga", "docs_id": "1806.10768", "section": ["nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scale-Invariant Structures of Spiral Waves. Spiral waves are considered to be one of the potential mechanisms that maintains complex arrhythmias such as atrial and ventricular fibrillation. The aim of the present study was to quantify the complex dynamics of spiral waves as the organizing manifolds of information flow at multiple scales. We simulated spiral waves using a numerical model of cardiac excitation in a two-dimensional (2-D) lattice. We created a renormalization group by coarse graining and re-scaling the original time series in multiple spatiotemporal scales, and quantified the Lagrangian coherent structures (LCS) of the information flow underlying the spiral waves. To quantify the scale-invariant structures, we compared the value of finite-time Lyapunov exponent (FTLE) between the corresponding components of the 2-D lattice in each spatiotemporal scale of the renormalization group with that of the original scale. Both the repelling and the attracting LCS changed across the different spatial and temporal scales of the renormalization group. However, despite the change across the scales, some LCS were scale-invariant. The patterns of those scale-invariant structures were not obvious from the trajectory of the spiral waves based on voltage mapping of the lattice. Some Lagrangian coherent structures of information flow underlying spiral waves are preserved across multiple spatiotemporal scales."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the relationship between Lagrangian coherent structures (LCS) and the scale-invariant properties of spiral waves in the study?\n\nA) All LCS remained constant across different spatiotemporal scales of the renormalization group.\n\nB) The repelling LCS were scale-invariant, while the attracting LCS changed across different scales.\n\nC) Some LCS exhibited scale-invariance, despite overall changes in both repelling and attracting LCS across scales.\n\nD) The scale-invariant LCS were directly observable from the voltage mapping of the spiral wave trajectories.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that \"Both the repelling and the attracting LCS changed across the different spatial and temporal scales of the renormalization group. However, despite the change across the scales, some LCS were scale-invariant.\" This indicates that while there were overall changes in LCS across scales, some structures maintained scale-invariance.\n\nOption A is incorrect because the passage clearly states that LCS changed across scales, not that all remained constant.\n\nOption B is incorrect as it misrepresents the information given. The study doesn't specify that only repelling LCS were scale-invariant; it states that some LCS, in general, showed scale-invariance.\n\nOption D is incorrect because the passage explicitly mentions that \"The patterns of those scale-invariant structures were not obvious from the trajectory of the spiral waves based on voltage mapping of the lattice.\"\n\nThis question tests the student's ability to comprehend and synthesize complex information about the scale-invariant properties of LCS in spiral waves, requiring a nuanced understanding of the study's findings."}, "22": {"documentation": {"title": "Variable selection in doubly truncated regression", "source": "Ming Zheng, Chanjuan Lin, Wen Yu", "docs_id": "2110.10393", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variable selection in doubly truncated regression. Doubly truncated data arise in many areas such as astronomy, econometrics, and medical studies. For the regression analysis with doubly truncated response variables, the existence of double truncation may bring bias for estimation as well as affect variable selection. We propose a simultaneous estimation and variable selection procedure for the doubly truncated regression, allowing a diverging number of regression parameters. To remove the bias introduced by the double truncation, a Mann-Whitney-type loss function is used. The adaptive LASSO penalty is then added into the loss function to achieve simultaneous estimation and variable selection. An iterative algorithm is designed to optimize the resulting objective function. We establish the consistency and the asymptotic normality of the proposed estimator. The oracle property of the proposed selection procedure is also obtained. Some simulation studies are conducted to show the finite sample performance of the proposed approach. We also apply the method to analyze a real astronomical data."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of doubly truncated regression, which of the following statements is NOT correct?\n\nA) The adaptive LASSO penalty is used to achieve simultaneous estimation and variable selection.\n\nB) The proposed method uses a Mann-Whitney-type loss function to address bias introduced by double truncation.\n\nC) The oracle property of the proposed selection procedure is established for a fixed number of regression parameters only.\n\nD) The consistency and asymptotic normality of the proposed estimator are established in the study.\n\nCorrect Answer: C\n\nExplanation:\nA is correct according to the text: \"The adaptive LASSO penalty is then added into the loss function to achieve simultaneous estimation and variable selection.\"\n\nB is correct as stated: \"To remove the bias introduced by the double truncation, a Mann-Whitney-type loss function is used.\"\n\nC is incorrect. The text mentions \"allowing a diverging number of regression parameters\" and states that \"The oracle property of the proposed selection procedure is also obtained.\" It does not limit this to a fixed number of parameters.\n\nD is correct as explicitly stated: \"We establish the consistency and the asymptotic normality of the proposed estimator.\"\n\nThe correct answer is C because it contradicts the information provided in the document, while the other options are accurately derived from the given text."}, "23": {"documentation": {"title": "Coherent Contributions of Nuclear Mesons to Electroproduction and the\n  HERMES Effect", "source": "G. A. Miller, S. J. Brodsky, and M. Karliner", "docs_id": "hep-ph/0002156", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coherent Contributions of Nuclear Mesons to Electroproduction and the\n  HERMES Effect. We show that nuclear sigma, omega, and pi mesons can contribute coherently to enhance the electroproduction cross section on nuclei for longitudinal virtual photons at low Q^2 while depleting the cross section for transverse photons. We are able to describe recent HERMES inelastic lepton-nucleus scattering data at low Q^2 and small x using photon-meson and meson-nucleus couplings which are consistent with (but not determined by) existing constraints from meson decay widths, nuclear structure, deep inelastic scattering, and lepton pair production data. We find that while nuclear-coherent pion currents are not important for the present data, they could be observed at different kinematics. Our model for coherent meson electroproduction requires the assumption of mesonic currents and couplings which can be verified in separate experiments. The observation of nuclear-coherent mesons in the final state would verify our theory and allow the identification of a specific dynamical mechanism for higher-twist processes."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the role of nuclear mesons in the HERMES effect, according to the Arxiv documentation?\n\nA) Nuclear mesons contribute incoherently to suppress both longitudinal and transverse virtual photon cross sections at all Q^2 values.\n\nB) Coherent contributions from nuclear sigma, omega, and pi mesons enhance the electroproduction cross section for longitudinal virtual photons at high Q^2 while increasing the cross section for transverse photons.\n\nC) Nuclear-coherent pion currents are the primary mechanism responsible for the observed HERMES effect in current experimental data.\n\nD) Coherent contributions from nuclear sigma, omega, and pi mesons enhance the electroproduction cross section for longitudinal virtual photons at low Q^2 while depleting the cross section for transverse photons.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that \"nuclear sigma, omega, and pi mesons can contribute coherently to enhance the electroproduction cross section on nuclei for longitudinal virtual photons at low Q^2 while depleting the cross section for transverse photons.\" This directly aligns with option D.\n\nOption A is incorrect because the mesons contribute coherently, not incoherently, and they enhance rather than suppress the cross section for longitudinal virtual photons.\n\nOption B is incorrect because the enhancement occurs at low Q^2, not high Q^2, and the cross section for transverse photons is depleted, not increased.\n\nOption C is incorrect because the documentation states that \"nuclear-coherent pion currents are not important for the present data,\" indicating that they are not the primary mechanism for the observed HERMES effect in current experimental data."}, "24": {"documentation": {"title": "Water at Positive and Negative Pressures", "source": "H. E. Stanley, M. C. Barbosa, S. Mossa, P. A. Netz, F. Sciortino, F.\n  W. Starr, and M. Yamada", "docs_id": "cond-mat/0206542", "section": ["cond-mat.soft", "cond-mat.mtrl-sci", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Water at Positive and Negative Pressures. We review recent results of molecular dynamics simulations of two models of liquid water, the extended simple point charge (SPC/E) and the Mahoney-Jorgensen transferable intermolecular potential with five points (TIP5P), which is closer to real water than previously-proposed classical pairwise additive potentials. Simulations of the TIP5P model for a wide range of deeply supercooled states, including both positive and negative pressures, reveal (i) the existence of a non-monotonic ``nose-shaped'' temperature of maximum density (TMD) line and a non-reentrant spinodal, (ii) the presence of a low temperature phase transition. The TMD that changes slope from negative to positive as P decreases and, notably, the point of crossover between the two behaviors is located at ambient pressure (temperature approx 4 C, and density approx 1 g/cm^3). Simulations on the dynamics of the SPC/E model reveal (iii) the dynamics at negative pressure shows a minimum in the diffusion constant $D$ when the density is decreased at constant temperature, complementary to the known maximum of D at higher pressures, and (iv) the loci of minima of D relative to the spinodal shows that they are inside the thermodynamically metastable regions of the phase-diagram. These dynamical results reflect the initial enhancement and subsequent breakdown of the tetrahedral structure and of the hydrogen bond network as the density decreases."}, "answer": "Question: Which of the following statements accurately describes the behavior of the Temperature of Maximum Density (TMD) line for the TIP5P model of liquid water, as revealed by molecular dynamics simulations?\n\nA) The TMD line is monotonic and shows a positive slope at all pressures.\n\nB) The TMD line is non-monotonic, with a \"nose-shaped\" curve that changes from a negative to a positive slope as pressure increases.\n\nC) The TMD line is non-monotonic, with a \"nose-shaped\" curve that changes from a negative to a positive slope as pressure decreases, with the crossover point at ambient pressure.\n\nD) The TMD line is linear and intersects the spinodal at low temperatures.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that simulations of the TIP5P model reveal \"the existence of a non-monotonic 'nose-shaped' temperature of maximum density (TMD) line\" and that \"The TMD that changes slope from negative to positive as P decreases and, notably, the point of crossover between the two behaviors is located at ambient pressure.\" This description matches exactly with option C.\n\nOption A is incorrect because it describes a monotonic TMD line with a positive slope, which contradicts the non-monotonic, \"nose-shaped\" behavior described in the passage.\n\nOption B is incorrect because it describes the slope change occurring as pressure increases, while the passage states that it occurs as pressure decreases.\n\nOption D is incorrect because it describes a linear TMD line, which does not match the non-monotonic, \"nose-shaped\" behavior described in the passage. Additionally, there is no mention of the TMD line intersecting the spinodal in the given information."}, "25": {"documentation": {"title": "Optimally Investing to Reach a Bequest Goal", "source": "Erhan Bayraktar and Virginia R. Young", "docs_id": "1503.00961", "section": ["q-fin.MF", "math.OC", "math.PR", "q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimally Investing to Reach a Bequest Goal. We determine the optimal strategy for investing in a Black-Scholes market in order to maximize the probability that wealth at death meets a bequest goal $b$, a type of goal-seeking problem, as pioneered by Dubins and Savage (1965, 1976). The individual consumes at a constant rate $c$, so the level of wealth required for risklessly meeting consumption equals $c/r$, in which $r$ is the rate of return of the riskless asset. Our problem is related to, but different from, the goal-reaching problems of Browne (1997). First, Browne (1997, Section 3.1) maximizes the probability that wealth reaches $b < c/r$ before it reaches $a < b$. Browne's game ends when wealth reaches $b$. By contrast, for the problem we consider, the game continues until the individual dies or until wealth reaches 0; reaching $b$ and then falling below it before death does not count. Second, Browne (1997, Section 4.2) maximizes the expected discounted reward of reaching $b > c/r$ before wealth reaches $c/r$. If one interprets his discount rate as a hazard rate, then our two problems are {\\it mathematically} equivalent for the special case for which $b > c/r$, with ruin level $c/r$. However, we obtain different results because we set the ruin level at 0, thereby allowing the game to continue when wealth falls below $c/r$."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of optimally investing to reach a bequest goal in a Black-Scholes market, how does the problem described in the given text differ from Browne's (1997) goal-reaching problems?\n\nA) The described problem ends when wealth reaches the bequest goal b, while Browne's problem continues until death or ruin.\n\nB) The described problem allows wealth to fall below c/r and continue, while Browne's problem ends when wealth reaches c/r.\n\nC) The described problem maximizes the expected discounted reward of reaching b, while Browne's problem maximizes the probability of reaching b.\n\nD) The described problem sets the ruin level at c/r, while Browne's problem sets it at 0.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The key differences described in the text are:\n\n1. In the described problem, the \"game\" continues until death or until wealth reaches 0, even if wealth falls below c/r. In contrast, Browne's problem (Section 4.2) ends when wealth reaches c/r (interpreted as the ruin level in his model).\n\n2. The described problem sets the ruin level at 0, allowing the game to continue when wealth falls below c/r, while Browne's model doesn't allow this.\n\n3. The described problem doesn't end when wealth reaches b (the bequest goal), unlike Browne's problem in Section 3.1.\n\n4. While there's a mathematical equivalence for b > c/r, the different ruin levels lead to different results.\n\nOption A is incorrect because it reverses the continuation conditions. Option C is incorrect because both problems involve maximizing probabilities, not expected discounted rewards. Option D is incorrect because it reverses the ruin levels for the two problems."}, "26": {"documentation": {"title": "The (super)conformal BMS$_3$ algebra", "source": "Oscar Fuentealba, Hernan A. Gonzalez, Alfredo Perez, David Tempo and\n  Ricardo Troncoso", "docs_id": "2011.08197", "section": ["hep-th", "gr-qc", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The (super)conformal BMS$_3$ algebra. The conformal extension of the BMS$_{3}$ algebra is constructed. Apart from an infinite number of 'superdilatations,' in order to incorporate 'superspecial conformal transformations,' the commutator of the latter with supertranslations strictly requires the presence of nonlinear terms in the remaining generators. The algebra appears to be very rigid, in the sense that its central extensions as well as the nonlinear terms coefficients become determined by the central charge of the Virasoro subalgebra. The wedge algebra corresponds to the conformal group in three spacetime dimensions $SO(3,2)$, so that the full algebra can also be interpreted as an infinite-dimensional nonlinear extension of the AdS$_{4}$ algebra with nontrivial central charges. Moreover, since the Lorentz subalgebra ($sl(2,R)$) is non-principally embedded within the conformal (wedge) algebra, according to the conformal weight of the generators, the conformal extension of BMS$_{3}$ can be further regarded as a $W_{(2,2,2,1)}$ algebra. An explicit canonical realization of the conformal extension of BMS$_{3}$ is then shown to emerge from the asymptotic structure of conformal gravity in 3D, endowed with a new set of boundary conditions. The supersymmetric extension is also briefly addressed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the nature and implications of the conformal extension of the BMS\u2083 algebra as presented in the Arxiv documentation?\n\nA) The conformal extension of BMS\u2083 is a linear algebra with a finite number of generators, corresponding to the Poincar\u00e9 group in four dimensions.\n\nB) The algebra includes an infinite number of 'superdilatations' and 'superspecial conformal transformations', but maintains a linear structure without central extensions.\n\nC) The conformal extension of BMS\u2083 is a nonlinear algebra with infinite-dimensional generators, central extensions determined by the Virasoro subalgebra's central charge, and can be interpreted as an extension of the AdS\u2084 algebra.\n\nD) The wedge algebra of the conformal extension corresponds to the conformal group in four spacetime dimensions SO(4,2), with a principally embedded Lorentz subalgebra.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures several key aspects of the conformal extension of BMS\u2083 as described in the documentation:\n\n1. It mentions the nonlinear nature of the algebra, which is emphasized in the text when discussing the commutator of superspecial conformal transformations with supertranslations.\n2. It correctly states that the algebra has infinite-dimensional generators, including 'superdilatations' and 'superspecial conformal transformations'.\n3. It notes that the central extensions are determined by the central charge of the Virasoro subalgebra, which is explicitly stated in the document.\n4. It accurately describes the algebra as an extension of the AdS\u2084 algebra, which is mentioned in the text.\n\nAnswer A is incorrect because it describes a finite, linear algebra related to the Poincar\u00e9 group, which contradicts the infinite-dimensional, nonlinear nature of the described algebra.\n\nAnswer B is incorrect because it fails to acknowledge the nonlinear terms and central extensions, which are crucial features of the algebra.\n\nAnswer D is incorrect on two counts: the wedge algebra corresponds to SO(3,2) (AdS\u2084), not SO(4,2), and the Lorentz subalgebra is explicitly stated to be non-principally embedded within the conformal algebra."}, "27": {"documentation": {"title": "Improved rates for prediction and identification of partially observed\n  linear dynamical systems", "source": "Holden Lee", "docs_id": "2011.10006", "section": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved rates for prediction and identification of partially observed\n  linear dynamical systems. Identification of a linear time-invariant dynamical system from partial observations is a fundamental problem in control theory. Particularly challenging are systems exhibiting long-term memory. A natural question is how learn such systems with non-asymptotic statistical rates depending on the inherent dimensionality (order) $d$ of the system, rather than on the possibly much larger memory length. We propose an algorithm that given a single trajectory of length $T$ with gaussian observation noise, learns the system with a near-optimal rate of $\\widetilde O\\left(\\sqrt\\frac{d}{T}\\right)$ in $\\mathcal{H}_2$ error, with only logarithmic, rather than polynomial dependence on memory length. We also give bounds under process noise and improved bounds for learning a realization of the system. Our algorithm is based on multi-scale low-rank approximation: SVD applied to Hankel matrices of geometrically increasing sizes. Our analysis relies on careful application of concentration bounds on the Fourier domain -- we give sharper concentration bounds for sample covariance of correlated inputs and for $\\mathcal H_\\infty$ norm estimation, which may be of independent interest."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of learning partially observed linear dynamical systems, which of the following statements best describes the key innovation and performance of the proposed algorithm?\n\nA) It achieves a learning rate of O(d/T) in H2 error, with polynomial dependence on memory length.\n\nB) It uses single-scale high-rank approximation and achieves optimal performance in the presence of process noise.\n\nC) It attains a near-optimal rate of \u00d5(\u221a(d/T)) in H2 error, with logarithmic dependence on memory length, using multi-scale low-rank approximation.\n\nD) It focuses on systems with short-term memory and achieves optimal performance using Fourier domain analysis without concentration bounds.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The question tests understanding of the key aspects of the proposed algorithm and its performance characteristics as described in the documentation.\n\nA is incorrect because the learning rate is \u00d5(\u221a(d/T)), not O(d/T), and the dependence on memory length is logarithmic, not polynomial.\n\nB is incorrect as the algorithm uses multi-scale low-rank approximation, not single-scale high-rank approximation. While the document mentions improved bounds under process noise, this is not the key innovation.\n\nC is correct. It accurately describes the algorithm's performance (near-optimal rate of \u00d5(\u221a(d/T)) in H2 error), its key feature (logarithmic dependence on memory length), and the method used (multi-scale low-rank approximation).\n\nD is incorrect because the algorithm focuses on systems exhibiting long-term memory, not short-term. Additionally, while Fourier domain analysis is used, the concentration bounds are a crucial part of the analysis, not absent from it.\n\nThis question tests the student's ability to synthesize information from the documentation and identify the key aspects of the proposed algorithm and its performance characteristics."}, "28": {"documentation": {"title": "Complexity Measures and Features for Times Series classification", "source": "Francisco J. Bald\\'an and Jos\\'e M. Ben\\'itez", "docs_id": "2002.12036", "section": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity Measures and Features for Times Series classification. Classification of time series is a growing problem in different disciplines due to the progressive digitalization of the world. Currently, the state-of-the-art in time series classification is dominated by The Hierarchical Vote Collective of Transformation-based Ensembles. This algorithm is composed of several classifiers of different domains distributed in five large modules. The combination of the results obtained by each module weighed based on an internal evaluation process allows this algorithm to obtain the best results in state-of-the-art. One Nearest Neighbour with Dynamic Time Warping remains the base classifier in any time series classification problem for its simplicity and good results. Despite their performance, they share a weakness, which is that they are not interpretable. In the field of time series classification, there is a tradeoff between accuracy and interpretability. In this work, we propose a set of characteristics capable of extracting information on the structure of the time series to face time series classification problems. The use of these characteristics allows the use of traditional classification algorithms in time series problems. The experimental results of our proposal show no statistically significant differences from the second and third best models of the state-of-the-art. Apart from competitive results in accuracy, our proposal is able to offer interpretable results based on the set of characteristics proposed"}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the trade-off in time series classification and the contribution of the proposed set of characteristics in this study?\n\nA) The trade-off is between speed and accuracy, and the proposed characteristics improve processing time while maintaining state-of-the-art accuracy.\n\nB) The trade-off is between interpretability and accuracy, and the proposed characteristics offer competitive accuracy while providing interpretable results.\n\nC) The trade-off is between complexity and accuracy, and the proposed characteristics simplify the classification process at the cost of slightly reduced accuracy.\n\nD) The trade-off is between generalizability and accuracy, and the proposed characteristics improve performance across various domains but with lower overall accuracy.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage explicitly states that \"In the field of time series classification, there is a tradeoff between accuracy and interpretability.\" The authors propose a set of characteristics that can extract information on the structure of time series for classification problems. The text mentions that their proposal shows \"no statistically significant differences from the second and third best models of the state-of-the-art\" in terms of accuracy, while also being \"able to offer interpretable results based on the set of characteristics proposed.\" This directly addresses the trade-off between interpretability and accuracy, making B the most accurate statement among the options provided."}, "29": {"documentation": {"title": "Real-space imaging of confined magnetic skyrmion tubes", "source": "M. T. Birch, D. Cort\\'es-Ortu\\~no, L. A. Turnbull, M. N. Wilson, F.\n  Gro{\\ss}, N. Tr\\\"ager, A. Laurenson, N. Bukin, S. H. Moody, M. Weigand, G.\n  Sch\\\"utz, H. Popescu, R. Fan, P. Steadman, J. A. T. Verezhak, G.\n  Balakrishnan, J. C. Loudon, A. C. Twitchett-Harrison, O. Hovorka, H. Fangohr,\n  F. Ogrin, J. Gr\\\"afe, P. D. Hatton", "docs_id": "1909.04528", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Real-space imaging of confined magnetic skyrmion tubes. Magnetic skyrmions are topologically nontrivial particles with a potential application as information elements in future spintronic device architectures. While they are commonly portrayed as two dimensional objects, in reality magnetic skyrmions are thought to exist as elongated, tube-like objects extending through the thickness of the sample. Study of this skyrmion tube (SkT) state is highly relevant for investigating skyrmion metastability and for implementation in recently proposed magnonic computing. However, direct experimental imaging of skyrmion tubes has yet to be reported. Here, we demonstrate the first real-space observation of skyrmion tubes in a lamella of FeGe using resonant magnetic x-ray imaging and comparative micromagnetic simulations, confirming their extended structure. The formation of these structures at the edge of the sample highlights the importance of confinement and edge effects in the stabilisation of the SkT state, opening the door to further investigations into this unexplored dimension of the skyrmion spin texture."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the significance of the research findings on skyrmion tubes (SkTs) in FeGe?\n\nA) The study proves that skyrmions are purely two-dimensional objects, contradicting previous theories about their structure.\n\nB) The research demonstrates the first real-space observation of skyrmion tubes, confirming their extended structure and highlighting the importance of confinement and edge effects in their stabilization.\n\nC) The findings suggest that skyrmion tubes have no potential applications in spintronic devices or magnonic computing.\n\nD) The study shows that skyrmion tubes can only be observed using electron microscopy techniques, not x-ray imaging.\n\nCorrect Answer: B\n\nExplanation: \nThe correct answer is B. The research described in the text represents the first direct experimental imaging of skyrmion tubes in a lamella of FeGe using resonant magnetic x-ray imaging. This observation confirms the extended, tube-like structure of skyrmions through the thickness of the sample, which was previously theorized but not directly observed. Additionally, the formation of these structures at the sample's edge emphasizes the importance of confinement and edge effects in stabilizing the skyrmion tube state.\n\nAnswer A is incorrect because the study actually confirms the three-dimensional nature of skyrmions as extended tubes, not two-dimensional objects.\n\nAnswer C is incorrect because the text explicitly mentions the potential applications of skyrmions in spintronic devices and magnonic computing.\n\nAnswer D is incorrect because the study successfully used resonant magnetic x-ray imaging to observe the skyrmion tubes, not electron microscopy."}, "30": {"documentation": {"title": "Quantum Bose liquids with logarithmic nonlinearity: Self-sustainability\n  and emergence of spatial extent", "source": "Alexander V. Avdeenkov and Konstantin G. Zloshchastiev", "docs_id": "1108.0847", "section": ["cond-mat.quant-gas", "astro-ph.SR", "hep-th", "nucl-th", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum Bose liquids with logarithmic nonlinearity: Self-sustainability\n  and emergence of spatial extent. The Gross-Pitaevskii (GP) equation is a long-wavelength approach widely used to describe the dilute Bose-Einstein condensates (BEC). However, in many physical situations, such as higher densities, this approximation unlikely suffices hence one might need models which would account for long-range correlations and multi-body interactions. We show that the Bose liquid described by the logarithmic wave equation has a number of drastic differences from the GP one. It possesses the self-sustainability property: while the free GP condensate tends to spill all over the available volume the logarithmic one tends to form a Gaussian-type droplet - even in the absence of an external trapping potential. The quasi-particle modes of the logarithmic BEC are shown to acquire a finite size despite the bare particles being assumed point-like, i.e., the spatial extent emerges here as a result of quantum many-body correlations. Finally, we study the elementary excitations and demonstrate that the background density changes the topological structure of their momentum space which, in turn, affects their dispersion relations. Depending on the density the latter can be of the massive relativistic, massless relativistic, tachyonic and quaternionic type."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements accurately describes a key difference between the Bose liquid described by the logarithmic wave equation and the one described by the Gross-Pitaevskii (GP) equation?\n\nA) The logarithmic Bose liquid requires an external trapping potential to form a stable condensate, while the GP condensate does not.\n\nB) The logarithmic Bose liquid tends to form a Gaussian-type droplet without an external trapping potential, exhibiting self-sustainability.\n\nC) The GP condensate forms localized structures, while the logarithmic Bose liquid spreads uniformly across the available volume.\n\nD) The quasi-particle modes in the logarithmic Bose liquid have zero spatial extent, unlike those in the GP condensate.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the Bose liquid described by the logarithmic wave equation \"possesses the self-sustainability property: while the free GP condensate tends to spill all over the available volume the logarithmic one tends to form a Gaussian-type droplet - even in the absence of an external trapping potential.\" This is a key difference between the two models, highlighting the self-sustainability of the logarithmic Bose liquid.\n\nOption A is incorrect because it's the opposite of what the passage states. The logarithmic Bose liquid does not require an external trapping potential to form a stable structure.\n\nOption C is also incorrect, as it reverses the behaviors of the two models. The GP condensate is the one that tends to spread, not the logarithmic Bose liquid.\n\nOption D is incorrect because the passage actually states that \"The quasi-particle modes of the logarithmic BEC are shown to acquire a finite size despite the bare particles being assumed point-like,\" indicating that they do have spatial extent."}, "31": {"documentation": {"title": "Modified Dispersion Relations and trans-Planckian Physics", "source": "Massimiliano Rinaldi (Bologna University)", "docs_id": "0711.0824", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modified Dispersion Relations and trans-Planckian Physics. We consider modified dispersion relations in quantum field theory on curved space-time. Such relations, despite breaking the local Lorentz invariance at high energy, are considered in several phenomenological approaches to quantum gravity. Their existence involves a modification of the formalism of quantum field theory, starting from the problem of finding the scalar Green's functions up to the renormalization of various quantum expectation values. In this work we consider a simple example of such modifications, in the case of ultra-static metric. We show how to overcome the lack of Lorentz invariance by introducing a preferred frame, with respect to which we can express the Green's functions as an integral over all frequencies of a space-dependent function. The latter can be expanded in momentum space, and by integrating over all frequencies, we finally find the expansion of the Green's function up to four derivatives of the metric tensor. The relation with the proper-time formalism is also discussed."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of modified dispersion relations in quantum field theory on curved space-time, which of the following statements is correct regarding the approach described in the document?\n\nA) The Green's functions can be expressed as an integral over all momenta of a time-dependent function due to the preservation of local Lorentz invariance.\n\nB) The expansion of the Green's function is limited to two derivatives of the metric tensor due to the ultra-static nature of the metric.\n\nC) The formalism introduces a preferred frame to overcome the lack of Lorentz invariance, allowing the expression of Green's functions as an integral over all frequencies of a space-dependent function.\n\nD) The proper-time formalism is incompatible with the modified dispersion relations and is therefore not discussed in relation to this approach.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. The document explicitly states that to overcome the lack of Lorentz invariance, a preferred frame is introduced. With respect to this frame, the Green's functions can be expressed as an integral over all frequencies of a space-dependent function.\n\nAnswer A is incorrect because the approach deals with frequency integrals, not momentum integrals, and it acknowledges the breaking of local Lorentz invariance, not its preservation.\n\nAnswer B is incorrect because the document mentions that the expansion of the Green's function goes up to four derivatives of the metric tensor, not just two.\n\nAnswer D is incorrect because the document specifically mentions that the relation with the proper-time formalism is discussed, indicating that it is not incompatible with the approach.\n\nThis question tests the understanding of key concepts in the modified dispersion relations approach, including the breaking of Lorentz invariance, the introduction of a preferred frame, and the method of expressing Green's functions."}, "32": {"documentation": {"title": "Jet propulsion without inertia", "source": "Saverio E. Spagnolie and Eric Lauga", "docs_id": "1005.0591", "section": ["physics.flu-dyn", "cond-mat.soft", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jet propulsion without inertia. A body immersed in a highly viscous fluid can locomote by drawing in and expelling fluid through pores at its surface. We consider this mechanism of jet propulsion without inertia in the case of spheroidal bodies, and derive both the swimming velocity and the hydrodynamic efficiency. Elementary examples are presented, and exact axisymmetric solutions for spherical, prolate spheroidal, and oblate spheroidal body shapes are provided. In each case, entirely and partially porous (i.e. jetting) surfaces are considered, and the optimal jetting flow profiles at the surface for maximizing the hydrodynamic efficiency are determined computationally. The maximal efficiency which may be achieved by a sphere using such jet propulsion is 12.5%, a significant improvement upon traditional flagella-based means of locomotion at zero Reynolds number. Unlike other swimming mechanisms which rely on the presentation of a small cross section in the direction of motion, the efficiency of a jetting body at low Reynolds number increases as the body becomes more oblate, and limits to approximately 162% in the case of a flat plate swimming along its axis of symmetry. Our results are discussed in the light of slime extrusion mechanisms occurring in many cyanobacteria."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A spherical microorganism uses jet propulsion without inertia in a highly viscous fluid. Which of the following statements is TRUE regarding its locomotion efficiency compared to other shapes and traditional propulsion methods?\n\nA) Its maximum hydrodynamic efficiency is lower than that of flagella-based locomotion at zero Reynolds number.\n\nB) Its efficiency would increase if it changed to a more prolate spheroidal shape.\n\nC) Its maximum hydrodynamic efficiency is 12.5%, which is higher than traditional flagella-based locomotion at zero Reynolds number.\n\nD) Its efficiency would be highest if it transformed into a flat plate shape swimming perpendicular to its axis of symmetry.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. According to the documentation, the maximal efficiency that can be achieved by a sphere using jet propulsion is 12.5%, which is explicitly stated to be \"a significant improvement upon traditional flagella-based means of locomotion at zero Reynolds number.\" This directly supports option C and contradicts option A.\n\nOption B is incorrect because the document states that \"the efficiency of a jetting body at low Reynolds number increases as the body becomes more oblate,\" not more prolate.\n\nOption D is partially correct in suggesting that a flat plate shape would be more efficient, but it's incorrect in stating that it would swim perpendicular to its axis of symmetry. The document specifies that the highest efficiency (approximately 162%) is achieved \"in the case of a flat plate swimming along its axis of symmetry.\"\n\nThis question tests understanding of the relative efficiencies of different shapes and propulsion methods in highly viscous fluids at low Reynolds numbers, requiring careful reading and integration of multiple pieces of information from the text."}, "33": {"documentation": {"title": "Pseudo-Anosov flows in toroidal manifolds", "source": "Thierry Barbot and Sergio Fenley", "docs_id": "1007.0578", "section": ["math.GT", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pseudo-Anosov flows in toroidal manifolds. We first prove rigidity results for pseudo-Anosov flows in prototypes of toroidal 3-manifolds: we show that a pseudo-Anosov flow in a Seifert fibered manifold is up to finite covers topologically equivalent to a geodesic flow and we show that a pseudo-Anosov flow in a solv manifold is topologically equivalent to a suspension Anosov flow. Then we study the interaction of a general pseudo-Anosov flow with possible Seifert fibered pieces in the torus decomposition: if the fiber is associated with a periodic orbit of the flow, we show that there is a standard and very simple form for the flow in the piece using Birkhoff annuli. This form is strongly connected with the topology of the Seifert piece. We also construct a large new class of examples in many graph manifolds, which is extremely general and flexible. We construct other new classes of examples, some of which are generalized pseudo-Anosov flows which have one prong singularities and which show that the above results in Seifert fibered and solvable manifolds do not apply to one prong pseudo-Anosov flows. Finally we also analyse immersed and embedded incompressible tori in optimal position with respect to a pseudo-Anosov flow."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of pseudo-Anosov flows in toroidal 3-manifolds, which of the following statements is correct regarding the behavior of these flows in specific manifold types?\n\nA) In a Seifert fibered manifold, a pseudo-Anosov flow is always topologically equivalent to a suspension Anosov flow, regardless of the number of covers.\n\nB) In a solv manifold, a pseudo-Anosov flow is topologically equivalent to a geodesic flow after taking finite covers.\n\nC) In a Seifert fibered piece where the fiber is associated with a periodic orbit of the flow, the flow takes a standard form using Birkhoff annuli, which is strongly connected to the topology of the Seifert piece.\n\nD) Generalized pseudo-Anosov flows with one-prong singularities in Seifert fibered and solvable manifolds follow the same rigidity results as regular pseudo-Anosov flows.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that when studying the interaction of a general pseudo-Anosov flow with Seifert fibered pieces in the torus decomposition, if the fiber is associated with a periodic orbit of the flow, there is a standard and very simple form for the flow in the piece using Birkhoff annuli. This form is described as being strongly connected with the topology of the Seifert piece.\n\nAnswer A is incorrect because it confuses the results for Seifert fibered manifolds and solv manifolds. In Seifert fibered manifolds, the flow is topologically equivalent to a geodesic flow, not a suspension Anosov flow.\n\nAnswer B is incorrect for the same reason as A. It's in solv manifolds where the pseudo-Anosov flow is topologically equivalent to a suspension Anosov flow, not a geodesic flow.\n\nAnswer D is incorrect because the documentation explicitly states that the results for Seifert fibered and solvable manifolds do not apply to one-prong pseudo-Anosov flows. These one-prong flows are mentioned as examples that show the limits of the rigidity results."}, "34": {"documentation": {"title": "Changepoint detection in random coefficient autoregressive models", "source": "Lajos Horvath and Lorenzo Trapani", "docs_id": "2104.13440", "section": ["math.ST", "econ.EM", "stat.OT", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Changepoint detection in random coefficient autoregressive models. We propose a family of CUSUM-based statistics to detect the presence of changepoints in the deterministic part of the autoregressive parameter in a Random Coefficient AutoRegressive (RCA) sequence. In order to ensure the ability to detect breaks at sample endpoints, we thoroughly study weighted CUSUM statistics, analysing the asymptotics for virtually all possible weighing schemes, including the standardised CUSUM process (for which we derive a Darling-Erdos theorem) and even heavier weights (studying the so-called R\\'enyi statistics). Our results are valid irrespective of whether the sequence is stationary or not, and no prior knowledge of stationarity or lack thereof is required. Technically, our results require strong approximations which, in the nonstationary case, are entirely new. Similarly, we allow for heteroskedasticity of unknown form in both the error term and in the stochastic part of the autoregressive coefficient, proposing a family of test statistics which are robust to heteroskedasticity, without requiring any prior knowledge as to the presence or type thereof. Simulations show that our procedures work very well in finite samples. We complement our theory with applications to financial, economic and epidemiological time series."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of changepoint detection in Random Coefficient AutoRegressive (RCA) models, which of the following statements is correct regarding the proposed CUSUM-based statistics?\n\nA) The statistics are only applicable to stationary sequences and require prior knowledge of stationarity.\n\nB) The approach is limited to detecting breaks in the middle of the sample and cannot detect changes at sample endpoints.\n\nC) The method requires homoskedasticity in both the error term and the stochastic part of the autoregressive coefficient.\n\nD) The statistics are robust to heteroskedasticity of unknown form and can detect breaks even at sample endpoints.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that the proposed family of CUSUM-based statistics is designed to detect changepoints in RCA sequences, with several key features:\n\n1. They can detect breaks at sample endpoints, which is achieved through the study of weighted CUSUM statistics.\n2. The results are valid for both stationary and non-stationary sequences, and no prior knowledge of stationarity is required.\n3. The method allows for heteroskedasticity of unknown form in both the error term and the stochastic part of the autoregressive coefficient.\n4. The test statistics are robust to heteroskedasticity without requiring any prior knowledge of its presence or type.\n\nOptions A, B, and C are incorrect because they contradict these features. The proposed method is not limited to stationary sequences (A), can detect breaks at sample endpoints (B), and does not require homoskedasticity (C)."}, "35": {"documentation": {"title": "Sparse Identification of Nonlinear Dynamics for Rapid Model Recovery", "source": "Markus Quade, Markus Abel, J. Nathan Kutz, Steven L. Brunton", "docs_id": "1803.00894", "section": ["physics.data-an", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparse Identification of Nonlinear Dynamics for Rapid Model Recovery. Big data has become a critically enabling component of emerging mathematical methods aimed at the automated discovery of dynamical systems, where first principles modeling may be intractable. However, in many engineering systems, abrupt changes must be rapidly characterized based on limited, incomplete, and noisy data. Many leading automated learning techniques rely on unrealistically large data sets and it is unclear how to leverage prior knowledge effectively to re-identify a model after an abrupt change. In this work, we propose a conceptual framework to recover parsimonious models of a system in response to abrupt changes in the low-data limit. First, the abrupt change is detected by comparing the estimated Lyapunov time of the data with the model prediction. Next, we apply the sparse identification of nonlinear dynamics (SINDy) regression to update a previously identified model with the fewest changes, either by addition, deletion, or modification of existing model terms. We demonstrate this sparse model recovery on several examples for abrupt system change detection in periodic and chaotic dynamical systems. Our examples show that sparse updates to a previously identified model perform better with less data, have lower runtime complexity, and are less sensitive to noise than identifying an entirely new model. The proposed abrupt-SINDy architecture provides a new paradigm for the rapid and efficient recovery of a system model after abrupt changes."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the advantage of the abrupt-SINDy architecture over traditional model identification methods when dealing with abrupt changes in dynamical systems?\n\nA) It requires larger datasets to accurately identify new model terms\nB) It has a higher runtime complexity compared to identifying a new model from scratch\nC) It performs better with less data and is less sensitive to noise\nD) It relies solely on the estimated Lyapunov time for model recovery\n\nCorrect Answer: C\n\nExplanation: The abrupt-SINDy architecture, as described in the document, offers several advantages over traditional model identification methods when dealing with abrupt changes in dynamical systems. The correct answer, C, highlights two key benefits mentioned in the text:\n\n1. It performs better with less data: The document states, \"Our examples show that sparse updates to a previously identified model perform better with less data.\" This is particularly important in situations where limited data is available after an abrupt change.\n\n2. It is less sensitive to noise: The text mentions that the method is \"less sensitive to noise than identifying an entirely new model.\"\n\nOption A is incorrect because the method is designed for the \"low-data limit\" and doesn't require larger datasets.\n\nOption B is wrong as the document explicitly states that the method has \"lower runtime complexity\" compared to identifying a new model from scratch.\n\nOption D is incorrect because while the estimated Lyapunov time is used for detecting the abrupt change, it's not the sole basis for model recovery. The method also employs sparse identification of nonlinear dynamics (SINDy) regression to update the model.\n\nThis question tests the student's understanding of the key advantages of the abrupt-SINDy method and requires careful reading and synthesis of the information provided in the document."}, "36": {"documentation": {"title": "A mathematical insight in the epithelial-mesenchymal-like transition in\n  cancer cells and its effect in the invasion of the extracellular matrix", "source": "Nadja Hellmann, Niklas Kolbe, and Nikolaos Sfakianakis", "docs_id": "1505.04268", "section": ["q-bio.CB", "math.NA", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A mathematical insight in the epithelial-mesenchymal-like transition in\n  cancer cells and its effect in the invasion of the extracellular matrix. Current biological knowledge supports the existence of a secondary group of cancer cells within the body of the tumour that exhibits stem cell-like properties. These cells are termed Cancer Stem Cells (CSCs}, and as opposed to the more usual Differentiated Cancer Cells (DCCs), they exhibit higher motility, they are more resilient to therapy, and are able to metastasize to secondary locations within the organism and produce new tumours. The origin of the CSCs is not completely clear; they seem to stem from the DCCs via a transition process related to the Epithelial-Mesenchymal Transition (EMT) that can also be found in normal tissue. In the current work we model and numerically study the transition between these two types of cancer cells, and the resulting \"ensemble\" invasion of the extracellular matrix. This leads to the derivation and numerical simulation of two systems: an algebraic-elliptic system for the transition and an advection-reaction-diffusion system of Keller-Segel taxis type for the invasion."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the relationship between Cancer Stem Cells (CSCs) and Differentiated Cancer Cells (DCCs), and their role in tumor progression?\n\nA) CSCs are less motile than DCCs and are primarily responsible for tumor growth at the primary site.\n\nB) DCCs exhibit stem cell-like properties and are more resistant to therapy compared to CSCs.\n\nC) CSCs originate from DCCs through a process similar to the Epithelial-Mesenchymal Transition (EMT) and possess higher metastatic potential.\n\nD) CSCs and DCCs have identical properties, and the transition between them has no significant impact on tumor invasion.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that Cancer Stem Cells (CSCs) exhibit stem cell-like properties, higher motility, and increased resilience to therapy compared to Differentiated Cancer Cells (DCCs). CSCs are also described as having the ability to metastasize and produce new tumors at secondary locations. The origin of CSCs is suggested to be from DCCs through a transition process related to the Epithelial-Mesenchymal Transition (EMT). \n\nOption A is incorrect because it contradicts the information given about CSCs being more motile and having metastatic potential. \n\nOption B is incorrect as it reverses the properties of CSCs and DCCs. \n\nOption D is incorrect because the documentation clearly distinguishes between the properties of CSCs and DCCs and emphasizes the importance of the transition between them in tumor invasion.\n\nThis question tests the student's understanding of the key differences between CSCs and DCCs, their origins, and their roles in cancer progression as described in the given text."}, "37": {"documentation": {"title": "The resolved fraction of the Cosmic X-ray Background", "source": "A.Moretti (1), S.Campana (1), D. Lazzati (2), G.Tagliaferri (1)\n  (1)INAF-O.A.Brera ITALY, (2) IoA Cambridge UK", "docs_id": "astro-ph/0301555", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The resolved fraction of the Cosmic X-ray Background. We present the X-ray source number counts in two energy bands (0.5-2 and 2-10 keV) from a very large source sample: we combine data of six different surveys, both shallow wide field and deep pencil beam, performed with three different satellites (ROSAT, Chandra and XMM-Newton). The sample covers with good statistics the largest possible flux range so far: [2.4*10^-17 - 10^-11] cgs in the soft band and [2.1*10^-16 - 8*10^{-12}]cgs in the hard band. Integrating the flux distributions over this range and taking into account the (small) contribution of the brightest sources we derive the flux density generated by discrete sources in both bands. After a critical review of the literature values of the total Cosmic X--Ray Background (CXB) we conclude that, with the present data, the 94.3%, and 88.8% of the soft and hard CXB can be ascribed to discrete source emission. If we extrapolate the analytical form of the Log N--Log S distribution beyond the flux limit of our catalog in the soft band we find that the flux from discrete sources at ~3*10^-18 cgs is consistent with the entire CXB, whereas in the hard band it accounts for only 93% of the total CXB at most, hinting for a faint and obscured population to arise at even fainter fluxes."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Based on the study of X-ray source number counts across multiple surveys and energy bands, what conclusion can be drawn about the resolved fraction of the Cosmic X-ray Background (CXB) in the hard band (2-10 keV), and what does this imply for future observations?\n\nA) 100% of the hard CXB is resolved, suggesting no need for further deep surveys in this energy range.\n\nB) 88.8% of the hard CXB is resolved, with extrapolation suggesting 93% at most, indicating a potential faint and obscured population at lower fluxes.\n\nC) 94.3% of the hard CXB is resolved, with extrapolation suggesting complete resolution at fluxes around 3*10^-18 cgs.\n\nD) Less than 50% of the hard CXB is resolved, necessitating significantly deeper surveys to account for the majority of the background.\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the study's findings and their implications. Answer B is correct because the documentation states that 88.8% of the hard CXB can be ascribed to discrete source emission within the observed flux range. Furthermore, extrapolating beyond the flux limit suggests that discrete sources would account for at most 93% of the total hard CXB, hinting at a faint and obscured population at even fainter fluxes.\n\nAnswer A is incorrect because the study does not claim 100% resolution of the hard CXB. Answer C confuses the findings for the soft band (0.5-2 keV) with those of the hard band. Answer D significantly understates the resolved fraction and contradicts the study's findings.\n\nThis question requires careful reading and interpretation of the provided information, making it suitable for an advanced exam on astrophysics or X-ray astronomy."}, "38": {"documentation": {"title": "Agents' beliefs and economic regimes polarization in interacting markets", "source": "Fausto Cavalli, Ahmad Naimzada, Nicol\\`o Pecora and Marina Pireddu", "docs_id": "1805.00387", "section": ["q-fin.GN", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Agents' beliefs and economic regimes polarization in interacting markets. In the present paper a model of a market consisting of real and financial interacting sectors is studied. Agents populating the stock market are assumed to be not able to observe the true underlying fundamental, and their beliefs are biased by either optimism or pessimism. Depending on the relevance they give to beliefs, they select the best performing strategy in an evolutionary perspective. The real side of the economy is described within a multiplier-accelerator framework with a nonlinear, bounded investment function. We show that strongly polarized beliefs in an evolutionary framework can introduce multiplicity of steady states, which, consisting in enhanced or depressed levels of income, reflect and reproduce the optimistic or pessimistic nature of the agents' beliefs. The polarization of these steady states, which coexist with an unbiased steady state, positively depends on that of the beliefs and on their relevance. Moreover, with a mixture of analytical and numerical tools, we show that such static characterization is inherited also at the dynamical level, with possibly complex attractors that are characterized by endogenously fluctuating pessimistic and optimistic levels of national income and price. This framework, when stochastic perturbations are included, is able to account for stylized facts commonly observed in real financial markets, such as fat tails and excess volatility in the returns distributions, as well as bubbles and crashes for stock prices."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the model described, which combination of factors contributes to the emergence of multiple steady states in the economy?\n\nA) The interaction between real and financial sectors, coupled with agents' ability to observe true fundamentals\nB) Strongly polarized beliefs in an evolutionary framework, combined with a linear investment function\nC) Weakly polarized beliefs in a static framework, along with a nonlinear, bounded investment function\nD) Strongly polarized beliefs in an evolutionary framework, combined with a nonlinear, bounded investment function\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that \"strongly polarized beliefs in an evolutionary framework can introduce multiplicity of steady states.\" It also mentions that \"the real side of the economy is described within a multiplier-accelerator framework with a nonlinear, bounded investment function.\" These two factors combined contribute to the emergence of multiple steady states in the economy.\n\nOption A is incorrect because the agents are described as \"not able to observe the true underlying fundamental,\" contrary to what this option suggests.\n\nOption B is incorrect because it mentions a linear investment function, whereas the model uses a nonlinear, bounded investment function.\n\nOption C is incorrect because it refers to weakly polarized beliefs and a static framework, while the model emphasizes strongly polarized beliefs in an evolutionary framework.\n\nThis question tests the student's understanding of the key components of the model and their ability to identify the factors that lead to its main results."}, "39": {"documentation": {"title": "An Unconstrained Layer-Peeled Perspective on Neural Collapse", "source": "Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, Weijie J. Su", "docs_id": "2110.02796", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Unconstrained Layer-Peeled Perspective on Neural Collapse. Neural collapse is a highly symmetric geometric pattern of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the relationship between the unconstrained layer-peeled model (ULPM) and neural collapse?\n\nA) The ULPM proves that neural collapse only occurs in the presence of explicit regularization or weight decay.\n\nB) The ULPM demonstrates that neural collapse is a transient phenomenon that disappears as training progresses.\n\nC) The ULPM shows that gradient flow converges to critical points exhibiting neural collapse in its global minimizer, and all critical points except these global minimizers are strict saddle points.\n\nD) The ULPM indicates that neural collapse is unrelated to the generalization performance and robustness of trained networks.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that the authors \"prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer.\" Additionally, it mentions that \"all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon.\" This directly supports statement C.\n\nAnswer A is incorrect because the passage actually states that the results hold \"when explicit regularization or weight decay is not used.\"\n\nAnswer B is wrong because neural collapse is described as emerging during the \"terminal phase of training,\" not as a transient phenomenon that disappears.\n\nAnswer D contradicts the passage, which states that neural collapse has \"profound implications on the generalization performance and robustness of the trained networks.\""}, "40": {"documentation": {"title": "Spectral Top-Down Recovery of Latent Tree Models", "source": "Yariv Aizenbud, Ariel Jaffe, Meng Wang, Amber Hu, Noah Amsel, Boaz\n  Nadler, Joseph T. Chang, Yuval Kluger", "docs_id": "2102.13276", "section": ["stat.ML", "cs.LG", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral Top-Down Recovery of Latent Tree Models. Modeling the distribution of high dimensional data by a latent tree graphical model is a prevalent approach in multiple scientific domains. A common task is to infer the underlying tree structure, given only observations of its terminal nodes. Many algorithms for tree recovery are computationally intensive, which limits their applicability to trees of moderate size. For large trees, a common approach, termed divide-and-conquer, is to recover the tree structure in two steps. First, recover the structure separately of multiple, possibly random subsets of the terminal nodes. Second, merge the resulting subtrees to form a full tree. Here, we develop Spectral Top-Down Recovery (STDR), a deterministic divide-and-conquer approach to infer large latent tree models. Unlike previous methods, STDR partitions the terminal nodes in a non random way, based on the Fiedler vector of a suitable Laplacian matrix related to the observed nodes. We prove that under certain conditions, this partitioning is consistent with the tree structure. This, in turn, leads to a significantly simpler merging procedure of the small subtrees. We prove that STDR is statistically consistent and bound the number of samples required to accurately recover the tree with high probability. Using simulated data from several common tree models in phylogenetics, we demonstrate that STDR has a significant advantage in terms of runtime, with improved or similar accuracy."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation and advantage of the Spectral Top-Down Recovery (STDR) method for latent tree model recovery?\n\nA) It uses random partitioning of terminal nodes to improve computational efficiency.\nB) It employs a novel merging algorithm that can handle trees of any size.\nC) It utilizes the Fiedler vector of a Laplacian matrix to non-randomly partition terminal nodes.\nD) It achieves perfect accuracy in tree recovery regardless of sample size.\n\nCorrect Answer: C\n\nExplanation: The key innovation of STDR is its use of the Fiedler vector of a Laplacian matrix related to the observed nodes to partition the terminal nodes in a non-random way. This approach is described as deterministic and is contrasted with previous methods that use random subsets. The non-random partitioning is said to be consistent with the tree structure under certain conditions, which leads to a simpler merging procedure for subtrees. This method provides advantages in terms of runtime while maintaining or improving accuracy, especially for large trees. Options A, B, and D are incorrect or not supported by the given information. Option A describes random partitioning, which is explicitly contrasted with STDR's approach. Option B overstates the capabilities of the merging algorithm. Option D is not supported, as the document mentions statistical consistency and probability bounds, not perfect accuracy regardless of sample size."}, "41": {"documentation": {"title": "Using mathematical modeling to ask meaningful biological questions\n  through combination of bifurcation analysis and population heterogeneity", "source": "Irina Kareva", "docs_id": "1802.05539", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using mathematical modeling to ask meaningful biological questions\n  through combination of bifurcation analysis and population heterogeneity. Classical approaches to analyzing dynamical systems, including bifurcation analysis, can provide invaluable insights into underlying structure of a mathematical model, and the spectrum of all possible dynamical behaviors. However, these models frequently fail to take into account population heterogeneity, which, while critically important to understanding and predicting the behavior of any evolving system, is a common simplification that is made in analysis of many mathematical models of ecological systems. Attempts to include population heterogeneity frequently result in expanding system dimensionality, effectively preventing qualitative analysis. Reduction Theorem, or Hidden keystone variable (HKV) method, allows incorporating population heterogeneity while still permitting the use of previously existing classical bifurcation analysis. A combination of these methods allows visualization of evolutionary trajectories and making meaningful predictions about dynamics over time of evolving populations. Here, we discuss three examples of combination of these methods to augment understanding of evolving ecological systems. We demonstrate what new meaningful questions can be asked through this approach, and propose that the large existing literature of fully analyzed models can reveal new and meaningful dynamical behaviors with the application of the HKV-method, if the right questions are asked."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: Which of the following best describes the primary advantage of combining the Hidden Keystone Variable (HKV) method with classical bifurcation analysis in ecological modeling?\n\nA) It allows for the complete elimination of population heterogeneity in models.\nB) It increases the dimensionality of the system, enabling more complex analysis.\nC) It permits the incorporation of population heterogeneity without preventing qualitative analysis.\nD) It replaces classical bifurcation analysis with a more advanced technique.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that the Hidden Keystone Variable (HKV) method, also referred to as the Reduction Theorem, \"allows incorporating population heterogeneity while still permitting the use of previously existing classical bifurcation analysis.\" This combination enables researchers to include important heterogeneity factors without losing the ability to perform qualitative analysis, which is often hindered by increased system dimensionality.\n\nOption A is incorrect because the goal is not to eliminate population heterogeneity, but to incorporate it into the model.\n\nOption B is incorrect because the HKV method actually helps avoid increasing system dimensionality, which is a common problem when trying to include population heterogeneity.\n\nOption D is incorrect because the method doesn't replace classical bifurcation analysis, but rather complements it, allowing both techniques to be used together.\n\nThis question tests the student's understanding of the key advantage of combining these methods in ecological modeling and their ability to identify the correct relationship between population heterogeneity, system dimensionality, and qualitative analysis."}, "42": {"documentation": {"title": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning", "source": "Dingquan Wang, Jason Eisner", "docs_id": "1710.03877", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning. We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the innovative approach and key findings of the research on predicting syntactic typology of languages?\n\nA) The study uses unsupervised learning on real language corpora to predict word-order patterns, outperforming traditional grammar induction methods.\n\nB) The research employs supervised learning on synthetic languages to predict typological properties, demonstrating improved performance when combining real and synthetic language data.\n\nC) The study focuses on developing neural features for POS sequences, which alone can accurately predict a language's deeper structure without additional training data.\n\nD) The research shows that grammar induction baselines are superior to supervised learning methods when predicting basic word-order facts of novel languages.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately captures the key aspects of the research described in the text. The study's innovation lies in treating the prediction of syntactic typology as a supervised learning problem, using a large collection of synthetic languages as training data. This approach allows the system to identify surface features of a language's POS sequence that correlate with its deeper structure.\n\nThe text explicitly states that adding many synthetic languages to the training data helps when given a small set of real languages, which is reflected in answer B. Additionally, the study shows that this supervised learning approach outperforms a grammar induction baseline by a large margin, further supporting the correctness of answer B.\n\nAnswer A is incorrect because it mischaracterizes the approach as unsupervised learning, which the text specifically contrasts with the study's supervised learning method.\n\nAnswer C is incorrect because while the study mentions both hand-engineered and neural features, it doesn't claim that neural features alone are sufficient or superior. The text emphasizes the combination of real and synthetic language data, not just the feature type.\n\nAnswer D is incorrect because it directly contradicts the findings reported in the text, which states that the proposed system outperforms grammar induction baselines, not the other way around."}, "43": {"documentation": {"title": "Quorum sensing in populations of spatially extended chaotic oscillators\n  coupled indirectly via a heterogeneous environment", "source": "Bing-Wei Li, Xiao-Zhi Cao, and Chenbo Fu", "docs_id": "1612.05926", "section": ["nlin.CD", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quorum sensing in populations of spatially extended chaotic oscillators\n  coupled indirectly via a heterogeneous environment. Many biological and chemical systems could be modeled by a population of oscillators coupled indirectly via a dynamical environment. Essentially, the environment by which the individual elements communicate is heterogeneous. Nevertheless, most of previous works considered the homogeneous case only. Here, we investigated the dynamical behaviors in a population of spatially distributed chaotic oscillators immersed in a heterogeneous environment. Various dynamical synchronization states such as oscillation death, phase synchronization, and complete synchronized oscillation as well as their transitions were found. More importantly, we uncovered a non-traditional quorum sensing transition: increasing the density would first lead to collective oscillation from oscillation quench, but further increasing the population density would lead to degeneration from complete synchronization to phase synchronization or even from phase synchronization to desynchronization. The underlying mechanism of this finding was attributed to the dual roles played by the population density. Further more, by treating the indirectly coupled systems effectively to the system with directly local coupling, we applied the master stability function approach to predict the occurrence of the complete synchronized oscillation, which were in agreement with the direct numerical simulations of the full system. The possible candidates of the experimental realization on our model was also discussed."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In a population of spatially extended chaotic oscillators coupled indirectly via a heterogeneous environment, which of the following statements accurately describes the non-traditional quorum sensing transition observed as population density increases?\n\nA) The system transitions directly from oscillation quench to complete synchronization, maintaining this state as density further increases.\n\nB) The system moves from oscillation quench to collective oscillation, then to complete synchronization, and remains in this state regardless of further density increases.\n\nC) The system progresses from oscillation quench to collective oscillation, then to complete synchronization, but may degenerate to phase synchronization or even desynchronization as density continues to increase.\n\nD) The system always maintains a state of phase synchronization, regardless of changes in population density.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation describes a non-traditional quorum sensing transition where increasing the density first leads to collective oscillation from oscillation quench. However, further increasing the population density can lead to degeneration from complete synchronization to phase synchronization or even from phase synchronization to desynchronization. This complex behavior is attributed to the dual roles played by population density in the system. Options A, B, and D do not accurately capture this nuanced transition process described in the text."}, "44": {"documentation": {"title": "Bottleneck crossover between classical and quantum superfluid turbulence", "source": "Victor S. L'vov, Sergei V. Nazarenko and Oleksii Rudenko", "docs_id": "nlin/0612018", "section": ["nlin.CD", "nlin.SI", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bottleneck crossover between classical and quantum superfluid turbulence. We consider superfluid turbulence near absolute zero of temperature generated by classical means, e.g. towed grid or rotation but not by counterflow. We argue that such turbulence consists of a {\\em polarized} tangle of mutually interacting vortex filaments with quantized vorticity. For this system we predict and describe a bottleneck accumulation of the energy spectrum at the classical-quantum crossover scale $\\ell$. Demanding the same energy flux through scales, the value of the energy at the crossover scale should exceed the Kolmogorov-41 spectrum by a large factor $\\ln^{10/3} (\\ell/a_0)$ ($\\ell$ is the mean intervortex distance and $a_0$ is the vortex core radius) for the classical and quantum spectra to be matched in value. One of the important consequences of the bottleneck is that it causes the mean vortex line density to be considerably higher that based on K41 alone, and this should be taken into account in (re)interpretation of new (and old) experiments as well as in further theoretical studies."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In superfluid turbulence near absolute zero generated by classical means, what is the predicted effect at the classical-quantum crossover scale and how does it impact the energy spectrum?\n\nA) A bottleneck depletion of energy, causing the spectrum to be lower than the Kolmogorov-41 prediction by a factor of ln^(10/3) (\u2113/a\u2080)\n\nB) A bottleneck accumulation of energy, causing the spectrum to exceed the Kolmogorov-41 prediction by a factor of ln^(10/3) (\u2113/a\u2080)\n\nC) A smooth transition with no significant deviation from the Kolmogorov-41 spectrum at the crossover scale\n\nD) A sharp discontinuity in the energy spectrum, with no relation to the Kolmogorov-41 prediction\n\nCorrect Answer: B\n\nExplanation: The documentation states that there is a \"bottleneck accumulation of the energy spectrum at the classical-quantum crossover scale \u2113.\" It further specifies that \"the value of the energy at the crossover scale should exceed the Kolmogorov-41 spectrum by a large factor ln^(10/3) (\u2113/a\u2080).\" This directly corresponds to option B. Options A and C are incorrect as they do not reflect the bottleneck accumulation or the magnitude of deviation from K41. Option D is incorrect as it describes a discontinuity, which is not mentioned in the text."}, "45": {"documentation": {"title": "Information of income position and its impact on perceived tax burden\n  and preference for redistribution: An Internet Survey Experiment", "source": "Eiji Yamamura", "docs_id": "2106.11537", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information of income position and its impact on perceived tax burden\n  and preference for redistribution: An Internet Survey Experiment. A customized internet survey experiment is conducted in Japan to examine how individuals' relative income position influences preferences for income redistribution and individual perceptions regarding income tax burden. I first asked respondents about their perceived income position in their country and their preferences for redistribution and perceived tax burden. In the follow-up survey for the treatment group, I provided information on their true income position and asked the same questions as in the first survey. For the control group, I did not provide their true income position and asked the same questions. I gathered a large sample that comprised observations of the treatment group (4,682) and the control group (2,268). The key findings suggest that after being informed of individuals' real income position, (1) individuals who thought their income position was higher than the true one perceived their tax burden to be larger, (2) individuals' preference for redistribution hardly changes, and (3) irreciprocal individuals perceive their tax burden to be larger and are more likely to prefer redistribution. However, the share of irreciprocal ones is small. This leads Japan to be a non-welfare state."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the study on income position perception and its impact on tax burden and redistribution preferences in Japan, which of the following conclusions can be drawn from the key findings?\n\nA) The majority of participants changed their preference for redistribution after learning their true income position.\n\nB) Providing accurate income position information led to increased support for welfare state policies among most participants.\n\nC) Irreciprocal individuals showed a stronger tendency to perceive higher tax burdens and prefer redistribution, but they represented a minority of the sample.\n\nD) Individuals who overestimated their income position were more likely to perceive their tax burden as lower after receiving accurate information.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key findings of the study suggest that irreciprocal individuals perceived their tax burden to be larger and were more likely to prefer redistribution after learning their true income position. However, the share of these irreciprocal individuals was small, which contributes to Japan remaining a non-welfare state.\n\nOption A is incorrect because the study found that individuals' preferences for redistribution hardly changed after receiving information about their true income position.\n\nOption B is incorrect as the study does not indicate increased support for welfare state policies. In fact, it concludes that Japan remains a non-welfare state.\n\nOption D is incorrect because the study states that individuals who thought their income position was higher than the true one perceived their tax burden to be larger, not lower, after receiving accurate information."}, "46": {"documentation": {"title": "Measuring Price Discovery between Nearby and Deferred Contracts in\n  Storable and Non-Storable Commodity Futures Markets", "source": "Zhepeng Hu, Mindy Mallory, Teresa Serra, and Philip Garcia", "docs_id": "1711.03506", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measuring Price Discovery between Nearby and Deferred Contracts in\n  Storable and Non-Storable Commodity Futures Markets. Futures market contracts with varying maturities are traded concurrently and the speed at which they process information is of value in understanding the pricing discovery process. Using price discovery measures, including Putnins (2013) information leadership share and intraday data, we quantify the proportional contribution of price discovery between nearby and deferred contracts in the corn and live cattle futures markets. Price discovery is more systematic in the corn than in the live cattle market. On average, nearby contracts lead all deferred contracts in price discovery in the corn market, but have a relatively less dominant role in the live cattle market. In both markets, the nearby contract loses dominance when its relative volume share dips below 50%, which occurs about 2-3 weeks before expiration in corn and 5-6 weeks before expiration in live cattle. Regression results indicate that the share of price discovery is most closely linked to trading volume but is also affected, to far less degree, by time to expiration, backwardation, USDA announcements and market crashes. The effects of these other factors vary between the markets which likely reflect the difference in storability as well as other market-related characteristics."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of price discovery between nearby and deferred contracts in commodity futures markets, which of the following statements is most accurate?\n\nA) The nearby contract consistently dominates price discovery in both corn and live cattle markets until expiration.\n\nB) The relative volume share of the nearby contract has no impact on its dominance in price discovery.\n\nC) Price discovery patterns are more consistent in the corn market compared to the live cattle market, with nearby contracts generally leading in corn futures.\n\nD) USDA announcements and market crashes are the primary factors affecting the share of price discovery in both markets.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation states that \"Price discovery is more systematic in the corn than in the live cattle market\" and \"On average, nearby contracts lead all deferred contracts in price discovery in the corn market, but have a relatively less dominant role in the live cattle market.\" This indicates that the corn market shows more consistent patterns in price discovery, with nearby contracts generally leading.\n\nOption A is incorrect because the nearby contract loses dominance when its relative volume share falls below 50%, which happens before expiration in both markets.\n\nOption B is false because the document clearly states that the nearby contract loses dominance when its relative volume share dips below 50%.\n\nOption D is incorrect because while USDA announcements and market crashes do affect the share of price discovery, they are not the primary factors. The documentation states that \"the share of price discovery is most closely linked to trading volume\" and other factors, including USDA announcements and market crashes, affect it \"to far less degree.\""}, "47": {"documentation": {"title": "Assessing Threat of Adversarial Examples on Deep Neural Networks", "source": "Abigail Graese, Andras Rozsa, Terrance E. Boult", "docs_id": "1610.04256", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Assessing Threat of Adversarial Examples on Deep Neural Networks. Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the impact of the normal image acquisition process on adversarial examples in deep neural networks, according to the research?\n\nA) The image acquisition process enhances the effectiveness of adversarial examples, making them a greater security threat.\n\nB) The image acquisition process has no significant effect on adversarial examples or their impact on deep neural networks.\n\nC) The image acquisition process introduces transformations that negate the effect of adversarial perturbations, reducing their threat.\n\nD) The image acquisition process creates new types of adversarial examples that are more difficult for deep neural networks to classify correctly.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The research described in the document states that the normal image acquisition process, which involves small transformations that occur when capturing an image in real-world applications (such as using a scanner or camera), negates the effect of carefully crafted perturbations in adversarial examples. This results in correct classifications by the deep neural network, thereby decreasing the potential impact of the proposed security threat. \n\nOption A is incorrect because the research suggests the opposite - that the image acquisition process reduces, not enhances, the effectiveness of adversarial examples. \n\nOption B is incorrect because the research clearly indicates that the image acquisition process has a significant effect on adversarial examples, neutralizing their impact. \n\nOption D is incorrect because the research does not mention the creation of new types of adversarial examples through the image acquisition process. Instead, it focuses on how this process mitigates the threat of existing adversarial examples."}, "48": {"documentation": {"title": "Deep Demosaicing for Edge Implementation", "source": "Ramchalam Kinattinkara Ramakrishnan, Shangling Jui and Vahid Patrovi\n  Nia", "docs_id": "1904.00775", "section": ["cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Demosaicing for Edge Implementation. Most digital cameras use sensors coated with a Color Filter Array (CFA) to capture channel components at every pixel location, resulting in a mosaic image that does not contain pixel values in all channels. Current research on reconstructing these missing channels, also known as demosaicing, introduces many artifacts, such as zipper effect and false color. Many deep learning demosaicing techniques outperform other classical techniques in reducing the impact of artifacts. However, most of these models tend to be over-parametrized. Consequently, edge implementation of the state-of-the-art deep learning-based demosaicing algorithms on low-end edge devices is a major challenge. We provide an exhaustive search of deep neural network architectures and obtain a pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the performance criterion versus the number of parameters as the model complexity that beats the state-of-the-art. Architectures on the pareto front can then be used to choose the best architecture for a variety of resource constraints. Simple architecture search methods such as exhaustive search and grid search require some conditions of the loss function to converge to the optimum. We clarify these conditions in a brief theoretical study."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the challenges and proposed solution for deep learning-based demosaicing on edge devices?\n\nA) Deep learning models for demosaicing are typically under-parametrized, making them easily implementable on low-end edge devices.\n\nB) The research proposes using classical demosaicing techniques instead of deep learning models to overcome edge implementation challenges.\n\nC) The study suggests an exhaustive search of neural network architectures to find a pareto front balancing CPSNR performance and model complexity for various resource constraints.\n\nD) The paper focuses on increasing the number of parameters in deep learning models to improve demosaicing quality on edge devices.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that while deep learning demosaicing techniques outperform classical methods, they tend to be over-parametrized, making edge implementation on low-end devices challenging. To address this, the researchers conducted an exhaustive search of neural network architectures to obtain a pareto front that balances Color Peak Signal to Noise Ratio (CPSNR) performance against the number of parameters (model complexity). This approach allows for selecting the best architecture based on various resource constraints, making it suitable for edge implementation.\n\nOption A is incorrect because the document states that deep learning models are typically over-parametrized, not under-parametrized.\n\nOption B is incorrect as the research does not propose reverting to classical techniques but instead focuses on optimizing deep learning models for edge implementation.\n\nOption D is incorrect because the goal is not to increase the number of parameters, but to find a balance between performance and model complexity for edge devices."}, "49": {"documentation": {"title": "Generalized perturbations in neutrino mixing", "source": "Jiajun Liao, D. Marfatia, K. Whisnant", "docs_id": "1506.03013", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalized perturbations in neutrino mixing. We derive expressions for the neutrino mixing parameters that result from complex perturbations on (1) the Majorana neutrino mass matrix (in the basis of charged lepton mass eigenstates) and on (2) the charged lepton mass matrix, for arbitrary initial (unperturbed) mixing matrices. In the first case, we find that the phases of the elements of the perturbation matrix, and the initial values of the Dirac and Majorana phases, strongly impact the leading order corrections to the neutrino mixing parameters and phases. For experimentally compatible scenarios wherein the initial neutrino mass matrix has $\\mu-\\tau$ symmetry, we find that the Dirac phase can take any value under small perturbations. Similarly, in the second case, perturbations to the charged lepton mass matrix can generate large corrections to the mixing angles and phases of the PMNS matrix. As an illustration of our generalized procedure, we apply it to a situation in which nonstandard scalar and nonstandard vector interactions simultaneously affect neutrino oscillations."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of generalized perturbations in neutrino mixing, which of the following statements is correct regarding the effects of complex perturbations on the Majorana neutrino mass matrix?\n\nA) The leading order corrections to neutrino mixing parameters and phases are primarily determined by the magnitudes of the perturbation matrix elements, with little influence from their phases.\n\nB) For scenarios with initial \u03bc-\u03c4 symmetry in the neutrino mass matrix, small perturbations always result in a Dirac phase close to 0 or \u03c0.\n\nC) The initial values of the Dirac and Majorana phases have negligible impact on the corrections to the neutrino mixing parameters under perturbations.\n\nD) The phases of the perturbation matrix elements, along with the initial Dirac and Majorana phases, significantly affect the leading order corrections to neutrino mixing parameters and phases.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that \"the phases of the elements of the perturbation matrix, and the initial values of the Dirac and Majorana phases, strongly impact the leading order corrections to the neutrino mixing parameters and phases.\" This directly supports option D and contradicts options A and C. Furthermore, the text mentions that for scenarios with initial \u03bc-\u03c4 symmetry, \"the Dirac phase can take any value under small perturbations,\" which contradicts option B's claim of the Dirac phase being restricted to values close to 0 or \u03c0."}, "50": {"documentation": {"title": "Adaptive Exploration in Linear Contextual Bandit", "source": "Botao Hao, Tor Lattimore, Csaba Szepesvari", "docs_id": "1910.06996", "section": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive Exploration in Linear Contextual Bandit. Contextual bandits serve as a fundamental model for many sequential decision making tasks. The most popular theoretically justified approaches are based on the optimism principle. While these algorithms can be practical, they are known to be suboptimal asymptotically. On the other hand, existing asymptotically optimal algorithms for this problem do not exploit the linear structure in an optimal way and suffer from lower-order terms that dominate the regret in all practically interesting regimes. We start to bridge the gap by designing an algorithm that is asymptotically optimal and has good finite-time empirical performance. At the same time, we make connections to the recent literature on when exploration-free methods are effective. Indeed, if the distribution of contexts is well behaved, then our algorithm acts mostly greedily and enjoys sub-logarithmic regret. Furthermore, our approach is adaptive in the sense that it automatically detects the nice case. Numerical results demonstrate significant regret reductions by our method relative to several baselines."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the novel algorithm presented in the Arxiv documentation on \"Adaptive Exploration in Linear Contextual Bandit\"?\n\nA) It is based solely on the optimism principle and achieves asymptotic optimality.\n\nB) It exploits linear structure optimally but suffers from dominant lower-order terms in all practical regimes.\n\nC) It is asymptotically optimal, has good finite-time performance, and adapts to favorable context distributions with sub-logarithmic regret.\n\nD) It always requires extensive exploration and cannot operate in a mostly greedy manner under any circumstances.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation describes an algorithm that bridges the gap between existing approaches. It is stated to be asymptotically optimal and has good finite-time empirical performance. Additionally, the algorithm is adaptive and can detect when the distribution of contexts is well-behaved, allowing it to act mostly greedily and enjoy sub-logarithmic regret in such cases.\n\nAnswer A is incorrect because while the algorithm achieves asymptotic optimality, it is not solely based on the optimism principle. The document actually states that optimism-based approaches are suboptimal asymptotically.\n\nAnswer B is incorrect because the new algorithm is designed to overcome the problem of lower-order terms dominating regret in practical regimes, which was a limitation of existing asymptotically optimal algorithms.\n\nAnswer D is incorrect because the document explicitly states that under favorable conditions (well-behaved context distributions), the algorithm can act mostly greedily and achieve sub-logarithmic regret, contradicting the claim of always requiring extensive exploration."}, "51": {"documentation": {"title": "Measurement of flow harmonics with multi-particle cumulants in Pb+Pb\n  collisions at $\\sqrt{s_{NN}}=2.76$ TeV with the ATLAS detector", "source": "ATLAS Collaboration", "docs_id": "1408.4342", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of flow harmonics with multi-particle cumulants in Pb+Pb\n  collisions at $\\sqrt{s_{NN}}=2.76$ TeV with the ATLAS detector. ATLAS measurements of the azimuthal anisotropy in lead-lead collisions at $\\sqrt{s_{NN}}=2.76$ TeV are shown using a dataset of approximately 7 $\\mu$b$^{-1}$ collected at the LHC in 2010. The measurements are performed for charged particles with transverse momenta $0.5<p_T<20$ GeV and in the pseudorapidity range $|\\eta|<2.5$. The anisotropy is characterized by the Fourier coefficients, $v_n$, of the charged-particle azimuthal angle distribution for n = 2-4. The Fourier coefficients are evaluated using multi-particle cumulants calculated with the generating function method. Results on the transverse momentum, pseudorapidity and centrality dependence of the $v_n$ coefficients are presented. The elliptic flow, $v_2$, is obtained from the two-, four-, six- and eight-particle cumulants while higher-order coefficients, $v_3$ and $v_4$, are determined with two- and four-particle cumulants. Flow harmonics $v_n$ measured with four-particle cumulants are significantly reduced compared to the measurement involving two-particle cumulants. A comparison to $v_n$ measurements obtained using different analysis methods and previously reported by the LHC experiments is also shown. Results of measurements of flow fluctuations evaluated with multi-particle cumulants are shown as a function of transverse momentum and the collision centrality. Models of the initial spatial geometry and its fluctuations fail to describe the flow fluctuations measurements."}, "answer": "Question: In the ATLAS measurements of azimuthal anisotropy in lead-lead collisions at \u221as_NN = 2.76 TeV, which of the following statements is correct regarding the flow harmonics vn?\n\nA) v2, v3, and v4 are all obtained using two-, four-, six-, and eight-particle cumulants.\n\nB) v2 is obtained from two- and four-particle cumulants only, while v3 and v4 are determined with six- and eight-particle cumulants.\n\nC) v2 is obtained from two-, four-, six-, and eight-particle cumulants, while v3 and v4 are determined with two- and four-particle cumulants only.\n\nD) v2, v3, and v4 are all determined using only two-particle cumulants to ensure consistency across all harmonics.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that \"The elliptic flow, v2, is obtained from the two-, four-, six- and eight-particle cumulants while higher-order coefficients, v3 and v4, are determined with two- and four-particle cumulants.\" This directly corresponds to the statement in option C. \n\nOption A is incorrect because it suggests that v3 and v4 are also obtained using six- and eight-particle cumulants, which is not mentioned in the text. \n\nOption B is incorrect as it reverses the methods used for v2 versus v3 and v4. \n\nOption D is incorrect because it claims only two-particle cumulants are used for all harmonics, which contradicts the information provided in the document.\n\nThis question tests the student's ability to carefully read and understand the specific methods used for different flow harmonics in the study."}, "52": {"documentation": {"title": "A Hybrid Multiscale Model for Cancer Invasion of the Extracellular\n  Matrix", "source": "Nikolaos Sfakianakis, Anotida Madzvamuse, Mark A.J. Chaplain", "docs_id": "1805.10541", "section": ["q-bio.CB", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Hybrid Multiscale Model for Cancer Invasion of the Extracellular\n  Matrix. The ability to locally degrade the extracellular matrix (ECM) and interact with the tumour microenvironment is a key process distinguishing cancer from normal cells, and is a critical step in the metastatic spread of the tumour. The invasion of the surrounding tissue involves the coordinated action between cancer cells, the ECM, the matrix degrading enzymes, and the epithelial-to-mesenchymal transition (EMT). This is a regulatory process through which epithelial cells (ECs) acquire mesenchymal characteristics and transform to mesenchymal-like cells (MCs). In this paper, we present a new mathematical model which describes the transition from a collective invasion strategy for the ECs to an individual invasion strategy for the MCs. We achieve this by formulating a coupled hybrid system consisting of partial and stochastic differential equations that describe the evolution of the ECs and the MCs, respectively. This approach allows one to reproduce in a very natural way fundamental qualitative features of the current biomedical understanding of cancer invasion that are not easily captured by classical modelling approaches, for example, the invasion of the ECM by self-generated gradients and the appearance of EC invasion islands outside of the main body of the tumour."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the hybrid multiscale model for cancer invasion presented in the paper?\n\nA) It uses only partial differential equations to model both epithelial and mesenchymal-like cell behavior.\n\nB) It employs a combination of partial differential equations for epithelial cells and stochastic differential equations for mesenchymal-like cells.\n\nC) It utilizes only stochastic differential equations to model the entire invasion process.\n\nD) It uses agent-based modeling to simulate individual cell behavior for both cell types.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper describes a new mathematical model that uses a hybrid approach, combining partial differential equations (PDEs) to model the behavior of epithelial cells (ECs) and stochastic differential equations (SDEs) to model the behavior of mesenchymal-like cells (MCs). This hybrid approach allows the model to capture the transition from collective invasion strategy (ECs) to individual invasion strategy (MCs).\n\nAnswer A is incorrect because the model does not use only PDEs for both cell types. \n\nAnswer C is incorrect because the model does not use only SDEs for the entire process; it combines PDEs and SDEs.\n\nAnswer D is incorrect because the paper does not mention using agent-based modeling. Instead, it describes a coupled hybrid system of PDEs and SDEs.\n\nThis question tests the understanding of the mathematical modeling approach used in the paper and requires careful reading and comprehension of the complex methodology described."}, "53": {"documentation": {"title": "Assessing the significance of knockout cascades in metabolic networks", "source": "Oriol G\\\"uell, Francesc Sagu\\'es, Georg Basler, Zoran Nikoloski, M.\n  \\'Angeles Serrano", "docs_id": "1210.2908", "section": ["q-bio.MN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Assessing the significance of knockout cascades in metabolic networks. Complex networks have been shown to be robust against random structural perturbations, but vulnerable against targeted attacks. Robustness analysis usually simulates the removal of individual or sets of nodes, followed by the assessment of the inflicted damage. For complex metabolic networks, it has been suggested that evolutionary pressure may favor robustness against reaction removal. However, the removal of a reaction and its impact on the network may as well be interpreted as selective regulation of pathway activities, suggesting a tradeoff between the efficiency of regulation and vulnerability. Here, we employ a cascading failure algorithm to simulate the removal of single and pairs of reactions from the metabolic networks of two organisms, and estimate the significance of the results using two different null models: degree preserving and mass-balanced randomization. Our analysis suggests that evolutionary pressure promotes larger cascades of non-viable reactions, and thus favors the ability of efficient metabolic regulation at the expense of robustness."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: According to the study on metabolic networks described in the text, which of the following statements best represents the researchers' findings regarding evolutionary pressure on these networks?\n\nA) Evolutionary pressure favors robustness against reaction removal, making metabolic networks more resistant to cascading failures.\n\nB) Evolutionary pressure promotes smaller cascades of non-viable reactions to maintain network stability.\n\nC) Evolutionary pressure favors larger cascades of non-viable reactions, potentially enhancing the efficiency of metabolic regulation at the cost of robustness.\n\nD) Evolutionary pressure has no significant impact on the structure of metabolic networks or their response to reaction removal.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text states: \"Our analysis suggests that evolutionary pressure promotes larger cascades of non-viable reactions, and thus favors the ability of efficient metabolic regulation at the expense of robustness.\" This directly supports option C, indicating that evolutionary pressure favors larger cascades of non-viable reactions, which can enhance regulatory efficiency but reduce overall network robustness.\n\nOption A is incorrect because it contradicts the findings presented in the text. The study suggests that evolutionary pressure actually favors vulnerability in some aspects, not robustness against reaction removal.\n\nOption B is incorrect because it states the opposite of what the research found. The study indicates larger cascades, not smaller ones.\n\nOption D is incorrect because the text clearly states that evolutionary pressure does have a significant impact on metabolic networks, specifically in promoting larger cascades of non-viable reactions.\n\nThis question tests the student's ability to accurately interpret and synthesize the complex findings presented in the research, particularly the counterintuitive idea that evolution might favor certain vulnerabilities for the sake of regulatory efficiency."}, "54": {"documentation": {"title": "Order-v^4 Relativistic Corrections to Gluon Fragmentation into ^3S_1\n  Quarkonium", "source": "Geoffrey T. Bodwin (Argonne), U-Rae Kim, Jungil Lee (Korea U.)", "docs_id": "1301.3532", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Order-v^4 Relativistic Corrections to Gluon Fragmentation into ^3S_1\n  Quarkonium. We compute the relativistic corrections to the color-singlet contribution to gluon fragmentation into a J/psi at relative order v^4, making use of the nonrelativistic QCD (NRQCD) factorization approach. The corresponding full-QCD process exhibits infrared divergences that manifest themselves as single and double poles in epsilon in 4-2epsilon dimensions. We isolate the infrared-divergent contributions and treat them analytically. In the matching of full QCD to NRQCD, the pole contributions are absorbed into long-distance NRQCD matrix elements. The renormalizations of the ultraviolet divergences of the long-distance NRQCD matrix elements involve Born and one-loop single-pole counterterm contributions and Born double-pole counterterm contributions. While the order-v^4 contribution enhances the J/psi hadroproduction rate for the color-singlet channel substantially, this contribution is not important numerically in comparison with the color-octet contributions. We also find that the ^3P_J color-octet channel in the gluon fragmentation function contributes to J/psi hadroproduction significantly in comparison with the complete contribution of next-to-leading order in alpha_s in that channel."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements accurately describes the process and findings of the relativistic corrections to gluon fragmentation into J/psi at order v^4?\n\nA) The infrared-divergent contributions are treated numerically, and the pole contributions are eliminated in the matching of full QCD to NRQCD.\n\nB) The order-v^4 contribution significantly enhances the J/psi hadroproduction rate for the color-singlet channel, making it the dominant contribution compared to color-octet channels.\n\nC) The renormalization of ultraviolet divergences in long-distance NRQCD matrix elements involves only single-pole counterterm contributions at the Born level.\n\nD) The infrared-divergent contributions are treated analytically, and the pole contributions are absorbed into long-distance NRQCD matrix elements in the matching process.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The text explicitly states that the authors \"isolate the infrared-divergent contributions and treat them analytically.\" It also mentions that \"In the matching of full QCD to NRQCD, the pole contributions are absorbed into long-distance NRQCD matrix elements.\"\n\nOption A is incorrect because the infrared-divergent contributions are treated analytically, not numerically, and the pole contributions are absorbed, not eliminated.\n\nOption B is incorrect because while the order-v^4 contribution does enhance the J/psi hadroproduction rate for the color-singlet channel, the text states that \"this contribution is not important numerically in comparison with the color-octet contributions.\"\n\nOption C is incorrect because the renormalization process involves both single and double-pole counterterm contributions, as stated in the text: \"The renormalizations of the ultraviolet divergences of the long-distance NRQCD matrix elements involve Born and one-loop single-pole counterterm contributions and Born double-pole counterterm contributions.\""}, "55": {"documentation": {"title": "Methanol as a tracer of fundamental constants", "source": "S. A. Levshakov, M. G. Kozlov, and D. Reimers", "docs_id": "1106.1569", "section": ["astro-ph.CO", "physics.atom-ph", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Methanol as a tracer of fundamental constants. The methanol molecule CH3OH has a complex microwave spectrum with a large number of very strong lines. This spectrum includes purely rotational transitions as well as transitions with contributions of the internal degree of freedom associated with the hindered rotation of the OH group. The latter takes place due to the tunneling of hydrogen through the potential barriers between three equivalent potential minima. Such transitions are highly sensitive to changes in the electron-to-proton mass ratio, mu = m_e/m_p, and have different responses to mu-variations. The highest sensitivity is found for the mixed rotation-tunneling transitions at low frequencies. Observing methanol lines provides more stringent limits on the hypothetical variation of mu than ammonia observation with the same velocity resolution. We show that the best quality radio astronomical data on methanol maser lines constrain the variability of mu in the Milky Way at the level of |Delta mu/mu| < 28x10^{-9} (1sigma) which is in line with the previously obtained ammonia result, |Delta mu/mu| < 29x10^{-9} (1\\sigma). This estimate can be further improved if the rest frequencies of the CH3OH microwave lines will be measured more accurately."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about methanol (CH3OH) as a tracer of fundamental constants is NOT correct?\n\nA) Methanol's microwave spectrum includes purely rotational transitions and transitions involving the hindered rotation of the OH group.\n\nB) The tunneling of hydrogen through potential barriers between three equivalent potential minima contributes to methanol's spectral transitions.\n\nC) Methanol transitions are less sensitive to changes in the electron-to-proton mass ratio compared to ammonia transitions.\n\nD) The best current constraint on the variability of the electron-to-proton mass ratio (\u03bc) in the Milky Way using methanol maser lines is |\u0394\u03bc/\u03bc| < 28x10^(-9) (1\u03c3).\n\nCorrect Answer: C\n\nExplanation: \nA is correct as the passage states that methanol's spectrum includes \"purely rotational transitions as well as transitions with contributions of the internal degree of freedom associated with the hindered rotation of the OH group.\"\n\nB is correct as the text mentions \"the tunneling of hydrogen through the potential barriers between three equivalent potential minima.\"\n\nC is incorrect. The passage actually states that \"Observing methanol lines provides more stringent limits on the hypothetical variation of mu than ammonia observation with the same velocity resolution.\" This means methanol transitions are more sensitive, not less sensitive, to changes in the electron-to-proton mass ratio compared to ammonia.\n\nD is correct as the document explicitly states \"the best quality radio astronomical data on methanol maser lines constrain the variability of mu in the Milky Way at the level of |Delta mu/mu| < 28x10^{-9} (1sigma).\"\n\nThe incorrect statement C makes this a challenging question, as it requires careful reading and understanding of the relative sensitivities of methanol and ammonia to changes in the fundamental constants."}, "56": {"documentation": {"title": "Night-to-Night Variability of Sleep Electroencephalography-Based Brain\n  Age Measurements", "source": "Jacob Hogan, Haoqi Sun, Luis Paixao, Mike Westmeijer, Pooja Sikka,\n  Jing Jin, Ryan Tesh, Madalena Cardoso, Sydney S. Cash, Oluwaseun Akeju,\n  Robert Thomas, M. Brandon Westover", "docs_id": "2003.01248", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Night-to-Night Variability of Sleep Electroencephalography-Based Brain\n  Age Measurements. Objective Brain Age Index (BAI), calculated from sleep electroencephalography (EEG), has been proposed as a biomarker of brain health. This study quantifies night-to-night variability of BAI and establishes probability thresholds for inferring underlying brain pathology based on a patient's BAI. Methods 86 patients with multiple nights of consecutive EEG recordings were selected from Epilepsy Monitoring Unit patients whose EEGs reported as being within normal limits. BAI was calculated for each 12-hour segment of patient data using a previously described algorithm, and night-to-night variability in BAI was measured. Results The within-patient night-to-night standard deviation in BAI was 7.5 years. Estimates of BAI derived by averaging over 2, 3, and 4 nights had standard deviations of 4.7, 3.7, and 3.0 years, respectively. Conclusions Averaging BAI over n nights reduces night-to-night variability of BAI by a factor of the square root of n, rendering BAI more suitable as a biomarker of brain health at the individual level. Significance With increasing ease of EEG acquisition including wearable technology, BAI has the potential to track brain health and detect deviations from normal physiologic function. In a clinical setting, BAI could be used to identify patients who should undergo further investigation or monitoring."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A neurologist is considering using Brain Age Index (BAI) as a screening tool for potential brain pathology. Based on the study's findings on night-to-night variability, what is the most appropriate approach to obtain a reliable BAI measurement for an individual patient?\n\nA) Calculate BAI from a single night of sleep EEG recording\nB) Average BAI measurements from two consecutive nights of sleep EEG recordings\nC) Average BAI measurements from four consecutive nights of sleep EEG recordings\nD) Use the highest BAI measurement obtained over multiple nights of recording\n\nCorrect Answer: C\n\nExplanation: The study found that the night-to-night variability in BAI measurements is significant, with a within-patient standard deviation of 7.5 years for a single night's measurement. The research shows that averaging BAI over multiple nights reduces this variability. Specifically, averaging over 4 nights reduced the standard deviation to 3.0 years, which is the lowest variability reported in the study. This makes option C the most appropriate approach for obtaining a reliable BAI measurement.\n\nOption A is incorrect because a single night's measurement has the highest variability (SD of 7.5 years). Option B, while better than a single night, still has a higher variability (SD of 4.7 years) compared to four nights. Option D is incorrect because using the highest measurement would not account for night-to-night variability and could lead to overestimation of brain age.\n\nThe question tests understanding of statistical concepts like variability reduction through averaging, as well as the practical application of research findings in a clinical context."}, "57": {"documentation": {"title": "Simulation and estimation of an agent-based market-model with a matching\n  engine", "source": "Ivan Jericevich and Patrick Chang and Tim Gebbie", "docs_id": "2108.07806", "section": ["q-fin.TR", "cs.MA", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simulation and estimation of an agent-based market-model with a matching\n  engine. An agent-based model with interacting low frequency liquidity takers inter-mediated by high-frequency liquidity providers acting collectively as market makers can be used to provide realistic simulated price impact curves. This is possible when agent-based model interactions occur asynchronously via order matching using a matching engine in event time to replace sequential calendar time market clearing. Here the matching engine infrastructure has been modified to provide a continuous feed of order confirmations and updates as message streams in order to conform more closely to live trading environments. The resulting trade and quote message data from the simulations are then aggregated, calibrated and visualised. Various stylised facts are presented along with event visualisations and price impact curves. We argue that additional realism in modelling can be achieved with a small set of agent parameters and simple interaction rules once interactions are reactive, asynchronous and in event time. We argue that the reactive nature of market agents may be a fundamental property of financial markets and when accounted for can allow for parsimonious modelling without recourse to additional sources of noise."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation and argument presented in the agent-based market model described in the Arxiv documentation?\n\nA) The model primarily relies on additional sources of noise to achieve realistic market simulations.\n\nB) The model uses a complex set of agent parameters and intricate interaction rules to replicate market behavior.\n\nC) The model emphasizes the importance of synchronous interactions in calendar time for market clearing.\n\nD) The model highlights the significance of reactive, asynchronous interactions in event time, mediated by a matching engine, to achieve realistic market simulations with minimal parameters.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation emphasizes that the model achieves additional realism by implementing reactive, asynchronous interactions in event time, facilitated by a matching engine. This approach allows for parsimonious modeling (i.e., with a small set of agent parameters and simple interaction rules) without needing to introduce additional sources of noise.\n\nAnswer A is incorrect because the documentation explicitly states that the model achieves realism without recourse to additional sources of noise.\n\nAnswer B is incorrect as the documentation mentions using a \"small set of agent parameters and simple interaction rules,\" contradicting the idea of complex parameters and intricate rules.\n\nAnswer C is incorrect because the model specifically replaces \"sequential calendar time market clearing\" with asynchronous interactions via a matching engine in event time.\n\nThe key innovation lies in the use of the matching engine for asynchronous interactions in event time, which the authors argue may be a fundamental property of financial markets, allowing for more realistic simulations with simpler models."}, "58": {"documentation": {"title": "Dynamical Phenomena in an Optical-Wavelength Phonon Laser (Phaser):\n  Nonlinear Resonances and Self-Organized Mode Alternation", "source": "D. N. Makovetskii", "docs_id": "1101.0482", "section": ["physics.optics", "cond-mat.other", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Phenomena in an Optical-Wavelength Phonon Laser (Phaser):\n  Nonlinear Resonances and Self-Organized Mode Alternation. This is a part of an overview of my early studies on nonlinear spin-phonon dynamics in solid state optical-wavelength phonon lasers (phasers) started in 1984. The main goal of this work is a short description and a qualitative analysis of experimental data on low-frequency nonlinear resonances revealed in a nonautonomous ruby phaser. Under phaser pumping modulation near these resonances, an unusual kind of self-organized motions in the ruby spin-phonon system was observed by me in 1984 for the first time. The original technique of optical-wavelength microwave-frequency acoustic stimulated emission (SE) detection and microwave-frequency power spectra (MFPS) analysis was used in these experiments (description of the technique see: D.N.Makovetskii, Cand. Sci. Diss., Kharkov, 1983). The real time evolution of MFPS was studied using this technique at scales up to several hours. The phenomenon of the self-organized periodic alternation of SE phonon modes was experimentally revealed at hyperlow frequencies from about 0.1 Hz to 0.001 Hz and less (for the phaser pumping modulation at approximately 10 Hz). The nature of this phenomenon was investigated by me later in details (see: arXiv:cond-mat/0303188v1 ; arXiv:cond-mat/0410460v1 ; Tech. Phys. Letters, 2001, Vol.27, No.6, P.511-514 ; Tech. Phys., 2004, Vol.49, No.2, P.224-231)."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In the study of nonlinear spin-phonon dynamics in solid state optical-wavelength phonon lasers (phasers), what unique phenomenon was observed by the researcher in 1984 under specific experimental conditions?\n\nA) Continuous emission of phonons at a single frequency\nB) Random fluctuations in the microwave-frequency power spectra\nC) Self-organized periodic alternation of stimulated emission phonon modes\nD) Constant hyperlow frequency oscillations at 10 Hz\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that \"The phenomenon of the self-organized periodic alternation of SE phonon modes was experimentally revealed at hyperlow frequencies from about 0.1 Hz to 0.001 Hz and less (for the phaser pumping modulation at approximately 10 Hz).\" This unique observation was made by the researcher in 1984 using a specialized technique for detecting optical-wavelength microwave-frequency acoustic stimulated emission (SE) and analyzing microwave-frequency power spectra (MFPS).\n\nOption A is incorrect because the phenomenon observed was not continuous emission at a single frequency, but rather an alternation between different modes.\n\nOption B is incorrect as the observed phenomenon was not random fluctuations, but a self-organized periodic alternation.\n\nOption D is incorrect because while the phaser pumping modulation was at approximately 10 Hz, the observed phenomenon occurred at much lower frequencies (0.1 Hz to 0.001 Hz and less).\n\nThis question tests the student's ability to identify and understand the key experimental finding described in the passage, distinguishing it from other plausible but incorrect interpretations of the research."}, "59": {"documentation": {"title": "Vortex Nucleation in a Dissipative Variant of the Nonlinear\n  Schr\\\"odinger Equation under Rotation", "source": "R. Carretero-Gonzalez, P.G. Kevrekidis, and T. Kolokolnikov", "docs_id": "1412.0615", "section": ["cond-mat.quant-gas", "math.AP", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vortex Nucleation in a Dissipative Variant of the Nonlinear\n  Schr\\\"odinger Equation under Rotation. In the present work, we motivate and explore the dynamics of a dissipative variant of the nonlinear Schr{\\\"o}dinger equation under the impact of external rotation. As in the well established Hamiltonian case, the rotation gives rise to the formation of vortices. We show, however, that the most unstable mode leading to this instability scales with an appropriate power of the chemical potential $\\mu$ of the system, increasing proportionally to $\\mu^{2/3}$. The precise form of the relevant formula, obtained through our asymptotic analysis, provides the most unstable mode as a function of the atomic density and the trap strength. We show how these unstable modes typically nucleate a large number of vortices in the periphery of the atomic cloud. However, through a pattern selection mechanism, prompted by symmetry-breaking, only few isolated vortices are pulled in sequentially from the periphery towards the bulk of the cloud resulting in highly symmetric stable vortex configurations with far fewer vortices than the original unstable mode. These results may be of relevance to the experimentally tractable realm of finite temperature atomic condensates."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the dissipative variant of the nonlinear Schr\u00f6dinger equation under rotation, how does the most unstable mode leading to vortex formation scale with respect to the chemical potential \u03bc of the system?\n\nA) Proportional to \u03bc^(1/2)\nB) Proportional to \u03bc^(2/3)\nC) Proportional to \u03bc\nD) Proportional to \u03bc^(3/2)\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the key finding in the document regarding the scaling of the most unstable mode in the dissipative variant of the nonlinear Schr\u00f6dinger equation under rotation. The correct answer is B, as the document explicitly states that the most unstable mode \"increases proportionally to \u03bc^(2/3)\". This scaling relationship is a crucial difference from the well-established Hamiltonian case and represents a significant finding of the study. Options A, C, and D represent incorrect scaling relationships that are not supported by the information provided in the document."}}