{"0": {"documentation": {"title": "The Spin Distribution of Fast Spinning Neutron Stars in Low Mass X-Ray\n  Binaries: Evidence for Two Sub-Populations", "source": "A. Patruno, B. Haskell, N. Andersson", "docs_id": "1705.07669", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Spin Distribution of Fast Spinning Neutron Stars in Low Mass X-Ray\n  Binaries: Evidence for Two Sub-Populations. We study the current sample of rapidly rotating neutron stars in both accreting and non-accreting binaries in order to determine whether the spin distribution of accreting neutron stars in low-mass X-ray binaries can be reconciled with current accretion torque models. We perform a statistical analysis of the spin distributions and show that there is evidence for two sub-populations among low-mass X-ray binaries, one at relatively low spin frequency, with an average of ~300 Hz and a broad spread, and a peaked population at higher frequency with average spin frequency of ~575 Hz. We show that the two sub-populations are separated by a cut-point at a frequency of ~540 Hz. We also show that the spin frequency of radio millisecond pulsars does not follow a log-normal distribution and shows no evidence for the existence of distinct sub-populations. We discuss the uncertainties of different accretion models and speculate that either the accreting neutron star cut-point marks the onset of gravitational waves as an efficient mechanism to remove angular momentum or some of the neutron stars in the fast sub-population do not evolve into radio millisecond pulsars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most likely explanation for the observed separation between the two sub-populations of neutron stars in low-mass X-ray binaries, as indicated by the cut-point at a frequency of ~540 Hz?\n\nA) The accreting neutron star cut-point marks the onset of gravitational waves as an efficient mechanism to remove angular momentum.\nB) The two sub-populations are separated by a difference in their magnetic field strengths.\nC) The observed separation is due to a difference in the accretion rates of the neutron stars.\nD) The neutron stars in the fast sub-population are more likely to be in binary systems with more massive companions.\n\nCorrect Answer: A) The accreting neutron star cut-point marks the onset of gravitational waves as an efficient mechanism to remove angular momentum.\n\nExplanation: The question requires the test-taker to understand the implications of the observed separation between the two sub-populations of neutron stars. The correct answer, A, is supported by the text, which suggests that the accreting neutron star cut-point may mark the onset of gravitational waves as an efficient mechanism to remove angular momentum. This would explain why the fast sub-population is separated from the slow sub-population. The other options are incorrect because they do not provide a plausible explanation for the observed separation. Option B is incorrect because there is no mention of magnetic field strengths in the text. Option C is incorrect because the text does not suggest that accretion rates are the cause of the separation. Option D is incorrect because the text does not provide any information about the masses of the companions in the binary systems."}, "1": {"documentation": {"title": "Conch Maximal Subrings", "source": "Alborz Azarang", "docs_id": "2009.05995", "section": ["math.AC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conch Maximal Subrings. It is shown that if $R$ is a ring, $p$ a prime element of an integral domain $D\\leq R$ with $\\bigcap_{n=1}^\\infty p^nD=0$ and $p\\in U(R)$, then $R$ has a conch maximal subring (see \\cite{faith}). We prove that either a ring $R$ has a conch maximal subring or $U(S)=S\\cap U(R)$ for each subring $S$ of $R$ (i.e., each subring of $R$ is closed with respect to taking inverse, see \\cite{invsub}). In particular, either $R$ has a conch maximal subring or $U(R)$ is integral over the prime subring of $R$. We observe that if $R$ is an integral domain with $|R|=2^{2^{\\aleph_0}}$, then either $R$ has a maximal subring or $|Max(R)|=2^{\\aleph_0}$, and in particular if in addition $dim(R)=1$, then $R$ has a maximal subring. If $R\\subseteq T$ be an integral ring extension, $Q\\in Spec(T)$, $P:=Q\\cap R$, then we prove that whenever $R$ has a conch maximal subring $S$ with $(S:R)=P$, then $T$ has a conch maximal subring $V$ such that $(V:T)=Q$ and $V\\cap R=S$. It is shown that if $K$ is an algebraically closed field which is not algebraic over its prime subring and $R$ is affine ring over $K$, then for each prime ideal $P$ of $R$ with $ht(P)\\geq dim(R)-1$, there exists a maximal subring $S$ of $R$ with $(S:R)=P$. If $R$ is a normal affine integral domain over a field $K$, then we prove that $R$ is an integrally closed maximal subring of a ring $T$ if and only if $dim(R)=1$ and in particular in this case $(R:T)=0$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $R$ be an integral domain with a prime element $p$ such that $\\bigcap_{n=1}^\\infty p^nR=0$ and $p\\in U(R)$. Consider the following statements:\n\nA) $R$ has a conch maximal subring.\nB) For each subring $S$ of $R$, $U(S)=S\\cap U(R)$.\nC) If $R$ is an integral domain with $|R|=2^{2^{\\aleph_0}}$, then either $R$ has a maximal subring or $|Max(R)|=2^{\\aleph_0}$.\nD) If $R$ is a normal affine integral domain over a field $K$, then $R$ is an integrally closed maximal subring of a ring $T$ if and only if $dim(R)=1$.\n\nCorrect Answer: A\n\nExplanation: The correct answer is A) $R$ has a conch maximal subring. This is a direct consequence of the statement in the documentation that if $R$ is a ring, $p$ a prime element of an integral domain $D\\leq R$ with $\\bigcap_{n=1}^\\infty p^nD=0$ and $p\\in U(R)$, then $R$ has a conch maximal subring. The other options are not necessarily true. For example, option B is not necessarily true, as there are subrings $S$ of $R$ for which $U(S)\\neq S\\cap U(R)$. Option C is also not necessarily true, as the statement only holds for integral domains with a specific cardinality. Option D is not necessarily true, as the statement only holds for normal affine integral domains over fields."}, "2": {"documentation": {"title": "Transient excitation of Higgs and high-harmonic generation in\n  superconductors with quench-drive spectroscopy", "source": "Matteo Puviani, Rafael Haenel and Dirk Manske", "docs_id": "2112.12123", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transient excitation of Higgs and high-harmonic generation in\n  superconductors with quench-drive spectroscopy. Time-resolved terahertz spectroscopies are powerful techniques to excite and probe non-equilibrium states of superconductors, directly addressing collective modes. In pump-probe spectroscopies, THz pulses are used to quench a system, which is subsequently probed by either a THz or optical pulse. In contrast, third harmonic generation experiments employ a single multicycle driving pulse and measure the induced third harmonic. In this work, we analyze a new spectroscopy setup where both, a quench and a drive, are applied and 2D spectra as a function of time and quench-drive-delay are recorded. We calculate the time evolution of the nonlinear current generated in the superconductor within a Anderson-pseudospin framework and analyze all experimental signatures in 2D Fourier space using a quasi-equilibrium approach. We analyze the superconducting response in Fourier space with respect to both the frequencies corresponding to the real time and the quench-drive delay time. In particular, we show the presence of a transient modulation of higher harmonics, induced by a wave mixing process of the drive with the quench pulse, which excites the quasiparticles and the Higgs mode at the same time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the new spectroscopy setup described in the paper, and how does it differ from traditional pump-probe spectroscopies?\n\nA) To measure the nonlinear current generated in the superconductor using a single multicycle driving pulse and a THz pulse, and to analyze the induced third harmonic.\nB) To analyze the superconducting response in Fourier space with respect to both the frequencies corresponding to the real time and the quench-drive delay time, and to calculate the time evolution of the nonlinear current generated in the superconductor.\nC) To excite and probe non-equilibrium states of superconductors using time-resolved terahertz spectroscopies, and to directly address collective modes.\nD) To measure the transient excitation of Higgs and high-harmonic generation in superconductors using a quench-drive spectroscopy setup, and to analyze the experimental signatures in 2D Fourier space.\n\n**Correct Answer:** D) To measure the transient excitation of Higgs and high-harmonic generation in superconductors using a quench-drive spectroscopy setup, and to analyze the experimental signatures in 2D Fourier space.\n\n**Explanation:** The correct answer is D) because the paper describes a new spectroscopy setup that combines quench-drive spectroscopy with third harmonic generation experiments. The primary goal of this setup is to measure the transient excitation of Higgs and high-harmonic generation in superconductors, which is achieved by applying both a quench and a drive to the system. The experimental signatures are then analyzed in 2D Fourier space, which is a key aspect of the paper.\n\n**Candidate A:** This option is incorrect because it describes a traditional pump-probe spectroscopy setup, which is not the primary goal of the new spectroscopy setup described in the paper.\n\n**Candidate B:** This option is partially correct, as it mentions the calculation of the time evolution of the nonlinear current generated in the superconductor and the analysis of the superconducting response in Fourier space. However, it does not accurately describe the primary goal of the new spectroscopy setup.\n\n**Candidate C:** This option is partially correct, as it mentions the use of time-resolved terahertz spectroscopies to excite and probe non-equilibrium states of superconductors. However, it does not accurately describe the primary goal of the new spectroscopy setup, which is to measure the transient excitation of Higgs and high-harmonic generation in superconductors."}, "3": {"documentation": {"title": "Irreversible Evolution of a Wave Packet in The Rigged Hilbert Space\n  Quantum Mechanics", "source": "Giulia Marcucci and Claudio Conti", "docs_id": "1607.08109", "section": ["quant-ph", "math-ph", "math.MP", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Irreversible Evolution of a Wave Packet in The Rigged Hilbert Space\n  Quantum Mechanics. It is well known that a state with complex energy cannot be the eigenstate of a self-adjoint operator, like the Hamiltonian. Resonances, i.e. states with exponentially decaying observables, are not vectors belonging to the conventional Hilbert space. One can describe these resonances in an unusual mathematical formalism, based on the so-called Rigged Hilbert Space (RHS). In the RHS, the states with complex energy are denoted as Gamow Vectors (GV), and they model decay processes. We study GV of the Reversed Harmonic Oscillator (RHO), and we analytically and numerically investigate the unstable evolution of wave packets. We introduce the background function to study initial data not composed only by a summation of GV and we analyse different wave packets belonging to specific function spaces. Our work furnishes support to the idea that irreversible wave propagations can be investigated by means of Rigged Hilbert Space Quantum Mechanics and provides insights for the experimental investigation of irreversible dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mathematical formalism used to describe resonances in Quantum Mechanics, and how does it differ from the conventional Hilbert space?\n\nA) Rigged Hilbert Space (RHS) is used to describe resonances, which are modeled as Gamow Vectors (GV) that cannot be part of the conventional Hilbert space.\n\nB) The conventional Hilbert space is used to describe resonances, which are modeled as Gamow Vectors (GV) that are part of the Rigged Hilbert Space (RHS).\n\nC) The Rigged Hilbert Space (RHS) is used to describe states with complex energy, which are denoted as Gamow Vectors (GV) and model decay processes.\n\nD) The conventional Hilbert space is used to describe states with complex energy, which are denoted as Gamow Vectors (GV) and model decay processes.\n\nCorrect Answer: C) The Rigged Hilbert Space (RHS) is used to describe states with complex energy, which are denoted as Gamow Vectors (GV) and model decay processes.\n\nExplanation: The correct answer is C) because the documentation states that the Rigged Hilbert Space (RHS) is used to describe states with complex energy, which are denoted as Gamow Vectors (GV) and model decay processes. The RHS is a mathematical formalism that allows for the description of resonances, which are states with exponentially decaying observables. The other options are incorrect because they either describe the conventional Hilbert space or incorrectly state the relationship between the RHS and GV."}, "4": {"documentation": {"title": "Crosslinking and depletion determine spatial instabilities in\n  cytoskeletal active matter", "source": "Guillaume Sarfati, Ananyo Maitra, Raphael Voituriez, Jean-Christophe\n  Galas, Andr\\'e Estevez-Torres", "docs_id": "2112.11361", "section": ["cond-mat.soft", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Crosslinking and depletion determine spatial instabilities in\n  cytoskeletal active matter. Active gels made of cytoskeletal proteins are valuable materials with attractive non-equilibrium properties such as spatial self-organization and self-propulsion. At least four typical routes to spatial patterning have been reported to date in different types of cytoskeletal active gels: bending and buckling instabilities in extensile systems, and global and local contraction instabilities in contractile gels. Here we report the observation of these four instabilities in a single type of active gel and we show that they are controlled by two parameters: the concentrations of ATP and depletion agent. We demonstrate that as the ATP concentration decreases, the concentration of passive motors increases until the gel undergoes a gelation transition. At this point, buckling is selected against bending, while global contraction is favored over local ones. Our observations are coherent with a hydrodynamic model of a viscoelastic active gel where the filaments are cross-linked with a characteristic time that diverges as the ATP concentration decreases. Our work thus provides a unified view of spatial instabilities in cytoskeletal active matter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the concentration of ATP affects the spatial instabilities in cytoskeletal active gels, as observed in the study?\n\nA) The concentration of ATP directly influences the cross-linking of filaments, leading to changes in the gel's viscoelastic properties.\nB) The concentration of ATP determines the number of active motors in the gel, which in turn affects the gel's ability to undergo gelation transitions.\nC) The concentration of ATP controls the rate of filament cross-linking, leading to changes in the gel's mechanical properties and the selection of different instabilities.\nD) The concentration of ATP has no effect on the spatial instabilities in cytoskeletal active gels, as the instabilities are solely determined by the depletion agent.\n\nCorrect Answer: C) The concentration of ATP controls the rate of filament cross-linking, leading to changes in the gel's mechanical properties and the selection of different instabilities.\n\nExplanation: The study suggests that as the ATP concentration decreases, the concentration of passive motors increases, leading to a gelation transition. At this point, the gel undergoes a transition from bending and buckling instabilities to global and local contraction instabilities. This transition is controlled by the rate of filament cross-linking, which is influenced by the concentration of ATP. Therefore, option C is the correct answer.\n\nNote: The other options are incorrect because:\n\n* Option A oversimplifies the relationship between ATP concentration and filament cross-linking.\n* Option B is incorrect because the study does not suggest that the number of active motors is the primary mechanism by which ATP affects spatial instabilities.\n* Option D is incorrect because the study clearly shows that the concentration of ATP has a significant effect on the spatial instabilities in cytoskeletal active gels."}, "5": {"documentation": {"title": "The Parameterized Complexity of some Permutation Group Problems", "source": "Vikraman Arvind", "docs_id": "1301.0379", "section": ["cs.CC", "cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Parameterized Complexity of some Permutation Group Problems. In this paper we study the parameterized complexity of two well-known permutation group problems which are NP-complete. 1. Given a permutation group G=<S>, subgroup of $S_n$, and a parameter $k$, find a permutation $\\pi$ in G such that $|{i\\in [n]\\mid \\pi(i)\\ne i}|$ is at least $k$. This generalizes the well-known NP-complete problem of finding a fixed-point free permutation in G. (this is the case when $k=n$). We show that this problem with parameter $k$ is fixed parameter tractable. In the process, we give a simple deterministic polynomial-time algorithm for finding a fixed point free element in a transitive permutation group, answering an open question of Cameron. 2. Next we consider the problem of computing a base for a permutation group G=<S>. A base for G is a subset B of $[n]$ such that the subgroup of G that fixes B pointwise is trivial. This problem is known to be NP-complete. We show that it is fixed parameter tractable for the case of cyclic permutation groups and for permutation groups of constant orbit size. For more general classes of permutation groups we do not know whether the problem is in FPT or is W[1]-hard."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the parameterized complexity of the problem of finding a base for a permutation group G=<S>, and how does it relate to the fixed-point free permutation problem?\n\nA) The problem is NP-complete and fixed parameter tractable for cyclic permutation groups.\nB) The problem is W[1]-hard for all classes of permutation groups.\nC) The problem is fixed parameter tractable for permutation groups of constant orbit size and NP-complete otherwise.\nD) The problem is in P for all classes of permutation groups.\n\nCorrect Answer: C) The problem is fixed parameter tractable for permutation groups of constant orbit size and NP-complete otherwise.\n\nExplanation: The correct answer is C) because the paper states that the problem of finding a base for a permutation group G=<S> is fixed parameter tractable for permutation groups of constant orbit size. Additionally, the paper mentions that the problem is NP-complete for more general classes of permutation groups. The other options are incorrect because they do not accurately reflect the parameterized complexity of the problem as stated in the paper. Option A is incorrect because the problem is not fixed parameter tractable for all cyclic permutation groups. Option B is incorrect because the problem is not W[1]-hard for all classes of permutation groups. Option D is incorrect because the problem is not in P for all classes of permutation groups."}, "6": {"documentation": {"title": "Super-resolution in recovering embedded electromagnetic sources in high\n  contrast media", "source": "Habib Ammari, Bowen Li, Jun Zou", "docs_id": "2001.07116", "section": ["math.AP", "math.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Super-resolution in recovering embedded electromagnetic sources in high\n  contrast media. The purpose of this work is to provide a rigorous mathematical analysis of the expected super-resolution phenomenon in the time-reversal imaging of electromagnetic (EM) radiating sources embedded in a high contrast medium. It is known that the resolution limit is essentially determined by the sharpness of the imaginary part of the EM Green's tensor for the associated background. We first establish the close connection between the resolution and the material parameters and the resolvent of the electric integral operator, via the Lippmann-Schwinger representation formula. We then present an insightful characterization of the spectral structure of the integral operator for a general bounded domain and derive the pole-pencil decomposition of its resolvent in the high contrast regime. For the special case of a spherical domain, we provide some quantitative asymptotic behavior of the eigenvalues and eigenfunctions. These mathematical findings shall enable us to provide a concise and rigorous illustration of the super-resolution in the EM source reconstruction in high contrast media. Some numerical examples are also presented to verify our main theoretical results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of super-resolution in time-reversal imaging of electromagnetic sources embedded in high contrast media, what is the primary factor that determines the resolution limit, according to the work presented in the Arxiv documentation?\n\nA) The sharpness of the real part of the EM Green's tensor for the associated background\nB) The sharpness of the imaginary part of the EM Green's tensor for the associated background\nC) The material parameters of the medium and the eigenvalues of the electric integral operator\nD) The spectral structure of the integral operator for a general bounded domain\n\nCorrect Answer: B) The sharpness of the imaginary part of the EM Green's tensor for the associated background\n\nExplanation: The correct answer is B) The sharpness of the imaginary part of the EM Green's tensor for the associated background. According to the work presented in the Arxiv documentation, the resolution limit is essentially determined by the sharpness of the imaginary part of the EM Green's tensor for the associated background. This is established through the Lippmann-Schwinger representation formula and the pole-pencil decomposition of the resolvent in the high contrast regime.\n\nCandidate A is incorrect because the real part of the EM Green's tensor is not the primary factor that determines the resolution limit. Candidate C is incorrect because while the material parameters of the medium and the eigenvalues of the electric integral operator are related to the resolution limit, they are not the primary factor. Candidate D is incorrect because the spectral structure of the integral operator for a general bounded domain is not the primary factor that determines the resolution limit."}, "7": {"documentation": {"title": "Deviations from the Fundamental Plane and the Peculiar Velocities of\n  Clusters", "source": "R. A. Gibbons, A. S. Fruchter, and G. D. Bothun", "docs_id": "astro-ph/9903380", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deviations from the Fundamental Plane and the Peculiar Velocities of\n  Clusters. We have fit the Fundamental Plane of Ellipticals (FP) to over 400 early-type galaxies in 20 nearby clusters (4000 < cz < 11000 km/s), using our own photometry and spectroscopy as well as measurements culled from the literature. We find that the quality-of-fit, r.m.s.[log(sigma)], to the average fundamental plane <FP> varies substantially among these clusters. A statistically significant gap in r.m.s.[log(sigma)] roughly separates the clusters which fit <FP> well from those that do not. In addition, these two groups of clusters show distinctly different behavior in their peculiar velocity (PV) distributions. Assuming galaxies are drawn from a single underlying population, cluster PV should not be correlated with r.m.s.[log(sigma)]. Instead, the clusters with below average scatter display no motion with respect to the cosmic microwave background (CMB) within our measurement errors (~250 km/s), while clusters in the poor-fit group typically show large PVs. Furthermore, we find that all X-ray bright clusters in our sample fit the <FP> well, suggesting that early-type galaxies in the most massive, virialized clusters form a more uniform population than do cluster ellipticals as a whole, and that these clusters participate in a quiet Hubble flow."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the relationship between the quality-of-fit to the Fundamental Plane of Ellipticals (FP) and the peculiar velocities of clusters, according to the study?\n\nA) Clusters with a higher quality-of-fit to <FP> tend to have larger peculiar velocities, indicating a correlation between the two.\nB) Clusters with a lower quality-of-fit to <FP> tend to have smaller peculiar velocities, indicating a correlation between the two.\nC) Clusters with a higher quality-of-fit to <FP> tend to have smaller peculiar velocities, while clusters with a lower quality-of-fit to <FP> tend to have larger peculiar velocities, indicating a lack of correlation between the two.\nD) The study found no correlation between the quality-of-fit to <FP> and the peculiar velocities of clusters.\n\nCorrect Answer: C) Clusters with a higher quality-of-fit to <FP> tend to have smaller peculiar velocities, while clusters with a lower quality-of-fit to <FP> tend to have larger peculiar velocities, indicating a lack of correlation between the two.\n\nExplanation: The study found that clusters with a higher quality-of-fit to <FP> (i.e., those with a lower scatter in the FP) tend to have smaller peculiar velocities, while clusters with a lower quality-of-fit to <FP> tend to have larger peculiar velocities. This suggests that the quality-of-fit to <FP> is not correlated with the peculiar velocities of clusters, as expected if galaxies are drawn from a single underlying population."}, "8": {"documentation": {"title": "Hermitizing the HAL QCD potential in the derivative expansion", "source": "Sinya Aoki, Takumi Iritani and Koichi Yazaki", "docs_id": "1909.00656", "section": ["hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hermitizing the HAL QCD potential in the derivative expansion. A formalism is given to hermitize the HAL QCD potential, which needs to be non-hermitian except the leading order (LO) local term in the derivative expansion as the Nambu-Bethe-Salpeter (NBS) wave functions for different energies are not orthogonal to each other. It is shown that the non-hermitian potential can be hermitized order by order to all orders in the derivative expansion. In particular, the next-to-leading order (NLO) potential can be exactly hermitized without approximation. The formalism is then applied to a simple case of $\\Xi \\Xi (^{1}S_{0}) $ scattering, for which the HAL QCD calculation is available to the NLO. The NLO term gives relatively small corrections to the scattering phase shift and the LO analysis seems justified in this case. We also observe that the local part of the hermitized NLO potential works better than that of the non-hermitian NLO potential. The hermitian version of the HAL QCD potential is desirable for comparing it with phenomenological interactions and also for using it as a two-body interaction in many body systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main motivation for hermitizing the HAL QCD potential in the derivative expansion, and what are the implications of this hermitization for the analysis of scattering phase shifts?\n\nA) Hermitization is necessary to ensure the potential is real-valued, which is a requirement for phenomenological comparisons. However, this hermitization may introduce errors in the analysis of scattering phase shifts.\n\nB) Hermitization is required to ensure the NBS wave functions are orthogonal to each other, which is a necessary condition for the correct analysis of scattering phase shifts. This hermitization can be achieved order by order in the derivative expansion.\n\nC) Hermitization is not necessary, as the HAL QCD potential is already hermitian except for the leading order local term. This term can be hermitized separately, but it is not required to hermitize the entire potential.\n\nD) Hermitization is only necessary for the next-to-leading order (NLO) potential, as the leading order (LO) local term can be hermitized exactly without approximation.\n\n**Correct Answer:** B) Hermitization is required to ensure the NBS wave functions are orthogonal to each other, which is a necessary condition for the correct analysis of scattering phase shifts. This hermitization can be achieved order by order in the derivative expansion.\n\n**Explanation:** The correct answer is B) Hermitization is required to ensure the NBS wave functions are orthogonal to each other, which is a necessary condition for the correct analysis of scattering phase shifts. This hermitization can be achieved order by order in the derivative expansion. The other options are incorrect because hermitization is not only necessary for the NLO potential (option D), and it is not sufficient to hermitize only the LO local term (option C). Additionally, hermitization is not only required for the NBS wave functions to be orthogonal (option A)."}, "9": {"documentation": {"title": "A computational model implementing subjectivity with the 'Room Theory'.\n  The case of detecting Emotion from Text", "source": "Carlo Lipizzi, Dario Borrelli, Fernanda de Oliveira Capela", "docs_id": "2005.06059", "section": ["cs.CL", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A computational model implementing subjectivity with the 'Room Theory'.\n  The case of detecting Emotion from Text. This work introduces a new method to consider subjectivity and general context dependency in text analysis and uses as example the detection of emotions conveyed in text. The proposed method takes into account subjectivity using a computational version of the Framework Theory by Marvin Minsky (1974) leveraging on the Word2Vec approach to text vectorization by Mikolov et al. (2013), used to generate distributed representation of words based on the context where they appear. Our approach is based on three components: 1. a framework/'room' representing the point of view; 2. a benchmark representing the criteria for the analysis - in this case the emotion classification, from a study of human emotions by Robert Plutchik (1980); and 3. the document to be analyzed. By using similarity measure between words, we are able to extract the relative relevance of the elements in the benchmark - intensities of emotions in our case study - for the document to be analyzed. Our method provides a measure that take into account the point of view of the entity reading the document. This method could be applied to all the cases where evaluating subjectivity is relevant to understand the relative value or meaning of a text. Subjectivity can be not limited to human reactions, but it could be used to provide a text with an interpretation related to a given domain (\"room\"). To evaluate our method, we used a test case in the political domain."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the \"Room Theory\" computational model for detecting subjectivity in text, what is the primary purpose of the \"benchmark\" component, and how does it relate to the framework/theory of Marvin Minsky (1974)?\n\n**A)** The benchmark is used to evaluate the overall sentiment of the text, while the framework is used to generate a distributed representation of words based on context.\n\n**B)** The benchmark is a representation of the criteria for analysis, which in this case is the emotion classification, as proposed by Robert Plutchik (1980), and is used to extract the relative relevance of elements in the benchmark for the document to be analyzed.\n\n**C)** The benchmark is a measure of the point of view of the entity reading the document, while the framework is a computational version of the Framework Theory by Marvin Minsky (1974) that leverages Word2Vec approach to text vectorization.\n\n**D)** The benchmark is a representation of the document to be analyzed, while the framework is a measure of the similarity between words, used to extract the relative relevance of elements in the benchmark for the document to be analyzed.\n\n**Correct Answer:** B) The benchmark is a representation of the criteria for analysis, which in this case is the emotion classification, as proposed by Robert Plutchik (1980), and is used to extract the relative relevance of elements in the benchmark for the document to be analyzed.\n\n**Explanation:** The correct answer is B) because the benchmark is indeed a representation of the criteria for analysis, specifically the emotion classification, as proposed by Robert Plutchik (1980). The framework, on the other hand, is a computational version of the Framework Theory by Marvin Minsky (1974) that leverages Word2Vec approach to text vectorization. The benchmark is used to extract the relative relevance of elements in the benchmark for the document to be analyzed, which is a key aspect of the proposed method."}, "10": {"documentation": {"title": "Electron exchange and polarization effects on electron captures and\n  neutron emissions by nuclei in white dwarfs and neutron stars", "source": "Nicolas Chamel, Anthea Fantina", "docs_id": "2103.05947", "section": ["nucl-th", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electron exchange and polarization effects on electron captures and\n  neutron emissions by nuclei in white dwarfs and neutron stars. In dense stellar environments, nuclei may become unstable against electron captures and/or neutron emissions. These processes are of particular importance for determining the internal constitution of white-dwarf cores and neutron-star crusts. In this paper, the role of electron exchange and polarization effects is studied. In particular, the instability condition for the onset of electron captures and neutron emissions is extended so as to account for electron exchange and polarization. Moreover, general analytical expressions for the corresponding density and pressure are derived. The corrections to the electron-capture threshold in white-dwarf cores are found to be very small. Likewise, the neutron-drip density and pressure in the crusts of accreting and nonaccreting neutron stars are only slightly shifted. Depending on the nuclear mass model employed, electron polarization may change the composition of the crust of nonaccreting neutron stars. On the other hand, the current uncertainties in the masses of neutron-rich Kr and Sr isotopes are found to be more important than electron exchange and polarization effects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion regarding the role of electron exchange and polarization effects on electron captures and neutron emissions by nuclei in white dwarfs and neutron stars, according to the study mentioned in the provided documentation?\n\nA) Electron exchange and polarization effects significantly alter the composition of white-dwarf cores and neutron-star crusts.\nB) Electron exchange and polarization effects have a negligible impact on the internal constitution of white-dwarf cores and neutron-star crusts.\nC) Electron exchange and polarization effects are crucial in determining the neutron-drip density and pressure in the crusts of accreting and nonaccreting neutron stars.\nD) Electron exchange and polarization effects are more important than current uncertainties in the masses of neutron-rich Kr and Sr isotopes.\n\n**Correct Answer:** B) Electron exchange and polarization effects have a negligible impact on the internal constitution of white-dwarf cores and neutron-star crusts.\n\n**Explanation:** The study found that the corrections to the electron-capture threshold in white-dwarf cores and the neutron-drip density and pressure in the crusts of accreting and nonaccreting neutron stars are only slightly shifted due to electron exchange and polarization effects. This suggests that these effects have a negligible impact on the internal constitution of these objects."}, "11": {"documentation": {"title": "Cost-sensitive Multi-class AdaBoost for Understanding Driving Behavior\n  with Telematics", "source": "Banghee So and Jean-Philippe Boucher and Emiliano A. Valdez", "docs_id": "2007.03100", "section": ["stat.AP", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cost-sensitive Multi-class AdaBoost for Understanding Driving Behavior\n  with Telematics. Powered with telematics technology, insurers can now capture a wide range of data, such as distance traveled, how drivers brake, accelerate or make turns, and travel frequency each day of the week, to better decode driver's behavior. Such additional information helps insurers improve risk assessments for usage-based insurance (UBI), an increasingly popular industry innovation. In this article, we explore how to integrate telematics information to better predict claims frequency. For motor insurance during a policy year, we typically observe a large proportion of drivers with zero claims, a less proportion with exactly one claim, and far lesser with two or more claims. We introduce the use of a cost-sensitive multi-class adaptive boosting (AdaBoost) algorithm, which we call SAMME.C2, to handle such imbalances. To calibrate SAMME.C2 algorithm, we use empirical data collected from a telematics program in Canada and we find improved assessment of driving behavior with telematics relative to traditional risk variables. We demonstrate our algorithm can outperform other models that can handle class imbalances: SAMME, SAMME with SMOTE, RUSBoost, and SMOTEBoost. The sampled data on telematics were observations during 2013-2016 for which 50,301 are used for training and another 21,574 for testing. Broadly speaking, the additional information derived from vehicle telematics helps refine risk classification of drivers of UBI."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A telematics company is using a cost-sensitive multi-class adaptive boosting (AdaBoost) algorithm, SAMME.C2, to predict claims frequency for motor insurance. The company has collected empirical data from a telematics program in Canada, which includes observations from 2013-2016. The data consists of 50,301 samples for training and 21,574 samples for testing.\n\n**A)** The use of SAMME.C2 algorithm has improved the assessment of driving behavior with telematics relative to traditional risk variables, but it has not been shown to outperform other models that can handle class imbalances, such as SAMME, SAMME with SMOTE, RUSBoost, and SMOTEBoost.\n\n**B)** The telematics data has been used to refine risk classification of drivers of usage-based insurance (UBI), but the company has not reported any significant improvements in claims frequency prediction.\n\n**C)** The SAMME.C2 algorithm has been calibrated using the empirical data collected from the telematics program in Canada, and it has been shown to outperform other models that can handle class imbalances.\n\n**D)** The company has used a cost-sensitive multi-class AdaBoost algorithm, SAMME.C2, to predict claims frequency for motor insurance, but the algorithm has not been evaluated on a separate test dataset.\n\n**Correct Answer:** C) The SAMME.C2 algorithm has been calibrated using the empirical data collected from the telematics program in Canada, and it has been shown to outperform other models that can handle class imbalances.\n\n**Explanation:** The correct answer is C) because the documentation states that the SAMME.C2 algorithm has been calibrated using the empirical data collected from the telematics program in Canada, and it has been shown to outperform other models that can handle class imbalances. This is supported by the fact that the company has demonstrated improved assessment of driving behavior with telematics relative to traditional risk variables."}, "12": {"documentation": {"title": "How does geographical distance translate into genetic distance?", "source": "Ver\\'onica Mir\\'o Pina, Emmanuel Schertzer", "docs_id": "1703.00357", "section": ["q-bio.PE", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How does geographical distance translate into genetic distance?. Geographic structure can affect patterns of genetic differentiation and speciation rates. In this article, we investigate the dynamics of genetic distances in a geographically structured metapopulation. We model the metapopulation as a weighted directed graph, with d vertices corresponding to d subpopulations that evolve according to an individual based model. The dynamics of the genetic distances is then controlled by two types of transitions -mutation and migration events. We show that, under a rare mutation - rare migration regime, intra subpopulation diversity can be neglected and our model can be approximated by a population based model. We show that under a large population - large number of loci limit, the genetic distance between two subpopulations converges to a deterministic quantity that can asymptotically be expressed in terms of the hitting time between two random walks in the metapopulation graph. Our result shows that the genetic distance between two subpopulations does not only depend on the direct migration rates between them but on the whole metapopulation structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** How does the metapopulation structure influence the genetic distance between subpopulations, and what are the implications of this influence on the dynamics of genetic differentiation and speciation rates?\n\n**A)** The metapopulation structure has no effect on the genetic distance between subpopulations, as genetic drift and mutation are the primary drivers of genetic change.\n\n**B)** The genetic distance between subpopulations converges to a deterministic quantity that can be expressed in terms of the hitting time between two random walks in the metapopulation graph, but this convergence is only achieved under a rare mutation-rare migration regime.\n\n**C)** The metapopulation structure influences the genetic distance between subpopulations through the direct migration rates between them, but not through the whole metapopulation structure, as intra-subpopulation diversity can be neglected.\n\n**D)** The genetic distance between subpopulations is determined by the metapopulation structure, and under a large population-large number of loci limit regime, the genetic distance converges to a deterministic quantity that can be expressed in terms of the hitting time between two random walks in the metapopulation graph.\n\n**Correct Answer:** D) The genetic distance between subpopulations is determined by the metapopulation structure, and under a large population-large number of loci limit regime, the genetic distance converges to a deterministic quantity that can be expressed in terms of the hitting time between two random walks in the metapopulation graph.\n\n**Explanation:**\n\nThe correct answer, D, is supported by the article, which states that the genetic distance between two subpopulations does not only depend on the direct migration rates between them, but on the whole metapopulation structure. Additionally, the article shows that under a large population-large number of loci limit regime, the genetic distance between two subpopulations converges to a deterministic quantity that can be expressed in terms of the hitting time between two random walks in the metapopulation graph.\n\nCandidate A is incorrect because the article does not suggest that the metapopulation structure has no effect on the genetic distance between subpopulations.\n\nCandidate B is partially correct, as it mentions the rare mutation-rare migration regime, but it does not fully capture the relationship between the metapopulation structure and the genetic distance between subpopulations.\n\nCandidate C is incorrect because it suggests that intra-subpopulation diversity can be neglected, which is not supported by the article. The article shows that the genetic distance between subpopulations can be approximated by a population-based model, but this approximation is only valid under certain conditions."}, "13": {"documentation": {"title": "Dissecting the Colour-Magnitude Diagram: A Homogeneous Catalogue of\n  Stellar Populations in Globular Clusters", "source": "Nathan Leigh, Alison Sills, Christian Knigge", "docs_id": "1105.3192", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dissecting the Colour-Magnitude Diagram: A Homogeneous Catalogue of\n  Stellar Populations in Globular Clusters. We present a homogeneous catalogue for blue straggler, red giant branch, horizontal branch and main-sequence turn-off stars in a sample of 35 clusters taken from the ACS Survey for Globular Clusters. As a result of the superior photometry and relatively large field of view offered by the ACS data, this new catalogue is a significant improvement upon the one presented in Leigh, Sills & knigge (2007). Using our catalogue, we study and compare the radial distributions of the different stellar populations. We have confirmed our previous result (Knigge, Leigh & Sills 2009) that there is a clear, but sub-linear, correlation between the number of blue stragglers found in the cluster core and the total stellar mass contained within it. By considering a larger spatial extent than just the core, our results suggest that mass segregation is not the dominant effect contributing to the observed sub-linearity. We also investigate the radial distributions of the different stellar populations in our sample of clusters. Our results are consistent with a linear relationship between the number of stars in these populations and the total mass enclosed within the same radius. Therefore, we conclude that the cluster dynamics does not significantly affect the relative distributions of these populations in our sample."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Dissecting the Colour-Magnitude Diagram: A Homogeneous Catalogue of Stellar Populations in Globular Clusters\" investigate the radial distributions of blue straggler, red giant branch, horizontal branch, and main-sequence turn-off stars in a sample of 35 globular clusters. According to the study, what is the primary conclusion regarding the relationship between the number of blue stragglers and the total stellar mass contained within the cluster core?\n\nA) The number of blue stragglers is directly proportional to the total stellar mass contained within the cluster core.\nB) The number of blue stragglers is inversely proportional to the total stellar mass contained within the cluster core.\nC) There is a clear, but sub-linear, correlation between the number of blue stragglers and the total stellar mass contained within the cluster core.\nD) The number of blue stragglers is independent of the total stellar mass contained within the cluster core.\n\n**Correct Answer:** C) There is a clear, but sub-linear, correlation between the number of blue stragglers and the total stellar mass contained within the cluster core.\n\n**Explanation:** The study confirms a previous result that there is a clear, but sub-linear, correlation between the number of blue stragglers and the total stellar mass contained within the cluster core. This means that as the total stellar mass increases, the number of blue stragglers also increases, but the relationship is not perfectly linear. The correct answer reflects this finding."}, "14": {"documentation": {"title": "E-Learning and its Socioeconomics", "source": "Avni Singh", "docs_id": "2107.05041", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "E-Learning and its Socioeconomics. While controversial, e-learning has become an essential tool for all kinds of education: especially within the kindergarten-to-twelfth sector. However, pockets of this sector lack access, mainly economically underserved students. This paper explores the options available to underserved and aptly resourced members of the kindergarten-to-twelfth educational sector: a 250-million-person market, with only 9 million students enrolled in online education. The paper also provides a brief overview of the options and challenges of making e-learning available to everyone in the kindergarten-to-twelfth educational sector. To establish whether e-learning is beneficial, it also discusses the results of a survey conducted on students and educators who have experienced e-learning, with the results showing that it is beneficial, with a general trend of teachers showing more comfort with online learning than students. The paper utilizes primary and secondary resources for this purpose, with information both from the internet, and from surveys conducted within people from the system: parents, students, and teachers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary challenges in making e-learning available to economically underserved students in the kindergarten-to-twelfth educational sector, and how do the results of the survey conducted on students and educators suggest that e-learning can be beneficial for this population?\n\nA) The primary challenges are lack of access to technology and internet connectivity, and the results of the survey show that students are more comfortable with online learning than teachers.\n\nB) The primary challenges are the high cost of e-learning platforms and the need for specialized training for educators, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nC) The primary challenges are the lack of qualified educators and the need for more resources to support online learning, and the results of the survey show that students are more comfortable with online learning than teachers.\n\nD) The primary challenges are the lack of access to e-learning platforms and the need for more resources to support online learning, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nCorrect Answer: D) The primary challenges are the lack of access to e-learning platforms and the need for more resources to support online learning, and the results of the survey show that teachers are more comfortable with online learning than students.\n\nExplanation: The correct answer is D) because the documentation states that \"pockets of this sector lack access, mainly economically underserved students\" and that \"only 9 million students enrolled in online education\" suggests that access to e-learning platforms is a significant challenge. Additionally, the results of the survey show that teachers are more comfortable with online learning than students, which supports the idea that the primary challenges are related to access and resources."}, "15": {"documentation": {"title": "Testing of Advanced Technique For Linear Lattice and Closed Orbit\n  Correction By Modeling Its Application For IOTA Ring At Fermilab", "source": "A. Romanov (Fermilab)", "docs_id": "1805.03244", "section": ["physics.acc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Testing of Advanced Technique For Linear Lattice and Closed Orbit\n  Correction By Modeling Its Application For IOTA Ring At Fermilab. Many modern and most future accelerators rely on precise configuration of lattice and trajectory. The Integrable Optics Test Accelerator (IOTA) at Fermilab that is coming to final stages of construction will be used to test advanced approaches of control over particles dynamics. Various experiments planned at IOTA require high flexibility of lattice configuration as well as high precision of lattice and closed orbit control. Dense element placement does not allow to have ideal configuration of diagnostics and correctors for all planned experiments. To overcome this limitations advanced method of lattice analysis is proposed that can also be beneficial for other machines. Developed algorithm is based on LOCO approach, extended with various sets of other experimental data, such as dispersion, BPM-to-BPM phase advances, beam shape information from synchrotron light monitors, responses of closed orbit bumps to variations of focusing elements and other. Extensive modeling of corrections for a big number of random seed errors is used to illustrate benefits from developed approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Integrable Optics Test Accelerator (IOTA) at Fermilab is designed to test advanced techniques for controlling particle dynamics. What is the primary limitation of the current lattice configuration at IOTA, and how does the proposed advanced method of lattice analysis address this limitation?\n\n**A)** The primary limitation is the lack of precise control over the beam's trajectory, which is addressed by the proposed method's use of LOCO approach and additional experimental data.\n\n**B)** The primary limitation is the dense element placement, which restricts the flexibility of lattice configuration and diagnostics, and is addressed by the proposed method's use of advanced modeling and correction techniques.\n\n**C)** The primary limitation is the lack of high-precision control over the closed orbit, which is addressed by the proposed method's use of extensive modeling of corrections for random seed errors.\n\n**D)** The primary limitation is the lack of precise control over the beam's shape, which is addressed by the proposed method's use of dispersion, BPM-to-BPM phase advances, and beam shape information from synchrotron light monitors.\n\n**Correct Answer:** B) The primary limitation is the dense element placement, which restricts the flexibility of lattice configuration and diagnostics, and is addressed by the proposed method's use of advanced modeling and correction techniques.\n\n**Explanation:** The correct answer is B) because the documentation states that \"Dense element placement does not allow to have ideal configuration of diagnostics and correctors for all planned experiments.\" This limitation is addressed by the proposed advanced method of lattice analysis, which uses advanced modeling and correction techniques to overcome this limitation. The other options are incorrect because they do not accurately summarize the primary limitation and the proposed solution."}, "16": {"documentation": {"title": "Controlling volatility of wind-solar power", "source": "Hans Lustfeld", "docs_id": "2102.00587", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Controlling volatility of wind-solar power. The main advantage of wind and solar power plants is the power production free of CO2. Their main disadvantage is the volatility of the generated power. According to the estimates of H.-W. Sinn[1], suppressing this volatility requires pumped-storage plants with a huge capacity, several orders of magnitude larger than the present available capacity in Germany[2]. Sinn concluded that wind-solar power can be used only together with conventional power plants as backups. However, based on German power data[3] of 2019 we show that the required storage capacity can significantly be reduced, provided i) a surplus of wind-solar power plants is supplied, ii) smart meters are installed, iii) partly a different kind of wind turbines and solar panels are used in Germany. Our calculations suggest that all the electric energy, presently produced in Germany, can be obtained from wind-solar power alone. And our results let us predict that wind-solar power can be used to produce in addition the energy for transportation, warm water, space heating and in part for process heating, meaning an increase of the present electric energy production by a factor of about 5[1]. Of course, to put such a prediction on firm ground the present calculations have to be confirmed for a period of many years. And it should be kept in mind, that in any case a huge number of wind turbines and solar panels is required."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the study on controlling volatility of wind-solar power, what is the main advantage of wind and solar power plants, and what is the main disadvantage?\n\n**A)** Wind and solar power plants have a huge capacity and produce power free of CO2, but their main disadvantage is that they are not reliable.\n**B)** Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for long-term energy production.\n**C)** Wind and solar power plants have a huge capacity and produce power free of CO2, but their main disadvantage is that they are not economically viable.\n**D)** Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for energy production.\n\n**Correct Answer:** D) Wind and solar power plants produce power free of CO2, but their main disadvantage is that they are not suitable for energy production.\n\n**Explanation:** The correct answer is D) because the study states that the main advantage of wind and solar power plants is that they produce power free of CO2, and the main disadvantage is that they are not suitable for energy production due to their volatility. The other options are incorrect because they either misstate the main advantage or the main disadvantage of wind and solar power plants."}, "17": {"documentation": {"title": "Bonding nature and optical contrast of $TiTe_2$/$Sb_2Te_3$ phase-change\n  heterostructure", "source": "Xudong Wang, Yue Wu, Yuxing Zhou, Volker L. Deringer, Wei Zhang", "docs_id": "2103.13583", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bonding nature and optical contrast of $TiTe_2$/$Sb_2Te_3$ phase-change\n  heterostructure. Chalcogenide phase-change materials (PCMs) are regarded as the leading candidate for storage-class non-volatile memory and neuro-inspired computing. Recently, using the $TiTe_2$/$Sb_2Te_3$ material combination, a new framework - phase-change heterostructure (PCH), has been developed and proved to effectively suppress the noise and drift in electrical resistance upon memory programming, largely reducing the inter-device variability. However, the atomic-scale structural and chemical nature of PCH remains to be fully understood. In this work, we carry out thorough ab initio simulations to assess the bonding characteristics of the PCH. We show that the $TiTe_2$ crystalline nanolayers do not chemically interact with the surrounding $Sb_2Te_3$, and are stabilized by strong covalent and electrostatic Ti-Te interactions, which create a prohibitively high barrier for atomic migrations along the pulsing direction. We also find significant contrast in computed dielectric functions in the PCH, suggesting possible optical applications of this class of devices. With the more confined space and therefore constrained phase transition compared to traditional PCM devices, the recently introduced class of PCH-based devices may lead to improvements in phase-change photonic and optoelectronic applications with much lower stochasticity during programming."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the high barrier to atomic migrations along the pulsing direction in the phase-change heterostructure (PCH) of $TiTe_2$/$Sb_2Te_3$?\n\nA) The strong covalent and electrostatic Ti-Te interactions between the $TiTe_2$ crystalline nanolayers and the surrounding $Sb_2Te_3$.\nB) The significant contrast in computed dielectric functions in the PCH, which suggests possible optical applications of this class of devices.\nC) The confinement of the space within the PCH, which leads to a more constrained phase transition compared to traditional PCM devices.\nD) The fact that the $TiTe_2$ crystalline nanolayers do not chemically interact with the surrounding $Sb_2Te_3$.\n\nCorrect Answer: D) The fact that the $TiTe_2$ crystalline nanolayers do not chemically interact with the surrounding $Sb_2Te_3$.\n\nExplanation: The correct answer is D) because the documentation states that the $TiTe_2$ crystalline nanolayers \"do not chemically interact with the surrounding $Sb_2Te_3$\", which creates a prohibitively high barrier for atomic migrations along the pulsing direction. This is the primary reason for the high barrier to atomic migrations, making option D the correct answer. Options A, B, and C are incorrect because they do not directly address the reason for the high barrier to atomic migrations."}, "18": {"documentation": {"title": "Effect of splitting of the neutron and proton effective mass on nuclear\n  symmetry energy at finite temperature", "source": "Li Ou, Zhuxia Li, Yingxun Zhang, Min Liu", "docs_id": "1012.1932", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effect of splitting of the neutron and proton effective mass on nuclear\n  symmetry energy at finite temperature. We present the temperature and density dependence of symmetry energy for nuclear matter at finite temperature based on the approach of the thermodynamics with Skyrme energy density functional. We first classify the Skyrme interactions into 7 groups according to the range of neutron and proton effective mass in neutron matter limit(99.99 per cent neutron in the matter). We find that there is obvious correlation between the temperature dependence of the symmetry energy and the splitting of the neutron and proton effective mass. For some Skyrme interactions with $m^{*}_{n}>m^{*}_{p}$ and strong splitting of the neutron and proton effective mass in asymmetric nuclear matter, a transition of the temperature dependence of symmetry energy from decreasing with temperature at low densities to increasing with temperature at high densities appears. For other Skyrme interactions, we do not observe such phenomenon. Our study show that the symmetry energy in hot asymmetric matter not only depends on symmetry potential part but also on the splitting of the neutron and proton effective mass to a certain extent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main conclusion of the study on the effect of splitting of the neutron and proton effective mass on nuclear symmetry energy at finite temperature, as presented in the given Arxiv documentation?\n\nA) The symmetry energy in hot asymmetric matter only depends on the symmetry potential part.\nB) The symmetry energy in hot asymmetric matter depends on the splitting of the neutron and proton effective mass to a certain extent.\nC) The temperature dependence of symmetry energy is independent of the splitting of the neutron and proton effective mass.\nD) The study found no correlation between the temperature dependence of symmetry energy and the splitting of the neutron and proton effective mass.\n\n**Correct Answer:** B) The symmetry energy in hot asymmetric matter depends on the splitting of the neutron and proton effective mass to a certain extent.\n\n**Explanation:** The study found that the symmetry energy in hot asymmetric matter not only depends on the symmetry potential part but also on the splitting of the neutron and proton effective mass to a certain extent. This is evident from the fact that for some Skyrme interactions with $m^{*}_{n}>m^{*}_{p}$ and strong splitting of the neutron and proton effective mass, a transition of the temperature dependence of symmetry energy from decreasing with temperature at low densities to increasing with temperature at high densities appears."}, "19": {"documentation": {"title": "Forensic Similarity for Digital Images", "source": "Owen Mayer, Matthew C. Stamm", "docs_id": "1902.04684", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Forensic Similarity for Digital Images. In this paper we introduce a new digital image forensics approach called forensic similarity, which determines whether two image patches contain the same forensic trace or different forensic traces. One benefit of this approach is that prior knowledge, e.g. training samples, of a forensic trace are not required to make a forensic similarity decision on it in the future. To do this, we propose a two part deep-learning system composed of a CNN-based feature extractor and a three-layer neural network, called the similarity network. This system maps pairs of image patches to a score indicating whether they contain the same or different forensic traces. We evaluated system accuracy of determining whether two image patches were 1) captured by the same or different camera model, 2) manipulated by the same or different editing operation, and 3) manipulated by the same or different manipulation parameter, given a particular editing operation. Experiments demonstrate applicability to a variety of forensic traces, and importantly show efficacy on \"unknown\" forensic traces that were not used to train the system. Experiments also show that the proposed system significantly improves upon prior art, reducing error rates by more than half. Furthermore, we demonstrated the utility of the forensic similarity approach in two practical applications: forgery detection and localization, and database consistency verification."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary benefit of the proposed forensic similarity approach in the paper \"Forensic Similarity for Digital Images\", and how does it differ from prior art in terms of required training samples?\n\nA) The primary benefit is that it requires prior knowledge of a forensic trace to make a decision, and it improves upon prior art by requiring more extensive training samples.\nB) The primary benefit is that it does not require prior knowledge of a forensic trace to make a decision, and it improves upon prior art by reducing the need for extensive training samples.\nC) The primary benefit is that it improves upon prior art by requiring more extensive training samples, but it does not require prior knowledge of a forensic trace to make a decision.\nD) The primary benefit is that it requires prior knowledge of a forensic trace to make a decision, but it improves upon prior art by reducing the need for extensive training samples.\n\nCorrect Answer: B) The primary benefit is that it does not require prior knowledge of a forensic trace to make a decision, and it improves upon prior art by reducing the error rates by more than half.\n\nExplanation: The correct answer is B) because the paper states that one benefit of the proposed approach is that prior knowledge, e.g. training samples, of a forensic trace are not required to make a forensic similarity decision on it in the future. This means that the approach does not require prior knowledge of a forensic trace to make a decision, which is the primary benefit. Additionally, the paper states that the proposed system significantly improves upon prior art, reducing error rates by more than half, which is also a key point."}, "20": {"documentation": {"title": "Drewnowski's index to measure lifespan variation: Revisiting the Gini\n  coefficient of the life table", "source": "Jos\\'e Manuel Aburto, Ugofilippo Basellini, Annette Baudisch,\n  Francisco Villavicencio", "docs_id": "2111.11256", "section": ["q-bio.PE", "econ.GN", "q-fin.EC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Drewnowski's index to measure lifespan variation: Revisiting the Gini\n  coefficient of the life table. The Gini coefficient of the life table is a concentration index that provides information on lifespan variation. Originally proposed by economists to measure income and wealth inequalities, it has been widely used in population studies to investigate variation in ages at death. We focus on a complementary indicator, Drewnowski's index, which is as a measure of equality. We study its mathematical properties and analyze how changes over time relate to changes in life expectancy. Further, we identify the threshold age below which mortality improvements are translated into decreasing lifespan variation and above which these improvements translate into increasing lifespan inequality. We illustrate our theoretical findings simulating scenarios of mortality improvement in the Gompertz model. Our experiments demonstrate how Drewnowski's index can serve as an indicator of the shape of mortality patterns. These properties, along with our analytical findings, support studying lifespan variation alongside life expectancy trends in multiple species."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Revisiting the Gini coefficient of the life table, Drewnowski's index is a measure of equality that complements the Gini coefficient. According to the text, what is the threshold age below which mortality improvements are translated into decreasing lifespan variation, and above which these improvements translate into increasing lifespan inequality?\n\n**A)** The threshold age is below 50 years, and above 70 years.\n**B)** The threshold age is below 30 years, and above 60 years.\n**C)** The threshold age is below 40 years, and above 80 years.\n**D)** The threshold age is below 20 years, and above 90 years.\n\n**Correct Answer:** B) The threshold age is below 30 years, and above 60 years.\n\n**Explanation:** According to the text, the threshold age below which mortality improvements are translated into decreasing lifespan variation is below 30 years, and above which these improvements translate into increasing lifespan inequality is above 60 years. This is because the text states that \"Further, we identify the threshold age below which mortality improvements are translated into decreasing lifespan variation and above which these improvements translate into increasing lifespan inequality.\""}, "21": {"documentation": {"title": "Radiative corrections to decay amplitudes in lattice QCD", "source": "Davide Giusti, Vittorio Lubicz, Guido Martinelli, Christopher\n  Sachrajda, Francesco Sanfilippo, Silvano Simula and Nazario Tantalo", "docs_id": "1811.06364", "section": ["hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Radiative corrections to decay amplitudes in lattice QCD. The precision of lattice QCD computations of many quantities has reached such a precision that isospin-breaking corrections, including electromagnetism, must be included if further progress is to be made in extracting fundamental information, such as the values of Cabibbo-Kobayashi-Maskawa matrix elements, from experimental measurements. We discuss the framework for including radiative corrections in leptonic and semileptonic decays of hadrons, including the treatment of infrared divergences. We briefly review isospin breaking in leptonic decays and present the first numerical results for the ratio $\\Gamma(K_{\\mu2})/\\Gamma(\\pi_{\\mu2})$ in which these corrections have been included. We also discuss the additional theoretical issues which arise when including electromagnetic corrections to semileptonic decays, such as $K_{\\ell3}$ decays. The separate definition of strong isospin-breaking effects and those due to electromagnetism requires a convention. We define and advocate conventions based on hadronic schemes, in which a chosen set of hadronic quantities, hadronic masses for example, are set equal in QCD and in QCD+QED. This is in contrast with schemes which have been largely used to date, in which the renormalised $\\alpha_s(\\mu)$ and quark masses are set equal in QCD and in QCD+QED in some renormalisation scheme and at some scale $\\mu$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the key challenge in including radiative corrections in lattice QCD computations, and how do the authors address this challenge in their framework?\n\nA) The challenge is to reconcile the precision of lattice QCD computations with the need to include isospin-breaking corrections, and the authors address this by using a renormalisation scheme that sets the renormalised \u03b1s(\u03bc) and quark masses equal in QCD and QCD+QED.\n\nB) The challenge is to treat infrared divergences in leptonic and semileptonic decays, and the authors address this by defining a convention based on hadronic schemes, where a chosen set of hadronic quantities, such as hadronic masses, are set equal in QCD and QCD+QED.\n\nC) The challenge is to distinguish between strong isospin-breaking effects and those due to electromagnetism, and the authors address this by using a separate definition of strong isospin-breaking effects and those due to electromagnetism.\n\nD) The challenge is to include electromagnetic corrections to semileptonic decays, such as K\u21133 decays, and the authors address this by using a framework that includes the treatment of infrared divergences and a convention based on hadronic schemes.\n\nCorrect Answer: B) The challenge is to treat infrared divergences in leptonic and semileptonic decays, and the authors address this by defining a convention based on hadronic schemes, where a chosen set of hadronic quantities, such as hadronic masses, are set equal in QCD and QCD+QED.\n\nExplanation: The correct answer is B) because the authors explicitly state that the challenge is to treat infrared divergences in leptonic and semileptonic decays, and they address this by defining a convention based on hadronic schemes. This convention involves setting a chosen set of hadronic quantities, such as hadronic masses, equal in QCD and QCD+QED. The other options are incorrect because they either do not accurately summarize the challenge or the solution, or they are incomplete. Option A is incorrect because it mentions the renormalisation scheme, but does not address the challenge of treating infrared divergences. Option C is incorrect because it mentions the distinction between strong isospin-breaking effects and those due to electromagnetism, but does not address the challenge of treating infrared divergences. Option D is incorrect because it mentions the inclusion of electromagnetic corrections to semileptonic decays, but does not address the challenge of treating infrared divergences."}, "22": {"documentation": {"title": "Optimal energy-conserving discontinuous Galerkin methods for linear\n  symmetric hyperbolic systems", "source": "Guosheng Fu, Chi-Wang Shu", "docs_id": "1804.10307", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal energy-conserving discontinuous Galerkin methods for linear\n  symmetric hyperbolic systems. We propose energy-conserving discontinuous Galerkin (DG) methods for symmetric linear hyperbolic systems on general unstructured meshes. Optimal a priori error estimates of order $k+1$ are obtained for the semi-discrete scheme in one dimension, and in multi-dimensions on Cartesian meshes when tensor-product polynomials of degree $k$ are used. A high-order energy-conserving Lax-Wendroff time discretization is also presented. Extensive numerical results in one dimension, and two dimensions on both rectangular and triangular meshes are presented to support the theoretical findings and to assess the new methods. One particular method (with the doubling of unknowns) is found to be optimally convergent on triangular meshes for all the examples considered in this paper. The method is also compared with the classical (dissipative) upwinding DG method and (conservative) DG method with a central flux. It is numerically observed for the new method to have a superior performance for long-time simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed energy-conserving discontinuous Galerkin method over the classical dissipative upwinding DG method, as demonstrated in the numerical results?\n\nA) The new method is more computationally expensive due to the increased number of unknowns.\nB) The new method is more stable and accurate for long-time simulations, particularly on triangular meshes.\nC) The new method is more suitable for problems with high-frequency waves, while the classical method is better for low-frequency waves.\nD) The new method is more difficult to implement due to the need for tensor-product polynomials of degree k.\n\nCorrect Answer: B) The new method is more stable and accurate for long-time simulations, particularly on triangular meshes.\n\nExplanation: The question requires the test-taker to analyze the advantages of the proposed energy-conserving discontinuous Galerkin method over the classical dissipative upwinding DG method. The correct answer, B, is supported by the numerical results presented in the paper, which show that the new method is superior for long-time simulations on triangular meshes. The other options are incorrect because they do not accurately reflect the advantages of the new method. Option A is incorrect because the increased number of unknowns is a characteristic of the new method, but not its primary advantage. Option C is incorrect because the new method is not specifically designed for high-frequency waves, and option D is incorrect because the implementation of the new method is not necessarily more difficult than the classical method."}, "23": {"documentation": {"title": "Micro-level dynamics in hidden action situations with limited\n  information", "source": "Stephan Leitner and Friederike Wall", "docs_id": "2107.06002", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Micro-level dynamics in hidden action situations with limited\n  information. The hidden-action model provides an optimal sharing rule for situations in which a principal assigns a task to an agent who makes an effort to carry out the task assigned to him. However, the principal can only observe the task outcome but not the agent's actual action. The hidden-action model builds on somewhat idealized assumptions about the principal's and the agent's capabilities related to information access. We propose an agent-based model that relaxes some of these assumptions. Our analysis lays particular focus on the micro-level dynamics triggered by limited information access. For the principal's sphere, we identify the so-called Sisyphus effect that explains why the optimal sharing rule can generally not be achieved if the information is limited, and we identify factors that moderate this effect. In addition, we analyze the behavioral dynamics in the agent's sphere. We show that the agent might make even more of an effort than optimal under unlimited information, which we refer to as excess effort. Interestingly, the principal can control the probability of making an excess effort via the incentive mechanism. However, how much excess effort the agent finally makes is out of the principal's direct control."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the hidden-action model, what is the primary effect of limited information access on the principal's ability to achieve the optimal sharing rule, and how does this effect manifest in the agent's behavior?\n\n**A)** The limited information access leads to a decrease in the agent's effort, resulting in a suboptimal sharing rule. The agent's behavior is characterized by a decrease in excess effort.\n\n**B)** The limited information access leads to an increase in the agent's effort, resulting in a suboptimal sharing rule. The agent's behavior is characterized by an increase in excess effort.\n\n**C)** The limited information access leads to a decrease in the agent's effort, resulting in a suboptimal sharing rule. The agent's behavior is characterized by a decrease in excess effort, and the principal can control the probability of this decrease via the incentive mechanism.\n\n**D)** The limited information access leads to an increase in the agent's effort, resulting in an optimal sharing rule. The agent's behavior is characterized by a decrease in excess effort, and the principal can control the probability of this decrease via the incentive mechanism.\n\n**Correct Answer:** C) The limited information access leads to a decrease in the agent's effort, resulting in a suboptimal sharing rule. The agent's behavior is characterized by a decrease in excess effort, and the principal can control the probability of this decrease via the incentive mechanism.\n\n**Explanation:** The correct answer is C) because the hidden-action model proposes that limited information access leads to the Sisyphus effect, which explains why the optimal sharing rule cannot be achieved. This effect results in a decrease in the agent's effort, leading to a suboptimal sharing rule. Additionally, the model shows that the agent may make even more of an effort than optimal under unlimited information, referred to as excess effort. The principal can control the probability of this excess effort via the incentive mechanism, but the actual amount of excess effort made by the agent is out of the principal's direct control."}, "24": {"documentation": {"title": "Pair hopping in systems of strongly interacting hard-core bosons", "source": "Alvin J.R. Heng, Wenan Guo, Anders W. Sandvik and Pinaki Sengupta", "docs_id": "1903.04103", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pair hopping in systems of strongly interacting hard-core bosons. We have used the Stochastic Series Expansion quantum Monte Carlo method to study interacting hard-core bosons on the square lattice, with pair-hopping processes supplementing the standard single-particle hopping. Such pair hopping arises in effective models for frustrated quantum magnets. Our goal is to investigate the effects of the pair hopping process on the commonly observed superfluid, insulating (Mott), and super-solid ground-state phases in the standard hard-core boson model with various interaction terms. The model is specifically motivated by the observation of finite dispersion of 2-magnon bound states in neutron diffraction experiments SrCu$_2$(BO$_3$)$_2$. Our results show that the pair hopping has different effects on Mott phases at different filling fractions, \"melting\" them at different critical pair-hopping amplitudes. Thus, it appears that pair hopping may have an important role in determining which out of a potentially large number of Mott phases (stabilized by details of the charge-diagonal interaction terms) actually survive the totality of quantum fluctuations present."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary effect of pair hopping on the Mott phases in the standard hard-core boson model, according to the study presented in the Arxiv documentation?\n\nA) Pair hopping enhances the stability of the Mott phases at all filling fractions.\nB) Pair hopping has no effect on the Mott phases, as it only affects the single-particle hopping process.\nC) Pair hopping \"melts\" the Mott phases at different critical pair-hopping amplitudes, depending on the filling fraction.\nD) Pair hopping only stabilizes the Mott phases at specific filling fractions, while destabilizing them at others.\n\nCorrect Answer: C) Pair hopping \"melts\" the Mott phases at different critical pair-hopping amplitudes, depending on the filling fraction.\n\nExplanation: The study suggests that pair hopping has different effects on the Mott phases at different filling fractions, and that it can \"melt\" these phases at different critical pair-hopping amplitudes. This implies that the correct answer is C, which accurately reflects the findings of the study. The other options are incorrect because they either overstate or misrepresent the effect of pair hopping on the Mott phases."}, "25": {"documentation": {"title": "Interplay between Secondary and Tertiary Structure Formation in Protein\n  Folding Cooperativity", "source": "Tristan Bereau, Michael Bachmann, and Markus Deserno", "docs_id": "1107.0313", "section": ["q-bio.BM", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interplay between Secondary and Tertiary Structure Formation in Protein\n  Folding Cooperativity. Protein folding cooperativity is defined by the nature of the finite-size thermodynamic transition exhibited upon folding: two-state transitions show a free energy barrier between the folded and unfolded ensembles, while downhill folding is barrierless. A microcanonical analysis, where the energy is the natural variable, has shown better suited to unambiguously characterize the nature of the transition compared to its canonical counterpart. Replica exchange molecular dynamics simulations of a high resolution coarse-grained model allow for the accurate evaluation of the density of states, in order to extract precise thermodynamic information, and measure its impact on structural features. The method is applied to three helical peptides: a short helix shows sharp features of a two-state folder, while a longer helix and a three-helix bundle exhibit downhill and two-state transitions, respectively. Extending the results of lattice simulations and theoretical models, we find that it is the interplay between secondary structure and the loss of non-native tertiary contacts which determines the nature of the transition."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary factor that determines the nature of the thermodynamic transition in protein folding cooperativity, according to the study on Interplay between Secondary and Tertiary Structure Formation in Protein Folding Cooperativity?\n\n**A)** The presence of non-native tertiary contacts\n**B)** The loss of secondary structure elements\n**C)** The interplay between secondary structure and the loss of non-native tertiary contacts\n**D)** The length of the protein sequence\n\n**Correct Answer:** C) The interplay between secondary structure and the loss of non-native tertiary contacts\n\n**Explanation:** The study suggests that the interplay between secondary structure and the loss of non-native tertiary contacts is the primary factor that determines the nature of the thermodynamic transition in protein folding cooperativity. This is evident from the statement \"Extending the results of lattice simulations and theoretical models, we find that it is the interplay between secondary structure and the loss of non-native tertiary contacts which determines the nature of the transition.\" This indicates that the correct answer is option C, which highlights the interplay between these two factors. Options A and B are incorrect because they only consider one aspect of the transition, while option D is unrelated to the study's findings."}, "26": {"documentation": {"title": "Job market effects of COVID-19 on urban Ukrainian households", "source": "Tymofii Brik and Maksym Obrizan", "docs_id": "2007.15704", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Job market effects of COVID-19 on urban Ukrainian households. The employment status of billions of people has been affected by the COVID epidemic around the Globe. New evidence is needed on how to mitigate the job market crisis, but there exists only a handful of studies mostly focusing on developed countries. We fill in this gap in the literature by using novel data from Ukraine, a transition country in Eastern Europe, which enacted strict quarantine policies early on. We model four binary outcomes to identify respondents (i) who are not working during quarantine, (ii) those who are more likely to work from home, (iii) respondents who are afraid of losing a job, and, finally, (iv) survey participants who have savings for 1 month or less if quarantine is further extended. Our findings suggest that respondents employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine. Females, better educated respondents, and those who lived in Kyiv were more likely to work remotely. Working in the public sector also made people more confident about their future employment perspectives. Although our findings are limited to urban households only, they provide important early evidence on the correlates of job market outcomes, expectations, and financial security, indicating potential deterioration of socio-economic inequalities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the job market effects of COVID-19 on urban Ukrainian households. What are the key factors that distinguish respondents who secured their jobs during the quarantine from those who did not?\n\n**A)** Respondents who were employed in the private sector and had lower levels of education were more likely to secure their jobs during the quarantine.\n\n**B)** Respondents who were employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine.\n\n**C)** Respondents who were employed in the service sector and had higher levels of education were more likely to secure their jobs during the quarantine.\n\n**D)** Respondents who were employed in the manufacturing sector and had lower levels of education were more likely to secure their jobs during the quarantine.\n\n**Correct Answer:** B) Respondents who were employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine.\n\n**Explanation:** According to the study, respondents employed in public administration, programming and IT, as well as highly qualified specialists, were more likely to secure their jobs during the quarantine. This suggests that certain types of jobs and qualifications provided greater job security during this period."}, "27": {"documentation": {"title": "An unbiased ALMA spectral survey of the LkCa 15 and MWC 480\n  protoplanetary disks", "source": "Ryan A. Loomis, Karin I. \u007f\\\"Oberg, Sean M. Andrews, Edwin Bergin,\n  Jennifer Bergner, Geoffrey A. Blake, L. Ilsedore Cleeves, Ian Czekala, Jane\n  Huang, Romane Le Gal, Francois Menard, Jamila Pegues, Chunhua Qi, Catherine\n  Walsh, Jonathan P. Williams, and David J. Wilner", "docs_id": "2006.16187", "section": ["astro-ph.SR", "astro-ph.EP", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An unbiased ALMA spectral survey of the LkCa 15 and MWC 480\n  protoplanetary disks. The volatile contents of protoplanetary disks both set the potential for planetary chemistry and provide valuable probes of defining disk system characteristics such as stellar mass, gas mass, ionization, and temperature structure. Current disk molecular inventories are fragmented, however, giving an incomplete picture: unbiased spectral line surveys are needed to assess the volatile content. We present here an overview of such a survey of the protoplanetary disks around the Herbig Ae star MWC 480 and the T Tauri star LkCa 15 in ALMA Band 7, spanning $\\sim$36 GHz from 275--317 GHz and representing an order of magnitude increase in sensitivity over previous single-dish surveys. We detect 14 molecular species (including isotopologues), with 5 species (C$^{34}$S, $^{13}$CS, H$_{2}$CS, DNC, and C$_2$D) detected for the first time in protoplanetary disks. Significant differences are observed in the molecular inventories of MWC~480 and LkCa~15, and we discuss how these results may be interpreted in light of the different physical conditions of these two disk systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the authors' unbiased ALMA spectral survey of the LkCa 15 and MWC 480 protoplanetary disks, and how does this survey contribute to our understanding of protoplanetary disk system characteristics?\n\n**A)** The survey aims to detect the presence of water vapor in the disks, which is essential for planetary formation. However, this is not the primary motivation, as the survey detects a broader range of molecular species.\n\n**B)** The survey seeks to investigate the effects of stellar mass on the molecular inventories of protoplanetary disks. While this is a related aspect, it is not the primary motivation.\n\n**C)** The survey aims to assess the volatile content of protoplanetary disks, which is crucial for understanding planetary chemistry and disk system characteristics such as gas mass, ionization, and temperature structure. This is the correct answer, as the survey's primary motivation is to provide a more comprehensive picture of the volatile content of protoplanetary disks.\n\n**D)** The survey is designed to detect the presence of complex organic molecules in the disks, which is essential for understanding the origins of life. While this is a related aspect, it is not the primary motivation.\n\n**Correct Answer:** C) The survey aims to assess the volatile content of protoplanetary disks, which is crucial for understanding planetary chemistry and disk system characteristics such as gas mass, ionization, and temperature structure."}, "28": {"documentation": {"title": "Rate-Optimal Cluster-Randomized Designs for Spatial Interference", "source": "Michael P. Leung", "docs_id": "2111.04219", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rate-Optimal Cluster-Randomized Designs for Spatial Interference. We consider a potential outcomes model in which interference may be present between any two units but the extent of interference diminishes with spatial distance. The causal estimand is the global average treatment effect, which compares counterfactual outcomes when all units are treated to outcomes when none are. We study a class of designs in which space is partitioned into clusters that are randomized into treatment and control. For each design, we estimate the treatment effect using a Horovitz-Thompson estimator that compares the average outcomes of units with all neighbors treated to units with no neighbors treated, where the neighborhood radius is of the same order as the cluster size dictated by the design. We derive the estimator's rate of convergence as a function of the design and degree of interference and use this to obtain estimator-design pairs in this class that achieve near-optimal rates of convergence under relatively minimal assumptions on interference. We prove that the estimators are asymptotically normal and provide a variance estimator. Finally, we discuss practical implementation of the designs by partitioning space using clustering algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a spatial interference model where the extent of interference diminishes with spatial distance. Suppose we have a cluster-randomized design with a neighborhood radius of order 1, and we want to estimate the global average treatment effect using a Horovitz-Thompson estimator. What is the rate of convergence of the estimator under the following assumptions:\n\n* The design is a balanced incomplete block (BIB) design with 10 clusters, each with 5 units.\n* The degree of interference is moderate, with an average correlation coefficient of 0.5 between units.\n* The treatment effect is non-zero and non-constant.\n\n**A)** The rate of convergence is O(1/n), where n is the number of clusters.\n**B)** The rate of convergence is O(1/n^2), where n is the number of clusters.\n**C)** The rate of convergence is O(1/n^3), where n is the number of clusters.\n**D)** The rate of convergence is O(1/n^4), where n is the number of clusters.\n\n**Correct Answer:** C) The rate of convergence is O(1/n^3), where n is the number of clusters.\n\n**Explanation:** The Horovitz-Thompson estimator has a rate of convergence of O(1/n^3) under the given assumptions, as shown in the paper. This is because the estimator is based on the average outcomes of units with all neighbors treated, and the degree of interference diminishes with spatial distance. The balanced incomplete block design with 10 clusters and 5 units each provides a sufficient sample size to achieve this rate of convergence. The moderate degree of interference with an average correlation coefficient of 0.5 between units also supports this rate of convergence."}, "29": {"documentation": {"title": "Evolutionary Rotation in Switching Incentive Zero-Sum Games", "source": "Zhijian Wang and Bin Xu", "docs_id": "1203.2591", "section": ["stat.ME", "math.ST", "nlin.CD", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolutionary Rotation in Switching Incentive Zero-Sum Games. In a laboratory experiment, round by round, individual interactions should lead to the social evolutionary rotation in population strategy state space. Successive switching the incentive parameter should lead to successive change of the rotation ---- both of its direction and its strength. In data from a switching payoff matrix experiment of extended 2x2 games (Binmore, Swierzbinski and Proulx, 2001 [1]), we find the changing of the social evolutionary rotation can be distinguished quantitatively. The evolutionary rotation can be captured by evolutionary dynamics. With eigenvalue from the Jacobian of a constrained replicator dynamics model, an interpretation for observed rotation strength is given. In addition, equality-of-populations rank test shows that relative response coefficient of a group could persist cross the switching parameter games. The data has successively been used to support Von Neumann's minimax theory. Using the old data, with observed evolutionary rotation, this report provides a new insight into evolutionary game theory and experimental social dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of evolutionary game theory, what is the primary mechanism by which the social evolutionary rotation in population strategy state space is affected by successive changes in the incentive parameter in switching incentive zero-sum games?\n\n**A)** The evolutionary rotation is directly proportional to the magnitude of the incentive parameter changes.\n**B)** The evolutionary rotation is influenced by the eigenvalues of the Jacobian of the constrained replicator dynamics model, which capture the rotation's strength and direction.\n**C)** The evolutionary rotation is solely determined by the relative response coefficient of a group, which persists across different switching parameter games.\n**D)** The evolutionary rotation is a result of the equality-of-populations rank test, which measures the relative performance of different population strategies.\n\n**Correct Answer:** B) The evolutionary rotation is influenced by the eigenvalues of the Jacobian of the constrained replicator dynamics model, which capture the rotation's strength and direction.\n\n**Explanation:** The correct answer is based on the documentation, which states that \"With eigenvalue from the Jacobian of a constrained replicator dynamics model, an interpretation for observed rotation strength is given.\" This indicates that the eigenvalues of the Jacobian of the constrained replicator dynamics model are used to capture the strength and direction of the evolutionary rotation. The other options are incorrect because they either oversimplify the relationship between the incentive parameter changes and the evolutionary rotation (A), focus on the relative response coefficient (C), or incorrectly attribute the evolutionary rotation to the equality-of-populations rank test (D)."}, "30": {"documentation": {"title": "Market Fragility, Systemic Risk, and Ricci Curvature", "source": "Romeil Sandhu, Tryphon Georgiou, Allen Tannenbaum", "docs_id": "1505.05182", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Market Fragility, Systemic Risk, and Ricci Curvature. Measuring systemic risk or fragility of financial systems is a ubiquitous task of fundamental importance in analyzing market efficiency, portfolio allocation, and containment of financial contagions. Recent attempts have shown that representing such systems as a weighted graph characterizing the complex web of interacting agents over some information flow (e.g., debt, stock returns, shareholder ownership) may provide certain keen insights. Here, we show that fragility, or the ability of system to be prone to failures in the face of random perturbations, is negatively correlated with geometric notion of Ricci curvature. The key ingredient relating fragility and curvature is entropy. As a proof of concept, we examine returns from a set of stocks comprising the S\\&P 500 over a 15 year span to show that financial crashes are more robust compared to normal \"business as usual\" fragile market behavior - i.e., Ricci curvature is a \"crash hallmark.\" Perhaps more importantly, this work lays the foundation of understanding of how to design systems and policy regulations in a manner that can combat financial instabilities exposed during the 2007-2008 crisis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key relationship between fragility, Ricci curvature, and entropy in the context of financial systems, as proposed in the paper \"Market Fragility, Systemic Risk, and Ricci Curvature\"?\n\nA) Fragility is directly proportional to Ricci curvature, and entropy is a measure of the system's ability to absorb shocks.\nB) Fragility is negatively correlated with Ricci curvature, and entropy is a key ingredient in understanding the relationship between the two.\nC) Ricci curvature is a measure of the system's fragility, and entropy is a proxy for the system's ability to withstand random perturbations.\nD) Entropy is a measure of the system's Ricci curvature, and fragility is a consequence of the system's ability to absorb shocks.\n\n**Correct Answer:** B) Fragility is negatively correlated with Ricci curvature, and entropy is a key ingredient in understanding the relationship between the two.\n\n**Explanation:**\n\nThe correct answer is B) Fragility is negatively correlated with Ricci curvature, and entropy is a key ingredient in understanding the relationship between the two. This is because the paper proposes that fragility is negatively correlated with Ricci curvature, and entropy plays a crucial role in understanding this relationship. The authors suggest that entropy is a key ingredient in relating fragility and curvature, and that it provides a way to quantify the system's ability to withstand random perturbations.\n\nThe incorrect answers are:\n\nA) Fragility is directly proportional to Ricci curvature, and entropy is a measure of the system's ability to absorb shocks. This is the opposite of the correct relationship, and entropy is not a measure of the system's ability to absorb shocks.\n\nC) Ricci curvature is a measure of the system's fragility, and entropy is a proxy for the system's ability to withstand random perturbations. This is not accurate, as Ricci curvature is not a direct measure of fragility, and entropy is not a proxy for the system's ability to withstand random perturbations.\n\nD) Entropy is a measure of the system's Ricci curvature, and fragility is a consequence of the system's ability to absorb shocks. This is also incorrect, as entropy is not a measure of the system's Ricci curvature, and fragility is not a consequence of the system's ability to absorb shocks."}, "31": {"documentation": {"title": "(Sm,Zr)Fe$_{12-x}$M$_x$ (M=Zr,Ti,Co) for permanent-magnet applications:\n  Ab initio material design integrated with experimental characterization", "source": "Munehisa Matsumoto, Takafumi Hawai, Kanta Ono", "docs_id": "1911.02348", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "(Sm,Zr)Fe$_{12-x}$M$_x$ (M=Zr,Ti,Co) for permanent-magnet applications:\n  Ab initio material design integrated with experimental characterization. In rare-earth permanent magnets (REPM's), trade-off's between intrinsic magnetic properties are often encountered. A recent example is SmFe$_{12}$ where excellent magnetic properties can be achieved at the sacrifice of bulk structure stability. Bulk structure stability is sustained by the presence of the third substitute element as is the case with SmFe$_{11}$Ti, where Ti degrades magnetic properties. It is now in high demand to find out with which chemical composition a good compromise in the trade-off between structure stability and strong ferromagnetism is reached. We inspect the effects of representative substitute elements, Zr, Ti, and Co in SmFe$_{12}$ by combining ab initio data with experimental data from neutron diffraction. The trend in the intrinsic properties with respect to the concentration of substitute elements are monitored and a systematic way to search the best compromise is constructed. A certain minimum amount of Ti is identified with respect to the added amount of Co and Zr. It is found that Zr brings about a positive effect on magnetization, in line with recent experimental developments, and we argue that this can be understood as an effective doping of extra electrons."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary effect of adding Zr to the SmFe12 alloy, as suggested by the ab initio material design and experimental characterization?\n\nA){Zr degrades the magnetic properties of the alloy}\nB){Zr brings about a positive effect on magnetization, similar to recent experimental developments}\nC){Zr has no significant effect on the magnetic properties of the alloy}\nD){Zr is necessary to achieve bulk structure stability in the alloy}\n\nCorrect Answer: B){Zr brings about a positive effect on magnetization, similar to recent experimental developments}\n\nExplanation: The correct answer is B) because the text states that \"Zr brings about a positive effect on magnetization, in line with recent experimental developments\". This suggests that the addition of Zr to the SmFe12 alloy has a beneficial effect on the magnetic properties of the material. The other options are incorrect because they do not accurately reflect the information provided in the text. Option A is incorrect because the text states that Zr has a positive effect on magnetization, not a negative one. Option C is incorrect because the text suggests that Zr has a significant effect on the magnetic properties of the alloy. Option D is incorrect because the text states that Zr is not necessary to achieve bulk structure stability, but rather that it brings about a positive effect on magnetization."}, "32": {"documentation": {"title": "Delocalized SPM rogue waves in normal dispersion cascaded supercontinuum\n  generation", "source": "Rasmus Eilk{\\oe}r Hansen, Rasmus Dybbro Engelsholm, Christian\n  Rosenberg Petersen, and Ole Bang", "docs_id": "2007.05909", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Delocalized SPM rogue waves in normal dispersion cascaded supercontinuum\n  generation. In the numerical modelling of cascaded mid-infrared (IR) supercontinuum generation (SCG) we have studied how an ensemble of spectrally and temporally distributed solitons from the long-wavelength part of an SC evolves and interacts when coupled into the normal dispersion regime of a highly nonlinear chalcogenide fiber. This has revealed a novel fundamental phenomenon - the generation of a temporally and spectrally delocalized high energy rogue wave in the normal dispersion regime in the form of a strongly self-phase-modulation (SPM) broadened pulse. Along the local SPM shape the rogue wave is localized both temporally and spectrally. We demonstrate that this novel form of rogue wave is generated by inter-pulse Raman amplification between the SPM lobes of the many pulses causing the initially most delayed pulse to swallow the energy of all the other pulses. We further demonstrate that this novel type of rogue wave generation is a key effect in efficient long-wavelength mid-IR SCG based on the cascading of SC spectra and demonstrate how the mid-IR SC spectrum can be shaped by manipulating the rogue wave."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which a temporally and spectrally delocalized high-energy rogue wave is generated in the normal dispersion regime of a highly nonlinear chalcogenide fiber during cascaded mid-infrared supercontinuum generation?\n\nA) Inter-pulse Raman amplification between the SPM lobes of the many pulses causing the initially most delayed pulse to swallow the energy of all the other pulses.\nB) Nonlinear optical effects due to the interaction between the solitons and the fiber's material properties.\nC) Stimulated emission of photons from the fiber's core, leading to the formation of a coherent beam.\nD) Thermal effects caused by the high-intensity pulses, resulting in a broadening of the pulse spectrum.\n\nCorrect Answer: A) Inter-pulse Raman amplification between the SPM lobes of the many pulses causing the initially most delayed pulse to swallow the energy of all the other pulses.\n\nExplanation: The correct answer is supported by the text, which states that the novel form of rogue wave generation is caused by inter-pulse Raman amplification between the SPM lobes of the many pulses. This process leads to the initially most delayed pulse swallowing the energy of all the other pulses, resulting in the formation of a temporally and spectrally delocalized high-energy rogue wave. The other options are incorrect because they do not accurately describe the primary mechanism of rogue wave generation in this context."}, "33": {"documentation": {"title": "Total Deep Variation: A Stable Regularizer for Inverse Problems", "source": "Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock", "docs_id": "2006.08789", "section": ["cs.CV", "cs.NA", "math.NA", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Total Deep Variation: A Stable Regularizer for Inverse Problems. Various problems in computer vision and medical imaging can be cast as inverse problems. A frequent method for solving inverse problems is the variational approach, which amounts to minimizing an energy composed of a data fidelity term and a regularizer. Classically, handcrafted regularizers are used, which are commonly outperformed by state-of-the-art deep learning approaches. In this work, we combine the variational formulation of inverse problems with deep learning by introducing the data-driven general-purpose total deep variation regularizer. In its core, a convolutional neural network extracts local features on multiple scales and in successive blocks. This combination allows for a rigorous mathematical analysis including an optimal control formulation of the training problem in a mean-field setting and a stability analysis with respect to the initial values and the parameters of the regularizer. In addition, we experimentally verify the robustness against adversarial attacks and numerically derive upper bounds for the generalization error. Finally, we achieve state-of-the-art results for numerous imaging tasks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the Total Deep Variation regularizer in the context of inverse problems, and how does it differ from classical handcrafted regularizers?\n\n**A)** The Total Deep Variation regularizer is a handcrafted approach that uses a pre-defined set of features to solve inverse problems, whereas classical regularizers are limited in their ability to adapt to new data.\n\n**B)** The Total Deep Variation regularizer is a data-driven approach that uses a convolutional neural network to extract local features on multiple scales and in successive blocks, allowing for a more robust and flexible solution to inverse problems.\n\n**C)** The Total Deep Variation regularizer is a classical approach that relies on a fixed set of parameters and is not sensitive to the initial values of the problem, whereas handcrafted regularizers are often sensitive to these initial values.\n\n**D)** The Total Deep Variation regularizer is a variational approach that uses a mean-field setting to optimize the training problem, allowing for a more stable and efficient solution to inverse problems.\n\n**Correct Answer:** B) The Total Deep Variation regularizer is a data-driven approach that uses a convolutional neural network to extract local features on multiple scales and in successive blocks, allowing for a more robust and flexible solution to inverse problems.\n\n**Explanation:** The correct answer, B, highlights the key contribution of the Total Deep Variation regularizer, which is its use of a data-driven approach to extract local features on multiple scales and in successive blocks. This allows for a more robust and flexible solution to inverse problems, as opposed to classical handcrafted regularizers. The other options are incorrect because they either misrepresent the Total Deep Variation regularizer (A and C) or describe a different aspect of the approach (D)."}, "34": {"documentation": {"title": "Shortcut Graphs and Groups", "source": "Nima Hoda", "docs_id": "1811.05036", "section": ["math.GR", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shortcut Graphs and Groups. We introduce shortcut graphs and groups. Shortcut graphs are graphs in which cycles cannot embed without metric distortion. Shortcut groups are groups which act properly and cocompactly on shortcut graphs. These notions unify a surprisingly broad family of graphs and groups of interest in geometric group theory and metric graph theory, including: the 1-skeletons of systolic and quadric complexes (in particular finitely presented C(6) and C(4)-T(4) small cancellation groups), 1-skeletons of finite dimensional CAT(0) cube complexes, hyperbolic graphs, standard Cayley graphs of finitely generated Coxeter groups and the standard Cayley graph of the Baumslag-Solitar group BS(1,2). Most of these examples satisfy a strong form of the shortcut property. The shortcut properties also have important geometric group theoretic consequences. We show that shortcut groups are finitely presented and have exponential isoperimetric and isodiametric functions. We show that groups satisfying the strong form of the shortcut property have polynomial isoperimetric and isodiametric functions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a shortcut group G acting properly and cocompactly on a shortcut graph \u0393. Suppose that G is a finitely presented group and \u0393 is a 1-skeleton of a finite dimensional CAT(0) cube complex.\n\nA) The isoperimetric function of G is polynomial.\nB) The isodiametric function of G is polynomial.\nC) The isoperimetric function of G is exponential.\nD) The isodiametric function of G is exponential.\n\nCorrect Answer: C) The isoperimetric function of G is exponential.\n\nExplanation: The documentation states that shortcut groups have exponential isoperimetric and isodiametric functions. This is a consequence of the shortcut property, which implies that cycles in the graph cannot embed without metric distortion. This distortion leads to exponential growth in the length of paths and the diameter of the graph, resulting in exponential isoperimetric and isodiametric functions."}, "35": {"documentation": {"title": "Viewing angle analysis of reconstructed image from digital Fresnel\n  hologram with enhanced numerical aperture", "source": "Byung Gyu Chae", "docs_id": "2012.00248", "section": ["physics.optics", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Viewing angle analysis of reconstructed image from digital Fresnel\n  hologram with enhanced numerical aperture. The viewing-angle enlargement of a holographic image is a crucial factor for realizing the holographic display. The numerical aperture (NA) of digital hologram other than a pixel specification has been known to determine the angular field extent of image. Here, we provide a valid foundation for the dependence of viewing angle on the hologram numerical aperture by investigating mathematically the internal structure of the sampled point spread function showing a self-similarity of its modulating curves and especially, analyzing this scheme on the basis of quantum mechanical framework. The enhanced-NA Fresnel hologram generates the multiple images with a high resolution, which can lead to the higher viewing angle represented as the NA of whole aperture of hologram. Optical experiment shows the consistent result with quantum mechanical description of viewing angle of holographic images. Finally, we discuss the method for enlarging viewing angle of holographic image without sacrificing image size by using this scheme."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary factor that determines the angular field extent of a holographic image, according to the study on digital Fresnel holograms with enhanced numerical aperture?\n\nA) The size of the hologram pixels\nB) The numerical aperture (NA) of the digital hologram\nC) The wavelength of the light used in the hologram\nD) The resolution of the hologram's sampling scheme\n\n**A)** The size of the hologram pixels\n**B)** The numerical aperture (NA) of the digital hologram\n**C)** The wavelength of the light used in the hologram\n**D)** The resolution of the hologram's sampling scheme\n\n**Correct Answer:** B) The numerical aperture (NA) of the digital hologram\n\n**Explanation:** The study on digital Fresnel holograms with enhanced numerical aperture highlights the importance of the numerical aperture (NA) in determining the angular field extent of a holographic image. The NA is a measure of the hologram's ability to capture and reconstruct images with high resolution, and it is the primary factor that determines the viewing angle of the holographic image. The other options are not directly related to the primary factor that determines the angular field extent of a holographic image."}, "36": {"documentation": {"title": "Reciprocal Learning Networks for Human Trajectory Prediction", "source": "Hao Sun, Zhiqun Zhao and Zhihai He", "docs_id": "2004.04340", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reciprocal Learning Networks for Human Trajectory Prediction. We observe that the human trajectory is not only forward predictable, but also backward predictable. Both forward and backward trajectories follow the same social norms and obey the same physical constraints with the only difference in their time directions. Based on this unique property, we develop a new approach, called reciprocal learning, for human trajectory prediction. Two networks, forward and backward prediction networks, are tightly coupled, satisfying the reciprocal constraint, which allows them to be jointly learned. Based on this constraint, we borrow the concept of adversarial attacks of deep neural networks, which iteratively modifies the input of the network to match the given or forced network output, and develop a new method for network prediction, called reciprocal attack for matched prediction. It further improves the prediction accuracy. Our experimental results on benchmark datasets demonstrate that our new method outperforms the state-of-the-art methods for human trajectory prediction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key insight that enables the development of a new approach, called reciprocal learning, for human trajectory prediction, and how does it differ from traditional approaches?\n\nA) The observation that human trajectories are only predictable in one direction, requiring separate forward and backward prediction networks.\nB) The discovery that human trajectories follow the same social norms and physical constraints in both forward and backward directions, allowing for a single network to be trained on both.\nC) The realization that human trajectories can be predicted using a single network, but only if the network is trained on a large dataset of labeled examples.\nD) The understanding that human trajectories are inherently unpredictable, and that any attempt to predict them will always be subject to error.\n\n**Correct Answer:** B) The discovery that human trajectories follow the same social norms and physical constraints in both forward and backward directions, allowing for a single network to be trained on both.\n\n**Explanation:** The correct answer is based on the key insight mentioned in the documentation, which states that both forward and backward trajectories follow the same social norms and obey the same physical constraints, with the only difference being their time directions. This unique property enables the development of a single network that can be trained on both forward and backward trajectories, allowing for reciprocal learning. The other options are incorrect because they do not accurately reflect the key insight mentioned in the documentation."}, "37": {"documentation": {"title": "A Note on the Quantile Formulation", "source": "Zuo Quan Xu", "docs_id": "1403.7269", "section": ["q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Note on the Quantile Formulation. Many investment models in discrete or continuous-time settings boil down to maximizing an objective of the quantile function of the decision variable. This quantile optimization problem is known as the quantile formulation of the original investment problem. Under certain monotonicity assumptions, several schemes to solve such quantile optimization problems have been proposed in the literature. In this paper, we propose a change-of-variable and relaxation method to solve the quantile optimization problems without using the calculus of variations or making any monotonicity assumptions. The method is demonstrated through a portfolio choice problem under rank-dependent utility theory (RDUT). We show that this problem is equivalent to a classical Merton's portfolio choice problem under expected utility theory with the same utility function but a different pricing kernel explicitly determined by the given pricing kernel and probability weighting function. With this result, the feasibility, well-posedness, attainability and uniqueness issues for the portfolio choice problem under RDUT are solved. It is also shown that solving functional optimization problems may reduce to solving probabilistic optimization problems. The method is applicable to general models with law-invariant preference measures including portfolio choice models under cumulative prospect theory (CPT) or RDUT, Yaari's dual model, Lopes' SP/A model, and optimal stopping models under CPT or RDUT."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a portfolio choice problem under rank-dependent utility theory (RDUT) where the investor's utility function is given by U(x) = \u222b[0,1] u(x) dF(x), where u(x) is the utility function and F(x) is the probability weighting function. The investor's goal is to maximize the expected utility of their portfolio, subject to a constraint on the portfolio's value.\n\n**A)** The optimal portfolio can be found by solving a classical Merton's portfolio choice problem under expected utility theory, where the pricing kernel is explicitly determined by the given pricing kernel and probability weighting function.\n\n**B)** The optimal portfolio can be found by solving a quantile optimization problem, where the objective function is the quantile function of the decision variable.\n\n**C)** The optimal portfolio can be found by solving a probabilistic optimization problem, where the objective function is a function of the probability distribution of the portfolio's value.\n\n**D)** The optimal portfolio can be found by solving a functional optimization problem, where the objective function is a function of the portfolio's value and the probability weighting function.\n\n**Correct Answer:** A) The optimal portfolio can be found by solving a classical Merton's portfolio choice problem under expected utility theory, where the pricing kernel is explicitly determined by the given pricing kernel and probability weighting function.\n\n**Explanation:** The correct answer is A) because the paper demonstrates that the portfolio choice problem under RDUT is equivalent to a classical Merton's portfolio choice problem under expected utility theory, with the same utility function but a different pricing kernel explicitly determined by the given pricing kernel and probability weighting function. This equivalence allows us to solve the portfolio choice problem under RDUT using classical optimization techniques, without having to resort to more advanced methods such as quantile optimization or probabilistic optimization."}, "38": {"documentation": {"title": "Modelling System of Systems Interface Contract Behaviour", "source": "Oldrich Faldik (Mendel University, Brno, Czech Republic), Richard\n  Payne (Newcastle University, Newcastle upon Tyne, United Kingdom), John\n  Fitzgerald (Newcastle University, Newcastle upon Tyne, United Kingdom),\n  Barbora Buhnova (Masaryk University, Brno, Czech Republic)", "docs_id": "1703.07037", "section": ["cs.SE", "cs.FL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modelling System of Systems Interface Contract Behaviour. A key challenge in System of Systems (SoS) engineering is the analysis and maintenance of global properties under SoS evolution, and the integration of new constituent elements. There is a need to model the constituent systems composing a SoS in order to allow the analysis of emergent behaviours at the SoS boundary. The Contract pattern allows the engineer to specify constrained behaviours to which constituent systems are required to conform in order to be a part of the SoS. However, the Contract pattern faces some limitations in terms of its accessibility and suitability for verifying contract compatibility. To address these deficiencies, we propose the enrichment of the Contract pattern, which hitherto has been defined using SysML and the COMPASS Modelling Language (CML), by utilising SysML and Object Constraint Language (OCL). In addition, we examine the potential of interface automata, a notation for improving loose coupling between interfaces of constituent systems defined according to the contract, as a means of enabling the verification of contract compatibility. The approach is demonstrated using a case study in audio/video content streaming."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of System of Systems (SoS) engineering, what is the primary challenge in modelling the interface contract behaviour of constituent systems, and how does the proposed enrichment of the Contract pattern using SysML and OCL address this challenge?\n\n**A)** The primary challenge is the lack of standardization in SoS modelling languages, and the proposed enrichment addresses this by providing a more comprehensive and standardized approach to modelling interface contract behaviour.\n\n**B)** The primary challenge is the difficulty in verifying contract compatibility between constituent systems, and the proposed enrichment addresses this by introducing interface automata, which enables the formal verification of contract compatibility.\n\n**C)** The primary challenge is the complexity of SoS evolution, and the proposed enrichment addresses this by providing a more modular and flexible approach to modelling interface contract behaviour, allowing for easier integration of new constituent elements.\n\n**D)** The primary challenge is the limited expressiveness of the Contract pattern, and the proposed enrichment addresses this by introducing OCL, which provides a more powerful and expressive language for specifying interface contract behaviour.\n\n**Correct Answer:** D) The primary challenge is the limited expressiveness of the Contract pattern, and the proposed enrichment addresses this by introducing OCL, which provides a more powerful and expressive language for specifying interface contract behaviour.\n\n**Explanation:** The question requires the candidate to understand the challenges in modelling interface contract behaviour in SoS engineering and how the proposed enrichment of the Contract pattern addresses these challenges. The correct answer, D, highlights the limited expressiveness of the Contract pattern and how the introduction of OCL provides a more powerful and expressive language for specifying interface contract behaviour. The other options are incorrect because they either misrepresent the primary challenge or oversimplify the solution."}, "39": {"documentation": {"title": "Topological structures in the equities market network", "source": "Greg Leibon, Scott D. Pauls, Daniel N. Rockmore, Robert Savell", "docs_id": "0805.3470", "section": ["q-fin.PR", "physics.data-an", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological structures in the equities market network. We present a new method for articulating scale-dependent topological descriptions of the network structure inherent in many complex systems. The technique is based on \"Partition Decoupled Null Models,'' a new class of null models that incorporate the interaction of clustered partitions into a random model and generalize the Gaussian ensemble. As an application we analyze a correlation matrix derived from four years of close prices of equities in the NYSE and NASDAQ. In this example we expose (1) a natural structure composed of two interacting partitions of the market that both agrees with and generalizes standard notions of scale (eg., sector and industry) and (2) structure in the first partition that is a topological manifestation of a well-known pattern of capital flow called \"sector rotation.'' Our approach gives rise to a natural form of multiresolution analysis of the underlying time series that naturally decomposes the basic data in terms of the effects of the different scales at which it clusters. The equities market is a prototypical complex system and we expect that our approach will be of use in understanding a broad class of complex systems in which correlation structures are resident."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary contribution of the \"Partition Decoupled Null Models\" approach presented in the paper \"Topological structures in the equities market network\"?\n\nA) It provides a new method for analyzing the correlation matrix of equities prices in the NYSE and NASDAQ.\nB) It introduces a new class of null models that incorporate the interaction of clustered partitions into a random model, which generalizes the Gaussian ensemble.\nC) It reveals a natural structure composed of two interacting partitions of the market that both agree with and generalize standard notions of scale.\nD) It demonstrates a topological manifestation of a well-known pattern of capital flow called \"sector rotation\" in the first partition.\n\nCorrect Answer: B) It introduces a new class of null models that incorporate the interaction of clustered partitions into a random model, which generalizes the Gaussian ensemble.\n\nExplanation: The correct answer is B) because the paper states that the technique is based on \"Partition Decoupled Null Models,\" a new class of null models that incorporate the interaction of clustered partitions into a random model and generalize the Gaussian ensemble. This is the primary contribution of the approach, as it provides a new framework for analyzing complex systems like the equities market network. The other options are incorrect because they describe specific applications or results of the approach, rather than its primary contribution."}, "40": {"documentation": {"title": "Complexity-Oriented Per-shot Video Coding Optimization", "source": "Hongcheng Zhong and Jun Xu and Chen Zhu and Donghui Feng and Li Song", "docs_id": "2112.12424", "section": ["eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity-Oriented Per-shot Video Coding Optimization. Current per-shot encoding schemes aim to improve the compression efficiency by shot-level optimization. It splits a source video sequence into shots and imposes optimal sets of encoding parameters to each shot. Per-shot encoding achieved approximately 20% bitrate savings over baseline fixed QP encoding at the expense of pre-processing complexity. However, the adjustable parameter space of the current per-shot encoding schemes only has spatial resolution and QP/CRF, resulting in a lack of encoding flexibility. In this paper, we extend the per-shot encoding framework in the complexity dimension. We believe that per-shot encoding with flexible complexity will help in deploying user-generated content. We propose a rate-distortion-complexity optimization process for encoders and a methodology to determine the coding parameters under the constraints of complexities and bitrate ladders. Experimental results show that our proposed method achieves complexity constraints ranging from 100% to 3% in a dense form compared to the slowest per-shot anchor. With similar complexities of the per-shot scheme fixed in specific presets, our proposed method achieves BDrate gain up to -19.17%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of current per-shot encoding schemes, and how does the proposed method in the paper address this limitation?\n\nA) The current per-shot encoding schemes only optimize for spatial resolution, but the proposed method introduces a new dimension of complexity optimization.\nB) The current per-shot encoding schemes only optimize for QP/CRF, but the proposed method introduces a new dimension of complexity optimization.\nC) The current per-shot encoding schemes only optimize for bitrate, but the proposed method introduces a new dimension of complexity optimization.\nD) The current per-shot encoding schemes only optimize for pre-processing complexity, but the proposed method introduces a new dimension of complexity optimization.\n\nCorrect Answer: B) The current per-shot encoding schemes only optimize for QP/CRF, but the proposed method introduces a new dimension of complexity optimization.\n\nExplanation: The paper states that the adjustable parameter space of the current per-shot encoding schemes only has spatial resolution and QP/CRF, resulting in a lack of encoding flexibility. This means that the current schemes only optimize for two parameters, whereas the proposed method introduces a new dimension of complexity optimization, allowing for more flexible encoding."}, "41": {"documentation": {"title": "Diffusive transport without detailed balance in motile bacteria: Does\n  microbiology need statistical physics?", "source": "M. E. Cates", "docs_id": "1208.3957", "section": ["cond-mat.stat-mech", "cond-mat.soft", "physics.bio-ph", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diffusive transport without detailed balance in motile bacteria: Does\n  microbiology need statistical physics?. Microbiology is the science of microbes, particularly bacteria. Many bacteria are motile: they are capable of self-propulsion. Among these, a significant class execute so-called run-and-tumble motion: they follow a fairly straight path for a certain distance, then abruptly change direction before repeating the process. This dynamics has something in common with Brownian motion (it is diffusive at large scales), and also something in contrast. Specifically, motility parameters such as the run speed and tumble rate depend on the local environment and hence can vary in space. When they do so, even if a steady state is reached, this is not generally invariant under time-reversal: the principle of detailed balance, which restores the microscopic time-reversal symmetry of systems in thermal equilibrium, is mesoscopically absent in motile bacteria. This lack of detailed balance (allowed by the flux of chemical energy that drives motility) creates pitfalls for the unwary modeller. Here I review some statistical mechanical models for bacterial motility, presenting them as a paradigm for exploring diffusion without detailed balance. I also discuss the extent to which statistical physics is useful in understanding real or potential microbiological experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary consequence of the lack of detailed balance in the motility dynamics of run-and-tumble motion in motile bacteria, and how does this impact the application of statistical physics in microbiology?\n\n**A)** The lack of detailed balance leads to a loss of time-reversal symmetry, which is not a significant issue in microbiology. This allows for the use of traditional statistical mechanical models to describe bacterial motility.\n\n**B)** The lack of detailed balance creates a non-equilibrium system, which is not suitable for application of statistical physics. This requires the development of new models that account for the non-equilibrium nature of bacterial motility.\n\n**C)** The lack of detailed balance leads to a violation of the principle of conservation of energy, which is a fundamental concept in statistical physics. This requires a re-evaluation of the underlying assumptions of statistical physics in microbiology.\n\n**D)** The lack of detailed balance allows for the use of non-equilibrium statistical mechanics, which can provide new insights into the dynamics of bacterial motility. This enables the development of models that can accurately describe the complex behavior of motile bacteria.\n\n**Correct Answer:** D) The lack of detailed balance allows for the use of non-equilibrium statistical mechanics, which can provide new insights into the dynamics of bacterial motility. This enables the development of models that can accurately describe the complex behavior of motile bacteria.\n\n**Explanation:** The correct answer is D) because the lack of detailed balance in motile bacteria's run-and-tumble motion creates an opportunity for the application of non-equilibrium statistical mechanics. This approach can provide new insights into the dynamics of bacterial motility, which is a complex and non-equilibrium process. The other options are incorrect because they either downplay the significance of the lack of detailed balance (A) or incorrectly state that it leads to a loss of time-reversal symmetry (B) or a violation of the principle of conservation of energy (C)."}, "42": {"documentation": {"title": "Phonon and light read out of a Li$_2$MoO$_4$ crystal with multiplexed\n  kinetic inductance detectors", "source": "N. Casali, L. Cardani, I. Colantoni, A. Cruciani, S. Di Domizio, M.\n  Martinez, G. Pettinari, M. Vignati", "docs_id": "1907.03647", "section": ["physics.ins-det", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phonon and light read out of a Li$_2$MoO$_4$ crystal with multiplexed\n  kinetic inductance detectors. Molybdenum based crystals such as Li$_2$MoO$_4$ and CaMoO$_4$ are emerging as leading candidates for next generation experiments searching for neutrino-less double beta decay with cryogenic calorimeters (CUPID, AMoRE). The exquisite energy resolution and high radio-purity of these crystals come at the cost of a potentially detrimental background source: the two neutrinos double beta decay of $^{100}$Mo. Indeed, the fast half-life of this decay mode, combined with the slow response of cryogenic calorimeters, would result in pile-up events in the energy region of interest for neutrino-less double beta decay, reducing the experimental sensitivity. This background can be suppressed using fast and high sensitivity cryogenic light detectors, provided that the scintillation time constant itself does not limit the time resolution. We developed a new detection technique exploiting the high sensitivity, the fast time response and the multiplexing capability of Kinetic Inductance Detectors. We applied the proposed technique to a $2\\times2\\times2$ cm$^3$ Li$_2$MoO$_4$ crystal, which was chosen as baseline option for CUPID. We measured simultaneously both the phonon and scintillation signals with KIDs. We derived the scintillation time constant of this compound at millikelvin temperatures obtaining $\\tau_{scint} = 84.5\\pm4.5\\rm{(syst)}\\pm1.0\\rm{(stat)}$ $\\mu$s, constant between 10 and 190 mK."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in using cryogenic calorimeters (CUPID, AMoRE) to search for neutrino-less double beta decay, and how can it be addressed using Kinetic Inductance Detectors (KIDs)?\n\n**A)** The primary challenge is the slow response of cryogenic calorimeters, which can lead to pile-up events in the energy region of interest. KIDs can address this challenge by providing fast time response and multiplexing capability.\n\n**B)** The primary challenge is the high radio-purity of molybdenum-based crystals, which can lead to background events from the two neutrinos double beta decay of $^{100}$Mo. KIDs can address this challenge by providing high sensitivity and fast time response.\n\n**C)** The primary challenge is the fast half-life of the two neutrinos double beta decay of $^{100}$Mo, which can lead to pile-up events in the energy region of interest. KIDs can address this challenge by providing fast time response and multiplexing capability.\n\n**D)** The primary challenge is the low energy resolution of cryogenic calorimeters, which can lead to reduced experimental sensitivity. KIDs can address this challenge by providing high sensitivity and fast time response.\n\n**Correct Answer:** C) The primary challenge is the fast half-life of the two neutrinos double beta decay of $^{100}$Mo, which can lead to pile-up events in the energy region of interest. KIDs can address this challenge by providing fast time response and multiplexing capability.\n\n**Explanation:** The correct answer is C) because the fast half-life of the two neutrinos double beta decay of $^{100}$Mo is a significant challenge in using cryogenic calorimeters to search for neutrino-less double beta decay. The slow response of cryogenic calorimeters can lead to pile-up events in the energy region of interest, reducing the experimental sensitivity. KIDs can address this challenge by providing fast time response and multiplexing capability, which can help to suppress the background events and improve the experimental sensitivity."}, "43": {"documentation": {"title": "Impact of Intervals on the Emotional Effect in Western Music", "source": "Cengiz Kaygusuz, Julian Zuluaga", "docs_id": "1812.04723", "section": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Impact of Intervals on the Emotional Effect in Western Music. Every art form ultimately aims to invoke an emotional response over the audience, and music is no different. While the precise perception of music is a highly subjective topic, there is an agreement in the \"feeling\" of a piece of music in broad terms. Based on this observation, in this study, we aimed to determine the emotional feeling associated with short passages of music; specifically by analyzing the melodic aspects. We have used the dataset put together by Eerola et. al. which is comprised of labeled short passages of film music. Our initial survey of the dataset indicated that other than \"happy\" and \"sad\" labels do not possess a melodic structure. We transcribed the main melody of the happy and sad tracks and used the intervals between the notes to classify them. Our experiments have shown that treating a melody as a bag-of-intervals do not possess any predictive power whatsoever, whereas counting intervals with respect to the key of the melody yielded a classifier with 85% accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary approach used in the study \"Impact of Intervals on the Emotional Effect in Western Music\" to analyze the emotional feeling associated with short passages of music, and what is the key finding regarding the use of intervals in this analysis?\n\nA) The study used a bag-of-features approach to analyze the melodic aspects of music, and found that intervals do not possess any predictive power. \nB) The study used a bag-of-intervals approach to analyze the melodic aspects of music, and found that counting intervals with respect to the key of the melody yielded a classifier with 85% accuracy. \nC) The study used a bag-of-features approach to analyze the melodic aspects of music, and found that counting intervals with respect to the key of the melody yielded a classifier with 85% accuracy. \nD) The study used a bag-of-intervals approach to analyze the melodic aspects of music, but found that intervals do possess predictive power.\n\nCorrect Answer: B) The study used a bag-of-intervals approach to analyze the melodic aspects of music, and found that counting intervals with respect to the key of the melody yielded a classifier with 85% accuracy.\n\nExplanation: The study used a bag-of-intervals approach to analyze the melodic aspects of music, but this approach did not yield any predictive power. However, when the intervals were counted with respect to the key of the melody, the classifier achieved an accuracy of 85%. This suggests that the key of the melody plays a significant role in determining the emotional feeling associated with a piece of music."}, "44": {"documentation": {"title": "Dynamic Peer-to-Peer Competition", "source": "L. F. Caram, C. F. Caiafa, A. N. Proto and M. Ausloos", "docs_id": "1004.5020", "section": ["nlin.CD", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Peer-to-Peer Competition. The dynamic behavior of a multiagent system in which the agent size $s_{i}$ is variable it is studied along a Lotka-Volterra approach. The agent size has hereby for meaning the fraction of a given market that an agent is able to capture (market share). A Lotka-Volterra system of equations for prey-predator problems is considered, the competition factor being related to the difference in size between the agents in a one-on-one competition. This mechanism introduces a natural self-organized dynamic competition among agents. In the competition factor, a parameter $\\sigma$ is introduced for scaling the intensity of agent size similarity, which varies in each iteration cycle. The fixed points of this system are analytically found and their stability analyzed for small systems (with $n=5$ agents). We have found that different scenarios are possible, from chaotic to non-chaotic motion with cluster formation as function of the $\\sigma$ parameter and depending on the initial conditions imposed to the system. The present contribution aim is to show how a realistic though minimalist nonlinear dynamics model can be used to describe market competition (companies, brokers, decision makers) among other opinion maker communities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the Lotka-Volterra approach to modeling dynamic peer-to-peer competition in a multiagent system, where the agent size represents the fraction of a given market that an agent is able to capture (market share). How does the introduction of the competition factor, parameterized by \u03c3, influence the stability of the system's fixed points, and what scenarios are possible for small systems (n=5 agents) with varying initial conditions?\n\n**A)** The introduction of \u03c3 leads to a deterministic competition factor, resulting in a stable fixed point for all systems, regardless of initial conditions.\n\n**B)** The competition factor is a stochastic process, introducing randomness in the system's dynamics, leading to chaotic behavior for all systems with varying initial conditions.\n\n**C)** The stability of the fixed points is influenced by the value of \u03c3, with \u03c3=1 leading to a stable fixed point for all systems, and \u03c3>1 leading to unstable fixed points for systems with initial conditions favoring larger agents.\n\n**D)** The Lotka-Volterra approach is a linear model, and the introduction of \u03c3 does not affect the stability of the fixed points, which remain constant for all systems with varying initial conditions.\n\n**Correct Answer:** C) The stability of the fixed points is influenced by the value of \u03c3, with \u03c3=1 leading to a stable fixed point for all systems, and \u03c3>1 leading to unstable fixed points for systems with initial conditions favoring larger agents.\n\n**Explanation:** The correct answer, C, is based on the analysis of the Lotka-Volterra approach to modeling dynamic peer-to-peer competition. The introduction of the competition factor, parameterized by \u03c3, influences the stability of the system's fixed points. For small systems (n=5 agents), the stability of the fixed points depends on the value of \u03c3. When \u03c3=1, the fixed point is stable for all systems, regardless of initial conditions. However, when \u03c3>1, the fixed point becomes unstable, and systems with initial conditions favoring larger agents exhibit chaotic behavior. This is in line with the findings of the study, which showed that different scenarios are possible, from chaotic to non-chaotic motion with cluster formation, depending on the value of \u03c3 and initial conditions."}, "45": {"documentation": {"title": "The Role of Contextual Information in Best Arm Identification", "source": "Masahiro Kato and Kaito Ariu", "docs_id": "2106.14077", "section": ["cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Role of Contextual Information in Best Arm Identification. We study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. Although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. Our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. We show the instance-specific sample complexity lower bounds for the problem. Then, we propose a context-aware version of the \"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. We demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of Garivier & Kaufmann (2016). We experimentally confirm that context information contributes to faster best-arm identification."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of best-arm identification with fixed confidence and available contextual information in stochastic bandits, what is the primary goal of the proposed \"Track-and-Stop\" strategy, and how does it differ from the approach of Garivier & Kaufmann (2016)?\n\n**A)** The primary goal is to minimize the number of samplings required to identify the best arm, while also considering the contextual distribution of the rewards. The \"Track-and-Stop\" strategy differs from Garivier & Kaufmann (2016) in that it uses contextual information to improve the efficiency of best-arm identification.\n\n**B)** The primary goal is to maximize the expected number of arm draws, while also minimizing the error rate. The \"Track-and-Stop\" strategy differs from Garivier & Kaufmann (2016) in that it uses a different allocation strategy to achieve this goal.\n\n**C)** The primary goal is to identify the best arm with a minimal number of samplings under a given value of the error rate, while also considering the contextual distribution of the rewards. The \"Track-and-Stop\" strategy differs from Garivier & Kaufmann (2016) in that it uses contextual information to improve the efficiency of best-arm identification.\n\n**D)** The primary goal is to minimize the error rate, while also maximizing the expected number of arm draws. The \"Track-and-Stop\" strategy differs from Garivier & Kaufmann (2016) in that it uses a different allocation strategy to achieve this goal.\n\n**Correct Answer:** C) The primary goal is to identify the best arm with a minimal number of samplings under a given value of the error rate, while also considering the contextual distribution of the rewards. The \"Track-and-Stop\" strategy differs from Garivier & Kaufmann (2016) in that it uses contextual information to improve the efficiency of best-arm identification.\n\n**Explanation:** The correct answer is C) because it accurately summarizes the primary goal of the \"Track-and-Stop\" strategy, which is to identify the best arm with a minimal number of samplings under a given value of the error rate, while also considering the contextual distribution of the rewards. The strategy differs from Garivier & Kaufmann (2016) in that it uses contextual information to improve the efficiency of best-arm identification, as stated in the original documentation."}, "46": {"documentation": {"title": "Fingerprinting defects in diamond: Partitioning the vibrational spectrum", "source": "Danny E.P. Vanpoucke", "docs_id": "2001.06277", "section": ["cond-mat.mtrl-sci", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fingerprinting defects in diamond: Partitioning the vibrational spectrum. In this work, we present a computational scheme for isolating the vibrational spectrum of a defect in a solid. By quantifying the defect character of the atom-projected vibrational spectra, the contributing atoms are identified and the strength of their contribution determined. This method could be used to systematically improve phonon fragment calculations. More interestingly, using the atom-projected vibrational spectra of the defect atoms directly, it is possible to obtain a well-converged defect spectrum at lower computational cost, which also incorporates the host-lattice interactions. Using diamond as the host material, four test case defects, each presenting a distinctly different vibrational behaviour, are considered: a heavy substitutional dopant (Eu), two intrinsic defects (neutral vacancy and split interstitial), and the negatively charged N-vacancy center. The heavy dopant and split interstitial present localized modes at low and high frequencies, respectively, showing little overlap with the host spectrum. In contrast, the neutral vacancy and the N-vacancy center show a broad contribution to the upper spectral range of the host spectrum, making them challenging to extract. Independent of the vibrational behaviour, the main atoms contributing to the defect spectrum can be clearly identified. Recombination of their atom-projected spectra results in the isolated defect spectrum."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of using atom-projected vibrational spectra of the defect atoms directly to obtain a well-converged defect spectrum at lower computational cost?\n\nA) It allows for the incorporation of host-lattice interactions without additional computational cost.\nB) It enables the identification of the contributing atoms and their strength of contribution without the need for recombination of their spectra.\nC) It facilitates the extraction of the defect spectrum by reducing the overlap with the host spectrum.\nD) It provides a more accurate representation of the defect spectrum by incorporating the vibrational behavior of the defect atoms.\n\nCorrect Answer: D) It provides a more accurate representation of the defect spectrum by incorporating the vibrational behavior of the defect atoms.\n\nExplanation: The correct answer is D) because the question specifically asks about the primary advantage of using atom-projected vibrational spectra of the defect atoms directly. The correct answer highlights the key benefit of this approach, which is that it provides a more accurate representation of the defect spectrum by incorporating the vibrational behavior of the defect atoms. The other options, while related to the topic, do not accurately capture the primary advantage of this approach. Option A is incorrect because it is not the primary advantage, but rather a consequence of using this approach. Option B is incorrect because it is a related benefit, but not the primary advantage. Option C is incorrect because it is not a direct result of using atom-projected vibrational spectra of the defect atoms directly."}, "47": {"documentation": {"title": "Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations\n  for Various Needle Geometries and Locations", "source": "Niclas Kath, Heinz Handels, Andre Mastmeyer", "docs_id": "1907.05709", "section": ["physics.med-ph", "cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations\n  for Various Needle Geometries and Locations. Purpose: Radio-frequency ablations play an important role in the therapy of malignant liver lesions. The navigation of a needle to the lesion poses a challenge for both the trainees and intervening physicians. Methods: This publication presents a new GPU-based, accurate method for the simulation of radio-frequency ablations for lesions at the needle tip in general and for an existing visuo-haptic 4D VR simulator. The method is implemented real-time capable with Nvidia CUDA. Results: It performs better than a literature method concerning the theoretical characteristic of monotonic convergence of the bioheat PDE and a in vitro gold standard with significant improvements (p < 0.05) in terms of Pearson correlations. It shows no failure modes or theoretically inconsistent individual simulation results after the initial phase of 10 seconds. On the Nvidia 1080 Ti GPU it achieves a very high frame rendering performance of >480 Hz. Conclusion: Our method provides a more robust and safer real-time ablation planning and intraoperative guidance technique, especially avoiding the over-estimation of the ablated tissue death zone, which is risky for the patient in terms of tumor recurrence. Future in vitro measurements and optimization shall further improve the conservative estimate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed GPU-based method for simulating radio-frequency ablations, as demonstrated in the study?\n\nA) It provides a more realistic visualization of the ablated tissue death zone, leading to better treatment planning.\nB) It achieves a higher frame rendering performance on the Nvidia 1080 Ti GPU, making it suitable for real-time intraoperative guidance.\nC) It offers a more conservative estimate of the ablated tissue death zone, reducing the risk of tumor recurrence and improving patient outcomes.\nD) It allows for more accurate navigation of the needle to the lesion, improving the training experience for medical professionals.\n\n**Correct Answer:** C) It offers a more conservative estimate of the ablated tissue death zone, reducing the risk of tumor recurrence and improving patient outcomes.\n\n**Explanation:** The study highlights the importance of accurately estimating the ablated tissue death zone to prevent tumor recurrence. The proposed method provides a more conservative estimate, which is a significant advantage in the context of radio-frequency ablation therapy. This is evident from the conclusion, which states that the method \"provides a more robust and safer real-time ablation planning and intraoperative guidance technique.\""}, "48": {"documentation": {"title": "Electric Field Control of Soliton Motion and Stacking in Trilayer\n  Graphene", "source": "Matthew Yankowitz, Joel I-Jan Wang, A. Glen Birdwell, Yu-An Chen, K.\n  Watanabe, T. Taniguchi, Philippe Jacquod, Pablo San-Jose, Pablo\n  Jarillo-Herrero, Brian J. LeRoy", "docs_id": "1401.7663", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electric Field Control of Soliton Motion and Stacking in Trilayer\n  Graphene. The crystal structure of a material plays an important role in determining its electronic properties. Changing from one crystal structure to another involves a phase transition which is usually controlled by a state variable such as temperature or pressure. In the case of trilayer graphene, there are two common stacking configurations (Bernal and rhombohedral) which exhibit very different electronic properties. In graphene flakes with both stacking configurations, the region between them consists of a localized strain soliton where the carbon atoms of one graphene layer shift by the carbon-carbon bond distance. Here we show the ability to move this strain soliton with a perpendicular electric field and hence control the stacking configuration of trilayer graphene with only an external voltage. Moreover, we find that the free energy difference between the two stacking configurations scales quadratically with electric field, and thus rhombohedral stacking is favored as the electric field increases. This ability to control the stacking order in graphene opens the way to novel devices which combine structural and electrical properties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the stacking configuration of trilayer graphene is controlled in the experiment described?\n\nA) By applying a temperature gradient to the graphene sample\nB) By using a mechanical stress to induce a phase transition\nC) By applying a perpendicular electric field to move the strain soliton\nD) By using a chemical vapor deposition process to alter the crystal structure\n\nCorrect Answer: C) By applying a perpendicular electric field to move the strain soliton\n\nExplanation: The experiment described in the documentation shows that the stacking configuration of trilayer graphene can be controlled by applying a perpendicular electric field, which moves the strain soliton and changes the stacking order. This is a key finding in the study, as it demonstrates a novel method for controlling the electronic properties of graphene. The other options are incorrect because they do not accurately describe the mechanism by which the stacking configuration is controlled in the experiment."}, "49": {"documentation": {"title": "Causal Spillover Effects Using Instrumental Variables", "source": "Gonzalo Vazquez-Bare", "docs_id": "2003.06023", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal Spillover Effects Using Instrumental Variables. I set up a potential outcomes framework to analyze spillover effects using instrumental variables. I characterize the population compliance types in a setting in which spillovers can occur on both treatment take-up and outcomes, and provide conditions for identification of the marginal distribution of compliance types. I show that intention-to-treat (ITT) parameters aggregate multiple direct and spillover effects for different compliance types, and hence do not have a clear link to causally interpretable parameters. Moreover, rescaling ITT parameters by first-stage estimands generally recovers a weighted combination of average effects where the sum of weights is larger than one. I then analyze identification of causal direct and spillover effects under one-sided noncompliance, and show that causal effects can be estimated by 2SLS in this case. I illustrate the proposed methods using data from an experiment on social interactions and voting behavior. I also introduce an alternative assumption, independence of peers' types, that identifies parameters of interest under two-sided noncompliance by restricting the amount of heterogeneity in average potential outcomes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of causal spillover effects using instrumental variables, what is the primary limitation of using intention-to-treat (ITT) parameters in estimating causal direct and spillover effects, and how can rescaling ITT parameters by first-stage estimands address this limitation?\n\n**A)** ITT parameters are not identifiable under two-sided noncompliance, and rescaling recovers a weighted combination of average effects where the sum of weights is equal to one.\n\n**B)** ITT parameters aggregate multiple direct and spillover effects for different compliance types, and rescaling ITT parameters by first-stage estimands recovers a weighted combination of average effects where the sum of weights is larger than one.\n\n**C)** ITT parameters are not estimable under one-sided noncompliance, and rescaling ITT parameters by first-stage estimands recovers a weighted combination of average effects where the sum of weights is equal to one.\n\n**D)** ITT parameters are not causal under two-sided noncompliance, and rescaling ITT parameters by first-stage estimands recovers a weighted combination of average effects where the sum of weights is equal to one.\n\n**Correct Answer:** B) ITT parameters aggregate multiple direct and spillover effects for different compliance types, and rescaling ITT parameters by first-stage estimands recovers a weighted combination of average effects where the sum of weights is larger than one.\n\n**Explanation:** The correct answer is B) because the documentation states that ITT parameters aggregate multiple direct and spillover effects for different compliance types, and rescaling ITT parameters by first-stage estimands generally recovers a weighted combination of average effects where the sum of weights is larger than one. This is a key limitation of using ITT parameters in estimating causal direct and spillover effects, and rescaling ITT parameters by first-stage estimands is a proposed solution to address this limitation."}, "50": {"documentation": {"title": "Fermi Surface and Spectral Functions of a Hole Doped Spin-Fermion Model\n  for Cuprates", "source": "M. Moraghebi, C. Buhler, S. Yunoki, A. Moreo", "docs_id": "cond-mat/0011366", "section": ["cond-mat.supr-con", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fermi Surface and Spectral Functions of a Hole Doped Spin-Fermion Model\n  for Cuprates. Using numerical techniques we study the spectral function $A(k,\\omega)$ of a spin-fermion model for cuprates in the regime where magnetic and charge domains (stripes) are developed upon hole-doping. From $A(k,\\omega)$ we study the electronic dynamics and determine the Fermi Surface (FS), which is compared with angular resolved photoemission results for $La_{2-x}Sr_xCuO_2$. A pseudogap is observed in the density of states at the chemical potential for all finite dopings. The striped ground state appears to be metallic in this model since there is finite spectral weight at the chemical potential, but the electronic hopping seems to be stronger perpendicular to the stripes rather than along them. The band structure is not rigid, contrary to the behavior found in mean-field studies, and changes with doping. Both mid-gap (stripe induced) and valence band states determine the FS. For vertical (horizontal) stripes, a clear FS appears close to $(\\pi,0)$ $((0,\\pi))$, while no FS is observed close to $(0,\\pi)$ $((\\pi,0))$. Along the diagonal direction the spectral function shows a clear quasi-particle peak close to (0,0), but its weight is reduced as the chemical potential is approached. A weak FS develops along this direction as the system is doped."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the Fermi Surface (FS) and spectral functions of a hole-doped spin-fermion model for cuprates, as described in the provided Arxiv documentation. What is the primary difference in the observed Fermi Surface (FS) for vertical (horizontal) stripes versus diagonal direction, and what implications does this have for the electronic dynamics of the system?\n\n**A)** The FS for vertical stripes is a clear, well-defined pocket near $(\\pi,0)$, while the FS for diagonal direction is a weak, diffuse region near $(0,0)$. This suggests that the system is more metallic in the vertical direction, with stronger electronic hopping along the stripes.\n\n**B)** The FS for vertical stripes is a clear, well-defined pocket near $(0,\\pi)$, while the FS for diagonal direction is a clear, well-defined peak near $(0,0)$. This suggests that the system is more metallic in the diagonal direction, with stronger electronic hopping perpendicular to the stripes.\n\n**C)** The FS for vertical stripes is a weak, diffuse region near $(\\pi,0)$, while the FS for diagonal direction is a clear, well-defined peak near $(0,0)$. This suggests that the system is more metallic in the diagonal direction, with weaker electronic hopping along the stripes.\n\n**D)** The FS for vertical stripes is a clear, well-defined pocket near $(0,\\pi)$, while the FS for diagonal direction is a weak, diffuse region near $(0,0)$. This suggests that the system is more metallic in the vertical direction, with weaker electronic hopping perpendicular to the stripes.\n\n**Correct Answer:** C) The FS for vertical stripes is a weak, diffuse region near $(\\pi,0)$, while the FS for diagonal direction is a clear, well-defined peak near $(0,0)$. This suggests that the system is more metallic in the diagonal direction, with weaker electronic hopping along the stripes.\n\n**Explanation:** The correct answer is C) because the documentation states that \"For vertical (horizontal) stripes, a clear FS appears close to $(\\pi,0)$ $((0,\\pi))$, while no FS is observed close to $(0,\\pi)$ $((\\pi,0))$\". This suggests that the system is more metallic in the vertical direction, with weaker electronic hopping along the stripes. Additionally, the documentation states that \"Along the diagonal direction the spectral function shows a clear quasi-particle peak close to (0,0), but its weight is reduced as the chemical potential is approached\", which supports the idea that the system is more metallic in the diagonal direction."}, "51": {"documentation": {"title": "Evaluation of Banking Sectors Development in Bangladesh in light of\n  Financial Reform", "source": "Nusrat Jahan, K.M. Golam Muhiuddin", "docs_id": "2005.11669", "section": ["q-fin.GN", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of Banking Sectors Development in Bangladesh in light of\n  Financial Reform. Historically, the performance of the banking sector has been weak, characterized by weak asset quality, inadequate provisioning, and negative capitalization of state-owned banks. To overcome these problems, the initial phase of banking reform (1980-1990) focused on the promotion of private ownership and denationalization of nationalized commercial banks (SCBs). During the second phase of reform, Financial Sector Reform Project (FSRP) of World Bank was launched in 1990 with the focus on gradual deregulations of the interest rate structure, providing market-oriented incentives for priority sector lending and improvement in the debt recovery environment. Moreover, a large number of private commercial banks were granted licenses during the second phase of reforms. Bangladesh Bank adopted Basel-I norms in 1996 and Basel-II during 2010. Moreover, the Central Bank Strengthening Project initiated in 2003 focused on effective regulatory and supervisory system, particularly strengthening the legal framework of banking sector. This study evaluates how successfully the banking sector of Bangladesh has evolved over the past decades in light of financial reform measures undertaken to strengthen this sector."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Evaluate the effectiveness of the banking sector reform measures implemented in Bangladesh from 1980 to 2010, with a focus on the impact of Financial Sector Reform Project (FSRP) and the adoption of Basel-I and Basel-II norms.\n\n**A)** The banking sector reform measures in Bangladesh have been successful in improving asset quality and provisioning, but have had limited impact on negative capitalization of state-owned banks.\n\n**B)** The FSRP and adoption of Basel-I and Basel-II norms have led to significant improvements in the banking sector's interest rate structure, priority sector lending, and debt recovery environment, but have also resulted in increased competition and market volatility.\n\n**C)** The banking sector reform measures in Bangladesh have been ineffective in addressing the underlying issues of weak asset quality and inadequate provisioning, and have instead led to increased consolidation and concentration of banking assets.\n\n**D)** The FSRP and adoption of Basel-I and Basel-II norms have had a positive impact on the banking sector's capitalization, but have also led to a decline in the sector's overall performance and competitiveness.\n\n**Correct Answer:** B) The FSRP and adoption of Basel-I and Basel-II norms have led to significant improvements in the banking sector's interest rate structure, priority sector lending, and debt recovery environment, but have also resulted in increased competition and market volatility.\n\n**Explanation:** The correct answer is based on the documentation provided, which states that the FSRP and adoption of Basel-I and Basel-II norms have led to improvements in the banking sector's interest rate structure, priority sector lending, and debt recovery environment. However, it also notes that these reforms have resulted in increased competition and market volatility, which is reflected in option B. Options A, C, and D are incorrect because they do not accurately reflect the documentation's description of the banking sector reform measures' impact."}, "52": {"documentation": {"title": "Near-field imaging of surface-plasmon vortex-modes around a single\n  elliptical nanohole in a gold film", "source": "Claudia Triolo, Salvatore Savasta, Alessio Settineri, Sebastiano\n  Trusso, Rosalba Saija, Nisha Rani Agarwal, Salvatore Patan\\`e", "docs_id": "1810.04902", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near-field imaging of surface-plasmon vortex-modes around a single\n  elliptical nanohole in a gold film. We present scanning near-field images of surface plasmon modes around a single elliptical nanohole in 88 nm thick Au film. We find that rotating surface plasmon vortex modes carrying extrinsic orbital angular momentum can be induced under linearly polarized illumination. The vortex modes are obtained only when the incident polarization direction differs from one of the ellipse axes. Such a direct observation of the vortex modes is possible thanks to the ability of the SNOM technique to obtain information on both the amplitude and the phase of the near field. The presence of the vortex mode is determined by the rotational symmetry breaking of the system and it can be considered the counterpart of the photonic spin Hall effect. Finite element method calculations show that such a vorticity originates from the presence of nodal points where the phase of the field is undefined, leading to a circulation of the energy flow. The configuration producing vortex modes corresponds to a nonzero total topological charge (+1)."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the underlying physical mechanism responsible for the emergence of surface plasmon vortex modes in a gold film with a single elliptical nanohole, as observed in the study on near-field imaging of surface-plasmon vortex-modes around a single elliptical nanohole in a gold film?\n\nA) The presence of extrinsic orbital angular momentum in the incident light\nB) The rotational symmetry breaking of the system due to the elliptical shape of the nanohole\nC) The finite element method calculations show that the vorticity originates from the presence of nodal points where the phase of the field is undefined, leading to a circulation of the energy flow\nD) The photonic spin Hall effect, which is a phenomenon where the rotation of the polarization direction induces a topological charge\n\nCorrect Answer: C) The finite element method calculations show that the vorticity originates from the presence of nodal points where the phase of the field is undefined, leading to a circulation of the energy flow\n\nExplanation: The correct answer is C) because the study mentions that the finite element method calculations show that the vorticity originates from the presence of nodal points where the phase of the field is undefined, leading to a circulation of the energy flow. This is a key finding in the study and is responsible for the emergence of surface plasmon vortex modes. The other options are not supported by the text and are therefore incorrect. Option A is incorrect because the study does not mention the presence of extrinsic orbital angular momentum in the incident light as the underlying mechanism. Option B is incorrect because while the elliptical shape of the nanohole does contribute to the rotational symmetry breaking, it is not the underlying mechanism responsible for the emergence of surface plasmon vortex modes. Option D is incorrect because the photonic spin Hall effect is mentioned as a counterpart to the observed phenomenon, but it is not the underlying mechanism responsible for the emergence of surface plasmon vortex modes."}, "53": {"documentation": {"title": "Observation and calibration strategies for large-scale multi-beam\n  velocity-resolved mapping of the [CII] emission in the Orion molecular cloud", "source": "R. Higgins, S. Kabanovic, C. Pabst, D. Teyssier, J. R. Goicoechea, O.\n  Berne, E. Chambers, M. Wolfire, S. Suri, C. Buchbender, Y. Okada, M. Mertens,\n  A. Parikka, R. Aladro, H. Richter, R. G\\\"usten, J. Stutzki, A.G.G.M. Tielens", "docs_id": "2106.15620", "section": ["astro-ph.IM", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Observation and calibration strategies for large-scale multi-beam\n  velocity-resolved mapping of the [CII] emission in the Orion molecular cloud. Context. The [CII] 158micron far-infrared fine-structure line is one of the dominant cooling lines of the star-forming interstellar medium (ISM). Hence [CII] emission originates in and thus can be used to trace a range of ISM processes. Velocity-resolved large-scale mapping of [CII] in star-forming regions provides a unique perspective of the kinematics of these regions and their interactions with the exciting source of radiation. Aims. We explore the scientific applications of large-scale mapping of velocity-resolved [CII] observations. With the [CII] observations, we investigate the effect of stellar feedback on the ISM. We present the details of observation, calibration, and data reduction using a heterodyne array receiver mounted on an airborne observatory. Results. A square-degree [CII] map with a spectral resolution of 0.3 km/s is presented. The scientific potential of this data is summarized with discussion of mechanical and radiative stellar feedback, filament tracing using [CII], [CII] opacity effects, [CII] and carbon recombination lines, and [CII] interaction with the large molecular cloud. The data quality and calibration is discussed in detail, and new techniques are presented to mitigate the effects of unavoidable instrument deficiencies (e.g. baseline stability) and thus to improve the data quality. A comparison with a smaller [CII] map taken with the Herschel/Heterodyne Instrument for the Far-Infrared (HIFI) spectrometer is presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary application of large-scale velocity-resolved [CII] mapping in star-forming regions, and how does it relate to the study of stellar feedback on the interstellar medium (ISM)?\n\nA) To study the kinematics of star-forming regions and their interactions with the exciting source of radiation, and to investigate the effect of stellar feedback on the ISM.\n\nB) To trace the filament structure of molecular clouds and to investigate the opacity effects of [CII] emission.\n\nC) To study the interaction between [CII] emission and the large molecular cloud, and to investigate the effects of carbon recombination lines on the ISM.\n\nD) To compare the data quality and calibration of [CII] observations taken with different instruments, such as the Herschel/Heterodyne Instrument for the Far-Infrared (HIFI) spectrometer.\n\nCorrect Answer: A) To study the kinematics of star-forming regions and their interactions with the exciting source of radiation, and to investigate the effect of stellar feedback on the ISM.\n\nExplanation: The question requires the test-taker to understand the main application of large-scale velocity-resolved [CII] mapping in star-forming regions, which is to study the kinematics of these regions and their interactions with the exciting source of radiation. Additionally, the test-taker needs to understand how this application relates to the study of stellar feedback on the ISM. The correct answer is supported by the text, which states that the [CII] observations are used to investigate the effect of stellar feedback on the ISM. The other options are incorrect because they focus on specific aspects of the [CII] mapping, such as filament tracing or opacity effects, rather than the primary application of the technique."}, "54": {"documentation": {"title": "An Advanced NCRF Linac Concept for a High Energy e$^+$e$^-$ Linear\n  Collider", "source": "Karl L. Bane, Timothy L. Barklow, Martin Breidenbach, Craig P.\n  Burkhart, Eric A. Fauve, Alysson R. Gold, Vincent Heloin, Zenghai Li, Emilio\n  A. Nanni, Mamdouh Nasr, Marco Oriunno, James McEwan Paterson, Michael E.\n  Peskin, Tor O. Raubenheimer, Sami G. Tantawi", "docs_id": "1807.10195", "section": ["physics.acc-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Advanced NCRF Linac Concept for a High Energy e$^+$e$^-$ Linear\n  Collider. We have explored a concept for an advanced Normal-Conducting Radio-Frequency (NCRF) C-band linear accelerator (linac) structure to achieve a high gradient, high power e$^+$e$^-$ linear collider in the TeV class. This design study represents the first comprehensive investigation for an emerging class of distributed coupling accelerator topology exploring nominal cavity geometries, frequency and temperature of operation. The structure features internal manifolds for distributing RF power separately to each cell, permitting the full structure geometry to be designed for high shunt impedance and low breakdown. Optimized within operational constraints, we find that it is advantageous for the structure to be cooled directly by liquid nitrogen (LN), further increasing the shunt impedance. A crucial part of this design process has been cost optimization, which is largely driven by the cost of peak RF power. The first operation of a distributed coupling structure at cryogenic temperatures and the nominal operating gradient 120 MeV/m is also presented, demonstrating the feasibility of achieving high-gradient performance with a cryogenically-cooled normal-conducting accelerating structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a cryogenically-cooled normal-conducting accelerating structure in a high-energy e$^+$e$^-$ linear collider, as demonstrated in the study on an Advanced NCRF Linac Concept?\n\nA) Reduced peak RF power requirements\nB) Increased shunt impedance and low breakdown\nC) Improved frequency stability and reduced frequency drift\nD) Enhanced thermal management and reduced heat dissipation\n\nCorrect Answer: B) Increased shunt impedance and low breakdown\n\nExplanation: The study highlights the importance of cooling the structure directly by liquid nitrogen (LN) to further increase the shunt impedance, which is a critical parameter for achieving high-gradient performance in a linear collider. This is a key advantage of the proposed design, as it enables the structure to be optimized for high shunt impedance and low breakdown, leading to improved performance and efficiency."}, "55": {"documentation": {"title": "Strings with Non-Relativistic Conformal Symmetry and Limits of the\n  AdS/CFT Correspondence", "source": "Troels Harmark, Jelle Hartong, Lorenzo Menculini, Niels A. Obers, Ziqi\n  Yan", "docs_id": "1810.05560", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strings with Non-Relativistic Conformal Symmetry and Limits of the\n  AdS/CFT Correspondence. We find a Polyakov-type action for strings moving in a torsional Newton-Cartan geometry. This is obtained by starting with the relativistic Polyakov action and fixing the momentum of the string along a non-compact null isometry. For a flat target space, we show that the world-sheet theory becomes the Gomis-Ooguri action. From a target space perspective these strings are non-relativistic but their world-sheet theories are still relativistic. We show that one can take a scaling limit in which also the world-sheet theory becomes non-relativistic with an infinite-dimensional symmetry algebra given by the Galilean conformal algebra. This scaling limit can be taken in the context of the AdS/CFT correspondence and we show that it is realized by the `Spin Matrix Theory' limits of strings on AdS$_5$ $\\times$ $S^5$. Spin Matrix theory arises as non-relativistic limits of the AdS/CFT correspondence close to BPS bounds. The duality between non-relativistic strings and Spin Matrix theory provides a holographic duality of its own and points towards a framework for more tractable holographic dualities whereby non-relativistic strings are dual to near BPS limits of the dual field theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the Polyakov-type action for strings moving in a torsional Newton-Cartan geometry and the Gomis-Ooguri action, and how does the scaling limit of the world-sheet theory relate to the Galilean conformal algebra?\n\nA) The Polyakov-type action is a relativistic action that reduces to the Gomis-Ooguri action in flat target space, and the scaling limit of the world-sheet theory leads to an infinite-dimensional symmetry algebra given by the Galilean conformal algebra.\n\nB) The Polyakov-type action is a non-relativistic action that reduces to the Gomis-Ooguri action in flat target space, and the scaling limit of the world-sheet theory leads to a finite-dimensional symmetry algebra given by the Lorentz conformal algebra.\n\nC) The Polyakov-type action is a non-relativistic action that reduces to the Gomis-Ooguri action in flat target space, and the scaling limit of the world-sheet theory leads to an infinite-dimensional symmetry algebra given by the Galilean conformal algebra, which is a limit of the AdS/CFT correspondence.\n\nD) The Polyakov-type action is a relativistic action that reduces to the Gomis-Ooguri action in flat target space, and the scaling limit of the world-sheet theory leads to a finite-dimensional symmetry algebra given by the Poincar\u00e9 conformal algebra.\n\nCorrect Answer: A) The Polyakov-type action is a relativistic action that reduces to the Gomis-Ooguri action in flat target space, and the scaling limit of the world-sheet theory leads to an infinite-dimensional symmetry algebra given by the Galilean conformal algebra.\n\nExplanation: The correct answer is A) because the Polyakov-type action is indeed a relativistic action that reduces to the Gomis-Ooguri action in flat target space. Additionally, the scaling limit of the world-sheet theory is shown to lead to an infinite-dimensional symmetry algebra given by the Galilean conformal algebra, which is a key result of the paper. The other options are incorrect because they either misstate the relationship between the Polyakov-type action and the Gomis-Ooguri action, or incorrectly describe the symmetry algebra obtained in the scaling limit."}, "56": {"documentation": {"title": "Second-Scale $^9\\text{Be}^+$ Spin Coherence in a Compact Penning Trap", "source": "Brian J. McMahon and Brian C. Sawyer", "docs_id": "2110.03053", "section": ["physics.atom-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Second-Scale $^9\\text{Be}^+$ Spin Coherence in a Compact Penning Trap. We report microwave spectroscopy of co-trapped $^9\\text{Be}^+$ and $^{40}\\text{Ca}^+$ within a compact permanent-magnet-based Penning ion trap. The trap is constructed with a reconfigurable array of NdFeB rings providing a 0.654 T magnetic field that is near the 0.6774-T magnetic-field-insensitive hyperfine transition in $^9\\text{Be}^+$. Performing Ramsey spectroscopy on this hyperfine transition, we demonstrate nuclear spin coherence with a contrast decay time of >1 s. The $^9\\text{Be}^+$ is sympathetically cooled by a Coulomb crystal of $^{40}\\text{Ca}^+$, which minimizes $^9\\text{Be}^+$ illumination and thus mitigates reactive loss. Introducing a unique high-magnetic-field optical detection scheme for $^{40}\\text{Ca}^+$, we perform spin state readout without a 729~nm shelving laser. We record a fractional trap magnetic field instability below 20 ppb (<13 nT) at 43 s of averaging time with no magnetic shielding and only passive thermal isolation. We discuss potential applications of this compact, reconfigurable Penning trap."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a Coulomb crystal of $^{40}\\text{Ca}^+$ to sympathetically cool $^9\\text{Be}^+$ in the compact Penning trap?\n\nA){The Coulomb crystal reduces the number of ions in the trap, minimizing reactive loss.}\nB){The Coulomb crystal provides a stable and uniform magnetic field, which is essential for the hyperfine transition.}\nC){The Coulomb crystal allows for the use of a reconfigurable array of NdFeB rings, enabling the creation of a 0.654 T magnetic field.}\nD){The Coulomb crystal minimizes the illumination of $^9\\text{Be}^+$, reducing the rate of reactive loss and enabling the demonstration of nuclear spin coherence with a contrast decay time of >1 s.}\n\n**Correct Answer:** D){The Coulomb crystal minimizes the illumination of $^9\\text{Be}^+$, reducing the rate of reactive loss and enabling the demonstration of nuclear spin coherence with a contrast decay time of >1 s.}\n\n**Explanation:** The correct answer is D) because the documentation states that the Coulomb crystal of $^{40}\\text{Ca}^+$ \"minimizes $^9\\text{Be}^+$ illumination and thus mitigates reactive loss.\" This is the primary advantage of using a Coulomb crystal to sympathetically cool $^9\\text{Be}^+$ in the compact Penning trap.\n\nThe other options are incorrect because:\n\nA) The Coulomb crystal does not reduce the number of ions in the trap, but rather provides a stable and uniform environment for the ions.\n\nB) The magnetic field provided by the NdFeB rings is not directly related to the Coulomb crystal.\n\nC) The reconfigurable array of NdFeB rings is a feature of the Penning trap, but not a direct result of using a Coulomb crystal."}, "57": {"documentation": {"title": "A Simple Explanation for Taxon Abundance Patterns", "source": "Johan Chu and Chris Adami (Caltech)", "docs_id": "physics/0002001", "section": ["physics.bio-ph", "nlin.AO", "physics.data-an", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Simple Explanation for Taxon Abundance Patterns. For taxonomic levels higher than species, the abundance distributions of number of subtaxa per taxon tend to approximate power laws, but often show strong deviationns from such a law. Previously, these deviations were attributed to finite-time effects in a continuous time branching process at the generic level. Instead, we describe here a simple discrete branching process which generates the observed distributions and find that the distribution's deviation from power-law form is not caused by disequilibration, but rather that it is time-independent and determined by the evolutionary properties of the taxa of interest. Our model predicts-with no free parameters-the rank-frequency distribution of number of families in fossil marine animal orders obtained from the fossil record. We find that near power-law distributions are statistically almost inevitable for taxa higher than species. The branching model also sheds light on species abundance patterns, as well as on links between evolutionary processes, self-organized criticality and fractals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary explanation for the deviation of taxon abundance patterns from power-law distributions at taxonomic levels higher than species, according to the study \"A Simple Explanation for Taxon Abundance Patterns\"?\n\nA) Finite-time effects in a continuous time branching process at the generic level\nB) Disequilibration of the branching process\nC) The evolutionary properties of the taxa of interest, which are time-independent\nD) The fossil record's sampling bias\n\nCorrect Answer: C) The evolutionary properties of the taxa of interest, which are time-independent\n\nExplanation: The study suggests that the deviation from power-law distributions is not caused by finite-time effects or disequilibration, but rather by the evolutionary properties of the taxa of interest, which are time-independent. This implies that the branching model is able to capture the underlying dynamics of taxon abundance patterns without relying on external factors such as time or sampling bias.\n\nCandidate A is incorrect because the study actually attributes the deviations to finite-time effects, not the other way around. Candidate B is incorrect because the study finds that disequilibration is not the cause of the deviations. Candidate D is incorrect because the study does not mention the fossil record's sampling bias as a factor in the deviations."}, "58": {"documentation": {"title": "Cross-denoising Network against Corrupted Labels in Medical Image\n  Segmentation with Domain Shift", "source": "Qinming Zhang, Luyan Liu, Kai Ma, Cheng Zhuo, Yefeng Zheng", "docs_id": "2006.10990", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross-denoising Network against Corrupted Labels in Medical Image\n  Segmentation with Domain Shift. Deep convolutional neural networks (DCNNs) have contributed many breakthroughs in segmentation tasks, especially in the field of medical imaging. However, \\textit{domain shift} and \\textit{corrupted annotations}, which are two common problems in medical imaging, dramatically degrade the performance of DCNNs in practice. In this paper, we propose a novel robust cross-denoising framework using two peer networks to address domain shift and corrupted label problems with a peer-review strategy. Specifically, each network performs as a mentor, mutually supervised to learn from reliable samples selected by the peer network to combat with corrupted labels. In addition, a noise-tolerant loss is proposed to encourage the network to capture the key location and filter the discrepancy under various noise-contaminant labels. To further reduce the accumulated error, we introduce a class-imbalanced cross learning using most confident predictions at the class-level. Experimental results on REFUGE and Drishti-GS datasets for optic disc (OD) and optic cup (OC) segmentation demonstrate the superior performance of our proposed approach to the state-of-the-art methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed \"peer-review strategy\" in the Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift?\n\n**A)** To reduce the impact of domain shift on the network's performance\n**B)** To improve the robustness of the network to corrupted labels\n**C)** To enhance the network's ability to capture key locations in medical images\n**D)** To increase the class imbalance in the training dataset\n\n**Correct Answer:** B) To improve the robustness of the network to corrupted labels\n\n**Explanation:** The peer-review strategy is designed to address the problems of corrupted labels and domain shift in medical image segmentation. By having each network perform as a mentor and being mutually supervised, the network learns from reliable samples selected by the peer network. This approach aims to improve the network's robustness to corrupted labels, which is a key challenge in medical image segmentation. The correct answer, B, highlights the primary goal of this strategy. The other options, while related to the topic, are not the primary goal of the peer-review strategy."}, "59": {"documentation": {"title": "The leverage effect and other stylized facts displayed by Bitcoin\n  returns", "source": "F.N.M. de Sousa Filho, J.N. Silva, M.A. Bertella and E. Brigatti", "docs_id": "2004.05870", "section": ["q-fin.ST", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The leverage effect and other stylized facts displayed by Bitcoin\n  returns. In this paper, we explore some stylized facts of the Bitcoin market using the BTC-USD exchange rate time series of historical intraday data from 2013 to 2020. Bitcoin presents some very peculiar idiosyncrasies, like the absence of macroeconomic fundamentals or connections with underlying assets or benchmarks, an asymmetry between demand and supply and the presence of inefficiency in the form of strong arbitrage opportunity. Nevertheless, all these elements seem to be marginal in the definition of the structural statistical properties of this virtual financial asset, which result to be analogous to general individual stocks or indices. In contrast, we find some clear differences, compared to fiat money exchange rates time series, in the values of the linear autocorrelation and, more surprisingly, in the presence of the leverage effect. We also explore the dynamics of correlations, monitoring the shifts in the evolution of the Bitcoin market. This analysis is able to distinguish between two different regimes: a stochastic process with weaker memory signatures and closer to Gaussianity between the Mt. Gox incident and the late 2015, and a dynamics with relevant correlations and strong deviations from Gaussianity before and after this interval."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the stylized facts of the Bitcoin market using the BTC-USD exchange rate time series of historical intraday data from 2013 to 2020. What are the key differences between the structural statistical properties of Bitcoin and fiat money exchange rates, and how do these differences manifest in the values of linear autocorrelation and the presence of the leverage effect?\n\n**A)** The structural statistical properties of Bitcoin are similar to those of individual stocks or indices, but with stronger memory signatures and deviations from Gaussianity. The leverage effect is absent in Bitcoin due to the presence of macroeconomic fundamentals and connections with underlying assets or benchmarks.\n\n**B)** The structural statistical properties of Bitcoin are analogous to those of fiat money exchange rates, with weaker memory signatures and closer to Gaussianity. The leverage effect is present in Bitcoin due to the absence of macroeconomic fundamentals and connections with underlying assets or benchmarks.\n\n**C)** The structural statistical properties of Bitcoin are distinct from those of individual stocks or indices, with stronger memory signatures and deviations from Gaussianity. The leverage effect is present in Bitcoin due to the presence of inefficiency in the form of strong arbitrage opportunities.\n\n**D)** The structural statistical properties of Bitcoin are similar to those of individual stocks or indices, but with weaker memory signatures and closer to Gaussianity. The leverage effect is present in Bitcoin due to the presence of inefficiency in the form of strong arbitrage opportunities.\n\n**Correct Answer:** D) The structural statistical properties of Bitcoin are similar to those of individual stocks or indices, but with weaker memory signatures and closer to Gaussianity. The leverage effect is present in Bitcoin due to the presence of inefficiency in the form of strong arbitrage opportunities.\n\n**Explanation:** The correct answer is D) because the paper states that \"all these elements seem to be marginal in the definition of the structural statistical properties of this virtual financial asset, which result to be analogous to general individual stocks or indices.\" Additionally, the paper notes that the presence of inefficiency in the form of strong arbitrage opportunities is a key factor in the presence of the leverage effect in Bitcoin."}}