{"0": {"documentation": {"title": "Collective response of nuclei: Comparison between experiments and\n  extended mean-field calculations", "source": "Denis Lacroix, Sakir Ayik and Philippe Chomaz", "docs_id": "nucl-th/0102045", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Collective response of nuclei: Comparison between experiments and\n  extended mean-field calculations. The giant monopole, dipole and quadrupole responses in $^{40}$Ca, $^{90}$Zr, $^{120}$Sn and $^{208}$Pb are investigated using linear response treatment based on a stochastic one-body transport theory. Effects of the coupling to low-lying surface modes (coherent mechanism) and the incoherent mechanism due to nucleon-nucleon collisions are included beyond the usual mean-field description. We emphasize the importance of both mechanism in the fragmentation and damping of giant resonance. Calculated spectra are compared with experiment in terms of percentage of Energy-Weighted Sum-Rules in various energy regions. We obtained reasonable agreement in all cases. A special attention as been given to the fragmentation of the Giant Quadrupole Resonance in calcium and lead. In particular, the equal splitting of the $2^{+}$ in $^{40}$Ca is correctly reproduced. In addition, the appearance of fine structure in the response $^{208}$Pb is partly described by the calculations in which the coherent mechanism play an important role."}, "answer": "Here is the difficult multi-form question based on the provided documentation:\n\nWhat are the key features of the stochastic one-body transport theory used in the linear response treatment to investigate the giant monopole, dipole, and quadrupole responses in certain nuclei, and how do the calculations account for the effects of low-lying surface modes and nucleon-nucleon collisions?\n\nA) The stochastic one-body transport theory is used to calculate the energy-weighted sum-rules in various energy regions, and the calculations account for the effects of low-lying surface modes and nucleon-nucleon collisions by including both mechanisms in the mean-field description.\n\nB) The stochastic one-body transport theory is used to investigate the giant monopole, dipole, and quadrupole responses in certain nuclei, but the calculations do not account for the effects of low-lying surface modes and nucleon-nucleon collisions.\n\nC) The stochastic one-body transport theory is used to calculate the energy-weighted sum-rules in various energy regions, and the calculations account for the effects of low-lying surface modes and nucleon-nucleon collisions by including only the incoherent mechanism.\n\nD) The stochastic one-body transport theory is used to investigate the giant monopole, dipole, and quadrupole responses in certain nuclei, but the calculations do not account for the effects of low-lying surface modes and nucleon-nucleon collisions, and instead rely solely on the mean-field description.\n\nCorrect Answer: A) The stochastic one-body transport theory is used to calculate the energy-weighted sum-rules in various energy regions, and the calculations account for the effects of low-lying surface modes and nucleon-nucleon collisions by including both mechanisms in the mean-field description.\n\nExplanation: The correct answer is A) because the documentation states that the stochastic one-body transport theory is used to calculate the energy-weighted sum-rules in various energy regions, and that the calculations account for the effects of low-lying surface modes and nucleon-nucleon collisions by including both mechanisms in the mean-field description. This is supported by the fact that the calculations obtained reasonable agreement with experiments in all cases, and that the coherent mechanism plays an important role in describing the fragmentation and damping of giant resonances."}, "1": {"documentation": {"title": "High-energy break-up of 6Li as a tool to study the Big-Bang\n  nucleosynthesis reaction 2H(alpha,gamma)6Li", "source": "F.Hammache, M.Heil, S.Typel, D.Galaviz, K.S\\\"ummerer, A.Coc, F.Uhlig,\n  F.Attallah, M.Caamano, D.Cortina, H.Geissel, M.Hellstr\\\"om, N.Iwasa,\n  J.Kiener, P.Koczon, B.Kohlmeyer, P.Mohr, E.Schwab, K.Schwarz, F.Sch\\\"umann,\n  P.Senger, O.Sorlin, V.Tatischeff, J.P.Thibaud, E.Vangioni, A.Wagner, and\n  W.Walus", "docs_id": "1011.6179", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-energy break-up of 6Li as a tool to study the Big-Bang\n  nucleosynthesis reaction 2H(alpha,gamma)6Li. The recently claimed observations of non-negligible amounts of 6Li in old halo stars have renewed interest in the Big-Bang Nucleosynthesis (BBN) of 6Li. One important ingredient in the predicted BBN abundance of 6Li is the low-energy 2H(alpha,gamma)6Li cross section. Up to now, the only available experimental result for this cross section showed an almost constant astrophysical S-factor below 400 keV, contrary to theoretical expectations. We report on a new measurement of the 2H(alpha,gamma)6Li reaction using the break-up of 6Li at 150 A MeV. Even though we cannot separate experimentally the Coulomb contribution from the nuclear one, we find clear evidence for Coulomb-nuclear interference by analyzing the scattering-angular distributions. This is in-line with our theoretical description which indicates a drop of the S_24-factor at low energies as predicted also by most other models. Consequently, we find even lower upper limits for the calculated primordial 6Li abundance than before."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary conclusion drawn from the experimental measurement of the 2H(alpha,gamma)6Li reaction using the break-up of 6Li at 150 A MeV, and how does it impact our understanding of the Big-Bang Nucleosynthesis (BBN) of 6Li?\n\nA) The experimental measurement confirms the theoretical expectation of a constant astrophysical S-factor below 400 keV, which would imply a higher primordial 6Li abundance.\n\nB) The experimental measurement provides evidence for Coulomb-nuclear interference, which leads to a drop in the S_24-factor at low energies, resulting in lower upper limits for the calculated primordial 6Li abundance.\n\nC) The experimental measurement fails to detect any significant effects due to Coulomb-nuclear interference, which would suggest that the predicted BBN abundance of 6Li remains unchanged.\n\nD) The experimental measurement is inconclusive, and further studies are needed to determine the impact on the BBN of 6Li.\n\nCorrect Answer: B) The experimental measurement provides evidence for Coulomb-nuclear interference, which leads to a drop in the S_24-factor at low energies, resulting in lower upper limits for the calculated primordial 6Li abundance.\n\nExplanation: The correct answer is B) because the experimental measurement shows clear evidence for Coulomb-nuclear interference, which is in line with the theoretical description. This interference leads to a drop in the S_24-factor at low energies, resulting in lower upper limits for the calculated primordial 6Li abundance. The other options are incorrect because they do not accurately reflect the conclusions drawn from the experimental measurement. Option A is incorrect because the experimental measurement contradicts the theoretical expectation of a constant astrophysical S-factor. Option C is incorrect because the experimental measurement does provide evidence for Coulomb-nuclear interference. Option D is incorrect because the experimental measurement does provide a clear conclusion about the impact on the BBN of 6Li."}, "2": {"documentation": {"title": "Robust Wireless Fingerprinting via Complex-Valued Neural Networks", "source": "Soorya Gopalakrishnan, Metehan Cekic, Upamanyu Madhow", "docs_id": "1905.09388", "section": ["eess.SP", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Wireless Fingerprinting via Complex-Valued Neural Networks. A \"wireless fingerprint\" which exploits hardware imperfections unique to each device is a potentially powerful tool for wireless security. Such a fingerprint should be able to distinguish between devices sending the same message, and should be robust against standard spoofing techniques. Since the information in wireless signals resides in complex baseband, in this paper, we explore the use of neural networks with complex-valued weights to learn fingerprints using supervised learning. We demonstrate that, while there are potential benefits to using sections of the signal beyond just the preamble to learn fingerprints, the network cheats when it can, using information such as transmitter ID (which can be easily spoofed) to artificially inflate performance. We also show that noise augmentation by inserting additional white Gaussian noise can lead to significant performance gains, which indicates that this counter-intuitive strategy helps in learning more robust fingerprints. We provide results for two different wireless protocols, WiFi and ADS-B, demonstrating the effectiveness of the proposed method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using complex-valued neural networks to learn wireless fingerprints, as proposed in the paper \"Robust Wireless Fingerprinting via Complex-Valued Neural Networks\"?\n\nA) They can learn fingerprints from only the preamble of the signal, making them more resistant to spoofing attacks.\nB) They can learn fingerprints from sections of the signal beyond just the preamble, but may artificially inflate performance by using transmitter ID information.\nC) They can learn fingerprints from sections of the signal beyond just the preamble, and can also improve performance by adding white Gaussian noise to the signal.\nD) They can only learn fingerprints from signals with a specific frequency range, and are not effective for other types of signals.\n\n**Correct Answer:** C) They can learn fingerprints from sections of the signal beyond just the preamble, and can also improve performance by adding white Gaussian noise to the signal.\n\n**Explanation:** The correct answer is C) because the paper demonstrates that using sections of the signal beyond just the preamble can lead to more robust fingerprints, and that adding white Gaussian noise can also improve performance. This is in contrast to candidate A, which is incorrect because the paper shows that relying only on the preamble can lead to performance issues. Candidate B is also incorrect because the paper shows that using transmitter ID information can artificially inflate performance, rather than being a genuine advantage. Candidate D is incorrect because the paper does not specify any limitations on the frequency range of the signal."}, "3": {"documentation": {"title": "Permutations on Weierstrass Prym eigenforms", "source": "Rodolfo Guti\\'errez-Romo and Angel Pardo", "docs_id": "2111.13638", "section": ["math.DS", "math.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Permutations on Weierstrass Prym eigenforms. Let $X \\in \\mathcal{H}(2)$ be a Veech surface of discriminant $D$ and let $G(X)$ be the permutation group induced by the affine group of $X$ on the set of Weierstrass points of $X$. We show that $G(X) \\cong \\mathrm{Dih}_4$ if $D \\equiv_{4} 0$, $G(X) \\cong \\mathrm{Dih}_5$ if $D \\equiv_{8} 5$, and $G(X) \\cong \\mathrm{Dih}_6$ if $D \\equiv_{8} 1$, where $\\mathrm{Dih}_n$ is the dihedral group of order $2n$. Thus, $G(X)$ is a weak invariant, as it can distinguish the residue class of $D$ mod $8$, but it cannot tell different spin invariants apart when $D \\equiv_{8} 1$. Moreover, we show that the same groups arise when we only consider the action of the parabolic elements of the Veech group of $X$ on the Weierstrass points. We prove a similar result in the Weierstrass Prym locus in genus three. Indeed, we show that the permutation group induced by the affine group on the set of fixed points for the Prym involution of a Weierstrass Prym eigenform of discriminant $D$ is isomorphic to $\\mathrm{Sym}_2$ when $D$ is an even quadratic residue modulo $16$, and it is isomorphic to $\\mathrm{Sym}_3$ otherwise. In genus three, the same group is also obtained when we consider only parabolic elements. Since the permutation action of the affine group on Prym fixed points is trivial in genus four and no Weierstrass Prym eigenforms exist for genus five or larger, we completely classify this action for all Weierstrass Prym eigenforms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $X \\in \\mathcal{H}(2)$ be a Veech surface of discriminant $D$ and let $G(X)$ be the permutation group induced by the affine group of $X$ on the set of Weierstrass points of $X$. What is the isomorphism type of $G(X)$ when $D \\equiv_{8} 0$, $D \\equiv_{8} 5$, and $D \\equiv_{8} 1$, respectively?\n\nA){$\\mathrm{Dih}_4$}\nB){$\\mathrm{Dih}_5$}\nC){$\\mathrm{Dih}_6$}\nD){$\\mathrm{Sym}_2$ or $\\mathrm{Sym}_3$}\n\nCorrect Answer: C){$\\mathrm{Dih}_6$}\n\nExplanation: According to the documentation, when $D \\equiv_{8} 0$, $G(X) \\cong \\mathrm{Dih}_4$, but this is not the case when $D \\equiv_{8} 0$. When $D \\equiv_{8} 1$, $G(X) \\cong \\mathrm{Dih}_6$. Therefore, the correct answer is C){$\\mathrm{Dih}_6$}."}, "4": {"documentation": {"title": "NMR parameters in alkali, alkaline earth and rare earth fluorides from\n  first principle calculations", "source": "Aymeric Sadoc, Monique Body, Christophe Legein, Mamata Biswal, Franck\n  Fayon, Xavier Rocquefelte, and Florent Boucher", "docs_id": "1105.1645", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "NMR parameters in alkali, alkaline earth and rare earth fluorides from\n  first principle calculations. 19F isotropic chemical shifts for alkali, alkaline earth and rare earth of column 3 basic fluorides are measured and the corresponding isotropic chemical shieldings are calculated using the GIPAW method. When using PBE exchange correlation functional for the treatment of the cationic localized empty orbitals of Ca2+, Sc3+ (3d) and La3+ (4f), a correction is needed to accurately calculate 19F chemical shieldings. We show that the correlation between experimental isotropic chemical shifts and calculated isotropic chemical shieldings established for the studied compounds allows to predict 19F NMR spectra of crystalline compounds with a relatively good accuracy. In addition, we experimentally determine the quadrupolar parameters of 25Mg in MgF2 and calculate the electric field gradient of 25Mg in MgF2 and 139La in LaF3 using both PAW and LAPW methods. The orientation of the EFG components in the crystallographic frame, provided by DFT calculations, is analysed in term of electron densities. It is shown that consideration of the quadrupolar charge deformation is essential for the analysis of slightly distorted environments or highly irregular polyhedra."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of using the PBE exchange correlation functional for calculating 19F chemical shieldings in cationic localized empty orbitals of Ca2+, Sc3+ (3d), and La3+ (4f) in fluorides?\n\n**A)** It does not account for electron correlation effects.\n**B)** It does not consider the effects of quadrupolar interactions.\n**C)** It requires a correction to accurately calculate 19F chemical shieldings.\n**D)** It is only applicable to alkali and alkaline earth fluorides.\n\n**Correct Answer:** C) It requires a correction to accurately calculate 19F chemical shieldings.\n\n**Explanation:** The documentation states that when using the PBE exchange correlation functional, a correction is needed to accurately calculate 19F chemical shieldings for cationic localized empty orbitals of Ca2+, Sc3+ (3d), and La3+ (4f). This suggests that the PBE functional is not sufficient on its own to provide accurate results, and a correction is necessary to overcome this limitation.\n\n**Candidate A)** is incorrect because electron correlation effects are not mentioned as a limitation of the PBE functional in the documentation.\n\n**Candidate B)** is incorrect because quadrupolar interactions are mentioned in the documentation, but not as the primary limitation of the PBE functional.\n\n**Candidate D)** is incorrect because the PBE functional is mentioned as being applicable to a broader range of fluorides, including rare earth fluorides, in the documentation."}, "5": {"documentation": {"title": "The Testing Multiplier: Fear vs Containment", "source": "Francesco Furno", "docs_id": "2012.03834", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Testing Multiplier: Fear vs Containment. I study the economic effects of testing during the outbreak of a novel disease. I propose a model where testing permits isolation of the infected and provides agents with information about the prevalence and lethality of the disease. Additional testing reduces the perceived lethality of the disease, but might increase the perceived risk of infection. As a result, more testing could increase the perceived risk of dying from the disease - i.e. \"stoke fear\" - and cause a fall in economic activity, despite improving health outcomes. Two main insights emerge. First, increased testing is beneficial to the economy and pays for itself if performed at a sufficiently large scale, but not necessarily otherwise. Second, heterogeneous risk perceptions across age-groups can have important aggregate consequences. For a SARS-CoV-2 calibration of the model, heterogeneous risk perceptions across young and old individuals mitigate GDP losses by 50% and reduce the death toll by 30% relative to a scenario in which all individuals have the same perceptions of risk."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Testing Multiplier: Fear vs Containment model suggests that increased testing can have both positive and negative effects on the economy. What is the correct interpretation of the model's finding that \"more testing could increase the perceived risk of dying from the disease - i.e. 'stoke fear' - and cause a fall in economic activity\"?\n\nA) Increased testing will always lead to a decrease in economic activity, regardless of the level of testing.\nB) The model suggests that increased testing can mitigate economic losses by reducing the perceived risk of infection, but not the perceived risk of death.\nC) The model indicates that increased testing can increase the perceived risk of death from the disease, but this effect is offset by the benefits of improved health outcomes, resulting in no net change in economic activity.\nD) The model shows that increased testing can reduce the perceived risk of death from the disease, but this effect is more pronounced among older individuals, leading to a 50% reduction in GDP losses and a 30% reduction in death tolls.\n\n**Correct Answer:** D) The model shows that increased testing can reduce the perceived risk of death from the disease, but this effect is more pronounced among older individuals, leading to a 50% reduction in GDP losses and a 30% reduction in death tolls.\n\n**Explanation:** The correct answer is D) because the model suggests that increased testing can reduce the perceived risk of death from the disease, but this effect is more pronounced among older individuals, leading to a 50% reduction in GDP losses and a 30% reduction in death tolls. This is because the model indicates that heterogeneous risk perceptions across age-groups can have important aggregate consequences, and in this case, the reduction in perceived risk of death among older individuals leads to a significant reduction in economic losses and death tolls."}, "6": {"documentation": {"title": "In-medium Production of Kaons at the Mean-Field Level", "source": "J. Schaffner, J. Bondorf (Niels Bohr Institute), I.N. Mishustin (Niels\n  Bohr Insitute & Kurchatov Institute)", "docs_id": "nucl-th/9607058", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "In-medium Production of Kaons at the Mean-Field Level. The in-medium mass and energy of kaons and antikaons are studied within the Relativistic Mean Field approach and compared with predictions from chiral models by taking care of kaon-nucleon scattering data. Implications for the subthreshold production of kaons and antikaons in heavy-ion collisions are discussed. We find only small corrections due to in-medium effects on the mean-field level for the relevant production processes for kaons. The production of kaons is even less favourable at high density due to repulsive vector interactions. We conclude that one has to go beyond mean-field approaches and take fluctuations and secondary production processes into account to explain the recently measured enhancement of kaon production at subthreshold energies. The situation is different for antikaons where in-medium effects strongly enhances their production rates. We also see strong in-medium modifications of the annihilation processes of antikaons and Lambda's which might be visible in flow measurements. At high density, we predict that the threshold energy for antikaon and Lambda production and annihilation become equal leading to similar numbers of antikaons and Lambda's in the dense zone of a relativistic heavy ion collision."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the main conclusion regarding the production of kaons and antikaons in heavy-ion collisions, and what implications does this have for the interpretation of recent experimental results?\n\nA) The production of kaons is significantly enhanced at high density due to attractive scalar interactions, leading to a large increase in kaon yields. This is consistent with recent experimental results.\n\nB) The production of antikaons is strongly suppressed at high density due to repulsive vector interactions, leading to a decrease in antikaon yields. This is consistent with recent experimental results.\n\nC) The production of kaons is only slightly affected by in-medium effects, and the recent experimental enhancement of kaon production at subthreshold energies cannot be explained by mean-field approaches. This is consistent with the findings of the study.\n\nD) The production of antikaons is significantly enhanced at high density due to attractive vector interactions, leading to a large increase in antikaon yields. This is consistent with recent experimental results.\n\nCorrect Answer: C) The production of kaons is only slightly affected by in-medium effects, and the recent experimental enhancement of kaon production at subthreshold energies cannot be explained by mean-field approaches. This is consistent with the findings of the study.\n\nExplanation: The study concludes that in-medium effects have only small corrections to the mean-field level for the production of kaons, and that the recent experimental enhancement of kaon production at subthreshold energies cannot be explained by mean-field approaches. This suggests that fluctuations and secondary production processes must be taken into account to explain the observed enhancement. In contrast, the study finds that in-medium effects strongly enhance the production rates of antikaons, and that the threshold energy for antikaon and Lambda production and annihilation become equal at high density."}, "7": {"documentation": {"title": "Fast-neutron induced background in LaBr3:Ce detectors", "source": "J. Kiener, V. Tatischeff, I. Deloncle, N. de S\\'er\\'eville, P.\n  Laurent, C. Blondel, M. Chabot, R. Chipaux, A. Coc, S. Dubos, A. Gostoji\\`c,\n  N. Goutev, C. Hamadache, F. Hammache, B. Horeau, O. Limousin, S. Ouichaoui,\n  G. Pr\\'evot, R. Rodr\\'iguez-Gas\\'en and M. S. Yavahchova", "docs_id": "1512.00305", "section": ["physics.ins-det", "astro-ph.IM", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast-neutron induced background in LaBr3:Ce detectors. The response of a scintillation detector with a cylindrical 1.5-inch LaBr3:Ce crystal to incident neutrons has been measured in the energy range En = 2-12 MeV. Neutrons were produced by proton irradiation of a Li target at Ep = 5-14.6 MeV with pulsed proton beams. Using the time-of-flight information between target and detector, energy spectra of the LaBr3:Ce detector resulting from fast neutron interactions have been obtained at 4 different neutron energies. Neutron-induced gamma rays emitted by the LaBr3:Ce crystal were also measured in a nearby Ge detector at the lowest proton beam energy. In addition, we obtained data for neutron irradiation of a large-volume high-purity Ge detector and of a NE-213 liquid scintillator detector, both serving as monitor detectors in the experiment. Monte-Carlo type simulations for neutron interactions in the liquid scintillator, the Ge and LaBr3:Ce crystals have been performed and compared with measured data. Good agreement being obtained with the data, we present the results of simulations to predict the response of LaBr3:Ce detectors for a range of crystal sizes to neutron irradiation in the energy range En = 0.5-10 MeV"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of using a large-volume high-purity Ge detector and a NE-213 liquid scintillator detector as monitor detectors in the experiment described in the Arxiv documentation?\n\n**A)** To measure the energy spectra of the LaBr3:Ce detector resulting from fast neutron interactions\n**B)** To simulate the neutron interactions in the LaBr3:Ce crystal and compare with measured data\n**C)** To monitor the neutron irradiation and provide a reference for the LaBr3:Ce detector response\n**D)** To measure the gamma rays emitted by the LaBr3:Ce crystal\n\n**Correct Answer:** C) To monitor the neutron irradiation and provide a reference for the LaBr3:Ce detector response\n\n**Explanation:** The large-volume high-purity Ge detector and the NE-213 liquid scintillator detector are used as monitor detectors to measure the neutron irradiation and provide a reference for the LaBr3:Ce detector response. This allows for the calibration and validation of the LaBr3:Ce detector response to neutron irradiation. The other options are incorrect because the LaBr3:Ce detector is the primary detector being measured, the Ge detector is used to measure gamma rays emitted by the LaBr3:Ce crystal, and the liquid scintillator detector is used to simulate neutron interactions.\n\n**Additional question:** What is the energy range of the neutron irradiation used in the experiment?\n\n**A)** 0.5-10 MeV\n**B)** 2-12 MeV\n**C)** 5-14.6 MeV\n**D)** 10-20 MeV\n\n**Correct Answer:** A) 0.5-10 MeV\n\n**Explanation:** The energy range of the neutron irradiation used in the experiment is 0.5-10 MeV, as stated in the introduction of the Arxiv documentation."}, "8": {"documentation": {"title": "Cosmogenic activation of silicon", "source": "R. Saldanha, R. Thomas, R.H.M. Tsang, A.E. Chavarria, R. Bunker, J.L.\n  Burnett, S.R. Elliott, A. Matalon, P. Mitra, A. Piers, P. Privitera, K.\n  Ramanathan, R. Smida", "docs_id": "2007.10584", "section": ["physics.ins-det", "astro-ph.CO", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cosmogenic activation of silicon. The production of $^{3}$H, $^{7}$Be, and $^{22}$Na by interactions of cosmic-ray particles with silicon can produce radioactive backgrounds in detectors used to search for rare events. Through controlled irradiation of silicon CCDs and wafers with a neutron beam that mimics the cosmic-ray neutron spectrum, followed by direct counting, we determined that the production rate from cosmic-ray neutrons at sea level is ($112 \\pm 24$) atoms/(kg day) for $^{3}$H, ($8.1 \\pm 1.9 $) atoms/(kg day) for $^{7}$Be, and ($43.0 \\pm 7.1 $) atoms/(kg day) for $^{22}$Na. Complementing these results with the current best estimates of activation cross sections for cosmic-ray particles other than neutrons, we obtain a total sea-level cosmic-ray production rate of ($124 \\pm 24$) atoms/(kg day) for $^{3}$H, ($9.4 \\pm 2.0 $) atoms/(kg day) for $^{7}$Be, and ($49.6 \\pm 7.3 $) atoms/(kg day) for $^{22}$Na. These measurements will help constrain background estimates and determine the maximum time that silicon-based detectors can remain unshielded during detector fabrication before cosmogenic backgrounds impact the sensitivity of next-generation rare-event searches."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study on cosmogenic activation of silicon report a total sea-level cosmic-ray production rate of ($124 \\pm 24$) atoms/(kg day) for $^{3}$H, ($9.4 \\pm 2.0 $) atoms/(kg day) for $^{7}$Be, and ($49.6 \\pm 7.3 $) atoms/(kg day) for $^{22}$Na. However, when complementing these results with the current best estimates of activation cross sections for cosmic-ray particles other than neutrons, the production rates for $^{3}$H and $^{7}$Be decrease, while the production rate for $^{22}$Na increases. What is the most likely explanation for this discrepancy?\n\nA) The activation cross sections for cosmic-ray particles other than neutrons are not well-constrained, leading to large uncertainties in the production rates.\nB) The authors' measurement of the production rate for $^{22}$Na is biased due to instrumental effects, while the other production rates are accurate.\nC) The production rates for $^{3}$H and $^{7}$Be are suppressed by the presence of other cosmic-ray particles, while the production rate for $^{22}$Na is enhanced by the presence of these particles.\nD) The discrepancy is due to a systematic error in the authors' measurement of the production rate for $^{22}$Na.\n\n**Correct Answer:** C) The production rates for $^{3}$H and $^{7}$Be are suppressed by the presence of other cosmic-ray particles, while the production rate for $^{22}$Na is enhanced by the presence of these particles.\n\n**Explanation:** The correct answer is based on the understanding that the production rates for $^{3}$H and $^{7}$Be are suppressed by the presence of other cosmic-ray particles, while the production rate for $^{22}$Na is enhanced by the presence of these particles. This is because the activation cross sections for cosmic-ray particles other than neutrons are not well-constrained, leading to large uncertainties in the production rates. The correct answer requires the student to analyze the data and understand the underlying physics of cosmogenic activation."}, "9": {"documentation": {"title": "Quantum Field Theories on Manifolds with Curved Boundaries: Scalar\n  Fields", "source": "D.M. McAvity and H. Osborn", "docs_id": "cond-mat/9206009", "section": ["cond-mat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum Field Theories on Manifolds with Curved Boundaries: Scalar\n  Fields. A framework allowing for perturbative calculations to be carried out for quantum field theories with arbitrary smoothly curved boundaries is described. It is based on an expansion of the heat kernel derived earlier for arbitrary mixed Dirichlet and Neumann boundary conditions. The method is applied to a general renormalisable scalar field theory in four dimensions using dimensional regularisation to two loops and expanding about arbitrary background fields. Detailed results are also specialised to an $O(n)$ symmetric model with a single coupling constant. Extra boundary terms are introduced into the action which give rise to either Dirichlet or generalised Neumann boundary conditions for the quantum fields. For plane boundaries the resulting renormalisation group functions are in accord with earlier results but here the additional terms depending on the extrinsic curvature of the boundary are found. Various consistency relations are also checked and the implications of conformal invariance at the critical point where the $\\beta$ function vanishes are also derived. The local Scr\\\"odinger equation for the wave functional defined by the functional integral under deformations of the boundary is also verified to two loops. Its consistency with the renormalisation group to all orders in perturbation theory is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the framework of Quantum Field Theories on Manifolds with Curved Boundaries: Scalar Fields, what is the physical interpretation of the extra boundary terms introduced into the action, and how do they affect the renormalisation group functions for the quantum fields?\n\nA) The extra boundary terms represent a non-trivial topological defect in the boundary, which affects the renormalisation group functions by introducing an additional scale-dependent factor.\n\nB) The extra boundary terms are a consequence of the dimensional regularisation procedure, and they do not affect the renormalisation group functions, which remain unchanged.\n\nC) The extra boundary terms give rise to a non-zero extrinsic curvature of the boundary, which affects the renormalisation group functions by introducing a non-trivial dependence on the boundary geometry.\n\nD) The extra boundary terms are a manifestation of the conformal invariance of the theory at the critical point, and they do not affect the renormalisation group functions, which remain unchanged.\n\nCorrect Answer: C) The extra boundary terms give rise to a non-zero extrinsic curvature of the boundary, which affects the renormalisation group functions by introducing a non-trivial dependence on the boundary geometry.\n\nExplanation: The extra boundary terms introduced into the action are a consequence of the generalised Neumann boundary conditions for the quantum fields. These terms give rise to a non-zero extrinsic curvature of the boundary, which affects the renormalisation group functions by introducing a non-trivial dependence on the boundary geometry. This is evident from the fact that the resulting renormalisation group functions are in accord with earlier results for plane boundaries, but also depend on the extrinsic curvature of the boundary."}, "10": {"documentation": {"title": "Mean-performance of Sharp Restart II: Inequality Roadmap", "source": "Iddo Eliazar and Shlomi Reuveni", "docs_id": "2102.13154", "section": ["cond-mat.stat-mech", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mean-performance of Sharp Restart II: Inequality Roadmap. Restarting a deterministic process always impedes its completion. However, it is known that restarting a random process can also lead to an opposite outcome -- expediting completion. Hence, the effect of restart is contingent on the underlying statistical heterogeneity of the process' completion times. To quantify this heterogeneity we bring a novel approach to restart: the methodology of inequality indices, which is widely applied in economics and in the social sciences to measure income and wealth disparity. Using this approach we establish an `inequality roadmap' for the mean-performance of sharp restart: a whole new set of universal inequality criteria that determine when restart with sharp timers (i.e. with fixed deterministic timers) decreases/increases mean completion. The criteria are based on a host of inequality indices including Bonferroni, Gini, Pietra, and other Lorenz-curve indices; each index captures a different angle of the restart-inequality interplay. Utilizing the fact that sharp restart can match the mean-performance of any general restart protocol, we prove -- with unprecedented precision and resolution -- the validity of the following statement: restart impedes/expedites mean completion when the underlying statistical heterogeneity is low/high."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Mean-performance of Sharp Restart II: Inequality Roadmap, what is the primary advantage of using the methodology of inequality indices to quantify the effect of restart on a random process?\n\n**A)** It allows for the estimation of the expected completion time of the process.\n**B)** It provides a universal framework for comparing the performance of different restart protocols.\n**C)** It enables the identification of the underlying statistical heterogeneity of the process' completion times.\n**D)** It facilitates the calculation of the mean-performance of sharp restart.\n\n**Correct Answer:** C) It enables the identification of the underlying statistical heterogeneity of the process' completion times.\n\n**Explanation:** The correct answer is C) because the methodology of inequality indices is used to quantify the statistical heterogeneity of the process' completion times, which is a key factor in determining the effect of restart on the process. The other options are incorrect because they do not accurately capture the primary advantage of using inequality indices in this context. Option A is incorrect because it refers to a different aspect of process completion. Option B is incorrect because while inequality indices do provide a framework for comparing restart protocols, this is not their primary advantage in this context. Option D is incorrect because it refers to a different aspect of process performance."}, "11": {"documentation": {"title": "Roberge-Weiss transitions at different center symmetry breaking patterns\n  in a $\\mathbb{Z}_{3}$-QCD model", "source": "Xiu-Fei Li and Zhao Zhang", "docs_id": "1812.01373", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Roberge-Weiss transitions at different center symmetry breaking patterns\n  in a $\\mathbb{Z}_{3}$-QCD model. We study how the Roberge-Weiss (RW) transition depends on the pattern of center symmetry breaking using a $\\mathbb{Z}_{3}$-QCD model. We adopt flavor-dependent quark imaginary chemical potentials, namely $(\\mu_{u},\\mu_{d},\\mu_{s})/iT=(\\theta-2\\pi{C}/3,\\,\\theta,\\,\\theta+2\\pi{C}/3)$ with $C\\in[0,1]$. The RW periodicity is guaranteed and the center symmetry of $\\mathbb{Z}_{3}$-QCD is explicitly broken when $C\\neq{1}$ or/and quark masses are non-degenerate. For $N_{f}=3$ and $C\\neq{1}$, the RW transition occurs at $\\theta=\\theta_{RW}=(2k+1)\\pi/3\\,(k\\in\\mathbb{Z})$, which becomes stronger with decrease of $C$. When $C={1}$, the $\\theta_{RW}$ turns into $2k\\pi/3$ for $N_{f}=2+1$, but keeps $(2k+1)\\pi/3$ for $N_{f}=1+2$; in both cases, the RW transitions get stronger with the mass mismatch. For other $C\\neq{0}$ cases, the $\\theta_{RW}$'s are not integral multiples of $\\pi/3$. We find that the RW transition is more sensitive to the deviation of $C$ from one compared to the mass non-degeneracy and thus the strength of the traditional RW transition with $C=0$ is the strongest. The nature of RW endpoints and its implications to deconfinement transition are investigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the Roberge-Weiss transition and the center symmetry breaking pattern in the $\\mathbb{Z}_{3}$-QCD model, and how does it depend on the value of $C$?\n\nA) The RW transition is guaranteed to occur at $\\theta=\\theta_{RW}=(2k+1)\\pi/3$ for all values of $C$, and its strength increases with the mass non-degeneracy.\n\nB) The RW transition is more sensitive to the deviation of $C$ from one compared to the mass non-degeneracy, and its strength increases with the decrease of $C$.\n\nC) The RW transition occurs at $\\theta=\\theta_{RW}=(2k+1)\\pi/3$ for $N_{f}=3$ and $C\\neq{1}$, but its strength is unaffected by the value of $C$.\n\nD) The RW transition is only guaranteed to occur at $\\theta=\\theta_{RW}=(2k+1)\\pi/3$ when $C=0$, and its strength increases with the mass non-degeneracy.\n\nCorrect Answer: B) The RW transition is more sensitive to the deviation of $C$ from one compared to the mass non-degeneracy, and its strength increases with the decrease of $C$."}, "12": {"documentation": {"title": "Feature Exploration for Knowledge-guided and Data-driven Approach Based\n  Cuffless Blood Pressure Measurement", "source": "Xiaorong Ding, Bryan P Yan, Yuan-Ting Zhang, Jing Liu, Peng Su and Ni\n  Zhao", "docs_id": "1908.10245", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Feature Exploration for Knowledge-guided and Data-driven Approach Based\n  Cuffless Blood Pressure Measurement. This study explores extended feature space that is indicative of blood pressure (BP) changes for better estimation of continuous BP in an unobtrusive way. A total of 222 features were extracted from noninvasively acquired electrocardiogram (ECG) and photoplethysmogram (PPG) signals with the subject undergoing coronary angiography and/or percutaneous coronary intervention, during which intra-arterial BP was recorded simultaneously with the subject at rest and while administering drugs to induce BP variations. The association between the extracted features and the BP components, i.e. systolic BP (SBP), diastolic BP (DBP), mean BP (MBP), and pulse pressure (PP) were analyzed and evaluated in terms of correlation coefficient, cross sample entropy, and mutual information, respectively. Results show that the most relevant indicator for both SBP and MBP is the pulse full width at half maximum, and for DBP and PP, the amplitude between the peak of the first derivative of PPG (dPPG) to the valley of the second derivative of PPG (sdPPG) and the time interval between the peak of R wave and the sdPPG, respectively. As potential inputs to either the knowledge-guided model or data-driven method for cuffless BP calibration, the proposed expanded features are expected to improve the estimation accuracy of cuffless BP."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary feature that is most relevant for estimating systolic blood pressure (SBP) and mean blood pressure (MBP) in the proposed study on cuffless blood pressure measurement?\n\n**A)** Amplitude between the peak of the first derivative of PPG (dPPG) and the valley of the second derivative of PPG (sdPPG)\n**B)** Pulse full width at half maximum\n**C)** Time interval between the peak of R wave and the sdPPG\n**D)** Mean of the extracted features from ECG and PPG signals\n\n**Correct Answer:** B) Pulse full width at half maximum\n\n**Explanation:** The study found that the pulse full width at half maximum is the most relevant indicator for both SBP and MBP. This feature is indicative of blood pressure changes and can be used to improve the estimation accuracy of cuffless blood pressure. The other options are not the primary feature for estimating SBP and MBP, although they may be relevant for estimating diastolic blood pressure (DBP) and pulse pressure (PP)."}, "13": {"documentation": {"title": "The Dynamics of Group Codes: Dual Abelian Group Codes and Systems", "source": "G. David Forney Jr. and Mitchell D. Trott", "docs_id": "cs/0408038", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Dynamics of Group Codes: Dual Abelian Group Codes and Systems. Fundamental results concerning the dynamics of abelian group codes (behaviors) and their duals are developed. Duals of sequence spaces over locally compact abelian groups may be defined via Pontryagin duality; dual group codes are orthogonal subgroups of dual sequence spaces. The dual of a complete code or system is finite, and the dual of a Laurent code or system is (anti-)Laurent. If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp. The controllability properties of C are the observability properties of C^\\perp. In particular, C is (strongly) controllable if and only if C^\\perp is (strongly) observable, and the controller memory of C is the observer memory of C^\\perp. The controller granules of C act as the character groups of the observer granules of C^\\perp. Examples of minimal observer-form encoder and syndrome-former constructions are given. Finally, every observer granule of C is an \"end-around\" controller granule of C."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a sequence space over a locally compact abelian group G, and let C be a complete code. What is the relationship between the state spaces of C and the character groups of the state spaces of its dual code C^\\perp?\n\nA) The state spaces of C are isomorphic to the character groups of the state spaces of C^\\perp.\nB) The state spaces of C are isomorphic to the character groups of the state spaces of C^\\perp, but only if C is strongly controllable.\nC) The state spaces of C act as the character groups of the state spaces of C^\\perp, but the converse is not necessarily true.\nD) The state spaces of C are not related to the character groups of the state spaces of C^\\perp.\n\n**Correct Answer:** C) The state spaces of C act as the character groups of the state spaces of C^\\perp, but the converse is not necessarily true.\n\n**Explanation:**\n\nThe correct answer is C) because the documentation states that \"If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp.\" This means that the state spaces of C are related to the character groups of the state spaces of C^\\perp, but it does not imply that the converse is true. In other words, the character groups of the state spaces of C^\\perp do not necessarily act as the state spaces of C.\n\nThe incorrect answers are:\n\nA) This answer is incorrect because it implies a stronger relationship between the state spaces of C and the character groups of the state spaces of C^\\perp than what is stated in the documentation.\n\nB) This answer is incorrect because it introduces an additional condition (strong controllability) that is not mentioned in the documentation.\n\nD) This answer is incorrect because it states that there is no relationship between the state spaces of C and the character groups of the state spaces of C^\\perp, which is not supported by the documentation."}, "14": {"documentation": {"title": "A lower prevalence for recessive disorders in a random mating population\n  is a transient phenomenon during and after a growth phase", "source": "Luis A. La Rocca, Julia Frank, Heidi Beate Bentzen, Jean-Tori Pantel,\n  Konrad Gerischer, Anton Bovier and Peter M. Krawitz", "docs_id": "2012.04968", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A lower prevalence for recessive disorders in a random mating population\n  is a transient phenomenon during and after a growth phase. Despite increasing data from population-wide sequencing studies, the risk for recessive disorders in consanguineous partnerships is still heavily debated. An important aspect that has not sufficiently been investigated theoretically, is the influence of inbreeding on mutation load and incidence rates when the population sizes change. We therefore developed a model to study these dynamics for a wide range of growth and mating conditions. In the phase of population expansion and shortly afterwards, our simulations show that there is a drop of diseased individuals at the expense of an increasing mutation load for random mating, while both parameters remain almost constant in highly consanguineous partnerships. This explains the empirical observation in present times that a high degree of consanguinity is associated with an increased risk of autosomal recessive disorders. However, it also states that the higher frequency of severe recessive disorders with developmental delay in inbred populations is a transient phenomenon before a mutation-selection balance is reached again."}, "answer": "Here is the exam question:\n\nWhat is the primary conclusion drawn from the study on the influence of inbreeding on mutation load and incidence rates in populations undergoing growth and mating conditions, and how does it relate to the observed empirical data on autosomal recessive disorders?\n\nA) The study suggests that consanguineous partnerships lead to a higher mutation load and increased incidence rates of recessive disorders, which is consistent with the empirical data.\nB) The study indicates that inbreeding has a negligible effect on mutation load and incidence rates, and that consanguineous partnerships do not increase the risk of autosomal recessive disorders.\nC) The study reveals that the drop in diseased individuals in random mating populations is a permanent phenomenon, whereas in consanguineous partnerships, both parameters remain constant, leading to a higher risk of autosomal recessive disorders.\nD) The study demonstrates that the mutation load and incidence rates of recessive disorders are inversely related to the degree of consanguinity, with highly consanguineous partnerships leading to a lower risk of autosomal recessive disorders.\n\nCorrect Answer: A) The study suggests that consanguineous partnerships lead to a higher mutation load and increased incidence rates of recessive disorders, which is consistent with the empirical data.\n\nExplanation: The study concludes that in random mating populations, there is a drop in diseased individuals at the expense of an increasing mutation load, while in highly consanguineous partnerships, both parameters remain almost constant. This suggests that consanguineous partnerships lead to a higher mutation load and increased incidence rates of recessive disorders, which is consistent with the empirical data."}, "15": {"documentation": {"title": "TASI lectures on cosmological observables and string theory", "source": "Eva Silverstein", "docs_id": "1606.03640", "section": ["hep-th", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TASI lectures on cosmological observables and string theory. These lectures provide an updated pedagogical treatment of the theoretical structure and phenomenology of some basic mechanisms for inflation, along with an overview of the structure of cosmological uplifts of holographic duality. A full treatment of the problem requires `ultraviolet completion' because of the sensitivity of inflation to quantum gravity effects, including back reaction and non-adiabatic production of heavy degrees of freedom. Cosmological observations imply accelerated expansion of the late universe, and provide increasingly precise constraints and discovery potential on the amplitude and shape of primordial tensor and scalar perturbations, and some of their correlation functions. Most backgrounds of string theory have positive potential energy, with a rich but still highly constrained landscape of solutions. The theory contains novel mechanisms for inflation, some subject to significant observational tests. Although the detailed ultraviolet completion is not accessible experimentally, some of these mechanisms directly stimulate a more systematic analysis of the space of low energy theories and signatures relevant for analysis of data, which is sensitive to physics orders of magnitude above the energy scale of inflation as a result of long time evolution (dangerous irrelevance) and the substantial amount of data. Portions of these lectures appeared previously in Les Houches 2013, \"Post-Planck Cosmology\" ."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation for the need for \"ultraviolet completion\" in the study of inflation, and how does it relate to the sensitivity of inflation to quantum gravity effects?\n\nA) The need for ultraviolet completion arises from the requirement to match the amplitude and shape of primordial tensor and scalar perturbations with cosmological observations, which impose increasingly precise constraints on the theory.\n\nB) The primary motivation for ultraviolet completion is the requirement to account for the non-adiabatic production of heavy degrees of freedom, which is a significant source of back reaction in the early universe.\n\nC) The need for ultraviolet completion is driven by the observation that most backgrounds of string theory have positive potential energy, leading to a rich but highly constrained landscape of solutions.\n\nD) The primary motivation for ultraviolet completion is the requirement to systematically analyze the space of low-energy theories and signatures relevant for analysis of data, which is sensitive to physics orders of magnitude above the energy scale of inflation.\n\nCorrect Answer: B) The primary motivation for ultraviolet completion is the requirement to account for the non-adiabatic production of heavy degrees of freedom, which is a significant source of back reaction in the early universe.\n\nExplanation: The correct answer is B) because the documentation states that the sensitivity of inflation to quantum gravity effects, including back reaction and non-adiabatic production of heavy degrees of freedom, requires an \"ultraviolet completion\" of the theory. This is a key motivation for the need for such a completion, as it allows for a more accurate understanding of the early universe and the effects of quantum gravity on inflation. The other options are incorrect because they do not accurately capture the primary motivation for ultraviolet completion. Option A is incorrect because while the amplitude and shape of primordial tensor and scalar perturbations are constrained by cosmological observations, this is not the primary motivation for ultraviolet completion. Option C is incorrect because the observation that most backgrounds of string theory have positive potential energy is related to the rich but highly constrained landscape of solutions, but it is not the primary motivation for ultraviolet completion. Option D is incorrect because while systematic analysis of the space of low-energy theories and signatures is relevant for analysis of data, it is not the primary motivation for ultraviolet completion."}, "16": {"documentation": {"title": "Multivariate risk measures: a constructive approach based on selections", "source": "Ignacio Cascos and Ilya Molchanov", "docs_id": "1301.1496", "section": ["q-fin.RM", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multivariate risk measures: a constructive approach based on selections. Since risky positions in multivariate portfolios can be offset by various choices of capital requirements that depend on the exchange rules and related transaction costs, it is natural to assume that the risk measures of random vectors are set-valued. Furthermore, it is reasonable to include the exchange rules in the argument of the risk measure and so consider risk measures of set-valued portfolios. This situation includes the classical Kabanov's transaction costs model, where the set-valued portfolio is given by the sum of a random vector and an exchange cone, but also a number of further cases of additional liquidity constraints. We suggest a definition of the risk measure based on calling a set-valued portfolio acceptable if it possesses a selection with all individually acceptable marginals. The obtained selection risk measure is coherent (or convex), law invariant and has values being upper convex closed sets. We describe the dual representation of the selection risk measure and suggest efficient ways of approximating it from below and from above. In case of Kabanov's exchange cone model, it is shown how the selection risk measure relates to the set-valued risk measures considered by Kulikov (2008), Hamel and Heyde (2010), and Hamel, Heyde and Rudloff (2013)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a multivariate portfolio with a set-valued risk measure defined as the selection of a portfolio with individually acceptable marginals. Suppose we have two exchange cone models: Kabanov's model and Hamel, Heyde, and Rudloff's model. Which of the following statements about the selection risk measure is true?\n\nA) The selection risk measure is law-invariant and has values that are upper convex closed sets, but it is not coherent.\nB) The selection risk measure is coherent and has values that are upper convex closed sets, but it is not law-invariant.\nC) The selection risk measure is both law-invariant and coherent, and its values are upper convex closed sets.\nD) The selection risk measure is neither law-invariant nor coherent, and its values are not upper convex closed sets.\n\n**Correct Answer:** C) The selection risk measure is both law-invariant and coherent, and its values are upper convex closed sets.\n\n**Explanation:**\n\nThe selection risk measure is defined as the selection of a portfolio with individually acceptable marginals. This definition implies that the selection risk measure is coherent, as it is based on the selection of a portfolio with acceptable marginals. Additionally, the selection risk measure is law-invariant, as it does not depend on the underlying distribution of the random vector. The values of the selection risk measure are upper convex closed sets, as they are defined as the set of all selections of portfolios with individually acceptable marginals.\n\nThe correct answer, C, is the only option that accurately reflects the properties of the selection risk measure. Options A and B are incorrect, as the selection risk measure is both law-invariant and coherent. Option D is also incorrect, as the selection risk measure has values that are upper convex closed sets."}, "17": {"documentation": {"title": "Maximum Roaming Multi-Task Learning", "source": "Lucas Pascal and Pietro Michiardi and Xavier Bost and Benoit Huet and\n  Maria A. Zuluaga", "docs_id": "2006.09762", "section": ["cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Maximum Roaming Multi-Task Learning. Multi-task learning has gained popularity due to the advantages it provides with respect to resource usage and performance. Nonetheless, the joint optimization of parameters with respect to multiple tasks remains an active research topic. Sub-partitioning the parameters between different tasks has proven to be an efficient way to relax the optimization constraints over the shared weights, may the partitions be disjoint or overlapping. However, one drawback of this approach is that it can weaken the inductive bias generally set up by the joint task optimization. In this work, we present a novel way to partition the parameter space without weakening the inductive bias. Specifically, we propose Maximum Roaming, a method inspired by dropout that randomly varies the parameter partitioning, while forcing them to visit as many tasks as possible at a regulated frequency, so that the network fully adapts to each update. We study the properties of our method through experiments on a variety of visual multi-task data sets. Experimental results suggest that the regularization brought by roaming has more impact on performance than usual partitioning optimization strategies. The overall method is flexible, easily applicable, provides superior regularization and consistently achieves improved performances compared to recent multi-task learning formulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed Maximum Roaming method in multi-task learning, and how does it differ from traditional partitioning optimization strategies?\n\nA) Maximum Roaming improves performance by reducing the number of tasks, while traditional partitioning strategies focus on optimizing individual tasks.\nB) Maximum Roaming provides superior regularization, allowing the network to adapt to each task at a regulated frequency, whereas traditional partitioning strategies rely on disjoint or overlapping partitions.\nC) Maximum Roaming is more computationally efficient than traditional partitioning strategies, as it reduces the number of parameters to optimize.\nD) Maximum Roaming is less effective than traditional partitioning strategies, as it introduces unnecessary randomness into the parameter partitioning process.\n\nCorrect Answer: B) Maximum Roaming provides superior regularization, allowing the network to adapt to each task at a regulated frequency, whereas traditional partitioning strategies rely on disjoint or overlapping partitions.\n\nExplanation: The correct answer, B, highlights the key advantage of Maximum Roaming, which is its ability to provide superior regularization by allowing the network to adapt to each task at a regulated frequency. This is in contrast to traditional partitioning strategies, which may rely on disjoint or overlapping partitions that can weaken the inductive bias. The other options are incorrect because they do not accurately capture the primary advantage of Maximum Roaming. Option A is incorrect because Maximum Roaming does not reduce the number of tasks, but rather allows the network to adapt to each task at a regulated frequency. Option C is incorrect because Maximum Roaming does not necessarily reduce the computational complexity of the optimization process. Option D is incorrect because Maximum Roaming is actually more effective than traditional partitioning strategies, not less effective."}, "18": {"documentation": {"title": "Physics-inspired analysis of the two-class income distribution in the\n  USA in 1983-2018", "source": "Danial Ludwig and Victor M. Yakovenko", "docs_id": "2110.03140", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Physics-inspired analysis of the two-class income distribution in the\n  USA in 1983-2018. The first part of this paper is a brief survey of the approaches to economic inequality based on ideas from statistical physics and kinetic theory. These include the Boltzmann kinetic equation, the time-reversal symmetry, the ergodicity hypothesis, entropy maximization, and the Fokker-Planck equation. The origins of the exponential Boltzmann-Gibbs distribution and the Pareto power law are discussed in relation to additive and multiplicative stochastic processes. The second part of the paper analyzes income distribution data in the USA for the time period 1983-2018 using a two-class decomposition. We present overwhelming evidence that the lower class (more than 90% of the population) is described by the exponential distribution, whereas the upper class (about 4% of the population in 2018) by the power law. We show that the significant growth of inequality during this time period is due to the sharp increase in the upper-class income share, whereas relative inequality within the lower class remains constant. We speculate that the expansion of the upper-class population and income shares may be due to increasing digitization and non-locality of the economy in the last 40 years."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper analyze the income distribution in the USA from 1983 to 2018 using a two-class decomposition. According to the paper, what is the primary distribution that describes the lower class (more than 90% of the population) in the USA during this time period?\n\nA) Exponential distribution\nB) Power law distribution\nC) Pareto distribution\nD) Lognormal distribution\n\n**Correct Answer:** A) Exponential distribution\n\n**Explanation:** The paper states that \"the lower class (more than 90% of the population) is described by the exponential distribution\". This suggests that the majority of the population in the lower class follows an exponential distribution, which is characterized by a constant rate of increase in income.\n\n**Note:** The other options are incorrect because the paper specifically states that the upper class (about 4% of the population in 2018) is described by the power law distribution, not the lower class. The Pareto distribution is mentioned in the context of the origins of the power law, but it is not the distribution that describes the lower class. The lognormal distribution is not mentioned at all in the paper."}, "19": {"documentation": {"title": "Measuring Systemic Risk: Robust Ranking Techniques Approach", "source": "Amirhossein Sadoghi", "docs_id": "1503.06317", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measuring Systemic Risk: Robust Ranking Techniques Approach. In this research, we introduce a robust metric to identify Systemically Important Financial Institution (SIFI) in a financial network by taking into account both common idiosyncratic shocks and contagion through counterparty exposures. We develop an efficient algorithm to rank financial institutions by formulating a fixed point problem and reducing it to a non-smooth convex optimization problem. We then study the underlying distribution of the proposed metric and analyze the performance of the algorithm by using different financial network structures. Overall, our findings suggest that the level of interconnection and position of institutions in the financial network are important elements to measure systemic risk and identify SIFIs. Results show that increasing the levels of out- and in-degree connections of an institution can have a diverse impact on its systemic ranking. Additionally, on the empirical side, we investigate the factors which lead to the identification of Global Systemic Important Banks (G-SIB) by using a panel dataset of the largest banks in each country. Our empirical results supports the main findings of the theoretical model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the research paper \"Measuring Systemic Risk: Robust Ranking Techniques Approach\", what is the primary goal of the proposed algorithm for ranking financial institutions by systemic risk?\n\n**A)** To identify the most interconnected institutions in the financial network\n**B)** To formulate a fixed point problem and reduce it to a non-smooth convex optimization problem\n**C)** To analyze the performance of the algorithm using different financial network structures\n**D)** To study the underlying distribution of the proposed metric and identify Systemically Important Financial Institutions (SIFIs)\n\n**Correct Answer:** D) To study the underlying distribution of the proposed metric and identify Systemically Important Financial Institutions (SIFIs)\n\n**Explanation:** The correct answer is D) because the research paper aims to introduce a robust metric to identify SIFIs in a financial network, taking into account both common idiosyncratic shocks and contagion through counterparty exposures. The proposed algorithm is a tool to achieve this goal, and the study of the underlying distribution of the metric is essential to understand how it works and how it can be used to identify SIFIs.\n\nThe other options are incorrect because:\n\n* A) While the algorithm does take into account the level of interconnection in the financial network, this is not its primary goal.\n* B) Formulating a fixed point problem and reducing it to a non-smooth convex optimization problem is a methodological aspect of the algorithm, but not its primary goal.\n* C) Analyzing the performance of the algorithm using different financial network structures is an important aspect of the research, but not the primary goal of the algorithm itself."}, "20": {"documentation": {"title": "Diffusive process under Lifshitz scaling and pandemic scenarios", "source": "M.A. Anacleto, F.A. Brito, A.R. de Queiroz, E. Passos, J.R.L. Santos", "docs_id": "2005.03506", "section": ["q-bio.PE", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diffusive process under Lifshitz scaling and pandemic scenarios. We here propose to model active and cumulative cases data from COVID-19 by a continuous effective model based on a modified diffusion equation under Lifshitz scaling with a dynamic diffusion coefficient. The proposed model is rich enough to capture different aspects of a complex virus diffusion as humanity has been recently facing. The model being continuous it is bound to be solved analytically and/or numerically. So, we investigate two possible models where the diffusion coefficient associated with possible types of contamination are captured by some specific profiles. The active cases curves here derived were able to successfully describe the pandemic behavior of Germany and Spain. Moreover, we also predict some scenarios for the evolution of COVID-19 in Brazil. Furthermore, we depicted the cumulative cases curves of COVID-19, reproducing the spreading of the pandemic between the cities of S\\~ao Paulo and S\\~ao Jos\\'e dos Campos, Brazil. The scenarios also unveil how the lockdown measures can flatten the contamination curves. We can find the best profile of the diffusion coefficient that better fit the real data of pandemic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Diffusive process under Lifshitz scaling and pandemic scenarios\" propose a continuous effective model to model active and cumulative cases data from COVID-19. The model is based on a modified diffusion equation under Lifshitz scaling with a dynamic diffusion coefficient. The authors investigate two possible models where the diffusion coefficient is captured by specific profiles.\n\n**A)** The authors assume that the diffusion coefficient is constant throughout the pandemic, and its value is determined by the initial conditions of the outbreak.\n\n**B)** The authors propose a model where the diffusion coefficient is a function of the population density, with a higher value in densely populated areas.\n\n**C)** The authors use a non-local diffusion equation, where the diffusion coefficient is a function of the spatial distance between the infected and healthy individuals.\n\n**D)** The authors assume that the diffusion coefficient is a function of the time of the pandemic, with a higher value in the early stages of the outbreak.\n\n**Correct Answer:** B) The authors propose a model where the diffusion coefficient is a function of the population density, with a higher value in densely populated areas.\n\n**Explanation:** The correct answer is B) because the authors mention that they investigate a model where the diffusion coefficient is captured by a specific profile, which is related to the population density. This is evident from the statement \"The proposed model is rich enough to capture different aspects of a complex virus diffusion as humanity has been recently facing\" and the fact that they use a dynamic diffusion coefficient. The other options are incorrect because they do not accurately reflect the authors' proposal. Option A is incorrect because the authors do not assume a constant diffusion coefficient. Option C is incorrect because the authors do not use a non-local diffusion equation. Option D is incorrect because the authors do not assume that the diffusion coefficient is a function of time."}, "21": {"documentation": {"title": "Linear Equilibria for Dynamic LQG Games with Asymmetric Information and\n  Dependent Types", "source": "Nasimeh Heydaribeni and Achilleas Anastasopoulos", "docs_id": "1909.04834", "section": ["econ.GN", "cs.SY", "eess.SY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Linear Equilibria for Dynamic LQG Games with Asymmetric Information and\n  Dependent Types. We consider a non-zero-sum linear quadratic Gaussian (LQG) dynamic game with asymmetric information. Each player observes privately a noisy version of a (hidden) state of the world $V$, resulting in dependent private observations. We study perfect Bayesian equilibria (PBE) for this game with equilibrium strategies that are linear in players' private estimates of $V$. The main difficulty arises from the fact that players need to construct estimates on other players' estimate on $V$, which in turn would imply that an infinite hierarchy of estimates on estimates needs to be constructed, rendering the problem unsolvable. We show that this is not the case: each player's estimate on other players' estimates on $V$ can be summarized into her own estimate on $V$ and some appropriately defined public information. Based on this finding we characterize the PBE through a backward/forward algorithm akin to dynamic programming for the standard LQG control problem. Unlike the standard LQG problem, however, Kalman filter covariance matrices, as well as some other required quantities, are observation-dependent and thus cannot be evaluated off-line through a forward recursion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Linear Equilibria for Dynamic LQG Games with Asymmetric Information and Dependent Types paper, what is the key insight that allows the construction of perfect Bayesian equilibria (PBE) for the game, despite the presence of asymmetric information and dependent private observations?\n\n**A)** The players can perfectly estimate the hidden state of the world $V$ using their private observations.\n\n**B)** Each player's estimate on other players' estimates on $V$ can be summarized into her own estimate on $V$ and some appropriately defined public information.\n\n**C)** The game can be reduced to a standard LQG control problem by ignoring the asymmetric information and dependent private observations.\n\n**D)** The PBE can be characterized through a forward recursion, similar to the standard LQG control problem.\n\n**Correct Answer:** B) Each player's estimate on other players' estimates on $V$ can be summarized into her own estimate on $V$ and some appropriately defined public information.\n\n**Explanation:** The correct answer is B) because the paper shows that each player's estimate on other players' estimates on $V$ can be condensed into her own estimate on $V$ and some public information, allowing the construction of PBE. This insight is crucial in resolving the difficulty of constructing estimates on other players' estimates on $V$, which would otherwise require an infinite hierarchy of estimates. The other options are incorrect because they do not accurately reflect the key insight from the paper. Option A is incorrect because the players do not perfectly estimate the hidden state of the world $V$. Option C is incorrect because the game cannot be reduced to a standard LQG control problem due to the asymmetric information and dependent private observations. Option D is incorrect because the PBE is characterized through a backward/forward algorithm, not a forward recursion."}, "22": {"documentation": {"title": "Calorimetric classification of track-like signatures in liquid argon\n  TPCs using MicroBooNE data", "source": "MicroBooNE collaboration: P. Abratenko, R. An, J. Anthony, J. Asaadi,\n  A. Ashkenazi, S. Balasubramanian, B. Baller, C. Barnes, G. Barr, V. Basque,\n  L. Bathe-Peters, O. Benevides Rodrigues, S. Berkman, A. Bhanderi, A. Bhat, M.\n  Bishai, A. Blake, T. Bolton, L. Camilleri, D. Caratelli, I. Caro Terrazas, R.\n  Castillo Fernandez, F. Cavanna, G. Cerati, Y. Chen, E. Church, D. Cianci,\n  J.M. Conrad, M. Convery, L. Cooper-Troendle, J.I. Crespo-Anadon, M. Del\n  Tutto, S.R. Dennis, D. Devitt, R. Diurba, R. Dorrill, K. Duffy, S. Dytman, B.\n  Eberly, A. Ereditato, J.J. Evans, R. Fine, G.A. Fiorentini Aguirre, R.S.\n  Fitzpatrick, B.T. Fleming, N. Foppiani, D. Franco, A.P. Furmanski, D.\n  Garcia-Gamez, S. Gardiner, G. Ge, S. Gollapinni, O. Goodwin, E. Gramellini,\n  P. Green, H. Greenlee, W. Gu, R. Guenette, P. Guzowski, L. Hagaman, E. Hall,\n  P. Hamilton, O. Hen, G.A. Horton-Smith, A. Hourlier, R. Itay, C. James, X.\n  Ji, L. Jiang, J.H. Jo, R.A. Johnson, Y.J. Jwa, N. Kamp, N. Kaneshige, G.\n  Karagiorgi, W. Ketchum, M. Kirby, T. Kobilarcik, I. Kreslo, R. LaZur, I.\n  Lepetic, K. Li, Y. Li, K. Lin, B.R. Littlejohn, W.C. Louis, X. Luo, K.\n  Manivannan, C. Mariani, D. Marsden, J. Marshall, D.A. Martinez Caicedo, K.\n  Mason, A. Mastbaum, N. McConkey, V. Meddage, T. Mettler, K. Miller, J. Mills,\n  K. Mistry, T. Mohayai, A. Mogan, J. Moon, M. Mooney, A.F. Moor, C.D. Moore,\n  L. Mora Lepin, J. Mousseau, M. Murphy, D. Naples, A. Navrer-Agasson, R.K.\n  Neely, J. Nowak, M. Nunes, O. Palamara, V. Paolone, A. Papadopoulou, V.\n  Papavassiliou, S.F. Pate, A. Paudel, Z. Pavlovic, E. Piasetzky, I.\n  Ponce-Pinto, S. Prince, X. Qian, J.L. Raaf, V. Radeka, A. Rafique, M.\n  Reggiani-Guzzo, L. Ren, L.C.J. Rice, L. Rochester, J. Rodriguez Rondon, H.E.\n  Rogers, M. Rosenberg, M. Ross-Lonergan, G. Scanavini, D.W. Schmitz, A.\n  Schukraft, W. Seligman, M.H. Shaevitz, R. Sharankova, J. Sinclair, A. Smith,\n  E.L. Snider, M. Soderberg, S. Soldner-Rembold, P. Spentzouris, J. Spitz, M.\n  Stancari, J. St. John, T. Strauss, K. Sutton, S. Sword-Fehlberg, A.M. Szelc,\n  N. Tagg, W. Tang, K. Terao, C.Thorpe, D. Totani, M. Toups, Y.-T. Tsai, M.A.\n  Uchida, T. Usher, W. Van De Pontseele, B. Viren, M. Weber, H. Wei, Z.\n  Williams, S. Wolbers, T. Wongjirad, M. Wospakrik, N. Wright, W. Wu, E.\n  Yandel, T. Yang, G. Yarbrough, L.E. Yates, G.P. Zeller, J. Zennamo, C. Zhang", "docs_id": "2109.02460", "section": ["physics.ins-det", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Calorimetric classification of track-like signatures in liquid argon\n  TPCs using MicroBooNE data. The MicroBooNE liquid argon time projection chamber located at Fermilab is a neutrino experiment dedicated to the study of short-baseline oscillations, the measurements of neutrino cross sections in liquid argon, and to the research and development of this novel detector technology. Accurate and precise measurements of calorimetry are essential to the event reconstruction and are achieved by leveraging the TPC to measure deposited energy per unit length along the particle trajectory, with mm resolution. We describe the non-uniform calorimetric reconstruction performance in the detector, showing dependence on the angle of the particle trajectory. Such non-uniform reconstruction directly affects the performance of the particle identification algorithms which infer particle type from calorimetric measurements. This work presents a new particle identification method which accounts for and effectively addresses such non-uniformity. The newly developed method shows improved performance compared to previous algorithms, illustrated by a 94% proton selection efficiency and a 10% muon mis-identification rate, with a fairly loose selection of tracks performed on beam data. The performance is further demonstrated by identifying exclusive final states in $\\nu_{\\mu} CC$ interactions. While developed using MicroBooNE data and simulation, this method is easily applicable to future LArTPC experiments, such as SBND, ICARUS, and DUNE."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The MicroBooNE liquid argon time projection chamber is a neutrino experiment that uses a Time Projection Chamber (TPC) to measure deposited energy per unit length along the particle trajectory. What is the primary advantage of the newly developed particle identification method that accounts for the non-uniform calorimetric reconstruction performance in the detector?\n\nA) It provides a 100% proton selection efficiency and a 0% muon mis-identification rate.\nB) It shows improved performance compared to previous algorithms, with a 94% proton selection efficiency and a 10% muon mis-identification rate.\nC) It is only applicable to future LArTPC experiments, such as SBND, ICARUS, and DUNE, and not to MicroBooNE data.\nD) It is a simple and straightforward method that does not require any complex simulations or data analysis.\n\n**Correct Answer:** B) It shows improved performance compared to previous algorithms, with a 94% proton selection efficiency and a 10% muon mis-identification rate.\n\n**Explanation:** The correct answer is B) because the question asks about the primary advantage of the newly developed particle identification method. The method is described as showing improved performance compared to previous algorithms, with specific numbers provided for proton selection efficiency and muon mis-identification rate. This indicates that the method has a clear benefit over previous approaches, making option B the correct answer. Options A, C, and D are incorrect because they either exaggerate or misrepresent the information provided in the documentation."}, "23": {"documentation": {"title": "Stability analysis of financial contagion due to overlapping portfolios", "source": "Fabio Caccioli, Munik Shrestha, Cristopher Moore, J. Doyne Farmer", "docs_id": "1210.5987", "section": ["q-fin.GN", "cs.SI", "physics.soc-ph", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability analysis of financial contagion due to overlapping portfolios. Common asset holdings are widely believed to have been the primary vector of contagion in the recent financial crisis. We develop a network approach to the amplification of financial contagion due to the combination of overlapping portfolios and leverage, and we show how it can be understood in terms of a generalized branching process. By studying a stylized model we estimate the circumstances under which systemic instabilities are likely to occur as a function of parameters such as leverage, market crowding, diversification, and market impact. Although diversification may be good for individual institutions, it can create dangerous systemic effects, and as a result financial contagion gets worse with too much diversification. Under our model there is a critical threshold for leverage; below it financial networks are always stable, and above it the unstable region grows as leverage increases. The financial system exhibits \"robust yet fragile\" behavior, with regions of the parameter space where contagion is rare but catastrophic whenever it occurs. Our model and methods of analysis can be calibrated to real data and provide simple yet powerful tools for macroprudential stress testing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between diversification and financial contagion in the context of overlapping portfolios and leverage. How does the model presented in the study suggest that diversification can create systemic effects, and what are the implications of this finding for macroprudential stress testing?\n\n**A)** The model suggests that diversification is a key factor in reducing financial contagion, as it allows institutions to spread risk across multiple assets and reduce their exposure to any one particular market. However, the study also shows that excessive diversification can lead to systemic effects, as it can create a \"crowded\" market where institutions are forced to hold similar assets, increasing the likelihood of contagion.\n\n**B)** The model indicates that diversification has no significant impact on financial contagion, as institutions can simply adjust their portfolios to reduce their exposure to any one particular market. However, the study also shows that leverage can exacerbate the effects of diversification, leading to a higher likelihood of systemic instability.\n\n**C)** The model suggests that diversification can actually increase financial contagion, as institutions may be forced to hold similar assets in order to maintain their diversification benefits. This can lead to a \"crowded\" market where institutions are more likely to be affected by any one particular market, increasing the likelihood of systemic instability.\n\n**D)** The model indicates that the relationship between diversification and financial contagion is complex and depends on various factors, including leverage and market crowding. The study suggests that institutions should aim to strike a balance between diversification and risk management in order to minimize the risk of systemic instability.\n\n**Correct Answer:** C) The model suggests that diversification can actually increase financial contagion, as institutions may be forced to hold similar assets in order to maintain their diversification benefits. This can lead to a \"crowded\" market where institutions are more likely to be affected by any one particular market, increasing the likelihood of systemic instability.\n\n**Explanation:** This answer choice is correct because the study suggests that excessive diversification can lead to a \"crowded\" market where institutions are forced to hold similar assets, increasing the likelihood of contagion. This is in line with the model's finding that diversification can create systemic effects, particularly when combined with leverage. The other answer choices do not accurately capture the relationship between diversification and financial contagion as described in the study."}, "24": {"documentation": {"title": "A Lipschitz Matrix for Parameter Reduction in Computational Science", "source": "Jeffrey M. Hokanson and Paul G. Constantine", "docs_id": "1906.00105", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Lipschitz Matrix for Parameter Reduction in Computational Science. We introduce the Lipschitz matrix: a generalization of the scalar Lipschitz constant for functions with many inputs. Among the Lipschitz matrices compatible a particular function, we choose the smallest such matrix in the Frobenius norm to encode the structure of this function. The Lipschitz matrix then provides a function-dependent metric on the input space. Altering this metric to reflect a particular function improves the performance of many tasks in computational science. Compared to the Lipschitz constant, the Lipschitz matrix reduces the worst-case cost of approximation, integration, and optimization; if the Lipschitz matrix is low-rank, this cost no longer depends on the dimension of the input, but instead on the rank of the Lipschitz matrix defeating the curse of dimensionality. Both the Lipschitz constant and matrix define uncertainty away from point queries of the function and by using the Lipschitz matrix we can reduce uncertainty. If we build a minimax space-filling design of experiments in the Lipschitz matrix metric, we can further reduce this uncertainty. When the Lipschitz matrix is approximately low-rank, we can perform parameter reduction by constructing a ridge approximation whose active subspace is the span of the dominant eigenvectors of the Lipschitz matrix. In summary, the Lipschitz matrix provides a new tool for analyzing and performing parameter reduction in complex models arising in computational science."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Lipschitz matrix, what is the primary advantage of using a minimax space-filling design of experiments in the Lipschitz matrix metric, and how does it relate to reducing uncertainty in parameter estimation?\n\n**A)** A minimax space-filling design of experiments in the Lipschitz matrix metric allows for more efficient exploration of the input space, leading to improved performance in tasks such as optimization and integration. However, it does not necessarily reduce uncertainty.\n\n**B)** By using a minimax space-filling design of experiments in the Lipschitz matrix metric, the uncertainty in parameter estimation can be reduced, as the design is optimized to minimize the maximum possible error in the estimation of the function's behavior.\n\n**C)** The minimax space-filling design of experiments in the Lipschitz matrix metric is primarily used to reduce the worst-case cost of approximation, but it does not directly impact uncertainty in parameter estimation.\n\n**D)** A minimax space-filling design of experiments in the Lipschitz matrix metric is not a relevant concept in the context of the Lipschitz matrix, and its use does not affect uncertainty in parameter estimation.\n\n**Correct Answer:** B) By using a minimax space-filling design of experiments in the Lipschitz matrix metric, the uncertainty in parameter estimation can be reduced, as the design is optimized to minimize the maximum possible error in the estimation of the function's behavior.\n\n**Explanation:** The correct answer is B) because the minimax space-filling design of experiments in the Lipschitz matrix metric is specifically designed to reduce uncertainty in parameter estimation by minimizing the maximum possible error in the estimation of the function's behavior. This is achieved by optimizing the design to explore the input space in a way that minimizes the worst-case error, which in turn reduces uncertainty in parameter estimation. The other options are incorrect because they either misrepresent the primary advantage of the minimax space-filling design or fail to acknowledge its relationship to uncertainty reduction."}, "25": {"documentation": {"title": "Oscillatory motion of a droplet in an active poroelastic two-phase model", "source": "Dirk Alexander Kulawiak, Jakob L\\\"ober, Markus B\\\"ar, and Harald Engel", "docs_id": "1803.00337", "section": ["cond-mat.soft", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Oscillatory motion of a droplet in an active poroelastic two-phase model. We investigate flow-driven amoeboid motility as exhibited by microplasmodia of Physarum polycephalum. A poroelastic two-phase model with rigid boundaries is extended to the case of free boundaries and substrate friction. The cytoskeleton is modeled as an active viscoelastic solid permeated by a fluid phase describing the cytosol. A feedback loop between a chemical regulator, active mechanical deformations, and induced flows gives rise to oscillatory and irregular motion accompanied by spatio-temporal contraction patterns. We cover extended parameter regimes of active tension and substrate friction by numerical simulations in one spatial dimension and reproduce experimentally observed oscillation periods and amplitudes. In line with experiments, the model predicts alternating forward and backward ectoplasmatic flow at the boundaries with reversed flow in the center. However, for all cases of periodic and irregular motion, we observe practically no net motion. A simple theoretical argument shows that directed motion is not possible with a spatially independent substrate friction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the oscillatory motion of a droplet in an active poroelastic two-phase model, as described in the given Arxiv documentation. What is the primary reason for the observed lack of net motion, despite the presence of oscillatory and irregular motion patterns?\n\n**A)** The model's assumption of a spatially independent substrate friction leads to a cancellation of opposing forces, resulting in no net motion.\n\n**B)** The feedback loop between the chemical regulator, active mechanical deformations, and induced flows creates a self-sustaining oscillatory pattern, but the substrate friction prevents the droplet from moving in a net direction.\n\n**C)** The cytoskeleton's active viscoelastic properties and the fluid phase's permeability lead to a complex interplay of forces, resulting in oscillatory motion, but the rigid boundaries and substrate friction prevent the droplet from moving in a net direction.\n\n**D)** The model's parameter regimes of active tension and substrate friction are not sufficient to overcome the opposing forces, resulting in no net motion.\n\n**Correct Answer:** A) The model's assumption of a spatially independent substrate friction leads to a cancellation of opposing forces, resulting in no net motion.\n\n**Explanation:** The correct answer is A) because the documentation states that \"A simple theoretical argument shows that directed motion is not possible with a spatially independent substrate friction.\" This suggests that the model's assumption of a spatially independent substrate friction leads to a cancellation of opposing forces, resulting in no net motion. The other options are incorrect because they do not accurately reflect the explanation provided in the documentation. Option B is incorrect because the documentation does not mention a self-sustaining oscillatory pattern. Option C is incorrect because the documentation does not mention the cytoskeleton's active viscoelastic properties as a primary factor in the lack of net motion. Option D is incorrect because the documentation does not suggest that the parameter regimes of active tension and substrate friction are insufficient to overcome opposing forces."}, "26": {"documentation": {"title": "Positrons in Surface Physics", "source": "Christoph Hugenschmidt", "docs_id": "1611.04430", "section": ["cond-mat.mtrl-sci", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Positrons in Surface Physics. Within the last decade powerful methods have been developed to study surfaces using bright low-energy positron beams. These novel analysis tools exploit the unique properties of positron interaction with surfaces, which comprise the absence of exchange interaction, repulsive crystal potential and positron trapping in delocalized surface states at low energies. By applying reflection high-energy positron diffraction (RHEPD) one can benefit from the phenomenon of total reflection below a critical angle that is not present in electron surface diffraction. Therefore, RHEPD allows the determination of the atom positions of (reconstructed) surfaces with outstanding accuracy. The main advantages of positron annihilation induced Auger-electron spectroscopy (PAES) are the missing secondary electron background in the energy region of Auger-transitions and its topmost layer sensitivity for elemental analysis. In order to enable the investigation of the electron polarization at surfaces low-energy spin-polarized positrons are used to probe the outermost electrons of the surface. Furthermore, in fundamental research the preparation of well defined surfaces tailored for the production of bound leptonic systems plays an outstanding role. In this report, it is envisaged to cover both, the fundamental aspects of positron surface interaction and the present status of surface studies using modern positron beam techniques."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using reflection high-energy positron diffraction (RHEPD) in surface physics research?\n\nA) The ability to probe the outermost electrons of the surface with high energy\nB) The determination of the atom positions of (reconstructed) surfaces with outstanding accuracy\nC) The detection of secondary electrons in the energy region of Auger-transitions\nD) The analysis of elemental composition in the topmost layer of the surface\n\nCorrect Answer: B) The determination of the atom positions of (reconstructed) surfaces with outstanding accuracy\n\nExplanation: According to the documentation, RHEPD allows for the determination of the atom positions of (reconstructed) surfaces with outstanding accuracy due to the phenomenon of total reflection below a critical angle that is not present in electron surface diffraction. This is a key advantage of using RHEPD in surface physics research.\n\nCandidate A is incorrect because while RHEPD can probe the outermost electrons of the surface, it is not the primary advantage of this technique. Candidate C is incorrect because PAES (positron annihilation induced Auger-electron spectroscopy) is a different technique that has its own advantages, not RHEPD. Candidate D is incorrect because while RHEPD can provide information about the topmost layer of the surface, it is not the primary advantage of this technique."}, "27": {"documentation": {"title": "Spin 1/2^+, spin 3/2^+ and transition magnetic moments of low lying and\n  charmed baryons", "source": "Neetika Sharma, Harleen Dahiya, P.K. Chatley and Manmohan Gupta", "docs_id": "1003.4338", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin 1/2^+, spin 3/2^+ and transition magnetic moments of low lying and\n  charmed baryons. Magnetic moments of the low lying and charmed spin 1/2^+ and spin 3/2^+ baryons have been calculated in the SU(4) chiral constituent quark model (\\chiCQM) by including the contribution from c \\bar c fluctuations. Explicit calculations have been carried out for the contribution coming from the valence quarks, \"quark sea\" polarizations and their orbital angular momentum. The implications of such a model have also been studied for magnetic moments of the low lying spin 3/2^+ \\to 1/2^+ and 1/2^+ \\to 1/2^+ transitions as well as the transitions involving charmed baryons. We are able to achieve an excellent agreement with data for the case of low lying spin 1/2^+ baryons and spin 3/2^+ baryons. For the spin 1/2^+ and spin 3/2^+ charmed baryon magnetic moments, our results are consistent with the predictions of the QCD sum rules, Light Cone sum rules and Spectral sum rules. For the cases where \"light\" quarks dominate in the valence structure, the sea and orbital contributions are found to be fairly significant however, they cancel in the right direction to give the correct magnitude of the total magnetic moment. On the other hand, when there is an excess of \"heavy\" quarks, the contribution of the \"quark sea\" is almost negligible. The effects of configuration mixing and quark masses have also been investigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the SU(4) chiral constituent quark model (\u03c7CQM), what is the implication of the \"quark sea\" polarization contribution on the magnetic moments of low-lying and charmed baryons, and how does it cancel with the orbital angular momentum contribution in the case of \"light\" quark dominance?\n\n**A)** The \"quark sea\" polarization contribution is always significant and leads to an increase in the magnetic moment, regardless of the quark composition.\n\n**B)** The \"quark sea\" polarization contribution is negligible when there is an excess of \"heavy\" quarks, but becomes significant when \"light\" quarks dominate.\n\n**C)** The \"quark sea\" polarization contribution cancels with the orbital angular momentum contribution in all cases, resulting in a zero net magnetic moment.\n\n**D)** The \"quark sea\" polarization contribution is significant in the case of low-lying spin 1/2^+ and spin 3/2^+ baryons, but becomes negligible in the case of charmed baryons.\n\n**Correct Answer:** B) The \"quark sea\" polarization contribution is negligible when there is an excess of \"heavy\" quarks, but becomes significant when \"light\" quarks dominate.\n\n**Explanation:** According to the text, when there is an excess of \"heavy\" quarks, the contribution of the \"quark sea\" is almost negligible. On the other hand, when \"light\" quarks dominate in the valence structure, the sea and orbital contributions are found to be fairly significant, but they cancel in the right direction to give the correct magnitude of the total magnetic moment. Therefore, option B is the correct answer."}, "28": {"documentation": {"title": "NuSTAR Observations of the Magnetar 1E 2259+586", "source": "Julia K. Vogel (1), Romain Hasco\u007fet (2), Victoria M. Kaspi (3),\n  Hongjun An (3), Robert Archibald (3), Andrei M. Beloborodov (2), Steven E.\n  Boggs (4), Finn E. Christensen (5), William W. Craig (1 and 4), Eric V.\n  Gotthelf (2), Brian W. Grefenstette (6), Charles J. Hailey (2), Fiona A.\n  Harrison (6), Jamie A. Kennea (7), Kristin K. Madsen (6), Michael J.\n  Pivovaroff (1), Daniel Stern (8), and William W. Zhang (9) ((1) Physics\n  Division, Physical and Life Sciences Directorate, Lawrence Livermore National\n  Laboratory, Livermore, CA, USA, (2) Physics Department and Columbia\n  Astrophysics Laboratory, Columbia University, New York, NY, USA, (3)\n  Department of Physics, McGill University, Montreal, Quebec, Canada, (4) Space\n  Sciences Laboratory, University of California, Berkeley, CA, USA, (5) DTU\n  Space, National Space Institute, Technical University of Denmark, Lyngby,\n  Denmark, (6) Cahill Center for Astronomy and Astrophysics, California\n  Institute of Technology, Pasadena, CA, USA, (7) Department of Astronomy and\n  Astrophysics, Pennsylvania State University, University Park, PA, USA, (8)\n  Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA,\n  USA, (9) Goddard Space Flight Center, Greenbelt, MD, USA)", "docs_id": "1408.0768", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "NuSTAR Observations of the Magnetar 1E 2259+586. We report on new broad band spectral and temporal observations of the magnetar 1E 2259+586, which is located in the supernova remnant CTB 109. Our data were obtained simultaneously with the Nuclear Spectroscopic Telescope Array (NuSTAR) and Swift, and cover the energy range from 0.5-79 keV. We present pulse profiles in various energy bands and compare them to previous RXTE results. The NuSTAR data show pulsations above 20 keV for the first time and we report evidence that one of the pulses in the double-peaked pulse profile shifts position with energy. The pulsed fraction of the magnetar is shown to increase strongly with energy. Our spectral analysis reveals that the soft X-ray spectrum is well characterized by an absorbed double-blackbody or blackbody plus power-law model in agreement with previous reports. Our new hard X-ray data, however, suggests that an additional component, such as a power-law, is needed to describe the NuSTAR and Swift spectrum. We also fit the data with the recently developed coronal outflow model by Beloborodov for hard X-ray emission from magnetars. The outflow from a ring on the magnetar surface is statistically preferred over outflow from a polar cap."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the energy dependence of the pulsed fraction of the magnetar 1E 2259+586, as inferred from the NuSTAR observations?\n\nA) The pulsed fraction decreases with increasing energy.\nB) The pulsed fraction remains constant across the energy range.\nC) The pulsed fraction increases strongly with energy.\nD) The pulsed fraction is independent of energy.\n\nCorrect Answer: C) The pulsed fraction increases strongly with energy.\n\nExplanation: The question requires the test-taker to carefully read the text and identify the key finding regarding the energy dependence of the pulsed fraction. The correct answer is supported by the text, which states that \"the pulsed fraction of the magnetar is shown to increase strongly with energy.\" The other options are incorrect because they contradict the text's findings."}, "29": {"documentation": {"title": "Precision Measurement of the 29Si, 33S, and 36Cl Binding Energies", "source": "M. S. Dewey (1), E. G. Kessler Jr (1), R. D. Deslattes (1), H. G.\n  Borner (2), M. Jentschel (2), C. Doll (2), P. Mutti (2) ((1) National\n  Institute of Standards and Technology, (2) Institut Laue-Langevin)", "docs_id": "nucl-ex/0507011", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Precision Measurement of the 29Si, 33S, and 36Cl Binding Energies. The binding energies of 29Si, 33S, and 36Cl have been measured with a relative uncertainty $< 0.59 \\times 10^{-6}$ using a flat-crystal spectrometer. The unique features of these measurements are 1) nearly perfect crystals whose lattice spacing is known in meters, 2) a highly precise angle scale that is derived from first principles, and 3) a gamma-ray measurement facility that is coupled to a high flux reactor with near-core source capability. The binding energy is obtained by measuring all gamma-rays in a cascade scheme connecting the capture and ground states. The measurements require the extension of precision flat-crystal diffraction techniques to the 5 to 6 MeV energy region, a significant precision measurement challenge. The binding energies determined from these gamma-ray measurements are consistent with recent highly accurate atomic mass measurements within a relative uncertainty of $4.3 \\times 10^{-7}$. The gamma-ray measurement uncertainties are the dominant contributors to the uncertainty of this consistency test. The measured gamma-ray energies are in agreement with earlier precision gamma-ray measurements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary source of uncertainty in the consistency test between the binding energies determined from gamma-ray measurements and recent atomic mass measurements?\n\nA) The uncertainty in the lattice spacing of the crystal used in the measurements\nB) The uncertainty in the angle scale used to derive the gamma-ray energies\nC) The uncertainty in the gamma-ray measurement facility's near-core source capability\nD) The uncertainty in the extension of precision flat-crystal diffraction techniques to the 5 to 6 MeV energy region\n\nCorrect Answer: D) The uncertainty in the extension of precision flat-crystal diffraction techniques to the 5 to 6 MeV energy region\n\nExplanation: The question requires the test-taker to analyze the information provided and identify the primary source of uncertainty in the consistency test. The correct answer, D, is supported by the text, which states that \"The gamma-ray measurement uncertainties are the dominant contributors to the uncertainty of this consistency test.\" The other options are incorrect because they are not mentioned as sources of uncertainty in the text. Option A is related to the crystal used in the measurements, but it is not the primary source of uncertainty. Option B is related to the angle scale, but it is not mentioned as a source of uncertainty. Option C is related to the gamma-ray measurement facility, but it is not the primary source of uncertainty."}, "30": {"documentation": {"title": "Application of cross correlations between CMB and large scale structure\n  to constraints on the primordial non-Gaussianity", "source": "Yoshitaka Takeuchi, Kiyotomo Ichiki and Takahiko Matsubara", "docs_id": "1111.6835", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Application of cross correlations between CMB and large scale structure\n  to constraints on the primordial non-Gaussianity. The primordial non-Gaussianity of local type affects the clustering of dark matter halos, and the planned deep and wide photometric surveys are suitable for examining this class of non-Gaussianity. In our previous paper, we investigated the constraint from the cross correlation between CMB lensing potential and galaxy angular distribution on the primordial non-Gaussianity, without taking into account redshift slicing. To improve our previous analysis, in this paper, we add the galaxy lensing shear into our analysis and take into account redshift slicing to follow the redshift evolution of the clustering. By calculating 81 power spectra and using the Fisher matrix method, we find that the constraint on the primordial non-Gaussianity can be improved from {\\Delta}fNL \\sim 5.4 to 5.1 by including the galaxy-galaxy lensing shear cross correlations expected from the Hyper Suprime-Cam survey (HSC), in comparison with the constraint without any cross correlations. Moreover, the constraint can go down to {\\Delta}fNL \\sim 4.8 by including the galaxy-CMB lensing cross correlations from the ACTPol and Planck experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary effect of including galaxy-galaxy lensing shear cross correlations from the Hyper Suprime-Cam survey (HSC) and galaxy-CMB lensing cross correlations from the ACTPol and Planck experiments on the constraint on primordial non-Gaussianity, as estimated by the Fisher matrix method?\n\nA) The constraint on primordial non-Gaussianity is reduced by 1.4 units.\nB) The constraint on primordial non-Gaussianity is improved by 0.3 units, but the effect of HSC is negligible.\nC) The constraint on primordial non-Gaussianity is improved by 0.8 units, with the HSC contribution being the dominant factor.\nD) The constraint on primordial non-Gaussianity is reduced by 0.8 units, with the ACTPol and Planck contributions being more significant than the HSC contribution.\n\nCorrect Answer: C) The constraint on primordial non-Gaussianity is improved by 0.8 units, with the HSC contribution being the dominant factor.\n\nExplanation: According to the text, the constraint on primordial non-Gaussianity is improved from \u0394fNL ~ 5.4 to 5.1 by including the galaxy-galaxy lensing shear cross correlations from the HSC, and further improved to \u0394fNL ~ 4.8 by including the galaxy-CMB lensing cross correlations from the ACTPol and Planck experiments. This suggests that the HSC contribution is the dominant factor in improving the constraint, with an improvement of 0.8 units."}, "31": {"documentation": {"title": "Exploring asymmetric multifractal cross-correlations of price-volatility\n  and asymmetric volatility dynamics in cryptocurrency markets", "source": "Shinji Kakinaka and Ken Umeno", "docs_id": "2102.02865", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring asymmetric multifractal cross-correlations of price-volatility\n  and asymmetric volatility dynamics in cryptocurrency markets. Asymmetric relationship between price and volatility is a prominent feature of the financial market time series. This paper explores the price-volatility nexus in cryptocurrency markets and investigates the presence of asymmetric volatility effect between uptrend (bull) and downtrend (bear) regimes. The conventional GARCH-class models have shown that in cryptocurrency markets, asymmetric reactions of volatility to returns differ from those of other traditional financial assets. We address this issue from a viewpoint of fractal analysis, which can cover the nonlinear interactions and the self-similarity properties widely acknowledged in the field of econophysics. The asymmetric cross-correlations between price and volatility for Bitcoin (BTC), Ethereum (ETH), Ripple (XRP), and Litecoin (LTC) during the period from June 1, 2016 to December 28, 2020 are investigated using the MF-ADCCA method and quantified via the asymmetric DCCA coefficient. The approaches take into account the nonlinearity and asymmetric multifractal scaling properties, providing new insights in investigating the relationships in a dynamical way. We find that cross-correlations are stronger in downtrend markets than in uptrend markets for maturing BTC and ETH. In contrast, for XRP and LTC, inverted reactions are present where cross-correlations are stronger in uptrend markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Exploring asymmetric multifractal cross-correlations of price-volatility and asymmetric volatility dynamics in cryptocurrency markets\" investigate the presence of asymmetric volatility effects between uptrend and downtrend regimes in cryptocurrency markets. What is the primary approach used by the authors to address the issue of asymmetric reactions of volatility to returns in cryptocurrency markets?\n\n**A)** The authors use a traditional GARCH-class model to analyze the time series data.\n**B)** The authors employ a fractal analysis approach, specifically the MF-ADCCA method, to investigate nonlinear interactions and self-similarity properties in the data.\n**C)** The authors use a machine learning algorithm to identify patterns in the data and estimate the asymmetric volatility effects.\n**D)** The authors conduct a regression analysis to examine the relationship between price and volatility.\n\n**Correct Answer:** B) The authors employ a fractal analysis approach, specifically the MF-ADCCA method, to investigate nonlinear interactions and self-similarity properties in the data.\n\n**Explanation:** The authors address the issue of asymmetric reactions of volatility to returns in cryptocurrency markets by using a fractal analysis approach, specifically the MF-ADCCA method. This approach allows them to investigate nonlinear interactions and self-similarity properties in the data, providing new insights into the relationships between price and volatility in a dynamical way. The MF-ADCCA method is a suitable choice for this purpose because it can capture the complex and nonlinear dynamics of the financial market time series."}, "32": {"documentation": {"title": "Sampling of probability measures in the convex order by Wasserstein\n  projection", "source": "Aur\\'elien Alfonsi, Jacopo Corbetta and Benjamin Jourdain", "docs_id": "1709.05287", "section": ["math.PR", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sampling of probability measures in the convex order by Wasserstein\n  projection. In this paper, for $\\mu$ and $\\nu$ two probability measures on $\\mathbb{R}^d$ with finite moments of order $\\rho\\ge 1$, we define the respective projections for the $W_\\rho$-Wasserstein distance of $\\mu$ and $\\nu$ on the sets of probability measures dominated by $\\nu$ and of probability measures larger than $\\mu$ in the convex order. The $W_2$-projection of $\\mu$ can be easily computed when $\\mu$ and $\\nu$ have finite support by solving a quadratic optimization problem with linear constraints. In dimension $d=1$, Gozlan et al.~(2018) have shown that the projections do not depend on $\\rho$. We explicit their quantile functions in terms of those of $\\mu$ and $\\nu$. The motivation is the design of sampling techniques preserving the convex order in order to approximate Martingale Optimal Transport problems by using linear programming solvers. We prove convergence of the Wasserstein projection based sampling methods as the sample sizes tend to infinity and illustrate them by numerical experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Prove that the $W_\\rho$-Wasserstein projection of a probability measure $\\mu$ onto the set of probability measures dominated by $\\nu$ can be explicitly expressed in terms of the quantile functions of $\\mu$ and $\\nu$.\n\n**A)** The $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is given by $\\pi_\\nu(\\mu) = \\nu^{1/\\rho} \\circ \\mu^{1/\\rho}$.\n\n**B)** The $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is given by $\\pi_\\nu(\\mu) = \\nu^{1/\\rho} \\circ \\mu^{1/\\rho} \\circ \\nu^{1/\\rho}$.\n\n**C)** The $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is given by $\\pi_\\nu(\\mu) = \\nu^{1/\\rho} \\circ \\mu^{1/\\rho} \\circ \\nu^{1/\\rho} \\circ \\mu^{1/\\rho}$.\n\n**D)** The $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is given by $\\pi_\\nu(\\mu) = \\nu^{1/\\rho} \\circ \\mu^{1/\\rho} \\circ \\nu^{1/\\rho} \\circ \\mu^{1/\\rho} \\circ \\nu^{1/\\rho}$.\n\n**Correct Answer:** A) The $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is given by $\\pi_\\nu(\\mu) = \\nu^{1/\\rho} \\circ \\mu^{1/\\rho}$.\n\n**Explanation:** The correct answer can be obtained by using the definition of the $W_\\rho$-Wasserstein projection and the properties of the quantile function. Specifically, the $W_\\rho$-Wasserstein projection of $\\mu$ onto the set of probability measures dominated by $\\nu$ is defined as the unique probability measure $\\pi_\\nu$ such that $\\int \\pi_\\nu(x) dx = \\mu(\\{x: \\nu(x) \\geq \\mu(x)\\})$ and $\\int \\pi_\\nu(x) dx = \\nu(\\{x: \\mu(x) \\geq \\nu(x)\\})$. Using the definition of the quantile function, we can express $\\pi_\\nu(\\mu)$ as $\\nu^{1/\\rho} \\circ \\mu^{1/\\rho}$, which is the correct answer."}, "33": {"documentation": {"title": "On A Generalization of \"Eight Blocks to Madness\"", "source": "Kazuya Haraguchi", "docs_id": "1408.3696", "section": ["cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On A Generalization of \"Eight Blocks to Madness\". We consider a puzzle such that a set of colored cubes is given as an instance. Each cube has unit length on each edge and its surface is colored so that what we call the Surface Color Condition is satisfied. Given a palette of six colors, the condition requires that each face should have exactly one color and all faces should have different colors from each other. The puzzle asks to compose a 2x2x2 cube that satisfies the Surface Color Condition from eight suitable cubes in the instance. Note that cubes and solutions have 30 varieties respectively. In this paper, we give answers to three problems on the puzzle: (i) For every subset of the 30 solutions, is there an instance that has the subset exactly as its solution set? (ii) Create a maximum sized infeasible instance (i.e., one having no solution). (iii) Create a minimum sized universal instance (i.e., one having all 30 solutions). We solve the problems with the help of a computer search. We show that the answer to (i) is no. For (ii) and (iii), we show examples of the required instances, where their sizes are 23 and 12, respectively. The answer to (ii) solves one of the open problems that were raised in [E.Berkove et al., \"An Analysis of the (Colored Cubes)^3 Puzzle,\" Discrete Mathematics, 308 (2008) pp. 1033-1045]."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a puzzle involving colored cubes, where each cube has unit length on each edge and its surface is colored according to the Surface Color Condition. The puzzle asks to compose a 2x2x2 cube that satisfies the Surface Color Condition from eight suitable cubes in an instance. The documentation provides answers to three problems related to this puzzle.\n\n**A)** The answer to problem (i) is yes, for every subset of the 30 solutions, there exists an instance that has the subset exactly as its solution set.\n\n**B)** The answer to problem (ii) is no, there is no maximum sized infeasible instance, meaning that there is no instance with no solution.\n\n**C)** The answer to problem (iii) is yes, there exists a minimum sized universal instance, meaning that there is an instance with all 30 solutions.\n\n**D)** The answer to problem (i) is no, for every subset of the 30 solutions, there exists an instance that does not have the subset exactly as its solution set.\n\n**Correct Answer:** D) The answer to problem (i) is no, for every subset of the 30 solutions, there exists an instance that does not have the subset exactly as its solution set.\n\n**Explanation:** The documentation states that the answer to problem (i) is no, meaning that for every subset of the 30 solutions, there is no instance that has the subset exactly as its solution set. This is because the puzzle is designed to be solvable, but the number of possible solutions is large, making it difficult to find an instance that satisfies a specific subset of solutions."}, "34": {"documentation": {"title": "Nonlinear network dynamics under perturbations of the underlying graph", "source": "Anca Radulescu, Sergio Verduzco-Flores", "docs_id": "1406.5096", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear network dynamics under perturbations of the underlying graph. Many natural systems are organized as networks, in which the nodes (be they cells, individuals or populations) interact in a time-dependent fashion. The dynamic behavior of these networks depends on how these nodes are connected, which can be understood in terms of an adjacency matrix, and connection strengths. The object of our study is to relate connectivity to temporal behavior in networks of coupled nonlinear oscillators. We investigate the relationship between classes of system architectures and classes of their possible dynamics, when the nodes are coupled according to a connectivity scheme that obeys certain constrains, but also incorporates random aspects. We illustrate how the phase space dynamics and bifurcations of the system change when perturbing the underlying adjacency graph. We differentiate between the effects on dynamics of the following operations that directly modulate network connectivity: (1) increasing/decreasing edge weights, (2) increasing/decreasing edge density, (3) altering edge configuration by adding, deleting or moving edges. We discuss the significance of our results in the context of real life networks. Some interpretations lead us to draw conclusions that may apply to brain networks, synaptic restructuring and neural dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between the connectivity of a network of coupled nonlinear oscillators and its temporal behavior, considering the effects of perturbing the underlying adjacency graph. How do changes in edge weights, edge density, and edge configuration impact the phase space dynamics and bifurcations of the system?\n\n**A)** Increasing edge weights in a network of coupled nonlinear oscillators will lead to a more synchronized behavior, as the stronger connections between nodes facilitate the exchange of information and synchronize the oscillations.\n\n**B)** Altering the edge configuration by adding or deleting edges will have a more significant impact on the dynamics of the system than increasing or decreasing edge weights, as the changes in connectivity can lead to the emergence of new patterns and behaviors.\n\n**C)** Increasing edge density in a network of coupled nonlinear oscillators will lead to a more complex and chaotic behavior, as the increased number of connections between nodes can lead to the creation of complex feedback loops and the emergence of novel patterns.\n\n**D)** Decreasing edge density in a network of coupled nonlinear oscillators will lead to a more synchronized behavior, as the reduced number of connections between nodes can lead to a decrease in the exchange of information and the synchronization of oscillations.\n\n**Correct Answer:** C) Increasing edge density in a network of coupled nonlinear oscillators will lead to a more complex and chaotic behavior, as the increased number of connections between nodes can lead to the creation of complex feedback loops and the emergence of novel patterns.\n\n**Explanation:** This answer is correct because increasing edge density in a network of coupled nonlinear oscillators can lead to a more complex and chaotic behavior. This is because the increased number of connections between nodes can create complex feedback loops, which can lead to the emergence of novel patterns and behaviors. This is in line with the findings of the study, which illustrates how the phase space dynamics and bifurcations of the system change when perturbing the underlying adjacency graph.\n\nThe other options are incorrect because:\n\n* A) Increasing edge weights can lead to a more synchronized behavior, but it is not the most significant impact on the dynamics of the system.\n* B) Altering the edge configuration can have a significant impact on the dynamics of the system, but it is not the most significant impact compared to increasing or decreasing edge density.\n* D) Decreasing edge density can lead to a more synchronized behavior, but it is not the most significant impact on the dynamics of the system."}, "35": {"documentation": {"title": "Is the Multiverse Hypothesis capable of explaining the Fine Tuning of\n  Nature Laws and Constants? The Case of Cellular Automata", "source": "Francisco Jos\\'e Soler Gil and Manuel Alfonseca", "docs_id": "1105.4278", "section": ["nlin.CG", "astro-ph.CO", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Is the Multiverse Hypothesis capable of explaining the Fine Tuning of\n  Nature Laws and Constants? The Case of Cellular Automata. The objective of this paper is analyzing to which extent the multiverse hypothesis provides a real explanation of the peculiarities of the laws and constants in our universe. First we argue in favor of the thesis that all multiverses except Tegmark's <<mathematical multiverse>> are too small to explain the fine tuning, so that they merely shift the problem up one level. But the <<mathematical multiverse>> is surely too large. To prove this assessment, we have performed a number of experiments with cellular automata of complex behavior, which can be considered as universes in the mathematical multiverse. The analogy between what happens in some automata (in particular Conway's <<Game of Life>>) and the real world is very strong. But if the results of our experiments can be extrapolated to our universe, we should expect to inhabit -- in the context of the multiverse -- a world in which at least some of the laws and constants of nature should show a certain time dependence. Actually, the probability of our existence in a world such as ours would be mathematically equal to zero. In consequence, the results presented in this paper can be considered as an inkling that the hypothesis of the multiverse, whatever its type, does not offer an adequate explanation for the peculiarities of the physical laws in our world. A slightly reduced version of this paper has been published in the Journal for General Philosophy of Science, Springer, March 2013, DOI: 10.1007/s10838-013-9215-7."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nIs the multiverse hypothesis capable of explaining the fine-tuning of nature laws and constants, and if so, which type of multiverse is most suitable for this explanation?\n\nA) The Many-Worlds Interpretation, as it provides a vast number of universes with different physical laws and constants, increasing the probability of our existence in a world with fine-tuned laws.\n\nB) The Tegmark's Mathematical Multiverse, as it offers a vast, infinite number of mathematical universes, each with its own set of physical laws and constants, making it possible for our universe to be one of the many with fine-tuned laws.\n\nC) The Simulation Hypothesis, as it suggests that our universe is a simulation created by a more advanced civilization, which could have intentionally fine-tuned the laws and constants for our existence.\n\nD) None of the above, as the multiverse hypothesis, in general, is unable to provide a satisfactory explanation for the fine-tuning of nature laws and constants.\n\nCorrect Answer: B) The Tegmark's Mathematical Multiverse\n\nExplanation: The paper argues that the Many-Worlds Interpretation is too small to explain the fine-tuning of laws and constants, and the Simulation Hypothesis is too speculative. In contrast, the Tegmark's Mathematical Multiverse offers a vast, infinite number of universes, each with its own set of physical laws and constants, making it possible for our universe to be one of the many with fine-tuned laws. The paper's experiments with cellular automata also suggest that the mathematical multiverse is too large to provide a satisfactory explanation for the fine-tuning of laws and constants in our universe."}, "36": {"documentation": {"title": "Constraints on charm-anticharm asymmetry in the nucleon from lattice QCD", "source": "Raza Sabbir Sufian, Tianbo Liu, Andrei Alexandru, Stanley J. Brodsky,\n  Guy F. de T\\'eramond, Hans G\\\"unter Dosch, Terrence Draper, Keh-Fei Liu,\n  Yi-Bo Yang", "docs_id": "2003.01078", "section": ["hep-lat", "hep-ex", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraints on charm-anticharm asymmetry in the nucleon from lattice QCD. We present the first lattice QCD calculation of the charm quark contribution to the nucleon electromagnetic form factors $G^c_{E,M}(Q^2)$ in the momentum transfer range $0\\leq Q^2 \\leq 1.4$ $\\rm GeV^2$. The quark mass dependence, finite lattice spacing and volume corrections are taken into account simultaneously based on the calculation on three gauge ensembles including one at the physical pion mass. The nonzero value of the charm magnetic moment $\\mu^c_M=-0.00127(38)_{\\rm stat}(5)_{\\rm sys}$, as well as the Pauli form factor, reflects a nontrivial role of the charm sea in the nucleon spin structure. The nonzero $G^c_{E}(Q^2)$ indicates the existence of a nonvanishing asymmetric charm-anticharm sea in the nucleon. Performing a nonperturbative analysis based on holographic QCD and the generalized Veneziano model, we study the constraints on the $[c(x)-\\bar{c}(x)]$ distribution from the lattice QCD results presented here. Our results provide complementary information and motivation for more detailed studies of physical observables that are sensitive to intrinsic charm and for future global analyses of parton distributions including asymmetric charm-anticharm distribution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What can be inferred about the charm-anticharm asymmetry in the nucleon from the lattice QCD calculation of the charm quark contribution to the nucleon electromagnetic form factors?\n\nA) The charm magnetic moment is zero, indicating no charm-anticharm asymmetry.\nB) The Pauli form factor is zero, indicating no charm-anticharm asymmetry.\nC) The nonzero value of the charm magnetic moment and the Pauli form factor reflect a nontrivial role of the charm sea in the nucleon spin structure, indicating the existence of a nonvanishing asymmetric charm-anticharm sea in the nucleon.\nD) The lattice QCD results do not provide sufficient information to constrain the charm-anticharm distribution.\n\nCorrect Answer: C) The nonzero value of the charm magnetic moment and the Pauli form factor reflect a nontrivial role of the charm sea in the nucleon spin structure, indicating the existence of a nonvanishing asymmetric charm-anticharm sea in the nucleon.\n\nExplanation: The correct answer is based on the statement in the documentation that \"The nonzero value of the charm magnetic moment $\\mu^c_M=-0.00127(38)_{\\rm stat}(5)_{\\rm sys}$, as well as the Pauli form factor, reflects a nontrivial role of the charm sea in the nucleon spin structure.\" This indicates that the charm magnetic moment and the Pauli form factor are nonzero, which in turn suggests that there is a nonvanishing asymmetric charm-anticharm sea in the nucleon."}, "37": {"documentation": {"title": "An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data", "source": "Salman Mohamadi, Donald Adjeroh", "docs_id": "2111.02595", "section": ["q-bio.GN", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data. Investigation of age-related genes is of great importance for multiple purposes, for instance, improving our understanding of the mechanism of ageing, increasing life expectancy, age prediction, and other healthcare applications. In his work, starting with a set of 27,142 genes, we develop an information-theoretic framework for identifying genes that are associated with aging by applying unsupervised and semi-supervised learning techniques on human dermal fibroblast gene expression data. First, we use unsupervised learning and apply information-theoretic measures to identify key features for effective representation of gene expression values in the transcriptome data. Using the identified features, we perform clustering on the data. Finally, we apply semi-supervised learning on the clusters using different distance measures to identify novel genes that are potentially associated with aging. Performance assessment for both unsupervised and semi-supervised methods show the effectiveness of the framework."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the information-theoretic framework developed in the study \"An Information-Theoretic Framework for Identifying Age-Related Genes Using Human Dermal Fibroblast Transcriptome Data\"?\n\n**A)** To identify novel genes associated with aging using semi-supervised learning techniques\n**B)** To develop a machine learning model for predicting human lifespan based on gene expression data\n**C)** To apply unsupervised learning techniques to identify key features for effective representation of gene expression values in the transcriptome data\n**D)** To investigate the relationship between gene expression and aging using clustering analysis\n\n**Correct Answer:** C) To apply unsupervised learning techniques to identify key features for effective representation of gene expression values in the transcriptome data\n\n**Explanation:** The correct answer is C) because the study aims to develop an information-theoretic framework for identifying age-related genes by applying unsupervised learning techniques to identify key features for effective representation of gene expression values in the transcriptome data. This is stated in the introduction of the study: \"First, we use unsupervised learning and apply information-theoretic measures to identify key features for effective representation of gene expression values in the transcriptome data.\" The other options are incorrect because they are not the primary goal of the framework, although they may be related to the study's objectives."}, "38": {"documentation": {"title": "Domain structure of bulk ferromagnetic crystals in applied fields near\n  saturation", "source": "H. Knuepfer and C. B. Muratov", "docs_id": "1004.4292", "section": ["nlin.PS", "cond-mat.mtrl-sci", "math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Domain structure of bulk ferromagnetic crystals in applied fields near\n  saturation. We investigate the ground state of a uniaxial ferromagnetic plate with perpendicular easy axis and subject to an applied magnetic field normal to the plate. Our interest is the asymptotic behavior of the energy in macroscopically large samples near the saturation field. We establish the scaling of the critical value of the applied field strength below saturation at which the ground state changes from the uniform to a branched domain magnetization pattern and the leading order scaling behavior of the minimal energy. Furthermore, we derive a reduced sharp-interface energy giving the precise asymptotic behavior of the minimal energy in macroscopically large plates under a physically reasonable assumption of small deviations of the magnetization from the easy axis away from domain walls. On the basis of the reduced energy, and by a formal asymptotic analysis near the transition, we derive the precise asymptotic values of the critical field strength at which non-trivial minimizers (either local or global) emerge. The non-trivial minimal energy scaling is achieved by magnetization patterns consisting of long slender needle-like domains of magnetization opposing the applied field"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the domain structure of bulk ferromagnetic crystals in applied fields near saturation, what is the precise asymptotic behavior of the minimal energy in macroscopically large plates under the assumption of small deviations of the magnetization from the easy axis away from domain walls?\n\nA) The minimal energy scales as the square of the applied field strength, with a critical value at which the ground state changes from uniform to branched domain magnetization patterns.\n\nB) The minimal energy scales as the cube of the applied field strength, with a critical value at which the ground state changes from uniform to branched domain magnetization patterns.\n\nC) The minimal energy scales as the logarithm of the applied field strength, with a critical value at which the ground state changes from uniform to branched domain magnetization patterns.\n\nD) The minimal energy scales as the square root of the applied field strength, with a critical value at which the ground state changes from uniform to branched domain magnetization patterns.\n\n**Correct Answer:** C) The minimal energy scales as the logarithm of the applied field strength, with a critical value at which the ground state changes from uniform to branched domain magnetization patterns.\n\n**Explanation:** The correct answer is based on the reduced sharp-interface energy derived in the paper, which assumes small deviations of the magnetization from the easy axis away from domain walls. The reduced energy is given by a logarithmic function of the applied field strength, which leads to a logarithmic scaling of the minimal energy. This is a consequence of the formal asymptotic analysis near the transition, which shows that the ground state changes from uniform to branched domain magnetization patterns at a critical value of the applied field strength."}, "39": {"documentation": {"title": "Experimenting in Equilibrium", "source": "Stefan Wager and Kuang Xu", "docs_id": "1903.02124", "section": ["math.OC", "econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimenting in Equilibrium. Classical approaches to experimental design assume that intervening on one unit does not affect other units. There are many important settings, however, where this non-interference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. In this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. Our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobstrusive randomization with lightweight modeling, all while remaining in equilibrium. We can then use these estimates to optimize the system by gradient descent. Concretely, we focus on the problem of a platform that seeks to optimize supply-side payments p in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and show that our approach enables the platform to optimize p in large systems using vanishingly small perturbations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Experimenting in Equilibrium\", what is the primary assumption that the authors aim to address in their new approach to experimental design in large-scale stochastic systems?\n\nA) The assumption of perfect interference between units, where intervening on one unit always affects other units.\nB) The assumption of unstructured interference, where the effects of intervening on one unit are unpredictable and cannot be captured via mean-field modeling.\nC) The assumption of equilibrium, where the system is in a state of balance and no changes can be made without affecting other units.\nD) The assumption of non-interference, where intervening on one unit does not affect other units, which is a fundamental assumption in classical approaches to experimental design.\n\n**Correct Answer:** D) The assumption of non-interference, where intervening on one unit does not affect other units, which is a fundamental assumption in classical approaches to experimental design.\n\n**Explanation:** The correct answer is D) The assumption of non-interference, because the paper specifically highlights the limitation of classical approaches to experimental design, which assume that intervening on one unit does not affect other units. The authors aim to address this assumption in their new approach, which can handle structured interference and optimize system parameters using gradient descent."}, "40": {"documentation": {"title": "Bose-Einstein transition temperature in a dilute repulsive gas", "source": "Markus Holzmann, Jean-Noel J.N. Fuchs, Gordon G. Baym, Jean-Paul J.P.\n  Blaizot, Franck F. Laloe", "docs_id": "cond-mat/0310460", "section": ["cond-mat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bose-Einstein transition temperature in a dilute repulsive gas. We discuss certain specific features of the calculation of the critical temperature of a dilute repulsive Bose gas. Interactions modify the critical temperature in two different ways. First, for gases in traps, temperature shifts are introduced by a change of the density profile, arising itself from a modification of the equation of state of the gas (reduced compressibility); these shifts can be calculated simply within mean field theory. Second, even in the absence of a trapping potential (homogeneous gas in a box), temperature shifts are introduced by the interactions; they arise from the correlations introduced in the gas, and thus lie inherently beyond mean field theory - in fact, their evaluation requires more elaborate, non-perturbative, calculations. One illustration of this non-perturbative character is provided by the solution of self-consistent equations, which relate together non-linearly the various energy shifts of the single particle levels k. These equations predict that repulsive interactions shift the critical temperature (at constant density) by an amount which is positive, and simply proportional to the scattering length a; nevertheless, the numerical coefficient is difficult to compute. Physically, the increase of the temperature can be interpreted in terms of the reduced density fluctuations introduced by the repulsive interactions, which facilitate the propagation of large exchange cycles across the sample."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a dilute repulsive Bose gas, what is the physical interpretation of the temperature shift introduced by repulsive interactions, and how does it relate to the reduced density fluctuations in the gas?\n\n**A)** The temperature shift is a result of the increased thermal energy available to the gas, which allows for more frequent collisions between particles. This shift is independent of the scattering length a.\n\n**B)** The temperature shift is a consequence of the reduced compressibility of the gas, which arises from the modification of the equation of state. This shift is directly proportional to the scattering length a.\n\n**C)** The temperature shift is a result of the increased density fluctuations in the gas, which facilitate the propagation of large exchange cycles across the sample. This shift is positive and proportional to the scattering length a.\n\n**D)** The temperature shift is a result of the reduced density fluctuations in the gas, which arise from the modification of the equation of state. This shift is negative and independent of the scattering length a.\n\n**Correct Answer:** C) The temperature shift is a result of the increased density fluctuations in the gas, which facilitate the propagation of large exchange cycles across the sample. This shift is positive and proportional to the scattering length a.\n\n**Explanation:** The correct answer is based on the physical interpretation of the temperature shift introduced by repulsive interactions, as mentioned in the documentation. The increased density fluctuations in the gas facilitate the propagation of large exchange cycles across the sample, leading to a positive temperature shift that is proportional to the scattering length a. This requires an understanding of the non-perturbative character of the interactions and the role of density fluctuations in the gas."}, "41": {"documentation": {"title": "Efficient Mixing at low Reynolds numbers using polymer additives", "source": "Alexander Groisman and Victor Steinberg", "docs_id": "nlin/0104050", "section": ["nlin.CD", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Mixing at low Reynolds numbers using polymer additives. Mixing in fluids is a rapidly developing field of fluid mechanics \\cite{Sreen,Shr,War}, being an important industrial and environmental problem. The mixing of liquids at low Reynolds numbers is usually quite weak in simple flows, and it requires special devices to be efficient. Recently, the problem of mixing was solved analytically for a simple case of random flow, known as the Batchelor regime \\cite{Bat,Kraich,Fal,Sig,Fouxon}. Here we demonstrate experimentally that very viscous liquids at low Reynolds number, $Re$. Here we show that very viscous liquids containing a small amount of high molecular weight polymers can be mixed quite efficiently at very low Reynolds numbers, for a simple flow in a curved channel. A polymer concentration of only 0.001% suffices. The presence of the polymers leads to an elastic instability \\cite{LMS} and to irregular flow \\cite{Ours}, with velocity spectra corresponding to the Batchelor regime \\cite{Bat,Kraich,Fal,Sig,Fouxon}. Our detailed observations of the mixing in this regime enable us to confirm sevearl important theoretical predictions: the probability distributions of the concentration exhibit exponential tails \\cite{Fal,Fouxon}, moments of the distribution decay exponentially along the flow \\cite{Fouxon}, and the spatial correlation function of concentration decays logarithmically."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary effect of adding high molecular weight polymers to very viscous liquids at low Reynolds numbers on their mixing efficiency?\n\nA) The polymers increase the Reynolds number, leading to more efficient mixing.\nB) The polymers decrease the viscosity of the liquid, resulting in weaker mixing.\nC) The polymers introduce an elastic instability, leading to irregular flow and enhanced mixing.\nD) The polymers have no significant effect on the mixing efficiency of the liquid.\n\n**Correct Answer:** C) The polymers introduce an elastic instability, leading to irregular flow and enhanced mixing.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"The presence of the polymers leads to an elastic instability\" and \"with velocity spectra corresponding to the Batchelor regime\". This indicates that the addition of polymers to the liquid leads to an unstable flow regime, which is associated with enhanced mixing. The other options are incorrect because they do not accurately reflect the effects of adding polymers to the liquid. Option A is incorrect because the Reynolds number is not increased by the addition of polymers. Option B is incorrect because the viscosity of the liquid is not decreased by the addition of polymers. Option D is incorrect because the polymers do have a significant effect on the mixing efficiency of the liquid."}, "42": {"documentation": {"title": "Laser-Driven High-Velocity Microparticle Launcher In Atmosphere And\n  Under Vacuum", "source": "David Veysset, Yuchen Sun, Steven E. Kooi, Jet Lem, and Keith A.\n  Nelson", "docs_id": "1911.11572", "section": ["astro-ph.IM", "cond-mat.mtrl-sci", "physics.app-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Laser-Driven High-Velocity Microparticle Launcher In Atmosphere And\n  Under Vacuum. This paper presents a novel approach to launch single microparticles at high velocities under low vacuum conditions. In an all-optical table-top method, microparticles with sizes ranging from a few microns to tens of microns are accelerated to supersonic velocities depending on the particle mass. The acceleration is performed through a laser ablation process and the particles are monitored in free space using an ultra-high-speed multi-frame camera with nanosecond time resolution. Under low vacuum, we evaluate the current platform performance by measuring particle velocities for a range of particle types and sizes, and demonstrate blast wave suppression and drag reduction under vacuum. Showing an impact on polyethylene, we demonstrate the capability of the experimental setup to study materials behavior under high-velocity impact. The present method is relevant to space applications, particularly to rendezvous missions where velocities range from tens of m/s to a few km/s, as well as to a wide range of terrestrial applications including impact bonding and impact-induced erosion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the laser ablation process accelerates microparticles to supersonic velocities in the proposed high-velocity microparticle launcher?\n\nA) Photothermal expansion of the particle\nB) Laser-induced plasma expansion\nC) Electromagnetic radiation pressure\nD) Thermal conduction of energy to the particle\n\nCorrect Answer: B) Laser-induced plasma expansion\n\nExplanation: The correct answer is B) Laser-induced plasma expansion. The paper states that the acceleration is performed through a laser ablation process, which implies that the laser energy is used to create a plasma expansion that propels the microparticles to high velocities. This is a key aspect of the proposed method, and understanding this mechanism is essential to evaluating the performance of the experimental setup.\n\nCandidate A) Photothermal expansion of the particle is incorrect because while photothermal expansion may occur, it is not the primary mechanism by which the laser ablation process accelerates the microparticles.\n\nCandidate C) Electromagnetic radiation pressure is also incorrect because while radiation pressure may play a role in the acceleration process, it is not the primary mechanism.\n\nCandidate D) Thermal conduction of energy to the particle is incorrect because thermal conduction is not mentioned as a mechanism for accelerating the microparticles in the paper."}, "43": {"documentation": {"title": "Two-Stream Video Classification with Cross-Modality Attention", "source": "Lu Chi and Guiyu Tian and Yadong Mu and Qi Tian", "docs_id": "1908.00497", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-Stream Video Classification with Cross-Modality Attention. Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of the proposed cross-modality attention operation, and how does it differ from existing methods such as two-stream video classification?\n\nA) The proposed operation is designed to reduce the dimensionality of video features, whereas existing methods aim to improve the accuracy of video classification by fusing multi-modality information.\nB) The proposed operation is intended to alleviate the limitations of two-stream video classification by incorporating attention mechanisms to selectively focus on relevant modality information.\nC) The proposed operation is primarily focused on improving the robustness of video classification models to variations in lighting conditions, whereas existing methods aim to enhance the accuracy of video classification by fusing multi-modality information.\nD) The proposed operation is designed to reduce the computational complexity of video classification models, whereas existing methods aim to improve the accuracy of video classification by fusing multi-modality information.\n\nCorrect Answer: B) The proposed operation is intended to alleviate the limitations of two-stream video classification by incorporating attention mechanisms to selectively focus on relevant modality information.\n\nExplanation: The correct answer is B) because the documentation states that the proposed cross-modality attention operation is designed to obtain information from other modality in a more effective way than two-stream, and that it is a wrapper of the proposed attention operation, which is a key component of the CMA block. This suggests that the primary motivation behind the development of the proposed operation is to improve the accuracy of video classification by selectively focusing on relevant modality information."}, "44": {"documentation": {"title": "Following a Trend with an Exponential Moving Average: Analytical Results\n  for a Gaussian Model", "source": "D. S. Grebenkov and J. Serror", "docs_id": "1308.5658", "section": ["q-fin.ST", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Following a Trend with an Exponential Moving Average: Analytical Results\n  for a Gaussian Model. We investigate how price variations of a stock are transformed into profits and losses (P&Ls) of a trend following strategy. In the frame of a Gaussian model, we derive the probability distribution of P&Ls and analyze its moments (mean, variance, skewness and kurtosis) and asymptotic behavior (quantiles). We show that the asymmetry of the distribution (with often small losses and less frequent but significant profits) is reminiscent to trend following strategies and less dependent on peculiarities of price variations. At short times, trend following strategies admit larger losses than one may anticipate from standard Gaussian estimates, while smaller losses are ensured at longer times. Simple explicit formulas characterizing the distribution of P&Ls illustrate the basic mechanisms of momentum trading, while general matrix representations can be applied to arbitrary Gaussian models. We also compute explicitly annualized risk adjusted P&L and strategy turnover to account for transaction costs. We deduce the trend following optimal timescale and its dependence on both auto-correlation level and transaction costs. Theoretical results are illustrated on the Dow Jones index."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Following a Trend with an Exponential Moving Average: Analytical Results for a Gaussian Model\", what is the primary finding regarding the distribution of profits and losses (P&Ls) of a trend following strategy, and how does it relate to the asymmetry of the distribution?\n\n**A)** The distribution of P&Ls is symmetric, with equal probabilities of small losses and significant profits, and the asymmetry is a result of peculiarities in price variations.\n\n**B)** The distribution of P&Ls is skewed to the right, with small losses being more frequent than significant profits, and the asymmetry is reminiscent of trend following strategies.\n\n**C)** The distribution of P&Ls is normal, with a mean and variance that can be calculated using standard Gaussian estimates, and the asymmetry is a result of the specific characteristics of the price variations.\n\n**D)** The distribution of P&Ls is bimodal, with two distinct peaks corresponding to small losses and significant profits, and the asymmetry is a result of the auto-correlation level of the price variations.\n\n**Correct Answer:** B) The distribution of P&Ls is skewed to the right, with small losses being more frequent than significant profits, and the asymmetry is reminiscent of trend following strategies.\n\n**Explanation:** The correct answer is B) because the paper states that the asymmetry of the distribution is \"reminiscent to trend following strategies and less dependent on peculiarities of price variations\". This suggests that the distribution of P&Ls is skewed to the right, with small losses being more frequent than significant profits, which is consistent with the characteristics of trend following strategies."}, "45": {"documentation": {"title": "Search for heavy resonances decaying into a pair of $Z$ bosons in the\n  $\\ell^+\\ell^-\\ell'^+\\ell'^-$ and $\\ell^+\\ell^-\\nu\\bar\\nu$ final states using\n  139 fb$^{-1}$ of proton-proton collisions at $\\sqrt{s} = 13$ TeV with the\n  ATLAS detector", "source": "ATLAS Collaboration", "docs_id": "2009.14791", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for heavy resonances decaying into a pair of $Z$ bosons in the\n  $\\ell^+\\ell^-\\ell'^+\\ell'^-$ and $\\ell^+\\ell^-\\nu\\bar\\nu$ final states using\n  139 fb$^{-1}$ of proton-proton collisions at $\\sqrt{s} = 13$ TeV with the\n  ATLAS detector. A search for heavy resonances decaying into a pair of $Z$ bosons leading to $\\ell^+\\ell^-\\ell'^+\\ell'^-$ and $\\ell^+\\ell^-\\nu\\bar\\nu$ final states, where $\\ell$ stands for either an electron or a muon, is presented. The search uses proton-proton collision data at a centre-of-mass energy of 13 TeV collected from 2015 to 2018 that corresponds to the full integrated luminosity of 139 fb$^{-1}$ recorded by the ATLAS detector during Run 2 of the Large Hadron Collider. Different mass ranges spanning 200 GeV to 2000 GeV for the hypothetical resonances are considered, depending on the final state and model. In the absence of a significant observed excess, the results are interpreted as upper limits on the production cross section of a spin-0 or spin-2 resonance. The upper limits for the spin-0 resonance are translated to exclusion contours in the context of Type-I and Type-II two-Higgs-doublet models, and the limits for the spin-2 resonance are used to constrain the Randall--Sundrum model with an extra dimension giving rise to spin-2 graviton excitations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the ATLAS detector's search for heavy resonances decaying into a pair of Z bosons, what is the significance of the 139 fb$^{-1}$ of proton-proton collisions at $\\sqrt{s} = 13$ TeV, and how does it relate to the interpretation of the results in terms of upper limits on the production cross section of a spin-0 or spin-2 resonance?\n\nA) The 139 fb$^{-1}$ corresponds to a 5$\\sigma$ excess in the $\\ell^+\\ell^-\\ell'^+\\ell'^-$ final state, which would require further investigation to confirm the presence of a new resonance.\n\nB) The 139 fb$^{-1}$ is the minimum luminosity required to establish a 3$\\sigma$ exclusion contour for a spin-2 resonance in the Randall--Sundrum model with an extra dimension.\n\nC) The 139 fb$^{-1}$ is the total integrated luminosity collected by the ATLAS detector during Run 2 of the Large Hadron Collider, and it is used to translate the upper limits on the production cross section of a spin-0 or spin-2 resonance into exclusion contours in the context of Type-I and Type-II two-Higgs-doublet models.\n\nD) The 139 fb$^{-1}$ is the luminosity required to detect a 10 GeV resonance decaying into a pair of Z bosons, and it is used to constrain the production cross section of such a resonance.\n\nCorrect Answer: C) The 139 fb$^{-1}$ is the total integrated luminosity collected by the ATLAS detector during Run 2 of the Large Hadron Collider, and it is used to translate the upper limits on the production cross section of a spin-0 or spin-2 resonance into exclusion contours in the context of Type-I and Type-II two-Higgs-doublet models.\n\nExplanation: The correct answer is C) because the 139 fb$^{-1}$ is indeed the total integrated luminosity collected by the ATLAS detector during Run 2 of the Large Hadron Collider, and it is used to translate the upper limits on the production cross section of a spin-0 or spin-2 resonance into exclusion contours in the context of Type-I and Type-II two-Higgs-doublet models. The other options are incorrect because they either misinterpret the significance of the 139 fb$^{-1}$ (A and B) or provide an unrelated piece of information (D)."}, "46": {"documentation": {"title": "Cardy-Verlinde Formula and asymptotically flat rotating Charged black\n  holes", "source": "Jiliang Jing", "docs_id": "hep-th/0202052", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cardy-Verlinde Formula and asymptotically flat rotating Charged black\n  holes. The Cardy-Verlinde formula is generalized to the asymptotically flat rotating charged black holes in the Einstein-Maxwell theory and low-energy effective field theory describing string by using some typical spacetimes, such as the Kerr-Newman, Einstein-Maxwell-dilaton-axion, Kaluza-Klein, and Sen black holes. For the Kerr-Newman black hole, the definition of the Casimir energy takes the same form as that of the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, while the Cardy-Verlinde formula possesses different from since the Casimir energy does not appear in the extensive energy. The Einstein-Maxwell-dilaton-axion, Kaluza-Klein, and Sen black holes have special property: The definition of the Casimir energy for these black holes is similar to that of the Kerr-Newman black hole, but the Cardy-Verlinde formula takes the same form as that of the Kerr black hole. Furthermore, we also study the entropy bounds for the systems in which the matters surrounds these black holes. We find that the bound for the case of the Kerr-Newman black hole is related to its charge, and the bound for the cases of the EMDA, Kaluza-Klein, and Sen black holes can be expressed as a unified form. A surprising result is that the entropy bounds for the Kaluza-Klein and Sen black holes are tighter than the Bekenstein one."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Cardy-Verlinde formula for asymptotically flat rotating charged black holes in the Einstein-Maxwell theory and low-energy effective field theory describing strings. What is the relationship between the Casimir energy and the extensive energy for the Kerr-Newman black hole, and how does this relationship differ from that of the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes?\n\nA) The Casimir energy is equal to the extensive energy for all four black holes, and the Cardy-Verlinde formula is the same for all four black holes.\n\nB) The Casimir energy is equal to the extensive energy for the Kerr-Newman black hole, but differs from the extensive energy for the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, and the Cardy-Verlinde formula is different for the Kerr-Newman black hole.\n\nC) The Casimir energy is equal to the extensive energy for the Kerr-Newman black hole, but differs from the extensive energy for the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, and the Cardy-Verlinde formula is the same for the Kerr-Newman black hole.\n\nD) The Casimir energy is not equal to the extensive energy for the Kerr-Newman black hole, and the Cardy-Verlinde formula is not applicable to this black hole.\n\nCorrect Answer: B) The Casimir energy is equal to the extensive energy for the Kerr-Newman black hole, but differs from the extensive energy for the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, and the Cardy-Verlinde formula is different for the Kerr-Newman black hole.\n\nExplanation: The correct answer is B) because the Casimir energy definition is the same for the Kerr-Newman black hole as for the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, but the Cardy-Verlinde formula differs from that of the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes. This is stated in the provided documentation: \"For the Kerr-Newman black hole, the definition of the Casimir energy takes the same form as that of the Kerr-Newman-AdS$_4$ and Kerr-Newman-dS$_4$ black holes, while the Cardy-Verlinde formula possesses different from since the Casimir energy does not appear in the extensive energy.\""}, "47": {"documentation": {"title": "Online Multiobjective Minimax Optimization and Applications", "source": "Georgy Noarov, Mallesh Pai, Aaron Roth", "docs_id": "2108.03837", "section": ["cs.LG", "cs.DS", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online Multiobjective Minimax Optimization and Applications. We introduce a simple but general online learning framework, in which at every round, an adaptive adversary introduces a new game, consisting of an action space for the learner, an action space for the adversary, and a vector valued objective function that is convex-concave in every coordinate. The learner and the adversary then play in this game. The learner's goal is to play so as to minimize the maximum coordinate of the cumulative vector-valued loss. The resulting one-shot game is not convex-concave, and so the minimax theorem does not apply. Nevertheless, we give a simple algorithm that can compete with the setting in which the adversary must announce their action first, with optimally diminishing regret. We demonstrate the power of our simple framework by using it to derive optimal bounds and algorithms across a variety of domains. This includes no regret learning: we can recover optimal algorithms and bounds for minimizing external regret, internal regret, adaptive regret, multigroup regret, subsequence regret, and a notion of regret in the sleeping experts setting. Next, we use it to derive a variant of Blackwell's Approachability Theorem, which we term \"Fast Polytope Approachability\". Finally, we are able to recover recently derived algorithms and bounds for online adversarial multicalibration and related notions (mean-conditioned moment multicalibration, and prediction interval multivalidity)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of online multiobjective minimax optimization and applications, what is the primary goal of the learner in playing the game introduced by the adaptive adversary, and how does this goal relate to the minimax theorem?\n\n**A)** The learner's goal is to maximize the minimum coordinate of the cumulative vector-valued loss, and this goal is related to the minimax theorem in that it implies a trade-off between exploration and exploitation.\n\n**B)** The learner's goal is to minimize the maximum coordinate of the cumulative vector-valued loss, and this goal is related to the minimax theorem in that it implies a need for the learner to balance competing objectives.\n\n**C)** The learner's goal is to maximize the maximum coordinate of the cumulative vector-valued loss, and this goal is related to the minimax theorem in that it implies a need for the learner to prioritize one objective over others.\n\n**D)** The learner's goal is to minimize the minimum coordinate of the cumulative vector-valued loss, and this goal is related to the minimax theorem in that it implies a need for the learner to balance competing objectives.\n\n**Correct Answer:** B) The learner's goal is to minimize the maximum coordinate of the cumulative vector-valued loss, and this goal is related to the minimax theorem in that it implies a need for the learner to balance competing objectives.\n\n**Explanation:** The correct answer is B) because the learner's goal is to minimize the maximum coordinate of the cumulative vector-valued loss, which is a convex-concave objective function. This goal is related to the minimax theorem in that it implies a need for the learner to balance competing objectives, as the minimax theorem typically applies to games where the objective function is convex-concave. The other options are incorrect because they either misstate the learner's goal or imply a different relationship between the learner's goal and the minimax theorem."}, "48": {"documentation": {"title": "Martingale Benamou--Brenier: a probabilistic perspective", "source": "Julio Backhoff-Veraguas, Mathias Beiglb\\\"ock, Martin Huesmann, Sigrid\n  K\\\"allblad", "docs_id": "1708.04869", "section": ["math.PR", "math.CA", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Martingale Benamou--Brenier: a probabilistic perspective. In classical optimal transport, the contributions of Benamou-Brenier and McCann regarding the time-dependent version of the problem are cornerstones of the field and form the basis for a variety of applications in other mathematical areas. We suggest a Benamou-Brenier type formulation of the martingale transport problem for given $d$-dimensional distributions $\\mu, \\nu $ in convex order. The unique solution $M^*=(M_t^*)_{t\\in [0,1]}$ of this problem turns out to be a Markov-martingale which has several notable properties: In a specific sense it mimics the movement of a Brownian particle as closely as possible subject to the conditions $M^*_0\\sim\\mu, M^*_1\\sim \\nu$. Similar to McCann's displacement-interpolation, $M^*$ provides a time-consistent interpolation between $\\mu$ and $\\nu$. For particular choices of the initial and terminal law, $M^*$ recovers archetypical martingales such as Brownian motion, geometric Brownian motion, and the Bass martingale. Furthermore, it yields a natural approximation to the local vol model and a new approach to Kellerer's theorem. This article is parallel to the work of Huesmann-Trevisan, who consider a related class of problems from a PDE-oriented perspective."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the martingale transport problem for given $d$-dimensional distributions $\\mu, \\nu $ in convex order. Suppose we have a unique solution $M^*=(M_t^*)_{t\\in [0,1]}$ to this problem, which is a Markov-martingale. What are the notable properties of $M^*$, and how does it relate to the movement of a Brownian particle, as well as other martingales?\n\nA) $M^*$ is a deterministic function of the initial and terminal laws, and its movement is independent of the underlying probability space.\nB) $M^*$ provides a time-consistent interpolation between $\\mu$ and $\\nu$, and its movement is closely related to the displacement-interpolation of McCann's work.\nC) $M^*$ is a local martingale, and its movement is only defined on a finite time horizon.\nD) $M^*$ is a stochastic process that can be approximated by a Brownian motion, and its movement is closely related to the local vol model.\n\nCorrect Answer: B) $M^*$ provides a time-consistent interpolation between $\\mu$ and $\\nu$, and its movement is closely related to the displacement-interpolation of McCann's work.\n\nExplanation: The correct answer is B) because the problem statement mentions that $M^*$ provides a time-consistent interpolation between $\\mu$ and $\\nu$, and that its movement is closely related to the displacement-interpolation of McCann's work. This is a key property of the martingale transport problem, and it is mentioned in the problem statement as a notable property of $M^*$.\n\nThe other options are incorrect because:\n\nA) $M^*$ is not a deterministic function of the initial and terminal laws, but rather a stochastic process that depends on the underlying probability space.\n\nC) $M^*$ is not a local martingale, but rather a Markov-martingale, which is a more general class of stochastic processes.\n\nD) While $M^*$ can be approximated by a Brownian motion, this is not a direct consequence of the problem statement, and it is not a notable property of $M^*$ in the same way that the interpolation property is."}, "49": {"documentation": {"title": "Preventive and Active Safety Applications", "source": "Levent Guvenc", "docs_id": "2012.08031", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Preventive and Active Safety Applications. Road vehicle safety systems can be broadly classified into the two categories of passive and active systems. The aim of passive safety systems is to reduce risk of injury to the occupants of the vehicle during and after an accident like a crash or rollover. Passive safety systems include the design of safety restraints, design for crashworthiness, seat belts and air bags. In contrast to passive systems, the aim in active safety is to prevent an accident from occurring in the first place. As such, it makes sense to call them preventive systems also. Here, the concentration is on preventive and active safety systems. The current state of the art in some key preventive and active safety systems is presented in this paper, wherein the various techniques used are also explained briefly. In some cases, the presentation is complemented with results obtained in the research group of the author. A road map of expected future developments in the area of preventive and safety applications is also presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare and contrast the design objectives of passive safety systems and preventive/active safety systems in road vehicles, and explain how the latter aim to prevent accidents from occurring in the first place.\n\n**A)** Passive safety systems aim to reduce the risk of injury to occupants during and after an accident, whereas preventive/active safety systems focus on designing safety restraints and crashworthiness to prevent accidents from occurring in the first place. This approach is more effective in reducing the number of accidents.\n\n**B)** Passive safety systems prioritize the protection of occupants during an accident, whereas preventive/active safety systems aim to prevent accidents from occurring in the first place by using advanced technologies such as lane departure warning systems and autonomous emergency braking. This approach is more effective in reducing the severity of accidents.\n\n**C)** Passive safety systems rely on traditional safety features such as seat belts and airbags, whereas preventive/active safety systems incorporate advanced technologies such as sensors and machine learning algorithms to detect potential accidents and prevent them from occurring. This approach is more effective in reducing the risk of accidents.\n\n**D)** Passive safety systems focus on designing safety restraints and crashworthiness to reduce the risk of injury to occupants during an accident, whereas preventive/active safety systems aim to prevent accidents from occurring in the first place by using advanced technologies such as autonomous vehicles and smart infrastructure. This approach is more effective in reducing the number of accidents.\n\n**Correct Answer:** C) Passive safety systems rely on traditional safety features such as seat belts and airbags, whereas preventive/active safety systems incorporate advanced technologies such as sensors and machine learning algorithms to detect potential accidents and prevent them from occurring. This approach is more effective in reducing the risk of accidents.\n\n**Explanation:** This question requires the candidate to understand the design objectives of passive safety systems and preventive/active safety systems in road vehicles. The correct answer, option C, highlights the key difference between the two approaches, which is the use of advanced technologies in preventive/active safety systems to detect and prevent accidents. This requires the candidate to analyze the information provided in the documentation and apply their knowledge of safety systems to arrive at the correct answer."}, "50": {"documentation": {"title": "Asymmetric collapse by dissolution or melting in a uniform flow", "source": "Chris H. Rycroft and Martin Z. Bazant", "docs_id": "1507.00085", "section": ["physics.flu-dyn", "cond-mat.stat-mech", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric collapse by dissolution or melting in a uniform flow. An advection--diffusion-limited dissolution model of an object being eroded by a two-dimensional potential flow is presented. By taking advantage of the conformal invariance of the model, a numerical method is introduced that tracks the evolution of the object boundary in terms of a time-dependent Laurent series. Simulations of a variety of dissolving objects are shown, which shrink and then collapse to a single point in finite time. The simulations reveal a surprising exact relationship whereby the collapse point is the root of a non-analytic function given in terms of the flow velocity and the Laurent series coefficients describing the initial shape. This result is subsequently derived using residue calculus. The structure of the non-analytic function is examined for three different test cases, and a practical approach to determine the collapse point using a generalized Newton--Raphson root-finding algorithm is outlined. These examples also illustrate the possibility that the model breaks down in finite time prior to complete collapse, due to a topological singularity, as the dissolving boundary overlaps itself rather than breaking up into multiple domains (analogous to droplet pinch-off in fluid mechanics). In summary, the model raises fundamental mathematical questions about broken symmetries in finite-time singularities of both continuous and stochastic dynamical systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mathematical structure of the non-analytic function that describes the collapse point of a dissolving object in a uniform flow, as derived using residue calculus, and how does it relate to the flow velocity and the Laurent series coefficients describing the initial shape?\n\nA) The non-analytic function is a polynomial of degree 2, with coefficients that are functions of the flow velocity and the initial shape.\n\nB) The non-analytic function is a transcendental function, with a branch point at the collapse point, and its argument is a function of the flow velocity and the initial shape.\n\nC) The non-analytic function is a rational function, with a denominator that is a polynomial of degree 3, and its numerator is a polynomial of degree 2, with coefficients that are functions of the flow velocity and the initial shape.\n\nD) The non-analytic function is an elliptic function, with a period of \u03c0, and its argument is a function of the flow velocity and the initial shape.\n\nCorrect Answer: B) The non-analytic function is a transcendental function, with a branch point at the collapse point, and its argument is a function of the flow velocity and the initial shape.\n\nExplanation: The correct answer is based on the fact that the non-analytic function is derived using residue calculus, which is a mathematical technique used to study the behavior of functions with branch points. The fact that the function has a branch point at the collapse point is a key feature of the model, and it is related to the flow velocity and the initial shape. The other options are incorrect because they do not accurately describe the mathematical structure of the non-analytic function."}, "51": {"documentation": {"title": "Double Universe", "source": "E. Alfinito, R. Manka and G. Vitiello", "docs_id": "hep-th/9705134", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Double Universe. We discuss the canonical quantization of non-unitary time evolution in inflating Universe. We consider gravitational wave modes in the FRW metrics in a de Sitter phase and show that the vacuum is a two-mode SU(1,1) squeezed state of thermo field dynamics, thus exhibiting the link between inflationary evolution and thermal properties. In particular we discuss the entropy and the free energy of the system. The state space splits into many unitarily inequivalent representations of the canonical commutation relations parametrized by time $t$ and non-unitary time evolution is described as a trajectory in the space of the representations: the system evolves in time by running over unitarily inequivalent representations. The generator of time evolution is related to the entropy operator. A central ingredient in our discussion is the doubling of the degrees of freedom which turns out to be the bridge to the unified picture of non-unitary time evolution, squeezing and thermal properties in inflating metrics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Double Universe, what is the relationship between the generator of time evolution and the entropy operator, and how does this relate to the non-unitary time evolution in an inflating Universe?\n\nA) The generator of time evolution is directly proportional to the entropy operator, and this relationship is a fundamental aspect of the de Sitter phase.\n\nB) The generator of time evolution is inversely proportional to the entropy operator, and this relationship is a consequence of the thermo field dynamics in the FRW metrics.\n\nC) The generator of time evolution is a function of the entropy operator, but this relationship is only valid for specific unitarily inequivalent representations of the canonical commutation relations.\n\nD) The generator of time evolution is related to the entropy operator through a non-linear transformation, and this relationship is a key feature of the unified picture of non-unitary time evolution, squeezing, and thermal properties in inflating metrics.\n\nCorrect Answer: D) The generator of time evolution is related to the entropy operator through a non-linear transformation, and this relationship is a key feature of the unified picture of non-unitary time evolution, squeezing, and thermal properties in inflating metrics.\n\nExplanation: The correct answer is D) because the documentation states that the generator of time evolution is related to the entropy operator, and this relationship is a key feature of the unified picture of non-unitary time evolution, squeezing, and thermal properties in inflating metrics. The documentation also mentions that the state space splits into many unitarily inequivalent representations of the canonical commutation relations parametrized by time t, and that the system evolves in time by running over unitarily inequivalent representations. This suggests that the relationship between the generator of time evolution and the entropy operator is non-linear and depends on the specific representation being considered."}, "52": {"documentation": {"title": "AIC, Cp and estimators of loss for elliptically symmetric distributions", "source": "Aur\\'elie Boisbunon (CSIS), Stephane Canu (LITIS), Dominique\n  Fourdrinier (LITIS), William Strawderman, Martin T. Wells", "docs_id": "1308.2766", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AIC, Cp and estimators of loss for elliptically symmetric distributions. In this article, we develop a modern perspective on Akaike's Information Criterion and Mallows' Cp for model selection. Despite the diff erences in their respective motivation, they are equivalent in the special case of Gaussian linear regression. In this case they are also equivalent to a third criterion, an unbiased estimator of the quadratic prediction loss, derived from loss estimation theory. Our first contribution is to provide an explicit link between loss estimation and model selection through a new oracle inequality. We then show that the form of the unbiased estimator of the quadratic prediction loss under a Gaussian assumption still holds under a more general distributional assumption, the family of spherically symmetric distributions. One of the features of our results is that our criterion does not rely on the speci ficity of the distribution, but only on its spherical symmetry. Also this family of laws o ffers some dependence property between the observations, a case not often studied."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a linear regression model with a Gaussian distribution, where the response variable is elliptically symmetric. Develop an unbiased estimator of the quadratic prediction loss, and show that it is equivalent to Akaike's Information Criterion (AIC) and Mallows' Cp under the assumption of spherical symmetry.\n\n**A)** The unbiased estimator of the quadratic prediction loss is given by the negative log-likelihood of the Gaussian distribution, i.e., -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2, where \u03c3^2 is the variance of the error term and \u03b2 is the vector of regression coefficients.\n\n**B)** The unbiased estimator of the quadratic prediction loss is given by the negative log-likelihood of the Gaussian distribution, i.e., -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2, where \u03c3^2 is the variance of the error term and \u03b2 is the vector of regression coefficients. However, under the assumption of spherical symmetry, the estimator can be simplified to -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2.\n\n**C)** The unbiased estimator of the quadratic prediction loss is given by the negative log-likelihood of the Gaussian distribution, i.e., -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2, where \u03c3^2 is the variance of the error term and \u03b2 is the vector of regression coefficients. However, under the assumption of spherical symmetry, the estimator can be simplified to -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2 + (1/2)\u2211(x_i^T\u03b2)^2.\n\n**D)** The unbiased estimator of the quadratic prediction loss is given by the negative log-likelihood of the Gaussian distribution, i.e., -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2, where \u03c3^2 is the variance of the error term and \u03b2 is the vector of regression coefficients. However, under the assumption of spherical symmetry, the estimator can be simplified to -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2 + (1/2)\u2211(x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2.\n\n**Correct Answer:** B) The unbiased estimator of the quadratic prediction loss is given by the negative log-likelihood of the Gaussian distribution, i.e., -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2, where \u03c3^2 is the variance of the error term and \u03b2 is the vector of regression coefficients. However, under the assumption of spherical symmetry, the estimator can be simplified to -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2.\n\n**Explanation:** The correct answer is B) because it correctly states the unbiased estimator of the quadratic prediction loss under the assumption of spherical symmetry. The estimator is given by the negative log-likelihood of the Gaussian distribution, which is -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2. Under the assumption of spherical symmetry, the estimator can be simplified to -ln(\u03c0\u03c3^2/2) + (1/2\u03c3^2)\u2211(y_i - x_i^T\u03b2)^2 - (1/2)\u2211(x_i^T\u03b2)^2, which is the correct form of the estimator under spherical symmetry."}, "53": {"documentation": {"title": "Multi-Attention Based Ultra Lightweight Image Super-Resolution", "source": "Abdul Muqeet, Jiwon Hwang, Subin Yang, Jung Heum Kang, Yongwoo Kim,\n  Sung-Ho Bae", "docs_id": "2008.12912", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Attention Based Ultra Lightweight Image Super-Resolution. Lightweight image super-resolution (SR) networks have the utmost significance for real-world applications. There are several deep learning based SR methods with remarkable performance, but their memory and computational cost are hindrances in practical usage. To tackle this problem, we propose a Multi-Attentive Feature Fusion Super-Resolution Network (MAFFSRN). MAFFSRN consists of proposed feature fusion groups (FFGs) that serve as a feature extraction block. Each FFG contains a stack of proposed multi-attention blocks (MAB) that are combined in a novel feature fusion structure. Further, the MAB with a cost-efficient attention mechanism (CEA) helps us to refine and extract the features using multiple attention mechanisms. The comprehensive experiments show the superiority of our model over the existing state-of-the-art. We participated in AIM 2020 efficient SR challenge with our MAFFSRN model and won 1st, 3rd, and 4th places in memory usage, floating-point operations (FLOPs) and number of parameters, respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of the Multi-Attentive Feature Fusion Super-Resolution Network (MAFFSRN) proposed in the paper \"Multi-Attention Based Ultra Lightweight Image Super-Resolution\"?\n\nA) To improve the accuracy of image super-resolution models by increasing the number of layers and parameters.\nB) To reduce the memory and computational cost of existing deep learning-based image super-resolution methods.\nC) To enhance the feature extraction capabilities of image super-resolution models using multiple attention mechanisms.\nD) To develop a new type of attention mechanism that can be applied to other computer vision tasks.\n\nCorrect Answer: B) To reduce the memory and computational cost of existing deep learning-based image super-resolution methods.\n\nExplanation: The question requires the test-taker to understand the primary motivation behind the development of MAFFSRN, which is to address the limitations of existing deep learning-based image super-resolution methods in terms of memory and computational cost. The correct answer, B, is supported by the text, which states that \"Lightweight image super-resolution (SR) networks have the utmost significance for real-world applications\" and that the authors aim to \"tackle this problem\" by proposing MAFFSRN. The other options are incorrect because they do not accurately reflect the primary motivation behind MAFFSRN. Option A is incorrect because the text does not mention increasing the number of layers and parameters as a motivation for developing MAFFSRN. Option C is incorrect because while MAFFSRN does use multiple attention mechanisms, this is not its primary motivation. Option D is incorrect because the text does not mention developing a new type of attention mechanism as a motivation for developing MAFFSRN."}, "54": {"documentation": {"title": "CO J=1-0 observations of molecular gas interacting with galactic\n  supernova remnants G5.4-1.2, G5.55+0.32 and G5.71-0.08", "source": "H. S. Liszt", "docs_id": "0905.0438", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CO J=1-0 observations of molecular gas interacting with galactic\n  supernova remnants G5.4-1.2, G5.55+0.32 and G5.71-0.08. The field just West of the galactic supernova remnant W28 (l=6.4\\degr, b=-0.2\\degr) harbors 3 of 5 newly-discovered 1720 OH maser spots and two recently-discovered candidate supernova candidates (one of which is a $\\gamma$-ray source), as well as several compact and classical HII regions. Here, we analyze a datacube of CO J=1-0 emission having 1\\arcmin and 1 \\kms resolution, made with on-the-fly mapping over the region $5\\degr \\le l \\le 6\\degr, -1\\degr \\le b \\le 0.5\\degr$}. {Extended and often very bright CO emission was detected at the velocities of the 1720 MHz OH masers and around the supernova remnant G5.55+0.32 which lacks a maser. A new bipolar outflow which is marginally resolved at 1\\arcmin resolution and strong in CO (12K) was detected at the periphery of G5.55+0.32, coincident with an MSX source; there is also a bright rim of CO just beyond the periphery of the radio remnant. The OH maser near G5.71-0.08 lies on a shell of strongly-emitting molecular gas (up to 20K) . At the -21 \\kms velocity of G5.4-1.2, CO covers much of the field but is weak (3 K) and undisturbed near the remnant. The extended molecular gas around the compact H II region and outflow in G5.89-0.39 (W28A2) is shown for the first time.}"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary characteristic of the CO J=1-0 emission observed in the field just west of the galactic supernova remnant W28, and how does it relate to the presence of OH maser spots and supernova candidates in the region?\n\nA){The CO J=1-0 emission is extended and often very bright, and its detection is correlated with the presence of OH maser spots and supernova candidates.}\nB){The CO J=1-0 emission is weak and undisturbed near the galactic supernova remnant W28, and its detection is not correlated with the presence of OH maser spots and supernova candidates.}\nC){The CO J=1-0 emission is marginally resolved at 1\\arcmin resolution and strong in CO (12K) at the periphery of the supernova remnant G5.55+0.32, coincident with an MSX source.}\nD){The CO J=1-0 emission is a new bipolar outflow which is marginally resolved at 1\\arcmin resolution and strong in CO (12K) at the periphery of the supernova remnant G5.55+0.32, coincident with an MSX source.}\n\nCorrect Answer: A){The CO J=1-0 emission is extended and often very bright, and its detection is correlated with the presence of OH maser spots and supernova candidates.}\n\nExplanation: The question requires the test-taker to analyze the provided documentation and identify the primary characteristic of the CO J=1-0 emission observed in the field just west of the galactic supernova remnant W28. The correct answer, A, states that the CO J=1-0 emission is extended and often very bright, and its detection is correlated with the presence of OH maser spots and supernova candidates. This is supported by the text, which describes the detection of extended and often very bright CO emission at the velocities of the 1720 MHz OH masers and around the supernova remnant G5.55+0.32. The other options are incorrect because they either describe a specific observation (C and D) or a characteristic that is not supported by the text (B)."}, "55": {"documentation": {"title": "Asymptotically Safe Starobinsky Inflation", "source": "Edmund J. Copeland, Christoph Rahmede, Ippocratis D. Saltas", "docs_id": "1311.0881", "section": ["gr-qc", "astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotically Safe Starobinsky Inflation. We revisit Starobinsky inflation in a quantum gravitational context, by means of the exact Renormalisation Group (RG). We calculate the non-perturbative beta functions for Newton's `constant' G and the dimensionless R^2 coupling, and show that there exists an attractive UV fixed point where the latter one vanishes but not the former one, and we provide the corresponding beta functions. The smallness of the R^2 coupling, required for agreement with inflationary observables, is naturally ensured by its vanishing at the UV fixed point, ensuring the smallness of the primordial fluctuations, as well as providing a theoretical motivation for the initial conditions needed for successful inflation in this context. We discuss the corresponding RG dynamics, showing both how inflationary and classical observations define the renormalisation conditions for the couplings, and also how the UV regime is connected with lower energies along the RG flow. Finally, we discuss the consistency of our results when higher order curvature corrections are included, and show that they are robust to the inclusion of R^3 corrections."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Asymptotically Safe Starobinsky Inflation, what is the implication of the attractive UV fixed point on the dimensionless R^2 coupling, and how does it relate to the smallness of the primordial fluctuations?\n\nA) The vanishing of the R^2 coupling at the UV fixed point ensures the smallness of the primordial fluctuations, but does not provide a theoretical motivation for the initial conditions needed for successful inflation.\n\nB) The vanishing of the R^2 coupling at the UV fixed point provides a theoretical motivation for the initial conditions needed for successful inflation, but does not ensure the smallness of the primordial fluctuations.\n\nC) The vanishing of the R^2 coupling at the UV fixed point ensures the smallness of the primordial fluctuations, and also provides a theoretical motivation for the initial conditions needed for successful inflation, as well as naturally ensuring the smallness of the R^2 coupling.\n\nD) The vanishing of the R^2 coupling at the UV fixed point does not have any implications on the smallness of the primordial fluctuations, and is unrelated to the initial conditions needed for successful inflation.\n\nCorrect Answer: C) The vanishing of the R^2 coupling at the UV fixed point ensures the smallness of the primordial fluctuations, and also provides a theoretical motivation for the initial conditions needed for successful inflation, as well as naturally ensuring the smallness of the R^2 coupling."}, "56": {"documentation": {"title": "Insights on the Theory of Robust Games", "source": "Giovanni Paolo Crespi and Davide Radi and Matteo Rocca", "docs_id": "2002.00225", "section": ["econ.TH", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Insights on the Theory of Robust Games. A robust game is a distribution-free model to handle ambiguity generated by a bounded set of possible realizations of the values of players' payoff functions. The players are worst-case optimizers and a solution, called robust-optimization equilibrium, is guaranteed by standard regularity conditions. The paper investigates the sensitivity to the level of uncertainty of this equilibrium. Specifically, we prove that it is an epsilon-Nash equilibrium of the nominal counterpart game, where the epsilon-approximation measures the extra profit that a player would obtain by reducing his level of uncertainty. Moreover, given an epsilon-Nash equilibrium of a nominal game, we prove that it is always possible to introduce uncertainty such that the epsilon-Nash equilibrium is a robust-optimization equilibrium. An example shows that a robust Cournot duopoly model can admit multiple and asymmetric robust-optimization equilibria despite only a symmetric Nash equilibrium exists for the nominal counterpart game."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a robust game with two players, where the payoff functions are uncertain and bounded. Suppose we have a nominal game with a symmetric Nash equilibrium, where each player's payoff function is a linear function of the other player's action. The robust game is defined as a distribution-free model that handles ambiguity generated by a bounded set of possible realizations of the values of players' payoff functions.\n\n**A)** The robust-optimization equilibrium of the robust game is always a Nash equilibrium of the nominal game.\n\n**B)** If the nominal game has a symmetric Nash equilibrium, then the robust-optimization equilibrium of the robust game is also symmetric.\n\n**C)** The epsilon-Nash equilibrium of the robust game is always a robust-optimization equilibrium of the nominal game.\n\n**D)** In a robust Cournot duopoly model, the number of robust-optimization equilibria is always equal to the number of Nash equilibria of the nominal game.\n\n**Correct Answer:** C) The epsilon-Nash equilibrium of the robust game is always a robust-optimization equilibrium of the nominal game.\n\n**Explanation:** The paper proves that the epsilon-Nash equilibrium of the robust game is an epsilon-Nash equilibrium of the nominal game, where epsilon measures the extra profit that a player would obtain by reducing his level of uncertainty. This implies that the epsilon-Nash equilibrium of the robust game is also a robust-optimization equilibrium of the nominal game, as it satisfies the standard regularity conditions for robust-optimization equilibria.\n\nNote: The other options are incorrect because:\n\n* A) The robust-optimization equilibrium is not necessarily a Nash equilibrium of the nominal game.\n* B) The robust-optimization equilibrium may not be symmetric, even if the nominal game has a symmetric Nash equilibrium.\n* D) The number of robust-optimization equilibria in a Cournot duopoly model is not necessarily equal to the number of Nash equilibria of the nominal game."}, "57": {"documentation": {"title": "Exploring complex networks by means of adaptive walkers", "source": "Luce Prignano, Yamir Moreno and Albert Diaz-Guilera", "docs_id": "1203.1439", "section": ["nlin.AO", "cond-mat.dis-nn", "cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring complex networks by means of adaptive walkers. Finding efficient algorithms to explore large networks with the aim of recovering information about their structure is an open problem. Here, we investigate this challenge by proposing a model in which random walkers with previously assigned home nodes navigate through the network during a fixed amount of time. We consider that the exploration is successful if the walker gets the information gathered back home, otherwise, no data is retrieved. Consequently, at each time step, the walkers, with some probability, have the choice to either go backward approaching their home or go farther away. We show that there is an optimal solution to this problem in terms of the average information retrieved and the degree of the home nodes and design an adaptive strategy based on the behavior of the random walker. Finally, we compare different strategies that emerge from the model in the context of network reconstruction. Our results could be useful for the discovery of unknown connections in large scale networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the adaptive walker model for exploring complex networks, what is the primary goal of the random walkers, and how does the success of the exploration strategy depend on the behavior of the walkers?\n\n**A)** The primary goal of the random walkers is to maximize the number of steps taken, and the success of the exploration strategy depends on the degree of the home nodes.\n\n**B)** The primary goal of the random walkers is to gather information about the network structure, and the success of the exploration strategy depends on the probability of the walkers choosing to go backward or farther away.\n\n**C)** The primary goal of the random walkers is to minimize the time taken to explore the network, and the success of the exploration strategy depends on the degree of the home nodes.\n\n**D)** The primary goal of the random walkers is to maximize the amount of information retrieved, and the success of the exploration strategy depends on the probability of the walkers choosing to go backward or farther away.\n\n**Correct Answer:** D) The primary goal of the random walkers is to maximize the amount of information retrieved, and the success of the exploration strategy depends on the probability of the walkers choosing to go backward or farther away.\n\n**Explanation:** The correct answer is D) because the problem statement explicitly states that the exploration is successful if the walker gets the information gathered back home, and the success of the exploration strategy depends on the probability of the walkers choosing to go backward or farther away. This indicates that the primary goal of the random walkers is to maximize the amount of information retrieved, and the success of the exploration strategy depends on the behavior of the walkers.\n\n**Explanation of incorrect options:**\n\n* A) The degree of the home nodes is mentioned in the problem statement as a factor that affects the success of the exploration strategy, but it is not the primary goal of the random walkers.\n* B) The probability of the walkers choosing to go backward or farther away is mentioned as a factor that affects the success of the exploration strategy, but it is not the primary goal of the random walkers.\n* C) The time taken to explore the network is not mentioned as a factor that affects the success of the exploration strategy, and the degree of the home nodes is not the primary goal of the random walkers."}, "58": {"documentation": {"title": "Secure Federated Transfer Learning", "source": "Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, Qiang Yang", "docs_id": "1812.03337", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secure Federated Transfer Learning. Machine learning relies on the availability of a vast amount of data for training. However, in reality, most data are scattered across different organizations and cannot be easily integrated under many legal and practical constraints. In this paper, we introduce a new technique and framework, known as federated transfer learning (FTL), to improve statistical models under a data federation. The federation allows knowledge to be shared without compromising user privacy, and enables complimentary knowledge to be transferred in the network. As a result, a target-domain party can build more flexible and powerful models by leveraging rich labels from a source-domain party. A secure transfer cross validation approach is also proposed to guard the FTL performance under the federation. The framework requires minimal modifications to the existing model structure and provides the same level of accuracy as the non-privacy-preserving approach. This framework is very flexible and can be effectively adapted to various secure multi-party machine learning tasks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the Federated Transfer Learning (FTL) framework, and how does it address the challenges of integrating data from different organizations?\n\nA) To enable the sharing of knowledge among parties while maintaining user privacy, and to improve model accuracy by leveraging rich labels from source-domain parties.\n\nB) To facilitate the transfer of models between parties while ensuring data confidentiality, and to enhance the robustness of statistical models under data federation.\n\nC) To develop a secure method for cross-validation that protects the performance of FTL under federation, and to provide a flexible framework for various secure multi-party machine learning tasks.\n\nD) To create a decentralized framework for data sharing and model training, and to ensure that the accuracy of non-privacy-preserving approaches is comparable to FTL.\n\nCorrect Answer: A) To enable the sharing of knowledge among parties while maintaining user privacy, and to improve model accuracy by leveraging rich labels from source-domain parties.\n\nExplanation: The correct answer is A) because the FTL framework is designed to enable the sharing of knowledge among parties while maintaining user privacy, and to improve model accuracy by leveraging rich labels from source-domain parties. This is stated in the introduction of the paper: \"The federation allows knowledge to be shared without compromising user privacy, and enables complimentary knowledge to be transferred in the network.\""}, "59": {"documentation": {"title": "Numerical Calculation of Convection with Reduced Speed of Sound\n  Technique", "source": "H. Hotta, M. Rempel, T. Yokoyama, Y. Iida, and Y. Fan", "docs_id": "1201.1061", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical Calculation of Convection with Reduced Speed of Sound\n  Technique. Context. The anelastic approximation is often adopted in numerical calculation with low Mach number, such as stellar internal convection. This approximation requires frequent global communication, because of an elliptic partial differential equation. Frequent global communication is negative factor for the parallel computing with a large number of CPUs. Aims. The main purpose of this paper is to test the validity of a method that artificially reduces the speed of sound for the compressible fluid equations in the context of stellar internal convection. The reduction of speed of sound allows for larger time steps in spite of low Mach number, while the numerical scheme remains fully explicit and the mathematical system is hyperbolic and thus does not require frequent global communication. Methods. Two and three dimensional compressible hydrodynamic equations are solved numerically. Some statistical quantities of solutions computed with different effective Mach numbers (due to reduction of speed of sound) are compared to test the validity of our approach. Results. Numerical simulations with artificially reduced speed of sound are a valid approach as long as the effective Mach number (based on the reduced speed of sound) remains less than 0.7."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of artificially reducing the speed of sound in numerical calculations of convection with low Mach number, such as stellar internal convection?\n\nA) Reduced computational complexity\nB) Increased accuracy of numerical solutions\nC) Ability to use larger time steps without compromising numerical stability\nD) Improved scalability for parallel computing with a large number of CPUs\n\nCorrect Answer: C) Ability to use larger time steps without compromising numerical stability\n\nExplanation: The correct answer is C) Ability to use larger time steps without compromising numerical stability. According to the documentation, the reduction of speed of sound allows for larger time steps in spite of low Mach number, which is a significant advantage in numerical calculations. The other options are incorrect because they do not accurately reflect the primary advantage of artificially reducing the speed of sound. Option A is incorrect because the reduction of speed of sound does not necessarily reduce computational complexity. Option B is incorrect because the accuracy of numerical solutions is not directly related to the reduction of speed of sound. Option D is incorrect because the scalability of parallel computing is not directly related to the reduction of speed of sound."}}