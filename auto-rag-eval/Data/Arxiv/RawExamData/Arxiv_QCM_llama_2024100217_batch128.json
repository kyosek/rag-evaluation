{"0": {"documentation": {"title": "Phase reconstruction from oscillatory data with iterated Hilbert\n  transform embeddings -- benefits and limitations", "source": "Erik Gengel and Arkady Pikovsky", "docs_id": "2111.10300", "section": ["physics.data-an", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase reconstruction from oscillatory data with iterated Hilbert\n  transform embeddings -- benefits and limitations. In the data analysis of oscillatory systems, methods based on phase reconstruction are widely used to characterize phase-locking properties and inferring the phase dynamics. The main component in these studies is an extraction of the phase from a time series of an oscillating scalar observable. We discuss a practical procedure of phase reconstruction by virtue of a recently proposed method termed \\textit{iterated Hilbert transform embeddings}. We exemplify the potential benefits and limitations of the approach by applying it to a generic observable of a forced Stuart-Landau oscillator. Although in many cases, unavoidable amplitude modulation of the observed signal does not allow for perfect phase reconstruction, in cases of strong stability of oscillations and a high frequency of the forcing, iterated Hilbert transform embeddings significantly improve the quality of the reconstructed phase. We also demonstrate that for significant amplitude modulation, iterated embeddings do not provide any improvement."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary benefits and limitations of using iterated Hilbert transform embeddings for phase reconstruction in oscillatory data analysis, and how do these benefits and limitations manifest in the context of a forced Stuart-Landau oscillator?\n\nA) The primary benefit is improved phase reconstruction accuracy, but the limitation is that it is only applicable to systems with strong stability and high frequency forcing. \nB) The primary benefit is that it can handle significant amplitude modulation, but the limitation is that it does not improve phase reconstruction accuracy in such cases. \nC) The primary benefit is that it provides no improvement in phase reconstruction accuracy for systems with significant amplitude modulation, but the limitation is that it improves accuracy in cases of strong stability and high frequency forcing. \nD) The primary benefit is that it improves phase reconstruction accuracy, but the limitation is that it is only applicable to systems with weak stability and low frequency forcing.\n\nCorrect Answer: A) The primary benefit is improved phase reconstruction accuracy, but the limitation is that it is only applicable to systems with strong stability and high frequency forcing. \n\nExplanation: The correct answer is A) because the documentation states that iterated Hilbert transform embeddings \"significantly improve the quality of the reconstructed phase\" in cases of strong stability of oscillations and a high frequency of the forcing. However, it also notes that in cases of significant amplitude modulation, iterated embeddings \"do not provide any improvement\". This suggests that the primary benefit of iterated Hilbert transform embeddings is improved phase reconstruction accuracy in specific conditions, but the limitation is that it is only applicable to those conditions."}, "1": {"documentation": {"title": "Entanglement of Vacuum States With the de Sitter Horizon: Consequences\n  on Holographic Dark Energy", "source": "Rafael Pav\\~ao, Ricardo Faleiro, Alex H. Blin, Brigitte Hiller", "docs_id": "1607.02115", "section": ["gr-qc", "hep-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entanglement of Vacuum States With the de Sitter Horizon: Consequences\n  on Holographic Dark Energy. The aim of this article is to study the effect of an Event Horizon on the entanglement of the Quantum Vacuum and how entanglement, together with the Holographic Principle, may explain the current value of the Cosmological Constant, in light of recent theories. Entanglement is tested for vacuum states very near and very far from the Horizon of a de Sitter Universe, using the Peres-Horodecki (PPT) criterion. A scalar vacuum field ($\\hat{\\phi}$) is averaged inside two boxes of volume $V$ in different spatial positions such that it acquires the structure of a bipartite Quantum Harmonic Oscillator, for which the PPT criterion is a necessary but not sufficient condition of separability. Entanglement is found between states obtained from boxes shaped as spherical shells with thickness of the order of one Planck distance ($l_p$), when one of the states is near the Horizon, and the other state is anywhere in the Universe. Entanglement disappears when the distance of the state near the horizon and the Horizon increases to around $5l_p$. If we consider the Horizon not as a surface but as a spherical shell of thickness $l_p$, then this means that there is entanglement between the states in the Horizon and the rest of the Universe. When both states are at distances larger than $\\sim 5 l_p$ from the Horizon, no entanglement is found."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the article \"Entanglement of Vacuum States With the de Sitter Horizon: Consequences on Holographic Dark Energy\", what is the implication of the Peres-Horodecki (PPT) criterion on the entanglement of vacuum states near and far from the de Sitter Horizon?\n\n**A)** The PPT criterion is a sufficient condition for separability, implying that entanglement disappears when the distance between the two states increases.\n\n**B)** The PPT criterion is a necessary but not sufficient condition for separability, indicating that entanglement is present when the distance between the two states is less than a certain threshold.\n\n**C)** The PPT criterion is a sufficient condition for entanglement, meaning that entanglement is guaranteed to exist when the distance between the two states is greater than a certain threshold.\n\n**D)** The PPT criterion is a necessary condition for entanglement, implying that entanglement disappears when the distance between the two states increases.\n\n**Correct Answer:** B) The PPT criterion is a necessary but not sufficient condition for separability, indicating that entanglement is present when the distance between the two states is less than a certain threshold.\n\n**Explanation:** The article states that the PPT criterion is \"a necessary but not sufficient condition of separability\", which means that it is a necessary condition for entanglement to exist, but not a sufficient condition to guarantee its presence. This implies that entanglement is present when the distance between the two states is less than a certain threshold, but it is not guaranteed to exist when the distance is greater than that threshold."}, "2": {"documentation": {"title": "Magnetic Two-Dimensional Chromium Trihalides: A Theoretical Perspective", "source": "D. Soriano, M. I. Katsnelson, J. Fern\\'andez-Rossier", "docs_id": "2008.08855", "section": ["cond-mat.mes-hall", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Magnetic Two-Dimensional Chromium Trihalides: A Theoretical Perspective. The discovery of ferromagnetic order in monolayer 2D crystals has opened a new venue in the field of two dimensional (2D) materials. 2D magnets are not only interesting on their own, but their integration in van der Waals heterostructures allows for the observation of new and exotic effects in the ultrathin limit. The family of Chromium trihalides, CrI$_3$, CrBr$_3$ and CrCl$_3$, is, so far, the most studied among magnetic 2D crystals. In this mini-review, we provide a perspective of the state of the art of the theoretical understanding of magnetic 2D trihalides, most of which will also be relevant for other 2D magnets, such as vanadium trihalides. We discuss both the well-established facts, such as the origin of the magnetic moment and magnetic anisotropy and address as well open issues such as the nature of the anisotropic spin couplings and the magnitude of the magnon gap. Recent theoretical predictions on Moir\\' e magnets and magnetic skyrmions are also discussed. Finally, we give some prospects about the future interest of these materials and possible device applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the study of magnetic 2D trihalides, such as CrI$_3$, CrBr$_3$, and CrCl$_3$, and how do these materials relate to the observation of new and exotic effects in van der Waals heterostructures?\n\n**A)** The discovery of ferromagnetic order in monolayer 2D crystals has opened a new venue in the field of two-dimensional (2D) materials, and the study of magnetic 2D trihalides is driven by the desire to understand the underlying mechanisms of this phenomenon.\n\n**B)** The family of Chromium trihalides is the most studied among magnetic 2D crystals due to their potential applications in spintronics and other device technologies.\n\n**C)** Theoretical understanding of magnetic 2D trihalides is crucial for predicting the behavior of other 2D magnets, such as vanadium trihalides, and for designing new materials with specific properties.\n\n**D)** The study of magnetic 2D trihalides is primarily driven by the need to understand the origin of the magnetic moment and magnetic anisotropy in these materials.\n\n**Correct Answer:** A) The discovery of ferromagnetic order in monolayer 2D crystals has opened a new venue in the field of two-dimensional (2D) materials, and the study of magnetic 2D trihalides is driven by the desire to understand the underlying mechanisms of this phenomenon.\n\n**Explanation:** The correct answer is A) because the introduction of the paper explicitly states that the discovery of ferromagnetic order in monolayer 2D crystals has opened a new venue in the field of two-dimensional (2D) materials, and the study of magnetic 2D trihalides is driven by the desire to understand the underlying mechanisms of this phenomenon. The other options, while related to the topic, are not the primary motivation behind the study of magnetic 2D trihalides. Option B is incorrect because while the study of magnetic 2D trihalides may have potential applications in spintronics and other device technologies, this is not the primary motivation. Option C is incorrect because while theoretical understanding of magnetic 2D trihalides is crucial for predicting the behavior of other 2D magnets, this is not the primary motivation. Option D is incorrect because while the origin of the magnetic moment and magnetic anisotropy is an important aspect of magnetic 2D trihalides, it is not the primary motivation behind their study."}, "3": {"documentation": {"title": "Dimensionality reduction and band quantization induced by potassium\n  intercalation in 1$T$-HfTe$_2$", "source": "Y. Nakata, K. Sugawara, A. Chainani, K. Yamauchi, K. Nakayama, S.\n  Souma, P.-Y. Chuang, C.-M. Cheng, T. Oguchi, K. Ueno, T. Takahashi, and T.\n  Sato", "docs_id": "1907.04962", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dimensionality reduction and band quantization induced by potassium\n  intercalation in 1$T$-HfTe$_2$. We have performed angle-resolved photoemission spectroscopy on transition-metal dichalcogenide 1$T$-HfTe$_2$ to elucidate the evolution of electronic states upon potassium (K) deposition. In pristine HfTe$_2$, an in-plane hole pocket and electron pockets are observed at the Brillouin-zone center and corner, respectively, indicating the semimetallic nature of bulk HfTe$_2$, with dispersion perpendicular to the plane. In contrast, the band structure of heavily K-dosed HfTe$_2$ is obviously different from that of bulk, and resembles the band structure calculated for monolayer HfTe$_2$. It was also observed that lightly K-dosed HfTe$_2$ is characterized by quantized bands originating from bilayer and trilayer HfTe$_2$, indicative of staging. The results suggest that the dimensionality-crossover from 3D (dimensional) to 2D electronic states due to systematic K intercalation takes place via staging in a single sample. The study provides a new strategy for controlling the dimensionality and functionality of novel quantum materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which potassium (K) intercalation in 1T-HfTe2 induces dimensionality reduction and band quantization, as observed in angle-resolved photoemission spectroscopy?\n\nA) The formation of a 2D electron gas at the surface of the material, which leads to quantization of the electronic states.\nB) The staging of bilayer and trilayer HfTe2, resulting in a crossover from 3D to 2D electronic states.\nC) The creation of a semimetallic state at the interface between the HfTe2 layers, which leads to the formation of in-plane hole pockets and electron pockets.\nD) The introduction of a 3D electron gas at the bulk of the material, which leads to the formation of dispersion perpendicular to the plane.\n\nCorrect Answer: B) The staging of bilayer and trilayer HfTe2, resulting in a crossover from 3D to 2D electronic states.\n\nExplanation: The correct answer is B) The staging of bilayer and trilayer HfTe2, resulting in a crossover from 3D to 2D electronic states. This is because the study observed that lightly K-dosed HfTe2 is characterized by quantized bands originating from bilayer and trilayer HfTe2, indicative of staging. This staging mechanism is responsible for the dimensionality-crossover from 3D to 2D electronic states due to systematic K intercalation.\n\nCandidate A is incorrect because it suggests the formation of a 2D electron gas at the surface of the material, which is not supported by the study. Candidate C is incorrect because it suggests the creation of a semimetallic state at the interface between the HfTe2 layers, which is not the primary mechanism observed in the study. Candidate D is incorrect because it suggests the introduction of a 3D electron gas at the bulk of the material, which is not supported by the study."}, "4": {"documentation": {"title": "Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems", "source": "Shan An, Fangru Zhou, Mei Yang, Haogang Zhu, Changhong Fu, and\n  Konstantinos A. Tsintotas", "docs_id": "2108.10506", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems. Estimating a scene's depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline's capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a novel, low-complexity network architecture for real-time monocular human depth estimation and segmentation, as proposed in the paper?\n\n**A)** To improve the accuracy of pedestrian detection in outdoor environments.\n**B)** To enable fast and accurate human depth estimation and segmentation in indoor environments for applications on resource-constrained platforms.\n**C)** To develop a framework for semantic segmentation of objects in complex scenes.\n**D)** To enhance the performance of monocular camera-based object tracking systems.\n\n**Correct Answer:** B) To enable fast and accurate human depth estimation and segmentation in indoor environments for applications on resource-constrained platforms.\n\n**Explanation:** The question requires the test-taker to understand the main objective of the proposed network architecture, which is to achieve fast and accurate human depth estimation and segmentation in indoor environments for applications on resource-constrained platforms, such as battery-powered aerial, micro-aerial, and ground vehicles. The correct answer is supported by the text, which states that the paper aims to \"achieve collision avoidance against moving pedestrians\" in indoor environments. The other options are incorrect because they do not accurately reflect the primary motivation behind the proposed network architecture."}, "5": {"documentation": {"title": "Time-Aware Language Models as Temporal Knowledge Bases", "source": "Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel\n  Gillick, Jacob Eisenstein, William W. Cohen", "docs_id": "2106.15110", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time-Aware Language Models as Temporal Knowledge Bases. Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum -- those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently ``refreshed'' as new data arrives, without the need for retraining from scratch."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A language model (LM) is trained on a dataset that contains information about the President of the United States from 2015 to 2020. However, the LM is not aware of the fact that Joe Biden became the President in 2021. What is a potential problem with this LM in the closed-book setting?\n\nA) The LM may not be able to recognize the name \"Joe Biden\" as a valid President.\nB) The LM may not be able to memorize the fact that the President of the United States is a human being.\nC) The LM may not be able to recognize the fact that the President of the United States is a specific individual who has held the office for a certain number of years.\nD) The LM may not be able to recognize the fact that the President of the United States is a public figure.\n\n**Correct Answer:** A) The LM may not be able to recognize the name \"Joe Biden\" as a valid President.\n\n**Explanation:** The LM is trained on a dataset that only contains information about the President from 2015 to 2020, but it is not aware of the fact that Joe Biden became the President in 2021. This means that the LM may not have seen the name \"Joe Biden\" in its training data, and therefore may not be able to recognize it as a valid President. This is a problem because the LM is expected to memorize facts about the President, including their name, in the closed-book setting.\n\n**Additional Question:** What is a potential benefit of jointly modeling text with its timestamp, as proposed in the paper?\n\nA) The model can only be trained on a single snapshot of data.\nB) The model can only be used for tasks that require temporal context.\nC) The model can be efficiently \"refreshed\" as new data arrives.\nD) The model can only be used for tasks that require memorization of seen facts.\n\n**Correct Answer:** C) The model can be efficiently \"refreshed\" as new data arrives.\n\n**Explanation:** The paper proposes a technique for jointly modeling text with its timestamp, which allows the model to be efficiently \"refreshed\" as new data arrives. This means that the model can be updated with new information without the need for retraining from scratch, which can be a significant advantage in real-world applications where data is constantly changing."}, "6": {"documentation": {"title": "A new N=1 AdS4 Vacuum of Maximal Supergravity", "source": "Nikolay Bobev, Thomas Fischbacher, Krzysztof Pilch", "docs_id": "1909.10969", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new N=1 AdS4 Vacuum of Maximal Supergravity. The recent comprehensive numerical study of critical points of the scalar potential of four-dimensional N=8, SO(8) gauged supergravity using Machine Learning software has led to a discovery of a new N=1 vacuum with a triality-invariant SO(3) symmetry. Guided by the numerical data for that point, we obtain a consistent SO(3)xZ2-invariant truncation of the N=8 theory to an N=1 supergravity with three chiral multiplets. Critical points of the truncated scalar potential include both the N=1 point as well as two new non-supersymmetric and perturbatively unstable points not found by previous searches. Studying the structure of the submanifold of SO(3)xZ2-invariant supergravity scalars, we find that it has a simple interpretation as a submanifold of the 14-dimensional Z2^3-invariant scalar manifold (SU(1,1)/U(1))^7, for which we find a rather remarkable superpotential whose structure matches the single bit error correcting (7, 4) Hamming code. This 14-dimensional scalar manifold contains approximately one quarter of the known critical points. We also show that there exists a smooth supersymmetric domain wall which interpolates between the new N=1 AdS4 solution and the maximally supersymmetric AdS4 vacuum. Using holography, this result indicates the existence of an N=1 RG flow from the ABJM SCFT to a new strongly interacting conformal fixed point in the IR."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the N=8, SO(8) gauged supergravity theory and its truncation to an N=1 supergravity with three chiral multiplets. What is the significance of the 14-dimensional Z2^3-invariant scalar manifold (SU(1,1)/U(1))^7, and how does it relate to the superpotential structure of the theory?\n\nA) The 14-dimensional scalar manifold is a submanifold of the space of all possible supergravity solutions, and its structure is irrelevant to the theory's dynamics.\n\nB) The 14-dimensional scalar manifold is a submanifold of the space of all possible supergravity solutions, and its structure is crucial in determining the theory's supersymmetry breaking pattern.\n\nC) The 14-dimensional scalar manifold is a submanifold of the space of all possible supergravity solutions, and its structure is related to the existence of a smooth supersymmetric domain wall between the new N=1 AdS4 solution and the maximally supersymmetric AdS4 vacuum.\n\nD) The 14-dimensional scalar manifold is a submanifold of the space of all possible supergravity solutions, and its structure is a manifestation of the theory's triality-invariant SO(3) symmetry.\n\nCorrect Answer: C) The 14-dimensional scalar manifold is a submanifold of the space of all possible supergravity solutions, and its structure is related to the existence of a smooth supersymmetric domain wall between the new N=1 AdS4 solution and the maximally supersymmetric AdS4 vacuum.\n\nExplanation: The correct answer is C) because the 14-dimensional scalar manifold is indeed a submanifold of the space of all possible supergravity solutions, and its structure is related to the existence of a smooth supersymmetric domain wall between the new N=1 AdS4 solution and the maximally supersymmetric AdS4 vacuum. This domain wall is a key feature of the theory, as it allows for the interpolation between the new N=1 AdS4 solution and the maximally supersymmetric AdS4 vacuum, and has implications for the theory's holographic properties. The other options are incorrect because they do not accurately capture the significance of the 14-dimensional scalar manifold in the context of the theory."}, "7": {"documentation": {"title": "Optimizing a jump-diffusion model of a starving forager", "source": "Nikhil Krishnan and Zachary P. Kilpatrick", "docs_id": "1807.06740", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimizing a jump-diffusion model of a starving forager. We analyze the movement of a starving forager on a one-dimensional periodic lattice, where each location contains one unit of food. As the forager lands on sites with food, it consumes the food, leaving the sites empty. If the forager lands consecutively on $s$ empty sites, then it will starve. The forager has two modes of movement: it can either diffuse, by moving with equal probability to adjacent sites on the lattice, or it can jump to a uniformly randomly chosen site on the lattice. We show that the lifetime $T$ of the forager in either paradigm can be approximated by the sum of the cover time $\\tau_{\\rm cover}$ and the starvation time $s$, when $s$ far exceeds the number $n$ of lattice sites. Our main findings focus on the hybrid model, where the forager has a probability of either jumping or diffusing. The lifetime of the forager varies non-monotonically according to $p_j$, the probability of jumping. By examining a small system, analyzing a heuristic model, and using direct numerical simulation, we explore the tradeoff between jumps and diffusion, and show that the strategy that maximizes the forager lifetime is a mixture of both modes of movement."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the movement of a starving forager on a one-dimensional periodic lattice, where each location contains one unit of food. How does the lifetime of the forager change when the probability of jumping ($p_j$) is increased, and what is the optimal strategy for maximizing the forager's lifetime?\n\n**A)** The lifetime of the forager increases as the probability of jumping ($p_j$) decreases, and the optimal strategy is to always jump to a uniformly randomly chosen site on the lattice.\n\n**B)** The lifetime of the forager decreases as the probability of jumping ($p_j$) increases, and the optimal strategy is to always diffuse to adjacent sites on the lattice.\n\n**C)** The lifetime of the forager is independent of the probability of jumping ($p_j$), and the optimal strategy is to alternate between jumping and diffusing.\n\n**D)** The lifetime of the forager increases as the probability of jumping ($p_j$) increases, and the optimal strategy is to jump to a uniformly randomly chosen site on the lattice when the forager lands consecutively on $s$ empty sites.\n\n**Correct Answer:** D) The lifetime of the forager increases as the probability of jumping ($p_j$) increases, and the optimal strategy is to jump to a uniformly randomly chosen site on the lattice when the forager lands consecutively on $s$ empty sites.\n\n**Explanation:** According to the documentation, the lifetime of the forager varies non-monotonically according to $p_j$, the probability of jumping. When $s$ far exceeds the number $n$ of lattice sites, the lifetime of the forager can be approximated by the sum of the cover time $\\tau_{\\rm cover}$ and the starvation time $s$. The correct answer, D, states that the lifetime of the forager increases as the probability of jumping ($p_j$) increases, and the optimal strategy is to jump to a uniformly randomly chosen site on the lattice when the forager lands consecutively on $s$ empty sites. This is consistent with the findings of the study, which shows that the strategy that maximizes the forager lifetime is a mixture of both modes of movement."}, "8": {"documentation": {"title": "Human Social Cycling Spectrum", "source": "Wang Zhijian, Yao Qingmei", "docs_id": "2012.03315", "section": ["econ.TH", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Human Social Cycling Spectrum. This paper investigates the reality and accuracy of evolutionary game dynamics theory in human game behavior experiments. In classical game theory, the central concept is Nash equilibrium, which reality and accuracy has been well known since the firstly illustration by the O'Neill game experiment in 1987. In game dynamics theory, the central approach is dynamics equations, however, its reality and accuracy is rare known, especially in high dimensional games. By develop a new approach, namely the eigencycle approach, with the eigenvectors from the game dynamics equations, we discover the fine structure of the cycles in the same experiments. We show that, the eigencycle approach can increase the accuracy by an order of magnitude in the human dynamic hehavior data. As the eigenvector is fundamental in dynamical systems theory which has applications in natural, social, and virtual worlds, the power of the eigencycles is expectedly. Inspired by the high dimensional eigencycles, we suggest that, the mathematical concept, namely 'invariant manifolds', could be a candidate as the central concept for the game dynamics theory, like the fixed point concept for classical game theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Human Social Cycling Spectrum\" propose a new approach to understanding game dynamics theory, specifically the eigencycle approach. What is the primary advantage of this approach, according to the paper, in terms of increasing the accuracy of human dynamic behavior data?\n\nA) It reduces the dimensionality of the game dynamics equations, making them more manageable.\nB) It increases the resolution of the game dynamics equations, allowing for a more detailed analysis of the cycles.\nC) It increases the accuracy of the game dynamics equations by an order of magnitude, as demonstrated in the paper.\nD) It provides a more intuitive understanding of the game dynamics equations, making them easier to interpret.\n\n**Correct Answer:** C) It increases the accuracy of the game dynamics equations by an order of magnitude, as demonstrated in the paper.\n\n**Explanation:** The paper states that the eigencycle approach can increase the accuracy of human dynamic behavior data by an order of magnitude. This is a key finding of the paper and highlights the potential power of the eigencycle approach in understanding game dynamics theory. The other options are incorrect because they do not accurately reflect the primary advantage of the eigencycle approach as described in the paper."}, "9": {"documentation": {"title": "Invariant higher-order variational problems", "source": "F. Gay-Balmaz, D. D. Holm, D. M. Meier, T. S. Ratiu, F.-X. Vialard", "docs_id": "1012.5060", "section": ["nlin.CD", "math-ph", "math.AP", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Invariant higher-order variational problems. We investigate higher-order geometric $k$-splines for template matching on Lie groups. This is motivated by the need to apply diffeomorphic template matching to a series of images, e.g., in longitudinal studies of Computational Anatomy. Our approach formulates Euler-Poincar\\'e theory in higher-order tangent spaces on Lie groups. In particular, we develop the Euler-Poincar\\'e formalism for higher-order variational problems that are invariant under Lie group transformations. The theory is then applied to higher-order template matching and the corresponding curves on the Lie group of transformations are shown to satisfy higher-order Euler-Poincar\\'{e} equations. The example of SO(3) for template matching on the sphere is presented explicitly. Various cotangent bundle momentum maps emerge naturally that help organize the formulas. We also present Hamiltonian and Hamilton-Ostrogradsky Lie-Poisson formulations of the higher-order Euler-Poincar\\'e theory for applications on the Hamiltonian side."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a higher-order variational problem on the Lie group of diffeomorphisms, SO(3), for template matching on the sphere. The problem is formulated using the Euler-Poincar\u00e9 theory in higher-order tangent spaces on Lie groups. Which of the following momentum maps emerges naturally from this formulation?\n\nA) The momentum map associated with the Hamiltonian formulation of the Euler-Poincar\u00e9 theory, which is given by the Lie-Poisson bracket of the Hamiltonian vector field with the momentum vector field.\n\nB) The momentum map associated with the Hamilton-Ostrogradsky formulation of the Euler-Poincar\u00e9 theory, which is given by the Lie-Poisson bracket of the Hamiltonian vector field with the Ostrogradsky vector field.\n\nC) The momentum map associated with the Euler-Poincar\u00e9 theory, which is given by the Lie-Poisson bracket of the Euler-Poincar\u00e9 vector field with the momentum vector field.\n\nD) The momentum map associated with the Hamiltonian formulation of the Euler-Poincar\u00e9 theory, which is given by the Lie-Poisson bracket of the Hamiltonian vector field with the cotangent bundle vector field.\n\nCorrect Answer: C) The momentum map associated with the Euler-Poincar\u00e9 theory, which is given by the Lie-Poisson bracket of the Euler-Poincar\u00e9 vector field with the momentum vector field.\n\nExplanation: The correct answer is C) because the Euler-Poincar\u00e9 theory is formulated in higher-order tangent spaces on Lie groups, and the momentum map that emerges naturally from this formulation is given by the Lie-Poisson bracket of the Euler-Poincar\u00e9 vector field with the momentum vector field. This is a key result in the paper, and it is used to organize the formulas and provide a Hamiltonian formulation of the higher-order Euler-Poincar\u00e9 theory. The other options are incorrect because they are either related to the Hamiltonian or Hamilton-Ostrogradsky formulations, which are not the primary formulation used in the paper."}, "10": {"documentation": {"title": "Sparse Covariance Estimation in Logit Mixture Models", "source": "Youssef M Aboutaleb, Mazen Danaf, Yifei Xie, and Moshe Ben-Akiva", "docs_id": "2001.05034", "section": ["stat.ME", "econ.EM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparse Covariance Estimation in Logit Mixture Models. This paper introduces a new data-driven methodology for estimating sparse covariance matrices of the random coefficients in logit mixture models. Researchers typically specify covariance matrices in logit mixture models under one of two extreme assumptions: either an unrestricted full covariance matrix (allowing correlations between all random coefficients), or a restricted diagonal matrix (allowing no correlations at all). Our objective is to find optimal subsets of correlated coefficients for which we estimate covariances. We propose a new estimator, called MISC, that uses a mixed-integer optimization (MIO) program to find an optimal block diagonal structure specification for the covariance matrix, corresponding to subsets of correlated coefficients, for any desired sparsity level using Markov Chain Monte Carlo (MCMC) posterior draws from the unrestricted full covariance matrix. The optimal sparsity level of the covariance matrix is determined using out-of-sample validation. We demonstrate the ability of MISC to correctly recover the true covariance structure from synthetic data. In an empirical illustration using a stated preference survey on modes of transportation, we use MISC to obtain a sparse covariance matrix indicating how preferences for attributes are related to one another."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of logit mixture models, what is the primary objective of the MISC estimator, and how does it differ from traditional approaches to estimating covariance matrices?\n\n**A)** The MISC estimator aims to minimize the mean squared error between the estimated and true covariance matrices, while also ensuring that the estimated covariance matrix is block diagonal. This approach is useful for identifying correlated coefficients in logit mixture models.\n\n**B)** The MISC estimator seeks to maximize the likelihood of the data given the estimated covariance matrix, while also incorporating prior knowledge about the sparsity level of the covariance matrix. This approach is useful for identifying correlated coefficients in logit mixture models.\n\n**C)** The MISC estimator uses a mixed-integer optimization program to find an optimal block diagonal structure specification for the covariance matrix, corresponding to subsets of correlated coefficients, for any desired sparsity level. This approach is useful for identifying correlated coefficients in logit mixture models.\n\n**D)** The MISC estimator uses Markov Chain Monte Carlo (MCMC) posterior draws from the unrestricted full covariance matrix to estimate the optimal sparsity level of the covariance matrix. This approach is useful for identifying correlated coefficients in logit mixture models.\n\n**Correct Answer:** C) The MISC estimator uses a mixed-integer optimization program to find an optimal block diagonal structure specification for the covariance matrix, corresponding to subsets of correlated coefficients, for any desired sparsity level. This approach is useful for identifying correlated coefficients in logit mixture models.\n\n**Explanation:** The correct answer is C) because it accurately describes the primary objective of the MISC estimator. The MISC estimator uses a mixed-integer optimization program to find an optimal block diagonal structure specification for the covariance matrix, corresponding to subsets of correlated coefficients, for any desired sparsity level. This approach is useful for identifying correlated coefficients in logit mixture models. The other options are incorrect because they do not accurately describe the primary objective of the MISC estimator. Option A is incorrect because it mentions minimizing the mean squared error, which is not the primary objective of the MISC estimator. Option B is incorrect because it mentions maximizing the likelihood of the data, which is not the primary objective of the MISC estimator. Option D is incorrect because it mentions using MCMC posterior draws to estimate the optimal sparsity level, which is not the primary objective of the MISC estimator."}, "11": {"documentation": {"title": "Electron core ionization in compressed alkali metal cesium", "source": "V F Degtyareva", "docs_id": "1703.03972", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electron core ionization in compressed alkali metal cesium. Elements of group I in the Periodic table have valence electrons of s-type and are usually considered as simple metals. Crystal structures of these elements at ambient pressure are close-packed and high-symmetry of bcc and fcc types, defined by electrostatic (Madelung) energy. Diverse structures were found under high pressure with decrease of the coordination number, packing fraction and symmetry. Formation of complex structures can be understood within the model of Fermi sphere - Brillouin zone interactions and supported by Hume-Rothery arguments. With the volume decrease there is a gain in the band structure energy accompanied by a formation of many-faced Brillouin zone polyhedrons. Under compression to less than a half of the initial volume the interatomic distances become close to or smaller than the ionic radius which should lead to the electron core ionization. At strong compression it is necessary to assume that for alkali metals the valence electron band overlaps with the upper core electrons which increases the valence electron count under compression."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the expected outcome of compressing alkali metal cesium to less than a half of its initial volume, according to the model of Fermi sphere - Brillouin zone interactions?\n\nA) The formation of a simple bcc or fcc crystal structure\nB) The overlap of the valence electron band with the upper core electrons, leading to electron core ionization\nC) A decrease in the coordination number, packing fraction, and symmetry of the crystal structure\nD) The creation of a new, high-symmetry crystal structure with a different lattice parameter\n\n**Correct Answer:** B) The overlap of the valence electron band with the upper core electrons, leading to electron core ionization\n\n**Explanation:** According to the provided documentation, under compression to less than a half of the initial volume, the interatomic distances become close to or smaller than the ionic radius, which should lead to electron core ionization. This is because the valence electron band overlaps with the upper core electrons, increasing the valence electron count under compression. This is supported by the model of Fermi sphere - Brillouin zone interactions and Hume-Rothery arguments."}, "12": {"documentation": {"title": "3D Reconstruction from public webcams", "source": "Tianyu Wu, Konrad Schindler and Cenek Albl", "docs_id": "2108.09476", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "3D Reconstruction from public webcams. We investigate the possibility of 3D scene reconstruction from two or more overlapping webcam streams. A large, and growing, number of webcams observe places of interest and are publicly accessible. The question naturally arises: can we make use of this free data source for 3D computer vision? It turns out that the task to reconstruct scene structure from webcam streams is very different from standard structure-from-motion (SfM), and conventional SfM pipelines fail. In the webcam setting there are very few views of the same scene, in most cases only the minimum of two. These viewpoints often have large baselines and/or scale differences, their overlap is rather limited, and besides unknown internal and external calibration also their temporal synchronisation is unknown. On the other hand, they record rather large fields of view continuously over long time spans, so that they regularly observe dynamic objects moving through the scene. We show how to leverage recent advances in several areas of computer vision to adapt SfM reconstruction to this particular scenario and reconstruct the unknown camera poses, the 3D scene structure, and the 3D trajectories of dynamic objects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the main challenges in using webcam streams for 3D scene reconstruction, and how do these challenges differ from those encountered in standard structure-from-motion (SfM) pipelines?\n\n**A)** The main challenges are the limited number of views, large baselines, and unknown internal and external calibration, which are similar to those in SfM pipelines.\n\n**B)** The main challenges are the large fields of view, continuous observation over long time spans, and the presence of dynamic objects, which are similar to those in SfM pipelines.\n\n**C)** The main challenges are the limited overlap, large baselines, and unknown temporal synchronization of the webcam streams, which are distinct from those in SfM pipelines.\n\n**D)** The main challenges are the limited number of views, large fields of view, and unknown internal and external calibration, which are similar to those in SfM pipelines.\n\n**Correct Answer:** C) The main challenges are the limited overlap, large baselines, and unknown temporal synchronization of the webcam streams, which are distinct from those in SfM pipelines.\n\n**Explanation:** The correct answer is C) because the documentation highlights the unique challenges of using webcam streams for 3D scene reconstruction, including the limited overlap, large baselines, and unknown temporal synchronization of the streams. These challenges are distinct from those encountered in standard SfM pipelines, which are typically used for reconstruction from multiple views with known overlap and calibration. The other options are incorrect because they either downplay or misrepresent the unique challenges of using webcam streams for 3D scene reconstruction."}, "13": {"documentation": {"title": "The (super)conformal BMS$_3$ algebra", "source": "Oscar Fuentealba, Hernan A. Gonzalez, Alfredo Perez, David Tempo and\n  Ricardo Troncoso", "docs_id": "2011.08197", "section": ["hep-th", "gr-qc", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The (super)conformal BMS$_3$ algebra. The conformal extension of the BMS$_{3}$ algebra is constructed. Apart from an infinite number of 'superdilatations,' in order to incorporate 'superspecial conformal transformations,' the commutator of the latter with supertranslations strictly requires the presence of nonlinear terms in the remaining generators. The algebra appears to be very rigid, in the sense that its central extensions as well as the nonlinear terms coefficients become determined by the central charge of the Virasoro subalgebra. The wedge algebra corresponds to the conformal group in three spacetime dimensions $SO(3,2)$, so that the full algebra can also be interpreted as an infinite-dimensional nonlinear extension of the AdS$_{4}$ algebra with nontrivial central charges. Moreover, since the Lorentz subalgebra ($sl(2,R)$) is non-principally embedded within the conformal (wedge) algebra, according to the conformal weight of the generators, the conformal extension of BMS$_{3}$ can be further regarded as a $W_{(2,2,2,1)}$ algebra. An explicit canonical realization of the conformal extension of BMS$_{3}$ is then shown to emerge from the asymptotic structure of conformal gravity in 3D, endowed with a new set of boundary conditions. The supersymmetric extension is also briefly addressed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the conformal extension of the BMS$_{3}$ algebra and the AdS$_{4}$ algebra, and how does it relate to the central charges of the Virasoro subalgebra?\n\nA) The conformal extension of BMS$_{3}$ is equivalent to the AdS$_{4}$ algebra, with the central charges of the Virasoro subalgebra determining the coefficients of the nonlinear terms.\n\nB) The conformal extension of BMS$_{3}$ is a nonlinear extension of the AdS$_{4}$ algebra, with the central charges of the Virasoro subalgebra determining the central extensions of the algebra.\n\nC) The conformal extension of BMS$_{3}$ is a $W_{(2,2,2,1)}$ algebra, and the AdS$_{4}$ algebra is a subalgebra of the conformal extension, with the central charges of the Virasoro subalgebra determining the coefficients of the nonlinear terms.\n\nD) The conformal extension of BMS$_{3}$ is a supersymmetric extension of the AdS$_{4}$ algebra, with the central charges of the Virasoro subalgebra determining the supersymmetry generators.\n\nCorrect Answer: C) The conformal extension of BMS$_{3}$ is a $W_{(2,2,2,1)}$ algebra, and the AdS$_{4}$ algebra is a subalgebra of the conformal extension, with the central charges of the Virasoro subalgebra determining the coefficients of the nonlinear terms.\n\nExplanation: The conformal extension of BMS$_{3}$ is indeed a $W_{(2,2,2,1)}$ algebra, which is a nonlinear extension of the AdS$_{4}$ algebra. The central charges of the Virasoro subalgebra determine the coefficients of the nonlinear terms in the conformal extension. This relationship is a key feature of the conformal extension of BMS$_{3}$, and is a consequence of the conformal weight of the generators and the non-principal embedding of the Lorentz subalgebra within the conformal algebra."}, "14": {"documentation": {"title": "Incorporating Views on Marginal Distributions in the Calibration of Risk\n  Models", "source": "Santanu Dey, Sandeep Juneja, Karthyek R. A. Murthy", "docs_id": "1411.0570", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Incorporating Views on Marginal Distributions in the Calibration of Risk\n  Models. Entropy based ideas find wide-ranging applications in finance for calibrating models of portfolio risk as well as options pricing. The abstracted problem, extensively studied in the literature, corresponds to finding a probability measure that minimizes relative entropy with respect to a specified measure while satisfying constraints on moments of associated random variables. These moments may correspond to views held by experts in the portfolio risk setting and to market prices of liquid options for options pricing models. However, it is reasonable that in the former settings, the experts may have views on tails of risks of some securities. Similarly, in options pricing, significant literature focuses on arriving at the implied risk neutral density of benchmark instruments through observed market prices. With the intent of calibrating models to these more general stipulations, we develop a unified entropy based methodology to allow constraints on both moments as well as marginal distributions of functions of underlying securities. This is applied to Markowitz portfolio framework, where a view that a particular portfolio incurs heavy tailed losses is shown to lead to fatter and more reasonable tails for losses of component securities. We also use this methodology to price non-traded options using market information such as observed option prices and implied risk neutral densities of benchmark instruments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of calibrating risk models, what is the primary goal of minimizing relative entropy with respect to a specified measure, and how does this goal relate to the views held by experts in the portfolio risk setting?\n\n**A)** Minimizing relative entropy is the primary goal of calibrating risk models, as it allows for the incorporation of expert views on moments of associated random variables, thereby ensuring that the calibrated model is consistent with market prices of liquid options.\n\n**B)** Minimizing relative entropy is the primary goal of calibrating risk models, as it enables the incorporation of expert views on marginal distributions of functions of underlying securities, thereby allowing for more general stipulations to be satisfied.\n\n**C)** Minimizing relative entropy is not the primary goal of calibrating risk models, as it is primarily concerned with minimizing the difference between two probability measures, rather than incorporating expert views.\n\n**D)** Minimizing relative entropy is the primary goal of calibrating risk models, as it allows for the incorporation of expert views on tails of risks of some securities, thereby enabling the calibration of models to more general stipulations.\n\n**Correct Answer:** D) Minimizing relative entropy is the primary goal of calibrating risk models, as it allows for the incorporation of expert views on tails of risks of some securities, thereby enabling the calibration of models to more general stipulations.\n\n**Explanation:** The correct answer is D) because the abstracted problem in the paper corresponds to finding a probability measure that minimizes relative entropy with respect to a specified measure while satisfying constraints on moments of associated random variables. The paper also mentions that experts may have views on tails of risks of some securities, and that the proposed methodology allows for the incorporation of these views, thereby enabling the calibration of models to more general stipulations."}, "15": {"documentation": {"title": "The Two-Way Wiretap Channel: Achievable Regions and Experimental Results", "source": "Aly El Gamal, O. Ozan Koyluoglu, Moustafa Youssef, and Hesham El Gamal", "docs_id": "1006.0778", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Two-Way Wiretap Channel: Achievable Regions and Experimental Results. This work considers the two-way wiretap channel in which two legitimate users, Alice and Bob, wish to exchange messages securely in the presence of a passive eavesdropper Eve. In the full-duplex scenario, where each node can transmit and receive simultaneously, we obtain new achievable secrecy rate regions based on the idea of allowing the two users to jointly optimize their channel prefixing distributions and binning codebooks in addition to key sharing. The new regions are shown to be strictly larger than the known ones for a wide class of discrete memoryless and Gaussian channels. In the half-duplex case, where a user can only transmit or receive on any given degree of freedom, we introduce the idea of randomized scheduling and establish the significant gain it offers in terms of the achievable secrecy sum-rate. We further develop an experimental setup based on a IEEE 802.15.4-enabled sensor boards, and use this testbed to show that one can exploit the two-way nature of the communication, via appropriately randomizing the transmit power levels and transmission schedule, to introduce significant ambiguity at a noiseless Eve."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the two-way wiretap channel, what is the primary advantage of allowing Alice and Bob to jointly optimize their channel prefixing distributions and binning codebooks in addition to key sharing, as proposed in the work \"The Two-Way Wiretap Channel: Achievable Regions and Experimental Results\"?\n\n**A)** The joint optimization enables the users to reduce the impact of Eve's eavesdropping on the secrecy rate.\n**B)** The joint optimization allows the users to exploit the two-way nature of the channel to introduce ambiguity at Eve's end, thereby increasing the achievable secrecy sum-rate.\n**C)** The joint optimization improves the key sharing process by enabling the users to share more secure keys.\n**D)** The joint optimization reduces the computational complexity of the channel prefixing and binning process.\n\n**Correct Answer:** B) The joint optimization allows the users to exploit the two-way nature of the channel to introduce ambiguity at Eve's end, thereby increasing the achievable secrecy sum-rate.\n\n**Explanation:** The correct answer is B) because the work \"The Two-Way Wiretap Channel: Achievable Regions and Experimental Results\" proposes a new approach to the two-way wiretap channel, where Alice and Bob jointly optimize their channel prefixing distributions and binning codebooks in addition to key sharing. This joint optimization enables the users to exploit the two-way nature of the channel, allowing them to introduce ambiguity at Eve's end, which in turn increases the achievable secrecy sum-rate. The other options are incorrect because they do not accurately reflect the primary advantage of the joint optimization proposed in the work."}, "16": {"documentation": {"title": "The Augmented Synthetic Control Method", "source": "Eli Ben-Michael, Avi Feller, Jesse Rothstein", "docs_id": "1811.04170", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Augmented Synthetic Control Method. The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The \"synthetic control\" is a weighted average of control units that balances the treated unit's pre-treatment outcomes as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pre-treatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pre-treatment fit is infeasible. Analogous to bias correction for inexact matching, Augmented SCM uses an outcome model to estimate the bias due to imperfect pre-treatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pre-treatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Augmented Synthetic Control Method (ASCM) is proposed as an extension of the Synthetic Control Method (SCM) to settings where the pre-treatment fit is infeasible. What is the primary goal of using an outcome model in ASCM, and how does it differ from bias correction for inexact matching?\n\n**A)** The outcome model is used to estimate the bias due to imperfect pre-treatment fit, and then the original SCM estimate is de-biased. This approach is similar to bias correction for inexact matching, but uses a more robust method to estimate the bias.\n\n**B)** The outcome model is used to directly control pre-treatment fit while minimizing extrapolation from the convex hull. This approach is distinct from bias correction for inexact matching, as it focuses on controlling pre-treatment fit rather than estimating bias.\n\n**C)** The outcome model is used to estimate the bias due to imperfect pre-treatment fit, and then the original SCM estimate is de-biased. This approach is similar to bias correction for inexact matching, but uses a more robust method to estimate the bias, such as ridge regression.\n\n**D)** The outcome model is used to directly control pre-treatment fit while minimizing extrapolation from the convex hull. This approach is distinct from bias correction for inexact matching, as it focuses on controlling pre-treatment fit rather than estimating bias, and uses ridge regression as the outcome model.\n\n**Correct Answer:** B) The outcome model is used to directly control pre-treatment fit while minimizing extrapolation from the convex hull. This approach is distinct from bias correction for inexact matching, as it focuses on controlling pre-treatment fit rather than estimating bias.\n\n**Explanation:** The correct answer is B) because it accurately describes the primary goal of using an outcome model in ASCM. The outcome model is used to directly control pre-treatment fit while minimizing extrapolation from the convex hull, which is a key feature of the ASCM approach. This approach is distinct from bias correction for inexact matching, as it focuses on controlling pre-treatment fit rather than estimating bias. The other options are incorrect because they either describe the outcome model as being used to estimate bias (A and C) or do not accurately describe the primary goal of using the outcome model (D)."}, "17": {"documentation": {"title": "Variability of Hot Supergiant IRAS 19336-0400 in the Early Phase of its\n  Planetary Nebula Ionization", "source": "V.P. Arkhipova, M.A. Burlak, V.F. Esipov, N.P. Ikonnikova, G.V.\n  Komissarova", "docs_id": "1111.2190", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variability of Hot Supergiant IRAS 19336-0400 in the Early Phase of its\n  Planetary Nebula Ionization. We present photoelectric and spectral observations of a hot candidate proto-planetary nebula - early B-type supergiant with emission lines in spectrum - IRAS 19336-0400. The light and color curves display fast irregular brightness variations with maximum amplitudes Delta V=0.30 mag, Delta B=0.35 mag, Delta U=0.40 mag and color-brightness correlations. By the variability characteristics IRAS 19336-0400 appears similar to other hot proto-planetary nebulae. Based on low-resolution spectra in the range lambda 4000-7500 A we have derived absolute intensities of the emission lines H_alpha, H_beta, H_gamma, [SII], [NII], physical conditions in gaseous nebula: n_e=10^4 cm^{-3}, T_e=7000 \\pm 1000 K. The emission line H_alpha, H_beta equivalent widths are found to be considerably variable and related to light changes. By UBV-photometry and spectroscopy the color excess has been estimated: E_{B-V}=0.50-0.54. Joint photometric and spectral data analysis allows us to assume that the star variability is caused by stellar wind variations."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the most likely cause of the variability in the light and color curves of the hot supergiant IRAS 19336-0400, as observed in the early phase of its planetary nebula ionization?\n\n**A)** The variability is caused by changes in the density of the circumstellar gas.\n**B)** The variability is caused by stellar wind variations, as suggested by joint photometric and spectral data analysis.\n**C)** The variability is caused by the presence of a binary companion star.\n**D)** The variability is caused by the star's intrinsic variability, such as pulsations or rotation.\n\n**Correct Answer:** B) The variability is caused by stellar wind variations, as suggested by joint photometric and spectral data analysis.\n\n**Explanation:** The correct answer is supported by the text, which states that \"joint photometric and spectral data analysis allows us to assume that the star variability is caused by stellar wind variations.\" This conclusion is based on the observation of color-brightness correlations and the variability of emission lines, which are consistent with the effects of stellar wind variations. The other options are not supported by the text and are therefore incorrect."}, "18": {"documentation": {"title": "Towards Safer Self-Driving Through Great PAIN (Physically Adversarial\n  Intelligent Networks)", "source": "Piyush Gupta, Demetris Coleman, Joshua E. Siegel", "docs_id": "2003.10662", "section": ["cs.LG", "cs.MA", "cs.SY", "eess.SY", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Safer Self-Driving Through Great PAIN (Physically Adversarial\n  Intelligent Networks). Automated vehicles' neural networks suffer from overfit, poor generalizability, and untrained edge cases due to limited data availability. Researchers synthesize randomized edge-case scenarios to assist in the training process, though simulation introduces potential for overfit to latent rules and features. Automating worst-case scenario generation could yield informative data for improving self driving. To this end, we introduce a \"Physically Adversarial Intelligent Network\" (PAIN), wherein self-driving vehicles interact aggressively in the CARLA simulation environment. We train two agents, a protagonist and an adversary, using dueling double deep Q networks (DDDQNs) with prioritized experience replay. The coupled networks alternately seek-to-collide and to avoid collisions such that the \"defensive\" avoidance algorithm increases the mean-time-to-failure and distance traveled under non-hostile operating conditions. The trained protagonist becomes more resilient to environmental uncertainty and less prone to corner case failures resulting in collisions than the agent trained without an adversary."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary goal of the \"Physically Adversarial Intelligent Network\" (PAIN) introduced in the paper, and how does it contribute to improving the resilience of self-driving vehicles?\n\n**A)** To generate worst-case scenarios for training self-driving vehicles, thereby improving their ability to handle edge cases.\n**B)** To develop a more robust and resilient neural network architecture for self-driving vehicles, by pitting a \"protagonist\" agent against an \"adversary\" agent in a simulated environment.\n**C)** To reduce the need for large amounts of labeled data for training self-driving vehicles, by leveraging the adversarial interactions between the two agents.\n**D)** To improve the generalizability of self-driving vehicles by training them on a diverse set of scenarios, including those that are unlikely to occur in real-world driving.\n\n**Correct Answer:** B) To develop a more robust and resilient neural network architecture for self-driving vehicles, by pitting a \"protagonist\" agent against an \"adversary\" agent in a simulated environment.\n\n**Explanation:** The correct answer is B) because the paper describes the PAIN as a framework for training self-driving vehicles using a \"dueling double deep Q networks\" (DDDQN) architecture, where two agents interact aggressively in a simulated environment. The primary goal of this framework is to develop a more robust and resilient neural network architecture for self-driving vehicles, by pitting the \"protagonist\" agent (the self-driving vehicle) against the \"adversary\" agent (which seeks to collide with the protagonist). This adversarial interaction helps to improve the protagonist's ability to handle edge cases and reduce the risk of corner case failures, resulting in improved safety and resilience."}, "19": {"documentation": {"title": "Evolutionary dynamics of the most populated genotype on rugged fitness\n  landscapes", "source": "Kavita Jain", "docs_id": "0706.0406", "section": ["q-bio.PE", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolutionary dynamics of the most populated genotype on rugged fitness\n  landscapes. We consider an asexual population evolving on rugged fitness landscapes which are defined on the multi-dimensional genotypic space and have many local optima. We track the most populated genotype as it changes when the population jumps from a fitness peak to a better one during the process of adaptation. This is done using the dynamics of the shell model which is a simplified version of the quasispecies model for infinite populations and standard Wright-Fisher dynamics for large finite populations. We show that the population fraction of a genotype obtained within the quasispecies model and the shell model match for fit genotypes and at short times, but the dynamics of the two models are identical for questions related to the most populated genotype. We calculate exactly several properties of the jumps in infinite populations some of which were obtained numerically in previous works. We also present our preliminary simulation results for finite populations. In particular, we measure the jump distribution in time and find that it decays as $t^{-2}$ as in the quasispecies problem."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the shell model and the quasispecies model, what is the relationship between the population fraction of a genotype and the dynamics of the most populated genotype, and how do these relationships differ between the two models?\n\n**A)** The population fraction of a genotype is identical to the dynamics of the most populated genotype in both the shell model and the quasispecies model.\n\n**B)** The population fraction of a genotype is identical to the dynamics of the most populated genotype in the shell model, but differs from it in the quasispecies model, where it decays as $t^{-2}$.\n\n**C)** The population fraction of a genotype is identical to the dynamics of the most populated genotype in the quasispecies model, but differs from it in the shell model, where it decays as $t^{-2}$.\n\n**D)** The population fraction of a genotype is identical to the dynamics of the most populated genotype in both the shell model and the quasispecies model, but only for fit genotypes and at short times.\n\n**Correct Answer:** D) The population fraction of a genotype is identical to the dynamics of the most populated genotype in both the shell model and the quasispecies model, but only for fit genotypes and at short times.\n\n**Explanation:** The correct answer is D) because the question states that the population fraction of a genotype obtained within the quasispecies model and the shell model match for fit genotypes and at short times. However, the question also states that the dynamics of the two models are identical for questions related to the most populated genotype, which implies that the population fraction of a genotype is identical to the dynamics of the most populated genotype in both models. Therefore, the correct answer is D), which specifies that this relationship holds only for fit genotypes and at short times."}, "20": {"documentation": {"title": "Instanton rate constant calculations using interpolated potential energy\n  surfaces in non-redundant, rotationally and translationally invariant\n  coordinates", "source": "Sean R. McConnell, Johannes K\\\"astner", "docs_id": "2009.05622", "section": ["physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Instanton rate constant calculations using interpolated potential energy\n  surfaces in non-redundant, rotationally and translationally invariant\n  coordinates. A trivial flaw in the utilization of artificial neural networks in interpolating chemical potential energy surfaces (PES) whose descriptors are Cartesian coordinates is their dependence on simple translations and rotations of the molecule under consideration. A different set of descriptors can be chosen to circumvent this problem, internuclear distances, inverse internuclear distances or z-matrix coordinates are three such descriptors. The objective is to use an interpolated PES in instanton rate constant calculations, hence information on the energy, gradient and Hessian is required at coordinates in the vicinity of the tunneling path. Instanton theory relies on smoothly fitted Hessians, therefore we use energy, gradients and Hessians in the training procedure. A major challenge is presented in the proper back-transformation of the output gradients and Hessians from internal coordinates to Cartesian coordinates. We perform comparisons between our method, a previous approach and on-the-fly rate constant calcuations on the hydrogen abstraction from methanol and on the hydrogen addition to isocyanic acid."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in utilizing artificial neural networks to interpolate potential energy surfaces (PES) in instanton rate constant calculations, and how can it be addressed?\n\nA) The neural networks are not able to capture the complex interactions between atoms, requiring a different set of descriptors.\nB) The neural networks are too computationally intensive, requiring a more powerful computing infrastructure.\nC) The neural networks are not able to handle the smooth fitting of Hessians required by instanton theory, requiring a different optimization algorithm.\nD) The neural networks are not able to properly back-transform the output gradients and Hessians from internal coordinates to Cartesian coordinates, requiring a different coordinate system.\n\nCorrect Answer: D) The neural networks are not able to properly back-transform the output gradients and Hessians from internal coordinates to Cartesian coordinates, requiring a different coordinate system.\n\nExplanation: The correct answer is D) because the documentation states that a major challenge in the method is the proper back-transformation of the output gradients and Hessians from internal coordinates to Cartesian coordinates. This is a critical step in the process, as it allows the results to be interpreted in the context of the original molecular system. The other options are incorrect because they do not address the specific challenge mentioned in the documentation. Option A is incorrect because the documentation does not mention the need for a different set of descriptors. Option B is incorrect because the documentation does not mention the need for a more powerful computing infrastructure. Option C is incorrect because the documentation does not mention the need for a different optimization algorithm."}, "21": {"documentation": {"title": "The principle of microreversibility and the fluctuation relations for\n  quantum systems driven out of equilibrium", "source": "Hiroshi Matsuoka", "docs_id": "1210.8085", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The principle of microreversibility and the fluctuation relations for\n  quantum systems driven out of equilibrium. For classical systems driven out of equilibrium, Crooks derived a relation (the Crooks-Jarzynski relation), whose special cases include a relation (the Crooks relation) equivalent to the Kawasaki non-linear response relation. We derive a quantum extension of the Crooks-Jarzynski relation without explicitly using the principle of microreversibility. Its special cases lead to the Jarzynski equality and the standard linear response theory with a Green-Kubo formula with a canonical correlation function. We also derive a quantum extension of the Crooks relation using the principle of microreversibility. Its special cases lead to the Jarzynski equality, the Crooks transient fluctuation theorem, and the fluctuation theorem for current or shear stress, which leads to a Green-Kubo formula with a symmetrized correlation function. For each quantum Crooks relation, there exists a corresponding quantum Crooks-Jarzynski relation. Using either relation, we can derive the Jarzynski equality, the fluctuation theorems mentioned above, and the standard linear response theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the Crooks-Jarzynski relation and the principle of microreversibility in quantum systems driven out of equilibrium, and how do the special cases of each relation lead to the Jarzynski equality and the standard linear response theory?\n\nA) The Crooks-Jarzynski relation is a direct extension of the principle of microreversibility, and its special cases lead to the Jarzynski equality and the standard linear response theory through a canonical correlation function.\n\nB) The Crooks-Jarzynski relation is a quantum extension of the principle of microreversibility, and its special cases lead to the Jarzynski equality and the standard linear response theory through a symmetrized correlation function.\n\nC) The Crooks-Jarzynski relation is a special case of the principle of microreversibility, and its special cases lead to the Jarzynski equality and the standard linear response theory through a Green-Kubo formula with a canonical correlation function.\n\nD) The Crooks-Jarzynski relation is a quantum extension of the principle of microreversibility, and its special cases lead to the Jarzynski equality and the standard linear response theory through a Green-Kubo formula with a symmetrized correlation function, but only when the system is in a canonical ensemble.\n\nCorrect Answer: B) The Crooks-Jarzynski relation is a quantum extension of the principle of microreversibility, and its special cases lead to the Jarzynski equality and the standard linear response theory through a symmetrized correlation function."}, "22": {"documentation": {"title": "FoolHD: Fooling speaker identification by Highly imperceptible\n  adversarial Disturbances", "source": "Ali Shahin Shamsabadi, Francisco Sep\\'ulveda Teixeira, Alberto Abad,\n  Bhiksha Raj, Andrea Cavallaro, Isabel Trancoso", "docs_id": "2011.08483", "section": ["cs.SD", "cs.LG", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "FoolHD: Fooling speaker identification by Highly imperceptible\n  adversarial Disturbances. Speaker identification models are vulnerable to carefully designed adversarial perturbations of their input signals that induce misclassification. In this work, we propose a white-box steganography-inspired adversarial attack that generates imperceptible adversarial perturbations against a speaker identification model. Our approach, FoolHD, uses a Gated Convolutional Autoencoder that operates in the DCT domain and is trained with a multi-objective loss function, in order to generate and conceal the adversarial perturbation within the original audio files. In addition to hindering speaker identification performance, this multi-objective loss accounts for human perception through a frame-wise cosine similarity between MFCC feature vectors extracted from the original and adversarial audio files. We validate the effectiveness of FoolHD with a 250-speaker identification x-vector network, trained using VoxCeleb, in terms of accuracy, success rate, and imperceptibility. Our results show that FoolHD generates highly imperceptible adversarial audio files (average PESQ scores above 4.30), while achieving a success rate of 99.6% and 99.2% in misleading the speaker identification model, for untargeted and targeted settings, respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the multi-objective loss function used in the FoolHD approach, and how does it relate to human perception?\n\n**A)** To maximize the speaker identification accuracy while minimizing the imperceptibility of the adversarial perturbation.\n**B)** To balance the trade-off between hindering speaker identification performance and preserving human perception through frame-wise cosine similarity between MFCC feature vectors.\n**C)** To optimize the Gated Convolutional Autoencoder for generating imperceptible adversarial perturbations.\n**D)** To enhance the robustness of the speaker identification model against adversarial attacks.\n\n**Correct Answer:** B) To balance the trade-off between hindering speaker identification performance and preserving human perception through frame-wise cosine similarity between MFCC feature vectors.\n\n**Explanation:** The multi-objective loss function used in FoolHD is designed to balance two competing objectives: hindering speaker identification performance and preserving human perception. The function accounts for human perception by incorporating a frame-wise cosine similarity between MFCC feature vectors extracted from the original and adversarial audio files. This ensures that the generated adversarial perturbation is not only imperceptible but also preserves the natural speech patterns and acoustic characteristics of the original audio, making it more difficult for the speaker identification model to detect."}, "23": {"documentation": {"title": "Optimal design of experiments by combining coarse and fine measurements", "source": "Alpha A. Lee, Michael P. Brenner, Lucy J. Colwell", "docs_id": "1702.06001", "section": ["physics.data-an", "cond-mat.soft", "cond-mat.stat-mech", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal design of experiments by combining coarse and fine measurements. In many contexts it is extremely costly to perform enough high quality experimental measurements to accurately parameterize a predictive quantitative model. However, it is often much easier to carry out large numbers of experiments that indicate whether each sample is above or below a given threshold. Can many such categorical or \"coarse\" measurements be combined with a much smaller number of high resolution or \"fine\" measurements to yield accurate models? Here, we demonstrate an intuitive strategy, inspired by statistical physics, wherein the coarse measurements are used to identify the salient features of the data, while the fine measurements determine the relative importance of these features. A linear model is inferred from the fine measurements, augmented by a quadratic term that captures the correlation structure of the coarse data. We illustrate our strategy by considering the problems of predicting the antimalarial potency and aqueous solubility of small organic molecules from their 2D molecular structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Optimal design of experiments by combining coarse and fine measurements, what is the primary goal of using coarse measurements to identify salient features of the data, while fine measurements determine their relative importance?\n\n**A)** To reduce the dimensionality of the data and improve model interpretability\n**B)** To capture the correlation structure of the coarse data and improve model accuracy\n**C)** To identify the most informative features of the data and prioritize their inclusion in the model\n**D)** To reduce the computational cost of model training and inference\n\n**Correct Answer:** C) To identify the most informative features of the data and prioritize their inclusion in the model\n\n**Explanation:** The correct answer is C) To identify the most informative features of the data and prioritize their inclusion in the model. The documentation states that the coarse measurements are used to identify the salient features of the data, while the fine measurements determine the relative importance of these features. This suggests that the primary goal is to identify the most informative features and prioritize their inclusion in the model, which is a key aspect of the proposed strategy.\n\nThe other options are incorrect because:\n\n* A) Reducing dimensionality and improving model interpretability is a secondary benefit of using coarse measurements, but not the primary goal.\n* B) Capturing the correlation structure of the coarse data is a key aspect of the model, but not the primary goal of using coarse measurements.\n* D) Reducing computational cost is not mentioned in the documentation as a goal of the proposed strategy."}, "24": {"documentation": {"title": "Topological structure and interaction strengths in model food webs", "source": "Christopher Quince, Paul Higgs and Alan McKane", "docs_id": "q-bio/0402014", "section": ["q-bio.PE", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological structure and interaction strengths in model food webs. We report the results of carrying out a large number of simulations on a coevolutionary model of multispecies communities. A wide range of parameter values were investigated which allowed a rather complete picture of the change in behaviour of the model as these parameters were varied to be built up. Our main interest was in the nature of the community food webs constructed via the simulations. We identify the range of parameter values which give rise to realistic food webs and give arguments which allow some of the structure which is found to be understood in an intuitive way. Since the webs are evolved according to the rules of the model, the strengths of the predator-prey links are not determined a priori, and emerge from the process of constructing the web. We measure the distribution of these link strengths, and find that there are a large number of weak links, in agreement with recent suggestions. We also review some of the data on food webs available in the literature, and make some tentative comparisons with our results. The difficulties of making such comparisons and the possible future developments of the model are also briefly discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Topological structure and interaction strengths in model food webs\" investigated the coevolutionary dynamics of multispecies communities using a large number of simulations. What is the main finding regarding the distribution of link strengths in the simulated food webs, and how does it relate to recent suggestions in the field?\n\n**A)** The authors found that the distribution of link strengths is uniform, and this is in disagreement with recent suggestions that there are a large number of weak links.\n\n**B)** The authors found that the distribution of link strengths is skewed towards strong links, and this is consistent with recent suggestions that there are a large number of weak links.\n\n**C)** The authors found that the distribution of link strengths is bimodal, with a large number of weak links and a smaller number of strong links, but the exact distribution is not well understood.\n\n**D)** The authors found that the distribution of link strengths is independent of the parameter values used in the simulations, and this is in agreement with recent suggestions that the structure of food webs is determined by random chance.\n\n**Correct Answer:** B) The authors found that the distribution of link strengths is skewed towards strong links, and this is consistent with recent suggestions that there are a large number of weak links.\n\n**Explanation:** The correct answer is B) because the authors of the study found that the distribution of link strengths in the simulated food webs is skewed towards strong links, which is consistent with recent suggestions that there are a large number of weak links. This is a key finding of the study, and it is supported by the fact that the authors measure the distribution of link strengths and find that there are a large number of weak links. The other options are incorrect because they do not accurately reflect the findings of the study. Option A is incorrect because the authors found that there are a large number of weak links, not that the distribution is uniform. Option C is incorrect because the authors found that the distribution is skewed towards strong links, not bimodal. Option D is incorrect because the distribution of link strengths is not independent of the parameter values used in the simulations."}, "25": {"documentation": {"title": "On the Mixing of Diffusing Particles", "source": "E. Ben-Naim", "docs_id": "1010.2563", "section": ["cond-mat.stat-mech", "cond-mat.soft", "math.CO", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Mixing of Diffusing Particles. We study how the order of N independent random walks in one dimension evolves with time. Our focus is statistical properties of the inversion number m, defined as the number of pairs that are out of sort with respect to the initial configuration. In the steady-state, the distribution of the inversion number is Gaussian with the average <m>~N^2/4 and the standard deviation sigma N^{3/2}/6. The survival probability, S_m(t), which measures the likelihood that the inversion number remains below m until time t, decays algebraically in the long-time limit, S_m t^{-beta_m}. Interestingly, there is a spectrum of N(N-1)/2 distinct exponents beta_m(N). We also find that the kinetics of first-passage in a circular cone provides a good approximation for these exponents. When N is large, the first-passage exponents are a universal function of a single scaling variable, beta_m(N)--> beta(z) with z=(m-<m>)/sigma. In the cone approximation, the scaling function is a root of a transcendental equation involving the parabolic cylinder equation, D_{2 beta}(-z)=0, and surprisingly, numerical simulations show this prediction to be exact."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Consider a system of N independent random walks in one dimension, where the inversion number m is defined as the number of pairs that are out of sort with respect to the initial configuration. In the steady-state, the distribution of the inversion number is Gaussian with the average <m>~N^2/4 and the standard deviation sigma N^{3/2}/6. What is the scaling function beta(z) in the cone approximation, where z=(m-<m>)/sigma, and how does it relate to the parabolic cylinder equation D_{2 beta}(-z)=0?\n\nA) The scaling function beta(z) is a polynomial function of z, specifically beta(z) = z^2.\nB) The scaling function beta(z) is a transcendental function of z, specifically beta(z) = tanh(z^2).\nC) The scaling function beta(z) is a root of the parabolic cylinder equation D_{2 beta}(-z)=0, and its value is given by beta(z) = -z.\nD) The scaling function beta(z) is a universal function that depends only on the value of m, and its value is given by beta(z) = m/(N^2).\n\nCorrect Answer: C) The scaling function beta(z) is a root of the parabolic cylinder equation D_{2 beta}(-z)=0, and its value is given by beta(z) = -z.\n\nExplanation: The correct answer is based on the fact that the scaling function beta(z) is a root of the parabolic cylinder equation D_{2 beta}(-z)=0, as stated in the original text. The value of beta(z) is given by beta(z) = -z, which is a direct consequence of the equation. The other options are incorrect because they do not accurately describe the relationship between the scaling function and the parabolic cylinder equation."}, "26": {"documentation": {"title": "A Penalty Decomposition Algorithm with Greedy Improvement for\n  Mean-Reverting Portfolios with Sparsity and Volatility Constraints", "source": "Ahmad Mousavi and Jinglai Shen", "docs_id": "2104.07887", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Penalty Decomposition Algorithm with Greedy Improvement for\n  Mean-Reverting Portfolios with Sparsity and Volatility Constraints. Mean-reverting portfolios with few assets, but high variance, are of great interest for investors in financial markets. Such portfolios are straightforwardly profitable because they include a small number of assets whose prices not only oscillate predictably around a long-term mean but also possess enough volatility. Roughly speaking, sparsity minimizes trading costs, volatility provides arbitrage opportunities, and mean-reversion property equips investors with ideal investment strategies. Finding such favorable portfolios can be formulated as a nonconvex quadratic optimization problem with an additional sparsity constraint. To the best of our knowledge, there is no method for solving this problem and enjoying favorable theoretical properties yet. In this paper, we develop an effective two-stage algorithm for this problem. In the first stage, we apply a tailored penalty decomposition method for finding a stationary point of this nonconvex problem. For a fixed penalty parameter, the block coordinate descent method is utilized to find a stationary point of the associated penalty subproblem. In the second stage, we improve the result from the first stage via a greedy scheme that solves restricted nonconvex quadratically constrained quadratic programs (QCQPs). We show that the optimal value of such a QCQP can be obtained by solving their semidefinite relaxations. Numerical experiments on S\\&P 500 are conducted to demonstrate the effectiveness of the proposed algorithm."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a portfolio optimization problem where the goal is to find a mean-reverting portfolio with sparsity and volatility constraints. The objective function is a nonconvex quadratic function, and the constraints include a sparsity constraint and a volatility constraint. Develop an effective two-stage algorithm to solve this problem.\n\n**A)** The proposed algorithm in the paper is a single-stage algorithm that uses a tailored penalty decomposition method to find a stationary point of the nonconvex problem. The algorithm then improves the result using a greedy scheme that solves restricted nonconvex quadratically constrained quadratic programs (QCQPs).\n\n**B)** The proposed algorithm in the paper is a two-stage algorithm that first applies a tailored penalty decomposition method to find a stationary point of the nonconvex problem. The algorithm then improves the result using a block coordinate descent method to find a local minimum of the associated penalty subproblem.\n\n**C)** The proposed algorithm in the paper is a two-stage algorithm that first applies a tailored penalty decomposition method to find a stationary point of the nonconvex problem. The algorithm then improves the result using a greedy scheme that solves semidefinite relaxations of the associated QCQPs.\n\n**D)** The proposed algorithm in the paper is a single-stage algorithm that uses a block coordinate descent method to find a local minimum of the nonconvex problem. The algorithm then improves the result using a tailored penalty decomposition method to find a stationary point of the associated penalty subproblem.\n\n**Correct Answer:** C) The proposed algorithm in the paper is a two-stage algorithm that first applies a tailored penalty decomposition method to find a stationary point of the nonconvex problem. The algorithm then improves the result using a greedy scheme that solves semidefinite relaxations of the associated QCQPs.\n\n**Explanation:** The correct answer is C) because the paper describes a two-stage algorithm that first applies a tailored penalty decomposition method to find a stationary point of the nonconvex problem, and then improves the result using a greedy scheme that solves semidefinite relaxations of the associated QCQPs. This is consistent with the description in the paper. The other options are incorrect because they do not accurately describe the proposed algorithm."}, "27": {"documentation": {"title": "Pinned modes in two-dimensional lossy lattices with local gain and\n  nonlinearity", "source": "Edwin Ding, A. Y. S. Tang, K. W. Chow, and Boris A. Malomed", "docs_id": "1404.5056", "section": ["nlin.PS", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pinned modes in two-dimensional lossy lattices with local gain and\n  nonlinearity. We introduce a system with one or two amplified nonlinear sites (\"hot spots\", HSs) embedded into a two-dimensional linear lossy lattice. The system describes an array of evanescently coupled optical or plasmonic waveguides, with gain applied at selected HS cores. The subject of the analysis is discrete solitons pinned to the HSs. The shape of the localized modes is found in quasi-analytical and numerical forms, using a truncated lattice for the analytical consideration. Stability eigenvalues are computed numerically, and the results are supplemented by direct numerical simulations. In the case of self-focusing nonlinearity, the modes pinned to a single HS are stable or unstable when the nonlinearity includes the cubic loss or gain, respectively. If the nonlinearity is self-defocusing, the unsaturated cubic gain acting at the HS supports stable modes in a small parametric area, while weak cubic loss gives rise to a bistability of the discrete solitons. Symmetric and antisymmetric modes pinned to a symmetric set of two HSs are considered too."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In a two-dimensional lossy lattice with local gain and nonlinearity, what is the condition for the stability of discrete solitons pinned to a single hot spot (HS) in terms of the nonlinearity?\n\nA) The nonlinearity must be self-focusing for stability.\nB) The nonlinearity must be self-defocusing for stability.\nC) The nonlinearity must be cubic gain for stability.\nD) The nonlinearity must be cubic loss for stability.\n\nCorrect Answer: B) The nonlinearity must be self-defocusing for stability.\n\nExplanation: According to the documentation, in the case of self-defocusing nonlinearity, the unsaturated cubic gain acting at the HS supports stable modes in a small parametric area. Therefore, the correct answer is B) The nonlinearity must be self-defocusing for stability."}, "28": {"documentation": {"title": "Which bills are lobbied? Predicting and interpreting lobbying activity\n  in the US", "source": "Ivan Slobozhan, Peter Ormosi, Rajesh Sharma", "docs_id": "2005.06386", "section": ["econ.GN", "cs.CL", "cs.LG", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Which bills are lobbied? Predicting and interpreting lobbying activity\n  in the US. Using lobbying data from OpenSecrets.org, we offer several experiments applying machine learning techniques to predict if a piece of legislation (US bill) has been subjected to lobbying activities or not. We also investigate the influence of the intensity of the lobbying activity on how discernible a lobbied bill is from one that was not subject to lobbying. We compare the performance of a number of different models (logistic regression, random forest, CNN and LSTM) and text embedding representations (BOW, TF-IDF, GloVe, Law2Vec). We report results of above 0.85% ROC AUC scores, and 78% accuracy. Model performance significantly improves (95% ROC AUC, and 88% accuracy) when bills with higher lobbying intensity are looked at. We also propose a method that could be used for unlabelled data. Through this we show that there is a considerably large number of previously unlabelled US bills where our predictions suggest that some lobbying activity took place. We believe our method could potentially contribute to the enforcement of the US Lobbying Disclosure Act (LDA) by indicating the bills that were likely to have been affected by lobbying but were not filed as such."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Predicting and interpreting lobbying activity in the US\" used various machine learning techniques to analyze lobbying data from OpenSecrets.org. Which of the following text embedding representations was found to significantly improve the performance of the models when predicting lobbying activity in bills with higher lobbying intensity?\n\nA) BOW (Bag-of-Words)\nB) TF-IDF (Term Frequency-Inverse Document Frequency)\nC) GloVe (Global Vectors for Word Representation)\nD) Law2Vec (a custom vector representation for legal texts)\n\n**Correct Answer:** C) GloVe (Global Vectors for Word Representation)\n\n**Explanation:** The paper reports that the performance of the models significantly improves (95% ROC AUC and 88% accuracy) when bills with higher lobbying intensity are looked at. This suggests that the text embedding representation used is able to capture the nuances of the language used in the bills, which is a key aspect of the GloVe representation. GloVe is a word embedding technique that represents words as vectors in a high-dimensional space, allowing for more accurate capture of semantic relationships between words. In the context of lobbying activity, GloVe may be able to capture the subtle differences in language used by lobbyists to influence lawmakers, leading to more accurate predictions of lobbying activity."}, "29": {"documentation": {"title": "Does the anomalous solar chemical composition come from planet\n  formation?", "source": "I. Ramirez (MPA), J. Melendez (CAUP), M. Asplund (MPA)", "docs_id": "0911.1893", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Does the anomalous solar chemical composition come from planet\n  formation?. We derive the abundance of 19 elements in a sample of 64 stars with fundamental parameters very similar to solar, which minimizes the impact of systematic errors in our spectroscopic 1D-LTE differential analysis, using high-resolution (R=60,000), high signal-to-noise ratio (S/N=200) spectra. The estimated errors in the elemental abundances relative to solar are as small as 0.025 dex. The abundance ratios [X/Fe] as a function of [Fe/H] agree closely with previously established patterns of Galactic thin-disk chemical evolution. Interestingly, the majority of our stars show a significant correlation between [X/Fe] and condensation temperature (Tc). In the sample of 22 stars with parameters closest to solar, we find that, on average, low Tc elements are depleted with respect to high Tc elements in the solar twins relative to the Sun by about 0.08 dex (20%). An increasing trend is observed for the abundances as a function of Tc for 900<Tc<1800 K, while abundances of lower Tc elements appear to be roughly constant. We speculate that this is a signature of the planet formation that occurred around the Sun but not in the majority of solar twins. If this hypothesis is correct, stars with planetary systems like ours, although rare (frequency of 15%), may be identified through a very detailed inspection of the chemical compositions of their host stars."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: What is the primary conclusion drawn from the analysis of the elemental abundances in a sample of 64 stars with fundamental parameters similar to the Sun, regarding the possible origin of the anomalous solar chemical composition?\n\nA) The anomalous solar chemical composition is likely due to the presence of a large planet in the solar system.\nB) The abundance ratios [X/Fe] as a function of [Fe/H] agree closely with previously established patterns of Galactic thin-disk chemical evolution, but the correlation with condensation temperature (Tc) suggests an alternative explanation.\nC) The majority of the stars in the sample show a significant correlation between [X/Fe] and condensation temperature (Tc), indicating that low Tc elements are depleted with respect to high Tc elements in the solar twins relative to the Sun.\nD) The estimated errors in the elemental abundances relative to solar are too large to draw any conclusions about the origin of the anomalous solar chemical composition.\n\nCorrect Answer: B) The abundance ratios [X/Fe] as a function of [Fe/H] agree closely with previously established patterns of Galactic thin-disk chemical evolution, but the correlation with condensation temperature (Tc) suggests an alternative explanation.\n\nExplanation: The correct answer is B because the analysis of the elemental abundances in the sample of 64 stars shows that the abundance ratios [X/Fe] as a function of [Fe/H] agree closely with previously established patterns of Galactic thin-disk chemical evolution. However, the correlation between [X/Fe] and condensation temperature (Tc) suggests an alternative explanation for the anomalous solar chemical composition, which is that it may be a signature of the planet formation that occurred around the Sun but not in the majority of solar twins."}, "30": {"documentation": {"title": "A Generalization of Permanent Inequalities and Applications in Counting\n  and Optimization", "source": "Nima Anari and Shayan Oveis Gharan", "docs_id": "1702.02937", "section": ["cs.DS", "cs.DM", "cs.IT", "math.CO", "math.IT", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Generalization of Permanent Inequalities and Applications in Counting\n  and Optimization. A polynomial $p\\in\\mathbb{R}[z_1,\\dots,z_n]$ is real stable if it has no roots in the upper-half complex plane. Gurvits's permanent inequality gives a lower bound on the coefficient of the $z_1z_2\\dots z_n$ monomial of a real stable polynomial $p$ with nonnegative coefficients. This fundamental inequality has been used to attack several counting and optimization problems. Here, we study a more general question: Given a stable multilinear polynomial $p$ with nonnegative coefficients and a set of monomials $S$, we show that if the polynomial obtained by summing up all monomials in $S$ is real stable, then we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$. We also prove generalizations of this theorem to (real stable) polynomials that are not multilinear. We use our theorem to give a new proof of Schrijver's inequality on the number of perfect matchings of a regular bipartite graph, generalize a recent result of Nikolov and Singh, and give deterministic polynomial time approximation algorithms for several counting problems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the main contribution of the paper \"A Generalization of Permanent Inequalities and Applications in Counting\" regarding real stable polynomials?\n\nA) It introduces a new method for computing the permanent of a matrix.\nB) It provides a lower bound on the coefficient of the $z_1z_2\\dots z_n$ monomial of a real stable polynomial $p$ with nonnegative coefficients.\nC) It shows that if the polynomial obtained by summing up all monomials in $S$ is real stable, then the sum of coefficients of monomials of $p$ that are in $S$ can be lower bounded.\nD) It proves that real stable polynomials have a specific property regarding their roots in the complex plane.\n\nCorrect Answer: C) It shows that if the polynomial obtained by summing up all monomials in $S$ is real stable, then the sum of coefficients of monomials of $p$ that are in $S$ can be lower bounded.\n\nExplanation: The correct answer is C) because the paper states that if the polynomial obtained by summing up all monomials in $S$ is real stable, then the sum of coefficients of monomials of $p$ that are in $S$ can be lower bounded. This is the main contribution of the paper, as it generalizes Gurvits's permanent inequality to a more general setting. The other options are incorrect because they either describe a different aspect of the paper or are not supported by the text."}, "31": {"documentation": {"title": "Microsecond Time Resolution Optical Photometry using a H.E.S.S.\n  Cherenkov Telescope", "source": "C. Deil (1), W. Domainko (1), G. Hermann (1) ((1) Max-Planck-Institut\n  f\\\"ur Kernphysik, Heidelberg, Germany)", "docs_id": "0810.3155", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microsecond Time Resolution Optical Photometry using a H.E.S.S.\n  Cherenkov Telescope. We have constructed an optical photometer with microsecond time resolution, which is currently being operated on one of the H.E.S.S. telescopes. H.E.S.S. is an array of four Cherenkov telescopes, each with a 107 m^2 mirror, located in the Khomas highland in Namibia. In its normal mode of operation H.E.S.S. observes Cherenkov light from air showers generated by very high energy gamma-rays in the upper atmosphere. Our detector consists of seven photomultipliers, one in the center to record the lightcurve from the target and six concentric photomultipliers as a veto system to reject disturbing signals e.g. from meteorites or lightning at the horizon. The data acquisition system has been designed to continuously record the signals with zero deadtime. The Crab pulsar has been observed to verify the performance of the instrument and the GPS timing system. Compact galactic targets were observed to search for flares on timescales of a few microseconds to ~ 100 milliseconds. The design and sensitivity of the instrument as well as the data analysis method are presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary purpose of the veto system in the optical photometer described in the paper, and how does it contribute to the overall performance of the instrument?\n\nA) To record the lightcurve from the target and provide a continuous signal\nB) To reject disturbing signals from meteorites or lightning at the horizon, thereby improving the sensitivity of the instrument\nC) To measure the time resolution of the instrument, allowing for the detection of microsecond-scale events\nD) To provide a backup system in case of a primary photomultiplier failure\n\nCorrect Answer: B) To reject disturbing signals from meteorites or lightning at the horizon, thereby improving the sensitivity of the instrument\n\nExplanation: The veto system consists of six concentric photomultipliers that are used to reject disturbing signals, such as those from meteorites or lightning at the horizon. By rejecting these signals, the instrument can improve its sensitivity and reduce false positives, allowing for more accurate measurements of the target signal. This is a critical aspect of the instrument's design, as it enables the detection of microsecond-scale events and improves the overall performance of the optical photometer."}, "32": {"documentation": {"title": "Coherently Enhanced Wireless Power Transfer", "source": "Alex Krasnok, Denis G. Baranov, Andrey Generalov, Sergey Li, and\n  Andrea Alu", "docs_id": "1801.01182", "section": ["physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coherently Enhanced Wireless Power Transfer. Extraction of electromagnetic energy by an antenna from impinging external radiation is at the basis of wireless communications and power transfer (WPT). The maximum of transferred energy is ensured when the antenna is conjugately matched, i.e., when it is resonant and it has an equal coupling with free space and its load, which is not easily implemented in near-field WPT. Here, we introduce the concept of coherently enhanced wireless power transfer. We show that a principle similar to the one underlying the operation of coherent perfect absorbers can be employed to improve the overall performance of WPT and potentially achieve its dynamic control. The concept relies on coherent excitation of the waveguide connected to the antenna load with a backward propagating signal of specific amplitude and phase. This signal creates a suitable interference pattern at the load resulting in a modification of the local wave impedance, which in turn enables conjugate matching and a largely increased amount of energy extracted to the waveguide. We develop an illustrative theoretical model describing this concept, demonstrate it with full-wave numerical simulations for the canonical example of a dipole antenna, and verify it experimentally in both near-field and far-field regimes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the fundamental principle underlying the concept of coherently enhanced wireless power transfer, and how does it differ from traditional wireless power transfer methods?\n\n**A)** The principle relies on the antenna being resonant and having an equal coupling with free space and its load, which is a characteristic of traditional wireless power transfer methods.\n\n**B)** The principle relies on the coherent excitation of the waveguide connected to the antenna load with a backward propagating signal of specific amplitude and phase, which enables conjugate matching and increased energy extraction.\n\n**C)** The principle relies on the use of a perfect absorber to absorb the external radiation and convert it into electrical energy, which is then transferred to the load.\n\n**D)** The principle relies on the use of a resonant cavity to store and amplify the external radiation, which is then transferred to the load.\n\n**Correct Answer:** B) The principle relies on the coherent excitation of the waveguide connected to the antenna load with a backward propagating signal of specific amplitude and phase, which enables conjugate matching and increased energy extraction.\n\n**Explanation:** The correct answer is B) because the concept of coherently enhanced wireless power transfer relies on the coherent excitation of the waveguide connected to the antenna load with a backward propagating signal of specific amplitude and phase. This signal creates a suitable interference pattern at the load, resulting in a modification of the local wave impedance, which enables conjugate matching and a largely increased amount of energy extracted to the waveguide. This principle is similar to the one underlying the operation of coherent perfect absorbers, but with a focus on enhancing wireless power transfer."}, "33": {"documentation": {"title": "HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition", "source": "Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Ian Reid", "docs_id": "2011.00450", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition. Visual place recognition needs to be robust against appearance variability due to natural and man-made causes. Training data collection should thus be an ongoing process to allow continuous appearance changes to be recorded. However, this creates an unboundedly-growing database that poses time and memory scalability challenges for place recognition methods. To tackle the scalability issue for visual place recognition in autonomous driving, we develop a Hidden Markov Model approach with a two-tiered memory management. Our algorithm, dubbed HM$^4$, exploits temporal look-ahead to transfer promising candidate images between passive storage and active memory when needed. The inference process takes into account both promising images and a coarse representations of the full database. We show that this allows constant time and space inference for a fixed coverage area. The coarse representations can also be updated incrementally to absorb new data. To further reduce the memory requirements, we derive a compact image representation inspired by Locality Sensitive Hashing (LSH). Through experiments on real world data, we demonstrate the excellent scalability and accuracy of the approach under appearance changes and provide comparisons against state-of-the-art techniques."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge that the HM4 algorithm aims to address in visual place recognition, and how does it attempt to overcome this challenge?\n\nA) The challenge of collecting a large dataset with diverse appearances, and HM4 addresses this by using a two-tiered memory management system that allows for efficient transfer of promising candidate images between passive storage and active memory.\n\nB) The challenge of maintaining a large database with constant memory requirements, and HM4 addresses this by using a compact image representation inspired by Locality Sensitive Hashing (LSH).\n\nC) The challenge of handling appearance variability due to natural and man-made causes, and HM4 addresses this by using a coarse representation of the full database that can be updated incrementally.\n\nD) The challenge of achieving real-time inference, and HM4 addresses this by exploiting temporal look-ahead to transfer promising candidate images between passive storage and active memory.\n\n**Correct Answer:** A) The challenge of collecting a large dataset with diverse appearances, and HM4 addresses this by using a two-tiered memory management system that allows for efficient transfer of promising candidate images between passive storage and active memory.\n\n**Explanation:** The question requires the test-taker to understand the primary challenge addressed by the HM4 algorithm, which is the need for continuous appearance changes to be recorded in a large dataset. The correct answer highlights the two-tiered memory management system as the key innovation that allows HM4 to overcome this challenge. The other options, while related to the topic, do not accurately capture the primary challenge addressed by HM4."}, "34": {"documentation": {"title": "Treatment Policy Learning in Multiobjective Settings with Fully Observed\n  Outcomes", "source": "Soorajnath Boominathan, Michael Oberst, Helen Zhou, Sanjat Kanjilal,\n  David Sontag", "docs_id": "2006.00927", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Treatment Policy Learning in Multiobjective Settings with Fully Observed\n  Outcomes. In several medical decision-making problems, such as antibiotic prescription, laboratory testing can provide precise indications for how a patient will respond to different treatment options. This enables us to \"fully observe\" all potential treatment outcomes, but while present in historical data, these results are infeasible to produce in real-time at the point of the initial treatment decision. Moreover, treatment policies in these settings often need to trade off between multiple competing objectives, such as effectiveness of treatment and harmful side effects. We present, compare, and evaluate three approaches for learning individualized treatment policies in this setting: First, we consider two indirect approaches, which use predictive models of treatment response to construct policies optimal for different trade-offs between objectives. Second, we consider a direct approach that constructs such a set of policies without intermediate models of outcomes. Using a medical dataset of Urinary Tract Infection (UTI) patients, we show that all approaches learn policies that achieve strictly better performance on all outcomes than clinicians, while also trading off between different objectives. We demonstrate additional benefits of the direct approach, including flexibly incorporating other goals such as deferral to physicians on simple cases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Treatment Policy Learning in Multiobjective Settings with Fully Observed Outcomes, what is the primary advantage of the direct approach over the indirect approaches, and how does it differ from the indirect approaches in terms of incorporating additional goals?\n\n**A)** The direct approach is more computationally expensive than the indirect approaches, but it can incorporate additional goals such as deferral to physicians on simple cases. However, it requires more data to learn effective policies.\n\n**B)** The direct approach is less accurate than the indirect approaches, but it can incorporate additional goals such as deferral to physicians on simple cases. However, it requires more expertise to design effective policies.\n\n**C)** The direct approach is more flexible than the indirect approaches, as it can incorporate additional goals such as deferral to physicians on simple cases, and it does not require intermediate models of outcomes. However, it may require more data to learn effective policies.\n\n**D)** The direct approach is less scalable than the indirect approaches, as it requires more data to learn effective policies and it may not be suitable for large-scale medical decision-making problems. However, it can incorporate additional goals such as deferral to physicians on simple cases.\n\n**Correct Answer:** C) The direct approach is more flexible than the indirect approaches, as it can incorporate additional goals such as deferral to physicians on simple cases, and it does not require intermediate models of outcomes. However, it may require more data to learn effective policies.\n\n**Explanation:** The direct approach is more flexible than the indirect approaches because it does not require intermediate models of outcomes, which can be complex and difficult to design. Instead, it constructs a set of policies directly from the data, allowing it to incorporate additional goals such as deferral to physicians on simple cases. However, the direct approach may require more data to learn effective policies, which can be a limitation."}, "35": {"documentation": {"title": "Inequality in economic shock exposures across the global firm-level\n  supply network", "source": "Abhijit Chakraborty and Tobias Reisch and Christian Diem and Stefan\n  Thurner", "docs_id": "2112.00415", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inequality in economic shock exposures across the global firm-level\n  supply network. For centuries, national economies created wealth by engaging in international trade and production. The resulting international supply networks not only increase wealth for countries, but also create systemic risk: economic shocks, triggered by company failures in one country, may propagate to other countries. Using global supply network data on the firm-level, we present a method to estimate a country's exposure to direct and indirect economic losses caused by the failure of a company in another country. We show the network of systemic risk-flows across the world. We find that rich countries expose poor countries much more to systemic risk than the other way round. We demonstrate that higher systemic risk levels are not compensated with a risk premium in GDP, nor do they correlate with economic growth. Systemic risk around the globe appears to be distributed more unequally than wealth. These findings put the often praised benefits for developing countries from globalized production in a new light, since they relate them to the involved risks in the production processes. Exposure risks present a new dimension of global inequality, that most affects the poor in supply shock crises. It becomes fully quantifiable with the proposed method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on inequality in economic shock exposures across the global firm-level supply network suggests that rich countries expose poor countries to systemic risk. However, the study also finds that higher systemic risk levels are not compensated with a risk premium in GDP, nor do they correlate with economic growth. What can be inferred about the relationship between economic exposure and economic growth in developing countries?\n\n**A)** Economic exposure is a direct cause of economic growth in developing countries.\n**B)** Economic exposure is a significant factor that affects economic growth in developing countries, but its impact is not fully quantifiable.\n**C)** Economic exposure has no impact on economic growth in developing countries.\n**D)** Economic exposure is a necessary condition for economic growth in developing countries.\n\n**Correct Answer:** B) Economic exposure is a significant factor that affects economic growth in developing countries, but its impact is not fully quantifiable.\n\n**Explanation:** The study suggests that higher systemic risk levels are not compensated with a risk premium in GDP, nor do they correlate with economic growth. This implies that economic exposure has a significant impact on economic growth in developing countries, but the relationship is complex and not fully quantifiable. The correct answer requires the test-taker to analyze the study's findings and infer the relationship between economic exposure and economic growth."}, "36": {"documentation": {"title": "Vertex stability and topological transitions in vertex models of foams\n  and epithelia", "source": "Meryl A. Spencer, Zahera Jabeen, David K. Lubensky", "docs_id": "1609.08696", "section": ["q-bio.TO", "cond-mat.soft", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vertex stability and topological transitions in vertex models of foams\n  and epithelia. In computer simulations of dry foams and of epithelial tissues, vertex models are often used to describe the shape and motion of individual cells. Although these models have been widely adopted, relatively little is known about their basic theoretical properties. For example, while fourfold vertices in real foams are always unstable, it remains unclear whether a simplified vertex model description has the same behavior. Here, we study vertex stability and the dynamics of T1 topological transitions in vertex models. We show that, when all edges have the same tension, stationary fourfold vertices in these models do indeed always break up. In contrast, when tensions are allowed to depend on edge orientation, fourfold vertices can become stable, as is observed in some biological systems. More generally, our formulation of vertex stability leads to an improved treatment of T1 transitions in simulations and paves the way for studies of more biologically realistic models that couple topological transitions to the dynamics of regulatory proteins."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main finding regarding the stability of fourfold vertices in vertex models of foams and epithelia, and how does it depend on the tension of the edges?\n\nA) Fourfold vertices are always stable when all edges have the same tension, but become unstable when tensions depend on edge orientation.\nB) Fourfold vertices are always unstable, regardless of the tension of the edges.\nC) Fourfold vertices can become stable when tensions are allowed to depend on edge orientation, but always break up when all edges have the same tension.\nD) Fourfold vertices are stable when tensions are allowed to depend on edge orientation, but always break up when all edges have the same tension.\n\n**Correct Answer:** C) Fourfold vertices can become stable when tensions are allowed to depend on edge orientation, but always break up when all edges have the same tension.\n\n**Explanation:** The correct answer is C) Fourfold vertices can become stable when tensions are allowed to depend on edge orientation, but always break up when all edges have the same tension. This is because the study shows that when tensions are allowed to depend on edge orientation, fourfold vertices can become stable, whereas when all edges have the same tension, they always break up. This finding has implications for the modeling of biological systems, such as foams and epithelial tissues, where vertex stability and topological transitions play a crucial role."}, "37": {"documentation": {"title": "Convex Dynamics and Applications", "source": "R. L. Adler, B. Kitchens, M. Martens, C. Pugh, M. Shub and C. Tresser", "docs_id": "math/0402115", "section": ["math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convex Dynamics and Applications. This paper proves a theorem about bounding orbits of a time dependent dynamical system. The maps that are involved are examples in convex dynamics, by which we mean the dynamics of piecewise isometries where the pieces are convex. The theorem came to the attention of the authors in connection with the problem of digital halftoning. \\textit{Digital halftoning} is a family of printing technologies for getting full color images from only a few different colors deposited at dots all of the same size. The simplest version consist in obtaining grey scale images from only black and white dots. A corollary of the theorem is that for \\textit{error diffusion}, one of the methods of digital halftoning, averages of colors of the printed dots converge to averages of the colors taken from the same dots of the actual images. Digital printing is a special case of a much wider class of scheduling problems to which the theorem applies. Convex dynamics has roots in classical areas of mathematics such as symbolic dynamics, Diophantine approximation, and the theory of uniform distributions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a time-dependent dynamical system defined by a piecewise isometry, where the pieces are convex. Suppose we have a digital halftoning process that uses error diffusion to produce a grey scale image from a few different colors deposited at dots of the same size. According to the theorem proven in the paper \"Convex Dynamics and Applications\", what can be concluded about the convergence of averages of colors of the printed dots?\n\nA) The averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, but only for a finite number of iterations.\n\nB) The averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, but only for a specific type of convex dynamical system.\n\nC) The averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, but only for a particular digital halftoning process.\n\nD) The averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, for all convex dynamical systems and digital halftoning processes.\n\n**Correct Answer:** D) The averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, for all convex dynamical systems and digital halftoning processes.\n\n**Explanation:** The theorem proven in the paper \"Convex Dynamics and Applications\" states that for a time-dependent dynamical system defined by a piecewise isometry, where the pieces are convex, the averages of colors of the printed dots converge to the averages of the colors taken from the same dots of the actual image, for all digital halftoning processes. This result has implications for a wide range of applications, including digital printing and scheduling problems. The correct answer reflects this general result, rather than a specific limitation or exception."}, "38": {"documentation": {"title": "Realtime Mobile Bandwidth and Handoff Predictions in 4G/5G Networks", "source": "Lifan Mei, Jinrui Gou, Yujin Cai, Houwei Cao and Yong Liu", "docs_id": "2104.12959", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Realtime Mobile Bandwidth and Handoff Predictions in 4G/5G Networks. Mobile apps are increasingly relying on high-throughput and low-latency content delivery, while the available bandwidth on wireless access links is inherently time-varying. The handoffs between base stations and access modes due to user mobility present additional challenges to deliver a high level of user Quality-of-Experience (QoE). The ability to predict the available bandwidth and the upcoming handoffs will give applications valuable leeway to make proactive adjustments to avoid significant QoE degradation. In this paper, we explore the possibility and accuracy of realtime mobile bandwidth and handoff predictions in 4G/LTE and 5G networks. Towards this goal, we collect long consecutive traces with rich bandwidth, channel, and context information from public transportation systems. We develop Recurrent Neural Network models to mine the temporal patterns of bandwidth evolution in fixed-route mobility scenarios. Our models consistently outperform the conventional univariate and multivariate bandwidth prediction models. For 4G \\& 5G co-existing networks, we propose a new problem of handoff prediction between 4G and 5G, which is important for low-latency applications like self-driving strategy in realistic 5G scenarios. We develop classification and regression based prediction models, which achieve more than 80\\% accuracy in predicting 4G and 5G handoffs in a recent 5G dataset."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of real-time mobile bandwidth and handoff prediction models in 4G/5G networks, and how do these models address the challenges posed by user mobility?\n\n**A)** To improve the accuracy of bandwidth prediction models in fixed-route mobility scenarios, thereby enabling proactive adjustments to maintain a high level of user Quality-of-Experience (QoE).\n\n**B)** To develop classification and regression models that can predict 4G and 5G handoffs with high accuracy, thereby optimizing low-latency applications such as self-driving strategy in realistic 5G scenarios.\n\n**C)** To mine the temporal patterns of bandwidth evolution in fixed-route mobility scenarios, thereby improving the accuracy of bandwidth prediction models and enabling proactive adjustments to maintain a high level of user QoE.\n\n**D)** To develop Recurrent Neural Network models that can predict the available bandwidth and upcoming handoffs in 4G/LTE and 5G networks, thereby enabling proactive adjustments to maintain a high level of user QoE.\n\n**Correct Answer:** D) To develop Recurrent Neural Network models that can predict the available bandwidth and upcoming handoffs in 4G/LTE and 5G networks, thereby enabling proactive adjustments to maintain a high level of user QoE.\n\n**Explanation:** The correct answer is D) because the question asks about the primary motivation behind the development of real-time mobile bandwidth and handoff prediction models. The correct answer states that these models aim to predict the available bandwidth and upcoming handoffs in 4G/LTE and 5G networks, thereby enabling proactive adjustments to maintain a high level of user QoE. This is in line with the motivation stated in the paper, which is to provide valuable leeway to applications to make proactive adjustments to avoid significant QoE degradation.\n\nThe other options are incorrect because:\n\n* A) While the models do improve the accuracy of bandwidth prediction, this is not the primary motivation.\n* B) The models do predict 4G and 5G handoffs, but this is not the primary motivation.\n* C) The models do mine the temporal patterns of bandwidth evolution, but this is not the primary motivation."}, "39": {"documentation": {"title": "Constructing acoustic timefronts using random matrix theory", "source": "Katherine C. Hegewisch and Steven Tomsovic", "docs_id": "1206.4709", "section": ["math-ph", "math.MP", "nlin.CD", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constructing acoustic timefronts using random matrix theory. In a recent letter [Europhys. Lett. 97, 34002 (2012)], random matrix theory is introduced for long-range acoustic propagation in the ocean. The theory is expressed in terms of unitary propagation matrices that represent the scattering between acoustic modes due to sound speed fluctuations induced by the ocean's internal waves. The scattering exhibits a power-law decay as a function of the differences in mode numbers thereby generating a power-law, banded, random unitary matrix ensemble. This work gives a more complete account of that approach and extends the methods to the construction of an ensemble of acoustic timefronts. The result is a very efficient method for studying the statistical properties of timefronts at various propagation ranges that agrees well with propagation based on the parabolic equation. It helps identify which information about the ocean environment survives in the timefronts and how to connect features of the data to the surviving environmental information. It also makes direct connections to methods used in other disordered wave guide contexts where the use of random matrix theory has a multi-decade history."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of using random matrix theory to construct acoustic timefronts, as described in the given Arxiv documentation?\n\nA) It provides a more accurate representation of the ocean's internal waves.\nB) It allows for the efficient study of statistical properties of timefronts at various propagation ranges.\nC) It enables the direct connection of features of the data to the surviving environmental information.\nD) It offers a more complete account of the approach to long-range acoustic propagation in the ocean.\n\nCorrect Answer: B) It allows for the efficient study of statistical properties of timefronts at various propagation ranges.\n\nExplanation: The correct answer is B) because the documentation states that the result of using random matrix theory is \"a very efficient method for studying the statistical properties of timefronts at various propagation ranges\". This indicates that the primary advantage of using this approach is its efficiency in studying the statistical properties of timefronts. The other options, while related to the topic, are not the primary advantage of using random matrix theory to construct acoustic timefronts."}, "40": {"documentation": {"title": "Modified SIR Model Yielding a Logistic Solution", "source": "Paul A. Reiser", "docs_id": "2006.01550", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modified SIR Model Yielding a Logistic Solution. The SIR pandemic model suffers from an unrealistic assumption: The rate of removal from the infectious class of individuals is assumed to be proportional to the number of infectious individuals. This means that a change in the rate of infection is simultaneous with an equal change in the rate of removal. A more realistic assumption is that an individual is removed at a certain time interval after having been infected. A simple modified SIR model is proposed which implements this delay, resulting in a single delay differential equation which comprises the model. A solution to this equation which is applicable to a pandemic is of the form A+B L(t) where L(t) is a logistic function, and A and B are constants. While the classical SIR model is often an oversimplification of pandemic behavior, it is instructive in that many of the fundamental dynamics and descriptors of pandemics are clearly and simply defined. The logistic model is generally used descriptively, dealing as it does with only the susceptible and infected classes and the rate of transfer between them. The present model presents a full but modified SIR model with a simpler logistic solution which is more realistic and equally instructive."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A modified SIR model is proposed to account for the delay in removal from the infectious class of individuals. The model is described by a single delay differential equation of the form:\n\ndS/dt = -\u03b2SI / (S + I) + \u03b3I(1 - L(t))\n\ndI/dt = \u03b2SI / (S + I) - \u03b3I(1 - L(t))\n\ndL/dt = \u03b1I - \u03b3L\n\nwhere S, I, and L represent the number of susceptible, infected, and recovered individuals, respectively, and \u03b2, \u03b3, and \u03b1 are constants. The logistic function L(t) is defined as:\n\nL(t) = (I / (I0 + K)) \\* e^(\u03b1t)\n\nwhere I0 and K are constants.\n\n**A)** The model is a full SIR model with a logistic solution, but the logistic function is not explicitly defined in the differential equations.\n\n**B)** The model is a modified SIR model with a delay differential equation, but the logistic function is not a solution to the equation.\n\n**C)** The model is a modified SIR model with a delay differential equation, and the logistic function L(t) is a solution to the equation, where L(t) = (I / (I0 + K)) \\* e^(\u03b1t).\n\n**D)** The model is a logistic model, but it is not a modified SIR model.\n\n**Correct Answer:** C) The model is a modified SIR model with a delay differential equation, and the logistic function L(t) is a solution to the equation, where L(t) = (I / (I0 + K)) \\* e^(\u03b1t).\n\n**Explanation:** The correct answer is C) because the model is indeed a modified SIR model with a delay differential equation, and the logistic function L(t) is a solution to the equation, as described in the provided documentation. The logistic function is defined as L(t) = (I / (I0 + K)) \\* e^(\u03b1t), which matches the form of the solution provided in the documentation."}, "41": {"documentation": {"title": "Emergence of grid-like representations by training recurrent neural\n  networks to perform spatial localization", "source": "Christopher J. Cueva and Xue-Xin Wei", "docs_id": "1803.07770", "section": ["q-bio.NC", "cs.AI", "cs.NE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergence of grid-like representations by training recurrent neural\n  networks to perform spatial localization. Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which recurrent neural networks (RNNs) emerge grid-like spatial response patterns, and what implications does this have for understanding the neural code underlying spatial navigation?\n\n**A)** The RNNs learn to represent space using a hierarchical encoding scheme, where grid cells are the highest-level units and border cells are the lowest-level units.\n\n**B)** The RNNs are trained to perform spatial localization tasks using velocity inputs, which leads to the emergence of grid-like spatial response patterns due to the predominant recurrent connections in the neural circuits.\n\n**C)** The RNNs are able to learn a spatial representation of space by exploiting the symmetries of the 2D arena, resulting in the emergence of grid-like spatial response patterns.\n\n**D)** The RNNs are able to learn a spatial representation of space by using a combination of velocity inputs and topological features of the environment, resulting in the emergence of grid-like spatial response patterns.\n\n**Correct Answer:** B) The RNNs are trained to perform spatial localization tasks using velocity inputs, which leads to the emergence of grid-like spatial response patterns due to the predominant recurrent connections in the neural circuits.\n\n**Explanation:** The correct answer is B) because the documentation states that the RNNs were trained to perform navigation tasks in 2D arenas based on velocity inputs, which led to the emergence of grid-like spatial response patterns. The predominant recurrent connections in the neural circuits are also mentioned as a key factor in the emergence of these patterns. The other options are incorrect because they do not accurately reflect the mechanism described in the documentation. Option A is incorrect because it suggests a hierarchical encoding scheme, which is not mentioned in the documentation. Option C is incorrect because it suggests that the RNNs learn a spatial representation of space by exploiting symmetries of the 2D arena, which is not mentioned in the documentation. Option D is incorrect because it suggests that the RNNs learn a spatial representation of space by using a combination of velocity inputs and topological features of the environment, which is not mentioned in the documentation."}, "42": {"documentation": {"title": "Measurement of the CKM angle gamma from a combination of B->Dh analyses", "source": "LHCb collaboration: R. Aaij, C. Abellan Beteta, B. Adeva, M. Adinolfi,\n  C. Adrover, A. Affolder, Z. Ajaltouni, J. Albrecht, F. Alessio, M. Alexander,\n  S. Ali, G. Alkhazov, P. Alvarez Cartelle, A.A. Alves Jr, S. Amato, S. Amerio,\n  Y. Amhis, L. Anderlini, J. Anderson, R. Andreassen, R.B. Appleby, O. Aquines\n  Gutierrez, F. Archilli, A. Artamonov, M. Artuso, E. Aslanides, G. Auriemma,\n  S. Bachmann, J.J. Back, C. Baesso, V. Balagura, W. Baldini, R.J. Barlow, C.\n  Barschel, S. Barsuk, W. Barter, Th. Bauer, A. Bay, J. Beddow, F. Bedeschi, I.\n  Bediaga, S. Belogurov, K. Belous, I. Belyaev, E. Ben-Haim, G. Bencivenni, S.\n  Benson, J. Benton, A. Berezhnoy, R. Bernet, M.-O. Bettler, M. van Beuzekom,\n  A. Bien, S. Bifani, T. Bird, A. Bizzeti, P.M. Bj{\\o}rnstad, T. Blake, F.\n  Blanc, J. Blouw, S. Blusk, V. Bocci, A. Bondar, N. Bondar, W. Bonivento, S.\n  Borghi, A. Borgia, T.J.V. Bowcock, E. Bowen, C. Bozzi, T. Brambach, J. van\n  den Brand, J. Bressieux, D. Brett, M. Britsch, T. Britton, N.H. Brook, H.\n  Brown, I. Burducea, A. Bursche, G. Busetto, J. Buytaert, S. Cadeddu, O.\n  Callot, M. Calvi, M. Calvo Gomez, A. Camboni, P. Campana, D. Campora Perez,\n  A. Carbone, G. Carboni, R. Cardinale, A. Cardini, H. Carranza-Mejia, L.\n  Carson, K. Carvalho Akiba, G. Casse, L. Castillo Garcia, M. Cattaneo, Ch.\n  Cauet, M. Charles, Ph. Charpentier, P. Chen, N. Chiapolini, M. Chrzaszcz, K.\n  Ciba, X. Cid Vidal, G. Ciezarek, P.E.L. Clarke, M. Clemencic, H.V. Cliff, J.\n  Closier, C. Coca, V. Coco, J. Cogan, E. Cogneras, P. Collins, A.\n  Comerma-Montells, A. Contu, A. Cook, M. Coombes, S. Coquereau, G. Corti, B.\n  Couturier, G.A. Cowan, D.C. Craik, S. Cunliffe, R. Currie, C. D'Ambrosio, P.\n  David, P.N.Y. David, A. Davis, I. De Bonis, K. De Bruyn, S. De Capua, M. De\n  Cian, J.M. De Miranda, L. De Paula, W. De Silva, P. De Simone, D. Decamp, M.\n  Deckenhoff, L. Del Buono, N. D\\'el\\'eage, D. Derkach, O. Deschamps, F.\n  Dettori, A. Di Canto, H. Dijkstra, M. Dogaru, S. Donleavy, F. Dordei, A.\n  Dosil Su\\'arez, D. Dossett, A. Dovbnya, F. Dupertuis, R. Dzhelyadin, A.\n  Dziurda, A. Dzyuba, S. Easo, U. Egede, V. Egorychev, S. Eidelman, D. van\n  Eijk, S. Eisenhardt, U. Eitschberger, R. Ekelhof, L. Eklund, I. El Rifai, Ch.\n  Elsasser, D. Elsby, A. Falabella, C. F\\\"arber, G. Fardell, C. Farinelli, S.\n  Farry, V. Fave, D. Ferguson, V. Fernandez Albor, F. Ferreira Rodrigues, M.\n  Ferro-Luzzi, S. Filippov, M. Fiore, C. Fitzpatrick, M. Fontana, F.\n  Fontanelli, R. Forty, O. Francisco, M. Frank, C. Frei, M. Frosini, S. Furcas,\n  E. Furfaro, A. Gallas Torreira, D. Galli, M. Gandelman, P. Gandini, Y. Gao,\n  J. Garofoli, P. Garosi, J. Garra Tico, L. Garrido, C. Gaspar, R. Gauld, E.\n  Gersabeck, M. Gersabeck, T. Gershon, Ph. Ghez, V. Gibson, V.V. Gligorov, C.\n  G\\\"obel, D. Golubkov, A. Golutvin, A. Gomes, H. Gordon, M. Grabalosa\n  G\\'andara, R. Graciani Diaz, L.A. Granado Cardoso, E. Graug\\'es, G. Graziani,\n  A. Grecu, E. Greening, S. Gregson, P. Griffith, O. Gr\\\"unberg, B. Gui, E.\n  Gushchin, Yu. Guz, T. Gys, C. Hadjivasiliou, G. Haefeli, C. Haen, S.C.\n  Haines, S. Hall, T. Hampson, S. Hansmann-Menzemer, N. Harnew, S.T. Harnew, J.\n  Harrison, T. Hartmann, J. He, V. Heijne, K. Hennessy, P. Henrard, J.A.\n  Hernando Morata, E. van Herwijnen, A. Hicheur, E. Hicks, D. Hill, M.\n  Hoballah, C. Hombach, P. Hopchev, W. Hulsbergen, P. Hunt, T. Huse, N.\n  Hussain, D. Hutchcroft, D. Hynds, V. Iakovenko, M. Idzik, P. Ilten, R.\n  Jacobsson, A. Jaeger, E. Jans, P. Jaton, A. Jawahery, F. Jing, M. John, D.\n  Johnson, C.R. Jones, C. Joram, B. Jost, M. Kaballo, S. Kandybei, M. Karacson,\n  T.M. Karbach, I.R. Kenyon, U. Kerzel, T. Ketel, A. Keune, B. Khanji, O.\n  Kochebina, I. Komarov, R.F. Koopman, P. Koppenburg, M. Korolev, A.\n  Kozlinskiy, L. Kravchuk, K. Kreplin, M. Kreps, G. Krocker, P. Krokovny, F.\n  Kruse, M. Kucharczyk, V. Kudryavtsev, T. Kvaratskheliya, V.N. La Thi, D.\n  Lacarrere, G. Lafferty, A. Lai, D. Lambert, R.W. Lambert, E. Lanciotti, G.\n  Lanfranchi, C. Langenbruch, T. Latham, C. Lazzeroni, R. Le Gac, J. van\n  Leerdam, J.-P. Lees, R. Lef\\`evre, A. Leflat, J. Lefran\\c{c}ois, S. Leo, O.\n  Leroy, T. Lesiak, B. Leverington, Y. Li, L. Li Gioi, M. Liles, R. Lindner, C.\n  Linn, B. Liu, G. Liu, S. Lohn, I. Longstaff, J.H. Lopes, E. Lopez Asamar, N.\n  Lopez-March, H. Lu, D. Lucchesi, J. Luisier, H. Luo, F. Machefert, I.V.\n  Machikhiliyan, F. Maciuc, O. Maev, S. Malde, G. Manca, G. Mancinelli, U.\n  Marconi, R. M\\\"arki, J. Marks, G. Martellotti, A. Martens, A. Mart\\'in\n  S\\'anchez, M. Martinelli, D. Martinez Santos, D. Martins Tostes, A.\n  Massafferri, R. Matev, Z. Mathe, C. Matteuzzi, E. Maurice, A. Mazurov, B. Mc\n  Skelly, J. McCarthy, A. McNab, R. McNulty, B. Meadows, F. Meier, M. Meissner,\n  M. Merk, D.A. Milanes, M.-N. Minard, J. Molina Rodriguez, S. Monteil, D.\n  Moran, P. Morawski, M.J. Morello, R. Mountain, I. Mous, F. Muheim, K.\n  M\\\"uller, R. Muresan, B. Muryn, B. Muster, P. Naik, T. Nakada, R. Nandakumar,\n  I. Nasteva, M. Needham, N. Neufeld, A.D. Nguyen, T.D. Nguyen, C. Nguyen-Mau,\n  M. Nicol, V. Niess, R. Niet, N. Nikitin, T. Nikodem, A. Nomerotski, A.\n  Novoselov, A. Oblakowska-Mucha, V. Obraztsov, S. Oggero, S. Ogilvy, O.\n  Okhrimenko, R. Oldeman, M. Orlandea, J.M. Otalora Goicochea, P. Owen, A.\n  Oyanguren, B.K. Pal, A. Palano, M. Palutan, J. Panman, A. Papanestis, M.\n  Pappagallo, C. Parkes, C.J. Parkinson, G. Passaleva, G.D. Patel, M. Patel,\n  G.N. Patrick, C. Patrignani, C. Pavel-Nicorescu, A. Pazos Alvarez, A.\n  Pellegrino, G. Penso, M. Pepe Altarelli, S. Perazzini, D.L. Perego, E. Perez\n  Trigo, A. P\\'erez-Calero Yzquierdo, P. Perret, M. Perrin-Terrin, G. Pessina,\n  K. Petridis, A. Petrolini, A. Phan, E. Picatoste Olloqui, B. Pietrzyk, T.\n  Pila\\v{r}, D. Pinci, S. Playfer, M. Plo Casasus, F. Polci, G. Polok, A.\n  Poluektov, E. Polycarpo, A. Popov, D. Popov, B. Popovici, C. Potterat, A.\n  Powell, J. Prisciandaro, A. Pritchard, C. Prouve, V. Pugatch, A. Puig\n  Navarro, G. Punzi, W. Qian, J.H. Rademacker, B. Rakotomiaramanana, M. Rama,\n  M.S. Rangel, I. Raniuk, N. Rauschmayr, G. Raven, S. Redford, M.M. Reid, A.C.\n  dos Reis, S. Ricciardi, A. Richards, K. Rinnert, V. Rives Molina, D.A. Roa\n  Romero, P. Robbe, E. Rodrigues, P. Rodriguez Perez, S. Roiser, V. Romanovsky,\n  A. Romero Vidal, J. Rouvinet, T. Ruf, F. Ruffini, H. Ruiz, P. Ruiz Valls, G.\n  Sabatino, J.J. Saborido Silva, N. Sagidova, P. Sail, B. Saitta, V. Salustino\n  Guimaraes, C. Salzmann, B. Sanmartin Sedes, M. Sannino, R. Santacesaria, C.\n  Santamarina Rios, E. Santovetti, M. Sapunov, A. Sarti, C. Satriano, A. Satta,\n  M. Savrie, D. Savrina, P. Schaack, M. Schiller, H. Schindler, M. Schlupp, M.\n  Schmelling, B. Schmidt, O. Schneider, A. Schopper, M.-H. Schune, R.\n  Schwemmer, B. Sciascia, A. Sciubba, M. Seco, A. Semennikov, K. Senderowska,\n  I. Sepp, N. Serra, J. Serrano, P. Seyfert, M. Shapkin, I. Shapoval, P.\n  Shatalov, Y. Shcheglov, T. Shears, L. Shekhtman, O. Shevchenko, V.\n  Shevchenko, A. Shires, R. Silva Coutinho, T. Skwarnicki, N.A. Smith, E.\n  Smith, M. Smith, M.D. Sokoloff, F.J.P. Soler, F. Soomro, D. Souza, B. Souza\n  De Paula, B. Spaan, A. Sparkes, P. Spradlin, F. Stagni, S. Stahl, O.\n  Steinkamp, S. Stoica, S. Stone, B. Storaci, M. Straticiuc, U. Straumann, V.K.\n  Subbiah, L. Sun, S. Swientek, V. Syropoulos, M. Szczekowski, P. Szczypka, T.\n  Szumlak, S. T'Jampens, M. Teklishyn, E. Teodorescu, F. Teubert, C. Thomas, E.\n  Thomas, J. van Tilburg, V. Tisserand, M. Tobin, S. Tolk, D. Tonelli, S.\n  Topp-Joergensen, N. Torr, E. Tournefier, S. Tourneur, M.T. Tran, M. Tresch,\n  A. Tsaregorodtsev, P. Tsopelas, N. Tuning, M. Ubeda Garcia, A. Ukleja, D.\n  Urner, U. Uwer, V. Vagnoni, G. Valenti, R. Vazquez Gomez, P. Vazquez\n  Regueiro, S. Vecchi, J.J. Velthuis, M. Veltri, G. Veneziano, M. Vesterinen,\n  B. Viaud, D. Vieira, X. Vilasis-Cardona, A. Vollhardt, D. Volyanskyy, D.\n  Voong, A. Vorobyev, V. Vorobyev, C. Vo\\ss, H. Voss, R. Waldi, R. Wallace, S.\n  Wandernoth, J. Wang, D.R. Ward, N.K. Watson, A.D. Webber, D. Websdale, M.\n  Whitehead, J. Wicht, J. Wiechczynski, D. Wiedner, L. Wiggers, G. Wilkinson,\n  M.P. Williams, M. Williams, F.F. Wilson, J. Wishahi, M. Witek, S.A. Wotton,\n  S. Wright, S. Wu, K. Wyllie, Y. Xie, Z. Xing, Z. Yang, R. Young, X. Yuan, O.\n  Yushchenko, M. Zangoli, M. Zavertyaev, F. Zhang, L. Zhang, W.C. Zhang, Y.\n  Zhang, A. Zhelezov, A. Zhokhov, L. Zhong, A. Zvyagin", "docs_id": "1305.2050", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of the CKM angle gamma from a combination of B->Dh analyses. A combination of three LHCb measurements of the CKM angle gamma is presented. The decays B->DK and B->Dpi are used, where D denotes an admixture of D0 and D0-bar mesons, decaying into K+K-, pi+pi-, K+-pi-+, K+-pi-+pi+-pi-+, KSpi+pi-, or KSK+K- final states. All measurements use a dataset corresponding to 1.0 fb-1 of integrated luminosity. Combining results from B->DK decays alone a best-fit value of gamma = 72.0 deg is found, and confidence intervals are set gamma in [56.4,86.7] deg at 68% CL, gamma in [42.6,99.6] deg at 95% CL. The best-fit value of gamma found from a combination of results from B->Dpi decays alone, is gamma = 18.9 deg, and the confidence intervals gamma in [7.4,99.2] deg or [167.9,176.4] deg at 68% CL, are set, without constraint at 95% CL. The combination of results from B->DK and B->Dpi decays gives a best-fit value of gamma = 72.6 deg and the confidence intervals gamma in [55.4,82.3] deg at 68% CL, gamma in [40.2,92.7] deg at 95% CL are set. All values are expressed modulo 180 deg, and are obtained taking into account the effect of D0-D0bar mixing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the measurement of the CKM angle gamma from a combination of B->Dh analyses, what is the best-fit value of gamma obtained from the combination of results from B->DK and B->Dpi decays, and what is the corresponding confidence interval at 95% CL?\n\nA) gamma = 73.2 deg, [38.5, 107.9] deg\nB) gamma = 72.6 deg, [40.2, 92.7] deg\nC) gamma = 18.9 deg, [7.4, 99.2] deg\nD) gamma = 72.0 deg, [42.6, 99.6] deg\n\nCorrect Answer: B) gamma = 72.6 deg, [40.2, 92.7] deg\n\nExplanation: The correct answer is B) gamma = 72.6 deg, [40.2, 92.7] deg. This is because the combination of results from B->DK and B->Dpi decays gives a best-fit value of gamma = 72.6 deg, and the confidence intervals gamma in [55.4, 82.3] deg at 68% CL, gamma in [40.2, 92.7] deg at 95% CL are set. The other options are incorrect because they do not match the best-fit value and confidence intervals obtained from the combination of B->DK and B->Dpi decays."}, "43": {"documentation": {"title": "Temporal dissipative solitons in time-delay feedback systems", "source": "Serhiy Yanchuk, Stefan Ruschel, Jan Sieber, Matthias Wolfrum", "docs_id": "1901.03524", "section": ["nlin.PS", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal dissipative solitons in time-delay feedback systems. Localized states are a universal phenomenon observed in spatially distributed dissipative nonlinear systems. Known as dissipative solitons, auto-solitons, spot or pulse solutions, these states play an important role in data transmission using optical pulses, neural signal propagation, and other processes. While this phenomenon was thoroughly studied in spatially extended systems, temporally localized states are gaining attention only recently, driven primarily by applications from fiber or semiconductor lasers. Here we present a theory for temporal dissipative solitons (TDS) in systems with time-delayed feedback. In particular, we derive a system with an advanced argument, which determines the profile of the TDS. We also provide a complete classification of the spectrum of TDS into interface and pseudo-continuous spectrum. We illustrate our theory with two examples: a generic delayed phase oscillator, which is a reduced model for an injected laser with feedback, and the FitzHugh-Nagumo neuron with delayed feedback. Finally, we discuss possible destabilization mechanisms of TDS and show an example where the TDS delocalizes and its pseudo-continuous spectrum develops a modulational instability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary difference between the spectrum of temporal dissipative solitons (TDS) in systems with time-delayed feedback, as classified by the authors, and the spectrum of auto-solitons in spatially extended dissipative nonlinear systems?\n\nA) The TDS spectrum is pseudo-continuous, while the auto-soliton spectrum is interface.\nB) The TDS spectrum is interface, while the auto-soliton spectrum is pseudo-continuous.\nC) The TDS spectrum is pseudo-continuous, while the auto-soliton spectrum is interface, but only in the presence of time-delayed feedback.\nD) The TDS spectrum is interface, while the auto-soliton spectrum is pseudo-continuous, but only in the presence of spatial nonlinearity.\n\nCorrect Answer: B) The TDS spectrum is interface, while the auto-soliton spectrum is pseudo-continuous.\n\nExplanation: According to the documentation, the authors provide a complete classification of the spectrum of TDS into interface and pseudo-continuous spectrum. In contrast, the spectrum of auto-solitons in spatially extended dissipative nonlinear systems is known to be pseudo-continuous. Therefore, the primary difference between the two spectra is that the TDS spectrum is interface, while the auto-soliton spectrum is pseudo-continuous."}, "44": {"documentation": {"title": "Two-term relative cluster tilting subcategories, $\\tau-$tilting modules\n  and silting subcategories", "source": "Panyue Zhou and Bin Zhu", "docs_id": "1811.12588", "section": ["math.RT", "math.CT", "math.RA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-term relative cluster tilting subcategories, $\\tau-$tilting modules\n  and silting subcategories. Let $\\mathcal{C}$ be a triangulated category with shift functor $[1]$ and $\\mathcal{R}$ a rigid subcategory of $\\mathcal{C}$. We introduce the notions of two-term $\\mathcal{R}[1]$-rigid subcategories, two-term (weak) $\\mathcal{R}[1]$-cluster tilting subcategories and two-term maximal $\\mathcal{R}[1]$-rigid subcategories, and discuss relationship between them. Our main result shows that there exists a bijection between the set of two-term $\\mathcal{R}[1]$-rigid subcategories of $\\mathcal{C}$ and the set of $\\tau$-rigid subcategories of $\\mod\\mathcal{R}$, which induces a one-to-one correspondence between the set of two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ and the set of support $\\tau$-tilting subcategories of $\\mod\\mathcal{R}$. This generalizes the main results in \\cite{YZZ} where $\\mathcal{R}$ is a cluster tilting subcategory. When $\\mathcal{R}$ is a silting subcategory, we prove that the two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories are precisely two-term silting subcategories in \\cite{IJY}. Thus the bijection above induces the bijection given by Iyama-J{\\o}rgensen-Yang in \\cite{IJY}"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $\\mathcal{C}$ be a triangulated category with shift functor $[1]$ and $\\mathcal{R}$ a rigid subcategory of $\\mathcal{C}$. Suppose that $\\mathcal{R}$ is a silting subcategory. Prove that the two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are precisely the two-term silting subcategories of $\\mod\\mathcal{R}$.\n\nA) The two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are precisely the two-term silting subcategories of $\\mod\\mathcal{R}$.\n\nB) The two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are not necessarily the two-term silting subcategories of $\\mod\\mathcal{R}$.\n\nC) The two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are precisely the two-term silting subcategories of $\\mod\\mathcal{R}$, but only when $\\mathcal{R}$ is a cluster tilting subcategory.\n\nD) The two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are not related to the two-term silting subcategories of $\\mod\\mathcal{R}$.\n\nCorrect Answer: A) The two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are precisely the two-term silting subcategories of $\\mod\\mathcal{R}$.\n\nExplanation: This question requires the student to understand the relationship between two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ and two-term silting subcategories of $\\mod\\mathcal{R}$, as established in the provided documentation. The correct answer is based on the main result of the paper, which states that there exists a bijection between the set of two-term $\\mathcal{R}[1]$-rigid subcategories of $\\mathcal{C}$ and the set of $\\tau$-rigid subcategories of $\\mod\\mathcal{R}$, and that this bijection induces a one-to-one correspondence between the set of two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ and the set of support $\\tau$-tilting subcategories of $\\mod\\mathcal{R}$. Since $\\mathcal{R}$ is a silting subcategory, the two-term weak $\\mathcal{R}[1]$-cluster tilting subcategories of $\\mathcal{C}$ are precisely the two-term silting subcategories of $\\mod\\mathcal{R}$."}, "45": {"documentation": {"title": "Multicomponent Gas Diffusion in Porous Electrodes", "source": "Yeqing Fu, Yi Jiang, Abhijit Dutta, Aravind Mohanram, John D. Pietras,\n  Martin Z. Bazant", "docs_id": "1409.2965", "section": ["physics.chem-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multicomponent Gas Diffusion in Porous Electrodes. Multicomponent gas transport is investigated with unprecedented precision by AC impedance analysis of porous YSZ anode-supported solid oxide fuel cells. A fuel gas mixture of H2-H2O-N2 is fed to the anode, and impedance data are measured across the range of hydrogen partial pressure (10-100%) for open circuit conditions at three temperatures (800C, 850C and 900C) and for 300mA applied current at 800C. For the first time, analytical formulae for the diffusion resistance (Rb) of three standard models of multicomponent gas transport (Fick, Stefan-Maxwell, and Dusty Gas) are derived and tested against the impedance data. The tortuosity is the only fitting parameter since all the diffusion coefficients are known. Only the Dusty Gas model leads to a remarkable data collapse for over twenty experimental conditions, using a constant tortuosity consistent with permeability measurements and the Bruggeman relation. These results establish the accuracy of the Dusty Gas model for multicomponent gas diffusion in porous media and confirm the efficacy of electrochemical impedance analysis to precisely determine transport mechanisms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the Dusty Gas model for multicomponent gas diffusion in porous media, as demonstrated by the AC impedance analysis of porous YSZ anode-supported solid oxide fuel cells?\n\nA) It provides a more complex and realistic representation of gas transport mechanisms.\nB) It is more computationally intensive than the Fick and Stefan-Maxwell models.\nC) It leads to a remarkable data collapse for over twenty experimental conditions, using a constant tortuosity consistent with permeability measurements and the Bruggeman relation.\nD) It is only applicable to specific types of porous media, such as YSZ.\n\nCorrect Answer: C) It leads to a remarkable data collapse for over twenty experimental conditions, using a constant tortuosity consistent with permeability measurements and the Bruggeman relation.\n\nExplanation: The correct answer is C) because the Dusty Gas model is able to accurately predict the impedance data for a wide range of experimental conditions, using a constant tortuosity that is consistent with the permeability measurements and the Bruggeman relation. This demonstrates the model's ability to accurately capture the complex transport mechanisms in porous media. The other options are incorrect because they do not accurately summarize the primary advantage of the Dusty Gas model. Option A is incorrect because the Dusty Gas model is not necessarily more complex or realistic, but rather more accurate. Option B is incorrect because the computational intensity of the model is not the primary advantage. Option D is incorrect because the Dusty Gas model is not limited to specific types of porous media, but rather can be applied to a wide range of materials."}, "46": {"documentation": {"title": "Computational explorations of the Thompson group T for the amenability\n  problem of F", "source": "S. Haagerup, U. Haagerup, M. Ramirez-Solano", "docs_id": "1705.00198", "section": ["math.GR", "math.OA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computational explorations of the Thompson group T for the amenability\n  problem of F. It is a long standing open problem whether the Thompson group $F$ is an amenable group. In this paper we show that if $A$, $B$, $C$ denote the standard generators of Thompson group $T$ and $D:=C B A^{-1}$ then $$\\sqrt2+\\sqrt3\\,<\\,\\frac1{\\sqrt{12}}||(I+C+C^2)(I+D+D^2+D^3)||\\,\\le\\, 2+\\sqrt2.$$ Moreover, the upper bound is attained if the Thompson group $F$ is amenable. Here, the norm of an element in the group ring $\\mathbb{C} T$ is computed in $B(\\ell^2(T))$ via the regular representation of $T$. Using the \"cyclic reduced\" numbers $\\tau(((C+C^2)(D+D^2+D^3))^n)$, $n\\in\\mathbb{N}$, and some methods from our previous paper [arXiv:1409.1486] we can obtain precise lower bounds as well as good estimates of the spectral distributions of $\\frac1{12}((I+C+C^2)(I+D+D^2+D^3))^*(I+C+C^2)(I+D+D^2+D^3),$ where $\\tau$ is the tracial state on the group von Neumann algebra $L(T)$. Our extensive numerical computations suggest that $$\\frac1{\\sqrt{12}}||(I+C+C^2)(I+D+D^2+D^3)||\\approx 3.28,$$ and thus that $F$ might be non-amenable. However, we can in no way rule out that $\\frac1{\\sqrt{12}}||(I+C+C^2)(I+D+D^2+D^3)||=\\, 2+\\sqrt2$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $T$ be the Thompson group, and let $A$, $B$, $C$ denote its standard generators. Define $D:=C B A^{-1}$. Compute the norm of the element $\\frac{1}{\\sqrt{12}}||(I+C+C^2)(I+D+D^2+D^3)||$ in the group ring $\\mathbb{C} T$, and determine whether the Thompson group $T$ is amenable based on the given bounds.\n\nA){The norm is equal to 2, and the Thompson group is amenable.}\nB){The norm is equal to $\\sqrt{2}+\\sqrt{3}$, and the Thompson group is non-amenable.}\nC){The norm is equal to 2, and the Thompson group is non-amenable.}\nD){The norm is equal to $\\sqrt{2}+\\sqrt{3}$, and the Thompson group is amenable.}\n\nCorrect Answer: B){The norm is equal to $\\sqrt{2}+\\sqrt{3}$, and the Thompson group is non-amenable.}\n\nExplanation: The correct answer is B) because the paper states that the norm of the element is greater than $\\sqrt{2}+\\sqrt{3}$, and less than or equal to 2. The paper also mentions that the upper bound is attained if the Thompson group is amenable, but the authors' numerical computations suggest that the norm is approximately 3.28, which is greater than 2. Therefore, the Thompson group is likely non-amenable."}, "47": {"documentation": {"title": "Diagonal Preconditioning: Theory and Algorithms", "source": "Zhaonan Qu, Yinyu Ye, Zhengyuan Zhou", "docs_id": "2003.07545", "section": ["cs.LG", "econ.EM", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diagonal Preconditioning: Theory and Algorithms. Diagonal preconditioning has been a staple technique in optimization and machine learning. It often reduces the condition number of the design or Hessian matrix it is applied to, thereby speeding up convergence. However, rigorous analyses of how well various diagonal preconditioning procedures improve the condition number of the preconditioned matrix and how that translates into improvements in optimization are rare. In this paper, we first provide an analysis of a popular diagonal preconditioning technique based on column standard deviation and its effect on the condition number using random matrix theory. Then we identify a class of design matrices whose condition numbers can be reduced significantly by this procedure. We then study the problem of optimal diagonal preconditioning to improve the condition number of any full-rank matrix and provide a bisection algorithm and a potential reduction algorithm with $O(\\log(\\frac{1}{\\epsilon}))$ iteration complexity, where each iteration consists of an SDP feasibility problem and a Newton update using the Nesterov-Todd direction, respectively. Finally, we extend the optimal diagonal preconditioning algorithm to an adaptive setting and compare its empirical performance at reducing the condition number and speeding up convergence for regression and classification problems with that of another adaptive preconditioning technique, namely batch normalization, that is essential in training machine learning models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Diagonal Preconditioning: Theory and Algorithms\n\n**Part A:** (30 points)\n\nConsider a full-rank matrix A with a condition number \u03ba(A) = 100. A diagonal preconditioner P is applied to A, resulting in a preconditioned matrix A^P = P^(-1)AP. If the column standard deviation of A is \u03c3, what is the expected condition number of A^P, \u03ba(A^P)?\n\nA) 100\nB) 10\nC) 100/\u03c3\nD) 100/\u03c3^2\n\n**Part B:** (40 points)\n\nSuppose we want to optimize the condition number of a matrix A using the optimal diagonal preconditioning algorithm. What is the expected number of iterations required to achieve a reduction in the condition number by a factor of 10, assuming an initial condition number \u03ba(A) = 1000 and a target condition number \u03ba(A^P) = 100?\n\nA) 10\nB) 20\nC) O(log(10))\nD) O(log(100))\n\n**Part C:** (30 points)\n\nConsider a regression problem with a design matrix A and a regression coefficient vector \u03b2. Suppose we use the optimal diagonal preconditioning algorithm to precondition A, resulting in a preconditioned matrix A^P. What is the expected impact of this preconditioning on the convergence rate of the optimization algorithm, assuming that the initial solution is close to the optimal solution?\n\nA) The convergence rate remains the same.\nB) The convergence rate is reduced by a factor of 10.\nC) The convergence rate is increased by a factor of 10.\nD) The convergence rate is reduced by a factor of 100.\n\n**Part D:** (40 points)\n\nSuppose we want to compare the performance of the optimal diagonal preconditioning algorithm with batch normalization in an adaptive setting. What is the expected difference in the reduction of the condition number between the two methods, assuming that the initial condition number \u03ba(A) = 1000 and the target condition number \u03ba(A^P) = 100?\n\nA) 10\nB) 20\nC) 50\nD) O(log(100))\n\n**Correct Answer:**\n\nA) Part A: C) 100/\u03c3\nB) Part B: C) O(log(10))\nC) Part C: B) The convergence rate is reduced by a factor of 10.\nD) Part D: C) 50"}, "48": {"documentation": {"title": "Decreasing market value of variable renewables can be avoided by policy\n  action", "source": "T. Brown, L. Reichenberg", "docs_id": "2002.05209", "section": ["q-fin.GN", "econ.GN", "math.OC", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decreasing market value of variable renewables can be avoided by policy\n  action. Although recent studies have shown that electricity systems with shares of wind and solar above 80% can be affordable, economists have raised concerns about market integration. Correlated generation from variable renewable sources depresses market prices, which can cause wind and solar to cannibalise their own revenues and prevent them from covering their costs from the market. This cannibalisation appears to set limits on the integration of wind and solar, and thus to contradict studies that show that high shares are cost effective. Here we show from theory and with simulation examples how market incentives interact with prices, revenue and costs for renewable electricity systems. The decline in average revenue seen in some recent literature is due to an implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas. This decline is mathematically guaranteed regardless of whether the subsidised technology is variable or not. If instead the driving policy is a carbon dioxide cap or tax, wind and solar shares can rise without cannibalising their own market revenue, even at penetrations of wind and solar above 80%. The strong dependence of market value on the policy regime means that market value needs to be used with caution as a measure of market integration. Declining market value is not necessarily a sign of integration problems, but rather a result of policy choices."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary reason for the decline in average revenue of variable renewable sources, as observed in some recent studies, and how does this relate to the policy regime?\n\nA) The decline is due to the cannibalisation of wind and solar revenues by their own output, which prevents them from covering their costs from the market.\nB) The decline is due to the implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas, which depresses market prices.\nC) The decline is due to the lack of market integration, which prevents wind and solar from competing effectively with fossil fuels.\nD) The decline is due to the high costs of transmission and distribution infrastructure, which increases the cost of integrating variable renewables into the grid.\n\n**Correct Answer:** B) The decline is due to the implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas, which depresses market prices.\n\n**Explanation:** The correct answer is B) because the text states that the decline in average revenue is due to an implicit policy assumption that technologies are forced into the system, whether it be with subsidies or quotas. This assumption is mathematically guaranteed regardless of whether the subsidised technology is variable or not. The other options are incorrect because they do not accurately reflect the reasoning provided in the text. Option A is incorrect because the text states that cannibalisation of revenues is not the primary reason for the decline in average revenue. Option C is incorrect because the text does not mention market integration as a reason for the decline in average revenue. Option D is incorrect because the text does not mention transmission and distribution infrastructure as a reason for the decline in average revenue."}, "49": {"documentation": {"title": "Precision mass measurements of neutron-rich scandium isotopes refine the\n  evolution of $N=32$ and $N=34$ shell closures", "source": "E. Leistenschneider, E. Dunling, G. Bollen, B.A. Brown, J. Dilling, A.\n  Hamaker, J.D. Holt, A. Jacobs, A.A. Kwiatkowski, T. Miyagi, W.S. Porter, D.\n  Puentes, M. Redshaw, M.P. Reiter, R. Ringle, R. Sandler, C.S.\n  Sumithrarachchi, A.A. Valverde, I.T. Yandow and the TITAN Collaboration", "docs_id": "2006.01302", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Precision mass measurements of neutron-rich scandium isotopes refine the\n  evolution of $N=32$ and $N=34$ shell closures. We report high-precision mass measurements of $^{50-55}$Sc isotopes performed at the LEBIT facility at NSCL and at the TITAN facility at TRIUMF. Our results provide a substantial reduction of their uncertainties and indicate significant deviations, up to 0.7 MeV, from the previously recommended mass values for $^{53-55}$Sc. The results of this work provide an important update to the description of emerging closed-shell phenomena at neutron numbers $N=32$ and $N=34$ above proton-magic $Z=20$. In particular, they finally enable a complete and precise characterization of the trends in ground state binding energies along the $N=32$ isotone, confirming that the empirical neutron shell gap energies peak at the doubly-magic $^{52}$Ca. Moreover, our data, combined with other recent measurements, does not support the existence of closed neutron shell in $^{55}$Sc at $N=34$. The results were compared to predictions from both \\emph{ab initio} and phenomenological nuclear theories, which all had success describing $N=32$ neutron shell gap energies but were highly disparate in the description of the $N=34$ isotone."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred from the high-precision mass measurements of $^{50-55}$Sc isotopes regarding the description of emerging closed-shell phenomena at neutron numbers $N=32$ and $N=34$?\n\nA) The existence of a closed neutron shell at $N=34$ in $^{55}$Sc is confirmed.\nB) The empirical neutron shell gap energies peak at the doubly-magic $^{52}$Ca, but the existence of a closed neutron shell at $N=34$ is not supported.\nC) The results of the mass measurements do not provide sufficient evidence to confirm or refute the existence of closed-shell phenomena at $N=32$ and $N=34$.\nD) The mass measurements are inconsistent with the predictions of both \\emph{ab initio} and phenomenological nuclear theories.\n\nCorrect Answer: B) The empirical neutron shell gap energies peak at the doubly-magic $^{52}$Ca, but the existence of a closed neutron shell at $N=34$ is not supported.\n\nExplanation: The question requires the test-taker to carefully read and understand the results of the mass measurements and their implications for the description of emerging closed-shell phenomena. The correct answer, B, is supported by the text, which states that the results \"finally enable a complete and precise characterization of the trends in ground state binding energies along the $N=32$ isotone, confirming that the empirical neutron shell gap energies peak at the doubly-magic $^{52}$Ca\" and that the data \"does not support the existence of closed neutron shell in $^{55}$Sc at $N=34$\". The other options are incorrect because they either misinterpret the results or are not supported by the text."}, "50": {"documentation": {"title": "Electromagnetic decays of the neutral pion", "source": "Esther Weil, Gernot Eichmann, Christian S. Fischer, Richard Williams", "docs_id": "1704.06046", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electromagnetic decays of the neutral pion. We complement studies of the neutral pion transition form factor pi^0 --> gamma^(*) gamma^(*) with calculations for the electromagnetic decay widths of the processes pi^0 --> e^+ e^-, pi^0 --> e^+ e^- gamma and pi^0 --> e^+ e^- e^+ e^-. Their common feature is that the singly- or doubly-virtual transition form factor serves as a vital input that is tested in the non-perturbative low-momentum region of QCD. We determine this form factor from a well-established and symmetry-preserving truncation of the Dyson-Schwinger equations. Our results for the three- and four-body decays match results of previous theoretical calculations and experimental measurements. For the rare decay we employ a numerical method to calculate the process directly by deforming integration contours, which in principle can be generalized to arbitrary integrals as long as the analytic structure of the integrands are known. Our result for the rare decay is in agreement with dispersive calculations but still leaves a 2 sigma discrepancy between theory and experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary method used to determine the transition form factor in the electromagnetic decay of the neutral pion, and what is the significance of this form factor in the non-perturbative low-momentum region of QCD?\n\nA) The transition form factor is determined from a perturbative calculation of the Dyson-Schwinger equations, and it is used to describe the behavior of the pion in the high-energy region of QCD.\nB) The transition form factor is determined from a symmetry-preserving truncation of the Dyson-Schwinger equations, and it is used to describe the behavior of the pion in the low-energy region of QCD.\nC) The transition form factor is determined from a numerical method that employs a deformation of integration contours, and it is used to describe the behavior of the pion in the high-energy region of QCD.\nD) The transition form factor is determined from a perturbative calculation of the Dyson-Schwinger equations, and it is used to describe the behavior of the pion in the low-energy region of QCD.\n\nCorrect Answer: B) The transition form factor is determined from a symmetry-preserving truncation of the Dyson-Schwinger equations, and it is used to describe the behavior of the pion in the low-energy region of QCD.\n\nExplanation: The correct answer is B) because the documentation states that the transition form factor is determined from a \"well-established and symmetry-preserving truncation of the Dyson-Schwinger equations\", which is used to describe the behavior of the pion in the non-perturbative low-momentum region of QCD. This is a key point in the documentation, and the other options are incorrect because they do not accurately describe the method used to determine the transition form factor."}, "51": {"documentation": {"title": "Excursion and contour uncertainty regions for latent Gaussian models", "source": "David Bolin and Finn Lindgren", "docs_id": "1211.3946", "section": ["stat.ME", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Excursion and contour uncertainty regions for latent Gaussian models. An interesting statistical problem is to find regions where some studied process exceeds a certain level. Estimating such regions so that the probability for exceeding the level in the entire set is equal to some predefined value is a difficult problem that occurs in several areas of applications ranging from brain imaging to astrophysics. In this work, a method for solving this problem, as well as the related problem of finding uncertainty regions for contour curves, for latent Gaussian models is proposed. The method is based on using a parametric family for the excursion sets in combination with a sequential importance sampling method for estimating joint probabilities. The accuracy of the method is investigated using simulated data and two environmental applications are presented. In the first application, areas where the air pollution in the Piemonte region in northern Italy exceeds the daily limit value, set by the European Union for human health protection, are estimated. In the second application, regions in the African Sahel that experienced an increase in vegetation after the drought period in the early 1980s are estimated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A latent Gaussian model is used to estimate the probability of exceeding a certain level in a studied process. The goal is to find regions where the process exceeds the level with a probability equal to a predefined value, such as 95%. What is the main challenge in solving this problem, and how does the proposed method address this challenge?\n\n**A)** The main challenge is to accurately estimate the joint probability of the process exceeding the level, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method.\n\n**B)** The main challenge is to determine the optimal level at which to exceed, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method.\n\n**C)** The main challenge is to account for the uncertainty in the data, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method.\n\n**D)** The main challenge is to balance the accuracy of the estimate with computational efficiency, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method.\n\n**Correct Answer:** A) The main challenge is to accurately estimate the joint probability of the process exceeding the level, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method.\n\n**Explanation:** The correct answer is A) because the documentation states that estimating regions where the process exceeds a certain level with a predefined probability is a difficult problem, and the proposed method addresses this challenge by using a parametric family for the excursion sets and a sequential importance sampling method. This method is designed to accurately estimate the joint probability of the process exceeding the level, which is the main challenge in solving this problem."}, "52": {"documentation": {"title": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification", "source": "Chenkai Xu, Hongwei Lin, Xuansu Fang", "docs_id": "2006.11119", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification. In this paper, we propose a novel stock index model, namely the manifold feature(MF) index, to reflect the overall price activity of the entire stock market. Based on the theory of manifold learning, the researched stock dataset is assumed to be a low-dimensional manifold embedded in a higher-dimensional Euclidean space. After data preprocessing, its manifold structure and discrete Laplace-Beltrami operator(LBO) matrix are constructed. We propose a high-dimensional data feature detection method to detect feature points on the eigenvectors of LBO, and the stocks corresponding to these feature points are considered as the constituent stocks of the MF index. Finally, the MF index is generated by a weighted formula using the price and market capitalization of these constituents. The stock market studied in this research is the Shanghai Stock Exchange(SSE). We propose four metrics to compare the MF index series and the SSE index series (SSE 50, SSE 100, SSE 150, SSE 180 and SSE 380). From the perspective of data approximation, the results demonstrate that our indexes are closer to the stock market than the SSE index series. From the perspective of risk premium, MF indexes have higher stability and lower risk."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption made about the stock dataset in the proposed Manifold Feature (MF) index model, and how does it relate to the theory of manifold learning?\n\n**A)** The stock dataset is assumed to be a high-dimensional Euclidean space, and the MF index model aims to reduce its dimensionality.\n\n**B)** The stock dataset is assumed to be a low-dimensional manifold embedded in a higher-dimensional Euclidean space, and the MF index model aims to preserve its manifold structure.\n\n**C)** The stock dataset is assumed to be a high-dimensional Euclidean space, and the MF index model aims to detect feature points on the eigenvectors of a discrete Laplace-Beltrami operator (LBO) matrix.\n\n**D)** The stock dataset is assumed to be a low-dimensional manifold embedded in a higher-dimensional Euclidean space, and the MF index model aims to generate a weighted formula using the price and market capitalization of the constituent stocks.\n\n**Correct Answer:** B) The stock dataset is assumed to be a low-dimensional manifold embedded in a higher-dimensional Euclidean space, and the MF index model aims to preserve its manifold structure.\n\n**Explanation:** The correct answer is B because the documentation states that the stock dataset is assumed to be a \"low-dimensional manifold embedded in a higher-dimensional Euclidean space\", which is a key assumption in the MF index model. The model aims to preserve this manifold structure and use it to generate the MF index. The other options are incorrect because they either misstate the assumption about the stock dataset or misdescribe the goal of the MF index model."}, "53": {"documentation": {"title": "The Gamma-ray burst 050904 : evidence for a termination shock ?", "source": "B. Gendre (1), A. Galli (1),(2),(3), A. Corsi (1),(2),(4), A. Klotz\n  (5),(6), L. Piro (1), G. Stratta (7), M. Boer (6), Y. Damerdji (5),(6) ((1)\n  IASF-Roma/INAF, (2) Universita degli Studi di Roma \"La Sapienza\", (3) INFN -\n  Sezione di Trieste, (4)INFN - Sezione di Roma, (5) CESR, (6) Observatoire de\n  Haute-Provence, (7) LATT)", "docs_id": "astro-ph/0603431", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Gamma-ray burst 050904 : evidence for a termination shock ?. We analyze optical and X-ray observations of GRB 050904 obtained with TAROT and SWIFT. We perform temporal and spectral analysis of the X-ray and optical data. We find significant absorption in the early phase of the X-ray light curve, with some evidence (3 sigma level) of variability. We interpret this as a progressive photo-ionization. We investigate the environment of the burst and constrain its density profile. We find that the overall behavior of the afterglow is compatible with a fireball expanding in a wind environment during the first 2000 seconds after the burst (observer frame). On the other hand, the late (after 0.5 days, observer frame) afterglow is consistent with an interstellar medium, suggesting the possible presence of a termination shock. We estimate the termination shock position to be R_t ~ 1.8 x 10^{-2} pc, and the wind density parameter to be A_* ~ 1.8. We try to explain the simultaneous flares observed in optical and X-ray bands in light of different models : delayed external shock from a thick shell, inverse Compton emission from reverse shock, inverse Compton emission from late internal shocks or a very long internal shock activity. Among these models, those based on a single emission mechanism, are unable to account for the broad-band observations. Models invoking late internal shocks, with the inclusion of IC emission, or a properly tuned very long internal shock activity, offer possible explanations."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the observational evidence presented in the study on GRB 050904, specifically the temporal and spectral analysis of the X-ray and optical data. What is the most likely explanation for the simultaneous flares observed in optical and X-ray bands, and how do the different models presented in the study account for the broad-band observations?\n\n**A)** The simultaneous flares can be explained by a delayed external shock from a thick shell, which would produce a broad-band emission in both optical and X-ray bands. However, this model is unable to account for the progressive photo-ionization observed in the early phase of the X-ray light curve.\n\n**B)** The flares can be attributed to inverse Compton emission from reverse shock, which would produce a broad-band emission in both optical and X-ray bands. However, this model is unable to explain the progressive photo-ionization observed in the early phase of the X-ray light curve.\n\n**C)** The flares can be explained by a combination of late internal shocks and inverse Compton emission, which would produce a broad-band emission in both optical and X-ray bands. This model is able to account for the progressive photo-ionization observed in the early phase of the X-ray light curve and the simultaneous flares observed in both optical and X-ray bands.\n\n**D)** The flares can be explained by a very long internal shock activity, which would produce a broad-band emission in both optical and X-ray bands. However, this model is unable to account for the progressive photo-ionization observed in the early phase of the X-ray light curve.\n\n**Correct Answer:** C) The flares can be explained by a combination of late internal shocks and inverse Compton emission, which would produce a broad-band emission in both optical and X-ray bands. This model is able to account for the progressive photo-ionization observed in the early phase of the X-ray light curve and the simultaneous flares observed in both optical and X-ray bands.\n\n**Explanation:** The study presents evidence for a termination shock in the afterglow of GRB 050904, with a position of R_t ~ 1.8 x 10^{-2} pc and a wind density parameter of A_* ~ 1.8. The simultaneous flares observed in optical and X-ray bands can be explained by a combination of late internal shocks and inverse Compton emission. This model is able to account for the broad-band observations, including the progressive photo-ionization observed in the early phase of the X-ray light curve. The other options are unable to explain the simultaneous flares and the progressive photo-ionization observed in the early phase of the X-ray light curve."}, "54": {"documentation": {"title": "$\\bar{K} + N \\to K + \\Xi$ reaction and $S=-1$ hyperon resonances", "source": "Benjamin C. Jackson, Yongseok Oh, H. Haberzettl, K. Nakayama", "docs_id": "1503.00845", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$\\bar{K} + N \\to K + \\Xi$ reaction and $S=-1$ hyperon resonances. The $\\bar{K} + N \\to K + \\Xi$ reaction is studied for center-of-momentum energies ranging from threshold to 3 GeV in an effective Lagrangian approach that includes the hyperon $s$- and $u$-channel contributions as well as a phenomenological contact amplitude. The latter accounts for the rescattering term in the scattering equation and possible short-range dynamics not included explicitly in the model. Existing data are well reproduced and three above-the-threshold resonances were found to be required to describe the data, namely, the $\\Lambda(1890)$, $\\Sigma(2030)$, and $\\Sigma(2250)$. For the latter resonance we have assumed the spin-parity of $J^P=5/2^-$ and a mass of 2265 MeV. The $\\Sigma(2030)$ resonance is crucial in achieving a good reproduction of not only the measured total and differential cross sections, but also the recoil polarization asymmetry. More precise data are required before a more definitive statement can be made about the other two resonances, in particular, about the $\\Sigma(2250)$ resonance that is introduced to describe a small bump structure observed in the total cross section of $K^- + p \\to K^+ + \\Xi^-$. The present analysis also reveals a peculiar behavior of the total cross section data in the threshold energy region in $K^- + p \\to K^+ + \\Xi^-$, where the $P$- and $D$-waves dominate instead of the usual $S$-wave. Predictions for the target-recoil asymmetries of the $\\bar{K} + N \\to K + \\Xi$ reaction are also presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the spin-parity of the $\\Sigma(2250)$ resonance, and what is its mass?\n\nA) $J^P=3/2^-$, 2245 MeV\nB) $J^P=5/2^-$, 2265 MeV\nC) $J^P=1/2^-$, 2250 MeV\nD) $J^P=3/2^+$, 2250 MeV\n\nCorrect Answer: B) $J^P=5/2^-$, 2265 MeV\n\nExplanation: According to the documentation, the spin-parity of the $\\Sigma(2250)$ resonance is $J^P=5/2^-$, and its mass is 2265 MeV."}, "55": {"documentation": {"title": "Long-term Memory and Volatility Clustering in Daily and High-frequency\n  Price Changes", "source": "GabJin Oh, Cheol-Jun Um, Seunghwann Kim", "docs_id": "physics/0601174", "section": ["physics.soc-ph", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Long-term Memory and Volatility Clustering in Daily and High-frequency\n  Price Changes. We study the long-term memory in diverse stock market indices and foreign exchange rates using the Detrended Fluctuation Analysis(DFA). For all daily and high-frequency market data studied, no significant long-term memory property is detected in the return series, while a strong long-term memory property is found in the volatility time series. The possible causes of the long-term memory property are investigated using the return data filtered by the AR(1) model, reflecting the short-term memory property, and the GARCH(1,1) model, reflecting the volatility clustering property, respectively. Notably, we found that the memory effect in the AR(1) filtered return and volatility time series remains unchanged, while the long-term memory property either disappeared or diminished significantly in the volatility series of the GARCH(1,1) filtered data. We also found that in the high-frequency data the long-term memory property may be generated by the volatility clustering as well as higher autocorrelation. Our results imply that the long-term memory property of the volatility time series can be attributed to the volatility clustering observed in the financial time series."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What can be inferred from the study on long-term memory and volatility clustering in daily and high-frequency price changes, and how do the results relate to the underlying financial time series?\n\nA) The long-term memory property in the return series is solely caused by the volatility clustering property, and the GARCH(1,1) model is sufficient to explain the observed phenomenon.\nB) The study suggests that the long-term memory property in the volatility time series is a result of the short-term memory property in the return series, and the AR(1) model is sufficient to explain the observed phenomenon.\nC) The results imply that the long-term memory property of the volatility time series can be attributed to the volatility clustering observed in the financial time series, and this effect is not solely caused by the return series.\nD) The study found no significant long-term memory property in the return series, and the long-term memory property in the volatility time series is a result of the high-frequency data.\n\n**Correct Answer:** C) The results imply that the long-term memory property of the volatility time series can be attributed to the volatility clustering observed in the financial time series, and this effect is not solely caused by the return series.\n\n**Explanation:** The study found that no significant long-term memory property was detected in the return series, but a strong long-term memory property was found in the volatility time series. The results also showed that the memory effect in the AR(1) filtered return and volatility time series remained unchanged, while the long-term memory property either disappeared or diminished significantly in the volatility series of the GARCH(1,1) filtered data. This suggests that the long-term memory property in the volatility time series is not solely caused by the return series, but rather by the volatility clustering observed in the financial time series."}, "56": {"documentation": {"title": "A Temporal Difference Reinforcement Learning Theory of Emotion: unifying\n  emotion, cognition and adaptive behavior", "source": "Joost Broekens", "docs_id": "1807.08941", "section": ["cs.AI", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Temporal Difference Reinforcement Learning Theory of Emotion: unifying\n  emotion, cognition and adaptive behavior. Emotions are intimately tied to motivation and the adaptation of behavior, and many animal species show evidence of emotions in their behavior. Therefore, emotions must be related to powerful mechanisms that aid survival, and, emotions must be evolutionary continuous phenomena. How and why did emotions evolve in nature, how do events get emotionally appraised, how do emotions relate to cognitive complexity, and, how do they impact behavior and learning? In this article I propose that all emotions are manifestations of reward processing, in particular Temporal Difference (TD) error assessment. Reinforcement Learning (RL) is a powerful computational model for the learning of goal oriented tasks by exploration and feedback. Evidence indicates that RL-like processes exist in many animal species. Key in the processing of feedback in RL is the notion of TD error, the assessment of how much better or worse a situation just became, compared to what was previously expected (or, the estimated gain or loss of utility - or well-being - resulting from new evidence). I propose a TDRL Theory of Emotion and discuss its ramifications for our understanding of emotions in humans, animals and machines, and present psychological, neurobiological and computational evidence in its support."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the Temporal Difference Reinforcement Learning Theory of Emotion, what is the proposed underlying mechanism for the emergence of emotions in nature, and how does it relate to the processing of feedback in reinforcement learning?\n\n**A)** Emotions are manifestations of cognitive complexity, and the proposed mechanism is the assessment of cognitive dissonance through the evaluation of conflicting emotions.\n\n**B)** Emotions are manifestations of reward processing, specifically the assessment of Temporal Difference (TD) error, which is the evaluation of how much better or worse a situation just became compared to what was previously expected.\n\n**C)** Emotions are manifestations of motivation, and the proposed mechanism is the activation of the brain's reward system in response to novel and potentially rewarding stimuli.\n\n**D)** Emotions are manifestations of social learning, and the proposed mechanism is the imitation of emotional responses in others, which is facilitated by the mirror neuron system.\n\n**Correct Answer:** B) Emotions are manifestations of reward processing, specifically the assessment of Temporal Difference (TD) error, which is the evaluation of how much better or worse a situation just became compared to what was previously expected.\n\n**Explanation:** The correct answer is based on the documentation, which states that the author proposes a Temporal Difference Reinforcement Learning Theory of Emotion, where emotions are seen as manifestations of reward processing, specifically the assessment of TD error. This theory is supported by evidence from psychology, neurobiology, and computation, and is seen as a unifying framework for understanding emotions in humans, animals, and machines."}, "57": {"documentation": {"title": "Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning", "source": "Xin Huang, Duan Li, Daniel Zhuoyu Long", "docs_id": "1906.09035", "section": ["eess.SY", "cs.LG", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning. Stochastic control with both inherent random system noise and lack of knowledge on system parameters constitutes the core and fundamental topic in reinforcement learning (RL), especially under non-episodic situations where online learning is much more demanding. This challenge has been notably addressed in Bayesian RL recently where some approximation techniques have been developed to find suboptimal policies. While existing approaches mainly focus on approximating the value function, or on involving Thompson sampling, we propose a novel two-layer solution scheme in this paper to approximate the optimal policy directly, by combining the time-decomposition based dynamic programming (DP) at the lower layer and the scenario-decomposition based revised progressive hedging algorithm (PHA) at the upper layer, for a type of Bayesian RL problem. The key feature of our approach is to separate reducible system uncertainty from irreducible one at two different layers, thus decomposing and conquering. We demonstrate our solution framework more especially via the linear-quadratic-Gaussian problem with unknown gain, which, although seemingly simple, has been a notorious subject over more than half century in dual control."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Bayesian Reinforcement Learning, what is the primary challenge addressed by the proposed two-layer solution scheme in the paper \"Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\"?\n\n**A)** Approximating the value function using Thompson sampling\n**B)** Separating reducible system uncertainty from irreducible one to conquer the challenge of stochastic control with both inherent random system noise and lack of knowledge on system parameters\n**C)** Developing a novel approach to online learning for non-episodic situations\n**D)** Improving the performance of existing approximation techniques for Bayesian RL\n\n**Correct Answer:** B) Separating reducible system uncertainty from irreducible one to conquer the challenge of stochastic control with both inherent random system noise and lack of knowledge on system parameters\n\n**Explanation:** The correct answer is B) because the proposed two-layer solution scheme addresses the challenge of stochastic control with both inherent random system noise and lack of knowledge on system parameters by separating reducible system uncertainty from irreducible one at two different layers. This allows for a more effective decomposition and conquest of the challenge, as stated in the paper. The other options are incorrect because they do not accurately capture the primary challenge addressed by the proposed solution scheme."}, "58": {"documentation": {"title": "Clustering and Recognition of Spatiotemporal Features through\n  Interpretable Embedding of Sequence to Sequence Recurrent Neural Networks", "source": "Kun Su, Eli Shlizerman", "docs_id": "1905.12176", "section": ["cs.LG", "eess.SP", "q-bio.NC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Clustering and Recognition of Spatiotemporal Features through\n  Interpretable Embedding of Sequence to Sequence Recurrent Neural Networks. Encoder-decoder recurrent neural network models (RNN Seq2Seq) have achieved great success in ubiquitous areas of computation and applications. It was shown to be successful in modeling data with both temporal and spatial dependencies for translation or prediction tasks. In this study, we propose an embedding approach to visualize and interpret the representation of data by these models. Furthermore, we show that the embedding is an effective method for unsupervised learning and can be utilized to estimate the optimality of model training. In particular, we demonstrate that embedding space projections of the decoder states of RNN Seq2Seq model trained on sequences prediction are organized in clusters capturing similarities and differences in the dynamics of these sequences. Such performance corresponds to an unsupervised clustering of any spatio-temporal features and can be employed for time-dependent problems such as temporal segmentation, clustering of dynamic activity, self-supervised classification, action recognition, failure prediction, etc. We test and demonstrate the application of the embedding methodology to time-sequences of 3D human body poses. We show that the methodology provides a high-quality unsupervised categorization of movements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed embedding approach in the study \"Clustering and Recognition of Spatiotemporal Features through Interpretable Embedding of Sequence to Sequence Recurrent Neural Networks\"?\n\nA) The development of a new type of recurrent neural network architecture that can handle both temporal and spatial dependencies.\nB) The demonstration of the effectiveness of embedding space projections in visualizing and interpreting the representation of data by RNN Seq2Seq models.\nC) The proposal of a new method for unsupervised learning that can be used to estimate the optimality of model training.\nD) The application of the embedding methodology to time-sequences of 3D human body poses and the demonstration of its ability to provide high-quality unsupervised categorization of movements.\n\n**Correct Answer:** B) The demonstration of the effectiveness of embedding space projections in visualizing and interpreting the representation of data by RNN Seq2Seq models.\n\n**Explanation:** The correct answer is B because the study proposes an embedding approach to visualize and interpret the representation of data by RNN Seq2Seq models, which is the primary contribution of the study. The other options are related to the study, but they are not the primary contribution. Option A is incorrect because the study does not propose a new type of recurrent neural network architecture. Option C is incorrect because while the study does demonstrate the effectiveness of the embedding approach in estimating the optimality of model training, this is not the primary contribution of the study. Option D is incorrect because while the study does apply the embedding methodology to time-sequences of 3D human body poses, this is a specific application of the approach, not the primary contribution of the study."}, "59": {"documentation": {"title": "Spinors, Inflation, and Non-Singular Cyclic Cosmologies", "source": "C. Armendariz-Picon and Patrick B. Greene", "docs_id": "hep-th/0301129", "section": ["hep-th", "astro-ph", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spinors, Inflation, and Non-Singular Cyclic Cosmologies. We consider toy cosmological models in which a classical, homogeneous, spinor field provides a dominant or sub-dominant contribution to the energy-momentum tensor of a flat Friedmann-Robertson-Walker universe. We find that, if such a field were to exist, appropriate choices of the spinor self-interaction would generate a rich variety of behaviors, quite different from their widely studied scalar field counterparts. We first discuss solutions that incorporate a stage of cosmic inflation and estimate the primordial spectrum of density perturbations seeded during such a stage. Inflation driven by a spinor field turns out to be unappealing as it leads to a blue spectrum of perturbations and requires considerable fine-tuning of parameters. We next find that, for simple, quartic spinor self-interactions, non-singular cyclic cosmologies exist with reasonable parameter choices. These solutions might eventually be incorporated into a successful past- and future-eternal cosmological model free of singularities. In an Appendix, we discuss the classical treatment of spinors and argue that certain quantum systems might be approximated in terms of such fields."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary drawback of inflation driven by a spinor field, according to the study, and how does it differ from inflation driven by scalar fields?\n\nA) It leads to a red spectrum of perturbations and requires minimal fine-tuning of parameters.\nB) It leads to a blue spectrum of perturbations and requires considerable fine-tuning of parameters.\nC) It leads to a flat spectrum of perturbations and requires no fine-tuning of parameters.\nD) It leads to a blue spectrum of perturbations and requires no fine-tuning of parameters.\n\nCorrect Answer: B) It leads to a blue spectrum of perturbations and requires considerable fine-tuning of parameters.\n\nExplanation: The study states that inflation driven by a spinor field \"turns out to be unappealing\" because it leads to a blue spectrum of perturbations, which is undesirable. Additionally, it requires \"considerable fine-tuning of parameters\", indicating that it is not a viable option. This question requires the test-taker to carefully read the documentation and identify the primary drawback of spinor-driven inflation."}}