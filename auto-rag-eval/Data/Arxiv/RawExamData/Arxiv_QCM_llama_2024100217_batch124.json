{"0": {"documentation": {"title": "Risk and optimal policies in bandit experiments", "source": "Karun Adusumilli", "docs_id": "2112.06363", "section": ["econ.EM", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Risk and optimal policies in bandit experiments. This paper provides a decision theoretic analysis of bandit experiments. The bandit setting corresponds to a dynamic programming problem, but solving this directly is typically infeasible. Working within the framework of diffusion asymptotics, we define a suitable notion of asymptotic Bayes risk for bandit settings. For normally distributed rewards, the minimal Bayes risk can be characterized as the solution to a nonlinear second-order partial differential equation (PDE). Using a limit of experiments approach, we show that this PDE characterization also holds asymptotically under both parametric and non-parametric distribution of the rewards. The approach further describes the state variables it is asymptotically sufficient to restrict attention to, and therefore suggests a practical strategy for dimension reduction. The upshot is that we can approximate the dynamic programming problem defining the bandit setting with a PDE which can be efficiently solved using sparse matrix routines. We derive near-optimal policies from the numerical solutions to these equations. The proposed policies substantially dominate existing methods such Thompson sampling. The framework also allows for substantial generalizations to the bandit problem such as time discounting and pure exploration motives."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of bandit experiments, what is the key insight provided by the paper \"Risk and optimal policies in bandit experiments\" regarding the characterization of the minimal Bayes risk, and how does this relate to the solution of a nonlinear second-order partial differential equation (PDE)?\n\nA) The minimal Bayes risk can be characterized as the solution to a linear first-order PDE, which can be efficiently solved using iterative methods.\n\nB) The minimal Bayes risk can be characterized as the solution to a nonlinear second-order PDE, which can be efficiently solved using sparse matrix routines.\n\nC) The minimal Bayes risk can be characterized as the solution to a nonlinear second-order PDE, but this PDE characterization only holds asymptotically under parametric distribution of the rewards.\n\nD) The minimal Bayes risk can be characterized as the solution to a nonlinear second-order PDE, but this PDE characterization only holds asymptotically under non-parametric distribution of the rewards.\n\n**Correct Answer:** B) The minimal Bayes risk can be characterized as the solution to a nonlinear second-order PDE, which can be efficiently solved using sparse matrix routines.\n\n**Explanation:** The paper provides a decision-theoretic analysis of bandit experiments, showing that the minimal Bayes risk can be characterized as the solution to a nonlinear second-order partial differential equation (PDE). This PDE characterization is asymptotically valid under both parametric and non-parametric distributions of the rewards. The key insight is that this PDE can be efficiently solved using sparse matrix routines, making it a practical strategy for dimension reduction."}, "1": {"documentation": {"title": "Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of\n  Absolute Difference", "source": "Saeideh Nabipour, Masoume Gholizade, Nima Nabipour", "docs_id": "2107.02762", "section": ["cs.AR", "cs.CR", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of\n  Absolute Difference. Euclids algorithm is widely used in calculating of GCD (Greatest Common Divisor) of two positive numbers. There are various fields where this division is used such as channel coding, cryptography, and error correction codes. This makes the GCD a fundamental algorithm in number theory, so a number of methods have been discovered to efficiently compute it. The main contribution of this paper is to investigate a method that computes the GCD of two 32-bit numbers based on Euclidean algorithm which targets six different Xilinx chips. The complexity of this method that we call Optimized_GCDSAD is achieved by utilizing Sum of Absolute Difference (SAD) block which is based on a fast carry-out generation function. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency), area delay product (ADP) and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7. A detailed comparative analysis indicates that the proposed Optimized_GCDSAD method based on SAD block outperforms previously known results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the paper \"Area-Delay-Efficient FPGA Design of 32-bit Euclid's GCD based on Sum of Absolute Difference\" in terms of optimizing the Euclidean algorithm for calculating the GCD of two positive numbers?\n\n**A)** The development of a new algorithm that reduces the number of iterations required to compute the GCD.\n**B)** The implementation of a hardware design that utilizes a Sum of Absolute Difference (SAD) block to achieve optimized area-delay product (ADP) complexity.\n**C)** The creation of a software tool that automates the process of calculating the GCD of two numbers using Euclidean algorithm.\n**D)** The proposal of a new method for reducing the latency of the Euclidean algorithm by using a fast carry-out generation function.\n\n**Correct Answer:** B) The implementation of a hardware design that utilizes a Sum of Absolute Difference (SAD) block to achieve optimized area-delay product (ADP) complexity.\n\n**Explanation:** The paper presents a method for computing the GCD of two 32-bit numbers based on Euclidean algorithm, which targets six different Xilinx chips. The main contribution of this paper is the implementation of a hardware design that utilizes a Sum of Absolute Difference (SAD) block to achieve optimized area-delay product (ADP) complexity. This design is evaluated based on criteria such as time (latency), ADP, and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7, and the results indicate that the proposed Optimized_GCDSAD method outperforms previously known results."}, "2": {"documentation": {"title": "An algorithm for online tensor prediction", "source": "John Pothier, Josh Girson, Shuchin Aeron", "docs_id": "1507.07974", "section": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An algorithm for online tensor prediction. We present a new method for online prediction and learning of tensors ($N$-way arrays, $N >2$) from sequential measurements. We focus on the specific case of 3-D tensors and exploit a recently developed framework of structured tensor decompositions proposed in [1]. In this framework it is possible to treat 3-D tensors as linear operators and appropriately generalize notions of rank and positive definiteness to tensors in a natural way. Using these notions we propose a generalization of the matrix exponentiated gradient descent algorithm [2] to a tensor exponentiated gradient descent algorithm using an extension of the notion of von-Neumann divergence to tensors. Then following a similar construction as in [3], we exploit this algorithm to propose an online algorithm for learning and prediction of tensors with provable regret guarantees. Simulations results are presented on semi-synthetic data sets of ratings evolving in time under local influence over a social network. The result indicate superior performance compared to other (online) convex tensor completion methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** An algorithm for online tensor prediction is proposed in the given Arxiv documentation. The algorithm is based on a recently developed framework of structured tensor decompositions. What is the key insight that allows the authors to generalize notions of rank and positive definiteness to tensors in a natural way?\n\n**A)** The authors treat 3-D tensors as linear operators and exploit the properties of tensor decompositions to define a notion of rank and positive definiteness.\n\n**B)** The authors use a matrix exponentiated gradient descent algorithm and extend it to a tensor exponentiated gradient descent algorithm using an extension of the notion of von-Neumann divergence to tensors.\n\n**C)** The authors propose a generalization of the matrix exponentiated gradient descent algorithm to a tensor exponentiated gradient descent algorithm using a structured tensor decomposition framework.\n\n**D)** The authors exploit the properties of tensor decompositions to define a notion of rank and positive definiteness for tensors, allowing them to generalize the matrix exponentiated gradient descent algorithm to a tensor exponentiated gradient descent algorithm.\n\n**Correct Answer:** D) The authors exploit the properties of tensor decompositions to define a notion of rank and positive definiteness for tensors, allowing them to generalize the matrix exponentiated gradient descent algorithm to a tensor exponentiated gradient descent algorithm.\n\n**Explanation:** The correct answer is D) because the documentation states that the authors \"generalize notions of rank and positive definiteness to tensors in a natural way\" by treating 3-D tensors as linear operators and exploiting the properties of tensor decompositions. This allows them to define a notion of rank and positive definiteness for tensors, which is essential for the proposed tensor exponentiated gradient descent algorithm. The other options are incorrect because they either focus on the matrix exponentiated gradient descent algorithm (B and C) or provide a partial description of the key insight (A)."}, "3": {"documentation": {"title": "Femtoscopy of Pb-Pb and pp collisions at the LHC with the ALICE\n  experiment", "source": "A. Kisiel (for the ALICE collaboration)", "docs_id": "1109.5553", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Femtoscopy of Pb-Pb and pp collisions at the LHC with the ALICE\n  experiment. We report on the results of femtoscopic analysis of Pb-Pb collisions at sqrt(s_NN)=2.76 TeV and pp collisions at sqrt(s)=0.9, 2.76 and 7 TeV with identical pions and kaons. Detailed femtoscopy studies in heavy-ion collisions at SPS and RHIC have shown that emission region sizes (\"HBT radii\") decrease with increasing pair transverse momentum k_T, which is understood as a manifestation of the collective behavior of matter. The trend was predicted to persist at the LHC. The data from Pb-Pb collisions confirm the existence of a flowing medium and provide strict constraints on the dynamical models. Similar analysis is carried out for pp collisions for pions and kaons and qualitative similarities to heavy-ion data are seen, especially in collisions producing large number of particles. The observed trends give insight into the soft particle production mechanism in pp collisions. 3D radii were also found to universally scale with event multiplicity in heavy-ion collisions. We extend the range of multiplicities both upwards with the Pb-Pb data and downwards with the pp data to test the scaling in new areas. In particular the high multiplicity pp collisions reach particle densities comparable to the ones measured in peripheral Cu-Cu and Au-Au collisions at RHIC. This allows for the first time to directly compare freeze-out sizes for systems with very different initial states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the soft particle production mechanism in pp collisions based on the femtoscopy analysis of Pb-Pb and pp collisions at the LHC?\n\nA) The soft particle production mechanism in pp collisions is similar to the collective behavior of matter observed in heavy-ion collisions.\nB) The observed trends in femtoscopy analysis confirm the existence of a flowing medium in pp collisions, similar to Pb-Pb collisions.\nC) The high multiplicity pp collisions reach particle densities comparable to those measured in peripheral Cu-Cu and Au-Au collisions at RHIC, allowing for the first time to directly compare freeze-out sizes for systems with very different initial states.\nD) The femtoscopy analysis of Pb-Pb and pp collisions at the LHC provides strict constraints on the dynamical models of soft particle production in pp collisions.\n\nCorrect Answer: D) The femtoscopy analysis of Pb-Pb and pp collisions at the LHC provides strict constraints on the dynamical models of soft particle production in pp collisions.\n\nExplanation: The correct answer is D) because the femtoscopy analysis of Pb-Pb and pp collisions at the LHC provides strict constraints on the dynamical models of soft particle production in pp collisions. The analysis confirms the existence of a flowing medium in Pb-Pb collisions and provides insight into the soft particle production mechanism in pp collisions. The other options are incorrect because they do not accurately summarize the main conclusion of the femtoscopy analysis. Option A is incorrect because the soft particle production mechanism in pp collisions is not directly compared to the collective behavior of matter in heavy-ion collisions. Option B is incorrect because the femtoscopy analysis does not confirm the existence of a flowing medium in pp collisions. Option C is incorrect because while the high multiplicity pp collisions do reach particle densities comparable to those measured in peripheral Cu-Cu and Au-Au collisions at RHIC, this is not the main conclusion of the femtoscopy analysis."}, "4": {"documentation": {"title": "Chaos in three coupled rotators: From Anosov dynamics to hyperbolic\n  attractors", "source": "Sergey P. Kuznetsov", "docs_id": "1708.04493", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chaos in three coupled rotators: From Anosov dynamics to hyperbolic\n  attractors. Starting from Anosov chaotic dynamics of geodesic flow on a surface of negative curvature, we develop and consider a number of self-oscillatory systems including those with hinged mechanical coupling of three rotators and a system of rotators interacting through a potential function. These results are used to design an electronic circuit for generation of rough (structurally stable) chaos. Results of numerical integration of the model equations of different degree of accuracy are presented and discussed. Also, circuit simulation of the electronic generator is provided using the NI Multisim environment. Portraits of attractors, waveforms of generated oscillations, Lyapunov exponents, and spectra are considered and found to be in good correspondence for the dynamics on the attractive sets of the self-oscillatory systems and for the original Anosov geodesic flow. The hyperbolic nature of the dynamics is tested numerically using a criterion based on statistics of angles of intersection of stable and unstable subspaces of the perturbation vectors at a reference phase trajectory on the attractor."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Chaos in three coupled rotators: From Anosov dynamics to hyperbolic attractors\", what is the primary criterion used to test the hyperbolic nature of the dynamics in the self-oscillatory systems, and how is it numerically implemented?\n\nA) The Lyapunov exponent is used to measure the rate of divergence of nearby trajectories, and it is numerically implemented by computing the statistics of angles of intersection of stable and unstable subspaces of the perturbation vectors at a reference phase trajectory on the attractor.\n\nB) The Poincar\u00e9 index is used to measure the topological properties of the attractor, and it is numerically implemented by computing the number of times the stable and unstable manifolds intersect the attractor.\n\nC) The Kolmogorov-Sinai entropy is used to measure the complexity of the attractor, and it is numerically implemented by computing the ratio of the entropy of the system to the entropy of a random process.\n\nD) The fractal dimension is used to measure the complexity of the attractor, and it is numerically implemented by computing the ratio of the fractal dimension of the attractor to the fractal dimension of a random process.\n\n**Correct Answer:** A) The Lyapunov exponent is used to measure the rate of divergence of nearby trajectories, and it is numerically implemented by computing the statistics of angles of intersection of stable and unstable subspaces of the perturbation vectors at a reference phase trajectory on the attractor.\n\n**Explanation:** The Lyapunov exponent is a measure of the rate of divergence of nearby trajectories in a dynamical system. In the context of the paper, it is used to test the hyperbolic nature of the dynamics in the self-oscillatory systems. The Lyapunov exponent is numerically implemented by computing the statistics of angles of intersection of stable and unstable subspaces of the perturbation vectors at a reference phase trajectory on the attractor. This is done to determine whether the system exhibits hyperbolic behavior, which is a necessary condition for the existence of hyperbolic attractors."}, "5": {"documentation": {"title": "Asymptotic analysis of microscopic impenetrability constraints for\n  atomistic systems", "source": "Andrea Braides, Maria Stella Gelli", "docs_id": "1505.02919", "section": ["math-ph", "math.AP", "math.MP", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotic analysis of microscopic impenetrability constraints for\n  atomistic systems. In this paper we analyze a two-dimensional discrete model of nearest-neighbour Lennard-Jones interactions under the microscopical constraint that points on a lattice triangle maintain their order. This can be understood as a microscopical non-interpenetration constraint and amounts to the positiveness of the determinant of the gradient of the piecewise-affine interpolations of the discrete displacement. Under such a constraint we examine the continuum fracture energy deriving from a discrete-to-continuum analysis at a scaling where surface energy is preponderant. We give a lower bound by an anisotropic Griffith energy. This bound is optimal if the macroscopic displacement satisfies some opening-crack conditions on the fracture site. We show that if such conditions are not satisfied then the computation of the energy due to continuum cracks may involve non-local effects necessary to bypass the positive-determinant constraint on crack surfaces and at points where more cracks meet. Even when the limit fracture energy may be described by a surface energy density, this may depend on the crack orientation both in the reference and in the deformed configuration. While these effects lead to very interesting analytical issues, they call into question the necessity of the determinant constraint for fracture problems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the significance of the determinant constraint on the gradient of the piecewise-affine interpolations of the discrete displacement in the context of microscopic impenetrability constraints for atomistic systems, and how does it relate to the computation of fracture energy in the presence of continuum cracks?\n\nA) The determinant constraint is a necessary condition for the existence of a stable fracture, and its violation leads to non-local effects that require the use of anisotropic Griffith energy.\n\nB) The determinant constraint is a sufficient condition for the computation of fracture energy, and its satisfaction guarantees the optimal bound on the fracture energy.\n\nC) The determinant constraint is a byproduct of the discrete-to-continuum analysis, and its violation does not affect the computation of fracture energy in the presence of continuum cracks.\n\nD) The determinant constraint is a fundamental limit on the fracture energy, and its violation leads to a non-analytical solution that requires numerical methods to solve.\n\nCorrect Answer: A) The determinant constraint is a necessary condition for the existence of a stable fracture, and its violation leads to non-local effects that require the use of anisotropic Griffith energy.\n\nExplanation: The correct answer is A) because the determinant constraint is a necessary condition for the existence of a stable fracture, as it ensures that the microscopical non-interpenetration constraint is satisfied. The violation of this constraint leads to non-local effects that require the use of anisotropic Griffith energy, which is a lower bound on the fracture energy. This is evident from the text, which states that the bound is optimal if the macroscopic displacement satisfies some opening-crack conditions on the fracture site."}, "6": {"documentation": {"title": "Economic Performance Through Time: A Dynamical Theory", "source": "Daniel Seligson and Anne McCants", "docs_id": "1905.02956", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Economic Performance Through Time: A Dynamical Theory. The central problems of Development Economics are the explanation of the gross disparities in the global distribution, $\\cal{D}$, of economic performance, $\\cal{E}$, and the persistence, $\\cal{P}$, of said distribution. Douglass North argued, epigrammatically, that institutions, $\\cal{I}$, are the rules of the game, meaning that $\\cal{I}$ determines or at least constrains $\\cal{E}$. This promised to explain $\\cal{D}$. 65,000 citations later, the central problems remain unsolved. North's institutions are informal, slowly changing cultural norms as well as roads, guilds, and formal legislation that may change overnight. This definition, mixing the static and the dynamic, is unsuited for use in a necessarily time dependent theory of developing economies. We offer here a suitably precise definition of $\\cal{I}$, a dynamical theory of economic development, a new measure of the economy, an explanation of $\\cal{P}$, a bivariate model that explains half of $\\cal{D}$, and a critical reconsideration of North's epigram."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the text, what is the central problem that remains unsolved in Development Economics, despite the significant impact of Douglass North's work on the field?\n\nA) The explanation of the persistence of economic growth in developed economies\nB) The explanation of the gross disparities in the global distribution of economic performance, $\\cal{D}$, and the persistence, $\\cal{P}$, of said distribution\nC) The development of a new measure of economic performance that can accurately capture the complexities of economic development\nD) The critical reconsideration of the role of institutions, $\\cal{I}$, in shaping economic outcomes\n\n**Correct Answer:** B) The explanation of the gross disparities in the global distribution of economic performance, $\\cal{D}$, and the persistence, $\\cal{P}$, of said distribution\n\n**Explanation:** The text states that the central problems of Development Economics are the explanation of the gross disparities in the global distribution, $\\cal{D}$, of economic performance, $\\cal{E}$, and the persistence, $\\cal{P}$, of said distribution. Despite the significant impact of Douglass North's work on the field, these problems remain unsolved. Therefore, option B is the correct answer.\n\n**Candidate A Explanation:** This option is incorrect because it focuses on the explanation of economic growth in developed economies, which is not the central problem addressed in the text.\n\n**Candidate C Explanation:** This option is incorrect because while the development of a new measure of economic performance is mentioned in the text, it is not the central problem that remains unsolved.\n\n**Candidate D Explanation:** This option is incorrect because while the critical reconsideration of the role of institutions is mentioned in the text, it is not the central problem that remains unsolved. The text actually argues that North's definition of institutions is unsuited for use in a time-dependent theory of developing economies."}, "7": {"documentation": {"title": "Generic conditions for stable hybrid stars", "source": "Mark G. Alford, Sophia Han, Madappa Prakash", "docs_id": "1302.4732", "section": ["astro-ph.SR", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generic conditions for stable hybrid stars. We study the mass-radius curve of hybrid stars, assuming a single first-order phase transition between nuclear and quark matter, with a sharp interface between the quark matter core and nuclear matter mantle. We use a generic parameterization of the quark matter equation of state, which has a constant, i.e. density-independent, speed of sound (\"CSS\"). We argue that this parameterization provides a framework for comparison and empirical testing of models of quark matter. We obtain the phase diagram of possible forms of the hybrid star mass-radius relation, where the control parameters are the transition pressure, energy density discontinuity, and the quark matter speed of sound. We find that this diagram is sensitive to the quark matter parameters but fairly insensitive to details of the nuclear matter equation of state. We calculate the maximum hybrid star mass as a function of the parameters of the quark matter EoS, and find that there are reasonable values of those parameters that give rise to hybrid stars with mass above $2\\,M_\\odot$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion regarding the mass-radius curve of hybrid stars, as derived from the study of the phase transition between nuclear and quark matter?\n\nA) The mass-radius curve is highly sensitive to the details of the nuclear matter equation of state.\nB) The mass-radius curve is fairly insensitive to the details of the nuclear matter equation of state, but sensitive to the quark matter parameters.\nC) The mass-radius curve is only dependent on the transition pressure and energy density discontinuity.\nD) The mass-radius curve is not affected by the quark matter speed of sound.\n\nCorrect Answer: B) The mass-radius curve is fairly insensitive to the details of the nuclear matter equation of state, but sensitive to the quark matter parameters.\n\nExplanation: The study concludes that the phase diagram of possible forms of the hybrid star mass-radius relation is sensitive to the quark matter parameters, but fairly insensitive to details of the nuclear matter equation of state. This suggests that the mass-radius curve is primarily influenced by the properties of the quark matter, rather than the nuclear matter."}, "8": {"documentation": {"title": "Isospin Symmetry Breaking within the HLS Model: A Full ($\\rho, \\omega,\n  \\phi$) Mixing Scheme", "source": "M. Benayoun and H.B. O'Connell", "docs_id": "nucl-th/0107047", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isospin Symmetry Breaking within the HLS Model: A Full ($\\rho, \\omega,\n  \\phi$) Mixing Scheme. We study the way isospin symmetry violation can be generated within the Hidden Local Symmetry (HLS) Model. We show that isospin symmetry breaking effects on pseudoscalar mesons naturally induces correspondingly effects within the physics of vector mesons, through kaon loops. In this way, one recovers all features traditionally expected from $\\rho-\\omg$ mixing and one finds support for the Orsay phase modelling of the $e^+e^- \\ra \\pi^+ \\pi^-$ amplitude. We then examine an effective procedure which generates mixing in the whole $\\rho$, $\\omg$, $\\phi$ sector of the HLS Model. The corresponding model allows us to account for all two body decays of light mesons accessible to the HLS model in modulus and phase, leaving aside the $\\rho \\ra \\pi \\pi$ and $K^* \\ra K \\pi$ modes only, which raise a specific problem. Comparison with experimental data is performed and covers modulus and phase information; this represents 26 physics quantities successfully described with very good fit quality within a constrained model which accounts for SU(3) breaking, nonet symmetry breaking in the pseudoscalar sector and, now, isospin symmetry breaking."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Hidden Local Symmetry (HLS) Model, describe the mechanism by which isospin symmetry breaking affects the mixing of pseudoscalar mesons, and explain how this, in turn, influences the behavior of vector mesons, including the $\\rho$ and $\\omega$ mesons.\n\n**A)** Isospin symmetry breaking in the HLS Model is mediated by kaon loops, which induce mixing in the pseudoscalar sector, leading to a corresponding effect on the vector meson sector, resulting in a $\\rho-\\omega$ mixing scheme.\n\n**B)** The HLS Model predicts that isospin symmetry breaking in the pseudoscalar sector leads to a suppression of the $\\rho-\\omega$ mixing scheme, resulting in a reduced mixing between the two vector mesons.\n\n**C)** The Orsay phase modelling of the $e^+e^- \\ra \\pi^+ \\pi^-$ amplitude is a consequence of the isospin symmetry breaking effects on the pseudoscalar mesons, which in turn influence the behavior of the vector mesons, including the $\\rho$ and $\\omega$ mesons.\n\n**D)** The effective procedure for generating mixing in the whole $\\rho$, $\\omega$, $\\phi$ sector of the HLS Model relies on the introduction of a new symmetry breaking mechanism, which is not related to the isospin symmetry breaking effects on pseudoscalar mesons.\n\n**Correct Answer:** A) Isospin symmetry breaking in the HLS Model is mediated by kaon loops, which induce mixing in the pseudoscalar sector, leading to a corresponding effect on the vector meson sector, resulting in a $\\rho-\\omega$ mixing scheme.\n\n**Explanation:** The correct answer is A) because the documentation states that isospin symmetry breaking effects on pseudoscalar mesons naturally induce corresponding effects within the physics of vector mesons, through kaon loops. This leads to a $\\rho-\\omega$ mixing scheme, which is a key feature of the HLS Model. The other options are incorrect because they either misrepresent the relationship between isospin symmetry breaking and mixing in the vector meson sector (B), or incorrectly attribute the Orsay phase modelling to the isospin symmetry breaking effects on pseudoscalar mesons (C), or introduce a new symmetry breaking mechanism that is not related to the isospin symmetry breaking effects on pseudoscalar mesons (D)."}, "9": {"documentation": {"title": "Capacity Region of the Finite-State Multiple Access Channel with and\n  without Feedback", "source": "Haim Permuter, Tsachy Weissman", "docs_id": "0708.0271", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Capacity Region of the Finite-State Multiple Access Channel with and\n  without Feedback. The capacity region of the Finite-State Multiple Access Channel (FS-MAC) with feedback that may be an arbitrary time-invariant function of the channel output samples is considered. We characterize both an inner and an outer bound for this region, using Masseys's directed information. These bounds are shown to coincide, and hence yield the capacity region, of FS-MACs where the state process is stationary and ergodic and not affected by the inputs. Though `multi-letter' in general, our results yield explicit conclusions when applied to specific scenarios of interest. E.g., our results allow us to: - Identify a large class of FS-MACs, that includes the additive mod-2 noise MAC where the noise may have memory, for which feedback does not enlarge the capacity region. - Deduce that, for a general FS-MAC with states that are not affected by the input, if the capacity (region) without feedback is zero, then so is the capacity (region) with feedback. - Deduce that the capacity region of a MAC that can be decomposed into a `multiplexer' concatenated by a point-to-point channel (with, without, or with partial feedback), the capacity region is given by $\\sum_{m} R_m \\leq C$, where C is the capacity of the point to point channel and m indexes the encoders. Moreover, we show that for this family of channels source-channel coding separation holds."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Finite-State Multiple Access Channel (FS-MAC) with feedback, where the state process is stationary and ergodic, and not affected by the inputs. Suppose that the capacity region of the FS-MAC without feedback is zero. What can be concluded about the capacity region of the FS-MAC with feedback?\n\nA) The capacity region with feedback is also zero.\nB) The capacity region with feedback is equal to the capacity region without feedback.\nC) The capacity region with feedback is strictly larger than the capacity region without feedback.\nD) The capacity region with feedback is equal to the capacity region without feedback, but only for a specific type of feedback.\n\n**Correct Answer:** A) The capacity region with feedback is also zero.\n\n**Explanation:** According to the documentation, if the capacity (region) without feedback is zero, then so is the capacity (region) with feedback. This is because the bounds for the capacity region with feedback coincide with the bounds for the capacity region without feedback, and the capacity region without feedback is zero. Therefore, the capacity region with feedback must also be zero.\n\n**Note:** The other options are incorrect because:\n\n* B) The capacity region with feedback is not necessarily equal to the capacity region without feedback.\n* C) The capacity region with feedback is not strictly larger than the capacity region without feedback.\n* D) The capacity region with feedback is not limited to a specific type of feedback."}, "10": {"documentation": {"title": "Safely Learning Dynamical Systems from Short Trajectories", "source": "Amir Ali Ahmadi, Abraar Chaudhry, Vikas Sindhwani, Stephen Tu", "docs_id": "2011.12257", "section": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Safely Learning Dynamical Systems from Short Trajectories. A fundamental challenge in learning to control an unknown dynamical system is to reduce model uncertainty by making measurements while maintaining safety. In this work, we formulate a mathematical definition of what it means to safely learn a dynamical system by sequentially deciding where to initialize the next trajectory. In our framework, the state of the system is required to stay within a given safety region under the (possibly repeated) action of all dynamical systems that are consistent with the information gathered so far. For our first two results, we consider the setting of safely learning linear dynamics. We present a linear programming-based algorithm that either safely recovers the true dynamics from trajectories of length one, or certifies that safe learning is impossible. We also give an efficient semidefinite representation of the set of initial conditions whose resulting trajectories of length two are guaranteed to stay in the safety region. For our final result, we study the problem of safely learning a nonlinear dynamical system. We give a second-order cone programming based representation of the set of initial conditions that are guaranteed to remain in the safety region after one application of the system dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Safely Learning Dynamical Systems from Short Trajectories\n\nConsider a linear dynamical system with a safety region defined by a convex polytope in the state space. The system's dynamics are represented by a matrix A, and the initial condition is represented by a vector x0. The goal is to safely learn the system's dynamics by sequentially deciding where to initialize the next trajectory.\n\n**A)** The algorithm presented in the paper can safely recover the true dynamics from a single trajectory of length one if and only if the safety region is a convex polytope.\n\n**B)** The algorithm can safely learn the system's dynamics by certifying that safe learning is impossible if and only if the safety region is empty.\n\n**C)** The semidefinite representation of the set of initial conditions whose resulting trajectories of length two are guaranteed to stay in the safety region can be computed using a linear programming-based algorithm.\n\n**D)** The second-order cone programming based representation of the set of initial conditions that are guaranteed to remain in the safety region after one application of the system dynamics can be computed using a semidefinite programming-based algorithm.\n\n**Correct Answer:** C) The semidefinite representation of the set of initial conditions whose resulting trajectories of length two are guaranteed to stay in the safety region can be computed using a linear programming-based algorithm.\n\n**Explanation:** The correct answer is C) because the paper presents a linear programming-based algorithm for computing the semidefinite representation of the set of initial conditions whose resulting trajectories of length two are guaranteed to stay in the safety region. This algorithm is used to efficiently compute the set of initial conditions that are safe to learn the system's dynamics.\n\nThe other options are incorrect because:\n\n* A) The paper does not provide a direct relationship between the safety region being a convex polytope and the ability to safely recover the true dynamics from a single trajectory of length one.\n* B) The paper does not provide a direct relationship between the safety region being empty and the impossibility of safe learning.\n* D) The paper does not provide a direct relationship between the second-order cone programming based representation and the use of a semidefinite programming-based algorithm."}, "11": {"documentation": {"title": "Decomposition scheme matters more than you may think", "source": "Anna Naszodi", "docs_id": "2104.09141", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decomposition scheme matters more than you may think. This paper promotes the application of a path-independent decomposition scheme. Besides presenting some theoretical arguments supporting this decomposition scheme, this study also illustrates the difference between the path-independent decomposition scheme and a popular sequential decomposition with an empirical application of the two schemes. The empirical application is about identifying a directly unobservable phenomenon, i.e. the changing social gap between people from different educational strata, through its effect on marriages and cohabitations. It exploits census data from four waves between 1977 and 2011 about the American, French, Hungarian, Portuguese, and Romanian societies. For some societies and periods, the outcome of the decomposition is found to be highly sensitive to the choice of the decomposition scheme. These examples illustrate the point that a careful selection of the decomposition scheme is crucial for adequately documenting the dynamics of unobservable factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The paper \"Decomposition scheme matters more than you may think\" highlights the importance of selecting an appropriate decomposition scheme in analyzing the dynamics of unobservable factors. Which of the following statements best summarizes the main argument of the paper?\n\nA) The choice of decomposition scheme has a negligible impact on the results of social gap analysis.\nB) A path-independent decomposition scheme is always more accurate than a sequential decomposition scheme.\nC) The decomposition scheme can significantly affect the outcome of social gap analysis, and a careful selection is crucial.\nD) The paper only presents theoretical arguments supporting the use of a path-independent decomposition scheme.\n\n**Correct Answer:** C) The decomposition scheme can significantly affect the outcome of social gap analysis, and a careful selection is crucial.\n\n**Explanation:**\n\nThe paper argues that the choice of decomposition scheme can have a significant impact on the results of social gap analysis, particularly when dealing with unobservable factors. The authors illustrate this point through an empirical application of two decomposition schemes, one path-independent and one sequential, on data from five societies. The results show that the outcome of the decomposition is highly sensitive to the choice of scheme, highlighting the importance of selecting an appropriate decomposition scheme.\n\nCandidate A is incorrect because the paper explicitly states that the choice of decomposition scheme can have a significant impact on the results. Candidate B is incorrect because the paper does not claim that a path-independent decomposition scheme is always more accurate than a sequential decomposition scheme. Candidate D is incorrect because the paper presents both theoretical arguments and empirical evidence supporting the use of a path-independent decomposition scheme."}, "12": {"documentation": {"title": "The Network Dynamics of Social and Technological Conventions", "source": "Joshua Becker", "docs_id": "2003.12112", "section": ["physics.soc-ph", "cs.SI", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Network Dynamics of Social and Technological Conventions. The selection of social and technological conventions represents an important form of collective decision-making. While the ergodic properties of coordination models suggest that the optimal strategy will spread in the long run, lock-in effects mean that the first strategy to obtain widespread adoption is likely to stick--it's possible for everyone to do one thing but prefer another. The present paper examines how network structure impacts the likelihood that the optimal strategy will be widely adopted as the first equilibrium. This analysis focuses on the role of central nodes, which play a beneficial role in spreading innovations by increasing the speed of adoption, but can inadvertently promote suboptimal solutions at the expense of other, better solutions. Centralized networks have a faster rate of convention formation but a lower likelihood of optimal selection. Surprisingly, however, this finding does not indicate a speed/optimality tradeoff: dense networks are both fast and optimal."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the Network Dynamics of Social and Technological Conventions, what is the relationship between the structure of a network and the likelihood of the optimal strategy being widely adopted as the first equilibrium?\n\n**A)** Centralized networks are more likely to lead to optimal selection, as they allow for faster convention formation and promote the spread of innovations.\n\n**B)** Dense networks are more likely to lead to optimal selection, as they facilitate the exchange of information and promote the adoption of better solutions.\n\n**C)** Centralized networks are more likely to lead to suboptimal selection, as they can inadvertently promote the adoption of inferior solutions at the expense of other, better solutions.\n\n**D)** The structure of a network has no impact on the likelihood of the optimal strategy being widely adopted as the first equilibrium.\n\n**Correct Answer:** B) Dense networks are more likely to lead to optimal selection, as they facilitate the exchange of information and promote the adoption of better solutions.\n\n**Explanation:** The correct answer is B) Dense networks are more likely to lead to optimal selection, as they facilitate the exchange of information and promote the adoption of better solutions. This is because dense networks allow for more efficient information exchange, which can lead to the faster adoption of optimal solutions. In contrast, centralized networks may promote the adoption of inferior solutions, as they can create \"islands\" of adoption that are not connected to other parts of the network. The correct answer requires an understanding of the relationship between network structure and the spread of innovations, as well as the concept of optimal selection."}, "13": {"documentation": {"title": "Condition number analysis and preconditioning of the finite cell method", "source": "F. de Prenter, C.V. Verhoosel, G.J. van Zwieten, E.H. van Brummelen", "docs_id": "1601.05129", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Condition number analysis and preconditioning of the finite cell method. The (Isogeometric) Finite Cell Method - in which a domain is immersed in a structured background mesh - suffers from conditioning problems when cells with small volume fractions occur. In this contribution, we establish a rigorous scaling relation between the condition number of (I)FCM system matrices and the smallest cell volume fraction. Ill-conditioning stems either from basis functions being small on cells with small volume fractions, or from basis functions being nearly linearly dependent on such cells. Based on these two sources of ill-conditioning, an algebraic preconditioning technique is developed, which is referred to as Symmetric Incomplete Permuted Inverse Cholesky (SIPIC). A detailed numerical investigation of the effectivity of the SIPIC preconditioner in improving (I)FCM condition numbers and in improving the convergence speed and accuracy of iterative solvers is presented for the Poisson problem and for two- and three-dimensional problems in linear elasticity, in which Nitche's method is applied in either the normal or tangential direction. The accuracy of the preconditioned iterative solver enables mesh convergence studies of the finite cell method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary source of ill-conditioning in the Isogeometric Finite Cell Method (I)FCM system matrices, and how does it relate to the smallest cell volume fraction?\n\nA) Basis functions being nearly linearly dependent on cells with small volume fractions is the primary source of ill-conditioning, and it is directly proportional to the smallest cell volume fraction.\n\nB) Basis functions being small on cells with small volume fractions is the primary source of ill-conditioning, and it is inversely proportional to the smallest cell volume fraction.\n\nC) Basis functions being nearly linearly dependent on cells with small volume fractions is the primary source of ill-conditioning, and it is directly proportional to the square of the smallest cell volume fraction.\n\nD) Basis functions being small on cells with small volume fractions is the primary source of ill-conditioning, and it is inversely proportional to the square of the smallest cell volume fraction.\n\n**Correct Answer:** B) Basis functions being small on cells with small volume fractions is the primary source of ill-conditioning, and it is inversely proportional to the smallest cell volume fraction.\n\n**Explanation:** According to the documentation, the primary source of ill-conditioning in the IFCM system matrices is the basis functions being small on cells with small volume fractions. This is because the basis functions are not well-represented on cells with small volume fractions, leading to a poor conditioning of the system matrix. The correct answer states that this is inversely proportional to the smallest cell volume fraction, meaning that as the smallest cell volume fraction decreases, the ill-conditioning of the system matrix increases."}, "14": {"documentation": {"title": "The importance of being discrete: on the inaccuracy of continuous\n  approximations in auction theory", "source": "Itzhak Rasooly and Carlos Gavidia-Calderon", "docs_id": "2006.03016", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The importance of being discrete: on the inaccuracy of continuous\n  approximations in auction theory. While auction theory views bids and valuations as continuous variables, real-world auctions are necessarily discrete. In this paper, we use a combination of analytical and computational methods to investigate whether incorporating discreteness substantially changes the predictions of auction theory, focusing on the case of uniformly distributed valuations so that our results bear on the majority of auction experiments. In some cases, we find that introducing discreteness changes little. For example, the first-price auction with two bidders and an even number of values has a symmetric equilibrium that closely resembles its continuous counterpart and converges to its continuous counterpart as the discretisation goes to zero. In others, however, we uncover discontinuity results. For instance, introducing an arbitrarily small amount of discreteness into the all-pay auction makes its symmetric, pure-strategy equilibrium disappear; and appears (based on computational experiments) to rob the game of pure-strategy equilibria altogether. These results raise questions about the continuity approximations on which auction theory is based and prompt a re-evaluation of the experimental literature."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nWhat is the primary finding of the paper \"The importance of being discrete: on the inaccuracy of continuous approximations in auction theory\" regarding the impact of introducing discreteness into auction theory?\n\nA) The introduction of discreteness has no significant effect on the predictions of auction theory.\nB) The introduction of discreteness leads to the disappearance of symmetric, pure-strategy equilibria in all auction types.\nC) The introduction of discreteness has a negligible effect on the convergence of auction theory to its continuous counterpart.\nD) The introduction of discreteness substantially changes the predictions of auction theory, particularly in cases where valuations are uniformly distributed.\n\nCorrect Answer: D) The introduction of discreteness substantially changes the predictions of auction theory, particularly in cases where valuations are uniformly distributed.\n\nExplanation: The paper investigates the impact of introducing discreteness into auction theory and finds that it can lead to significant changes in the predictions of auction theory, particularly in cases where valuations are uniformly distributed. This is evident from the examples provided in the text, such as the all-pay auction where introducing an arbitrarily small amount of discreteness makes its symmetric, pure-strategy equilibrium disappear. Therefore, option D is the correct answer."}, "15": {"documentation": {"title": "A mathematical model of national-level food system sustainability", "source": "Conor Goold, Simone Pfuderer, William H. M. James, Nik Lomax, Fiona\n  Smith, Lisa M. Collins", "docs_id": "2012.08355", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A mathematical model of national-level food system sustainability. The global food system faces various endogeneous and exogeneous, biotic and abiotic risk factors, including a rising human population, higher population densities, price volatility and climate change. Quantitative models play an important role in understanding food systems' expected responses to shocks and stresses. Here, we present a stylised mathematical model of a national-level food system that incorporates domestic supply of a food commodity, international trade, consumer demand, and food commodity price. We derive a critical compound parameter signalling when domestic supply will become unsustainable and the food system entirely dependent on imports, which results in higher commodity prices, lower consumer demand and lower inventory levels. Using Bayesian estimation, we apply the dynamic food systems model to infer the sustainability of the UK pork industry. We find that the UK pork industry is currently sustainable but because the industry is dependent on imports to meet demand, a decrease in self-sufficiency below 50% (current levels are 60-65%) would lead it close to the critical boundary signalling its collapse. Our model provides a theoretical foundation for future work to determine more complex causal drivers of food system vulnerability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A mathematical model of national-level food system sustainability is presented, which incorporates domestic supply, international trade, consumer demand, and food commodity price. The model identifies a critical compound parameter that signals when domestic supply will become unsustainable and the food system entirely dependent on imports. Using Bayesian estimation, the model is applied to the UK pork industry. What is the threshold of self-sufficiency in the UK pork industry below which it is likely to collapse, according to the model?\n\nA) 40%\nB) 50%\nC) 60%\nD) 70%\n\n**Correct Answer:** B) 50%\n\n**Explanation:** According to the model, a decrease in self-sufficiency below 50% would lead the UK pork industry close to the critical boundary signalling its collapse. This is stated in the documentation: \"Our model finds that the UK pork industry is currently sustainable but because the industry is dependent on imports to meet demand, a decrease in self-sufficiency below 50% (current levels are 60-65%) would lead it close to the critical boundary signalling its collapse.\""}, "16": {"documentation": {"title": "The X-ray coronae of the two brightest galaxies in the Coma cluster", "source": "J. S. Sanders, A. C. Fabian, M. Sun, E. Churazov, A. Simionescu, S. A.\n  Walker, N. Werner", "docs_id": "1401.3131", "section": ["astro-ph.GA", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The X-ray coronae of the two brightest galaxies in the Coma cluster. We use deep Chandra X-ray Observatory observations to examine the coronae of the two brightest cluster galaxies in the Coma cluster of galaxies, NGC 4874 and NGC 4889. We find that NGC 4889 hosts a central depression in X-ray surface brightness consistent with a cavity or pair of cavities of radius 0.6 kpc. If the central cavity is associated with an AGN outburst and contains relativistic material, its enthalpy should be around 5x10^55 erg. The implied heating power of this cavity would be around an order of magnitude larger than the energy lost by X-ray emission. It would be the smallest and youngest known cavity in a brightest cluster galaxy and the lack of over pressuring implies heating is still gentle. In contrast, NGC 4874 does not show any evidence for cavities, although it hosts a well-known wide-angle-tail radio source which is visible outside the region occupied by the X-ray corona. These two galaxies show that AGN feedback can behave in varied ways in the same cluster environment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the heating mechanism in the central cavity of NGC 4889, given that its implied heating power is around an order of magnitude larger than the energy lost by X-ray emission, and the lack of overpressuring?\n\nA) The cavity is likely being heated by a slow and gentle process, such as adiabatic cooling.\nB) The cavity is likely being heated by a rapid and energetic process, such as a supernova explosion.\nC) The cavity is likely being heated by an AGN outburst, which would contain relativistic material and have an enthalpy of around 5x10^55 erg.\nD) The cavity is likely being heated by a combination of thermal and non-thermal processes, but the exact mechanism is unknown.\n\nCorrect Answer: C) The cavity is likely being heated by an AGN outburst, which would contain relativistic material and have an enthalpy of around 5x10^55 erg.\n\nExplanation: The correct answer is C) because the text states that the central depression in X-ray surface brightness is consistent with a cavity or pair of cavities of radius 0.6 kpc, and that the enthalpy of the cavity should be around 5x10^55 erg if it is associated with an AGN outburst. This suggests that the cavity is being heated by an AGN outburst, which would contain relativistic material and have a high enthalpy. The other options are incorrect because they do not provide a plausible explanation for the observed X-ray surface brightness and the lack of overpressuring. Option A is incorrect because a slow and gentle process would not produce a cavity with such a high enthalpy. Option B is incorrect because a supernova explosion would not produce a cavity with such a high enthalpy, and would not be consistent with the observed X-ray surface brightness. Option D is incorrect because while it is possible that the cavity is being heated by a combination of thermal and non-thermal processes, the text does not provide enough information to support this conclusion."}, "17": {"documentation": {"title": "Normalizers of Operator Algebras and Reflexivity", "source": "A. Katavolos and I.G. Todorov", "docs_id": "math/0005178", "section": ["math.OA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Normalizers of Operator Algebras and Reflexivity. The set of normalizers between von Neumann (or, more generally, reflexive) algebras A and B, (that is, the set of all operators x such that xAx* is a subset of B and x*Bx is a subset of A) possesses `local linear structure': it is a union of reflexive linear spaces. These spaces belong to the interesting class of normalizing linear spaces, namely, those linear spaces U for which UU*U is a subset of U. Such a space is reflexive whenever it is ultraweakly closed, and then it is of the form U={x:xp=h(p)x, for all p in P}, where P is a set of projections and h a certain map defined on P. A normalizing space consists of normalizers between appropriate von Neumann algebras A and B. Necessary and sufficient conditions are found for a normalizing space to consist of normalizers between two reflexive algebras. Normalizing spaces which are bimodules over maximal abelian selfadjoint algebras consist of operators `supported' on sets of the form [f=g] where f and g are appropriate Borel functions. They also satisfy spectral synthesis in the sense of Arveson."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let A and B be two von Neumann algebras, and let U be a normalizing space between them. Suppose that U is ultraweakly closed and consists of normalizers between A and B. Which of the following statements are true?\n\nA) U is reflexive if and only if it is of the form U={x:xp=h(p)x, for all p in P}, where P is a set of projections and h a certain map defined on P.\n\nB) U is a bimodule over a maximal abelian selfadjoint algebra if and only if it consists of operators supported on sets of the form [f=g], where f and g are Borel functions.\n\nC) U satisfies spectral synthesis if and only if it is a normalizing space between two reflexive algebras.\n\nD) All normalizing spaces between two von Neumann algebras are of the form U={x:xp=h(p)x, for all p in P}, where P is a set of projections and h a certain map defined on P.\n\nCorrect Answer: A\n\nExplanation:\n\nStatement A is true because it is a necessary and sufficient condition for a normalizing space to be reflexive. If U is reflexive, then it is of the form U={x:xp=h(p)x, for all p in P}, where P is a set of projections and h a certain map defined on P. Conversely, if U is of this form, then it is reflexive.\n\nStatement B is false because it is not a necessary and sufficient condition for a normalizing space to be a bimodule over a maximal abelian selfadjoint algebra. While normalizing spaces that are bimodules over maximal abelian selfadjoint algebras do consist of operators supported on sets of the form [f=g], this is not a necessary condition for a normalizing space to be a bimodule.\n\nStatement C is false because spectral synthesis is a property of normalizing spaces between two reflexive algebras, but it is not a necessary and sufficient condition for a normalizing space to satisfy spectral synthesis. Normalizing spaces that satisfy spectral synthesis may not be between two reflexive algebras.\n\nStatement D is false because not all normalizing spaces between two von Neumann algebras are of the form U={x:xp=h(p)x, for all p in P}, where P is a set of projections and h a certain map defined on P. This is a specific condition that is only true for reflexive normalizing spaces."}, "18": {"documentation": {"title": "Additive unit structure of endomorphism rings and invariance of modules", "source": "Pedro A. Guil Asensio, T. C. Quynh, Ashish K. Srivastava", "docs_id": "1610.06638", "section": ["math.RA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Additive unit structure of endomorphism rings and invariance of modules. We use the type theory for rings of operators due to Kaplansky to describe the structure of modules that are invariant under automorphisms of their injective envelopes. Also, we highlight the importance of Boolean rings in the study of such modules. As a consequence of this approach, we are able to further the study initiated by Dickson and Fuller regarding when a module invariant under automorphisms of its injective envelope is invariant under any endomorphism of it. In particular, we find conditions for several classes of noetherian rings which ensure that modules invariant under automorphisms of their injective envelopes are quasi-injective. In the case of a commutative noetherian ring, we show that any automorphism-invariant module is quasi-injective. We also provide multiple examples that show that our conditions are the best possible, in the sense that if we relax them further then there exist automorphism-invariant modules which are not quasi-injective. We finish this paper by dualizing our results to the automorphism-coinvariant case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the invariance of modules under automorphisms of their injective envelopes and the quasi-injectivity of such modules, and how do the conditions for quasi-injectivity vary depending on the type of ring?\n\nA) The invariance of modules under automorphisms of their injective envelopes is equivalent to the quasi-injectivity of such modules, and the conditions for quasi-injectivity are independent of the type of ring.\n\nB) The invariance of modules under automorphisms of their injective envelopes is a necessary but not sufficient condition for quasi-injectivity, and the conditions for quasi-injectivity vary depending on the type of ring, with commutative noetherian rings providing the most restrictive conditions.\n\nC) The invariance of modules under automorphisms of their injective envelopes is a sufficient but not necessary condition for quasi-injectivity, and the conditions for quasi-injectivity are determined by the Boolean ring structure of the underlying ring.\n\nD) The invariance of modules under automorphisms of their injective envelopes is equivalent to the quasi-injectivity of such modules, and the conditions for quasi-injectivity are determined by the type of ring, with Boolean rings providing the most restrictive conditions.\n\nCorrect Answer: B) The invariance of modules under automorphisms of their injective envelopes is a necessary but not sufficient condition for quasi-injectivity, and the conditions for quasi-injectivity vary depending on the type of ring, with commutative noetherian rings providing the most restrictive conditions.\n\nExplanation: The correct answer is B) because the documentation states that the invariance of modules under automorphisms of their injective envelopes is a necessary but not sufficient condition for quasi-injectivity. Additionally, the conditions for quasi-injectivity vary depending on the type of ring, with commutative noetherian rings providing the most restrictive conditions. The other options are incorrect because they either overstate the relationship between invariance and quasi-injectivity or misrepresent the conditions for quasi-injectivity."}, "19": {"documentation": {"title": "Measurement of flow harmonics with multi-particle cumulants in Pb+Pb\n  collisions at $\\sqrt{s_{NN}}=2.76$ TeV with the ATLAS detector", "source": "ATLAS Collaboration", "docs_id": "1408.4342", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of flow harmonics with multi-particle cumulants in Pb+Pb\n  collisions at $\\sqrt{s_{NN}}=2.76$ TeV with the ATLAS detector. ATLAS measurements of the azimuthal anisotropy in lead-lead collisions at $\\sqrt{s_{NN}}=2.76$ TeV are shown using a dataset of approximately 7 $\\mu$b$^{-1}$ collected at the LHC in 2010. The measurements are performed for charged particles with transverse momenta $0.5<p_T<20$ GeV and in the pseudorapidity range $|\\eta|<2.5$. The anisotropy is characterized by the Fourier coefficients, $v_n$, of the charged-particle azimuthal angle distribution for n = 2-4. The Fourier coefficients are evaluated using multi-particle cumulants calculated with the generating function method. Results on the transverse momentum, pseudorapidity and centrality dependence of the $v_n$ coefficients are presented. The elliptic flow, $v_2$, is obtained from the two-, four-, six- and eight-particle cumulants while higher-order coefficients, $v_3$ and $v_4$, are determined with two- and four-particle cumulants. Flow harmonics $v_n$ measured with four-particle cumulants are significantly reduced compared to the measurement involving two-particle cumulants. A comparison to $v_n$ measurements obtained using different analysis methods and previously reported by the LHC experiments is also shown. Results of measurements of flow fluctuations evaluated with multi-particle cumulants are shown as a function of transverse momentum and the collision centrality. Models of the initial spatial geometry and its fluctuations fail to describe the flow fluctuations measurements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary difference in the measurement of flow harmonics $v_n$ between the use of two-particle cumulants and four-particle cumulants, and how does this impact the results?\n\nA) The use of two-particle cumulants yields a more precise measurement of $v_2$, while four-particle cumulants are more sensitive to higher-order harmonics.\n\nB) Four-particle cumulants are more robust against experimental uncertainties, but two-particle cumulants provide a more detailed picture of the flow fluctuations.\n\nC) The use of four-particle cumulants results in a significant reduction of flow harmonics $v_n$ compared to two-particle cumulants, which is attributed to the inclusion of higher-order correlations.\n\nD) Two-particle cumulants are more suitable for measuring the elliptic flow $v_2$, while four-particle cumulants are better suited for studying higher-order harmonics.\n\nCorrect Answer: C) The use of four-particle cumulants results in a significant reduction of flow harmonics $v_n$ compared to two-particle cumulants, which is attributed to the inclusion of higher-order correlations.\n\nExplanation: The correct answer is C) because the documentation states that \"Flow harmonics $v_n$ measured with four-particle cumulants are significantly reduced compared to the measurement involving two-particle cumulants.\" This suggests that the inclusion of higher-order correlations in four-particle cumulants leads to a reduction in the measured flow harmonics."}, "20": {"documentation": {"title": "Testing the performance of technical trading rules in the Chinese market", "source": "Shan Wang (ECUST), Zhi-Qiang Jiang (ECUST), Sai-Ping Li (Academia\n  Sinica), Wei-Xing Zhou (ECUST)", "docs_id": "1504.06397", "section": ["q-fin.TR", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Testing the performance of technical trading rules in the Chinese market. Technical trading rules have a long history of being used by practitioners in financial markets. Their profitable ability and efficiency of technical trading rules are yet controversial. In this paper, we test the performance of more than seven thousands traditional technical trading rules on the Shanghai Securities Composite Index (SSCI) from May 21, 1992 through June 30, 2013 and Shanghai Shenzhen 300 Index (SHSZ 300) from April 8, 2005 through June 30, 2013 to check whether an effective trading strategy could be found by using the performance measurements based on the return and Sharpe ratio. To correct for the influence of the data-snooping effect, we adopt the Superior Predictive Ability test to evaluate if there exists a trading rule that can significantly outperform the benchmark. The result shows that for SSCI, technical trading rules offer significant profitability, while for SHSZ 300, this ability is lost. We further partition the SSCI into two sub-series and find that the efficiency of technical trading in sub-series, which have exactly the same spanning period as that of SHSZ 300, is severely weakened. By testing the trading rules on both indexes with a five-year moving window, we find that the financial bubble from 2005 to 2007 greatly improve the effectiveness of technical trading rules. This is consistent with the predictive ability of technical trading rules which appears when the market is less efficient."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What can be inferred from the study on the performance of technical trading rules in the Chinese market, specifically regarding the Shanghai Securities Composite Index (SSCI) and the Shanghai Shenzhen 300 Index (SHSZ 300)?\n\n**A)** The study suggests that technical trading rules are equally effective on both SSCI and SHSZ 300 indexes.\n**B)** The study found that technical trading rules are more profitable on the SSCI index than on the SHSZ 300 index.\n**C)** The study indicates that the efficiency of technical trading rules is severely weakened on the SSCI index compared to the SHSZ 300 index.\n**D)** The study suggests that the financial bubble from 2005 to 2007 improved the effectiveness of technical trading rules on both indexes.\n\n**Correct Answer:** D) The study suggests that the financial bubble from 2005 to 2007 improved the effectiveness of technical trading rules on both indexes.\n\n**Explanation:** The study found that the financial bubble from 2005 to 2007 greatly improved the effectiveness of technical trading rules on both indexes, which is consistent with the predictive ability of technical trading rules that appears when the market is less efficient. This suggests that the financial bubble created a more favorable environment for technical trading rules to be effective."}, "21": {"documentation": {"title": "Online Memorization of Random Firing Sequences by a Recurrent Neural\n  Network", "source": "Patrick Murer and Hans-Andrea Loeliger", "docs_id": "2001.02920", "section": ["cs.IT", "cs.LG", "cs.NE", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online Memorization of Random Firing Sequences by a Recurrent Neural\n  Network. This paper studies the capability of a recurrent neural network model to memorize random dynamical firing patterns by a simple local learning rule. Two modes of learning/memorization are considered: The first mode is strictly online, with a single pass through the data, while the second mode uses multiple passes through the data. In both modes, the learning is strictly local (quasi-Hebbian): At any given time step, only the weights between the neurons firing (or supposed to be firing) at the previous time step and those firing (or supposed to be firing) at the present time step are modified. The main result of the paper is an upper bound on the probability that the single-pass memorization is not perfect. It follows that the memorization capacity in this mode asymptotically scales like that of the classical Hopfield model (which, in contrast, memorizes static patterns). However, multiple-rounds memorization is shown to achieve a higher capacity (with a nonvanishing number of bits per connection/synapse). These mathematical findings may be helpful for understanding the functions of short-term memory and long-term memory in neuroscience."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the memorization capabilities of the single-pass and multiple-pass learning modes of the recurrent neural network model described in the paper \"Online Memorization of Random Firing Sequences by a Recurrent Neural Network\". How do these modes differ in terms of their memorization capacity, and what are the implications of these findings for our understanding of short-term and long-term memory in neuroscience?\n\n**A)** The single-pass learning mode has a higher memorization capacity than the multiple-pass learning mode, as it allows the network to learn from a single pass through the data. In contrast, the multiple-pass learning mode requires multiple iterations to achieve the same level of memorization.\n\n**B)** The multiple-pass learning mode has a higher memorization capacity than the single-pass learning mode, as it allows the network to learn from multiple iterations of the data. In contrast, the single-pass learning mode is limited to a single pass through the data.\n\n**C)** The memorization capacity of the single-pass learning mode is equivalent to that of the multiple-pass learning mode, as both modes are capable of memorizing random dynamical firing patterns. However, the multiple-pass learning mode achieves a higher capacity due to its ability to learn from multiple iterations of the data.\n\n**D)** The single-pass learning mode has a lower memorization capacity than the multiple-pass learning mode, as it is limited to a single pass through the data. In contrast, the multiple-pass learning mode requires multiple iterations to achieve the same level of memorization.\n\n**Correct Answer:** C) The memorization capacity of the single-pass learning mode is equivalent to that of the multiple-pass learning mode, as both modes are capable of memorizing random dynamical firing patterns. However, the multiple-pass learning mode achieves a higher capacity due to its ability to learn from multiple iterations of the data.\n\n**Explanation:** The paper describes two modes of learning/memorization for the recurrent neural network model: single-pass and multiple-pass. The single-pass mode learns from a single pass through the data, while the multiple-pass mode learns from multiple iterations of the data. The main result of the paper is an upper bound on the probability that the single-pass memorization is not perfect, which shows that the memorization capacity of the single-pass mode is equivalent to that of the classical Hopfield model. However, the multiple-pass mode achieves a higher capacity due to its ability to learn from multiple iterations of the data. This finding has implications for our understanding of short-term and long-term memory in neuroscience, as it suggests that multiple-rounds memorization may be more effective for achieving high-capacity memory."}, "22": {"documentation": {"title": "Supply and demand shocks in the COVID-19 pandemic: An industry and\n  occupation perspective", "source": "R. Maria del Rio-Chanona, Penny Mealy, Anton Pichler, Francois Lafond,\n  Doyne Farmer", "docs_id": "2004.06759", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supply and demand shocks in the COVID-19 pandemic: An industry and\n  occupation perspective. We provide quantitative predictions of first order supply and demand shocks for the U.S. economy associated with the COVID-19 pandemic at the level of individual occupations and industries. To analyze the supply shock, we classify industries as essential or non-essential and construct a Remote Labor Index, which measures the ability of different occupations to work from home. Demand shocks are based on a study of the likely effect of a severe influenza epidemic developed by the US Congressional Budget Office. Compared to the pre-COVID period, these shocks would threaten around 22% of the US economy's GDP, jeopardise 24% of jobs and reduce total wage income by 17%. At the industry level, sectors such as transport are likely to have output constrained by demand shocks, while sectors relating to manufacturing, mining and services are more likely to be constrained by supply shocks. Entertainment, restaurants and tourism face large supply and demand shocks. At the occupation level, we show that high-wage occupations are relatively immune from adverse supply and demand-side shocks, while low-wage occupations are much more vulnerable. We should emphasize that our results are only first-order shocks -- we expect them to be substantially amplified by feedback effects in the production network."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of the COVID-19 pandemic on the US economy, considering both supply and demand shocks, and explain how these shocks affect different industries and occupations.\n\n**A)** The pandemic is likely to have a significant impact on the US economy, with supply shocks affecting industries such as manufacturing, mining, and services, while demand shocks will primarily affect sectors like transport and entertainment. High-wage occupations are relatively immune to these shocks, whereas low-wage occupations are more vulnerable.\n\n**B)** The pandemic will have a substantial impact on the US economy, with demand shocks threatening around 22% of GDP, jeopardizing 24% of jobs, and reducing total wage income by 17%. Supply shocks will primarily affect essential industries, such as healthcare and food production, while non-essential industries, like tourism and restaurants, will face large supply and demand shocks.\n\n**C)** The pandemic will have a limited impact on the US economy, with supply shocks affecting only essential industries, such as healthcare and transportation, while demand shocks will primarily affect low-wage occupations, like retail and hospitality. High-wage occupations, such as finance and technology, are relatively immune to these shocks.\n\n**D)** The pandemic will have a significant impact on the US economy, with supply shocks affecting industries like manufacturing, mining, and services, while demand shocks will primarily affect high-wage occupations, such as finance and technology. Low-wage occupations, like retail and hospitality, are relatively immune to these shocks.\n\n**Correct Answer:** B) The pandemic will have a substantial impact on the US economy, with demand shocks threatening around 22% of GDP, jeopardizing 24% of jobs, and reducing total wage income by 17%. Supply shocks will primarily affect essential industries, such as healthcare and food production, while non-essential industries, like tourism and restaurants, will face large supply and demand shocks.\n\n**Explanation:** The correct answer is based on the documentation, which states that demand shocks would threaten around 22% of the US economy's GDP, jeopardize 24% of jobs, and reduce total wage income by 17%. Additionally, the documentation highlights that essential industries, such as healthcare and food production, are more likely to be affected by supply shocks, while non-essential industries, like tourism and restaurants, face large supply and demand shocks."}, "23": {"documentation": {"title": "Triple-Product Correlations in B -> V1 V2$ Decays and New Physics", "source": "Alakabha Datta and David London", "docs_id": "hep-ph/0303159", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Triple-Product Correlations in B -> V1 V2$ Decays and New Physics. In this paper we examine T-violating triple-product correlations (TP's) in B -> V1 V2 decays. TP's are excellent probes of physics beyond the standard model (SM) for two reasons: (i) within the SM, most TP's are expected to be tiny, and (ii) unlike direct CP asymmetries, TP's are not suppressed by the small strong phases which are expected in B decays. TP's are obtained via the angular analysis of B -> V1 V2. In a general analysis based on factorization, we demonstrate that the most promising decays for measuring TP's in the SM involve excited final-state vector mesons, and we provide estimates of such TP's. We find that there are only a handful of decays in which large TP's are possible, and the size of these TP's depends strongly on the size of nonfactorizable effects. We show that TP's which vanish in the SM can be very large in models with new physics. The measurement of a nonzero TP asymmetry in a decay where none is expected would specifically point to new physics involving large couplings to the right-handed b-quark."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of B -> V1 V2 decays, what is the primary advantage of using triple-product correlations (TP's) as a probe of physics beyond the standard model (SM)?\n\nA) TP's are more sensitive to CP-violating effects than direct CP asymmetries.\nB) TP's are less suppressed by small strong phases in B decays.\nC) TP's are more difficult to measure than direct CP asymmetries.\nD) TP's are more sensitive to new physics involving large couplings to the right-handed b-quark.\n\nCorrect Answer: B) TP's are less suppressed by small strong phases in B decays.\n\nExplanation: The correct answer is B) TP's are less suppressed by small strong phases in B decays. According to the documentation, TP's are not suppressed by small strong phases, which are expected in B decays, unlike direct CP asymmetries. This makes TP's a more promising probe of physics beyond the SM."}, "24": {"documentation": {"title": "A necessary and sufficient condition to play games in quantum mechanical\n  settings", "source": "Sahin Kaya Ozdemir, Junichi Shimamura, and Nobuyuki Imoto", "docs_id": "quant-ph/0703006", "section": ["quant-ph", "math-ph", "math.MP", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A necessary and sufficient condition to play games in quantum mechanical\n  settings. Quantum game theory is a multidisciplinary field which combines quantum mechanics with game theory by introducing non-classical resources such as entanglement, quantum operations and quantum measurement. By transferring two-player-two strategy (2x2) dilemma containing classical games into quantum realm, dilemmas can be resolved in quantum pure strategies if entanglement is distributed between the players who use quantum operations. Moreover, players receive the highest sum of payoffs available in the game, which are otherwise impossible in classical pure strategies. Encouraged by the observation of rich dynamics of physical systems with many interacting parties and the power of entanglement in quantum versions of 2x2 games, it became generally accepted that quantum versions can be easily extended to N-player situations by simply allowing N-partite entangled states. In this article, however, we show that this is not generally true because the reproducibility of classical tasks in quantum domain imposes limitations on the type of entanglement and quantum operators. We propose a benchmark for the evaluation of quantum and classical versions of games, and derive the necessary and sufficient conditions for a physical realization. We give examples of entangled states that can and cannot be used, and the characteristics of quantum operators used as strategies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation on the extension of quantum versions of 2x2 games to N-player situations, as proposed in the article?\n\nA) The complexity of entanglement increases exponentially with the number of players.\nB) The reproducibility of classical tasks in the quantum domain imposes limitations on the type of entanglement and quantum operators.\nC) The power of entanglement in quantum versions of 2x2 games is limited by the number of interacting parties.\nD) The use of quantum operations and measurement is restricted to specific types of entangled states.\n\n**Correct Answer:** B) The reproducibility of classical tasks in the quantum domain imposes limitations on the type of entanglement and quantum operators.\n\n**Explanation:** The article states that the reproducibility of classical tasks in the quantum domain imposes limitations on the type of entanglement and quantum operators, which is the correct answer. The other options are incorrect because they do not accurately reflect the limitation proposed in the article. Option A is incorrect because the complexity of entanglement does not necessarily increase exponentially with the number of players. Option C is incorrect because the power of entanglement is not limited by the number of interacting parties. Option D is incorrect because the use of quantum operations and measurement is not restricted to specific types of entangled states."}, "25": {"documentation": {"title": "A Search for variable stars in the four open star clusters", "source": "Alok Durgapal, Geeta Rangwal, D. Bisht, Harmeen Kaur, R. K. S. Yadav\n  and J. C. Pandey", "docs_id": "2003.02442", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Search for variable stars in the four open star clusters. We present a CCD photometric survey for the search of variable stars in four open clusters namely Berkeley 69, King 5, King 7, and Berkeley 20. The time series observations were carried out for 1 and/or 2 nights for each of the clusters in the year 1998, which have led to identify nineteen variable stars in these clusters. Out of these 19 variable stars, five stars show delta Scuti like variability and two stars show W UMa type variability. In other stars, we could not find the periods and hence the type of variability due to the lack of sufficient data. The periods of delta Scuti type stars are found to be in the range of 0.13 to 0.21 days, whereas the two stars in the cluster Berkeley 20, which showed W UMa type variability have orbital periods of 0.396 and 0.418 days, respectively. Using the Gaia data, the basic parameters of the clusters Berkeley 69, King 7 and King 5 are also revised. The age and reddening are estimated to be 0.79 \\pm 0.09 Gyr and 0.68 \\pm 0.03 mag for Berkeley 69, 0.79 \\pm 0.09 Gyr and 1.22 \\pm 0.03mag for the cluster King 7 and 1.59 \\pm 0.19 Gyr and 0.63 \\pm 0.02 mag for the cluster King 5, respectively. Signature of mass segregation is found in the clusters King 7 and King 5."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the results of the CCD photometric survey for variable stars in the four open star clusters. What can be inferred about the variability patterns of the delta Scuti and W UMa type stars, and how do the periods of these stars compare to the estimated ages of the clusters?\n\n**A)** The delta Scuti type stars show a wide range of periods, indicating a complex variability pattern, while the W UMa type stars have relatively short periods, suggesting a more stable orbital pattern. The periods of these stars are consistent with the estimated ages of the clusters.\n\n**B)** The delta Scuti type stars have periods that are significantly shorter than the estimated ages of the clusters, indicating a possible mass segregation effect. In contrast, the W UMa type stars have periods that are consistent with the estimated ages of the clusters, suggesting a more stable orbital pattern.\n\n**C)** The periods of the delta Scuti type stars are consistent with the estimated ages of the clusters, indicating a possible mass segregation effect. The W UMa type stars have periods that are significantly longer than the estimated ages of the clusters, suggesting a possible age discrepancy.\n\n**D)** The periods of the delta Scuti type stars are significantly shorter than the estimated ages of the clusters, indicating a possible mass segregation effect. The W UMa type stars have periods that are consistent with the estimated ages of the clusters, suggesting a more stable orbital pattern.\n\n**Correct Answer:** B) The delta Scuti type stars have periods that are significantly shorter than the estimated ages of the clusters, indicating a possible mass segregation effect. In contrast, the W UMa type stars have periods that are consistent with the estimated ages of the clusters, suggesting a more stable orbital pattern.\n\n**Explanation:** The periods of the delta Scuti type stars (0.13-0.21 days) are significantly shorter than the estimated ages of the clusters (0.79-1.59 Gyr), indicating a possible mass segregation effect. In contrast, the periods of the W UMa type stars (0.396 and 0.418 days) are consistent with the estimated ages of the clusters, suggesting a more stable orbital pattern. This suggests that the delta Scuti type stars are more likely to be young and massive, while the W UMa type stars are more likely to be older and less massive."}, "26": {"documentation": {"title": "Chaos exponents of SYK traversable wormholes", "source": "Tomoki Nosaka and Tokiro Numasawa", "docs_id": "2009.10759", "section": ["hep-th", "cond-mat.stat-mech", "cond-mat.str-el", "nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chaos exponents of SYK traversable wormholes. In this paper we study the chaos exponent, the exponential growth rate of the out-of-time-ordered four point functions, in a two coupled SYK models which exhibits a first order phase transition between the high temperature black hole phase and the low temperature gapped phase interpreted as a traversable wormhole. We see that as the temperature decreases the chaos exponent exhibits a discontinuous fall-off from the value of order the universal bound $2\\pi/\\beta$ at the critical temperature of the phase transition, which is consistent with the expected relation between black holes and strong chaos. Interestingly, the chaos exponent is small but non-zero even in the wormhole phase. This is surprising but consistent with the observation on the decay rate of the two point function [arXiv:2003.03916], and we found the chaos exponent and the decay rate indeed obey the same temperature dependence in this regime. We also studied the chaos exponent of a closely related model with single SYK term, and found that the chaos exponent of this model is always greater than that of the two coupled model in the entire parameter space."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the SYK traversable wormhole model, what is the expected relation between the chaos exponent and the universal bound $2\\pi/\\beta$ at the critical temperature of the phase transition?\n\nA) The chaos exponent is always greater than the universal bound.\nB) The chaos exponent is always less than the universal bound.\nC) The chaos exponent exhibits a discontinuous fall-off from the universal bound at the critical temperature, consistent with the expected relation between black holes and strong chaos.\nD) The chaos exponent is equal to the universal bound at all temperatures.\n\nCorrect Answer: C) The chaos exponent exhibits a discontinuous fall-off from the universal bound at the critical temperature, consistent with the expected relation between black holes and strong chaos.\n\nExplanation: The correct answer is based on the statement in the documentation that \"as the temperature decreases the chaos exponent exhibits a discontinuous fall-off from the value of order the universal bound $2\\pi/\\beta$ at the critical temperature of the phase transition\". This indicates that the chaos exponent is related to the universal bound at the critical temperature, and that there is a discontinuous change in the chaos exponent as the temperature decreases."}, "27": {"documentation": {"title": "The influence of the elementary charge on the canonical quantization of\n  LC -circuits", "source": "E. Papp, D. Racolta, L. Aur, Z. Szakacs", "docs_id": "1607.07356", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The influence of the elementary charge on the canonical quantization of\n  LC -circuits. In this paper one deals with the quantization of mesoscopic LC-circuits under the influence of an external time dependent voltage. The canonically conjugated variables, such as given by the electric charge and the magnetic flux, get established by resorting to the hamiltonian equations of motion provided by both Faraday and Kirchhoff laws . This time the discretization of the electric charge is accounted for, so that magnetic flux operators one looks for should proceed in terms of discrete derivatives. However, the flux operators one deals witg are not Hermitian, which means that subsequent symmetrizations are in order. The eigenvalues characterizing such operators cab be readily established in terms of twisted boundary conditions. Besides the discrete Schrodinger equation with nearest-neighbor hoppings, a nontrivial next nearest neighbor generalization has also been established. Such issues open the way to the derivation of persistent currents in terms of effective k-dependent Hamiltonians. Handling the time dependent voltage within the nearest neighbor description leadsto the derivation of dynamic localization effects in L-ring configurations, such as discussed before by Dunlap and Kenkre The onset of the magnetic flux quantum has also been discussed in some more detail."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the canonical quantization of LC-circuits under the influence of an external time-dependent voltage, what is the primary challenge in deriving effective Hamiltonians for the magnetic flux operators, and how do twisted boundary conditions address this issue?\n\n**A)** The primary challenge lies in the non-Hermiticity of the magnetic flux operators, which can be resolved by applying a unitary transformation to the Hamiltonian. Twisted boundary conditions are not directly relevant to this issue.\n\n**B)** The primary challenge is the discretization of the electric charge, which necessitates the use of discrete derivatives to describe the magnetic flux operators. Twisted boundary conditions can be used to establish the eigenvalues of these operators.\n\n**C)** The primary challenge is the need to handle time-dependent voltage within the nearest-neighbor description, which leads to dynamic localization effects in L-ring configurations. Twisted boundary conditions are not a relevant solution to this issue.\n\n**D)** The primary challenge is the non-Hermiticity of the magnetic flux operators, which can be addressed by symmetrizing the Hamiltonian using twisted boundary conditions. This approach allows for the derivation of effective k-dependent Hamiltonians.\n\n**Correct Answer:** D) The primary challenge is the non-Hermiticity of the magnetic flux operators, which can be addressed by symmetrizing the Hamiltonian using twisted boundary conditions. This approach allows for the derivation of effective k-dependent Hamiltonians.\n\n**Explanation:** The correct answer is D) because the non-Hermiticity of the magnetic flux operators is a key challenge in deriving effective Hamiltonians for LC-circuits. Twisted boundary conditions provide a way to symmetrize the Hamiltonian, which is essential for establishing the eigenvalues of the magnetic flux operators. This approach enables the derivation of effective k-dependent Hamiltonians, which are crucial for understanding persistent currents and other phenomena in mesoscopic LC-circuits."}, "28": {"documentation": {"title": "{\\lambda}-statistical convergent function sequences in intuitionistic\n  fuzzy normed spaces", "source": "Vatan Karakaya, Necip \\c{S}im\\c{s}ek, M\\\"uzeyyen Ert\\\"urk, Faik\n  G\\\"ursoy", "docs_id": "1112.3328", "section": ["math.FA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "{\\lambda}-statistical convergent function sequences in intuitionistic\n  fuzzy normed spaces. Fuzzy logic was introduced by Zadeh in 1965. Since then, the importance of fuzzy logic has come increasingly to the present.There are many applications of fuzzy logic in the field of science and engineering, e.g. population dynamics (Barros), chaos control (Feng,Fradkov), computer programming (Giles), nonlinear dynamical systems (Hong), etc. The concept of intuitionistic fuzzy set, as a generalization of fuzzy logic, was introduced by Atanassov in 1986. Quite recently Park has introduced the concept of intuitionistic fuzzy metric space, and Saadati and Park studied the notion of intuitionistic fuzzy normed space. Intuitionistic fuzzy analogues of many concept in classical analysis was studied by many authors (Mursaleen, Rsaadati, Jebril, Dinda, etc.). The concept of statistical convergence was introduced by Fast. Mursaleen defined {\\lambda}-statistical convergence in Muhammed. Also the concept of statistical convergence was studied in intuitionistic fuzzy normed space in Karakus..Quite recently, Karakaya et al. defined and studied statistical convergence of function sequences in intuitionistic fuzzy normed spaces. Mohiuddine and Lohani defined and studied {\\lambda}-statistical convergence in intuitionistic fuzzy normed spaces (Lohani). In this paper, we shall study concept {\\lambda}-statistical convergence for function sequences and investigate some basic properties related to the concept in intuitionistic fuzzy normed space."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of intuitionistic fuzzy normed spaces, what is the definition of {\\lambda}-statistical convergence for function sequences, and how does it relate to the concept of statistical convergence introduced by Fast and Mursaleen?\n\nA) {\\lambda}-statistical convergence is defined as a function sequence {f_n} converging to f if for every \u03b5 > 0, there exists N \u2208 \u2115 such that for all n \u2265 N, |f_n(x) - f(x)| < \u03b5 for all x in the domain, where f_n(x) and f(x) are fuzzy numbers.\n\nB) {\\lambda}-statistical convergence is defined as a function sequence {f_n} converging to f if for every \u03b5 > 0, there exists N \u2208 \u2115 such that for all n \u2265 N, |f_n(x) - f(x)| < \u03b5 for all x in the domain, where f_n(x) and f(x) are fuzzy numbers, and the convergence is measured by the {\\lambda}-statistical distance.\n\nC) {\\lambda}-statistical convergence is defined as a function sequence {f_n} converging to f if for every \u03b5 > 0, there exists N \u2208 \u2115 such that for all n \u2265 N, |f_n(x) - f(x)| < \u03b5 for all x in the domain, where f_n(x) and f(x) are fuzzy numbers, and the convergence is measured by the {\\lambda}-statistical distance, which is defined as the supremum of the set of all fuzzy numbers that are less than or equal to the difference between f_n(x) and f(x).\n\nD) {\\lambda}-statistical convergence is defined as a function sequence {f_n} converging to f if for every \u03b5 > 0, there exists N \u2208 \u2115 such that for all n \u2265 N, |f_n(x) - f(x)| < \u03b5 for all x in the domain, where f_n(x) and f(x) are fuzzy numbers, and the convergence is measured by the {\\lambda}-statistical distance, which is defined as the infimum of the set of all fuzzy numbers that are greater than or equal to the difference between f_n(x) and f(x).\n\nCorrect Answer: C) {\\lambda}-statistical convergence is defined as a function sequence {f_n} converging to f if for every \u03b5 > 0, there exists N \u2208 \u2115 such that for all n \u2265 N, |f_n(x) - f(x)| < \u03b5 for all x in the domain, where f_n(x) and f(x) are fuzzy numbers, and the convergence is measured by the {\\lambda}-statistical distance, which is defined as the supremum of the set of all fuzzy numbers that are less than or equal to the difference between f_n(x) and f(x).\n\nExplanation: The correct answer is C) because it accurately defines {\\lambda}-statistical convergence for function sequences in intuitionistic fuzzy normed spaces. The definition involves the supremum of the set of all fuzzy numbers that are less than or equal to the difference between f_n(x) and f(x), which is a key aspect of the concept. The other options are incorrect because they either omit or misstate the definition of {\\lambda}-statistical convergence."}, "29": {"documentation": {"title": "Dipole model at Next-to-Leading Order meets HERA data", "source": "G. Beuf, H. H\\\"anninen, T. Lappi, H. M\\\"antysaari", "docs_id": "2008.05233", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dipole model at Next-to-Leading Order meets HERA data. Deep inelastic scattering (DIS) total cross section data at small-x as measured by the HERA experiments is well described by Balitsky-Kovchegov (BK) evolution in the leading order dipole picture. Recently the full Next-to-Leading Order (NLO) dipole picture total cross sections have become available for DIS, and a working factorization scheme has been devised which subtracts the soft gluon divergence present at NLO. We report our recently published work in which we make the first comparisons of the NLO DIS total cross sections to HERA data. The non-perturbative initial condition to BK evolution is fixed by fitting the HERA reduced cross section data. As the NLO results for the DIS total cross section are currently available only in the massless quark limit, we also fit a light-quark-only cross section constructed with a parametrization of published total and heavy quark data. We find an excellent description of the HERA data. Since the full NLO BK equation is computationally expensive, we use a number of beyond LO prescriptions for the evolution that include most important higher order corrections enhanced by large transverse logarithms, including the recent version of the equation formulated in terms of the target momentum fraction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the dipole model at Next-to-Leading Order, what is the primary advantage of using a working factorization scheme to subtract the soft gluon divergence present at NLO, as described in the recent work on Deep Inelastic Scattering (DIS) total cross section data at small-x as measured by the HERA experiments?\n\nA) It allows for a more accurate description of the non-perturbative initial condition to BK evolution.\nB) It enables the use of a light-quark-only cross section constructed with a parametrization of published total and heavy quark data.\nC) It facilitates the inclusion of higher order corrections enhanced by large transverse logarithms in the full NLO BK equation.\nD) It provides a more computationally efficient method for evaluating the NLO DIS total cross section.\n\nCorrect Answer: C) It facilitates the inclusion of higher order corrections enhanced by large transverse logarithms in the full NLO BK equation.\n\nExplanation: The correct answer is C) because the working factorization scheme is designed to subtract the soft gluon divergence present at NLO, which allows for the inclusion of higher order corrections enhanced by large transverse logarithms in the full NLO BK equation. This is a key advantage of using this scheme, as it enables a more accurate description of the DIS total cross section data at small-x. The other options are not correct because they describe other aspects of the work, such as the use of a light-quark-only cross section (B) or the fitting of the non-perturbative initial condition to BK evolution (A), but do not address the primary advantage of the working factorization scheme."}, "30": {"documentation": {"title": "Optimization Methods for Large-Scale Machine Learning", "source": "L\\'eon Bottou, Frank E. Curtis, Jorge Nocedal", "docs_id": "1606.04838", "section": ["stat.ML", "cs.LG", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimization Methods for Large-Scale Machine Learning. This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of conventional gradient-based nonlinear optimization techniques in the context of large-scale machine learning, as discussed in the paper \"Optimization Methods for Large-Scale Machine Learning\"?\n\nA) They are computationally expensive and require large amounts of memory.\nB) They are sensitive to the choice of initial parameters and may converge to local optima.\nC) They are unable to handle the stochastic nature of large-scale machine learning problems.\nD) They are limited by the availability of second-order derivative information.\n\n**Correct Answer:** C) They are unable to handle the stochastic nature of large-scale machine learning problems.\n\n**Explanation:** The paper highlights that conventional gradient-based nonlinear optimization techniques typically falter in large-scale machine learning applications, which are characterized by stochastic gradient (SG) methods. This is because these methods are designed to handle deterministic optimization problems, whereas large-scale machine learning problems involve stochastic noise and uncertainty. Therefore, option C is the correct answer.\n\n**Candidate A:** This option is incorrect because while computational expense and memory requirements can be issues in large-scale machine learning, they are not the primary limitation of conventional gradient-based nonlinear optimization techniques.\n\n**Candidate B:** This option is partially correct, as conventional gradient-based nonlinear optimization techniques can be sensitive to the choice of initial parameters and may converge to local optima. However, this is not the primary limitation in the context of large-scale machine learning.\n\n**Candidate D:** This option is incorrect because second-order derivative information is not the primary limitation of conventional gradient-based nonlinear optimization techniques in large-scale machine learning. While second-order derivative approximations are an important area of research, they are not the primary challenge faced by these methods in this context."}, "31": {"documentation": {"title": "Social interaction, noise and antibiotic-mediated switches in the\n  intestinal microbiota", "source": "Vanni Bucci, Serena Bradde, Giulio Biroli and Joao B. Xavier", "docs_id": "1203.2883", "section": ["q-bio.QM", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Social interaction, noise and antibiotic-mediated switches in the\n  intestinal microbiota. The intestinal microbiota plays important roles in digestion and resistance against entero-pathogens. As with other ecosystems, its species composition is resilient against small disturbances but strong perturbations such as antibiotics can affect the consortium dramatically. Antibiotic cessation does not necessarily restore pre-treatment conditions and disturbed microbiota are often susceptible to pathogen invasion. Here we propose a mathematical model to explain how antibiotic-mediated switches in the microbiota composition can result from simple social interactions between antibiotic-tolerant and antibiotic-sensitive bacterial groups. We build a two-species (e.g. two functional-groups) model and identify regions of domination by antibiotic-sensitive or antibiotic-tolerant bacteria, as well as a region of multistability where domination by either group is possible. Using a new framework that we derived from statistical physics, we calculate the duration of each microbiota composition state. This is shown to depend on the balance between random fluctuations in the bacterial densities and the strength of microbial interactions. The singular value decomposition of recent metagenomic data confirms our assumption of grouping microbes as antibiotic-tolerant or antibiotic-sensitive in response to a single antibiotic. Our methodology can be extended to multiple bacterial groups and thus it provides an ecological formalism to help interpret the present surge in microbiome data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A mathematical model is proposed to explain how antibiotic-mediated switches in the intestinal microbiota composition can result from simple social interactions between antibiotic-tolerant and antibiotic-sensitive bacterial groups. The model is based on a two-species system, where the balance between random fluctuations in bacterial densities and the strength of microbial interactions determines the duration of each microbiota composition state. According to the model, what is the key factor that determines the duration of each microbiota composition state?\n\nA) The strength of microbial interactions\nB) The balance between random fluctuations in bacterial densities and the strength of microbial interactions\nC) The type of antibiotic used\nD) The initial population size of each bacterial group\n\n**Correct Answer:** B) The balance between random fluctuations in bacterial densities and the strength of microbial interactions\n\n**Explanation:** The correct answer is B) The balance between random fluctuations in bacterial densities and the strength of microbial interactions. According to the model, the duration of each microbiota composition state depends on the balance between these two factors. The model suggests that when the balance is in favor of the strength of microbial interactions, the microbiota composition state is more stable, and when the balance is in favor of random fluctuations, the microbiota composition state is more unstable. This requires an understanding of the mathematical framework used to derive the model and the underlying assumptions about the interactions between the bacterial groups."}, "32": {"documentation": {"title": "The Massive Supermembrane on a Knot", "source": "M.P. Garcia del Moral, P. Leon, A. Restuccia", "docs_id": "2101.04018", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Massive Supermembrane on a Knot. We obtain the Hamiltonian formulation of the 11D Supermembrane theory non-trivially compactified on a twice-punctured torus times a 9D Minkowski space-time. It corresponds to a M2-brane formulated in 11D space with ten non-compact dimensions. The critical points like the poles and the zeros of the fields describing the embedding of the Supermembrane in the target space are treated rigorously. The non-trivial compactification generates non-trivial mass terms appearing in the bosonic potential, which dominate the full supersymmetric potential and should render the spectrum of the (regularized) Supermembrane discrete with finite multiplicity. The behaviour of the fields around the punctures generates a cosmological term in the Hamiltonian of the theory. The massive supermembrane can also be seen as a nontrivial uplift of a supermembrane torus bundle with parabolic monodromy in $M_9\\times T^2$. The moduli of the theory is the one associated with the punctured torus, hence it keeps all the nontriviality of the torus moduli even after the decompactification process to ten noncompact dimensions. The formulation of the theory on a punctured torus bundle is characterized by the $(1,1)-Knots$ associated with the monodromies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the 11D Supermembrane theory non-trivially compactified on a twice-punctured torus times a 9D Minkowski space-time. What is the nature of the moduli space of this theory, and how does it relate to the $(1,1)-Knots$ associated with the monodromies?\n\nA) The moduli space is a smooth manifold, and the $(1,1)-Knots$ are topological defects that arise from the compactification process.\n\nB) The moduli space is a discrete set of points, and the $(1,1)-Knots$ are non-trivial deformations of the torus that generate non-trivial mass terms in the bosonic potential.\n\nC) The moduli space is a complex manifold, and the $(1,1)-Knots$ are complex-valued functions that describe the embedding of the Supermembrane in the target space.\n\nD) The moduli space is a Lie group, and the $(1,1)-Knots$ are group-valued objects that describe the non-trivial compactification of the Supermembrane.\n\nCorrect Answer: B) The moduli space is a discrete set of points, and the $(1,1)-Knots$ are non-trivial deformations of the torus that generate non-trivial mass terms in the bosonic potential.\n\nExplanation: The correct answer is B) because the documentation states that the non-trivial compactification generates non-trivial mass terms appearing in the bosonic potential, which dominate the full supersymmetric potential and should render the spectrum of the (regularized) Supermembrane discrete with finite multiplicity. This implies that the moduli space is discrete, and the $(1,1)-Knots$ are non-trivial deformations of the torus that generate these mass terms."}, "33": {"documentation": {"title": "The Forest Behind the Tree: Heterogeneity in How US Governor's Party\n  Affects Black Workers", "source": "Guy Tchuente, Johnson Kakeu, John Nana Francois", "docs_id": "2110.00582", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Forest Behind the Tree: Heterogeneity in How US Governor's Party\n  Affects Black Workers. Income inequality is a distributional phenomenon. This paper examines the impact of U.S governor's party allegiance (Republican vs Democrat) on ethnic wage gap. A descriptive analysis of the distribution of yearly earnings of Whites and Blacks reveals a divergence in their respective shapes over time suggesting that aggregate analysis may mask important heterogeneous effects. This motivates a granular estimation of the comparative causal effect of governors' party affiliation on labor market outcomes. We use a regression discontinuity design (RDD) based on marginal electoral victories and samples of quantiles groups by wage and hours worked. Overall, the distributional causal estimations show that the vast majority of subgroups of black workers earnings are not affected by democrat governors' policies, suggesting the possible existence of structural factors in the labor markets that contribute to create and keep a wage trap and/or hour worked trap for most of the subgroups of black workers. Democrat governors increase the number of hours worked of black workers at the highest quartiles of earnings. A bivariate quantiles groups analysis shows that democrats decrease the total hours worked for black workers who have the largest number of hours worked and earn the least. Black workers earning more and working fewer hours than half of the sample see their number of hours worked increase under a democrat governor."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the paper \"The Forest Behind the Tree: Heterogeneity in How US Governor's Party Affects Black Workers\" find that Democrat governors increase the number of hours worked of black workers at the highest quartiles of earnings. However, they also observe that Democrat governors decrease the total hours worked for black workers who have the largest number of hours worked and earn the least. Which of the following statements best summarizes the authors' findings?\n\nA) Democrat governors have a positive effect on the earnings of black workers, but a negative effect on their hours worked.\nB) Democrat governors have a negative effect on the earnings of black workers, but a positive effect on their hours worked.\nC) Democrat governors have a positive effect on the earnings of black workers, but no effect on their hours worked.\nD) Democrat governors have a negative effect on the earnings of black workers, but a negative effect on their hours worked.\n\n**Correct Answer:** B) Democrat governors have a negative effect on the earnings of black workers, but a positive effect on their hours worked.\n\n**Explanation:** The authors find that Democrat governors increase the number of hours worked of black workers at the highest quartiles of earnings, which suggests a positive effect on hours worked. However, they also observe that Democrat governors decrease the total hours worked for black workers who have the largest number of hours worked and earn the least, which suggests a negative effect on earnings. Therefore, the correct answer is B) Democrat governors have a negative effect on the earnings of black workers, but a positive effect on their hours worked."}, "34": {"documentation": {"title": "Jets, Stickiness and Anomalous Transport", "source": "Xavier Leoncini and George M. Zaslavsky", "docs_id": "nlin/0209029", "section": ["nlin.CD", "cond-mat", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jets, Stickiness and Anomalous Transport. Dynamical and statistical properties of the vortex and passive particle advection in chaotic flows generated by four and sixteen point vortices are investigated. General transport properties of these flows are found anomalous and exhibit a superdiffusive behavior with typical second moment exponent (\\mu \\sim 1.75). The origin of this anomaly is traced back to the presence of coherent structures within the flow, the vortex cores and the region far from where vortices are located. In the vicinity of these regions stickiness is observed and the motion of tracers is quasi-ballistic. The chaotic nature of the underlying flow dictates the choice for thorough analysis of transport properties. Passive tracer motion is analyzed by measuring the mutual relative evolution of two nearby tracers. Some tracers travel in each other vicinity for relatively large times. This is related to an hidden order for the tracers which we call jets. Jets are localized and found in sticky regions. Their structure is analyzed and found to be formed of a nested sets of jets within jets. The analysis of the jet trapping time statistics shows a quantitative agreement with the observed transport exponent."}, "answer": "Here is the difficult multi-form question:\n\n**Question:** In the context of the Arxiv paper \"Jets, Stickiness and Anomalous Transport\", what is the origin of the superdiffusive behavior observed in the transport properties of chaotic flows generated by four and sixteen point vortices?\n\nA) The presence of coherent structures within the flow, specifically the vortex cores and the region far from where vortices are located, leads to stickiness and quasi-ballistic motion of tracers.\n\nB) The chaotic nature of the underlying flow is the sole cause of the superdiffusive behavior, with no specific structural features contributing to the anomaly.\n\nC) The mutual relative evolution of two nearby tracers is the key to understanding the origin of the superdiffusive behavior, as it reveals hidden order in the form of jets.\n\nD) The nested sets of jets within jets are the primary cause of the superdiffusive behavior, with the structure of these jets being the underlying mechanism.\n\n**Correct Answer:** A) The presence of coherent structures within the flow, specifically the vortex cores and the region far from where vortices are located, leads to stickiness and quasi-ballistic motion of tracers.\n\n**Explanation:** The correct answer is A) because the paper explicitly states that the origin of the superdiffusive behavior is traced back to the presence of coherent structures within the flow, specifically the vortex cores and the region far from where vortices are located. These structures lead to stickiness, which in turn causes the motion of tracers to be quasi-ballistic. The other options are incorrect because they either oversimplify the origin of the superdiffusive behavior (B) or focus on a specific aspect of the phenomenon (C and D) rather than the underlying cause."}, "35": {"documentation": {"title": "Improved Calculation of Thermal Fission Energy", "source": "X. B. Ma, W. L. Zhong, L. Z. Wang, Y. X. Chen, J. Cao", "docs_id": "1212.6625", "section": ["nucl-ex", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved Calculation of Thermal Fission Energy. Thermal fission energy is one of the basic parameters needed in the calculation of antineutrino flux for reactor neutrino experiments. It is useful to improve the precision of the thermal fission energy calculation for current and future reactor neutrino experiments, which are aimed at more precise determination of neutrino oscillation parameters. In this article, we give new values for thermal fission energies of some common thermal reactor fuel isotopes, with improvements on three aspects. One is more recent input data acquired from updated nuclear databases. the second one is a consideration of the production yields of fission fragments from both thermal and fast incident neutrons for each of the four main fuel isotopes. The last one is more carefully calculation of the average energy taken away by antineutrinos in thermal fission with the comparison of antineutrino spectrum from different models. The change in calculated antineutrino flux due to the new values of thermal fission energy is about 0.32%, and the uncertainties of the new values are about 50% smaller."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the three main improvements made in the calculation of thermal fission energy in the article \"Improved Calculation of Thermal Fission Energy\"?\n\nA) The use of more recent input data from updated nuclear databases, consideration of the production yields of fission fragments from both thermal and fast incident neutrons, and a more careful calculation of the average energy taken away by antineutrinos.\n\nB) The use of more recent input data from updated nuclear databases, consideration of the production yields of fission fragments from both thermal and fast incident neutrons, and a more careful calculation of the average energy taken away by antineutrinos with a comparison of antineutrino spectra from different models.\n\nC) The use of more recent input data from updated nuclear databases, consideration of the production yields of fission fragments from thermal incident neutrons only, and a more careful calculation of the average energy taken away by antineutrinos.\n\nD) The use of more recent input data from updated nuclear databases, consideration of the production yields of fission fragments from fast incident neutrons only, and a more careful calculation of the average energy taken away by antineutrinos.\n\n**Correct Answer:** B) The use of more recent input data from updated nuclear databases, consideration of the production yields of fission fragments from both thermal and fast incident neutrons, and a more careful calculation of the average energy taken away by antineutrinos with a comparison of antineutrino spectra from different models.\n\n**Explanation:** The correct answer is B) because the article states that the three main improvements made in the calculation of thermal fission energy are: (1) the use of more recent input data from updated nuclear databases, (2) consideration of the production yields of fission fragments from both thermal and fast incident neutrons, and (3) a more careful calculation of the average energy taken away by antineutrinos with a comparison of antineutrino spectra from different models."}, "36": {"documentation": {"title": "Tree! I am no Tree! I am a Low Dimensional Hyperbolic Embedding", "source": "Rishi Sonthalia, Anna C. Gilbert", "docs_id": "2005.03847", "section": ["cs.LG", "math.MG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tree! I am no Tree! I am a Low Dimensional Hyperbolic Embedding. Given data, finding a faithful low-dimensional hyperbolic embedding of the data is a key method by which we can extract hierarchical information or learn representative geometric features of the data. In this paper, we explore a new method for learning hyperbolic representations by taking a metric-first approach. Rather than determining the low-dimensional hyperbolic embedding directly, we learn a tree structure on the data. This tree structure can then be used directly to extract hierarchical information, embedded into a hyperbolic manifold using Sarkar's construction \\cite{sarkar}, or used as a tree approximation of the original metric. To this end, we present a novel fast algorithm \\textsc{TreeRep} such that, given a $\\delta$-hyperbolic metric (for any $\\delta \\geq 0$), the algorithm learns a tree structure that approximates the original metric. In the case when $\\delta = 0$, we show analytically that \\textsc{TreeRep} exactly recovers the original tree structure. We show empirically that \\textsc{TreeRep} is not only many orders of magnitude faster than previously known algorithms, but also produces metrics with lower average distortion and higher mean average precision than most previous algorithms for learning hyperbolic embeddings, extracting hierarchical information, and approximating metrics via tree metrics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach taken in the proposed algorithm, TreeRep, for learning hyperbolic representations of data, and how does it differ from previous methods?\n\n**A)** TreeRep learns a hyperbolic embedding directly from the data, using a traditional machine learning approach.\n**B)** TreeRep takes a metric-first approach, learning a tree structure on the data that can be used to extract hierarchical information, embedded into a hyperbolic manifold, or approximated as a tree metric.\n**C)** TreeRep uses a neural network to learn a tree-like structure from the data, but does not necessarily learn a hyperbolic embedding.\n**D)** TreeRep is a traditional algorithm that relies on a fixed hyperbolic metric, rather than learning a metric from the data.\n\n**Correct Answer:** B) TreeRep takes a metric-first approach, learning a tree structure on the data that can be used to extract hierarchical information, embedded into a hyperbolic manifold, or approximated as a tree metric.\n\n**Explanation:** The correct answer is B) because the paper states that TreeRep takes a metric-first approach, learning a tree structure on the data that can be used in various ways. This approach differs from traditional machine learning methods, which typically learn a hyperbolic embedding directly from the data. The other options are incorrect because they do not accurately reflect the approach taken by TreeRep. Option A is incorrect because TreeRep does not learn a hyperbolic embedding directly. Option C is incorrect because TreeRep does not necessarily learn a tree-like structure from the data. Option D is incorrect because TreeRep is a flexible algorithm that can learn a metric from the data, rather than relying on a fixed hyperbolic metric."}, "37": {"documentation": {"title": "RIF Regression via Sensitivity Curves", "source": "Javier Alejo, Gabriel Montes-Rojas, Walter Sosa-Escudero", "docs_id": "2112.01435", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RIF Regression via Sensitivity Curves. This paper proposes an empirical method to implement the recentered influence function (RIF) regression of Firpo, Fortin and Lemieux (2009), a relevant method to study the effect of covariates on many statistics beyond the mean. In empirically relevant situations where the influence function is not available or difficult to compute, we suggest to use the \\emph{sensitivity curve} (Tukey, 1977) as a feasible alternative. This may be computationally cumbersome when the sample size is large. The relevance of the proposed strategy derives from the fact that, under general conditions, the sensitivity curve converges in probability to the influence function. In order to save computational time we propose to use a cubic splines non-parametric method for a random subsample and then to interpolate to the rest of the cases where it was not computed. Monte Carlo simulations show good finite sample properties. We illustrate the proposed estimator with an application to the polarization index of Duclos, Esteban and Ray (2004)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"RIF Regression via Sensitivity Curves\" propose an empirical method to implement the recentered influence function (RIF) regression of Firpo, Fortin and Lemieux (2009) as an alternative to the influence function when it is not available or difficult to compute. What is the main advantage of using the sensitivity curve as a feasible alternative to the influence function, and how does it relate to the influence function in terms of convergence?\n\n**A)** The sensitivity curve is a more robust estimator that is less sensitive to outliers, and it converges to the influence function in probability as the sample size increases.\n\n**B)** The sensitivity curve is a non-parametric method that can be computationally cumbersome, but it converges to the influence function in probability under general conditions.\n\n**C)** The sensitivity curve is a parametric method that is more efficient than the influence function, and it converges to the influence function in probability as the sample size increases.\n\n**D)** The sensitivity curve is a deterministic method that always produces the same result, and it converges to the influence function in probability under specific conditions.\n\n**Correct Answer:** B) The sensitivity curve is a non-parametric method that can be computationally cumbersome, but it converges to the influence function in probability under general conditions.\n\n**Explanation:** The correct answer is B) because the paper states that the sensitivity curve converges in probability to the influence function under general conditions, and that it is a feasible alternative to the influence function when it is not available or difficult to compute. The other options are incorrect because they either misstate the relationship between the sensitivity curve and the influence function, or they introduce additional assumptions that are not supported by the text."}, "38": {"documentation": {"title": "Breast lesion segmentation in ultrasound images with limited annotated\n  data", "source": "Bahareh Behboodi, Mina Amiri, Rupert Brooks, Hassan Rivaz", "docs_id": "2001.07322", "section": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Breast lesion segmentation in ultrasound images with limited annotated\n  data. Ultrasound (US) is one of the most commonly used imaging modalities in both diagnosis and surgical interventions due to its low-cost, safety, and non-invasive characteristic. US image segmentation is currently a unique challenge because of the presence of speckle noise. As manual segmentation requires considerable efforts and time, the development of automatic segmentation algorithms has attracted researchers attention. Although recent methodologies based on convolutional neural networks have shown promising performances, their success relies on the availability of a large number of training data, which is prohibitively difficult for many applications. Therefore, in this study we propose the use of simulated US images and natural images as auxiliary datasets in order to pre-train our segmentation network, and then to fine-tune with limited in vivo data. We show that with as little as 19 in vivo images, fine-tuning the pre-trained network improves the dice score by 21% compared to training from scratch. We also demonstrate that if the same number of natural and simulation US images is available, pre-training on simulation data is preferable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using simulated US images and natural images as auxiliary datasets in the proposed segmentation network, and how does it impact the performance of the network when fine-tuned with limited in vivo data?\n\nA) The use of simulated US images and natural images allows for the pre-training of the network with a large amount of data, which improves the network's ability to generalize to new, unseen data. However, this approach may not necessarily improve the performance of the network when fine-tuned with limited in vivo data.\n\nB) The use of simulated US images and natural images as auxiliary datasets enables the pre-training of the network with a large amount of data, which improves the network's performance when fine-tuned with limited in vivo data. This approach is particularly useful when the amount of available in vivo data is limited, as demonstrated in the study.\n\nC) The use of simulated US images and natural images as auxiliary datasets is a necessary step in the development of automatic segmentation algorithms, but it does not necessarily improve the performance of the network when fine-tuned with limited in vivo data.\n\nD) The use of simulated US images and natural images as auxiliary datasets is a potential solution to the problem of limited annotated data, but it does not necessarily improve the performance of the network when fine-tuned with limited in vivo data.\n\nCorrect Answer: B) The use of simulated US images and natural images as auxiliary datasets enables the pre-training of the network with a large amount of data, which improves the network's performance when fine-tuned with limited in vivo data. This approach is particularly useful when the amount of available in vivo data is limited, as demonstrated in the study.\n\nExplanation: The correct answer is B) because it accurately summarizes the primary advantage of using simulated US images and natural images as auxiliary datasets. The study demonstrates that pre-training the network with a large amount of data (simulated US images and natural images) improves the network's performance when fine-tuned with limited in vivo data (only 19 images). This approach is particularly useful when the amount of available in vivo data is limited, as it allows the network to learn from a larger amount of data and generalize better to new, unseen data."}, "39": {"documentation": {"title": "$\\Pi-$nets: Deep Polynomial Neural Networks", "source": "Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis\n  Panagakis, Jiankang Deng, Stefanos Zafeiriou", "docs_id": "2003.03828", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "$\\Pi-$nets: Deep Polynomial Neural Networks. Deep Convolutional Neural Networks (DCNNs) is currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\\Pi$-Nets, a new class of DCNNs. $\\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. $\\Pi$-Nets can be implemented using special kind of skip connections and their parameters can be represented via high-order tensors. We empirically demonstrate that $\\Pi$-Nets have better representation power than standard DCNNs and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\\Pi$-Nets produce state-of-the-art results in challenging tasks, such as image generation. Lastly, our framework elucidates why recent generative models, such as StyleGAN, improve upon their predecessors, e.g., ProGAN."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of $\\Pi$-Nets over traditional Deep Convolutional Neural Networks (DCNNs) in terms of representation power?\n\n**A)** $\\Pi$-Nets can be implemented using only linear activation functions, whereas DCNNs use non-linear activation functions.\n\n**B)** $\\Pi$-Nets can learn high-order polynomial relationships between inputs and outputs, whereas DCNNs are limited to linear relationships.\n\n**C)** $\\Pi$-Nets can be trained using only convolutional layers, whereas DCNNs use a combination of convolutional and fully connected layers.\n\n**D)** $\\Pi$-Nets can be parallelized more easily than DCNNs, resulting in faster training times.\n\n**Correct Answer:** B) $\\Pi$-Nets can learn high-order polynomial relationships between inputs and outputs, whereas DCNNs are limited to linear relationships.\n\n**Explanation:** The correct answer is B) because the paper states that $\\Pi$-Nets have better representation power than standard DCNNs, and that they can learn high-order polynomial relationships between inputs and outputs. This is in contrast to DCNNs, which are limited to linear relationships. The other options are incorrect because they do not accurately reflect the advantages of $\\Pi$-Nets over DCNNs. Option A is incorrect because $\\Pi$-Nets can use non-linear activation functions, and option C is incorrect because $\\Pi$-Nets can use a variety of architectures, not just convolutional layers. Option D is also incorrect because the paper does not mention anything about the parallelization of $\\Pi$-Nets compared to DCNNs."}, "40": {"documentation": {"title": "Topological Ghosts: the Teeming of the Shrews", "source": "Nemanja Kaloper and McCullen Sandora", "docs_id": "1211.2868", "section": ["hep-th", "astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological Ghosts: the Teeming of the Shrews. We consider dynamics of spacetime volume-filling form fields with \"wrong sign\" kinetic terms, such as in so-called Type-II$^*$ string theories. Locally, these form fields are just additive renormalizations of the cosmological constant. They have no fluctuating degrees of freedom. However, once the fields are coupled to membranes charged under them, their configurations are unstable: by a process analogous to Schwinger pair production the field space-filling flux increases. This reduces the cosmological constant, and preserves the null energy condition, since the processes that can violate it by reducing the form flux are very suppressed. The increase of the form flux implies that as time goes on the probability for further membrane nucleation {\\it increases}, in contrast to the usual case where the field approaches its vacuum value and ceases to induce further transitions. Thus, in such models spaces with tiny positive vacuum energy are ultimately unstable, but the instability may be slow and localized. In a cosmological setting, this instability can enhance black hole rate formation, by locally making the vacuum energy negative at late times, which constrains the scales controlling membrane dynamics, and may even collapse a large region of the visible universe."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of Topological Ghosts: the Teeming of the Shrews, what is the consequence of the increase in form flux on the probability of further membrane nucleation, and how does this relate to the formation of black holes in a cosmological setting?\n\nA) The increase in form flux leads to a decrease in the probability of further membrane nucleation, resulting in a stable vacuum energy that constrains the scales controlling membrane dynamics.\nB) The increase in form flux leads to an increase in the probability of further membrane nucleation, which in turn enhances the formation of black holes by locally making the vacuum energy negative at late times.\nC) The increase in form flux has no effect on the probability of further membrane nucleation, and therefore does not impact the formation of black holes in a cosmological setting.\nD) The increase in form flux leads to a decrease in the formation of black holes, as the negative vacuum energy at late times constrains the scales controlling membrane dynamics.\n\nCorrect Answer: B) The increase in form flux leads to an increase in the probability of further membrane nucleation, which in turn enhances the formation of black holes by locally making the vacuum energy negative at late times.\n\nExplanation: According to the documentation, the increase in form flux implies that the probability for further membrane nucleation increases, which in turn enhances the formation of black holes by locally making the vacuum energy negative at late times. This is because the processes that can violate the null energy condition by reducing the form flux are very suppressed, and the increase in form flux preserves the null energy condition. Therefore, option B is the correct answer."}, "41": {"documentation": {"title": "Multi-Moments in Time: Learning and Interpreting Models for Multi-Action\n  Video Understanding", "source": "Mathew Monfort, Bowen Pan, Kandan Ramakrishnan, Alex Andonian, Barry A\n  McNamara, Alex Lascelles, Quanfu Fan, Dan Gutfreund, Rogerio Feris, Aude\n  Oliva", "docs_id": "1911.00232", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Moments in Time: Learning and Interpreting Models for Multi-Action\n  Video Understanding. Videos capture events that typically contain multiple sequential, and simultaneous, actions even in the span of only a few seconds. However, most large-scale datasets built to train models for action recognition in video only provide a single label per video. Consequently, models can be incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled and do not learn the full spectrum of information present in each video in training. Towards this goal, we present the Multi-Moments in Time dataset (M-MiT) which includes over two million action labels for over one million three second videos. This multi-label dataset introduces novel challenges on how to train and analyze models for multi-action detection. Here, we present baseline results for multi-action recognition using loss functions adapted for long tail multi-label learning, provide improved methods for visualizing and interpreting models trained for multi-label action detection and show the strength of transferring models trained on M-MiT to smaller datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What are the primary challenges in training models for multi-action detection in videos, and how does the Multi-Moments in Time (M-MiT) dataset address these challenges?\n\nA) The primary challenge is that videos are too short, and the dataset should be expanded to include longer videos. However, M-MiT addresses this by providing a large number of action labels for short videos.\n\nB) The primary challenge is that most large-scale datasets only provide a single label per video, leading to models being incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled. M-MiT addresses this by providing a multi-label dataset with over two million action labels for over one million three-second videos.\n\nC) The primary challenge is that models are not able to learn the full spectrum of information present in each video due to the lack of explicit labeling. M-MiT addresses this by providing a dataset that introduces novel challenges on how to train and analyze models for multi-action detection.\n\nD) The primary challenge is that videos are too complex, and the dataset should be simplified to include only simple actions. However, M-MiT addresses this by providing a dataset that includes over two million action labels for over one million three-second videos.\n\nCorrect Answer: B) The primary challenge is that most large-scale datasets only provide a single label per video, leading to models being incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled. M-MiT addresses this by providing a multi-label dataset with over two million action labels for over one million three-second videos."}, "42": {"documentation": {"title": "Activity-based contact network scaling and epidemic propagation in\n  metropolitan areas", "source": "Nishant Kumar, Jimi B. Oke, Bat-hen Nahmias-Biran", "docs_id": "2006.06039", "section": ["physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Activity-based contact network scaling and epidemic propagation in\n  metropolitan areas. Given the growth of urbanization and emerging pandemic threats, more sophisticated models are required to understand disease propagation and investigate the impacts of intervention strategies across various city types. We introduce a fully mechanistic, activity-based and highly spatio-temporally resolved epidemiological model which leverages on person-trajectories obtained from integrated mobility demand and supply models in full-scale cities. Simulating COVID-19 evolution in two full-scale cities with representative synthetic populations and mobility patterns, we analyze activity-based contact networks. We observe that transit contacts are scale-free in both cities, work contacts are Weibull distributed, and shopping or leisure contacts are exponentially distributed. We also investigate the impact of the transit network, finding that its removal dampens disease propagation, while work is also critical to post-peak disease spreading. Our framework, validated against existing case and mortality data, demonstrates the potential for tracking and tracing, along with detailed socio-demographic and mobility analyses of epidemic control strategies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on activity-based contact network scaling and epidemic propagation in metropolitan areas propose a fully mechanistic, activity-based, and highly spatio-temporally resolved epidemiological model. What is the primary advantage of this model in understanding disease propagation, according to the study?\n\nA) It provides a more detailed analysis of shopping and leisure contacts.\nB) It allows for the simulation of COVID-19 evolution in cities with representative synthetic populations and mobility patterns.\nC) It leverages on person-trajectories obtained from integrated mobility demand and supply models, enabling the investigation of the impact of the transit network on disease propagation.\nD) It uses a Weibull distribution to model work contacts, which is more accurate than other distributions.\n\n**Correct Answer:** C) It leverages on person-trajectories obtained from integrated mobility demand and supply models, enabling the investigation of the impact of the transit network on disease propagation.\n\n**Explanation:** The correct answer is C) because the study highlights the importance of leveraging person-trajectories from integrated mobility demand and supply models to investigate the impact of the transit network on disease propagation. This is a key aspect of the model's design and a major advantage of the study's approach. The other options are incorrect because they either focus on specific aspects of the model (A and D) or are too broad (B)."}, "43": {"documentation": {"title": "Heavy quark production at RHIC and LHC within a partonic transport model", "source": "Jan Uphoff, Oliver Fochler, Zhe Xu, Carsten Greiner", "docs_id": "1003.4200", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heavy quark production at RHIC and LHC within a partonic transport model. The production and space-time evolution of charm and bottom quarks in nucleus-nucleus collisions at RHIC and LHC are investigated with the partonic transport model BAMPS (Boltzmann Approach of MultiParton Scatterings). Heavy quarks, produced in primary hard parton scatterings during nucleon-nucleon collisions, are sampled using the Monte Carlo event generator PYTHIA or the leading order mini-jet model in conjunction with the Glauber model, revealing a strong sensitivity on the parton distribution functions, scales, and heavy quark mass. In a comprehensive study exploring different charm masses, K factors, and possible initial gluon conditions, secondary production and the evolution of heavy quarks are examined within a fully dynamic BAMPS simulation for central heavy ion collisions at RHIC and LHC. Although charm production in the quark-gluon plasma can be neglected at RHIC, it is significant at LHC but very sensitive to the initial conditions and the charm mass. Bottom production in the quark-gluon plasma, however, is negligible both at RHIC and LHC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary difference in the production of charm and bottom quarks in nucleus-nucleus collisions at RHIC and LHC, and how do the results depend on the initial conditions and heavy quark mass?\n\nA) At RHIC, charm production is significant and depends on the initial gluon conditions, while bottom production is negligible. At LHC, bottom production is significant and depends on the charm mass, while charm production is negligible.\n\nB) At RHIC, charm production is negligible, while bottom production is significant and depends on the initial gluon conditions. At LHC, charm production is significant and depends on the charm mass, while bottom production is negligible.\n\nC) At RHIC, charm production is significant and depends on the initial gluon conditions, while bottom production is negligible. At LHC, bottom production is significant and depends on the charm mass, while charm production is negligible.\n\nD) At RHIC, bottom production is significant and depends on the initial gluon conditions, while charm production is negligible. At LHC, charm production is significant and depends on the charm mass, while bottom production is negligible.\n\nCorrect Answer: B) At RHIC, charm production is negligible, while bottom production is significant and depends on the initial gluon conditions. At LHC, charm production is significant and depends on the charm mass, while bottom production is negligible.\n\nExplanation: The correct answer is B) because the documentation states that \"charm production in the quark-gluon plasma can be neglected at RHIC, but it is significant at LHC but very sensitive to the initial conditions and the charm mass.\" This means that at RHIC, charm production is negligible, while bottom production is significant and depends on the initial gluon conditions. At LHC, charm production is significant and depends on the charm mass, while bottom production is negligible."}, "44": {"documentation": {"title": "Nonchaotic Stagnant Motion in a Marginal Quasiperiodic Gradient System", "source": "Takahito Mitsui", "docs_id": "0801.1370", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonchaotic Stagnant Motion in a Marginal Quasiperiodic Gradient System. A one-dimensional dynamical system with a marginal quasiperiodic gradient is presented as a mathematical extension of a nonuniform oscillator. The system exhibits a nonchaotic stagnant motion, which is reminiscent of intermittent chaos. In fact, the density function of residence times near stagnation points obeys an inverse-square law, due to a mechanism similar to type-I intermittency. However, unlike intermittent chaos, in which the alternation between long stagnant phases and rapid moving phases occurs in a random manner, here the alternation occurs in a quasiperiodic manner. In particular, in case of a gradient with the golden ratio, the renewal of the largest residence time occurs at positions corresponding to the Fibonacci sequence. Finally, the asymptotic long-time behavior, in the form of a nested logarithm, is theoretically derived. Compared with the Pomeau-Manneville intermittency, a significant difference in the relaxation property of the long-time average of the dynamical variable is found."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the marginal quasiperiodic gradient system, what is the theoretical derivation of the asymptotic long-time behavior, and how does it differ from the Pomeau-Manneville intermittency?\n\nA) The asymptotic long-time behavior is a simple exponential decay, and it differs from Pomeau-Manneville intermittency in its lack of relaxation property.\n\nB) The asymptotic long-time behavior is a nested logarithm, and it differs from Pomeau-Manneville intermittency in its significant difference in the relaxation property of the long-time average of the dynamical variable.\n\nC) The asymptotic long-time behavior is a periodic oscillation, and it differs from Pomeau-Manneville intermittency in its quasiperiodic nature.\n\nD) The asymptotic long-time behavior is a random walk, and it differs from Pomeau-Manneville intermittency in its non-random nature.\n\nCorrect Answer: B) The asymptotic long-time behavior is a nested logarithm, and it differs from Pomeau-Manneville intermittency in its significant difference in the relaxation property of the long-time average of the dynamical variable.\n\nExplanation: The correct answer is based on the statement in the documentation that \"the asymptotic long-time behavior, in the form of a nested logarithm, is theoretically derived\" and that \"a significant difference in the relaxation property of the long-time average of the dynamical variable is found\" compared to Pomeau-Manneville intermittency. This indicates that the correct answer is B, which describes the asymptotic long-time behavior as a nested logarithm and highlights the difference in relaxation property."}, "45": {"documentation": {"title": "Bayesian Mixed Effects Models for Zero-inflated Compositions in\n  Microbiome Data Analysis", "source": "Boyu Ren, Sergio Bacallado, Stefano Favaro, Tommi Vatanen, Curtis\n  Huttenhower and Lorenzo Trippa", "docs_id": "1711.01241", "section": ["stat.ME", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Mixed Effects Models for Zero-inflated Compositions in\n  Microbiome Data Analysis. Detecting associations between microbial compositions and sample characteristics is one of the most important tasks in microbiome studies. Most of the existing methods apply univariate models to single microbial species separately, with adjustments for multiple hypothesis testing. We propose a Bayesian analysis for a generalized mixed effects linear model tailored to this application. The marginal prior on each microbial composition is a Dirichlet Process, and dependence across compositions is induced through a linear combination of individual covariates, such as disease biomarkers or the subject's age, and latent factors. The latent factors capture residual variability and their dimensionality is learned from the data in a fully Bayesian procedure. The proposed model is tested in data analyses and simulation studies with zero-inflated compositions. In these settings, within each sample, a large proportion of counts per microbial species are equal to zero. In our Bayesian model a priori the probability of compositions with absent microbial species is strictly positive. We propose an efficient algorithm to sample from the posterior and visualizations of model parameters which reveal associations between covariates and microbial compositions. We evaluate the proposed method in simulation studies, and then analyze a microbiome dataset for infants with type 1 diabetes which contains a large proportion of zeros in the sample-specific microbial compositions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Bayesian Mixed Effects Model for Zero-inflated Compositions in Microbiome Data Analysis, what is the primary purpose of the Dirichlet Process prior on each microbial composition, and how does it contribute to the model's ability to detect associations between microbial compositions and sample characteristics?\n\n**A)** The Dirichlet Process prior is used to model the distribution of microbial compositions, allowing for a flexible and non-parametric representation of the data. This prior enables the model to capture the underlying structure of the microbiome data and detect associations between covariates and microbial compositions.\n\n**B)** The Dirichlet Process prior is used to model the distribution of covariates, allowing for a flexible and non-parametric representation of the data. This prior enables the model to capture the underlying structure of the covariates and detect associations between covariates and microbial compositions.\n\n**C)** The Dirichlet Process prior is used to model the residual variability in the data, allowing for a flexible and non-parametric representation of the error terms. This prior enables the model to capture the underlying structure of the residual variability and detect associations between covariates and microbial compositions.\n\n**D)** The Dirichlet Process prior is used to model the latent factors, allowing for a flexible and non-parametric representation of the underlying structure of the data. This prior enables the model to capture the underlying structure of the latent factors and detect associations between covariates and microbial compositions.\n\n**Correct Answer:** A) The Dirichlet Process prior is used to model the distribution of microbial compositions, allowing for a flexible and non-parametric representation of the data. This prior enables the model to capture the underlying structure of the microbiome data and detect associations between covariates and microbial compositions.\n\n**Explanation:** The Dirichlet Process prior is a non-parametric prior distribution that allows for a flexible representation of the microbial composition data. It is used to model the distribution of microbial compositions, which enables the model to capture the underlying structure of the microbiome data. The prior also allows for the detection of associations between covariates and microbial compositions, which is essential for understanding the relationships between microbial compositions and sample characteristics."}, "46": {"documentation": {"title": "3D RNA and functional interactions from evolutionary couplings", "source": "Caleb Weinreb, Adam J. Riesselman, John B. Ingraham, Torsten Gross,\n  Chris Sander, Debora S. Marks", "docs_id": "1510.01420", "section": ["q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "3D RNA and functional interactions from evolutionary couplings. Non-coding RNAs are ubiquitous, but the discovery of new RNA gene sequences far outpaces research on their structure and functional interactions. We mine the evolutionary sequence record to derive precise information about function and structure of RNAs and RNA-protein complexes. As in protein structure prediction, we use maximum entropy global probability models of sequence co-variation to infer evolutionarily constrained nucleotide-nucleotide interactions within RNA molecules, and nucleotide-amino acid interactions in RNA-protein complexes. The predicted contacts allow all-atom blinded 3D structure prediction at good accuracy for several known RNA structures and RNA-protein complexes. For unknown structures, we predict contacts in 160 non-coding RNA families. Beyond 3D structure prediction, evolutionary couplings help identify important functional interactions, e.g., at switch points in riboswitches and at a complex nucleation site in HIV. Aided by accelerating sequence accumulation, evolutionary coupling analysis can accelerate the discovery of functional interactions and 3D structures involving RNA."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach used to derive precise information about the function and structure of non-coding RNAs and RNA-protein complexes from evolutionary sequence records?\n\nA) Maximum entropy global probability models of sequence co-variation are used to infer evolutionarily constrained nucleotide-nucleotide interactions within RNA molecules, and nucleotide-amino acid interactions in RNA-protein complexes.\n\nB) Evolutionary couplings are used to identify important functional interactions, such as those found at switch points in riboswitches and at complex nucleation sites in HIV.\n\nC) The predicted contacts from evolutionary couplings are used to infer the 3D structure of RNA molecules and RNA-protein complexes.\n\nD) The sequence accumulation rate is used to accelerate the discovery of functional interactions and 3D structures involving RNA, but does not provide information about their structure or function.\n\n**Correct Answer:** A) Maximum entropy global probability models of sequence co-variation are used to infer evolutionarily constrained nucleotide-nucleotide interactions within RNA molecules, and nucleotide-amino acid interactions in RNA-protein complexes.\n\n**Explanation:** The question requires the test-taker to understand the primary approach used to derive information about the function and structure of non-coding RNAs and RNA-protein complexes from evolutionary sequence records. The correct answer, A, states that maximum entropy global probability models of sequence co-variation are used to infer evolutionarily constrained nucleotide-nucleotide interactions within RNA molecules, and nucleotide-amino acid interactions in RNA-protein complexes. This approach is mentioned in the provided documentation as a key method for inferring the structure and function of RNAs and RNA-protein complexes.\n\nThe incorrect answers, B, C, and D, are plausible but incorrect. Answer B mentions evolutionary couplings, which are used to identify functional interactions, but not to derive information about structure. Answer C mentions the use of predicted contacts, but this is not the primary approach used to derive information about structure. Answer D mentions the sequence accumulation rate, which is used to accelerate the discovery of functional interactions and 3D structures, but does not provide information about their structure or function."}, "47": {"documentation": {"title": "Polarization Resolved Optical Excitation of Charge-Transfer Excitons in\n  PEN:PFP Co-Crystalline Films: Limits of Non-Periodic Modelling", "source": "Darius G\\\"under, Ana M. Valencia, Michele Guerrini, Tobias Breuer,\n  Caterina Cocchi, and Gregor Witte", "docs_id": "2110.00373", "section": ["cond-mat.mtrl-sci", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Polarization Resolved Optical Excitation of Charge-Transfer Excitons in\n  PEN:PFP Co-Crystalline Films: Limits of Non-Periodic Modelling. Charge-transfer excitons (CTX) at organic donor/acceptor interfaces are considered important intermediates for charge separation in photovoltaic devices. Crystalline model systems provide microscopic insights into the nature of such states as they enable microscopic structure-property investigations. Here, we use angular-resolved UV/Vis absorption spectroscopy to characterize the CTX of crystalline pentacene:perfluoro-pentacene (PEN:PFP) films allowing to determine the polarization of this state. This analysis is complemented by first-principles many-body calculations, performed on the three-dimensional PEN:PFP co-crystal, which confirm that the lowest-energy excitation is a CTX. Analogous simulations performed on bimolecular clusters are unable to reproduce this state. We ascribe this failure to the lack of long-range interactions and wave-function periodicity in these cluster calculations, which appear to remain a valid tool for modelling properties of organic materials ruled by local intermolecular couplings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason why first-principles many-body calculations performed on bimolecular clusters are unable to reproduce the lowest-energy excitation in crystalline PEN:PFP co-crystals?\n\nA) The lack of long-range interactions in these cluster calculations is not a valid assumption for organic materials.\nB) The wave-function periodicity in these cluster calculations is not sufficient to capture the long-range interactions present in the crystalline structure.\nC) The bimolecular clusters do not have the necessary degree of disorder to reproduce the CTX state.\nD) The calculations are limited by the small size of the bimolecular clusters, which do not accurately represent the three-dimensional structure of the PEN:PFP co-crystal.\n\nCorrect Answer: B) The wave-function periodicity in these cluster calculations is not sufficient to capture the long-range interactions present in the crystalline structure.\n\nExplanation: The correct answer is B) because the text states that the failure of bimolecular cluster calculations to reproduce the CTX state is due to the lack of long-range interactions and wave-function periodicity in these calculations. This suggests that the calculations are not able to capture the long-range interactions present in the crystalline structure, which is necessary to reproduce the CTX state. The other options are incorrect because they do not accurately reflect the reason provided in the text. Option A is incorrect because the text does not suggest that the lack of long-range interactions is not a valid assumption for organic materials. Option C is incorrect because the text does not mention disorder as a factor in the failure of bimolecular cluster calculations. Option D is incorrect because the text does not suggest that the size of the bimolecular clusters is a limiting factor in the calculations."}, "48": {"documentation": {"title": "Distributed Detection of Cycles", "source": "Pierre Fraigniaud and Dennis Olivetti", "docs_id": "1706.03992", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributed Detection of Cycles. Distributed property testing in networks has been introduced by Brakerski and Patt-Shamir (2011), with the objective of detecting the presence of large dense sub-networks in a distributed manner. Recently, Censor-Hillel et al. (2016) have shown how to detect 3-cycles in a constant number of rounds by a distributed algorithm. In a follow up work, Fraigniaud et al. (2016) have shown how to detect 4-cycles in a constant number of rounds as well. However, the techniques in these latter works were shown not to generalize to larger cycles $C_k$ with $k\\geq 5$. In this paper, we completely settle the problem of cycle detection, by establishing the following result. For every $k\\geq 3$, there exists a distributed property testing algorithm for $C_k$-freeness, performing in a constant number of rounds. All these results hold in the classical CONGEST model for distributed network computing. Our algorithm is 1-sided error. Its round-complexity is $O(1/\\epsilon)$ where $\\epsilon\\in(0,1)$ is the property testing parameter measuring the gap between legal and illegal instances."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the paper \"Distributed Detection of Cycles\" in terms of cycle detection in distributed networks?\n\nA) The authors propose a new algorithm for detecting 3-cycles in a constant number of rounds, but it does not generalize to larger cycles.\nB) The authors show that it is possible to detect 4-cycles in a constant number of rounds, but the techniques used do not generalize to larger cycles.\nC) The authors establish a distributed property testing algorithm for $C_k$-freeness, which detects the presence of cycles of all sizes $k\\geq 3$ in a constant number of rounds.\nD) The authors prove that it is impossible to detect cycles in a distributed network in a constant number of rounds.\n\n**Correct Answer:** C) The authors establish a distributed property testing algorithm for $C_k$-freeness, which detects the presence of cycles of all sizes $k\\geq 3$ in a constant number of rounds.\n\n**Explanation:** The correct answer is C) because the paper states that the authors \"completely settle the problem of cycle detection\" by establishing a distributed property testing algorithm for $C_k$-freeness, which detects the presence of cycles of all sizes $k\\geq 3$ in a constant number of rounds. This is a key contribution of the paper, as it generalizes the results of previous works on detecting 3-cycles and 4-cycles to larger cycles."}, "49": {"documentation": {"title": "Multiplicative renormalizability and quark propagator", "source": "J.C.R. Bloch", "docs_id": "hep-ph/0202073", "section": ["hep-ph", "hep-lat", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiplicative renormalizability and quark propagator. The renormalized Dyson-Schwinger equation for the quark propagator is studied, in Landau gauge, in a novel truncation which preserves multiplicative renormalizability. The renormalization constants are formally eliminated from the integral equations, and the running coupling explicitly enters the kernels of the new equations. To construct a truncation which preserves multiplicative renormalizability, and reproduces the correct leading order perturbative behavior, non-trivial cancellations involving the full quark-gluon vertex are assumed in the quark self-energy loop. A model for the running coupling is introduced, with infrared fixed point in agreement with previous Dyson-Schwinger studies of the gauge sector, and with correct logarithmic tail. Dynamical chiral symmetry breaking is investigated, and the generated quark mass is of the order of the extension of the infrared plateau of the coupling, and about three times larger than in the Abelian approximation, which violates multiplicative renormalizability. The generated scale is of the right size for hadronic phenomenology, without requiring an infrared enhancement of the running coupling."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main goal of the novel truncation of the Dyson-Schwinger equation for the quark propagator, and what is the key feature that distinguishes it from previous approaches?\n\nA) To eliminate the running coupling from the integral equations, and to preserve the Abelian approximation.\nB) To introduce a model for the running coupling that includes an infrared fixed point, and to investigate dynamical chiral symmetry breaking.\nC) To construct a truncation that preserves multiplicative renormalizability, and to reproduce the correct leading order perturbative behavior.\nD) To study the quark-gluon vertex in the Landau gauge, and to analyze the infrared plateau of the coupling.\n\nCorrect Answer: C) To construct a truncation that preserves multiplicative renormalizability, and to reproduce the correct leading order perturbative behavior.\n\nExplanation: The correct answer is C) because the novel truncation aims to preserve multiplicative renormalizability, which means that the renormalization constants are formally eliminated from the integral equations, and the running coupling explicitly enters the kernels of the new equations. This is a key feature that distinguishes the approach from previous ones, which may have violated multiplicative renormalizability. The other options are incorrect because they either focus on the running coupling (B), the Abelian approximation (A), or the quark-gluon vertex (D), but do not capture the main goal of the truncation."}, "50": {"documentation": {"title": "Cerebellar-Inspired Learning Rule for Gain Adaptation of Feedback\n  Controllers", "source": "Ivan Herreros, Xerxes D. Arsiwalla, Cosimo Della Santina, Jordi-Ysard\n  Puigbo, Antonio Bicchi, Paul Verschure", "docs_id": "1707.01484", "section": ["q-bio.NC", "cond-mat.dis-nn", "cs.SY", "nlin.AO", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cerebellar-Inspired Learning Rule for Gain Adaptation of Feedback\n  Controllers. How does our nervous system successfully acquire feedback control strategies in spite of a wide spectrum of response dynamics from different musculo-skeletal systems? The cerebellum is a crucial brain structure in enabling precise motor control in animals. Recent advances suggest that synaptic plasticity of cerebellar Purkinje cells involves molecular mechanisms that mimic the dynamics of the efferent motor system that they control allowing them to match the timing of their learning rule to behavior. Counter-Factual Predictive Control (CFPC) is a cerebellum-based feed-forward control scheme that exploits that principle for acquiring anticipatory actions. CFPC extends the classical Widrow-Hoff/Least Mean Squares by inserting a forward model of the downstream closed-loop system in its learning rule. Here we apply that same insight to the problem of learning the gains of a feedback controller. To that end, we frame a Model-Reference Adaptive Control (MRAC) problem and derive an adaptive control scheme treating the gains of a feedback controller as if they were the weights of an adaptive linear unit. Our results demonstrate that rather than being exclusively confined to cerebellar learning, the approach of controlling plasticity with a forward model of the subsystem controlled, an approach that we term as Model-Enhanced Least Mean Squares (ME-LMS), can provide a solution to wide set of adaptive control problems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Model-Enhanced Least Mean Squares (ME-LMS) approach in adaptive control, as described in the paper \"Cerebellar-Inspired Learning Rule for Gain Adaptation of Feedback Controllers\"?\n\n**A)** It allows for the incorporation of prior knowledge about the system dynamics into the learning rule.\n**B)** It enables the adaptation of gains in real-time, without the need for explicit modeling of the system.\n**C)** It provides a framework for learning the gains of a feedback controller by treating them as weights of an adaptive linear unit, leveraging the insight from cerebellar Purkinje cells' synaptic plasticity.\n**D)** It is a variant of the classical Widrow-Hoff/Least Mean Squares algorithm, with an additional step of forward modeling the downstream closed-loop system.\n\n**Correct Answer:** C) It provides a framework for learning the gains of a feedback controller by treating them as weights of an adaptive linear unit, leveraging the insight from cerebellar Purkinje cells' synaptic plasticity.\n\n**Explanation:** The correct answer, C, highlights the key innovation of the ME-LMS approach, which is to treat the gains of a feedback controller as weights of an adaptive linear unit, and to leverage the insight from cerebellar Purkinje cells' synaptic plasticity to match the timing of the learning rule to behavior. This approach allows for the adaptation of gains in a way that is informed by the dynamics of the system being controlled, rather than relying solely on empirical data. The other options are incorrect because they do not accurately capture the primary advantage of the ME-LMS approach. Option A is related to the forward modeling step, but it is not the primary advantage. Option B is incorrect because the ME-LMS approach does not necessarily enable real-time adaptation without explicit modeling. Option D is incorrect because the ME-LMS approach is a distinct variant of the classical Widrow-Hoff/Least Mean Squares algorithm, rather than simply an additional step."}, "51": {"documentation": {"title": "The Core of an Economy with an Endogenous Social Division of Labour", "source": "Robert P. Gilles", "docs_id": "1809.01470", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Core of an Economy with an Endogenous Social Division of Labour. This paper considers the core of a competitive market economy with an endogenous social division of labour. The theory is founded on the notion of a \"consumer-producer\", who consumes as well as produces commodities. First, we show that the Core of such an economy with an endogenous social division of labour can be founded on deviations of coalitions of arbitrary size, extending the seminal insights of Vind and Schmeidler for pure exchange economies. Furthermore, we establish the equivalence between the Core and the set of competitive equilibria for continuum economies with an endogenous social division of labour. Our analysis also concludes that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly. This is formulated as a Core equivalence result stated for a Structured Core concept based on renegotiations among fully specialised economic agents, i.e., coalitions that use only fully developed internal divisions of labour. Our approach bridges the gap between standard economies with social production and coalition production economies. Therefore, a more straightforward and natural interpretation of coalitional improvement and the Core can be developed than for coalition production economies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary contribution of the paper \"The Core of an Economy with an Endogenous Social Division of Labour\" in bridging the gap between standard economies with social production and coalition production economies?\n\nA) It establishes a new definition of the Core that relies solely on the Edgeworthian barter process.\nB) It demonstrates that the Core is equivalent to the set of competitive equilibria for continuum economies with an endogenous social division of labour.\nC) It shows that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly, leading to a more straightforward interpretation of coalitional improvement and the Core.\nD) It proves that the Core is a subset of the set of competitive equilibria for standard economies with social production.\n\nCorrect Answer: C) It shows that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly, leading to a more straightforward interpretation of coalitional improvement and the Core.\n\nExplanation: The correct answer is C) because the paper specifically states that self-organisation in a social division of labour can be incorporated into the Edgeworthian barter process directly, leading to a more straightforward and natural interpretation of coalitional improvement and the Core. This is a key contribution of the paper, as it bridges the gap between standard economies with social production and coalition production economies. The other options are incorrect because they do not accurately summarize the primary contribution of the paper. Option A is incorrect because the paper does not establish a new definition of the Core that relies solely on the Edgeworthian barter process. Option B is incorrect because while the paper does establish an equivalence between the Core and the set of competitive equilibria, this is not its primary contribution. Option D is incorrect because the paper does not prove that the Core is a subset of the set of competitive equilibria for standard economies with social production."}, "52": {"documentation": {"title": "A Robust Nonlinear RLS Type Adaptive Filter for\n  Second-Order-Intermodulation Distortion Cancellation in FDD LTE and 5G Direct\n  Conversion Transceivers", "source": "Andreas Gebhard, Oliver Lang, Michael Lunglmayr, Christian Motz, Ram\n  Sunil Kanumalli, Christina Auer, Thomas Paireder, Matthias Wagner, Harald\n  Pretl and Mario Huemer", "docs_id": "1807.04051", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Robust Nonlinear RLS Type Adaptive Filter for\n  Second-Order-Intermodulation Distortion Cancellation in FDD LTE and 5G Direct\n  Conversion Transceivers. Transceivers operating in frequency division duplex experience a transmitter leakage (TxL) signal into the receiver due to the limited duplexer stop-band isolation. This TxL signal in combination with the second-order nonlinearity of the receive mixer may lead to a baseband (BB) second-order intermodulation distortion (IMD2) with twice the transmit signal bandwidth. In direct conversion receivers, this nonlinear IMD2 interference may cause a severe signal-to-interference-plus-noise ratio degradation of the wanted receive signal. This contribution presents a nonlinear Wiener model recursive least-squares (RLS) type adaptive filter for the cancellation of the IMD2 interference in the digital BB. The included channel-select-, and DC-notch filter at the output of the proposed adaptive filter ensure that the provided IMD2 replica includes the receiver front-end filtering. A second, robust version of the nonlinear RLS algorithm is derived which provides numerical stability for highly correlated input signals which arise in e.g. LTE-A intra-band multi-cluster transmission scenarios. The performance of the proposed algorithms is evaluated by numerical simulations and by measurement data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the nonlinear Wiener model recursive least-squares (RLS) type adaptive filter proposed in the paper, and how does it address the issue of second-order intermodulation distortion (IMD2) in FDD LTE and 5G direct conversion transceivers?\n\n**A)** The primary goal is to improve the channel selectivity of the receiver front-end, and the adaptive filter is designed to cancel out the IMD2 interference by using a nonlinear Wiener model.\n\n**B)** The primary goal is to reduce the signal-to-interference-plus-noise ratio (SIR) degradation caused by IMD2 interference, and the adaptive filter is designed to cancel out the IMD2 interference by using a nonlinear RLS algorithm.\n\n**C)** The primary goal is to mitigate the effects of transmitter leakage (TxL) signal on the receiver, and the adaptive filter is designed to cancel out the TxL signal by using a nonlinear Wiener model.\n\n**D)** The primary goal is to improve the DC-notch filter performance, and the adaptive filter is designed to cancel out the IMD2 interference by using a nonlinear RLS algorithm.\n\n**Correct Answer:** B) The primary goal is to reduce the signal-to-interference-plus-noise ratio (SIR) degradation caused by IMD2 interference, and the adaptive filter is designed to cancel out the IMD2 interference by using a nonlinear RLS algorithm.\n\n**Explanation:** The correct answer is B) because the paper states that the nonlinear Wiener model recursive least-squares (RLS) type adaptive filter is designed to cancel out the second-order intermodulation distortion (IMD2) interference in the digital baseband, which causes a severe signal-to-interference-plus-noise ratio degradation of the wanted receive signal. The adaptive filter uses a nonlinear RLS algorithm to achieve this goal."}, "53": {"documentation": {"title": "Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators", "source": "Yangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan\n  Chen, Minyi Guo, Yuhao Zhu", "docs_id": "2110.03901", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators. Many of today's deep neural network accelerators, e.g., Google's TPU and NVIDIA's tensor core, are built around accelerating the general matrix multiplication (i.e., GEMM). However, supporting convolution on GEMM-based accelerators is not trivial. The naive method explicitly lowers the convolution to GEMM, commonly known as im2col, which introduces significant performance and memory overhead. Existing implicit im2col algorithms require unscalable hardware and are inefficient in supporting important convolution variants such as strided convolution. In this paper, we propose a memory-efficient and hardware-friendly implicit im2col algorithm used by Google's TPU, which dynamically converts a convolution into a GEMM with practically zero performance and memory overhead, fully unleashing the power of GEMM engines. Through comprehensive experimental results, we quantitatively argue that this algorithm has been adopted in commercial closed-source platforms, and we are the first to describe its high-level idea and implementation details. Finally, we show that our algorithm can also be generally applied to Nvidia's Tensor Cores (TC), matching and out-performing the measured performance on TCs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in supporting convolution on General Matrix Multiplication (GEMM)-based accelerators, and how does the proposed implicit im2col algorithm address this challenge?\n\n**A)** The primary challenge is the need for unscalable hardware to support convolution, and the proposed algorithm introduces significant performance and memory overhead.\n\n**B)** The primary challenge is the inefficiency of existing implicit im2col algorithms in supporting important convolution variants such as strided convolution, and the proposed algorithm dynamically converts a convolution into a GEMM with practically zero performance and memory overhead.\n\n**C)** The primary challenge is the lack of memory efficiency in existing implicit im2col algorithms, and the proposed algorithm is hardware-friendly but does not address the performance overhead.\n\n**D)** The primary challenge is the need for a new hardware architecture to support convolution, and the proposed algorithm is a software solution that can be applied to existing GEMM-based accelerators.\n\n**Correct Answer:** B) The primary challenge is the inefficiency of existing implicit im2col algorithms in supporting important convolution variants such as strided convolution, and the proposed algorithm dynamically converts a convolution into a GEMM with practically zero performance and memory overhead.\n\n**Explanation:** The question requires the test-taker to understand the challenges in supporting convolution on GEMM-based accelerators and the proposed implicit im2col algorithm's solution. The correct answer (B) highlights the inefficiency of existing implicit im2col algorithms and the proposed algorithm's ability to dynamically convert a convolution into a GEMM with minimal overhead. The incorrect answers (A, C, and D) either misrepresent the primary challenge or the proposed algorithm's characteristics."}, "54": {"documentation": {"title": "Semiclassical analysis of quantum asymptotic fields in the Yukawa theory", "source": "Zied Ammari, Marco Falconi, Marco Olivieri", "docs_id": "2111.03352", "section": ["math-ph", "math.AP", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiclassical analysis of quantum asymptotic fields in the Yukawa theory. In this article, we study the asymptotic fields of the Yukawa particle-field model of quantum physics, in the semiclassical regime $\\hslash\\to 0$, with an interaction subject to an ultraviolet cutoff. We show that the transition amplitudes between final (respectively initial) states converge towards explicit quantities involving the outgoing (respectively incoming) wave operators of the nonlinear Schr\\\"odinger-Klein-Gordon (S-KG) equation. Thus, we rigorously link the scattering theory of the Yukawa model to that of the Schr\\\"odinger-Klein-Gordon equation. Moreover, we prove that the asymptotic vacuum states of the Yukawa model have a phase space concentration property around classical radiationless solutions. Under further assumptions, we show that the S-KG energy admits a unique minimizer modulo symmetries and identify exactly the semiclassical measure of Yukawa ground states. Some additional consequences of asymptotic completeness are also discussed, and some further open questions are raised."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Yukawa theory, what is the relationship between the transition amplitudes of the particle-field model in the semiclassical regime and the nonlinear Schr\u00f6dinger-Klein-Gordon (S-KG) equation, and how does this relationship impact the phase space concentration property of the asymptotic vacuum states?\n\nA) The transition amplitudes converge towards the explicit quantities involving the outgoing wave operators of the S-KG equation, which in turn imply a phase space concentration property around classical radiationless solutions.\n\nB) The transition amplitudes converge towards the explicit quantities involving the incoming wave operators of the S-KG equation, which in turn imply a phase space concentration property around classical radiationless solutions.\n\nC) The transition amplitudes converge towards the explicit quantities involving the nonlinear Schr\u00f6dinger-Klein-Gordon (S-KG) equation itself, which in turn implies a phase space concentration property around classical radiationless solutions.\n\nD) The transition amplitudes converge towards the explicit quantities involving the wave operators of the Yukawa model, which in turn imply a phase space concentration property around classical radiationless solutions.\n\nCorrect Answer: A) The transition amplitudes converge towards the explicit quantities involving the outgoing wave operators of the S-KG equation, which in turn imply a phase space concentration property around classical radiationless solutions.\n\nExplanation: The correct answer is A) because the article states that the transition amplitudes between final (respectively initial) states converge towards explicit quantities involving the outgoing (respectively incoming) wave operators of the nonlinear Schr\u00f6dinger-Klein-Gordon (S-KG) equation. This implies that the phase space concentration property of the asymptotic vacuum states is related to the classical radiationless solutions of the S-KG equation."}, "55": {"documentation": {"title": "The Singular Angle of Nonlinear Systems", "source": "Chao Chen, Wei Chen, Di Zhao, Sei Zhen Khong, Li Qiu", "docs_id": "2109.01629", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Singular Angle of Nonlinear Systems. In this paper, we introduce an angle notion, called the singular angle, for stable nonlinear systems from an input-output perspective. The proposed system singular angle, based on the angle between $\\mathcal{L}_2$-signals, describes an upper bound for the \"rotating effect\" from the system input to output signals. It is, thus, different from the recently appeared nonlinear system phase which adopts the complexification of real-valued signals using the Hilbert transform. It can quantify the passivity and serve as an angular counterpart to the system $\\mathcal{L}_2$-gain. It also provides an alternative to the nonlinear system phase. A nonlinear small angle theorem, which involves a comparison of the loop system angle with $\\pi$, is established for feedback stability analysis. When dealing with multi-input multi-output linear time-invariant (LTI) systems, we further come up with the frequency-wise and $\\mathcal{H}_\\infty$ singular angle notions based on the matrix singular angle, and develop corresponding LTI small angle theorems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a nonlinear system with a singular angle, and a multi-input multi-output (MIMO) linear time-invariant (LTI) system with a frequency-wise singular angle. Suppose we have two feedback control systems:\n\nSystem 1: A nonlinear system with a singular angle, described by the angle between $\\mathcal{L}_2$-signals, and a feedback gain matrix $K$.\n\nSystem 2: An MIMO LTI system with a frequency-wise singular angle, described by the matrix singular angle, and a feedback gain matrix $K$.\n\nUnder what conditions on the feedback gain matrix $K$ can we establish the stability of both systems using the nonlinear small angle theorem and the LTI small angle theorem, respectively?\n\n**A)** The feedback gain matrix $K$ must satisfy the condition $\\|K\\|_{\\mathcal{L}_2} < 1$ for both systems.\n\n**B)** The feedback gain matrix $K$ must satisfy the condition $\\|K\\|_{\\mathcal{H}_\\infty} < 1$ for both systems.\n\n**C)** The feedback gain matrix $K$ must satisfy the condition $\\|K\\|_{\\mathcal{L}_2} < \\pi$ for System 1 and $\\|K\\|_{\\mathcal{H}_\\infty} < \\pi$ for System 2.\n\n**D)** The feedback gain matrix $K$ must satisfy the condition $\\|K\\|_{\\mathcal{L}_2} < \\pi$ for System 1 and $\\|K\\|_{\\mathcal{H}_\\infty} < 1$ for System 2.\n\n**Correct Answer:** C) The feedback gain matrix $K$ must satisfy the condition $\\|K\\|_{\\mathcal{L}_2} < \\pi$ for System 1 and $\\|K\\|_{\\mathcal{H}_\\infty} < 1$ for System 2.\n\n**Explanation:**\n\nFor System 1, the nonlinear small angle theorem states that the system is stable if the loop system angle is less than $\\pi$. Since the singular angle is an upper bound for the \"rotating effect\" from the system input to output signals, we can conclude that the system is stable if the feedback gain matrix $K$ satisfies the condition $\\|K\\|_{\\mathcal{L}_2} < \\pi$.\n\nFor System 2, the LTI small angle theorem states that the system is stable if the matrix singular angle is less than $\\pi$. Since the frequency-wise singular angle is a special case of the matrix singular angle, we can conclude that the system is stable if the feedback gain matrix $K$ satisfies the condition $\\|K\\|_{\\mathcal{H}_\\infty} < 1$.\n\nTherefore, the correct answer is C, which requires both conditions to be satisfied for the stability of both systems."}, "56": {"documentation": {"title": "Kpop: A kernel balancing approach for reducing specification assumptions\n  in survey weighting", "source": "Erin Hartman, Chad Hazlett and Ciara Sterbenz", "docs_id": "2107.08075", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kpop: A kernel balancing approach for reducing specification assumptions\n  in survey weighting. With the precipitous decline in response rates, researchers and pollsters have been left with highly non-representative samples, relying on constructed weights to make these samples representative of the desired target population. Though practitioners employ valuable expert knowledge to choose what variables, $X$ must be adjusted for, they rarely defend particular functional forms relating these variables to the response process or the outcome. Unfortunately, commonly-used calibration weights -- which make the weighted mean $X$ in the sample equal that of the population -- only ensure correct adjustment when the portion of the outcome and the response process left unexplained by linear functions of $X$ are independent. To alleviate this functional form dependency, we describe kernel balancing for population weighting (kpop). This approach replaces the design matrix $\\mathbf{X}$ with a kernel matrix, $\\mathbf{K}$ encoding high-order information about $\\mathbf{X}$. Weights are then found to make the weighted average row of $\\mathbf{K}$ among sampled units approximately equal that of the target population. This produces good calibration on a wide range of smooth functions of $X$, without relying on the user to explicitly specify those functions. We describe the method and illustrate it by application to polling data from the 2016 U.S. presidential election."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: What is the primary limitation of commonly-used calibration weights in survey weighting, and how does the kernel balancing approach (kpop) address this limitation?\n\nA) Calibration weights only work for linear relationships between variables, whereas kpop can handle non-linear relationships.\nB) Calibration weights assume independence between the portion of the outcome and the response process left unexplained by linear functions of X, whereas kpop does not make this assumption.\nC) Calibration weights are only effective for small samples, whereas kpop can handle large samples.\nD) Calibration weights are only suitable for categorical variables, whereas kpop can handle continuous variables.\n\nCorrect Answer: B) Calibration weights assume independence between the portion of the outcome and the response process left unexplained by linear functions of X, whereas kpop does not make this assumption.\n\nExplanation: The correct answer is B because the documentation states that calibration weights only ensure correct adjustment when the portion of the outcome and the response process left unexplained by linear functions of X are independent. In contrast, kpop does not rely on this assumption, allowing it to produce good calibration on a wide range of smooth functions of X."}, "57": {"documentation": {"title": "Chambers of Arrangements of Hyperplanes and Arrow's Impossibility\n  Theorem", "source": "Hiroaki Terao", "docs_id": "math/0608591", "section": ["math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chambers of Arrangements of Hyperplanes and Arrow's Impossibility\n  Theorem. Let ${\\mathcal A}$ be a nonempty real central arrangement of hyperplanes and ${\\rm \\bf Ch}$ be the set of chambers of ${\\mathcal A}$. Each hyperplane $H$ defines a half-space $H^{+} $ and the other half-space $H^{-}$. Let $B = \\{+, -\\}$. For $H\\in {\\mathcal A}$, define a map $\\epsilon_{H}^{+} : {\\rm \\bf Ch} \\to B$ by $\\epsilon_{H}^{+} (C)=+ \\text{(if} C\\subseteq H^{+}) \\text{and} \\epsilon_{H}^{+} (C)= - \\text{(if} C\\subseteq H^{-}).$ Define $\\epsilon_{H}^{-}=-\\epsilon_{H}^{+}.$ Let ${\\rm \\bf Ch}^{m} = {\\rm \\bf Ch}\\times{\\rm \\bf Ch}\\times...\\times{\\rm \\bf Ch} (m\\text{times}).$ Then the maps $\\epsilon_{H}^{\\pm}$ induce the maps $\\epsilon_{H}^{\\pm} : {\\rm \\bf Ch}^{m} \\to B^{m} $. We will study the admissible maps $\\Phi : {\\rm \\bf Ch}^{m} \\to {\\rm \\bf Ch}$ which are compatible with every $\\epsilon_{H}^{\\pm}$. Suppose $|{\\mathcal A}|\\geq 3$ and $m\\geq 2$. Then we will show that ${\\mathcal A}$ is indecomposable if and only if every admissible map is a projection to a omponent. When ${\\mathcal A}$ is a braid arrangement, which is indecomposable, this result is equivalent to Arrow's impossibility theorem in economics. We also determine the set of admissible maps explicitly for every nonempty real central arrangement."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\n**Question:** Let ${\\mathcal A}$ be a nonempty real central arrangement of hyperplanes and ${\\rm \\bf Ch}$ be the set of chambers of ${\\mathcal A}$. Suppose $|{\\mathcal A}|\\geq 3$ and $m\\geq 2$. Consider the maps $\\epsilon_{H}^{\\pm} : {\\rm \\bf Ch}^{m} \\to B^{m}$ induced by the hyperplanes $H\\in {\\mathcal A}$. Determine the set of admissible maps $\\Phi : {\\rm \\bf Ch}^{m} \\to {\\rm \\bf Ch}$ that are compatible with every $\\epsilon_{H}^{\\pm}$.\n\n**A)** The set of admissible maps is the set of all projections to a component of ${\\mathcal A}$.\n\n**B)** The set of admissible maps is the set of all maps that preserve the intersection of chambers.\n\n**C)** The set of admissible maps is the set of all maps that are continuous with respect to the topology of ${\\rm \\bf Ch}^{m}$.\n\n**D)** The set of admissible maps is the set of all maps that satisfy the condition $\\Phi(C_{1}\\cap C_{2})=\\Phi(C_{1})\\cap\\Phi(C_{2})$ for all $C_{1},C_{2}\\in {\\rm \\bf Ch}$.\n\n**Correct Answer:** D) The set of admissible maps is the set of all maps that satisfy the condition $\\Phi(C_{1}\\cap C_{2})=\\Phi(C_{1})\\cap\\Phi(C_{2})$ for all $C_{1},C_{2}\\in {\\rm \\bf Ch}$.\n\n**Explanation:** This answer is correct because the condition $\\Phi(C_{1}\\cap C_{2})=\\Phi(C_{1})\\cap\\Phi(C_{2})$ ensures that the map $\\Phi$ is compatible with every $\\epsilon_{H}^{\\pm}$, as it preserves the intersection of chambers. This condition is a necessary and sufficient condition for a map to be admissible.\n\nThe other options are incorrect because:\n\n* A) This option is not necessarily true, as a projection to a component of ${\\mathcal A}$ may not be compatible with every $\\epsilon_{H}^{\\pm}$.\n* B) This option is not necessarily true, as a map that preserves the intersection of chambers may not be compatible with every $\\epsilon_{H}^{\\pm}$.\n* C) This option is not necessarily true, as a continuous map with respect to the topology of ${\\rm \\bf Ch}^{m}$ may not be compatible with every $\\epsilon_{H}^{\\pm}$."}, "58": {"documentation": {"title": "Strategic COVID-19 vaccine distribution can simultaneously elevate\n  social utility and equity", "source": "Lin Chen, Fengli Xu, Zhenyu Han, Kun Tang, Pan Hui, James Evans, Yong\n  Li", "docs_id": "2111.06689", "section": ["cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Strategic COVID-19 vaccine distribution can simultaneously elevate\n  social utility and equity. Balancing social utility and equity in distributing limited vaccines represents a critical policy concern for protecting against the prolonged COVID-19 pandemic. What is the nature of the trade-off between maximizing collective welfare and minimizing disparities between more and less privileged communities? To evaluate vaccination strategies, we propose a novel epidemic model that explicitly accounts for both demographic and mobility differences among communities and their association with heterogeneous COVID-19 risks, then calibrate it with large-scale data. Using this model, we find that social utility and equity can be simultaneously improved when vaccine access is prioritized for the most disadvantaged communities, which holds even when such communities manifest considerable vaccine reluctance. Nevertheless, equity among distinct demographic features are in tension due to their complex correlation in society. We design two behavior-and-demography-aware indices, community risk and societal harm, which capture the risks communities face and those they impose on society from not being vaccinated, to inform the design of comprehensive vaccine distribution strategies. Our study provides a framework for uniting utility and equity-based considerations in vaccine distribution, and sheds light on how to balance multiple ethical values in complex settings for epidemic control."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study propose a novel epidemic model that accounts for demographic and mobility differences among communities and their association with heterogeneous COVID-19 risks. However, they also acknowledge that equity among distinct demographic features are in tension due to their complex correlation in society. What are the two behavior-and-demography-aware indices designed by the authors to capture the risks communities face and those they impose on society from not being vaccinated, and how do these indices inform the design of comprehensive vaccine distribution strategies?\n\n**A)** Community risk and social cohesion\n**B)** Societal harm and demographic disparities\n**C)** Vaccine efficacy and mobility patterns\n**D)** Health outcomes and economic burden\n\n**Correct Answer:** B) Societal harm and demographic disparities\n\n**Explanation:** The authors design two behavior-and-demography-aware indices: community risk and societal harm. Community risk captures the risks that a community faces from not being vaccinated, while societal harm captures the risks that a community imposes on society from not being vaccinated. These indices are used to inform the design of comprehensive vaccine distribution strategies that balance social utility and equity. The correct answer requires an understanding of the authors' proposed indices and their role in addressing the trade-off between maximizing collective welfare and minimizing disparities between more and less privileged communities."}, "59": {"documentation": {"title": "A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems", "source": "Francesco Andreuzzi and Nicola Demo and Gianluigi Rozza", "docs_id": "2110.09155", "section": ["math.NA", "cs.NA", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems. Dynamic mode decomposition (DMD) has recently become a popular tool for the non-intrusive analysis of dynamical systems. Exploiting the proper orthogonal decomposition as dimensionality reduction technique, DMD is able to approximate a dynamical system as a sum of (spatial) basis evolving linearly in time, allowing for a better understanding of the physical phenomena or for a future forecasting. We propose in this contribution an extension of the DMD to parametrized dynamical systems, focusing on the future forecasting of the output of interest in a parametric context. Initially, all the snapshots -- for different parameters and different time instants -- are projected to the reduced space, employing the DMD (or one of its variants) to approximate the reduced snapshots for a future instants. Still exploiting the low dimension of the reduced space, the predicted reduced snapshots are then combined using a regression technique, enabling the possibility to approximate any untested parametric configuration in any future instant. We are going to present here the algorithmic core of the aforementioned method, presenting at the end three different test cases with incremental complexity: a simple dynamical system with a linear parameter dependency, a heat problem with nonlinear parameter dependency and a fluid dynamics problem with nonlinear parameter dependency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Describe the key differences between the proposed extension of Dynamic Mode Decomposition (DMD) for parametrized dynamical systems and traditional DMD, and explain how the proposed method enables future forecasting of the output of interest in a parametric context.\n\n**A)** The proposed extension of DMD uses a different dimensionality reduction technique, such as PCA, whereas traditional DMD relies on the proper orthogonal decomposition. This allows for more accurate approximations of the dynamical system.\n\n**B)** The proposed method employs a regression technique to combine predicted reduced snapshots, whereas traditional DMD uses a different approach to combine the approximated snapshots. This enables the possibility to approximate any untested parametric configuration in any future instant.\n\n**C)** The proposed extension of DMD is only applicable to systems with linear parameter dependencies, whereas traditional DMD can handle systems with nonlinear parameter dependencies. This is because the proposed method uses a different approach to handle nonlinear parameter dependencies.\n\n**D)** The proposed method is only suitable for systems with a small number of parameters, whereas traditional DMD can handle systems with a large number of parameters. This is because the proposed method uses a reduced space to approximate the dynamical system.\n\n**Correct Answer:** B) The proposed method employs a regression technique to combine predicted reduced snapshots, whereas traditional DMD uses a different approach to combine the approximated snapshots. This enables the possibility to approximate any untested parametric configuration in any future instant.\n\n**Explanation:** The proposed extension of DMD for parametrized dynamical systems uses a regression technique to combine predicted reduced snapshots, which allows for the approximation of any untested parametric configuration in any future instant. This is in contrast to traditional DMD, which does not use a regression technique to combine the approximated snapshots. The use of a regression technique enables the proposed method to handle nonlinear parameter dependencies and to provide more accurate approximations of the dynamical system."}}