{"0": {"documentation": {"title": "Scaling and correlation in financial data", "source": "Rama Cont (CEA Saclay & CNRS Nice)", "docs_id": "cond-mat/9705075", "section": ["cond-mat.stat-mech", "nlin.AO", "cond-mat.dis-nn", "nlin.AO", "physics.data-an", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling and correlation in financial data. The statistical properties of the increments x(t+T) - x(t) of a financial time series depend on the time resolution T on which the increments are considered. A non-parametric approach is used to study the scale dependence of the empirical distribution of the price increments x(t+T) - x(t) of S&P Index futures, for time scales T, ranging from a few minutes to a few days using high-frequency price data. We show that while the variance increases linearly with the timescale, the kurtosis exhibits anomalous scaling properties, indicating a departure from the iid hypothesis. Study of the dependence structure of the increments shows that although the autocorrelation function decays rapidly to zero in a few minutes, the correlation of their squares exhibits a slow power law decay with exponent 0.37, indicating persistence in the scale of fluctuations. We establish a link between the scaling behavior and the dependence structure of the increments : in particular, the anomalous scaling of kurtosis may be explained by \"long memory\" properties of the square of the increments."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: According to the study on S&P Index futures, which of the following statements best describes the relationship between the time scale T and the statistical properties of price increments?\n\nA) The variance and kurtosis both increase linearly with the time scale T, supporting the iid hypothesis.\n\nB) The variance increases linearly with T, while the kurtosis exhibits anomalous scaling, indicating a departure from the iid hypothesis.\n\nC) Both variance and kurtosis exhibit anomalous scaling, strongly rejecting the iid hypothesis for all time scales.\n\nD) The variance shows anomalous scaling, while the kurtosis increases linearly with T, partially supporting the iid hypothesis.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation explicitly states that \"while the variance increases linearly with the timescale, the kurtosis exhibits anomalous scaling properties, indicating a departure from the iid hypothesis.\" This directly corresponds to option B.\n\nOption A is incorrect because it states that both variance and kurtosis increase linearly, which contradicts the anomalous scaling of kurtosis mentioned in the text.\n\nOption C is incorrect because it claims both variance and kurtosis exhibit anomalous scaling, whereas the text only mentions anomalous scaling for kurtosis.\n\nOption D is incorrect as it reverses the behavior of variance and kurtosis described in the document.\n\nThe question tests the student's ability to carefully read and interpret the given information, distinguishing between the behavior of different statistical properties (variance and kurtosis) and their implications for the iid (independent and identically distributed) hypothesis."}, "1": {"documentation": {"title": "Orbital stability of ensembles of particles in regions of magnetic\n  reconnection in Earth's magneto-tail", "source": "Christoph Lhotka and Philippe Bourdin and Elke Pilat-Lohinger", "docs_id": "1907.13478", "section": ["physics.plasm-ph", "astro-ph.EP", "nlin.CD", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orbital stability of ensembles of particles in regions of magnetic\n  reconnection in Earth's magneto-tail. We investigate the collective behaviour of particle orbits in the vicinity of magnetic reconnection in Earth's magneto-tail. Various regions of different kinds of orbital stability of particle motions are found. We locate regimes of temporary capture of particle orbits in configuration space as well as locations, where strong particle accelerations take place. With this study we are able to provide a detailed map, i.e. the topology, of high and low acceleration centers close to the reconnection site. Quasi-regular and chaotic kinds of motions of elementary particles can be determined as well. The orbital stability of particle orbits is obtained by a statistical analysis of the outcome of the system of variational equations of particle orbits within the framework of particle-in-cell simulations. Using the concept of Lyapunov Characteristic Numbers to ensembles of particle orbits we introduce Lyapunov Ensemble Averages to describe the response of particle orbits to local perturbations induced by the electro-magnetic field."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following best describes the primary method used in the study to analyze orbital stability of particle ensembles near magnetic reconnection sites in Earth's magneto-tail?\n\nA) Direct observation of particle trajectories using satellite data\nB) Theoretical modeling using Maxwell's equations\nC) Statistical analysis of variational equations within particle-in-cell simulations\nD) Spectroscopic analysis of magnetic field fluctuations\n\nCorrect Answer: C\n\nExplanation: The correct answer is C) Statistical analysis of variational equations within particle-in-cell simulations. The documentation explicitly states, \"The orbital stability of particle orbits is obtained by a statistical analysis of the outcome of the system of variational equations of particle orbits within the framework of particle-in-cell simulations.\" This method combines computational modeling (particle-in-cell simulations) with statistical analysis of variational equations to investigate orbital stability.\n\nOption A is incorrect because the study does not mention using direct satellite observations. Option B, while related to electromagnetic phenomena, is not the primary method described for this particular study. Option D is incorrect as spectroscopic analysis is not mentioned and would not directly provide information about orbital stability in this context.\n\nThe study introduces the concept of Lyapunov Ensemble Averages, derived from Lyapunov Characteristic Numbers, to describe how particle orbits respond to local perturbations in the electromagnetic field. This sophisticated approach allows the researchers to map regions of different orbital stabilities and identify areas of high and low particle acceleration near reconnection sites."}, "2": {"documentation": {"title": "Proton pairing in neutron stars from chiral effective field theory", "source": "Yeunhwan Lim and Jeremy W. Holt", "docs_id": "1709.08793", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Proton pairing in neutron stars from chiral effective field theory. We study the ${}^{1}S_0$ proton pairing gap in beta-equilibrated neutron star matter within the framework of chiral effective field theory. We focus on the role of three-body forces, which strongly modify the effective proton-proton spin-singlet interaction in dense matter. We find that three-body forces generically reduce both the size of the pairing gap and the maximum density at which proton pairing may occur. The pairing gap is computed within BCS theory, and model uncertainties are estimated by varying the nuclear potential and the choice of single-particle spectrum in the gap equation. We find that a second-order perturbative treatment of the single-particle spectrum suppresses the proton ${}^{1}S_0$ pairing gap relative to the use of a free spectrum. We estimate the critical temperature for the onset of proton superconductivity to be $T_c = (3.7 - 6.0)\\times 10^{9} $ K, which is consistent with previous theoretical results in the literature and marginally within the range deduced from a recent Bayesian analysis of neutron star cooling observations."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements about proton pairing in neutron stars, as described in the study using chiral effective field theory, is NOT correct?\n\nA) Three-body forces generally decrease the magnitude of the pairing gap and the maximum density for proton pairing.\n\nB) The critical temperature for proton superconductivity onset is estimated to be between 3.7 \u00d7 10^9 K and 6.0 \u00d7 10^9 K.\n\nC) A second-order perturbative treatment of the single-particle spectrum enhances the proton ^1S_0 pairing gap compared to using a free spectrum.\n\nD) The study focuses on the ^1S_0 proton pairing gap in beta-equilibrated neutron star matter.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the document states that \"a second-order perturbative treatment of the single-particle spectrum suppresses the proton ^1S_0 pairing gap relative to the use of a free spectrum,\" which is the opposite of what option C claims. All other statements (A, B, and D) are correct according to the given information. This question tests the reader's ability to carefully interpret the scientific information provided and identify a statement that contradicts the findings of the study."}, "3": {"documentation": {"title": "Effects of van der Waals Interactions in the Adsorption of Isooctane and\n  Ethanol on Fe(100) Surfaces", "source": "Pedro O. Bedolla, Gregor Feldbauer, Michael Wolloch, Stefan J. Eder,\n  Nicole D\\\"orr, Peter Mohn, Josef Redinger and Andr\\'as Vernes", "docs_id": "1405.2208", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of van der Waals Interactions in the Adsorption of Isooctane and\n  Ethanol on Fe(100) Surfaces. Van der Waals (vdW) forces play a fundamental role in the structure and behavior of diverse systems. Thanks to development of functionals that include non-local correlation, it is possible to study the effects of vdW interactions in systems of industrial and tribological interest. Here we simulated within the framework of density functional theory (DFT) the adsorption of isooctane (2,2,4-trimethylpentane) and ethanol on a Fe(100) surface, employing various exchange-correlation functionals to take vdW forces into account. In particular, this paper discusses the effect of vdW forces on the magnitude of adsorption energies, equilibrium geometries and their role in the binding mechanism. According to our calculations, vdW interactions increase the adsorption energies and reduce the equilibrium distances. Nevertheless, they do not influence the spatial configuration of the adsorbed molecules. Their effect on the electronic density is a non-isotropic, delocalized accumulation of charge between the molecule and the slab. In conclusion, vdW forces are essential for the adsorption of isooctane and ethanol on a bcc Fe(100) surface."}, "answer": "Here's a challenging multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the impact of van der Waals (vdW) forces on the adsorption of isooctane and ethanol on Fe(100) surfaces, according to the DFT simulations discussed in the paper?\n\nA) vdW forces decrease adsorption energies and increase equilibrium distances, while significantly altering the spatial configuration of adsorbed molecules.\n\nB) vdW forces have no significant effect on adsorption energies or equilibrium distances, but cause an isotropic accumulation of charge between the molecule and the slab.\n\nC) vdW forces increase adsorption energies and reduce equilibrium distances, without affecting the spatial configuration of adsorbed molecules, and result in a non-isotropic, delocalized charge accumulation.\n\nD) vdW forces only affect the electronic density, causing an isotropic charge accumulation, but have no impact on adsorption energies or equilibrium geometries.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key findings from the paper. The document states that vdW interactions increase adsorption energies and reduce equilibrium distances. It also mentions that vdW forces do not influence the spatial configuration of the adsorbed molecules. Furthermore, the paper describes the effect on electronic density as a \"non-isotropic, delocalized accumulation of charge between the molecule and the slab.\" \n\nOption A is incorrect because it contradicts the paper's findings on adsorption energies, equilibrium distances, and spatial configuration. Option B is wrong because it states that vdW forces have no significant effect on adsorption energies or equilibrium distances, which is contrary to the paper's conclusions. Option D is incorrect because it only acknowledges the effect on electronic density while ignoring the impact on adsorption energies and equilibrium distances, and it incorrectly describes the charge accumulation as isotropic."}, "4": {"documentation": {"title": "On-Board Federated Learning for Dense LEO Constellations", "source": "Nasrin Razmi, Bho Matthiesen, Armin Dekorsy, and Petar Popovski", "docs_id": "2111.12769", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On-Board Federated Learning for Dense LEO Constellations. Mega-constellations of small-size Low Earth Orbit (LEO) satellites are currently planned and deployed by various private and public entities. While global connectivity is the main rationale, these constellations also offer the potential to gather immense amounts of data, e.g., for Earth observation. Power and bandwidth constraints together with motives like privacy, limiting delay, or resiliency make it desirable to process this data directly within the constellation. We consider the implementation of on-board federated learning (FL) orchestrated by an out-of-constellation parameter server (PS) and propose a novel communication scheme tailored to support FL. It leverages intra-orbit inter-satellite links, the predictability of satellite movements and partial aggregating to massively reduce the training time and communication costs. In particular, for a constellation with 40 satellites equally distributed among five low Earth orbits and the PS in medium Earth orbit, we observe a 29x speed-up in the training process time and a 8x traffic reduction at the PS over the baseline."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of on-board federated learning for dense LEO constellations, which of the following combinations of features contributes most significantly to the reported 29x speed-up in training process time and 8x traffic reduction at the parameter server?\n\nA) Global connectivity and Earth observation capabilities\nB) Power constraints and privacy concerns\nC) Intra-orbit inter-satellite links and predictable satellite movements\nD) Small-size satellites and out-of-constellation parameter server\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation specifically mentions that the novel communication scheme leverages \"intra-orbit inter-satellite links, the predictability of satellite movements and partial aggregating to massively reduce the training time and communication costs.\" These features are directly responsible for the significant improvements in training speed and traffic reduction.\n\nOption A is incorrect because while global connectivity and Earth observation are mentioned as rationales for mega-constellations, they are not cited as factors contributing to the improved performance of the federated learning system.\n\nOption B mentions power constraints and privacy concerns, which are reasons for wanting to process data within the constellation, but they don't directly contribute to the speed-up or traffic reduction.\n\nOption D includes small-size satellites, which are a characteristic of the constellation but not a factor in the improved performance. The out-of-constellation parameter server is part of the system architecture but is not mentioned as a key factor in the performance improvements."}, "5": {"documentation": {"title": "On Gerber-Shiu functions and optimal dividend distribution for a\n  L\\'{e}vy risk process in the presence of a penalty function", "source": "F. Avram, Z. Palmowski, M. R. Pistorius", "docs_id": "1110.4965", "section": ["math.PR", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Gerber-Shiu functions and optimal dividend distribution for a\n  L\\'{e}vy risk process in the presence of a penalty function. This paper concerns an optimal dividend distribution problem for an insurance company whose risk process evolves as a spectrally negative L\\'{e}vy process (in the absence of dividend payments). The management of the company is assumed to control timing and size of dividend payments. The objective is to maximize the sum of the expected cumulative discounted dividend payments received until the moment of ruin and a penalty payment at the moment of ruin, which is an increasing function of the size of the shortfall at ruin; in addition, there may be a fixed cost for taking out dividends. A complete solution is presented to the corresponding stochastic control problem. It is established that the value-function is the unique stochastic solution and the pointwise smallest stochastic supersolution of the associated HJB equation. Furthermore, a necessary and sufficient condition is identified for optimality of a single dividend-band strategy, in terms of a particular Gerber-Shiu function. A number of concrete examples are analyzed."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the optimal dividend distribution problem for a L\u00e9vy risk process with a penalty function, which of the following statements is correct?\n\nA) The value-function is always the unique stochastic solution of the HJB equation, but not necessarily its pointwise smallest stochastic supersolution.\n\nB) The optimal strategy always involves multiple dividend-bands, regardless of the specific Gerber-Shiu function.\n\nC) The penalty payment at the moment of ruin is a decreasing function of the size of the shortfall at ruin.\n\nD) A necessary and sufficient condition for the optimality of a single dividend-band strategy can be expressed in terms of a particular Gerber-Shiu function.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that \"a necessary and sufficient condition is identified for optimality of a single dividend-band strategy, in terms of a particular Gerber-Shiu function.\" This directly corresponds to option D.\n\nOption A is incorrect because the documentation states that the value-function is both the unique stochastic solution and the pointwise smallest stochastic supersolution of the HJB equation.\n\nOption B is incorrect as the paper does not claim that multiple dividend-bands are always optimal. In fact, it discusses conditions for the optimality of a single dividend-band strategy.\n\nOption C is incorrect because the documentation states that the penalty payment is an increasing function of the size of the shortfall at ruin, not a decreasing function."}, "6": {"documentation": {"title": "Jordan Geometries - an Approach by Inversions", "source": "Wolfgang Bertram (IECL)", "docs_id": "1308.5888", "section": ["math.RA", "math.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jordan Geometries - an Approach by Inversions. Jordan geometries are defined as spaces equipped with point reflections depending on triples of points, exchanging two of the points and fixing the third. In a similar way, symmetric spaces have been defined by Loos (Symmetric Spaces I, 1969) as spaces equipped with point reflections depending on a point and fixing this point; therefore the theories of Jordan geometries and of symmetric spaces are closely related to each other -- in order to describe this link, the notion of symmetry actions of torsors and of symmetric spaces is introduced. Jordan geometries give rise both to symmetry actions of certain abelian torsors and of certain symmetric spaces, which in a sense are dual to each other. By using an algebraic differential calculus generalizing the classical Weil functors (see arxiv:1402.2619), we attach a tangent object to such geometries, namely a Jordan pair, respectively a Jordan algebra. The present approach works equally well over base rings in which 2 is not invertible (and in particular over the integers), and hence can be seen as a globalization of quadratic Jordan pairs; it also has a very transparent relation with the theory of associative geometries developped by M. Kinyon and the author."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the relationship between Jordan geometries and symmetric spaces, and their connection to algebraic structures?\n\nA) Jordan geometries are defined by point reflections on pairs of points, while symmetric spaces use triples of points, and they give rise to Lie algebras as their tangent objects.\n\nB) Jordan geometries and symmetric spaces are identical concepts, both defined by point reflections on triples of points, and their tangent objects are always associative algebras.\n\nC) Jordan geometries are defined by point reflections on triples of points, symmetric spaces by reflections on single points, and they give rise to Jordan pairs and Jordan algebras respectively as tangent objects.\n\nD) Symmetric spaces are generalizations of Jordan geometries, both using point reflections on triples of points, and their tangent objects are always Lie groups.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the given information, Jordan geometries are defined as spaces equipped with point reflections depending on triples of points, exchanging two of the points and fixing the third. In contrast, symmetric spaces are defined by point reflections depending on a single point and fixing this point. \n\nThe text also mentions that by using an algebraic differential calculus, we can attach tangent objects to these geometries: a Jordan pair for Jordan geometries, and a Jordan algebra for symmetric spaces. This corresponds directly to the statement in option C.\n\nOptions A and B are incorrect because they misrepresent the definitions of Jordan geometries and symmetric spaces. Option D is wrong because it incorrectly states that symmetric spaces are generalizations of Jordan geometries and mistakenly identifies Lie groups as the tangent objects.\n\nThis question tests the understanding of the fundamental definitions of Jordan geometries and symmetric spaces, their relationships, and their connections to algebraic structures, which are key concepts presented in the given text."}, "7": {"documentation": {"title": "Near-deterministic activation of room temperature quantum emitters in\n  hexagonal boron nitride", "source": "Nicholas V. Proscia, Zav Shotan, Harishankar Jayakumar, Prithvi Reddy,\n  Michael Dollar, Audrius Alkauskas, Marcus Doherty, Carlos A. Meriles, Vinod\n  M. Menon", "docs_id": "1712.01352", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near-deterministic activation of room temperature quantum emitters in\n  hexagonal boron nitride. Applications of quantum science to computing, cryptography and imaging are on their way to becoming key next generation technologies. Owing to the high-speed transmission and exceptional noise properties of photons, quantum photonic architectures are likely to play a central role. A long-standing hurdle, however, has been the realization of robust, device-compatible single photon sources that can be activated and controlled on demand. Here we use strain engineering to create large arrays of quantum emitters in two-dimensional hexagonal boron nitride (hBN). The large energy gap inherent to this Van der Waals material stabilizes the emitters at room temperature within nanoscale regions defined by substrate-induced deformation of the flake. Combining analytical and numerical modeling we show that emitter activation is likely the result of carrier trapping in deformation potential wells localized near the points where the hBN flake reaches the highest curvature. These findings, therefore, hint at novel opportunities for the manipulation of single photon sources through the combined control of strain and external electrostatic potentials under ambient conditions."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the mechanism behind the activation of quantum emitters in hexagonal boron nitride (hBN) as proposed by the researchers?\n\nA) The quantum emitters are activated by direct bandgap transitions in the hBN lattice.\n\nB) The emitters are created by intentionally introducing defects into the hBN structure through ion implantation.\n\nC) The activation occurs due to carrier trapping in deformation potential wells localized at points of highest curvature in the strained hBN flake.\n\nD) The quantum emitters are formed by spontaneous exciton recombination at the edges of the hBN flakes.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"Combining analytical and numerical modeling we show that emitter activation is likely the result of carrier trapping in deformation potential wells localized near the points where the hBN flake reaches the highest curvature.\" This directly corresponds to the mechanism described in option C.\n\nOption A is incorrect because the activation is not simply due to bandgap transitions, but rather involves strain-induced deformation.\n\nOption B is incorrect as the method described uses strain engineering, not ion implantation to create the emitters.\n\nOption D is incorrect because the emitters are not specifically formed at the edges of the flakes, but rather in nanoscale regions defined by substrate-induced deformation throughout the flake.\n\nThis question tests the student's ability to comprehend and interpret the key findings of the research, particularly the proposed mechanism for quantum emitter activation in strained hBN."}, "8": {"documentation": {"title": "Two-particle correlations in continuum dipole transitions in Borromean\n  nuclei", "source": "K. Hagino, H. Sagawa, T. Nakamura, and S. Shimoura", "docs_id": "0904.4775", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-particle correlations in continuum dipole transitions in Borromean\n  nuclei. We discuss the energy and angular distributions of two emitted neutrons from the dipole excitation of typical weakly-bound Borromean nuclei, $^{11}$Li and $^6$He. To this end, we use a three-body model with a density dependent contact interaction between the valence neutrons. Our calculation indicates that the energy distributions for the valence neutrons are considerably different between the two nuclei, although they show similar strong dineutron correlations in the ground state to each other. This different behaviour of the energy distribution primarily reflects the interaction between the neutron and the core nucleus, rather than the interaction between the valence neutrons. That is, the difference can be attributed to the presence of s-wave virtual state in the neutron-core system in $^{11}$Li, which is absent in $^6$He. It is pointed out that the angular distribution for $^{11}$Li in the low energy region shows a clear manifestation of the strong dineutron correlation, whereas the angular distribution for $^{6}$He exhibits a strong anticorrelation effect."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best explains the difference in energy distributions of emitted neutrons between \u00b9\u00b9Li and \u2076He, despite their similar dineutron correlations in the ground state?\n\nA) The difference is primarily due to the interaction between the valence neutrons in each nucleus.\n\nB) The presence of an s-wave virtual state in the neutron-core system of \u00b9\u00b9Li, which is absent in \u2076He, is the main factor.\n\nC) The angular distribution of emitted neutrons in the low energy region is responsible for the difference.\n\nD) The density-dependent contact interaction between valence neutrons causes the disparity in energy distributions.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage explicitly states that the difference in energy distributions \"can be attributed to the presence of s-wave virtual state in the neutron-core system in \u00b9\u00b9Li, which is absent in \u2076He.\" This interaction between the neutron and the core nucleus is highlighted as the primary factor, rather than the interaction between valence neutrons (eliminating option A) or the density-dependent contact interaction (eliminating option D). While the angular distribution is mentioned in the text, it is not cited as the cause for the difference in energy distributions, thus eliminating option C."}, "9": {"documentation": {"title": "Online reviews can predict long-term returns of individual stocks", "source": "Junran Wu, Ke Xu and Jichang Zhao", "docs_id": "1905.03189", "section": ["econ.GN", "cs.SI", "q-fin.CP", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online reviews can predict long-term returns of individual stocks. Online reviews are feedback voluntarily posted by consumers about their consumption experiences. This feedback indicates customer attitudes such as affection, awareness and faith towards a brand or a firm and demonstrates inherent connections with a company's future sales, cash flow and stock pricing. However, the predicting power of online reviews for long-term returns on stocks, especially at the individual level, has received little research attention, making a comprehensive exploration necessary to resolve existing debates. In this paper, which is based exclusively on online reviews, a methodology framework for predicting long-term returns of individual stocks with competent performance is established. Specifically, 6,246 features of 13 categories inferred from more than 18 million product reviews are selected to build the prediction models. With the best classifier selected from cross-validation tests, a satisfactory increase in accuracy, 13.94%, was achieved compared to the cutting-edge solution with 10 technical indicators being features, representing an 18.28% improvement relative to the random value. The robustness of our model is further evaluated and testified in realistic scenarios. It is thus confirmed for the first time that long-term returns of individual stocks can be predicted by online reviews. This study provides new opportunities for investors with respect to long-term investments in individual stocks."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best represents the primary contribution and implications of the study described in the Arxiv documentation?\n\nA) The study demonstrates that technical indicators are more reliable than online reviews for predicting long-term stock returns.\n\nB) The research proves that online reviews can accurately predict short-term fluctuations in individual stock prices.\n\nC) The study establishes a methodology framework using online reviews to predict long-term returns of individual stocks with improved accuracy over existing methods.\n\nD) The research concludes that online reviews are unreliable indicators of a company's future performance and should not be used for investment decisions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the study described in the documentation focuses on establishing a methodology framework that uses online reviews to predict long-term returns of individual stocks. The research achieved a 13.94% increase in accuracy compared to existing solutions using technical indicators, representing an 18.28% improvement relative to random selection. This framework, based exclusively on online reviews, demonstrates for the first time that such reviews can be used to predict long-term stock returns at the individual level, providing new opportunities for investors in long-term stock investments.\n\nOption A is incorrect because the study shows that online reviews outperformed technical indicators in prediction accuracy.\n\nOption B is incorrect as the study specifically focuses on long-term returns, not short-term fluctuations.\n\nOption D is incorrect because the study's findings support the use of online reviews for predicting stock performance, rather than dismissing them as unreliable."}, "10": {"documentation": {"title": "Symmetry-reduced Dynamic Mode Decomposition of Near-wall Turbulence", "source": "Elena Marensi, G\\\"okhan Yaln{\\i}z, Bj\\\"orn Hof and Nazmi Burak Budanur", "docs_id": "2101.07516", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Symmetry-reduced Dynamic Mode Decomposition of Near-wall Turbulence. Data-driven dimensionality reduction methods such as proper orthogonal decomposition (POD) and dynamic mode decomposition (DMD) have proven to be useful for exploring complex phenomena within fluid dynamics and beyond. A well-known challenge for these techniques is posed by the continuous symmetries, e.g. translations and rotations, of the system under consideration as drifts in the data dominate the modal expansions without providing an insight into the dynamics of the problem. In the present study, we address this issue for the pressure-driven flow in a rectangular channel by formulating a continuous symmetry reduction method that eliminates the translations simultaneously in the streamwise and spanwise directions. As an application, we consider turbulence in a minimal flow unit at a Reynolds number (based on the centerline velocity and half-channel height) Re = 2000 and compute the symmetry-reduced dynamic mode decomposition (SRDMD) of sliding data windows of varying durations. SRDMD of channel flow reveals episodes of turbulent time evolution that can be approximated by a low-dimensional linear expansion."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of symmetry-reduced dynamic mode decomposition (SRDMD) applied to channel flow turbulence, which of the following statements is most accurate?\n\nA) SRDMD eliminates translations only in the streamwise direction, improving the analysis of rotational symmetries in the flow.\n\nB) SRDMD reveals that turbulent time evolution in channel flow can always be perfectly described by a low-dimensional linear expansion.\n\nC) SRDMD addresses the challenge of continuous symmetries by formulating a method that eliminates translations simultaneously in the streamwise and spanwise directions.\n\nD) SRDMD is primarily useful for high Reynolds number flows (Re > 10,000) where traditional POD methods fail to capture important dynamics.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that SRDMD addresses the challenge posed by continuous symmetries \"by formulating a continuous symmetry reduction method that eliminates the translations simultaneously in the streamwise and spanwise directions.\" This approach allows for a more insightful analysis of the underlying dynamics without being dominated by drifts in the data.\n\nOption A is incorrect because SRDMD eliminates translations in both streamwise and spanwise directions, not just streamwise. It also doesn't specifically address rotational symmetries.\n\nOption B is an overstatement. The text mentions that SRDMD reveals \"episodes of turbulent time evolution that can be approximated by a low-dimensional linear expansion,\" but it doesn't claim this is always true or perfect.\n\nOption D is incorrect because the study mentioned uses a Reynolds number of 2000, which is not considered high in the context of turbulent flows. The effectiveness of SRDMD is not limited to high Reynolds number flows as suggested in this option."}, "11": {"documentation": {"title": "Fifteen Minutes of Fame: The Dynamics of Information Access on the Web", "source": "Z. Dezso, E. Almaas, A. Lukacs, B. Racz, I. Szakadat, A.-L. Barabasi", "docs_id": "physics/0505087", "section": ["physics.soc-ph", "cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fifteen Minutes of Fame: The Dynamics of Information Access on the Web. While current studies on complex networks focus on systems that change relatively slowly in time, the structure of the most visited regions of the Web is altered at the timescale from hours to days. Here we investigate the dynamics of visitation of a major news portal, representing the prototype for such a rapidly evolving network. The nodes of the network can be classified into stable nodes, that form the time independent skeleton of the portal, and news documents. The visitation of the two node classes are markedly different, the skeleton acquiring visits at a constant rate, while a news document's visitation peaking after a few hours. We find that the visitation pattern of a news document decays as a power law, in contrast with the exponential prediction provided by simple models of site visitation. This is rooted in the inhomogeneous nature of the browsing pattern characterizing individual users: the time interval between consecutive visits by the same user to the site follows a power law distribution, in contrast with the exponential expected for Poisson processes. We show that the exponent characterizing the individual user's browsing patterns determines the power-law decay in a document's visitation. Finally, our results document the fleeting quality of news and events: while fifteen minutes of fame is still an exaggeration in the online media, we find that access to most news items significantly decays after 36 hours of posting."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: The study on the dynamics of information access on the Web found that the visitation pattern of news documents decays as a power law. What is the primary reason given for this observation?\n\nA) The exponential nature of site visitation models\nB) The homogeneous browsing patterns of individual users\nC) The inhomogeneous nature of individual users' browsing patterns\nD) The constant visitation rate of the website's skeleton\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study found that the visitation pattern of a news document decays as a power law, which contrasts with the exponential prediction provided by simple models of site visitation. This power-law decay is attributed to the inhomogeneous nature of the browsing pattern characterizing individual users. Specifically, the time interval between consecutive visits by the same user to the site follows a power law distribution, rather than the exponential distribution expected for Poisson processes.\n\nOption A is incorrect because the study actually contrasts its findings with the exponential predictions of simple site visitation models.\n\nOption B is incorrect because the study specifically mentions the inhomogeneous (not homogeneous) nature of individual users' browsing patterns.\n\nOption D is incorrect because while the skeleton of the website does acquire visits at a constant rate, this is not the reason for the power-law decay in news document visitation.\n\nThis question tests the student's ability to identify the key factors influencing the observed patterns in web traffic and distinguish between different types of statistical distributions in network behavior."}, "12": {"documentation": {"title": "Scaling of Harmonic Oscillator Eigenfunctions and Their Nodal Sets\n  Around the Caustic", "source": "Boris Hanin, Steve Zelditch, Peng Zhou", "docs_id": "1602.06848", "section": ["math-ph", "math.MP", "math.PR", "math.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling of Harmonic Oscillator Eigenfunctions and Their Nodal Sets\n  Around the Caustic. We study the scaling asymptotics of the eigenspace projection kernels $\\Pi_{\\hbar, E}(x,y)$ of the isotropic Harmonic Oscillator $- \\hbar ^2 \\Delta + |x|^2$ of eigenvalue $E = \\hbar(N + \\frac{d}{2})$ in the semi-classical limit $\\hbar \\to 0$. The principal result is an explicit formula for the scaling asymptotics of $\\Pi_{\\hbar, E}(x,y)$ for $x,y$ in a $\\hbar^{2/3}$ neighborhood of the caustic $\\mathcal C_E$ as $\\hbar \\to 0.$ The scaling asymptotics are applied to the distribution of nodal sets of Gaussian random eigenfunctions around the caustic as $\\hbar \\to 0$. In previous work we proved that the density of zeros of Gaussian random eigenfunctions of $\\hat{H}_{\\hbar}$ have different orders in the Planck constant $\\hbar$ in the allowed and forbidden regions: In the allowed region the density is of order $\\hbar^{-1}$ while it is $\\hbar^{-1/2}$ in the forbidden region. Our main result on nodal sets is that the density of zeros is of order $\\hbar^{-\\frac{2}{3}}$ in an $\\hbar^{\\frac{2}{3}}$-tube around the caustic. This tube radius is the `critical radius'. For annuli of larger inner and outer radii $\\hbar^{\\alpha}$ with $0< \\alpha < \\frac{2}{3}$ we obtain density results which interpolate between this critical radius result and our prior ones in the allowed and forbidden region. We also show that the Hausdorff $(d-2)$-dimensional measure of the intersection of the nodal set with the caustic is of order $\\hbar^{- \\frac{2}{3}}$."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Consider the isotropic Harmonic Oscillator $- \\hbar ^2 \\Delta + |x|^2$ with eigenvalue $E = \\hbar(N + \\frac{d}{2})$ in the semi-classical limit $\\hbar \\to 0$. According to the research, how does the density of zeros of Gaussian random eigenfunctions scale in different regions as $\\hbar \\to 0$?\n\nA) Allowed region: $\\hbar^{-1}$, Forbidden region: $\\hbar^{-1/2}$, $\\hbar^{2/3}$-tube around caustic: $\\hbar^{-1/3}$\nB) Allowed region: $\\hbar^{-1/2}$, Forbidden region: $\\hbar^{-1}$, $\\hbar^{2/3}$-tube around caustic: $\\hbar^{-2/3}$\nC) Allowed region: $\\hbar^{-1}$, Forbidden region: $\\hbar^{-1/2}$, $\\hbar^{2/3}$-tube around caustic: $\\hbar^{-2/3}$\nD) Allowed region: $\\hbar^{-2/3}$, Forbidden region: $\\hbar^{-1/3}$, $\\hbar^{2/3}$-tube around caustic: $\\hbar^{-1}$\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the scaling behavior of zero density in different regions of the harmonic oscillator potential. According to the research, the density of zeros scales differently in three regions:\n\n1. In the allowed region, the density is of order $\\hbar^{-1}$.\n2. In the forbidden region, the density is of order $\\hbar^{-1/2}$.\n3. In an $\\hbar^{2/3}$-tube around the caustic (the critical radius), the density is of order $\\hbar^{-2/3}$.\n\nOption C correctly captures all these scaling behaviors. Options A, B, and D contain incorrect scalings for one or more regions, making them incorrect choices."}, "13": {"documentation": {"title": "Potentials of a family of arrangements of hyperplanes and elementary\n  subarrangements", "source": "Andrew Prudhom and Alexander Varchenko", "docs_id": "1611.03944", "section": ["math.AG", "math.DG", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Potentials of a family of arrangements of hyperplanes and elementary\n  subarrangements. We consider the Frobenius algebra of functions on the critical set of the master function of a weighted arrangement of hyperplanes in $\\C^k$ with normal crossings. We construct two potential functions (of first and second kind) of variables labeled by hyperplanes of the arrangement and prove that the matrix coefficients of the Grothendieck residue bilinear form on the algebra are given by the $2k$-th derivatives of the potential function of first kind and the matrix coefficients of the multiplication operators on the algebra are given by the $(2k+1)$-st derivatives of the potential function of second kind. Thus the two potentials completely determine the Frobenius algebra. The presence of these potentials is a manifestation of a Frobenius like structure similar to the Frobenius manifold structure. We introduce the notion of an elementary subarrangement of an arrangement with normal crossings. It turns out that our potential functions are local in the sense that the potential functions are sums of contributions from elementary subarrangements of the given arrangement. This is a new phenomenon of locality of the Grothendieck residue bilinear form and multiplication on the algebra. It is known that this Frobenius algebra of functions on the critical set is isomorphic to the Bethe algebra of this arrangement. (That Bethe algebra is an analog of the Bethe algebras in the theory of quantum integrable models.) Thus our potential functions describe that Bethe algebra too."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements accurately describes the relationship between the potential functions and the Frobenius algebra of functions on the critical set of the master function of a weighted arrangement of hyperplanes?\n\nA) The potential function of the first kind determines the matrix coefficients of the multiplication operators, while the potential function of the second kind determines the matrix coefficients of the Grothendieck residue bilinear form.\n\nB) The 2k-th derivatives of the potential function of the first kind give the matrix coefficients of the Grothendieck residue bilinear form, and the (2k+1)-st derivatives of the potential function of the second kind give the matrix coefficients of the multiplication operators.\n\nC) The potential functions are global in nature and cannot be decomposed into contributions from subarrangements of the given arrangement.\n\nD) The potential functions are irrelevant to the structure of the Frobenius algebra and are merely a mathematical curiosity.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the documentation, the matrix coefficients of the Grothendieck residue bilinear form on the algebra are given by the 2k-th derivatives of the potential function of the first kind. Additionally, the matrix coefficients of the multiplication operators on the algebra are given by the (2k+1)-st derivatives of the potential function of the second kind. This relationship demonstrates how the two potential functions completely determine the Frobenius algebra.\n\nOption A is incorrect because it reverses the roles of the two potential functions. Option C is incorrect because the documentation explicitly states that the potential functions are local, being sums of contributions from elementary subarrangements. Option D is entirely false, as the potential functions are crucial in determining the structure of the Frobenius algebra, not irrelevant or merely curious."}, "14": {"documentation": {"title": "The optical applications of 3D sub-wavelength block-copolymer\n  nanostructured functional materials", "source": "Zsolt Poole, Aidong Yan, Paul Ohodnicki, Kevin Chen", "docs_id": "1504.08346", "section": ["cond-mat.mes-hall", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The optical applications of 3D sub-wavelength block-copolymer\n  nanostructured functional materials. A method to engineer the refractive indices of functional materials (TiO2, ZnO, SnO2, SiO2), by nanostructuring in the deep sub-wavelength regime (<20nm), is presented. Block-copolymer templating combined with a wet processing route is used to realize 3D functional nanostructures with continuously adjustable refractive indices from 1.17 to 2.2. Wet processing accessed refractive index engineering can be applied to address a variety of realizability concerns in attaining design specified refractive index values and refractive index gradients in 1D, 2D, and 3D that arise as the results of optical design techniques such as thin film optimization methods, transformation optics and conformal mapping. Refractive index optimized multi-layer anti-reflection coatings on crystalline silicon, which reduce light reflections from 38% down to ~3% with a wide angular span, are demonstrated with the developed wet processing route. A high temperature oxygen free fiber optic hydrogen sensor realized by accessing nano-engineering enabled refractive indices is also presented. The functionality of the sensor is characterized with a fiber Bragg grating, transmission based interrogation, and optical frequency domain reflectometry. The latter demonstrates the potential of the developed sensor for the detection of chemical gradients for applications such as in high temperature hydrogen driven fuel cells."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A researcher wants to develop a high-temperature oxygen-free fiber optic hydrogen sensor using block-copolymer templating and wet processing. Which combination of techniques would be most appropriate for characterizing the functionality of this sensor?\n\nA) Fiber Bragg grating and X-ray diffraction\nB) Transmission-based interrogation and scanning electron microscopy\nC) Optical frequency domain reflectometry and atomic force microscopy\nD) Fiber Bragg grating, transmission-based interrogation, and optical frequency domain reflectometry\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation specifically mentions that the functionality of the high-temperature oxygen-free fiber optic hydrogen sensor is characterized using three techniques: fiber Bragg grating, transmission-based interrogation, and optical frequency domain reflectometry. These methods are particularly suitable for analyzing the optical properties and performance of the sensor.\n\nOption A is incorrect because while fiber Bragg grating is mentioned, X-ray diffraction is not discussed in the context of sensor characterization.\n\nOption B is partially correct with transmission-based interrogation, but scanning electron microscopy is not mentioned for sensor characterization in this context.\n\nOption C is partially correct with optical frequency domain reflectometry, but atomic force microscopy is not mentioned for sensor characterization.\n\nOnly option D correctly combines all three techniques explicitly stated in the documentation for characterizing the sensor's functionality."}, "15": {"documentation": {"title": "Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution", "source": "Alberto Pretto, St\\'ephanie Aravecchia, Wolfram Burgard, Nived\n  Chebrolu, Christian Dornhege, Tillmann Falck, Freya Fleckenstein, Alessandra\n  Fontenla, Marco Imperoli, Raghav Khanna, Frank Liebisch, Philipp Lottes,\n  Andres Milioto, Daniele Nardi, Sandro Nardi, Johannes Pfeifer, Marija\n  Popovi\\'c, Ciro Potena, C\\'edric Pradalier, Elisa Rothacker-Feder, Inkyu Sa,\n  Alexander Schaefer, Roland Siegwart, Cyrill Stachniss, Achim Walter, Wera\n  Winterhalter, Xiaolong Wu and Juan Nieto", "docs_id": "1911.03098", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution. The application of autonomous robots in agriculture is gaining increasing popularity thanks to the high impact it may have on food security, sustainability, resource use efficiency, reduction of chemical treatments, and the optimization of human effort and yield. With this vision, the Flourish research project aimed to develop an adaptable robotic solution for precision farming that combines the aerial survey capabilities of small autonomous unmanned aerial vehicles (UAVs) with targeted intervention performed by multi-purpose unmanned ground vehicles (UGVs). This paper presents an overview of the scientific and technological advances and outcomes obtained in the project. We introduce multi-spectral perception algorithms and aerial and ground-based systems developed for monitoring crop density, weed pressure, crop nitrogen nutrition status, and to accurately classify and locate weeds. We then introduce the navigation and mapping systems tailored to our robots in the agricultural environment, as well as the modules for collaborative mapping. We finally present the ground intervention hardware, software solutions, and interfaces we implemented and tested in different field conditions and with different crops. We describe a real use case in which a UAV collaborates with a UGV to monitor the field and to perform selective spraying without human intervention."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following combinations best describes the primary objectives and technological advancements of the Flourish research project in precision farming?\n\nA) Aerial mapping using LiDAR, ground-based crop harvesting, and automated irrigation systems\nB) Multi-spectral aerial imaging, targeted ground intervention, and manual weed classification\nC) Multi-spectral perception, collaborative aerial-ground mapping, and autonomous selective spraying\nD) Satellite imagery analysis, robotic soil sampling, and automated fertilizer application\n\nCorrect Answer: C\n\nExplanation: The correct answer is C) Multi-spectral perception, collaborative aerial-ground mapping, and autonomous selective spraying. \n\nThis answer accurately reflects the key aspects of the Flourish project as described in the document:\n\n1. Multi-spectral perception: The project developed \"multi-spectral perception algorithms\" for monitoring crop density, weed pressure, and crop nitrogen nutrition status.\n\n2. Collaborative aerial-ground mapping: The document mentions \"modules for collaborative mapping\" and describes a use case where \"a UAV collaborates with a UGV to monitor the field.\"\n\n3. Autonomous selective spraying: The project implemented \"ground intervention hardware, software solutions\" and demonstrated the ability \"to perform selective spraying without human intervention.\"\n\nOption A is incorrect as it mentions LiDAR and automated irrigation, which are not specifically mentioned in the document. Option B is wrong because it includes manual weed classification, while the project aimed for autonomous classification. Option D is incorrect as it refers to satellite imagery and automated fertilizer application, which are not mentioned as part of the Flourish project."}, "16": {"documentation": {"title": "Exploring the expansion dynamics of the universe from galaxy cluster\n  surveys", "source": "Deng Wang, Xin-He Meng", "docs_id": "1709.04134", "section": ["astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring the expansion dynamics of the universe from galaxy cluster\n  surveys. To understand the expansion dynamics of the universe from galaxy cluster scales, using the angular diameter distance (ADD) data from two different galaxy cluster surveys, we constrain four cosmological models to explore the underlying value of $H_0$ and employ the model-independent Gaussian Processes to investigate the evolution of the equation of state of dark energy. The ADD data in the X-ray bands consists of two samples covering the redshift ranges [0.023, 0.784] and [0.14, 0.89], respectively. We find that: (i) For these two samples, the obtained values of $H_0$ are more consistent with the recent local observation by Riess et al. than the global measurement by the Plank Collaboration, and the $\\Lambda$CDM model is still preferred utilizing the information criterions; (ii) For the first sample, there is no evidence of dynamical dark energy (DDE) at the $2\\sigma$ confidence level (CL); (iii) For the second one, the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior in the relatively low redshift range over the $2\\sigma$ CL, which gives a hint that the late-time universe may be actually dominated by the DDE from galaxy cluster scales; (iv) By adding a combination of Type Ia Supernovae, cosmic chronometers and Planck-2015 shift parameter and HII galaxy measurements into both ADD samples, the DDE exists evidently over the $2\\sigma$ CL."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: Based on the analysis of angular diameter distance (ADD) data from two galaxy cluster surveys, which of the following statements is NOT supported by the findings described in the document?\n\nA) The obtained values of H0 from both samples align more closely with local observations by Riess et al. than with global measurements by the Planck Collaboration.\n\nB) The \u039bCDM model is still the preferred model when using information criterions for both samples.\n\nC) Evidence of dynamical dark energy (DDE) was found at the 2\u03c3 confidence level in both samples without additional data.\n\nD) The second sample showed a phantom-crossing behavior in the equation of state of dark energy at relatively low redshifts, suggesting possible DDE dominance.\n\nCorrect Answer: C\n\nExplanation: The question asks for the statement that is NOT supported by the findings. Option C is incorrect because the document states that for the first sample, there was no evidence of dynamical dark energy (DDE) at the 2\u03c3 confidence level. Evidence for DDE in both samples was only found after adding additional data from Type Ia Supernovae, cosmic chronometers, Planck-2015 shift parameter, and HII galaxy measurements.\n\nOptions A, B, and D are all supported by the document:\nA) The document explicitly states that H0 values from both samples are more consistent with Riess et al.'s local observations.\nB) The document mentions that the \u039bCDM model is still preferred using information criterions.\nD) For the second sample, the document describes a phantom-crossing behavior in the equation of state of dark energy at low redshifts, hinting at possible DDE dominance."}, "17": {"documentation": {"title": "Traces of the nuclear liquid-gas phase transition in the analytic\n  properties of hot QCD", "source": "Oleh Savchuk, Volodymyr Vovchenko, Roman V. Poberezhnyuk, Mark I.\n  Gorenstein, Horst Stoecker", "docs_id": "1909.04461", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Traces of the nuclear liquid-gas phase transition in the analytic\n  properties of hot QCD. The nuclear liquid-gas transition at normal nuclear densities, $n \\sim n_0 = 0.16$ fm$^{-3}$, and small temperatures, $T \\sim 20$ MeV, has a large influence on analytic properties of the QCD grand-canonical thermodynamic potential. A classical van der Waals equation is used to determine these unexpected features due to dense cold matter qualitatively. The existence of the nuclear matter critical point results in thermodynamic branch points, which are located at complex chemical potential values, for $T > T_c \\simeq 20$ MeV, and exhibit a moderate model dependence up to rather large temperatures $T \\lesssim 100$ MeV. The behavior at higher temperatures is studied using the van der Waals hadron resonance gas (vdW-HRG) model. The baryon-baryon interactions have a decisive influence on the QCD thermodynamics close to $\\mu_B = 0$. In particular, nuclear matter singularities limit the radius of convergence $r_{\\mu_B/T}$ of the Taylor expansion in $\\mu_B/T$, with $r_{\\mu_B/T} \\sim 2-3$ values at $T \\sim 140-170$ MeV obtained in the vdW-HRG model."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: According to the van der Waals hadron resonance gas (vdW-HRG) model, which of the following statements about the radius of convergence r_\u03bcB/T of the Taylor expansion in \u03bcB/T at temperatures T ~ 140-170 MeV is correct?\n\nA) r_\u03bcB/T is primarily determined by quark-gluon interactions and is typically greater than 5.\n\nB) r_\u03bcB/T is largely independent of baryon-baryon interactions and remains constant across all temperatures.\n\nC) r_\u03bcB/T is limited by nuclear matter singularities and has values of approximately 2-3.\n\nD) r_\u03bcB/T increases exponentially with temperature, reaching values greater than 10 at T ~ 170 MeV.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the vdW-HRG model's predictions for the radius of convergence of the Taylor expansion in \u03bcB/T. According to the passage, the model shows that nuclear matter singularities limit this radius of convergence. Specifically, it states that \"r_\u03bcB/T ~ 2-3 values at T ~ 140-170 MeV obtained in the vdW-HRG model.\" This directly corresponds to option C.\n\nOption A is incorrect because the passage emphasizes the importance of baryon-baryon interactions, not quark-gluon interactions, and the given range is too high.\n\nOption B is wrong because the text explicitly states that baryon-baryon interactions have a decisive influence, contradicting the idea of independence.\n\nOption D is incorrect because it suggests a much larger value for r_\u03bcB/T than what the model predicts, and there's no mention of exponential increase with temperature."}, "18": {"documentation": {"title": "Gift Contagion in Online Groups: Evidence From WeChat Red Packets", "source": "Yuan Yuan, Tracy Liu, Chenhao Tan, Qian Chen, Alex Pentland, Jie Tang", "docs_id": "1906.09698", "section": ["econ.GN", "cs.HC", "cs.SI", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gift Contagion in Online Groups: Evidence From WeChat Red Packets. Gifts are important instruments for forming bonds in interpersonal relationships. Our study analyzes the phenomenon of gift contagion in online groups. Gift contagion encourages social bonds of prompting further gifts; it may also promote group interaction and solidarity. Using data on 36 million online red packet gifts on China's social site WeChat, we leverage a natural experimental design to identify the social contagion of gift giving in online groups. Our natural experiment is enabled by the randomization of the gift amount allocation algorithm on WeChat, which addresses the common challenge of causal identifications in observational data. Our study provides evidence of gift contagion: on average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours. Decomposing this effect, we find that it is mainly driven by the extensive margin -- more recipients are triggered to send red packets. Moreover, we find that this effect is stronger for \"luckiest draw\" recipients, suggesting the presence of a group norm regarding the next red packet sender. Finally, we investigate the moderating effects of group- and individual-level social network characteristics on gift contagion as well as the causal impact of receiving gifts on group network structure. Our study has implications for promoting group dynamics and designing marketing strategies for product adoption."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: According to the study on gift contagion in WeChat red packets, which of the following statements best describes the causal effect of receiving gifts on subsequent gift-giving behavior?\n\nA) Receiving a gift causes recipients to send exactly the same amount back to the group within 24 hours.\nB) On average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours.\nC) The gift contagion effect is primarily driven by recipients sending larger amounts rather than more recipients sending gifts.\nD) Receiving a gift has no significant impact on the likelihood of recipients sending gifts back to the group.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The study explicitly states that \"on average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours.\" This finding demonstrates a clear causal relationship between receiving gifts and subsequent gift-giving behavior.\n\nOption A is incorrect because the study does not claim that recipients send exactly the same amount back.\n\nOption C is incorrect because the study actually found that the effect is mainly driven by the extensive margin, meaning more recipients are triggered to send red packets, not larger amounts.\n\nOption D is incorrect as it contradicts the study's findings, which provide evidence of gift contagion and a positive impact on subsequent gift-giving behavior.\n\nThis question tests the student's ability to accurately interpret and recall specific quantitative findings from the research, as well as understand the concept of causal effects in the context of gift contagion."}, "19": {"documentation": {"title": "Love Thy Neighbor? Perceived Community Abidance and Private Compliance\n  to COVID-19 Norms in India", "source": "Upasak Das, Prasenjit Sarkhel, Sania Ashraf", "docs_id": "2010.12350", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Love Thy Neighbor? Perceived Community Abidance and Private Compliance\n  to COVID-19 Norms in India. Compliance with measures like social distancing, hand-washing and wearing masks have emerged as the dominant strategy to combat health risk from the COVID-19 pandemic. These behaviors are often argued to be pro-social, where one must incur private cost to benefit or protect others. Using self-reported data across India (n=934) through online survey, we assess if changes in perceived community compliance can predict changes in individual compliance behavior, controlling for the potential confounders. We observe statistically significant and positive relationship between the two, even after accounting for omitted variable bias, plausibly allowing us to view the results from a plausible causal lens. Further, we find subsequent lockdowns such as the ones imposed in India, have a detrimental effect on individual compliance though the gains from higher perceived community compliance seems to offset this loss. We also find that sensitization through community can be particularly effective for people with pre-existing co-morbidities. Our findings underscore the need for multi-level behavioral interventions involving local actors and community institutions to sustain private compliance during the pandemic."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best reflects the findings of the study on COVID-19 compliance behaviors in India?\n\nA) Individual compliance with COVID-19 measures is solely determined by government-imposed lockdowns.\n\nB) Perceived community compliance has a negative impact on individual compliance behaviors.\n\nC) The study found no significant relationship between perceived community compliance and individual compliance behaviors.\n\nD) Perceived community compliance positively influences individual compliance, potentially offsetting the negative effects of subsequent lockdowns.\n\nCorrect Answer: D\n\nExplanation: The study found a statistically significant and positive relationship between perceived community compliance and individual compliance behavior, even after controlling for potential confounders. Additionally, the research indicated that while subsequent lockdowns had a detrimental effect on individual compliance, the gains from higher perceived community compliance seemed to offset this loss. This finding supports answer D as the most accurate reflection of the study's results.\n\nOption A is incorrect because the study does not suggest that individual compliance is solely determined by lockdowns. In fact, it indicates that lockdowns may have a negative effect on compliance.\n\nOption B is incorrect as it contradicts the study's findings, which show a positive relationship between perceived community compliance and individual compliance.\n\nOption C is also incorrect, as the study explicitly states that there is a statistically significant and positive relationship between perceived community compliance and individual compliance."}, "20": {"documentation": {"title": "A Unified Approach for Drawdown (Drawup) of Time-Homogeneous Markov\n  Processes", "source": "David Landriault and Bin Li and Hongzhong Zhang", "docs_id": "1702.07786", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Unified Approach for Drawdown (Drawup) of Time-Homogeneous Markov\n  Processes. Drawdown (resp. drawup) of a stochastic process, also referred as the reflected process at its supremum (resp. infimum), has wide applications in many areas including financial risk management, actuarial mathematics and statistics. In this paper, for general time-homogeneous Markov processes, we study the joint law of the first passage time of the drawdown (resp. drawup) process, its overshoot, and the maximum of the underlying process at this first passage time. By using short-time pathwise analysis, under some mild regularity conditions, the joint law of the three drawdown quantities is shown to be the unique solution to an integral equation which is expressed in terms of fundamental two-sided exit quantities of the underlying process. Explicit forms for this joint law are found when the Markov process has only one-sided jumps or is a L\\'{e}vy process (possibly with two-sided jumps). The proposed methodology provides a unified approach to study various drawdown quantities for the general class of time-homogeneous Markov processes."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the unified approach for drawdown analysis of time-homogeneous Markov processes, which of the following statements is correct regarding the joint law of the first passage time of the drawdown process, its overshoot, and the maximum of the underlying process at this first passage time?\n\nA) It is always explicitly solvable for any time-homogeneous Markov process.\nB) It is expressed as a differential equation involving the fundamental two-sided exit quantities of the underlying process.\nC) It is the unique solution to an integral equation expressed in terms of fundamental two-sided exit quantities of the underlying process, under certain regularity conditions.\nD) It can only be determined for L\u00e9vy processes with one-sided jumps.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"under some mild regularity conditions, the joint law of the three drawdown quantities is shown to be the unique solution to an integral equation which is expressed in terms of fundamental two-sided exit quantities of the underlying process.\"\n\nOption A is incorrect because explicit forms are only mentioned for specific cases (one-sided jumps or L\u00e9vy processes), not for all time-homogeneous Markov processes.\n\nOption B is incorrect because the equation is described as an integral equation, not a differential equation.\n\nOption D is too restrictive. While explicit forms are mentioned for processes with one-sided jumps or L\u00e9vy processes (possibly with two-sided jumps), the general approach applies to a broader class of time-homogeneous Markov processes."}, "21": {"documentation": {"title": "Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios", "source": "Meysam Masoudi, Amin Azari, Emre Altug Yavuz, and Cicek Cavdar", "docs_id": "1711.00581", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios. IoT networks with grant-free radio access, like SigFox and LoRa, offer low-cost durable communications over unlicensed band. These networks are becoming more and more popular due to the ever-increasing need for ultra durable, in terms of battery lifetime, IoT networks. Most studies evaluate the system performance assuming single radio access technology deployment. In this paper, we study the impact of coexisting competing radio access technologies on the system performance. Considering $\\mathpzc K$ technologies, defined by time and frequency activity factors, bandwidth, and power, which share a set of radio resources, we derive closed-form expressions for the successful transmission probability, expected battery lifetime, and experienced delay as a function of distance to the serving access point. Our analytical model, which is validated by simulation results, provides a tool to evaluate the coexistence scenarios and analyze how introduction of a new coexisting technology may degrade the system performance in terms of success probability and battery lifetime. We further investigate solutions in which this destructive effect could be compensated, e.g., by densifying the network to a certain extent and utilizing joint reception."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In a scenario where multiple IoT technologies coexist and share radio resources, which of the following statements is most accurate regarding the impact on system performance and potential mitigation strategies?\n\nA) The introduction of new coexisting technologies always improves the overall success probability and battery lifetime of existing IoT networks.\n\nB) Network densification and joint reception are ineffective in compensating for the negative effects of coexisting technologies.\n\nC) The analytical model provides closed-form expressions for success probability, battery lifetime, and delay, but these are independent of the distance to the serving access point.\n\nD) Coexisting competing radio access technologies can degrade system performance, but strategies like network densification and joint reception may help mitigate these effects.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation clearly states that the introduction of a new coexisting technology may degrade the system performance in terms of success probability and battery lifetime. However, it also mentions that solutions such as densifying the network to a certain extent and utilizing joint reception could compensate for this destructive effect. \n\nAnswer A is incorrect because the text indicates that new coexisting technologies typically degrade, not improve, system performance. \n\nAnswer B is wrong because the document specifically mentions network densification and joint reception as potential solutions to mitigate the negative effects of coexistence.\n\nAnswer C is partially correct in mentioning the closed-form expressions, but it's incorrect in stating that these are independent of distance to the serving access point. The text explicitly states that these expressions are derived \"as a function of distance to the serving access point.\""}, "22": {"documentation": {"title": "Integrated Design of Unmanned Aerial Mobility Network: A Data-Driven\n  Risk-Averse Approach", "source": "Wenjuan Hou, Tao Fang, Zhi Pei, Qiao-Chu He", "docs_id": "2004.13000", "section": ["math.OC", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrated Design of Unmanned Aerial Mobility Network: A Data-Driven\n  Risk-Averse Approach. The real challenge in drone-logistics is to develop an economically-feasible Unmanned Aerial Mobility Network (UAMN). In this paper, we propose an integrated airport location (strategic decision) and routes planning (operational decision) optimization framework to minimize the total cost of the network, while guaranteeing flow constraints, capacity constraints, and electricity constraints. To facility expensive long-term infrastructure planning facing demand uncertainty, we develop a data-driven risk-averse two-stage stochastic optimization model based on the Wasserstein distance. We develop a reformulation technique which simplifies the worst-case expectation term in the original model, and obtain a fractable Min-Max solution procedure correspondingly. Using Lagrange multipliers, we successfully decompose decision variables and reduce the complexity of computation. To provide managerial insights, we design specific numerical examples. For example, we find that the optimal network configuration is affected by the \"pooling effects\" in channel capacities. A nice feature of our DRO framework is that the optimal network design is relatively robust under demand uncertainty. Interestingly, a candidate node without historical demand records can be chosen to locate an airport. We demonstrate the application of our model for a real medical resources transportation problem with our industry partner, collecting donated blood to a blood bank in Hangzhou, China."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the context of developing an economically-feasible Unmanned Aerial Mobility Network (UAMN), which of the following statements is NOT true regarding the proposed integrated optimization framework?\n\nA) It aims to minimize the total cost of the network while considering flow, capacity, and electricity constraints.\n\nB) It uses a data-driven risk-averse two-stage stochastic optimization model based on the Wasserstein distance.\n\nC) The framework guarantees optimal network performance regardless of demand uncertainty.\n\nD) It incorporates both strategic decisions (airport location) and operational decisions (routes planning).\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the framework does not guarantee optimal network performance regardless of demand uncertainty. Instead, the documentation states that the framework is designed to be \"relatively robust under demand uncertainty,\" which implies that it aims to mitigate the effects of uncertainty but does not eliminate them entirely.\n\nOption A is true as the documentation explicitly states that the framework aims to \"minimize the total cost of the network, while guaranteeing flow constraints, capacity constraints, and electricity constraints.\"\n\nOption B is correct because the paper mentions developing \"a data-driven risk-averse two-stage stochastic optimization model based on the Wasserstein distance.\"\n\nOption D is accurate as the framework is described as an \"integrated airport location (strategic decision) and routes planning (operational decision) optimization framework.\""}, "23": {"documentation": {"title": "Nested Hybrid Cylindrical Array Design and DoA Estimation for Massive\n  IoT Networks", "source": "Zhipeng Lin, Tiejun Lv, Wei Ni, J. Andrew Zhang, and Ren Ping Liu", "docs_id": "2007.10155", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nested Hybrid Cylindrical Array Design and DoA Estimation for Massive\n  IoT Networks. Reducing cost and power consumption while maintaining high network access capability is a key physical-layer requirement of massive Internet of Things (mIoT) networks. Deploying a hybrid array is a cost- and energy-efficient way to meet the requirement, but would penalize system degree of freedom (DoF) and channel estimation accuracy. This is because signals from multiple antennas are combined by a radio frequency (RF) network of the hybrid array. This paper presents a novel hybrid uniform circular cylindrical array (UCyA) for mIoT networks. We design a nested hybrid beamforming structure based on sparse array techniques and propose the corresponding channel estimation method based on the second-order channel statistics. As a result, only a small number of RF chains are required to preserve the DoF of the UCyA. We also propose a new tensor-based two-dimensional (2-D) direction-of-arrival (DoA) estimation algorithm tailored for the proposed hybrid array. The algorithm suppresses the noise components in all tensor modes and operates on the signal data model directly, hence improving estimation accuracy with an affordable computational complexity. Corroborated by a Cramer-Rao lower bound (CRLB) analysis, simulation results show that the proposed hybrid UCyA array and the DoA estimation algorithm can accurately estimate the 2-D DoAs of a large number of IoT devices."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which combination of features best describes the novel hybrid uniform circular cylindrical array (UCyA) design proposed in this paper for massive IoT networks?\n\nA) Sparse array techniques, first-order channel statistics, and tensor-based 1-D DoA estimation\nB) Dense array techniques, second-order channel statistics, and matrix-based 2-D DoA estimation\nC) Sparse array techniques, second-order channel statistics, and tensor-based 2-D DoA estimation\nD) Dense array techniques, first-order channel statistics, and matrix-based 1-D DoA estimation\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the paper describes a novel hybrid UCyA design that incorporates:\n\n1. Sparse array techniques: The design uses a nested hybrid beamforming structure based on sparse array techniques.\n2. Second-order channel statistics: The channel estimation method is based on the second-order channel statistics.\n3. Tensor-based 2-D DoA estimation: The paper proposes a new tensor-based two-dimensional (2-D) direction-of-arrival (DoA) estimation algorithm specifically tailored for the proposed hybrid array.\n\nOptions A, B, and D are incorrect because they contain one or more elements that contradict the information provided in the document. The use of first-order channel statistics, 1-D DoA estimation, or dense array techniques are not mentioned or are inconsistent with the described approach."}, "24": {"documentation": {"title": "A convolutional neural network for defect classification in Bragg\n  coherent X-ray diffraction", "source": "Bruce Lim, Ewen Bellec, Maxime Dupraz, Steven Leake, Andrea Resta,\n  Alessandro Coati, Michael Sprung, Ehud Almog, Eugen Rabkin, Tobias Sch\\\"ulli\n  and Marie-Ingrid Richard", "docs_id": "2106.16179", "section": ["cond-mat.mtrl-sci", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A convolutional neural network for defect classification in Bragg\n  coherent X-ray diffraction. Coherent diffraction imaging enables the imaging of individual defects, such as dislocations or stacking faults, in materials.These defects and their surrounding elastic strain fields have a critical influence on the macroscopic properties and functionality of materials. However, their identification in Bragg coherent diffraction imaging remains a challenge and requires significant data mining. The ability to identify defects from the diffraction pattern alone would be a significant advantage when targeting specific defect types and accelerates experiment design and execution. Here, we exploit a computational tool based on a three-dimensional (3D) parametric atomistic model and a convolutional neural network to predict dislocations in a crystal from its 3D coherent diffraction pattern. Simulated diffraction patterns from several thousands of relaxed atomistic configurations of nanocrystals are used to train the neural network and to predict the presence or absence of dislocations as well as their type(screw or edge). Our study paves the way for defect recognition in 3D coherent diffraction patterns for material science"}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of Bragg coherent X-ray diffraction imaging, which of the following statements best describes the primary innovation and potential impact of the convolutional neural network (CNN) approach discussed in the document?\n\nA) The CNN can directly image individual defects in materials without the need for diffraction patterns.\n\nB) The CNN allows for real-time identification of defects during the imaging process, eliminating the need for post-processing.\n\nC) The CNN can predict the presence and type of dislocations in a crystal based solely on its 3D coherent diffraction pattern, potentially accelerating experiment design and execution.\n\nD) The CNN replaces the need for atomistic models in simulating diffraction patterns for nanocrystals.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document describes a computational tool that combines a 3D parametric atomistic model with a convolutional neural network to predict dislocations in a crystal from its 3D coherent diffraction pattern alone. This approach has the potential to accelerate experiment design and execution by allowing researchers to target specific defect types without the need for extensive data mining of the actual images.\n\nAnswer A is incorrect because the CNN doesn't directly image the defects; it predicts them from diffraction patterns.\n\nAnswer B is incorrect because the method doesn't involve real-time identification during imaging, but rather analysis of the diffraction patterns.\n\nAnswer D is incorrect because the CNN doesn't replace atomistic models; in fact, the atomistic models are used to generate the simulated diffraction patterns that train the neural network."}, "25": {"documentation": {"title": "The Shapley Taylor Interaction Index", "source": "Kedar Dhamdhere and Ashish Agarwal and Mukund Sundararajan", "docs_id": "1902.05622", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Shapley Taylor Interaction Index. The attribution problem, that is the problem of attributing a model's prediction to its base features, is well-studied. We extend the notion of attribution to also apply to feature interactions. The Shapley value is a commonly used method to attribute a model's prediction to its base features. We propose a generalization of the Shapley value called Shapley-Taylor index that attributes the model's prediction to interactions of subsets of features up to some size k. The method is analogous to how the truncated Taylor Series decomposes the function value at a certain point using its derivatives at a different point. In fact, we show that the Shapley Taylor index is equal to the Taylor Series of the multilinear extension of the set-theoretic behavior of the model. We axiomatize this method using the standard Shapley axioms -- linearity, dummy, symmetry and efficiency -- and an additional axiom that we call the interaction distribution axiom. This new axiom explicitly characterizes how interactions are distributed for a class of functions that model pure interaction. We contrast the Shapley-Taylor index against the previously proposed Shapley Interaction index (cf. [9]) from the cooperative game theory literature. We also apply the Shapley Taylor index to three models and identify interesting qualitative insights."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: The Shapley-Taylor index is described as a generalization of the Shapley value. Which of the following statements best characterizes the key innovation of the Shapley-Taylor index in relation to feature attribution?\n\nA) It focuses solely on individual feature importance, disregarding interactions.\nB) It attributes the model's prediction to interactions of subsets of features up to a certain size k.\nC) It is based on a completely different set of axioms than the original Shapley value.\nD) It is specifically designed for linear models and cannot be applied to complex non-linear functions.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The Shapley-Taylor index extends the concept of attribution beyond individual features to include feature interactions. Specifically, it \"attributes the model's prediction to interactions of subsets of features up to some size k.\" This is a key innovation over the traditional Shapley value, which focuses on individual feature attribution.\n\nOption A is incorrect because the Shapley-Taylor index explicitly considers feature interactions, not just individual importance.\n\nOption C is incorrect because the Shapley-Taylor index is based on the standard Shapley axioms (linearity, dummy, symmetry, and efficiency) plus an additional axiom called the interaction distribution axiom. It doesn't use a completely different set of axioms.\n\nOption D is incorrect because there's no indication in the text that the Shapley-Taylor index is limited to linear models. In fact, the ability to capture complex interactions suggests it can be applied to non-linear functions as well."}, "26": {"documentation": {"title": "On-Chain Auctions with Deposits", "source": "Jan Christoph Schlegel, Akaki Mamageishvili", "docs_id": "2103.16681", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On-Chain Auctions with Deposits. Second-price auctions with deposits are frequently used in blockchain environments. An auction takes place on-chain: bidders deposit an amount that fully covers their bid (but possibly exceeds it) in a smart contract. The deposit is used as insurance against bidders not honoring their bid if they win. The deposit, but not the bid, is publicly observed during the bidding phase of the auction. The visibility of deposits can fundamentally change the strategic structure of the auction if bidding happens sequentially: Bidding is costly since deposit are costly to make. Thus, deposits can be used as a costly signal for a high valuation. This is the source of multiple inefficiencies: To engage in costly signalling, a bidder who bids first and has a high valuation will generally over-deposit in equilibrium, i.e.~deposit more than he will bid. If high valuations are likely there can, moreover, be entry deterrence through high deposits: a bidder who bids first can deter subsequent bidders from entering the auction. Pooling can happen in equilibrium, where bidders of different valuations deposit the same amount. The auction fails to allocate the item to the bidder with the highest valuation."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In a blockchain-based second-price auction with deposits, which of the following scenarios is most likely to occur when bidding happens sequentially and deposits are publicly visible?\n\nA) The auction always allocates the item to the bidder with the highest valuation, as deposits accurately reflect bidders' true valuations.\n\nB) Bidders consistently deposit exactly the amount they intend to bid, maximizing efficiency and transparency.\n\nC) A high-valuation bidder who bids first may over-deposit to signal their valuation, potentially deterring subsequent bidders from entering the auction.\n\nD) Deposits have no impact on bidding strategy, as bidders focus solely on their intended bid amounts.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation highlights several key points that support this answer:\n\n1. Deposits are publicly visible during the bidding phase, while actual bids are not.\n2. Making deposits is costly, so they can be used as a costly signal for high valuation.\n3. A bidder with a high valuation who bids first will generally over-deposit in equilibrium to engage in costly signaling.\n4. High deposits by an initial bidder can deter subsequent bidders from entering the auction, especially if high valuations are likely.\n\nOption A is incorrect because the documentation explicitly states that the auction can fail to allocate the item to the bidder with the highest valuation due to these strategic considerations.\n\nOption B is incorrect as it contradicts the concept of over-depositing for signaling purposes.\n\nOption D is incorrect because the visibility of deposits fundamentally changes the strategic structure of the auction, impacting bidding strategies significantly.\n\nThis question tests understanding of the complex strategic implications of visible deposits in sequential blockchain auctions."}, "27": {"documentation": {"title": "Volatility is rough", "source": "Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum", "docs_id": "1410.3394", "section": ["q-fin.ST", "q-fin.MF", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Volatility is rough. Estimating volatility from recent high frequency data, we revisit the question of the smoothness of the volatility process. Our main result is that log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable time scale. This leads us to adopt the fractional stochastic volatility (FSV) model of Comte and Renault. We call our model Rough FSV (RFSV) to underline that, in contrast to FSV, H<1/2. We demonstrate that our RFSV model is remarkably consistent with financial time series data; one application is that it enables us to obtain improved forecasts of realized volatility. Furthermore, we find that although volatility is not long memory in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it. This sheds light on why long memory of volatility has been widely accepted as a stylized fact. Finally, we provide a quantitative market microstructure-based foundation for our findings, relating the roughness of volatility to high frequency trading and order splitting."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: According to the Rough Fractional Stochastic Volatility (RFSV) model described in the text, which of the following statements is most accurate regarding the behavior of log-volatility and its implications?\n\nA) Log-volatility behaves as a fractional Brownian motion with Hurst exponent H > 1/2, indicating long-term dependence and smooth trajectories.\n\nB) The RFSV model suggests that volatility exhibits long memory, consistent with widely accepted stylized facts in financial time series.\n\nC) Log-volatility behaves as a fractional Brownian motion with Hurst exponent H \u2248 0.1, implying rougher trajectories and improved forecasting of realized volatility.\n\nD) The model demonstrates that high-frequency trading and order splitting lead to smoother volatility patterns in financial markets.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text states that \"log-volatility behaves essentially as a fractional Brownian motion with Hurst exponent H of order 0.1, at any reasonable time scale.\" This low Hurst exponent (H < 1/2) implies rougher trajectories, which is why the model is called \"Rough FSV.\" The document also mentions that this model enables improved forecasts of realized volatility.\n\nOption A is incorrect because it states H > 1/2, which contradicts the rough nature of the volatility described in the text.\n\nOption B is incorrect because the text actually challenges the notion of long memory in volatility. It states that \"although volatility is not long memory in the RFSV model, classical statistical procedures aiming at detecting volatility persistence tend to conclude the presence of long memory in data generated from it.\"\n\nOption D is incorrect because the text suggests that high-frequency trading and order splitting are related to the roughness of volatility, not to smoother patterns."}, "28": {"documentation": {"title": "Hamilton--Jacobi meet M\\\"obius", "source": "Alon E. Faraggi and Marco Matone", "docs_id": "1503.01286", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hamilton--Jacobi meet M\\\"obius. Adaptation of the Hamilton--Jacobi formalism to quantum mechanics leads to a cocycle condition, which is invariant under $D$--dimensional M\\\"obius transformations with Euclidean or Minkowski metrics. In this paper we aim to provide a pedagogical presentation of the proof of the M\\\"obius symmetry underlying the cocycle condition. The M\\\"obius symmetry implies energy quantization and undefinability of quantum trajectories, without assigning any prior interpretation to the wave function. As such, the Hamilton--Jacobi formalism, augmented with the global M\\\"obius symmetry, provides an alternative starting point, to the axiomatic probability interpretation of the wave function, for the formulation of quantum mechanics and the quantum spacetime. The M\\\"obius symmetry can only be implemented consistently if spatial space is compact, and correspondingly if there exist a finite ultraviolet length scale. Evidence for non--trivial space topology may exist in the cosmic microwave background radiation."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: The Hamilton-Jacobi formalism adapted to quantum mechanics leads to a cocycle condition with an underlying symmetry. What are the implications of this symmetry for quantum mechanics and spacetime, and what observational evidence might support this theory?\n\nA) The symmetry is invariant under 3-dimensional Lorentz transformations, implies wave function collapse, and is supported by gravitational wave observations.\n\nB) The symmetry is invariant under D-dimensional M\u00f6bius transformations, implies energy quantization and undefinability of quantum trajectories, and may be evidenced in the cosmic microwave background radiation.\n\nC) The symmetry is invariant under 4-dimensional conformal transformations, implies the existence of hidden variables, and is supported by dark matter observations.\n\nD) The symmetry is invariant under D-dimensional Poincar\u00e9 transformations, implies the many-worlds interpretation, and may be evidenced in large-scale structure formation.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the cocycle condition is invariant under D-dimensional M\u00f6bius transformations. It also mentions that this symmetry implies energy quantization and undefinability of quantum trajectories. Furthermore, the text suggests that evidence for the non-trivial space topology implied by this theory may exist in the cosmic microwave background radiation.\n\nAnswer A is incorrect because it mentions 3-dimensional Lorentz transformations and wave function collapse, which are not discussed in the given text.\n\nAnswer C is incorrect because it refers to 4-dimensional conformal transformations and hidden variables, which are not mentioned in the documentation.\n\nAnswer D is incorrect as it mentions Poincar\u00e9 transformations and the many-worlds interpretation, which are not part of the described theory."}, "29": {"documentation": {"title": "Nonparametric Estimation and Inference in Economic and Psychological\n  Experiments", "source": "Raffaello Seri, Samuele Centorrino, Michele Bernasconi", "docs_id": "1904.11156", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonparametric Estimation and Inference in Economic and Psychological\n  Experiments. The goal of this paper is to provide some tools for nonparametric estimation and inference in psychological and economic experiments. We consider an experimental framework in which each of $n$subjects provides $T$ responses to a vector of $T$ stimuli. We propose to estimate the unknown function $f$ linking stimuli to responses through a nonparametric sieve estimator. We give conditions for consistency when either $n$ or $T$ or both diverge. The rate of convergence depends upon the error covariance structure, that is allowed to differ across subjects. With these results we derive the optimal divergence rate of the dimension of the sieve basis with both $n$ and $T$. We provide guidance about the optimal balance between the number of subjects and questions in a laboratory experiment and argue that a large $n$is often better than a large $T$. We derive conditions for asymptotic normality of functionals of the estimator of $T$ and apply them to obtain the asymptotic distribution of the Wald test when the number of constraints under the null is finite and when it diverges along with other asymptotic parameters. Lastly, we investigate the previous properties when the conditional covariance matrix is replaced by an estimator."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of nonparametric estimation in economic and psychological experiments, which of the following statements is correct regarding the optimal balance between the number of subjects (n) and the number of questions (T)?\n\nA) A large T is always preferable to a large n for achieving better estimation results.\nB) The optimal balance between n and T depends solely on the dimension of the sieve basis.\nC) The paper argues that a large n is often better than a large T for estimation purposes.\nD) The number of subjects (n) and questions (T) should always be equal for optimal results.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states: \"We provide guidance about the optimal balance between the number of subjects and questions in a laboratory experiment and argue that a large n is often better than a large T.\"\n\nOption A is incorrect because the paper actually suggests the opposite, favoring a large n over a large T.\n\nOption B is incorrect because while the dimension of the sieve basis is important, it's not the sole factor determining the optimal balance between n and T. The error covariance structure and other factors also play a role.\n\nOption D is incorrect as there's no mention of n and T needing to be equal. In fact, the paper discusses scenarios where either n or T or both diverge, implying they can be different."}, "30": {"documentation": {"title": "On time reversal in photoacoustic tomography for tissue similar to water", "source": "Richard Kowar", "docs_id": "1308.0498", "section": ["math.AP", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On time reversal in photoacoustic tomography for tissue similar to water. This paper is concerned with time reversal in \\emph{photoacoustic tomography} (PAT) of dissipative media that are similar to water. Under an appropriate condition, it is shown that the time reversal method in \\cite{Wa11,AmBrGaWa11} based on the non-causal thermo-viscous wave equation can be used if the non-causal data is replaced by a \\emph{time shifted} set of causal data. We investigate a similar imaging functional for time reversal and an operator equation with the time reversal image as right hand side. If required, an enhanced image can be obtained by solving this operator equation. Although time reversal (for noise-free data) does not lead to the exact initial pressure function, the theoretical and numerical results of this paper show that regularized time reversal in dissipative media similar to water is a valuable method. We note that the presented time reversal method can be considered as an alternative to the causal approach in \\cite{KaSc13} and a similar operator equation may hold for their approach."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In photoacoustic tomography (PAT) of dissipative media similar to water, what modification to the time reversal method allows it to be used with the non-causal thermo-viscous wave equation?\n\nA) Using a frequency-domain approach instead of time-domain\nB) Applying a spatial filter to the recorded data\nC) Replacing the non-causal data with a time shifted set of causal data\nD) Implementing a machine learning algorithm to process the data\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"Under an appropriate condition, it is shown that the time reversal method [...] based on the non-causal thermo-viscous wave equation can be used if the non-causal data is replaced by a time shifted set of causal data.\" This modification allows the time reversal method to be applied in dissipative media similar to water, even when using the non-causal thermo-viscous wave equation.\n\nOption A is incorrect because the document doesn't mention using a frequency-domain approach. Option B is incorrect as there's no mention of applying a spatial filter. Option D is incorrect because machine learning is not discussed in the given text.\n\nThis question tests the student's understanding of the key modification proposed in the paper for applying time reversal in PAT for media similar to water."}, "31": {"documentation": {"title": "Detailed illustration of accuracy of presently used nuclear-mass models", "source": "Adam Sobiczewski, Yuri A. Litvinov, Michal Palczewski", "docs_id": "1903.00886", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detailed illustration of accuracy of presently used nuclear-mass models. The accuracy of description of measured nuclear masses by presently used nuclear-mass models is studied. Twelve models of various kinds are considered, eleven of the global character and one local model specially adapted to description of heavy nuclei. To decrease the number of nuclei over which the accuracy is averaged, the global region (Z,N >= 8) is divided into four subregions, in which the accuracy is studied separately. Still, to reach the best precision, the accuracy is finally investigated without any averaging, for each nucleus separately. The results are presented in a form of colored maps, large enough to be easily and accurately read. Besides the accuracy of the studied models, also their predictive power is considered. It is found that the accuracy of description of mass strongly depends on a nuclear-mass model and on the region of nuclei to which the model is applied. The best accuracy is obtained by the recent two Chinese models WS3+ and WS4+. Generally, no clear, strong correlation between the accuracy of description of already known masses by a given model and its predictive power for new masses is observed. Still, such correlation is found for separate models and in separate regions of nuclei. More often for the macroscopic-microscopic models than for the other approaches."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the relationship between the accuracy of a nuclear mass model in describing known masses and its predictive power for new masses, according to the study?\n\nA) There is a strong, consistent correlation between a model's accuracy for known masses and its predictive power across all types of models and nuclear regions.\n\nB) No correlation exists between a model's accuracy for known masses and its predictive power in any case.\n\nC) A clear correlation is observed only for macroscopic-microscopic models, but not for other approaches.\n\nD) While no strong overall correlation is observed, some correlation exists for individual models and in specific nuclear regions, with macroscopic-microscopic models showing this tendency more frequently.\n\nCorrect Answer: D\n\nExplanation: The question tests the understanding of a complex relationship described in the document. Option A is incorrect because the document explicitly states that no clear, strong correlation is observed overall. Option B is too extreme, as the document does mention some correlations in specific cases. Option C is partially correct but oversimplifies the findings. Option D accurately captures the nuanced conclusion presented in the document, noting that while there's no strong overall correlation, some correlation exists for individual models and regions, with macroscopic-microscopic models showing this tendency more often."}, "32": {"documentation": {"title": "Optimally coordinated traffic diversion by statistical physics", "source": "Tak Shing Tai and Chi Ho Yeung", "docs_id": "2103.14508", "section": ["physics.soc-ph", "cond-mat.stat-mech", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimally coordinated traffic diversion by statistical physics. Road accidents or maintenance often lead to the blockage of roads, causing severe traffic congestion. Diverted routes after road blockage are often decided individually and have no coordination. Here, we employ the cavity approach in statistical physics to obtain both analytical results and optimization algorithms to optimally divert and coordinate individual vehicle routes after road blockage. Depending on the number and the location of the blocked roads, we found that there can be a significant change in traveling path of individual vehicles, and a large increase in the average traveling distance and cost. Interestingly, traveling distance decreases but traveling cost increases for some instances of diverted traffic. By comparing networks with different topology and connectivity, we observe that the number of alternative routes play a crucial role in suppressing the increase in traveling cost after road blockage. We tested our algorithm using the England highway network and found that coordinated diversion can suppress the increase in traveling cost by as much as 66$\\%$ in the scenarios studied. These results reveal the advantages brought by the optimally coordinated traffic diversion after road blockage."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the relationship between traveling distance and traveling cost in optimally coordinated traffic diversion scenarios, as observed in the study?\n\nA) Traveling distance and cost always increase proportionally after road blockage.\nB) Traveling distance and cost always decrease proportionally after optimal diversion.\nC) In some instances, traveling distance decreases while traveling cost increases.\nD) Traveling distance and cost are always inversely related in diverted traffic scenarios.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states: \"Interestingly, traveling distance decreases but traveling cost increases for some instances of diverted traffic.\" This counterintuitive finding highlights the complexity of traffic diversion scenarios and the need for optimal coordination.\n\nOption A is incorrect because the study doesn't indicate that distance and cost always increase proportionally. Option B is false as the study doesn't suggest that both always decrease. Option D is incorrect because the relationship between distance and cost is not always inverse; in fact, the study points out an instance where they move in opposite directions.\n\nThis question tests the student's ability to carefully read and interpret scientific findings, particularly when they present counterintuitive or complex relationships."}, "33": {"documentation": {"title": "Safe Sample Screening for Robust Support Vector Machine", "source": "Zhou Zhai, Bin Gu, Xiang Li, Heng Huang", "docs_id": "1912.11217", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Safe Sample Screening for Robust Support Vector Machine. Robust support vector machine (RSVM) has been shown to perform remarkably well to improve the generalization performance of support vector machine under the noisy environment. Unfortunately, in order to handle the non-convexity induced by ramp loss in RSVM, existing RSVM solvers often adopt the DC programming framework which is computationally inefficient for running multiple outer loops. This hinders the application of RSVM to large-scale problems. Safe sample screening that allows for the exclusion of training samples prior to or early in the training process is an effective method to greatly reduce computational time. However, existing safe sample screening algorithms are limited to convex optimization problems while RSVM is a non-convex problem. To address this challenge, in this paper, we propose two safe sample screening rules for RSVM based on the framework of concave-convex procedure (CCCP). Specifically, we provide screening rule for the inner solver of CCCP and another rule for propagating screened samples between two successive solvers of CCCP. To the best of our knowledge, this is the first work of safe sample screening to a non-convex optimization problem. More importantly, we provide the security guarantee to our sample screening rules to RSVM. Experimental results on a variety of benchmark datasets verify that our safe sample screening rules can significantly reduce the computational time."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the main innovation and challenge addressed by the proposed safe sample screening rules for Robust Support Vector Machine (RSVM)?\n\nA) They introduce a new loss function to replace the ramp loss in RSVM, making it a convex optimization problem.\nB) They provide the first safe sample screening approach for a non-convex optimization problem, specifically tailored for RSVM using CCCP.\nC) They eliminate the need for DC programming in RSVM, making it computationally efficient for large-scale problems.\nD) They propose a new solver that completely avoids the non-convexity induced by ramp loss in RSVM.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The key innovation described in the document is that this work introduces the first safe sample screening rules for a non-convex optimization problem, specifically for Robust Support Vector Machine (RSVM). The authors address the challenge of applying safe sample screening to RSVM, which is non-convex due to the ramp loss, by developing two screening rules based on the concave-convex procedure (CCCP) framework.\n\nOption A is incorrect because the work doesn't introduce a new loss function to replace the ramp loss. Instead, it deals with the existing non-convexity.\n\nOption C is incorrect because the work doesn't eliminate DC programming. In fact, it uses the CCCP framework, which is a form of DC programming.\n\nOption D is incorrect because the work doesn't propose a new solver that avoids non-convexity. Rather, it introduces screening rules to make the existing non-convex problem more computationally efficient.\n\nThe correct answer highlights the novelty of applying safe sample screening to a non-convex problem and its specific application to RSVM using CCCP, which are the main contributions described in the document."}, "34": {"documentation": {"title": "The necessity and power of random, under-sampled experiments in biology", "source": "Brian Cleary and Aviv Regev", "docs_id": "2012.12961", "section": ["q-bio.QM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The necessity and power of random, under-sampled experiments in biology. A vast array of transformative technologies developed over the past decade has enabled measurement and perturbation at ever increasing scale, yet our understanding of many systems remains limited by experimental capacity. Overcoming this limitation is not simply a matter of reducing costs with existing approaches; for complex biological systems it will likely never be possible to comprehensively measure and perturb every combination of variables of interest. There is, however, a growing body of work - much of it foundational and precedent setting - that extracts a surprising amount of information from highly under sampled data. For a wide array of biological questions, especially the study of genetic interactions, approaches like these will be crucial to obtain a comprehensive understanding. Yet, there is no coherent framework that unifies these methods, provides a rigorous mathematical foundation to understand their limitations and capabilities, allows us to understand through a common lens their surprising successes, and suggests how we might crystalize the key concepts to transform experimental biology. Here, we review prior work on this topic - both the biology and the mathematical foundations of randomization and low dimensional inference - and propose a general framework to make data collection in a wide array of studies vastly more efficient using random experiments and composite experiments."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following best describes the main challenge and proposed solution in studying complex biological systems, as outlined in the text?\n\nA) The challenge is the high cost of experiments, and the solution is to simply reduce costs of existing approaches.\n\nB) The challenge is the inability to measure all variables, and the solution is to develop more comprehensive measurement technologies.\n\nC) The challenge is the limitation of experimental capacity, and the solution is to use random, under-sampled experiments and low dimensional inference.\n\nD) The challenge is the lack of a unified mathematical framework, and the solution is to focus solely on genetic interaction studies.\n\nCorrect Answer: C\n\nExplanation: The text emphasizes that for complex biological systems, it will likely never be possible to comprehensively measure and perturb every combination of variables of interest, even with advanced technologies. This represents the main challenge. The proposed solution is not simply about reducing costs (A) or developing more comprehensive technologies (B), but rather about extracting maximum information from highly under-sampled data using approaches like randomization and low dimensional inference. While the lack of a unified framework is mentioned (D), it's not presented as the main challenge, and the solution is not limited to genetic interaction studies. Option C correctly captures both the central challenge of limited experimental capacity and the proposed solution of using random, under-sampled experiments to overcome this limitation."}, "35": {"documentation": {"title": "The economics of minority language use: theory and empirical evidence\n  for a language game model", "source": "Stefan Sperlich and Jose-Ramon Uriarte", "docs_id": "1908.11604", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The economics of minority language use: theory and empirical evidence\n  for a language game model. Language and cultural diversity is a fundamental aspect of the present world. We study three modern multilingual societies -- the Basque Country, Ireland and Wales -- which are endowed with two, linguistically distant, official languages: $A$, spoken by all individuals, and $B$, spoken by a bilingual minority. In the three cases it is observed a decay in the use of minoritarian $B$, a sign of diversity loss. However, for the \"Council of Europe\" the key factor to avoid the shift of $B$ is its use in all domains. Thus, we investigate the language choices of the bilinguals by means of an evolutionary game theoretic model. We show that the language population dynamics has reached an evolutionary stable equilibrium where a fraction of bilinguals have shifted to speak $A$. Thus, this equilibrium captures the decline in the use of $B$. To test the theory we build empirical models that predict the use of $B$ for each proportion of bilinguals. We show that model-based predictions fit very well the observed use of Basque, Irish, and Welsh."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the evolutionary game theoretic model described for bilingual societies with languages A and B, what does the evolutionary stable equilibrium indicate about the use of the minority language B?\n\nA) All bilinguals have completely shifted to speaking only language A\nB) The use of language B is increasing among bilinguals\nC) A portion of bilinguals have shifted to speaking language A, leading to a decline in B's use\nD) The use of both languages A and B is perfectly balanced among bilinguals\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the model's key findings. Option A is incorrect because the model doesn't suggest a complete shift away from B. Option B contradicts the observed decay in B's use. Option D doesn't align with the described equilibrium. \n\nThe correct answer, C, accurately reflects the model's conclusion that \"the language population dynamics has reached an evolutionary stable equilibrium where a fraction of bilinguals have shifted to speak A. Thus, this equilibrium captures the decline in the use of B.\" This equilibrium state explains the observed decay in the use of the minority language B in the studied multilingual societies."}, "36": {"documentation": {"title": "Characterizing the Orbital and Dynamical State of the HD 82943 Planetary\n  System With Keck Radial Velocity Data", "source": "Xianyu Tan (1), Matthew J. Payne (2), Man Hoi Lee (3), Eric B. Ford\n  (4), Andrew W. Howard (5), John. A. Johnson (6), Geoff W. Marcy (7), Jason T.\n  Wright (8) ((1) U Arizona, (2) CfA, (3) HKU (4) U Florida, (5) U Hawaii, (6)\n  Caltech, (7) UC Berkeley, (8) Penn State)", "docs_id": "1306.0687", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterizing the Orbital and Dynamical State of the HD 82943 Planetary\n  System With Keck Radial Velocity Data. We present an updated analysis of radial velocity data of the HD 82943 planetary system based on 10 years of measurements obtained with the Keck telescope. Previous studies have shown that the HD 82943 system has two planets that are likely in 2:1 mean-motion resonance (MMR), with the orbital periods about 220 and 440 days (Lee et al. 2006). However, alternative fits that are qualitatively different have also been suggested, with two planets in a 1:1 resonance (Gozdziewski & Konacki 2006) or three planets in a Laplace 4:2:1 resonance (Beauge et al. 2008). Here we use \\c{hi}2 minimization combined with parameter grid search to investigate the orbital parameters and dynamical states of the qualitatively different types of fits, and we compare the results to those obtained with the differential evolution Markov chain Monte Carlo method. Our results support the coplanar 2:1 MMR configuration for the HD 82943 system, and show no evidence for either the 1:1 or 3-planet Laplace resonance fits. The inclination of the system with respect to the sky plane is well constrained at about 20(+4.9 -5.5) degree, and the system contains two planets with masses of about 4.78 MJ and 4.80 MJ (where MJ is the mass of Jupiter) and orbital periods of about 219 and 442 days for the inner and outer planet, respectively. The best fit is dynamically stable with both eccentricity-type resonant angles {\\theta}1 and {\\theta}2 librating around 0 degree."}, "answer": "Here's a multi-form question based on the given text:\n\nQuestion: The HD 82943 planetary system has been the subject of multiple studies with differing conclusions. Based on the most recent analysis using 10 years of Keck telescope data, which of the following best describes the current understanding of this system?\n\nA) Two planets in a 1:1 mean-motion resonance, as suggested by Gozdziewski & Konacki (2006)\n\nB) Three planets in a Laplace 4:2:1 resonance, as proposed by Beauge et al. (2008)\n\nC) Two planets in a 2:1 mean-motion resonance, with masses of about 4.78 MJ and 4.80 MJ, and orbital periods of approximately 219 and 442 days\n\nD) A single planet with a highly eccentric orbit, causing the appearance of multiple planets in the radial velocity data\n\nCorrect Answer: C\n\nExplanation: The question tests the student's ability to identify the most up-to-date and supported configuration of the HD 82943 planetary system based on the given information. Option C is correct because it accurately reflects the findings presented in the text, which states that the results support a coplanar 2:1 mean-motion resonance configuration with two planets having masses of about 4.78 MJ and 4.80 MJ, and orbital periods of about 219 and 442 days.\n\nOption A is incorrect because the text explicitly states that there is no evidence for the 1:1 resonance fit. Option B is also incorrect as the analysis shows no evidence for the 3-planet Laplace resonance. Option D is a distractors that is not mentioned in the text and does not align with the presented findings.\n\nThis question challenges students to carefully read and interpret scientific findings, distinguish between different proposed models, and identify the most current and well-supported conclusion based on the latest data analysis."}, "37": {"documentation": {"title": "The Impact of Nuclear Physics Uncertainties on Galactic Chemical\n  Evolution Predictions", "source": "Benoit C\\^ot\\'e, Pavel Denissenkov, Falk Herwig, Chris L. Fryer,\n  Krzysztof Belczynski, Nicole Vassh, Matthew R. Mumpower, Jonas Lippuner,\n  Marco Pignatari, Ashley J. Ruiter", "docs_id": "1911.00035", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Impact of Nuclear Physics Uncertainties on Galactic Chemical\n  Evolution Predictions. Modeling the evolution of the elements in the Milky Way is a multidisciplinary and challenging task. In addition to simulating the 13 billion years evolution of our Galaxy, chemical evolution simulations must keep track of the elements synthesized and ejected from every astrophysical site of interest (e.g., supernova, compact binary merger). The elemental abundances of such ejecta, which are a fundamental input for chemical evolution codes, are usually taken from theoretical nucleosynthesis calculations performed by the nuclear astrophysics community. Therefore, almost all chemical evolution predictions rely on the nuclear physics behind those calculations. In this proceedings, we highlight the impact of nuclear physics uncertainties on galactic chemical evolution predictions. We demonstrate that nuclear physics and galactic evolution uncertainties both have a significant impact on interpreting the origin of neutron-capture elements in our Solar System. Those results serve as a motivation to create and maintain collaborations between the fields of nuclear astrophysics and galaxy evolution."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the relationship between nuclear physics uncertainties and galactic chemical evolution predictions, as discussed in the Arxiv documentation?\n\nA) Nuclear physics uncertainties have a negligible impact on galactic chemical evolution predictions compared to astrophysical uncertainties.\n\nB) Galactic chemical evolution simulations are independent of nuclear physics calculations and focus solely on tracking element distribution over time.\n\nC) Nuclear physics uncertainties significantly affect predictions of neutron-capture element origins in the Solar System, highlighting the need for interdisciplinary collaboration.\n\nD) The impact of nuclear physics uncertainties on galactic chemical evolution is well-understood and has been fully accounted for in current models.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that \"nuclear physics and galactic evolution uncertainties both have a significant impact on interpreting the origin of neutron-capture elements in our Solar System.\" This underscores the importance of nuclear physics uncertainties in galactic chemical evolution predictions. Furthermore, the text emphasizes that this interdependence serves as a motivation for collaboration between nuclear astrophysics and galaxy evolution fields.\n\nOption A is incorrect because the text does not suggest that nuclear physics uncertainties have a negligible impact compared to astrophysical uncertainties. In fact, it states that both have significant impacts.\n\nOption B is incorrect as the documentation clearly indicates that chemical evolution simulations rely on nucleosynthesis calculations from the nuclear astrophysics community.\n\nOption D is incorrect because the text implies that the impact of nuclear physics uncertainties is still an active area of research and not fully accounted for, hence the call for continued collaboration between fields."}, "38": {"documentation": {"title": "Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using\n  Recurrent Neural Networks", "source": "Asim Mazin, Mohamed Elkourdi, and Richard D. Gitlin", "docs_id": "1809.01096", "section": ["eess.SP", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using\n  Recurrent Neural Networks. Millimeter wave (mmWave) is a key technology to support high data rate demands for 5G applications. Highly directional transmissions are crucial at these frequencies to compensate for high isotropic pathloss. This reliance on di- rectional beamforming, however, makes the cell discovery (cell search) challenging since both base station (gNB) and user equipment (UE) jointly perform a search over angular space to locate potential beams to initiate communication. In the cell discovery phase, sequential beam sweeping is performed through the angular coverage region in order to transmit synchronization signals. The sweeping pattern can either be a linear rotation or a hopping pattern that makes use of additional information. This paper proposes beam sweeping pattern prediction, based on the dynamic distribution of user traffic, using a form of recurrent neural networks (RNNs) called Gated Recurrent Unit (GRU). The spatial distribution of users is inferred from data in call detail records (CDRs) of the cellular network. Results show that the users spatial distribution and their approximate location (direction) can be accurately predicted based on CDRs data using GRU, which is then used to calculate the sweeping pattern in the angular domain during cell search."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of mmWave 5G technology, which of the following statements best describes the innovative approach proposed in the paper to improve beam sweeping efficiency during cell discovery?\n\nA) Using machine learning to predict user equipment locations based on historical connection data\nB) Implementing a fixed linear rotation pattern for beam sweeping to cover all angles uniformly\nC) Utilizing Gated Recurrent Units (GRU) to predict beam sweeping patterns based on Call Detail Records (CDRs)\nD) Increasing the power of synchronization signals to compensate for high isotropic pathloss\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. The paper proposes using Gated Recurrent Units (GRU), a type of recurrent neural network, to predict beam sweeping patterns based on Call Detail Records (CDRs). This approach aims to optimize the beam sweeping process by leveraging historical user traffic data to predict the spatial distribution of users.\n\nAnswer A is close but not specific enough. While the approach does use machine learning to predict user locations, it specifically mentions using GRUs and CDRs, which are not mentioned in this option.\n\nAnswer B is incorrect as the paper explicitly states that the sweeping pattern can be either a linear rotation or a hopping pattern. The proposed method aims to improve upon simple linear rotation by using predictive techniques.\n\nAnswer D is incorrect because while increasing power might help with pathloss, it's not the approach discussed in the paper for improving beam sweeping efficiency.\n\nThe key innovation lies in using GRUs to analyze CDRs and predict user distributions, which then informs a more efficient beam sweeping strategy during cell search."}, "39": {"documentation": {"title": "Spin 1/2^+, spin 3/2^+ and transition magnetic moments of low lying and\n  charmed baryons", "source": "Neetika Sharma, Harleen Dahiya, P.K. Chatley and Manmohan Gupta", "docs_id": "1003.4338", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin 1/2^+, spin 3/2^+ and transition magnetic moments of low lying and\n  charmed baryons. Magnetic moments of the low lying and charmed spin 1/2^+ and spin 3/2^+ baryons have been calculated in the SU(4) chiral constituent quark model (\\chiCQM) by including the contribution from c \\bar c fluctuations. Explicit calculations have been carried out for the contribution coming from the valence quarks, \"quark sea\" polarizations and their orbital angular momentum. The implications of such a model have also been studied for magnetic moments of the low lying spin 3/2^+ \\to 1/2^+ and 1/2^+ \\to 1/2^+ transitions as well as the transitions involving charmed baryons. We are able to achieve an excellent agreement with data for the case of low lying spin 1/2^+ baryons and spin 3/2^+ baryons. For the spin 1/2^+ and spin 3/2^+ charmed baryon magnetic moments, our results are consistent with the predictions of the QCD sum rules, Light Cone sum rules and Spectral sum rules. For the cases where \"light\" quarks dominate in the valence structure, the sea and orbital contributions are found to be fairly significant however, they cancel in the right direction to give the correct magnitude of the total magnetic moment. On the other hand, when there is an excess of \"heavy\" quarks, the contribution of the \"quark sea\" is almost negligible. The effects of configuration mixing and quark masses have also been investigated."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the SU(4) chiral constituent quark model (\u03c7CQM) described, which of the following statements is true regarding the magnetic moments of charmed baryons with an excess of \"heavy\" quarks?\n\nA) The sea and orbital contributions are significant and tend to cancel each other out.\nB) The \"quark sea\" contribution is negligible compared to the valence quark contribution.\nC) The sea and orbital contributions are significant and tend to reinforce each other.\nD) The valence quark contribution is negligible compared to the \"quark sea\" contribution.\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the model's predictions for different types of baryons. The correct answer is B because the documentation states: \"On the other hand, when there is an excess of \"heavy\" quarks, the contribution of the \"quark sea\" is almost negligible.\" This directly supports the statement in option B.\n\nOption A is incorrect because it describes the behavior for baryons dominated by \"light\" quarks, not those with an excess of \"heavy\" quarks.\n\nOption C is incorrect as it contradicts the information given about both light and heavy quark dominated baryons.\n\nOption D is incorrect because it inverts the relationship between valence and sea contributions for heavy quark baryons described in the text.\n\nThis question requires careful reading and interpretation of the given information, making it suitable for a challenging exam question."}, "40": {"documentation": {"title": "The Earth Regeneration Effect of Solar Neutrinos: a Numerical Treatment\n  with Three Active Neutrino Flavors", "source": "Jai Sam Kim and Kyungsoo Lee", "docs_id": "hep-ph/0006137", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Earth Regeneration Effect of Solar Neutrinos: a Numerical Treatment\n  with Three Active Neutrino Flavors. We introduce an integrated algorithm for full scale investigation of the earth regeneration effect of solar neutrinos with all three active neutrinos. We illustrate that the earth effect on low energy solar neutrinos ($E_\\nu < 20$ MeV) causes large variations in the survival probabilities for large values of the mixing angle $\\theta_{12}$ for a given value of $\\theta_{13}$. But it is maximal for small values of $\\theta_{13}$ and diminishes for large values at a given value of $\\theta_{12}$. The nadir angle dependence is sensitive to the value of $E'\\equiv E/\\Delta_{12}$. As far as low energy solar neutrinos are concerned we notice that the earth effect is more pronounced for core penetration. We confirm that the earth effect leaves the SMA region almost intact regardless of $\\theta_{13}$ and that it is significant for high energy $^8$B and $hep$ neutrinos in the LMA region. We also show that for large values of $\\theta_{13}$, the earth effect is significant for high energy neutrinos ($E'\\gtrsim 10^9$) and it can be large at any nadir angle."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements accurately describes the Earth regeneration effect on solar neutrinos according to the numerical treatment with three active neutrino flavors?\n\nA) The Earth effect is most pronounced for high energy solar neutrinos (E > 20 MeV) and is independent of the mixing angle \u03b813.\n\nB) For low energy solar neutrinos (E < 20 MeV), the Earth effect causes large variations in survival probabilities for small values of \u03b812, regardless of \u03b813.\n\nC) The Earth effect is maximal for small values of \u03b813 and diminishes for large values at a given value of \u03b812, with more pronounced effects for core-penetrating neutrinos in the low energy range.\n\nD) The nadir angle dependence of the Earth effect is primarily determined by the value of \u03b812 and is insensitive to the ratio E/\u039412.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects several key points from the documentation:\n\n1. The Earth effect is maximal for small values of \u03b813 and diminishes for large values at a given value of \u03b812.\n2. For low energy solar neutrinos (E < 20 MeV), the Earth effect is more pronounced for core penetration.\n3. The effect causes large variations in survival probabilities, but this is for large values of \u03b812, not small values as stated in option B.\n\nOption A is incorrect because the document states that the Earth effect is significant for low energy solar neutrinos (E < 20 MeV), not just high energy ones, and it is not independent of \u03b813.\n\nOption B is incorrect because it mistakenly states that the large variations occur for small values of \u03b812, when the opposite is true.\n\nOption D is incorrect because the nadir angle dependence is actually sensitive to the value of E' \u2261 E/\u039412, not primarily determined by \u03b812."}, "41": {"documentation": {"title": "The Self-Accelerating Universe with Vectors in Massive Gravity", "source": "Kazuya Koyama (ICG, Portsmouth), Gustavo Niz (ICG, Portsmouth) and\n  Gianmassimo Tasinato (ICG, Portsmouth)", "docs_id": "1110.2618", "section": ["hep-th", "astro-ph.CO", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Self-Accelerating Universe with Vectors in Massive Gravity. We explore the possibility of realising self-accelerated expansion of the Universe taking into account the vector components of a massive graviton. The effective action in the decoupling limit contains an infinite number of terms, once the vector degrees of freedom are included. These can be re-summed in physically interesting situations, which result in non-polynomial couplings between the scalar and vector modes. We show there are self-accelerating background solutions for this effective action, with the possibility of having a non-trivial profile for the vector fields. We then study fluctuations around these solutions and show that there is always a ghost, if a background vector field is present. When the background vector field is switched off, the ghost can be avoided, at the price of entering into a strong coupling regime, in which the vector fluctuations have vanishing kinetic terms. Finally we show that the inclusion of a bare cosmological constant does not change the previous conclusions and it does not lead to a ghost mode in the absence of a background vector field."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of self-accelerating Universe models with vectors in massive gravity, which of the following statements is correct regarding the presence of a ghost in the fluctuations around background solutions?\n\nA) A ghost is always present when there is a non-trivial background vector field, but can be avoided by introducing a bare cosmological constant.\n\nB) A ghost is always absent when there is a non-trivial background vector field, but appears when the background vector field is switched off.\n\nC) A ghost is always present when there is a non-trivial background vector field, but can be avoided by switching off the background vector field at the cost of entering a strong coupling regime.\n\nD) A ghost is never present in the fluctuations, regardless of the presence or absence of a background vector field or a bare cosmological constant.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the complex relationship between background vector fields, ghosts, and strong coupling in the context of self-accelerating Universe models with massive gravity. The correct answer is C because:\n\n1. The documentation states that \"there is always a ghost, if a background vector field is present.\"\n2. It also mentions that \"When the background vector field is switched off, the ghost can be avoided, at the price of entering into a strong coupling regime, in which the vector fluctuations have vanishing kinetic terms.\"\n3. The inclusion of a bare cosmological constant does not change these conclusions.\n\nOption A is incorrect because the bare cosmological constant does not help avoid the ghost. Option B is the opposite of what the documentation states. Option D is incorrect as it contradicts the presence of ghosts under certain conditions as described in the text."}, "42": {"documentation": {"title": "Leveraging Knowledge Bases And Parallel Annotations For Music Genre\n  Translation", "source": "Elena V. Epure, Anis Khlif, Romain Hennequin", "docs_id": "1907.08698", "section": ["cs.SD", "cs.IR", "cs.LG", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leveraging Knowledge Bases And Parallel Annotations For Music Genre\n  Translation. Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following best describes the hybrid translation approach proposed in the paper for music genre translation?\n\nA) A method that combines knowledge-based taxonomy mapping with maximum likelihood logistic regression\nB) A technique that uses only statistical translation modeled with maximum likelihood logistic regression\nC) An approach that exclusively relies on knowledge-based translation modeled as taxonomy mapping\nD) A model using maximum a posteriori logistic regression with priors given by the knowledge-based translation\n\nCorrect Answer: D\n\nExplanation: The hybrid translation approach described in the paper is modeled using maximum a posteriori logistic regression with priors given by the knowledge-based translation. This method combines elements of both knowledge-based and statistical approaches, making it distinct from options A, B, and C. \n\nOption A is incorrect because it mentions maximum likelihood instead of maximum a posteriori. Option B is incorrect as it only describes the statistical translation approach without the knowledge-based component. Option C is incorrect as it only describes the knowledge-based approach without the statistical component. \n\nThe hybrid approach (option D) is designed to leverage both the knowledge representation from taxonomy mapping and the statistical power of logistic regression, making it the most effective solution according to the paper, especially in cases where only few common annotations exist between source and target tag systems."}, "43": {"documentation": {"title": "Towards simulating a realistic data analysis with an optimised angular\n  power spectrum of spectroscopic galaxy surveys", "source": "Guglielmo Faggioli, Konstantinos Tanidis and Stefano Camera", "docs_id": "2009.08473", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards simulating a realistic data analysis with an optimised angular\n  power spectrum of spectroscopic galaxy surveys. The angular power spectrum is a natural tool to analyse the observed galaxy number count fluctuations. In a standard analysis, the angular galaxy distribution is sliced into concentric redshift bins and all correlations of its harmonic coefficients between bin pairs are considered---a procedure referred to as `tomography'. However, the unparalleled quality of data from oncoming spectroscopic galaxy surveys for cosmology will render this method computationally unfeasible, given the increasing number of bins. Here, we put to test against synthetic data a novel method proposed in a previous study to save computational time. According to this method, the whole galaxy redshift distribution is subdivided into thick bins, neglecting the cross-bin correlations among them; each of the thick bin is, however, further subdivided into thinner bins, considering in this case all the cross-bin correlations. We create a simulated data set that we then analyse in a Bayesian framework. We confirm that the newly proposed method saves computational time and gives results that surpass those of the standard approach."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of analyzing the angular power spectrum of spectroscopic galaxy surveys, what is the primary advantage of the novel method described in the passage compared to the standard tomographic approach?\n\nA) It increases the accuracy of cosmological parameter estimation\nB) It reduces the number of redshift bins required for analysis\nC) It improves the resolution of the angular galaxy distribution\nD) It decreases computational time while maintaining result quality\n\nCorrect Answer: D\n\nExplanation: The passage states that the novel method \"saves computational time and gives results that surpass those of the standard approach.\" This directly corresponds to option D. The method achieves this by subdividing the galaxy redshift distribution into thick bins and neglecting cross-bin correlations between these thick bins, while still considering cross-bin correlations within subdivisions of each thick bin. This approach is designed to address the computational challenges posed by the increasing number of bins in high-quality data from upcoming spectroscopic galaxy surveys, without sacrificing the quality of results.\n\nOption A is incorrect because while the results are said to surpass the standard approach, there's no specific mention of improved accuracy in cosmological parameter estimation. Option B is incorrect because the method actually uses both thick and thin bins, rather than reducing the overall number of bins. Option C is not supported by the passage, which doesn't discuss improving the resolution of the angular galaxy distribution."}, "44": {"documentation": {"title": "A Differential Model of the Complex Cell", "source": "Miles Hansard and Radu Horaud", "docs_id": "2012.09027", "section": ["q-bio.NC", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Differential Model of the Complex Cell. The receptive fields of simple cells in the visual cortex can be understood as linear filters. These filters can be modelled by Gabor functions, or by Gaussian derivatives. Gabor functions can also be combined in an `energy model' of the complex cell response. This paper proposes an alternative model of the complex cell, based on Gaussian derivatives. It is most important to account for the insensitivity of the complex response to small shifts of the image. The new model uses a linear combination of the first few derivative filters, at a single position, to approximate the first derivative filter, at a series of adjacent positions. The maximum response, over all positions, gives a signal that is insensitive to small shifts of the image. This model, unlike previous approaches, is based on the scale space theory of visual processing. In particular, the complex cell is built from filters that respond to the \\twod\\ differential structure of the image. The computational aspects of the new model are studied in one and two dimensions, using the steerability of the Gaussian derivatives. The response of the model to basic images, such as edges and gratings, is derived formally. The response to natural images is also evaluated, using statistical measures of shift insensitivity. The relevance of the new model to the cortical image representation is discussed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation of the proposed complex cell model in comparison to traditional energy models?\n\nA) It uses Gabor functions instead of Gaussian derivatives to model simple cell receptive fields.\nB) It combines multiple Gaussian derivative filters at different positions to achieve shift insensitivity.\nC) It uses a linear combination of derivative filters at a single position to approximate filters at adjacent positions.\nD) It relies solely on the first derivative filter to model complex cell responses.\n\nCorrect Answer: C\n\nExplanation: The key innovation of the proposed complex cell model is that it uses a linear combination of the first few derivative filters at a single position to approximate the first derivative filter at a series of adjacent positions. This approach differs from traditional energy models that typically use Gabor functions or combine filters at multiple positions.\n\nOption A is incorrect because the model uses Gaussian derivatives, not Gabor functions. \n\nOption B is incorrect because the model specifically uses filters at a single position, not multiple positions.\n\nOption D is incorrect because the model uses a combination of derivative filters, not just the first derivative filter.\n\nThe correct answer (C) captures the essence of the new model's approach, which aims to achieve shift insensitivity through a novel combination of filters at a single position, aligning with the scale space theory of visual processing."}, "45": {"documentation": {"title": "Charge Conjugation and Pairing in a model Cu$_{5}$O$_{4}$ Cluster", "source": "Michele Cini, Adalberto Balzarotti, and Gianluca Stefanucci", "docs_id": "cond-mat/9811116", "section": ["cond-mat.supr-con", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Charge Conjugation and Pairing in a model Cu$_{5}$O$_{4}$ Cluster. Highly-symmetric three-band Hubbard Cu-O clusters have peculiar properties when the hole number is such that they admit W=0 hole pairs. These are two-hole eigenstates of the on-site Hubbard repulsion with eigenvalue 0, get bound by correlation effects when dressed by the interaction with the background, and cause superconducting flux quantization. We study the Cu$_{5}$O$_{4}$ cluster by exact diagonalization and show that bound electron pairs of $^{1}$% B$_{2}$ symmetry are obtained at an appropriate filling, and quantize flux like the hole pairs. The basic mechanism for pairing in this model is the second-order exchange diagram, and an approximate charge conjugation symmetry holds between electron and hole pairs. Further, the flux quantization property requires that the W=0 pairs of $d$ symmetry have $s$ symmetry couterparts, still with W=0; the former are due to a spin fluctuation, while the latter arise from a charge fluctuation mechanism. The simultaneous existence of both is an essential property of our model and is required for any model of superconducting $d$ pairs."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the Cu\u2085O\u2084 cluster model described, which of the following statements is NOT correct regarding the properties of W=0 pairs and flux quantization?\n\nA) W=0 hole pairs are two-hole eigenstates of the on-site Hubbard repulsion with eigenvalue 0.\n\nB) Both electron pairs of \u00b9B\u2082 symmetry and hole pairs exhibit superconducting flux quantization.\n\nC) The model requires only d-symmetry W=0 pairs for flux quantization, as they arise from spin fluctuations.\n\nD) An approximate charge conjugation symmetry exists between electron and hole pairs in this model.\n\nCorrect Answer: C\n\nExplanation: Option C is incorrect and thus the correct answer to this question asking for which statement is NOT correct. The documentation states that for flux quantization, the model requires both d-symmetry and s-symmetry W=0 pairs. The d-symmetry pairs arise from spin fluctuations, while the s-symmetry pairs come from charge fluctuation mechanisms. The simultaneous existence of both types is described as an essential property of the model.\n\nOptions A, B, and D are all correct according to the given information:\nA) The document explicitly states this property of W=0 hole pairs.\nB) The text mentions that both hole pairs and bound electron pairs of \u00b9B\u2082 symmetry quantize flux.\nD) The document mentions an approximate charge conjugation symmetry between electron and hole pairs."}, "46": {"documentation": {"title": "Reduced Bias for respondent driven sampling: accounting for non-uniform\n  edge sampling probabilities in people who inject drugs in Mauritius", "source": "Miles Q. Ott, Krista J. Gile, Matthew T. Harrison, Lisa G. Johnston,\n  Joseph W. Hogan", "docs_id": "1712.09149", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reduced Bias for respondent driven sampling: accounting for non-uniform\n  edge sampling probabilities in people who inject drugs in Mauritius. People who inject drugs are an important population to study in order to reduce transmission of blood-borne illnesses including HIV and Hepatitis. In this paper we estimate the HIV and Hepatitis C prevalence among people who inject drugs, as well as the proportion of people who inject drugs who are female in Mauritius. Respondent driven sampling (RDS), a widely adopted link-tracing sampling design used to collect samples from hard-to-reach human populations, was used to collect this sample. The random walk approximation underlying many common RDS estimators assumes that each social relation (edge) in the underlying social network has an equal probability of being traced in the collection of the sample. This assumption does not hold in practice. We show that certain RDS estimators are sensitive to the violation of this assumption. In order to address this limitation in current methodology, and the impact it may have on prevalence estimates, we present a new method for improving RDS prevalence estimators using estimated edge inclusion probabilities, and apply this to data from Mauritius."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Respondent Driven Sampling (RDS) for studying people who inject drugs in Mauritius, which of the following statements most accurately describes the limitation of current RDS methodology and the proposed solution?\n\nA) The random walk approximation assumes unequal edge sampling probabilities, and the new method introduces bias to correct for this.\n\nB) The random walk approximation assumes equal edge sampling probabilities, but this assumption holds true in practice, so no new method is needed.\n\nC) The random walk approximation assumes equal edge sampling probabilities, which is violated in practice, and the new method uses estimated edge inclusion probabilities to improve prevalence estimators.\n\nD) The random walk approximation assumes non-uniform edge sampling, and the new method simplifies this to assume equal probabilities for all edges.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that the random walk approximation underlying many common RDS estimators assumes that each social relation (edge) in the underlying social network has an equal probability of being traced in the collection of the sample. However, it also mentions that this assumption does not hold in practice. To address this limitation, the authors present a new method for improving RDS prevalence estimators using estimated edge inclusion probabilities. This directly corresponds to option C, which accurately summarizes both the problem with the current methodology and the proposed solution.\n\nOption A is incorrect because it misrepresents the assumption of the random walk approximation. Option B is wrong because it incorrectly states that the assumption holds true in practice. Option D is incorrect as it reverses the problem and the solution, misrepresenting both the current methodology and the proposed improvement."}, "47": {"documentation": {"title": "Color superconductivity and charge neutrality in Yukawa theory", "source": "Mark G. Alford, Kamal Pangeni and Andreas Windisch", "docs_id": "1712.02407", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Color superconductivity and charge neutrality in Yukawa theory. It is generally believed that when Cooper pairing occurs between two different species of fermions, their Fermi surfaces become locked together so that the resultant state remains 'neutral', with equal number densities of the two species, even when subjected to a chemical potential that couples to the difference in number densities. This belief is based on mean-field calculations in models with a zero-range interaction, where the anomalous self-energy is independent of energy and momentum. Following up on an early report of a deviation from neutrality in a Dyson-Schwinger calculation of color-flavor-locked quark matter, we investigate the neutrality of a two-species condensate using a Yukawa model which has a finite-range interaction. In a mean field calculation we obtain the full energy-momentum dependence of the self energy and find that the energy dependence leads to a population imbalance in the Cooper-paired phase when it is stressed by a species-dependent chemical potential. This gives some support to the suggestion that the color-flavor-locked phase of quark matter might not be an insulator."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a two-species fermion system with Cooper pairing, what factor in the Yukawa model calculation leads to a deviation from the expected charge neutrality when a species-dependent chemical potential is applied?\n\nA) The use of a zero-range interaction\nB) The energy independence of the anomalous self-energy\nC) The momentum dependence of the self-energy\nD) The energy dependence of the self-energy\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the key findings from the Yukawa model study described in the document. The correct answer is D because the document explicitly states that \"the energy dependence leads to a population imbalance in the Cooper-paired phase when it is stressed by a species-dependent chemical potential.\"\n\nOption A is incorrect because the document mentions that the traditional belief of neutrality is based on models with zero-range interaction, but the Yukawa model used here has a finite-range interaction.\n\nOption B is incorrect because energy independence of the anomalous self-energy is associated with the traditional mean-field calculations that predict neutrality, not the deviation from it.\n\nOption C, while partially true as the full energy-momentum dependence of the self-energy was obtained, is not specifically identified as the cause of the population imbalance. The document emphasizes the energy dependence as the key factor.\n\nThis question challenges students to carefully interpret the results of the study and understand the specific factor that leads to the unexpected behavior in the Cooper-paired system."}, "48": {"documentation": {"title": "Duality in a hyperbolic interaction model integrable even in a strong\n  confinement: Multi-soliton solutions and field theory", "source": "Aritra Kumar Gon, Manas Kulkarni", "docs_id": "1904.06709", "section": ["nlin.SI", "cond-mat.stat-mech", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Duality in a hyperbolic interaction model integrable even in a strong\n  confinement: Multi-soliton solutions and field theory. Models that remain integrable even in confining potentials are extremely rare and almost non-existent. Here, we consider a one-dimensional hyperbolic interaction model, which we call as the Hyperbolic Calogero (HC) model. This is classically integrable even in confining potentials (which have box-like shapes). We present a first-order formulation of the HC model in an external confining potential. Using the rich property of duality, we find multi-soliton solutions of this confined integrable model. Absence of solitons correspond to the equilibrium solution of the model. We demonstrate the dynamics of multi-soliton solutions via brute-force numerical simulations. We studied the physics of soliton collisions and quenches using numerical simulations. We have examined the motion of dual complex variables and found an analytic expression for the time period in a certain limit. We give the field theory description of this model and find the background solution (absence of solitons) analytically in the large-N limit. Analytical expressions of soliton solutions have been obtained in the absence of external confining potential. Our work is of importance to understand the general features of trapped interacting particles that remain classically integrable and can be of relevance to the collective behaviour of trapped cold atomic gases as well."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: The Hyperbolic Calogero (HC) model is unique because:\n\nA) It is classically integrable only in the absence of confining potentials\nB) It exhibits soliton solutions only in strong confinement\nC) It remains classically integrable even in confining potentials with box-like shapes\nD) It lacks duality properties in its first-order formulation\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The Hyperbolic Calogero (HC) model is described as being \"classically integrable even in confining potentials (which have box-like shapes).\" This is highlighted as a rare and significant property of the model. \n\nOption A is incorrect because the model is integrable even with confining potentials, not only in their absence. \n\nOption B is incorrect because the model exhibits multi-soliton solutions both with and without confinement, not only in strong confinement. \n\nOption D is incorrect because the documentation mentions using \"the rich property of duality\" to find multi-soliton solutions, indicating that duality properties are present in the model's formulation.\n\nThis question tests the understanding of the unique properties of the HC model and its significance in the field of integrable systems with confinement."}, "49": {"documentation": {"title": "Dynamic Structural Impact of the COVID-19 Outbreak on the Stock Market\n  and the Exchange Rate: A Cross-country Analysis Among BRICS Nations", "source": "Rupam Bhattacharyya, Sheo Rama, Atul Kumar, Indrajit Banerjee", "docs_id": "2102.05554", "section": ["econ.GN", "q-fin.EC", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Structural Impact of the COVID-19 Outbreak on the Stock Market\n  and the Exchange Rate: A Cross-country Analysis Among BRICS Nations. COVID-19 has impacted the economy of almost every country in the world. Of particular interest are the responses of the economic indicators of developing nations (such as BRICS) to the COVID-19 shock. As an extension to our earlier work on the dynamic associations of pandemic growth, exchange rate, and stock market indices in the context of India, we look at the same question with respect to the BRICS nations. We use structural variable autoregression (SVAR) to identify the dynamic underlying associations across the normalized growth measurements of the COVID-19 cumulative case, recovery, and death counts, and those of the exchange rate, and stock market indices, using data over 203 days (March 12 - September 30, 2020). Using impulse response analyses, the COVID-19 shock to the growth of exchange rate was seen to persist for around 10+ days, and that for stock exchange was seen to be around 15 days. The models capture the contemporaneous nature of these shocks and the subsequent responses, potentially guiding to inform policy decisions at a national level. Further, causal inference-based analyses would allow us to infer relationships that are stronger than mere associations."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the SVAR analysis of BRICS nations' economic indicators during the COVID-19 pandemic, which of the following statements is most accurate regarding the persistence of COVID-19 shocks on economic variables?\n\nA) The COVID-19 shock to stock exchange growth persisted for around 10 days, while the shock to exchange rate growth lasted about 15 days.\n\nB) Both the exchange rate and stock market shocks showed identical persistence of approximately 12-13 days.\n\nC) The COVID-19 shock to exchange rate growth persisted for around 10+ days, while the shock to stock exchange growth lasted about 15 days.\n\nD) The study found no significant persistence of COVID-19 shocks on either the exchange rate or stock market growth.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the documentation, the impulse response analyses showed that \"the COVID-19 shock to the growth of exchange rate was seen to persist for around 10+ days, and that for stock exchange was seen to be around 15 days.\" This directly corresponds to the statement in option C.\n\nOption A is incorrect because it reverses the persistence periods for the exchange rate and stock exchange.\n\nOption B is incorrect as it suggests equal persistence for both variables, which is not supported by the given information.\n\nOption D is incorrect because the study did find significant persistence of COVID-19 shocks on both the exchange rate and stock market growth, contrary to this statement.\n\nThis question tests the student's ability to carefully read and interpret research findings, distinguishing between similar but crucially different statements about economic impacts."}, "50": {"documentation": {"title": "Optimising Stochastic Routing for Taxi Fleets with Model Enhanced\n  Reinforcement Learning", "source": "Shen Ren, Qianxiao Li, Liye Zhang, Zheng Qin and Bo Yang", "docs_id": "2010.11738", "section": ["cs.LG", "nlin.AO", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimising Stochastic Routing for Taxi Fleets with Model Enhanced\n  Reinforcement Learning. The future of mobility-as-a-Service (Maas)should embrace an integrated system of ride-hailing, street-hailing and ride-sharing with optimised intelligent vehicle routing in response to a real-time, stochastic demand pattern. We aim to optimise routing policies for a large fleet of vehicles for street-hailing services, given a stochastic demand pattern in small to medium-sized road networks. A model-based dispatch algorithm, a high performance model-free reinforcement learning based algorithm and a novel hybrid algorithm combining the benefits of both the top-down approach and the model-free reinforcement learning have been proposed to route the \\emph{vacant} vehicles. We design our reinforcement learning based routing algorithm using proximal policy optimisation and combined intrinsic and extrinsic rewards to strike a balance between exploration and exploitation. Using a large-scale agent-based microscopic simulation platform to evaluate our proposed algorithms, our model-free reinforcement learning and hybrid algorithm show excellent performance on both artificial road network and community-based Singapore road network with empirical demands, and our hybrid algorithm can significantly accelerate the model-free learner in the process of learning."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of optimizing routing policies for a large fleet of vehicles in street-hailing services, which of the following statements best describes the novel hybrid algorithm mentioned in the research?\n\nA) It combines model-based dispatch and proximal policy optimization techniques\nB) It integrates ride-hailing and ride-sharing services with street-hailing\nC) It merges the benefits of a top-down approach with model-free reinforcement learning\nD) It uses only intrinsic rewards to balance exploration and exploitation\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document explicitly states that the researchers proposed \"a novel hybrid algorithm combining the benefits of both the top-down approach and the model-free reinforcement learning.\" This hybrid approach aims to leverage the strengths of both methods to optimize routing policies for vacant vehicles in street-hailing services.\n\nOption A is incorrect because while the research does mention a model-based dispatch algorithm and uses proximal policy optimization in their reinforcement learning approach, the hybrid algorithm specifically combines top-down and model-free reinforcement learning methods.\n\nOption B is incorrect as it describes a general concept of integrated mobility services rather than the specific hybrid algorithm mentioned in the research.\n\nOption D is incorrect because the research mentions using \"combined intrinsic and extrinsic rewards\" in their reinforcement learning approach, not just intrinsic rewards.\n\nThis question tests the reader's ability to identify and understand the key innovations presented in the research, particularly the novel hybrid algorithm that combines different approaches to vehicle routing optimization."}, "51": {"documentation": {"title": "A simple method for measuring inequality", "source": "Thitithep Sitthiyot and Kanyarat Holasut", "docs_id": "2112.15284", "section": ["econ.GN", "q-fin.EC", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A simple method for measuring inequality. To simultaneously overcome the limitation of the Gini index in that it is less sensitive to inequality at the tails of income distribution and the limitation of the inter-decile ratios that ignore inequality in the middle of income distribution, an inequality index is introduced. It comprises three indicators, namely, the Gini index, the income share held by the top 10%, and the income share held by the bottom 10%. The data from the World Bank database and the Organization for Economic Co-operation and Development Income Distribution Database between 2005 and 2015 are used to demonstrate how the inequality index works. The results show that it can distinguish income inequality among countries that share the same Gini index but have different income gaps between the top 10% and the bottom 10%. It could also distinguish income inequality among countries that have the same ratio of income share held by the top 10% to income share held by the bottom 10% but differ in the values of the Gini index. In addition, the inequality index could capture the dynamics where the Gini index of a country is stable over time but the ratio of income share of the top 10% to income share of the bottom 10% is increasing. Furthermore, the inequality index could be applied to other scientific disciplines as a measure of statistical heterogeneity and for size distributions of any non-negative quantities."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A new inequality index is proposed to address limitations of existing measures. Which of the following statements about this index is NOT correct?\n\nA) It combines the Gini index with income shares of the top and bottom 10%\nB) It can differentiate between countries with the same Gini index but different top-to-bottom income ratios\nC) It is exclusively designed for measuring income inequality in economics\nD) It can capture changes in inequality even when the Gini index remains stable\n\nCorrect Answer: C\n\nExplanation:\nA is correct as the index comprises three indicators: the Gini index, income share of the top 10%, and income share of the bottom 10%.\nB is correct as the index can distinguish between countries with the same Gini but different income gaps between top and bottom 10%.\nC is incorrect because the document states that the index could be applied to other scientific disciplines as a measure of statistical heterogeneity.\nD is correct as the index can capture dynamics where the Gini is stable but the ratio of top 10% to bottom 10% income shares is changing.\n\nThe question tests understanding of the index's composition, capabilities, and potential applications beyond economics."}, "52": {"documentation": {"title": "Computational Tools for Evaluating Phylogenetic and Hierarchical\n  Clustering Trees", "source": "John Chakerian and Susan Holmes", "docs_id": "1006.1015", "section": ["stat.AP", "q-bio.PE", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computational Tools for Evaluating Phylogenetic and Hierarchical\n  Clustering Trees. Inferential summaries of tree estimates are useful in the setting of evolutionary biology, where phylogenetic trees have been built from DNA data since the 1960's. In bioinformatics, psychometrics and data mining, hierarchical clustering techniques output the same mathematical objects, and practitioners have similar questions about the stability and `generalizability' of these summaries. This paper provides an implementation of the geometric distance between trees developed by Billera, Holmes and Vogtmann (2001) [BHV] equally applicable to phylogenetic trees and hieirarchical clustering trees, and shows some of the applications in statistical inference for which this distance can be useful. In particular, since BHV have shown that the space of trees is negatively curved (a CAT(0) space), a natural representation of a collection of trees is a tree. We compare this representation to the Euclidean approximations of treespace made available through Multidimensional Scaling of the matrix of distances between trees. We also provide applications of the distances between trees to hierarchical clustering trees constructed from microarrays. Our method gives a new way of evaluating the influence both of certain columns (positions, variables or genes) and of certain rows (whether species, observations or arrays)."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the relationship between the Billera-Holmes-Vogtmann (BHV) distance and its applications in tree analysis?\n\nA) The BHV distance is primarily used for phylogenetic trees in evolutionary biology and cannot be applied to hierarchical clustering trees in other fields.\n\nB) The BHV distance allows for Euclidean approximations of treespace, which is the preferred method for representing collections of trees.\n\nC) The BHV distance enables the representation of a collection of trees as a tree itself, due to the negatively curved (CAT(0)) nature of treespace.\n\nD) The BHV distance is solely used for comparing the influence of rows in hierarchical clustering trees constructed from microarrays.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"since BHV have shown that the space of trees is negatively curved (a CAT(0) space), a natural representation of a collection of trees is a tree.\" This directly supports the statement in option C.\n\nOption A is incorrect because the document explicitly mentions that the BHV distance is \"equally applicable to phylogenetic trees and hierarchical clustering trees.\"\n\nOption B is incorrect because while the document does mention Euclidean approximations of treespace through Multidimensional Scaling, it presents this as a comparison to the tree representation, not as the preferred method.\n\nOption D is too narrow in scope. While the document does mention using distances between trees to evaluate the influence of rows and columns in hierarchical clustering trees from microarrays, this is just one application and not the sole use of the BHV distance."}, "53": {"documentation": {"title": "Construction of Type-II Backlund Transformation for the mKdV Hierarchy", "source": "J.F. Gomes, A.L. Retore and A.H. Zimerman", "docs_id": "1505.01024", "section": ["nlin.SI", "hep-th", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Construction of Type-II Backlund Transformation for the mKdV Hierarchy. From an algebraic construction of the mKdV hierarchy we observe that the space component of the Lax operator play a role of an universal algebraic object. This fact induces the universality of a gauge transformation that relates two field configurations of a given member of the hierarchy. Such gauge transformation generates the Backlund transformation (BT). In this paper we propose a systematic construction of Backlund Transformation for the entire mKdV hierarchy form the known Type-II BT of the sinh-Gordon theory. We explicitly construct the BT of the first few integrable models associated to positive and negative grade-time evolutions. Solutions of these transformations for several cases describing the transition from vacuum-vacuum and the vacuum to one-soliton solutions which determines the value for the auxiliary field and the the Backlund parameter respectively, independently of the model. The same follows for the scattering of two one-soliton solutions. The resultant delay is determined by a condition independent of the model considered."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements accurately describes the construction and properties of the B\u00e4cklund Transformation (BT) for the mKdV hierarchy as presented in the Arxiv documentation?\n\nA) The BT is derived from the time component of the Lax operator and is specific to each member of the mKdV hierarchy.\n\nB) The construction of the BT is based on the Type-I BT of the sine-Gordon theory and applies only to positive grade-time evolutions.\n\nC) The space component of the Lax operator serves as a universal algebraic object, leading to a gauge transformation that generates the BT for the entire mKdV hierarchy.\n\nD) The B\u00e4cklund parameter and auxiliary field values are model-dependent and vary for different types of solutions within the hierarchy.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that the space component of the Lax operator plays a role as a universal algebraic object, which leads to the universality of a gauge transformation that generates the B\u00e4cklund Transformation for the entire mKdV hierarchy. \n\nAnswer A is incorrect because it mentions the time component instead of the space component of the Lax operator and suggests specificity rather than universality.\n\nAnswer B is wrong on two counts: it mentions Type-I BT of the sine-Gordon theory, whereas the document refers to Type-II BT of the sinh-Gordon theory, and it incorrectly limits the application to positive grade-time evolutions, while the document mentions both positive and negative grade-time evolutions.\n\nAnswer D is incorrect because the document states that the B\u00e4cklund parameter and auxiliary field values are determined independently of the model, contradicting the statement in this option."}, "54": {"documentation": {"title": "Nucleon Polarisabilities from Compton Scattering on the Deuteron", "source": "Harald W. Griesshammer (TU M\"unchen), Gautam Rupak (TRIUMF)", "docs_id": "nucl-th/0012096", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nucleon Polarisabilities from Compton Scattering on the Deuteron. An analytic calculation of the differential cross section for elastic Compton scattering on the deuteron at photon energies \\omega in the range of 25-50 MeV is presented to next-to-next-to-leading order, i.e. to an accuracy of \\sim 3%. The calculation is model-independent and performed in the low energy nuclear Effective Field Theory without dynamical pions. The iso-scalar, scalar electric and magnetic nucleon polarisabilities \\alpha_0 and \\beta_0 enter as free parameters with a theoretical uncertainty of about 20%. Using data at $\\omega_{Lab}=49 MeV$ we find $\\alpha_0=8.4\\pm 3.0(exp)\\pm 1.7(theor)$, $\\beta_0=8.9\\pm 3.9(exp)\\pm 1.8(theor)$, each in units of $10^{-4} fm^3$. With the experimental constraint for the iso-scalar Baldin sum rule, $\\alpha_0=7.2\\pm 2.1(exp)\\pm 1.6(theor)$, $\\beta_0=6.9\\mp 2.1(exp)\\mp 1.6(theor)$. A more accurate result can be achieved by: (i) better experimental data, and (ii) a higher order theoretical calculation including contributions from a couple of so far undetermined four-nucleon-two-photon operators."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: A researcher is analyzing the results of an experiment on elastic Compton scattering on the deuteron at photon energies between 25-50 MeV. The calculation was performed to next-to-next-to-leading order in a low energy nuclear Effective Field Theory without dynamical pions. Given the following information:\n\n1. The calculation has an accuracy of ~3%\n2. The iso-scalar, scalar electric and magnetic nucleon polarisabilities \u03b1_0 and \u03b2_0 have a theoretical uncertainty of about 20%\n3. Using data at \u03c9_Lab = 49 MeV, \u03b1_0 = 8.4 \u00b1 3.0(exp) \u00b1 1.7(theor) and \u03b2_0 = 8.9 \u00b1 3.9(exp) \u00b1 1.8(theor), each in units of 10^-4 fm^3\n4. With the experimental constraint for the iso-scalar Baldin sum rule, \u03b1_0 = 7.2 \u00b1 2.1(exp) \u00b1 1.6(theor) and \u03b2_0 = 6.9 \u2213 2.1(exp) \u2213 1.6(theor)\n\nWhich of the following statements is most accurate regarding potential improvements to this result?\n\nA) Increasing the photon energy range to 20-60 MeV would significantly improve the accuracy of the calculation.\nB) Reducing the theoretical uncertainty of \u03b1_0 and \u03b2_0 to 10% would be sufficient to achieve a more accurate result.\nC) Obtaining better experimental data and performing a higher order theoretical calculation including contributions from undetermined four-nucleon-two-photon operators would lead to a more accurate result.\nD) Increasing the accuracy of the calculation to next-to-next-to-next-to-leading order would be sufficient to significantly improve the results.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage explicitly states that \"A more accurate result can be achieved by: (i) better experimental data, and (ii) a higher order theoretical calculation including contributions from a couple of so far undetermined four-nucleon-two-photon operators.\" This directly corresponds to option C.\n\nOption A is incorrect because the given photon energy range (25-50 MeV) is not mentioned as a limiting factor, and there's no indication that expanding it would improve accuracy.\n\nOption B is incorrect because while reducing theoretical uncertainty might help, it's not mentioned as a primary method for improving accuracy. Moreover, the 20% uncertainty is for the polarisabilities, not the overall calculation (which has ~3% accuracy).\n\nOption D is incorrect because while increasing the order of the calculation might help, it's not specifically mentioned. The passage suggests including contributions from undetermined operators, which is more specific than simply increasing the order of calculation."}, "55": {"documentation": {"title": "Lagrangian Neural Networks", "source": "Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David\n  Spergel, Shirley Ho", "docs_id": "2003.04630", "section": ["cs.LG", "math.DS", "physics.comp-ph", "physics.data-an", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lagrangian Neural Networks. Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the advantage of Lagrangian Neural Networks (LNNs) over Hamiltonian-based models in physics simulations?\n\nA) LNNs can only model systems with known canonical coordinates\nB) LNNs require less computational power than Hamiltonian models\nC) LNNs can parameterize arbitrary Lagrangians without requiring canonical coordinates\nD) LNNs are specifically designed for modeling relativistic particles\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key advantage of Lagrangian Neural Networks (LNNs) over Hamiltonian-based models is that they can parameterize arbitrary Lagrangians using neural networks without requiring canonical coordinates. This is explicitly stated in the passage: \"In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute.\"\n\nOption A is incorrect because LNNs are specifically useful when canonical coordinates are unknown or difficult to compute, not just for systems with known canonical coordinates.\n\nOption B is not mentioned in the passage and is not a distinguishing feature of LNNs.\n\nOption D is too specific. While the passage mentions that LNNs can model relativistic particles, this is just one example of their capabilities, not their primary advantage over Hamiltonian models.\n\nThe correct answer highlights the flexibility and broader applicability of LNNs in physics simulations, especially for systems where traditional Hamiltonian approaches might struggle."}, "56": {"documentation": {"title": "Response of an oscillatory delay differential equation to a periodic\n  stimulus", "source": "Daniel C. De Souza and Michael C. Mackey", "docs_id": "1805.05434", "section": ["math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Response of an oscillatory delay differential equation to a periodic\n  stimulus. Periodic hematological diseases such as cyclical neutropenia or cyclical thrombocytopenia, with their characteristic oscillations of circulating neutrophils or platelets, may pose grave problems for patients. Likewise, periodically administered chemotherapy has the unintended side effect of establishing periodic fluctuations in circulating white cells, red cell precursors and/or platelets. These fluctuations, either spontaneous or induced, often have serious consequences for the patient (e.g. neutropenia, anemia, or thrombocytopenia respectively) which exogenously administered cytokines can partially correct. The question of when and how to administer these drugs is a difficult one for clinicians and not easily answered. In this paper we use a simple model consisting of a delay differential equation with a piecewise linear nonlinearity, that has a periodic solution, to model the effect of a periodic disease or periodic chemotherapy. We then examine the response of this toy model to both single and periodic perturbations, meant to mimic the drug administration, as a function of the drug dose and the duration and frequency of its administration to best determine how to avoid side effects."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: A clinician is treating a patient with cyclical neutropenia using a cytokine therapy. The patient's neutrophil count follows a periodic oscillation. The clinician wants to determine the optimal timing and dosage of the cytokine to minimize side effects. Which of the following approaches best aligns with the methodology described in the paper?\n\nA) Use a linear regression model to predict neutrophil counts and administer a fixed dose at regular intervals.\n\nB) Employ a delay differential equation model with a piecewise linear nonlinearity to simulate the periodic neutrophil fluctuations and test various perturbation scenarios.\n\nC) Rely solely on the patient's symptoms to guide the timing and dosage of cytokine administration.\n\nD) Apply a simple harmonic oscillator model to predict neutrophil counts and administer the cytokine at the peak of each cycle.\n\nCorrect Answer: B\n\nExplanation: The correct approach aligns with the methodology described in the paper, which uses \"a simple model consisting of a delay differential equation with a piecewise linear nonlinearity, that has a periodic solution, to model the effect of a periodic disease.\" The paper then examines \"the response of this toy model to both single and periodic perturbations, meant to mimic the drug administration, as a function of the drug dose and the duration and frequency of its administration to best determine how to avoid side effects.\" This approach allows for a more sophisticated analysis of the periodic system and the effects of interventions, compared to the other options which are either too simplistic (A and D) or lack a quantitative basis (C)."}, "57": {"documentation": {"title": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification", "source": "Chenkai Xu, Hongwei Lin, Xuansu Fang", "docs_id": "2006.11119", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification. In this paper, we propose a novel stock index model, namely the manifold feature(MF) index, to reflect the overall price activity of the entire stock market. Based on the theory of manifold learning, the researched stock dataset is assumed to be a low-dimensional manifold embedded in a higher-dimensional Euclidean space. After data preprocessing, its manifold structure and discrete Laplace-Beltrami operator(LBO) matrix are constructed. We propose a high-dimensional data feature detection method to detect feature points on the eigenvectors of LBO, and the stocks corresponding to these feature points are considered as the constituent stocks of the MF index. Finally, the MF index is generated by a weighted formula using the price and market capitalization of these constituents. The stock market studied in this research is the Shanghai Stock Exchange(SSE). We propose four metrics to compare the MF index series and the SSE index series (SSE 50, SSE 100, SSE 150, SSE 180 and SSE 380). From the perspective of data approximation, the results demonstrate that our indexes are closer to the stock market than the SSE index series. From the perspective of risk premium, MF indexes have higher stability and lower risk."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the methodology and advantages of the Manifold Feature (MF) index as presented in the paper?\n\nA) The MF index uses principal component analysis to select constituent stocks and outperforms traditional indices in terms of market representation.\n\nB) The MF index is based on neural networks and provides better risk-adjusted returns compared to the SSE index series.\n\nC) The MF index utilizes manifold learning theory to detect feature points on eigenvectors of the Laplace-Beltrami operator, resulting in an index that more closely approximates the stock market with higher stability and lower risk.\n\nD) The MF index employs cluster analysis to identify representative stocks and shows improved liquidity compared to existing market indices.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key aspects of the Manifold Feature (MF) index as described in the paper. The MF index is based on manifold learning theory, which assumes the stock dataset is a low-dimensional manifold embedded in a higher-dimensional space. The method uses the discrete Laplace-Beltrami operator (LBO) matrix and detects feature points on its eigenvectors to select constituent stocks. The paper states that the resulting MF index more closely approximates the stock market than the SSE index series, and from a risk premium perspective, it demonstrates higher stability and lower risk.\n\nOptions A, B, and D are incorrect as they mention techniques (principal component analysis, neural networks, and cluster analysis) that are not described in the given text as being part of the MF index methodology. Additionally, these options do not accurately reflect the stated advantages of the MF index as presented in the paper."}, "58": {"documentation": {"title": "Fluctuations, Response, and Resonances in a Simple Atmospheric Model", "source": "Andrey Gritsun and Valerio Lucarini", "docs_id": "1604.04386", "section": ["physics.ao-ph", "cond-mat.stat-mech", "nlin.CD", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fluctuations, Response, and Resonances in a Simple Atmospheric Model. We study the response of a simple quasi-geostrophic barotropic model of the atmosphere to various classes of perturbations affecting its forcing and its dissipation using the formalism of the Ruelle response theory. We investigate the geometry of such perturbations by constructing the covariant Lyapunov vectors of the unperturbed system and discover in one specific case - orographic forcing - a substantial projection of the forcing onto the stable directions of the flow. This results into a resonant response shaped as a Rossby-like wave that has no resemblance to the unforced variability in the same range of spatial and temporal scales. Such a climatic surprise corresponds to a violation of the fluctuation-dissipation theorem, in agreement with the basic tenets of nonequilibrium statistical mechanics. The resonance can be attributed to a specific group of rarely visited unstable periodic orbits of the unperturbed system. Our results reinforce the idea of using basic methods of nonequilibrium statistical mechanics and high-dimensional chaotic dynamical systems to approach the problem of understanding climate dynamics."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of a quasi-geostrophic barotropic model of the atmosphere, what key phenomenon was observed when applying orographic forcing, and what does this imply about the system's behavior?\n\nA) The forcing projected strongly onto the unstable directions, resulting in chaotic behavior that resembled the unforced variability.\n\nB) The forcing projected substantially onto the stable directions, leading to a resonant response in the form of a Rossby-like wave that differed from the unforced variability.\n\nC) The forcing had minimal impact on both stable and unstable directions, resulting in no significant change to the system's behavior.\n\nD) The forcing amplified existing variability patterns, reinforcing the validity of the fluctuation-dissipation theorem in this context.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that for orographic forcing, there was \"a substantial projection of the forcing onto the stable directions of the flow.\" This resulted in \"a resonant response shaped as a Rossby-like wave that has no resemblance to the unforced variability in the same range of spatial and temporal scales.\" This outcome is described as a \"climatic surprise\" and corresponds to a violation of the fluctuation-dissipation theorem, which is consistent with nonequilibrium statistical mechanics principles. The resonance is attributed to specific unstable periodic orbits of the unperturbed system. This finding emphasizes the importance of using nonequilibrium statistical mechanics and high-dimensional chaotic dynamical systems approaches in understanding climate dynamics."}, "59": {"documentation": {"title": "Measurements of long-range near-side angular correlations in\n  $\\sqrt{s_{\\text{NN}}}=5$TeV proton-lead collisions in the forward region", "source": "LHCb collaboration: R. Aaij, C. Abell\\'an Beteta, B. Adeva, M.\n  Adinolfi, A. Affolder, Z. Ajaltouni, S. Akar, J. Albrecht, F. Alessio, M.\n  Alexander, S. Ali, G. Alkhazov, P. Alvarez Cartelle, A.A. Alves Jr, S. Amato,\n  S. Amerio, Y. Amhis, L. An, L. Anderlini, J. Anderson, G. Andreassi, M.\n  Andreotti, J.E. Andrews, R.B. Appleby, O. Aquines Gutierrez, F. Archilli, P.\n  d'Argent, A. Artamonov, M. Artuso, E. Aslanides, G. Auriemma, M. Baalouch, S.\n  Bachmann, J.J. Back, A. Badalov, C. Baesso, W. Baldini, R.J. Barlow, C.\n  Barschel, S. Barsuk, W. Barter, V. Batozskaya, V. Battista, A. Bay, L.\n  Beaucourt, J. Beddow, F. Bedeschi, I. Bediaga, L.J. Bel, V. Bellee, N.\n  Belloli, I. Belyaev, E. Ben-Haim, G. Bencivenni, S. Benson, J. Benton, A.\n  Berezhnoy, R. Bernet, A. Bertolin, M.-O. Bettler, M. van Beuzekom, A. Bien,\n  S. Bifani, P. Billoir, T. Bird, A. Birnkraut, A. Bizzeti, T. Blake, F. Blanc,\n  J. Blouw, S. Blusk, V. Bocci, A. Bondar, N. Bondar, W. Bonivento, S. Borghi,\n  M. Borsato, T.J.V. Bowcock, E. Bowen, C. Bozzi, S. Braun, M. Britsch, T.\n  Britton, J. Brodzicka, N.H. Brook, E. Buchanan, C. Burr, A. Bursche, J.\n  Buytaert, S. Cadeddu, R. Calabrese, M. Calvi, M. Calvo Gomez, P. Campana, D.\n  Campora Perez, L. Capriotti, A. Carbone, G. Carboni, R. Cardinale, A.\n  Cardini, P. Carniti, L. Carson, K. Carvalho Akiba, G. Casse, L. Cassina, L.\n  Castillo Garcia, M. Cattaneo, Ch. Cauet, G. Cavallero, R. Cenci, M. Charles,\n  Ph. Charpentier, M. Chefdeville, S. Chen, S.-F. Cheung, N. Chiapolini, M.\n  Chrzaszcz, X. Cid Vidal, G. Ciezarek, P.E.L. Clarke, M. Clemencic, H.V.\n  Cliff, J. Closier, V. Coco, J. Cogan, E. Cogneras, V. Cogoni, L. Cojocariu,\n  G. Collazuol, P. Collins, A. Comerma-Montells, A. Contu, A. Cook, M. Coombes,\n  S. Coquereau, G. Corti, M. Corvo, B. Couturier, G.A. Cowan, D.C. Craik, A.\n  Crocombe, M. Cruz Torres, S. Cunliffe, R. Currie, C. D'Ambrosio, E.\n  Dall'Occo, J. Dalseno, P.N.Y. David, A. Davis, O. De Aguiar Francisco, K. De\n  Bruyn, S. De Capua, M. De Cian, J.M. De Miranda, L. De Paula, P. De Simone,\n  C.-T. Dean, D. Decamp, M. Deckenhoff, L. Del Buono, N. D\\'el\\'eage, M.\n  Demmer, D. Derkach, O. Deschamps, F. Dettori, B. Dey, A. Di Canto, F. Di\n  Ruscio, H. Dijkstra, S. Donleavy, F. Dordei, M. Dorigo, A. Dosil Su\\'arez, D.\n  Dossett, A. Dovbnya, K. Dreimanis, L. Dufour, G. Dujany, F. Dupertuis, P.\n  Durante, R. Dzhelyadin, A. Dziurda, A. Dzyuba, S. Easo, U. Egede, V.\n  Egorychev, S. Eidelman, S. Eisenhardt, U. Eitschberger, R. Ekelhof, L.\n  Eklund, I. El Rifai, Ch. Elsasser, S. Ely, S. Esen, H.M. Evans, T. Evans, A.\n  Falabella, C. F\\\"arber, N. Farley, S. Farry, R. Fay, D. Ferguson, V.\n  Fernandez Albor, F. Ferrari, F. Ferreira Rodrigues, M. Ferro-Luzzi, S.\n  Filippov, M. Fiore, M. Fiorini, M. Firlej, C. Fitzpatrick, T. Fiutowski, K.\n  Fohl, P. Fol, M. Fontana, F. Fontanelli, D. C. Forshaw, R. Forty, M. Frank,\n  C. Frei, M. Frosini, J. Fu, E. Furfaro, A. Gallas Torreira, D. Galli, S.\n  Gallorini, S. Gambetta, M. Gandelman, P. Gandini, Y. Gao, J. Garc\\'ia\n  Pardi\\~nas, J. Garra Tico, L. Garrido, D. Gascon, C. Gaspar, R. Gauld, L.\n  Gavardi, G. Gazzoni, D. Gerick, E. Gersabeck, M. Gersabeck, T. Gershon, Ph.\n  Ghez, S. Gian\\`i, V. Gibson, O.G. Girard, L. Giubega, V.V. Gligorov, C.\n  G\\\"obel, D. Golubkov, A. Golutvin, A. Gomes, C. Gotti, M. Grabalosa\n  G\\'andara, R. Graciani Diaz, L.A. Granado Cardoso, E. Graug\\'es, E.\n  Graverini, G. Graziani, A. Grecu, E. Greening, S. Gregson, P. Griffith, L.\n  Grillo, O. Gr\\\"unberg, B. Gui, E. Gushchin, Yu. Guz, T. Gys, T. Hadavizadeh,\n  C. Hadjivasiliou, G. Haefeli, C. Haen, S.C. Haines, S. Hall, B. Hamilton, X.\n  Han, S. Hansmann-Menzemer, N. Harnew, S.T. Harnew, J. Harrison, J. He, T.\n  Head, V. Heijne, A. Heister, K. Hennessy, P. Henrard, L. Henry, J.A. Hernando\n  Morata, E. van Herwijnen, M. He{\\ss}, A. Hicheur, D. Hill, M. Hoballah, C.\n  Hombach, W. Hulsbergen, T. Humair, N. Hussain, D. Hutchcroft, D. Hynds, M.\n  Idzik, P. Ilten, R. Jacobsson, A. Jaeger, J. Jalocha, E. Jans, A. Jawahery,\n  F. Jing, M. John, D. Johnson, C.R. Jones, C. Joram, B. Jost, N. Jurik, S.\n  Kandybei, W. Kanso, M. Karacson, T.M. Karbach, S. Karodia, M. Kecke, M.\n  Kelsey, I.R. Kenyon, M. Kenzie, T. Ketel, E. Khairullin, B. Khanji, C.\n  Khurewathanakul, T. Kirn, S. Klaver, K. Klimaszewski, O. Kochebina, M.\n  Kolpin, I. Komarov, R.F. Koopman, P. Koppenburg, M. Kozeiha, L. Kravchuk, K.\n  Kreplin, M. Kreps, G. Krocker, P. Krokovny, F. Kruse, W. Krzemien, W.\n  Kucewicz, M. Kucharczyk, V. Kudryavtsev, A. K. Kuonen, K. Kurek, T.\n  Kvaratskheliya, D. Lacarrere, G. Lafferty, A. Lai, D. Lambert, G. Lanfranchi,\n  C. Langenbruch, B. Langhans, T. Latham, C. Lazzeroni, R. Le Gac, J. van\n  Leerdam, J.-P. Lees, R. Lef\\`evre, A. Leflat, J. Lefran\\c{c}ois, E. Lemos\n  Cid, O. Leroy, T. Lesiak, B. Leverington, Y. Li, T. Likhomanenko, M. Liles,\n  R. Lindner, C. Linn, F. Lionetto, B. Liu, X. Liu, D. Loh, I. Longstaff, J.H.\n  Lopes, D. Lucchesi, M. Lucio Martinez, H. Luo, A. Lupato, E. Luppi, O.\n  Lupton, A. Lusiani, F. Machefert, F. Maciuc, O. Maev, K. Maguire, S. Malde,\n  A. Malinin, G. Manca, G. Mancinelli, P. Manning, A. Mapelli, J. Maratas, J.F.\n  Marchand, U. Marconi, C. Marin Benito, P. Marino, J. Marks, G. Martellotti,\n  M. Martin, M. Martinelli, D. Martinez Santos, F. Martinez Vidal, D. Martins\n  Tostes, A. Massafferri, R. Matev, A. Mathad, Z. Mathe, C. Matteuzzi, A.\n  Mauri, B. Maurin, A. Mazurov, M. McCann, J. McCarthy, A. McNab, R. McNulty,\n  B. Meadows, F. Meier, M. Meissner, D. Melnychuk, M. Merk, E Michielin, D.A.\n  Milanes, M.-N. Minard, D.S. Mitzel, J. Molina Rodriguez, I.A. Monroy, S.\n  Monteil, M. Morandin, P. Morawski, A. Mord\\`a, M.J. Morello, J. Moron, A.B.\n  Morris, R. Mountain, F. Muheim, D. M\\\"uller, J. M\\\"uller, K. M\\\"uller, V.\n  M\\\"uller, M. Mussini, B. Muster, P. Naik, T. Nakada, R. Nandakumar, A. Nandi,\n  I. Nasteva, M. Needham, N. Neri, S. Neubert, N. Neufeld, M. Neuner, A.D.\n  Nguyen, T.D. Nguyen, C. Nguyen-Mau, V. Niess, R. Niet, N. Nikitin, T.\n  Nikodem, A. Novoselov, D.P. O'Hanlon, A. Oblakowska-Mucha, V. Obraztsov, S.\n  Ogilvy, O. Okhrimenko, R. Oldeman, C.J.G. Onderwater, B. Osorio Rodrigues,\n  J.M. Otalora Goicochea, A. Otto, P. Owen, A. Oyanguren, A. Palano, F.\n  Palombo, M. Palutan, J. Panman, A. Papanestis, M. Pappagallo, L.L.\n  Pappalardo, C. Pappenheimer, W. Parker, C. Parkes, G. Passaleva, G.D. Patel,\n  M. Patel, C. Patrignani, A. Pearce, A. Pellegrino, G. Penso, M. Pepe\n  Altarelli, S. Perazzini, P. Perret, L. Pescatore, K. Petridis, A. Petrolini,\n  M. Petruzzo, E. Picatoste Olloqui, B. Pietrzyk, T. Pila\\v{r}, D. Pinci, A.\n  Pistone, A. Piucci, S. Playfer, M. Plo Casasus, T. Poikela, F. Polci, A.\n  Poluektov, I. Polyakov, E. Polycarpo, A. Popov, D. Popov, B. Popovici, C.\n  Potterat, E. Price, J.D. Price, J. Prisciandaro, A. Pritchard, C. Prouve, V.\n  Pugatch, A. Puig Navarro, G. Punzi, W. Qian, R. Quagliani, B. Rachwal, J.H.\n  Rademacker, M. Rama, M.S. Rangel, I. Raniuk, N. Rauschmayr, G. Raven, F.\n  Redi, S. Reichert, M.M. Reid, A.C. dos Reis, S. Ricciardi, S. Richards, M.\n  Rihl, K. Rinnert, V. Rives Molina, P. Robbe, A.B. Rodrigues, E. Rodrigues,\n  J.A. Rodriguez Lopez, P. Rodriguez Perez, S. Roiser, V. Romanovsky, A. Romero\n  Vidal, J. W. Ronayne, M. Rotondo, J. Rouvinet, T. Ruf, P. Ruiz Valls, J.J.\n  Saborido Silva, N. Sagidova, P. Sail, B. Saitta, V. Salustino Guimaraes, C.\n  Sanchez Mayordomo, B. Sanmartin Sedes, R. Santacesaria, C. Santamarina Rios,\n  M. Santimaria, E. Santovetti, A. Sarti, C. Satriano, A. Satta, D.M. Saunders,\n  D. Savrina, S. Schael, M. Schiller, H. Schindler, M. Schlupp, M. Schmelling,\n  T. Schmelzer, B. Schmidt, O. Schneider, A. Schopper, M. Schubiger, M.-H.\n  Schune, R. Schwemmer, B. Sciascia, A. Sciubba, A. Semennikov, A. Sergi, N.\n  Serra, J. Serrano, L. Sestini, P. Seyfert, M. Shapkin, I. Shapoval, Y.\n  Shcheglov, T. Shears, L. Shekhtman, V. Shevchenko, A. Shires, B.G. Siddi, R.\n  Silva Coutinho, L. Silva de Oliveira, G. Simi, M. Sirendi, N. Skidmore, T.\n  Skwarnicki, E. Smith, E. Smith, I.T. Smith, J. Smith, M. Smith, H. Snoek,\n  M.D. Sokoloff, F.J.P. Soler, F. Soomro, D. Souza, B. Souza De Paula, B.\n  Spaan, P. Spradlin, S. Sridharan, F. Stagni, M. Stahl, S. Stahl, S. Stefkova,\n  O. Steinkamp, O. Stenyakin, S. Stevenson, S. Stoica, S. Stone, B. Storaci, S.\n  Stracka, M. Straticiuc, U. Straumann, L. Sun, W. Sutcliffe, K. Swientek, S.\n  Swientek, V. Syropoulos, M. Szczekowski, T. Szumlak, S. T'Jampens, A.\n  Tayduganov, T. Tekampe, M. Teklishyn, G. Tellarini, F. Teubert, C. Thomas, E.\n  Thomas, J. van Tilburg, V. Tisserand, M. Tobin, J. Todd, S. Tolk, L.\n  Tomassetti, D. Tonelli, S. Topp-Joergensen, N. Torr, E. Tournefier, S.\n  Tourneur, K. Trabelsi, M.T. Tran, M. Tresch, A. Trisovic, A. Tsaregorodtsev,\n  P. Tsopelas, N. Tuning, A. Ukleja, A. Ustyuzhanin, U. Uwer, C. Vacca, V.\n  Vagnoni, G. Valenti, A. Vallier, R. Vazquez Gomez, P. Vazquez Regueiro, C.\n  V\\'azquez Sierra, S. Vecchi, M. van Veghel, J.J. Velthuis, M. Veltri, G.\n  Veneziano, M. Vesterinen, B. Viaud, D. Vieira, M. Vieites Diaz, X.\n  Vilasis-Cardona, V. Volkov, A. Vollhardt, D. Volyanskyy, D. Voong, A.\n  Vorobyev, V. Vorobyev, C. Vo{\\ss}, J.A. de Vries, R. Waldi, C. Wallace, R.\n  Wallace, J. Walsh, S. Wandernoth, J. Wang, D.R. Ward, N.K. Watson, D.\n  Websdale, A. Weiden, M. Whitehead, G. Wilkinson, M. Wilkinson, M. Williams,\n  M.P. Williams, M. Williams, T. Williams, F.F. Wilson, J. Wimberley, J.\n  Wishahi, W. Wislicki, M. Witek, G. Wormser, S.A. Wotton, S. Wright, K.\n  Wyllie, Y. Xie, Z. Xu, Z. Yang, J. Yu, X. Yuan, O. Yushchenko, M. Zangoli, M.\n  Zavertyaev, L. Zhang, Y. Zhang, A. Zhelezov, A. Zhokhov, L. Zhong, V. Zhukov,\n  S. Zucchelli", "docs_id": "1512.00439", "section": ["nucl-ex", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurements of long-range near-side angular correlations in\n  $\\sqrt{s_{\\text{NN}}}=5$TeV proton-lead collisions in the forward region. Two-particle angular correlations are studied in proton-lead collisions at a nucleon-nucleon centre-of-mass energy of $\\sqrt{s_{\\text{NN}}}=5$TeV, collected with the LHCb detector at the LHC. The analysis is based on data recorded in two beam configurations, in which either the direction of the proton or that of the lead ion is analysed. The correlations are measured in the laboratory system as a function of relative pseudorapidity, $\\Delta\\eta$, and relative azimuthal angle, $\\Delta\\phi$, for events in different classes of event activity and for different bins of particle transverse momentum. In high-activity events a long-range correlation on the near side, $\\Delta\\phi \\approx 0$, is observed in the pseudorapidity range $2.0<\\eta<4.9$. This measurement of long-range correlations on the near side in proton-lead collisions extends previous observations into the forward region up to $\\eta=4.9$. The correlation increases with growing event activity and is found to be more pronounced in the direction of the lead beam. However, the correlation in the direction of the lead and proton beams are found to be compatible when comparing events with similar absolute activity in the direction analysed."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the study of two-particle angular correlations in proton-lead collisions at \u221as_NN = 5 TeV, which of the following statements is most accurate regarding the observed long-range correlation on the near side (\u0394\u03c6 \u2248 0)?\n\nA) It was observed uniformly across all pseudorapidity ranges and event activity levels.\nB) It was most prominent in the backward rapidity region (\u03b7 < 2.0) and increased with decreasing event activity.\nC) It was detected in the forward pseudorapidity range 2.0 < \u03b7 < 4.9 for high-activity events and was more pronounced in the direction of the lead beam.\nD) It showed equal strength in both proton and lead beam directions, regardless of the absolute activity in the analyzed direction.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that \"In high-activity events a long-range correlation on the near side, \u0394\u03c6 \u2248 0, is observed in the pseudorapidity range 2.0 < \u03b7 < 4.9.\" It also mentions that \"The correlation increases with growing event activity and is found to be more pronounced in the direction of the lead beam.\"\n\nAnswer A is incorrect because the correlation was not observed uniformly across all pseudorapidity ranges and event activities, but specifically in the forward region and for high-activity events.\n\nAnswer B is incorrect because it contradicts the information given. The correlation was observed in the forward region (not backward) and increased with increasing (not decreasing) event activity.\n\nAnswer D is incorrect because while the correlations in the lead and proton beam directions were found to be compatible for similar absolute activity, the overall observation was that the correlation was more pronounced in the direction of the lead beam."}}