{"0": {"documentation": {"title": "Resolution of the St. Petersburg paradox using Von Mises axiom of\n  randomness", "source": "Andrea Berdondini", "docs_id": "1907.11054", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resolution of the St. Petersburg paradox using Von Mises axiom of\n  randomness. In this article we will propose a completely new point of view for solving one of the most important paradoxes concerning game theory. The solution develop shifts the focus from the result to the strategy s ability to operate in a cognitive way by exploiting useful information about the system. In order to determine from a mathematical point of view if a strategy is cognitive, we use Von Mises' axiom of randomness. Based on this axiom, the knowledge of useful information consequently generates results that cannot be reproduced randomly. Useful information in this case may be seen as a significant datum for the recipient, for their present or future decision-making process. Finally, by resolving the paradox from this new point of view, we will demonstrate that an expected gain that tends toward infinity is not always a consequence of a cognitive and non-random strategy. Therefore, this result leads us to define a hierarchy of values in decision-making, where the cognitive aspect, whose statistical consequence is a divergence from random behaviour, turns out to be more important than the expected gain."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the article, what is the main contribution of the proposed solution to the St. Petersburg paradox, and how does it relate to the concept of \"cognitive\" strategies in game theory?\n\nA) The solution resolves the paradox by assuming that the expected gain tends towards infinity, which is a consequence of a non-cognitive and random strategy.\nB) The solution resolves the paradox by demonstrating that a cognitive and non-random strategy can generate results that cannot be reproduced randomly, and that this is more important than the expected gain.\nC) The solution resolves the paradox by shifting the focus from the result to the strategy's ability to operate in a cognitive way by exploiting useful information about the system.\nD) The solution resolves the paradox by introducing a new axiom of randomness, which is incompatible with the concept of cognitive strategies.\n\n**Correct Answer:** C) The solution resolves the paradox by shifting the focus from the result to the strategy's ability to operate in a cognitive way by exploiting useful information about the system.\n\n**Explanation:** The correct answer is C) because the article states that the proposed solution \"shifts the focus from the result to the strategy's ability to operate in a cognitive way by exploiting useful information about the system.\" This is the main contribution of the solution to the St. Petersburg paradox, and it is the key to resolving the paradox from a new point of view. The other options are incorrect because they either misrepresent the solution or introduce concepts that are not relevant to the article."}, "1": {"documentation": {"title": "The paradox of the infinity", "source": "Mohamed Ayad and Omar Kihel", "docs_id": "2106.01311", "section": ["math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The paradox of the infinity. \\textit{Let $E$ be an infinite set on which a property $(\\bf P)$ is defined. Suppose that $E=\\cup_{i\\in I} E_i$ is a partition, where each $E_i$ is infinite. Suppose also that, in each $E_i$, the number of elements satisfying $(\\bf P)$ is finite. Then, clearly the density of the elements satisfying $(\\bf P)$ is 0 in every $E_i$. Is it possible that the density of the subset of $E$ containing all the elements satisfying $(\\bf P)$ will be at least equal to $ 1/2$?} We were first confronted with this situation while reading the paper of Arno et al. [1]. In fact, it is in the paper [1] where it is shown that the density of certain algebraic numbers in $\\overline{\\mathbb{Q}}$, which we will call Arno et al. numbers in section 5, is equal to $1/\\zeta(3)$. We have partitioned $\\overline{\\mathbb{Q}}$ in a way that suggests these Arno et al. numbers are rare. This phenomenom struck us as contradictory, which lead us to consider the situation in greater detail. We will show in the sequel, through two examples, that the answer to the above question may be positive. At first glance, this problem resembles to the so called Simpson paradox in probability and statistics. In this paper, when we say the density, we mean the natural density."}, "answer": "Here is the exam question:\n\n**Question:** Consider a set $E$ that is the union of an infinite number of infinite subsets $E_i$, where each $E_i$ is partitioned into elements that satisfy a property $(\\bf P)$ and elements that do not satisfy $(\\bf P)$. Suppose that the number of elements satisfying $(\\bf P)$ in each $E_i$ is finite. Is it possible that the density of the subset of $E$ containing all the elements satisfying $(\\bf P)$ is at least equal to $1/2$?\n\nA) Yes, it is possible to construct a set $E$ where the density of the subset of elements satisfying $(\\bf P)$ is greater than $1/2$.\n\nB) No, the density of the subset of elements satisfying $(\\bf P)$ in $E$ must be less than $1/2$.\n\nC) The problem statement is incomplete, and more information is needed to determine the answer.\n\nD) The answer depends on the specific properties of the subsets $E_i$ and the property $(\\bf P)$.\n\n**Correct Answer:** A) Yes, it is possible to construct a set $E$ where the density of the subset of elements satisfying $(\\bf P)$ is greater than $1/2$.\n\n**Explanation:** The paradox of the infinity states that it is possible to construct a set $E$ where the density of the subset of elements satisfying a property $(\\bf P)$ is greater than $1/2$, even if the density of the subset of elements satisfying $(\\bf P)$ in each individual subset $E_i$ is less than $1/2$. This is because the density of the subset of elements satisfying $(\\bf P)$ in $E$ is determined by the proportion of elements satisfying $(\\bf P)$ in the entire set $E$, not just in each individual subset $E_i$. Therefore, it is possible to construct a set $E$ where the density of the subset of elements satisfying $(\\bf P)$ is greater than $1/2$, even if the density of the subset of elements satisfying $(\\bf P)$ in each individual subset $E_i$ is less than $1/2$."}, "2": {"documentation": {"title": "Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty", "source": "Niko Hauzenberger and Florian Huber and Massimiliano Marcellino and\n  Nico Petz", "docs_id": "2112.01995", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty. We develop a non-parametric multivariate time series model that remains agnostic on the precise relationship between a (possibly) large set of macroeconomic time series and their lagged values. The main building block of our model is a Gaussian Process prior on the functional relationship that determines the conditional mean of the model, hence the name of Gaussian Process Vector Autoregression (GP-VAR). We control for changes in the error variances by introducing a stochastic volatility specification. To facilitate computation in high dimensions and to introduce convenient statistical properties tailored to match stylized facts commonly observed in macro time series, we assume that the covariance of the Gaussian Process is scaled by the latent volatility factors. We illustrate the use of the GP-VAR by analyzing the effects of macroeconomic uncertainty, with a particular emphasis on time variation and asymmetries in the transmission mechanisms. Using US data, we find that uncertainty shocks have time-varying effects, they are less persistent during recessions but their larger size in these specific periods causes more than proportional effects on real growth and employment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Gaussian Process Vector Autoregression (GP-VAR) model, what is the primary purpose of introducing a stochastic volatility specification, and how does it facilitate computation in high dimensions?\n\n**A)** To control for changes in the error variances by modeling the volatility of the error terms as a function of time, thereby allowing for more accurate predictions of future errors.\n\n**B)** To introduce convenient statistical properties tailored to match stylized facts commonly observed in macro time series, such as the presence of time-varying volatility and asymmetries in the transmission mechanisms.\n\n**C)** To scale the covariance of the Gaussian Process by the latent volatility factors, thereby allowing for more accurate modeling of the relationships between macroeconomic time series.\n\n**D)** To control for changes in the error variances by introducing a stochastic volatility specification that is independent of the latent volatility factors, thereby allowing for more accurate predictions of future errors.\n\n**Correct Answer:** B) To introduce convenient statistical properties tailored to match stylized facts commonly observed in macro time series, such as the presence of time-varying volatility and asymmetries in the transmission mechanisms.\n\n**Explanation:** The correct answer is B) because the introduction of a stochastic volatility specification in the GP-VAR model allows for the modeling of time-varying volatility and asymmetries in the transmission mechanisms, which are common features of macro time series data. This specification facilitates computation in high dimensions by providing a more flexible and accurate representation of the relationships between macroeconomic time series. The other options are incorrect because they do not accurately capture the primary purpose of introducing a stochastic volatility specification in the GP-VAR model."}, "3": {"documentation": {"title": "Robust Compressed Sensing Under Matrix Uncertainties", "source": "Yipeng Liu", "docs_id": "1311.4924", "section": ["cs.IT", "cs.CV", "math.IT", "math.RT", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Compressed Sensing Under Matrix Uncertainties. Compressed sensing (CS) shows that a signal having a sparse or compressible representation can be recovered from a small set of linear measurements. In classical CS theory, the sampling matrix and representation matrix are assumed to be known exactly in advance. However, uncertainties exist due to sampling distortion, finite grids of the parameter space of dictionary, etc. In this paper, we take a generalized sparse signal model, which simultaneously considers the sampling and representation matrix uncertainties. Based on the new signal model, a new optimization model for robust sparse signal reconstruction is proposed. This optimization model can be deduced with stochastic robust approximation analysis. Both convex relaxation and greedy algorithms are used to solve the optimization problem. For the convex relaxation method, a sufficient condition for recovery by convex relaxation is given; For the greedy algorithm, it is realized by the introduction of a pre-processing of the sensing matrix and the measurements. In numerical experiments, both simulated data and real-life ECG data based results show that the proposed method has a better performance than the current methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Robust Compressed Sensing Under Matrix Uncertainties, what is the primary assumption made in classical Compressed Sensing theory that is relaxed in the proposed generalized sparse signal model?\n\nA) The sampling matrix is assumed to be perfectly orthogonal.\nB) The representation matrix is assumed to be perfectly invertible.\nC) The sampling matrix and representation matrix are assumed to be known exactly in advance.\nD) The signal is assumed to be perfectly sparse.\n\n**Correct Answer:** C) The sampling matrix and representation matrix are assumed to be known exactly in advance.\n\n**Explanation:** The question requires the test-taker to understand the context of the paper and the assumptions made in classical Compressed Sensing theory. The correct answer, C, highlights the key difference between classical CS theory and the proposed generalized sparse signal model, which relaxes the assumption of exact knowledge of the sampling and representation matrices. The incorrect options, A, B, and D, are plausible but incorrect assumptions that are not explicitly mentioned in the paper."}, "4": {"documentation": {"title": "A comparison of mixed-variables Bayesian optimization approaches", "source": "Jhouben Cuesta-Ramirez and Rodolphe Le Riche and Olivier Roustant and\n  Guillaume Perrin and Cedric Durantin and Alain Gliere", "docs_id": "2111.01533", "section": ["math.OC", "cs.LG", "stat.AP", "stat.CO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A comparison of mixed-variables Bayesian optimization approaches. Most real optimization problems are defined over a mixed search space where the variables are both discrete and continuous. In engineering applications, the objective function is typically calculated with a numerically costly black-box simulation.General mixed and costly optimization problems are therefore of a great practical interest, yet their resolution remains in a large part an open scientific question. In this article, costly mixed problems are approached through Gaussian processes where the discrete variables are relaxed into continuous latent variables. The continuous space is more easily harvested by classical Bayesian optimization techniques than a mixed space would. Discrete variables are recovered either subsequently to the continuous optimization, or simultaneously with an additional continuous-discrete compatibility constraint that is handled with augmented Lagrangians. Several possible implementations of such Bayesian mixed optimizers are compared. In particular, the reformulation of the problem with continuous latent variables is put in competition with searches working directly in the mixed space. Among the algorithms involving latent variables and an augmented Lagrangian, a particular attention is devoted to the Lagrange multipliers for which a local and a global estimation techniques are studied. The comparisons are based on the repeated optimization of three analytical functions and a beam design problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the approaches of reformulating the problem with continuous latent variables versus searching directly in the mixed space for Bayesian mixed optimizers. How do the use of augmented Lagrangians and local/global estimation techniques of Lagrange multipliers impact the performance of these approaches?\n\n**A)** Reformulating the problem with continuous latent variables is more effective than searching directly in the mixed space, as it allows for the use of classical Bayesian optimization techniques to harvest the continuous space. However, the discrete variables must be recovered subsequently, which can be computationally expensive.\n\n**B)** Searching directly in the mixed space is more effective than reformulating the problem with continuous latent variables, as it avoids the need for additional computational steps to recover the discrete variables. However, the problem may be more challenging to optimize due to the mixed nature of the search space.\n\n**C)** The use of augmented Lagrangians and local/global estimation techniques of Lagrange multipliers can significantly impact the performance of Bayesian mixed optimizers. Reformulating the problem with continuous latent variables can lead to more efficient optimization, but the choice of Lagrange multiplier estimation technique can affect the convergence of the algorithm.\n\n**D)** The choice between reformulating the problem with continuous latent variables and searching directly in the mixed space depends on the specific problem at hand. If the objective function is computationally expensive to evaluate, reformulating the problem with continuous latent variables may be more effective. However, if the problem has a simple mixed search space, searching directly in the mixed space may be sufficient.\n\n**Correct Answer:** C) The use of augmented Lagrangians and local/global estimation techniques of Lagrange multipliers can significantly impact the performance of Bayesian mixed optimizers. Reformulating the problem with continuous latent variables can lead to more efficient optimization, but the choice of Lagrange multiplier estimation technique can affect the convergence of the algorithm.\n\n**Explanation:** This question requires the test-taker to analyze the trade-offs between different approaches to Bayesian mixed optimization and understand the impact of augmented Lagrangians and Lagrange multiplier estimation techniques on the performance of the algorithm. The correct answer highlights the importance of considering these factors when designing a Bayesian mixed optimizer."}, "5": {"documentation": {"title": "Pairing dynamics and solitonic excitations in collisions of medium-mass,\n  identical nuclei", "source": "Piotr Magierski, Andrzej Makowski, Matthew C. Barton, Kazuyuki\n  Sekizawa, Gabriel Wlaz{\\l}owski", "docs_id": "2111.05135", "section": ["nucl-th", "cond-mat.quant-gas", "cond-mat.supr-con", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pairing dynamics and solitonic excitations in collisions of medium-mass,\n  identical nuclei. We present results of collisions of $^{90}Zr+^{90}Zr$ and $^{96}Zr+^{96}Zr$ obtained within time-dependent density functional theory (TDDFT) extended to superfluid systems, known as time-dependent superfluid local density approximation (TDSLDA). We discuss qualitatively new features occurring during collisions of two superfluid nuclei at energies in the vicinity of the Coulomb barrier. We show that a solitonic excitation an abrupt pairing phase distortion reported previously [P.Magierski et al., Phys. Rev. Lett. 119, 042501 (2017)], increases the barrier for capture generating effective repulsion between colliding nuclei. Moreover, we demonstrate that pairing field leads to qualitatively different dynamics at the Coulomb barrier which manifests itself in a slower evolution of deformation towards compact system. Last but not least, we show that magnitude of pairing correlations can be dynamically enhanced after collision. We interpret it as a dynamically-induced symmetry breaking, which leads to large-amplitude oscillations of pairing field and bear similarity to the pairing Higgs mechanism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of the pairing field on the dynamics of collisions between superfluid nuclei at energies near the Coulomb barrier, as described in the study on pairing dynamics and solitonic excitations in collisions of medium-mass, identical nuclei?\n\nA) The pairing field leads to a decrease in the barrier for capture, resulting in a more efficient capture of the nuclei.\nB) The pairing field increases the barrier for capture, generating effective repulsion between colliding nuclei and slowing down the evolution of deformation towards a compact system.\nC) The pairing field has no effect on the dynamics of collisions between superfluid nuclei at energies near the Coulomb barrier.\nD) The pairing field leads to a decrease in the magnitude of pairing correlations, resulting in a loss of pairing field and a decrease in the amplitude of oscillations.\n\nCorrect Answer: B) The pairing field increases the barrier for capture, generating effective repulsion between colliding nuclei and slowing down the evolution of deformation towards a compact system.\n\nExplanation: The study describes how the pairing field leads to a qualitatively different dynamics at the Coulomb barrier, manifesting itself in a slower evolution of deformation towards a compact system. This is due to the increase in the barrier for capture, which generates effective repulsion between colliding nuclei. This effect is a result of the pairing field, which is a key feature of superfluid systems. The correct answer, B, accurately reflects this understanding."}, "6": {"documentation": {"title": "Hadron-Hadron Correlation and Interaction from Heavy-Ion Collisions", "source": "Akira Ohnishi, Kenji Morita, Kenta Miyahara, Tetsuo Hyodo", "docs_id": "1603.05761", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hadron-Hadron Correlation and Interaction from Heavy-Ion Collisions. We investigate the $\\Lambda\\Lambda$ and $K^-p$ intensity correlations in high-energy heavy-ion collisions. First, we examine the dependence of the $\\Lambda\\Lambda$ correlation on the $\\Lambda\\Lambda$ interaction and the $\\Lambda\\Lambda$ pair purity probability $\\lambda$. For small $\\lambda$, the correlation function needs to be suppressed by the $\\Lambda\\Lambda$ interaction in order to explain the recently measured $\\Lambda\\Lambda$ correlation data. By comparison, when we adopt the $\\lambda$ value evaluated from the experimentally measured $\\Sigma^0/\\Lambda$ ratio, the correlation function needs to be enhanced by the interaction. We demonstrate that these two cases correspond to the two analyses which gave opposite signs of the $\\Lambda\\Lambda$ scattering length. Next, we discuss the $K^-p$ correlation function. By using the local $\\bar{K}N$ potential which reproduces the kaonic hydrogen data by SIDDHARTA, we obtain the $K^-p$ correlation function. We find that the $K^-p$ correlation can provide a complementary information with the $K^{-}p$ elastic scattering amplitude."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the investigation of the $\\Lambda\\Lambda$ and $K^-p$ intensity correlations in high-energy heavy-ion collisions, and how does it relate to the analysis of the $\\Lambda\\Lambda$ scattering length?\n\nA) The $\\Lambda\\Lambda$ correlation function needs to be suppressed by the $\\Lambda\\Lambda$ interaction to explain the measured data, which corresponds to the analysis with a negative $\\Lambda\\Lambda$ scattering length.\n\nB) The $\\Lambda\\Lambda$ correlation function needs to be enhanced by the $\\Lambda\\Lambda$ interaction to explain the measured data, which corresponds to the analysis with a positive $\\Lambda\\Lambda$ scattering length.\n\nC) The $\\Lambda\\Lambda$ correlation function is independent of the $\\Lambda\\Lambda$ interaction and pair purity probability, and the $K^-p$ correlation function provides a complementary information.\n\nD) The $\\Lambda\\Lambda$ correlation function is enhanced by the $\\Lambda\\Lambda$ interaction when using the experimentally measured $\\Sigma^0/\\Lambda$ ratio, but suppressed when using a small $\\lambda$ value.\n\nCorrect Answer: B) The $\\Lambda\\Lambda$ correlation function needs to be enhanced by the $\\Lambda\\Lambda$ interaction to explain the measured data, which corresponds to the analysis with a positive $\\Lambda\\Lambda$ scattering length.\n\nExplanation: The correct answer is B) because the investigation shows that when using the experimentally measured $\\Sigma^0/\\Lambda$ ratio, the correlation function needs to be enhanced by the interaction, which corresponds to the analysis with a positive $\\Lambda\\Lambda$ scattering length. This is in contrast to the analysis with a negative $\\Lambda\\Lambda$ scattering length, which requires suppression of the correlation function by the interaction for small $\\lambda$ values."}, "7": {"documentation": {"title": "Stochastic Trust Region Inexact Newton Method for Large-scale Machine\n  Learning", "source": "Vinod Kumar Chauhan and Anuj Sharma and Kalpana Dahiya", "docs_id": "1812.10426", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stochastic Trust Region Inexact Newton Method for Large-scale Machine\n  Learning. Nowadays stochastic approximation methods are one of the major research direction to deal with the large-scale machine learning problems. From stochastic first order methods, now the focus is shifting to stochastic second order methods due to their faster convergence and availability of computing resources. In this paper, we have proposed a novel Stochastic Trust RegiOn Inexact Newton method, called as STRON, to solve large-scale learning problems which uses conjugate gradient (CG) to inexactly solve trust region subproblem. The method uses progressive subsampling in the calculation of gradient and Hessian values to take the advantage of both, stochastic and full-batch regimes. We have extended STRON using existing variance reduction techniques to deal with the noisy gradients and using preconditioned conjugate gradient (PCG) as subproblem solver, and empirically proved that they do not work as expected, for the large-scale learning problems. Finally, our empirical results prove efficacy of the proposed method against existing methods with bench marked datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of stochastic trust region inexact Newton methods, such as the proposed STRON method, for large-scale machine learning problems?\n\n**A)** To reduce the computational cost of solving large-scale optimization problems by using a single pass through the data.\n**B)** To leverage the advantages of both stochastic and full-batch regimes by progressively subsampling the data during gradient and Hessian calculations.\n**C)** To improve the convergence rate of stochastic first-order methods by incorporating second-order information.\n**D)** To develop a method that can handle noisy gradients and large-scale datasets using preconditioned conjugate gradient.\n\n**Correct Answer:** B) To leverage the advantages of both stochastic and full-batch regimes by progressively subsampling the data during gradient and Hessian calculations.\n\n**Explanation:** The question requires the test-taker to understand the underlying motivation behind the development of the STRON method. The correct answer, B, highlights the key advantage of the method, which is to combine the benefits of both stochastic and full-batch regimes by progressively subsampling the data. This allows the method to adapt to the available computing resources and data availability, making it more efficient and effective for large-scale machine learning problems.\n\nThe incorrect options, A, C, and D, are plausible but incorrect motivations. Option A is related to the computational cost, but it is not the primary motivation behind the development of STRON. Option C is related to the convergence rate of stochastic first-order methods, but it is not directly related to the STRON method. Option D is related to handling noisy gradients, but it is not the primary motivation behind the development of STRON."}, "8": {"documentation": {"title": "The relationship between IMF broad based financial development index and\n  international trade: Evidence from India", "source": "Ummuhabeeba Chaliyan and Mini P. Thomas", "docs_id": "2112.01749", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The relationship between IMF broad based financial development index and\n  international trade: Evidence from India. This study investigates whether a uni-directional or bi-directional causal relationship exists between financial development and international trade for Indian economy, during the time period from 1980 to 2019. Three measures of financial development created by IMF, namely, financial institutional development index, financial market development index and a composite index of financial development is utilized for the empirical analysis. Johansen cointegration, vector error correction model and vector auto regressive model are estimated to examine the long run relationship and short run dynamics among the variables of interest. The econometric results indicate that there is indeed a long run association between the composite index of financial development and trade openness. Cointegration is also found to exist between trade openness and index of financial market development. However, there is no evidence of cointegration between financial institutional development and trade openness. Granger causality test results indicate the presence of uni-directional causality running from composite index of financial development to trade openness. Financial market development is also found to Granger cause trade openness. Empirical evidence thus underlines the importance of formulating policies which recognize the role of well-developed financial markets in promoting international trade."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study investigates the relationship between financial development and international trade for the Indian economy during the period from 1980 to 2019. The econometric results indicate that there is a long run association between the composite index of financial development and trade openness. However, the Granger causality test results suggest that there is no evidence of cointegration between financial institutional development and trade openness. Which of the following statements best summarizes the findings of the study?\n\nA) There is a uni-directional causal relationship running from financial institutional development to trade openness.\nB) There is a bi-directional causal relationship between financial development and trade openness.\nC) There is a long run association between the composite index of financial development and trade openness, but no causal relationship exists.\nD) Financial market development Granger causes trade openness, but there is no causal relationship between financial institutional development and trade openness.\n\n**Correct Answer:** C) There is a long run association between the composite index of financial development and trade openness, but no causal relationship exists.\n\n**Explanation:** The study finds a long run association between the composite index of financial development and trade openness, but the Granger causality test results indicate that there is no evidence of cointegration between financial institutional development and trade openness. This suggests that while there is a long run association between financial development and trade openness, there is no causal relationship between the two variables."}, "9": {"documentation": {"title": "Constraining new physics with a novel measurement of the $^{23}$Ne\n  $\\beta$-decay branching ratio", "source": "Yonatan Mishnayot, Ayala Glick-Magid, Hitesh Rahangdale, Guy Ron,\n  Doron Gazit, Jason T. Harke, Micha Hass, Ben Ohayon, Aaron Gallant, Nicholas\n  D. Scielzo, Sergey Vaintruab, Richard O. Hughes, Tsviki Hirsch, Christian\n  Forss\\'en, Daniel Gazda, Peter Gysbers, Javier Men\\'endez, Petr Navr\\'atil,\n  Leonid Weissman, Arik Kreisel, Boaz Kaizer, Hodaya Daphna, Maayan Buzaglo", "docs_id": "2107.14355", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraining new physics with a novel measurement of the $^{23}$Ne\n  $\\beta$-decay branching ratio. Measurements of the beta-neutrino correlation coefficient (a$_{\\beta\\nu}$) in nuclear beta decay, together with the Fierz interference term (b$_F$), provide a robust test for the existence of exotic interactions beyond the Standard Model of Particle Physics. The extraction of these quantities from the recoil ion spectra in $\\beta$-decay requires accurate knowledge, decay branching ratios, and high-precision calculations of higher order nuclear effects. Here, we report on a new measurement of the $^{23}$Ne $\\beta$-decay branching ratio, which allows a reanalysis of existing high-precision measurements. Together with new theoretical calculations of nuclear structure effects, augmented with robust theoretical uncertainty, this measurement improves on the current knowledge of a$_{\\beta\\nu}$ in $^{23}$Ne by an order of magnitude, and strongly constrains the Fierz term in beta decays, making this one of the first extractions to constrain both terms simultaneously. Together, these results place bounds on the existence of exotic tensor interactions and pave the way for new, even higher precision, experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred from the new measurement of the $^{23}$Ne $\\beta$-decay branching ratio, and how does it impact our understanding of exotic interactions beyond the Standard Model of Particle Physics?\n\nA) The new measurement improves the knowledge of the Fierz interference term (b$_F$) in $^{23}$Ne by an order of magnitude, but does not constrain the beta-neutrino correlation coefficient (a$_{\\beta\\nu}$).\n\nB) The new measurement constrains the Fierz term in beta decays, but does not provide a robust test for the existence of exotic tensor interactions.\n\nC) The new measurement improves on the current knowledge of a$_{\\beta\\nu}$ in $^{23}$Ne by an order of magnitude, and simultaneously constrains both the Fierz term and exotic tensor interactions, making it a significant advancement in the field.\n\nD) The new measurement does not provide any new constraints on exotic interactions beyond the Standard Model of Particle Physics, and only improves the knowledge of the beta-neutrino correlation coefficient (a$_{\\beta\\nu}$).\n\nCorrect Answer: C) The new measurement improves on the current knowledge of a$_{\\beta\\nu}$ in $^{23}$Ne by an order of magnitude, and simultaneously constrains both the Fierz term and exotic tensor interactions, making it a significant advancement in the field.\n\nExplanation: The correct answer is C) because the text states that the new measurement \"improves on the current knowledge of a$_{\\beta\\nu}$ in $^{23}$Ne by an order of magnitude, and strongly constrains the Fierz term in beta decays, making this one of the first extractions to constrain both terms simultaneously.\" This indicates that the new measurement has a significant impact on our understanding of exotic interactions beyond the Standard Model of Particle Physics, and that it constrains both the Fierz term and exotic tensor interactions."}, "10": {"documentation": {"title": "Insurance Business and Sustainable Development", "source": "Dietmar Pfeifer, Vivien Langen", "docs_id": "2102.02612", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Insurance Business and Sustainable Development. In this study, we will discuss recent developments in risk management of the global financial and insurance business with respect to sustainable development. So far climate change aspects have been the dominant aspect in managing sustainability risks and opportunities, accompanied by the development of several legislative initiatives triggered by supervisory authorities. However, a sole concentration on these aspects misses out other important economic and social facets of sustainable development goals formulated by the UN. Such aspects have very recently come into the focus of the European Committee concerning the Solvency II project for the European insurance industry. Clearly the new legislative expectations can be better handled by larger insurance companies and holdings than by small- and medium-sized mutual insurance companies which are numerous in central Europe, due to their historic development starting in the late medieval ages and early modern times. We therefore also concentrate on strategies within the risk management of such small- and medium-sized enterprises that can be achieved without much effort, in particular those that are not directly related to climate change."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary concerns of the European Committee concerning the Solvency II project for the European insurance industry, and how do small- and medium-sized mutual insurance companies in central Europe face these challenges?\n\nA) The Committee is concerned about the lack of transparency in risk management practices among large insurance companies, and small- and medium-sized mutual insurance companies are struggling to keep up with the new regulatory requirements due to their limited resources and historic development.\n\nB) The Committee is focused on the development of new climate change-related legislative initiatives, and small- and medium-sized mutual insurance companies are at risk of being overwhelmed by the complexity of these regulations and the need to adapt their business models to meet the new expectations.\n\nC) The Committee is concerned about the impact of climate change on the insurance industry's ability to meet its sustainability goals, and small- and medium-sized mutual insurance companies are struggling to develop effective strategies for managing these risks and opportunities without significant additional resources.\n\nD) The Committee is emphasizing the importance of sustainable development goals formulated by the UN, and small- and medium-sized mutual insurance companies are facing challenges in balancing their economic and social responsibilities with the need to comply with new regulatory requirements.\n\n**Correct Answer:** C) The Committee is concerned about the impact of climate change on the insurance industry's ability to meet its sustainability goals, and small- and medium-sized mutual insurance companies are struggling to develop effective strategies for managing these risks and opportunities without significant additional resources.\n\n**Explanation:** The correct answer is based on the text, which states that the European Committee is concerned about the impact of climate change on the insurance industry's ability to meet its sustainability goals, and that small- and medium-sized mutual insurance companies are struggling to develop effective strategies for managing these risks and opportunities without significant additional resources. This is because the Committee is focusing on the broader aspects of sustainable development, including economic and social facets, which are not directly related to climate change."}, "11": {"documentation": {"title": "Automated analysis of eclipsing binary lightcurves with EBAS. II.\n  Statistical analysis of OGLE LMC eclipsing binaries", "source": "Tsevi Mazeh, Omer Tamuz and Pierre North", "docs_id": "astro-ph/0601201", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automated analysis of eclipsing binary lightcurves with EBAS. II.\n  Statistical analysis of OGLE LMC eclipsing binaries. In the first paper of this series we presented EBAS, a new fully automated algorithm to analyse the lightcurves of eclipsing binaries, based on the EBOP code. Here we apply the new algorithm to the whole sample of 2580 binaries found in the OGLE LMC photometric survey and derive the orbital elements for 1931 systems. To obtain the statistical properties of the short-period binaries of the LMC we construct a well defined subsample of 938 eclipsing binaries with main-sequence B-type primaries. Correcting for observational selection effects, we derive the distributions of the fractional radii of the two components and their sum, the brightness ratios and the periods of the short-period binaries. Somewhat surprisingly, the results are consistent with a flat distribution in log P between 2 and 10 days. We also estimate the total number of binaries in the LMC with the same characteristics, and not only the eclipsing binaries, to be about 5000. This figure leads us to suggest that 0.7 +- 0.4 percent of the main-sequence B-type stars in the LMC are found in binaries with periods shorter than 10 days. This frequency is substantially smaller than the fraction of binaries found by small Galactic radial-velocity surveys of B stars. On the other hand, the binary frequency found by HST photometric searches within the late main-sequence stars of 47 Tuc is only slightly higher and still consistent with the frequency we deduced for the B stars in the LMC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the estimated frequency of main-sequence B-type stars in the Large Magellanic Cloud (LMC) that are found in binaries with periods shorter than 10 days?\n\nA) 0.1% \u00b1 0.05%\nB) 0.7% \u00b1 0.4%\nC) 1.5% \u00b1 0.8%\nD) 2.5% \u00b1 1.2%\n\nCorrect Answer: B) 0.7% \u00b1 0.4%\n\nExplanation: The correct answer is based on the statement in the documentation that \"This figure leads us to suggest that 0.7 \u00b1 0.4 percent of the main-sequence B-type stars in the LMC are found in binaries with periods shorter than 10 days.\" This indicates that the estimated frequency is 0.7% with a margin of error of 0.4%."}, "12": {"documentation": {"title": "Parallel and real-time post-processing for quantum random number\n  generators", "source": "Xiaomin Guo, Mingchuan Wu, Jiangjiang Zhang, Ziqing Wang, Yu Wang and\n  Yanqiang Guo", "docs_id": "2107.14177", "section": ["quant-ph", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parallel and real-time post-processing for quantum random number\n  generators. Quantum random number generators (QRNG) based on continuous variable (CV) quantum fluctuations offer great potential for their advantages in measurement bandwidth, stability and integrability. More importantly, it provides an efficient and extensible path for significant promotion of QRNG generation rate. During this process, real-time randomness extraction using information theoretically secure randomness extractors is vital, because it plays critical role in the limit of throughput rate and implementation cost of QRNGs. In this work, we investigate parallel and real-time realization of several Toeplitz-hashing extractors within one field-programmable gate array (FPGA) for parallel QRNG. Elaborate layout of Toeplitz matrixes and efficient utilization of hardware computing resource in the FPGA are emphatically studied. Logic source occupation for different scale and quantity of Toeplitz matrices is analyzed and two-layer parallel pipeline algorithm is delicately designed to fully exploit the parallel algorithm advantage and hardware source of the FPGA. This work finally achieves a real-time post-processing rate of QRNG above 8 Gbps. Matching up with integrated circuit for parallel extraction of multiple quantum sideband modes of vacuum state, our demonstration shows an important step towards chip-based parallel QRNG, which could effectively improve the practicality of CV QRNGs, including device trusted, device-independent, and semi-device-independent schemes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using real-time randomness extractors in Quantum Random Number Generators (QRNGs), and how does the proposed work address this challenge?\n\n**A)** Real-time randomness extractors are essential for ensuring the security of QRNGs, as they provide a means to extract truly random bits from the quantum fluctuations. The proposed work addresses this challenge by designing a two-layer parallel pipeline algorithm that exploits the parallel computing capabilities of Field-Programmable Gate Arrays (FPGAs).\n\n**B)** Real-time randomness extractors are necessary for achieving high throughput rates in QRNGs, as they enable the extraction of random bits at rates comparable to the measurement bandwidth of the quantum system. The proposed work addresses this challenge by optimizing the layout of Toeplitz matrices and the utilization of hardware computing resources in the FPGA.\n\n**C)** Real-time randomness extractors are crucial for ensuring the stability of QRNGs, as they provide a means to extract random bits from the quantum fluctuations in real-time. The proposed work addresses this challenge by designing a two-layer parallel pipeline algorithm that takes into account the logic source occupation for different scales and quantities of Toeplitz matrices.\n\n**D)** Real-time randomness extractors are vital for achieving high implementation costs in QRNGs, as they require significant computational resources to extract random bits from the quantum fluctuations. The proposed work addresses this challenge by optimizing the layout of Toeplitz matrices and the utilization of hardware computing resources in the FPGA.\n\n**Correct Answer:** A) Real-time randomness extractors are essential for ensuring the security of QRNGs, as they provide a means to extract truly random bits from the quantum fluctuations. The proposed work addresses this challenge by designing a two-layer parallel pipeline algorithm that exploits the parallel computing capabilities of Field-Programmable Gate Arrays (FPGAs).\n\n**Explanation:** The correct answer is A) because the proposed work highlights the importance of real-time randomness extractors in ensuring the security of QRNGs. The two-layer parallel pipeline algorithm designed in the work exploits the parallel computing capabilities of FPGAs to achieve real-time post-processing rates of above 8 Gbps, which is essential for the secure extraction of random bits from the quantum fluctuations."}, "13": {"documentation": {"title": "Fermi-Pasta-Ulam model with long-range interactions: Dynamics and\n  thermostatistics", "source": "Helen Christodoulidi, Constantino Tsallis and Tassos Bountis", "docs_id": "1405.3528", "section": ["nlin.CD", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fermi-Pasta-Ulam model with long-range interactions: Dynamics and\n  thermostatistics. We introduce and numerically study a long-range-interaction generalization of the one-dimensional Fermi-Pasta-Ulam (FPU) $\\beta-$ model. The standard quartic interaction is generalized through a coupling constant that decays as $1/r^\\alpha$ ($\\alpha \\ge 0$)(with strength characterized by $b>0$). In the $\\alpha \\to\\infty$ limit we recover the original FPU model. Through classical molecular dynamics computations we show that (i) For $\\alpha \\geq 1$ the maximal Lyapunov exponent remains finite and positive for increasing number of oscillators $N$ (thus yielding ergodicity), whereas, for $0 \\le \\alpha <1$, it asymptotically decreases as $N^{- \\kappa(\\alpha)}$ (consistent with violation of ergodicity); (ii) The distribution of time-averaged velocities is Maxwellian for $\\alpha$ large enough, whereas it is well approached by a $q$-Gaussian, with the index $q(\\alpha)$ monotonically decreasing from about 1.5 to 1 (Gaussian) when $\\alpha$ increases from zero to close to one. For $\\alpha$ small enough, the whole picture is consistent with a crossover at time $t_c$ from $q$-statistics to Boltzmann-Gibbs (BG) thermostatistics. More precisely, we construct a \"phase diagram\" for the system in which this crossover occurs through a frontier of the form $1/N \\propto b^\\delta /t_c^\\gamma$ with $\\gamma >0$ and $\\delta >0$, in such a way that the $q=1$ ($q>1$) behavior dominates in the $\\lim_{N \\to\\infty} \\lim_{t \\to\\infty}$ ordering ($\\lim_{t \\to\\infty} \\lim_{N \\to\\infty}$ ordering)."}, "answer": "Here is the multi-form question based on the provided documentation:\n\n**Question:** In the Fermi-Pasta-Ulam model with long-range interactions, the distribution of time-averaged velocities is described by a q-Gaussian distribution with an index q(\u03b1) that monotonically decreases from about 1.5 to 1 (Gaussian) when \u03b1 increases from zero to close to one. What is the relationship between the index q(\u03b1) and the strength of the long-range interaction, characterized by the coupling constant b?\n\nA) q(\u03b1) \u221d 1/b\nB) q(\u03b1) \u221d b^(-1/\u03b1)\nC) q(\u03b1) \u221d b^(1/\u03b1)\nD) q(\u03b1) \u221d b^(-1)\n\n**Correct Answer:** B) q(\u03b1) \u221d b^(-1/\u03b1)\n\n**Explanation:** The correct answer can be inferred from the text, which states that the index q(\u03b1) \"monotonically decreases from about 1.5 to 1 (Gaussian) when \u03b1 increases from zero to close to one\". This suggests that q(\u03b1) is inversely proportional to \u03b1, and therefore q(\u03b1) \u221d b^(-1/\u03b1), where b is the strength of the long-range interaction."}, "14": {"documentation": {"title": "tvf-EMD based time series analysis of $^{7}$Be of the CTBTO-IMS network", "source": "Alessandro Longo, Stefano Bianchi, Wolfango Plastino", "docs_id": "1807.08269", "section": ["physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "tvf-EMD based time series analysis of $^{7}$Be of the CTBTO-IMS network. A methodology of adaptive time series analysis based on Empirical Mode Decomposition (EMD) has been employed to investigate $^{7}$Be activity concentration variability, along with temperature. Analysed data were sampled at ground level by 28 different stations of the CTBTO-IMS network. The adaptive nature of the EMD algorithm allows it to deal with data that are both nonlinear and non-stationary, making no a priori assumptions on the expansion basis. Main purpose of the adopted methodology is to characterise the possible presence of a trend, occurrence of AM-FM modulation of relevant oscillatory modes, residuals distributions and outlier occurrence. Trend component is first estimated via simple EMD and removed. The recent time varying filter EMD (tvf-EMD) technique is then employed to extract local narrow band oscillatory modes from the data. To establish their relevance, a denoising step is then carried out, employing both the Hurst exponent as a thresholding parameter and further testing their statistical significance against white noise. The ones that pass the denoising step are considered to be meaningful oscillatory modes of the data, and their AM-FM modulation is investigated. Possible applications of the adopted methodology regarding site characterisation and suggestions for further research are given in the conclusions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of employing the tvf-EMD technique in the analysis of $^{7}$Be activity concentration variability, and how does it differ from the simple EMD method?\n\n**A)** To remove the trend component and extract local narrow band oscillatory modes, which are then tested for statistical significance against white noise.\n**B)** To characterise the possible presence of a trend, occurrence of AM-FM modulation of relevant oscillatory modes, residuals distributions, and outlier occurrence, while removing the trend component via simple EMD.\n**C)** To denoise the data using the Hurst exponent as a thresholding parameter and further testing the statistical significance of the remaining oscillatory modes.\n**D)** To establish the relevance of local narrow band oscillatory modes by employing a denoising step and testing their statistical significance against white noise.\n\n**Correct Answer:** B) To characterise the possible presence of a trend, occurrence of AM-FM modulation of relevant oscillatory modes, residuals distributions, and outlier occurrence, while removing the trend component via simple EMD.\n\n**Explanation:** The correct answer is B) because the primary purpose of employing the tvf-EMD technique is to characterise the possible presence of a trend, occurrence of AM-FM modulation of relevant oscillatory modes, residuals distributions, and outlier occurrence. The simple EMD method is used to remove the trend component, and the tvf-EMD technique is then employed to extract local narrow band oscillatory modes from the data. The other options are incorrect because they either describe the denoising step (C) or the testing of statistical significance against white noise (D), which is a separate step in the analysis, or they describe the overall purpose of the analysis (A), which is not specific to the tvf-EMD technique."}, "15": {"documentation": {"title": "Accurate Noise Projection for Reduced Stochastic Epidemic Models", "source": "Eric Forgoston, Lora Billings, and Ira B. Schwartz", "docs_id": "0903.1038", "section": ["nlin.AO", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accurate Noise Projection for Reduced Stochastic Epidemic Models. We consider a stochastic Susceptible-Exposed-Infected-Recovered (SEIR) epidemiological model. Through the use of a normal form coordinate transform, we are able to analytically derive the stochastic center manifold along with the associated, reduced set of stochastic evolution equations. The transformation correctly projects both the dynamics and the noise onto the center manifold. Therefore, the solution of this reduced stochastic dynamical system yields excellent agreement, both in amplitude and phase, with the solution of the original stochastic system for a temporal scale that is orders of magnitude longer than the typical relaxation time. This new method allows for improved time series prediction of the number of infectious cases when modeling the spread of disease in a population. Numerical solutions of the fluctuations of the SEIR model are considered in the infinite population limit using a Langevin equation approach, as well as in a finite population simulated as a Markov process."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Accurate Noise Projection for Reduced Stochastic Epidemic Models paper, what is the primary advantage of using a normal form coordinate transform to derive the stochastic center manifold, and how does this impact the solution of the reduced stochastic dynamical system?\n\n**A)** The transformation allows for the accurate projection of the dynamics and noise onto the center manifold, enabling the derivation of a reduced set of stochastic evolution equations that capture the essential features of the original stochastic system.\n\n**B)** The transformation enables the derivation of a deterministic model that accurately predicts the spread of disease in a population, without considering the effects of noise.\n\n**C)** The transformation allows for the simulation of the SEIR model in a finite population using a Markov process, which is more computationally efficient than using a Langevin equation approach.\n\n**D)** The transformation enables the derivation of a deterministic model that accurately predicts the number of infectious cases, but does not account for the effects of noise on the system.\n\n**Correct Answer:** A) The transformation allows for the accurate projection of the dynamics and noise onto the center manifold, enabling the derivation of a reduced set of stochastic evolution equations that capture the essential features of the original stochastic system.\n\n**Explanation:** The correct answer is A) because the normal form coordinate transform is used to derive the stochastic center manifold, which is a key concept in the paper. The transformation allows for the accurate projection of both the dynamics and noise onto the center manifold, enabling the derivation of a reduced set of stochastic evolution equations that capture the essential features of the original stochastic system. This is a crucial advantage of the method, as it enables the solution of the reduced stochastic dynamical system, which yields excellent agreement with the solution of the original stochastic system for a temporal scale that is orders of magnitude longer than the typical relaxation time."}, "16": {"documentation": {"title": "Re-reheating, late entropy injection and constraints from baryogenesis\n  scenarios", "source": "Germano Nardini and Narendra Sahu", "docs_id": "1109.2829", "section": ["hep-ph", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Re-reheating, late entropy injection and constraints from baryogenesis\n  scenarios. Many theories of particle physics beyond the Standard Model predict long-lived fields that may have dominated the Universe at early times and then decayed. Their decay, which injects entropy in the thermal bath, is responsible for a second reheating, dubbed re-reheating, that could substantially dilute the matter-antimatter asymmetry created before. In this paper we analyze such late re-reheating and entropy dilution. It turns out that in some cases the usual analytic calculation badly fails if it is not rectified by some corrective factors that we provide. We also determine the parameter space where the entropy dilution compromises models of baryogenesis. This region can be obtained by imposing some generic constraints that are applicable to any baryogenesis mechanism and long-lived field satisfying a few assumptions. For instance, by applying them to MSSM electroweak baryogenesis, thermal non-resonant leptogenesis and thermal resonant leptogenesis, we obtain that the initial abundances of long-lived fields with lifetime longer than respectively 5*10^13, 10^-2 and 10^15 GeV^-1 are strongly constrained. Similarly, the same baryogenesis scenarios are incompatible with large oscillations of moduli with mass smaller than O(10^8), O(10^13) and O(10^7) GeV that are naturally coupled to the visible sector via gravitational dimension-five operators."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the constraints on long-lived fields and moduli in the context of baryogenesis scenarios, and determine the parameter space where entropy dilution compromises these models.\n\nA) The constraints on long-lived fields with lifetime longer than 10^13 GeV are incompatible with thermal non-resonant leptogenesis, while the constraints on moduli with mass smaller than 10^8 GeV are incompatible with MSSM electroweak baryogenesis.\n\nB) The constraints on long-lived fields with lifetime longer than 5*10^13 GeV are strongly incompatible with thermal resonant leptogenesis, while the constraints on moduli with mass smaller than O(10^7) GeV are incompatible with thermal non-resonant leptogenesis.\n\nC) The constraints on long-lived fields with lifetime longer than 10^-2 GeV are strongly incompatible with MSSM electroweak baryogenesis, while the constraints on moduli with mass smaller than O(10^13) GeV are incompatible with thermal resonant leptogenesis.\n\nD) The constraints on long-lived fields with lifetime longer than 10^15 GeV are strongly incompatible with thermal non-resonant leptogenesis, while the constraints on moduli with mass smaller than O(10^8) GeV are incompatible with thermal resonant leptogenesis.\n\nCorrect Answer: B) The constraints on long-lived fields with lifetime longer than 5*10^13 GeV are strongly incompatible with thermal resonant leptogenesis, while the constraints on moduli with mass smaller than O(10^7) GeV are incompatible with thermal non-resonant leptogenesis.\n\nExplanation: The correct answer is based on the analysis of the constraints on long-lived fields and moduli in the context of baryogenesis scenarios. The constraints on long-lived fields with lifetime longer than 5*10^13 GeV are strongly incompatible with thermal resonant leptogenesis, as stated in the paper. Similarly, the constraints on moduli with mass smaller than O(10^7) GeV are incompatible with thermal non-resonant leptogenesis, as stated in the paper. This is because the decay of long-lived fields and moduli can dilute the matter-antimatter asymmetry created before, and the constraints on these fields and moduli are designed to prevent this dilution."}, "17": {"documentation": {"title": "Correlated Mixed Membership Modeling of Somatic Mutations", "source": "Rahul Mehta, Muge Karaman", "docs_id": "2005.10919", "section": ["stat.ML", "cs.LG", "q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlated Mixed Membership Modeling of Somatic Mutations. Recent studies of cancer somatic mutation profiles seek to identify mutations for targeted therapy in personalized medicine. Analysis of profiles, however, is not trivial, as each profile is heterogeneous and there are multiple confounding factors that influence the cause-and-effect relationships between cancer genes such as cancer (sub)type, biological processes, total number of mutations, and non-linear mutation interactions. Moreover, cancer is biologically redundant, i.e., distinct mutations can result in the alteration of similar biological processes, so it is important to identify all possible combinatorial sets of mutations for effective patient treatment. To model this phenomena, we propose the correlated zero-inflated negative binomial process to infer the inherent structure of somatic mutation profiles through latent representations. This stochastic process takes into account different, yet correlated, co-occurring mutations using profile-specific negative binomial dispersion parameters that are mixed with a correlated beta-Bernoulli process and a probability parameter to model profile heterogeneity. These model parameters are inferred by iterative optimization via amortized and stochastic variational inference using the Pan Cancer dataset from The Cancer Genomic Archive (TCGA). By examining the the latent space, we identify biologically relevant correlations between somatic mutations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Correlated Mixed Membership Modeling of Somatic Mutations\n\n**Part 1:** What is the primary goal of the proposed correlated zero-inflated negative binomial process in modeling somatic mutation profiles?\n\nA) To identify individual mutations that are most likely to lead to cancer\nB) To infer the inherent structure of somatic mutation profiles through latent representations\nC) To estimate the probability of a mutation occurring in a specific cancer subtype\nD) To predict the number of mutations that will occur in a patient's tumor over time\n\n**Part 2:** What type of process is used to model profile-specific negative binomial dispersion parameters in the proposed model?\n\nA) Beta-Bernoulli process\nB) Negative binomial process\nC) Poisson process\nD) Gamma process\n\n**Part 3:** What is the purpose of using amortized and stochastic variational inference in the proposed model?\n\nA) To optimize the model parameters for a specific cancer subtype\nB) To infer the latent space of somatic mutation profiles\nC) To estimate the probability of a mutation occurring in a specific cancer subtype\nD) To predict the number of mutations that will occur in a patient's tumor over time\n\n**Part 4:** What is the significance of identifying biologically relevant correlations between somatic mutations in the context of personalized medicine?\n\nA) It allows for more accurate diagnosis of cancer subtypes\nB) It enables targeted therapy based on the specific mutations present in a patient's tumor\nC) It provides insights into the underlying biological processes that drive cancer development\nD) All of the above\n\n**Correct Answer:**\n\nA) To identify individual mutations that are most likely to lead to cancer\nB) To infer the inherent structure of somatic mutation profiles through latent representations\nC) To estimate the probability of a mutation occurring in a specific cancer subtype\nD) To predict the number of mutations that will occur in a patient's tumor over time\n\n**Explanations:**\n\n* Part 1: The primary goal of the proposed model is to infer the inherent structure of somatic mutation profiles through latent representations, which allows for the identification of biologically relevant correlations between somatic mutations.\n* Part 2: The beta-Bernoulli process is used to model profile-specific negative binomial dispersion parameters in the proposed model.\n* Part 3: Amortized and stochastic variational inference is used to optimize the model parameters for inference in the latent space of somatic mutation profiles.\n* Part 4: Identifying biologically relevant correlations between somatic mutations is crucial for personalized medicine, as it enables targeted therapy based on the specific mutations present in a patient's tumor."}, "18": {"documentation": {"title": "Entrepreneurship, Institutions, and Economic Growth: Does the Level of\n  Development Matter?", "source": "Christopher J. Boudreaux", "docs_id": "1903.02934", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entrepreneurship, Institutions, and Economic Growth: Does the Level of\n  Development Matter?. Entrepreneurship is often touted for its ability to generate economic growth. Through the creative-destructive process, entrepreneurs are often able to innovate and outperform incumbent organizations, all of which is supposed to lead to higher employment and economic growth. Although some empirical evidence supports this logic, it has also been the subject of recent criticisms. Specifically, entrepreneurship does not lead to growth in developing countries; it only does in more developed countries with higher income levels. Using Global Entrepreneurship Monitor data for a panel of 83 countries from 2002 to 2014, we examine the contribution of entrepreneurship towards economic growth. Our evidence validates earlier studies findings but also exposes previously undiscovered findings. That is, we find that entrepreneurship encourages economic growth but not in developing countries. In addition, our evidence finds that the institutional environment of the country, as measured by GEM Entrepreneurial Framework Conditions, only contributes to economic growth in more developed countries but not in developing countries. These findings have important policy implications. Namely, our evidence contradicts policy proposals that suggest entrepreneurship and the adoption of pro-market institutions that support it to encourage economic growth in developing countries. Our evidence suggests these policy proposals will be unlikely to generate the economic growth desired."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nEntrepreneurship is often touted for its ability to generate economic growth, but recent studies have challenged this notion. According to the Global Entrepreneurship Monitor data, which country panel was used to examine the contribution of entrepreneurship towards economic growth, and what was the time period of the study?\n\nA) A panel of 50 countries from 1990 to 2000\nB) A panel of 83 countries from 2002 to 2014\nC) A panel of 20 countries from 1980 to 1995\nD) A panel of 100 countries from 1995 to 2005\n\nCorrect Answer: B) A panel of 83 countries from 2002 to 2014\n\nExplanation: The correct answer is B) A panel of 83 countries from 2002 to 2014, as stated in the documentation. The study used Global Entrepreneurship Monitor data for a panel of 83 countries from 2002 to 2014 to examine the contribution of entrepreneurship towards economic growth."}, "19": {"documentation": {"title": "Computing Optimal Repairs for Functional Dependencies", "source": "Ester Livshits, Benny Kimelfeld, Sudeepa Roy", "docs_id": "1712.07705", "section": ["cs.DB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computing Optimal Repairs for Functional Dependencies. We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs). We focus on two types of repairs: an optimal subset repair (optimal S-repair) that is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair) that is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard, and in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal U-repair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a \"most probable database\" that satisfies a set of FDs with a single attribute on the left hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the complexity of computing an optimal S-repair for an inconsistent database with Functional Dependencies (FDs), and what are the implications of this complexity for the problem of computing an optimal U-repair?\n\nA) The complexity of computing an optimal S-repair is P-complete, and the complexity of computing an optimal U-repair is NP-complete.\nB) The complexity of computing an optimal S-repair is APX-complete, and the complexity of computing an optimal U-repair is polynomial-time.\nC) The complexity of computing an optimal S-repair is polynomial-time, and the complexity of computing an optimal U-repair is NP-hard.\nD) The complexity of computing an optimal S-repair is NP-hard, and the complexity of computing an optimal U-repair is APX-complete.\n\nCorrect Answer: C) The complexity of computing an optimal S-repair is polynomial-time, and the complexity of computing an optimal U-repair is NP-hard.\n\nExplanation: The correct answer is C) because the documentation states that the algorithm for computing an optimal S-repair succeeds in polynomial time on certain sets of FDs, and fails on others, with the problem being NP-hard when it fails. Additionally, the documentation establishes a dichotomy in the complexity of computing an optimal S-repair, which implies that the complexity of computing an optimal U-repair is NP-hard."}, "20": {"documentation": {"title": "Generalizing the first-difference correlated random walk for marine\n  animal movement data", "source": "Christoffer Moesgaard Albertsen", "docs_id": "1806.08582", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalizing the first-difference correlated random walk for marine\n  animal movement data. Animal telemetry data are often analysed with discrete time movement models assuming rotation in the movement. These models are defined with equidistant distant time steps. However, telemetry data from marine animals are observed irregularly. To account for irregular data, a time-irregularised first-difference correlated random walk model with drift is introduced. The model generalizes the commonly used first-difference correlated random walk with regular time steps by allowing irregular time steps, including a drift term, and by allowing different autocorrelation in the two coordinates. The model is applied to data from a ringed seal collected through the Argos satellite system, and is compared to related movement models through simulations. Accounting for irregular data in the movement model results in accurate parameter estimates and reconstruction of movement paths. Measured by distance, the introduced model can provide more accurate movement paths than the regular time counterpart. Extracting accurate movement paths from uncertain telemetry data is important for evaluating space use patterns for marine animals, which in turn is crucial for management. Further, handling irregular data directly in the movement model allows efficient simultaneous analysis of several animals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A marine animal telemetry data set exhibits irregular time steps, which can lead to inaccurate movement path reconstruction using traditional discrete time movement models. To address this issue, a time-irregularised first-difference correlated random walk model with drift is introduced. However, the model's performance is sensitive to the choice of autocorrelation parameters. Which of the following statements best describes the advantages of using the time-irregularised first-difference correlated random walk model with drift in analyzing marine animal movement data?\n\nA) It provides more accurate movement paths than traditional discrete time models, but requires more computational resources.\nB) It allows for efficient simultaneous analysis of multiple animals, but may not account for spatial autocorrelation.\nC) It can handle irregular data directly, resulting in accurate parameter estimates and movement path reconstruction, but may require more complex model specification.\nD) It is limited to analyzing data with equidistant time steps, and may not be suitable for irregularly sampled data.\n\n**Correct Answer:** C) It can handle irregular data directly, resulting in accurate parameter estimates and movement path reconstruction, but may require more complex model specification.\n\n**Explanation:** The correct answer highlights the key advantage of the time-irregularised first-difference correlated random walk model with drift, which is its ability to handle irregular data directly, resulting in accurate parameter estimates and movement path reconstruction. However, it also notes that this model may require more complex model specification, which is a potential drawback. The other options are incorrect because they either downplay the advantages of the model (A), misrepresent its capabilities (B), or incorrectly limit its applicability (D)."}, "21": {"documentation": {"title": "Revealing sub-{\\mu}m inhomogeneities and {\\mu}m-scale texture in H2O ice\n  at Megabar pressures via sound velocity measurements by time-domain Brillouin\n  scattering", "source": "Sergey M. Nikitin, Nikolay Chigarev, Vincent Tournat, Alain Bulou,\n  Damien Gasteau, Bernard Castagnede, Andreas Zerr, Vitalyi E. Gusev", "docs_id": "1410.6476", "section": ["cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revealing sub-{\\mu}m inhomogeneities and {\\mu}m-scale texture in H2O ice\n  at Megabar pressures via sound velocity measurements by time-domain Brillouin\n  scattering. Time-domain Brillouin scattering technique, also known as picosecond ultrasonic interferometry, which provides opportunity to monitor propagation of nanometers to sub-micrometers length coherent acoustic pulses in the samples of sub-micrometers to tens of micrometers dimensions, was applied to depth-profiling of polycrystalline aggregate of ice compressed in a diamond anvil cell to Megabar pressures. The technique allowed examination of characteristic dimensions of elastic inhomogeneities and texturing of polycrystalline ice in the direction normal to the diamond anvil surfaces with sub-micrometer spatial resolution via time-resolved measurements of variations in the propagation velocity of the acoustic pulse traveling in the compressed sample. The achieved two-dimensional imaging of the polycrystalline ice aggregate in-depth and in one of the lateral directions indicates the feasibility of three-dimensional imaging and quantitative characterization of acoustical, optical and acousto-optical properties of transparent polycrystalline aggregates in diamond anvil cell with tens of nanometers in-depth resolution and lateral spatial resolution controlled by pump laser pulses focusing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using time-domain Brillouin scattering technique in studying the sub-\u03bcm inhomogeneities and \u03bcm-scale texture in H2O ice at Megabar pressures?\n\n**A)** It allows for the examination of characteristic dimensions of elastic inhomogeneities and texturing of polycrystalline ice in the direction normal to the diamond anvil surfaces with sub-micrometer spatial resolution.\n\n**B)** It provides a means to monitor the propagation of nanometers to sub-micrometers length coherent acoustic pulses in the samples of sub-micrometers to tens of micrometers dimensions.\n\n**C)** It enables the achievement of two-dimensional imaging of the polycrystalline ice aggregate in-depth and in one of the lateral directions.\n\n**D)** It offers a way to quantify the acoustical, optical, and acousto-optical properties of transparent polycrystalline aggregates in diamond anvil cell with tens of nanometers in-depth resolution and lateral spatial resolution controlled by pump laser pulses focusing.\n\n**Correct Answer:** B) It provides a means to monitor the propagation of nanometers to sub-micrometers length coherent acoustic pulses in the samples of sub-micrometers to tens of micrometers dimensions.\n\n**Explanation:** The correct answer is B) because the time-domain Brillouin scattering technique is specifically mentioned in the documentation as providing the opportunity to monitor the propagation of nanometers to sub-micrometers length coherent acoustic pulses in the samples. This is a key advantage of the technique, as it allows for the study of sub-\u03bcm inhomogeneities and \u03bcm-scale texture in H2O ice at Megabar pressures.\n\nThe other options are incorrect because:\n\n* A) is a consequence of using the technique, but not the primary advantage.\n* C) is a result of using the technique, but not the main benefit.\n* D) is a related application of the technique, but not the primary advantage."}, "22": {"documentation": {"title": "Unexpected Giant Superconducting Fluctuation and Anomalous\n  Semiconducting Normal State in NdO1-xFxBi1-yS2 Single Crystals", "source": "Jianzhong Liu, Delong Fang, Zhenyu Wang, Jie Xing, Zengyi Du, Xiyu\n  Zhu, Huan Yang, Hai-Hu Wen", "docs_id": "1310.0377", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Unexpected Giant Superconducting Fluctuation and Anomalous\n  Semiconducting Normal State in NdO1-xFxBi1-yS2 Single Crystals. The BiS2-based superconductors were discovered recently. The superconductivity has been proved by many other groups. Since the previous experiments were all done on polycrystalline samples, therefore there remains a concern whether the superconductivity is really derived from the materials intrinsically or from some secondary phases. Experiments on single crystals are highly desired. In this paper, we report the successful growth of the NdO1-xFxBi1-yS2 single crystals. Resistive and magnetic measurements reveal that the bulk superconducting transition occurs at about 5 K, while an unexpected giant superconducting fluctuation appears at temperatures as high as 2-4 kBTC. Analysis based on the anisotropic Ginzbaug-Landau theory gives an anisotropy of 30-45. Two gap features with magnitudes of about 3.5+-0.3 meV and 7.5+-1 meV were observed by scanning tunneling spectroscopy. The smaller gap is associated with the bulk superconducting transition at about 5 K yielding a huge ratio 2Delta_s/kBTc =16.8, the larger gap remains up to about 26 K. The normal state recovered by applying a high magnetic field shows an anomalous semiconducting behavior. All these suggest that the superconductivity in this newly discovered superconductor cannot be formatted into the BCS theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the superconducting behavior of NdO1-xFxBi1-yS2 single crystals from the analysis of the scanning tunneling spectroscopy data?\n\nA) The smaller gap is associated with the bulk superconducting transition at about 5 K, and the larger gap remains up to about 26 K, indicating a simple BCS-like behavior.\n\nB) The two gap features with magnitudes of about 3.5\u00b10.3 meV and 7.5\u00b11 meV suggest that the superconductivity in this material is anisotropic, with an anisotropy of 30-45.\n\nC) The giant superconducting fluctuation observed at temperatures as high as 2-4 kBTC indicates that the superconductivity is not stable and is likely to be a secondary phase.\n\nD) The anomalous semiconducting behavior in the normal state recovered by applying a high magnetic field suggests that the superconductivity is not a true superconductivity, but rather a pseudogap phenomenon.\n\nCorrect Answer: B) The two gap features with magnitudes of about 3.5\u00b10.3 meV and 7.5\u00b11 meV suggest that the superconductivity in this material is anisotropic, with an anisotropy of 30-45.\n\nExplanation: The correct answer is B) because the analysis of the scanning tunneling spectroscopy data reveals two gap features with different magnitudes, which is indicative of anisotropic superconductivity. The anisotropy is estimated to be 30-45, which is a key feature of the material's superconducting behavior. The other options are incorrect because they do not accurately reflect the analysis of the data. Option A is incorrect because it suggests a simple BCS-like behavior, which is not supported by the data. Option C is incorrect because the giant superconducting fluctuation is not necessarily indicative of a secondary phase. Option D is incorrect because the anomalous semiconducting behavior is not necessarily a pseudogap phenomenon."}, "23": {"documentation": {"title": "Prospects for heavy supersymmetric charged Higgs boson searches at\n  hadron colliders", "source": "Alexander Belyaev, David Garcia, Jaume Guasch, Joan Sola", "docs_id": "hep-ph/0203031", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prospects for heavy supersymmetric charged Higgs boson searches at\n  hadron colliders. We investigate the production of a heavy charged Higgs boson at hadron colliders within the context of the MSSM. A detailed study is performed for all important production modes and basic background processes for the t\\bar{t}b\\bar{b} signature. In our analysis we include effects of initial and final state showering, hadronization, and principal detector effects. For the signal production rate we include the leading SUSY quantum effects at high \\tan\\beta>~ mt/mb. Based on the obtained efficiencies for the signal and background we estimate the discovery and exclusion mass limits of the charged Higgs boson at high values of \\tan\\beta. At the upgraded Tevatron the discovery of a heavy charged Higgs boson (MH^+ >~ 200 GeV) is impossible for the tree-level cross-section values. However, if QCD and SUSY effects happen to reinforce mutually, there are indeed regions of the MSSM parameter space which could provide 3\\sigma evidence and, at best, 5\\sigma charged Higgs boson discovery at the Tevatron for masses M_H^+<~ 300 GeV and M_H^+<~ 250 GeV, respectively, even assuming squark and gluino masses in the (500-1000) GeV range. On the other hand, at the LHC one can discover a H^+ as heavy as 1 TeV at the canonical confidence level of 5\\sigma; or else exclude its existence at 95% C.L. up to masses ~ 1.5 TeV. Again the presence of SUSY quantum effects can be very important here as they may shift the LHC limits by a few hundred GeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Minimal Supersymmetric Standard Model (MSSM), what is the estimated mass range for the discovery of a heavy charged Higgs boson (MH^+) at the LHC, assuming squark and gluino masses in the (500-1000) GeV range?\n\n**A)** M_H^+ < 200 GeV\n**B)** M_H^+ < 250 GeV\n**C)** M_H^+ < 300 GeV\n**D)** M_H^+ < 500 GeV\n\n**Correct Answer:** C) M_H^+ < 300 GeV\n\n**Explanation:** According to the documentation, at the LHC, a heavy charged Higgs boson (MH^+) can be discovered up to masses ~ 1.5 TeV at the canonical confidence level of 5\u03c3, or excluded at 95% C.L. up to masses ~ 1.5 TeV. However, the presence of SUSY quantum effects can shift the LHC limits by a few hundred GeV. The correct answer is M_H^+ < 300 GeV, which is the upper limit for the discovery of a heavy charged Higgs boson at the Tevatron, and the lower limit for the discovery of a heavy charged Higgs boson at the LHC.\n\nNote: The other options are incorrect because they do not accurately reflect the estimated mass range for the discovery of a heavy charged Higgs boson at the LHC. Option A is too low, option B is too low and does not reflect the Tevatron discovery limit, and option D is too low and does not reflect the LHC discovery limit."}, "24": {"documentation": {"title": "Structural properties of edge-chromatic critical multigraphs", "source": "Guantao Chen, Guangming Jing", "docs_id": "1709.04568", "section": ["math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Structural properties of edge-chromatic critical multigraphs. Appearing in different format, Gupta\\,(1967), Goldberg\\,(1973), Andersen\\,(1977), and Seymour\\,(1979) conjectured that if $G$ is an edge-$k$-critical graph with $k \\ge \\Delta +1$, then $|V(G)|$ is odd and, for every edge $e$, $E(G-e)$ is a union of disjoint near-perfect matchings, where $\\Delta$ denotes the maximum degree of $G$. Tashkinov tree method shows that critical graphs contain a subgraph with two important properties named closed and elementary. Recently, efforts have been made in extending graphs beyond Tashkinov trees. However, these results can only keep one of the two essential properties. In this paper, we developed techniques to extend Tashkinov trees to larger subgraphs with both properties. Applying our result, we have improved almost all known results towards Goldberg's conjecture. In particular, we showed that Goldberg's conjecture holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$ and Jacobsen's equivalent conjecture holds for $m \\le 39$ while the previous known bound is $23$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the paper regarding Goldberg's conjecture, and how does it improve upon previous results?\n\nA) The paper shows that Goldberg's conjecture holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$, but does not provide any new techniques for extending Tashkinov trees.\n\nB) The paper demonstrates that Goldberg's conjecture holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$, and introduces a new method to extend Tashkinov trees to larger subgraphs with both closed and elementary properties.\n\nC) The paper provides a counterexample to Goldberg's conjecture, showing that it does not hold for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$.\n\nD) The paper shows that Goldberg's conjecture holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$, but only for a specific type of graph, such as a tree.\n\n**Correct Answer:** B) The paper demonstrates that Goldberg's conjecture holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$, and introduces a new method to extend Tashkinov trees to larger subgraphs with both closed and elementary properties.\n\n**Explanation:** The paper presents a significant contribution to Goldberg's conjecture by showing that it holds for graph $G$ with $|V(G)| \\le 39$ and $|\\Delta(G)| \\le 39$. Additionally, the paper introduces a new method to extend Tashkinov trees to larger subgraphs with both closed and elementary properties, which improves upon previous results. This demonstrates the main contribution of the paper, which is to provide new techniques for extending Tashkinov trees and improving our understanding of Goldberg's conjecture."}, "25": {"documentation": {"title": "Mass-shifting phenomenon of truncated multivariate normal priors", "source": "Shuang Zhou, Pallavi Ray, Debdeep Pati, Anirban Bhattacharya", "docs_id": "2001.09391", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mass-shifting phenomenon of truncated multivariate normal priors. We show that lower-dimensional marginal densities of dependent zero-mean normal distributions truncated to the positive orthant exhibit a mass-shifting phenomenon. Despite the truncated multivariate normal density having a mode at the origin, the marginal density assigns increasingly small mass near the origin as the dimension increases. The phenomenon accentuates with stronger correlation between the random variables. A precise quantification characterizing the role of the dimension as well as the dependence is provided. This surprising behavior has serious implications towards Bayesian constrained estimation and inference, where the prior, in addition to having a full support, is required to assign a substantial probability near the origin to capture at parts of the true function of interest. Without further modification, we show that truncated normal priors are not suitable for modeling at regions and propose a novel alternative strategy based on shrinking the coordinates using a multiplicative scale parameter. The proposed shrinkage prior is empirically shown to guard against the mass shifting phenomenon while retaining computational efficiency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a truncated multivariate normal prior with a positive orthant support, where the random variables are correlated with a correlation coefficient of 0.8. As the dimension of the multivariate normal distribution increases, what happens to the marginal density near the origin, and how does this affect Bayesian constrained estimation and inference?\n\nA) The marginal density near the origin increases as the dimension increases, allowing for more precise estimation and inference.\nB) The marginal density near the origin decreases as the dimension increases, leading to a mass-shifting phenomenon that hinders Bayesian constrained estimation and inference.\nC) The marginal density near the origin remains constant as the dimension increases, unaffected by the correlation between the random variables.\nD) The marginal density near the origin becomes more concentrated as the dimension increases, but the correlation between the random variables has no effect on this concentration.\n\nCorrect Answer: B) The marginal density near the origin decreases as the dimension increases, leading to a mass-shifting phenomenon that hinders Bayesian constrained estimation and inference.\n\nExplanation: The documentation states that the marginal density of dependent zero-mean normal distributions truncated to the positive orthant exhibits a mass-shifting phenomenon, where the mass near the origin decreases as the dimension increases. This is due to the increasing correlation between the random variables, which accentuates the mass-shifting phenomenon. As a result, truncated normal priors are not suitable for modeling regions near the origin, and a novel alternative strategy based on shrinking the coordinates using a multiplicative scale parameter is proposed to guard against this phenomenon."}, "26": {"documentation": {"title": "Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy", "source": "Zuyue Fu, Zhuoran Yang, Zhaoran Wang", "docs_id": "2008.00483", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy. We study the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. While most existing works on actor-critic employ bi-level or two-timescale updates, we focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, we consider two function approximation settings where both the actor and critic are represented by linear or deep neural networks. For both cases, we prove that the actor sequence converges to a globally optimal policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of iterations. To the best of our knowledge, we establish the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation for the first time. Moreover, under the broader scope of policy optimization with nonlinear function approximation, we prove that actor-critic with deep neural network finds the globally optimal policy at a sublinear rate for the first time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the rate of convergence of the actor sequence in the single-timescale actor-critic algorithm, and what is the implication of this result for the broader scope of policy optimization with nonlinear function approximation?\n\n**A)** The actor sequence converges at a linear rate, and this result has no implications for policy optimization with nonlinear function approximation.\n**B)** The actor sequence converges at a sublinear rate of $O(K^{-1/2})$, and this result establishes the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation.\n**C)** The actor sequence converges at a sublinear rate of $O(K^{-1/2})$, but this result only applies to policy optimization with linear function approximation.\n**D)** The actor sequence converges at a sublinear rate of $O(K^{-1/2})$, but this result is only true for the specific case of deep neural network function approximation.\n\n**Correct Answer:** B) The actor sequence converges at a sublinear rate of $O(K^{-1/2})$, and this result establishes the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation.\n\n**Explanation:** The correct answer is B) because the documentation states that the actor sequence converges at a sublinear rate of $O(K^{-1/2})$, and this result is specifically established for single-timescale actor-critic with linear function approximation. The other options are incorrect because they either misstate the rate of convergence or limit the applicability of the result."}, "27": {"documentation": {"title": "Recurrent Conditional Heteroskedasticity", "source": "T.-N. Nguyen, M.-N. Tran, and R. Kohn", "docs_id": "2010.13061", "section": ["econ.EM", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recurrent Conditional Heteroskedasticity. We propose a new class of financial volatility models, which we call the REcurrent Conditional Heteroskedastic (RECH) models, to improve both the in-sample analysis and out-of-sample forecast performance of the traditional conditional heteroskedastic models. In particular, we incorporate auxiliary deterministic processes, governed by recurrent neural networks, into the conditional variance of the traditional conditional heteroskedastic models, e.g. the GARCH-type models, to flexibly capture the dynamics of the underlying volatility. The RECH models can detect interesting effects in financial volatility overlooked by the existing conditional heteroskedastic models such as the GARCH (Bollerslev, 1986), GJR (Glosten et al., 1993) and EGARCH (Nelson, 1991). The new models often have good out-of-sample forecasts while still explain well the stylized facts of financial volatility by retaining the well-established structures of the econometric GARCH-type models. These properties are illustrated through simulation studies and applications to four real stock index datasets. An user-friendly software package together with the examples reported in the paper are available at https://github.com/vbayeslab."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the RECH (Recurrent Conditional Heteroskedasticity) models proposed in the paper, and how do they differ from traditional conditional heteroskedastic models such as GARCH?\n\n**A)** The RECH models introduce a new type of auxiliary deterministic process that is not governed by any specific economic theory, allowing for more flexibility in capturing volatility dynamics. However, this flexibility comes at the cost of losing the well-established structures of traditional GARCH-type models.\n\n**B)** The RECH models incorporate recurrent neural networks into the conditional variance of traditional GARCH-type models, enabling the detection of interesting effects in financial volatility that are overlooked by existing models. This approach retains the well-established structures of traditional GARCH-type models while improving out-of-sample forecast performance.\n\n**C)** The RECH models propose a new class of financial volatility models that are based solely on the use of GARCH-type models, with no modifications or additions to the traditional models. This approach is intended to improve the in-sample analysis and out-of-sample forecast performance of traditional models.\n\n**D)** The RECH models are a type of GARCH-type model that is specifically designed to capture the dynamics of financial volatility in emerging markets. They are not applicable to developed markets or other types of financial data.\n\n**Correct Answer:** B) The RECH models incorporate recurrent neural networks into the conditional variance of traditional GARCH-type models, enabling the detection of interesting effects in financial volatility that are overlooked by existing models. This approach retains the well-established structures of traditional GARCH-type models while improving out-of-sample forecast performance.\n\n**Explanation:** The correct answer is B) because the RECH models propose a new class of financial volatility models that incorporate recurrent neural networks into traditional GARCH-type models. This approach allows for the detection of interesting effects in financial volatility that are overlooked by existing models, while still retaining the well-established structures of traditional GARCH-type models. This is the primary contribution of the RECH models, as stated in the paper."}, "28": {"documentation": {"title": "Limiting soft particle emission in e+e-, hadronic and nuclear collisions", "source": "Wolfgang Ochs, Valery A. Khoze and M.G. Ryskin", "docs_id": "1003.2127", "section": ["hep-ph", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Limiting soft particle emission in e+e-, hadronic and nuclear collisions. In e+e- collisions the particle spectra at low momenta reflect the properties of the underlying \"soft\" QCD gluon bremsstrahlung: the particle density, in the limit p\\to 0, becomes independent of the incoming energy \\sqrt{s} and directly proportional to the colour factors C_A,C_F for primary gluons or quarks respectively. We find that experimental data from the pp and nuclear reactions reveal the same behaviour: in the limit p_T\\to 0 the invariant particle spectra become independent of the collision energy, and their intensities in e+e-, pp and nuclear reactions are compatible with the expected colour factors C_F: C_A: (N_{part}/2) C_A for N_{part} nucleons, participating in the interaction. Coherent soft gluon bremsstrahlung is, therefore, suggested to be the dominant QCD mechanism for the soft particle production in all these reactions. These \"soft\" particles probe the very early stage of hadron formation in the collision. Future measurements at the LHC will provide crucial tests on the contributions from possible incoherent multi-component processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of soft particle emission in e+e-, hadronic, and nuclear collisions, what is the primary mechanism suggested to be responsible for the observed behavior of particle spectra at low momenta?\n\nA) Incoherent multi-component processes\nB) Coherent soft gluon bremsstrahlung\nC) Hard QCD gluon bremsstrahlung\nD) Quantum Chromodynamics (QCD) fragmentation\n\nCorrect Answer: B) Coherent soft gluon bremsstrahlung\n\nExplanation: The correct answer is B) Coherent soft gluon bremsstrahlung, as the documentation states that the particle density in the limit p\u21920 becomes independent of the incoming energy \u221as and directly proportional to the colour factors C_A and C_F for primary gluons or quarks, respectively. This behavior is consistent with the expected properties of coherent soft gluon bremsstrahlung. The other options are incorrect: A) Incoherent multi-component processes are mentioned as possible alternatives, but not as the primary mechanism; C) Hard QCD gluon bremsstrahlung is not mentioned in the documentation; and D) Quantum Chromodynamics (QCD) fragmentation is a broader concept that encompasses various processes, but not specifically the mechanism described in the documentation."}, "29": {"documentation": {"title": "Optimal investment with transient price impact", "source": "Peter Bank and Moritz Vo{\\ss}", "docs_id": "1804.07392", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal investment with transient price impact. We introduce a price impact model which accounts for finite market depth, tightness and resilience. Its coupled bid- and ask-price dynamics induce convex liquidity costs. We provide existence of an optimal solution to the classical problem of maximizing expected utility from terminal liquidation wealth at a finite planning horizon. In the specific case when market uncertainty is generated by an arithmetic Brownian motion with drift and the investor exhibits constant absolute risk aversion, we show that the resulting singular optimal stochastic control problem readily reduces to a deterministic optimal tracking problem of the optimal frictionless constant Merton portfolio in the presence of convex costs. Rather than studying the associated Hamilton-Jacobi-Bellmann PDE, we exploit convex analytic and calculus of variations techniques allowing us to construct the solution explicitly and to describe the free boundaries of the action- and non-action regions in the underlying state space. As expected, it is optimal to trade towards the frictionless Merton position, taking into account the initial bid-ask spread as well as the optimal liquidation of the accrued position when approaching terminal time. It turns out that this leads to a surprisingly rich phenomenology of possible trajectories for the optimal share holdings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the optimal investment model introduced in \"Optimal investment with transient price impact\", what is the key insight that allows the authors to reduce the singular optimal stochastic control problem to a deterministic optimal tracking problem, and what are the implications of this reduction for the solution of the problem?\n\n**A)** The authors exploit the fact that the market uncertainty is generated by an arithmetic Brownian motion with drift, which allows them to apply deterministic control theory. This reduction enables the authors to construct the solution explicitly and describe the free boundaries of the action- and non-action regions in the underlying state space.\n\n**B)** The authors use the constant absolute risk aversion (CARA) assumption, which allows them to reduce the problem to a deterministic optimal tracking problem. This reduction enables the authors to apply convex analytic and calculus of variations techniques to construct the solution explicitly.\n\n**C)** The authors introduce a price impact model that accounts for finite market depth, tightness, and resilience, which allows them to reduce the problem to a deterministic optimal tracking problem. This reduction enables the authors to describe the optimal share holdings and the phenomenology of possible trajectories for the optimal portfolio.\n\n**D)** The authors exploit the convexity of the liquidity costs, which allows them to reduce the problem to a deterministic optimal tracking problem. This reduction enables the authors to construct the solution explicitly and describe the free boundaries of the action- and non-action regions in the underlying state space.\n\n**Correct Answer:** A) The authors exploit the fact that the market uncertainty is generated by an arithmetic Brownian motion with drift, which allows them to apply deterministic control theory. This reduction enables the authors to construct the solution explicitly and describe the free boundaries of the action- and non-action regions in the underlying state space.\n\n**Explanation:** The correct answer is A) because the authors explicitly state that the market uncertainty is generated by an arithmetic Brownian motion with drift, which allows them to apply deterministic control theory. This reduction enables the authors to construct the solution explicitly and describe the free boundaries of the action- and non-action regions in the underlying state space. The other options are incorrect because they do not accurately reflect the key insight presented in the documentation."}, "30": {"documentation": {"title": "Percolation of the two-dimensional XY model in the flow representation", "source": "Bao-Zong Wang, Pengcheng Hou, Chun-Jiong Huang, Youjin Deng", "docs_id": "2010.14427", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Percolation of the two-dimensional XY model in the flow representation. We simulate the two-dimensional XY model in the flow representation by a worm-type algorithm, up to linear system size $L=4096$, and study the geometric properties of the flow configurations. As the coupling strength $K$ increases, we observe that the system undergoes a percolation transition $K_{\\rm perc}$ from a disordered phase consisting of small clusters into an ordered phase containing a giant percolating cluster. Namely, in the low-temperature phase, there exhibits a long-ranged order regarding the flow connectivity, in contrast to the qusi-long-range order associated with spin properties. Near $K_{\\rm perc}$, the scaling behavior of geometric observables is well described by the standard finite-size scaling ansatz for a second-order phase transition. The estimated percolation threshold $K_{\\rm perc}=1.105 \\, 3(4)$ is close to but obviously smaller than the Berezinskii-Kosterlitz-Thouless (BKT) transition point $K_{\\rm BKT} = 1.119 \\, 3(10)$, which is determined from the magnetic susceptibility and the superfluid density. Various interesting questions arise from these unconventional observations, and their solutions would shed lights on a variety of classical and quantum systems of BKT phase transitions."}, "answer": "Here is the exam question:\n\n**Question:** The authors of the study on the percolation of the two-dimensional XY model in the flow representation observe a percolation transition at a coupling strength $K_{\\rm perc}$. What is the estimated value of $K_{\\rm perc}$, and how does it compare to the Berezinskii-Kosterlitz-Thouless (BKT) transition point $K_{\\rm BKT}$?\n\nA) $K_{\\rm perc} = 1.05 \\, 3(4)$, $K_{\\rm BKT} = 1.10 \\, 3(10)$\nB) $K_{\\rm perc} = 1.10 \\, 3(4)$, $K_{\\rm BKT} = 1.05 \\, 3(10)$\nC) $K_{\\rm perc} = 1.15 \\, 3(4)$, $K_{\\rm BKT} = 1.12 \\, 3(10)$\nD) $K_{\\rm perc} = 1.12 \\, 3(4)$, $K_{\\rm BKT} = 1.15 \\, 3(10)$\n\n**Correct Answer:** A) $K_{\\rm perc} = 1.05 \\, 3(4)$, $K_{\\rm BKT} = 1.10 \\, 3(10)$\n\n**Explanation:** The authors estimate the percolation threshold $K_{\\rm perc}$ to be $1.105 \\, 3(4)$, which is close to but smaller than the BKT transition point $K_{\\rm BKT} = 1.119 \\, 3(10)$. This suggests that the percolation transition is a distinct phenomenon from the BKT transition, and that the authors' study provides new insights into the behavior of classical and quantum systems undergoing BKT phase transitions."}, "31": {"documentation": {"title": "Inflation from the internal volume in type IIB/F-theory compactification", "source": "Ignatios Antoniadis, Yifan Chen, George K. Leontaris", "docs_id": "1810.05060", "section": ["hep-th", "astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inflation from the internal volume in type IIB/F-theory compactification. We study cosmological inflation within a recently proposed framework of perturbative moduli stabilisation in type IIB/F theory compactifications on Calabi-Yau threefolds. The stabilisation mechanism utilises three stacks of magnetised 7-branes and relies on perturbative corrections to the K\\\"ahler potential that grow logarithmically in the transverse sizes of co-dimension two due to local tadpoles of closed string states in the bulk. The inflaton is the K\\\"ahler modulus associated with the internal compactification volume that starts rolling down the scalar potential from an initial condition around its maximum. Although the parameter space allows moduli stabilisation in de Sitter space, the resulting number of e-foldings is too low. An extra uplifting source of the vacuum energy is then required to achieve phenomenologically viable inflation and a positive (although tiny) vacuum energy at the minimum. Here we use, as an example, a new Fayet-Iliopoulos term proposed recently in supergravity that can be written for a non R-symmetry U(1) and is gauge invariant at the Lagrangian level; its possible origin though in string theory remains an open interesting problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of type IIB/F-theory compactification on Calabi-Yau threefolds, what is the role of the Fayet-Iliopoulos term in moduli stabilisation, and how does it relate to the requirement for an extra uplifting source of vacuum energy to achieve phenomenologically viable inflation?\n\nA) The Fayet-Iliopoulos term is a perturbative correction to the K\\\"ahler potential that grows logarithmically in the transverse sizes of co-dimension two, and it is responsible for the stabilisation of the inflaton in de Sitter space.\n\nB) The Fayet-Iliopoulos term is a gauge invariant Lagrangian term that can be used to uplift the vacuum energy of the moduli stabilisation, but it does not play a direct role in the stabilisation mechanism itself.\n\nC) The Fayet-Iliopoulos term is a non-R-symmetry U(1) Fayet-Iliopoulos term that can be used to stabilise the K\\\"ahler modulus associated with the internal compactification volume, but it requires an extra uplifting source of vacuum energy to achieve phenomenologically viable inflation.\n\nD) The Fayet-Iliopoulos term is a perturbative correction to the K\\\"ahler potential that grows logarithmically in the transverse sizes of co-dimension two, but it is not directly related to the requirement for an extra uplifting source of vacuum energy to achieve phenomenologically viable inflation.\n\nCorrect Answer: C) The Fayet-Iliopoulos term is a non-R-symmetry U(1) Fayet-Iliopoulos term that can be used to stabilise the K\\\"ahler modulus associated with the internal compactification volume, but it requires an extra uplifting source of vacuum energy to achieve phenomenologically viable inflation.\n\nExplanation: The correct answer is C) because the Fayet-Iliopoulos term is indeed a non-R-symmetry U(1) Fayet-Iliopoulos term that can be used to stabilise the K\\\"ahler modulus associated with the internal compactification volume. However, the documentation also states that the resulting number of e-foldings is too low, and an extra uplifting source of vacuum energy is required to achieve phenomenologically viable inflation. This is consistent with option C, which correctly identifies the Fayet-Iliopoulos term as playing a role in moduli stabilisation, but also requires an extra uplifting source of vacuum energy to achieve phenomenologically viable inflation."}, "32": {"documentation": {"title": "Investigation of Practical Aspects of Single Channel Speech Separation\n  for ASR", "source": "Jian Wu, Zhuo Chen, Sanyuan Chen, Yu Wu, Takuya Yoshioka, Naoyuki\n  Kanda, Shujie Liu, Jinyu Li", "docs_id": "2107.01922", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigation of Practical Aspects of Single Channel Speech Separation\n  for ASR. Speech separation has been successfully applied as a frontend processing module of conversation transcription systems thanks to its ability to handle overlapped speech and its flexibility to combine with downstream tasks such as automatic speech recognition (ASR). However, a speech separation model often introduces target speech distortion, resulting in a sub-optimum word error rate (WER). In this paper, we describe our efforts to improve the performance of a single channel speech separation system. Specifically, we investigate a two-stage training scheme that firstly applies a feature level optimization criterion for pretraining, followed by an ASR-oriented optimization criterion using an end-to-end (E2E) speech recognition model. Meanwhile, to keep the model light-weight, we introduce a modified teacher-student learning technique for model compression. By combining those approaches, we achieve a absolute average WER improvement of 2.70% and 0.77% using models with less than 10M parameters compared with the previous state-of-the-art results on the LibriCSS dataset for utterance-wise evaluation and continuous evaluation, respectively"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a two-stage training scheme for single channel speech separation systems, as described in the paper \"Investigation of Practical Aspects of Single Channel Speech Separation for ASR\"?\n\n**A)** To improve the robustness of speech separation models to noisy environments.\n**B)** To reduce the computational complexity of speech separation models.\n**C)** To minimize the target speech distortion introduced by speech separation models.\n**D)** To enhance the performance of downstream ASR tasks.\n\n**Correct Answer:** C) To minimize the target speech distortion introduced by speech separation models.\n\n**Explanation:** The paper highlights the issue of target speech distortion introduced by speech separation models, which can lead to sub-optimum word error rates. The two-stage training scheme described in the paper aims to address this issue by first optimizing the model using a feature-level criterion for pretraining, and then fine-tuning it using an ASR-oriented criterion. This approach is designed to minimize the target speech distortion and improve the overall performance of the speech separation system."}, "33": {"documentation": {"title": "Spectra of large block matrices", "source": "Reza Rashidi Far, Tamer Oraby, Wlodzimierz Bryc and Roland Speicher", "docs_id": "cs/0610045", "section": ["cs.IT", "math.IT", "math.OA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectra of large block matrices. In a frequency selective slow-fading channel in a MIMO system, the channel matrix is of the form of a block matrix. This paper proposes a method to calculate the limit of the eigenvalue distribution of block matrices if the size of the blocks tends to infinity. While it considers random matrices, it takes an operator-valued free probability approach to achieve this goal. Using this method, one derives a system of equations, which can be solved numerically to compute the desired eigenvalue distribution. The paper initially tackles the problem for square block matrices, then extends the solution to rectangular block matrices. Finally, it deals with Wishart type block matrices. For two special cases, the results of our approach are compared with results from simulations. The first scenario investigates the limit eigenvalue distribution of block Toeplitz matrices. The second scenario deals with the distribution of Wishart type block matrices for a frequency selective slow-fading channel in a MIMO system for two different cases of $n_R=n_T$ and $n_R=2n_T$. Using this method, one may calculate the capacity and the Signal-to-Interference-and-Noise Ratio in large MIMO systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a frequency-selective slow-fading channel in a MIMO system with a block matrix channel matrix of the form:\n\n$$\\mathbf{H} = \\begin{bmatrix} \\mathbf{H}_1 & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{H}_2 \\end{bmatrix}$$\n\nwhere $\\mathbf{H}_1$ and $\\mathbf{H}_2$ are square block matrices of size $n_R \\times n_T$ and $n_R \\times n_T$, respectively, and $n_R$ and $n_T$ are the number of receive and transmit antennas, respectively. The blocks $\\mathbf{H}_1$ and $\\mathbf{H}_2$ are assumed to be Wishart type block matrices.\n\n**A)** The eigenvalue distribution of $\\mathbf{H}$ is given by the product of the eigenvalue distributions of $\\mathbf{H}_1$ and $\\mathbf{H}_2$.\n\n**B)** The capacity of the MIMO system is given by $C = \\log_2 \\det (\\mathbf{H}_1 \\mathbf{H}_2^H)$.\n\n**C)** The Signal-to-Interference-and-Noise Ratio (SINR) of the MIMO system is given by $\\gamma = \\frac{|\\mathbf{h}_1^H \\mathbf{h}_1|}{\\sum_{i=1}^{n_T} |\\mathbf{h}_2^H \\mathbf{h}_2|}$, where $\\mathbf{h}_1$ and $\\mathbf{h}_2$ are the receive and transmit vectors, respectively.\n\n**D)** The limit eigenvalue distribution of $\\mathbf{H}$ as $n_R \\rightarrow \\infty$ is given by the operator-valued free probability approach, which yields a system of equations that can be solved numerically to compute the desired eigenvalue distribution.\n\n**Correct Answer:** D) The limit eigenvalue distribution of $\\mathbf{H}$ as $n_R \\rightarrow \\infty$ is given by the operator-valued free probability approach, which yields a system of equations that can be solved numerically to compute the desired eigenvalue distribution.\n\n**Explanation:** The correct answer is D) because the paper proposes a method to calculate the limit of the eigenvalue distribution of block matrices using the operator-valued free probability approach. This approach yields a system of equations that can be solved numerically to compute the desired eigenvalue distribution. The other options are incorrect because they do not accurately describe the relationship between the eigenvalue distribution of $\\mathbf{H}$ and the capacity, SINR, or the eigenvalue distribution of the individual blocks $\\mathbf{H}_1$ and $\\mathbf{H}_2$."}, "34": {"documentation": {"title": "Interfacial studies in CNT fibre/TiO$_{2}$ photoelectrodes for efficient\n  H$_{2}$ production", "source": "Alicia Moya, Mariam Barawi, Bel\\'en Alem\\'an, Patrick Zeller, Matteo\n  Amati, Alfonso Monreal-Bernal, Luca Gregoratti, V\\'ictor A. de la Pe\\~na\n  O'Shea, and Juan J. Vilatela", "docs_id": "2012.01109", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interfacial studies in CNT fibre/TiO$_{2}$ photoelectrodes for efficient\n  H$_{2}$ production. An attractive class of materials for photo(electro)chemical reactions are hybrids based on semiconducting metal oxides and nanocarbons (e.g. carbon nanotubes (CNT), graphene), where the nanocarbon acts as a highly-stable conductive scaffold onto which the nanostructured inorganic phase can be immobilised; an architecture that maximises surface area and minimises charge transport/transfer resistance. TiO$_{2}$/CNT photoanodes produced by atomic layer deposition on CNT fabrics are shown to be efficient for H$_{2}$ production ($0.07 \\mu mol/min$ $H_{2}$ at $0.2V$ $vs Ag/AgCl$), nearly doubling the performance of TiO$_{2}$ deposited on planar substrates, with $100 \\%$ Faradaic efficiency. The results are rationalised based on electrochemical impedance spectroscopy measurements showing a large reduction in photoelectron transport resistance compared to control samples and a higher surface area. The low TiO$_{2}$/CNT interfacial charge transfer resistance ($10 \\Omega$) is consistent with the presence of an interfacial Ti-O-C bond and corresponding electronic hybridisation determined by spatially-resolved Scanning Photoelectron Microscopy (SPEM) using synchrotron radiation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using TiO2/CNT photoanodes produced by atomic layer deposition on CNT fabrics for H2 production, as evidenced by the results of the study?\n\nA) The high surface area of the TiO2 layer allows for increased adsorption of water molecules, leading to a higher rate of H2 production.\nB) The presence of an interfacial Ti-O-C bond and corresponding electronic hybridization reduces the charge transfer resistance between the TiO2 and CNT phases, resulting in a more efficient H2 production.\nC) The use of CNT fabrics as a scaffold for TiO2 deposition increases the mechanical strength of the photoanode, making it more suitable for practical applications.\nD) The low TiO2/CNT interfacial charge transfer resistance is due to the high conductivity of the CNT phase, which allows for faster electron transport.\n\nCorrect Answer: B) The presence of an interfacial Ti-O-C bond and corresponding electronic hybridization reduces the charge transfer resistance between the TiO2 and CNT phases, resulting in a more efficient H2 production.\n\nExplanation: The correct answer is B) because the study shows that the TiO2/CNT photoanodes have a low interfacial charge transfer resistance of 10 \u03a9, which is consistent with the presence of an interfacial Ti-O-C bond and corresponding electronic hybridization. This reduced charge transfer resistance is attributed to the hybridization of the Ti-O-C bond, which allows for more efficient electron transfer between the TiO2 and CNT phases, leading to a higher rate of H2 production. The other options are incorrect because they do not accurately describe the primary advantage of using TiO2/CNT photoanodes for H2 production."}, "35": {"documentation": {"title": "Effective time-independent analysis for quantum kicked systems", "source": "Jayendra N. Bandyopadhyay, Tapomoy Guha Sarkar", "docs_id": "1409.1335", "section": ["quant-ph", "cond-mat.other", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effective time-independent analysis for quantum kicked systems. We present a mapping of potentially chaotic time-dependent quantum kicked systems to an equivalent effective time-independent scenario, whereby the system is rendered integrable. The time-evolution is factorized into an initial kick, followed by an evolution dictated by a time-independent Hamiltonian and a final kick. This method is applied to the kicked top model. The effective time-independent Hamiltonian thus obtained, does not suffer from spurious divergences encountered if the traditional Baker-Cambell-Hausdorff treatment is used. The quasienergy spectrum of the Floquet operator is found to be in excellent agreement with the energy levels of the effective Hamiltonian for a wide range of system parameters. The density of states for the effective system exhibits sharp peak-like features, pointing towards quantum criticality. The dynamics in the classical limit of the integrable effective Hamiltonian shows remarkable agreement with the non-integrable map corresponding to the actual time-dependent system in the non-chaotic regime. This suggests that the effective Hamiltonian serves as a substitute for the actual system in the non-chaotic regime at both the quantum and classical level."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a quantum kicked system with a time-independent Hamiltonian and a Floquet operator that exhibits quasienergy spectrum features indicative of quantum criticality. Analyze the implications of the effective time-independent Hamiltonian obtained through the mapping method described in the paper on the system's behavior in the non-chaotic regime.\n\nA) The effective Hamiltonian is a good approximation of the actual system only in the chaotic regime, where the quasienergy spectrum exhibits sharp peak-like features.\n\nB) The density of states for the effective system exhibits sharp peak-like features, pointing towards quantum criticality, and the dynamics in the classical limit of the integrable effective Hamiltonian shows remarkable agreement with the non-integrable map corresponding to the actual time-dependent system.\n\nC) The quasienergy spectrum of the Floquet operator is found to be in excellent agreement with the energy levels of the effective Hamiltonian for a wide range of system parameters, but the effective Hamiltonian does not suffer from spurious divergences encountered if the traditional Baker-Cambell-Hausdorff treatment is used.\n\nD) The effective Hamiltonian is a substitute for the actual system in the non-chaotic regime at both the quantum and classical level, but the mapping method does not account for the effects of decoherence on the system's behavior.\n\nCorrect Answer: B) The density of states for the effective system exhibits sharp peak-like features, pointing towards quantum criticality, and the dynamics in the classical limit of the integrable effective Hamiltonian shows remarkable agreement with the non-integrable map corresponding to the actual time-dependent system.\n\nExplanation: The correct answer is B) because the paper states that the density of states for the effective system exhibits sharp peak-like features, pointing towards quantum criticality, and the dynamics in the classical limit of the integrable effective Hamiltonian shows remarkable agreement with the non-integrable map corresponding to the actual time-dependent system. This suggests that the effective Hamiltonian serves as a substitute for the actual system in the non-chaotic regime at both the quantum and classical level."}, "36": {"documentation": {"title": "Resummed Perturbation Theory of Galaxy Clustering", "source": "Xin Wang, Alex Szalay", "docs_id": "1204.0019", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resummed Perturbation Theory of Galaxy Clustering. The relationship between observed tracers such as galaxies and the underlying dark matter distribution is crucial in extracting cosmological information. As the linear bias model breaks down at quasi-linear scales, the standard perturbative approach of the nonlinear Eulerian bias model (EBM) is not accurate enough in describing galaxy clustering. In this paper, we discuss such a model in the context of resummed perturbation theory, and further generalize it to incorporate the subsequent gravitational evolution by combining with a Lagrangian description of galaxies' motion. The multipoint propagators we constructed for such model also exhibit exponential damping similar to their dark matter counterparts, therefore the convergence property of statistics built upon these quantities is improved. This is achieved by applying both Eulerian and Lagrangian resummation techniques of dark matter field developed in recent years. As inherited from the Lagrangian description of galaxy density evolution, our approach automatically incorporates the non-locality induced by gravitational evolution after the formation of the tracer, and also allows us to include a continuous galaxy formation history by temporally weighted-averaging relevant quantities with the galaxy formation rate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of the standard perturbative approach of the nonlinear Eulerian bias model (EBM) in describing galaxy clustering, and how does the proposed resummed perturbation theory address this limitation?\n\nA) The standard perturbative approach of the nonlinear Eulerian bias model (EBM) is limited by its inability to capture the non-locality induced by gravitational evolution after the formation of the tracer, and the proposed resummed perturbation theory addresses this limitation by incorporating a Lagrangian description of galaxies' motion.\n\nB) The standard perturbative approach of the nonlinear Eulerian bias model (EBM) is limited by its inability to accurately describe the exponential damping of multipoint propagators, and the proposed resummed perturbation theory addresses this limitation by applying Eulerian and Lagrangian resummation techniques of dark matter fields.\n\nC) The standard perturbative approach of the nonlinear Eulerian bias model (EBM) is limited by its inability to capture the continuous galaxy formation history, and the proposed resummed perturbation theory addresses this limitation by temporally weighting-averaging relevant quantities with the galaxy formation rate.\n\nD) The standard perturbative approach of the nonlinear Eulerian bias model (EBM) is limited by its inability to accurately describe the quasi-linear scales, and the proposed resummed perturbation theory addresses this limitation by combining with a Lagrangian description of galaxies' motion.\n\nCorrect Answer: B) The standard perturbative approach of the nonlinear Eulerian bias model (EBM) is limited by its inability to accurately describe the exponential damping of multipoint propagators, and the proposed resummed perturbation theory addresses this limitation by applying Eulerian and Lagrangian resummation techniques of dark matter fields.\n\nExplanation: The correct answer is B) because the documentation states that the linear bias model breaks down at quasi-linear scales, and the standard perturbative approach of the nonlinear Eulerian bias model (EBM) is not accurate enough in describing galaxy clustering at these scales. The proposed resummed perturbation theory addresses this limitation by applying Eulerian and Lagrangian resummation techniques of dark matter fields, which exhibit exponential damping similar to their dark matter counterparts. This improvement in convergence property is crucial for extracting cosmological information from galaxy clustering."}, "37": {"documentation": {"title": "Quantum Chaos and Quantum Computing Structures", "source": "Carlos Pedro Gon\\c{c}alves", "docs_id": "1208.2610", "section": ["nlin.CD", "cond-mat.stat-mech", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum Chaos and Quantum Computing Structures. A system of quantum computing structures is introduced and proven capable of making emerge, on average, the orbits of classical bounded nonlinear maps on \\mathbb{C} through the iterative action of path-dependent quantum gates. The effects of emerging nonlinear dynamics and chaos upon the quantum averages of relevant observables and quantum probabilities are exemplified for a version of Chirikov's standard map on \\mathbb{C} . Both the individual orbits and ensemble properties are addressed so that the Poincar\\'e map for Chirikov's standard map, in the current quantum setting, is reinterpreted in terms of a quantum ensemble which is then formally introduced within the formalized system of quantum computing structures, in terms of quantum register machines, revealing three phases of quantum ensemble dynamics: the regular, the chaotic and an intermediate phase called complex quantum stochastic phase which shares similarities to the edge of chaos notion from classical cellular automata and classical random boolean networks' evolutionary computation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Quantum Chaos and Quantum Computing Structures, what is the significance of the emergence of nonlinear dynamics and chaos in the quantum averages of relevant observables and quantum probabilities for a version of Chirikov's standard map on \u2102?\n\n**A)** The emergence of nonlinear dynamics and chaos leads to a decrease in the accuracy of quantum predictions, resulting in a loss of control over the system.\n\n**B)** The emergence of nonlinear dynamics and chaos results in the creation of a new, more complex phase space that is not accessible in the classical limit.\n\n**C)** The emergence of nonlinear dynamics and chaos leads to the formation of a quantum ensemble that exhibits three phases of dynamics: regular, chaotic, and complex quantum stochastic phase, which shares similarities with the edge of chaos notion from classical cellular automata and classical random boolean networks' evolutionary computation.\n\n**D)** The emergence of nonlinear dynamics and chaos is a direct consequence of the path-dependent quantum gates in the system, leading to a loss of determinism in the system's behavior.\n\n**Correct Answer:** C) The emergence of nonlinear dynamics and chaos leads to the formation of a quantum ensemble that exhibits three phases of dynamics: regular, chaotic, and complex quantum stochastic phase, which shares similarities with the edge of chaos notion from classical cellular automata and classical random boolean networks' evolutionary computation.\n\n**Explanation:** The correct answer, C, is supported by the documentation, which states that the emergence of nonlinear dynamics and chaos leads to the formation of a quantum ensemble that exhibits three phases of dynamics: regular, chaotic, and complex quantum stochastic phase. This is a key concept in the paper, and the correct answer demonstrates an understanding of the relationship between nonlinear dynamics, chaos, and the behavior of quantum ensembles. The other options are incorrect because they do not accurately reflect the significance of nonlinear dynamics and chaos in the context of the paper. Option A is incorrect because the emergence of nonlinear dynamics and chaos does not lead to a decrease in the accuracy of quantum predictions. Option B is incorrect because the emergence of nonlinear dynamics and chaos does not create a new, more complex phase space that is not accessible in the classical limit. Option D is incorrect because the emergence of nonlinear dynamics and chaos is not a direct consequence of the path-dependent quantum gates in the system, but rather a result of the complex interactions between the quantum gates and the nonlinear dynamics."}, "38": {"documentation": {"title": "Inverse Jacobi multiplier as a link between conservative systems and\n  Poisson structures", "source": "Isaac A. Garc\\'ia, Benito Hern\\'andez-Bermejo", "docs_id": "1910.10373", "section": ["math-ph", "math.CA", "math.MP", "math.SG", "nlin.SI", "physics.class-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inverse Jacobi multiplier as a link between conservative systems and\n  Poisson structures. Some aspects of the relationship between conservativeness of a dynamical system (namely the preservation of a finite measure) and the existence of a Poisson structure for that system are analyzed. From the local point of view, due to the Flow-Box Theorem we restrict ourselves to neighborhoods of singularities. In this sense, we characterize Poisson structures around the typical zero-Hopf singularity in dimension 3 under the assumption of having a local analytic first integral with non-vanishing first jet by connecting with the classical Poincar\\'e center problem. From the global point of view, we connect the property of being strictly conservative (the invariant measure must be positive) with the existence of a Poisson structure depending on the phase space dimension. Finally, weak conservativeness in dimension two is introduced by the extension of inverse Jacobi multipliers as weak solutions of its defining partial differential equation and some of its applications are developed. Examples including Lotka-Volterra systems, quadratic isochronous centers, and non-smooth oscillators are provided."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a conservative dynamical system in dimension 3 with a local analytic first integral and a non-vanishing first jet. Characterize the Poisson structure around the typical zero-Hopf singularity in this system.\n\nA) The Poisson structure is given by the classical Poincar\u00e9 center problem, which is a 2-form that is invariant under the flow of the system.\n\nB) The Poisson structure is a 3-form that is defined by the inverse Jacobi multiplier, which is a function that satisfies a certain partial differential equation.\n\nC) The Poisson structure is a 2-form that is determined by the first integral and the non-vanishing first jet, and is invariant under the flow of the system.\n\nD) The Poisson structure is a 1-form that is defined by the inverse Jacobi multiplier, which is a function that satisfies a certain partial differential equation, and is related to the classical Poincar\u00e9 center problem.\n\nCorrect Answer: B) The Poisson structure is a 3-form that is defined by the inverse Jacobi multiplier, which is a function that satisfies a certain partial differential equation.\n\nExplanation: The correct answer is B) because the Poisson structure around the typical zero-Hopf singularity in a conservative dynamical system in dimension 3 is indeed defined by the inverse Jacobi multiplier, which is a function that satisfies a certain partial differential equation. This is stated in the documentation as \"From the local point of view, due to the Flow-Box Theorem we restrict ourselves to neighborhoods of singularities. In this sense, we characterize Poisson structures around the typical zero-Hopf singularity in dimension 3 under the assumption of having a local analytic first integral with non-vanishing first jet by connecting with the classical Poincar\\'e center problem.\"\n\nThe other options are incorrect because:\n\n* A) The classical Poincar\u00e9 center problem is a 2-form, not a 3-form.\n* C) The Poisson structure is not necessarily a 2-form, and is not invariant under the flow of the system.\n* D) The inverse Jacobi multiplier is a function that satisfies a certain partial differential equation, but the Poisson structure is a 3-form, not a 1-form."}, "39": {"documentation": {"title": "Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching", "source": "Elliot Anshelevich and Wennan Zhu", "docs_id": "1707.01608", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching. We study ordinal approximation algorithms for maximum-weight bipartite matchings. Such algorithms only know the ordinal preferences of the agents/nodes in the graph for their preferred matches, but must compete with fully omniscient algorithms which know the true numerical edge weights (utilities). %instead of only their relative orderings. Ordinal approximation is all about being able to produce good results with only limited information. Because of this, one important question is how much better the algorithms can be as the amount of information increases. To address this question for forming high-utility matchings between agents in $\\mathcal{X}$ and $\\mathcal{Y}$, we consider three ordinal information types: when we know the preference order of only nodes in $\\mathcal{X}$ for nodes in $\\mathcal{Y}$, when we know the preferences of both $\\mathcal{X}$ and $\\mathcal{Y}$, and when we know the total order of the edge weights in the entire graph, although not the weights themselves. We also consider settings where only the top preferences of the agents are known to us, instead of their full preference orderings. We design new ordinal approximation algorithms for each of these settings, and quantify how well such algorithms perform as the amount of information given to them increases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a maximum-weight bipartite matching problem where the agents in set $\\mathcal{X}$ have the following preference orderings for their preferred matches in set $\\mathcal{Y}$: $y_1 > y_2 > y_3 > y_4$. The agents in set $\\mathcal{Y}$ have the following preference orderings for their preferred matches in set $\\mathcal{X}$: $x_1 > x_2 > x_3 > x_4$. The edge weights between the agents are not known, but the total order of the edge weights in the entire graph is known. Design an ordinal approximation algorithm that can produce a high-utility matching in this setting, and analyze its performance in terms of the amount of information given to it.\n\n**A)** Design an algorithm that uses the total order of the edge weights to rank the agents in set $\\mathcal{X}$ and set $\\mathcal{Y}$ based on their utility, and then use a greedy approach to match the agents.\n\n**B)** Design an algorithm that uses the preference orderings of the agents in set $\\mathcal{X}$ and set $\\mathcal{Y}$ to construct a matching that is close to the optimal solution, and then use a local search approach to refine the matching.\n\n**C)** Design an algorithm that uses the total order of the edge weights to construct a matching that is close to the optimal solution, and then use a dynamic programming approach to refine the matching.\n\n**D)** Design an algorithm that uses the preference orderings of the agents in set $\\mathcal{X}$ and set $\\mathcal{Y}$ to construct a matching that is close to the optimal solution, and then use a simulated annealing approach to refine the matching.\n\n**Correct Answer:** **C)** Design an algorithm that uses the total order of the edge weights to construct a matching that is close to the optimal solution, and then use a dynamic programming approach to refine the matching.\n\n**Explanation:** The correct answer is C because the algorithm can use the total order of the edge weights to rank the agents in set $\\mathcal{X}$ and set $\\mathcal{Y}$ based on their utility, and then use dynamic programming to refine the matching. This approach allows the algorithm to take advantage of the total order of the edge weights to produce a high-utility matching, while also using dynamic programming to refine the matching and improve its performance. The other options are incorrect because they do not take advantage of the total order of the edge weights, or they use approaches that are not well-suited for this problem."}, "40": {"documentation": {"title": "Subgrid-scale parametrization of unresolved scales in forced Burgers\n  equation using Generative Adversarial Networks (GAN)", "source": "Jeric Alcala and Ilya Timofeyev", "docs_id": "2007.06692", "section": ["physics.comp-ph", "nlin.CD", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Subgrid-scale parametrization of unresolved scales in forced Burgers\n  equation using Generative Adversarial Networks (GAN). Stochastic subgrid-scale parametrizations aim to incorporate effects of unresolved processes in an effective model by sampling from a distribution usually described in terms of resolved modes. This is an active research area in climate, weather and ocean science where processes evolved in a wide range of spatial and temporal scales. In this study, we evaluate the performance of conditional generative adversarial network (GAN) in parametrizing subgrid-scale effects in a finite-difference discretization of stochastically forced Burgers equation. We define resolved modes as local spatial averages and deviations from these averages are the unresolved degrees of freedom. We train a Wasserstein GAN (WGAN) conditioned on the resolved variables to learn the distribution of subgrid flux tendencies for resolved modes and, thus, represent the effect of unresolved scales. The resulting WGAN is then used in an effective model to reproduce the statistical features of resolved modes. We demonstrate that various stationary statistical quantities such as spectrum, moments, autocorrelation, etc. are well approximated by this effective model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study on Subgrid-scale parametrization of unresolved scales in forced Burgers equation using Generative Adversarial Networks (GAN), what is the primary objective of training a Wasserstein GAN (WGAN) conditioned on the resolved variables?\n\n**A)** To learn the distribution of subgrid flux tendencies for unresolved modes\n**B)** To approximate the statistical features of resolved modes\n**C)** To reduce the computational complexity of the finite-difference discretization of the stochastically forced Burgers equation\n**D)** To improve the stability of the effective model by incorporating subgrid-scale effects\n\n**Correct Answer:** A) To learn the distribution of subgrid flux tendencies for unresolved modes\n\n**Explanation:** The primary objective of training a WGAN conditioned on the resolved variables is to learn the distribution of subgrid flux tendencies for unresolved modes. This is achieved by sampling from a distribution usually described in terms of resolved modes, which are local spatial averages and deviations from these averages are the unresolved degrees of freedom. The resulting WGAN is then used in an effective model to reproduce the statistical features of resolved modes.\n\nNote: The other options are incorrect because:\n\n* B) is incorrect because while the WGAN does approximate the statistical features of resolved modes, this is not its primary objective.\n* C) is incorrect because reducing computational complexity is not the primary objective of training a WGAN.\n* D) is incorrect because improving stability is not the primary objective of training a WGAN, although it may be a secondary benefit."}, "41": {"documentation": {"title": "Molecular Imprinting: The missing piece in the puzzle of abiogenesis?", "source": "K. Eric Drexler", "docs_id": "1807.07065", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Molecular Imprinting: The missing piece in the puzzle of abiogenesis?. In a neglected 2005 paper, Nobel Laureate Paul Lauterbur proposed that molecular imprinting in amorphous materials -- a phenomenon with an extensive experimental literature -- played a key role in abiogenesis. The present paper builds on Lauterbur's idea to propose imprint-mediated templating (IMT), a mechanism for prebiotic peptide replication that could potentially avoid a range of difficulties arising in classic gene-first and metabolism-first models of abiogenesis. Unlike models that propose prebiotic RNA synthesis, activation, and polymerization based on unknown chemistries, peptide/IMT models are compatible with demonstrably realistic prebiotic chemistries: synthesis of dilute mixtures of racemic amino acids from atmospheric gases, and polymerization of unactivated amino acids on hot, intermittently-wetted surfaces. Starting from a peptide/IMT-based genetics, plausible processes could support the elaboration of genetic and metabolic complexity in an early-Earth environment, both explaining the emergence of homochirality and providing a potential bridge to nucleic acid metabolism. Peptide/IMT models suggest directions for both theoretical and experimental inquiry."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nWhat is the primary advantage of the peptide/IMT (imprint-mediated templating) model of abiogenesis proposed in the paper, and how does it differ from classic gene-first and metabolism-first models of abiogenesis?\n\nA) Peptide/IMT models rely on unknown chemistries for prebiotic RNA synthesis, activation, and polymerization, whereas classic models propose realistic prebiotic chemistries for peptide/IMT.\nB) Peptide/IMT models are incompatible with demonstrably realistic prebiotic chemistries, whereas classic models are compatible with these chemistries.\nC) Peptide/IMT models propose prebiotic RNA synthesis, activation, and polymerization based on unknown chemistries, whereas classic models propose realistic prebiotic chemistries for peptide/IMT.\nD) Peptide/IMT models suggest directions for both theoretical and experimental inquiry, whereas classic models do not.\n\nCorrect Answer: D) Peptide/IMT models suggest directions for both theoretical and experimental inquiry, whereas classic models do not.\n\nExplanation: The correct answer is D) because the paper states that \"Peptide/IMT models suggest directions for both theoretical and experimental inquiry\" (emphasis added). This indicates that the peptide/IMT model offers a new perspective that can guide further research in both theoretical and experimental areas. In contrast, the classic models of abiogenesis are not mentioned as suggesting directions for inquiry."}, "42": {"documentation": {"title": "Credit Default Swap Calibration and Counterparty Risk Valuation with a\n  Scenario based First Passage Model", "source": "Damiano Brigo, Marco Tarenghi", "docs_id": "0912.3031", "section": ["q-fin.PR", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Credit Default Swap Calibration and Counterparty Risk Valuation with a\n  Scenario based First Passage Model. In this work we develop a tractable structural model with analytical default probabilities depending on a random default barrier and possibly random volatility ideally associated with a scenario based underlying firm debt. We show how to calibrate this model using a chosen number of reference Credit Default Swap (CDS) market quotes. In general this model can be seen as a possible extension of the time-varying AT1P model in Brigo and Tarenghi (2004). The calibration capability of the Scenario Volatility/Barrier model (SVBAT1P), when keeping time-constant volatility, appears inferior to the one of AT1P with time-varying deterministic volatility. The SVBAT1P model, however, maintains the benefits of time-homogeneity and can lead to satisfactory calibration results, as we show in a case study where we compare different choices on scenarios and parameters. Similarly to AT1P, SVBAT1P is suited to pricing hybrid equity/credit derivatives and to evaluate counterparty risk in equity payoffs, and more generally to evaluate hybrid credit/equity payoffs. We consider the equity return swap in Brigo and Tarenghi (2004) and show its valuation under SVBAT1P with the same CDS and equity calibration input used earlier for AT1P, and further we hint at equity default swap valuation in the conclusions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Scenario Volatility/Barrier (SVBAT1P) model, what is the primary advantage of maintaining time-homogeneity in volatility, as opposed to the time-varying deterministic volatility approach of the AT1P model?\n\n**A)** SVBAT1P is more suitable for pricing complex credit derivatives.\n**B)** SVBAT1P can lead to more accurate calibration results when using a chosen number of reference CDS market quotes.\n**C)** SVBAT1P is more computationally efficient than AT1P.\n**D)** SVBAT1P is more suitable for evaluating counterparty risk in equity payoffs.\n\n**Correct Answer:** B) SVBAT1P can lead to more accurate calibration results when using a chosen number of reference CDS market quotes.\n\n**Explanation:** The correct answer is B) because the documentation states that the SVBAT1P model \"maintains the benefits of time-homogeneity\" and can lead to \"satisfactory calibration results\" when using a chosen number of reference CDS market quotes. This suggests that the primary advantage of time-homogeneity in SVBAT1P is its ability to achieve accurate calibration results, which is a key aspect of model development in credit risk modeling."}, "43": {"documentation": {"title": "Heat-bath Configuration Interaction: An efficient selected CI algorithm\n  inspired by heat-bath sampling", "source": "Adam Holmes, Norm Tubman, Cyrus Umrigar", "docs_id": "1606.07453", "section": ["physics.chem-ph", "cond-mat.str-el", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heat-bath Configuration Interaction: An efficient selected CI algorithm\n  inspired by heat-bath sampling. We introduce a new selected configuration interaction plus perturbation theory algorithm that is based on a deterministic analog of our recent efficient heat-bath sampling algorithm. This Heat-bath Configuration Interaction (HCI) algorithm makes use of two parameters that control the tradeoff between speed and accuracy, one which controls the selection of determinants to add to a variational wavefunction, and one which controls the the selection of determinants used to compute the perturbative correction to the variational energy. We show that HCI provides an accurate treatment of both static and dynamic correlation by computing the potential energy curve of the multireference carbon dimer in the cc-pVDZ basis. We then demonstrate the speed and accuracy of HCI by recovering the full configuration interaction energy of both the carbon dimer in the cc-pVTZ basis and the strongly-correlated chromium dimer in the Ahlrichs VDZ basis, correlating all electrons, to an accuracy of better than 1 mHa, in just a few minutes on a single core. These systems have full variational spaces of 3x10^14 and 2x10^22 determinants respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the Heat-bath Configuration Interaction (HCI) algorithm in terms of its tradeoff between speed and accuracy, and how does it compare to traditional configuration interaction methods?\n\nA) HCI provides faster computation times at the expense of accuracy, making it suitable for large systems with many electrons.\nB) HCI offers a more accurate treatment of static and dynamic correlation by allowing for the selection of determinants to add to the variational wavefunction and perturbative corrections to the variational energy, making it suitable for systems with complex electronic structures.\nC) HCI is limited to small systems due to its computational complexity, making it less suitable for large-scale simulations.\nD) HCI is a deterministic algorithm that relies on random sampling, making it less accurate than traditional CI methods.\n\nCorrect Answer: B) HCI offers a more accurate treatment of static and dynamic correlation by allowing for the selection of determinants to add to the variational wavefunction and perturbative corrections to the variational energy, making it suitable for systems with complex electronic structures.\n\nExplanation: The correct answer highlights the key advantage of HCI, which is its ability to balance speed and accuracy by allowing for the selection of determinants to add to the variational wavefunction and perturbative corrections to the variational energy. This feature makes HCI particularly suitable for systems with complex electronic structures, such as the carbon dimer and chromium dimer mentioned in the documentation. The other options are incorrect because they either misrepresent the tradeoff between speed and accuracy (A and D) or incorrectly limit the applicability of HCI (C)."}, "44": {"documentation": {"title": "Isolated Vortex and Vortex Lattice in a Holographic p-wave\n  Superconductor", "source": "James M. Murray and Zlatko Tesanovic", "docs_id": "1103.3232", "section": ["hep-th", "cond-mat.str-el", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isolated Vortex and Vortex Lattice in a Holographic p-wave\n  Superconductor. Using the holographic gauge-gravity duality, we find a solution for an isolated vortex and a vortex lattice in a 2+1 dimensional p-wave superconductor, which is described by the boundary theory dual to an SU(2) gauge theory in 3+1 dimensional anti-de Sitter space. Both $p_x+ip_y$ and $p_x-ip_y$ components of the superconducting order parameter, as well as the effects of a magnetic field on these components, are considered. The isolated vortex solution is studied, and it is found that the two order parameter components have different amplitudes due to the time reversal symmetry breaking. The vortex lattice for large magnetic fields is also studied, where it is argued that only one order parameter component will be nonzero sufficiently close to the upper critical field. The upper critical field exhibits a characteristic upward curvature, reflecting the effects of field-induced correlations captured by the holographic theory. The free energy is calculated perturbatively in this region of the phase diagram, and it is shown that the triangular vortex lattice is the thermodynamically preferred solution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the holographic gauge-gravity duality, what is the expected behavior of the upper critical field in a 2+1 dimensional p-wave superconductor, and how does it relate to the effects of a magnetic field on the superconducting order parameter components?\n\nA) The upper critical field exhibits a characteristic downward curvature, reflecting the effects of field-induced correlations captured by the holographic theory, and only one order parameter component is nonzero at high magnetic fields.\n\nB) The upper critical field exhibits a characteristic upward curvature, reflecting the effects of field-induced correlations captured by the holographic theory, and both order parameter components are nonzero at high magnetic fields.\n\nC) The upper critical field exhibits a characteristic upward curvature, reflecting the effects of field-induced correlations captured by the holographic theory, and only one order parameter component is nonzero sufficiently close to the upper critical field.\n\nD) The upper critical field exhibits a characteristic downward curvature, reflecting the effects of field-induced correlations captured by the holographic theory, and both order parameter components are nonzero sufficiently close to the upper critical field.\n\nCorrect Answer: C) The upper critical field exhibits a characteristic upward curvature, reflecting the effects of field-induced correlations captured by the holographic theory, and only one order parameter component is nonzero sufficiently close to the upper critical field.\n\nExplanation: The correct answer is C) because the documentation states that the upper critical field exhibits a characteristic upward curvature, reflecting the effects of field-induced correlations captured by the holographic theory. Additionally, it is argued that only one order parameter component will be nonzero sufficiently close to the upper critical field, which is consistent with option C. Options A and B are incorrect because they describe a downward curvature, which is not mentioned in the documentation. Option D is also incorrect because it states that both order parameter components are nonzero sufficiently close to the upper critical field, which is not supported by the documentation."}, "45": {"documentation": {"title": "Parametric inference with universal function approximators", "source": "Andreas Joseph", "docs_id": "1903.04209", "section": ["stat.ML", "cs.LG", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parametric inference with universal function approximators. Universal function approximators, such as artificial neural networks, can learn a large variety of target functions arbitrarily well given sufficient training data. This flexibility comes at the cost of the ability to perform parametric inference. We address this gap by proposing a generic framework based on the Shapley-Taylor decomposition of a model. A surrogate parametric regression analysis is performed in the space spanned by the Shapley value expansion of a model. This allows for the testing of standard hypotheses of interest. At the same time, the proposed approach provides novel insights into statistical learning processes themselves derived from the consistency and bias properties of the nonparametric estimators. We apply the framework to the estimation of heterogeneous treatment effects in simulated and real-world randomised experiments. We introduce an explicit treatment function based on higher-order Shapley-Taylor indices. This can be used to identify potentially complex treatment channels and help the generalisation of findings from experimental settings. More generally, the presented approach allows for a standardised use and communication of results from machine learning models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Shapley-Taylor decomposition framework for parametric inference with universal function approximators, what is the primary advantage of using a surrogate parametric regression analysis in the space spanned by the Shapley value expansion of a model?\n\n**A)** It allows for the estimation of heterogeneous treatment effects in simulated and real-world randomized experiments.\n**B)** It enables the testing of standard hypotheses of interest, such as the treatment effect, in a more efficient and accurate manner.\n**C)** It provides novel insights into statistical learning processes, including consistency and bias properties of nonparametric estimators.\n**D)** It enables the identification of potentially complex treatment channels and facilitates the generalization of findings from experimental settings.\n\n**Correct Answer:** B) It enables the testing of standard hypotheses of interest, such as the treatment effect, in a more efficient and accurate manner.\n\n**Explanation:** The correct answer is B) because the Shapley-Taylor decomposition framework is designed to address the gap between the flexibility of universal function approximators and the need for parametric inference. By performing a surrogate parametric regression analysis in the space spanned by the Shapley value expansion of a model, the framework enables the testing of standard hypotheses of interest, such as the treatment effect, in a more efficient and accurate manner. This is a key advantage of the proposed approach, as it allows for the standardization of results from machine learning models and facilitates the communication of findings from experimental settings.\n\nThe other options are incorrect because:\n\n* A) While the framework is applied to the estimation of heterogeneous treatment effects, this is not the primary advantage of using a surrogate parametric regression analysis.\n* C) The framework does provide novel insights into statistical learning processes, but this is not the primary advantage of using a surrogate parametric regression analysis.\n* D) The framework does enable the identification of potentially complex treatment channels, but this is not the primary advantage of using a surrogate parametric regression analysis."}, "46": {"documentation": {"title": "Multi-level resistance switching and random telegraph noise analysis of\n  nitride based memristors", "source": "Nikolaos Vasileiadis, Panagiotis Loukas, Panagiotis Karakolis,\n  Vassilios Ioannou-Sougleridis, Pascal Normand, Vasileios Ntinas,\n  Iosif-Angelos Fyrigos, Ioannis Karafyllidis, Georgios Ch. Sirakoulis and\n  Panagiotis Dimitrakis", "docs_id": "2103.09931", "section": ["physics.app-ph", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-level resistance switching and random telegraph noise analysis of\n  nitride based memristors. Resistance switching devices are of special importance because of their application in resistive memories (RRAM) which are promising candidates for replacing current nonvolatile memories and realize storage class memories. These devices exhibit usually memristive properties with many discrete resistance levels and implement artificial synapses. The last years, researchers have demonstrated memristive chips as accelerators in computing, following new in-memory and neuromorphic computational approaches. Many different metal oxides have been used as resistance switching materials in MIM or MIS structures. Understanding of the mechanism and the dynamics of resistance switching is very critical for the modeling and use of memristors in different applications. Here, we demonstrate the bipolar resistance switching of silicon nitride thin films using heavily doped Si and Cu as bottom and top-electrodes, respectively. Analysis of the current-voltage characteristics reveal that under space-charge limited conditions and appropriate current compliance setting, multi-level resistance operation can be achieved. Furthermore, a flexible tuning protocol for multi-level resistance switching was developed applying appropriate SET/RESET pulse sequences. Retention and random telegraph noise measurements performed at different resistance levels. The present results reveal the attractive properties of the examined devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key characteristics of the silicon nitride thin films used in the bipolar resistance switching devices demonstrated in the study, and how do they contribute to the development of multi-level resistance operation?\n\nA) The silicon nitride thin films exhibit high conductivity and low resistance, allowing for efficient current flow and minimal switching energy.\nB) The silicon nitride thin films display a high degree of flexibility and tunability, enabling the development of flexible tuning protocols for multi-level resistance switching.\nC) The silicon nitride thin films exhibit memristive properties with many discrete resistance levels, allowing for the implementation of artificial synapses and enabling the development of multi-level resistance operation.\nD) The silicon nitride thin films have a high degree of uniformity and consistency, ensuring reliable and reproducible switching behavior.\n\nCorrect Answer: C) The silicon nitride thin films exhibit memristive properties with many discrete resistance levels, allowing for the implementation of artificial synapses and enabling the development of multi-level resistance operation.\n\nExplanation: The correct answer is C) because the study demonstrates the bipolar resistance switching of silicon nitride thin films using heavily doped Si and Cu as bottom and top-electrodes, respectively. The analysis of the current-voltage characteristics reveals that under space-charge limited conditions and appropriate current compliance setting, multi-level resistance operation can be achieved. This is attributed to the memristive properties of the silicon nitride thin films, which exhibit many discrete resistance levels, allowing for the implementation of artificial synapses and enabling the development of multi-level resistance operation."}, "47": {"documentation": {"title": "Asymmetric motion of magnetically actuated artificial cilia", "source": "Srinivas Hanasoge, Matthew Ballard, Peter J. Hesketh, Alexander\n  Alexeev", "docs_id": "1806.04320", "section": ["physics.bio-ph", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric motion of magnetically actuated artificial cilia. Most microorganisms use hair-like cilia with asymmetric beating to perform vital bio-physical processes. In this paper, we demonstrate a novel fabrication method for creating magnetic artificial cilia capable of such biologically inspired asymmetrical beating pattern essential for creating microfluidic transport in low Reynolds number. The cilia are fabricated using a lithographic process in conjunction with deposition of magnetic nickel-iron permalloy to create flexible filaments that can be manipulated by varying an external magnetic field. A rotating permanent magnet is used to actuate the cilia. We examine the kinematics of a cilium and demonstrate that the cilium motion is defined by an interplay among elastic, magnetic, and viscous forces. Specifically, the forward stroke is induced by the rotation of the magnet which bends the cilium, whereas the recovery stroke is defined by the straightening of the deformed cilium, releasing the accumulated elastic potential energy. This difference in dominating forces acting during the forward stroke and the recovery stroke leads to an asymmetrical beating pattern of the cilium. Such magnetic cilia can find applications in microfluidic pumping, mixing, and other fluid handling processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the magnetic artificial cilia exhibit asymmetrical beating patterns, and how does this relate to the forces acting on the cilium during its forward and recovery strokes?\n\nA) The magnetic field induces a torsional force that causes the cilium to twist and bend, leading to an asymmetrical beating pattern.\nB) The elastic properties of the nickel-iron permalloy filament cause it to deform and straighten, resulting in an asymmetrical beating pattern.\nC) The viscous forces acting on the cilium during its forward stroke and recovery stroke lead to an asymmetrical beating pattern, with the forward stroke being driven by the rotation of the magnet and the recovery stroke being driven by the straightening of the deformed cilium.\nD) The magnetic field induces a magnetic moment in the nickel-iron permalloy filament, which causes it to rotate and beat asymmetrically.\n\nCorrect Answer: C) The viscous forces acting on the cilium during its forward stroke and recovery stroke lead to an asymmetrical beating pattern, with the forward stroke being driven by the rotation of the magnet and the recovery stroke being driven by the straightening of the deformed cilium.\n\nExplanation: The correct answer is based on the explanation provided in the documentation, which states that the forward stroke is induced by the rotation of the magnet, which bends the cilium, and the recovery stroke is defined by the straightening of the deformed cilium, releasing the accumulated elastic potential energy. This difference in dominating forces acting during the forward stroke and the recovery stroke leads to an asymmetrical beating pattern of the cilium."}, "48": {"documentation": {"title": "Nonseparable Multinomial Choice Models in Cross-Section and Panel Data", "source": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val and Whitney Newey", "docs_id": "1706.08418", "section": ["stat.ME", "econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonseparable Multinomial Choice Models in Cross-Section and Panel Data. Multinomial choice models are fundamental for empirical modeling of economic choices among discrete alternatives. We analyze identification of binary and multinomial choice models when the choice utilities are nonseparable in observed attributes and multidimensional unobserved heterogeneity with cross-section and panel data. We show that derivatives of choice probabilities with respect to continuous attributes are weighted averages of utility derivatives in cross-section models with exogenous heterogeneity. In the special case of random coefficient models with an independent additive effect, we further characterize that the probability derivative at zero is proportional to the population mean of the coefficients. We extend the identification results to models with endogenous heterogeneity using either a control function or panel data. In time stationary panel models with two periods, we find that differences over time of derivatives of choice probabilities identify utility derivatives \"on the diagonal,\" i.e. when the observed attributes take the same values in the two periods. We also show that time stationarity does not identify structural derivatives \"off the diagonal\" both in continuous and multinomial choice panel models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a nonseparable multinomial choice model with cross-section and panel data, what is the implication of time stationarity on the identification of utility derivatives \"on the diagonal\" versus \"off the diagonal\"?\n\n**A)** Time stationarity only identifies utility derivatives \"on the diagonal\" in continuous choice models, but not in multinomial choice models.\n\n**B)** Time stationarity identifies utility derivatives \"on the diagonal\" in both continuous and multinomial choice models, regardless of the level of endogeneity.\n\n**C)** Time stationarity does not identify utility derivatives \"on the diagonal\" in continuous choice models, but identifies them in multinomial choice models with exogenous heterogeneity.\n\n**D)** Time stationarity identifies utility derivatives \"on the diagonal\" in multinomial choice models with endogenous heterogeneity, but not in continuous choice models.\n\n**Correct Answer:** D) Time stationarity identifies utility derivatives \"on the diagonal\" in multinomial choice models with endogenous heterogeneity, but not in continuous choice models.\n\n**Explanation:** According to the documentation, in time-stationary panel models with two periods, differences over time of derivatives of choice probabilities identify utility derivatives \"on the diagonal\" in multinomial choice models with endogenous heterogeneity. However, time stationarity does not identify structural derivatives \"off the diagonal\" in both continuous and multinomial choice panel models. Therefore, option D is the correct answer."}, "49": {"documentation": {"title": "Competition between electronic Kerr and free carrier effects in an\n  ultimate-fast optically switched semiconductor microcavity", "source": "E. Y\\\"uce, G. Ctistis, J. Claudon, E. Dupuy, K. J. Boller, J. M.\n  G\\'erard and W. L. Vos", "docs_id": "1205.0105", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Competition between electronic Kerr and free carrier effects in an\n  ultimate-fast optically switched semiconductor microcavity. We have performed ultrafast pump-probe experiments on a GaAs-AlAs microcavity with a resonance near 1300 nm in the \"original\" telecom band. We concentrate on ultimate-fast optical switching of the cavity resonance that is measured as a function of pump-pulse energy. We observe that at low pump-pulse energies the switching of the cavity resonance is governed by the instantaneous electronic Kerr effect and is achieved within 300 fs. At high pump-pulse energies the index change induced by free carriers generated in the GaAs start to compete with the electronic Kerr effect and reduce the resonance frequency shift. We have developed an analytic model which predicts this competition in agreement with the experimental data. Our model includes a new term in the intensity-dependent refractive index that considers the effect of the probe pulse intensity, which is resonantly enhanced by the cavity. We calculate the effect of the resonantly enhanced probe light on the refractive index change induced by the electronic Kerr effect for cavities with different quality factors. By exploiting the linear regime where only the electronic Kerr effect is observed, we manage to retrieve the nondegenerate third order nonlinear susceptibility for GaAs from the cavity resonance shift as a function of pump-pulse energy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of ultrafast optically switched semiconductor microcavities, what is the primary mechanism governing the switching of the cavity resonance at low pump-pulse energies, and how does it compare to the effect of free carriers generated in the GaAs material at high pump-pulse energies?\n\n**A)** The electronic Kerr effect is the primary mechanism, and it is unaffected by the intensity of the probe pulse. At high pump-pulse energies, the free carrier effect becomes negligible.\n\n**B)** The free carrier effect is the primary mechanism, and it dominates the electronic Kerr effect at high pump-pulse energies. The intensity of the probe pulse has no significant impact on the refractive index change.\n\n**C)** The electronic Kerr effect is the primary mechanism, but it is modified by the resonantly enhanced probe light, which affects the refractive index change induced by the free carrier effect. At high pump-pulse energies, the free carrier effect becomes significant.\n\n**D)** The free carrier effect is the primary mechanism, but it is masked by the electronic Kerr effect at low pump-pulse energies. The intensity of the probe pulse has a negligible impact on the refractive index change.\n\n**Correct Answer:** C) The electronic Kerr effect is the primary mechanism, but it is modified by the resonantly enhanced probe light, which affects the refractive index change induced by the free carrier effect. At high pump-pulse energies, the free carrier effect becomes significant.\n\n**Explanation:** The correct answer is based on the text, which states that at low pump-pulse energies, the switching of the cavity resonance is governed by the instantaneous electronic Kerr effect, and it is achieved within 300 fs. However, at high pump-pulse energies, the index change induced by free carriers generated in the GaAs starts to compete with the electronic Kerr effect and reduce the resonance frequency shift. The model developed by the authors includes a new term in the intensity-dependent refractive index that considers the effect of the probe pulse intensity, which is resonantly enhanced by the cavity. This suggests that the electronic Kerr effect is modified by the resonantly enhanced probe light, and the free carrier effect becomes significant at high pump-pulse energies."}, "50": {"documentation": {"title": "Synchronization of Weakly Coupled Oscillators: Coupling, Delay and\n  Topology", "source": "Enrique Mallada and Ao Tang", "docs_id": "1303.7248", "section": ["math.OC", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronization of Weakly Coupled Oscillators: Coupling, Delay and\n  Topology. There are three key factors of a system of coupled oscillators that characterize the interaction among them: coupling (how to affect), delay (when to affect) and topology (whom to affect). For each of them, the existing work has mainly focused on special cases. With new angles and tools, this paper makes progress in relaxing some assumptions of these factors. There are three main results in this paper. First, by using results from algebraic graph theory, a sufficient condition is obtained which can be used to check equilibrium stability. This condition works for arbitrary topology. It generalizes existing results and also leads to a sufficient condition on the coupling function with which the system is guaranteed to reach synchronization. Second, it is known that identical oscillators with sin() coupling functions are guaranteed to synchronize in phase on a complete graph. Using our results, we demonstrate that for many cases certain structures instead of exact shape of the coupling function such as symmetry and concavity are the keys for global synchronization. Finally, the effect of heterogenous delays is investigated. We develop a new framework by constructing a non-delayed phase model that approximates the original one in the continuum limit. We further derive how its stability properties depend on the delay distribution. In particular, we show that heterogeneity, i.e. wider delay distribution, can help reach in-phase synchronization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of synchronization of weakly coupled oscillators, what are the three key factors that characterize the interaction among oscillators, and how do the existing works on these factors typically focus?\n\n**A)** Coupling, topology, and synchronization; existing works focus on special cases of these factors.\n**B)** Coupling, delay, and topology; existing works focus on the mathematical structure of the coupling function.\n**C)** Coupling, delay, and synchronization; existing works focus on the physical properties of the oscillators.\n**D)** Topology, synchronization, and heterogeneity; existing works focus on the algebraic graph theory framework.\n\n**Correct Answer:** B) Coupling, delay, and topology; existing works focus on the mathematical structure of the coupling function.\n\n**Explanation:** The correct answer is B) Coupling, delay, and topology; existing works focus on the mathematical structure of the coupling function. The documentation states that \"For each of them, the existing work has mainly focused on special cases.\" This implies that the existing works have typically focused on specific mathematical structures or properties of the coupling function, rather than the factors themselves. The correct answer requires the test-taker to understand the nuances of the existing works and the focus of the paper."}, "51": {"documentation": {"title": "Estimation and inference for the indirect effect in high-dimensional\n  linear mediation models", "source": "Ruixuan Rachel Zhou, Liewei Wang, Sihai Dave Zhao", "docs_id": "1910.12457", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimation and inference for the indirect effect in high-dimensional\n  linear mediation models. Mediation analysis is difficult when the number of potential mediators is larger than the sample size. In this paper we propose new inference procedures for the indirect effect in the presence of high-dimensional mediators for linear mediation models. We develop methods for both incomplete mediation, where a direct effect may exist, as well as complete mediation, where the direct effect is known to be absent. We prove consistency and asymptotic normality of our indirect effect estimators. Under complete mediation, where the indirect effect is equivalent to the total effect, we further prove that our approach gives a more powerful test compared to directly testing for the total effect. We confirm our theoretical results in simulations, as well as in an integrative analysis of gene expression and genotype data from a pharmacogenomic study of drug response. We present a novel analysis of gene sets to understand the molecular mechanisms of drug response, and also identify a genome-wide significant noncoding genetic variant that cannot be detected using standard analysis methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of linear mediation models, what is the primary challenge when estimating the indirect effect in the presence of high-dimensional mediators, and how do the proposed inference procedures address this challenge?\n\n**A)** The primary challenge is the non-linear relationship between the mediators and the outcome variable, and the proposed procedures address this by using non-linear regression models.\n\n**B)** The primary challenge is the high dimensionality of the mediator space, and the proposed procedures address this by using dimensionality reduction techniques such as principal component analysis.\n\n**C)** The primary challenge is the incomplete mediation, where a direct effect may exist, and the proposed procedures address this by using methods for estimating the indirect effect in the presence of both complete and incomplete mediation.\n\n**D)** The primary challenge is the lack of prior knowledge about the mediator effects, and the proposed procedures address this by using Bayesian methods to estimate the mediator effects.\n\n**Correct Answer:** C) The primary challenge is the incomplete mediation, where a direct effect may exist, and the proposed procedures address this by using methods for estimating the indirect effect in the presence of both complete and incomplete mediation.\n\n**Explanation:** The correct answer is C) because the documentation states that the paper proposes new inference procedures for the indirect effect in the presence of high-dimensional mediators, specifically for both incomplete mediation (where a direct effect may exist) and complete mediation (where the direct effect is known to be absent). The proposed procedures address the challenge of estimating the indirect effect in the presence of high-dimensional mediators, which is a key issue in mediation analysis."}, "52": {"documentation": {"title": "RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation", "source": "Liuyuan Deng, Ming Yang, Tianyi Li, Yuesheng He, and Chunxiang Wang", "docs_id": "1907.00135", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation. RGB-D semantic segmentation methods conventionally use two independent encoders to extract features from the RGB and depth data. However, there lacks an effective fusion mechanism to bridge the encoders, for the purpose of fully exploiting the complementary information from multiple modalities. This paper proposes a novel bottom-up interactive fusion structure to model the interdependencies between the encoders. The structure introduces an interaction stream to interconnect the encoders. The interaction stream not only progressively aggregates modality-specific features from the encoders but also computes complementary features for them. To instantiate this structure, the paper proposes a residual fusion block (RFB) to formulate the interdependences of the encoders. The RFB consists of two residual units and one fusion unit with gate mechanism. It learns complementary features for the modality-specific encoders and extracts modality-specific features as well as cross-modal features. Based on the RFB, the paper presents the deep multimodal networks for RGB-D semantic segmentation called RFBNet. The experiments on two datasets demonstrate the effectiveness of modeling the interdependencies and that the RFBNet achieved state-of-the-art performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of the interaction stream in the proposed bottom-up interactive fusion structure for RGB-D semantic segmentation?\n\nA) To aggregate modality-specific features from the encoders in a parallel manner\nB) To compute complementary features for the modality-specific encoders and aggregate them in a progressive manner\nC) To extract cross-modal features from the encoders and fuse them with modality-specific features\nD) To learn modality-specific features from the encoders and aggregate them in a hierarchical manner\n\n**Correct Answer:** B) To compute complementary features for the modality-specific encoders and aggregate them in a progressive manner\n\n**Explanation:** The interaction stream in the proposed fusion structure is designed to interconnect the encoders and model the interdependencies between them. The correct answer, B, highlights the key feature of the interaction stream, which is to compute complementary features for the modality-specific encoders and aggregate them in a progressive manner. This allows the network to fully exploit the complementary information from multiple modalities. The other options are incorrect because they either describe the aggregation process in a parallel or hierarchical manner, or focus solely on extracting cross-modal features, which is not the primary purpose of the interaction stream."}, "53": {"documentation": {"title": "Does Better Governance Guarantee Less Corruption? Evidence of Loss in\n  Effectiveness of the Rule of Law", "source": "Omar A. Guerrero and Gonzalo Casta\\~neda", "docs_id": "1902.00428", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Does Better Governance Guarantee Less Corruption? Evidence of Loss in\n  Effectiveness of the Rule of Law. Corruption is an endemic societal problem with profound implications in the development of nations. In combating this issue, cross-national evidence supporting the effectiveness of the rule of law seems at odds with poorly realized outcomes from reforms inspired in such literature. This paper provides an explanation for such contradiction. By taking a computational approach, we develop two methodological novelties into the empirical study of corruption: (1) generating large within-country variation by means of simulation (instead of cross-national data pooling), and (2) accounting for interactions between covariates through a spillover network. The latter (the network), seems responsible for a significant reduction in the effectiveness of the rule of law; especially among the least developed countries. We also find that effectiveness can be boosted by improving complementary policy issues that may lie beyond the governance agenda. Moreover, our simulations suggest that improvements to the rule of law are a necessary yet not sufficient condition to curve corruption."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Does Better Governance Guarantee Less Corruption? Evidence of Loss in Effectiveness of the Rule of Law\" suggest that the effectiveness of the rule of law can be reduced by interactions between covariates, which they model using a spillover network. What is the primary implication of this finding, according to the authors?\n\n**A)** That improving the rule of law is sufficient to reduce corruption, even in the absence of other policy issues.\n**B)** That the effectiveness of the rule of law is reduced in countries with strong social networks, which can facilitate corruption.\n**C)** That the effectiveness of the rule of law can be boosted by improving complementary policy issues that lie beyond the governance agenda.\n**D)** That the relationship between governance and corruption is mediated by a complex network of factors, including social and economic covariates.\n\n**Correct Answer:** C) That the effectiveness of the rule of law can be boosted by improving complementary policy issues that lie beyond the governance agenda.\n\n**Explanation:** The correct answer is based on the authors' finding that \"our simulations suggest that improvements to the rule of law are a necessary yet not sufficient condition to curve corruption.\" This implies that while improving the rule of law is important, it is not enough on its own to reduce corruption, and that other policy issues, such as those related to economic development or social welfare, may also play a role."}, "54": {"documentation": {"title": "Revealing the Intrinsic Magnetism of Non-Magnetic Glasses", "source": "Giancarlo Jug and Sandro Recchia", "docs_id": "2111.00614", "section": ["cond-mat.mes-hall", "cond-mat.dis-nn", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revealing the Intrinsic Magnetism of Non-Magnetic Glasses. Ordinary multi-component silicate glasses belong to a class of amorphous insulators normally displaying no special form of magnetism, save for the Larmor dominant diamagnetism from the constituent atoms' core electrons and the extrinsic Langevin paramagnetism due to the ubiquitous Fe-group dilute paramagnetic impurities. Here we show that the macroscopic magnetisation of three case-study glass types measured in a SQUID-magnetometer cannot be explained solely by means of the Larmor-Langevin contributions. In particular, we reveal a novel {\\em intrinsic} contribution to the bulk magnetisation due to the amorphous structure itself, a contribution that is peculiar both in its temperature and magnetic-field dependence and represents the first true magnetic effect in nominally non-magnetic glasses. The only theoretical interpretation we know of for such an effect and which can consistently explain the experimental data demands the re-thinking of the atomic organisation of glasses at the nanometric scale."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Revealing the Intrinsic Magnetism of Non-Magnetic Glasses\" claim to have discovered a novel intrinsic contribution to the bulk magnetization of amorphous silicate glasses. What is the primary theoretical interpretation that can consistently explain the experimental data, and what implications does it have for our understanding of the atomic organization of glasses at the nanometric scale?\n\n**A)** The authors propose that the intrinsic contribution is due to the presence of defects in the glass structure, which create localized magnetic moments. This interpretation is supported by the temperature and magnetic-field dependence of the effect.\n\n**B)** The authors suggest that the intrinsic contribution arises from the amorphous structure itself, which gives rise to a collective magnetic effect. This interpretation is consistent with the peculiar temperature and magnetic-field dependence of the effect.\n\n**C)** The authors argue that the intrinsic contribution is due to the presence of extrinsic paramagnetic impurities, which are not accounted for in the Larmor-Langevin model. This interpretation is supported by the experimental data, which shows a significant increase in magnetization at low temperatures.\n\n**D)** The authors propose that the intrinsic contribution is due to the presence of a new type of magnetic atom, which is not present in the glass structure. This interpretation is inconsistent with the experimental data, which shows no evidence of such an atom.\n\n**Correct Answer:** B) The authors suggest that the intrinsic contribution arises from the amorphous structure itself, which gives rise to a collective magnetic effect. This interpretation is consistent with the peculiar temperature and magnetic-field dependence of the effect.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that the intrinsic contribution is due to the amorphous structure itself, which gives rise to a collective magnetic effect. This interpretation is supported by the peculiar temperature and magnetic-field dependence of the effect, which cannot be explained by the Larmor-Langevin model. The other options are incorrect because they either propose alternative explanations (A and C) or are inconsistent with the experimental data (D)."}, "55": {"documentation": {"title": "Black holes in $f(\\mathbb Q)$ Gravity", "source": "Fabio D'Ambrosio, Shaun D.B. Fell, Lavinia Heisenberg and Simon Kuhn", "docs_id": "2109.03174", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Black holes in $f(\\mathbb Q)$ Gravity. We systematically study the field equations of $f(\\mathbb Q)$ gravity for spherically symmetric and stationary metric-affine spacetimes. Such spacetimes are described by a metric as well as a flat and torsionless affine connection. In the Symmetric Teleparallel Equivalent of GR (STEGR), the connection is pure gauge and hence unphysical. However, in the non-linear extension $f(\\Q)$, it is promoted to a dynamical field which changes the physics. Starting from a general metric-affine geometry, we construct the most general static and spherically symmetric forms of the metric and the affine connection. We then use these symmetry reduced geometric objects to prove that the field equations of $f(\\Q)$ gravity admit GR solutions as well as beyond-GR solutions, contrary to what has been claimed in the literature. We formulate precise criteria, under which conditions it is possible to obtain GR solutions and under which conditions it is possible to obtain beyond-GR solutions. We subsequently construct several perturbative corrections to the Schwarzschild solution for different choices of $f(\\Q)$, which in particular include a hair stemming from the now dynamical affine connection. We also present an exact beyond-GR vacuum solution. Lastly, we apply this method of constructing spherically symmetric and stationary solutions to $f(\\T)$ gravity, which reproduces similar solutions but without a dynamical connection."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of $f(\\mathbb Q)$ gravity, what is the relationship between the dynamical affine connection and the field equations, and how does this relationship impact the existence of solutions beyond General Relativity (GR)?\n\nA) The dynamical affine connection is a pure gauge field that does not affect the field equations, leading to GR solutions only.\n\nB) The dynamical affine connection is a dynamical field that modifies the field equations, allowing for both GR and beyond-GR solutions.\n\nC) The dynamical affine connection is a flat and torsionless connection that is unphysical, and its presence does not impact the existence of GR solutions.\n\nD) The dynamical affine connection is a non-linear extension of the connection in STEGR, which promotes the connection to a dynamical field that changes the physics, leading to GR solutions and beyond-GR solutions.\n\nCorrect Answer: D) The dynamical affine connection is a non-linear extension of the connection in STEGR, which promotes the connection to a dynamical field that changes the physics, leading to GR solutions and beyond-GR solutions.\n\nExplanation: The correct answer is D) because the documentation states that in the non-linear extension $f(\\mathbb Q)$, the connection is promoted to a dynamical field, which changes the physics and allows for both GR and beyond-GR solutions. The other options are incorrect because they either downplay the role of the dynamical affine connection (A and C) or incorrectly state that it only leads to GR solutions (A)."}, "56": {"documentation": {"title": "A possible shortcut for neutron-antineutron oscillation through mirror\n  world", "source": "Zurab Berezhiani", "docs_id": "2002.05609", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A possible shortcut for neutron-antineutron oscillation through mirror\n  world. Existing bounds on the neutron-antineutron mass mixing, $\\epsilon_{n\\bar n} < {\\rm few} \\times 10^{-24}$ eV, impose a severe upper limit on $n - \\bar n$ transition probability, $P_{n\\bar n}(t) < (t/0.1 ~{\\rm s})^2 \\times 10^{-18}$ or so, where $t$ is the neutron flight time. Here we propose a new mechanism of $n- \\bar n$ transition which is not induced by direct mass mixing $\\epsilon_{n\\bar n}$ but is mediated instead by the neutron mixings with the hypothetical states of mirror neutron $n'$ and mirror antineutron $\\bar{n}'$. The latter can be as large as $\\epsilon_{nn'}, \\epsilon_{n\\bar{n}'} \\sim 10^{-15}$ eV or so, without contradicting present experimental limits and nuclear stability bounds. The probabilities of $n-n'$ and $n-\\bar{n}'$ transitions, $P_{nn'}$ and $P_{n\\bar{n}'}$, depend on environmental conditions in mirror sector, and they can be resonantly amplified by applying the magnetic field of the proper value. This opens up a possibility of $n-\\bar n$ transition with the probability $P_{n\\bar n} \\simeq P_{nn'} P_{n\\bar{n}'}$ which can reach the values $\\sim 10^{-8} $ or even larger. For finding this effect in real experiments, the magnetic field should not be suppressed but properly varied. These mixings can be induced by new physics at the scale of few TeV which may also originate a new low scale co-baryogenesis mechanism between ordinary and mirror sectors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper propose a new mechanism for neutron-antineutron oscillation through the mirror world. What is the key difference between this proposed mechanism and the existing bounds on neutron-antineutron mass mixing, and how does it allow for a significant increase in the transition probability?\n\nA) The proposed mechanism relies on a direct mass mixing between the neutron and antineutron, whereas the existing bounds are based on a different type of mixing. However, the proposed mechanism can amplify the transition probability by resonantly amplifying the probabilities of $n-n'$ and $n-\\bar{n}'$ transitions.\n\nB) The proposed mechanism relies on a direct mass mixing between the neutron and antineutron, whereas the existing bounds are based on a different type of mixing. However, the proposed mechanism can amplify the transition probability by resonantly amplifying the probabilities of $n-n'$ and $n-\\bar{n}'$ transitions, which can be controlled by applying a magnetic field.\n\nC) The proposed mechanism relies on a direct mass mixing between the neutron and antineutron, whereas the existing bounds are based on a different type of mixing. However, the proposed mechanism can amplify the transition probability by resonantly amplifying the probabilities of $n-n'$ and $n-\\bar{n}'$ transitions, which can be controlled by applying a magnetic field, and this effect can be significant even at the scale of few TeV.\n\nD) The proposed mechanism relies on a direct mass mixing between the neutron and antineutron, whereas the existing bounds are based on a different type of mixing. However, the proposed mechanism can amplify the transition probability by resonantly amplifying the probabilities of $n-n'$ and $n-\\bar{n}'$ transitions, which can be controlled by applying a magnetic field, and this effect can be significant even at the scale of few TeV, and may also originate a new low-scale co-baryogenesis mechanism between ordinary and mirror sectors.\n\n**Correct Answer:** D) The proposed mechanism relies on a direct mass mixing between the neutron and antineutron, whereas the existing bounds are based on a different type of mixing. However, the proposed mechanism can amplify the transition probability by resonantly amplifying the probabilities of $n-n'$ and $n-\\bar{n}'$ transitions, which can be controlled by applying a magnetic field, and this effect can be significant even at the scale of few TeV, and may also originate a new low-scale co-baryogenesis mechanism between ordinary and mirror sectors.\n\n**Explanation:** The correct answer is D) because it accurately summarizes the key points of the proposed mechanism. The proposed mechanism relies on a different type of mixing, namely the mixings between the neutron and mirror neutron ($n-n'$) and between the neutron and mirror antineutron ($n-\\bar{n}'$), which can be controlled by applying a magnetic field. This allows for a significant increase in the transition probability, which can be even larger than the existing bounds on neutron-antineutron mass mixing. Additionally, the proposed mechanism may also originate a new low-scale co-baryogenesis mechanism between ordinary and mirror sectors, which is a key aspect of the paper."}, "57": {"documentation": {"title": "Optimal Ciliary Locomotion of Axisymmetric Microswimmers", "source": "Hanliang Guo, Hai Zhu, Ruowen Liu, Marc Bonnet, Shravan Veerapaneni", "docs_id": "2103.15642", "section": ["cond-mat.soft", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Ciliary Locomotion of Axisymmetric Microswimmers. Many biological microswimmers locomote by periodically beating the densely-packed cilia on their cell surface in a wave-like fashion. While the swimming mechanisms of ciliated microswimmers have been extensively studied both from the analytical and the numerical point of view, the optimization of the ciliary motion of microswimmers has received limited attention, especially for non-spherical shapes. In this paper, using an envelope model for the microswimmer, we numerically optimize the ciliary motion of a ciliate with an arbitrary axisymmetric shape. The forward solutions are found using a fast boundary integral method, and the efficiency sensitivities are derived using an adjoint-based method. Our results show that a prolate microswimmer with a 2:1 aspect ratio shares similar optimal ciliary motion as the spherical microswimmer, yet the swimming efficiency can increase two-fold. More interestingly, the optimal ciliary motion of a concave microswimmer can be qualitatively different from that of the spherical microswimmer, and adding a constraint to the ciliary length is found to improve, on average, the efficiency for such swimmers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary finding regarding the optimal ciliary motion of axisymmetric microswimmers, and how does it differ for prolate and concave microswimmers?\n\nA) The optimal ciliary motion for all microswimmers is a sinusoidal wave, with no differences between prolate and concave shapes.\nB) The optimal ciliary motion for prolate microswimmers is similar to that of spherical microswimmers, but with a two-fold increase in swimming efficiency.\nC) The optimal ciliary motion for concave microswimmers can be qualitatively different from that of spherical microswimmers, and adding a constraint to the ciliary length improves efficiency.\nD) The optimal ciliary motion for all microswimmers is a linear motion, with no differences between prolate and concave shapes.\n\nCorrect Answer: C) The optimal ciliary motion for concave microswimmers can be qualitatively different from that of spherical microswimmers, and adding a constraint to the ciliary length improves efficiency.\n\nExplanation: The correct answer is C) because the paper states that the optimal ciliary motion of a concave microswimmer can be qualitatively different from that of the spherical microswimmer, and adding a constraint to the ciliary length is found to improve, on average, the efficiency for such swimmers. This is a key finding of the paper, and the other options are incorrect. Option A is incorrect because the paper does not state that the optimal ciliary motion is a sinusoidal wave for all microswimmers. Option B is incorrect because the paper states that the optimal ciliary motion for prolate microswimmers is similar to that of spherical microswimmers, but with a two-fold increase in swimming efficiency, not the other way around. Option D is incorrect because the paper does not state that the optimal ciliary motion is a linear motion for all microswimmers."}, "58": {"documentation": {"title": "Possible origin of viscosity in the Keplerian accretion disks due to\n  secondary perturbation: Turbulent transport without magnetic field", "source": "Banibrata Mukhopadhyay, Kanak Saha", "docs_id": "1101.4613", "section": ["astro-ph.HE", "astro-ph.CO", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Possible origin of viscosity in the Keplerian accretion disks due to\n  secondary perturbation: Turbulent transport without magnetic field. The origin of hydrodynamic turbulence in rotating shear flow is a long standing puzzle. Resolving it is especially important in astrophysics when the flow angular momentum profile is Keplerian which forms an accretion disk having negligible molecular viscosity. Hence, any viscosity in such systems must be due to turbulence, arguably governed by magnetorotational instability especially when temperature T >~ 10^5. However, such disks around quiescent cataclysmic variables, protoplanetary and star-forming disks, the outer regions of disks in active galactic nuclei are practically neutral in charge because of their low temperature, and thus expected not to be coupled with the magnetic field appropriately to generate any transport due to the magnetorotational instability. This flow is similar to plane Couette flow including the Coriolis force, at least locally. What drives their turbulence and then transport, when such flows do not exhibit any unstable mode under linear hydrodynamic perturbation? We demonstrate that the threedimensional secondary disturbance to the primarily perturbed flow triggering elliptical instability may generate significant turbulent viscosity ranging 0.0001 <~ \\nu_t <~ 0.1 to explain transport in accretion flows."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism proposed to explain the origin of viscosity in Keplerian accretion disks, where the flow is similar to plane Couette flow and does not exhibit any unstable mode under linear hydrodynamic perturbation?\n\n**A)** Magnetorotational instability due to the presence of a strong magnetic field\n**B)** Turbulent transport triggered by a secondary disturbance to the primarily perturbed flow\n**C)** Viscous dissipation due to the Coriolis force\n**D)** Non-linear hydrodynamic effects in the outer regions of the disk\n\n**Correct Answer:** B) Turbulent transport triggered by a secondary disturbance to the primarily perturbed flow\n\n**Explanation:** The question requires the test-taker to understand the context of the documentation, which discusses the origin of viscosity in Keplerian accretion disks. The correct answer, B) Turbulent transport triggered by a secondary disturbance to the primarily perturbed flow, is supported by the text, which states that the authors demonstrate that a threedimensional secondary disturbance can generate significant turbulent viscosity. The other options are incorrect because they either refer to mechanisms that are not relevant to the context (A and D) or are not supported by the text (C)."}, "59": {"documentation": {"title": "Gapless color superconductivity at zero and at finite temperature", "source": "Mei Huang and Igor Shovkovy", "docs_id": "hep-ph/0307273", "section": ["hep-ph", "cond-mat.supr-con", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gapless color superconductivity at zero and at finite temperature. We present a detailed study of the recently proposed gapless color superconductivity in neutral two-flavor quark matter in beta-equilibrium at zero as well as at finite temperature. We clarify the nature of the fermionic quasiparticles, and discuss the physical properties of this gapless phase of quark matter. The appearance of two \"secondary\" spin-1 condensates in the gapless ground state is pointed out. The gluon screening properties are also discussed. It is found that finite temperature properties of the gapless color superconductor are rather unusual. One of the most striking results is that the ratio of the critical temperature to the gap at zero temperature is not a universal number. Moreover, this ratio could become arbitrarily large at some values of the coupling constant. In addition, the value of the gap parameter is not a monotonic function of temperature, although the phase transition is a second order phase transition. In an extreme case, in fact, a nonzero finite temperature gap can appear even if its zero temperature value is vanishing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the unusual property of the gapless color superconductor's critical temperature to gap ratio at finite temperature, and how does it relate to the coupling constant?\n\nA) The ratio is a universal number that decreases with increasing coupling constant.\nB) The ratio is a non-monotonic function of temperature, and it can become arbitrarily large at some values of the coupling constant.\nC) The ratio is a monotonic function of temperature, and it remains constant regardless of the coupling constant.\nD) The ratio is zero at all values of the coupling constant, indicating a phase transition at finite temperature.\n\nCorrect Answer: B) The ratio is a non-monotonic function of temperature, and it can become arbitrarily large at some values of the coupling constant.\n\nExplanation: The correct answer is B) because the documentation states that the ratio of the critical temperature to the gap at zero temperature is not a universal number, and it can become arbitrarily large at some values of the coupling constant. This indicates that the critical temperature to gap ratio is a non-monotonic function of temperature, and it can exhibit unusual behavior at finite temperatures."}}