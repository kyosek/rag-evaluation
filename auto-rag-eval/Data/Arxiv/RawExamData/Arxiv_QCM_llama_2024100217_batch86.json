{"0": {"documentation": {"title": "Spectral Top-Down Recovery of Latent Tree Models", "source": "Yariv Aizenbud, Ariel Jaffe, Meng Wang, Amber Hu, Noah Amsel, Boaz\n  Nadler, Joseph T. Chang, Yuval Kluger", "docs_id": "2102.13276", "section": ["stat.ML", "cs.LG", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral Top-Down Recovery of Latent Tree Models. Modeling the distribution of high dimensional data by a latent tree graphical model is a prevalent approach in multiple scientific domains. A common task is to infer the underlying tree structure, given only observations of its terminal nodes. Many algorithms for tree recovery are computationally intensive, which limits their applicability to trees of moderate size. For large trees, a common approach, termed divide-and-conquer, is to recover the tree structure in two steps. First, recover the structure separately of multiple, possibly random subsets of the terminal nodes. Second, merge the resulting subtrees to form a full tree. Here, we develop Spectral Top-Down Recovery (STDR), a deterministic divide-and-conquer approach to infer large latent tree models. Unlike previous methods, STDR partitions the terminal nodes in a non random way, based on the Fiedler vector of a suitable Laplacian matrix related to the observed nodes. We prove that under certain conditions, this partitioning is consistent with the tree structure. This, in turn, leads to a significantly simpler merging procedure of the small subtrees. We prove that STDR is statistically consistent and bound the number of samples required to accurately recover the tree with high probability. Using simulated data from several common tree models in phylogenetics, we demonstrate that STDR has a significant advantage in terms of runtime, with improved or similar accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the Spectral Top-Down Recovery (STDR) algorithm, and how does it differ from previous methods for tree recovery in latent tree graphical models?\n\n**A)** STDR partitions the terminal nodes randomly, similar to previous methods, and uses a more efficient merging procedure to form a full tree.\n\n**B)** STDR partitions the terminal nodes in a non-random way, based on the Fiedler vector of a suitable Laplacian matrix related to the observed nodes, and achieves a significantly simpler merging procedure.\n\n**C)** STDR is a random forest-based approach that uses multiple trees to recover the tree structure, and is particularly effective for large trees.\n\n**D)** STDR is a heuristic-based approach that uses a greedy algorithm to recover the tree structure, and is limited to small trees.\n\n**Correct Answer:** B) STDR partitions the terminal nodes in a non-random way, based on the Fiedler vector of a suitable Laplacian matrix related to the observed nodes, and achieves a significantly simpler merging procedure.\n\n**Explanation:** The correct answer is B) because the documentation states that STDR partitions the terminal nodes in a non-random way, based on the Fiedler vector of a suitable Laplacian matrix related to the observed nodes. This is a key innovation of the STDR algorithm, and allows for a significantly simpler merging procedure to form a full tree. The other options are incorrect because they do not accurately describe the main contribution of STDR. Option A is incorrect because STDR does not partition the nodes randomly. Option C is incorrect because STDR is not a random forest-based approach. Option D is incorrect because STDR is not a heuristic-based approach, and is not limited to small trees."}, "1": {"documentation": {"title": "Robust double auction mechanisms", "source": "Kiho Yoon", "docs_id": "2102.00669", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust double auction mechanisms. We study the robust double auction mechanisms, that is, the double auction mechanisms that satisfy dominant strategy incentive compatibility, ex-post individual rationality, ex-post budget balance and feasibility. We first establish that the price in any deterministic robust mechanism does not depend on the valuations of the trading players. We next establish that, with the non-bossiness assumption, the price in any deterministic robust mechanism does not depend on players' valuations at all, whether trading or non-trading, i.e., the price is posted in advance. Our main result is a characterization result that, with the non-bossiness assumption along with other assumptions on the properties of the mechanism, the posted price mechanism with an exogenous rationing rule is the only deterministic robust double auction mechanism. We also show that, even without the non-bossiness assumption, it is quite difficult to find a reasonable robust double auction mechanism other than the posted price mechanism with rationing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main result of the study on robust double auction mechanisms, assuming the non-bossiness assumption and other properties of the mechanism?\n\nA) The price in any deterministic robust mechanism depends on the valuations of the trading players.\nB) The price in any deterministic robust mechanism depends on the valuations of the trading players, but not on non-trading players.\nC) The posted price mechanism with an exogenous rationing rule is the only deterministic robust double auction mechanism.\nD) The price in any deterministic robust mechanism is always zero.\n\n**Correct Answer:** C) The posted price mechanism with an exogenous rationing rule is the only deterministic robust double auction mechanism.\n\n**Explanation:** The main result of the study is that, under the non-bossiness assumption and other assumptions, the posted price mechanism with an exogenous rationing rule is the only deterministic robust double auction mechanism. This means that, in a robust double auction mechanism, the price is posted in advance and is independent of the valuations of the trading players, whether trading or non-trading. The other options are incorrect because they do not accurately reflect the main result of the study."}, "2": {"documentation": {"title": "Pointwise convergence of Birkhoff averages for global observables", "source": "Marco Lenci, Sara Munday", "docs_id": "1804.05359", "section": ["math.DS", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pointwise convergence of Birkhoff averages for global observables. It is well-known that a strict analogue of the Birkhoff Ergodic Theorem in infinite ergodic theory is trivial; it states that for any infinite-measure-preserving ergodic system the Birkhoff average of every integrable function is almost everywhere zero. Nor does a different rescaling of the Birkhoff sum that leads to a non-degenerate pointwise limit exist. In this paper we give a version of Birkhoff's theorem for conservative, ergodic, infinite-measure-preserving dynamical systems where instead of integrable functions we use certain elements of $L^\\infty$, which we generically call global observables. Our main theorem applies to general systems but requires an hypothesis of \"approximate partial averaging\" on the observables. The idea behind the result, however, applies to more general situations, as we show with an example. Finally, by means of counterexamples and numerical simulations, we discuss the question of finding the optimal class of observables for which a Birkhoff theorem holds for infinite-measure-preserving systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a conservative, ergodic, infinite-measure-preserving dynamical system with a global observable $f \\in L^\\infty$. Suppose that the system satisfies the \"approximate partial averaging\" hypothesis, which states that for any $\\epsilon > 0$, there exists a finite set of times $t_1, \\ldots, t_n$ such that for any $x \\in \\mathbb{R}^d$, we have\n\n$$\\left| \\frac{1}{n} \\sum_{i=1}^n f(t_i x) - \\int f(x) dx \\right| < \\epsilon.$$\n\nWhat can be concluded about the pointwise convergence of the Birkhoff average of $f$?\n\nA) The Birkhoff average converges almost everywhere to the integral of $f$.\nB) The Birkhoff average converges almost everywhere to a non-degenerate limit.\nC) The Birkhoff average converges almost everywhere to zero.\nD) The Birkhoff average does not converge almost everywhere.\n\nCorrect Answer: C) The Birkhoff average converges almost everywhere to zero.\n\nExplanation: The paper states that the main theorem applies to general systems, but requires an hypothesis of \"approximate partial averaging\" on the observables. The idea behind the result, however, applies to more general situations, as shown with an example. The correct answer is based on the fact that the Birkhoff average of every integrable function is almost everywhere zero in infinite ergodic theory, and the paper provides a version of Birkhoff's theorem for conservative, ergodic, infinite-measure-preserving dynamical systems where instead of integrable functions, they use certain elements of $L^\\infty$, which they generically call global observables. The \"approximate partial averaging\" hypothesis is a weaker condition than the usual integrability assumption, and it is likely that the Birkhoff average converges almost everywhere to zero under this hypothesis."}, "3": {"documentation": {"title": "Nuclear structure investigation of even-even Sn isotopes within the\n  covariant density functional theory", "source": "Younes El Bassem and Mostafa Oulne", "docs_id": "1904.10318", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nuclear structure investigation of even-even Sn isotopes within the\n  covariant density functional theory. The current investigation aims to study the ground-state properties of one of the most interesting isotopic chains in the periodic table, 94-168Sn, from the proton drip line to the neutron drip line by using the covariant density functional theory, which is a modern theoretical tool for the description of nuclear structure phenomena. The physical observables of interest include the binding energy, separation energy, two-neutron shell gap, rms-radii for protons and neutrons, pairing energy and quadrupole deformation. The calculations are performed for a wide range of neutron numbers, starting from the proton-rich side up to the neutron-rich one, by using the density-dependent meson-exchange and the density dependent point-coupling effective interactions. The obtained results are discussed and compared with available experimental data and with the already existing results of relativistic Mean Field (RMF) model with NL3 functional. The shape phase transition for Sn isotopic chain is also investigated. A reasonable agreement is found between our calculated results and the available experimental data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the investigation into the nuclear structure of even-even Sn isotopes using the covariant density functional theory, and what physical observables are being studied?\n\nA) To investigate the ground-state properties of the Sn isotopic chain from the proton drip line to the neutron drip line, with a focus on the two-neutron shell gap and rms-radii for protons and neutrons.\nB) To study the pairing energy and quadrupole deformation of Sn isotopes, with a focus on the shape phase transition.\nC) To compare the results of the covariant density functional theory with the relativistic Mean Field (RMF) model with NL3 functional, with a focus on the binding energy and separation energy.\nD) To investigate the proton drip line and neutron drip line of the Sn isotopic chain, with a focus on the density-dependent meson-exchange and point-coupling effective interactions.\n\nCorrect Answer: A) To investigate the ground-state properties of the Sn isotopic chain from the proton drip line to the neutron drip line, with a focus on the two-neutron shell gap and rms-radii for protons and neutrons.\n\nExplanation: The question requires the student to understand the primary goal of the investigation, which is to study the ground-state properties of the Sn isotopic chain. The correct answer highlights the focus on the two-neutron shell gap and rms-radii for protons and neutrons, which are key physical observables being studied. The other options are incorrect because they focus on specific aspects of the study, but do not capture the primary goal of the investigation."}, "4": {"documentation": {"title": "Note on the Existence of Hydrogen Atoms in Higher Dimensional Euclidean\n  Spaces", "source": "Francisco Caruso, Jordan Martins, Vitor Oguri", "docs_id": "1205.3740", "section": ["quant-ph", "math-ph", "math.MP", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Note on the Existence of Hydrogen Atoms in Higher Dimensional Euclidean\n  Spaces. The question of whether hydrogen atoms can exist or not in spaces with a number of dimensions greater than 3 is revisited, considering higher dimensional Euclidean spaces. Previous results which lead to different answers to this question are briefly reviewed. The scenario where not only the kinematical term of Schr\\\"odinger equation is generalized to a D-dimensional space but also the electric charge conservation law (expressed here by the Poisson law) should actually remains valid is assumed. In this case, the potential energy in the Schr\\\"odinger equation goes like 1/r^{D-2}. The lowest quantum mechanical bound states and the corresponding wave functions are determined by applying the Numerov numerical method to solve Schr\\\"odinger's eigenvalue equation. States for different angular momentum quantum number (l = 0; 1) and dimensionality (5 \\leq D \\leq 10) are considered. One is lead to the result that hydrogen atoms in higher dimensions could actually exist. For the same range of the dimensionality D, the energy eigenvalues and wave functions are determined for l = 1. The most probable distance between the electron and the nucleus are then computed as a function of D showing the possibility of tiny bound states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Arxiv paper \"Note on the Existence of Hydrogen Atoms in Higher Dimensional Euclidean Spaces\", what is the potential energy term in the Schr\u00f6dinger equation for a hydrogen atom in a D-dimensional space, assuming the Poisson law for electric charge conservation remains valid?\n\nA) 1/r^D\nB) 1/r^(D-1)\nC) 1/r^(D-2)\nD) 1/r^D + 1/r^(D-1)\n\n**Correct Answer:** C) 1/r^(D-2)\n\n**Explanation:** The paper states that the potential energy in the Schr\u00f6dinger equation goes like 1/r^(D-2) in a D-dimensional space. This is a key assumption in the paper, as it allows for the possibility of hydrogen atoms existing in higher dimensions. The other options are incorrect, as they do not match the potential energy term derived from the Poisson law.\n\n**Additional Question:** What is the lowest quantum mechanical bound state for the hydrogen atom in a 6-dimensional space, assuming l = 0?\n\nA) 1/r^4\nB) 1/r^5\nC) 1/r^6\nD) 1/r^7\n\n**Correct Answer:** B) 1/r^5\n\n**Explanation:** The paper states that the lowest quantum mechanical bound states and corresponding wave functions are determined by applying the Numerov numerical method to solve Schr\u00f6dinger's eigenvalue equation. For l = 0 and D = 6, the lowest bound state is found to be 1/r^5."}, "5": {"documentation": {"title": "Plane Wave Elastography: A Frequency-Domain Ultrasound Shear Wave\n  Elastography Approach", "source": "Reza Khodayi-mehr, Matthew W. Urban, Michael M. Zavlanos, and Wilkins\n  Aquino", "docs_id": "2012.04121", "section": ["physics.med-ph", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Plane Wave Elastography: A Frequency-Domain Ultrasound Shear Wave\n  Elastography Approach. In this paper, we propose Plane Wave Elastography (PWE), a novel ultrasound shear wave elastography (SWE) approach. Currently, commercial methods for SWE rely on directional filtering based on the prior knowledge of the wave propagation direction, to remove complicated wave patterns formed due to reflection and refraction. The result is a set of decomposed directional waves that are separately analyzed to construct shear modulus fields that are then combined through compounding. Instead, PWE relies on a rigorous representation of the wave propagation using the frequency-domain scalar wave equation to automatically select appropriate propagation directions and simultaneously reconstruct shear modulus fields. Specifically, assuming a homogeneous, isotropic, incompressible, linear-elastic medium, we represent the solution of the wave equation using a linear combination of plane waves propagating in arbitrary directions. Given this closed-form solution, we formulate the SWE problem as a nonlinear least-squares optimization problem which can be solved very efficiently. Through numerous phantom studies, we show that PWE can handle complicated waveforms without prior filtering and is competitive with state-of-the-art that requires prior filtering based on the knowledge of propagation directions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Plane Wave Elastography (PWE) is a novel ultrasound shear wave elastography (SWE) approach that relies on a rigorous representation of wave propagation using the frequency-domain scalar wave equation. What is the key assumption made about the medium in which PWE operates, and how does it impact the formulation of the SWE problem?\n\n**A)** The medium is anisotropic, nonlinear, and incompressible, and the SWE problem is formulated as a linear least-squares optimization problem.\n\n**B)** The medium is homogeneous, isotropic, and incompressible, and the SWE problem is formulated as a nonlinear least-squares optimization problem.\n\n**C)** The medium is homogeneous, anisotropic, and nonlinear, and the SWE problem is formulated as a linear least-squares optimization problem.\n\n**D)** The medium is incompressible, nonlinear, and isotropic, and the SWE problem is formulated as a linear least-squares optimization problem.\n\n**Correct Answer:** B) The medium is homogeneous, isotropic, and incompressible, and the SWE problem is formulated as a nonlinear least-squares optimization problem.\n\n**Explanation:** According to the documentation, the key assumption made about the medium in which PWE operates is that it is homogeneous, isotropic, and incompressible. This assumption is crucial in formulating the SWE problem as a nonlinear least-squares optimization problem, which can be solved efficiently. The correct answer reflects this assumption and the corresponding formulation of the SWE problem."}, "6": {"documentation": {"title": "Evidence Aggregation for Treatment Choice", "source": "Takuya Ishihara and Toru Kitagawa", "docs_id": "2108.06473", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evidence Aggregation for Treatment Choice. Consider a planner who has to decide whether or not to introduce a new policy to a certain local population. The planner has only limited knowledge of the policy's causal impact on this population due to a lack of data but does have access to the publicized results of intervention studies performed for similar policies on different populations. How should the planner make use of and aggregate this existing evidence to make her policy decision? Building upon the paradigm of `patient-centered meta-analysis' proposed by Manski (2020; Towards Credible Patient-Centered Meta-Analysis, Epidemiology), we formulate the planner's problem as a statistical decision problem with a social welfare objective pertaining to the local population, and solve for an optimal aggregation rule under the minimax-regret criterion. We investigate the analytical properties, computational feasibility, and welfare regret performance of this rule. We also compare the minimax regret decision rule with plug-in decision rules based upon a hierarchical Bayes meta-regression or stylized mean-squared-error optimal prediction. We apply the minimax regret decision rule to two settings: whether to enact an active labor market policy given evidence from 14 randomized control trial studies; and whether to approve a drug (Remdesivir) for COVID-19 treatment using a meta-database of clinical trials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A planner is considering introducing a new policy to a local population, but has limited knowledge of its causal impact due to a lack of data. The planner has access to the publicized results of intervention studies performed for similar policies on different populations. How should the planner aggregate this existing evidence to make a policy decision, and what are the key considerations in choosing an aggregation rule?\n\n**A)** The planner should use a hierarchical Bayes meta-regression approach to aggregate the evidence, as it allows for the incorporation of prior knowledge and uncertainty.\n\n**B)** The planner should use a stylized mean-squared-error optimal prediction approach, as it provides a simple and intuitive way to aggregate the evidence and make a decision.\n\n**C)** The planner should use a minimax-regret decision rule, as it balances the trade-off between minimizing regret and maximizing expected social welfare, and is particularly well-suited for situations with limited data.\n\n**D)** The planner should use a plug-in decision rule based on the average treatment effect, as it is a simple and widely used approach that can be effective in certain situations.\n\n**Correct Answer:** C) The planner should use a minimax-regret decision rule, as it balances the trade-off between minimizing regret and maximizing expected social welfare, and is particularly well-suited for situations with limited data.\n\n**Explanation:** The minimax-regret decision rule is a suitable approach for this problem because it takes into account the uncertainty and limited data available to the planner. By minimizing regret, the planner can ensure that the decision is robust to the uncertainty in the data, and by maximizing expected social welfare, the planner can ensure that the decision is in the best interest of the local population. This approach is particularly well-suited for situations with limited data, as it can help to mitigate the effects of uncertainty and provide a more reliable decision."}, "7": {"documentation": {"title": "Epigenetic Tracking: a Model for Multicellular Biology", "source": "Alessandro Fontana", "docs_id": "1304.2174", "section": ["q-bio.CB", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Epigenetic Tracking: a Model for Multicellular Biology. Epigenetic Tracking is a mathematical model of biological cells, originally conceived to study embryonic development. Computer simulations proved the capacity of the model to generate complex 3-dimensional cellular structures, and the potential to reproduce the complexity typical of living beings. The most distinctive feature of this model is the presence in the body of a homogeneous distribution of stem cells, which are dinamically and continuously created during development from non-stem cells and reside in niches. Embryonic stem cells orchestrate early developmental events, adult stem cells direct late developmental and regeneration events, ageing stem cells cause ageing and cancer stem cells are responsible for cancer growth. The conceptual backbone provided by Epigenetic Tracking brings together a wide range of biological phenomena: for this reason, we think it can be proposed as a general model for multicellular biology. Despite, or perhaps due to its theoretical origin, the model allowed us to make predictions relevant to very diverse fields of biology, such as transposable elements, and cancer-related patterns of gene mutations. This paper contains a summary of the model and its implications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary function of the \"niches\" in the Epigenetic Tracking model, and how do they relate to the different types of stem cells mentioned in the paper?\n\nA) The niches are responsible for the creation of new stem cells, which then direct late developmental events.\nB) The niches are where adult stem cells reside and orchestrate early developmental events.\nC) The niches are where ageing stem cells cause ageing and cancer stem cells are responsible for cancer growth.\nD) The niches are a homogeneous distribution of stem cells that are dynamically created from non-stem cells during development.\n\n**Correct Answer:** D) The niches are a homogeneous distribution of stem cells that are dynamically created from non-stem cells during development.\n\n**Explanation:** According to the paper, the Epigenetic Tracking model features a homogeneous distribution of stem cells that are dynamically and continuously created from non-stem cells and reside in niches. This distribution is a key feature of the model, and it is responsible for the creation of different types of stem cells, including embryonic, adult, ageing, and cancer stem cells. The correct answer highlights the role of the niches in the creation of these stem cells, which is a crucial aspect of the model's functionality."}, "8": {"documentation": {"title": "Shape changing (intensity redistribution) collisions of solitons in\n  mixed coupled nonlinear Schr{\\\"o}dinger equations", "source": "T. Kanna, M. Lakshmanan, P. Tchofo Dinda, and Nail Akhmediev", "docs_id": "nlin/0511034", "section": ["nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shape changing (intensity redistribution) collisions of solitons in\n  mixed coupled nonlinear Schr{\\\"o}dinger equations. A novel kind of shape changing (intensity redistribution) collision with potential application to signal amplification is identified in the integrable $N$-coupled nonlinear Schr{\\\"o}dinger (CNLS) equations with mixed signs of focusing and defocusing type nonlinearity coefficients. The corresponding soliton solutions for N=2 case are obtained by using Hirota's bilinearization method. The distinguishing feature of the mixed sign CNLS equations is that the soliton solutions can both be singular and regular. Although the general soliton solution admits singularities we present parametric conditions for which non-singular soliton propagation can occur. The multisoliton solutions and a generalization of the results to multicomponent case with arbitrary $N$ are also presented. An appealing feature of soliton collision in the present case is that all the components of a soliton can simultaneously enhance their amplitudes, which can lead to new kind of amplification process without induced noise."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the integrable $N$-coupled nonlinear Schr\u00f6dinger (CNLS) equations with mixed signs of focusing and defocusing type nonlinearity coefficients, what is the distinguishing feature of the soliton solutions that allows for both singular and regular soliton propagation?\n\n**A)** The soliton solutions can only be singular when the nonlinearity coefficients have the same sign.\n**B)** The soliton solutions can only be regular when the nonlinearity coefficients have opposite signs.\n**C)** The soliton solutions can be singular or regular depending on the parametric conditions of the nonlinearity coefficients.\n**D)** The soliton solutions can only be singular when the nonlinearity coefficients are zero.\n\n**Correct Answer:** C) The soliton solutions can be singular or regular depending on the parametric conditions of the nonlinearity coefficients.\n\n**Explanation:** The correct answer is C) because the documentation states that \"Although the general soliton solution admits singularities we present parametric conditions for which non-singular soliton propagation can occur.\" This indicates that the soliton solutions can be either singular or regular, depending on the specific conditions of the nonlinearity coefficients."}, "9": {"documentation": {"title": "Community membership consistency applied to corporate board interlock\n  networks", "source": "Dafne E. van Kuppevelt, Rena Bakhshi, Eelke M. Heemskerk, Frank W.\n  Takes", "docs_id": "2008.00745", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Community membership consistency applied to corporate board interlock\n  networks. Community detection is a well established method for studying the meso scale structure of social networks. Applying a community detection algorithm results in a division of a network into communities that is often used to inspect and reason about community membership of specific nodes. This micro level interpretation step of community structure is a crucial step in typical social science research. However, the methodological caveat in this step is that virtually all modern community detection methods are non-deterministic and based on randomization and approximated results. This needs to be explicitly taken into consideration when reasoning about community membership of individual nodes. To do so, we propose a metric of community membership consistency, that provides node-level insights in how reliable the placement of that node into a community really is. In addition, it enables us to distinguish the community core members of a community. The usefulness of the proposed metrics is demonstrated on corporate board interlock networks, in which weighted links represent shared senior level directors between firms. Results suggest that the community structure of global business groups is centered around persistent communities consisting of core countries tied by geographical and cultural proximity. In addition, we identify fringe countries that appear to associate with a number of different global business communities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of community detection methods in social network analysis, and how does the proposed metric of community membership consistency address this limitation?\n\n**A)** Community detection methods are deterministic and provide exact results, but the proposed metric is based on randomization and approximated results, which can lead to inconsistent community membership assignments.\n\n**B)** Community detection methods are deterministic and provide exact results, but the proposed metric is based on randomization and approximated results, which can lead to inconsistent community membership assignments, and this limitation needs to be explicitly taken into consideration when reasoning about community membership of individual nodes.\n\n**C)** Community detection methods are deterministic and provide exact results, but the proposed metric is based on randomization and approximated results, which can lead to inconsistent community membership assignments, and this limitation can be addressed by using the proposed metric to distinguish the community core members of a community.\n\n**D)** Community detection methods are deterministic and provide exact results, but the proposed metric is based on randomization and approximated results, which can lead to inconsistent community membership assignments, and this limitation can be addressed by using the proposed metric to distinguish the community core members of a community, but only for fringe countries.\n\n**Correct Answer:** B) Community detection methods are deterministic and provide exact results, but the proposed metric is based on randomization and approximated results, which can lead to inconsistent community membership assignments, and this limitation needs to be explicitly taken into consideration when reasoning about community membership of individual nodes.\n\n**Explanation:** The correct answer is B) because the documentation states that \"virtually all modern community detection methods are non-deterministic and based on randomization and approximated results\", which means that community detection methods are not deterministic and provide exact results. The proposed metric of community membership consistency addresses this limitation by providing node-level insights into how reliable the placement of a node into a community is, and enabling the distinction of community core members."}, "10": {"documentation": {"title": "Experimental Evaluation of Vibration Influence on a Resonant MEMS\n  Scanning System for Automotive Lidars", "source": "Han Woong Yoo, Rene Riegler, David Brunner, Stephan Albert, Thomas\n  Thurner, Georg Schitter", "docs_id": "2010.08327", "section": ["eess.SY", "cs.SY", "eess.SP", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Experimental Evaluation of Vibration Influence on a Resonant MEMS\n  Scanning System for Automotive Lidars. This paper demonstrates a vibration test for a resonant MEMS scanning system in operation to evaluate the vibration immunity for automotive lidar applications. The MEMS mirror has a reinforcement structure on the backside of the mirror, causing vibration coupling by a mismatch between the center of mass and the rotation axis. An analysis of energy variation is proposed, showing direction dependency of vibration coupling. Vibration influences are evaluated by transient vibration response and vibration frequency sweep using a single tone vibration for translational y- and z- axis. The measurement results demonstrate standard deviation (STD) amplitude and frequency errors are up to 1.64 % and 0.26 %, respectively, for 2 grms single tone vibrations on y axis. The simulation results also show a good agreement with both measurements, proving the proposed vibration coupling mechanism of the MEMS mirror. The phased locked loop (PLL) improves the STD amplitude and frequency errors to 0.91 % and 0.15 % for y axis vibration, corresponding to 44.4 % and 43.0 % reduction, respectively, showing the benefit of a controlled MEMS mirror for reliable automotive MEMS lidars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A resonant MEMS scanning system for automotive lidars is subjected to 2 grms single tone vibrations on the y-axis. The vibration coupling mechanism of the MEMS mirror is analyzed using energy variation and transient vibration response. The measurement results show a standard deviation (STD) amplitude error of up to 1.64%. However, the use of a phased locked loop (PLL) improves the STD amplitude error to 0.91%. What is the percentage reduction in STD amplitude error achieved by using a PLL in the MEMS scanning system?\n\nA) 30.0%\nB) 44.4%\nC) 50.0%\nD) 60.0%\n\n**Correct Answer:** B) 44.4%\n\n**Explanation:** The correct answer can be obtained by subtracting the STD amplitude error after PLL (0.91%) from the STD amplitude error before PLL (1.64%) and then dividing by the original error and multiplying by 100. This gives (1.64 - 0.91) / 1.64 * 100 = 44.4%."}, "11": {"documentation": {"title": "Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability", "source": "Michael J. Proulx, Theodoros Eracleous, Ben Spencer, Anna Passfield,\n  Alexandra de Sousa, and Ali Mohammadi", "docs_id": "2105.13295", "section": ["cs.HC", "cs.MM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability. Sensory substitution has influenced the design of many tactile visual substitution systems with the aim of offering visual aids for the blind. This paper focuses on whether a novel electromagnetic vibrotactile display, a four by four vibrotactile matrix of taxels, can serve as an aid for dynamic communication for visually impaired people. A mixed methods approach was used to firstly assess whether pattern complexity affected undergraduate participants' perceptive success, and secondly, if participants total score positively correlated with their perceived success ratings. A thematic analysis was also conducted on participants' experiences with the vibrotactile display and what methods of interaction they used. The results indicated that complex patterns were less accurately perceived than simple and linear patterns respectively, and no significant correlation was found between participants' score and perceived success ratings. Additionally, most participants interacted with the vibrotactile display in similar ways using one finger to feel one taxel at a time; arguably, the most effective strategy from previous research. This technology could have applications to navigational and communication aids for the visually impaired and road users."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A researcher is designing a vibrotactile display for visually impaired individuals. The display consists of a 4x4 matrix of taxels, which are stimulated by electromagnetic actuators. The researcher wants to assess the usability of the display, particularly in terms of pattern complexity and participant interaction strategies. Which of the following statements best summarizes the findings of the study?\n\nA) The study found that complex patterns were more accurately perceived by participants than simple and linear patterns, and that participants' scores positively correlated with their perceived success ratings.\n\nB) The study found that complex patterns were less accurately perceived by participants than simple and linear patterns, and that participants' scores did not correlate with their perceived success ratings.\n\nC) The study found that participants' scores were significantly higher when they used a two-fingered interaction strategy compared to a one-fingered strategy, and that complex patterns were more accurately perceived.\n\nD) The study found that participants' scores were significantly higher when they used a one-fingered interaction strategy compared to a two-fingered strategy, and that complex patterns were less accurately perceived.\n\nCorrect Answer: B) The study found that complex patterns were less accurately perceived by participants than simple and linear patterns, and that participants' scores did not correlate with their perceived success ratings.\n\nExplanation: The correct answer is B) because the study found that complex patterns were less accurately perceived by participants than simple and linear patterns, as stated in the original text. Additionally, the study did not find a significant correlation between participants' scores and their perceived success ratings, as stated in the original text. The other options are incorrect because they either misstate the findings of the study or introduce new information not present in the original text."}, "12": {"documentation": {"title": "Computing Viscous Flow Along a 3D Open Tube Using the Immerse Interface\n  Method", "source": "Sarah E Patterson and Anita T Layton", "docs_id": "2112.12892", "section": ["physics.flu-dyn", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computing Viscous Flow Along a 3D Open Tube Using the Immerse Interface\n  Method. In a companion study \\cite{patterson2020computing2D}, we present a numerical method for simulating 2D viscous flow through an open compliant closed channel, drive by pressure gradient. We consider the highly viscous regime, where fluid dynamics is described by the Stokes equations, and the less viscous regime described by the Navier-Stokes equations. In this study, we extend the method to 3D tubular flow. The problem is formulated in axisymmetric cylindrical coordinates, an approach that is natural for tubular flow simulations and that substantially reduces computational cost. When the elastic tubular walls are stretched or compressed, they exert forces on the fluid. These singular forces introduce unsmoothness into the fluid solution. As in the companion 2D study \\cite{patterson2020computing2D}, we extend the immersed interface method to an open tube, and we compute solution to the model equations using the resulting method. Numerical results indicate that this new method preserves sharp jumps in the solution and its derivatives, and converges with second-order accuracy in both space and time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the immersed interface method for simulating 3D viscous flow along an open tube, what is the primary advantage of using axisymmetric cylindrical coordinates, and how does this approach impact the computational cost of the simulation?\n\nA) Axisymmetric cylindrical coordinates reduce the computational cost by allowing for a more efficient discretization of the spatial domain, but may introduce numerical instability in the solution.\nB) Axisymmetric cylindrical coordinates reduce the computational cost by substantially reducing the number of degrees of freedom required to solve the problem, making it more suitable for large-scale simulations.\nC) Axisymmetric cylindrical coordinates do not have a significant impact on the computational cost, as the method is primarily concerned with preserving sharp jumps in the solution and its derivatives.\nD) Axisymmetric cylindrical coordinates are not suitable for simulating 3D viscous flow along an open tube, as they do not account for the complex geometry of the tube.\n\nCorrect Answer: B) Axisymmetric cylindrical coordinates reduce the computational cost by substantially reducing the number of degrees of freedom required to solve the problem, making it more suitable for large-scale simulations."}, "13": {"documentation": {"title": "A hydrodynamic analytical model of fish tilt angle: Implications for the\n  modelling of the acoustic target strength", "source": "Federico Agust\\'in Membiela and Mat\\'ias Germ\\'an dell'Erba", "docs_id": "1707.00584", "section": ["physics.flu-dyn", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A hydrodynamic analytical model of fish tilt angle: Implications for the\n  modelling of the acoustic target strength. We implement a simple hydrodynamical model to study behavioural swimming tilt angle of open swimmbladder fish. For this purpose we study the stability of forces acting on a fish swimming horizontally with constant velocity. Additionally, the open swimbladder compression with the depth is modelled by Boyle's law. With these, our model gives an analytical solution relating the depth with the body tilt angle and the velocity. An interesting result for steady horizontal swimming is that the body tilt decreases with velocity almost like $v^{-1}$. Moreover, we give an expression for the maximum tilt angle. Then, by introducing the assumption of constant swimming power we relate the swimming velocity with the tilting. Furthermore, we show that the hydrodynamical influence of a temperature gradient produced by a thermocline seems to be negligible for the fish tilting. These results are considerably helpful for more realistic modelling of the \\emph{acoustic target strength} of fish. Finally, we tested our results by comparing the hydrodynamics solutions with others obtained from acoustic observations and simulations of target strength for Argentine anchovy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main finding regarding the relationship between the swimming velocity of open swimmbladder fish and their body tilt angle, according to the hydrodynamic analytical model presented in the study?\n\nA) The body tilt angle increases with velocity.\nB) The body tilt angle decreases with velocity, following a relationship of approximately $v^{-1}$.\nC) The swimming velocity has no effect on the body tilt angle.\nD) The body tilt angle remains constant with changes in swimming velocity.\n\n**Correct Answer:** B) The body tilt angle decreases with velocity, following a relationship of approximately $v^{-1}$.\n\n**Explanation:** The study presents a hydrodynamic analytical model that investigates the stability of forces acting on a fish swimming horizontally with constant velocity. The model shows that the body tilt angle decreases with velocity almost like $v^{-1}$, indicating a negative relationship between the two variables. This finding has implications for the modelling of the acoustic target strength of fish."}, "14": {"documentation": {"title": "Functional additive models on manifolds of planar shapes and forms", "source": "Almond St\\\"ocker, Sonja Greven", "docs_id": "2109.02624", "section": ["stat.ME", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Functional additive models on manifolds of planar shapes and forms. Defining shape and form as equivalence classes under translation, rotation and -- for shapes -- also scale, we extend generalized additive regression to models for the shape/form of planar curves and/or landmark configurations. The model respects the resulting quotient geometry of the response, employing the squared geodesic distance as loss function and a geodesic response function to map the additive predictor to the shape/form space. For fitting the model, we propose a Riemannian $L_2$-Boosting algorithm well suited for a potentially large number of possibly parameter-intensive model terms, which also yields automated model selection. We provide novel intuitively interpretable visualizations for (even non-linear) covariate effects in the shape/form space via suitable tensor-product factorization. The usefulness of the proposed framework is illustrated in an analysis of 1) astragalus shapes of wild and domesticated sheep and 2) cell forms generated in a biophysical model, as well as 3) in a realistic simulation study with response shapes and forms motivated from a dataset on bottle outlines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of functional additive models on manifolds of planar shapes and forms, what is the primary advantage of employing the squared geodesic distance as the loss function, and how does this relate to the proposed Riemannian $L_2$-Boosting algorithm?\n\nA) The squared geodesic distance allows for more accurate modeling of non-linear relationships between covariates and shape/form responses, enabling the Riemannian $L_2$-Boosting algorithm to effectively handle complex model terms.\n\nB) The squared geodesic distance provides a more robust measure of distance between shape/form configurations, reducing the impact of outliers and enabling the Riemannian $L_2$-Boosting algorithm to converge more efficiently.\n\nC) The squared geodesic distance is a more computationally efficient alternative to traditional Euclidean distance metrics, allowing the Riemannian $L_2$-Boosting algorithm to handle large datasets with ease.\n\nD) The squared geodesic distance is a more interpretable measure of distance between shape/form configurations, enabling the Riemannian $L_2$-Boosting algorithm to provide more intuitive insights into the relationships between covariates and shape/form responses.\n\nCorrect Answer: A) The squared geodesic distance allows for more accurate modeling of non-linear relationships between covariates and shape/form responses, enabling the Riemannian $L_2$-Boosting algorithm to effectively handle complex model terms.\n\nExplanation: The correct answer is A) because the squared geodesic distance is used as the loss function in the model, which allows for more accurate modeling of non-linear relationships between covariates and shape/form responses. This is a key advantage of the proposed framework, as it enables the Riemannian $L_2$-Boosting algorithm to effectively handle complex model terms and provide more accurate predictions. The other options are incorrect because they do not accurately capture the primary advantage of using the squared geodesic distance as the loss function."}, "15": {"documentation": {"title": "Victims of electoral violence and their children experience irreversible\n  stunting: The long-term welfare effects of electoral violence", "source": "Roxana Guti\\'errez-Romero", "docs_id": "2112.13849", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Victims of electoral violence and their children experience irreversible\n  stunting: The long-term welfare effects of electoral violence. Despite the extensive literature on civil conflict, little is known about the medium- and long-term effects of electoral violence on young children and adolescents. This paper shows that electoral violence of low scale yet recursive nature has a detrimental effect on the height of children and adolescents of affected households. Our identification strategy uses the variation of electoral violence across time and space in Kenya during 1992-2013. We find that infants and adolescents exposed to electoral violence are shorter as adults if compared to similar people not exposed to violence during their growing age. We also find inter-generation effects as the children of the victims of electoral violence, particularly boys, also have reduced height-for-age. Higher food prices and changes in diet experienced during outbreaks of violence are important mechanisms. No impact is found on the educational attainment of school-aged pupils as electoral violence has been concentrated during the school holidays."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the long-term effects of electoral violence on the physical growth and development of children and adolescents, using the study on Kenya from 1992-2013 as a case study. How do the findings of this study relate to the broader literature on civil conflict and its impact on human development?\n\n**A)** The study suggests that electoral violence has a negligible impact on the physical growth and development of children and adolescents, and that other factors such as food prices and diet are more significant determinants of growth outcomes.\n\n**B)** The study reveals that electoral violence has a detrimental effect on the height of children and adolescents, particularly boys, and that this effect is exacerbated by inter-generational transmission of trauma.\n\n**C)** The study finds no correlation between electoral violence and physical growth outcomes, and that the impact of violence on children's education is a more significant concern.\n\n**D)** The study demonstrates that electoral violence has a positive impact on the physical growth and development of children and adolescents, and that this effect is mediated by factors such as increased food security and access to education.\n\n**Correct Answer:** B) The study reveals that electoral violence has a detrimental effect on the height of children and adolescents, particularly boys, and that this effect is exacerbated by inter-generational transmission of trauma.\n\n**Explanation:** The study found that infants and adolescents exposed to electoral violence were shorter as adults compared to similar individuals not exposed to violence during their growing age. Additionally, the study found inter-generational effects, where the children of victims of electoral violence, particularly boys, also had reduced height-for-age. This suggests that electoral violence has a detrimental impact on physical growth and development, particularly for boys. The correct answer requires the test-taker to understand the study's findings and their implications for human development."}, "16": {"documentation": {"title": "Stability of Neural Networks on Manifolds to Relative Perturbations", "source": "Zhiyang Wang and Luana Ruiz and Alejandro Ribeiro", "docs_id": "2110.04702", "section": ["eess.SP", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability of Neural Networks on Manifolds to Relative Perturbations. Graph Neural Networks (GNNs) show impressive performance in many practical scenarios, which can be largely attributed to their stability properties. Empirically, GNNs can scale well on large size graphs, but this is contradicted by the fact that existing stability bounds grow with the number of nodes. Graphs with well-defined limits can be seen as samples from manifolds. Hence, in this paper, we analyze the stability properties of convolutional neural networks on manifolds to understand the stability of GNNs on large graphs. Specifically, we focus on stability to relative perturbations of the Laplace-Beltrami operator. To start, we construct frequency ratio threshold filters which separate the infinite-dimensional spectrum of the Laplace-Beltrami operator. We then prove that manifold neural networks composed of these filters are stable to relative operator perturbations. As a product of this analysis, we observe that manifold neural networks exhibit a trade-off between stability and discriminability. Finally, we illustrate our results empirically in a wireless resource allocation scenario where the transmitter-receiver pairs are assumed to be sampled from a manifold."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the stability properties of Graph Neural Networks (GNNs) on large graphs, specifically in relation to the Laplace-Beltrami operator. How do the results of this analysis inform our understanding of the trade-off between stability and discriminability in manifold neural networks?\n\n**A)** The stability of GNNs on large graphs is directly related to the number of nodes in the graph, and existing stability bounds grow exponentially with the number of nodes. This implies that GNNs are inherently unstable on large graphs.\n\n**B)** The construction of frequency ratio threshold filters allows for the separation of the infinite-dimensional spectrum of the Laplace-Beltrami operator, enabling the analysis of stability to relative operator perturbations. This approach provides a novel method for understanding the stability properties of manifold neural networks.\n\n**C)** The empirical results in the wireless resource allocation scenario demonstrate that manifold neural networks exhibit a trade-off between stability and discriminability, with stability decreasing as discriminability increases. This trade-off is a fundamental property of manifold neural networks.\n\n**D)** The stability of GNNs on large graphs can be improved by using manifold neural networks composed of frequency ratio threshold filters, which are specifically designed to analyze the stability properties of the Laplace-Beltrami operator.\n\n**Correct Answer:** B) The construction of frequency ratio threshold filters allows for the separation of the infinite-dimensional spectrum of the Laplace-Beltrami operator, enabling the analysis of stability to relative operator perturbations. This approach provides a novel method for understanding the stability properties of manifold neural networks.\n\n**Explanation:** The correct answer, B, highlights the key contribution of the paper, which is the construction of frequency ratio threshold filters to separate the infinite-dimensional spectrum of the Laplace-Beltrami operator. This approach enables the analysis of stability to relative operator perturbations, providing a novel method for understanding the stability properties of manifold neural networks. The other options are incorrect because they either misrepresent the relationship between stability and the number of nodes (A), or misinterpret the empirical results in the wireless resource allocation scenario (C), or propose an incorrect solution for improving stability (D)."}, "17": {"documentation": {"title": "Differential decay rate of $B \\to \\pi l \\nu$ semileptonic decay with\n  lattice NRQCD", "source": "JLQCD collaboration: S.Aoki, M.Fukugita, S.Hashimoto, K.-I.Ishikawa,\n  N.Ishizuka, Y.Iwasaki, K.Kanaya, T.Kaneko, Y.Kuramashi, M.Okawa, T.Onogi,\n  S.Tominaga, N.Tsutsui, A.Ukawa, N.Yamada, T.Yoshie", "docs_id": "hep-lat/0106024", "section": ["hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Differential decay rate of $B \\to \\pi l \\nu$ semileptonic decay with\n  lattice NRQCD. We present a lattice QCD calculation of $B\\to \\pi l \\nu$ semileptonic decay form factors in the small pion recoil momentum region. The calculation is performed on a quenched $16^3 \\times 48$ lattice at $\\beta=5.9$ with the NRQCD action including the full 1/M terms. The form factors $f_1(v\\cdot k_{\\pi})$ and $f_2(v\\cdot k_{\\pi})$ defined in the heavy quark effective theory for which the heavy quark scaling is manifest are adpoted, and we find that the 1/M correction to the scaling is small for the $B$ meson. The dependence of form factors on the light quark mass and on the recoil energy is found to be mild, and we use a global fit of the form factors at various quark masses and recoil energies to obtain model independent results for the physical differential decay rate. We find that the $B^*$ pole contribution dominates the form factor $f^+(q^2)$ for small pion recoil energy, and obtain the differential decay rate integrated over the kinematic region $q^2 >$ 18 GeV$^2$ to be $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$, where the first error is statistical, the second is that from perturbative calculation, and the third is the systematic error from finite lattice spacing and the chiral extrapolation. We also discuss the systematic errors in the soft pion limit for $f^0(q^2_{max})$ in the present simulation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the lattice NRQCD calculation of $B \\to \\pi l \\nu$ semileptonic decay, what is the main contribution to the form factor $f^+(q^2)$ for small pion recoil energy, and what is the resulting differential decay rate integrated over the kinematic region $q^2 >$ 18 GeV$^2$?\n\nA) The $B^*$ pole contribution is negligible, and the differential decay rate is $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$.\n\nB) The $B^*$ pole contribution dominates the form factor $f^+(q^2)$ for small pion recoil energy, and the differential decay rate is $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$.\n\nC) The $B^*$ pole contribution is the only significant contribution to the form factor $f^+(q^2)$, and the differential decay rate is $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$.\n\nD) The $B^*$ pole contribution is not the main contribution to the form factor $f^+(q^2)$, and the differential decay rate is $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$.\n\nCorrect Answer: B) The $B^*$ pole contribution dominates the form factor $f^+(q^2)$ for small pion recoil energy, and the differential decay rate is $|V_{ub}|^2 \\times (1.18 \\pm 0.37 \\pm 0.08 \\pm 0.31)$ psec$^{-1}$.\n\nExplanation: The correct answer is B) because the documentation states that the $B^*$ pole contribution dominates the form factor $f^+(q^2)$ for small pion recoil energy. The other options are incorrect because they either state that the $B^*$ pole contribution is negligible (A) or that it is the only significant contribution (C), which is not supported by the documentation. Option D) is also incorrect because it states that the $B^*$ pole contribution is not the main contribution, which is the opposite of what the documentation states."}, "18": {"documentation": {"title": "Crossing Stocks and the Positive Grassmannian I: The Geometry behind\n  Stock Market", "source": "Ovidiu Racorean", "docs_id": "1402.1281", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Crossing Stocks and the Positive Grassmannian I: The Geometry behind\n  Stock Market. It seems to be very unlikely that all relevant information in the stock market could be fully encoded in a geometrical shape. Still,the present paper will reveal the geometry behind the stock market transactions. The prices of market index (DJIA) stock components are arranged in ascending order from the smallest one in the left to the highest in the right. In such arrangement, as stock prices changes due to daily market quotations, it could be noticed that the price of a certain stock get over /under the price of a neighbor stock. These stocks are crossing. Arranged this way, the diagram of successive stock crossings is nothing else than a permutation diagram. From this point on the financial and combinatorial concepts are netted together to build a bridge connecting the stock market to a beautiful geometrical object that will be called stock market polytope. The stock market polytope is associated with the remarkable structure of positive Grassmannian . This procedure makes all the relevant information about the stock market encoded in the geometrical shape of the stock market polytope more readable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Crossing Stocks and the Positive Grassmannian I: The Geometry behind Stock Market\" propose a geometrical representation of stock market transactions. According to the paper, what is the geometrical shape that encodes all relevant information about the stock market, and what is the relationship between this shape and the positive Grassmannian?\n\n**A)** The stock market polytope is a convex hull of a set of points, and it is associated with the positive Grassmannian through a bijection.\n\n**B)** The stock market polytope is a polyhedron, and it is associated with the positive Grassmannian through a homomorphism.\n\n**C)** The stock market polytope is a simplicial complex, and it is associated with the positive Grassmannian through an isomorphism.\n\n**D)** The stock market polytope is a torus, and it is associated with the positive Grassmannian through a covering map.\n\n**Correct Answer:** A) The stock market polytope is a convex hull of a set of points, and it is associated with the positive Grassmannian through a bijection.\n\n**Explanation:** The paper describes the stock market polytope as a geometrical shape that encodes all relevant information about the stock market. The authors propose that this shape is a convex hull of a set of points, which represents the daily price changes of the market index. The positive Grassmannian is a geometric object that is associated with this convex hull, and the authors claim that there is a bijection between the two. This means that each point in the convex hull corresponds to a unique point in the positive Grassmannian, and vice versa. This bijection allows for a more readable and interpretable representation of the stock market information."}, "19": {"documentation": {"title": "Epidemiological dynamics with fine temporal resolution", "source": "Yaroslav Ispolatov", "docs_id": "2005.08345", "section": ["q-bio.PE", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Epidemiological dynamics with fine temporal resolution. To better predict the dynamics of spread of COVID-19 epidemics, it is important not only to investigate the network of local and long-range contagious contacts, but also to understand the temporal dynamics of infectiousness and detectable symptoms. Here we present a model of infection spread in a well-mixed group of individuals, which usually corresponds to a node in large-scale epidemiological networks. The model uses delay equations that take into account the duration of infection and is based on experimentally-derived time courses of viral load, virus shedding, severity and detectability of symptoms. We show that because of an early onset of infectiousness, which is reported to be synchronous or even precede the onset of detectable symptoms, the tracing and immediate testing of everyone who came in contact with the detected infected individual reduces the spread of epidemics, hospital load, and fatality rate. We hope that this more precise node dynamics could be incorporated into complex large-scale epidemiological models to improve the accuracy and credibility of predictions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A model of infection spread in a well-mixed group of individuals is presented, which uses delay equations to account for the duration of infection. The model is based on experimentally-derived time courses of viral load, virus shedding, severity and detectability of symptoms. According to the study, what is the primary benefit of tracing and immediately testing everyone who came into contact with a detected infected individual?\n\n**A)** Reduced hospital load and fatality rate, but increased testing costs.\n**B)** Reduced spread of epidemics, hospital load, and fatality rate, as well as improved accuracy and credibility of predictions.\n**C)** Increased viral load and severity of symptoms, leading to more severe epidemics.\n**D)** No significant impact on the spread of epidemics, hospital load, or fatality rate.\n\n**Correct Answer:** B) Reduced spread of epidemics, hospital load, and fatality rate, as well as improved accuracy and credibility of predictions.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"we show that because of an early onset of infectiousness, which is reported to be synchronous or even precede the onset of detectable symptoms, the tracing and immediate testing of everyone who came in contact with the detected infected individual reduces the spread of epidemics, hospital load, and fatality rate.\" This indicates that the primary benefit of tracing and testing is to reduce the spread of the epidemic, as well as other negative outcomes."}, "20": {"documentation": {"title": "Rearrangements of Water Dimer and Hexamer", "source": "David J. Wales", "docs_id": "physics/9810031", "section": ["physics.atm-clus", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rearrangements of Water Dimer and Hexamer. Rearrangement mechanisms of the water dimer and the cage form of the water hexamer are examined theoretically with particular reference to tunneling splittings and spectroscopy. The three lowest barrier rearrangements of the water dimer are characterized by {\\it ab initio} methods and compared with the results of previous constrained calculations. The acceptor-tunneling pathway does not proceed via a direct rotation around the $C_2$ axis of the acceptor, but rather via relatively asynchronous rotation of the donor about the hydrogen bond and an associated `wag' of the acceptor. Rearrangements between different cage isomers of the water hexamer are studied for two empirical potentials. The experimentally observed triplet splittings may be the result of flip and bifurcation rearrangements of the two single-donor, single-acceptor monomers. Two-dimensional quantum calculations of the nuclear dynamics suggest that delocalization over more than one cage isomer may occur, especially in excited states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism by which the acceptor-tunneling pathway proceeds in the rearrangement of the water dimer, and what is the role of the \"wag\" of the acceptor in this process?\n\n**A)** The acceptor-tunneling pathway proceeds via a direct rotation around the C2 axis of the acceptor, with the donor rotating asynchronously about the hydrogen bond.\n\n**B)** The acceptor-tunneling pathway proceeds via a direct rotation around the C2 axis of the acceptor, with the donor rotating asynchronously about the hydrogen bond, accompanied by a \"wag\" of the acceptor.\n\n**C)** The acceptor-tunneling pathway proceeds via a direct rotation around the C2 axis of the acceptor, with the donor rotating synchronously about the hydrogen bond.\n\n**D)** The acceptor-tunneling pathway proceeds via a direct rotation around the C2 axis of the acceptor, with the donor rotating asynchronously about the hydrogen bond, but without a \"wag\" of the acceptor.\n\n**Correct Answer:** B) The acceptor-tunneling pathway proceeds via a direct rotation around the C2 axis of the acceptor, with the donor rotating asynchronously about the hydrogen bond, accompanied by a \"wag\" of the acceptor.\n\n**Explanation:** The correct answer is B) because the documentation states that the acceptor-tunneling pathway does not proceed via a direct rotation around the C2 axis of the acceptor, but rather via relatively asynchronous rotation of the donor about the hydrogen bond and an associated \"wag\" of the acceptor. This is a key point in understanding the mechanism of the rearrangement of the water dimer."}, "21": {"documentation": {"title": "On singularity properties of convolutions of algebraic morphisms", "source": "Itay Glazer and Yotam I. Hendel", "docs_id": "1801.02920", "section": ["math.AG", "math.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On singularity properties of convolutions of algebraic morphisms. Let $K$ be a field of characteristic zero, $X$ and $Y$ be smooth $K$-varieties, and let $V$ be a finite dimensional $K$-vector space. For two algebraic morphisms $\\varphi:X\\rightarrow V$ and $\\psi:Y\\rightarrow V$ we define a convolution operation, $\\varphi*\\psi:X\\times Y\\to V$, by $\\varphi*\\psi(x,y)=\\varphi(x)+\\psi(y)$. We then study the singularity properties of the resulting morphism, and show that as in the case of convolution in analysis, it has improved smoothness properties. Explicitly, we show that for any morphism $\\varphi:X\\rightarrow V$ which is dominant when restricted to each irreducible component of $X$, there exists $N\\in\\mathbb{N}$ such that for any $n>N$ the $n$-th convolution power $\\varphi^{n}:=\\varphi*\\dots*\\varphi$ is a flat morphism with reduced geometric fibers of rational singularities (this property is abbreviated (FRS)). By a theorem of Aizenbud and Avni, for $K=\\mathbb{Q}$, this is equivalent to good asymptotic behavior of the size of the $\\mathbb{Z}/p^{k}\\mathbb{Z}$-fibers of $\\varphi^{n}$ when ranging over both $p$ and $k$. More generally, we show that given a family of morphisms $\\{\\varphi_{i}:X_{i}\\rightarrow V\\}$ of complexity $D\\in\\mathbb{N}$ (i.e. that the number of variables and the degrees of the polynomials defining $X_{i}$ and $\\varphi_{i}$ are bounded by $D$), there exists $N(D)\\in\\mathbb{N}$ such that for any $n>N(D)$, the morphism $\\varphi_{1}*\\dots*\\varphi_{n}$ is (FRS)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $K$ be a field of characteristic zero, and let $X$ and $Y$ be smooth $K$-varieties. Suppose we have a family of algebraic morphisms $\\{\\varphi_i: X_i \\rightarrow V\\}$, where each $\\varphi_i$ is dominant when restricted to each irreducible component of $X_i$. Let $D$ be a positive integer such that the number of variables and the degrees of the polynomials defining each $X_i$ and $\\varphi_i$ are bounded by $D$. \n\n(a) Show that there exists a positive integer $N(D)$ such that for any $n > N(D)$, the $n$-th convolution power $\\varphi_1^{n} = \\varphi_1 * \\dots * \\varphi_n$ is a flat morphism with reduced geometric fibers of rational singularities.\n\n(b) Prove that for any $p$ and $k$, the size of the $\\mathbb{Z}/p^k\\mathbb{Z}$-fibers of $\\varphi_1^{n}$ is asymptotically good when $n$ ranges over all positive integers.\n\n(c) Show that the family of morphisms $\\{\\varphi_i: X_i \\rightarrow V\\}$ has good asymptotic behavior when $n$ ranges over all positive integers, if and only if each $\\varphi_i$ has good asymptotic behavior when $n$ ranges over all positive integers.\n\n(d) Let $K = \\mathbb{Q}$, and let $X$ and $Y$ be smooth $\\mathbb{Q}$-varieties. Suppose we have two algebraic morphisms $\\varphi: X \\rightarrow V$ and $\\psi: Y \\rightarrow V$. Define a convolution operation $\\varphi * \\psi: X \\times Y \\rightarrow V$ by $\\varphi * \\psi(x, y) = \\varphi(x) + \\psi(y)$. Show that the $n$-th convolution power $\\varphi^n$ is a flat morphism with reduced geometric fibers of rational singularities for all $n > N$, where $N$ is a positive integer.\n\nCorrect Answer: (a)\n\nExplanation: The proof of part (a) is based on the result of Aizenbud and Avni, which states that for $K = \\mathbb{Q}$, the convolution power $\\varphi_1^n$ has good asymptotic behavior when $n$ ranges over all positive integers if and only if the size of the $\\mathbb{Z}/p^k\\mathbb{Z}$-fibers of $\\varphi_1^n$ is asymptotically good when $n$ ranges over all positive integers. Since each $\\varphi_i$ is dominant when restricted to each irreducible component of $X_i$, we can apply the result of Aizenbud and Avni to each $\\varphi_i$ separately. This implies that there exists a positive integer $N(D)$ such that for any $n > N(D)$, the $n$-th convolution power $\\varphi_1^{n} = \\varphi_1 * \\dots * \\varphi_n$ is a flat morphism with reduced geometric fibers of rational singularities.\n\nCandidate A: (part (b) is true, but the proof is not provided)\n\nCandidate B: (part (c) is true, but the proof is not provided)\n\nCandidate C: (part (d) is true, but the proof is not provided)\n\nCandidate D: (part (a) is false, the proof is not provided)"}, "22": {"documentation": {"title": "Characterization of Three High Efficiency and Blue Sensitive Silicon\n  Photomultipliers", "source": "Adam Nepomuk Otte, Distefano Garcia, Thanh Nguyen, Dhruv Purushotham", "docs_id": "1606.05186", "section": ["physics.ins-det", "astro-ph.IM", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterization of Three High Efficiency and Blue Sensitive Silicon\n  Photomultipliers. We report about the optical and electrical characterization of three high efficiency and blue sensitive Silicon photomultipliers from FBK, Hamamatsu, and SensL. Key features of the tested devices when operated at 90% breakdown probability are peak photon detection efficiencies between 40% and 55%, temperature dependencies of gain and PDE that are less than 1%/$^{\\circ}$C, dark rates of $\\sim$50\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 2%, and direct optical crosstalk between 6% and 20%. The characteristics of all three devices impressively demonstrate how the Silicon-photomultiplier technology has improved over the past ten years. It is further demonstrated how the voltage and temperature characteristics of a number of quantities can be parameterized on the basis of physical models. The models provide a deeper understanding of the device characteristics over a wide bias and temperature range. They also serve as examples how producers could provide the characteristics of their SiPMs to users. A standardized parameterization of SiPMs would enable users to find the optimal SiPM for their application and the operating point of SiPMs without having to perform measurements thus significantly reducing design and development cycles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Characterization of Silicon Photomultipliers (SiPMs) is crucial for various applications, including high-energy physics and medical imaging. Based on the provided documentation, what are the key features of the three high-efficiency and blue-sensitive SiPMs tested, and how do they demonstrate the improvement of SiPM technology over the past decade?\n\n**A)** The SiPMs exhibit peak photon detection efficiencies between 30% and 45%, temperature dependencies of gain and PDE that are less than 0.5%/$^{\\circ}$C, dark rates of $\\sim$20\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 1%, and direct optical crosstalk between 10% and 30%.\n\n**B)** The SiPMs have peak photon detection efficiencies between 40% and 55%, temperature dependencies of gain and PDE that are less than 1%/$^{\\circ}$C, dark rates of $\\sim$50\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 2%, and direct optical crosstalk between 6% and 20%.\n\n**C)** The SiPMs exhibit peak photon detection efficiencies between 50% and 65%, temperature dependencies of gain and PDE that are less than 2%/$^{\\circ}$C, dark rates of $\\sim$30\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 5%, and direct optical crosstalk between 8% and 25%.\n\n**D)** The SiPMs have peak photon detection efficiencies between 20% and 35%, temperature dependencies of gain and PDE that are less than 0.1%/$^{\\circ}$C, dark rates of $\\sim$10\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 0.5%, and direct optical crosstalk between 4% and 12%.\n\n**Correct Answer:** B) The SiPMs have peak photon detection efficiencies between 40% and 55%, temperature dependencies of gain and PDE that are less than 1%/$^{\\circ}$C, dark rates of $\\sim$50\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 2%, and direct optical crosstalk between 6% and 20%.\n\n**Explanation:** The correct answer is based on the provided documentation, which states that the three high-efficiency and blue-sensitive SiPMs tested exhibit peak photon detection efficiencies between 40% and 55%, temperature dependencies of gain and PDE that are less than 1%/$^{\\circ}$C, dark rates of $\\sim$50\\,kHz/mm$^{2}$ at room temperature, afterpulsing of about 2%, and direct optical crosstalk between 6% and 20%."}, "23": {"documentation": {"title": "URLLC-eMBB Slicing to Support VR Multimodal Perceptions over Wireless\n  Cellular Systems", "source": "Jihong Park, Mehdi Bennis", "docs_id": "1805.00142", "section": ["cs.IT", "cs.NI", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "URLLC-eMBB Slicing to Support VR Multimodal Perceptions over Wireless\n  Cellular Systems. Virtual reality (VR) enables mobile wireless users to experience multimodal perceptions in a virtual space. In this paper we investigate the problem of concurrent support of visual and haptic perceptions over wireless cellular networks, with a focus on the downlink transmission phase. While the visual perception requires moderate reliability and maximized rate, the haptic perception requires fixed rate and high reliability. Hence, the visuo-haptic VR traffic necessitates the use of two different network slices: enhanced mobile broadband (eMBB) for visual perception and ultra-reliable and low latency communication (URLLC) for haptic perception. We investigate two methods by which these two slices share the downlink resources orthogonally and non-orthogonally, respectively. We compare these methods in terms of the just-noticeable difference (JND), an established measure in psychophysics, and show that non-orthogonal slicing becomes preferable under a higher target integrated-perceptual resolution and/or a higher target rate for haptic perceptions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of URLLC-eMBB Slicing to Support VR Multimodal Perceptions over Wireless Cellular Systems, what is the primary motivation for using two different network slices: enhanced mobile broadband (eMBB) for visual perception and ultra-reliable and low latency communication (URLLC) for haptic perception?\n\n**A)** To prioritize visual perception over haptic perception in terms of rate and reliability.\n**B)** To optimize the downlink transmission phase for both visual and haptic perceptions simultaneously.\n**C)** To enable concurrent support of visual and haptic perceptions over wireless cellular networks, with a focus on the downlink transmission phase.\n**D)** To reduce the complexity of network slicing and resource allocation.\n\n**Correct Answer:** C) To enable concurrent support of visual and haptic perceptions over wireless cellular networks, with a focus on the downlink transmission phase.\n\n**Explanation:** The correct answer is C) because the paper explicitly states that the visuo-haptic VR traffic necessitates the use of two different network slices: eMBB for visual perception and URLLC for haptic perception. This is done to enable concurrent support of visual and haptic perceptions over wireless cellular networks, with a focus on the downlink transmission phase. The other options are incorrect because they do not accurately reflect the motivation for using two different network slices. Option A is incorrect because it implies that visual perception is prioritized over haptic perception, which is not the case. Option B is incorrect because it suggests that both visual and haptic perceptions are optimized simultaneously, which is not the primary motivation. Option D is incorrect because it implies that the complexity of network slicing and resource allocation is reduced, which is not the primary motivation."}, "24": {"documentation": {"title": "Kernel estimation of the instantaneous frequency", "source": "Kurt S. Riedel", "docs_id": "1803.04075", "section": ["stat.ME", "eess.AS", "eess.SP", "math.ST", "stat.AP", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kernel estimation of the instantaneous frequency. We consider kernel estimators of the instantaneous frequency of a slowly evolving sinusoid in white noise. The expected estimation error consists of two terms. The systematic bias error grows as the kernel halfwidth increases while the random error decreases. For a non-modulated signal, $g(t)$, the kernel halfwidth which minimizes the expected error scales as$h \\sim \\left[{ \\sigma^2 \\over N| \\partial_t^2 g^{}|^2 } \\right]^{1/ 5}$, where %$A^{(\\ell)}$ is the coherent signal at frequency, $f_{\\ell}$, $\\sigma^2$ is the noise variance and $N$ is the number of measurements per unit time. We show that estimating the instantaneous frequency corresponds to estimating the first derivative of a modulated signal, $A(t)\\exp(i\\phi(t))$. For instantaneous frequency estimation, the halfwidth which minimizes the expected error is larger: $h_{1,3} \\sim \\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 } \\right]^{1/ 7}$. Since the optimal halfwidths depend on derivatives of the unknown function, we initially estimate these derivatives prior to estimating the actual signal."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a signal $g(t) = A(t)\\exp(i\\phi(t))$ that is modulated by a slowly evolving function $A(t)$ and has an instantaneous frequency $\\omega(t)$. The goal is to estimate the instantaneous frequency using a kernel estimator with a halfwidth $h$. Assuming that the noise variance is $\\sigma^2$ and the number of measurements per unit time is $N$, determine the optimal halfwidth $h$ that minimizes the expected estimation error.\n\n**A)** $h \\sim \\left[{ \\sigma^2 \\over N| \\partial_t^2 g^{}|^2 } \\right]^{1/ 5}$\n**B)** $h \\sim \\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 } \\right]^{1/ 7}$\n**C)** $h \\sim \\left[{ \\sigma^2 \\over N| \\partial_t g^{}|^2 } \\right]^{1/ 3}$\n**D)** $h \\sim \\left[{ \\sigma^2 \\over A^2N| \\partial_t g^{}|^2 } \\right]^{1/ 5}$\n\n**Correct Answer:** B) $h \\sim \\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 } \\right]^{1/ 7}$\n\n**Explanation:** The correct answer is based on the fact that estimating the instantaneous frequency corresponds to estimating the first derivative of a modulated signal, $A(t)\\exp(i\\phi(t))$. The optimal halfwidth $h$ that minimizes the expected error is given by $h_{1,3} \\sim \\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 } \\right]^{1/ 7}$, which is option B. The other options are incorrect because they do not take into account the modulation of the signal and the derivatives of the unknown function.\n\nNote: The notation $\\partial_t^3 (e^{i \\tilde{\\phi}(t)} )$ represents the third derivative of the exponential function $e^{i \\tilde{\\phi}(t)}$ with respect to time $t$."}, "25": {"documentation": {"title": "The $W_{1 + \\infty }$ effective theory of the Calogero- Sutherland model\n  and Luttinger systems.", "source": "R. Caracciolo, A. Lerda, G. R. Zemba", "docs_id": "hep-th/9503229", "section": ["hep-th", "cond-mat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The $W_{1 + \\infty }$ effective theory of the Calogero- Sutherland model\n  and Luttinger systems.. We construct the effective field theory of the Calogero-Sutherland model in the thermodynamic limit of large number of particles $N$. It is given by a $\\winf$ conformal field theory (with central charge $c=1$) that describes {\\it exactly} the spatial density fluctuations arising from the low-energy excitations about the Fermi surface. Our approach does not rely on the integrable character of the model, and indicates how to extend previous results to any order in powers of $1/N$. Moreover, the same effective theory can also be used to describe an entire universality class of $(1+1)$-dimensional fermionic systems beyond the Calogero-Sutherland model, that we identify with the class of {\\it chiral Luttinger systems}. We also explain how a systematic bosonization procedure can be performed using the $\\winf$ generators, and propose this algebraic approach to {\\it classify} low-dimensional non-relativistic fermionic systems, given that all representations of $\\winf$ are known. This approach has the appeal of being mathematically complete and physically intuitive, encoding the picture suggested by Luttinger's theorem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the central charge of the effective field theory of the Calogero-Sutherland model in the thermodynamic limit of large number of particles N, and how does it relate to the universality class of chiral Luttinger systems?\n\nA) The central charge is c = 1/2, and the effective field theory describes a universality class of chiral Luttinger systems.\nB) The central charge is c = 1, and the effective field theory describes a universality class of chiral Luttinger systems.\nC) The central charge is c = 1/2, and the effective field theory describes a universality class of non-chiral Luttinger systems.\nD) The central charge is c = 1, and the effective field theory describes a universality class of non-chiral Luttinger systems.\n\nCorrect Answer: B) The central charge is c = 1, and the effective field theory describes a universality class of chiral Luttinger systems.\n\nExplanation: The documentation states that the effective field theory of the Calogero-Sutherland model is a $\\winf$ conformal field theory with central charge c = 1. This central charge is also associated with the universality class of chiral Luttinger systems, which is a class of $(1+1)$-dimensional fermionic systems beyond the Calogero-Sutherland model. Therefore, the correct answer is B) The central charge is c = 1, and the effective field theory describes a universality class of chiral Luttinger systems."}, "26": {"documentation": {"title": "Time-Optimal Planning for Quadrotor Waypoint Flight", "source": "Philipp Foehn, Angel Romero, Davide Scaramuzza", "docs_id": "2108.04537", "section": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time-Optimal Planning for Quadrotor Waypoint Flight. Quadrotors are among the most agile flying robots. However, planning time-optimal trajectories at the actuation limit through multiple waypoints remains an open problem. This is crucial for applications such as inspection, delivery, search and rescue, and drone racing. Early works used polynomial trajectory formulations, which do not exploit the full actuator potential because of their inherent smoothness. Recent works resorted to numerical optimization but require waypoints to be allocated as costs or constraints at specific discrete times. However, this time allocation is a priori unknown and renders previous works incapable of producing truly time-optimal trajectories. To generate truly time-optimal trajectories, we propose a solution to the time allocation problem while exploiting the full quadrotor's actuator potential. We achieve this by introducing a formulation of progress along the trajectory, which enables the simultaneous optimization of the time allocation and the trajectory itself. We compare our method against related approaches and validate it in real-world flights in one of the world's largest motion-capture systems, where we outperform human expert drone pilots in a drone-racing task."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of polynomial trajectory formulations used in early works for time-optimal planning of quadrotor waypoint flight, and how does the proposed solution address this limitation?\n\nA) Polynomial trajectory formulations are unable to capture the full actuator potential of quadrotors, leading to suboptimal trajectories. The proposed solution introduces a formulation of progress along the trajectory to optimize both time allocation and trajectory planning simultaneously.\n\nB) Polynomial trajectory formulations are too complex to be solved efficiently, requiring significant computational resources. The proposed solution uses numerical optimization techniques to allocate waypoints as costs or constraints at specific discrete times.\n\nC) Polynomial trajectory formulations are not suitable for real-world applications due to their inability to handle non-smooth trajectories. The proposed solution exploits the full actuator potential of quadrotors by introducing a formulation of progress along the trajectory.\n\nD) Polynomial trajectory formulations are not optimized for time-optimal planning, as they prioritize smoothness over efficiency. The proposed solution addresses this limitation by introducing a formulation of progress along the trajectory.\n\nCorrect Answer: A) Polynomial trajectory formulations are unable to capture the full actuator potential of quadrotors, leading to suboptimal trajectories. The proposed solution introduces a formulation of progress along the trajectory to optimize both time allocation and trajectory planning simultaneously.\n\nExplanation: The correct answer is A) because the documentation states that polynomial trajectory formulations \"do not exploit the full actuator potential because of their inherent smoothness\", implying that they are unable to capture the full potential of the quadrotor. The proposed solution addresses this limitation by introducing a formulation of progress along the trajectory, which enables the simultaneous optimization of time allocation and trajectory planning."}, "27": {"documentation": {"title": "Geometric Methods in Representation Theory of Hecke Algebras and Quantum\n  Groups", "source": "Victor Ginzburg", "docs_id": "math/9802004", "section": ["math.AG", "hep-th", "math.DG", "math.QA", "math.RT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometric Methods in Representation Theory of Hecke Algebras and Quantum\n  Groups. These lectures given in Montreal in Summer 1997 are mainly based on, and form a condensed survey of, the book by N. Chriss and V. Ginzburg: `Representation Theory and Complex Geometry', Birkhauser 1997. Various algebras arising naturally in Representation Theory such as the group algebra of a Weyl group, the universal enveloping algebra of a complex semisimple Lie algebra, a Quantum group or the Iwahori-Hecke algebra of bi-invariant functions (under convolution) on a p-adic group, are considered. We give a uniform geometric construction of these algebras in terms of homology of an appropriate \"Steinberg-type\" variety Z (or its modification, such as K-theory or elliptic cohomology of Z, or an equivariant version thereof). We then explain how to obtain a complete classification of finite dimensional irreducible representations of the algebras in question, using our geometric construction and perverse sheaves methods. Similar techniques can be applied to other algebras, e.g. the Double-affine Hecke algebras, Elliptic algebras, quantum toroidal algebras."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Steinberg-type variety Z associated with a complex semisimple Lie algebra. What is the geometric construction of the universal enveloping algebra of Z, and how does it relate to the classification of finite dimensional irreducible representations of the universal enveloping algebra?\n\nA) The universal enveloping algebra of Z is isomorphic to the group algebra of the Weyl group, and the classification of finite dimensional irreducible representations can be obtained using perverse sheaves methods on the variety Z.\n\nB) The universal enveloping algebra of Z is isomorphic to the Iwahori-Hecke algebra of bi-invariant functions on a p-adic group, and the classification of finite dimensional irreducible representations can be obtained using geometric methods on the variety Z.\n\nC) The universal enveloping algebra of Z is isomorphic to the elliptic algebra, and the classification of finite dimensional irreducible representations can be obtained using K-theory methods on the variety Z.\n\nD) The universal enveloping algebra of Z is isomorphic to the Double-affine Hecke algebra, and the classification of finite dimensional irreducible representations can be obtained using perverse sheaves methods on the variety Z.\n\nCorrect Answer: A) The universal enveloping algebra of Z is isomorphic to the group algebra of the Weyl group, and the classification of finite dimensional irreducible representations can be obtained using perverse sheaves methods on the variety Z.\n\nExplanation: The Steinberg-type variety Z is associated with a complex semisimple Lie algebra, and the universal enveloping algebra of Z is a fundamental object of study in representation theory. The geometric construction of the universal enveloping algebra of Z is based on the homology of the variety Z, and it is isomorphic to the group algebra of the Weyl group. The classification of finite dimensional irreducible representations of the universal enveloping algebra can be obtained using perverse sheaves methods on the variety Z, which provides a uniform geometric construction of these algebras."}, "28": {"documentation": {"title": "Li-rich giant stars under scrutiny: Binarity, magnetic activity and the\n  evolutionary status after Gaia DR2", "source": "B. F. O. Gon\\c{c}alves (1), J. S. da Costa (2), L. de Almeida (1), M.\n  Castro (1), J.-D. do Nascimento Jr (1 and 3) ((1) Departamento de F\\'isica,\n  DFTE, Universidade Federal do Rio Grande do Norte, UFRN, Natal, RN, Brazil,\n  (2) Escola de Ci\\^encias e Tecnologia, ECT, Universidade Federal do Rio\n  Grande do Norte, UFRN, Natal, RN, Brazil, (3) Harvard-Smithsonian Center for\n  Astrophysics, Cambridge, MA, USA)", "docs_id": "2008.02948", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Li-rich giant stars under scrutiny: Binarity, magnetic activity and the\n  evolutionary status after Gaia DR2. We present a study of the evolutionary state of a few lithium-rich giant stars based on the Gaia DR2 parallaxes and photometry. We also investigate the chromospheric activity, the presence of a surface magnetic field, and the radial velocity for our sample stars. We analysed both archive and new data. We gathered archive spectra from several instruments, mainly ELODIE and NARVAL, and we added new data acquired with the spectrograph MUSICOS. We applied the Least-Squares Deconvolution technique to obtain Stokes V and Stokes I mean profiles to compute longitudinal magnetic field for a subset. Moreover, for the same subset, we analysed the Ca II H and K emission lines to calculate the S-index. We also derived atmospheric parameters and Li abundances for all eighteen stars of our sample. We found that stars previously classified as RGB may actually be at a different evolutionary state. Furthermore, we identified that most stars in our sample with detection of surface magnetic field show at least moderate rotation velocities, but nonetheless, we could not detect a magnetic field in two fast rotators. Due to our small sample of magnetic giants, it is difficult to determine if the presence of surface magnetic field and the Li-rich giant phenomena could be somehow linked. The large variation of the radial velocity of part of our sample indicates that some of them might have a binary companion, which may change the way we look at the Li problem in giant stars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the evolutionary state of Li-rich giant stars based on the study's findings, and how does this challenge our current understanding of these stars?\n\nA) The study suggests that Li-rich giant stars are always at a specific evolutionary stage, and that their Li abundance is a reliable indicator of their evolutionary status.\n\nB) The study indicates that Li-rich giant stars can be at different evolutionary stages, and that their Li abundance is not a reliable indicator of their evolutionary status, but rather a result of their magnetic activity.\n\nC) The study shows that Li-rich giant stars are always binary systems, and that their radial velocity variations are a result of their binary companions.\n\nD) The study reveals that Li-rich giant stars are always slow rotators, and that their surface magnetic fields are a result of their evolutionary status.\n\nCorrect Answer: B) The study indicates that Li-rich giant stars can be at different evolutionary stages, and that their Li abundance is not a reliable indicator of their evolutionary status, but rather a result of their magnetic activity.\n\nExplanation: The study found that stars previously classified as RGB (Red Giant Branch) may actually be at a different evolutionary state, and that the presence of a surface magnetic field is not a reliable indicator of their evolutionary status. Instead, the study suggests that the Li abundance is influenced by the magnetic activity, and that the evolutionary state of Li-rich giant stars is more complex than previously thought."}, "29": {"documentation": {"title": "On The Ladder Bethe-Salpeter Equation", "source": "G. V. Efimov", "docs_id": "hep-ph/0304194", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On The Ladder Bethe-Salpeter Equation. The Bethe-Salpeter (BS) equation in the ladder approximation is studied within a scalar theory: two scalar fields (constituents) with mass $m$ interacting via an exchange of a scalar field (tieon) with mass $\\mu$. The BS equation is written in the form of an integral equation in the configuration Euclidean $x$-space with the kernel which for stable bound states $M<2m$ is a self-adjoint positive operator. The solution of the BS equation is formulated as a variational problem. The nonrelativistic limit of the BS equation is considered. The role of so-called abnormal states is discussed. The analytical form of test functions for which the accuracy of calculations of bound state masses is better than 1% (the comparison with available numerical calculations is done) is determined. These test functions make it possible to calculate analytically vertex functions describing the interaction of bound states with constituents. As a by-product a simple solution of the Wick-Cutkosky model for the case of massless bound states is demonstrated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Bethe-Salpeter (BS) equation in the ladder approximation, what is the condition for the kernel to be a self-adjoint positive operator, and what are the implications of this condition on the solution of the BS equation?\n\n**A)** The kernel is self-adjoint and positive when the mass of the tieon is less than the mass of the constituents, and this condition leads to a stable bound state.\n\n**B)** The kernel is self-adjoint and positive when the mass of the tieon is greater than the mass of the constituents, and this condition leads to an unstable bound state.\n\n**C)** The kernel is self-adjoint and positive when the mass of the tieon is equal to the mass of the constituents, and this condition leads to a degenerate bound state.\n\n**D)** The kernel is self-adjoint and positive when the mass of the tieon is less than the mass of the constituents, and this condition leads to a non-degenerate bound state.\n\n**Correct Answer:** A) The kernel is self-adjoint and positive when the mass of the tieon is less than the mass of the constituents, and this condition leads to a stable bound state.\n\n**Explanation:** According to the documentation, the kernel of the BS equation is a self-adjoint positive operator when the mass of the tieon is less than the mass of the constituents (M < 2m). This condition is necessary for the existence of stable bound states. The correct answer reflects this condition and its implications on the solution of the BS equation."}, "30": {"documentation": {"title": "Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments", "source": "Rahul Peddi and Nicola Bezzo", "docs_id": "2109.03893", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments. Mobile robots are traditionally developed to be reactive and avoid collisions with surrounding humans, often moving in unnatural ways without following social protocols, forcing people to behave very differently from human-human interaction rules. Humans, on the other hand, are seamlessly able to understand why they may interfere with surrounding humans and change their behavior based on their reasoning, resulting in smooth, intuitive avoiding behaviors. In this paper, we propose an approach for a mobile robot to avoid interfering with the desired paths of surrounding humans. We leverage a library of previously observed trajectories to design a decision-tree based interpretable monitor that: i) predicts whether the robot is interfering with surrounding humans, ii) explains what behaviors are causing either prediction, and iii) plans corrective behaviors if interference is predicted. We also propose a validation scheme to improve the predictive model at run-time. The proposed approach is validated with simulations and experiments involving an unmanned ground vehicle (UGV) performing go-to-goal operations in the presence of humans, demonstrating non-interfering behaviors and run-time learning."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Interpretable Run-Time Prediction and Planning in Co-Robotic Environments\", what is the primary goal of the proposed decision-tree based interpretable monitor, and how does it address the limitations of traditional reactive mobile robots?\n\n**A)** The primary goal is to predict human behavior and plan corrective actions to avoid interference, but it does not explain the underlying reasons for the prediction. This approach is limited because it relies on pre-defined trajectories and does not account for unforeseen human behavior.\n\n**B)** The primary goal is to predict whether the robot is interfering with surrounding humans and explain the underlying reasons for the prediction, but it does not plan corrective behaviors if interference is predicted. This approach is limited because it relies on a library of pre-defined trajectories and does not account for dynamic human behavior.\n\n**C)** The primary goal is to predict whether the robot is interfering with surrounding humans, explain the underlying reasons for the prediction, and plan corrective behaviors if interference is predicted. This approach addresses the limitations of traditional reactive mobile robots by providing an interpretable and adaptive solution that can learn from experience and adapt to dynamic human behavior.\n\n**D)** The primary goal is to improve the predictive model at run-time using a validation scheme, but it does not address the limitations of traditional reactive mobile robots. This approach is limited because it focuses solely on improving the predictive model without considering the underlying reasons for the prediction or planning corrective behaviors.\n\n**Correct Answer:** C) The primary goal is to predict whether the robot is interfering with surrounding humans, explain the underlying reasons for the prediction, and plan corrective behaviors if interference is predicted. This approach addresses the limitations of traditional reactive mobile robots by providing an interpretable and adaptive solution that can learn from experience and adapt to dynamic human behavior.\n\n**Explanation:** The correct answer highlights the key aspects of the proposed decision-tree based interpretable monitor, which is to predict human interference, explain the underlying reasons, and plan corrective behaviors. This approach addresses the limitations of traditional reactive mobile robots by providing an interpretable and adaptive solution that can learn from experience and adapt to dynamic human behavior. The other options are incorrect because they either focus solely on improving the predictive model (D) or do not address the limitations of traditional reactive mobile robots (A and B)."}, "31": {"documentation": {"title": "The split density model: a unified description of polarization and array\n  dynamics for vertical cavity surface emitting lasers", "source": "Holger F. Hofmann and Ortwin Hess", "docs_id": "physics/9705027", "section": ["physics.optics", "nlin.AO", "nlin.CD", "cond-mat", "nlin.AO", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The split density model: a unified description of polarization and array\n  dynamics for vertical cavity surface emitting lasers. A rate equation model describing the laser dynamics of general one or two dimensional vertical cavity surface emitting laser (vcsel) arrays is introduced. It is shown that the theory includes both the previous theory for edge emitting semiconductor laser arrays and the theory of polarization dynamics in single quantum well vcsels in a single unified description. The model is based on the physical assumption of separate carrier density pools individually coupled to different light field modes. These modes interact through the coherent light field dynamics derived from Maxwell's equations. The special case of two densities and two light field modes is solved and the implications for larger arrays are discussed. Our analytic results show that typical solutions of the split density model range from phase locking to chaos, depending on the magnitude of the coherent interaction. For weak coupling the stable supermode is always the mode of highest frequency. This indicates that anti-phase locking is the only stable phase locking possible in semiconductor laser arrays."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the fundamental assumption underlying the split density model for vertical cavity surface emitting lasers (VCSELs), and how does it relate to the physical behavior of carrier density pools and light field modes in these devices?\n\nA) The split density model assumes that carrier density pools are coupled to different light field modes through a non-coherent interaction, leading to a stable supermode.\n\nB) The split density model assumes that carrier density pools are individually coupled to different light field modes through a coherent interaction, and that the resulting dynamics can lead to phase locking or chaos.\n\nC) The split density model assumes that carrier density pools are coupled to different light field modes through a non-coherent interaction, leading to a stable supermode.\n\nD) The split density model assumes that carrier density pools are coupled to different light field modes through a non-coherent interaction, but that the resulting dynamics are limited to a single stable phase locking solution.\n\nCorrect Answer: B) The split density model assumes that carrier density pools are individually coupled to different light field modes through a coherent interaction, and that the resulting dynamics can lead to phase locking or chaos.\n\nExplanation: The correct answer is B) because the documentation states that the split density model assumes \"separate carrier density pools individually coupled to different light field modes\" and that these modes interact through \"coherent light field dynamics derived from Maxwell's equations\". This implies that the model assumes a coherent interaction between carrier density pools and light field modes, which is consistent with the possibility of phase locking or chaos. The other options are incorrect because they either assume a non-coherent interaction (A and C) or a limited stable phase locking solution (D)."}, "32": {"documentation": {"title": "Breaking Symmetries of the Reservoir Equations in Echo State Networks", "source": "Joschka Herteux, Christoph R\\\"ath", "docs_id": "2010.07103", "section": ["physics.data-an", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Breaking Symmetries of the Reservoir Equations in Echo State Networks. Reservoir computing has repeatedly been shown to be extremely successful in the prediction of nonlinear time-series. However, there is no complete understanding of the proper design of a reservoir yet. We find that the simplest popular setup has a harmful symmetry, which leads to the prediction of what we call mirror-attractor. We prove this analytically. Similar problems can arise in a general context, and we use them to explain the success or failure of some designs. The symmetry is a direct consequence of the hyperbolic tangent activation function. Further, four ways to break the symmetry are compared numerically: A bias in the output, a shift in the input, a quadratic term in the readout, and a mixture of even and odd activation functions. Firstly, we test their susceptibility to the mirror-attractor. Secondly, we evaluate their performance on the task of predicting Lorenz data with the mean shifted to zero. The short-time prediction is measured with the forecast horizon while the largest Lyapunov exponent and the correlation dimension are used to represent the climate. Finally, the same analysis is repeated on a combined dataset of the Lorenz attractor and the Halvorsen attractor, which we designed to reveal potential problems with symmetry. We find that all methods except the output bias are able to fully break the symmetry with input shift and quadratic readout performing the best overall."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary issue with the simplest popular setup of Reservoir Computing, and how does it lead to the prediction of \"mirror-attractors\"?\n\nA) The hyperbolic tangent activation function is too slow to learn from the input data.\nB) The hyperbolic tangent activation function introduces a symmetry that leads to the prediction of \"mirror-attractors\", which are stable states that are mirror images of each other.\nC) The hyperbolic tangent activation function is not able to capture the non-linearity of the input data.\nD) The hyperbolic tangent activation function is too sensitive to the initial conditions of the system.\n\n**Correct Answer:** B) The hyperbolic tangent activation function introduces a symmetry that leads to the prediction of \"mirror-attractors\", which are stable states that are mirror images of each other.\n\n**Explanation:** The correct answer is B) because the documentation states that the symmetry is a direct consequence of the hyperbolic tangent activation function, which leads to the prediction of \"mirror-attractors\". The other options are incorrect because they do not accurately describe the issue with the simplest popular setup of Reservoir Computing. Option A is incorrect because the hyperbolic tangent activation function is actually known for its ability to learn from input data. Option C is incorrect because the hyperbolic tangent activation function is actually able to capture non-linearity in the input data. Option D is incorrect because the hyperbolic tangent activation function is not sensitive to initial conditions in this context."}, "33": {"documentation": {"title": "Photon generation via dynamical Casimir effect in an optomechanical\n  cavity as a closed quantum system", "source": "Nicol\\'as F. Del Grosso, Fernando C. Lombardo, Paula I. Villar", "docs_id": "1910.10218", "section": ["quant-ph", "cond-mat.other", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photon generation via dynamical Casimir effect in an optomechanical\n  cavity as a closed quantum system. We present an analytical and numerical analysis of the particle creation in an optomechanical cavity in parametric resonance. We treat both the electromagnetic field and the mirror as quantum degrees of freedom and study the dynamical evolution as a closed quantum system. We consider different initial states and investigate the spontaneous emission of photons from phonons in the mirror. We find that for initial phononic product states the evolution of the photon number can be described as a non-harmonic quantum oscillator, providing an useful tool so as to estimate the maximum and mean number of photons produced for arbitrary high energies. The efficiency of this mechanism is further analyzed for a detuned cavity as well as the possibility of stimulating the photon production by adding some initial ones to the cavity. We also find relationships for the maximum and mean entanglement between the mirror and the wall in these states. Additionally we study coherent states for the motion of the mirror to connect this model with previous results from quantum field theory with a classical mirror. Finally we study thermal states of phonons in the wall and the equilibration process that leads to a stationary distribution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the dynamical Casimir effect in an optomechanical cavity, what is the relationship between the maximum and mean number of photons produced in the cavity, and how does it depend on the initial phononic product states?\n\n**A)** The maximum and mean number of photons produced are directly proportional to the initial phononic product states, with no dependence on the cavity detuning.\n\n**B)** The maximum and mean number of photons produced are inversely proportional to the initial phononic product states, with a dependence on the cavity detuning.\n\n**C)** The maximum and mean number of photons produced can be described as a non-harmonic quantum oscillator, providing a useful tool to estimate the maximum and mean number of photons produced for arbitrary high energies, and this relationship holds regardless of the initial phononic product states or cavity detuning.\n\n**D)** The maximum and mean number of photons produced are independent of the initial phononic product states and cavity detuning, and can be calculated using a simple harmonic oscillator model.\n\n**Correct Answer:** C) The maximum and mean number of photons produced can be described as a non-harmonic quantum oscillator, providing a useful tool to estimate the maximum and mean number of photons produced for arbitrary high energies, and this relationship holds regardless of the initial phononic product states or cavity detuning.\n\n**Explanation:** The correct answer is C) because the documentation states that the evolution of the photon number can be described as a non-harmonic quantum oscillator, providing a useful tool to estimate the maximum and mean number of photons produced for arbitrary high energies. This relationship holds regardless of the initial phononic product states or cavity detuning, as mentioned in the documentation. The other options are incorrect because they either oversimplify the relationship between the maximum and mean number of photons produced or introduce incorrect dependencies on the initial phononic product states or cavity detuning."}, "34": {"documentation": {"title": "Search for two Higgs bosons in final states containing two photons and\n  two bottom quarks in proton-proton collisions at 8 TeV", "source": "CMS Collaboration", "docs_id": "1603.06896", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for two Higgs bosons in final states containing two photons and\n  two bottom quarks in proton-proton collisions at 8 TeV. A search is presented for the production of two Higgs bosons in final states containing two photons and two bottom quarks. Both resonant and nonresonant hypotheses are investigated. The analyzed data correspond to an integrated luminosity of 19.7 inverse femtobarns of proton-proton collisions at sqrt(s) = 8 TeV collected with the CMS detector. Good agreement is observed between data and predictions of the standard model (SM). Upper limits are set at 95% confidence level on the production cross section of new particles and compared to the prediction for the existence of a warped extra dimension. When the decay to two Higgs bosons is kinematically allowed, assuming a mass scale Lambda[R] = 1 TeV for the model, the data exclude a radion scalar at masses below 980 GeV. The first Kaluza-Klein excitation mode of the graviton in the RS1 Randall-Sundrum model is excluded for masses between 325 and 450 GeV. An upper limit of 0.71 pb is set on the nonresonant two-Higgs-boson cross section in the SM-like hypothesis. Limits are also derived on nonresonant production assuming anomalous Higgs boson couplings."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of the search for two Higgs bosons in final states containing two photons and two bottom quarks in proton-proton collisions at 8 TeV, what is the significance of the upper limit set on the nonresonant two-Higgs-boson cross section in the SM-like hypothesis?\n\nA) The upper limit is set at 95% confidence level, indicating a strong evidence for the existence of a warped extra dimension.\nB) The upper limit is set at 0.71 pb, indicating a possible existence of a radion scalar at masses below 980 GeV.\nC) The upper limit is set at 0.71 pb, indicating a possible existence of a graviton in the RS1 Randall-Sundrum model at masses between 325 and 450 GeV.\nD) The upper limit is set at 0.71 pb, indicating a possible existence of a nonresonant two-Higgs-boson cross section in the SM-like hypothesis.\n\nCorrect Answer: D) The upper limit is set at 0.71 pb, indicating a possible existence of a nonresonant two-Higgs-boson cross section in the SM-like hypothesis.\n\nExplanation: The correct answer is D) because the upper limit of 0.71 pb is set on the nonresonant two-Higgs-boson cross section in the SM-like hypothesis, indicating a possible existence of a nonresonant two-Higgs-boson production mechanism. The other options are incorrect because they either misinterpret the significance of the upper limit (A and C) or incorrectly state the upper limit value (B)."}, "35": {"documentation": {"title": "Robust Design of Power Minimizing Symbol-Level Precoder under Channel\n  Uncertainty", "source": "Alireza Haqiqatnejad, Farbod Kayhan and Bjorn Ottersten", "docs_id": "1805.02395", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Design of Power Minimizing Symbol-Level Precoder under Channel\n  Uncertainty. In this paper, we investigate the downlink transmission of a multiuser multiple-input single-output (MISO) channel under a symbol-level precoding (SLP) scheme, having imperfect channel knowledge at the transmitter. In defining the SLP problem, a general category of constructive interference regions (CIR) called distance preserving CIR (DPCIR) is adopted. In particular, we are interested in the robust SLP design minimizing the total transmit power while satisfying the users' quality-of-service (QoS) requirements. We consider two common models for the channel uncertainty region, namely, norm-bounded spherical and stochastic. For the spherical uncertainty model, a worst-case robust precoder is proposed, while for the stochastic uncertainties, we define a convex optimization problem with probabilistic constraints. We simulate the performance of the proposed robust approaches, and compare them with the existing methods. Through the simulation results, we also show that there is an essential trade-off between the two robust approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Robust Design of Power Minimizing Symbol-Level Precoder under Channel Uncertainty\", what is the primary objective of the proposed robust SLP design, and how does it differ from existing methods?\n\n**A)** The primary objective is to minimize the total transmit power while satisfying the users' quality-of-service (QoS) requirements, and the proposed robust SLP design differs from existing methods in that it uses a worst-case robust precoder for the spherical uncertainty model.\n\n**B)** The primary objective is to minimize the total transmit power while satisfying the users' quality-of-service (QoS) requirements, and the proposed robust SLP design differs from existing methods in that it defines a convex optimization problem with probabilistic constraints for the stochastic uncertainty model.\n\n**C)** The primary objective is to maximize the total transmit power while satisfying the users' quality-of-service (QoS) requirements, and the proposed robust SLP design differs from existing methods in that it uses a worst-case robust precoder for the stochastic uncertainty model.\n\n**D)** The primary objective is to minimize the total transmit power while satisfying the users' quality-of-service (QoS) requirements, and the proposed robust SLP design differs from existing methods in that it defines a convex optimization problem with probabilistic constraints for the norm-bounded spherical uncertainty model.\n\n**Correct Answer:** B) The primary objective is to minimize the total transmit power while satisfying the users' quality-of-service (QoS) requirements, and the proposed robust SLP design differs from existing methods in that it defines a convex optimization problem with probabilistic constraints for the stochastic uncertainty model.\n\n**Explanation:** The correct answer is B) because the paper proposes a convex optimization problem with probabilistic constraints for the stochastic uncertainty model, which is a key difference from existing methods. The other options are incorrect because they either misstate the primary objective or incorrectly describe the difference between the proposed robust SLP design and existing methods."}, "36": {"documentation": {"title": "Low Resource Audio-to-Lyrics Alignment From Polyphonic Music Recordings", "source": "Emir Demirel, Sven Ahlb\\\"ack, Simon Dixon", "docs_id": "2102.09202", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low Resource Audio-to-Lyrics Alignment From Polyphonic Music Recordings. Lyrics alignment in long music recordings can be memory exhaustive when performed in a single pass. In this study, we present a novel method that performs audio-to-lyrics alignment with a low memory consumption footprint regardless of the duration of the music recording. The proposed system first spots the anchoring words within the audio signal. With respect to these anchors, the recording is then segmented and a second-pass alignment is performed to obtain the word timings. We show that our audio-to-lyrics alignment system performs competitively with the state-of-the-art, while requiring much less computational resources. In addition, we utilise our lyrics alignment system to segment the music recordings into sentence-level chunks. Notably on the segmented recordings, we report the lyrics transcription scores on a number of benchmark test sets. Finally, our experiments highlight the importance of the source separation step for good performance on the transcription and alignment tasks. For reproducibility, we publicly share our code with the research community."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed audio-to-lyrics alignment system in the study \"Low Resource Audio-to-Lyrics Alignment From Polyphonic Music Recordings\"?\n\nA) It requires a single pass to perform alignment, reducing computational resources.\nB) It uses a second-pass alignment to obtain word timings, allowing for more accurate results.\nC) It segments music recordings into sentence-level chunks, enabling better transcription scores.\nD) It performs competitively with state-of-the-art systems while requiring much less computational resources.\n\n**Correct Answer:** D) It performs competitively with state-of-the-art systems while requiring much less computational resources.\n\n**Explanation:** The correct answer is D) because the study highlights that the proposed system performs competitively with state-of-the-art systems, but with a significant reduction in computational resources. This is a key advantage of the proposed system, as it addresses the memory exhaustion issue in single-pass alignment methods. The other options are incorrect because they either describe a different aspect of the system (single pass, second-pass alignment, or sentence-level chunking) or do not capture the primary advantage of the system."}, "37": {"documentation": {"title": "Credit risk: Taking fluctuating asset correlations into account", "source": "Thilo A. Schmitt and Rudi Sch\\\"afer and Thomas Guhr", "docs_id": "1601.03015", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Credit risk: Taking fluctuating asset correlations into account. In structural credit risk models, default events and the ensuing losses are both derived from the asset values at maturity. Hence it is of utmost importance to choose a distribution for these asset values which is in accordance with empirical data. At the same time, it is desirable to still preserve some analytical tractability. We achieve both goals by putting forward an ensemble approach for the asset correlations. Consistently with the data, we view them as fluctuating quantities, for which we may choose the average correlation as homogeneous. Thereby we can reduce the number of parameters to two, the average correlation between assets and the strength of the fluctuations around this average value. Yet, the resulting asset value distribution describes the empirical data well. This allows us to derive the distribution of credit portfolio losses. With Monte-Carlo simulations for the Value at Risk and Expected Tail Loss we validate the assumptions of our approach and demonstrate the necessity of taking fluctuating correlations into account."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of structural credit risk models, what is the primary motivation for choosing a distribution for asset values that is in accordance with empirical data, and what are the two key parameters that are used to achieve this goal?\n\n**A)** To ensure that the model is sensitive to changes in asset values, and to reduce the number of parameters to two, the average correlation between assets and the strength of fluctuations around this average value.\n\n**B)** To account for the impact of default events on asset values, and to simplify the model by reducing the number of parameters to two, the average correlation between assets and the strength of fluctuations around this average value.\n\n**C)** To preserve analytical tractability, and to ensure that the model is consistent with empirical data, by choosing a distribution for asset values that is in accordance with empirical data, and to reduce the number of parameters to two, the average correlation between assets and the strength of fluctuations around this average value.\n\n**D)** To estimate the Value at Risk and Expected Tail Loss of credit portfolio losses, and to simplify the model by reducing the number of parameters to two, the average correlation between assets and the strength of fluctuations around this average value.\n\n**Correct Answer:** C) To preserve analytical tractability, and to ensure that the model is consistent with empirical data, by choosing a distribution for asset values that is in accordance with empirical data, and to reduce the number of parameters to two, the average correlation between assets and the strength of fluctuations around this average value.\n\n**Explanation:** The correct answer is C) because the question asks about the primary motivation for choosing a distribution for asset values that is in accordance with empirical data, and the correct answer highlights the importance of preserving analytical tractability and ensuring that the model is consistent with empirical data. The other options are incorrect because they either focus on the impact of default events (B), simplify the model by reducing the number of parameters (A and D), or do not accurately capture the motivation for choosing a distribution for asset values (B)."}, "38": {"documentation": {"title": "Estimation of High Impedance Fault Location in Electrical Transmission\n  Lines Using Artificial Neural Networks and R-X Impedance Graph", "source": "Serkan Budak, Bahadir Akbal", "docs_id": "2011.03227", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimation of High Impedance Fault Location in Electrical Transmission\n  Lines Using Artificial Neural Networks and R-X Impedance Graph. It is very important to ensure continuity in the process from generation of electricity to transmission to cities. The most important part of the system is energy transmission lines and distance protection relays that protect these lines. The main function of the protection relays in electrical installations should be deactivated as soon as possible in the event of short circuits in the system. An accurate error location technique is required to make fast and efficient work. Distance relays are widely used as main and backup protection in transmission and distribution lines. Basically, distance protection relays determine the impedance of the line by comparing the voltage and current values. In this study, artificial neural network (ANN) has been used to accurately locate high impedance short circuit faults in 154 kV power transmission lines. The impedance diagram (R-X) of the circuit breaker, current-voltage transformer, overhead transmission line, distance protection relay and distance protection relay has been formed by using simulation program in order to make the study real. The data sets created by recording the image of the change of the impedance occurring at the time of high impedance short circuit fault. The related focal points in the images are given as input to different ANN models and predicted the short circuit faults occurring at different locations on the transmission lines with high accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of high impedance fault location in electrical transmission lines using artificial neural networks and R-X impedance graph, what is the primary advantage of using a simulation program to form the impedance diagram of the circuit breaker, current-voltage transformer, overhead transmission line, distance protection relay, and distance protection relay?\n\n**A)** It allows for the creation of a more complex and realistic model of the transmission line.\n**B)** It enables the use of more advanced algorithms for fault location estimation.\n**C)** It facilitates the comparison of voltage and current values to determine the impedance of the line.\n**D)** It provides a more accurate representation of the transmission line's impedance characteristics.\n\n**Correct Answer:** D) It provides a more accurate representation of the transmission line's impedance characteristics.\n\n**Explanation:** The correct answer is D) because the simulation program is used to create a realistic model of the transmission line's impedance characteristics, which is essential for accurate fault location estimation. The other options are incorrect because while they may be related to the topic, they are not the primary advantage of using a simulation program in this context. Option A is incorrect because the complexity of the model is not the primary advantage. Option B is incorrect because the use of advanced algorithms is not directly related to the simulation program. Option C is incorrect because the comparison of voltage and current values is a function of the distance protection relay, not the simulation program."}, "39": {"documentation": {"title": "Time-varying Graph Representation Learning via Higher-Order Skip-Gram\n  with Negative Sampling", "source": "Simone Piaggesi, Andr\\'e Panisson", "docs_id": "2006.14330", "section": ["cs.LG", "cs.SI", "physics.soc-ph", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time-varying Graph Representation Learning via Higher-Order Skip-Gram\n  with Negative Sampling. Representation learning models for graphs are a successful family of techniques that project nodes into feature spaces that can be exploited by other machine learning algorithms. Since many real-world networks are inherently dynamic, with interactions among nodes changing over time, these techniques can be defined both for static and for time-varying graphs. Here, we build upon the fact that the skip-gram embedding approach implicitly performs a matrix factorization, and we extend it to perform implicit tensor factorization on different tensor representations of time-varying graphs. We show that higher-order skip-gram with negative sampling (HOSGNS) is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. We empirically evaluate our approach using time-resolved face-to-face proximity data, showing that the learned time-varying graph representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction, and to predict the outcome of dynamical processes such as disease spreading. The source code and data are publicly available at https://github.com/simonepiaggesi/hosgns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Higher-Order Skip-Gram with Negative Sampling (HOSGNS) approach in representing time-varying graphs, as demonstrated in the provided Arxiv documentation?\n\n**A)** HOSGNS requires a significantly larger number of parameters compared to other approaches, making it computationally expensive.\n**B)** HOSGNS is able to disentangle the role of nodes and time, allowing for more accurate representation learning.\n**C)** HOSGNS is limited to static graphs and cannot be applied to time-varying graphs.\n**D)** HOSGNS relies solely on matrix factorization, whereas other approaches use tensor factorization.\n\n**Correct Answer:** B) HOSGNS is able to disentangle the role of nodes and time, allowing for more accurate representation learning.\n\n**Explanation:** The correct answer is B) because the provided documentation states that HOSGNS \"is able to disentangle the role of nodes and time\" and that it achieves this with a \"small fraction of the number of parameters needed by other approaches\". This suggests that the primary advantage of HOSGNS is its ability to separate the roles of nodes and time in the graph, leading to more accurate representation learning."}, "40": {"documentation": {"title": "Appearance of Random Matrix Theory in Deep Learning", "source": "Nicholas P Baskerville and Diego Granziol and Jonathan P Keating", "docs_id": "2102.06740", "section": ["cs.LG", "math-ph", "math.MP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Appearance of Random Matrix Theory in Deep Learning. We investigate the local spectral statistics of the loss surface Hessians of artificial neural networks, where we discover excellent agreement with Gaussian Orthogonal Ensemble statistics across several network architectures and datasets. These results shed new light on the applicability of Random Matrix Theory to modelling neural networks and suggest a previously unrecognised role for it in the study of loss surfaces in deep learning. Inspired by these observations, we propose a novel model for the true loss surfaces of neural networks, consistent with our observations, which allows for Hessian spectral densities with rank degeneracy and outliers, extensively observed in practice, and predicts a growing independence of loss gradients as a function of distance in weight-space. We further investigate the importance of the true loss surface in neural networks and find, in contrast to previous work, that the exponential hardness of locating the global minimum has practical consequences for achieving state of the art performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary finding of the study \"Appearance of Random Matrix Theory in Deep Learning\" regarding the local spectral statistics of the loss surface Hessians of artificial neural networks?\n\nA) The study found that the loss surface Hessians of artificial neural networks follow a Gaussian Orthogonal Ensemble statistics, but with a significant deviation from the expected distribution.\nB) The study discovered that the local spectral statistics of the loss surface Hessians of artificial neural networks are consistent with Gaussian Orthogonal Ensemble statistics across several network architectures and datasets.\nC) The study found that the true loss surfaces of neural networks can be modeled using a Gaussian Orthogonal Ensemble statistics, but with a significant impact on the exponential hardness of locating the global minimum.\nD) The study concluded that the local spectral statistics of the loss surface Hessians of artificial neural networks are inconsistent with Gaussian Orthogonal Ensemble statistics, and instead follow a different distribution.\n\nCorrect Answer: B) The study discovered that the local spectral statistics of the loss surface Hessians of artificial neural networks are consistent with Gaussian Orthogonal Ensemble statistics across several network architectures and datasets.\n\nExplanation: The correct answer is B) because the study explicitly states that it \"investigates the local spectral statistics of the loss surface Hessians of artificial neural networks, where we discover excellent agreement with Gaussian Orthogonal Ensemble statistics across several network architectures and datasets.\" This indicates that the primary finding of the study is the consistency between the local spectral statistics of the loss surface Hessians and Gaussian Orthogonal Ensemble statistics."}, "41": {"documentation": {"title": "Statistical Properties of the Keyboard Design with Extension to\n  Drug-Combination Trials", "source": "Haitao Pan, Ruitao Lin, and Ying Yuan", "docs_id": "1712.06718", "section": ["stat.ME", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Properties of the Keyboard Design with Extension to\n  Drug-Combination Trials. The keyboard design is a novel phase I dose-finding method that is simple and has good operating characteristics. This paper studies theoretical properties of the keyboard design, including the optimality of its decision rules, coherence in dose transition, and convergence to the target dose. Establishing these theoretical properties explains the mechanism of the design and provides assurance to practitioners regarding the behavior of the keyboard design. We further extend the keyboard design to dual-agent dose-finding trials, which inherit the same statistical properties and simplicity as the single-agent keyboard design. Extensive simulations are conducted to evaluate the performance of the proposed keyboard drug-combination design using a novel, random two-dimensional dose--toxicity scenario generating algorithm. The simulation results confirm the desirable and competitive operating characteristics of the keyboard design as established by the theoretical study. An R Shiny application is developed to facilitate implementing the keyboard combination design in practice."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of establishing the theoretical properties of the keyboard design, as described in the paper \"Statistical Properties of the Keyboard Design with Extension to Drug-Combination Trials\"?\n\nA) To evaluate the performance of the keyboard design in practice\nB) To provide assurance to practitioners regarding the behavior of the keyboard design\nC) To develop an R Shiny application for implementing the keyboard design\nD) To conduct extensive simulations to evaluate the performance of the keyboard design\n\n**Correct Answer:** B) To provide assurance to practitioners regarding the behavior of the keyboard design\n\n**Explanation:** The paper states that \"Establishing these theoretical properties explains the mechanism of the design and provides assurance to practitioners regarding the behavior of the keyboard design.\" This indicates that the primary purpose of establishing the theoretical properties is to provide assurance to practitioners, making option B the correct answer. Options A, C, and D are incorrect because they are related to other aspects of the paper, such as evaluating performance, developing an application, and conducting simulations, but not the primary purpose of establishing theoretical properties."}, "42": {"documentation": {"title": "Proximal Gradient Descent-Ascent: Variable Convergence under K{\\L}\n  Geometry", "source": "Ziyi Chen, Yi Zhou, Tengyu Xu, Yingbin Liang", "docs_id": "2102.04653", "section": ["math.OC", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Proximal Gradient Descent-Ascent: Variable Convergence under K{\\L}\n  Geometry. The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function values or gradient norms. However, the variable convergence of GDA has been proved only under convexity geometries, and there lacks understanding for general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequence to a critical point. By leveraging this Lyapunov function and the K{\\L} geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a critical point $x^*$, i.e., $x_t\\to x^*, y_t\\to y^*(x^*)$. Furthermore, over the full spectrum of the K{\\L}-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the K{\\L} parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Proximal Gradient Descent-Ascent (GDA) is widely applied to solve minimax optimization problems. However, the variable convergence of GDA has been proved only under convexity geometries. This paper studies the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. What is the key contribution of this paper, and how does it address the limitation of variable convergence of GDA?\n\nA) The paper introduces a new algorithm that combines GDA with a non-convex optimization technique, achieving faster convergence rates.\nB) The paper establishes a novel Lyapunov function that monotonically decreases in the minimax optimization process, driving the variable sequence to a critical point.\nC) The paper shows that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the K{\\L} parameter.\nD) The paper proves that proximal-GDA converges to a global optimum, rather than a local optimum, in nonconvex minimax optimization problems.\n\n**Correct Answer:** B) The paper establishes a novel Lyapunov function that monotonically decreases in the minimax optimization process, driving the variable sequence to a critical point.\n\n**Explanation:** The correct answer is B) because the paper specifically states that it establishes a novel Lyapunov function that monotonically decreases in the minimax optimization process, driving the variable sequence to a critical point. This is the key contribution of the paper, as it addresses the limitation of variable convergence of GDA by providing a formal proof of convergence for nonconvex minimax optimization problems. The other options are incorrect because they either introduce new algorithms or convergence rates, or prove convergence to a global optimum, which is not the focus of the paper."}, "43": {"documentation": {"title": "Evidence for the J^p=1/2^+ narrow state at 1650 MeV in the\n  photoproduction of KLambda", "source": "T. Mart", "docs_id": "1104.2389", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evidence for the J^p=1/2^+ narrow state at 1650 MeV in the\n  photoproduction of KLambda. We have investigated the existence of the J^p=1/2^+ narrow resonance predicted by the chiral soliton model by utilizing the kaon photoproduction process gamma + p --> K^+ + Lambda. For this purpose we have constructed two phenomenological models based on our previous effective Lagrangian model, which are able to describe kaon photoproduction from threshold up to W = 1730 MeV. By varying the mass (width) of an inserted P_{11} resonance from 1620 to 1730 MeV (0.1 to 1 MeV and 1 to 10 MeV) a number of fits has been performed in order to search for the resonance mass. Our result indicates that the most promising candidate mass (width) of this resonance is 1650 MeV (5 MeV). Although our calculation does not exclude the possibility of narrow resonances with masses of 1680, 1700 and 1720 MeV, the mass of 1650 MeV is obtained for all phenomenological models used in this investigation. Variations of the resonance width and $K\\Lambda$ branching ratio are found to have a mild effect on the chi^2. The possibility that the obtained result originates from other resonance states is also discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most promising candidate mass for the J^p=1/2^+ narrow resonance predicted by the chiral soliton model in the kaon photoproduction process gamma + p --> K^+ + Lambda?\n\nA) 1620 MeV\nB) 1650 MeV (5 MeV)\nC) 1680 MeV\nD) 1730 MeV\n\nCorrect Answer: B) 1650 MeV (5 MeV)\n\nExplanation: The text states that the most promising candidate mass for the J^p=1/2^+ narrow resonance is 1650 MeV, with a width of 5 MeV. This is obtained from a number of fits performed using different phenomenological models. While the other options are mentioned as possible masses, they are not the most promising candidate. Option A is too low, and options C and D are too high. Option B is the correct answer because it is the mass that is consistently obtained across all the phenomenological models used in the investigation."}, "44": {"documentation": {"title": "Anisotropy of TeV Cosmic Rays and the Outer Heliospheric Boundaries", "source": "P. Desiati and A. Lazarian", "docs_id": "1111.3075", "section": ["astro-ph.HE", "astro-ph.GA", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anisotropy of TeV Cosmic Rays and the Outer Heliospheric Boundaries. Cosmic rays in the energy range from about 10's GeV to several 100's TeV are observed on Earth with an energy-dependent anisotropy of order 0.01-0.1%, and a consistent topology that appears to significantly change at higher energy. The nearest and most recent galactic cosmic ray sources might stochastically dominate the observation and possibly explain a change in orientation of the anisotropy as a function of energy. However, the diffusion approximation is not able to explain its non-dipolar structure and, in particular, the significant contribution of small angular scale features. Particle propagation within the mean free path in the local interstellar medium might have a major role in determining the properties of galactic cosmic rays, such as their arrival distribution. In particular, scattering on perturbations induced in the local interstellar magnetic field by the heliosphere wake, may cause a re-distribution of anisotropic cosmic rays below about 100 TeV toward the direction of the elongated heliotail and of the local interstellar magnetic field in the outer heliosphere. Such scattering processes are considered responsible of the observed TeV cosmic ray global anisotropy and its fine angular structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism responsible for the observed global anisotropy and fine angular structure of TeV cosmic rays, according to the study \"Anisotropy of TeV Cosmic Rays and the Outer Heliospheric Boundaries\"?\n\nA) Diffusion approximation\nB) Scattering on perturbations induced in the local interstellar magnetic field by the heliosphere wake\nC) Stochastic dominance of the nearest and most recent galactic cosmic ray sources\nD) Galactic magnetic field re-distribution\n\nCorrect Answer: B) Scattering on perturbations induced in the local interstellar magnetic field by the heliosphere wake\n\nExplanation: The study suggests that scattering on perturbations induced in the local interstellar magnetic field by the heliosphere wake is responsible for the observed global anisotropy and fine angular structure of TeV cosmic rays. This is because such scattering processes can cause a re-distribution of anisotropic cosmic rays below about 100 TeV toward the direction of the elongated heliotail and of the local interstellar magnetic field in the outer heliosphere. The other options are incorrect because the diffusion approximation is not able to explain the non-dipolar structure of the anisotropy, stochastic dominance of galactic cosmic ray sources may explain a change in orientation of the anisotropy as a function of energy, and galactic magnetic field re-distribution is not mentioned as a primary mechanism in the study."}, "45": {"documentation": {"title": "Parameter Estimation in Searches for the Stochastic Gravitational-Wave\n  Background", "source": "Vuk Mandic, Eric Thrane, Stefanos Giampanis, Tania Regimbau", "docs_id": "1209.3847", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parameter Estimation in Searches for the Stochastic Gravitational-Wave\n  Background. The stochastic gravitational-wave background (SGWB) is expected to arise from the superposition of many independent and unresolved gravitational-wave signals of either cosmological or astrophysical origin. The spectral content of the SGWB carries signatures of the physics that generated it. We present a Bayesian framework for estimating the parameters associated with different SGWB models using data from gravitational-wave detectors. We apply this technique to recent results from LIGO to produce the first simultaneous 95% confidence level limits on multiple parameters in generic power-law SGWB models and in SGWB models of compact binary coalescences. We also estimate the sensitivity of the upcoming second-generation detectors such as Advanced LIGO/Virgo to these models and demonstrate how SGWB measurements can be combined and compared with observations of individual compact binary coalescences in order to build confidence in the origin of an observed SGWB signal. In doing so, we demonstrate a novel means of differentiating between different sources of the SGWB."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the stochastic gravitational-wave background (SGWB) analysis presented in the paper, what is the primary advantage of using a Bayesian framework for estimating the parameters associated with different SGWB models?\n\n**A)** It allows for the incorporation of prior knowledge about the SGWB models, enabling more accurate parameter estimation.\n**B)** It provides a more robust method for handling the uncertainty in the data, leading to more reliable parameter estimates.\n**C)** It enables the simultaneous estimation of multiple parameters in different SGWB models, facilitating a more comprehensive understanding of the SGWB.\n**D)** It allows for the comparison of the SGWB signal with observations of individual compact binary coalescences, providing a more nuanced understanding of the origin of the SGWB.\n\n**Correct Answer:** C) It enables the simultaneous estimation of multiple parameters in different SGWB models, facilitating a more comprehensive understanding of the SGWB.\n\n**Explanation:** The Bayesian framework presented in the paper allows for the simultaneous estimation of multiple parameters in different SGWB models, which is a key advantage in understanding the SGWB. By estimating the parameters of multiple models simultaneously, researchers can gain a more comprehensive understanding of the SGWB and its potential sources. This is particularly important in the context of the SGWB, where the signal is expected to arise from the superposition of many independent and unresolved gravitational-wave signals."}, "46": {"documentation": {"title": "The ultraviolet luminosity function of star-forming galaxies between\n  redshifts of 0.6 and 1.2", "source": "M.J. Page, T. Dwelly, I. McHardy, N. Seymour, K.O. Mason, M. Sharma,\n  J.A. Kennea, T.P. Sasseen, J.I. Rawlings, A.A. Breeveld, I. Ferreras, N.S.\n  Loaring, D.J. Walton, M. Symeonidis", "docs_id": "2106.08200", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The ultraviolet luminosity function of star-forming galaxies between\n  redshifts of 0.6 and 1.2. We use ultraviolet imaging taken with the XMM-Newton Optical Monitor telescope (XMM-OM), covering 280 square arcminutes in the UVW1 band (effective wavelength 2910 Angstroms) to measure rest-frame ultraviolet (1500 Angstrom) luminosity functions of galaxies with redshifts z between 0.6 and 1.2. The XMM-OM data are supplemented by a large body of optical and infrared imaging to provide photometric redshifts. The XMM-OM data have a significantly narrower point-spread-function (resulting in less source confusion) and simpler K-correction than the GALEX data previously employed in this redshift range. Ultraviolet-bright active galactic nuclei are excluded to ensure that the luminosity functions relate directly to the star-forming galaxy population. Binned luminosity functions and parametric Schechter-function fits are derived in two redshift intervals: 0.6<z<0.8 and 0.8<z<1.2. We find that the luminosity function evolves such that the characteristic absolute magnitude M* is brighter for 0.8<z<1.2 than for 0.6<z<0.8."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the XMM-Newton Optical Monitor (XMM-OM) data over the GALEX data in measuring the rest-frame ultraviolet luminosity functions of galaxies with redshifts between 0.6 and 1.2?\n\nA) The XMM-OM data have a broader point-spread-function, resulting in more source confusion.\nB) The XMM-OM data have a significantly narrower point-spread-function, resulting in less source confusion.\nC) The XMM-OM data have a more complex K-correction than the GALEX data.\nD) The XMM-OM data have a less accurate photometric redshifts than the GALEX data.\n\nCorrect Answer: B) The XMM-OM data have a significantly narrower point-spread-function, resulting in less source confusion.\n\nExplanation: The question requires the test-taker to carefully read the documentation and identify the key advantage of using the XMM-OM data. The correct answer is supported by the text, which states that the XMM-OM data have a \"significantly narrower point-spread-function\" compared to the GALEX data, resulting in \"less source confusion\". This is a key advantage in measuring the luminosity functions of galaxies. The other options are incorrect because they either misstate the advantage of the XMM-OM data (A and C) or are not supported by the text (D)."}, "47": {"documentation": {"title": "Regularized Zero-Forcing Interference Alignment for the Two-Cell MIMO\n  Interfering Broadcast Channel", "source": "Joonwoo Shin and Jaekyun Moon", "docs_id": "1301.1373", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Regularized Zero-Forcing Interference Alignment for the Two-Cell MIMO\n  Interfering Broadcast Channel. In this paper, we propose transceiver design strategies for the two-cell multiple-input multiple-output (MIMO) interfering broadcast channel where inter-cell interference (ICI) exists in addition to interuser interference (IUI). We first formulate the generalized zero-forcing interference alignment (ZF-IA) method based on the alignment of IUI and ICI in multi-dimensional subspace. We then devise a minimum weighted-mean-square-error (WMSE) method based on regularizing the precoders and decoders of the generalized ZF-IA scheme. In contrast to the existing weighted-sum-rate-maximizing transceiver, our method does not require an iterative calculation of the optimal weights. Because of this, the proposed scheme, while not designed specifically to maximize the sum rate, is computationally efficient and achieves a faster convergence compared to the known weighted-sum-rate maximizing scheme. Through analysis and simulation, we show the effectiveness of the proposed regularized ZF-IA scheme."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Regularized Zero-Forcing Interference Alignment for the Two-Cell MIMO Interfering Broadcast Channel, what is the primary advantage of the proposed minimum weighted-mean-square-error (WMSE) method over the existing weighted-sum-rate-maximizing transceiver scheme?\n\n**A)** The proposed WMSE method requires fewer computational resources.\n**B)** The proposed WMSE method achieves faster convergence compared to the weighted-sum-rate maximizing scheme.\n**C)** The proposed WMSE method is specifically designed to maximize the sum rate.\n**D)** The proposed WMSE method does not require an iterative calculation of the optimal weights.\n\n**Correct Answer:** D) The proposed WMSE method does not require an iterative calculation of the optimal weights.\n\n**Explanation:** The question requires the test-taker to understand the key advantage of the proposed WMSE method, which is that it does not require an iterative calculation of the optimal weights, unlike the existing weighted-sum-rate maximizing scheme. This requires the test-taker to analyze the pros and cons of the two methods and identify the correct advantage of the WMSE method."}, "48": {"documentation": {"title": "Low Complexity Delay-Constrained Beamforming for Multi-User MIMO Systems\n  with Imperfect CSIT", "source": "Vincent K. N. Lau, Fan Zhang, Ying Cui", "docs_id": "1305.1012", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low Complexity Delay-Constrained Beamforming for Multi-User MIMO Systems\n  with Imperfect CSIT. In this paper, we consider the delay-constrained beamforming control for downlink multi-user MIMO (MU- MIMO) systems with imperfect channel state information at the transmitter (CSIT). The delay-constrained control problem is formulated as an infinite horizon average cost partially observed Markov decision process. To deal with the curse of dimensionality, we introduce a virtual continuous time system and derive a closed-form approximate value function using perturbation analysis w.r.t. the CSIT errors. To deal with the challenge of the conditional packet error rate (PER), we build a tractable closed- form approximation using a Bernstein-type inequality. Based on the closed-form approximations of the relative value function and the conditional PER, we propose a conservative formulation of the original beamforming control problem. The conservative problem is non-convex and we transform it into a convex problem using the semidefinite relaxation (SDR) technique. We then propose an alternating iterative algorithm to solve the SDR problem. Finally, the proposed scheme is compared with various baselines through simulations and it is shown that significant performance gain can be achieved."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Low Complexity Delay-Constrained Beamforming for Multi-User MIMO Systems with Imperfect CSIT\", what is the primary approach used to deal with the curse of dimensionality in the delay-constrained beamforming control problem?\n\n**A)** The authors use a finite horizon approach to approximate the infinite horizon average cost partially observed Markov decision process.\n\n**B)** The authors introduce a virtual continuous time system and derive a closed-form approximate value function using perturbation analysis w.r.t. the CSIT errors.\n\n**C)** The authors use a dynamic programming approach to solve the original beamforming control problem.\n\n**D)** The authors employ a machine learning algorithm to learn the optimal beamforming control policy.\n\n**Correct Answer:** B) The authors introduce a virtual continuous time system and derive a closed-form approximate value function using perturbation analysis w.r.t. the CSIT errors.\n\n**Explanation:** The correct answer is B) because the paper introduces a virtual continuous time system to deal with the curse of dimensionality, and derives a closed-form approximate value function using perturbation analysis w.r.t. the CSIT errors. This approach allows the authors to approximate the infinite horizon average cost partially observed Markov decision process and obtain a tractable solution to the delay-constrained beamforming control problem. The other options are incorrect because they do not accurately describe the approach used in the paper."}, "49": {"documentation": {"title": "Uniform convergence of the empirical cumulative distribution function\n  under informative selection from a finite population", "source": "Daniel Bonn\\'ery, F. Jay Breidt, Fran\\c{c}ois Coquet", "docs_id": "1211.5468", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniform convergence of the empirical cumulative distribution function\n  under informative selection from a finite population. Consider informative selection of a sample from a finite population. Responses are realized as independent and identically distributed (i.i.d.) random variables with a probability density function (p.d.f.) f, referred to as the superpopulation model. The selection is informative in the sense that the sample responses, given that they were selected, are not i.i.d. f. In general, the informative selection mechanism may induce dependence among the selected observations. The impact of such dependence on the empirical cumulative distribution function (c.d.f.) is studied. An asymptotic framework and weak conditions on the informative selection mechanism are developed under which the (unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely, to a weighted version of the superpopulation c.d.f. This yields an analogue of the Glivenko-Cantelli theorem. A series of examples, motivated by real problems in surveys and other observational studies, shows that the conditions are verifiable for specified designs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a finite population with a superpopulation model of independent and identically distributed (i.i.d.) random variables with a probability density function (p.d.f.) f. The population is subject to informative selection, where the sample responses, given that they were selected, are not i.i.d. f. What are the conditions under which the (unweighted) empirical cumulative distribution function (c.d.f.) converges uniformly, in $L_2$ and almost surely, to a weighted version of the superpopulation c.d.f., and what are some examples of real-world problems in surveys and observational studies that can be modeled using this framework?\n\n**A)** The informative selection mechanism must be such that the sample responses are conditionally independent given the selection, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite second moment.\n\n**B)** The informative selection mechanism must be such that the sample responses are conditionally independent given the selection, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite fourth moment.\n\n**C)** The informative selection mechanism must be such that the sample responses are conditionally independent given the selection, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite third moment.\n\n**D)** The informative selection mechanism must be such that the sample responses are conditionally independent given the selection, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite first moment.\n\n**Correct Answer:** A) The informative selection mechanism must be such that the sample responses are conditionally independent given the selection, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite second moment.\n\n**Explanation:** The correct answer is based on the conditions developed in the original paper, which state that the informative selection mechanism must induce conditional independence among the selected observations, and the population size must be sufficiently large. Additionally, the superpopulation model must have a finite second moment, which is a necessary condition for the convergence of the empirical c.d.f. to a weighted version of the superpopulation c.d.f. The other options are incorrect because they do not accurately reflect the conditions developed in the original paper."}, "50": {"documentation": {"title": "Notes on the Third Law of Thermodynamics.I", "source": "F.Belgiorno", "docs_id": "physics/0210037", "section": ["physics.gen-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Notes on the Third Law of Thermodynamics.I. We analyze some aspects of the third law of thermodynamics. We first review both the entropic version (N) and the unattainability version (U) and the relation occurring between them. Then, we heuristically interpret (N) as a continuity boundary condition for thermodynamics at the boundary T=0 of the thermodynamic domain. On a rigorous mathematical footing, we discuss the third law both in Carath\\'eodory's approach and in Gibbs' one. Carath\\'eodory's approach is fundamental in order to understand the nature of the surface T=0. In fact, in this approach, under suitable mathematical conditions, T=0 appears as a leaf of the foliation of the thermodynamic manifold associated with the non-singular integrable Pfaffian form $\\delta Q_{rev}$. Being a leaf, it cannot intersect any other leaf $S=$ const. of the foliation. We show that (N) is equivalent to the requirement that T=0 is a leaf. In Gibbs' approach, the peculiar nature of T=0 appears to be less evident because the existence of the entropy is a postulate; nevertheless, it is still possible to conclude that the lowest value of the entropy has to belong to the boundary of the convex set where the function is defined."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the thermodynamic manifold associated with the non-singular integrable Pfaffian form $\\delta Q_{rev}$, and the foliation of this manifold by leaves of constant entropy $S=$ const. What is the mathematical condition that ensures the leaf $T=0$ is a leaf of the foliation, and what is the equivalent condition in Carath\\'eodory's approach?\n\nA) The Pfaffian form $\\delta Q_{rev}$ is a closed differential form.\nB) The entropy function $S$ is a convex function on the thermodynamic domain.\nC) The leaf $T=0$ is a leaf of the foliation if and only if the Pfaffian form $\\delta Q_{rev}$ is a non-singular integrable form.\nD) The lowest value of the entropy $S$ belongs to the boundary of the convex set where the function is defined.\n\nCorrect Answer: C) The leaf $T=0$ is a leaf of the foliation if and only if the Pfaffian form $\\delta Q_{rev}$ is a non-singular integrable form.\n\nExplanation: According to the documentation, in Carath\\'eodory's approach, the leaf $T=0$ appears as a leaf of the foliation of the thermodynamic manifold associated with the non-singular integrable Pfaffian form $\\delta Q_{rev}$. This means that the Pfaffian form must be a non-singular integrable form for $T=0$ to be a leaf of the foliation. This condition is equivalent to the requirement that $T=0$ is a leaf, as stated in the documentation."}, "51": {"documentation": {"title": "Bounds on determinants of perturbed diagonal matrices", "source": "Richard P. Brent, Judy-anne H. Osborn and Warren D. Smith", "docs_id": "1401.7084", "section": ["math.NA", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bounds on determinants of perturbed diagonal matrices. We give upper and lower bounds on the determinant of a perturbation of the identity matrix or, more generally, a perturbation of a nonsingular diagonal matrix. The matrices considered are, in general, diagonally dominant. The lower bounds are best possible, and in several cases they are stronger than well-known bounds due to Ostrowski and other authors. If $A = I-E$ is an $n \\times n$ matrix and the elements of $E$ are bounded in absolute value by $\\varepsilon \\le 1/n$, then a lower bound of Ostrowski (1938) is $\\det(A) \\ge 1-n\\varepsilon$. We show that if, in addition, the diagonal elements of $E$ are zero, then a best-possible lower bound is \\[\\det(A) \\ge (1-(n-1)\\varepsilon)\\,(1+\\varepsilon)^{n-1}.\\] Corresponding upper bounds are respectively \\[\\det(A) \\le (1 + 2\\varepsilon + n\\varepsilon^2)^{n/2}\\] and \\[\\det(A) \\le (1 + (n-1)\\varepsilon^2)^{n/2}.\\] The first upper bound is stronger than Ostrowski's bound (for $\\varepsilon < 1/n$) $\\det(A) \\le (1 - n\\varepsilon)^{-1}$. The second upper bound generalises Hadamard's inequality, which is the case $\\varepsilon = 1$. A necessary and sufficient condition for our upper bounds to be best possible for matrices of order $n$ and all positive $\\varepsilon$ is the existence of a skew-Hadamard matrix of order $n$."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Let $A$ be an $n \\times n$ matrix of the form $A = I - E$, where $I$ is the identity matrix and $E$ is a matrix with elements bounded in absolute value by $\\varepsilon \\le 1/n$. Suppose that the diagonal elements of $E$ are zero. Find the best possible lower bound for $\\det(A)$ in terms of $\\varepsilon$ and $n$.\n\n**A)** $\\det(A) \\ge (1 - (n-1)\\varepsilon)(1 + \\varepsilon)^{n-1}$\n**B)** $\\det(A) \\ge (1 - n\\varepsilon)^{n/2}$\n**C)** $\\det(A) \\ge (1 + (n-1)\\varepsilon^2)^{n/2}$\n**D)** $\\det(A) \\ge (1 + 2\\varepsilon + n\\varepsilon^2)^{n/2}$\n\n**Correct Answer:** A) $\\det(A) \\ge (1 - (n-1)\\varepsilon)(1 + \\varepsilon)^{n-1}$\n\n**Explanation:** The correct answer is based on the lower bound given in the Arxiv documentation, which states that if the diagonal elements of $E$ are zero, then a best-possible lower bound for $\\det(A)$ is $(1 - (n-1)\\varepsilon)(1 + \\varepsilon)^{n-1}$. This bound is derived by considering the properties of the matrix $A$ and the constraints on the elements of $E$. The other options are incorrect because they either do not match the lower bound given in the documentation or are upper bounds that are not best possible."}, "52": {"documentation": {"title": "Longitudinal foliation rigidity and Lipschitz-continuous invariant forms\n  for hyperbolic flows", "source": "Patrick Foulon, Boris Hasselblatt", "docs_id": "1006.0676", "section": ["math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal foliation rigidity and Lipschitz-continuous invariant forms\n  for hyperbolic flows. In several contexts the defining invariant structures of a hyperbolic dynamical system are smooth only in systems of algebraic origin (smooth rigidity), and we prove new results of this type for a class of flows. For a compact Riemannian manifold and a uniformly quasiconformal transversely symplectic Anosov flow we define the longitudinal KAM-cocycle and use it to prove a rigidity result: The joint stable/unstable subbundle is Zygmund-regular, and higher regularity implies vanishing of the longitudinal KAM-cocycle, which in turn implies that the subbundle is Lipschitz-continuous and indeed that the flow is smoothly conjugate to an algebraic one. To establish the latter, we prove results for algebraic Anosov systems that imply smoothness and a special structure for any Lipschitz-continuous invariant 1-form. Several features of the reasoning are interesting: The use of exterior calculus for Lipschitz-continuous forms, that the arguments for geodesic flows and infranilmanifoldautomorphisms are quite different, and the need for mixing as opposed to ergodicity in the latter case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let M be a compact Riemannian manifold and X be a uniformly quasiconformal transversely symplectic Anosov flow. Suppose that the joint stable/unstable subbundle is Zygmund-regular. What can be concluded about the longitudinal KAM-cocycle?\n\nA) It is Lipschitz-continuous if and only if the flow is algebraic.\nB) It is Lipschitz-continuous if and only if the flow is smoothly conjugate to an algebraic one.\nC) It is Lipschitz-continuous if and only if the flow is ergodic.\nD) It is Lipschitz-continuous if and only if the flow is mixing.\n\nCorrect Answer: B) It is Lipschitz-continuous if and only if the flow is smoothly conjugate to an algebraic one.\n\nExplanation: The correct answer follows from the main result of the paper, which states that if the joint stable/unstable subbundle is Zygmund-regular, then the longitudinal KAM-cocycle vanishes, implying that the subbundle is Lipschitz-continuous and the flow is smoothly conjugate to an algebraic one. The other options are incorrect because they do not accurately reflect the relationship between the longitudinal KAM-cocycle and the smooth conjugacy of the flow to an algebraic one."}, "53": {"documentation": {"title": "Adaptive solution of initial value problems by a dynamical Galerkin\n  scheme", "source": "Rodrigo M. Pereira, Natacha Nguyen van yen, Kai Schneider, Marie Farge", "docs_id": "2111.04863", "section": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive solution of initial value problems by a dynamical Galerkin\n  scheme. We study dynamical Galerkin schemes for evolutionary partial differential equations (PDEs), where the projection operator changes over time. When selecting a subset of basis functions, the projection operator is non-differentiable in time and an integral formulation has to be used. We analyze the projected equations with respect to existence and uniqueness of the solution and prove that non-smooth projection operators introduce dissipation, a result which is crucial for adaptive discretizations of PDEs, e.g., adaptive wavelet methods. For the Burgers equation we illustrate numerically that thresholding the wavelet coefficients, and thus changing the projection space, will indeed introduce dissipation of energy. We discuss consequences for the so-called `pseudo-adaptive' simulations, where time evolution and dealiasing are done in Fourier space, whilst thresholding is carried out in wavelet space. Numerical examples are given for the inviscid Burgers equation in 1D and the incompressible Euler equations in 2D and 3D."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a dynamical Galerkin scheme for the inviscid Burgers equation in 1D, where the projection operator is non-smooth and changes over time. Analyze the effect of thresholding the wavelet coefficients on the energy dissipation of the solution.\n\nA) The thresholding of wavelet coefficients introduces a non-physical dissipation of energy, which is not related to the physical viscosity of the system.\n\nB) The thresholding of wavelet coefficients introduces a dissipative term in the projected equations, which is proportional to the threshold value and the energy of the solution.\n\nC) The thresholding of wavelet coefficients has no effect on the energy dissipation of the solution, as the projection operator is smooth and the thresholding is only applied to the wavelet coefficients.\n\nD) The thresholding of wavelet coefficients introduces a dissipative term in the projected equations, which is proportional to the threshold value and the energy of the solution, but only for the spatial derivatives of the solution.\n\n**Correct Answer:** B) The thresholding of wavelet coefficients introduces a dissipative term in the projected equations, which is proportional to the threshold value and the energy of the solution.\n\n**Explanation:** The correct answer is based on the analysis of the projected equations and the effect of thresholding the wavelet coefficients on the energy dissipation of the solution. The thresholding of wavelet coefficients introduces a dissipative term in the projected equations, which is proportional to the threshold value and the energy of the solution. This is a crucial result for adaptive discretizations of PDEs, as it shows that the thresholding of wavelet coefficients can introduce dissipation of energy, which can affect the accuracy and stability of the numerical solution."}, "54": {"documentation": {"title": "Primordial Black Holes and Local Non-Gaussianity in Canonical Inflation", "source": "Samuel Passaglia, Wayne Hu, Hayato Motohashi", "docs_id": "1812.08243", "section": ["astro-ph.CO", "gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Primordial Black Holes and Local Non-Gaussianity in Canonical Inflation. Primordial black holes (PBHs) cannot be produced abundantly enough to be the dark matter in canonical single-field inflation under slow roll. This conclusion is robust to local non-Gaussian correlations between long- and short-wavelength curvature modes, which we show have no effect in slow roll on local primordial black hole abundances. For the prototypical model which evades this no go, ultra-slow roll (USR), these squeezed non-Gaussian correlations have at most an order unity effect on the variance of PBH-producing curvature fluctuations for models that would otherwise fail to form sufficient PBHs. Moreover, the transition out of USR, which is necessary for a successful model, suppresses even this small enhancement unless it causes a large increase in the inflaton kinetic energy in a fraction of an e-fold, which we call a large and fast transition. Along the way we apply the in-in formalism, the delta N formalism, and gauge transformations to compute non-Gaussianities and illuminate different aspects of the physical origin of these results. Local non-Gaussianity in the squeezed limit does not weaken the Gaussian conclusion that PBHs as dark matter in canonical single-field inflation require a complicated and fine-tuned potential shape with an epoch where slow roll is transiently violated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of canonical single-field inflation, what is the effect of ultra-slow roll (USR) on the abundance of primordial black holes (PBHs) and how does it relate to the requirement for a fine-tuned potential shape?\n\nA) USR enhances the abundance of PBHs by an order unity factor, but only if the transition out of USR is accompanied by a large increase in the inflaton kinetic energy.\n\nB) USR suppresses the abundance of PBHs, but only if the transition out of USR causes a large increase in the inflaton kinetic energy in a fraction of an e-fold.\n\nC) USR has no effect on the abundance of PBHs, as local non-Gaussian correlations between long- and short-wavelength curvature modes do not influence the slow roll regime.\n\nD) USR requires a large and fast transition to suppress the abundance of PBHs, but only if the transition is accompanied by a large increase in the inflaton kinetic energy.\n\nCorrect Answer: B) USR suppresses the abundance of PBHs, but only if the transition out of USR causes a large increase in the inflaton kinetic energy in a fraction of an e-fold.\n\nExplanation: The correct answer is B) USR suppresses the abundance of PBHs, but only if the transition out of USR causes a large increase in the inflaton kinetic energy in a fraction of an e-fold. This is because the transition out of USR suppresses the enhancement of PBH abundance caused by local non-Gaussian correlations, but only if the transition is accompanied by a large increase in the inflaton kinetic energy, which would otherwise cause a large and fast transition. This requires a careful analysis of the interplay between USR, non-Gaussian correlations, and the inflaton kinetic energy."}, "55": {"documentation": {"title": "A large deviation principle linking lineage statistics to fitness in\n  microbial populations", "source": "Ethan Levien, Trevor GrandPre and Ariel Amir", "docs_id": "2002.00019", "section": ["q-bio.PE", "cond-mat.stat-mech", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A large deviation principle linking lineage statistics to fitness in\n  microbial populations. In exponentially proliferating populations of microbes, the population typically doubles at a rate less than the average doubling time of a single-cell due to variability at the single-cell level. It is known that the distribution of generation times obtained from a single lineage is, in general, insufficient to determine a population's growth rate. Is there an explicit relationship between observables obtained from a single lineage and the population growth rate? We show that a population's growth rate can be represented in terms of averages over isolated lineages. This lineage representation is related to a large deviation principle that is a generic feature of exponentially proliferating populations. Due to the large deviation structure of growing populations, the number of lineages needed to obtain an accurate estimate of the growth rate depends exponentially on the duration of the lineages, leading to a non-monotonic convergence of the estimate, which we verify in both synthetic and experimental data sets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In exponentially proliferating microbial populations, what is the relationship between the growth rate of the population and the observables obtained from a single lineage, according to the large deviation principle described in the provided Arxiv documentation?\n\n**A)** The growth rate of the population is directly proportional to the average generation time of a single lineage.\n\n**B)** The growth rate of the population is equal to the average generation time of a single lineage.\n\n**C)** The growth rate of the population can be represented as an average over isolated lineages, and this representation is related to a large deviation principle that is a generic feature of exponentially proliferating populations.\n\n**D)** The growth rate of the population is independent of the observables obtained from a single lineage.\n\n**Correct Answer:** C) The growth rate of the population can be represented as an average over isolated lineages, and this representation is related to a large deviation principle that is a generic feature of exponentially proliferating populations.\n\n**Explanation:** The correct answer is C) because the provided documentation states that \"a population's growth rate can be represented in terms of averages over isolated lineages\" and that this representation is related to a large deviation principle, which is a generic feature of exponentially proliferating populations. The other options are incorrect because they do not accurately reflect the relationship between the growth rate of the population and the observables obtained from a single lineage. Option A implies a direct proportionality, which is not supported by the documentation. Option B implies an equality, which is also not supported. Option D implies independence, which is not mentioned in the documentation."}, "56": {"documentation": {"title": "Two-dimensional connective nanostructures of electrodeposited Zn on\n  Au(111) induced by spinodal decomposition", "source": "J. Dogel, R. Tsekov and W. Freyland", "docs_id": "1506.05206", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-dimensional connective nanostructures of electrodeposited Zn on\n  Au(111) induced by spinodal decomposition. Phase-formation of surface alloying by spinodal decomposition has been studied for the first time at an electrified interface. For this aim Zn was electrodeposited on Au(111) from the ionic liquid AlCl3-MBIC (58:42) containing 1 mM Zn(II) at different potentials in the underpotential range corresponding to submonolayer up to monolayer coverage. Structure evolution was observed by in situ electrochemical scanning tunneling microscopy (STM) at different times after starting the deposition via potential jumps and at temperatures of 298 K and 323 K. Spinodal or labyrinth two-dimensional structures predominate at middle coverage, both in deposition and dissolution experiments. They are characterized by a length scale of typically 5 nm which has been determined from the power spectral density of the STM images. Structure formation and surface alloying is governed by slow kinetics with a rate constant k with activation energy of 120 meV and preexponential factor of 0.17 Hz. The evolution of the structural features is described by a continuum model and is found to be in good agreement with the STM observations. From the experimental and model calculation results we conclude that the two-dimensional phase-formation in the Zn on Au(111) system is dominated by surface alloying. The phase separation of a Zn-rich and a Zn-Au alloy phase is governed by 2D spinodal decomposition."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism governing the two-dimensional phase-formation in the Zn on Au(111) system, as described in the study on spinodal decomposition?\n\nA) Surface diffusion\nB) 2D spinodal decomposition\nC) Grain boundary migration\nD) Ostwald ripening\n\nCorrect Answer: B) 2D spinodal decomposition\n\nExplanation: The study concludes that the two-dimensional phase-formation in the Zn on Au(111) system is dominated by surface alloying, and that the phase separation of a Zn-rich and a Zn-Au alloy phase is governed by 2D spinodal decomposition. This suggests that the primary mechanism is indeed 2D spinodal decomposition, which is a process of phase separation driven by the competition between the chemical potential of the components and the kinetic energy of the system.\n\nCandidate A) Surface diffusion is incorrect because while surface diffusion may play a role in the evolution of the structural features, it is not the primary mechanism governing the phase-formation.\n\nCandidate C) Grain boundary migration is incorrect because the study focuses on the two-dimensional phase-formation at the surface, rather than grain boundary migration.\n\nCandidate D) Ostwald ripening is incorrect because it is a process of phase separation driven by the difference in chemical potential between the components, but it is not the primary mechanism described in the study."}, "57": {"documentation": {"title": "Network induces burst synchronisation in cat brain", "source": "Ewandson L. Lameu, Fernando S. Borges, Rafael R. Borges, Antonio M.\n  Batista, Murilo S. Baptista, Ricardo L. Viana", "docs_id": "1503.02212", "section": ["q-bio.NC", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Network induces burst synchronisation in cat brain. The brain of mammals are divided into different cortical areas that are anatomically connected forming larger networks which perform cognitive tasks. The cat cerebral cortex is composed of 65 areas organised into the visual, auditory, somatosensory-motor and frontolimbic cognitive regions. We have built a network of networks, in which networks are connected among themselves according to the connections observed in the cat cortical areas aiming to study how inputs drive the synchronous behaviour in this cat brain-like network. We show that without external perturbations it is possible to observe high level of bursting synchronisation between neurons within almost all areas, except for the auditory area. Bursting synchronisation appears between neurons in the auditory region when an external perturbation is applied in another cognitive area. This is a clear evidence that pattern formation and collective behaviour in the brain might be a process mediated by other brain areas under stimulation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on network-induced burst synchronisation in the cat brain suggests that the brain's cognitive regions are interconnected in a complex manner. What is the primary conclusion drawn from the research regarding the role of external perturbations in inducing bursting synchronisation in the auditory area?\n\nA) External perturbations are necessary for bursting synchronisation to occur in the auditory area.\nB) External perturbations are sufficient to induce bursting synchronisation in the auditory area, but not necessary.\nC) External perturbations are not required for bursting synchronisation to occur in the auditory area, as it can occur spontaneously.\nD) External perturbations are not necessary for bursting synchronisation to occur in the auditory area, but can enhance its occurrence.\n\n**Correct Answer:** C) External perturbations are not required for bursting synchronisation to occur in the auditory area, as it can occur spontaneously.\n\n**Explanation:** According to the study, bursting synchronisation in the auditory area occurs without external perturbations, indicating that it can occur spontaneously. The research suggests that external perturbations are not necessary for bursting synchronisation to occur in the auditory area, but can enhance its occurrence when applied to other cognitive areas."}, "58": {"documentation": {"title": "Secrecy Performance of Small-Cell Networks with Transmitter Selection\n  and Unreliable Backhaul under Spectrum Sharing Environment", "source": "Jinghua Zhang, Chinmoy Kundu, Octavia A. Dobre, Emi Garcia-Palacios,\n  Nguyen-Son Vo", "docs_id": "2103.04479", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secrecy Performance of Small-Cell Networks with Transmitter Selection\n  and Unreliable Backhaul under Spectrum Sharing Environment. We investigate the secrecy performance of an underlay small-cell cognitive radio network under unreliable backhaul connections. The small-cell network shares the same spectrum with the primary network, ensuring that a desired outage probability constraint is always met in the primary network. {To improve the security of the small-cell cognitive network, we propose three sub-optimal small-cell transmitter selection schemes,} namely sub-optimal transmitter selection, minimal interference selection, and minimal eavesdropping selection. Closed-form expressions of the non-zero secrecy rate, secrecy outage probability, and ergodic secrecy capacity are provided for the schemes along with asymptotic expressions. {We also propose an optimal selection scheme and compare performances with the sub-optimal selection schemes.} {Computable expressions for the non-zero secrecy rate and secrecy outage probability are presented for the optimal selection scheme.} Our results show that by increasing the primary transmitter's power and the number of small-cell transmitters, the system performance improves. The selection scheme, the backhaul reliability, and the primary user quality-of-service constraint also have a significant impact on secrecy performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Secrecy Performance of Small-Cell Networks with Transmitter Selection and Unreliable Backhaul under Spectrum Sharing Environment, what is the primary factor that affects the secrecy performance of the small-cell cognitive network?\n\n**A)** The number of small-cell transmitters\n**B)** The primary transmitter's power and the number of small-cell transmitters\n**C)** The backhaul reliability and the primary user quality-of-service constraint\n**D)** The spectrum sharing environment and the primary network's outage probability constraint\n\n**Correct Answer:** B) The primary transmitter's power and the number of small-cell transmitters\n\n**Explanation:** According to the documentation, the results show that increasing the primary transmitter's power and the number of small-cell transmitters improves the system performance. This suggests that these two factors have a significant impact on the secrecy performance of the small-cell cognitive network. The other options, while related to the topic, are not the primary factor that affects secrecy performance."}, "59": {"documentation": {"title": "Ultraluminous X-ray Source 1E 0953.8+6918 (M81 X-9): An Intermediate\n  Mass Black Hole Candidate and its Environs", "source": "Q. Daniel Wang", "docs_id": "astro-ph/0201230", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ultraluminous X-ray Source 1E 0953.8+6918 (M81 X-9): An Intermediate\n  Mass Black Hole Candidate and its Environs. We present a ROSAT and ASCA study of the Einstein source X-9 and its relation to a shock-heated shell-like optical nebula in a tidal arm of the M81 group of interacting galaxies. Our ASCA observation of the source shows a flat and featureless X-ray spectrum well described by a multi-color disk blackbody model. The source most likely represents an optically thick accretion disk around an intermediate mass black hole in its high/soft state, similar to other variable ultraluminous X-ray sources observed in nearby disk galaxies. Using constraints derived from both the innermost stable orbit around a black hole and the Eddington luminosity, we find that the black hole is fast-rotating and that its mass is between $\\sim 20/({\\rm cos} i) {\\rm M}_{\\odot} - 110/({\\rm cos} i)^{1/2} {\\rm M}_{\\odot}$, where $i$ is the inclination angle of the disk. The inferred bolometric luminosity of the accretion disk is $\\sim (8 \\times 10^{39} {\\rm ergs s^{-1}})/({\\rm cos} i)^{1/2}$. Furthermore, we find that the optical nebula is very energetic and may contain large amounts of hot gas, accounting for a soft X-ray component as indicated by archival ROSAT PSPC data. The nebula is apparently associated with X-9; the latter may be powering the former and/or they could be formed in the same event (e.g., a hypernova). Such a connection, if confirmed, could have strong implications for understanding both the birth of intermediate mass black holes and the formation of energetic interstellar structures."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the most likely interpretation of the observed bolometric luminosity of the accretion disk around the intermediate mass black hole X-9, and how does it relate to the inclination angle of the disk?\n\nA) The bolometric luminosity is a direct measure of the black hole's mass, and its value is independent of the inclination angle.\nB) The bolometric luminosity is proportional to the square of the inclination angle, indicating that the disk is highly inclined and the black hole is highly rotating.\nC) The bolometric luminosity is inversely proportional to the inclination angle, suggesting that the disk is nearly edge-on and the black hole is slowly rotating.\nD) The bolometric luminosity is a function of the black hole's mass and the inclination angle, and its value can be used to constrain the mass and spin of the black hole.\n\nCorrect Answer: D) The bolometric luminosity is a function of the black hole's mass and the inclination angle, and its value can be used to constrain the mass and spin of the black hole.\n\nExplanation: The correct answer is D) because the bolometric luminosity is given as a function of the inclination angle, with the value being proportional to (8 \u00d7 10^39 ergs s^-1) / (cos i)^(1/2). This indicates that the luminosity is dependent on both the black hole's mass and the inclination angle, and can be used to constrain the mass and spin of the black hole. The other options are incorrect because they do not accurately reflect the relationship between the bolometric luminosity and the inclination angle."}}