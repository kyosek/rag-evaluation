{"0": {"documentation": {"title": "Amenability, locally finite spaces, and bi-lipschitz embeddings", "source": "Valerio Capraro", "docs_id": "1205.0346", "section": ["math.MG", "math.CO", "math.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Amenability, locally finite spaces, and bi-lipschitz embeddings. We define the isoperimetric constant for any locally finite metric space and we study the property of having isoperimetric constant equal to zero. This property, called Small Neighborhood property, clearly extends amenability to any locally finite space. Therefore, we start making a comparison between this property and other notions of amenability for locally finite metric spaces that have been proposed by Gromov, Lafontaine and Pansu, by Ceccherini-Silberstein, Grigorchuk and de la Harpe and by Block and Weinberger. We discuss possible applications of the property SN in the study of embedding a metric space into another one. In particular, we propose three results: we prove that a certain class of metric graphs that are isometrically embeddable into Hilbert spaces must have the property SN. We also show, by a simple example, that this result is not true replacing property SN with amenability. As a second result, we prove that \\emph{many} spaces with \\emph{uniform bounded geometry} having a bi-lipschitz embedding into Euclidean spaces must have the property SN. Finally, we prove a Bourgain-like theorem for metric trees: a metric tree with uniform bounded geometry and without property SN does not have bi-lipschitz embeddings into finite-dimensional Hilbert spaces."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following statements is NOT true regarding the Small Neighborhood (SN) property and its relation to embeddings of metric spaces?\n\nA) The SN property is a generalization of amenability for locally finite metric spaces.\n\nB) All metric graphs that are isometrically embeddable into Hilbert spaces must have the SN property.\n\nC) Many spaces with uniform bounded geometry having a bi-lipschitz embedding into Euclidean spaces must have the SN property.\n\nD) A metric tree with uniform bounded geometry and without the SN property cannot have bi-lipschitz embeddings into finite-dimensional Hilbert spaces.\n\nCorrect Answer: B\n\nExplanation: \nA is true: The Small Neighborhood (SN) property, defined as having an isoperimetric constant equal to zero, extends the concept of amenability to any locally finite space.\n\nB is false: The documentation states that \"a certain class of metric graphs\" that are isometrically embeddable into Hilbert spaces must have the SN property, not all metric graphs. This is the correct answer as it's not true for all cases.\n\nC is true: The document explicitly states this as one of the results of their study.\n\nD is true: This is the Bourgain-like theorem for metric trees mentioned in the final part of the documentation.\n\nThe question tests understanding of the key results and their limitations as presented in the Arxiv documentation."}, "1": {"documentation": {"title": "Viable Inflationary Models in a Ghost-free Gauss-Bonnet Theory of\n  Gravity", "source": "Shin'ichi Nojiri, S.D. Odintsov, V.K. Oikonomou, N. Chatzarakis,\n  Tanmoy Paul", "docs_id": "1907.00403", "section": ["gr-qc", "astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Viable Inflationary Models in a Ghost-free Gauss-Bonnet Theory of\n  Gravity. In this work we investigate the inflationary phenomenological implications of a recently developed ghost-free Gauss-Bonnet theory of gravity. The resulting theory can be viewed as a scalar Einstein-Gauss-Bonnet theory of gravity, so by employing the formalism for cosmological perturbations for the latter theory, we calculate the slow-roll indices and the observational indices, and we compare these with the latest observational data. Due to the presence of a freely chosen function in the model, in principle any cosmological evolution can be realized, so we specify the Hubble rate and the freely chosen function and we examine the phenomenology of the model. Specifically we focus on de Sitter, quasi-de Sitter and a cosmological evolution in which the Hubble rate evolves exponentially, with the last two being more realistic choices for describing inflation. As we demonstrate, the ghost-free model can produce inflationary phenomenology compatible with the observational data. We also briefly address the stability of first order scalar and tensor cosmological perturbations, for the exponential Hubble rate, and as we demonstrate, stability is achieved for the same range of values of the free parameters that guarantee the phenomenological viability of the models."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the ghost-free Gauss-Bonnet theory of gravity described in the paper, which of the following statements is correct regarding the inflationary phenomenology and cosmological evolution?\n\nA) The theory can only produce de Sitter inflation models compatible with observational data.\n\nB) The presence of a freely chosen function in the model restricts the possible cosmological evolutions that can be realized.\n\nC) The study focuses on three types of cosmological evolution: de Sitter, quasi-de Sitter, and exponential Hubble rate evolution, with all three being equally realistic for describing inflation.\n\nD) The stability of first order scalar and tensor cosmological perturbations is achieved for the same range of free parameter values that ensure the model's phenomenological viability.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that \"stability is achieved for the same range of values of the free parameters that guarantee the phenomenological viability of the models\" when discussing the stability of first order scalar and tensor cosmological perturbations for the exponential Hubble rate case.\n\nAnswer A is incorrect because the study investigates multiple inflationary models, not just de Sitter.\n\nAnswer B is incorrect because the document specifically mentions that \"due to the presence of a freely chosen function in the model, in principle any cosmological evolution can be realized.\"\n\nAnswer C is incorrect because while the study does focus on these three types of evolution, it states that the quasi-de Sitter and exponential Hubble rate evolutions are \"more realistic choices for describing inflation\" compared to de Sitter."}, "2": {"documentation": {"title": "Scanning tunneling spectroscopy reveals a silicon dangling bond charge\n  state transition", "source": "Hatem Labidi, Marco Taucer, Mohammad Rashidi, Mohammad Koleini, Lucian\n  Livadaru, Jason Pitters, Martin Cloutier, Mark Salomons, Robert A. Wolkow", "docs_id": "1503.00646", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scanning tunneling spectroscopy reveals a silicon dangling bond charge\n  state transition. We report the study of single dangling bonds (DB) on the hydrogen terminated silicon (100) surface using a low temperature scanning tunneling microscope (LT-STM). By investigating samples prepared with different annealing temperatures, we establish the critical role of subsurface arsenic dopants on the DB electronic properties. We show that when the near surface concentration of dopants is depleted as a result of $1250{\\deg}C$ flash anneals, a single DB exhibits a sharp conduction step in its I(V) spectroscopy that is not due to a density of states effect but rather corresponds to a DB charge state transition. The voltage position of this transition is perfectly correlated with bias dependent changes in STM images of the DB at different charge states. Density functional theory (DFT) calculations further highlight the role of subsurface dopants on DB properties by showing the influence of the DB-dopant distance on the DB state. We discuss possible theoretical models of electronic transport through the DB that could account for our experimental observations."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the relationship between subsurface arsenic dopants and the electronic properties of single dangling bonds (DBs) on the hydrogen-terminated silicon (100) surface, as revealed by the LT-STM study?\n\nA) Subsurface arsenic dopants have no significant impact on DB electronic properties, regardless of annealing temperature.\n\nB) High-temperature annealing at 1250\u00b0C increases the near-surface concentration of dopants, leading to a sharp conduction step in DB I(V) spectroscopy.\n\nC) Depletion of near-surface dopants due to high-temperature annealing results in a DB charge state transition, observable as a sharp conduction step in I(V) spectroscopy.\n\nD) The presence of subsurface arsenic dopants always causes a gradual change in DB conductance, independent of annealing conditions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that when the near-surface concentration of dopants is depleted as a result of 1250\u00b0C flash anneals, a single DB exhibits a sharp conduction step in its I(V) spectroscopy. This step corresponds to a DB charge state transition, not a density of states effect. The study establishes the critical role of subsurface arsenic dopants on DB electronic properties, and the depletion of these dopants near the surface (not an increase, as suggested in option B) leads to the observed charge state transition. Options A and D are incorrect as they contradict the findings of the study, which clearly shows the significant impact of dopants on DB properties."}, "3": {"documentation": {"title": "UAV-Assisted Secure Communications in Terrestrial Cognitive Radio\n  Networks: Joint Power Control and 3D Trajectory Optimization", "source": "Phu X. Nguyen, Van-Dinh Nguyen, Hieu V. Nguyen, and Oh-Soon Shin", "docs_id": "2003.09677", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UAV-Assisted Secure Communications in Terrestrial Cognitive Radio\n  Networks: Joint Power Control and 3D Trajectory Optimization. This paper considers secure communications for an underlay cognitive radio network (CRN) in the presence of an external eavesdropper (Eve). The secrecy performance of CRNs is usually limited by the primary receiver's interference power constraint. To overcome this issue, we propose to use an unmanned aerial vehicle (UAV) as a friendly jammer to interfere with Eve in decoding the confidential message from the secondary transmitter (ST). Our goal is to jointly optimize the transmit power and UAV's trajectory in the three-dimensional (3D) space to maximize the average achievable secrecy rate of the secondary system. The formulated optimization problem is nonconvex due to the nonconvexity of the objective and nonconvexity of constraints, which is very challenging to solve. To obtain a suboptimal but efficient solution to the problem, we first transform the original problem into a more tractable form and develop an iterative algorithm for its solution by leveraging the inner approximation framework. We further extend the proposed algorithm to the case of imperfect location information of Eve, where the average worst-case secrecy rate is considered as the objective function. Extensive numerical results are provided to demonstrate the merits of the proposed algorithms over existing approaches."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of UAV-assisted secure communications for terrestrial cognitive radio networks, which of the following statements is NOT true?\n\nA) The proposed system uses a UAV as a friendly jammer to interfere with the eavesdropper's ability to decode confidential messages.\n\nB) The optimization problem involves jointly optimizing the transmit power and the UAV's 3D trajectory to maximize the average achievable secrecy rate.\n\nC) The formulated optimization problem is convex and can be solved directly using standard optimization techniques.\n\nD) The proposed algorithm can be extended to scenarios where the eavesdropper's location information is imperfect.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that the formulated optimization problem is nonconvex due to the nonconvexity of both the objective and constraints. This makes it very challenging to solve and requires special techniques like the iterative algorithm developed in the paper. Options A, B, and D are all true statements according to the given information: the UAV is indeed used as a friendly jammer, the optimization involves joint power and trajectory optimization, and the algorithm can be extended to cases with imperfect eavesdropper location information."}, "4": {"documentation": {"title": "Spectrum-doubled Heavy Vector Bosons at the LHC", "source": "Thomas Appelquist, Yang Bai, James Ingoldby and Maurizio Piai", "docs_id": "1511.05473", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectrum-doubled Heavy Vector Bosons at the LHC. We study a simple effective field theory incorporating six heavy vector bosons together with the standard-model field content. The new particles preserve custodial symmetry as well as an approximate left-right parity symmetry. The enhanced symmetry of the model allows it to satisfy precision electroweak constraints and bounds from Higgs physics in a regime where all the couplings are perturbative and where the amount of fine-tuning is comparable to that in the standard model itself. We find that the model could explain the recently observed excesses in di-boson processes at invariant mass close to 2 TeV from LHC Run 1 for a range of allowed parameter space. The masses of all the particles differ by no more than roughly 10%. In a portion of the allowed parameter space only one of the new particles has a production cross section large enough to be detectable with the energy and luminosity of Run 1, both via its decay to WZ and to Wh, while the others have suppressed production rates. The model can be tested at the higher-energy and higher-luminosity run of the LHC even for an overall scale of the new particles higher than 3 TeV."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the study of spectrum-doubled heavy vector bosons at the LHC, which of the following statements is NOT correct regarding the model's characteristics and predictions?\n\nA) The model incorporates six heavy vector bosons while preserving custodial symmetry and an approximate left-right parity symmetry.\n\nB) The model can explain the di-boson process excesses observed at an invariant mass of about 2 TeV from LHC Run 1, while maintaining perturbative couplings.\n\nC) In some regions of the parameter space, all six new particles are equally detectable with similar production cross sections in LHC Run 1.\n\nD) The model predicts that the masses of all new particles are within approximately 10% of each other.\n\nCorrect Answer: C\n\nExplanation: Option C is incorrect and thus the correct answer to this question. The passage states that \"In a portion of the allowed parameter space only one of the new particles has a production cross section large enough to be detectable with the energy and luminosity of Run 1, while the others have suppressed production rates.\" This contradicts the statement in option C that all six particles are equally detectable.\n\nOptions A, B, and D are all correct according to the given information:\nA) The passage mentions six heavy vector bosons and the preservation of custodial symmetry and approximate left-right parity symmetry.\nB) The model can explain the di-boson excesses at 2 TeV from Run 1 while maintaining perturbative couplings.\nD) The passage states that \"The masses of all the particles differ by no more than roughly 10%.\""}, "5": {"documentation": {"title": "Prediction of Zoonosis Incidence in Human using Seasonal Auto Regressive\n  Integrated Moving Average (SARIMA)", "source": "Adhistya Erna Permanasari, Dayang Rohaya Awang Rambli, Dhanapal Durai\n  Dominic", "docs_id": "0910.0820", "section": ["cs.LG", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prediction of Zoonosis Incidence in Human using Seasonal Auto Regressive\n  Integrated Moving Average (SARIMA). Zoonosis refers to the transmission of infectious diseases from animal to human. The increasing number of zoonosis incidence makes the great losses to lives, including humans and animals, and also the impact in social economic. It motivates development of a system that can predict the future number of zoonosis occurrences in human. This paper analyses and presents the use of Seasonal Autoregressive Integrated Moving Average (SARIMA) method for developing a forecasting model that able to support and provide prediction number of zoonosis human incidence. The dataset for model development was collected on a time series data of human tuberculosis occurrences in United States which comprises of fourteen years of monthly data obtained from a study published by Centers for Disease Control and Prevention (CDC). Several trial models of SARIMA were compared to obtain the most appropriate model. Then, diagnostic tests were used to determine model validity. The result showed that the SARIMA(9,0,14)(12,1,24)12 is the fittest model. While in the measure of accuracy, the selected model achieved 0.062 of Theils U value. It implied that the model was highly accurate and a close fit. It was also indicated the capability of final model to closely represent and made prediction based on the tuberculosis historical dataset."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: A researcher is developing a forecasting model for zoonotic disease incidence using the SARIMA method. Which of the following statements best describes the implications of achieving a Theil's U value of 0.062 for the selected model?\n\nA) The model has moderate accuracy and requires significant improvements before practical application.\nB) The model is highly accurate and closely fits the historical data, indicating strong predictive capability.\nC) The model is overfitting the data and may not generalize well to future predictions.\nD) The Theil's U value suggests the model is unsuitable for zoonotic disease forecasting and alternative methods should be explored.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the passage, \"the selected model achieved 0.062 of Theils U value. It implied that the model was highly accurate and a close fit. It was also indicated the capability of final model to closely represent and made prediction based on the tuberculosis historical dataset.\" A Theil's U value close to 0 indicates high accuracy and good fit of the model to the historical data, suggesting strong predictive capability for future zoonotic disease incidence.\n\nOption A is incorrect because it contradicts the interpretation of the Theil's U value given in the passage. Option C is incorrect because while overfitting is a concern in predictive modeling, the passage does not suggest this is an issue with the achieved Theil's U value. Option D is incorrect as it directly contradicts the positive interpretation of the Theil's U value provided in the text."}, "6": {"documentation": {"title": "Teaching the Old Dog New Tricks: Supervised Learning with Constraints", "source": "Fabrizio Detassis, Michele Lombardi, Michela Milano", "docs_id": "2002.10766", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Teaching the Old Dog New Tricks: Supervised Learning with Constraints. Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven AI systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to ML training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on \"teaching\" constraint satisfaction to a supervised ML method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a na\\\"ive setup of our approach performs well on ML tasks with fairness constraints, and on classical datasets with synthetic constraints."}, "answer": "Here's a multi-form question based on the given text:\n\nQuestion: Which of the following best describes the novel approach to incorporating constraints in Machine Learning as presented in the Arxiv documentation?\n\nA) Applying constrained optimization techniques directly to ML training algorithms\nB) Modifying the ML model architecture to inherently satisfy constraints\nC) Using a decomposition scheme that alternates between a constraint solver and ML training steps\nD) Employing post-processing techniques to correct ML outputs for constraint satisfaction\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document describes a new strategy that uses a \"decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed).\" This approach is different from traditional methods like applying constrained optimization to training (option A), adjusting model design (option B), or correcting outputs (option D). Instead, it integrates a state-of-the-art constraint solver with the ML training process, allowing for the \"teaching\" of constraint satisfaction to the ML method."}, "7": {"documentation": {"title": "Analyzing Monotonic Linear Interpolation in Neural Network Loss\n  Landscapes", "source": "James Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard\n  Zemel, Roger Grosse", "docs_id": "2104.11044", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analyzing Monotonic Linear Interpolation in Neural Network Loss\n  Landscapes. Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. (2014) persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network - providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the implications and limitations of the Monotonic Linear Interpolation (MLI) property in neural networks, as discussed in the research?\n\nA) The MLI property is an inherent characteristic of all neural networks, regardless of architecture or training method.\n\nB) The MLI property suggests that neural network loss landscapes are convex, simplifying our understanding of their optimization process.\n\nC) The MLI property holds universally, but its presence doesn't necessarily indicate a simple or easily navigable loss landscape.\n\nD) The MLI property can be systematically violated by encouraging weights to move far from initialization, suggesting it's not a fundamental property of neural networks.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The passage states that \"while the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization.\" This directly supports option D and indicates that the MLI property is not a universal or fundamental characteristic of all neural networks.\n\nOption A is incorrect because the text shows that the MLI property can be violated under certain conditions, so it's not an inherent characteristic of all neural networks.\n\nOption B is incorrect because the passage explicitly mentions that neural networks have \"non-convex objectives,\" contradicting the idea that the MLI property suggests convexity in loss landscapes.\n\nOption C is partially correct in acknowledging the wide applicability of the MLI property, but it's incorrect in stating that it holds universally, as the research demonstrates ways to violate it.\n\nThe correct answer highlights the nuanced nature of the MLI property and its implications for understanding neural network loss landscapes, aligning with the research's conclusion that further study of global properties is needed."}, "8": {"documentation": {"title": "Spin-dependent scattering induced negative magnetoresistance in\n  topological insulator Bi2Te3 nanowires", "source": "Biplab Bhattacharyya, Bahadur Singh, R. P. Aloysius, Reena Yadav,\n  Chenliang Su, Hsin Lin, S. Auluck, Anurag Gupta, T. D. Senguttuvan, and\n  Sudhir Husale", "docs_id": "1905.10219", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin-dependent scattering induced negative magnetoresistance in\n  topological insulator Bi2Te3 nanowires. Studies of negative magnetoresistance in novel materials have recently been in the forefront of spintronic research. Here, we report an experimental observation of the temperature dependent negative magnetoresistance in Bi2Te3 topological insulator (TI) nanowires at ultralow temperatures (20 mK). We find a crossover from negative to positive magnetoresistance while increasing temperature under longitudinal magnetic field. We observe a large negative magnetoresistance which reaches -22% at 8T. The interplay between negative and positive magnetoresistance can be understood in terms of the competition between dephasing and spin-orbit scattering time scales. Based on the first-principles calculations within a density functional theory framework, we demonstrate that disorder (substitutional) by Ga+ ion milling process, which is used to fabricate nanowires, induces local magnetic moments in Bi2Te3 crystal that can lead to spin-dependent scattering of surface and bulk electrons. These experimental findings show a significant advance in the nanoscale spintronics applications based on longitudinal magnetoresistance in TIs. Our experimental results of large negative longitudinal magnetoresistance in 3D TIs further indicate that axial anomaly is a universal phenomenon in generic 3D metals."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of Bi2Te3 topological insulator nanowires, what phenomenon was observed and what is the proposed explanation for its occurrence?\n\nA) Positive magnetoresistance at all temperatures, caused by increased spin-orbit coupling\nB) Negative magnetoresistance only at high temperatures, due to reduced electron mobility\nC) Temperature-dependent crossover from negative to positive magnetoresistance, explained by competition between dephasing and spin-orbit scattering time scales\nD) Constant magnetoresistance across all temperatures, attributed to the topological protection of surface states\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study reports a temperature-dependent crossover from negative to positive magnetoresistance in Bi2Te3 topological insulator nanowires. At ultralow temperatures (20 mK), a large negative magnetoresistance of up to -22% at 8T was observed. As temperature increased under a longitudinal magnetic field, there was a transition from negative to positive magnetoresistance. \n\nThe researchers explain this phenomenon through the competition between dephasing and spin-orbit scattering time scales. Additionally, they propose that disorder introduced by the Ga+ ion milling process used in nanowire fabrication creates local magnetic moments in the Bi2Te3 crystal. These moments can lead to spin-dependent scattering of surface and bulk electrons, contributing to the observed magnetoresistance behavior.\n\nOption A is incorrect because positive magnetoresistance was not observed at all temperatures. Option B is wrong because negative magnetoresistance was observed at low, not high, temperatures. Option D is incorrect as the magnetoresistance was not constant and showed temperature dependence."}, "9": {"documentation": {"title": "A Stock Selection Method Based on Earning Yield Forecast Using Sequence\n  Prediction Models", "source": "Jessie Sun", "docs_id": "1905.04842", "section": ["cs.NE", "q-fin.MF", "q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Stock Selection Method Based on Earning Yield Forecast Using Sequence\n  Prediction Models. Long-term investors, different from short-term traders, focus on examining the underlying forces that affect the well-being of a company. They rely on fundamental analysis which attempts to measure the intrinsic value an equity. Quantitative investment researchers have identified some value factors to determine the cost of investment for a stock and compare different stocks. This paper proposes using sequence prediction models to forecast a value factor-the earning yield (EBIT/EV) of a company for stock selection. Two advanced sequence prediction models-Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) networks are studied. These two models can overcome the inherent problems of a standard Recurrent Neural Network, i.e., vanishing and exploding gradients. This paper firstly introduces the theories of the networks. And then elaborates the workflow of stock pool creation, feature selection, data structuring, model setup and model evaluation. The LSTM and GRU models demonstrate superior performance of forecast accuracy over a traditional Feedforward Neural Network model. The GRU model slightly outperformed the LSTM model."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the approach and findings of the research on stock selection using sequence prediction models?\n\nA) The study found that traditional Feedforward Neural Networks outperformed LSTM and GRU models in forecasting earning yield.\n\nB) The research focused on short-term trading strategies using technical analysis rather than fundamental analysis.\n\nC) LSTM and GRU models were used to forecast the price-to-earnings ratio, with LSTM slightly outperforming GRU.\n\nD) The study utilized LSTM and GRU models to predict the earning yield (EBIT/EV), with GRU showing marginally better performance than LSTM.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The research described in the documentation focused on using sequence prediction models, specifically Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, to forecast the earning yield (EBIT/EV) for stock selection. The study found that both LSTM and GRU models demonstrated superior performance in forecast accuracy compared to a traditional Feedforward Neural Network. Additionally, the GRU model slightly outperformed the LSTM model.\n\nOption A is incorrect because the study found that LSTM and GRU outperformed traditional Feedforward Neural Networks, not the other way around.\n\nOption B is incorrect as the research focused on long-term investment strategies using fundamental analysis, not short-term trading with technical analysis.\n\nOption C is partially correct in mentioning LSTM and GRU, but it incorrectly states that the study predicted the price-to-earnings ratio instead of the earning yield (EBIT/EV). It also incorrectly states that LSTM outperformed GRU, when the opposite was true."}, "10": {"documentation": {"title": "Study of 3he(e,e') Longitudinal Response Functions with the\n  Integral-Transform Method", "source": "V.Yu. Dobretsov, V.D. Efros and Bin Shao", "docs_id": "nucl-th/9409005", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Study of 3he(e,e') Longitudinal Response Functions with the\n  Integral-Transform Method. The method of integral transforms is first applied for studying the $^3$He longitudinal response functions. The transforms are calculated from localized bound-state-type solutions to an inhomogenous Schr\\\"odinger-type three-body equation. Several versions of local $s$-wave spin-dependent potentials supplemented with a singlet $p$-wave potential and with the proton-proton Coulomb interaction are used as a two-nucleon input. The conventional charge density operator is utilized. The three-body equations are solved with a high acuracy. It is found that the contribution of the $T=3/2$ final states to the problem is suppressed and it amounts about 15\\%. This might be ascribed to symmetry requirements. The contributions of the $p$-wave $NN$ interaction and of the Coulomb interaction are found to amount several per cent. Uncertainty due to different choices of $s$-wave $NN$ forces is of a similar magnitude provided that the low-energy $NN$ data are properly described. The results are compared with the integral transforms of the experimental response functions. For $q=300$ MeV/c experimental and theoretical results coincide within their uncertainties. For $q=500$ MeV/c a noticeable difference is detected."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: In the study of 3He longitudinal response functions using the integral-transform method, which of the following statements is NOT correct?\n\nA) The contribution of T=3/2 final states to the problem is approximately 15%\nB) The p-wave NN interaction and Coulomb interaction contributions amount to several percent\nC) For q=300 MeV/c, experimental and theoretical results show significant discrepancies\nD) The method utilizes localized bound-state-type solutions to an inhomogeneous Schr\u00f6dinger-type three-body equation\n\nCorrect Answer: C\n\nExplanation: \nA is correct as the text states \"the contribution of the T=3/2 final states to the problem is suppressed and it amounts about 15%.\"\nB is correct as the document mentions \"The contributions of the p-wave NN interaction and of the Coulomb interaction are found to amount several per cent.\"\nC is incorrect. The text actually states that \"For q=300 MeV/c experimental and theoretical results coincide within their uncertainties.\"\nD is correct as the passage mentions \"The transforms are calculated from localized bound-state-type solutions to an inhomogenous Schr\u00f6dinger-type three-body equation.\"\n\nThe question tests the student's ability to carefully read and interpret scientific information, distinguishing between correct and incorrect statements based on the given text."}, "11": {"documentation": {"title": "Multi-Period Trading via Convex Optimization", "source": "Stephen Boyd, Enzo Busseti, Steven Diamond, Ronald N. Kahn, Kwangmoo\n  Koh, Peter Nystrup, Jan Speth", "docs_id": "1705.00109", "section": ["q-fin.PM", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Period Trading via Convex Optimization. We consider a basic model of multi-period trading, which can be used to evaluate the performance of a trading strategy. We describe a framework for single-period optimization, where the trades in each period are found by solving a convex optimization problem that trades off expected return, risk, transaction cost and holding cost such as the borrowing cost for shorting assets. We then describe a multi-period version of the trading method, where optimization is used to plan a sequence of trades, with only the first one executed, using estimates of future quantities that are unknown when the trades are chosen. The single-period method traces back to Markowitz; the multi-period methods trace back to model predictive control. Our contribution is to describe the single-period and multi-period methods in one simple framework, giving a clear description of the development and the approximations made. In this paper we do not address a critical component in a trading algorithm, the predictions or forecasts of future quantities. The methods we describe in this paper can be thought of as good ways to exploit predictions, no matter how they are made. We have also developed a companion open-source software library that implements many of the ideas and methods described in the paper."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of multi-period trading via convex optimization, which of the following statements best describes the relationship between the single-period and multi-period methods?\n\nA) The single-period method is derived from the multi-period method and simplifies it for short-term trading.\n\nB) The multi-period method uses a sequence of independent single-period optimizations without considering future periods.\n\nC) The single-period method traces back to model predictive control, while the multi-period method is based on Markowitz's work.\n\nD) Both methods are described within a unified framework, with the multi-period method extending the single-period approach to plan a sequence of trades.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation explicitly states that the authors' contribution is to \"describe the single-period and multi-period methods in one simple framework.\" This unified approach allows for a clear description of how the multi-period method extends the single-period approach by planning a sequence of trades, while only executing the first one.\n\nAnswer A is incorrect because the single-period method is not derived from the multi-period method; rather, it serves as a foundation for the multi-period approach.\n\nAnswer B is incorrect because the multi-period method does consider future periods, using \"estimates of future quantities that are unknown when the trades are chosen.\"\n\nAnswer C incorrectly reverses the historical origins of the methods. The documentation states that the single-period method traces back to Markowitz, while the multi-period methods trace back to model predictive control."}, "12": {"documentation": {"title": "A TGAS/Gaia DR1 parallactic distance to the sigma Orionis cluster", "source": "J. A. Caballero", "docs_id": "1702.06046", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A TGAS/Gaia DR1 parallactic distance to the sigma Orionis cluster. With the new Tycho-Gaia Astrometric Solution, I derive a new parallactic distance to the young sigma Orionis open cluster, which is a cornerstone region for studying the formation and evolution of stars and substellar objects from tens of solar masses to a few Jupiter masses. I started with the list of the 46 brightest cluster stars of Caballero (2007). After identifying the 24 TGAS stars in the 30 arcmin-radius survey area and accounting for 11 FGKM-type dwarfs and giants in the fore- and background, I got a list of 13 cluster members and candidates with new parallaxes. Of them, I discarded five cluster member candidates with questionable features of youth and/or discordant parallaxes and proper motions, including a distant Herbig Ae/Be star, and proceeded with the remaining eight stars. The sigma Orionis mean heliocentric distance is d =360^{+44}_{-35} pc, which is consistent with a contemporaneous interferometric determination that concludes a two-decade dispute on the cluster distance. As a by-product, the re-classification of those five cluster member candidates, now interlopers, shows a manifest deficit of cluster stars between 1.2 and 2.1 Msol, which leaves the door open to new astrometric membership analyses with the next Gaia data releases."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the study of the \u03c3 Orionis cluster using TGAS/Gaia DR1 data, what critical step did the researcher take to refine the sample of stars used for distance calculation, and what was the final number of stars used?\n\nA) Removed all FGKM-type dwarfs and giants, leaving 35 stars for analysis\nB) Discarded 5 cluster member candidates with questionable youth features and/or discordant parallaxes, using 8 stars for final calculation\nC) Included all 46 brightest cluster stars from Caballero (2007) in the final analysis\nD) Eliminated all stars outside a 30 arcmin-radius survey area, resulting in 13 stars for distance determination\n\nCorrect Answer: B\n\nExplanation: The researcher started with 46 brightest cluster stars, identified 24 TGAS stars within a 30 arcmin-radius, and after accounting for fore- and background stars, had a list of 13 cluster members and candidates. Critically, they then discarded 5 cluster member candidates with questionable features of youth and/or discordant parallaxes and proper motions. This refinement step left 8 stars for the final distance calculation. This process demonstrates the careful selection criteria used to ensure the most accurate distance determination for the \u03c3 Orionis cluster."}, "13": {"documentation": {"title": "Query Complexity of Mastermind Variants", "source": "Aaron Berger, Christopher Chute, and Matthew Stone", "docs_id": "1607.04597", "section": ["math.CO", "cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Query Complexity of Mastermind Variants. We study variants of Mastermind, a popular board game in which the objective is sequence reconstruction. In this two-player game, the so-called \\textit{codemaker} constructs a hidden sequence $H = (h_1, h_2, \\ldots, h_n)$ of colors selected from an alphabet $\\mathcal{A} = \\{1,2,\\ldots, k\\}$ (\\textit{i.e.,} $h_i\\in\\mathcal{A}$ for all $i\\in\\{1,2,\\ldots, n\\}$). The game then proceeds in turns, each of which consists of two parts: in turn $t$, the second player (the \\textit{codebreaker}) first submits a query sequence $Q_t = (q_1, q_2, \\ldots, q_n)$ with $q_i\\in \\mathcal{A}$ for all $i$, and second receives feedback $\\Delta(Q_t, H)$, where $\\Delta$ is some agreed-upon function of distance between two sequences with $n$ components. The game terminates when $Q_t = H$, and the codebreaker seeks to end the game in as few turns as possible. Throughout we let $f(n,k)$ denote the smallest integer such that the codebreaker can determine any $H$ in $f(n,k)$ turns. We prove three main results: First, when $H$ is known to be a permutation of $\\{1,2,\\ldots, n\\}$, we prove that $f(n, n)\\ge n - \\log\\log n$ for all sufficiently large $n$. Second, we show that Knuth's Minimax algorithm identifies any $H$ in at most $nk$ queries. Third, when feedback is not received until all queries have been submitted, we show that $f(n,k)=\\Omega(n\\log k)$."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In a variant of Mastermind where the hidden sequence H is known to be a permutation of {1,2,...,n}, and n is sufficiently large, which of the following best describes the lower bound for f(n,n), the minimum number of turns needed for the codebreaker to determine H?\n\nA) f(n,n) \u2265 n - log n\nB) f(n,n) \u2265 n - log log n\nC) f(n,n) \u2265 n / log n\nD) f(n,n) \u2265 n - \u221an\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the first main result stated in the document. The correct answer is B, as the document explicitly states \"when H is known to be a permutation of {1,2,...,n}, we prove that f(n,n) \u2265 n - log log n for all sufficiently large n.\" This lower bound is tighter than options A and D, which overestimate the number of turns needed. Option C provides a different form of lower bound that is not mentioned in the given information and would generally be a weaker bound for large n. The \"log log n\" term in the correct answer reflects a subtle but important aspect of the game's complexity when H is a permutation."}, "14": {"documentation": {"title": "fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully\n  Sampled Multi-Coil MRI Data", "source": "Ruiyang Zhao, Burhaneddin Yaman, Yuxin Zhang, Russell Stewart, Austin\n  Dixon, Florian Knoll, Zhengnan Huang, Yvonne W. Lui, Michael S. Hansen,\n  Matthew P. Lungren", "docs_id": "2109.03812", "section": ["eess.IV", "cs.CV", "cs.LG", "physics.med-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully\n  Sampled Multi-Coil MRI Data. Improving speed and image quality of Magnetic Resonance Imaging (MRI) via novel reconstruction approaches remains one of the highest impact applications for deep learning in medical imaging. The fastMRI dataset, unique in that it contains large volumes of raw MRI data, has enabled significant advances in accelerating MRI using deep learning-based reconstruction methods. While the impact of the fastMRI dataset on the field of medical imaging is unquestioned, the dataset currently lacks clinical expert pathology annotations, critical to addressing clinically relevant reconstruction frameworks and exploring important questions regarding rendering of specific pathology using such novel approaches. This work introduces fastMRI+, which consists of 16154 subspecialist expert bounding box annotations and 13 study-level labels for 22 different pathology categories on the fastMRI knee dataset, and 7570 subspecialist expert bounding box annotations and 643 study-level labels for 30 different pathology categories for the fastMRI brain dataset. The fastMRI+ dataset is open access and aims to support further research and advancement of medical imaging in MRI reconstruction and beyond."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the primary contribution of the fastMRI+ dataset to the field of medical imaging research?\n\nA) It introduces a new deep learning algorithm for MRI reconstruction\nB) It provides raw MRI data for knee and brain scans\nC) It adds expert clinical pathology annotations to existing fastMRI data\nD) It presents a novel method for accelerating MRI scan times\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The fastMRI+ dataset's primary contribution is adding expert clinical pathology annotations to the existing fastMRI data. This is evident from the description which states that fastMRI+ consists of \"16154 subspecialist expert bounding box annotations and 13 study-level labels for 22 different pathology categories on the fastMRI knee dataset, and 7570 subspecialist expert bounding box annotations and 643 study-level labels for 30 different pathology categories for the fastMRI brain dataset.\"\n\nOption A is incorrect because the passage doesn't mention introducing a new deep learning algorithm. While the fastMRI dataset has enabled advances in deep learning-based reconstruction methods, this is not the primary contribution of fastMRI+.\n\nOption B is incorrect because providing raw MRI data was a feature of the original fastMRI dataset, not the new fastMRI+.\n\nOption D is incorrect because while the fastMRI dataset aims to improve MRI speed, the passage doesn't mention that fastMRI+ introduces a new method for accelerating MRI scan times.\n\nThe key point is that fastMRI+ adds clinical expert pathology annotations to address \"clinically relevant reconstruction frameworks and exploring important questions regarding rendering of specific pathology using such novel approaches.\""}, "15": {"documentation": {"title": "Average Time Spent by Levy Flights and Walks on an Interval with\n  Absorbing Boundaries", "source": "S. V. Buldyrev, S. Havlin, A. Ya. Kazakov, M. G. E. da Luz, E. P.\n  Raposo, H. E. Stanley, G. M. Viswanathan", "docs_id": "cond-mat/0012513", "section": ["cond-mat.soft", "cond-mat.dis-nn", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Average Time Spent by Levy Flights and Walks on an Interval with\n  Absorbing Boundaries. We consider a Levy flyer of order alpha that starts from a point x0 on an interval [O,L] with absorbing boundaries. We find a closed-form expression for the average number of flights the flyer takes and the total length of the flights it travels before it is absorbed. These two quantities are equivalent to the mean first passage times for Levy flights and Levy walks, respectively. Using fractional differential equations with a Riesz kernel, we find exact analytical expressions for both quantities in the continuous limit. We show that numerical solutions for the discrete Levy processes converge to the continuous approximations in all cases except the case of alpha approaching 2 and the cases of x0 near absorbing boundaries. For alpha larger than 2 when the second moment of the flight length distribution exists, our result is replaced by known results of classical diffusion. We show that if x0 is placed in the vicinity of absorbing boundaries, the average total length has a minimum at alpha=1, corresponding to the Cauchy distribution. We discuss the relevance of this result to the problem of foraging, which has received recent attention in the statistical physics literature."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A Levy flyer of order \u03b1 starts from a point x0 on an interval [0,L] with absorbing boundaries. Which of the following statements is correct regarding the average total length of flights before absorption?\n\nA) The average total length is minimized when \u03b1 = 2 for all starting positions x0.\nB) For \u03b1 > 2, the results are identical to those of classical diffusion for all x0.\nC) When x0 is near the absorbing boundaries, the average total length has a minimum at \u03b1 = 1.\nD) The average total length is always proportional to the number of flights, regardless of \u03b1.\n\nCorrect Answer: C\n\nExplanation:\nA) is incorrect because the documentation doesn't state that \u03b1 = 2 minimizes the average total length for all starting positions.\nB) is incorrect because while the results are replaced by classical diffusion for \u03b1 > 2, this is not true for all x0, especially near boundaries.\nC) is correct according to the documentation, which states \"We show that if x0 is placed in the vicinity of absorbing boundaries, the average total length has a minimum at alpha=1, corresponding to the Cauchy distribution.\"\nD) is incorrect because the average total length and the number of flights are separate quantities, equivalent to mean first passage times for Levy walks and Levy flights respectively, and their relationship depends on \u03b1."}, "16": {"documentation": {"title": "Deep excursion beyond the proton dripline. II. Toward the limits of\n  existence of nuclear structure", "source": "L.V. Grigorenko, I. Mukha, D. Kostyleva, C. Scheidenberger, L. Acosta,\n  E. Casarejos, V. Chudoba, A.A. Ciemny, W. Dominik, J.A. Due\\~nas, V. Dunin,\n  J. M. Espino, A. Estrad\\'e, F. Farinon, A. Fomichev, H. Geissel, A. Gorshkov,\n  Z. Janas, G. Kami\\'nski, O. Kiselev, R. Kn\\\"obel, S. Krupko, M. Kuich, Yu.A.\n  Litvinov, G. Marquinez-Dur\\'an, I. Martel, C. Mazzocchi, E.Yu. Nikolskii, C.\n  Nociforo, A. K. Ord\\'uz, M. Pf\\\"utzner, S. Pietri, M. Pomorski, A. Prochazka,\n  S. Rymzhanova, A.M. S\\'anchez-Ben\\'itez, P. Sharov, H. Simon, B. Sitar, R.\n  Slepnev, M. Stanoiu, P. Strmen, I. Szarka, M. Takechi, Y.K. Tanaka, H. Weick,\n  M. Winkler, J.S. Winfield, X. Xu, and M.V. Zhukov", "docs_id": "1804.01887", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep excursion beyond the proton dripline. II. Toward the limits of\n  existence of nuclear structure. Prospects of experimental studies of argon and chlorine isotopes located far beyond the proton dripline are studied by using systematics and cluster models. The deviations from the widespread systematics observed in $^{28,29}$Cl and $^{29,30}$Ar have been theoretically substantiated, and analogous deviations predicted for the lighter chlorine and argon isotopes. The limits of nuclear structure existence are predicted for Ar and Cl isotopic chains, with $^{26}$Ar and $^{25}$Cl found to be the lightest sufficiently long-living nuclear systems. By simultaneous measurements of protons and $\\gamma$-rays following decays of such systems as well as their $\\beta$-delayed emission, an interesting synergy effect may be achieved, which is demonstrated by the example of $^{30}$Cl and $^{31}$Ar ground state studies. Such synergy effect may be provided by the new EXPERT setup (EXotic Particle Emission and Radioactivity by Tracking), being operated inside the fragment separator and spectrometer facility at GSI, Darmstadt."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements best describes the findings and predictions regarding the limits of nuclear structure existence for argon and chlorine isotopes?\n\nA) 27Ar and 26Cl are predicted to be the lightest sufficiently long-living nuclear systems in their respective isotopic chains.\n\nB) The widespread systematics observed in 28,29Cl and 29,30Ar have been theoretically refuted, with no deviations expected for lighter isotopes.\n\nC) 26Ar and 25Cl are predicted to be the lightest sufficiently long-living nuclear systems, with deviations from widespread systematics theoretically substantiated for 28,29Cl and 29,30Ar and predicted for lighter isotopes.\n\nD) The limits of nuclear structure existence cannot be predicted for Ar and Cl isotopic chains due to insufficient experimental data.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the key points from the document. The text states that \"26Ar and 25Cl found to be the lightest sufficiently long-living nuclear systems.\" It also mentions that \"The deviations from the widespread systematics observed in 28,29Cl and 29,30Ar have been theoretically substantiated, and analogous deviations predicted for the lighter chlorine and argon isotopes.\" This information directly supports option C, making it the most comprehensive and accurate statement among the given choices."}, "17": {"documentation": {"title": "Non-steady Accretion in Protostars", "source": "Zhaohuan Zhu, Lee Hartmann, Charles Gammie", "docs_id": "0811.1762", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-steady Accretion in Protostars. Observations indicate that mass accretion rates onto low-mass protostars are generally lower than the rates of infall to their disks; this suggests that much of the protostellar mass must be accreted during rare, short outbursts of rapid accretion. We explore when protostellar disk accretion is likely to be highly variable. While constant $\\alpha$ disks can in principle adjust their accretion rates to match infall rates, protostellar disks are unlikely to have constant $\\alpha$. In particular we show that neither models with angular momentum ransport due solely to the magnetorotational instability (MRI) nor ravitational instability (GI) are likely to transport disk mass at rotostellar infall rates over the large range of radii needed to move infalling envelope material down to the central protostar. We show that the MRI and GI are likely to combine to produce outbursts of rapid accretion starting at a few AU. Our analysis is consistent with the time-dependent models of Armitage, Livio, & Pringle (2001) and agrees with our observational study of the outbursting object FU Ori."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best explains the discrepancy between observed mass accretion rates onto low-mass protostars and the rates of infall to their disks, and provides a mechanism for resolving this inconsistency?\n\nA) Constant \u03b1 disks can easily adjust their accretion rates to match infall rates, resulting in steady accretion over time.\n\nB) The magnetorotational instability (MRI) alone is sufficient to transport disk mass at protostellar infall rates across all disk radii.\n\nC) Gravitational instability (GI) is the primary mechanism for maintaining constant accretion rates from the outer disk to the protostar.\n\nD) The combination of MRI and GI likely produces outbursts of rapid accretion starting at a few AU, leading to episodic periods of high accretion rates.\n\nCorrect Answer: D\n\nExplanation: The question addresses the core issue presented in the text: the discrepancy between observed lower mass accretion rates onto protostars compared to the higher infall rates to their disks. The correct answer, D, accurately reflects the conclusion drawn in the text that the combination of MRI and GI is likely to produce outbursts of rapid accretion, which explains how protostars can accumulate mass through episodic high-accretion events rather than steady accretion.\n\nOption A is incorrect because the text states that protostellar disks are unlikely to have constant \u03b1, and constant \u03b1 disks are not the solution to the discrepancy.\n\nOption B is wrong because the text explicitly states that MRI alone is unlikely to transport disk mass at protostellar infall rates over the large range of radii needed.\n\nOption C is incorrect as the text does not suggest that GI alone is sufficient to maintain constant accretion rates; rather, it indicates that neither MRI nor GI alone can explain the observed phenomenon.\n\nThe correct answer integrates the key points from the text, addressing both the observed discrepancy and the proposed mechanism for resolving it through episodic accretion events."}, "18": {"documentation": {"title": "Continuous Time Random Walk with correlated waiting times. The crucial\n  role of inter-trade times in volatility clustering", "source": "Jaros{\\l}aw Klamut and Tomasz Gubiec", "docs_id": "1909.04986", "section": ["q-fin.ST", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Continuous Time Random Walk with correlated waiting times. The crucial\n  role of inter-trade times in volatility clustering. In many physical, social or economical phenomena we observe changes of a studied quantity only in discrete, irregularly distributed points in time. The stochastic process used by physicists to describe this kind of variables is the Continuous Time Random Walk (CTRW). Despite the popularity of this type of stochastic processes and strong empirical motivation, models with a long-term memory within the sequence of time intervals between observations are missing. Here, we fill this gap by introducing a new family of CTRWs. The memory is introduced to the model by the assumption that many consecutive time intervals can be the same. Surprisingly, in this process we can observe a slowly decaying nonlinear autocorrelation function without a fat-tailed distribution of time intervals. Our model applied to high-frequency stock market data can successfully describe the slope of decay of nonlinear autocorrelation function of stock market returns. The model achieves this result with no dependence between consecutive price changes. It proves the crucial role of inter-event times in the volatility clustering phenomenon observed in all stock markets."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of the Continuous Time Random Walk (CTRW) model described in the passage, which of the following statements best explains the novel contribution of the proposed model and its implications for understanding stock market behavior?\n\nA) The model introduces fat-tailed distribution of time intervals, leading to volatility clustering without correlated waiting times.\n\nB) The model demonstrates that price changes must be dependent for volatility clustering to occur in stock markets.\n\nC) The model shows that long-term memory in inter-trade times can produce nonlinear autocorrelation decay in returns without requiring dependent price changes or fat-tailed time interval distributions.\n\nD) The model proves that volatility clustering is solely a result of the frequency of trades, independent of the timing between trades.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the passage describes a new family of CTRWs that introduces long-term memory in the sequence of time intervals between observations. This model can produce slowly decaying nonlinear autocorrelation functions without requiring a fat-tailed distribution of time intervals. Importantly, it can describe the decay of nonlinear autocorrelation in stock market returns without assuming dependence between consecutive price changes. This highlights the crucial role of inter-event times in volatility clustering.\n\nOption A is incorrect because the model specifically achieves its results without a fat-tailed distribution of time intervals. \n\nOption B is wrong because the model demonstrates that volatility clustering can occur without dependent price changes.\n\nOption D is incorrect as it oversimplifies the model's implications. The model emphasizes the importance of the timing between trades (inter-event times), not just their frequency."}, "19": {"documentation": {"title": "A Spiking Neural Network (SNN) for detecting High Frequency Oscillations\n  (HFOs) in the intraoperative ECoG", "source": "Karla Burelo and Mohammadali Sharifshazileh and Niklaus Krayenb\\\"uhl\n  and Georgia Ramantani and Giacomo Indiveri and Johannes Sarnthein", "docs_id": "2011.08783", "section": ["eess.SP", "cs.NE", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Spiking Neural Network (SNN) for detecting High Frequency Oscillations\n  (HFOs) in the intraoperative ECoG. To achieve seizure freedom, epilepsy surgery requires the complete resection of the epileptogenic brain tissue. In intraoperative ECoG recordings, high frequency oscillations (HFOs) generated by epileptogenic tissue can be used to tailor the resection margin. However, automatic detection of HFOs in real-time remains an open challenge. Here we present a spiking neural network (SNN) for automatic HFO detection that is optimally suited for neuromorphic hardware implementation. We trained the SNN to detect HFO signals measured from intraoperative ECoG on-line, using an independently labeled dataset. We targeted the detection of HFOs in the fast ripple frequency range (250-500 Hz) and compared the network results with the labeled HFO data. We endowed the SNN with a novel artifact rejection mechanism to suppress sharp transients and demonstrate its effectiveness on the ECoG dataset. The HFO rates (median 6.6 HFO/min in pre-resection recordings) detected by this SNN are comparable to those published in the dataset (58 min, 16 recordings). The postsurgical seizure outcome was \"predicted\" with 100% accuracy for all 8 patients. These results provide a further step towards the construction of a real-time portable battery-operated HFO detection system that can be used during epilepsy surgery to guide the resection of the epileptogenic zone."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the novel contribution and potential impact of the Spiking Neural Network (SNN) described in this study for detecting High Frequency Oscillations (HFOs) in epilepsy surgery?\n\nA) It's the first neural network to ever detect HFOs in ECoG recordings.\nB) It incorporates a new artifact rejection mechanism and is optimized for neuromorphic hardware implementation, potentially enabling real-time portable HFO detection during surgery.\nC) It achieved 100% accuracy in detecting HFOs, surpassing all previous methods.\nD) It introduced a new frequency range for HFO detection, expanding beyond the traditional fast ripple range.\n\nCorrect Answer: B\n\nExplanation: \nOption B is correct because it accurately captures the key innovations and potential applications of the described SNN. The study mentions a \"novel artifact rejection mechanism\" and emphasizes that the SNN is \"optimally suited for neuromorphic hardware implementation.\" These features, combined with its ability to detect HFOs in real-time, suggest its potential use in a \"real-time portable battery-operated HFO detection system\" for guiding epilepsy surgery.\n\nOption A is incorrect because the study doesn't claim this is the first neural network to detect HFOs.\n\nOption C is inaccurate. While the study mentions 100% accuracy in \"predicting\" postsurgical seizure outcomes for 8 patients, it doesn't claim perfect HFO detection accuracy.\n\nOption D is wrong because the study targeted the established fast ripple frequency range (250-500 Hz) for HFOs, not a new range."}, "20": {"documentation": {"title": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms", "source": "Karl Michel Koerich, Mohammad Esmaeilpour, Sajjad Abdoli, Alceu de\n  Souza Britto Jr., Alessandro Lameiras Koerich", "docs_id": "1910.10106", "section": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms. This paper shows the susceptibility of spectrogram-based audio classifiers to adversarial attacks and the transferability of such attacks to audio waveforms. Some commonly used adversarial attacks to images have been applied to Mel-frequency and short-time Fourier transform spectrograms, and such perturbed spectrograms are able to fool a 2D convolutional neural network (CNN). Such attacks produce perturbed spectrograms that are visually imperceptible by humans. Furthermore, the audio waveforms reconstructed from the perturbed spectrograms are also able to fool a 1D CNN trained on the original audio. Experimental results on a dataset of western music have shown that the 2D CNN achieves up to 81.87% of mean accuracy on legitimate examples and such performance drops to 12.09% on adversarial examples. Likewise, the 1D CNN achieves up to 78.29% of mean accuracy on original audio samples and such performance drops to 27.91% on adversarial audio waveforms reconstructed from the perturbed spectrograms."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the key findings of the research on cross-representation transferability of adversarial attacks in audio classification?\n\nA) Adversarial attacks on spectrograms are effective against 2D CNNs but do not transfer to 1D CNNs operating on audio waveforms.\n\nB) The study found that adversarial attacks on spectrograms are only effective when using Mel-frequency spectrograms, not short-time Fourier transform spectrograms.\n\nC) Adversarial attacks on spectrograms can fool both 2D CNNs operating on spectrograms and 1D CNNs operating on reconstructed audio waveforms, with significant drops in classification accuracy for both.\n\nD) The research showed that adversarial attacks on spectrograms are easily detectable by human observers and cause significant audio distortions in reconstructed waveforms.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the research demonstrates that adversarial attacks applied to spectrograms (both Mel-frequency and short-time Fourier transform) can fool 2D CNNs, and when these perturbed spectrograms are converted back to audio waveforms, they can also fool 1D CNNs trained on original audio. The study shows significant drops in accuracy for both types of networks: from 81.87% to 12.09% for the 2D CNN on spectrograms, and from 78.29% to 27.91% for the 1D CNN on reconstructed audio waveforms.\n\nAnswer A is incorrect because the study explicitly shows that the attacks do transfer to 1D CNNs operating on audio waveforms. \n\nAnswer B is wrong as the study mentions both Mel-frequency and short-time Fourier transform spectrograms were used.\n\nAnswer D is incorrect because the document states that the perturbed spectrograms are \"visually imperceptible by humans,\" implying that the attacks are not easily detectable."}, "21": {"documentation": {"title": "A new nonlocal nonlinear Schroedinger equation and its soliton solutions", "source": "Jianke Yang", "docs_id": "1807.02185", "section": ["nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new nonlocal nonlinear Schroedinger equation and its soliton solutions. A new integrable nonlocal nonlinear Schroedinger (NLS) equation with clear physical motivations is proposed. This equation is obtained from a special reduction of the Manakov system, and it describes Manakov solutions whose two components are related by a parity symmetry. Since the Manakov system governs wave propagation in a wide variety of physical systems, this new nonlocal equation has clear physical meanings. Solitons and multi-solitons in this nonlocal equation are also investigated in the framework of Riemann-Hilbert formulations. Surprisingly, symmetry relations of discrete scattering data for this equation are found to be very complicated, where constraints between eigenvectors in the scattering data depend on the number and locations of the underlying discrete eigenvalues in a very complex manner. As a consequence, general $N$-solitons are difficult to obtain in the Riemann-Hilbert framework. However, one- and two-solitons are derived, and their dynamics investigated. It is found that two-solitons are generally not a nonlinear superposition of one-solitons, and they exhibit interesting dynamics such as meandering and sudden position shifts. As a generalization, other integrable and physically meaningful nonlocal equations are also proposed, which include NLS equations of reverse-time and reverse-space-time types as well as nonlocal Manakov equations of reverse-space, reverse-time and reverse-space-time types."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements about the new nonlocal nonlinear Schr\u00f6dinger equation described in the document is NOT correct?\n\nA) It is derived from a special reduction of the Manakov system and describes Manakov solutions with parity symmetry between components.\n\nB) The symmetry relations of discrete scattering data for this equation are straightforward and easy to characterize.\n\nC) General N-soliton solutions are challenging to obtain using the Riemann-Hilbert framework due to complex constraints in the scattering data.\n\nD) Two-soliton solutions in this equation can exhibit interesting dynamics such as meandering and sudden position shifts.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the document explicitly states that \"symmetry relations of discrete scattering data for this equation are found to be very complicated, where constraints between eigenvectors in the scattering data depend on the number and locations of the underlying discrete eigenvalues in a very complex manner.\" This contradicts the statement in option B that these relations are straightforward and easy to characterize.\n\nOption A is correct according to the document, which states that the equation is \"obtained from a special reduction of the Manakov system, and it describes Manakov solutions whose two components are related by a parity symmetry.\"\n\nOption C is also correct, as the document mentions that \"general N-solitons are difficult to obtain in the Riemann-Hilbert framework\" due to the complex constraints in the scattering data.\n\nOption D is correct as well, with the document noting that two-solitons \"exhibit interesting dynamics such as meandering and sudden position shifts.\""}, "22": {"documentation": {"title": "Data-Driven Distributed Intersection Management for Connected and\n  Automated Vehicles", "source": "Darshan Gadginmath, Pavankumar Tallapragada", "docs_id": "2005.03304", "section": ["math.OC", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data-Driven Distributed Intersection Management for Connected and\n  Automated Vehicles. This work addresses the problem of autonomous traffic management at an isolated intersection for connected and automated vehicles. We decompose the trajectory of each vehicle into two phases: the provisional phase and the coordinated phase. A vehicle, upon entering the region of interest, initially operates in the provisional phase, during which the vehicle optimizes its trajectory but is constrained from entering the intersection. Periodically, all the vehicles in their provisional phase switch to their coordinated phase, which is obtained by coordinated optimization of the sequence of the vehicles' intersection usage as well as their trajectories. For the coordinated phase, we propose a data driven solution, in which the intersection usage sequence is obtained through a data-driven online classification and the trajectories are computed sequentially. This approach also allows for the incorporation of macro information such as traffic arrival rates into the solution. The overall algorithm is provably safe and can be implemented in a distributed manner. Finally, we compare the proposed algorithm against traditional methods of intersection management and against some existing literature through simulations. Through simulations, we also demonstrate that the computation time per vehicle remains constant for the proposed algorithm over a wide range of traffic arrival rates."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the described data-driven distributed intersection management system for connected and automated vehicles, which of the following statements is NOT true regarding the coordinated phase?\n\nA) The intersection usage sequence is determined through data-driven online classification.\nB) The trajectories of vehicles are computed sequentially.\nC) The algorithm allows for the incorporation of macro information such as traffic arrival rates.\nD) All vehicles simultaneously optimize their trajectories through coordinated optimization.\n\nCorrect Answer: D\n\nExplanation:\nA, B, and C are correct statements about the coordinated phase according to the documentation. The coordinated phase does use data-driven online classification for determining the intersection usage sequence, computes trajectories sequentially, and incorporates macro information like traffic arrival rates.\n\nOption D is incorrect. The documentation states that in the coordinated phase, the intersection usage sequence is obtained through data-driven online classification, and then the trajectories are computed sequentially. It does not mention simultaneous optimization of all vehicle trajectories through coordinated optimization during this phase. The coordinated optimization is used to determine the sequence of intersection usage and trajectories, but the actual trajectory computation is done sequentially, not simultaneously for all vehicles.\n\nThis question tests the reader's understanding of the specific details of the coordinated phase in the proposed system, requiring careful attention to the information provided in the documentation."}, "23": {"documentation": {"title": "Emergence of grid-like representations by training recurrent neural\n  networks to perform spatial localization", "source": "Christopher J. Cueva and Xue-Xin Wei", "docs_id": "1803.07770", "section": ["q-bio.NC", "cs.AI", "cs.NE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergence of grid-like representations by training recurrent neural\n  networks to perform spatial localization. Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the significance of the research findings on recurrent neural networks (RNNs) trained for spatial navigation tasks, as presented in the Arxiv documentation?\n\nA) The emergence of grid-like patterns in RNNs proves that the mammalian brain uses identical neural architectures for spatial navigation.\n\nB) The study demonstrates that RNNs can only replicate grid cells, but not other types of spatial correlates found in the Entorhinal Cortex.\n\nC) The research suggests that diverse spatial representations, including grid cells and border cells, may be an efficient solution for encoding space in recurrent neural circuits.\n\nD) The order of emergence of spatial correlates in RNNs contradicts developmental observations in mammalian brains, challenging current neuroscientific theories.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C because the documentation states that the emergence of grid-like patterns, along with other spatial correlates such as border cells and band-like cells in trained RNNs, suggests that these diverse representations may be a natural and efficient solution for representing space in recurrent neural circuits. This aligns with the observed diversity of spatial correlates in the mammalian Entorhinal Cortex.\n\nAnswer A is incorrect because while the RNNs replicated some features observed in mammalian brains, the study does not claim that the neural architectures are identical.\n\nAnswer B is false because the study showed that RNNs could replicate not just grid cells, but also other spatial correlates like border cells and band-like cells.\n\nAnswer D is incorrect because the documentation explicitly states that the order of emergence of grid-like and border cells in the RNNs is consistent with observations from developmental studies, not contradictory."}, "24": {"documentation": {"title": "Complete Subset Averaging with Many Instruments", "source": "Seojeong Lee and Youngki Shin", "docs_id": "1811.08083", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complete Subset Averaging with Many Instruments. We propose a two-stage least squares (2SLS) estimator whose first stage is the equal-weighted average over a complete subset with $k$ instruments among $K$ available, which we call the complete subset averaging (CSA) 2SLS. The approximate mean squared error (MSE) is derived as a function of the subset size $k$ by the Nagar (1959) expansion. The subset size is chosen by minimizing the sample counterpart of the approximate MSE. We show that this method achieves the asymptotic optimality among the class of estimators with different subset sizes. To deal with averaging over a growing set of irrelevant instruments, we generalize the approximate MSE to find that the optimal $k$ is larger than otherwise. An extensive simulation experiment shows that the CSA-2SLS estimator outperforms the alternative estimators when instruments are correlated. As an empirical illustration, we estimate the logistic demand function in Berry, Levinsohn, and Pakes (1995) and find the CSA-2SLS estimate is better supported by economic theory than the alternative estimates."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Complete Subset Averaging (CSA) two-stage least squares (2SLS) estimator, which of the following statements is correct regarding the optimal subset size k?\n\nA) The optimal k is always smaller when dealing with a growing set of irrelevant instruments.\nB) The optimal k is determined by maximizing the sample counterpart of the approximate mean squared error (MSE).\nC) The optimal k remains constant regardless of the number of available instruments K.\nD) The optimal k is typically larger when accounting for a growing set of irrelevant instruments.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of how the optimal subset size k is determined in the CSA-2SLS method, particularly when dealing with irrelevant instruments. The correct answer is D because the documentation states: \"To deal with averaging over a growing set of irrelevant instruments, we generalize the approximate MSE to find that the optimal k is larger than otherwise.\"\n\nOption A is incorrect because it contradicts the given information; the optimal k is larger, not smaller, when dealing with irrelevant instruments.\n\nOption B is incorrect because the optimal k is chosen by minimizing, not maximizing, the sample counterpart of the approximate MSE.\n\nOption C is incorrect because the optimal k is not constant; it depends on various factors, including the presence of irrelevant instruments.\n\nThis question requires a deep understanding of the CSA-2SLS method and how it handles instrument selection, making it suitable for an advanced econometrics exam."}, "25": {"documentation": {"title": "Families of spatial solitons in a two-channel waveguide with the\n  cubic-quintic nonlinearity", "source": "Ze'ev Birnbaum and Boris A. Malomed (Department of Physical\n  Electronics, School of Electrical Engineering, Faculty of Engineering, Tel\n  Aviv University, Tel Aviv, Israel)", "docs_id": "0802.0667", "section": ["nlin.PS", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Families of spatial solitons in a two-channel waveguide with the\n  cubic-quintic nonlinearity. We present eight types of spatial optical solitons which are possible in a model of a planar waveguide that includes a dual-channel trapping structure and competing (cubic-quintic) nonlinearity. Among the families of trapped beams are symmetric and antisymmetric solitons of \"broad\" and \"narrow\" types, composite states, built as combinations of broad and narrow beams with identical or opposite signs (\"unipolar\" and \"bipolar\" states, respectively), and \"single-sided\" broad and narrow beams trapped, essentially, in a single channel. The stability of the families is investigated via eigenvalues of small perturbations, and is verified in direct simulations. Three species - narrow symmetric, broad antisymmetric, and unipolar composite states - are unstable to perturbations with real eigenvalues, while the other five families are stable. The unstable states do not decay, but, instead, spontaneously transform themselves into persistent breathers, which, in some cases, demonstrate dynamical symmetry breaking and chaotic internal oscillations. A noteworthy feature is a stability exchange between the broad and narrow antisymmetric states: in the limit when the two channels merge into one, the former species becomes stable, while the latter one loses its stability. Different branches of the stationary states are linked by four bifurcations, which take different forms in the model with the strong and weak inter-channel coupling."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In a two-channel waveguide with cubic-quintic nonlinearity, which of the following statements accurately describes the stability and behavior of spatial optical solitons?\n\nA) All eight types of spatial optical solitons are stable, with no spontaneous transformations occurring.\n\nB) The narrow symmetric, broad antisymmetric, and unipolar composite states are unstable with imaginary eigenvalues, leading to decay over time.\n\nC) Three unstable species transform into persistent breathers, sometimes exhibiting dynamical symmetry breaking and chaotic internal oscillations.\n\nD) In the limit where the two channels merge, both broad and narrow antisymmetric states become unstable simultaneously.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that \"Three species - narrow symmetric, broad antisymmetric, and unipolar composite states - are unstable to perturbations with real eigenvalues,\" and that these unstable states \"do not decay, but, instead, spontaneously transform themselves into persistent breathers, which, in some cases, demonstrate dynamical symmetry breaking and chaotic internal oscillations.\"\n\nOption A is incorrect because not all eight types are stable; three are explicitly mentioned as unstable.\n\nOption B is incorrect on two counts: the unstable states have real (not imaginary) eigenvalues, and they don't decay but transform into breathers.\n\nOption D is incorrect because when the channels merge, there's a stability exchange: \"the former species [broad antisymmetric] becomes stable, while the latter one [narrow antisymmetric] loses its stability,\" not both becoming unstable."}, "26": {"documentation": {"title": "Optimal execution with dynamic risk adjustment", "source": "Xue Cheng, Marina Di Giacinto, and Tai-Ho Wang", "docs_id": "1901.00617", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal execution with dynamic risk adjustment. This paper considers the problem of optimal liquidation of a position in a risky security in a financial market, where price evolution are risky and trades have an impact on price as well as uncertainty in the filling orders. The problem is formulated as a continuous time stochastic optimal control problem aiming at maximizing a generalized risk-adjusted profit and loss function. The expression of the risk adjustment is derived from the general theory of dynamic risk measures and is selected in the class of $g$-conditional risk measures. The resulting theoretical framework is nonclassical since the target function depends on backward components. We show that, under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed form solution and an explicit expression of the optimal liquidation policies. In this way it is immediate to quantify the impact of risk-adjustment on the profit and loss and on the expression of the optimal liquidation policies."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of optimal liquidation with dynamic risk adjustment, which of the following statements is correct regarding the solution and optimal liquidation policies?\n\nA) The problem is formulated as a discrete time deterministic optimal control problem.\n\nB) The risk adjustment is derived from static risk measures and is not conditional on market dynamics.\n\nC) A closed-form solution and explicit expression of optimal liquidation policies are obtained under a linear specification of the driver of a forward stochastic differential equation.\n\nD) Under a quadratic specification of the driver of a backward stochastic differential equation, a closed-form solution and explicit expression of optimal liquidation policies can be found.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"under a quadratic specification of the driver of a backward stochastic differential equation, it is possible to find a closed form solution and an explicit expression of the optimal liquidation policies.\" This directly corresponds to option D.\n\nOption A is incorrect because the problem is formulated as a continuous time (not discrete) stochastic (not deterministic) optimal control problem.\n\nOption B is incorrect as the risk adjustment is derived from dynamic risk measures, specifically from the class of g-conditional risk measures, not static ones.\n\nOption C is incorrect on multiple counts: it mentions a forward stochastic differential equation (the paper discusses backward SDEs) and a linear specification (the paper mentions a quadratic specification)."}, "27": {"documentation": {"title": "Pricing VIX Derivatives With Free Stochastic Volatility Model", "source": "Wei Lin, Shenghong Li and Shane Chern", "docs_id": "1703.06020", "section": ["q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pricing VIX Derivatives With Free Stochastic Volatility Model. In this paper, we relax the power parameter of instantaneous variance and develop a new stochastic volatility plus jumps model that generalize the Heston model and 3/2 model as special cases. This model has two distinctive features. First, we do not restrict the new parameter, letting the data speak as to its direction. The Generalized Methods of Moments suggests that the newly added parameter is to create varying volatility fluctuation in different period discovered in financial market. Moreover, upward and downward jumps are separately modeled to accommodate the market data. Our model is novel and highly tractable, which means that the quasi-closed-form solutions for future and option prices can be effectively derived. We have employed data on VIX future and corresponding option contracts to test this model to evaluate its ability of performing pricing and capturing features of the implied volatility. To sum up, the free stochastic volatility model with asymmetric jumps is able to adequately capture implied volatility dynamics and thus it can be seen as a superior model relative to the fixed volatility model in pricing VIX derivatives."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the key innovations and advantages of the \"free stochastic volatility model\" presented in the paper?\n\nA) It restricts the power parameter of instantaneous variance to create a more stable model than the Heston model.\n\nB) It combines the Heston model and 3/2 model without any additional parameters or features.\n\nC) It introduces a new parameter that allows for varying volatility fluctuation and incorporates asymmetric jumps, while maintaining tractability for quasi-closed-form solutions.\n\nD) It focuses solely on downward jumps to model market crashes more accurately than existing models.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key innovations described in the paper. The model introduces a new parameter that allows for varying volatility fluctuation, as evidenced by the statement \"The Generalized Methods of Moments suggests that the newly added parameter is to create varying volatility fluctuation in different period discovered in financial market.\" Additionally, the model incorporates both upward and downward jumps separately, as mentioned in \"Moreover, upward and downward jumps are separately modeled to accommodate the market data.\" The model's tractability is also highlighted, with the paper noting that \"quasi-closed-form solutions for future and option prices can be effectively derived.\"\n\nOption A is incorrect because the model actually relaxes the power parameter rather than restricting it.\n\nOption B is incorrect because while the model does generalize the Heston and 3/2 models, it also adds new features and parameters.\n\nOption D is incorrect because the model incorporates both upward and downward jumps, not just downward jumps."}, "28": {"documentation": {"title": "Passive RIS vs.Hybrid RIS: A Comparative Study on Channel Estimation", "source": "Rafaela Schroeder, Jiguang He, Markku Juntti", "docs_id": "2010.06981", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Passive RIS vs.Hybrid RIS: A Comparative Study on Channel Estimation. The reconfigurable intelligent surface (RIS) plays an important role in maintaining the connectivity in millimeter wave (mmWave) MIMO systems when the direct channel between the transceivers is blocked. However, it is difficult to acquire the channel state information (CSI), which is essential for the design of RIS phase control matrix and beamforming vectors at the transceivers. In this paper, we compare the channel estimation (CE) performance and achieved spectral efficiency (SE) of the purely passive and hybrid RIS architectures. CE is done via atomic norm minimization (ANM). For the purely passive RIS, we follow a two-stage procedure to sequentially estimate the channel parameters, while for the hybrid RIS we estimate the individual channels at the RIS based on the observations from active RIS elements assuming alternating uplink and downlink training. The simulation results show that the purely passive RIS brings better CE and SE performance compared to the hybrid RIS under the same training overhead. We further consider different setups for the hybrid RIS and study the tradeoffs among them."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of channel estimation for reconfigurable intelligent surface (RIS) systems, which of the following statements is correct?\n\nA) Hybrid RIS architectures consistently outperform purely passive RIS in terms of channel estimation accuracy and spectral efficiency.\n\nB) The purely passive RIS uses a single-stage procedure for estimating channel parameters, while the hybrid RIS uses a two-stage approach.\n\nC) Channel estimation for purely passive RIS is performed using alternating uplink and downlink training.\n\nD) Under the same training overhead, purely passive RIS demonstrates better channel estimation and spectral efficiency performance compared to hybrid RIS.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. According to the documentation, \"The simulation results show that the purely passive RIS brings better CE and SE performance compared to the hybrid RIS under the same training overhead.\" This directly supports the statement in option D.\n\nOption A is incorrect because the document states the opposite - purely passive RIS outperforms hybrid RIS in terms of channel estimation and spectral efficiency.\n\nOption B is incorrect because it reverses the procedures. The documentation mentions that for purely passive RIS, \"we follow a two-stage procedure to sequentially estimate the channel parameters,\" while for hybrid RIS, the estimation is based on observations from active RIS elements.\n\nOption C is incorrect because the alternating uplink and downlink training is associated with the hybrid RIS, not the purely passive RIS. The document states, \"for the hybrid RIS we estimate the individual channels at the RIS based on the observations from active RIS elements assuming alternating uplink and downlink training.\"\n\nThis question tests the student's understanding of the key differences between purely passive and hybrid RIS architectures in terms of channel estimation procedures and performance."}, "29": {"documentation": {"title": "On the Electric-Magnetic Duality Symmetry: Quantum Anomaly, Optical\n  Helicity, and Particle Creation", "source": "Ivan Agullo, Adrian del Rio and Jose Navarro-Salas", "docs_id": "1812.08211", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Electric-Magnetic Duality Symmetry: Quantum Anomaly, Optical\n  Helicity, and Particle Creation. It is well known that not every symmetry of a classical field theory is also a symmetry of its quantum version. When this occurs, we speak of quantum anomalies. The existence of anomalies imply that some classical Noether charges are no longer conserved in the quantum theory. In this paper, we discuss a new example for quantum electromagnetic fields propagating in the presence of gravity. We argue that the symmetry under electric-magnetic duality rotations of the source-free Maxwell action is anomalous in curved spacetimes. The classical Noether charge associated with these transformations accounts for the net circular polarization or the optical helicity of the electromagnetic field. Therefore, our results describe the way the spacetime curvature changes the helicity of photons and opens the possibility of extracting information from strong gravitational fields through the observation of the polarization of photons. We also argue that the physical consequences of this anomaly can be understood in terms of the asymmetric quantum creation of photons by the gravitational~field."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the context of electric-magnetic duality symmetry in curved spacetimes, which of the following statements is most accurate?\n\nA) The electric-magnetic duality symmetry of Maxwell's equations is always preserved in both classical and quantum theories, regardless of spacetime curvature.\n\nB) The anomaly in electric-magnetic duality symmetry in curved spacetimes leads to the conservation of optical helicity of electromagnetic fields.\n\nC) The quantum anomaly in electric-magnetic duality symmetry in curved spacetimes results in asymmetric creation of photons and changes in their helicity.\n\nD) The classical Noether charge associated with electric-magnetic duality transformations is unaffected by spacetime curvature in quantum electrodynamics.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text states that the symmetry under electric-magnetic duality rotations is anomalous in curved spacetimes, meaning it's not preserved in the quantum theory. This anomaly leads to changes in the helicity of photons due to spacetime curvature and can be understood in terms of asymmetric quantum creation of photons by the gravitational field. \n\nOption A is incorrect because the symmetry is not preserved in quantum theory in curved spacetimes. \n\nOption B is wrong because the anomaly actually leads to the non-conservation of optical helicity, not its conservation. \n\nOption D is incorrect because the classical Noether charge is no longer conserved in the quantum theory due to the anomaly, and it is affected by spacetime curvature."}, "30": {"documentation": {"title": "Forensics Analysis of Xbox One Game Console", "source": "Ali M. Al-Haj", "docs_id": "1904.00734", "section": ["cs.CR", "cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Forensics Analysis of Xbox One Game Console. Games console devices have been designed to be an entertainment system. However, the 8th generation games console have new features that can support criminal activities and investigators need to be aware of them. This paper highlights the forensics value of the Microsoft game console Xbox One, the latest version of their Xbox series. The Xbox One game console provides many features including web browsing, social networking, and chat functionality. From a forensic perspective, all those features will be a place of interest in forensic examinations. However, the available published literature focused on examining the physical hard drive artefacts, which are encrypted and cannot provide deep analysis of the user's usage of the console. In this paper, we carried out an investigation of the Xbox One games console by using two approaches: a physical investigation of the hard drive to identify the valuable file timestamp information and logical examination via the graphical user interface. Furthermore, this paper identifies potential valuable forensic data sources within the Xbox One and provides best practices guidance for collecting data in a forensically sound manner."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements best describes the forensic challenges and approaches for investigating the Xbox One game console, as presented in the research paper?\n\nA) The Xbox One's hard drive is unencrypted, allowing for easy access to user data through physical investigation.\n\nB) The paper focuses solely on logical examination via the graphical user interface, as physical investigation is not possible.\n\nC) Previous literature provides comprehensive analysis of user activities on the Xbox One through examination of encrypted hard drive artifacts.\n\nD) The research combines physical investigation of the hard drive for file timestamp information and logical examination via the graphical user interface.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The paper describes a two-pronged approach to investigating the Xbox One: physical investigation of the hard drive to identify valuable file timestamp information, and logical examination via the graphical user interface. This approach was necessitated by the limitations of previous research, which focused on examining encrypted hard drive artifacts that couldn't provide deep analysis of user activities. \n\nOption A is incorrect because the paper states that the hard drive is encrypted, not unencrypted. \n\nOption B is incorrect as the research uses both physical and logical examination methods, not just logical examination. \n\nOption C is incorrect because the paper explicitly states that previous literature focused on encrypted hard drive artifacts, which cannot provide deep analysis of user activities."}, "31": {"documentation": {"title": "Understanding the Dynamics of Glass-forming Liquids with Random Pinning\n  within the Random First Order Transition Theory", "source": "Saurish Chakrabarty, Rajsekhar Das, Smarajit Karmakar, and Chandan\n  Dasgupta", "docs_id": "1603.04648", "section": ["cond-mat.stat-mech", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the Dynamics of Glass-forming Liquids with Random Pinning\n  within the Random First Order Transition Theory. Extensive computer simulations are performed for a few model glass-forming liquids in both two and three dimensions to study their dynamics when a randomly chosen fraction of particles are frozen in their equilibrium positions. For all the studied systems, we find that the temperature-dependence of the $\\alpha$ relaxation time extracted from an overlap function related to the self part of the density autocorrelation function can be explained within the framework of the Random First Order Transition (RFOT) theory of the glass transition. We propose a scaling description to rationalize the simulation results and show that our data for the $\\alpha$ relaxation time for all temperatures and pin concentrations are consistent with this description. We find that the fragility parameter obtained from fits of the temperature dependence of the $\\alpha$ relaxation time to the Vogel-Fulcher-Tammann (VFT) form decreases by almost an order of magnitude as the pin concentration is increased from zero. Our scaling description relates the fragility parameter to the static length scale of RFOT and thus provides a physical understanding of fragility within the framework of the RFOT theory. Implications of these findings for the values of the exponents appearing in the RFOT theory are discussed."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: According to the study on glass-forming liquids with random pinning, which of the following statements is correct regarding the fragility parameter and its relationship to the Random First Order Transition (RFOT) theory?\n\nA) The fragility parameter increases by an order of magnitude as the pin concentration is increased from zero.\n\nB) The fragility parameter is unrelated to the static length scale of RFOT.\n\nC) The fragility parameter decreases by almost an order of magnitude as the pin concentration is increased from zero, and is related to the static length scale of RFOT.\n\nD) The fragility parameter remains constant regardless of pin concentration, but is inversely proportional to the static length scale of RFOT.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text states that \"the fragility parameter obtained from fits of the temperature dependence of the \u03b1 relaxation time to the Vogel-Fulcher-Tammann (VFT) form decreases by almost an order of magnitude as the pin concentration is increased from zero.\" Additionally, it mentions that \"Our scaling description relates the fragility parameter to the static length scale of RFOT and thus provides a physical understanding of fragility within the framework of the RFOT theory.\"\n\nOption A is incorrect because it states the opposite trend for the fragility parameter. Option B is wrong because the study explicitly relates the fragility parameter to the static length scale of RFOT. Option D is incorrect on both counts: it wrongly claims the fragility parameter remains constant and incorrectly describes its relationship to the static length scale."}, "32": {"documentation": {"title": "Vapor-cell-based atomic electrometry for detection frequencies below kHz", "source": "Yuan-Yu Jau and Tony Carter", "docs_id": "2002.04145", "section": ["physics.atom-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vapor-cell-based atomic electrometry for detection frequencies below kHz. Rydberg-assisted atomic electrometry using alkali-metal atoms contained inside a vacuum environment for detecting external electric fields (E-fields) at frequencies $<$ a few kHz has been quite challenging due to the low-frequency E-field screening effect that is caused by the alkali-metal atoms adsorbed on the inner surface of the container. We report a very slow E-field screening phenomenon with a time scale up to $\\sim$ second on a rubidium (Rb) vapor cell that is made of monocrystalline sapphire. Using this sapphire-made Rb vapor cell with optically induced, internal bias E-field, we demonstrate vapor-cell-based, low-frequency atomic electrometry that responds to the E-field strength linearly. Limited by the given experimental conditions, this demonstrated atomic electrometer uses an active volume of 11 mm$^3$ and delivers a spectral noise floor around $0.34$ (mV/m)/$\\sqrt{\\rm Hz}$ and the 3-dB low cut-off frequency around 770 Hz inside the vapor cell. This work investigates a regime of vapor-cell-based atomic electrometry that was seldom studied before, which may enable more applications that utilize atomic E-field sensing technology."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Rydberg-assisted atomic electrometry using alkali-metal atoms in a vacuum environment, which of the following statements best describes the breakthrough achieved with the sapphire-made rubidium vapor cell?\n\nA) It completely eliminated the low-frequency E-field screening effect caused by adsorbed alkali-metal atoms.\n\nB) It allowed for the detection of electric fields at frequencies above a few kHz without any screening effects.\n\nC) It demonstrated a very slow E-field screening phenomenon with a time scale up to ~second, enabling low-frequency atomic electrometry.\n\nD) It achieved a spectral noise floor of 0.34 (mV/m)/\u221aHz without the need for an optically induced internal bias E-field.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation describes a breakthrough using a sapphire-made rubidium vapor cell that demonstrated \"a very slow E-field screening phenomenon with a time scale up to ~second.\" This slow screening effect, combined with an optically induced internal bias E-field, allowed for the development of vapor-cell-based, low-frequency atomic electrometry that responds linearly to E-field strength. This achievement is significant because detecting E-fields at frequencies below a few kHz had been challenging due to screening effects.\n\nOption A is incorrect because the screening effect was not completely eliminated, but rather slowed down significantly.\n\nOption B is incorrect as the focus of the research was on improving low-frequency detection (below kHz), not high-frequency detection.\n\nOption D is incorrect because while the spectral noise floor of 0.34 (mV/m)/\u221aHz was achieved, the documentation explicitly mentions the use of an optically induced internal bias E-field, which was crucial for the experiment's success."}, "33": {"documentation": {"title": "Effects of CP-violating phases on Higgs boson production at hadron\n  colliders in the Minimal Supersymmetric Standard Model", "source": "A. Dedes (RAL) and S. Moretti (RAL and Uppsala University)", "docs_id": "hep-ph/9909418", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of CP-violating phases on Higgs boson production at hadron\n  colliders in the Minimal Supersymmetric Standard Model. If the soft Supersymmetry (SUSY) breaking masses and couplings are complex, then the associated CP-violating phases can in principle modify the known phenomenological pattern of the Minimal Supersymmetric Standard Model (MSSM). We investigate here their effects on Higgs boson production in the gluon-gluon fusion mode at the Tevatron and the Large Hadron Collider (LHC), by taking into account all experimental bounds available at present. The by far most stringent ones are those derived from the measurements of the Electric Dipole Moments (EDMs) of fermions. However, it has recently been suggested that, over a sizable portion of the MSSM parameter space, cancellations among the SUSY contributions to the EDMs can take place, so that the CP-violating phases can evade those limits. We find a strong dependence of the production rates of any neutral Higgs state upon the complex masses and couplings over such parts of the MSSM parameter space. We show these effects relatively to the ordinary MSSM rates as well as illustrate them at absolute cross section level at both colliders"}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of CP-violating phases in the Minimal Supersymmetric Standard Model (MSSM), which of the following statements is correct?\n\nA) CP-violating phases always lead to a decrease in Higgs boson production rates in gluon-gluon fusion at hadron colliders.\n\nB) The effects of CP-violating phases on Higgs boson production are negligible due to strict constraints from Electric Dipole Moment (EDM) measurements.\n\nC) Cancellations among SUSY contributions to EDMs can allow for significant CP-violating phases, potentially leading to strong variations in Higgs production rates.\n\nD) CP-violating phases in the MSSM primarily affect Higgs production through quark-antiquark annihilation rather than gluon-gluon fusion.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that while EDM measurements typically impose stringent constraints on CP-violating phases, recent research suggests that cancellations among SUSY contributions to EDMs can occur over a sizable portion of the MSSM parameter space. This allows for potentially larger CP-violating phases than previously thought possible. In these regions, the document indicates a strong dependence of Higgs production rates on complex masses and couplings, which can lead to significant variations in production rates compared to the ordinary MSSM.\n\nOption A is incorrect because the document doesn't state that CP-violating phases always decrease production rates; it implies that they can cause significant variations in both directions.\n\nOption B is incorrect because while EDM measurements do impose constraints, the document discusses ways these constraints can be evaded through cancellations.\n\nOption D is incorrect because the document specifically focuses on gluon-gluon fusion as the production mode affected by CP-violating phases, not quark-antiquark annihilation."}, "34": {"documentation": {"title": "High Quality Prediction of Protein Q8 Secondary Structure by Diverse\n  Neural Network Architectures", "source": "Iddo Drori, Isht Dwivedi, Pranav Shrestha, Jeffrey Wan, Yueqi Wang,\n  Yunchu He, Anthony Mazza, Hugh Krogh-Freeman, Dimitri Leggas, Kendal\n  Sandridge, Linyong Nan, Kaveri Thakoor, Chinmay Joshi, Sonam Goenka, Chen\n  Keasar, Itsik Pe'er", "docs_id": "1811.07143", "section": ["cs.LG", "q-bio.QM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High Quality Prediction of Protein Q8 Secondary Structure by Diverse\n  Neural Network Architectures. We tackle the problem of protein secondary structure prediction using a common task framework. This lead to the introduction of multiple ideas for neural architectures based on state of the art building blocks, used in this task for the first time. We take a principled machine learning approach, which provides genuine, unbiased performance measures, correcting longstanding errors in the application domain. We focus on the Q8 resolution of secondary structure, an active area for continuously improving methods. We use an ensemble of strong predictors to achieve accuracy of 70.7% (on the CB513 test set using the CB6133filtered training set). These results are statistically indistinguishable from those of the top existing predictors. In the spirit of reproducible research we make our data, models and code available, aiming to set a gold standard for purity of training and testing sets. Such good practices lower entry barriers to this domain and facilitate reproducible, extendable research."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the approach and outcomes of the protein secondary structure prediction study mentioned in the document?\n\nA) The study achieved a breakthrough accuracy of 80% on the Q8 resolution, significantly outperforming all existing predictors.\n\nB) The research focused solely on developing a single neural architecture that surpassed all previous methods in protein secondary structure prediction.\n\nC) The study utilized diverse neural network architectures and an ensemble approach, achieving 70.7% accuracy on the CB513 test set, which is comparable to top existing predictors.\n\nD) The researchers developed a new task framework that completely revolutionized the field, making all previous prediction methods obsolete.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the key points from the document. The study used \"diverse neural network architectures\" and an \"ensemble of strong predictors\" to achieve an accuracy of 70.7% on the CB513 test set. This result is described as \"statistically indistinguishable from those of the top existing predictors,\" which aligns with the statement in option C.\n\nOption A is incorrect because the document mentions 70.7% accuracy, not 80%, and does not claim to significantly outperform existing predictors.\n\nOption B is incorrect as the study used multiple neural architectures and an ensemble approach, not a single architecture.\n\nOption D is incorrect because while the study introduced new ideas and took a principled approach, it did not claim to revolutionize the field or make previous methods obsolete. Instead, it achieved results comparable to existing top predictors."}, "35": {"documentation": {"title": "Simple model of photo acoustic system for greenhouse effect", "source": "Akiko Fukuhara, Fumitoshi Kaneko, and Naohisa Ogawa", "docs_id": "1012.2513", "section": ["physics.pop-ph", "physics.ed-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simple model of photo acoustic system for greenhouse effect. The green house effect is caused by the gases which absorb infrared ray (IR) emitted by the earth. It is worthwhile if we can adjudicate on which gas causes the greenhouse effect in our class. For this purpose, one of our authors, Kaneko has designed an educational tool for testing greenhouse effect \\cite{Kaneko}. This system (hereafter abbreviated PAS) is constructed based on photo acoustic effect. Without difficulty and high cost, we can build PAS and check the IR absorption of gas. In this paper we give the simple theoretical basis for this PAS. The amplitude of sound observed in PAS depends on the modulation frequency of IR pulse. Its dependence can be explained by this simple model. Further we show the sound amplitude does not depend on the thermal diffusion, which provides the accuracy of amplitude as the IR absorption rate of the gas. According to this model, sound signal is not the sinusoidal function and it has higher harmonics. The theory and experiment are compared in the third harmonics by spectrum analysis. From this apparatus and theory, students can study not only the greenhouse effect but also the basics of physics."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the photo acoustic system (PAS) designed to test the greenhouse effect, which of the following statements is NOT true regarding the sound amplitude observed?\n\nA) It varies depending on the modulation frequency of the IR pulse\nB) It is independent of thermal diffusion in the gas\nC) It is directly proportional to the IR absorption rate of the gas\nD) It produces a purely sinusoidal signal without higher harmonics\n\nCorrect Answer: D\n\nExplanation: \nA is correct: The passage states that \"The amplitude of sound observed in PAS depends on the modulation frequency of IR pulse.\"\n\nB is correct: The text mentions that \"the sound amplitude does not depend on the thermal diffusion, which provides the accuracy of amplitude as the IR absorption rate of the gas.\"\n\nC is correct: Since the amplitude is independent of thermal diffusion and accurately represents the IR absorption rate, it is indeed directly proportional to the IR absorption rate of the gas.\n\nD is incorrect: The passage explicitly states that \"sound signal is not the sinusoidal function and it has higher harmonics.\" This contradicts the statement in option D, making it the correct answer to a question asking which statement is NOT true.\n\nThis question tests the student's understanding of the key characteristics of the PAS system and requires careful reading of the provided information to distinguish between correct and incorrect statements."}, "36": {"documentation": {"title": "Cost-Effective Millimeter Wave Communications with Lens Antenna Array", "source": "Yong Zeng and Rui Zhang", "docs_id": "1610.02511", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cost-Effective Millimeter Wave Communications with Lens Antenna Array. Millimeter wave (mmWave) communication is a promising technology for the fifth-generation (5G) wireless system. However, the large number of antennas used and the wide signal bandwidth in mmWave systems render the conventional multi-antenna techniques increasingly costly in terms of signal processing complexity, hardware implementation, and power consumption. In this article, we investigate cost-effective mmWave communications by first providing an overview of the main existing techniques that offer different trade-offs between performance and cost, and then focusing our discussion on a promising new technique based on the advanced lens antenna array. It is revealed that by exploiting the angle-dependent energy focusing property of lens arrays, together with the angular sparsity of the mmWave channels, mmWave lens-antenna system is able to achieve the capacity-optimal performance with very few radio-frequency (RF) chains and using the low-complexity single-carrier transmission, even for wide-band frequency-selective channels. Numerical results show that the lens-based system significantly outperforms the state-of-the-art designs for mmWave systems in both spectrum efficiency and energy efficiency."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the advantages of using lens antenna arrays in millimeter wave (mmWave) communications for 5G systems?\n\nA) They increase the number of radio-frequency (RF) chains required, improving signal quality\nB) They allow for complex multi-carrier transmission in wide-band frequency-selective channels\nC) They exploit the angle-dependent energy focusing property and channel angular sparsity to achieve optimal capacity with fewer RF chains\nD) They increase the signal processing complexity, leading to improved spectrum efficiency\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text states that lens antenna arrays exploit \"the angle-dependent energy focusing property of lens arrays, together with the angular sparsity of the mmWave channels\" to \"achieve the capacity-optimal performance with very few radio-frequency (RF) chains.\" This approach allows for optimal performance while reducing complexity and cost.\n\nAnswer A is incorrect because the lens array actually reduces the number of RF chains required, not increases them.\n\nAnswer B is incorrect because the text mentions that the lens-based system can use \"low-complexity single-carrier transmission, even for wide-band frequency-selective channels,\" not complex multi-carrier transmission.\n\nAnswer D is incorrect because one of the main advantages of the lens antenna array is that it reduces signal processing complexity, not increases it. The text specifically mentions that conventional multi-antenna techniques are \"increasingly costly in terms of signal processing complexity,\" which the lens array approach aims to address."}, "37": {"documentation": {"title": "The Laws of Motion of the Broker Call Rate in the United States", "source": "Alex Garivaltis", "docs_id": "1906.00946", "section": ["econ.EM", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.PM", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Laws of Motion of the Broker Call Rate in the United States. In this paper, which is the third installment of the author's trilogy on margin loan pricing, we analyze $1,367$ monthly observations of the U.S. broker call money rate, which is the interest rate at which stock brokers can borrow to fund their margin loans to retail clients. We describe the basic features and mean-reverting behavior of this series and juxtapose the empirically-derived laws of motion with the author's prior theories of margin loan pricing (Garivaltis 2019a-b). This allows us to derive stochastic differential equations that govern the evolution of the margin loan interest rate and the leverage ratios of sophisticated brokerage clients (namely, continuous time Kelly gamblers). Finally, we apply Merton's (1974) arbitrage theory of corporate liability pricing to study theoretical constraints on the risk premia that could be generated in the market for call money. Apparently, if there is no arbitrage in the U.S. financial markets, the implication is that the total volume of call loans must constitute north of $70\\%$ of the value of all leveraged portfolios."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: According to the paper's analysis of the U.S. broker call money rate, which of the following statements is most accurate regarding the implications of no arbitrage in U.S. financial markets?\n\nA) The total volume of call loans must be less than 50% of the value of all leveraged portfolios.\n\nB) The risk premia generated in the call money market are unconstrained.\n\nC) The total volume of call loans must constitute more than 70% of the value of all leveraged portfolios.\n\nD) Merton's arbitrage theory of corporate liability pricing is inapplicable to the call money market.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states, \"Apparently, if there is no arbitrage in the U.S. financial markets, the implication is that the total volume of call loans must constitute north of 70% of the value of all leveraged portfolios.\" This directly supports option C.\n\nOption A is incorrect because it contradicts the stated percentage, which is over 70%, not less than 50%.\n\nOption B is incorrect because the paper mentions studying \"theoretical constraints on the risk premia,\" implying that risk premia are indeed constrained, not unconstrained.\n\nOption D is incorrect because the document explicitly mentions applying \"Merton's (1974) arbitrage theory of corporate liability pricing to study theoretical constraints on the risk premia that could be generated in the market for call money,\" indicating that this theory is applicable to the call money market."}, "38": {"documentation": {"title": "Cocoa pollination, biodiversity-friendly production, and the global\n  market", "source": "Thomas Cherico Wanger, Francis Dennig, Manuel Toledo-Hern\\'andez, Teja\n  Tscharntke, Eric F. Lambin", "docs_id": "2112.02877", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cocoa pollination, biodiversity-friendly production, and the global\n  market. Production of cocoa, the third largest trade commodity globally has experienced climate related yield stagnation since 2016, forcing farmers to expand production in forested habitats and to shift from nature friendly agroforestry systems to intensive monocultures. The goal for future large-scale cocoa production combines high yields with biodiversity friendly management into a climate adapted smart agroforestry system (SAS). As pollination limitation is a key driver of global production, we use data of more than 150,000 cocoa farms and results of hand pollination experiments to show that manually enhancing cocoa pollination (hereafter manual pollination) can produce SAS. Manual pollination can triple farm yields and double farmers annual profit in the major producer countries Ivory Coast, Ghana, and Indonesia, and can increase global cocoa supplies by up to 13%. We propose a win win scenario to mitigate negative long term price and socioeconomic effects, whereby manual pollination compensates only for yield losses resulting from climate and disease related decreases in production area and conversion of monocultures into agroforestry systems. Our results highlight that yields in biodiversity friendly and climate adapted SAS can be similar to yields currently only achieved in monocultures. Adoption of manual pollination could be achieved through wider implementation of ecocertification standards, carbon markets, and zero deforestation pledges."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the potential impact and implications of manual pollination in cocoa production, as presented in the research?\n\nA) Manual pollination can increase global cocoa supplies by up to 50% without any negative consequences.\n\nB) Manual pollination can triple farm yields and double farmers' annual profit in major producer countries, while potentially increasing global cocoa supplies by up to 13%.\n\nC) Manual pollination is proposed as a solution to completely replace traditional farming methods in cocoa production.\n\nD) Manual pollination can only be effective in monoculture systems and is not compatible with biodiversity-friendly agroforestry approaches.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately reflects the findings presented in the research. The document states that manual pollination can triple farm yields and double farmers' annual profit in major producer countries like Ivory Coast, Ghana, and Indonesia. It also mentions that this method can increase global cocoa supplies by up to 13%.\n\nOption A is incorrect because the research does not claim a 50% increase in global supplies, and it acknowledges potential negative long-term price and socioeconomic effects that need to be mitigated.\n\nOption C is incorrect as the research does not propose manual pollination as a complete replacement for traditional methods, but rather as a component of a smart agroforestry system (SAS).\n\nOption D is incorrect because the research actually suggests that manual pollination can help achieve yields in biodiversity-friendly and climate-adapted SAS similar to those currently only achieved in monocultures."}, "39": {"documentation": {"title": "Managing Innovation in Technical Education: Revisiting the Developmental\n  Strategies of Politeknik Brunei", "source": "Bashir Ahmed Bhuiyan, Mohammad Shahansha Molla, and Masud Alam", "docs_id": "2111.02850", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Managing Innovation in Technical Education: Revisiting the Developmental\n  Strategies of Politeknik Brunei. The present study aims at exploring the strategies for managing innovation in technical education by using blended learning philosophy and practices with special reference to Politeknik Brunei. Based on literature review and desk research, the study found out salient characteristics, explored constraining factors, elicited strategies of Politeknik Brunei, and suggested some options and a framework for innovations management and development of effective blended teaching and learning. The limiting factors identified are the unwillingness of the top-level management, lack of structural support, dearth of readiness of the stakeholders, the gap between teacher's expectations and changed students characteristics, and blended teaching myopia on the way of effective application of blended learning strategies. Notable suggestions for strategic development are developing wide-angle vision and self-renewal processes, analyzing the environment for needs determination. Clarity of purpose and tasks, technological adaptability, data-driven decision making, prompt feedback, flipped classroom, and development of learning clusters are other dimensions that may go a long way toward innovating teaching-learning and the overall development of an academic institution. Finally, the study suggested important guidelines for applying the strategies and proposed framework for quality blended learning and managing innovations in technical education."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following combinations best represents the key challenges and strategies for managing innovation in technical education at Politeknik Brunei, as identified in the study?\n\nA) Challenges: Lack of structural support, unwillingness of top-level management\n   Strategies: Developing wide-angle vision, technological adaptability\n\nB) Challenges: Dearth of readiness of stakeholders, blended teaching myopia\n   Strategies: Flipped classroom, data-driven decision making\n\nC) Challenges: Gap between teacher expectations and student characteristics, lack of structural support\n   Strategies: Analyzing the environment for needs determination, development of learning clusters\n\nD) Challenges: Unwillingness of top-level management, blended teaching myopia\n   Strategies: Clarity of purpose and tasks, prompt feedback\n\nCorrect Answer: A\n\nExplanation: The correct answer is A because it accurately pairs two of the main challenges identified in the study (lack of structural support and unwillingness of top-level management) with two of the key suggested strategies (developing wide-angle vision and technological adaptability). While the other options contain elements that are mentioned in the study, they do not provide the most comprehensive and accurate pairing of challenges and strategies. Option A best represents the core findings of the study regarding the obstacles faced and the proposed solutions for managing innovation in technical education at Politeknik Brunei."}, "40": {"documentation": {"title": "Fusion of neutron rich oxygen isotopes in the crust of accreting neutron\n  stars", "source": "C. J. Horowitz, H. Dussan, and D. K. Berry", "docs_id": "0710.5714", "section": ["astro-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fusion of neutron rich oxygen isotopes in the crust of accreting neutron\n  stars. Fusion reactions in the crust of an accreting neutron star are an important source of heat, and the depth at which these reactions occur is important for determining the temperature profile of the star. Fusion reactions depend strongly on the nuclear charge $Z$. Nuclei with $Z\\le 6$ can fuse at low densities in a liquid ocean. However, nuclei with Z=8 or 10 may not burn until higher densities where the crust is solid and electron capture has made the nuclei neutron rich. We calculate the $S$ factor for fusion reactions of neutron rich nuclei including $^{24}$O + $^{24}$O and $^{28}$Ne + $^{28}$Ne. We use a simple barrier penetration model. The $S$ factor could be further enhanced by dynamical effects involving the neutron rich skin. This possible enhancement in $S$ should be studied in the laboratory with neutron rich radioactive beams. We model the structure of the crust with molecular dynamics simulations. We find that the crust of accreting neutron stars may contain micro-crystals or regions of phase separation. Nevertheless, the screening factors that we determine for the enhancement of the rate of thermonuclear reactions are insensitive to these features. Finally, we calculate the rate of thermonuclear $^{24}$O + $^{24}$O fusion and find that $^{24}$O should burn at densities near $10^{11}$ g/cm$^3$. The energy released from this and similar reactions may be important for the temperature profile of the star."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of fusion reactions in the crust of accreting neutron stars, which of the following statements is most accurate regarding the fusion of oxygen isotopes?\n\nA) Oxygen isotopes with Z=8 readily fuse in the liquid ocean at low densities.\n\nB) The fusion of 24O + 24O is expected to occur at densities around 10^9 g/cm^3.\n\nC) The S factor for 24O + 24O fusion is solely determined by barrier penetration and is not affected by the neutron-rich skin.\n\nD) 24O fusion is predicted to occur at densities near 10^11 g/cm^3 and may significantly impact the star's temperature profile.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"24O should burn at densities near 10^11 g/cm^3\" and that \"The energy released from this and similar reactions may be important for the temperature profile of the star.\"\n\nAnswer A is incorrect because the text mentions that \"nuclei with Z=8 or 10 may not burn until higher densities where the crust is solid,\" contradicting the idea that oxygen (Z=8) fuses readily in the liquid ocean.\n\nAnswer B is incorrect as it states a much lower density (10^9 g/cm^3) than what is mentioned in the text (10^11 g/cm^3) for 24O fusion.\n\nAnswer C is incorrect because the documentation suggests that \"The S factor could be further enhanced by dynamical effects involving the neutron rich skin,\" indicating that the neutron-rich skin could indeed affect the S factor beyond simple barrier penetration."}, "41": {"documentation": {"title": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II", "source": "Jiahao Chen and Nancy Makri", "docs_id": "0812.1552", "section": ["cond-mat.other", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II. The extent of exciton coherence in protein-pigment complexes has significant implications for the initial light harvesting step in photosynthetic organisms. In this work we model the main antenna protein of photosystem II, namely light harvesting complex II (LHC-II), with a single-exciton Hamiltonian with sites coupled via dipole-dipole interaction, with linear coupling to a dissipative phonon bath. With appropriate parameters, Monte Carlo path integral (MCPI) results of the exciton coherence length from 1 K to 500 K show that at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites, with persistent short-range coherence over the A2-B2 pair, A3-B3 pair and B1-B5-B6 triplet. Quasi-adiabatic path integral (QUAPI) calculations of the subsystems mentioned above show a smooth, incoherent relaxation towards thermodynamic equilibrium. The results obtained imply that with the exception of small coherent subsystems at cryogenic temperatures, excitons in LHC-II are more localized than in the analogous light harvesting complex II (LH-II) of the purple bacterium Rs. molischianum, which may be expected from the lower symmetry of the former."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Based on the Monte Carlo path integral (MCPI) results for light harvesting complex II (LHC-II) at thermodynamic equilibrium, which of the following statements is most accurate regarding exciton coherence at low temperatures?\n\nA) Excitons are delocalized across the entire LHC-II complex, showing long-range coherence.\nB) Excitons are primarily localized on 2 single chlorophyll pigment sites, with no coherence between sites.\nC) Excitons show persistent short-range coherence over specific pairs and triplets of chlorophyll sites, while being mostly localized on 2 single sites.\nD) Excitons in LHC-II exhibit the same coherence patterns as those in LH-II of purple bacterium Rs. molischianum.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that MCPI results show that at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites. However, it also mentions persistent short-range coherence over specific pairs (A2-B2, A3-B3) and a triplet (B1-B5-B6). This combination of localization and specific short-range coherence is best described by option C.\n\nOption A is incorrect because the results do not indicate long-range coherence across the entire complex. Option B is partially correct about localization but fails to acknowledge the persistent short-range coherence. Option D is incorrect because the document explicitly states that excitons in LHC-II are more localized than in LH-II of Rs. molischianum, indicating different coherence patterns."}, "42": {"documentation": {"title": "Reconstructing firm-level interactions: the Dutch input-output network", "source": "Leonardo Niccol\\`o Ialongo, Camille de Valk, Emiliano Marchese, Fabian\n  Jansen, Hicham Zmarrou, Tiziano Squartini, Diego Garlaschelli", "docs_id": "2111.15248", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconstructing firm-level interactions: the Dutch input-output network. Recent crises have shown that the knowledge of the structure of input-output networks at the firm level is crucial when studying economic resilience from the microscopic point of view of firms that rewire their connections under supply and demand shocks. Unfortunately, empirical inter-firm network data are rarely accessible and protected by confidentiality. The available methods of network reconstruction from partial information, which have been devised for financial exposures, are inadequate for inter-firm relationships because they treat all pairs of nodes as potentially interacting, thereby overestimating the rewiring capabilities of the system. Here we use two big data sets of transactions in the Netherlands to represent a large portion of the Dutch inter-firm network and document the properties of one of the few analysed networks of this kind. We, then, introduce a generalized maximum-entropy reconstruction method that preserves the production function of each firm in the data, i.e. the input and output flows of each node for each product type. We confirm that the new method becomes increasingly more reliable as a finer product resolution is considered and can therefore be used as a generative model of inter-firm networks with fine production constraints. The likelihood of the model, being related to the entropy, proxies the rewiring capability of the system for a fixed input-output configuration."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of reconstructing firm-level interactions in the Dutch input-output network, which of the following statements best describes the advantages of the newly introduced generalized maximum-entropy reconstruction method over existing methods?\n\nA) It treats all pairs of nodes as potentially interacting, thereby increasing the system's rewiring capabilities.\n\nB) It preserves the production function of each firm, including input and output flows for each product type, and becomes more reliable with finer product resolution.\n\nC) It provides complete empirical inter-firm network data without concerns for confidentiality.\n\nD) It focuses exclusively on financial exposures between firms, making it more accurate for economic resilience studies.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the text explicitly states that the new generalized maximum-entropy reconstruction method \"preserves the production function of each firm in the data, i.e. the input and output flows of each node for each product type.\" It also mentions that this method \"becomes increasingly more reliable as a finer product resolution is considered.\"\n\nAnswer A is incorrect because it describes a limitation of existing methods, not an advantage of the new method. The document criticizes current methods for treating all pairs of nodes as potentially interacting, which overestimates rewiring capabilities.\n\nAnswer C is incorrect because the document mentions that empirical inter-firm network data are rarely accessible and protected by confidentiality. The new method is a reconstruction technique, not a way to bypass confidentiality concerns.\n\nAnswer D is incorrect because the document states that existing methods devised for financial exposures are inadequate for inter-firm relationships. The new method is specifically designed to address the limitations of these financial exposure-focused approaches."}, "43": {"documentation": {"title": "On Dirac operators in $\\mathbb{R}^3$ with electrostatic and Lorentz\n  scalar $\\delta$-shell interactions", "source": "Jussi Behrndt, Pavel Exner, Markus Holzmann, and Vladimir Lotoreichik", "docs_id": "1901.11323", "section": ["math.SP", "math-ph", "math.AP", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Dirac operators in $\\mathbb{R}^3$ with electrostatic and Lorentz\n  scalar $\\delta$-shell interactions. In this article Dirac operators $A_{\\eta, \\tau}$ coupled with combinations of electrostatic and Lorentz scalar $\\delta$-shell interactions of constant strength $\\eta$ and $\\tau$, respectively, supported on compact surfaces $\\Sigma \\subset \\mathbb{R}^3$ are studied. In the rigorous definition of these operators the $\\delta$-potentials are modelled by coupling conditions at $\\Sigma$. In the proof of the self-adjointness of $A_{\\eta, \\tau}$ a Krein-type resolvent formula and a Birman-Schwinger principle are obtained. With their help a detailed study of the qualitative spectral properties of $A_{\\eta, \\tau}$ is possible. In particular, the essential spectrum of $A_{\\eta, \\tau}$ is determined, it is shown that at most finitely many discrete eigenvalues can appear, and several symmetry relations in the point spectrum are obtained. Moreover, the nonrelativistic limit of $A_{\\eta, \\tau}$ is computed and it is discussed that for some special interaction strengths $A_{\\eta, \\tau}$ is decoupled to two operators acting in the domains with the common boundary $\\Sigma$."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following statements about the Dirac operators A_{\u03b7,\u03c4} with electrostatic and Lorentz scalar \u03b4-shell interactions is NOT correct?\n\nA) The essential spectrum of A_{\u03b7,\u03c4} is fully determined in the study.\n\nB) The \u03b4-potentials in the rigorous definition of these operators are modeled by coupling conditions at the compact surface \u03a3.\n\nC) The study shows that an infinite number of discrete eigenvalues can appear in the spectrum of A_{\u03b7,\u03c4}.\n\nD) A Krein-type resolvent formula and a Birman-Schwinger principle are obtained in the proof of self-adjointness of A_{\u03b7,\u03c4}.\n\nCorrect Answer: C\n\nExplanation: \nOption C is incorrect and thus the correct answer to this question. The documentation states that \"at most finitely many discrete eigenvalues can appear,\" which contradicts the statement in option C about an infinite number of discrete eigenvalues.\n\nOptions A, B, and D are all correct according to the given information:\nA) The documentation mentions that \"the essential spectrum of A_{\u03b7,\u03c4} is determined.\"\nB) It's stated that \"the \u03b4-potentials are modelled by coupling conditions at \u03a3.\"\nD) The text explicitly mentions that \"a Krein-type resolvent formula and a Birman-Schwinger principle are obtained\" in the proof of self-adjointness.\n\nThis question tests the reader's careful comprehension of the technical details provided in the documentation, particularly their ability to identify a statement that contradicts the given information."}, "44": {"documentation": {"title": "Solving Dynamic Discrete Choice Models Using Smoothing and Sieve Methods", "source": "Dennis Kristensen and Patrick K. Mogensen and Jong Myun Moon and\n  Bertel Schjerning", "docs_id": "1904.05232", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Dynamic Discrete Choice Models Using Smoothing and Sieve Methods. We propose to combine smoothing, simulations and sieve approximations to solve for either the integrated or expected value function in a general class of dynamic discrete choice (DDC) models. We use importance sampling to approximate the Bellman operators defining the two functions. The random Bellman operators, and therefore also the corresponding solutions, are generally non-smooth which is undesirable. To circumvent this issue, we introduce a smoothed version of the random Bellman operator and solve for the corresponding smoothed value function using sieve methods. We show that one can avoid using sieves by generalizing and adapting the `self-approximating' method of Rust (1997) to our setting. We provide an asymptotic theory for the approximate solutions and show that they converge with root-N-rate, where $N$ is number of Monte Carlo draws, towards Gaussian processes. We examine their performance in practice through a set of numerical experiments and find that both methods perform well with the sieve method being particularly attractive in terms of computational speed and accuracy."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of solving Dynamic Discrete Choice (DDC) models, which combination of methods is proposed by the authors to address the non-smoothness issue of random Bellman operators while maintaining computational efficiency?\n\nA) Importance sampling, kernel smoothing, and neural networks\nB) Smoothing, simulations, and sieve approximations\nC) Monte Carlo integration, polynomial interpolation, and bootstrap sampling\nD) Quasi-Monte Carlo methods, spline smoothing, and genetic algorithms\n\nCorrect Answer: B\n\nExplanation: The correct answer is B) Smoothing, simulations, and sieve approximations. The documentation explicitly states that the authors \"propose to combine smoothing, simulations and sieve approximations to solve for either the integrated or expected value function in a general class of dynamic discrete choice (DDC) models.\" \n\nAnswer A is incorrect because while importance sampling is mentioned in the text, kernel smoothing and neural networks are not part of the proposed methodology.\n\nAnswer C is incorrect because although Monte Carlo methods are used (in the form of importance sampling), polynomial interpolation and bootstrap sampling are not mentioned as part of the core methodology.\n\nAnswer D is incorrect as quasi-Monte Carlo methods, spline smoothing, and genetic algorithms are not mentioned in the given text.\n\nThe combination of smoothing, simulations, and sieve approximations is specifically designed to address the non-smoothness of random Bellman operators while providing computational efficiency. The authors introduce a smoothed version of the random Bellman operator and solve for the corresponding smoothed value function using sieve methods, which they find to be particularly attractive in terms of computational speed and accuracy."}, "45": {"documentation": {"title": "Quantifying the Effects of the 2008 Recession using the Zillow Dataset", "source": "Arunav Gupta, Lucas Nguyen, Camille Dunning, Ka Ming Chan", "docs_id": "1912.11341", "section": ["econ.GN", "cs.LG", "cs.NA", "math.NA", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantifying the Effects of the 2008 Recession using the Zillow Dataset. This report explores the use of Zillow's housing metrics dataset to investigate the effects of the 2008 US subprime mortgage crisis on various US locales. We begin by exploring the causes of the recession and the metrics available to us in the dataset. We settle on using the Zillow Home Value Index (ZHVI) because it is seasonally adjusted and able to account for a variety of inventory factors. Then, we explore three methodologies for quantifying recession impact: (a) Principal Components Analysis, (b) Area Under Baseline, and (c) ARIMA modeling and Confidence Intervals. While PCA does not yield useable results, we ended up with six cities from both AUB and ARIMA analysis, the top 3 \"losers\" and \"gainers\" of the 2008 recession, as determined by each analysis. This gave us 12 cities in total. Finally, we tested the robustness of our analysis against three \"common knowledge\" metrics for the recession: geographic clustering, population trends, and unemployment rate. While we did find some overlap between the results of our analysis and geographic clustering, there was no positive regression outcome from comparing our methodologies to population trends and the unemployment rate."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the methodology and findings of the study on the 2008 recession using the Zillow dataset?\n\nA) The study primarily relied on Principal Components Analysis (PCA) to identify the most affected cities, with results showing a strong correlation to unemployment rates.\n\nB) The research used the Zillow Home Value Index (ZHVI) and employed both Area Under Baseline (AUB) and ARIMA modeling to identify the top 3 \"losers\" and \"gainers\", but found no significant relationship with common recession metrics.\n\nC) The study utilized raw Zillow data without seasonal adjustments and found that geographic clustering was the most reliable indicator of recession impact across all cities.\n\nD) The research concluded that population trends were the best predictor of recession impact, as determined by a combination of PCA and ARIMA modeling.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately reflects the key aspects of the study's methodology and findings. The research used the Zillow Home Value Index (ZHVI) due to its seasonal adjustments and ability to account for inventory factors. The study employed both Area Under Baseline (AUB) and ARIMA modeling to identify the top 3 \"losers\" and \"gainers\" of the 2008 recession. Importantly, while some overlap was found with geographic clustering, the study found no positive regression outcome when comparing their methodologies to population trends and unemployment rates, which are common recession metrics.\n\nOption A is incorrect because PCA did not yield usable results, and the study did not find a strong correlation with unemployment rates. Option C is wrong because the study specifically used seasonally adjusted data (ZHVI) and did not conclude that geographic clustering was the most reliable indicator across all cities. Option D is incorrect because the study did not conclude that population trends were the best predictor, and PCA was not used in combination with ARIMA modeling for the final analysis."}, "46": {"documentation": {"title": "Twistor String Structure of the Kerr-Schild Geometry and Consistency of\n  the Dirac-Kerr System", "source": "Alexander Burinskii", "docs_id": "0812.4908", "section": ["hep-th", "gr-qc", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Twistor String Structure of the Kerr-Schild Geometry and Consistency of\n  the Dirac-Kerr System. Kerr-Schild (KS) geometry of the rotating black-holes and spinning particles is based on the associated with Kerr theorem twistor structure which is defined by an analytic curve $F(Z)=0$ in the projective twistor space $Z \\in CP^3 .$ On the other hand, there is a complex Newman representation which describes the source of Kerr-Newman solution as a \"particle\" propagating along a complex world-line $X(\\t)\\in CM^4,$ and this world-line determines the parameters of the Kerr generating function $F(Z).$ The complex world line is really a world-sheet, $\\t= t + i \\sigma,$ and the Kerr source may be considered as a complex Euclidean string extended in the imaginary time direction $\\sigma$. The Kerr twistor structure turns out to be adjoined to the Kerr complex string source, forming a natural twistor-string construction similar to the Nair-Witten twistor-string. We show that twistor polarization of the Kerr-Newman solution may be matched with the {\\it massless} solutions of the Dirac equation, providing consistency of the Dirac-Kerr model of spinning particle (electron). It allows us to extend the Nair-Witten concept on the scattering of the gauge amplitudes in twistor space to include massive KS particles."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: The Kerr-Schild geometry of rotating black holes and spinning particles is associated with a twistor structure defined by an analytic curve F(Z)=0 in projective twistor space. This structure is related to a complex world-line X(\u03c4)\u2208CM^4, which determines the parameters of the Kerr generating function F(Z). How does this complex world-line relate to the concept of a string theory, and what implications does this have for the Dirac-Kerr model of spinning particles?\n\nA) The complex world-line represents a 1-dimensional object, contradicting string theory, and has no relevance to the Dirac-Kerr model.\n\nB) The complex world-line is actually a world-sheet with \u03c4 = t + i\u03c3, representing a complex Euclidean string extended in imaginary time, which allows for consistency between the Kerr-Newman solution and massless Dirac equation solutions.\n\nC) The complex world-line indicates that the Kerr source is a point particle, incompatible with string theory, but supports the Dirac-Kerr model through its complex nature.\n\nD) The complex world-line suggests a higher-dimensional object, supporting M-theory rather than string theory, and invalidates the Dirac-Kerr model for spinning particles.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The complex world-line X(\u03c4) is described as a world-sheet with \u03c4 = t + i\u03c3, where \u03c3 represents an extension in imaginary time. This makes the Kerr source effectively a complex Euclidean string. This string-like nature forms a twistor-string construction similar to the Nair-Witten twistor-string, bridging concepts from string theory with the Kerr-Schild geometry. Importantly, this structure allows for the matching of the twistor polarization of the Kerr-Newman solution with massless solutions of the Dirac equation. This compatibility provides consistency for the Dirac-Kerr model of spinning particles like electrons, extending the applicability of twistor-string concepts to massive Kerr-Schild particles."}, "47": {"documentation": {"title": "Evaluating and Improving Semi-analytic modelling of Dust in Galaxies\n  based on Radiative Transfer Calculations", "source": "Fabio Fontanot (1), Rachel S. Somerville (1), Laura Silva (2),\n  Pierluigi Monaco (3,2), Ramin Skibba (1) ((1) MPIA, Heidelberg (2) INAF-OATs\n  (3) DAUT, Trieste)", "docs_id": "0810.3918", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluating and Improving Semi-analytic modelling of Dust in Galaxies\n  based on Radiative Transfer Calculations. The treatment of dust attenuation is crucial in order to compare the predictions of galaxy formation models with multiwavelength observations. Most past studies have either used simple analytic prescriptions or else full radiative transfer (RT) calculations. Here, we couple star formation histories and morphologies predicted by the semi-analytic galaxy formation model MORGANA with RT calculations from the spectrophotometric and dust code GRASIL to create a library of galaxy SEDs from the UV/optical through the far Infrared, and compare the predictions of the RT calculations with analytic prescriptions. We consider a low and high redshift sample, as well as an additional library constructed with empirical, non-cosmological star formation histories and simple (pure bulge or disc) morphologies. Based on these libraries, we derive fitting formulae for the effective dust optical depth as a function of galaxy physical properties such as metallicity, gas mass, and radius. We show that such fitting formulae can predict the V-band optical depth with a scatter smaller than 0.4 dex for both the low and high redshift samples, but that there is a large galaxy-to-galaxy scatter in the shapes of attenuation curves, probably due to geometrical variations, which our simple recipe does not capture well. However, our new recipe provides a better approximation to the GRASIL results at optical wavelength than standard analytic prescriptions from the literature, particularly at high redshift."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the relationship between the new fitting formulae developed in this study and the GRASIL radiative transfer calculations for dust attenuation in galaxies?\n\nA) The new fitting formulae perfectly replicate the GRASIL results across all wavelengths and redshifts.\n\nB) The new fitting formulae provide a better approximation to the GRASIL results at optical wavelengths compared to standard analytic prescriptions, especially at high redshift.\n\nC) The new fitting formulae completely eliminate the need for full radiative transfer calculations in galaxy formation models.\n\nD) The new fitting formulae accurately capture the galaxy-to-galaxy variations in attenuation curve shapes caused by geometrical differences.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage states that \"our new recipe provides a better approximation to the GRASIL results at optical wavelength than standard analytic prescriptions from the literature, particularly at high redshift.\" This directly supports option B.\n\nOption A is incorrect because the text mentions there is still scatter in the predictions and that the recipe doesn't capture geometrical variations well, so it's not a perfect replication.\n\nOption C is too extreme. While the new formulae improve upon previous analytic prescriptions, there's no indication that they completely eliminate the need for full radiative transfer calculations.\n\nOption D is incorrect because the passage explicitly states that \"there is a large galaxy-to-galaxy scatter in the shapes of attenuation curves, probably due to geometrical variations, which our simple recipe does not capture well.\""}, "48": {"documentation": {"title": "Learning Hyperspectral Feature Extraction and Classification with\n  ResNeXt Network", "source": "Divinah Nyasaka, Jing Wang, Haron Tinega", "docs_id": "2002.02585", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Hyperspectral Feature Extraction and Classification with\n  ResNeXt Network. The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the key innovation and advantage of the MixedSN architecture for hyperspectral image classification?\n\nA) It uses only 3D Convolutional Neural Networks throughout the entire network to extract both spatial and spectral information.\n\nB) It employs 2D convolutions in the early layers and 3D convolutions in the top layers to balance computational cost and feature extraction.\n\nC) It utilizes 3D convolutions in the early layers for spectral-spatial information extraction and 2D convolutions in the top layers for semantic abstraction, reducing parameters while maintaining performance.\n\nD) It relies solely on 2D convolutions arranged in ResNeXt blocks to process hyperspectral images, significantly reducing computational complexity.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation of the MixedSN architecture is its strategic use of both 3D and 2D convolutions in different parts of the network. Specifically, it uses 3D convolutions in the early layers to model spectral-spatial information, which is crucial for hyperspectral image analysis. Then, it transitions to 2D convolutions in the top layers, which are responsible for semantic abstraction. This approach allows the network to effectively process the rich spectral-spatial information in hyperspectral images while reducing the overall number of parameters compared to a full 3D-CNN approach. This balance helps maintain classification performance while improving computational efficiency.\n\nOption A is incorrect because using only 3D CNNs throughout the network would lead to an explosion of parameters and high computational cost, which the MixedSN architecture specifically aims to avoid.\n\nOption B is incorrect because it reverses the order of 3D and 2D convolutions used in the MixedSN architecture.\n\nOption D is incorrect because it mentions using only 2D convolutions, which would not adequately capture the spectral-spatial relationships in hyperspectral images that are crucial for accurate classification."}, "49": {"documentation": {"title": "Super-hard X-Ray Emission from eta Carinae Observed with Suzaku", "source": "Akiko Sekiguchi, Masahiro Tsujimoto, Shunji Kitamoto, Manabu Ishida,\n  Kenji Hamaguchi, Hideyuki Mori, and Yohko Tsuboi", "docs_id": "0903.3307", "section": ["astro-ph.SR", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Super-hard X-Ray Emission from eta Carinae Observed with Suzaku. We present the Suzaku results of eta Carinae in the 5--50 keV range conducted twice around the apastron in 2005 August for 50 ks and in 2006 February for 20 ks. The X-ray Imaging Spectrometer (XIS) produced hard (5--12 keV) band spectra, resolving K shell lines from highly ionized Fe and Ni. The Hard X-ray Detector yielded a significant detection in the super-hard (15--50 keV) band, which is uncontaminated by near-by sources. We constrained the temperature of the optically-thin thermal plasma emission dominant in the hard band as 3--4 keV using the K-shell line features with the XIS. We found significant excess emission above the thermal emission in the super-hard band with the PIN, confirming the previous INTEGRAL ISGRI report. The entire 5--50 keV spectra were fitted by a combination of a thermal plasma model plus a flat power-law or a very hot thermal bremsstrahlung model for the excess emission. No significant change of the excess emission was found at different epochs within the systematic and statistical uncertainties and no flare-like flux amplification was seen in the hard band, indicating that the excess emission is a steady phenomenon. We argue that the super-hard emission is attributable to the inverse Compton of stellar UV photons by non-thermal electrons or to the thermal bremsstrahlung of very hot plasma, and not to the bremsstrahlung by non-thermal electrons colliding with cold ambient matter."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Based on the Suzaku observations of eta Carinae, which of the following statements is NOT a valid interpretation of the super-hard X-ray emission (15-50 keV) detected?\n\nA) It could be attributed to inverse Compton scattering of stellar UV photons by non-thermal electrons\nB) It might be caused by thermal bremsstrahlung from very hot plasma\nC) It is likely produced by bremsstrahlung from non-thermal electrons colliding with cold ambient matter\nD) It appears to be a steady phenomenon rather than showing significant variability\n\nCorrect Answer: C\n\nExplanation: The question asks for the statement that is NOT a valid interpretation based on the Suzaku observations. Options A, B, and D are all supported by the text, while option C is explicitly ruled out.\n\nA and B are mentioned as possible explanations for the super-hard emission in the last sentence: \"We argue that the super-hard emission is attributable to the inverse Compton of stellar UV photons by non-thermal electrons or to the thermal bremsstrahlung of very hot plasma.\"\n\nD is supported by the statement: \"No significant change of the excess emission was found at different epochs within the systematic and statistical uncertainties and no flare-like flux amplification was seen in the hard band, indicating that the excess emission is a steady phenomenon.\"\n\nC is directly contradicted by the text, which states: \"and not to the bremsstrahlung by non-thermal electrons colliding with cold ambient matter.\"\n\nThis question tests the student's ability to carefully read and interpret scientific findings, distinguishing between supported and unsupported explanations for observed phenomena."}, "50": {"documentation": {"title": "Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems", "source": "Kaidi Yang, Matthew W. Tsao, Xin Xu, Marco Pavone", "docs_id": "2008.08131", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems. Automated vehicles (AVs) are expected to be beneficial for Mobility-on-Demand (MoD), thanks to their ability of being globally coordinated. To facilitate the steady transition towards full autonomy, we consider the transition period of AV deployment, whereby an MoD system operates a mixed fleet of automated vehicles (AVs) and human-driven vehicles (HVs). In such systems, AVs are centrally coordinated by the operator, and the HVs might strategically respond to the coordination of AVs. We devise computationally tractable strategies to coordinate mixed fleets in MoD systems. Specifically, we model an MoD system with a mixed fleet using a Stackelberg framework where the MoD operator serves as the leader and human-driven vehicles serve as the followers. We develop two models: 1) a steady-state model to analyze the properties of the problem and determine the planning variables (e.g., compensations, prices, and the fleet size of AVs), and 2) a time-varying model to design a real-time coordination algorithm for AVs. The proposed models are validated using a case study inspired by real operational data of a MoD service in Singapore. Results show that the proposed algorithms can significantly improve system performance."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In a Mobility-on-Demand (MoD) system with a mixed fleet of automated vehicles (AVs) and human-driven vehicles (HVs), which of the following best describes the Stackelberg framework used to model the system?\n\nA) AVs are the leaders and HVs are the followers, with AVs making decisions first\nB) HVs are the leaders and AVs are the followers, with HVs making decisions first\nC) The MoD operator is the leader and AVs are the followers, with the operator making decisions first\nD) The MoD operator is the leader and HVs are the followers, with the operator making decisions first\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The passage explicitly states that the MoD system with a mixed fleet is modeled using a Stackelberg framework where \"the MoD operator serves as the leader and human-driven vehicles serve as the followers.\" This framework allows the operator to make strategic decisions first, considering how HVs might respond to the coordination of AVs.\n\nOption A is incorrect because AVs are not the leaders in this framework; they are centrally coordinated by the MoD operator.\n\nOption B is incorrect because HVs are not the leaders; they are described as followers in the Stackelberg framework.\n\nOption C is partially correct in identifying the MoD operator as the leader, but it incorrectly states that AVs are the followers. The passage indicates that AVs are centrally coordinated by the operator, while HVs are the ones that \"might strategically respond to the coordination of AVs.\""}, "51": {"documentation": {"title": "Dynamic Set Values for Nonzero Sum Games with Multiple Equilibriums", "source": "Zachary Feinstein, Birgit Rudloff, Jianfeng Zhang", "docs_id": "2002.00449", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Set Values for Nonzero Sum Games with Multiple Equilibriums. Nonzero sum games typically have multiple Nash equilibriums (or no equilibrium), and unlike the zero sum case, they may have different values at different equilibriums. Instead of focusing on the existence of individual equilibriums, we study the set of values over all equilibriums, which we call the set value of the game. The set value is unique by nature and always exists (with possible value $\\emptyset$). Similar to the standard value function in control literature, it enjoys many nice properties such as regularity, stability, and more importantly the dynamic programming principle. There are two main features in order to obtain the dynamic programming principle: (i) we must use closed-loop controls (instead of open-loop controls); (ii) we must allow for path dependent controls, even if the problem is in a state dependent (Markovian) setting. We shall consider both discrete and continuous time models with finite time horizon. For the latter we will also provide a duality approach through certain standard PDE (or path dependent PDE), which is quite efficient for numerically computing the set value of the game."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of nonzero sum games with multiple equilibriums, which of the following statements about the set value of the game is NOT correct?\n\nA) The set value is always unique and exists, even if it may be an empty set.\n\nB) The set value enjoys properties such as regularity and stability, similar to standard value functions in control literature.\n\nC) To obtain the dynamic programming principle for set values, open-loop controls must be used instead of closed-loop controls.\n\nD) The set value approach allows for the study of all possible equilibrium values rather than focusing on individual equilibriums.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C because it contradicts the information given in the document. The passage specifically states that \"There are two main features in order to obtain the dynamic programming principle: (i) we must use closed-loop controls (instead of open-loop controls).\" This means that closed-loop controls, not open-loop controls, are necessary for the dynamic programming principle in set values.\n\nOption A is correct according to the passage, which states that \"The set value is unique by nature and always exists (with possible value \u2205).\"\n\nOption B is also correct, as the document mentions that the set value \"enjoys many nice properties such as regularity, stability, and more importantly the dynamic programming principle.\"\n\nOption D is correct and aligns with the main idea presented in the passage about studying \"the set of values over all equilibriums\" instead of focusing on individual equilibriums."}, "52": {"documentation": {"title": "Kauffman's adjacent possible in word order evolution", "source": "Ramon Ferrer-i-Cancho", "docs_id": "1512.05582", "section": ["cs.CL", "cs.IT", "math.IT", "physics.data-an", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kauffman's adjacent possible in word order evolution. Word order evolution has been hypothesized to be constrained by a word order permutation ring: transitions involving orders that are closer in the permutation ring are more likely. The hypothesis can be seen as a particular case of Kauffman's adjacent possible in word order evolution. Here we consider the problem of the association of the six possible orders of S, V and O to yield a couple of primary alternating orders as a window to word order evolution. We evaluate the suitability of various competing hypotheses to predict one member of the couple from the other with the help of information theoretic model selection. Our ensemble of models includes a six-way model that is based on the word order permutation ring (Kauffman's adjacent possible) and another model based on the dual two-way of standard typology, that reduces word order to basic orders preferences (e.g., a preference for SV over VS and another for SO over OS). Our analysis indicates that the permutation ring yields the best model when favoring parsimony strongly, providing support for Kauffman's general view and a six-way typology."}, "answer": "Question: Which of the following statements best describes the relationship between Kauffman's adjacent possible theory and word order evolution, as discussed in the given text?\n\nA) Kauffman's adjacent possible theory suggests that word order evolution is completely random and unpredictable.\n\nB) The word order permutation ring hypothesis is a specific application of Kauffman's adjacent possible theory to language evolution.\n\nC) Kauffman's adjacent possible theory contradicts the idea of a word order permutation ring in language evolution.\n\nD) The adjacent possible theory applies only to the dual two-way model of standard typology in word order evolution.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text explicitly states that \"The hypothesis can be seen as a particular case of Kauffman's adjacent possible in word order evolution.\" This indicates that the word order permutation ring hypothesis, which suggests that transitions between word orders that are closer in the permutation ring are more likely, is a specific application of Kauffman's more general adjacent possible theory to the domain of language evolution.\n\nOption A is incorrect because the text does not suggest that word order evolution is random. Instead, it implies that there are constraints and patterns in how word orders change.\n\nOption C is incorrect because the text presents Kauffman's theory and the word order permutation ring as complementary rather than contradictory.\n\nOption D is incorrect because the text does not limit the application of the adjacent possible theory to the dual two-way model. In fact, the analysis in the text favors the six-way model based on the permutation ring, which is linked to Kauffman's theory."}, "53": {"documentation": {"title": "Measuring Dark Matter Profiles Non-Parametrically in Dwarf Spheroidals:\n  An Application to Draco", "source": "John R. Jardel, Karl Gebhardt, Maximilian Fabricius, Niv Drory,\n  Michael J. Williams", "docs_id": "1211.5376", "section": ["astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measuring Dark Matter Profiles Non-Parametrically in Dwarf Spheroidals:\n  An Application to Draco. We introduce a novel implementation of orbit-based (or Schwarzschild) modeling that allows dark matter density profiles to be calculated non-parametrically in nearby galaxies. Our models require no assumptions to be made about velocity anisotropy or the dark matter profile. The technique can be applied to any dispersion-supported stellar system, and we demonstrate its use by studying the Local Group dwarf spheroidal (dSph) galaxy Draco. We use existing kinematic data at larger radii and also present 12 new radial velocities within the central 13 pc obtained with the VIRUS-W integral field spectrograph on the 2.7m telescope at McDonald Observatory. Our non-parametric Schwarzschild models find strong evidence that the dark matter profile in Draco is cuspy for 20 < r < 700 pc. The profile for r > 20 pc is well-fit by a power law with slope \\alpha=-1.0 +/- 0.2, consistent with predictions from Cold Dark Matter (CDM) simulations. Our models confirm that, despite its low baryon content relative to other dSphs, Draco lives in a massive halo."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key findings and methodology of the study on dark matter profiles in the Draco dwarf spheroidal galaxy?\n\nA) The study used parametric modeling to confirm that Draco has a core-like dark matter profile, contradicting Cold Dark Matter simulations.\n\nB) The research employed traditional Jeans modeling techniques to determine that Draco's dark matter profile is consistent with MOND (Modified Newtonian Dynamics) predictions.\n\nC) The study introduced a novel non-parametric Schwarzschild modeling technique, finding evidence for a cuspy dark matter profile in Draco with a power-law slope of approximately -1.0, consistent with CDM simulations.\n\nD) The research combined new kinematic data from the central regions with existing data to conclude that Draco has a constant density dark matter core extending to 700 pc.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key aspects of the study as described in the documentation. The research introduced a new non-parametric implementation of Schwarzschild modeling, which allowed for dark matter density profiles to be calculated without making assumptions about velocity anisotropy or the profile shape. This technique was applied to Draco, combining new central kinematic data with existing data at larger radii. The study found strong evidence for a cuspy dark matter profile in Draco for radii between 20 and 700 pc, with a power-law slope of \u03b1=-1.0 \u00b1 0.2, which is consistent with Cold Dark Matter (CDM) simulations.\n\nOption A is incorrect because the study found a cuspy profile, not a core-like profile, and the results support rather than contradict CDM simulations. Option B is incorrect as the study did not use Jeans modeling and did not compare results to MOND predictions. Option D is incorrect because the study found a cuspy profile, not a constant density core, and the cusp extends to 20 pc, not 700 pc."}, "54": {"documentation": {"title": "Diffusion and localization of relative strategy scores in the Minority\n  Game", "source": "Mats Granath and Alvaro Perez-Diaz", "docs_id": "1509.08236", "section": ["cond-mat.dis-nn", "nlin.AO", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diffusion and localization of relative strategy scores in the Minority\n  Game. We study the equilibrium distribution of relative strategy scores of agents in the asymmetric phase ($\\alpha\\equiv P/N\\gtrsim 1$) of the basic Minority Game using sign-payoff, with $N$ agents holding two strategies over $P$ histories. We formulate a statistical model that makes use of the gauge freedom with respect to the ordering of an agent's strategies to quantify the correlation between the attendance and the distribution of strategies. The relative score $x\\in\\mathbb{Z}$ of the two strategies of an agent is described in terms of a one dimensional random walk with asymmetric jump probabilities, leading either to a static and asymmetric exponential distribution centered at $x=0$ for fickle agents or to diffusion with a positive or negative drift for frozen agents. In terms of scaled coordinates $x/\\sqrt{N}$ and $t/N$ the distributions are uniquely given by $\\alpha$ and in quantitative agreement with direct simulations of the game. As the model avoids the reformulation in terms of a constrained minimization problem it can be used for arbitrary payoff functions with little calculational effort and provides a transparent and simple formulation of the dynamics of the basic Minority Game in the asymmetric phase."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the asymmetric phase of the basic Minority Game with sign-payoff, how is the relative score x of an agent's two strategies described, and what are the possible outcomes for different types of agents?\n\nA) As a two-dimensional random walk with symmetric jump probabilities, leading to a Gaussian distribution for all agents\n\nB) As a one-dimensional random walk with asymmetric jump probabilities, resulting in either a static exponential distribution for fickle agents or diffusion with drift for frozen agents\n\nC) As a three-dimensional Brownian motion, causing a uniform distribution of scores for all agents regardless of their type\n\nD) As a deterministic process with fixed increments, leading to a linear increase in score difference over time for all agents\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that \"The relative score x\u2208\u2124 of the two strategies of an agent is described in terms of a one dimensional random walk with asymmetric jump probabilities.\" It then specifies two possible outcomes: \"leading either to a static and asymmetric exponential distribution centered at x=0 for fickle agents or to diffusion with a positive or negative drift for frozen agents.\"\n\nOption A is incorrect because it describes a two-dimensional walk with symmetric probabilities, which is not mentioned in the text. Option C is wrong as it introduces a three-dimensional Brownian motion and a uniform distribution, neither of which are discussed in the given information. Option D is incorrect because it describes a deterministic process, whereas the document clearly states that the process is a random walk."}, "55": {"documentation": {"title": "The survival of start-ups in time of crisis. A machine learning approach\n  to measure innovation", "source": "Marco Guerzoni, Consuelo R. Nava, Massimiliano Nuccio", "docs_id": "1911.01073", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The survival of start-ups in time of crisis. A machine learning approach\n  to measure innovation. This paper shows how data science can contribute to improving empirical research in economics by leveraging on large datasets and extracting information otherwise unsuitable for a traditional econometric approach. As a test-bed for our framework, machine learning algorithms allow us to create a new holistic measure of innovation built on a 2012 Italian Law aimed at boosting new high-tech firms. We adopt this measure to analyse the impact of innovativeness on a large population of Italian firms which entered the market at the beginning of the 2008 global crisis. The methodological contribution is organised in different steps. First, we train seven supervised learning algorithms to recognise innovative firms on 2013 firmographics data and select a combination of those with best predicting power. Second, we apply the former on the 2008 dataset and predict which firms would have been labelled as innovative according to the definition of the law. Finally, we adopt this new indicator as regressor in a survival model to explain firms' ability to remain in the market after 2008. Results suggest that the group of innovative firms are more likely to survive than the rest of the sample, but the survival premium is likely to depend on location."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: A study on Italian start-ups during the 2008 global crisis used machine learning to create a new measure of innovation. Which of the following statements most accurately describes the methodology and findings of this research?\n\nA) The study used a single machine learning algorithm to identify innovative firms based on 2008 data, and found that innovative firms had a higher survival rate regardless of their location.\n\nB) The research combined multiple supervised learning algorithms trained on 2013 data to predict innovative firms in 2008, and discovered that innovative firms were more likely to survive, with the survival advantage potentially influenced by location.\n\nC) The study applied unsupervised learning techniques to 2008 firmographics data to classify innovative firms, concluding that innovation had no significant impact on firm survival during the crisis.\n\nD) The research used traditional econometric methods to analyze the impact of a 2012 Italian Law on firm innovation, finding that the law's implementation directly increased the survival rate of high-tech firms.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately captures the key aspects of the study's methodology and findings. The research used multiple supervised learning algorithms (seven are mentioned) trained on 2013 data to create a measure of innovation. This measure was then applied to 2008 data to predict which firms would have been considered innovative. The study found that innovative firms were more likely to survive the 2008 crisis, but importantly, it noted that the survival premium likely depended on location. This nuanced finding is reflected in answer B, while the other options either oversimplify the methodology, misstate the findings, or describe approaches not mentioned in the given information."}, "56": {"documentation": {"title": "Photophysics of GaN single photon sources in the visible spectral range", "source": "Amanuel M. Berhane, Kwang-Yong Jeong, Carlo Bradac, Michael Walsh,\n  Dirk Englund, Milos Toth, and Igor Aharonovich", "docs_id": "1708.09161", "section": ["quant-ph", "cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photophysics of GaN single photon sources in the visible spectral range. In this work, we present a detailed photophysical analysis of recently-discovered optically stable, single photon emitters (SPEs) in Gallium Nitride (GaN). Temperature-resolved photoluminescence measurements reveal that the emission lines at 4 K are three orders of magnitude broader than the transform-limited widths expected from excited state lifetime measurements. The broadening is ascribed to ultra-fast spectral diffusion. Continuing the photophysics study on several emitters at room temperature (RT), a maximum average brightness of ~427 kCounts/s is measured. Furthermore, by determining the decay rates of emitters undergoing three-level optical transitions, radiative and non-radiative lifetimes are calculated at RT. Finally, polarization measurements from 14 emitters are used to determine visibility as well as dipole orientation of defect systems within the GaN crystal. Our results underpin some of the fundamental properties of SPE in GaN both at cryogenic and RT, and define the benchmark for future work in GaN-based single-photon technologies."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Based on the photophysical analysis of single photon emitters (SPEs) in Gallium Nitride (GaN), which of the following statements is correct regarding the emission line broadening observed at 4 K?\n\nA) The emission lines are exactly as narrow as predicted by the transform-limited widths from excited state lifetime measurements.\n\nB) The emission lines are three orders of magnitude narrower than the transform-limited widths expected from excited state lifetime measurements.\n\nC) The emission lines are three orders of magnitude broader than the transform-limited widths expected from excited state lifetime measurements, likely due to slow spectral diffusion.\n\nD) The emission lines are three orders of magnitude broader than the transform-limited widths expected from excited state lifetime measurements, attributed to ultra-fast spectral diffusion.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"Temperature-resolved photoluminescence measurements reveal that the emission lines at 4 K are three orders of magnitude broader than the transform-limited widths expected from excited state lifetime measurements. The broadening is ascribed to ultra-fast spectral diffusion.\" This directly corresponds to option D, which accurately describes both the magnitude of broadening and the attributed cause.\n\nOption A is incorrect because it contradicts the observed broadening. Option B is incorrect as it suggests narrowing instead of broadening. Option C is close but incorrectly attributes the broadening to slow spectral diffusion, whereas the documentation specifies ultra-fast spectral diffusion.\n\nThis question tests the student's ability to carefully read and interpret scientific findings, distinguishing between similar but critically different statements about spectral properties of quantum emitters."}, "57": {"documentation": {"title": "Quality analysis in acyclic production networks", "source": "Abraham Gutierrez, Sebastian Mueller", "docs_id": "1906.11609", "section": ["stat.AP", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quality analysis in acyclic production networks. The production network under examination consists of a number of workstations. Each workstation is a parallel configuration of machines performing the same kind of tasks on a given part. Parts move from one workstation to another and at each workstation a part is assigned randomly to a machine. We assume that the production network is acyclic, that is, a part does not return to a workstation where it previously received service. Furthermore, we assume that the quality of the end product is additive, that is, the sum of the quality contributions of the machines along the production path. The contribution of each machine is modeled by a separate random variable. Our main result is the construction of estimators that allow pairwise and multiple comparison of the means and variances of machines in the same workstation. These comparisons then may lead to the identification of unreliable machines. We also discuss the asymptotic distributions of the estimators that allow the use of standard statistical tests and decision making."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In an acyclic production network with multiple workstations, each containing parallel machines, what is the primary purpose of the estimators constructed according to the main result of the study?\n\nA) To determine the optimal sequence of workstations for maximizing product quality\nB) To compare the means and variances of machines within the same workstation for identifying unreliable machines\nC) To calculate the total quality contribution of all machines in the production network\nD) To predict the failure rate of individual machines based on their quality contributions\n\nCorrect Answer: B\n\nExplanation: The main result of the study, as stated in the documentation, is \"the construction of estimators that allow pairwise and multiple comparison of the means and variances of machines in the same workstation.\" The purpose of these comparisons is explicitly mentioned as potentially leading to \"the identification of unreliable machines.\" Therefore, option B correctly captures the primary purpose of the constructed estimators.\n\nOption A is incorrect because the study doesn't focus on determining optimal sequences of workstations. Option C is not the main purpose of the estimators, as the total quality is assumed to be additive and doesn't require special estimation techniques. Option D, while related to machine reliability, goes beyond the scope of what the estimators are designed to do according to the given information."}, "58": {"documentation": {"title": "Financial series prediction using Attention LSTM", "source": "Sangyeon Kim, Myungjoo Kang", "docs_id": "1902.10877", "section": ["cs.LG", "q-fin.ST", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Financial series prediction using Attention LSTM. Financial time series prediction, especially with machine learning techniques, is an extensive field of study. In recent times, deep learning methods (especially time series analysis) have performed outstandingly for various industrial problems, with better prediction than machine learning methods. Moreover, many researchers have used deep learning methods to predict financial time series with various models in recent years. In this paper, we will compare various deep learning models, such as multilayer perceptron (MLP), one-dimensional convolutional neural networks (1D CNN), stacked long short-term memory (stacked LSTM), attention networks, and weighted attention networks for financial time series prediction. In particular, attention LSTM is not only used for prediction, but also for visualizing intermediate outputs to analyze the reason of prediction; therefore, we will show an example for understanding the model prediction intuitively with attention vectors. In addition, we focus on time and factors, which lead to an easy understanding of why certain trends are predicted when accessing a given time series table. We also modify the loss functions of the attention models with weighted categorical cross entropy; our proposed model produces a 0.76 hit ratio, which is superior to those of other methods for predicting the trends of the KOSPI 200."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the key advantages and innovations of the Attention LSTM model as presented in the research paper?\n\nA) It consistently outperforms traditional machine learning methods in all financial prediction tasks.\n\nB) It allows for visualization of intermediate outputs, providing insight into the reasoning behind predictions and focuses on time and factors for easier interpretation of trend predictions.\n\nC) It exclusively uses weighted categorical cross entropy as its loss function, which is the sole reason for its superior performance.\n\nD) It combines 1D CNN and stacked LSTM architectures to achieve the highest hit ratio among all deep learning models.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the paper specifically highlights two key advantages of the Attention LSTM model:\n\n1. It allows for visualization of intermediate outputs to analyze the reason for prediction, which provides insight into the model's decision-making process.\n\n2. It focuses on time and factors, leading to an easier understanding of why certain trends are predicted when accessing a given time series table.\n\nAnswer A is incorrect because while deep learning methods have shown better performance than traditional machine learning methods, the paper doesn't claim that Attention LSTM consistently outperforms all other methods in all financial prediction tasks.\n\nAnswer C is incorrect because although the paper mentions modifying loss functions with weighted categorical cross entropy, this is not presented as the sole reason for the model's performance, nor is it exclusive to Attention LSTM.\n\nAnswer D is incorrect because the paper does not mention combining 1D CNN and stacked LSTM in this way. While these architectures are discussed, they are presented as separate models for comparison, not as components of the Attention LSTM model."}, "59": {"documentation": {"title": "Controlling volatility of wind-solar power", "source": "Hans Lustfeld", "docs_id": "2102.00587", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Controlling volatility of wind-solar power. The main advantage of wind and solar power plants is the power production free of CO2. Their main disadvantage is the volatility of the generated power. According to the estimates of H.-W. Sinn[1], suppressing this volatility requires pumped-storage plants with a huge capacity, several orders of magnitude larger than the present available capacity in Germany[2]. Sinn concluded that wind-solar power can be used only together with conventional power plants as backups. However, based on German power data[3] of 2019 we show that the required storage capacity can significantly be reduced, provided i) a surplus of wind-solar power plants is supplied, ii) smart meters are installed, iii) partly a different kind of wind turbines and solar panels are used in Germany. Our calculations suggest that all the electric energy, presently produced in Germany, can be obtained from wind-solar power alone. And our results let us predict that wind-solar power can be used to produce in addition the energy for transportation, warm water, space heating and in part for process heating, meaning an increase of the present electric energy production by a factor of about 5[1]. Of course, to put such a prediction on firm ground the present calculations have to be confirmed for a period of many years. And it should be kept in mind, that in any case a huge number of wind turbines and solar panels is required."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: According to the passage, which combination of factors could significantly reduce the required storage capacity for wind-solar power systems in Germany, contrary to H.-W. Sinn's estimates?\n\nA) Installing smart meters and using conventional power plants as backups\nB) Supplying a surplus of wind-solar power plants, installing smart meters, and using partly different types of wind turbines and solar panels\nC) Increasing pumped-storage plant capacity and using conventional power plants as backups\nD) Focusing solely on increasing the number of wind turbines and solar panels\n\nCorrect Answer: B\n\nExplanation: The passage states that the required storage capacity can be significantly reduced by implementing three key factors: i) supplying a surplus of wind-solar power plants, ii) installing smart meters, and iii) using partly different types of wind turbines and solar panels in Germany. This combination of factors directly contradicts Sinn's conclusion that wind-solar power can only be used with conventional power plants as backups. Option B correctly captures all three elements mentioned in the text as ways to reduce storage capacity requirements.\n\nOption A is incorrect because it doesn't mention the surplus of wind-solar power plants or the use of different types of turbines and panels. Option C is incorrect as it aligns with Sinn's original estimates rather than the new findings. Option D is incorrect because it only focuses on increasing the number of wind turbines and solar panels, which is mentioned as necessary but not sufficient to reduce storage capacity requirements."}}