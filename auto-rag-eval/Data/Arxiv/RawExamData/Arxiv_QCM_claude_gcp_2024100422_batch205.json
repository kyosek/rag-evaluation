{"0": {"documentation": {"title": "Sentiment Correlation in Financial News Networks and Associated Market\n  Movements", "source": "Xingchen Wan, Jie Yang, Slavi Marinov, Jan-Peter Calliess, Stefan\n  Zohren, Xiaowen Dong", "docs_id": "2011.06430", "section": ["cs.SI", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sentiment Correlation in Financial News Networks and Associated Market\n  Movements. In an increasingly connected global market, news sentiment towards one company may not only indicate its own market performance, but can also be associated with a broader movement on the sentiment and performance of other companies from the same or even different sectors. In this paper, we apply NLP techniques to understand news sentiment of 87 companies among the most reported on Reuters for a period of seven years. We investigate the propagation of such sentiment in company networks and evaluate the associated market movements in terms of stock price and volatility. Our results suggest that, in certain sectors, strong media sentiment towards one company may indicate a significant change in media sentiment towards related companies measured as neighbours in a financial network constructed from news co-occurrence. Furthermore, there exists a weak but statistically significant association between strong media sentiment and abnormal market return as well as volatility. Such an association is more significant at the level of individual companies, but nevertheless remains visible at the level of sectors or groups of companies."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements most accurately reflects the findings of the study on sentiment correlation in financial news networks and associated market movements?\n\nA) Strong media sentiment towards a company always leads to significant changes in stock prices of related companies in the same sector.\n\nB) The association between media sentiment and market performance is equally strong at both individual company and sector levels.\n\nC) News sentiment about a company can potentially indicate broader market movements, with a weak but statistically significant association to abnormal returns and volatility.\n\nD) The study found no correlation between media sentiment and market performance for companies in different sectors.\n\nCorrect Answer: C\n\nExplanation: Option C is the most accurate reflection of the study's findings. The research suggests that strong media sentiment towards one company may indicate changes in sentiment towards related companies, and there exists a weak but statistically significant association between strong media sentiment and abnormal market return as well as volatility. This association is more pronounced at the individual company level but is still observable at the sector level.\n\nOption A is incorrect because the study does not claim that strong media sentiment always leads to significant changes in stock prices of related companies. The findings are more nuanced and speak of potential indications rather than definitive outcomes.\n\nOption B is inaccurate because the study explicitly states that the association between media sentiment and market performance is more significant at the level of individual companies, but remains visible (though less significant) at the sector level.\n\nOption D is incorrect as the study does not claim there is no correlation between media sentiment and market performance for companies in different sectors. In fact, it suggests that sentiment can propagate even between companies from different sectors in some cases."}, "1": {"documentation": {"title": "Inverted effective SUSY with combined Z' and gravity mediation, and muon\n  anomalous magnetic moment", "source": "Jihn E. Kim", "docs_id": "1208.5484", "section": ["hep-ph", "astro-ph.CO", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inverted effective SUSY with combined Z' and gravity mediation, and muon\n  anomalous magnetic moment. Effective supersymmetry(SUSY) where stop is the lightest squark may run into a two-loop tachyonic problem in some Z' mediation models. In addition, a large A term or/and a large stop mass are needed to have about a 126 GeV Higgs boson with three families of quarks and leptons. Thus, we suggest an inverted effective SUSY(IeffSUSY) where stop mass is larger compared to those of the first two families. In this case, it is possible to have a significant correction to the anomalous magnetic moment of muon. A three family IeffSUSY in a Z' mediation scenario is explicitly studied with the Z' quantum number related to B-L. Here, we adopt both the Z' mediation and gravity mediation where the Z' mediation is the dominant one for stop, while the gravity mediation is the dominant one for the muonic leptons and Higgs multiplets. We present a numerical study based on a specific anomaly free model, and show the existence of the parameter region where all the phenomenological conditions are satisfied."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of Inverted Effective Supersymmetry (IeffSUSY) with combined Z' and gravity mediation, which of the following statements is correct?\n\nA) The stop is the lightest squark in this model, avoiding the two-loop tachyonic problem associated with some Z' mediation models.\n\nB) The gravity mediation is the dominant mechanism for stop mass generation, while Z' mediation primarily affects muonic leptons and Higgs multiplets.\n\nC) IeffSUSY allows for a significant correction to the anomalous magnetic moment of the muon, while maintaining a Higgs boson mass of approximately 126 GeV.\n\nD) The Z' quantum number in this model is unrelated to B-L, and the model requires only two families of quarks and leptons to be consistent.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that in Inverted Effective SUSY (IeffSUSY), the stop mass is larger compared to those of the first two families, unlike traditional effective SUSY. This setup allows for a significant correction to the anomalous magnetic moment of the muon. Additionally, the model maintains the ability to have a Higgs boson mass of about 126 GeV, which is consistent with observations.\n\nOption A is incorrect because IeffSUSY specifically has a heavier stop, not the lightest squark. Option B is incorrect as the documentation states that Z' mediation is dominant for stop, while gravity mediation is dominant for muonic leptons and Higgs multiplets. Option D is incorrect on both counts: the Z' quantum number is related to B-L according to the text, and the model considers three families of quarks and leptons, not two."}, "2": {"documentation": {"title": "Stable controllable giant vortex in a trapped Bose-Einstein condensate", "source": "S. K. Adhikari", "docs_id": "1906.11108", "section": ["cond-mat.quant-gas", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stable controllable giant vortex in a trapped Bose-Einstein condensate. In a harmonically-trapped rotating Bose-Einstein condensate (BEC), a vortex of large angular momentum decays to multiple vortices of unit angular momentum from an energetic consideration. We demonstrate the formation of a robust and dynamically stable giant vortex of large angular momentum in a harmonically trapped rotating BEC with a potential hill at the center, thus forming a Mexican hat like trapping potential. For a small inter-atomic interaction strength, a highly controllable stable giant vortex appears, whose angular momentum slowly increases as the angular frequency of rotation is increased. As the inter-atomic interaction strength is increased beyond a critical value, only vortices of unit angular momentum are formed, unless the strength of the potential hill at the center is also increased: for a stronger potential hill at the center a giant vortex is again formed. The dynamical stability of the giant vortex is demonstrated by real-time propagation numerically. These giant vortices of large angular momentum can be observed and studied experimentally in a highly controlled fashion."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a harmonically-trapped rotating Bose-Einstein condensate (BEC) with a potential hill at the center, which of the following statements is correct regarding the formation and stability of giant vortices?\n\nA) Giant vortices are always unstable and decay into multiple vortices of unit angular momentum, regardless of the inter-atomic interaction strength or potential hill strength.\n\nB) Giant vortices can only form when the inter-atomic interaction strength is above a critical value, and their angular momentum decreases as the angular frequency of rotation increases.\n\nC) Stable giant vortices with large angular momentum can form when the inter-atomic interaction strength is small, and their angular momentum slowly increases as the angular frequency of rotation increases.\n\nD) The formation of giant vortices is solely dependent on the strength of the potential hill at the center and is not affected by changes in the inter-atomic interaction strength.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the documentation, for a small inter-atomic interaction strength, a highly controllable stable giant vortex appears in a harmonically trapped rotating BEC with a potential hill at the center. The angular momentum of this giant vortex slowly increases as the angular frequency of rotation is increased. \n\nAnswer A is incorrect because the documentation states that giant vortices can be stable under certain conditions, not always unstable.\n\nAnswer B is incorrect on two counts: first, giant vortices form when the inter-atomic interaction strength is small, not above a critical value; second, their angular momentum increases, not decreases, with increasing angular frequency of rotation.\n\nAnswer D is incorrect because the formation of giant vortices is affected by both the inter-atomic interaction strength and the strength of the potential hill at the center. When the inter-atomic interaction strength increases beyond a critical value, a stronger potential hill is needed to form giant vortices again."}, "3": {"documentation": {"title": "The effective QCD phase diagram and the critical end point", "source": "Alejandro Ayala, Adnan Bashir, J.J. Cobos-Martinez, Saul\n  Hernandez-Ortiz, Alfredo Raya", "docs_id": "1411.4953", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The effective QCD phase diagram and the critical end point. We study the QCD phase diagram on the plane of temperature T and quark chemical potential mu, modelling the strong interactions with the linear sigma model coupled to quarks. The phase transition line is found from the effective potential at finite T and mu taking into accounts the plasma screening effects. We find the location of the critical end point (CEP) to be (mu^CEP/T_c,T^CEP/T_c) sim (1.2,0.8), where T_c is the (pseudo)critical temperature for the crossover phase transition at vanishing mu. This location lies within the region found by lattice inspired calculations. The results show that in the linear sigma model, the CEP's location in the phase diagram is expectedly determined solely through chiral symmetry breaking. The same is likely to be true for all other models which do not exhibit confinement, provided the proper treatment of the plasma infrared properties for the description of chiral symmetry restoration is implemented. Similarly, we also expect these corrections to be substantially relevant in the QCD phase diagram."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the study of the QCD phase diagram using the linear sigma model coupled to quarks, which of the following statements is correct regarding the critical end point (CEP) and its implications?\n\nA) The CEP is located at (\u03bc^CEP/T_c, T^CEP/T_c) \u2248 (0.8, 1.2), where T_c is the critical temperature for the first-order phase transition at zero chemical potential.\n\nB) The location of the CEP in the linear sigma model is determined primarily by the confinement properties of the model.\n\nC) The study suggests that plasma screening effects are negligible in determining the phase transition line and the location of the CEP.\n\nD) The results indicate that for models without confinement, proper treatment of plasma infrared properties is crucial for accurately describing chiral symmetry restoration and locating the CEP.\n\nCorrect Answer: D\n\nExplanation: \nOption A is incorrect because the given CEP coordinates are reversed and T_c is described incorrectly. The correct location is (\u03bc^CEP/T_c, T^CEP/T_c) \u2248 (1.2, 0.8), and T_c is the (pseudo)critical temperature for the crossover phase transition at vanishing \u03bc.\n\nOption B is incorrect because the study explicitly states that in the linear sigma model, the CEP's location is determined solely through chiral symmetry breaking, not confinement properties.\n\nOption C is incorrect because the study emphasizes the importance of taking into account plasma screening effects when finding the phase transition line from the effective potential.\n\nOption D is correct. The study concludes that for models without confinement, like the linear sigma model, proper treatment of plasma infrared properties is essential for accurately describing chiral symmetry restoration and determining the CEP's location. This is also expected to be relevant for the QCD phase diagram."}, "4": {"documentation": {"title": "Future of work: ethics", "source": "David Pastor-Escuredo", "docs_id": "2104.02580", "section": ["cs.CY", "cs.AI", "cs.HC", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Future of work: ethics. Work must be reshaped in the upcoming new era characterized by new challenges and the presence of new technologies and computational tools. Over-automation seems to be the driver of the digitalization process. Substitution is the paradigm leading Artificial Intelligence and robotics development against human cognition. Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities. Digital technology is characterized also by scalability because of its easy and inexpensive deployment. Thus, automation can lead to the absence of jobs and scalable negative impact in human development and the performance of business. A look at digitalization from the lens of Sustainable Development Goals can tell us how digitalization impact in different sectors and areas considering society as a complex interconnected system. Here, reflections on how AI and Data impact future of work and sustainable development are provided grounded on an ethical core that comprises human-level principles and also systemic principles."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best represents the ethical approach to the future of work as described in the passage?\n\nA) Over-automation and substitution of human labor with AI should be prioritized to maximize efficiency and productivity in all sectors.\n\nB) Digital technology should be designed to completely replace human cognition and capacities in order to eliminate human error from work processes.\n\nC) The scalability of digital technology justifies its widespread implementation across all industries, regardless of potential negative impacts on employment.\n\nD) Digital technology should be developed to augment human skills and improve the productive use of human cognition while considering the complex interconnections within society and aligning with Sustainable Development Goals.\n\nCorrect Answer: D\n\nExplanation: The passage emphasizes that work must be reshaped in light of new challenges and technologies, but cautions against over-automation and the substitution paradigm. Instead, it advocates for digital technology that enhances human skills and makes more productive use of human cognition. The text also highlights the importance of considering the Sustainable Development Goals and viewing society as a complex interconnected system when implementing digital technologies. This holistic and ethical approach is best captured in option D, which balances technological advancement with human-centric design and broader societal considerations."}, "5": {"documentation": {"title": "On the Human Control of a Multiple Quadcopters with a Cable-suspended\n  Payload System", "source": "Pratik Prajapati, Sagar Parekh, and Vineet Vashista", "docs_id": "2004.01841", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Human Control of a Multiple Quadcopters with a Cable-suspended\n  Payload System. A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of controlling multiple quadcopters with a cable-suspended payload, which of the following statements best describes the control architecture and its implications?\n\nA) The system uses a centralized control where all quadcopters are directly controlled by the human operator, allowing for maximum flexibility but increasing cognitive load.\n\nB) A decentralized control system is implemented where each quadcopter independently manages its position and orientation, potentially leading to conflicts in payload management.\n\nC) The leader-follower architecture allows for human control of the leader quadcopter, while followers use Payload Attitude Controller and Cable Attitude Controller, enabling efficient control in unknown environments.\n\nD) All quadcopters are autonomously controlled using advanced AI algorithms, completely eliminating the need for human input in payload transportation tasks.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states that a leader-follower architecture is used, where the leader quadcopter is directly controlled by a human operator, while the follower quadcopters are managed by the Payload Attitude Controller and Cable Attitude Controller. This approach strikes a balance between human control and automated assistance, making it suitable for on-the-go maneuvering in unknown environments.\n\nAnswer A is incorrect because it suggests all quadcopters are directly controlled by the human, which would be overly complex and not align with the described leader-follower architecture.\n\nAnswer B is incorrect as it describes a decentralized system, which could lead to conflicts in payload management. The documented approach uses coordinated control through the leader-follower structure.\n\nAnswer D is incorrect because it suggests full autonomy without human input, which contradicts the paper's focus on human control and the stated advantage of manual control for unknown environments."}, "6": {"documentation": {"title": "One-loop weak corrections to Higgs production", "source": "Valentin Hirschi, Simone Lionetti, Armin Schweitzer", "docs_id": "1902.10167", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "One-loop weak corrections to Higgs production. We compute mixed QCD-weak corrections to inclusive Higgs production at the LHC from the partonic process $g g \\rightarrow H q \\bar{q}$. We start from the UV- and IR-finite one-loop weak amplitude and consider its interference with the corresponding one-loop QCD amplitude. This contribution is a $\\mathcal{O}(\\alpha_s\\alpha)$ correction to the leading-order gluon-fusion cross section, and was not numerically assessed in previous works. We also compute the cross section from the square of this weak amplitude, suppressed by $\\mathcal{O}(\\alpha^2)$. Finally, we consider contributions from the partonic process $g q \\rightarrow H q$, which are one order lower in $\\alpha_s$, as a reference for the size of terms which are not enhanced by the large gluon luminosity. We find that, given the magnitude of the uncertainties on current state-of-the-art predictions for Higgs production, all contributions computed in this work can be safely ignored, both fully inclusively and in the boosted Higgs regime. This result supports the approximate factorisation of QCD and weak corrections to that process."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of Higgs production at the LHC, which of the following statements is correct regarding the mixed QCD-weak corrections computed from the partonic process $g g \\rightarrow H q \\bar{q}$?\n\nA) The interference between the one-loop weak amplitude and the one-loop QCD amplitude represents an $\\mathcal{O}(\\alpha_s^2\\alpha)$ correction to the leading-order gluon-fusion cross section.\n\nB) The square of the one-loop weak amplitude contributes a correction suppressed by $\\mathcal{O}(\\alpha)$ relative to the leading-order process.\n\nC) The computed corrections, including those from $g q \\rightarrow H q$, are significant and must be included in state-of-the-art predictions for Higgs production.\n\nD) The results support the approximate factorisation of QCD and weak corrections in Higgs production, and the computed contributions can be safely ignored given current uncertainties.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that all contributions computed in this work, including the mixed QCD-weak corrections from $g g \\rightarrow H q \\bar{q}$ and contributions from $g q \\rightarrow H q$, can be safely ignored given the current uncertainties in Higgs production predictions. This conclusion supports the approximate factorisation of QCD and weak corrections to the process.\n\nOption A is incorrect because the interference term is described as an $\\mathcal{O}(\\alpha_s\\alpha)$ correction, not $\\mathcal{O}(\\alpha_s^2\\alpha)$.\n\nOption B is incorrect as the square of the weak amplitude is suppressed by $\\mathcal{O}(\\alpha^2)$, not $\\mathcal{O}(\\alpha)$.\n\nOption C directly contradicts the conclusion of the study, which states that these corrections can be safely ignored."}, "7": {"documentation": {"title": "In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication", "source": "Takashi Nakamura, Valentin Goverdovsky, Danilo P. Mandic", "docs_id": "1705.03742", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication. The use of EEG as a biometrics modality has been investigated for about a decade, however its feasibility in real-world applications is not yet conclusively established, mainly due to the issues with collectability and reproducibility. To this end, we propose a readily deployable EEG biometrics system based on a `one-fits-all' viscoelastic generic in-ear EEG sensor (collectability), which does not require skilled assistance or cumbersome preparation. Unlike most existing studies, we consider data recorded over multiple recording days and for multiple subjects (reproducibility) while, for rigour, the training and test segments are not taken from the same recording days. A robust approach is considered based on the resting state with eyes closed paradigm, the use of both parametric (autoregressive model) and non-parametric (spectral) features, and supported by simple and fast cosine distance, linear discriminant analysis and support vector machine classifiers. Both the verification and identification forensics scenarios are considered and the achieved results are on par with the studies based on impractical on-scalp recordings. Comprehensive analysis over a number of subjects, setups, and analysis features demonstrates the feasibility of the proposed ear-EEG biometrics, and its potential in resolving the critical collectability, robustness, and reproducibility issues associated with current EEG biometrics."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following combinations best addresses the main challenges in EEG biometrics as described in the study?\n\nA) In-ear sensor, resting state with eyes open, non-parametric features only, data from a single recording day\nB) On-scalp electrodes, task-based paradigm, parametric features only, data from multiple recording days\nC) In-ear sensor, resting state with eyes closed, both parametric and non-parametric features, data from multiple recording days\nD) Generic scalp electrodes, eyes open and closed paradigms, spectral features only, data from non-consecutive days\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the key elements of the proposed EEG biometrics system described in the text. The study emphasizes:\n\n1. The use of a 'one-fits-all' viscoelastic generic in-ear EEG sensor to address collectability issues.\n2. A resting state with eyes closed paradigm for robustness.\n3. The use of both parametric (autoregressive model) and non-parametric (spectral) features.\n4. Data recorded over multiple recording days to ensure reproducibility.\n\nOption A is incorrect because it only uses non-parametric features and data from a single day, which doesn't address reproducibility. Option B is wrong as it uses on-scalp electrodes, which the study aims to avoid due to collectability issues. Option D is incorrect because it uses generic scalp electrodes instead of in-ear sensors and only spectral features, not both parametric and non-parametric as proposed in the study."}, "8": {"documentation": {"title": "Effects of the direct light in the surface detectors (SD) of the Pierre\n  Auger Observatory and their change in time", "source": "Pedro Alfonso Valencia Esquipula, Karen Salom\\'e Caballero Mora", "docs_id": "1703.07422", "section": ["physics.ins-det", "astro-ph.HE", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of the direct light in the surface detectors (SD) of the Pierre\n  Auger Observatory and their change in time. Cosmic Rays (CR) are particles which come to the earth from Universe. Their origin and production mechanisms are still unknown. The Pierre Auger Observatory is located in Mendoza, Argentina. It is dedicated to the study of CR. When CR arrive to the earth's atmosphere they produce a shower of secondary particles called \\textit{air shower}. The surface detector (SD) of the Pierre Auger Observatory consists of tanks full of pure water, where CR produce \\textit{Cherenkov radiation}, when going through them. This light is detected by three photomultiplier tubes (PMT) located on the top of each tank. Depending of the angle of arrival direction of the primary CR, each PMT is able to register different signal than the other. The goal of this study is to look at these effects of direct light on the PMT's to explore if they change in time. The obtained results may give information about the physical status of the tanks in order to monitor the work of the SD, and to estimate possible systematic effects on the measurements. The current results of this study are shown."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: The Pierre Auger Observatory studies cosmic rays by detecting Cherenkov radiation in water tanks. Which of the following statements best describes the purpose and potential outcomes of analyzing the effects of direct light on the photomultiplier tubes (PMTs) in these detectors over time?\n\nA) To determine the origin of cosmic rays in the universe\nB) To calibrate the energy measurements of incoming cosmic rays\nC) To monitor the physical condition of the detectors and identify potential systematic errors in measurements\nD) To improve the angular resolution of cosmic ray arrival directions\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage explicitly states that \"The goal of this study is to look at these effects of direct light on the PMT's to explore if they change in time. The obtained results may give information about the physical status of the tanks in order to monitor the work of the SD, and to estimate possible systematic effects on the measurements.\"\n\nOption A is incorrect because while the Pierre Auger Observatory studies cosmic rays, this specific analysis does not directly determine their origin.\n\nOption B is not mentioned in the passage and is not the primary goal of this particular study.\n\nOption D, while related to the overall function of the observatory, is not the focus of the direct light effects study described in the passage.\n\nThis question tests the student's ability to comprehend the specific goals of a scientific study within the broader context of cosmic ray research, requiring careful reading and interpretation of the given information."}, "9": {"documentation": {"title": "Performance Measurement for Deep Bayesian Neural Network", "source": "Yikuan Li and Yajie Zhu", "docs_id": "1903.08674", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Performance Measurement for Deep Bayesian Neural Network. Deep Bayesian neural network has aroused a great attention in recent years since it combines the benefits of deep neural network and probability theory. Because of this, the network can make predictions and quantify the uncertainty of the predictions at the same time, which is important in many life-threatening areas. However, most of the recent researches are mainly focusing on making the Bayesian neural network easier to train, and proposing methods to estimate the uncertainty. I notice there are very few works that properly discuss the ways to measure the performance of the Bayesian neural network. Although accuracy and average uncertainty are commonly used for now, they are too general to provide any insight information about the model. In this paper, we would like to introduce more specific criteria and propose several metrics to measure the model performance from different perspectives, which include model calibration measurement, data rejection ability and uncertainty divergence for samples from the same and different distributions."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the main contribution of the paper on Performance Measurement for Deep Bayesian Neural Networks?\n\nA) It proposes new training methods to make Bayesian neural networks easier to implement.\nB) It introduces novel algorithms for estimating uncertainty in deep learning models.\nC) It presents new metrics for evaluating Bayesian neural network performance beyond accuracy and average uncertainty.\nD) It compares the performance of Bayesian neural networks against traditional deep learning models.\n\nCorrect Answer: C\n\nExplanation: The key contribution of this paper is the introduction of new, more specific criteria and metrics for measuring the performance of Bayesian neural networks. The authors note that while accuracy and average uncertainty are commonly used, these measures are too general to provide detailed insights about the model. Instead, they propose several new metrics to assess performance from different perspectives, including model calibration measurement, data rejection ability, and uncertainty divergence for samples from the same and different distributions. This approach aims to provide a more comprehensive and nuanced evaluation of Bayesian neural network performance, addressing a gap in the current research which has mostly focused on training methods and uncertainty estimation."}, "10": {"documentation": {"title": "Hilbert's tenth problem, G\\\"odel's incompleteness, Halting problem, a\n  unifying perspective", "source": "Tarek Sayed Ahmed", "docs_id": "1812.00990", "section": ["math.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hilbert's tenth problem, G\\\"odel's incompleteness, Halting problem, a\n  unifying perspective. We formulate a property $P$ on a class of relations on the natural numbers, and formulate a general theorem on $P$, from which we get as corollaries the insolvability of Hilbert's tenth problem, G\\\"odel's incompleteness theorem, and Turing's halting problem. By slightly strengthening the property $P$, we get Tarski's definability theorem, namely that truth is not first order definable. The property $P$ together with a \"Cantor's diagonalization\" process emphasizes that all the above theorems are a variation on a theme, that of self reference and diagonalization combined. We relate our results to self referential paradoxes, including a formalisation of the Liar paradox, and fixed point theorems. We also discuss the property $P$ for arbitrary rings. We give a survey on Hilbert's tenth problem for quadratic rings and for the rationals pointing the way to ongoing research in main stream mathematics involving recursion theory, definability in model theory, algebraic geometry and number theory."}, "answer": "Question: Which of the following statements best describes the unifying perspective presented in the document for various fundamental theorems and problems in mathematical logic?\n\nA) The insolvability of these problems is primarily due to the limitations of first-order logic.\n\nB) Self-reference and diagonalization combined with a property P on relations of natural numbers form the core of these results.\n\nC) Tarski's definability theorem is fundamentally different from the other results mentioned.\n\nD) The halting problem and Hilbert's tenth problem are unrelated to G\u00f6del's incompleteness theorem.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the document explicitly states that the property P together with a \"Cantor's diagonalization\" process emphasizes that all the mentioned theorems (including Hilbert's tenth problem, G\u00f6del's incompleteness theorem, and the halting problem) are variations on a theme of self-reference and diagonalization combined. This unifying perspective is the central idea presented in the text.\n\nOption A is incorrect because while first-order logic has limitations, the document doesn't present this as the primary reason for the insolvability of these problems.\n\nOption C is incorrect because the document actually shows how Tarski's definability theorem can be derived from a slightly strengthened version of the same property P, indicating it's closely related to the other results.\n\nOption D is incorrect as the document explicitly presents these problems as related through the common theme of self-reference and diagonalization."}, "11": {"documentation": {"title": "Effective Description of Higher-Order Scalar-Tensor Theories", "source": "David Langlois, Michele Mancarella, Karim Noui, Filippo Vernizzi", "docs_id": "1703.03797", "section": ["hep-th", "astro-ph.CO", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effective Description of Higher-Order Scalar-Tensor Theories. Most existing theories of dark energy and/or modified gravity, involving a scalar degree of freedom, can be conveniently described within the framework of the Effective Theory of Dark Energy, based on the unitary gauge where the scalar field is uniform. We extend this effective approach by allowing the Lagrangian in unitary gauge to depend on the time derivative of the lapse function. Although this dependence generically signals the presence of an extra scalar degree of freedom, theories that contain only one propagating scalar degree of freedom, in addition to the usual tensor modes, can be constructed by requiring the initial Lagrangian to be degenerate. Starting from a general quadratic action, we derive the dispersion relations for the linear perturbations around Minkowski and a cosmological background. Our analysis directly applies to the recently introduced Degenerate Higher-Order Scalar-Tensor (DHOST) theories. For these theories, we find that one cannot recover a Poisson-like equation in the static linear regime except for the subclass that includes the Horndeski and so-called \"beyond Horndeski\" theories. We also discuss Lorentz-breaking models inspired by Horava gravity."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Effective Theory of Dark Energy, which of the following statements is correct regarding the extension that allows the Lagrangian in unitary gauge to depend on the time derivative of the lapse function?\n\nA) This extension always results in multiple propagating scalar degrees of freedom.\n\nB) This extension necessarily violates the unitary gauge condition.\n\nC) This extension can maintain a single propagating scalar degree of freedom if the initial Lagrangian is degenerate.\n\nD) This extension is incompatible with theories that include tensor modes.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"Although this dependence generically signals the presence of an extra scalar degree of freedom, theories that contain only one propagating scalar degree of freedom, in addition to the usual tensor modes, can be constructed by requiring the initial Lagrangian to be degenerate.\"\n\nOption A is incorrect because while the extension generally signals extra degrees of freedom, it's possible to construct theories with only one propagating scalar degree of freedom.\n\nOption B is incorrect as the extension is explicitly described within the framework of unitary gauge.\n\nOption D is incorrect because the document mentions that these theories can include \"the usual tensor modes\" alongside the scalar degree of freedom.\n\nThis question tests understanding of the subtle implications of extending the Effective Theory of Dark Energy and the conditions under which the number of degrees of freedom can be controlled."}, "12": {"documentation": {"title": "BARCHAN: Blob Alignment for Robust CHromatographic ANalysis", "source": "Camille Couprie, Laurent Duval, Maxime Moreaud, Sophie H\\'enon,\n  M\\'elinda Tebib, Vincent Souchon", "docs_id": "1702.07942", "section": ["cs.CV", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BARCHAN: Blob Alignment for Robust CHromatographic ANalysis. Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role into the elucidation of complex samples. The automation of the identification of peak areas is of prime interest to obtain a fast and repeatable analysis of chromatograms. To determine the concentration of compounds or pseudo-compounds, templates of blobs are defined and superimposed on a reference chromatogram. The templates then need to be modified when different chromatograms are recorded. In this study, we present a chromatogram and template alignment method based on peak registration called BARCHAN. Peaks are identified using a robust mathematical morphology tool. The alignment is performed by a probabilistic estimation of a rigid transformation along the first dimension, and a non-rigid transformation in the second dimension, taking into account noise, outliers and missing peaks in a fully automated way. Resulting aligned chromatograms and masks are presented on two datasets. The proposed algorithm proves to be fast and reliable. It significantly reduces the time to results for GCxGC analysis."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following best describes the BARCHAN method for chromatogram alignment in GCxGC analysis?\n\nA) A technique that uses machine learning to predict peak locations without the need for a reference chromatogram\n\nB) A method that applies only rigid transformations in both dimensions to align chromatograms\n\nC) An approach that uses mathematical morphology for peak identification and combines rigid and non-rigid transformations for alignment\n\nD) A fully manual process where analysts visually identify and align peaks between chromatograms\n\nCorrect Answer: C\n\nExplanation: The BARCHAN (Blob Alignment for Robust CHromatographic ANalysis) method is described in the text as using robust mathematical morphology for peak identification. It then performs alignment through a combination of a rigid transformation along the first dimension and a non-rigid transformation in the second dimension. This approach takes into account noise, outliers, and missing peaks in an automated way. \n\nOption A is incorrect because BARCHAN does not use machine learning and does require a reference chromatogram. \n\nOption B is incorrect because BARCHAN uses a combination of rigid and non-rigid transformations, not just rigid transformations in both dimensions. \n\nOption D is incorrect because BARCHAN is described as a fully automated method, not a manual process.\n\nThe correct answer, C, accurately summarizes the key aspects of the BARCHAN method as described in the provided text."}, "13": {"documentation": {"title": "Statistical properties of volatility return intervals of Chinese stocks", "source": "Fei Ren, Liang Guo, and Wei-Xing Zhou", "docs_id": "0807.1818", "section": ["q-fin.ST", "physics.data-an", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical properties of volatility return intervals of Chinese stocks. The statistical properties of the return intervals $\\tau_q$ between successive 1-min volatilities of 30 liquid Chinese stocks exceeding a certain threshold $q$ are carefully studied. The Kolmogorov-Smirnov (KS) test shows that 12 stocks exhibit scaling behaviors in the distributions of $\\tau_q$ for different thresholds $q$. Furthermore, the KS test and weighted KS test shows that the scaled return interval distributions of 6 stocks (out of the 12 stocks) can be nicely fitted by a stretched exponential function $f(\\tau/\\bar{\\tau})\\sim e^{- \\alpha (\\tau/\\bar{\\tau})^{\\gamma}}$ with $\\gamma\\approx0.31$ under the significance level of 5%, where $\\bar{\\tau}$ is the mean return interval. The investigation of the conditional probability distribution $P_q(\\tau | \\tau_0)$ and the mean conditional return interval $<\\tau| \\tau_0>$ demonstrates the existence of short-term correlation between successive return interval intervals. We further study the mean return interval $<\\tau| \\tau_0>$ after a cluster of $n$ intervals and the fluctuation $F(l)$ using detrended fluctuation analysis and find that long-term memory also exists in the volatility return intervals."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A study on the statistical properties of volatility return intervals of Chinese stocks found that for some stocks, the scaled return interval distributions could be fitted by a stretched exponential function. Which of the following statements best describes the findings and implications of this study?\n\nA) The stretched exponential function fit all 30 stocks studied, indicating universal behavior across the Chinese stock market.\n\nB) The function f(\u03c4/\u03c4\u0304) ~ e^(-\u03b1(\u03c4/\u03c4\u0304)^\u03b3) with \u03b3 \u2248 0.31 fit 6 out of 30 stocks, suggesting potential scaling behavior in a subset of the market.\n\nC) The Kolmogorov-Smirnov test showed that all stocks exhibited scaling behaviors, but only 6 could be fitted with the stretched exponential function.\n\nD) The study found no evidence of scaling behavior or stretched exponential distributions in any of the Chinese stocks analyzed.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that the Kolmogorov-Smirnov (KS) test showed that 12 out of 30 stocks exhibited scaling behaviors in the distributions of \u03c4_q for different thresholds q. Furthermore, it specifies that the scaled return interval distributions of 6 stocks (out of the 12 stocks with scaling behavior) could be nicely fitted by the stretched exponential function f(\u03c4/\u03c4\u0304) ~ e^(-\u03b1(\u03c4/\u03c4\u0304)^\u03b3) with \u03b3 \u2248 0.31 under a 5% significance level.\n\nOption A is incorrect because the function did not fit all 30 stocks, only 6 out of 30. Option C is partially correct about the 12 stocks showing scaling behavior, but incorrectly states that all stocks exhibited this, and misrepresents the number fitted by the stretched exponential function. Option D is entirely incorrect, as the study did find evidence of scaling behavior and stretched exponential distributions in some stocks.\n\nThis question tests the student's ability to carefully read and interpret statistical findings, understand the significance of partial results in a larger sample, and differentiate between related but distinct concepts (scaling behavior vs. fit to a specific function)."}, "14": {"documentation": {"title": "Uncovering fossils of the distant Milky Way with UNIONS: NGC 5466 and\n  its stellar stream", "source": "Jaclyn Jensen, Guillaume Thomas, Alan W. McConnachie, Else\n  Starkenburg, Khyati Malhan, Julio Navarro, Nicolas Martin, Benoit Famaey,\n  Rodrigo Ibata, Scott Chapman, Jean-Charles Cuillandre and Stephen Gwyn", "docs_id": "2108.04340", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uncovering fossils of the distant Milky Way with UNIONS: NGC 5466 and\n  its stellar stream. We examine the spatial clustering of blue horizontal branch (BHB) stars from the $\\textit{u}$-band of the Canada-France Imaging Survey (CFIS, a component of the Ultraviolet Near-Infrared Optical Northern Survey, or UNIONS). All major groupings of stars are associated with previously known satellites, and among these is NGC 5466, a distant (16 kpc) globular cluster. NGC 5466 reportedly possesses a long stellar stream, although no individual members of the stream have previously been identified. Using both BHBs and more numerous red giant branch stars cross-matched to $\\textit{Gaia}$ Data Release 2, we identify extended tidal tails from NGC 5466 that are both spatially and kinematically coherent. Interestingly, we find that this stream does not follow the same path as the previous detection at large distances from the cluster. We trace the stream across 31$^{\\circ}$ of sky and show that it exhibits a very strong distance gradient ranging from 10 $<$ R$_{helio}$ $<$ 30 kpc. We compare our observations to simple dynamical models of the stream and find that they are able to broadly reproduce the overall path and kinematics. The fact that NGC 5466 is so distant, traces a wide range of Galactic distances, has an identified progenitor, and appears to have recently had an interaction with the Galaxy's disk, makes it a unique test-case for dynamical modelling of the Milky Way."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: The study of NGC 5466 and its stellar stream reveals several unique characteristics. Which of the following combinations of features makes NGC 5466 particularly valuable for dynamical modeling of the Milky Way?\n\nA) Its proximity to Earth, short tidal tails, and interaction with the Galactic halo\nB) Its distant location, wide range of Galactic distances covered, and recent interaction with the Galactic disk\nC) Its large population of red giant branch stars, circular orbit, and constant stream distance\nD) Its high proper motion, metal-rich composition, and multiple progenitors\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the passage explicitly states that NGC 5466 is \"unique test-case for dynamical modelling of the Milky Way\" due to it being \"so distant, traces a wide range of Galactic distances, has an identified progenitor, and appears to have recently had an interaction with the Galaxy's disk.\" \n\nOption A is incorrect because NGC 5466 is not close to Earth (it's described as \"distant (16 kpc)\"), and there's no mention of interaction with the Galactic halo.\n\nOption C is incorrect because while red giant branch stars are mentioned, the orbit shape is not specified, and the stream actually exhibits \"a very strong distance gradient\" rather than a constant distance.\n\nOption D is incorrect as the cluster's proper motion and metallicity are not discussed, and only one progenitor (NGC 5466 itself) is mentioned, not multiple progenitors.\n\nThis question tests the student's ability to synthesize multiple pieces of information from the text and identify the key factors that make NGC 5466 valuable for studying Galactic dynamics."}, "15": {"documentation": {"title": "Large-scale Sustainable Search on Unconventional Computing Hardware", "source": "Kirill P. Kalinin and Natalia G. Berloff", "docs_id": "2104.02553", "section": ["cond-mat.dis-nn", "cs.ET", "cs.IR", "physics.comp-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large-scale Sustainable Search on Unconventional Computing Hardware. Since the advent of the Internet, quantifying the relative importance of web pages is at the core of search engine methods. According to one algorithm, PageRank, the worldwide web structure is represented by the Google matrix, whose principal eigenvector components assign a numerical value to web pages for their ranking. Finding such a dominant eigenvector on an ever-growing number of web pages becomes a computationally intensive task incompatible with Moore's Law. We demonstrate that special-purpose optical machines such as networks of optical parametric oscillators, lasers, and gain-dissipative condensates, may aid in accelerating the reliable reconstruction of principal eigenvectors of real-life web graphs. We discuss the feasibility of simulating the PageRank algorithm on large Google matrices using such unconventional hardware. We offer alternative rankings based on the minimisation of spin Hamiltonians. Our estimates show that special-purpose optical machines may provide dramatic improvements in power consumption over classical computing architectures."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the potential advantage of using special-purpose optical machines for implementing the PageRank algorithm, as discussed in the text?\n\nA) They can completely replace classical computing architectures for all web search tasks.\nB) They offer improved accuracy in ranking web pages compared to traditional methods.\nC) They provide significant improvements in power consumption while reliably reconstructing principal eigenvectors of web graphs.\nD) They eliminate the need for representing the web structure using the Google matrix.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The text explicitly states that \"special-purpose optical machines may provide dramatic improvements in power consumption over classical computing architectures\" while being capable of \"accelerating the reliable reconstruction of principal eigenvectors of real-life web graphs.\" This directly addresses the challenge of computational intensity in finding the dominant eigenvector for an ever-growing number of web pages.\n\nOption A is incorrect because the text doesn't suggest complete replacement of classical architectures, only that optical machines may aid in accelerating certain tasks.\n\nOption B is not supported by the text. The article doesn't claim improved accuracy, but rather focuses on computational efficiency and power consumption.\n\nOption D is incorrect because the text still refers to the use of the Google matrix to represent the web structure. The optical machines are proposed as a means to process this matrix more efficiently, not to eliminate it."}, "16": {"documentation": {"title": "A first look at browser-based Cryptojacking", "source": "Shayan Eskandari, Andreas Leoutsarakos, Troy Mursch, Jeremy Clark", "docs_id": "1803.02887", "section": ["cs.CR", "cs.CY", "cs.HC", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A first look at browser-based Cryptojacking. In this paper, we examine the recent trend towards in-browser mining of cryptocurrencies; in particular, the mining of Monero through Coinhive and similar code- bases. In this model, a user visiting a website will download a JavaScript code that executes client-side in her browser, mines a cryptocurrency, typically without her consent or knowledge, and pays out the seigniorage to the website. Websites may consciously employ this as an alternative or to supplement advertisement revenue, may offer premium content in exchange for mining, or may be unwittingly serving the code as a result of a breach (in which case the seigniorage is collected by the attacker). The cryptocurrency Monero is preferred seemingly for its unfriendliness to large-scale ASIC mining that would drive browser-based efforts out of the market, as well as for its purported privacy features. In this paper, we survey this landscape, conduct some measurements to establish its prevalence and profitability, outline an ethical framework for considering whether it should be classified as an attack or business opportunity, and make suggestions for the detection, mitigation and/or prevention of browser-based mining for non- consenting users."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary motivation for websites to use browser-based cryptojacking, as discussed in the paper?\n\nA) To enhance user privacy and security while browsing\nB) To replace or supplement traditional advertisement revenue\nC) To improve the overall performance of web browsers\nD) To promote the widespread adoption of cryptocurrency mining\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper explicitly states that websites may \"consciously employ this as an alternative or to supplement advertisement revenue.\" This indicates that a primary motivation for using browser-based cryptojacking is to generate revenue, either as a replacement for or in addition to traditional advertising methods.\n\nOption A is incorrect because the paper does not suggest that enhancing user privacy or security is a motivation for websites to use cryptojacking. In fact, it often occurs without the user's knowledge or consent.\n\nOption C is incorrect as the paper does not mention improving browser performance as a goal of cryptojacking. On the contrary, cryptojacking typically consumes additional resources and may negatively impact browser performance.\n\nOption D is incorrect because while cryptojacking does involve cryptocurrency mining, the paper does not indicate that promoting widespread adoption of mining is a primary motivation for websites. The focus is on revenue generation for the websites themselves."}, "17": {"documentation": {"title": "Identification of Peer Effects with Miss-specified Peer Groups: Missing\n  Data and Group Uncertainty", "source": "Christiern Rose", "docs_id": "2104.10365", "section": ["econ.EM", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification of Peer Effects with Miss-specified Peer Groups: Missing\n  Data and Group Uncertainty. We consider identification of peer effects under peer group miss-specification. Our model of group miss-specification allows for missing data and peer group uncertainty. Missing data can take the form of some individuals being entirely absent from the data, and the researcher need not have any information on these individuals and may not even know that they are missing. We show that peer effects are nevertheless identifiable under mild restrictions on the probabilities of observing individuals, and propose a GMM estimator to estimate the peer effects. In practice this means that the researcher need only have access to an individual/household level sample with group identifiers. The researcher may also be uncertain as to what is the relevant peer group for the outcome under study. We show that peer effects are nevertheless identifiable provided that the candidate peer groups are nested within one another (e.g. classroom, grade, school) and propose a non-linear least squares estimator. We conduct a Monte-Carlo experiment to demonstrate our identification results and the performance of the proposed estimators in a setting tailored to real data (the Dartmouth room-mate data)."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of identifying peer effects with miss-specified peer groups, which of the following statements is NOT correct according to the research described?\n\nA) The model allows for some individuals to be entirely absent from the dataset without the researcher's knowledge.\n\nB) Peer effects are identifiable under mild restrictions on the probabilities of observing individuals.\n\nC) The researcher must have access to a complete dataset with no missing individuals to identify peer effects.\n\nD) Peer effects are identifiable when candidate peer groups are nested within one another, even with group uncertainty.\n\nCorrect Answer: C\n\nExplanation: \nOption C is incorrect and thus the correct answer to this question which asks for the statement that is NOT correct. The documentation explicitly states that peer effects are identifiable even with missing data, and the researcher doesn't need to know about or have information on missing individuals. This contradicts the statement in option C.\n\nOption A is correct according to the text, which mentions that missing data can include some individuals being entirely absent from the data, without the researcher's knowledge.\n\nOption B is also correct, as the document states that peer effects are identifiable under mild restrictions on the probabilities of observing individuals.\n\nOption D is correct as well. The text indicates that peer effects are identifiable when candidate peer groups are nested (e.g., classroom, grade, school), even when there's uncertainty about the relevant peer group."}, "18": {"documentation": {"title": "Measurement of the Decays $\\boldsymbol{B\\to\\eta\\ell\\nu_\\ell}$ and\n  $\\boldsymbol{B\\to\\eta^\\prime\\ell\\nu_\\ell}$ in Fully Reconstructed Events at\n  Belle", "source": "Belle Collaboration: C. Bele\\~no, J. Dingfelder, P. Urquijo, H.\n  Aihara, S. Al Said, D. M. Asner, T. Aushev, R. Ayad, V. Babu, I. Badhrees, A.\n  M. Bakich, V. Bansal, P. Behera, B. Bhuyan, J. Biswal, A. Bobrov, M.\n  Bra\\v{c}ko, T. E. Browder, D. \\v{C}ervenkov, A. Chen, B. G. Cheon, R.\n  Chistov, S.-K. Choi, Y. Choi, D. Cinabro, N. Dash, S. Di Carlo, Z.\n  Dole\\v{z}al, S. Eidelman, H. Farhat, J. E. Fast, T. Ferber, A. Frey, B. G.\n  Fulsom, V. Gaur, N. Gabyshev, A. Garmash, R. Gillard, P. Goldenzweig, T.\n  Hara, H. Hayashii, M. T. Hedges, W.-S. Hou, T. Iijima, K. Inami, G. Inguglia,\n  A. Ishikawa, R. Itoh, Y. Iwasaki, H. B. Jeon, Y. Jin, D. Joffe, K. K. Joo, K.\n  H. Kang, G. Karyan, D. Y. Kim, J. B. Kim, K. T. Kim, M. J. Kim, Y. J. Kim, K.\n  Kinoshita, P. Kody\\v{s}, S. Korpar, D. Kotchetkov, P. Kri\\v{z}an, R.\n  Kulasiri, I. S. Lee, Y. Li, L. Li Gioi, J. Libby, D. Liventsev, M. Lubej, T.\n  Luo, M. Masuda, T. Matsuda, D. Matvienko, K. Miyabayashi, H. Miyata, H. K.\n  Moon, T. Mori, E. Nakano, M. Nakao, T. Nanut, K. J. Nath, M. Nayak, S.\n  Nishida, S. Ogawa, S. Okuno, H. Ono, B. Pal, C.-S. Park, C. W. Park, H. Park,\n  T. K. Pedlar, R. Pestotnik, L. E. Piilonen, M. Ritter, Y. Sakai, M. Salehi,\n  S. Sandilya, T. Sanuki, O. Schneider, G. Schnell, C. Schwanda, Y. Seino, K.\n  Senyo, O. Seon, M. E. Sevior, V. Shebalin, T.-A. Shibata, J.-G. Shiu, F.\n  Simon, E. Solovieva, M. Stari\\v{c}, T. Sumiyoshi, M. Takizawa, U. Tamponi, K.\n  Tanida, F. Tenchini, M. Uchida, T. Uglov, Y. Unno, S. Uno, Y. Usov, C. Van\n  Hulse, G. Varner, K. E. Varvell, A. Vinokurova, V. Vorobyev, C. H. Wang,\n  M.-Z. Wang, P. Wang, Y. Watanabe, E. Widmann, E. Won, Y. Yamashita, H. Ye, J.\n  Yelton, Y. Yook, Z. P. Zhang, V. Zhilich, V. Zhukova, V. Zhulanov, A. Zupanc", "docs_id": "1703.10216", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of the Decays $\\boldsymbol{B\\to\\eta\\ell\\nu_\\ell}$ and\n  $\\boldsymbol{B\\to\\eta^\\prime\\ell\\nu_\\ell}$ in Fully Reconstructed Events at\n  Belle. We report branching fraction measurements of the decays $B^+\\to\\eta\\ell^+\\nu_\\ell$ and $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ based on 711~fb$^{-1}$ of data collected near the $\\Upsilon(4S)$ resonance with the Belle experiment at the KEKB asymmetric-energy $e^+e^-$ collider. This data sample contains 772 million $B\\bar B$~events. One of the two $B$~mesons is fully reconstructed in a hadronic decay mode. Among the remaining (\"signal-$B$\") daughters, we search for the $\\eta$~meson in two decay channels, $\\eta\\to\\gamma\\gamma$ and $\\eta\\to\\pi^+\\pi^-\\pi^0$, and reconstruct the $\\eta^{\\prime}$~meson in $\\eta^\\prime\\to\\eta\\pi^+\\pi^-$ with subsequent decay of the $\\eta$ into $\\gamma\\gamma$. Combining the two $\\eta$ modes and using an extended maximum likelihood, the $B^+\\to\\eta\\ell^+\\nu_\\ell$ branching fraction is measured to be $(4.2\\pm 1.1 (\\rm stat.)\\pm 0.3 (\\rm syst.))\\times 10^{-5}$. For $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$, we observe no significant signal and set an upper limit of $0.72\\times 10^{-4}$ at 90\\% confidence level."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Based on the Belle experiment measurements, which of the following statements is correct regarding the branching fractions of B+ meson decays?\n\nA) The branching fraction of B+ \u2192 \u03b7\u2113+\u03bd\u2113 is (4.2 \u00b1 1.1 \u00b1 0.3) \u00d7 10^-4, while B+ \u2192 \u03b7'\u2113+\u03bd\u2113 has an upper limit of 0.72 \u00d7 10^-4 at 90% confidence level.\n\nB) The branching fraction of B+ \u2192 \u03b7\u2113+\u03bd\u2113 is (4.2 \u00b1 1.1 \u00b1 0.3) \u00d7 10^-5, and B+ \u2192 \u03b7'\u2113+\u03bd\u2113 has a measured value of 0.72 \u00d7 10^-4.\n\nC) The branching fraction of B+ \u2192 \u03b7\u2113+\u03bd\u2113 is (4.2 \u00b1 1.1 \u00b1 0.3) \u00d7 10^-5, while B+ \u2192 \u03b7'\u2113+\u03bd\u2113 has an upper limit of 0.72 \u00d7 10^-4 at 90% confidence level.\n\nD) Both B+ \u2192 \u03b7\u2113+\u03bd\u2113 and B+ \u2192 \u03b7'\u2113+\u03bd\u2113 have measured branching fractions on the order of 10^-5, with B+ \u2192 \u03b7'\u2113+\u03bd\u2113 having a slightly larger value.\n\nCorrect Answer: C\n\nExplanation: The question tests the understanding of the reported results from the Belle experiment. The correct answer is C because:\n\n1) The branching fraction for B+ \u2192 \u03b7\u2113+\u03bd\u2113 is indeed measured to be (4.2 \u00b1 1.1 (stat.) \u00b1 0.3 (syst.)) \u00d7 10^-5.\n2) For B+ \u2192 \u03b7'\u2113+\u03bd\u2113, no significant signal was observed, and an upper limit of 0.72 \u00d7 10^-4 at 90% confidence level was set.\n\nOption A is incorrect because it states the wrong order of magnitude for the B+ \u2192 \u03b7\u2113+\u03bd\u2113 branching fraction (10^-4 instead of 10^-5).\n\nOption B is incorrect because it suggests a measured value for B+ \u2192 \u03b7'\u2113+\u03bd\u2113, when in fact only an upper limit was established.\n\nOption D is incorrect because it implies that both decays have measured branching fractions, which is not the case for B+ \u2192 \u03b7'\u2113+\u03bd\u2113.\n\nThis question requires careful attention to the numerical values and the distinction between measured values and upper limits, making it challenging for students."}, "19": {"documentation": {"title": "A Comparison of Hybrid and End-to-End Models for Syllable Recognition", "source": "Sebastian P. Bayerl and Korbinian Riedhammer", "docs_id": "1909.12232", "section": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Comparison of Hybrid and End-to-End Models for Syllable Recognition. This paper presents a comparison of a traditional hybrid speech recognition system (kaldi using WFST and TDNN with lattice-free MMI) and a lexicon-free end-to-end (TensorFlow implementation of multi-layer LSTM with CTC training) models for German syllable recognition on the Verbmobil corpus. The results show that explicitly modeling prior knowledge is still valuable in building recognition systems. With a strong language model (LM) based on syllables, the structured approach significantly outperforms the end-to-end model. The best word error rate (WER) regarding syllables was achieved using kaldi with a 4-gram LM, modeling all syllables observed in the training set. It achieved 10.0% WER w.r.t. the syllables, compared to the end-to-end approach where the best WER was 27.53%. The work presented here has implications for building future recognition systems that operate independent of a large vocabulary, as typically used in a tasks such as recognition of syllabic or agglutinative languages, out-of-vocabulary techniques, keyword search indexing and medical speech processing."}, "answer": "Question: Which of the following statements best summarizes the key findings of the study comparing hybrid and end-to-end models for German syllable recognition?\n\nA) The end-to-end model significantly outperformed the hybrid model, achieving a 10.0% WER for syllable recognition.\n\nB) Both the hybrid and end-to-end models performed equally well, with no significant difference in their WER for syllable recognition.\n\nC) The hybrid model with a strong language model outperformed the end-to-end model, achieving a 10.0% WER compared to 27.53% for syllable recognition.\n\nD) The end-to-end model showed better performance in recognizing out-of-vocabulary words, making it more suitable for agglutinative languages.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study found that the hybrid model (kaldi using WFST and TDNN with lattice-free MMI) significantly outperformed the end-to-end model (TensorFlow implementation of multi-layer LSTM with CTC training) when using a strong language model based on syllables. The best word error rate (WER) for syllables was achieved using the hybrid approach with a 4-gram LM, reaching 10.0% WER, while the best performance of the end-to-end approach was 27.53% WER. This result emphasizes the value of explicitly modeling prior knowledge in building recognition systems.\n\nOption A is incorrect because it reverses the performance of the two models. Option B is incorrect as there was a significant difference in performance between the two approaches. Option D is not supported by the information provided in the text and misrepresents the strengths of the end-to-end model in this context."}, "20": {"documentation": {"title": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment", "source": "Abigail Copiaco, Christian Ritz, Stefano Fasciani, Nidhal Abdulaziz", "docs_id": "2104.13423", "section": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment. Access to informative databases is a crucial part of notable research developments. In the field of domestic audio classification, there have been significant advances in recent years. Although several audio databases exist, these can be limited in terms of the amount of information they provide, such as the exact location of the sound sources, and the associated noise levels. In this work, we detail our approach on generating an unbiased synthetic domestic audio database, consisting of sound scenes and events, emulated in both quiet and noisy environments. Data is carefully curated such that it reflects issues commonly faced in a dementia patients environment, and recreate scenarios that could occur in real-world settings. Similarly, the room impulse response generated is based on a typical one-bedroom apartment at Hebrew SeniorLife Facility. As a result, we present an 11-class database containing excerpts of clean and noisy signals at 5-seconds duration each, uniformly sampled at 16 kHz. Using our baseline model using Continues Wavelet Transform Scalograms and AlexNet, this yielded a weighted F1-score of 86.24 percent."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A research team is developing an audio classification system for monitoring dementia patients in their homes. They want to use the DASEE database for training and testing. Which of the following combinations of features would be MOST relevant and beneficial for this specific application?\n\nA) 11 audio classes, 5-second duration samples, 16 kHz sampling rate, clean and noisy signals\nB) Exact sound source locations, associated noise levels, room impulse response based on a typical apartment\nC) Continuous Wavelet Transform Scalograms, AlexNet architecture, 86.24% weighted F1-score\nD) Synthetic data generation, unbiased curation, reflection of dementia patient environment issues\n\nCorrect Answer: A\n\nExplanation: Option A is the most relevant and beneficial combination for this specific application because:\n\n1. The 11 audio classes directly relate to domestic scenes and events in a dementia patient's environment.\n2. The 5-second duration samples are appropriate for capturing short acoustic events.\n3. The 16 kHz sampling rate is sufficient for capturing most relevant acoustic information in a domestic setting.\n4. The inclusion of both clean and noisy signals helps in training a robust system that can work in real-world conditions.\n\nWhile the other options contain valuable information:\n- Option B focuses more on the technical aspects of sound recording and room acoustics.\n- Option C is about the specific machine learning approach used, which may not be the only or best method for all applications.\n- Option D describes the overall approach to database creation, but doesn't provide the specific features needed for immediate application in an audio classification system.\n\nOption A provides the most directly applicable features for developing an audio classification system for monitoring dementia patients in their homes."}, "21": {"documentation": {"title": "Elastic turbulence in curvilinear flows of polymer solutions", "source": "Alexander Groisman and Victor Steinberg", "docs_id": "nlin/0401006", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Elastic turbulence in curvilinear flows of polymer solutions. Following our first report (A. Groisman and V. Steinberg, $\\sl Nature$ $\\bf 405$, 53 (2000)) we present an extended account of experimental observations of elasticity induced turbulence in three different systems: a swirling flow between two plates, a Couette-Taylor (CT) flow between two cylinders, and a flow in a curvilinear channel (Dean flow). All three set-ups had high ratio of width of the region available for flow to radius of curvature of the streamlines. The experiments were carried out with dilute solutions of high molecular weight polyacrylamide in concentrated sugar syrups. High polymer relaxation time and solution viscosity ensured prevalence of non-linear elastic effects over inertial non-linearity, and development of purely elastic instabilities at low Reynolds number (Re) in all three flows. Above the elastic instability threshold, flows in all three systems exhibit features of developed turbulence. Those include: (i)randomly fluctuating fluid motion excited in a broad range of spatial and temporal scales; (ii) significant increase in the rates of momentum and mass transfer (compared to those expected for a steady flow with a smooth velocity profile). Phenomenology, driving mechanisms, and parameter dependence of the elastic turbulence are compared with those of the conventional high Re hydrodynamic turbulence in Newtonian fluids."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the study of elastic turbulence in curvilinear flows of polymer solutions, which of the following statements is NOT a characteristic feature of the observed turbulence above the elastic instability threshold?\n\nA) Randomly fluctuating fluid motion excited in a broad range of spatial and temporal scales\nB) Significant increase in the rates of momentum and mass transfer compared to steady flow with a smooth velocity profile\nC) Development of purely elastic instabilities at high Reynolds numbers\nD) Observation in multiple flow systems including swirling flow between two plates, Couette-Taylor flow, and Dean flow\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation specifically states that the experiments were conducted at low Reynolds numbers (Re), not high Reynolds numbers. The text mentions \"development of purely elastic instabilities at low Reynolds number (Re) in all three flows.\" \n\nOptions A and B are directly stated in the document as features of the observed elastic turbulence. Option D is also correct, as the study mentions these three specific flow systems. \n\nThis question tests the reader's ability to carefully distinguish between the characteristics of elastic turbulence as described in the document and a statement that contradicts the information provided."}, "22": {"documentation": {"title": "Fermionic phases and their transitions induced by competing finite-range\n  interactions", "source": "Marcin Szyniszewski, Henning Schomerus", "docs_id": "1808.02715", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fermionic phases and their transitions induced by competing finite-range\n  interactions. We identify ground states of one-dimensional fermionic systems subject to competing repulsive interactions of finite range, and provide phenomenological and fundamental signatures of these phases and their transitions. Commensurable particle densities admit multiple competing charge-ordered insulating states with various periodicities and internal structure. Our reference point are systems with interaction range $p=2$, where phase transitions between these charge-ordered configurations are known to be mediated by liquid and bond-ordered phases. For increased interaction range $p=4$, we find that the phase transitions can also appear to be abrupt, as well as being mediated by re-emergent ordered phases that cross over into liquid behavior. These considerations are underpinned by a classification of the competing charge-ordered states in the atomic limit for varying interaction range at the principal commensurable particle densities. We also consider the effects of disorder, leading to fragmentization of the ordered phases and localization of the liquid phases."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In a one-dimensional fermionic system with competing repulsive interactions of finite range p=4, what phenomenon is observed during phase transitions between charge-ordered configurations, according to the study?\n\nA) Phase transitions are always mediated by liquid phases\nB) Transitions are exclusively abrupt with no intermediate phases\nC) Transitions can appear abrupt or be mediated by re-emergent ordered phases that cross over into liquid behavior\nD) Only bond-ordered phases mediate the transitions between charge-ordered states\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the complex phase transition behavior in fermionic systems with extended interaction range. The correct answer is C because the documentation states: \"For increased interaction range p=4, we find that the phase transitions can also appear to be abrupt, as well as being mediated by re-emergent ordered phases that cross over into liquid behavior.\" This indicates that both abrupt transitions and transitions mediated by re-emergent ordered phases (which can become liquid-like) are possible.\n\nOption A is incorrect because it doesn't account for the possibility of abrupt transitions. Option B is wrong as it ignores the mediated transitions. Option D is incorrect because it mentions only bond-ordered phases, which were associated with p=2 systems in the text, not p=4 systems.\n\nThis question challenges students to carefully interpret the nuanced findings about phase transition behavior in these complex quantum systems."}, "23": {"documentation": {"title": "Interaction energy and itinerant ferromagnetism in a strongly\n  interacting Fermi gas in the absence of molecule formation", "source": "Lianyi He", "docs_id": "1405.5242", "section": ["cond-mat.quant-gas", "cond-mat.str-el", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interaction energy and itinerant ferromagnetism in a strongly\n  interacting Fermi gas in the absence of molecule formation. We investigate the interaction energy and the possibility of itinerant ferromagnetism in a strongly interacting Fermi gas at zero temperature in the absence of molecule formation. The interaction energy is obtained by summing the perturbative contributions of Galitskii-Feynman type to all orders in the gas parameter. It can be expressed by a simple phase space integral of an in-medium scattering phase shift. In both three and two dimensions (3D and 2D), the interaction energy shows a maximum before reaching the resonance from the Bose-Einstein condensate side, which provides a possible explanation of the experimental measurements of the interaction energy. This phenomenon can be theoretically explained by the qualitative change of the nature of the binary interaction in the medium. The appearance of an energy maximum has significant effects on the itinerant ferromagnetism. In 3D, the ferromagnetic transition is reentrant and itinerant ferromagnetism exists in a narrow window around the energy maximum. In 2D, the present theoretical approach suggests that itinerant ferromagnetism does not exist, which reflects the fact that the energy maximum becomes much lower than the energy of the fully polarized state."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In a strongly interacting Fermi gas at zero temperature without molecule formation, what phenomenon is observed in the interaction energy as the system approaches resonance from the Bose-Einstein condensate side, and how does this affect itinerant ferromagnetism in 3D and 2D?\n\nA) The interaction energy monotonically increases, leading to consistent ferromagnetism in both 3D and 2D.\n\nB) The interaction energy shows a maximum, causing reentrant ferromagnetism in 3D but no ferromagnetism in 2D.\n\nC) The interaction energy decreases continuously, preventing ferromagnetism in both 3D and 2D.\n\nD) The interaction energy oscillates, resulting in alternating ferromagnetic and paramagnetic phases in both 3D and 2D.\n\nCorrect Answer: B\n\nExplanation: The documentation states that the interaction energy shows a maximum before reaching the resonance from the Bose-Einstein condensate side in both 3D and 2D. This maximum has significant effects on itinerant ferromagnetism. In 3D, it causes a reentrant ferromagnetic transition, with itinerant ferromagnetism existing in a narrow window around the energy maximum. In 2D, the energy maximum becomes much lower than the energy of the fully polarized state, suggesting that itinerant ferromagnetism does not exist. This matches the description in option B, making it the correct answer."}, "24": {"documentation": {"title": "Longitudinal distribution of initial energy density and directed flow of\n  charged particles in relativistic heavy-ion collisions", "source": "Ze-Fang Jiang, Shanshan Cao, Xiang-Yu Wu, C. B. Yang and Ben-Wei Zhang", "docs_id": "2112.01916", "section": ["hep-ph", "hep-th", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal distribution of initial energy density and directed flow of\n  charged particles in relativistic heavy-ion collisions. We study the origin of the directed flow of charged particles produced in relativistic heavy-ion collisions. Three different initial conditions, Boz$\\dot{\\textrm{e}}$k-Wyskiel, CCNU and Shen-Alzhrani, of energy density distributions are coupled to the (3+1)-dimensional viscous hydrodynamic model CLVisc, and their effects on the development of the anisotropic medium geometry, pressure gradient and radial flow are systematically compared. By comparing to experimental data at both RHIC and LHC, we find that the directed flow provides a unique constraint on the tilt of the initial medium profile in the plane spanned by the impact parameter and space-time rapidity. Within mid-rapidity, the counter-clockwise tilt is shown to be a crucial source of the positive/negative force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, which drives a negative slope of the $x$ component of the medium flow velocity with respect to rapidity, and in the end the same feature of the charged particle directed flow."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of directed flow of charged particles in relativistic heavy-ion collisions, what is the primary factor that constrains the tilt of the initial medium profile in the plane spanned by the impact parameter and space-time rapidity?\n\nA) The magnitude of the radial flow\nB) The anisotropic medium geometry\nC) The viscosity of the hydrodynamic model\nD) The experimental data on directed flow at RHIC and LHC\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of a key finding from the study. The correct answer is D because the documentation explicitly states: \"By comparing to experimental data at both RHIC and LHC, we find that the directed flow provides a unique constraint on the tilt of the initial medium profile in the plane spanned by the impact parameter and space-time rapidity.\"\n\nWhile options A, B, and C are all factors mentioned in the study that affect the development of directed flow, they are not specifically identified as constraining the tilt of the initial medium profile. The experimental data on directed flow is singled out as providing this unique constraint.\n\nThis question is challenging because it requires careful reading and interpretation of the text, distinguishing between factors that influence the flow generally and the specific constraint on the initial medium profile tilt."}, "25": {"documentation": {"title": "Distributed Value of Information in Feedback Control over Multi-hop\n  Networks", "source": "Precious Ugo Abara, Sandra Hirche", "docs_id": "2107.07822", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributed Value of Information in Feedback Control over Multi-hop\n  Networks. Recent works in the domain of networked control systems have demonstrated that the joint design of medium access control strategies and control strategies for the closed-loop system is beneficial. However, several metrics introduced so far fail in either appropriately representing the network requirements or in capturing how valuable the data is. In this paper we propose a distributed value of information (dVoI) metric for the joint design of control and schedulers for medium access in a multi-loop system and multi-hop network. We start by providing conditions under certainty equivalent controller is optimal. Then we reformulate the joint control and communication problem as a Bellman-like equation. The corresponding dynamic programming problem is solved in a distributed fashion by the proposed VoI-based scheduling policies for the multi-loop multi-hop networked control system, which outperforms the well-known time-triggered periodic sampling policies. Additionally we show that the dVoI-based scheduling policies are independent of each other, both loop-wise and hop-wise. At last, we illustrate the results with a numerical example."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the distributed Value of Information (dVoI) metric for networked control systems, which of the following statements is NOT true?\n\nA) The dVoI metric is used for the joint design of control and schedulers for medium access in multi-loop systems and multi-hop networks.\n\nB) The joint control and communication problem is reformulated as a Bellman-like equation and solved using dynamic programming.\n\nC) dVoI-based scheduling policies are dependent on each other, both loop-wise and hop-wise.\n\nD) The proposed dVoI-based approach outperforms time-triggered periodic sampling policies.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C because it contradicts the information given in the document. The document states that \"the dVoI-based scheduling policies are independent of each other, both loop-wise and hop-wise.\" Options A, B, and D are all true according to the given information. \n\nA is correct as it describes the purpose of the dVoI metric. \nB is accurate as it mentions the reformulation of the problem and the use of dynamic programming. \nD is true as the document states that the proposed approach outperforms time-triggered periodic sampling policies.\n\nThis question tests the reader's understanding of the key aspects of the dVoI metric and its application in networked control systems, particularly focusing on the independence of the scheduling policies, which is a crucial characteristic of the proposed approach."}, "26": {"documentation": {"title": "A New Deep Learning Method for Image Deblurring in Optical Microscopic\n  Systems", "source": "Huangxuan Zhao, Ziwen Ke, Ningbo Chen, Ke Li, Lidai Wang, Xiaojing\n  Gong, Wei Zheng, Liang Song, Zhicheng Liu, Dong Liang, and Chengbo Liu", "docs_id": "1910.03928", "section": ["eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Deep Learning Method for Image Deblurring in Optical Microscopic\n  Systems. Deconvolution is the most commonly used image processing method to remove the blur caused by the point-spread-function (PSF) in optical imaging systems. While this method has been successful in deblurring, it suffers from several disadvantages including being slow, since it takes many iterations, suboptimal, in cases where experimental operator chosen to represent PSF is not optimal. In this paper, we are proposing a deep-learning-based deblurring method applicable to optical microscopic imaging systems. We tested the proposed method in database data, simulated data, and experimental data (include 2D optical microscopic data and 3D photoacoustic microscopic data), all of which showed much improved deblurred results compared to deconvolution. To quantify the improved performance, we compared our results against several deconvolution methods. Our results are better than conventional techniques and do not require multiple iterations or pre-determined experimental operator. Our method has the advantages of simple operation, short time to compute, good deblur results and wide application in all types of optical microscopic imaging systems. The deep learning approach opens up a new path for deblurring and can be applied in various biomedical imaging fields."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the advantages of the proposed deep learning-based deblurring method over traditional deconvolution techniques for optical microscopic imaging systems?\n\nA) It requires fewer iterations and is faster to compute, but still relies on a pre-determined experimental operator.\n\nB) It produces superior deblurring results, but is limited to 2D optical microscopic data only.\n\nC) It offers improved performance across various data types, eliminates the need for multiple iterations and pre-determined experimental operators, and has wide applicability in optical microscopic imaging systems.\n\nD) It is more accurate than deconvolution methods, but is slower to compute and requires extensive parameter tuning.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key advantages of the proposed deep learning-based deblurring method as described in the document. The method shows improved performance on database data, simulated data, and experimental data (including both 2D and 3D microscopic data). It does not require multiple iterations or pre-determined experimental operators, which are limitations of traditional deconvolution methods. The document also states that the method has \"simple operation, short time to compute, good deblur results and wide application in all types of optical microscopic imaging systems.\"\n\nOption A is incorrect because while it correctly states that the method is faster and requires fewer iterations, it incorrectly suggests that it still relies on a pre-determined experimental operator.\n\nOption B is incorrect because it limits the application to 2D data only, whereas the document mentions successful application to both 2D and 3D data.\n\nOption D is incorrect because it contradicts the document's statement about the method having a \"short time to compute\" and does not mention the elimination of the need for multiple iterations or pre-determined operators."}, "27": {"documentation": {"title": "Spectral Curves and Whitham Equations in Isomonodromic Problems of\n  Schlesinger Type", "source": "Kanehisa Takasaki (Kyoto University)", "docs_id": "solv-int/9704004", "section": ["nlin.SI", "hep-th", "math.QA", "nlin.SI", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral Curves and Whitham Equations in Isomonodromic Problems of\n  Schlesinger Type. It has been known since the beginning of this century that isomonodromic problems --- typically the Painlev\\'e transcendents --- in a suitable asymptotic region look like a kind of ``modulation'' of isospectral problem. This connection between isomonodromic and isospectral problems is reconsidered here in the light of recent studies related to the Seiberg-Witten solutions of $N = 2$ supersymmetric gauge theories. A general machinary is illustrated in a typical isomonodromic problem, namely the Schlesinger equation, which is reformulated to include a small parameter $\\epsilon$. In the small-$\\epsilon$ limit, solutions of this isomonodromic problem are expected to behave as a slowly modulated finite-gap solution of an isospectral problem. The modulation is caused by slow deformations of the spectral curve of the finite-gap solution. A modulation equation of this slow dynamics is derived by a heuristic method. An inverse period map of Seiberg-Witten type turns out to give general solutions of this modulation equation. This construction of general solution also reveals the existence of deformations of Seiberg-Witten type on the same moduli space of spectral curves. A prepotential is also constructed in the same way as the prepotential of the Seiberg-Witten theory."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the context of isomonodromic problems of Schlesinger type, which of the following statements is most accurate regarding the relationship between isomonodromic and isospectral problems in the small-\u03b5 limit?\n\nA) Isomonodromic problems behave exactly like isospectral problems without any modulation.\n\nB) Solutions of isomonodromic problems behave as rapidly modulated finite-gap solutions of an isospectral problem.\n\nC) Solutions of isomonodromic problems behave as slowly modulated finite-gap solutions of an isospectral problem, with modulation caused by rapid deformations of the spectral curve.\n\nD) Solutions of isomonodromic problems behave as slowly modulated finite-gap solutions of an isospectral problem, with modulation caused by slow deformations of the spectral curve.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The passage states that \"In the small-\u03b5 limit, solutions of this isomonodromic problem are expected to behave as a slowly modulated finite-gap solution of an isospectral problem. The modulation is caused by slow deformations of the spectral curve of the finite-gap solution.\" This directly corresponds to option D.\n\nOption A is incorrect because it ignores the modulation aspect entirely. Option B is wrong because it mentions \"rapidly modulated\" solutions, whereas the text specifies \"slowly modulated\" solutions. Option C is incorrect because it mentions \"rapid deformations\" of the spectral curve, while the text specifies \"slow deformations.\"\n\nThis question tests the student's understanding of the relationship between isomonodromic and isospectral problems in the asymptotic limit, as well as their ability to discern subtle differences in the description of the modulation process."}, "28": {"documentation": {"title": "The Impact of Digital Marketing on Sausage Manufacturing Companies in\n  the Altos of Jalisco", "source": "Guillermo Jose Navarro del Toro", "docs_id": "2101.06603", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Impact of Digital Marketing on Sausage Manufacturing Companies in\n  the Altos of Jalisco. One of the goals of any business, in addition to producing high-quality, community-accepted products, is to significantly increase sales. Unfortunately, there are regions where new marketing technologies that make it possible to reach a larger number of potential consumers, not only at the regional level, but also at the state and national level, are not yet used. This research, which included qualitative and quantitative methods, as well as interviews applied to owners, employees and clients of three sausage companies, seeks to measure the impact of digital marketing in the Altos of Jalisco, Mexico. Thus, in addition to inquiring about the degree of knowledge they have regarding information and communication technologies (ICT) to expand their markets to areas with higher population density, another goal is to know the opinion about their manufactured products, their quality and acceptance. It should not be forgotten that companies are moving to an increasingly connected world, which enables entrepreneurs to get their products to a greater number of consumers through the Internet and smart devices, such as cell phones, tablets and computers; and thus ensure the survival of the company and a longer stay in the market."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the primary focus and findings of the research on digital marketing in the Altos of Jalisco sausage manufacturing companies?\n\nA) The research primarily focused on improving the quality of sausages produced in the region to meet international standards.\n\nB) The study revealed that sausage manufacturers in the Altos of Jalisco are already extensively using digital marketing techniques to reach national markets.\n\nC) The research aimed to measure the impact of digital marketing and assess the knowledge of ICT among sausage manufacturers, while also gathering opinions on product quality and acceptance.\n\nD) The main conclusion of the study was that traditional marketing methods are more effective than digital marketing for sausage manufacturers in rural Mexican regions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the main aspects of the research as described in the documentation. The study aimed to measure the impact of digital marketing in the Altos of Jalisco sausage manufacturing companies, assess their knowledge of information and communication technologies (ICT), and gather opinions about their products' quality and acceptance. This answer captures the multi-faceted nature of the research, including both its digital marketing focus and its interest in product quality.\n\nOption A is incorrect because, while product quality is mentioned, it was not the primary focus of the research. Option B is incorrect because the documentation suggests that new marketing technologies are not yet widely used in the region. Option D is incorrect because the study does not conclude that traditional marketing methods are more effective; instead, it emphasizes the potential of digital marketing to reach larger markets."}, "29": {"documentation": {"title": "Thermodynamics of the disordered Hubbard model studied via numerical\n  linked-cluster expansions", "source": "Jacob Park and Ehsan Khatami", "docs_id": "2101.12721", "section": ["cond-mat.str-el", "cond-mat.dis-nn", "cond-mat.quant-gas"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermodynamics of the disordered Hubbard model studied via numerical\n  linked-cluster expansions. The interplay of disorder and strong correlations in quantum many-body systems remains an open question. That is despite much progress made in recent years with ultracold atoms in optical lattices to better understand phenomena such as many-body localization or the effect of disorder on Mott metal-insulator transitions. Here, we utilize the numerical linked-cluster expansion technique, extended to treat disordered quantum lattice models, and study exact thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries. We consider box distributions for the disorder in the onsite energy, the interaction strength, as well as the hopping amplitude and explore how energy, double occupancy, entropy, heat capacity and magnetic correlations of the system in the thermodynamic limit evolve as the strength of disorder changes. We compare our findings with those obtained from determinant quantum Monte Carlo simulations and discuss the relevance of our results to experiments with cold fermionic atoms in optical lattices."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the disordered Hubbard model studied via numerical linked-cluster expansions, which of the following statements is most accurate regarding the research approach and findings?\n\nA) The study exclusively focused on one-dimensional lattice geometries and found that disorder in the interaction strength had the most significant impact on thermodynamic properties.\n\nB) The research utilized determinant quantum Monte Carlo simulations as the primary method to calculate exact thermodynamic properties in the thermodynamic limit.\n\nC) The study employed numerical linked-cluster expansion techniques to examine exact thermodynamic properties of the disordered Fermi-Hubbard model on square and cubic lattices, considering various types of disorder.\n\nD) The research concluded that disorder has negligible effects on the magnetic correlations and heat capacity of the system across all strengths of disorder examined.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that the study used \"numerical linked-cluster expansion technique, extended to treat disordered quantum lattice models\" to study \"exact thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries.\" It also mentions considering \"box distributions for the disorder in the onsite energy, the interaction strength, as well as the hopping amplitude.\"\n\nOption A is incorrect because the study focused on square and cubic geometries, not one-dimensional lattices, and did not specify that interaction strength disorder had the most significant impact.\n\nOption B is wrong because while determinant quantum Monte Carlo simulations were mentioned for comparison, they were not the primary method used in this study.\n\nOption D is incorrect because the study explored how various properties, including magnetic correlations and heat capacity, evolved as the strength of disorder changed, rather than concluding that disorder had negligible effects."}, "30": {"documentation": {"title": "Multi-step processes in heavy-ion induced single-nucleon transfer\n  reactions", "source": "N. Keeley, K. W. Kemper and K. Rusek", "docs_id": "2007.13370", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-step processes in heavy-ion induced single-nucleon transfer\n  reactions. It was first noted during the 1970s that finite-range distorted wave Born approximation (FR-DWBA) calculations were unable satisfactorily to describe the shape of the angular distributions of many single-proton (and some single-neutron) transfer reactions induced by heavy ions, with calculations shifted to larger angles by up to ~ 4 degrees compared with the data. These reactions exhibited a significant mismatch, either of the reaction Q value or the grazing angular momentum of the entrance and exit channels, and it was speculated that the inclusion of multi-step transfer paths via excited state(s) of the projectile and/or ejectile could compensate for the effect of this mismatch and yield good descriptions of the data by shifting the calculated peaks to smaller angles. However, to date this has not been explicitly demonstrated for many reactions. In this work we show that inclusion of the two-step transfer path via the 4.44-MeV 2+ excited state of the 12C projectile in coupled channel Born approximation calculations enables a good description of the 208Pb(12C,11B)209Bi single-proton stripping data at four incident energies which could not be described by the FR-DWBA. We also show that inclusion of a similar reaction path for the 208Pb(12C,13C)207Pb single-neutron pickup reaction has a relatively minor influence, slightly improving the already good description obtained with the FR-DWBA."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the impact of including multi-step transfer paths in the analysis of heavy-ion induced single-nucleon transfer reactions?\n\nA) Multi-step paths consistently improve descriptions for both single-proton stripping and single-neutron pickup reactions.\n\nB) The inclusion of multi-step paths via excited states significantly improves the description of single-proton stripping reactions, but has minimal impact on single-neutron pickup reactions.\n\nC) Multi-step paths only affect reactions with a Q-value mismatch, not those with grazing angular momentum mismatches.\n\nD) The inclusion of multi-step paths consistently shifts calculated peaks to larger angles, improving agreement with experimental data.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The passage states that including the two-step transfer path via the 4.44-MeV 2+ excited state of the 12C projectile in coupled channel Born approximation calculations enables a good description of the 208Pb(12C,11B)209Bi single-proton stripping data, which could not be described by the FR-DWBA alone. However, for the 208Pb(12C,13C)207Pb single-neutron pickup reaction, the inclusion of a similar reaction path has a relatively minor influence, only slightly improving the already good description obtained with the FR-DWBA.\n\nOption A is incorrect because the impact is not consistent for both types of reactions. Option C is incorrect because the passage mentions both Q-value and grazing angular momentum mismatches without distinguishing their effects. Option D is incorrect because the multi-step paths actually shift calculated peaks to smaller angles, not larger ones, according to the passage."}, "31": {"documentation": {"title": "Probing the semi-macroscopic vacuum by higher-harmonic generation under\n  focused intense laser fields", "source": "Kensuke Homma, Dieter Habs, and Toshiki Tajima", "docs_id": "1103.1748", "section": ["hep-ph", "gr-qc", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Probing the semi-macroscopic vacuum by higher-harmonic generation under\n  focused intense laser fields. The invention of the laser immediately enabled the detection of nonlinear photon-matter interactions, as manifested for example by Franken et al.'s detection of second-harmonic generation. With the recent advancement in high-power, high-energy lasers and the examples of nonlinearity studies of the laser-matter interaction by virtue of properly arranging lasers and detectors, we envision the possibility of probing nonlinearities of the photon interaction in vacuum over substantial space-time scales, compared to the microscopic scale provided by high-energy accelerators. Specifically, we introduce the photon-photon interaction in a quasi-parallel colliding system and the detection of higher harmonics in that system. The method proposed should realize a far greater sensitivity of probing possible low-mass and weakly coupling fields that have been postulated. With the availability of a large number of coherent photons, we suggest a scheme for the detection of higher harmonics via the averaged resonant production and decay of these postulated fields within the uncertainty of the center-of-mass energy between incoming laser photons. The method carves out a substantial swath of new experimental parameter regimes on the coupling of these fields to photons, under appropriate laser technologies, even weaker than that of gravity in the mass range well below 1 eV."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following statements best describes the proposed method for probing nonlinearities of photon interaction in vacuum, as described in the given text?\n\nA) It relies on high-energy accelerators to study photon-photon interactions at microscopic scales.\n\nB) It uses a quasi-parallel colliding system of lasers to detect higher harmonics, potentially revealing low-mass and weakly coupling fields.\n\nC) It focuses on second-harmonic generation in laser-matter interactions, similar to Franken et al.'s experiments.\n\nD) It proposes using gravity as a benchmark for detecting photon-photon interactions in vacuum.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text describes a method that uses a quasi-parallel colliding system of lasers to detect higher harmonics, which could potentially reveal low-mass and weakly coupling fields. This approach is contrasted with high-energy accelerators (ruling out A) and is described as more sensitive than previous methods.\n\nOption A is incorrect because the text specifically mentions moving beyond the microscopic scale provided by high-energy accelerators.\n\nOption C is incorrect because while second-harmonic generation is mentioned as an early example of nonlinear photon-matter interactions, the proposed method focuses on higher harmonics in vacuum, not in matter.\n\nOption D is incorrect because gravity is mentioned only as a comparison point for the coupling strength of the postulated fields. The method aims to detect fields with even weaker couplings than gravity, not use gravity itself for detection."}, "32": {"documentation": {"title": "Procurements with Bidder Asymmetry in Cost and Risk-Aversion", "source": "Gaurab Aryal, Hanna Charankevich, Seungwon Jeong, Dong-Hyuk Kim", "docs_id": "2111.04626", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Procurements with Bidder Asymmetry in Cost and Risk-Aversion. We propose an empirical method to analyze data from first-price procurements where bidders are asymmetric in their risk-aversion (CRRA) coefficients and distributions of private costs. Our Bayesian approach evaluates the likelihood by solving type-symmetric equilibria using the boundary-value method and integrates out unobserved heterogeneity through data augmentation. We study a new dataset from Russian government procurements focusing on the category of printing papers. We find that there is no unobserved heterogeneity (presumably because the job is routine), but bidders are highly asymmetric in their cost and risk-aversion. Our counterfactual study shows that choosing a type-specific cost-minimizing reserve price marginally reduces the procurement cost; however, inviting one more bidder substantially reduces the cost, by at least 5.5%. Furthermore, incorrectly imposing risk-neutrality would severely mislead inference and policy recommendations, but the bias from imposing homogeneity in risk-aversion is small."}, "answer": "Here's a multi-form question based on the given documentation:\n\nQuestion: In the study of Russian government procurements for printing papers, which of the following statements is NOT supported by the findings?\n\nA) Bidders showed significant asymmetry in both cost and risk-aversion.\nB) Implementing a type-specific cost-minimizing reserve price had a substantial impact on reducing procurement costs.\nC) Adding one more bidder to the procurement process resulted in a significant cost reduction.\nD) Assuming risk-neutrality in the analysis would lead to inaccurate conclusions and policy recommendations.\n\nCorrect Answer: B\n\nExplanation:\nA is incorrect because the study explicitly states that \"bidders are highly asymmetric in their cost and risk-aversion.\"\n\nB is the correct answer because the study found that \"choosing a type-specific cost-minimizing reserve price marginally reduces the procurement cost,\" not substantially as stated in this option.\n\nC is incorrect as the study clearly states that \"inviting one more bidder substantially reduces the cost, by at least 5.5%.\"\n\nD is incorrect because the study mentions that \"incorrectly imposing risk-neutrality would severely mislead inference and policy recommendations.\"\n\nThe question tests the student's ability to carefully read and interpret research findings, distinguishing between significant and marginal effects in procurement strategies."}, "33": {"documentation": {"title": "Supersymmetric Regularization, Two-Loop QCD Amplitudes and Coupling\n  Shifts", "source": "Z. Bern, A. De Freitas, L. Dixon and H.L. Wong", "docs_id": "hep-ph/0202271", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supersymmetric Regularization, Two-Loop QCD Amplitudes and Coupling\n  Shifts. We present a definition of the four-dimensional helicity (FDH) regularization scheme valid for two or more loops. This scheme was previously defined and utilized at one loop. It amounts to a variation on the standard 't Hooft-Veltman scheme and is designed to be compatible with the use of helicity states for \"observed\" particles. It is similar to dimensional reduction in that it maintains an equal number of bosonic and fermionic states, as required for preserving supersymmetry. Supersymmetry Ward identities relate different helicity amplitudes in supersymmetric theories. As a check that the FDH scheme preserves supersymmetry, at least through two loops, we explicitly verify a number of these identities for gluon-gluon scattering (gg to gg) in supersymmetric QCD. These results also cross-check recent non-trivial two-loop calculations in ordinary QCD. Finally, we compute the two-loop shift between the FDH coupling and the standard MS-bar coupling, alpha_s. The FDH shift is identical to the one for dimensional reduction. The two-loop coupling shifts are then used to obtain the three-loop QCD beta function in the FDH and dimensional reduction schemes."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following statements about the four-dimensional helicity (FDH) regularization scheme is NOT correct according to the provided information?\n\nA) It is designed to be compatible with helicity states for \"observed\" particles and is a variation on the 't Hooft-Veltman scheme.\n\nB) It maintains an equal number of bosonic and fermionic states, similar to dimensional reduction, which is necessary for preserving supersymmetry.\n\nC) The two-loop shift between the FDH coupling and the standard MS-bar coupling is different from the shift for dimensional reduction.\n\nD) Supersymmetry Ward identities relating different helicity amplitudes in supersymmetric theories were verified for gluon-gluon scattering as a check of the FDH scheme's preservation of supersymmetry through two loops.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the document explicitly states that \"The FDH shift is identical to the one for dimensional reduction.\" This contradicts the statement in option C. All other options are correct according to the provided information:\n\nA is correct as the text mentions that FDH is \"a variation on the standard 't Hooft-Veltman scheme and is designed to be compatible with the use of helicity states for 'observed' particles.\"\n\nB is accurate as the document states that FDH \"maintains an equal number of bosonic and fermionic states, as required for preserving supersymmetry\" and is \"similar to dimensional reduction\" in this aspect.\n\nD is correct because the text mentions that \"As a check that the FDH scheme preserves supersymmetry, at least through two loops, we explicitly verify a number of these identities for gluon-gluon scattering (gg to gg) in supersymmetric QCD.\""}, "34": {"documentation": {"title": "Ptychographic X-ray Speckle Tracking", "source": "Andrew J. Morgan, Harry M. Quiney, Sa\\v{s}a Bajt, Henry N. Chapman", "docs_id": "2003.12686", "section": ["physics.optics", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ptychographic X-ray Speckle Tracking. We present a method for the measurement of the phase gradient of a wavefront by tracking the relative motion of speckles in projection holograms as a sample is scanned across the wavefront. By removing the need to obtain an un-distorted reference image of the sample, this method is suitable for the metrology of highly divergent wavefields. Such wavefields allow for large magnification factors, that, according to current imaging capabilities, will allow for nano-radian angular sensitivity and nano-scale sample projection imaging. Both the reconstruction algorithm and the imaging geometry are nearly identical to that of ptychography, except that the sample is placed downstream of the beam focus and that no coherent propagation is explicitly accounted for. Like other x-ray speckle tracking methods, it is robust to low-coherence x-ray sources making is suitable for lab based x-ray sources. Likewise it is robust to errors in the registered sample positions making it suitable for x-ray free-electron laser facilities, where beam pointing fluctuations can be problematic for wavefront metrology. We also present a modified form of the speckle tracking approximation, based on a second-order local expansion of the Fresnel integral. This result extends the validity of the speckle tracking approximation and may be useful for similar approaches in the field."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements about Ptychographic X-ray Speckle Tracking is NOT correct?\n\nA) It measures the phase gradient of a wavefront by tracking speckle motion in projection holograms.\n\nB) It requires an un-distorted reference image of the sample for accurate metrology.\n\nC) It is suitable for metrology of highly divergent wavefields, allowing for large magnification factors.\n\nD) The method is robust to low-coherence x-ray sources and errors in registered sample positions.\n\nCorrect Answer: B\n\nExplanation:\nA is correct: The method tracks the relative motion of speckles in projection holograms as a sample is scanned across the wavefront to measure the phase gradient.\n\nB is incorrect: The text explicitly states that this method removes \"the need to obtain an un-distorted reference image of the sample,\" making it suitable for highly divergent wavefields.\n\nC is correct: The document mentions that the method is suitable for highly divergent wavefields, which allow for large magnification factors.\n\nD is correct: The text states that the method is \"robust to low-coherence x-ray sources\" and \"robust to errors in the registered sample positions.\"\n\nThis question tests the reader's understanding of the key features and advantages of Ptychographic X-ray Speckle Tracking as described in the documentation."}, "35": {"documentation": {"title": "Random Network Behaviour of Protein Structures", "source": "Brinda K.V., Saraswathi Vishveshwara and Smitha Vishveshwara", "docs_id": "0912.5406", "section": ["physics.bio-ph", "cond-mat.other", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Random Network Behaviour of Protein Structures. Geometric and structural constraints greatly restrict the selection of folds adapted by protein backbones, and yet, folded proteins show an astounding diversity in functionality. For structure to have any bearing on function, it is thus imperative that, apart from the protein backbone, other tunable degrees of freedom be accountable. Here, we focus on side-chain interactions, which non-covalently link amino acids in folded proteins to form a network structure. At a coarse-grained level, we show that the network conforms remarkably well to realizations of random graphs and displays associated percolation behavior. Thus, within the rigid framework of the protein backbone that restricts the structure space, the side-chain interactions exhibit an element of randomness, which account for the functional flexibility and diversity shown by proteins. However, at a finer level, the network exhibits deviations from these random graphs which, as we demonstrate for a few specific examples, reflect the intrinsic uniqueness in the structure and stability, and perhaps specificity in the functioning of biological proteins."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the relationship between protein structure and function, according to the research on random network behavior of protein structures?\n\nA) Protein backbone structure alone determines the diverse functionality of proteins.\n\nB) Side-chain interactions form a completely random network with no relation to protein function.\n\nC) The network of side-chain interactions exhibits both random characteristics and specific deviations, contributing to functional diversity within backbone constraints.\n\nD) Protein function is solely determined by the geometric and structural constraints of the protein backbone.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the text states that while protein backbones are constrained by geometric and structural factors, side-chain interactions form a network that shows both random graph characteristics and specific deviations. This combination allows for functional flexibility and diversity within the constraints of the backbone structure.\n\nAnswer A is incorrect because the text emphasizes that backbone structure alone cannot account for the diversity in protein functionality.\n\nAnswer B is incorrect because while the side-chain interaction network shows random characteristics, it also exhibits deviations from random graphs that are linked to the specific structure, stability, and function of biological proteins.\n\nAnswer D is incorrect because the text explicitly states that other degrees of freedom, particularly side-chain interactions, must be accountable for function, not just the backbone constraints."}, "36": {"documentation": {"title": "Novel dual relation and constant in Hawking-Page phase transitions", "source": "Shao-Wen Wei, Yu-Xiao Liu, Robert B. Mann", "docs_id": "2006.11503", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Novel dual relation and constant in Hawking-Page phase transitions. Universal relations and constants have important applications in understanding a physical theory. In this article, we explore this issue for Hawking-Page phase transitions in Schwarzschild anti-de Sitter black holes. We find a novel exact dual relation between the minimum temperature of the ($d$+1)-dimensional black hole and the Hawking-Page phase transition temperature in $d$ dimensions, reminiscent of the holographic principle. Furthermore, we find that the normalized Ruppeiner scalar curvature is a universal constant at the Hawking-Page transition point. Since the Ruppeiner curvature can be treated as an indicator of the intensity of the interactions amongst black hole microstructures, we conjecture that this universal constant denotes an interaction threshold, beyond which a virtual black hole becomes a real one. This new dual relation and universal constant are fundamental in understanding Hawking-Page phase transitions, and might have new important applications in the black hole physics in the near future."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of Hawking-Page phase transitions for Schwarzschild anti-de Sitter black holes, which of the following statements is correct?\n\nA) The minimum temperature of a (d+1)-dimensional black hole is inversely proportional to the Hawking-Page phase transition temperature in d dimensions.\n\nB) The normalized Ruppeiner scalar curvature varies continuously at the Hawking-Page transition point.\n\nC) The novel dual relation between temperatures in different dimensions is analogous to the anthropic principle.\n\nD) The universal constant associated with the normalized Ruppeiner scalar curvature at the Hawking-Page transition point may represent a threshold for black hole formation.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the key findings presented in the Arxiv documentation. Option A is incorrect because the relation is described as \"dual\" but not specifically stated to be inverse. Option B is wrong because the normalized Ruppeiner scalar curvature is described as a \"universal constant\" at the transition point, not a continuously varying quantity. Option C is incorrect as the dual relation is said to be \"reminiscent of the holographic principle,\" not the anthropic principle. \n\nOption D is correct because the documentation states, \"Since the Ruppeiner curvature can be treated as an indicator of the intensity of the interactions amongst black hole microstructures, we conjecture that this universal constant denotes an interaction threshold, beyond which a virtual black hole becomes a real one.\" This directly supports the idea that the universal constant associated with the normalized Ruppeiner scalar curvature at the Hawking-Page transition point may represent a threshold for black hole formation."}, "37": {"documentation": {"title": "Topology of SO(5)-monopoles and three-dimensional, stable Dirac\n  semimetals", "source": "Alexander C. Tyner, Shouvik Sur, Danilo Puggioni, James M. Rondinelli,\n  and Pallab Goswami", "docs_id": "2012.12906", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.str-el", "hep-lat", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topology of SO(5)-monopoles and three-dimensional, stable Dirac\n  semimetals. The band-touching points of stable, three-dimensional, Kramers-degenerate, Dirac semimetals are singularities of a five-component, unit vector field and non-Abelian, $SO(5)$-Berry's connections, whose topological classification is an important, open problem. We solve this problem by performing second homotopy classification of Berry's connections. Using Abelian projected connections, the generic planes, orthogonal to the direction of nodal separation, and lying between two Dirac points are shown to be higher-order topological insulators, which support quantized, chromo-magnetic flux or relative Chern number, and gapped, edge states. The Dirac points are identified as a pair of unit-strength, $SO(5)$- monopole and anti-monopole, where the relative Chern number jumps by $\\pm 1$. Using these bulk invariants, we determine the topological universality class of different types of Dirac semimetals. We also describe a universal recipe for computing quantized, non-Abelian flux for Dirac materials from the windings of spectra of planar Wilson loops, displaying $SO(5)$-gauge invariance. With non-perturbative, analytical solutions of surface-states, we show the absence of helical Fermi arcs, and predict the fermiology and the spin-orbital textures. We also discuss the similarities and important topological distinction between the surface-states Hamiltonian and the generator of Polyakov loop of Berry's connections."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: In the context of stable, three-dimensional Dirac semimetals, what is the topological classification of the band-touching points and what does this classification imply about the surrounding planes?\n\nA) The band-touching points are classified as U(1) monopoles, and the surrounding planes are ordinary insulators.\n\nB) The band-touching points are classified as SO(5)-monopoles and anti-monopoles, and the generic planes between them are higher-order topological insulators supporting quantized chromo-magnetic flux.\n\nC) The band-touching points are classified as SU(2) instantons, and the surrounding planes exhibit fractional quantum Hall effect.\n\nD) The band-touching points are classified as Z2 vortices, and the surrounding planes are Chern insulators with integer Chern numbers.\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the topological nature of Dirac semimetals as described in the text. The correct answer is B because the text states that \"The Dirac points are identified as a pair of unit-strength, SO(5)-monopole and anti-monopole,\" and that \"the generic planes, orthogonal to the direction of nodal separation, and lying between two Dirac points are shown to be higher-order topological insulators, which support quantized, chromo-magnetic flux or relative Chern number.\" This classification has important implications for the topological properties of the material and its surface states. The other options present incorrect classifications or mismatched properties that do not align with the description provided in the text."}, "38": {"documentation": {"title": "Cointegration in functional autoregressive processes", "source": "Massimo Franchi and Paolo Paruolo", "docs_id": "1712.07522", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cointegration in functional autoregressive processes. This paper defines the class of $\\mathcal{H}$-valued autoregressive (AR) processes with a unit root of finite type, where $\\mathcal{H}$ is an infinite dimensional separable Hilbert space, and derives a generalization of the Granger-Johansen Representation Theorem valid for any integration order $d=1,2,\\dots$. An existence theorem shows that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer $d$ and displays a common trends representation with a finite number of common stochastic trends of the type of (cumulated) bilateral random walks and an infinite dimensional cointegrating space. A characterization theorem clarifies the connections between the structure of the AR operators and $(i)$ the order of integration, $(ii)$ the structure of the attractor space and the cointegrating space, $(iii)$ the expression of the cointegrating relations, and $(iv)$ the Triangular representation of the process. Except for the fact that the number of cointegrating relations that are integrated of order 0 is infinite, the representation of $\\mathcal{H}$-valued ARs with a unit root of finite type coincides with that of usual finite dimensional VARs, which corresponds to the special case $\\mathcal{H}=\\mathbb{R}^p$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Consider an $\\mathcal{H}$-valued autoregressive (AR) process with a unit root of finite type, where $\\mathcal{H}$ is an infinite dimensional separable Hilbert space. Which of the following statements is NOT true according to the Granger-Johansen Representation Theorem generalization for this process?\n\nA) The process is necessarily integrated of some finite integer order d.\n\nB) The process displays a common trends representation with a finite number of common stochastic trends.\n\nC) The cointegrating space is finite dimensional.\n\nD) The number of cointegrating relations that are integrated of order 0 is infinite.\n\nCorrect Answer: C\n\nExplanation: \nOption A is true according to the existence theorem mentioned in the text, which states that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer d.\n\nOption B is correct as the text mentions that the process displays a common trends representation with a finite number of common stochastic trends.\n\nOption C is incorrect and thus the correct answer to this question. The text explicitly states that the process has \"an infinite dimensional cointegrating space.\"\n\nOption D is true as per the last sentence of the given text, which states \"Except for the fact that the number of cointegrating relations that are integrated of order 0 is infinite...\"\n\nThis question tests the understanding of key concepts from the generalized Granger-Johansen Representation Theorem for $\\mathcal{H}$-valued AR processes, particularly focusing on the dimensionality aspects that differ from the finite-dimensional case."}, "39": {"documentation": {"title": "Microscopic derivation of density functional theory for superfluid\n  systems based on effective action formalism", "source": "Takeru Yokota, Haruki Kasuya, Kenichi Yoshida, Teiji Kunihiro", "docs_id": "2008.05919", "section": ["nucl-th", "cond-mat.supr-con", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microscopic derivation of density functional theory for superfluid\n  systems based on effective action formalism. Density-functional theory for superfluid systems is developed in the framework of the functional renormalization group based on the effective action formalism. We introduce the effective action for the particle-number and nonlocal pairing densities and demonstrate that the Hohenberg-Kohn theorem for superfluid systems is established in terms of the effective action. The flow equation for the effective action is then derived, where the flow parameter runs from $0$ to $1$, corresponding to the non-interacting and interacting systems. From the flow equation and the variational equation that the equilibrium density satisfies, we obtain the exact expression for the Kohn-Sham potential generalized to including the pairing potentials. The resultant Kohn-Sham potential has a nice feature that it expresses the microscopic formulae of the external, Hartree, pairing, and exchange-correlation terms, separately. It is shown that our Kohn-Sham potential gives the ground-state energy of the Hartree-Fock-Bogoliubov theory by neglecting the correlations. An advantage of our exact formalism lies in the fact that it provides ways to systematically improve the correlation part."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the microscopic derivation of density functional theory for superfluid systems using the effective action formalism, what is the significance of the flow parameter in the flow equation for the effective action?\n\nA) It represents the strength of the external potential applied to the system\nB) It runs from 0 to 1, corresponding to the transition from non-interacting to fully interacting systems\nC) It describes the temperature dependence of the superfluid density\nD) It quantifies the degree of particle-hole symmetry in the system\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that \"The flow equation for the effective action is then derived, where the flow parameter runs from 0 to 1, corresponding to the non-interacting and interacting systems.\" This flow parameter is a key concept in the functional renormalization group approach, allowing for a smooth interpolation between the non-interacting (0) and fully interacting (1) limits of the system.\n\nOption A is incorrect because the flow parameter is not directly related to the external potential. \nOption C is incorrect as the flow parameter is not described as being related to temperature or superfluid density in this context. \nOption D is incorrect because the flow parameter is not described as quantifying particle-hole symmetry.\n\nThis question tests the student's understanding of the functional renormalization group method and its application to superfluid systems in density functional theory."}, "40": {"documentation": {"title": "Libra: Fair Order-Matching for Electronic Financial Exchanges", "source": "Vasilios Mavroudis, Hayden Melton", "docs_id": "1910.00321", "section": ["cs.CR", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Libra: Fair Order-Matching for Electronic Financial Exchanges. While historically, economists have been primarily occupied with analyzing the behaviour of the markets, electronic trading gave rise to a new class of unprecedented problems associated with market fairness, transparency and manipulation. These problems stem from technical shortcomings that are not accounted for in the simple conceptual models used for theoretical market analysis. They, thus, call for more pragmatic market design methodologies that consider the various infrastructure complexities and their potential impact on the market procedures. First, we formally define temporal fairness and then explain why it is very difficult for order-matching policies to ensure it in continuous markets. Subsequently, we introduce a list of system requirements and evaluate existing \"fair\" market designs in various practical and adversarial scenarios. We conclude that they fail to retain their properties in the presence of infrastructure inefficiencies and sophisticated technical manipulation attacks. Based on these findings, we then introduce Libra, a \"fair\" policy that is resilient to gaming and tolerant of technical complications. Our security analysis shows that it is significantly more robust than existing designs, while Libra's deployment (in a live foreign currency exchange) validated both its considerably low impact on the operation of the market and its ability to reduce speed-based predatory trading."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Which of the following best describes the primary contribution of the Libra order-matching policy for electronic financial exchanges?\n\nA) It provides a theoretical framework for analyzing market behavior in traditional economic models.\n\nB) It introduces a new class of problems associated with market fairness and manipulation in electronic trading.\n\nC) It offers a robust and practical solution to ensure temporal fairness while being resilient to gaming and technical complications.\n\nD) It focuses solely on improving the speed of order execution in continuous markets.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. The Libra order-matching policy is presented as a solution that addresses the challenges of ensuring fairness in electronic financial exchanges while being resistant to manipulation and tolerant of technical issues.\n\nAnswer A is incorrect because the document emphasizes that traditional economic models are insufficient for addressing the new problems introduced by electronic trading.\n\nAnswer B is not the primary contribution of Libra. While the document does discuss these new problems, Libra is presented as a solution to these issues rather than introducing them.\n\nAnswer D is incorrect because Libra's focus is not solely on improving execution speed. Instead, it aims to provide a fair order-matching system that can withstand sophisticated technical manipulation attacks and infrastructure inefficiencies.\n\nThe correct answer, C, accurately captures the essence of Libra as described in the document. It is presented as a \"fair\" policy that is resilient to gaming, tolerant of technical complications, and able to ensure temporal fairness in electronic financial exchanges."}, "41": {"documentation": {"title": "Quasi-simultaneous 43 and 86 GHz SiO Maser Observations and Potential\n  Bias in the BAaDE Survey Are Resolved", "source": "Michael C. Stroh, Ylva M. Pihlstr\\\"om, Lor\\`ant O. Sjouwerman, Mark J.\n  Claussen, Mark R. Morris, and Michael R. Rich", "docs_id": "1808.02899", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quasi-simultaneous 43 and 86 GHz SiO Maser Observations and Potential\n  Bias in the BAaDE Survey Are Resolved. We observed the 43 GHz v=1, 2, and 3 and 86 GHz v=1 SiO maser transitions quasi-simultaneously for a Mira-variable-dominated sample of over 80 sources from the Bulge Asymmetries and Dynamical Evolution (BAaDE) project, using ATCA, and statistically compared the relative line strengths. On average, the 43 GHz v=1 line is brighter than the 86 GHz v=1 line by a factor of 1.36+/-0.15. As a result, an 86 GHz v=1 observed sample can be observed to 85.9% of the distance of a 43 GHz v=1 observed sample using the same sensitivity. We discuss what impact this may have on the BAaDE Galactic plane survey using the VLA and ALMA. Despite fewer v=3 detections, specific trends are discerned or strengthened when the 43 GHz v=3 line is detected. In particular the 43 and 86 GHz v=1 lines are on average equal for sources with no detectable 43 GHz v=3 emission, but the 43 GHz v=1 line strength is on average about twice as bright as the 86 GHz v=1 line for sources with detectable 43 GHz v=3 emission. Some weak correlations are found between line strengths and Midcourse Space Experiment flux densities and colors, which are tightened when considering only sources with detectable 43 GHz v=3 emission. We discuss these trends in the context of a radiative pumping model to highlight how the 43 GHz v=3 line, when coupled with the v=1 and v=2 lines, can further our understanding of variable conditions like density in the circumstellar envelopes."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Based on the study of SiO maser transitions in the BAaDE project, which of the following statements is most accurate regarding the relationship between 43 GHz and 86 GHz v=1 line strengths, and how does this relate to the detection of the 43 GHz v=3 line?\n\nA) The 43 GHz v=1 line is always brighter than the 86 GHz v=1 line, regardless of the presence of 43 GHz v=3 emission.\n\nB) The 43 GHz v=1 and 86 GHz v=1 lines are equal in strength for all sources, but the presence of 43 GHz v=3 emission indicates higher overall line strengths.\n\nC) When 43 GHz v=3 emission is not detected, the 43 GHz v=1 and 86 GHz v=1 lines are approximately equal in strength, but when v=3 is detected, the 43 GHz v=1 line becomes about twice as bright as the 86 GHz v=1 line.\n\nD) The 86 GHz v=1 line is consistently brighter than the 43 GHz v=1 line, and the detection of 43 GHz v=3 emission has no impact on this relationship.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the complex relationships between different SiO maser transitions observed in the study. The correct answer, C, accurately reflects the findings reported in the documentation. Specifically, the study found that for sources with no detectable 43 GHz v=3 emission, the 43 and 86 GHz v=1 lines are on average equal in strength. However, for sources where 43 GHz v=3 emission is detected, the 43 GHz v=1 line strength is on average about twice as bright as the 86 GHz v=1 line. This relationship provides insight into the varying conditions in circumstellar envelopes and demonstrates the importance of the v=3 line in understanding these systems."}, "42": {"documentation": {"title": "Business disruptions from social distancing", "source": "Mikl\\'os Koren, Rita Pet\\H{o}", "docs_id": "2003.13983", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Business disruptions from social distancing. Social distancing interventions can be effective against epidemics but are potentially detrimental for the economy. Businesses that rely heavily on face-to-face communication or close physical proximity when producing a product or providing a service are particularly vulnerable. There is, however, no systematic evidence about the role of human interactions across different lines of business and about which will be the most limited by social distancing. Here we provide theory-based measures of the reliance of U.S. businesses on human interaction, detailed by industry and geographic location. We find that 49 million workers work in occupations that rely heavily on face-to-face communication or require close physical proximity to other workers. Our model suggests that when businesses are forced to reduce worker contacts by half, they need a 12 percent wage subsidy to compensate for the disruption in communication. Retail, hotels and restaurants, arts and entertainment and schools are the most affected sectors. Our results can help target fiscal assistance to businesses that are most disrupted by social distancing."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: According to the Arxiv documentation, if businesses are required to reduce worker contacts by 50% due to social distancing measures, what percentage wage subsidy would be needed to compensate for the disruption in communication?\n\nA) 6%\nB) 12%\nC) 24%\nD) 49%\n\nCorrect Answer: B\n\nExplanation: The documentation states, \"Our model suggests that when businesses are forced to reduce worker contacts by half, they need a 12 percent wage subsidy to compensate for the disruption in communication.\" This directly corresponds to the correct answer, B) 12%.\n\nOption A (6%) is incorrect as it's half the required subsidy mentioned in the text.\nOption C (24%) is incorrect as it's double the required subsidy.\nOption D (49%) is incorrect as it refers to the number of million workers in occupations relying heavily on face-to-face communication or close physical proximity, not the wage subsidy percentage.\n\nThis question tests the student's ability to carefully read and extract specific numerical information from a complex text, distinguishing between different percentages and their contexts within the passage."}, "43": {"documentation": {"title": "Optical Crystals and Light-Bullets in Kerr Resonators", "source": "M. Tlidi, S. S. Gopalakrishnan, M. Taki, and K. Panajotov", "docs_id": "2107.14489", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optical Crystals and Light-Bullets in Kerr Resonators. Stable light bullets and clusters of them are presented in the monostable regime using the mean-field Lugiato-Lefever equation [Gopalakrishnan, Panajotov, Taki, and Tlidi, Phys. Rev. Lett. 126, 153902 (2021)]. It is shown that three-dimensional (3D) dissipative structures occur in a strongly nonlinear regime where modulational instability is subcritical. We provide a detailed analysis on the formation of optical 3D crystals in both the super- and sub-critical modulational instability regimes, and we highlight their link to the formation of light bullets in diffractive and dispersive Kerr resonators. We construct bifurcation diagrams associated with the formation of optical crystals in both monostable and bistable regimes. An analytical study has predicted the predominance of body-centered-cubic (bcc) crystals in the intracavity field over a large variety of other 3D solutions with less symmetry. These results have been obtained using a weakly nonlinear analysis but have never been checked numerically. We show numerically that indeed the most robust structures over other self-organized crystals are the bcc crystals. Finally, we show that light-bullets and clusters of them can occur also in a bistable regime."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements accurately describes the findings regarding three-dimensional (3D) dissipative structures and optical crystals in Kerr resonators, as presented in the research?\n\nA) 3D dissipative structures occur only in weakly nonlinear regimes where modulational instability is supercritical.\n\nB) Analytical studies predict that face-centered cubic (fcc) crystals are the predominant structure in the intracavity field.\n\nC) Light bullets and clusters are observed exclusively in the bistable regime of Kerr resonators.\n\nD) Body-centered-cubic (bcc) crystals are numerically shown to be the most robust structures among self-organized crystals in the intracavity field.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"We show numerically that indeed the most robust structures over other self-organized crystals are the bcc crystals.\" This directly supports the statement in option D.\n\nOption A is incorrect because the text mentions that 3D dissipative structures occur in a strongly nonlinear regime where modulational instability is subcritical, not weakly nonlinear and supercritical.\n\nOption B is incorrect as the analytical study actually predicted the predominance of body-centered-cubic (bcc) crystals, not face-centered cubic (fcc) crystals.\n\nOption C is false because the text explicitly states that \"Stable light bullets and clusters of them are presented in the monostable regime\" and later mentions that they can also occur in a bistable regime, indicating they are not exclusive to the bistable regime.\n\nThis question tests the student's ability to carefully read and interpret complex scientific findings, distinguishing between analytical predictions and numerical confirmations, and understanding the conditions under which certain phenomena occur in Kerr resonators."}, "44": {"documentation": {"title": "Increasing the Raw Key Rate in Energy-Time Entanglement Based Quantum\n  Key Distribution", "source": "Esmaeil Karimi, Emina Soljanin, and Philip Whiting", "docs_id": "2001.09049", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Increasing the Raw Key Rate in Energy-Time Entanglement Based Quantum\n  Key Distribution. A Quantum Key Distribution (QKD) protocol describes how two remote parties can establish a secret key by communicating over a quantum and a public classical channel that both can be accessed by an eavesdropper. QKD protocols using energy-time entangled photon pairs are of growing practical interest because of their potential to provide a higher secure key rate over long distances by carrying multiple bits per entangled photon pair. We consider a system where information can be extracted by measuring random times of a sequence of entangled photon arrivals. Our goal is to maximize the utility of each such pair. We propose a discrete time model for the photon arrival process, and establish a theoretical bound on the number of raw bits that can be generated under this model. We first analyse a well known simple binning encoding scheme, and show that it generates significantly lower information rate than what is theoretically possible. We then propose three adaptive schemes that increase the number of raw bits generated per photon, and compute and compare the information rates they offer. Moreover, the effect of public channel communication on the secret key rates of the proposed schemes is investigated."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of energy-time entanglement-based Quantum Key Distribution (QKD), which of the following statements is most accurate regarding the proposed adaptive schemes compared to the simple binning encoding scheme?\n\nA) The adaptive schemes generate fewer raw bits per photon pair but offer higher security against eavesdropping.\n\nB) The adaptive schemes and simple binning encoding scheme produce equal numbers of raw bits per photon pair.\n\nC) The adaptive schemes generate more raw bits per photon pair, approaching the theoretical bound more closely than the simple binning method.\n\nD) The adaptive schemes are theoretically sound but impractical to implement in real-world QKD systems.\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the key points in the document regarding the efficiency of different encoding schemes in energy-time entanglement-based QKD. The correct answer is C because the document states that the simple binning encoding scheme \"generates significantly lower information rate than what is theoretically possible,\" while the proposed adaptive schemes \"increase the number of raw bits generated per photon.\" The adaptive schemes are designed to maximize utility and approach the theoretical bound more closely, which is the main goal described in the passage. Options A and B are incorrect as they contradict the information provided. Option D is a plausible distractor but is not supported by the given information, which focuses on the theoretical advantages of the adaptive schemes without commenting on their practicality."}, "45": {"documentation": {"title": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies", "source": "Zhengyang Dong", "docs_id": "1805.12111", "section": ["q-fin.ST", "cs.CE", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies. Stock trend prediction is a challenging task due to the market's noise, and machine learning techniques have recently been successful in coping with this challenge. In this research, we create a novel framework for stock prediction, Dynamic Advisor-Based Ensemble (dynABE). dynABE explores domain-specific areas based on the companies of interest, diversifies the feature set by creating different \"advisors\" that each handles a different area, follows an effective model ensemble procedure for each advisor, and combines the advisors together in a second-level ensemble through an online update strategy we developed. dynABE is able to adapt to price pattern changes of the market during the active trading period robustly, without needing to retrain the entire model. We test dynABE on three cobalt-related companies, and it achieves the best-case misclassification error of 31.12% and an annualized absolute return of 359.55% with zero maximum drawdown. dynABE also consistently outperforms the baseline models of support vector machine, neural network, and random forest in all case studies."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation of the Dynamic Advisor-Based Ensemble (dynABE) framework for stock trend prediction?\n\nA) It uses a single, complex machine learning model to predict stock trends across all market sectors.\n\nB) It relies solely on historical price data to make predictions without considering domain-specific information.\n\nC) It creates separate \"advisors\" for different domain-specific areas and combines them using a two-level ensemble approach with an online update strategy.\n\nD) It requires complete retraining of the entire model whenever market conditions change.\n\nCorrect Answer: C\n\nExplanation: The key innovation of dynABE is its use of multiple \"advisors\" that each handle different domain-specific areas, combined with a two-level ensemble approach. The first level involves an ensemble procedure for each advisor, and the second level combines these advisors using an online update strategy. This allows dynABE to adapt to changing market conditions without needing to retrain the entire model, making option C the correct answer.\n\nOption A is incorrect because dynABE uses multiple advisors rather than a single complex model. Option B is wrong because dynABE explicitly incorporates domain-specific information, not just historical price data. Option D is incorrect because one of dynABE's key features is its ability to adapt without full retraining."}, "46": {"documentation": {"title": "Metasurface Freeform Nanophotonics", "source": "Alan Zhan, Shane Colburn, Christopher M. Dodson, Arka Majumdar", "docs_id": "1610.00019", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Metasurface Freeform Nanophotonics. Freeform optics aims to expand the toolkit of optical elements by allowing for more complex phase geometries beyond rotational symmetry. Complex, asymmetric curvatures are employed to enhance the performance of optical components while minimizing their weight and size. Unfortunately, these asymmetric forms are often difficult to manufacture at the nanoscale with current technologies. Metasurfaces are planar sub-wavelength structures that can control the phase, amplitude, and polarization of incident light, and can thereby mimic complex geometric curvatures on a flat, wavelength-scale thick surface. We present a methodology for designing analogues of freeform optics using a low contrast dielectric metasurface platform for operation at visible wavelengths. We demonstrate a cubic phase plate with a point spread function exhibiting enhanced depth of field over 300 {\\mu}m along the optical axis with potential for performing metasurface-based white light imaging, and an Alvarez lens with a tunable focal length range of over 2.5 mm with 100 {\\mu}m of total mechanical displacement. The adaptation of freeform optics to a sub-wavelength metasurface platform allows for the ultimate miniaturization of optical components and offers a scalable route toward implementing near-arbitrary geometric curvatures in nanophotonics."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary advantage of adapting freeform optics to metasurface platforms in nanophotonics?\n\nA) It allows for the creation of rotationally symmetric optical elements with enhanced performance.\n\nB) It enables the manufacturing of complex, asymmetric curvatures at the macroscale.\n\nC) It facilitates the ultimate miniaturization of optical components with near-arbitrary geometric curvatures.\n\nD) It increases the weight and size of optical components while improving their functionality.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that \"The adaptation of freeform optics to a sub-wavelength metasurface platform allows for the ultimate miniaturization of optical components and offers a scalable route toward implementing near-arbitrary geometric curvatures in nanophotonics.\" This directly supports the statement in option C.\n\nOption A is incorrect because freeform optics specifically aims to move beyond rotational symmetry, not create rotationally symmetric elements.\n\nOption B is incorrect because the focus is on nanoscale manufacturing, not macroscale.\n\nOption D is incorrect because the goal is to minimize weight and size while enhancing performance, not increase them.\n\nThis question tests the student's understanding of the key benefits of combining freeform optics with metasurface technology in the field of nanophotonics."}, "47": {"documentation": {"title": "Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right\n  Ventricle", "source": "Zhao-Yang Wang, Xiao-Yun Zhou, Peichao Li, and Celia Riga, and\n  Guang-Zhong Yang", "docs_id": "1909.08986", "section": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right\n  Ventricle. 3D shape instantiation which reconstructs the 3D shape of a target from limited 2D images or projections is an emerging technique for surgical intervention. It improves the currently less-informative and insufficient 2D navigation schemes for robot-assisted Minimally Invasive Surgery (MIS) to 3D navigation. Previously, a general and registration-free framework was proposed for 3D shape instantiation based on Kernel Partial Least Square Regression (KPLSR), requiring manually segmented anatomical structures as the pre-requisite. Two hyper-parameters including the Gaussian width and component number also need to be carefully adjusted. Deep Convolutional Neural Network (DCNN) based framework has also been proposed to reconstruct a 3D point cloud from a single 2D image, with end-to-end and fully automatic learning. In this paper, an Instantiation-Net is proposed to reconstruct the 3D mesh of a target from its a single 2D image, by using DCNN to extract features from the 2D image and Graph Convolutional Network (GCN) to reconstruct the 3D mesh, and using Fully Connected (FC) layers to connect the DCNN to GCN. Detailed validation was performed to demonstrate the practical strength of the method and its potential clinical use."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key advantages of the Instantiation-Net approach for 3D mesh reconstruction compared to previous methods?\n\nA) It eliminates the need for manual segmentation and hyper-parameter tuning while providing end-to-end learning.\n\nB) It uses Kernel Partial Least Square Regression (KPLSR) to improve accuracy over deep learning methods.\n\nC) It reconstructs 3D point clouds instead of 3D meshes, offering higher precision.\n\nD) It requires multiple 2D images for reconstruction, enhancing the overall accuracy.\n\nCorrect Answer: A\n\nExplanation: The Instantiation-Net approach offers several advantages over previous methods. Unlike the KPLSR-based method, which required manually segmented anatomical structures and careful adjustment of hyper-parameters (Gaussian width and component number), Instantiation-Net uses a Deep Convolutional Neural Network (DCNN) for feature extraction from 2D images and a Graph Convolutional Network (GCN) for 3D mesh reconstruction. This approach provides end-to-end and fully automatic learning, eliminating the need for manual segmentation and hyper-parameter tuning.\n\nOption B is incorrect because Instantiation-Net uses DCNN and GCN, not KPLSR. Option C is incorrect because Instantiation-Net reconstructs 3D meshes, not point clouds. Option D is incorrect because Instantiation-Net can reconstruct 3D meshes from a single 2D image, not multiple images."}, "48": {"documentation": {"title": "Thin homotopy and the holonomy approach to gauge theories", "source": "Claudio Meneses", "docs_id": "1904.10822", "section": ["math-ph", "math.DG", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thin homotopy and the holonomy approach to gauge theories. We survey several mathematical developments in the holonomy approach to gauge theory. A cornerstone of this approach is the introduction of group structures on spaces of based loops on a smooth manifold, relying on certain homotopy equivalence relations -- such as the so-called thin homotopy -- and the resulting interpretation of gauge fields as group homomorphisms to a Lie group $G$ satisfying a suitable smoothness condition, encoding the holonomy of a gauge orbit of smooth connections on a principal $G$-bundle. We also prove several structural results on thin homotopy, and in particular we clarify the difference between thin equivalence and retrace equivalence for piecewise-smooth based loops on a smooth manifold, which are often used interchangeably in the physics literature. We conclude by listing a set of questions on topological and functional analytic aspects of groups of based loops, which we consider to be fundamental to establish a rigorous differential geometric foundation of the holonomy formulation of gauge theory."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the holonomy approach to gauge theory, which of the following statements is correct regarding the relationship between thin homotopy and retrace equivalence for piecewise-smooth based loops on a smooth manifold?\n\nA) Thin homotopy and retrace equivalence are always identical for piecewise-smooth based loops.\n\nB) Thin homotopy is a stronger equivalence relation than retrace equivalence, meaning that all thin homotopic loops are retrace equivalent, but not vice versa.\n\nC) Retrace equivalence is a stronger equivalence relation than thin homotopy, meaning that all retrace equivalent loops are thin homotopic, but not vice versa.\n\nD) Thin homotopy and retrace equivalence are completely independent concepts with no overlap in their equivalence classes.\n\nCorrect Answer: B\n\nExplanation: The document states that the difference between thin equivalence and retrace equivalence for piecewise-smooth based loops on a smooth manifold is clarified, implying that they are not identical. It also mentions that these concepts are often used interchangeably in the physics literature, suggesting a close relationship but not complete equivalence. \n\nThe correct answer is B because thin homotopy is generally considered a finer equivalence relation than retrace equivalence. This means that if two loops are thin homotopic, they are also retrace equivalent, but the converse is not always true. Thin homotopy captures more subtle geometric information about the loops, while retrace equivalence mainly concerns the path traversed by the loops, regardless of how it's parameterized or slightly deformed.\n\nOptions A and D are incorrect as they represent extreme cases that don't align with the nuanced relationship described in the document. Option C reverses the actual relationship between the two concepts and is therefore also incorrect."}, "49": {"documentation": {"title": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling", "source": "Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang", "docs_id": "1803.04687", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling. This paper proposes a new method called Multimodal RNNs for RGB-D scene semantic segmentation. It is optimized to classify image pixels given two input sources: RGB color channels and Depth maps. It simultaneously performs training of two recurrent neural networks (RNNs) that are crossly connected through information transfer layers, which are learnt to adaptively extract relevant cross-modality features. Each RNN model learns its representations from its own previous hidden states and transferred patterns from the other RNNs previous hidden states; thus, both model-specific and crossmodality features are retained. We exploit the structure of quad-directional 2D-RNNs to model the short and long range contextual information in the 2D input image. We carefully designed various baselines to efficiently examine our proposed model structure. We test our Multimodal RNNs method on popular RGB-D benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the key innovation of the Multimodal RNNs method for RGB-D scene semantic segmentation?\n\nA) It uses a single RNN to process both RGB and depth information simultaneously.\nB) It employs two separate RNNs without any interaction between them.\nC) It utilizes two RNNs connected through learnable information transfer layers for cross-modality feature extraction.\nD) It relies solely on quad-directional 2D-RNNs without considering cross-modality information.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The key innovation of the Multimodal RNNs method is that it uses two recurrent neural networks (RNNs) that are connected through information transfer layers. These layers are learned to adaptively extract relevant cross-modality features between the RGB and depth inputs.\n\nAnswer A is incorrect because the method uses two RNNs, not a single one.\n\nAnswer B is incorrect because the RNNs in this method do interact through the information transfer layers, they are not separate.\n\nAnswer D is incorrect because while the method does use quad-directional 2D-RNNs, it also incorporates cross-modality information, which is a crucial aspect of the innovation.\n\nThe correct answer highlights the method's ability to retain both model-specific and cross-modality features, which is a key factor in its improved performance on RGB-D benchmarks."}, "50": {"documentation": {"title": "Lateral Casimir Force between Two Sinusoidally Corrugated Eccentric\n  Cylinders Using Proximity Force Approximation", "source": "M. R. Setare, and A. Seyedzahedi", "docs_id": "1402.3652", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lateral Casimir Force between Two Sinusoidally Corrugated Eccentric\n  Cylinders Using Proximity Force Approximation. This paper is devoted to the presentation of the lateral Casimir force between two sinusoidally corrugated eccentric cylinders. Despite that applying scattering matrix method explains the problem exactly, procedure of applying this method is somehow complicated specially at nonzero temperature. Using the proximity force approximation (PFA) helps to achieve the lateral Casimir force in a truly explicit manner. We assume the cylinders to be slightly eccentric with similar radiuses and separations much smaller than corrugations' wave length for the validity of PFA. For such short distances the effect of finite conductivity would be non negligible. In addition to the effect of finite conductivity, we investigate thermal corrections of the lateral Casimir force to reduce the inaccuracy of the result obtained by PFA. Assuming the Casimir force density between two parallel plates, the normal Casimir force between two cylinders is obtained. With the aid of additive summation of the Casimir energy between cylinders without corrugation, we obtain the lateral Casimir force between corrugated cylinders."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: A researcher is studying the lateral Casimir force between two sinusoidally corrugated eccentric cylinders. Which combination of conditions and methods would be most appropriate for applying the proximity force approximation (PFA) to calculate this force accurately?\n\nA) Cylinders with significantly different radii, large separation distance, and considering only zero temperature effects\nB) Highly eccentric cylinders, separation much larger than corrugations' wavelength, and including finite conductivity effects\nC) Slightly eccentric cylinders with similar radii, separation much smaller than corrugations' wavelength, and including both finite conductivity and thermal corrections\nD) Non-eccentric cylinders, separation equal to corrugations' wavelength, and using only the scattering matrix method\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it aligns with the conditions and methods described in the document for accurately applying the proximity force approximation (PFA) to calculate the lateral Casimir force between two sinusoidally corrugated eccentric cylinders. The document states that for the validity of PFA, we should assume \"the cylinders to be slightly eccentric with similar radiuses and separations much smaller than corrugations' wave length.\" Additionally, it mentions that \"For such short distances the effect of finite conductivity would be non negligible\" and that \"thermal corrections of the lateral Casimir force\" should be investigated \"to reduce the inaccuracy of the result obtained by PFA.\"\n\nOption A is incorrect because it doesn't match the required conditions for PFA and ignores important effects.\nOption B is incorrect because it contradicts the requirement of slight eccentricity and small separation distance.\nOption D is incorrect because it doesn't involve eccentricity and suggests using only the scattering matrix method, which the document describes as complicated, especially at nonzero temperature."}, "51": {"documentation": {"title": "TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image\n  Enhancement", "source": "Xinxu Wei, Xianshi Zhang, Shisen Wang, Yanlin Huang, and Yongjie Li", "docs_id": "2110.02477", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image\n  Enhancement. Low-light image enhancement is a challenging low-level computer vision task because after we enhance the brightness of the image, we have to deal with amplified noise, color distortion, detail loss, blurred edges, shadow blocks and halo artifacts. In this paper, we propose a Two-Stage Network with Channel Attention (denoted as TSN-CA) to enhance the brightness of the low-light image and restore the enhanced images from various kinds of degradation. In the first stage, we enhance the brightness of the low-light image in HSV space and use the information of H and S channels to help the recovery of details in V channel. In the second stage, we integrate Channel Attention (CA) mechanism into the skip connection of U-Net in order to restore the brightness-enhanced image from severe kinds of degradation in RGB space. We train and evaluate the performance of our proposed model on the LOL real-world and synthetic datasets. In addition, we test our model on several other commonly used datasets without Ground-Truth. We conduct extensive experiments to demonstrate that our method achieves excellent effect on brightness enhancement as well as denoising, details preservation and halo artifacts elimination. Our method outperforms many other state-of-the-art methods qualitatively and quantitatively."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the innovative approach of the TSN-CA model for low-light image enhancement?\n\nA) It uses a single-stage network with channel attention in RGB space to enhance brightness and reduce noise simultaneously.\n\nB) It employs a two-stage network, enhancing brightness in HSV space first, then using channel attention in RGB space for degradation restoration.\n\nC) It utilizes a U-Net architecture with channel attention in HSV space for both brightness enhancement and detail preservation.\n\nD) It applies a two-stage network that enhances brightness in RGB space first, then uses channel attention in HSV space for noise reduction.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the TSN-CA (Two-Stage Network with Channel Attention) model employs a unique two-stage approach. In the first stage, it enhances the brightness of the low-light image in HSV color space, utilizing information from the H and S channels to aid in detail recovery in the V channel. The second stage then operates in RGB space, integrating Channel Attention (CA) into the skip connections of a U-Net architecture to restore the brightness-enhanced image from various types of degradation.\n\nOption A is incorrect because it describes a single-stage network, which is not the case for TSN-CA. Option C is wrong because while it mentions U-Net and channel attention, it incorrectly states that these are used in HSV space for both stages. Option D is incorrect because it reverses the color spaces used in each stage \u2013 the model actually uses HSV for brightness enhancement and RGB for degradation restoration, not the other way around."}, "52": {"documentation": {"title": "Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting", "source": "Michael L. Wagman and Martin J. Savage", "docs_id": "1704.07356", "section": ["hep-lat", "cond-mat.stat-mech", "cond-mat.str-el", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting. Path integrals describing quantum many-body systems can be calculated with Monte Carlo sampling techniques, but average quantities are often subject to signal-to-noise ratios that degrade exponentially with time. A phase-reweighting technique inspired by recent observations of random walk statistics in correlation functions is proposed that allows energy levels to be extracted from late-time correlation functions with time-independent signal-to-noise ratios. Phase reweighting effectively includes dynamical refinement of source magnitudes but introduces a bias associated with the phase. This bias can be removed by performing an extrapolation, but at the expense of re-introducing a signal-to-noise problem. Lattice Quantum Chromodynamics calculations of the $\\rho$ and nucleon masses and of the $\\Xi\\Xi$ binding energy show consistency between standard results obtained using earlier-time correlation functions and phase-reweighted results using late-time correlation functions inaccessible to standard statistical analysis methods."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the phase reweighting technique for lattice QCD calculations, which of the following statements is correct?\n\nA) Phase reweighting eliminates the signal-to-noise problem completely without introducing any new challenges.\n\nB) The technique allows for extraction of energy levels from early-time correlation functions with time-independent signal-to-noise ratios.\n\nC) Phase reweighting introduces a bias associated with the phase, which can be removed through extrapolation, but this reintroduces a signal-to-noise problem.\n\nD) The method is ineffective for calculations involving the \u03c1 and nucleon masses, as well as the \u039e\u039e binding energy.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that phase reweighting introduces a bias associated with the phase, which can be removed by performing an extrapolation. However, this extrapolation process re-introduces a signal-to-noise problem. This highlights the trade-off inherent in the technique.\n\nAnswer A is incorrect because while the technique improves the signal-to-noise ratio, it doesn't eliminate the problem completely and introduces new challenges (bias and potential reintroduction of signal-to-noise issues).\n\nAnswer B is incorrect because the technique allows for extraction of energy levels from late-time correlation functions, not early-time ones.\n\nAnswer D is incorrect because the documentation explicitly states that lattice QCD calculations of the \u03c1 and nucleon masses and of the \u039e\u039e binding energy show consistency between standard results and phase-reweighted results."}, "53": {"documentation": {"title": "A rate of convergence result for the largest eigenvalue of complex white\n  Wishart matrices", "source": "Noureddine El Karoui", "docs_id": "math/0409610", "section": ["math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A rate of convergence result for the largest eigenvalue of complex white\n  Wishart matrices. It has been recently shown that if $X$ is an $n\\times N$ matrix whose entries are i.i.d. standard complex Gaussian and $l_1$ is the largest eigenvalue of $X^*X$, there exist sequences $m_{n,N}$ and $s_{n,N}$ such that $(l_1-m_{n,N})/s_{n,N}$ converges in distribution to $W_2$, the Tracy--Widom law appearing in the study of the Gaussian unitary ensemble. This probability law has a density which is known and computable. The cumulative distribution function of $W_2$ is denoted $F_2$. In this paper we show that, under the assumption that $n/N\\to \\gamma\\in(0,\\infty)$, we can find a function $M$, continuous and nonincreasing, and sequences $\\tilde{\\mu}_{n,N}$ and $\\tilde{\\sigma}_{n,N}$ such that, for all real $s_0$, there exists an integer $N(s_0,\\gamma)$ for which, if $(n\\wedge N)\\geq N(s_0,\\gamma)$, we have, with $l_{n,N}=(l_1-\\tilde{\\mu}_{n,N})/\\tilde{\\sigma}_{n,N}$, \\[\\forall s\\geq s_0\\qquad (n\\wedge N)^{2/3}|P(l_{n,N}\\leq s)-F_2(s)|\\leq M(s_0)\\exp(-s).\\] The surprisingly good 2/3 rate and qualitative properties of the bounding function help explain the fact that the limiting distribution $W_2$ is a good approximation to the empirical distribution of $l_{n,N}$ in simulations, an important fact from the point of view of (e.g., statistical) applications."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Consider a complex white Wishart matrix where X is an n\u00d7N matrix with i.i.d. standard complex Gaussian entries. Let l\u2081 be the largest eigenvalue of X*X. As n,N \u2192 \u221e with n/N \u2192 \u03b3 \u2208 (0,\u221e), what is the convergence rate of the properly scaled and centered largest eigenvalue to the Tracy-Widom distribution?\n\nA) (n\u2227N)^(1/2)\nB) (n\u2227N)^(2/3)\nC) (n\u2227N)^(3/4)\nD) (n\u2227N)^(1)\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the convergence rate result presented in the document. The correct answer is B) (n\u2227N)^(2/3). The document states that \"we have, with l_{n,N}=(l\u2081-\u03bc\u0303_{n,N})/\u03c3\u0303_{n,N}, \u2200s\u2265s\u2080 (n\u2227N)^(2/3)|P(l_{n,N}\u2264s)-F\u2082(s)|\u2264M(s\u2080)exp(-s).\" This indicates that the convergence rate is of order (n\u2227N)^(2/3), which the document describes as \"surprisingly good.\" \n\nOption A is incorrect as it suggests a slower convergence rate. Option C suggests a faster rate that is not supported by the given information. Option D suggests an even faster rate, which would be unusual for this type of problem.\n\nThe question challenges students to identify the correct convergence rate from the complex mathematical description and understand its significance in relation to the Tracy-Widom distribution approximation for the largest eigenvalue of complex white Wishart matrices."}, "54": {"documentation": {"title": "An Improved Quadrature Voltage-Controlled Oscillator with\n  Through-Silicon-Via Inductor in Three-dimensional Integrated Circuits", "source": "Dawei Li, Madhava Sarma Vemuri, Umamaheswara Rao Tida", "docs_id": "2001.10678", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Improved Quadrature Voltage-Controlled Oscillator with\n  Through-Silicon-Via Inductor in Three-dimensional Integrated Circuits. Low-power quadrature voltage-controlled oscillator (QVCO) design utilizing transformer-feedback and current-reuse techniques with increased frequency range is proposed in this paper. With increasing demand for QVCOs in on-chip applications, the conventional spiral inductor based approaches for QVCOs has become a major bottleneck due to their large size. To address this concern, we propose to replace the conventional spiral inductor based approaches with through-silicon-via (TSV) inductor based approach in three-dimensional integrated circuits (3D ICs). In addition, the proposed QVCO circuit can provide higher frequency range of operation compared with conventional designs. Experimental results show by replacing conventional spiral transformers with TSV transformers, up to 3.9x reduction in metal resource consumption. The proposed QVCOs achieves a phase noise of -114 $dBc/Hz$@1 $MHz$ and -111.2 $dBc/Hz$@1 $MHz$ at the carrier of 2.5 $GHz$ for toroidal TSV transformed based-QVCO and vertical spiral transformer based-QVCO respectively. The power consumption is only 1.5 $mW$ and 1.7 $mW$ for toroidal TSV transformed based-QVCO and vertical spiral transformer based-QVCO respectively, under the supply voltage of 0.7 $V$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: A new quadrature voltage-controlled oscillator (QVCO) design is proposed in the paper. Which combination of features and results best describes this novel QVCO design?\n\nA) Uses conventional spiral inductors, achieves -114 dBc/Hz@1 MHz phase noise, consumes 1.5 mW power\nB) Employs through-silicon-via (TSV) inductors, provides 3.9x reduction in metal resource consumption, operates at 2.5 GHz\nC) Utilizes transformer-feedback, achieves -111.2 dBc/Hz@1 MHz phase noise, consumes 1.7 mW power\nD) Uses current-reuse techniques, operates at 0.7 V supply voltage, achieves 3.9x increase in frequency range\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately combines multiple features and results of the proposed QVCO design:\n\n1. The design employs through-silicon-via (TSV) inductors, which is a key innovation mentioned in the paper to replace conventional spiral inductors.\n2. The paper states that replacing conventional spiral transformers with TSV transformers results in \"up to 3.9x reduction in metal resource consumption.\"\n3. The operating frequency of 2.5 GHz is explicitly mentioned for both the toroidal TSV transformer-based QVCO and the vertical spiral transformer-based QVCO.\n\nOptions A, C, and D each contain some correct information but also include inaccuracies or mix details from different aspects of the paper:\n\nA) Uses conventional spiral inductors (incorrect, as the paper proposes replacing these) and misattributes the power consumption.\nC) The phase noise figure is correct for the vertical spiral transformer-based QVCO, but this isn't the primary proposed design.\nD) While current-reuse techniques are mentioned, the 3.9x figure relates to metal resource reduction, not frequency range increase."}, "55": {"documentation": {"title": "A network approach to cartel detection in public auction markets", "source": "Johannes Wachs, J\\'anos Kert\\'esz", "docs_id": "1906.08667", "section": ["physics.soc-ph", "cs.SI", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A network approach to cartel detection in public auction markets. Competing firms can increase profits by setting prices collectively, imposing significant costs on consumers. Such groups of firms are known as cartels and because this behavior is illegal, their operations are secretive and difficult to detect. Cartels feel a significant internal obstacle: members feel short-run incentives to cheat. Here we present a network-based framework to detect potential cartels in bidding markets based on the idea that the chance a group of firms can overcome this obstacle and sustain cooperation depends on the patterns of its interactions. We create a network of firms based on their co-bidding behavior, detect interacting groups, and measure their cohesion and exclusivity, two group-level features of their collective behavior. Applied to a market for school milk, our method detects a known cartel and calculates that it has high cohesion and exclusivity. In a comprehensive set of nearly 150,000 public contracts awarded by the Republic of Georgia from 2011 to 2016, detected groups with high cohesion and exclusivity are significantly more likely to display traditional markers of cartel behavior. We replicate this relationship between group topology and the emergence of cooperation in a simulation model. Our method presents a scalable, unsupervised method to find groups of firms in bidding markets ideally positioned to form lasting cartels."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A network-based approach to cartel detection in public auction markets relies on which two key group-level features of collective behavior?\n\nA) Profitability and secrecy\nB) Cohesion and exclusivity\nC) Co-bidding and cheating\nD) Scalability and unsupervision\n\nCorrect Answer: B\n\nExplanation: The question tests understanding of the core concepts in the network-based approach to cartel detection described in the document. The correct answer is B) Cohesion and exclusivity. \n\nThe document explicitly states: \"We create a network of firms based on their co-bidding behavior, detect interacting groups, and measure their cohesion and exclusivity, two group-level features of their collective behavior.\" These two features are central to the method's ability to identify potential cartels.\n\nA) is incorrect because while profitability and secrecy are characteristics of cartels, they are not the specific features used in this network-based detection method.\n\nC) is incorrect because although co-bidding is used to create the network and cheating is mentioned as an obstacle for cartels, these are not the two key features measured to detect potential cartels.\n\nD) is incorrect as scalability and unsupervision are characteristics of the method itself, not the group-level features it measures in potential cartels.\n\nThis question requires careful reading and the ability to distinguish between various concepts mentioned in the text, making it challenging for an exam."}, "56": {"documentation": {"title": "Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training", "source": "Jiatong Shi, Nan Huo, Qin Jin", "docs_id": "2008.08647", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training. Mispronunciation detection is an essential component of the Computer-Assisted Pronunciation Training (CAPT) systems. State-of-the-art mispronunciation detection models use Deep Neural Networks (DNN) for acoustic modeling, and a Goodness of Pronunciation (GOP) based algorithm for pronunciation scoring. However, GOP based scoring models have two major limitations: i.e., (i) They depend on forced alignment which splits the speech into phonetic segments and independently use them for scoring, which neglects the transitions between phonemes within the segment; (ii) They only focus on phonetic segments, which fails to consider the context effects across phonemes (such as liaison, omission, incomplete plosive sound, etc.). In this work, we propose the Context-aware Goodness of Pronunciation (CaGOP) scoring model. Particularly, two factors namely the transition factor and the duration factor are injected into CaGOP scoring. The transition factor identifies the transitions between phonemes and applies them to weight the frame-wise GOP. Moreover, a self-attention based phonetic duration modeling is proposed to introduce the duration factor into the scoring model. The proposed scoring model significantly outperforms baselines, achieving 20% and 12% relative improvement over the GOP model on the phoneme-level and sentence-level mispronunciation detection respectively."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following best describes the main innovation of the Context-aware Goodness of Pronunciation (CaGOP) scoring model compared to traditional GOP models?\n\nA) It uses Deep Neural Networks for acoustic modeling instead of Hidden Markov Models\nB) It incorporates both transition and duration factors to address contextual effects in pronunciation\nC) It eliminates the need for forced alignment in speech segmentation\nD) It focuses exclusively on phonetic segments for more accurate scoring\n\nCorrect Answer: B\n\nExplanation: The Context-aware Goodness of Pronunciation (CaGOP) scoring model introduces two key innovations to address the limitations of traditional GOP models: the transition factor and the duration factor. The transition factor accounts for the transitions between phonemes within segments, addressing the first limitation mentioned in the text. The duration factor, implemented through self-attention based phonetic duration modeling, helps consider context effects across phonemes, addressing the second limitation. Option B correctly summarizes these main innovations.\n\nOption A is incorrect because both traditional GOP and CaGOP use Deep Neural Networks for acoustic modeling. Option C is incorrect because CaGOP still uses forced alignment, but improves upon it. Option D is incorrect because CaGOP actually expands beyond just focusing on phonetic segments to consider contextual effects."}, "57": {"documentation": {"title": "Photoproduction of $\\Lambda^*$ and $\\Sigma^*$ resonances with\n  $J^P=1/2^-$ off the proton", "source": "Sang-Ho Kim, K. P. Khemchandani, A. Martinez Torres, Seung-il Nam,\n  Atsushi Hosaka", "docs_id": "2101.08668", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photoproduction of $\\Lambda^*$ and $\\Sigma^*$ resonances with\n  $J^P=1/2^-$ off the proton. We study the photoproduction of the $\\Lambda(1405)$ and $\\Sigma(1400)$ hyperon resonances, the latter of which is not a well established state. We evaluate the $s$-, $t$- and $u$-channel diagrams in the Born approximation by employing the effective Lagrangians. A new ingredient is the inclusion of a nucleon resonance $N^*(1895)$ that is dynamically generated with predictions for its coupling to the $K\\Lambda(1405)$ and $K\\Sigma(1400)$ channels. To extend the applicability of the model to energies beyond the threshold region, we consider a Regge model for the $t$-channel $K$- and $K^*$-exchanges. Our results are in good agreement with the CLAS data available on $\\Lambda(1405)$, while for $\\Sigma(1400)$ we predict observables for its production. We also provide polarization observables for both hyperon productions, which can be useful in future experimental investigations. The present study provides new information on the nucleon resonance $N^*(1895)$ which can be an alternative source for generating the hyperon resonances $\\Lambda(1405)$ and $\\Sigma(1400)$."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements most accurately describes the role and significance of the N*(1895) resonance in the study of \u039b(1405) and \u03a3(1400) photoproduction, as presented in the research?\n\nA) It is a well-established nucleon resonance used to calibrate the experimental setup for hyperon production.\n\nB) It is a dynamically generated resonance with predicted couplings to K\u039b(1405) and K\u03a3(1400) channels, potentially serving as an alternative source for generating these hyperon resonances.\n\nC) It is a high-energy resonance used exclusively in the Regge model to extend the applicability of the study beyond the threshold region.\n\nD) It is a hypothetical resonance introduced to explain discrepancies between theoretical predictions and CLAS data for \u039b(1405) production.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation explicitly states that a \"new ingredient is the inclusion of a nucleon resonance N*(1895) that is dynamically generated with predictions for its coupling to the K\u039b(1405) and K\u03a3(1400) channels.\" Furthermore, the final sentence emphasizes that \"The present study provides new information on the nucleon resonance N*(1895) which can be an alternative source for generating the hyperon resonances \u039b(1405) and \u03a3(1400).\" This directly supports option B as the most accurate description of the N*(1895) resonance's role in the study.\n\nOption A is incorrect because the N*(1895) is not described as well-established or used for calibration. Option C is wrong because while a Regge model is mentioned for t-channel exchanges, it's not specifically linked to the N*(1895). Option D is incorrect as the N*(1895) is not described as hypothetical or introduced to explain discrepancies with CLAS data; in fact, the results are said to be in good agreement with CLAS data for \u039b(1405)."}, "58": {"documentation": {"title": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?", "source": "Marina Friedrich, S\\'ebastien Fries, Michael Pahle and Ottmar\n  Edenhofer", "docs_id": "1906.10572", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?. In 2018, allowance prices in the EU Emission Trading Scheme (EU ETS) experienced a run-up from persistently low levels in previous years. Regulators attribute this to a comprehensive reform in the same year, and are confident the new price level reflects an anticipated tighter supply of allowances. We ask if this is indeed the case, or if it is an overreaction of the market driven by speculation. We combine several econometric methods - time-varying coefficient regression, formal bubble detection as well as time stamping and crash odds prediction - to juxtapose the regulators' claim versus the concurrent explanation. We find evidence of a long period of explosive behaviour in allowance prices, starting in March 2018 when the reform was adopted. Our results suggest that the reform triggered market participants into speculation, and question regulators' confidence in its long-term outcome. This has implications for both the further development of the EU ETS, and the long lasting debate about taxes versus emission trading schemes."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: The 2018 price increase in EU Emission Trading Scheme (EU ETS) allowances has been attributed to a comprehensive reform. However, the study suggests an alternative explanation. Which of the following best describes the study's findings and their implications?\n\nA) The price increase was solely due to the reform, validating regulators' confidence in the new pricing mechanism.\n\nB) The study found no evidence of speculation and confirmed that the price increase accurately reflects future allowance scarcity.\n\nC) The research identified a period of explosive behavior in allowance prices, suggesting that the reform triggered market speculation rather than reflecting fundamental changes in supply.\n\nD) The study concluded that emission trading schemes are inherently flawed and should be replaced with carbon taxes immediately.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study combined several econometric methods to analyze the price trend in EU ETS allowances. It found evidence of a long period of explosive behavior in allowance prices, starting in March 2018 when the reform was adopted. This suggests that the reform triggered market participants into speculation, rather than reflecting an anticipated tighter supply of allowances as claimed by regulators.\n\nAnswer A is incorrect because the study questions the regulators' confidence in the reform's long-term outcome, rather than validating it.\n\nAnswer B is wrong because the study did find evidence of speculation and challenges the idea that the price increase accurately reflects future scarcity.\n\nAnswer D is too extreme and not supported by the study's findings. While the research does have implications for the debate between taxes and emission trading schemes, it doesn't conclude that trading schemes should be immediately replaced by taxes."}, "59": {"documentation": {"title": "Non interactive simulation of correlated distributions is decidable", "source": "Anindya De and Elchanan Mossel and Joe Neeman", "docs_id": "1701.01485", "section": ["cs.CC", "cs.IT", "math.IT", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non interactive simulation of correlated distributions is decidable. A basic problem in information theory is the following: Let $\\mathbf{P} = (\\mathbf{X}, \\mathbf{Y})$ be an arbitrary distribution where the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob be two players where Alice gets samples $\\{x_i\\}_{i \\ge 1}$ and Bob gets samples $\\{y_i\\}_{i \\ge 1}$ and for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What joint distributions $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction? Classical works in information theory by G{\\'a}cs-K{\\\"o}rner and Wyner answer this question when at least one of $\\mathbf{P}$ or $\\mathbf{Q}$ is the distribution on $\\{0,1\\} \\times \\{0,1\\}$ where each marginal is unbiased and identical. However, other than this special case, the answer to this question is understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that this problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times \\{0,1\\}$. We extend their result to $\\mathbf{Q}$ supported on any finite alphabet. We rely on recent results in Gaussian geometry (by the authors) as well as a new \\emph{smoothing argument} inspired by the method of \\emph{boosting} from learning theory and potential function arguments from complexity theory and additive combinatorics."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: In the context of non-interactive simulation of correlated distributions, which of the following statements is correct?\n\nA) The problem of simulating joint distributions Q is fully solved for all cases of input distribution P and output distribution Q.\n\nB) The G\u00e1cs-K\u00f6rner and Wyner results provide a complete solution for simulating any joint distribution Q given any input distribution P.\n\nC) The work by Ghazi, Kamath, and Sudan showed that the problem is decidable for Q supported on any finite alphabet.\n\nD) Recent research has extended the decidability result to Q supported on any finite alphabet, relying on Gaussian geometry and a new smoothing argument.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that recent work has extended the decidability result of Ghazi, Kamath, and Sudan (which was limited to Q supported on {0,1} x {0,1}) to Q supported on any finite alphabet. This extension relies on recent results in Gaussian geometry and a new smoothing argument inspired by boosting from learning theory and potential function arguments from complexity theory and additive combinatorics.\n\nOption A is incorrect because the problem is not fully solved for all cases; it's only decidable for certain scenarios.\n\nOption B is incorrect because the G\u00e1cs-K\u00f6rner and Wyner results only provide solutions for specific cases where at least one of P or Q is the distribution on {0,1} x {0,1} with unbiased and identical marginals.\n\nOption C is incorrect because it attributes the more general result to Ghazi, Kamath, and Sudan, when in fact their work was limited to Q supported on {0,1} x {0,1}."}}