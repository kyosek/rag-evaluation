{"0": {"documentation": {"title": "Anomalous switching in Nb/Ru/Sr2RuO4 topological junctions by chiral\n  domain wall motion", "source": "M. S. Anwar, Taketomo Nakamura, S. Yonezawa, M. Yakabe, R. Ishiguro,\n  H. Takayanagi and Y. Maeno", "docs_id": "1308.2460", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anomalous switching in Nb/Ru/Sr2RuO4 topological junctions by chiral\n  domain wall motion. A spontaneous symmetry breaking in a system often results in domain wall formation. The motion of such domain walls is utilized to realize novel devices like racetrack-memories, in which moving ferromagnetic domain walls store and carry information. Superconductors breaking time reversal symmetry can also form domains with degenerate chirality of their superconducting order parameter. Sr2RuO4 is the leading candidate of a chiral p-wave superconductor, expected to be accompanied by chiral domain structure. Here, we present that Nb/Ru/Sr2RuO4 topological superconducting-junctions, with which the phase winding of order parameter can be effectively probed by making use of real-space topology, exhibit unusual switching between higher and lower critical current states. This switching is well explained by chiral-domain-wall dynamics. The switching can be partly controlled by external parameters such as temperature, magnetic field and current. These results open up a possibility to utilize the superconducting chiral domain wall motion for future novel superconducting devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mechanism responsible for the unusual switching behavior observed in Nb/Ru/Sr2RuO4 topological superconducting-junctions, and how does it relate to the concept of chiral domain wall motion?\n\nA) The switching behavior is due to the formation of defects in the superconducting material, which disrupts the phase coherence of the superconducting order parameter.\n\nB) The switching behavior is caused by the motion of chiral domain walls, which are topological defects that arise from the spontaneous symmetry breaking in the system.\n\nC) The switching behavior is a result of the interaction between the superconducting material and the magnetic field, which induces a phase transition in the superconducting order parameter.\n\nD) The switching behavior is due to the thermal fluctuations in the superconducting material, which cause the phase coherence of the superconducting order parameter to break down.\n\nCorrect Answer: B) The switching behavior is caused by the motion of chiral domain walls, which are topological defects that arise from the spontaneous symmetry breaking in the system.\n\nExplanation: The correct answer is B) because the documentation states that the switching behavior in Nb/Ru/Sr2RuO4 topological superconducting-junctions is well-explained by chiral-domain-wall dynamics. The formation of chiral domain walls is a result of the spontaneous symmetry breaking in the system, which is a key concept in the theory of topological superconductivity. The other options are incorrect because they do not accurately describe the underlying mechanism responsible for the switching behavior. Option A is incorrect because defects in the superconducting material do not directly cause the switching behavior. Option C is incorrect because the interaction between the superconducting material and the magnetic field does not induce a phase transition in the superconducting order parameter. Option D is incorrect because thermal fluctuations in the superconducting material do not cause the phase coherence of the superconducting order parameter to break down."}, "1": {"documentation": {"title": "Defect Level Switching for Highly-Nonlinear and Hysteretic Electronic\n  Devices", "source": "Han Yin, Abinash Kumar, James M. LeBeau, R. Jaramillo", "docs_id": "2005.07935", "section": ["physics.app-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Defect Level Switching for Highly-Nonlinear and Hysteretic Electronic\n  Devices. Nonlinear and hysteretic electrical devices are needed for applications from circuit protection to next-generation computing. Widely-studied devices for resistive switching are based on mass transport, such as the drift of ions in an electric field, and on collective phenomena, such as insulator-metal transitions. We ask whether the large photoconductive response known in many semiconductors can be stimulated in the dark and harnessed to design electrical devices. We design and test devices based on photoconductive CdS, and our results are consistent with the hypothesis that resistive switching arises from point defects that switch between deep- and shallow-donor configurations: defect level switching (DLS). This new electronic device design principle - photoconductivity without photons - leverages decades of research on photoconductivity and defect spectroscopy. It is easily generalized and will enable the rational design of new nonlinear, hysteretic devices for future electronics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism underlying the resistive switching in the proposed defect level switching (DLS) devices, as described in the study on \"Defect Level Switching for Highly-Nonlinear and Hysteretic Electronic Devices\"?\n\nA) Collective phenomena, such as insulator-metal transitions\nB) Point defects that switch between deep- and shallow-donor configurations\nC) Mass transport, such as the drift of ions in an electric field\nD) Photoconductive response in semiconductors\n\nCorrect Answer: B) Point defects that switch between deep- and shallow-donor configurations\n\nExplanation: The study suggests that resistive switching in the proposed DLS devices arises from point defects that switch between deep- and shallow-donor configurations. This is consistent with the hypothesis that the large photoconductive response in semiconductors can be stimulated in the dark and harnessed to design electrical devices. The other options are incorrect because collective phenomena and mass transport are widely-studied mechanisms for resistive switching, but not the primary mechanism underlying DLS devices, and photoconductive response in semiconductors is a characteristic of the devices, but not the underlying mechanism."}, "2": {"documentation": {"title": "Review on Master Patient Index", "source": "Dr W.G Prabath Jayatissa, Prof Vajira H W Dissanayake, Dr Roshan\n  Hewapathirane", "docs_id": "1803.05994", "section": ["cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Review on Master Patient Index. In today's health care establishments there is a great diversity of information systems. Each with different specificities and capacities, proprietary communication methods, and hardly allow scalability. This set of characteristics hinders the interoperability of all these systems, in the search for the good of the patient. It is vulgar that, when we look at all the databases of each of these information systems, we come across different registers that refer to the same person; records with insufficient data; records with erroneous data due to errors or misunderstandings when inserting patient data; and records with outdated data. These problems cause duplicity, incoherence, discontinuation and dispersion in patient data. With the intention of minimizing these problems that the concept of a Master Patient Index is necessary. A Master Patient Index proposes a centralized repository, which indexes all patient records of a given set of information systems. Which is composed of a set of demographic data sufficient to unambiguously identify a person and a list of identifiers that identify the various records that the patient has in the repositories of each information system. This solution allows for synchronization between all the actors, minimizing incoherence, out datedness, lack of data, and a decrease in duplicate registrations. The Master Patient Index is an asset to patients, the medical staff and health care providers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of a Master Patient Index (MPI) in healthcare information systems, and how does it address the issues of interoperability, data consistency, and patient data management?\n\n**A)** A Master Patient Index is a decentralized repository that stores patient data from multiple information systems, allowing for data sharing and collaboration among healthcare providers. However, it may lead to data duplication and inconsistencies.\n\n**B)** A Master Patient Index is a centralized repository that indexes all patient records from various information systems, enabling synchronization and minimizing data inconsistencies, outdated data, and duplicate registrations. This solution improves patient data management and facilitates interoperability among healthcare providers.\n\n**C)** A Master Patient Index is a data warehouse that aggregates patient data from multiple information systems, but it does not address the issues of data consistency and interoperability. Instead, it may lead to data siloing and decreased data quality.\n\n**D)** A Master Patient Index is a data exchange standard that enables the sharing of patient data between information systems, but it does not provide a centralized repository for patient data management. This approach may lead to data fragmentation and inconsistencies.\n\n**Correct Answer:** B) A Master Patient Index is a centralized repository that indexes all patient records from various information systems, enabling synchronization and minimizing data inconsistencies, outdated data, and duplicate registrations. This solution improves patient data management and facilitates interoperability among healthcare providers.\n\n**Explanation:** The correct answer, B, accurately reflects the primary purpose of a Master Patient Index, which is to create a centralized repository that indexes all patient records from various information systems. This approach enables synchronization, minimizes data inconsistencies, outdated data, and duplicate registrations, ultimately improving patient data management and facilitating interoperability among healthcare providers. The other options (A, C, and D) are incorrect because they either misrepresent the purpose of a Master Patient Index or propose alternative approaches that do not address the issues of interoperability, data consistency, and patient data management."}, "3": {"documentation": {"title": "Improving living biomass C-stock loss estimates by combining optical\n  satellite, airborne laser scanning, and NFI data", "source": "Johannes Breidenbach, Janis Ivanovs, Annika Kangas, Thomas\n  Nord-Larsen, Mats Nilson, Rasmus Astrup", "docs_id": "2012.07921", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving living biomass C-stock loss estimates by combining optical\n  satellite, airborne laser scanning, and NFI data. Policy measures and management decisions aiming at enhancing the role of forests in mitigating climate-change require reliable estimates of C-stock dynamics in greenhouse gas inventories (GHGIs). Aim of this study was to assemble design-based estimators to provide estimates relevant for GHGIs using national forest inventory (NFI) data. We improve basic expansion (BE) estimates of living-biomass C-stock loss using field-data only, by leveraging with remotely-sensed auxiliary data in model-assisted (MA) estimates. Our case studies from Norway, Sweden, Denmark, and Latvia covered an area of >70 Mha. Landsat-based Forest Cover Loss (FCL) and one-time wall-to-wall airborne laser scanning (ALS) data served as auxiliary data. ALS provided information on the C-stock before a potential disturbance indicated by FCL. The use of FCL in MA estimators resulted in considerable efficiency gains which in most cases were further increased by using ALS in addition. A doubling of efficiency was possible for national estimates and even larger efficiencies were observed at the sub-national level. Average annual estimates were considerably more precise than pooled estimates using NFI data from all years at once. The combination of remotely-sensed with NFI field data yields reliable estimates which is not necessarily the case when using remotely-sensed data without reference observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of combining remotely-sensed auxiliary data with national forest inventory (NFI) data in estimating living-biomass C-stock loss, as demonstrated in the study?\n\n**A)** The use of remotely-sensed data allows for more accurate estimates of forest cover loss.\n**B)** The combination of remotely-sensed and NFI data yields more precise average annual estimates than using NFI data alone.\n**C)** The use of airborne laser scanning (ALS) data provides information on the C-stock before a potential disturbance, resulting in more accurate estimates.\n**D)** The combination of remotely-sensed and NFI data allows for more efficient national estimates and sub-national level estimates.\n\n**Correct Answer:** D) The combination of remotely-sensed and NFI data allows for more efficient national estimates and sub-national level estimates.\n\n**Explanation:** The study found that the use of Landsat-based Forest Cover Loss (FCL) and ALS data resulted in considerable efficiency gains in estimating living-biomass C-stock loss, with a doubling of efficiency possible for national estimates and even larger efficiencies observed at the sub-national level. This suggests that the combination of remotely-sensed and NFI data allows for more efficient estimation of C-stock loss, making option D the correct answer."}, "4": {"documentation": {"title": "Modulation of Control Authority in Adaptive HapticShared Control\n  Paradigms", "source": "Vahid Izadi, Amir H. Ghasemi", "docs_id": "2007.07436", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modulation of Control Authority in Adaptive HapticShared Control\n  Paradigms. This paper presents an adaptive haptic shared control framework wherein a driver and an automation system are physically connected through a motorized steering wheel. The automation system is modeled as an intelligent agent that is not only capable of making decisions but also monitoring the human's behavior and adjusting its behavior accordingly. To enable the automation system to smoothly exchange the control authority with the human partner, this paper introduces a novel self-regulating impedance controller for the automation system. To determine an optimal modulation policy, a cost function is defined. The terms of the cost function are assigned to minimize the performance error and reduce the disagreement between the human and automation system. To solve the optimal control problem, we employed a nonlinear model predictive approach and used the continuation generalized minimum residual method to solve the nonlinear cost function. To demonstrate the effectiveness of the proposed approach, simulation studies consider a scenario where the human and the automation system both detect an obstacle and negotiate on controlling the steering wheel so that the obstacle can be avoided safely. The simulations involve four interaction modes addressing the cooperation status (cooperative and uncooperative) and the desired direction of the control transfer (active safety and autopilot)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the adaptive haptic shared control framework presented in the paper \"Modulation of Control Authority in Adaptive Haptic Shared Control Paradigms\", what is the primary objective of the cost function defined to determine an optimal modulation policy?\n\n**A)** To minimize the performance error and reduce the disagreement between the human and automation system, while also considering the cooperation status of the interaction.\n\n**B)** To maximize the performance error and increase the disagreement between the human and automation system, in order to achieve active safety and autopilot control transfer.\n\n**C)** To optimize the nonlinear model predictive approach and use the continuation generalized minimum residual method to solve the nonlinear cost function, without considering the cost function's objective.\n\n**D)** To determine the optimal modulation policy that balances the performance error and disagreement between the human and automation system, while also considering the desired direction of the control transfer.\n\n**Correct Answer:** A) To minimize the performance error and reduce the disagreement between the human and automation system, while also considering the cooperation status of the interaction.\n\n**Explanation:** The correct answer is A) because the cost function is defined to minimize the performance error and reduce the disagreement between the human and automation system, as stated in the paper. Additionally, the cost function also considers the cooperation status of the interaction, which is an important aspect of the adaptive haptic shared control framework. The other options are incorrect because they either misrepresent the objective of the cost function (B), omit important considerations (C), or introduce irrelevant factors (D)."}, "5": {"documentation": {"title": "Implications of a frame dependent gravitational effective action for\n  perturbations on the Robertson-Walker Metric", "source": "Stephen L. Adler", "docs_id": "1704.00388", "section": ["gr-qc", "astro-ph.CO", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of a frame dependent gravitational effective action for\n  perturbations on the Robertson-Walker Metric. In earlier work we showed that a frame dependent effective action motivated by the postulates of three-space general coordinate invariance and Weyl scaling invariance exactly mimics a cosmological constant in Robertson-Walker (RW) spacetimes. Here we study the implications of this effective action for small fluctuations around a spatially flat RW background geometry. The equations for the conserving extension of the modified stress-energy tensor can be integrated in closed form, and involve only the metric perturbation $h_{00}$. Hence the equations for tensor and vector perturbations are unmodified, but there are Hubble scale additions to the scalar perturbation equations, which nonetheless admit no propagating wave solutions. Consequently, there are no modifications to standard gravitational wave propagation theory, but there may be observable implications for cosmology. We give a self-contained discussion, including an analysis of the restricted class of gauge transformations that act when a frame dependent effective action is present."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a frame-dependent effective action for perturbations on the Robertson-Walker metric, as described in the given Arxiv documentation. What is the consequence of this effective action on the propagation of scalar perturbations in the context of cosmology?\n\nA) The scalar perturbations will exhibit propagating wave solutions, leading to observable effects in the early universe.\nB) The scalar perturbations will not propagate, but will instead be modified by Hubble scale additions to the equations of motion.\nC) The effective action will not affect the propagation of tensor and vector perturbations, but will introduce new constraints on the gauge transformations.\nD) The effective action will not have any observable implications for cosmology, as it only affects the scalar perturbations.\n\nCorrect Answer: B) The scalar perturbations will not propagate, but will instead be modified by Hubble scale additions to the equations of motion.\n\nExplanation: The correct answer is B) because the documentation states that the equations for the conserving extension of the modified stress-energy tensor can be integrated in closed form, and involve only the metric perturbation $h_{00}$. This implies that the scalar perturbations will not propagate, but will instead be modified by Hubble scale additions to the equations of motion. The other options are incorrect because they either misinterpret the effect of the effective action on scalar perturbations (A), or incorrectly state that the effective action will not affect the propagation of tensor and vector perturbations (C), or that it will have no observable implications for cosmology (D)."}, "6": {"documentation": {"title": "Birdsong dialect patterns explained using magnetic domains", "source": "James Burridge and Steven Kenney", "docs_id": "1603.04429", "section": ["q-bio.PE", "q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Birdsong dialect patterns explained using magnetic domains. The songs and calls of many bird species, like human speech, form distinct regional dialects. We suggest that the process of dialect formation is analogous to the physical process of magnetic domain formation. We take the coastal breeding grounds of the Puget Sound white crowned sparrow as an example. Previous field studies suggest that birds of this species learn multiple songs early in life, and when establishing a territory for the first time, retain one of these dialects in order to match the majority of their neighbours. We introduce a simple lattice model of the process, showing that this matching behaviour can produce single dialect domains provided the death rate of adult birds is sufficiently low. We relate death rate to thermodynamic temperature in magnetic materials, and calculate the critical death rate by analogy with the Ising model. Using parameters consistent with the known behavior of these birds we show that coastal dialect domain shapes may be explained by viewing them as low temperature \"stripe states\"."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Explain how the formation of dialect domains in birdsong can be analogous to the physical process of magnetic domain formation, and what is the significance of the critical death rate in this context.\n\n**A)** The formation of dialect domains in birdsong can be analogous to magnetic domain formation because both involve the emergence of distinct patterns or structures from a complex system. In birds, dialect domains form through the process of territorial matching, where birds retain a specific dialect to match their neighbors, much like how magnetic domains form through the alignment of magnetic moments in a material. The critical death rate is significant because it determines the stability of these dialect domains, with low death rates allowing for the formation of large-scale dialect domains.\n\n**B)** The analogy between dialect domain formation in birdsong and magnetic domain formation is based on the idea that both involve the creation of distinct patterns or structures through a process of self-organization. However, the critical death rate is not directly related to the stability of dialect domains, but rather to the thermodynamic temperature of the system. In this context, the critical death rate is the temperature below which the system exhibits a phase transition, leading to the formation of large-scale dialect domains.\n\n**C)** The formation of dialect domains in birdsong can be seen as a form of \"stripe state\" behavior, where the birds' songs and calls form a pattern of alternating dialects that are stable at low temperatures. The critical death rate is significant because it determines the temperature below which this stripe state behavior becomes unstable, leading to the formation of larger dialect domains. This analogy with magnetic materials is based on the idea that both exhibit a phase transition at a critical temperature, leading to the emergence of a new, more stable state.\n\n**D)** The analogy between dialect domain formation in birdsong and magnetic domain formation is based on the idea that both involve the creation of distinct patterns or structures through a process of self-organization. However, the critical death rate is not directly related to the stability of dialect domains, but rather to the rate at which birds die and are replaced by new individuals. In this context, the critical death rate is the rate below which the system exhibits a phase transition, leading to the formation of large-scale dialect domains.\n\n**Correct Answer:** A) The formation of dialect domains in birdsong can be analogous to magnetic domain formation because both involve the emergence of distinct patterns or structures from a complex system. In birds, dialect domains form through the process of territorial matching, where birds retain a specific dialect to match their neighbors, much like how magnetic domains form through the alignment of magnetic moments in a material. The critical death rate is significant because it determines the stability of these dialect domains, with low death rates allowing for the formation of large-scale dialect domains."}, "7": {"documentation": {"title": "Studies of light nicleus clustering in relativistic multifragmentation\n  processes", "source": "V. Bradnova, M.M. Chernyavsky, A.Sh. Gaitinov, L.A. Goncharova, L.\n  Just, S.P. Kharlamov, A.D. Kovalenko, M. Haiduc, V.G. Larionova, F.G.\n  Lepekhin, A.I. Malakhov, G.I. Orlova, N.G. Peresadko, N.G. Polukhina, P.A.\n  Rukoyatkin, V.V. Rusakova, N.A. Salmanova, B.B. Simonov, S. Vokal, P.I.\n  Zarubin, and I.G. Zarubina", "docs_id": "nucl-ex/0604009", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Studies of light nicleus clustering in relativistic multifragmentation\n  processes. We give an overview of results and prospects of nuclear clustering studies on the grounds of the observations of interactions of light stable and radioactive nuclei with an initial energy above 1 A GeV in nuclear emulsions. Thank to the best spatial resolution and the full solid angle acceptance provided by nuclear emulsions, such an approach allows one to obtain unique and evident observations reecting cluster-like features in light nuclear structures. New results on dissociation of $^7$Be in very peripheral interactions with emulsion nuclei are presented. The importance of this research for the physics of few body nuclear systems and the related problems of nucleosynthesis is noted. The paper is illustrated with characteristic images obtained by means of a microscope equipped with a CCD camera. The discussed explorations are provided with the beams of the Synchrophasotron and Nuclotron of JINR, Dubna. Future investigations are suggested to be carried out in relativistic beams of He, Be, B, C, and N isotopes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the research on light nucleus clustering in relativistic multifragmentation processes, as presented in the Arxiv paper?\n\nA) The researchers are studying the properties of heavy nuclei in high-energy collisions.\nB) The use of nuclear emulsions provides a unique opportunity to observe cluster-like features in light nuclear structures.\nC) The study of $^7$Be dissociation in peripheral interactions with emulsion nuclei is a key aspect of the research.\nD) The results of the study have significant implications for our understanding of nucleosynthesis in astrophysical contexts.\n\nCorrect Answer: B) The use of nuclear emulsions provides a unique opportunity to observe cluster-like features in light nuclear structures.\n\nExplanation: The documentation states that nuclear emulsions offer \"the best spatial resolution and the full solid angle acceptance\", which allows for unique observations of cluster-like features in light nuclear structures. This suggests that the use of nuclear emulsions is a key advantage of the research, making option B the correct answer. Options A, C, and D are incorrect because they either misrepresent the focus of the research or are too narrow in scope."}, "8": {"documentation": {"title": "BuildingNet: Learning to Label 3D Buildings", "source": "Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova,\n  Melinos Averkiou, Andreas Andreou, Siddhartha Chaudhuri, Evangelos\n  Kalogerakis", "docs_id": "2110.04955", "section": ["cs.CV", "cs.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BuildingNet: Learning to Label 3D Buildings. We introduce BuildingNet: (a) a large-scale dataset of 3D building models whose exteriors are consistently labeled, (b) a graph neural network that labels building meshes by analyzing spatial and structural relations of their geometric primitives. To create our dataset, we used crowdsourcing combined with expert guidance, resulting in 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles. We include a benchmark for evaluating mesh and point cloud labeling. Buildings have more challenging structural complexity compared to objects in existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that our dataset can nurture the development of algorithms that are able to cope with such large-scale geometric data for both vision and graphics tasks e.g., 3D semantic segmentation, part-based generative models, correspondences, texturing, and analysis of point cloud data acquired from real-world buildings. Finally, we show that our mesh-based graph neural network significantly improves performance over several baselines for labeling 3D meshes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the BuildingNet dataset, and how does it differ from existing benchmarks such as ShapeNet and PartNet?\n\n**A)** BuildingNet is a large-scale dataset of 3D building models, but it does not provide any new insights into building categorization. It is primarily used for evaluating mesh and point cloud labeling algorithms.\n\n**B)** BuildingNet is a graph neural network that labels building meshes, but it does not provide a benchmark for evaluating mesh and point cloud labeling. It is primarily used for training algorithms that can cope with large-scale geometric data.\n\n**C)** BuildingNet is a large-scale dataset of 3D building models whose exteriors are consistently labeled, and it provides a benchmark for evaluating mesh and point cloud labeling. It differs from existing benchmarks such as ShapeNet and PartNet in that it covers several building categories and has more challenging structural complexity.\n\n**D)** BuildingNet is a crowdsourced dataset of 3D building models, but it does not provide any new insights into building categorization or structural complexity. It is primarily used for training algorithms that can cope with large-scale geometric data.\n\n**Correct Answer:** C) BuildingNet is a large-scale dataset of 3D building models whose exteriors are consistently labeled, and it provides a benchmark for evaluating mesh and point cloud labeling. It differs from existing benchmarks such as ShapeNet and PartNet in that it covers several building categories and has more challenging structural complexity.\n\n**Explanation:** The correct answer is C) because it accurately summarizes the primary contribution of BuildingNet, which is to provide a large-scale dataset of 3D building models with consistently labeled exteriors and a benchmark for evaluating mesh and point cloud labeling. Additionally, it highlights the key difference between BuildingNet and existing benchmarks such as ShapeNet and PartNet, which is the inclusion of several building categories and more challenging structural complexity."}, "9": {"documentation": {"title": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II", "source": "Jiahao Chen and Nancy Makri", "docs_id": "0812.1552", "section": ["cond-mat.other", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical Evidence of Small Coherent Subsystems at Low Temperatures in\n  Light Harvesting Complex II. The extent of exciton coherence in protein-pigment complexes has significant implications for the initial light harvesting step in photosynthetic organisms. In this work we model the main antenna protein of photosystem II, namely light harvesting complex II (LHC-II), with a single-exciton Hamiltonian with sites coupled via dipole-dipole interaction, with linear coupling to a dissipative phonon bath. With appropriate parameters, Monte Carlo path integral (MCPI) results of the exciton coherence length from 1 K to 500 K show that at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites, with persistent short-range coherence over the A2-B2 pair, A3-B3 pair and B1-B5-B6 triplet. Quasi-adiabatic path integral (QUAPI) calculations of the subsystems mentioned above show a smooth, incoherent relaxation towards thermodynamic equilibrium. The results obtained imply that with the exception of small coherent subsystems at cryogenic temperatures, excitons in LHC-II are more localized than in the analogous light harvesting complex II (LH-II) of the purple bacterium Rs. molischianum, which may be expected from the lower symmetry of the former."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary implication of the study on the extent of exciton coherence in protein-pigment complexes, specifically in the context of photosynthetic organisms?\n\nA) The existence of small coherent subsystems at low temperatures has no significant impact on the initial light harvesting step.\nB) The localization of excitons in LHC-II is more extensive than in LH-II, which may be attributed to the lower symmetry of LHC-II.\nC) The study suggests that excitons in LHC-II are more localized than in LH-II, with persistent short-range coherence over specific pairs of chlorophyll pigment sites, and that this localization is a result of the specific dipole-dipole interaction and linear coupling to a dissipative phonon bath.\nD) The results obtained imply that the initial light harvesting step in photosynthetic organisms is more efficient due to the presence of small coherent subsystems at cryogenic temperatures.\n\n**Correct Answer:** C) The study suggests that excitons in LHC-II are more localized than in LH-II, with persistent short-range coherence over specific pairs of chlorophyll pigment sites, and that this localization is a result of the specific dipole-dipole interaction and linear coupling to a dissipative phonon bath.\n\n**Explanation:** The study models the main antenna protein of photosystem II, LHC-II, with a single-exciton Hamiltonian and Monte Carlo path integral (MCPI) results show that at thermodynamic equilibrium, an exciton in LHC-II is localized mostly on 2 single chlorophyll pigment sites, with persistent short-range coherence over the A2-B2 pair, A3-B3 pair, and B1-B5-B6 triplet. This suggests that the localization of excitons in LHC-II is more extensive than in LH-II, which may be attributed to the specific dipole-dipole interaction and linear coupling to a dissipative phonon bath."}, "10": {"documentation": {"title": "Energetics and Structural Properties of Trapped Two-Component Fermi\n  Gases", "source": "J. von Stecher, Chris H. Greene, D. Blume", "docs_id": "0801.2747", "section": ["cond-mat.other"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energetics and Structural Properties of Trapped Two-Component Fermi\n  Gases. Using two different numerical methods, we study the behavior of two-component Fermi gases interacting through short-range s-wave interactions in a harmonic trap. A correlated Gaussian basis-set expansion technique is used to determine the energies and structural properties, i.e., the radial one-body densities and pair distribution functions, for small systems with either even or odd $N$, as functions of the s-wave scattering length and the mass ratio $\\kappa$ of the two species. Particular emphasis is put on a discussion of the angular momentum of the system in the BEC-BCS crossover regime. At unitarity, the excitation spectrum of the four-particle system with total angular momentum L=0 is calculated as a function of the mass ratio $\\kappa$. The results are analyzed from a hyperspherical perspective, which offers new insights into the problem. Additionally, fixed-node diffusion Monte Carlo calculations are performed for equal-mass Fermi gases with up to N=30 atoms. We focus on the odd-even oscillations of the ground state energy of the equal-mass unitary system having up to N=30 particles, which are related to the excitation gap of the system. Furthermore, we present a detailed analysis of the structural properties of these systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary focus of the study on two-component Fermi gases interacting through short-range s-wave interactions in a harmonic trap, as described in the Arxiv documentation?\n\nA) To investigate the effects of the s-wave scattering length on the excitation spectrum of the system\nB) To analyze the angular momentum of the system in the BEC-BCS crossover regime from a hyperspherical perspective\nC) To calculate the energies and structural properties of small systems with even or odd N, as functions of the s-wave scattering length and the mass ratio \u03ba\nD) To study the ground state energy of equal-mass Fermi gases with up to N=30 atoms and its relation to the excitation gap of the system\n\n**Correct Answer:** C) To calculate the energies and structural properties of small systems with even or odd N, as functions of the s-wave scattering length and the mass ratio \u03ba\n\n**Explanation:** The correct answer is C) because the study focuses on calculating the energies and structural properties of small systems with even or odd N, as functions of the s-wave scattering length and the mass ratio \u03ba. This is evident from the introduction of the study, which states that the authors \"study the behavior of two-component Fermi gases interacting through short-range s-wave interactions in a harmonic trap\" and that they use a correlated Gaussian basis-set expansion technique to determine the energies and structural properties.\n\nThe other options are incorrect because:\n\nA) While the s-wave scattering length is an important parameter in the study, it is not the primary focus of the study.\n\nB) The BEC-BCS crossover regime is discussed in the study, but it is not the primary focus.\n\nD) The study does discuss the ground state energy of equal-mass Fermi gases with up to N=30 atoms, but this is not the primary focus of the study."}, "11": {"documentation": {"title": "Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions", "source": "Wen Yan and Robert Blackwell", "docs_id": "2010.15155", "section": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions. Many different simulation methods for Stokes flow problems involve a common computationally intense task -- the summation of a kernel function over $O(N^2)$ pairs of points. One popular technique is the Kernel Independent Fast Multipole Method (KIFMM), which constructs a spatial adaptive octree for all points and places a small number of equivalent multipole and local equivalent points around each octree box, and completes the kernel sum with $O(N)$ cost, using these equivalent points. Simpler kernels can be used between these equivalent points to improve the efficiency of KIFMM. Here we present further extensions and applications to this idea, to enable efficient summations and flexible boundary conditions for various kernels. We call our method the Kernel Aggregated Fast Multipole Method (KAFMM), because it uses different kernel functions at different stages of octree traversal. We have implemented our method as an open-source software library STKFMM based on the high performance library PVFMM, with support for Laplace kernels, the Stokeslet, regularized Stokeslet, Rotne-Prager-Yamakawa (RPY) tensor, and the Stokes double-layer and traction operators. Open and periodic boundary conditions are supported for all kernels, and the no-slip wall boundary condition is supported for the Stokeslet and RPY tensor. The package is designed to be ready-to-use as well as being readily extensible to additional kernels."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Kernel Aggregated Fast Multipole Method (KAFMM) over the Kernel Independent Fast Multipole Method (KIFMM) in terms of computational efficiency?\n\nA) KAFMM uses a more complex spatial adaptive octree structure, leading to increased computational overhead.\nB) KAFMM employs a smaller number of equivalent multipole and local equivalent points, resulting in a higher computational cost.\nC) KAFMM uses different kernel functions at different stages of octree traversal, allowing for more efficient summations and flexible boundary conditions.\nD) KAFMM is limited to only Laplace kernels, whereas KIFMM supports a wider range of kernels.\n\n**Correct Answer:** C) KAFMM uses different kernel functions at different stages of octree traversal, allowing for more efficient summations and flexible boundary conditions.\n\n**Explanation:** The correct answer is C) because the documentation states that KAFMM uses different kernel functions at different stages of octree traversal, which enables efficient summations and flexible boundary conditions. This is a key advantage of KAFMM over KIFMM, as it allows for more efficient computation and flexibility in handling different types of boundary conditions. The other options are incorrect because they either describe a disadvantage of KAFMM (A and B) or a limitation of KAFMM (D)."}, "12": {"documentation": {"title": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information", "source": "Farouq Abdulaziz Masoudy", "docs_id": "1801.06966", "section": ["q-fin.GN", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information. Since exchange economy considerably varies in the market assets, asset prices have become an attractive research area for investigating and modeling ambiguous and uncertain information in today markets. This paper proposes a new generative uncertainty mechanism based on the Bayesian Inference and Correntropy (BIC) technique for accurately evaluating asset pricing in markets. This technique examines the potential processes of risk, ambiguity, and variations of market information in a controllable manner. We apply the new BIC technique to a consumption asset-pricing model in which the consumption variations are modeled using the Bayesian network model with observing the dynamics of asset pricing phenomena in the data. These dynamics include the procyclical deviations of price, the countercyclical deviations of equity premia and equity volatility, the leverage impact and the mean reversion of excess returns. The key findings reveal that the precise modeling of asset information can estimate price changes in the market effectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the proposed generative uncertainty mechanism based on the Bayesian Inference and Correntropy (BIC) technique in the context of asset pricing under uncertainty and ambiguity of information?\n\n**A)** To estimate the mean reversion of excess returns in the market.\n**B)** To model the procyclical deviations of price and the countercyclical deviations of equity premia and equity volatility.\n**C)** To precisely model asset information and estimate price changes in the market effectively.\n**D)** To apply the Bayesian network model to consumption asset-pricing models.\n\n**Correct Answer:** C) To precisely model asset information and estimate price changes in the market effectively.\n\n**Explanation:** The correct answer is C) because the paper proposes a new generative uncertainty mechanism based on the BIC technique to accurately evaluate asset pricing in markets. The technique examines the potential processes of risk, ambiguity, and variations of market information in a controllable manner, and the key findings reveal that the precise modeling of asset information can estimate price changes in the market effectively. The other options are incorrect because they are specific aspects of the model or data, but not the primary objective of the proposed mechanism."}, "13": {"documentation": {"title": "High-Power and High-Capacity Mobile Optical SWIPT", "source": "Mingliang Xiong, Qingwen Liu, Shengli Zhou, Shun Han, and Mingqing Liu", "docs_id": "2107.09299", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-Power and High-Capacity Mobile Optical SWIPT. The increasing demands of power supply and data rate for mobile devices promote the research of simultaneous wireless information and power transfer (SWIPT). Optical SWIPT, as known as simultaneous light information and power transfer (SLIPT), has the potential for providing high-capacity communication and high-power wireless charging. However, SLIPT technologies based on light-emitting diodes have low efficiency due to energy dissipation over the air. Laser-based SLIPT technologies need strict positioning accuracy and scanning resolution, which may lead to the increase of costs and complexity. In this paper, we propose a mobile SLIPT scheme based on spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation. The power and data are transferred via separated frequencies, while they share the same self-aligned resonant beam path, without the needs of receiver positioning and beam steering. We establish the analysis model of the resonant beam power and its second harmonic power. Numerical results show that the proposed system can achieve watt-level battery charging power and above 10-bit/s/Hz achievable rate at 8-m distance, which satisfies the requirements of most indoor mobile devices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional laser-based SLIPT technologies, and how does the proposed mobile SLIPT scheme addressed this limitation?\n\nA) The primary limitation is the high cost of laser-based SLIPT technologies, which can be addressed by using a more efficient resonator design.\nB) The primary limitation is the need for strict positioning accuracy and scanning resolution, which can be addressed by using a spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation.\nC) The primary limitation is the low efficiency of traditional laser-based SLIPT technologies due to energy dissipation over the air, which can be addressed by using a more efficient resonator design.\nD) The primary limitation is the high power consumption of traditional laser-based SLIPT technologies, which can be addressed by using a more efficient power supply system.\n\n**Correct Answer:** B) The primary limitation is the need for strict positioning accuracy and scanning resolution, which can be addressed by using a spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation.\n\n**Explanation:** The correct answer is B) because the documentation states that traditional laser-based SLIPT technologies need strict positioning accuracy and scanning resolution, which may lead to the increase of costs and complexity. The proposed mobile SLIPT scheme addresses this limitation by using a spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation, which allows for power and data transfer without the need for receiver positioning and beam steering."}, "14": {"documentation": {"title": "Lovasz Convolutional Networks", "source": "Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth,\n  Arun Rajkumar, Partha Talukdar", "docs_id": "1805.11365", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lovasz Convolutional Networks. Semi-supervised learning on graph structured data has received significant attention with the recent introduction of Graph Convolution Networks (GCN). While traditional methods have focused on optimizing a loss augmented with Laplacian regularization framework, GCNs perform an implicit Laplacian type regularization to capture local graph structure. In this work, we propose Lovasz Convolutional Network (LCNs) which are capable of incorporating global graph properties. LCNs achieve this by utilizing Lovasz's orthonormal embeddings of the nodes. We analyse local and global properties of graphs and demonstrate settings where LCNs tend to work better than GCNs. We validate the proposed method on standard random graph models such as stochastic block models (SBM) and certain community structure based graphs where LCNs outperform GCNs and learn more intuitive embeddings. We also perform extensive binary and multi-class classification experiments on real world datasets to demonstrate LCN's effectiveness. In addition to simple graphs, we also demonstrate the use of LCNs on hyper-graphs by identifying settings where they are expected to work better than GCNs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of Lovasz Convolutional Networks (LCNs) over Graph Convolution Networks (GCNs) in terms of incorporating global graph properties?\n\nA) LCNs require more training data to achieve similar performance as GCNs.\nB) LCNs perform implicit Laplacian type regularization to capture local graph structure, whereas GCNs perform explicit Laplacian regularization.\nC) LCNs are more computationally expensive than GCNs due to the need for Lovasz's orthonormal embeddings.\nD) LCNs are only applicable to simple graphs, whereas GCNs can be used on more complex graph structures.\n\nCorrect Answer: B) LCNs perform implicit Laplacian type regularization to capture local graph structure, whereas GCNs perform explicit Laplacian regularization.\n\nExplanation: The correct answer is B because the documentation states that GCNs perform implicit Laplacian type regularization to capture local graph structure, whereas LCNs achieve this by utilizing Lovasz's orthonormal embeddings, which is an explicit regularization approach. This highlights the key difference between the two architectures and the primary advantage of LCNs in incorporating global graph properties."}, "15": {"documentation": {"title": "A hierarchy of Palm measures for determinantal point processes with\n  gamma kernels", "source": "Alexander I. Bufetov and Grigori Olshanski", "docs_id": "1904.13371", "section": ["math.PR", "math-ph", "math.CO", "math.FA", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A hierarchy of Palm measures for determinantal point processes with\n  gamma kernels. The gamma kernels are a family of projection kernels $K^{(z,z')}=K^{(z,z')}(x,y)$ on a doubly infinite $1$-dimensional lattice. They are expressed through Euler's gamma function and depend on two continuous parameters $z,z'$. The gamma kernels initially arose from a model of random partitions via a limit transition. On the other hand, these kernels are closely related to unitarizable representations of the Lie algebra $\\mathfrak{su}(1,1)$. Every gamma kernel $K^{(z,z')}$ serves as a correlation kernel for a determinantal measure $M^{(z,z')}$, which lives on the space of infinite point configurations on the lattice. We examine chains of kernels of the form $$ \\ldots, K^{(z-1,z'-1)}, \\; K^{(z,z')},\\; K^{(z+1,z'+1)}, \\ldots, $$ and establish the following hierarchical relations inside any such chain: Given $(z,z')$, the kernel $K^{(z,z')}$ is a one-dimensional perturbation of (a twisting of) the kernel $K^{(z+1,z'+1)}$, and the one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to $M^{(z+1,z'+1)}$. We also explicitly compute the corresponding Radon-Nikod\\'ym derivatives and show that they are given by certain normalized multiplicative functionals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a chain of gamma kernels of the form $$ \\ldots, K^{(z-1,z'-1)}, \\; K^{(z,z')},\\; K^{(z+1,z'+1)}, \\ldots $$ where each kernel $K^{(z,z')}$ is a one-dimensional perturbation of (a twisting of) the kernel $K^{(z+1,z'+1)}$. Let $M^{(z,z')}$ be the determinantal measure corresponding to the kernel $K^{(z,z')}$.\n\nA) The one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to the measure $M^{(z+1,z'+1)}$ if and only if the Radon-Nikod\\'ym derivative $\\frac{dM^{(z,z')}}{dM^{(z+1,z'+1)}}$ is a normalized multiplicative functional.\n\nB) The chain of gamma kernels can be viewed as a sequence of unitarizable representations of the Lie algebra $\\mathfrak{su}(1,1)$, where each representation is a twisting of the previous one.\n\nC) The gamma kernels $K^{(z,z')}$ can be expressed as a product of two gamma functions, $\\Gamma(z)$ and $\\Gamma(z')$, where $z$ and $z'$ are continuous parameters.\n\nD) The determinantal measure $M^{(z,z')}$ can be represented as a limit of a sequence of measures $M^{(z_n,z_n')}$, where $z_n$ and $z_n'$ are discrete parameters that converge to $z$ and $z'$, respectively.\n\nCorrect Answer: A) The one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to the measure $M^{(z+1,z'+1)}$ if and only if the Radon-Nikod\\'ym derivative $\\frac{dM^{(z,z')}}{dM^{(z+1,z'+1)}}$ is a normalized multiplicative functional.\n\nExplanation: The correct answer follows from the hierarchical relations established in the documentation, which state that the kernel $K^{(z,z')}$ is a one-dimensional perturbation of (a twisting of) the kernel $K^{(z+1,z'+1)}$. This implies that the one-point Palm distributions for the measure $M^{(z,z')}$ are absolutely continuous with respect to the measure $M^{(z+1,z'+1)}$, and the Radon-Nikod\\'ym derivative $\\frac{dM^{(z,z')}}{dM^{(z+1,z'+1)}}$ is a normalized multiplicative functional."}, "16": {"documentation": {"title": "Single and multiple index functional regression models with\n  nonparametric link", "source": "Dong Chen, Peter Hall, Hans-Georg M\\\"uller", "docs_id": "1211.5018", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single and multiple index functional regression models with\n  nonparametric link. Fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. This difficulty has led to an emphasis on the so-called functional linear model, which is much more flexible than common linear models in finite dimension, but nevertheless imposes structural constraints on the relationship between predictors and responses. Recent advances have extended the linear approach by using it in conjunction with link functions, and by considering multiple indices, but the flexibility of this technique is still limited. For example, the link may be modeled parametrically or on a grid only, or may be constrained by an assumption such as monotonicity; multiple indices have been modeled by making finite-dimensional assumptions. In this paper we introduce a new technique for estimating the link function nonparametrically, and we suggest an approach to multi-index modeling using adaptively defined linear projections of functional data. We show that our methods enable prediction with polynomial convergence rates. The finite sample performance of our methods is studied in simulations, and is illustrated by an application to a functional regression problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of fully nonparametric methods for regression from functional data, and how does this limitation impact the accuracy of these methods?\n\n**A)** Fully nonparametric methods are limited by their inability to capture complex relationships between predictors and responses, leading to slower convergence rates and reduced accuracy. However, this limitation can be mitigated by using adaptive linear projections of functional data.\n\n**B)** Fully nonparametric methods are limited by their inability to model non-linear relationships between predictors and responses, leading to slower convergence rates and reduced accuracy. However, this limitation can be addressed by using parametric link functions.\n\n**C)** Fully nonparametric methods are limited by their inability to capture high-dimensional functions, leading to slower convergence rates and reduced accuracy. However, this limitation can be mitigated by using finite-dimensional assumptions.\n\n**D)** Fully nonparametric methods are limited by their inability to model multiple indices, leading to slower convergence rates and reduced accuracy. However, this limitation can be addressed by using adaptive linear projections of functional data.\n\n**Correct Answer:** A) Fully nonparametric methods are limited by their inability to capture complex relationships between predictors and responses, leading to slower convergence rates and reduced accuracy. However, this limitation can be mitigated by using adaptive linear projections of functional data.\n\n**Explanation:** The correct answer is A) because the documentation states that fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. This limitation is due to the inability of these methods to capture complex relationships between predictors and responses. The correct answer also mentions that this limitation can be mitigated by using adaptive linear projections of functional data, which is a key aspect of the new technique introduced in the paper."}, "17": {"documentation": {"title": "Information Design in Optimal Auctions", "source": "Yi-Chun, Chen and Xiangqian, Yang", "docs_id": "2010.08990", "section": ["econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information Design in Optimal Auctions. We study the information design problem in a single-unit auction setting. The information designer controls independent private signals according to which the buyers infer their binary private values. Assuming that the seller adopts the optimal auction due to Myerson (1981) in response, we characterize both the buyer-optimal information structure, which maximizes the buyers' surplus, and the sellerworst information structure, which minimizes the seller's revenue. We translate both information design problems into finite-dimensional, constrained optimization problems in which one can explicitly solve for the optimal information structures. In contrast to the case with one buyer (Roesler and Szentes, 2017 and Du, 2018), we show that with two or more buyers, the symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure. The good is always sold under the seller-worst information structure but not under the buyer-optimal information structure. Nevertheless, as the number of buyers goes to infinity, both symmetric information structures converge to no disclosure. We also show that in an ex ante symmetric setting, an asymmetric information structure is never seller-worst but can generate a strictly higher surplus for the buyers than the symmetric buyer-optimal information structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of optimal auctions, what is the relationship between the symmetric buyer-optimal information structure and the symmetric seller-worst information structure when there are two or more buyers?\n\n**A)** The symmetric buyer-optimal information structure is identical to the symmetric seller-worst information structure.\n**B)** The symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure, and the good is always sold under the seller-worst information structure but not under the buyer-optimal information structure.\n**C)** The symmetric buyer-optimal information structure is identical to the symmetric seller-worst information structure, and the good is always sold under the buyer-optimal information structure but not under the seller-worst information structure.\n**D)** The symmetric buyer-optimal information structure and the symmetric seller-worst information structure are asymptotically equivalent as the number of buyers goes to infinity.\n\n**Correct Answer:** B) The symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure, and the good is always sold under the seller-worst information structure but not under the buyer-optimal information structure.\n\n**Explanation:** According to the documentation, when there are two or more buyers, the symmetric buyer-optimal information structure is different from the symmetric seller-worst information structure. Additionally, the good is always sold under the seller-worst information structure but not under the buyer-optimal information structure. This is because the seller-worst information structure is designed to minimize the seller's revenue, while the buyer-optimal information structure is designed to maximize the buyers' surplus."}, "18": {"documentation": {"title": "Statistical Decision Properties of Imprecise Trials Assessing COVID-19\n  Drugs", "source": "Charles F. Manski and Aleksey Tetenov", "docs_id": "2006.00343", "section": ["econ.EM", "q-bio.QM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Decision Properties of Imprecise Trials Assessing COVID-19\n  Drugs. As the COVID-19 pandemic progresses, researchers are reporting findings of randomized trials comparing standard care with care augmented by experimental drugs. The trials have small sample sizes, so estimates of treatment effects are imprecise. Seeing imprecision, clinicians reading research articles may find it difficult to decide when to treat patients with experimental drugs. Whatever decision criterion one uses, there is always some probability that random variation in trial outcomes will lead to prescribing sub-optimal treatments. A conventional practice when comparing standard care and an innovation is to choose the innovation only if the estimated treatment effect is positive and statistically significant. This practice defers to standard care as the status quo. To evaluate decision criteria, we use the concept of near-optimality, which jointly considers the probability and magnitude of decision errors. An appealing decision criterion from this perspective is the empirical success rule, which chooses the treatment with the highest observed average patient outcome in the trial. Considering the design of recent and ongoing COVID-19 trials, we show that the empirical success rule yields treatment results that are much closer to optimal than those generated by prevailing decision criteria based on hypothesis tests."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A researcher is evaluating the effectiveness of a new treatment for COVID-19 based on the results of a randomized trial. The trial has a small sample size, and the estimated treatment effect is imprecise. Which of the following decision criteria would be most appealing to the researcher, considering the concept of near-optimality, and why?\n\nA) The empirical success rule, which chooses the treatment with the highest observed average patient outcome in the trial.\nB) The conventional practice of choosing the treatment with the highest statistically significant effect.\nC) The decision criterion that chooses the treatment with the lowest probability of decision error.\nD) The decision criterion that chooses the treatment with the highest probability of optimal treatment.\n\n**Correct Answer:** A) The empirical success rule, which chooses the treatment with the highest observed average patient outcome in the trial.\n\n**Explanation:** The empirical success rule is an appealing decision criterion because it jointly considers the probability and magnitude of decision errors. In the context of the COVID-19 pandemic, where treatment effects are imprecise, the empirical success rule can provide a more nuanced evaluation of treatment effectiveness. By choosing the treatment with the highest observed average patient outcome, the researcher can balance the need for precision with the need for practical decision-making. This approach is particularly relevant in the context of small sample sizes, where conventional statistical tests may not provide reliable estimates of treatment effects."}, "19": {"documentation": {"title": "A Contextual Risk Model for the Ellsberg Paradox", "source": "Diederik Aerts and Sandro Sozzo", "docs_id": "1105.1814", "section": ["physics.soc-ph", "q-fin.GN", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Contextual Risk Model for the Ellsberg Paradox. The Allais and Ellsberg paradoxes show that the expected utility hypothesis and Savage's Sure-Thing Principle are violated in real life decisions. The popular explanation in terms of 'ambiguity aversion' is not completely accepted. On the other hand, we have recently introduced a notion of 'contextual risk' to mathematically capture what is known as 'ambiguity' in the economics literature. Situations in which contextual risk occurs cannot be modeled by Kolmogorovian classical probabilistic structures, but a non-Kolmogorovian framework with a quantum-like structure is needed. We prove in this paper that the contextual risk approach can be applied to the Ellsberg paradox, and elaborate a 'sphere model' within our 'hidden measurement formalism' which reveals that it is the overall conceptual landscape that is responsible of the disagreement between actual human decisions and the predictions of expected utility theory, which generates the paradox. This result points to the presence of a 'quantum conceptual layer' in human thought which is superposed to the usually assumed 'classical logical layer'."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Ellsberg paradox, what is the primary limitation of the expected utility hypothesis and Savage's Sure-Thing Principle, and how does the contextual risk approach address this limitation?\n\n**A)** The expected utility hypothesis and Savage's Sure-Thing Principle are limited by the assumption of classical probabilistic structures, which fail to capture the complexities of human decision-making under ambiguity.\n\n**B)** The expected utility hypothesis and Savage's Sure-Thing Principle are limited by the assumption of a fixed and known probability distribution, which is incompatible with the presence of contextual risk in real-life decisions.\n\n**C)** The expected utility hypothesis and Savage's Sure-Thing Principle are limited by the failure to account for the role of cognitive biases and heuristics in human decision-making, which are not captured by classical probabilistic structures.\n\n**D)** The expected utility hypothesis and Savage's Sure-Thing Principle are limited by the assumption of a single, objective probability distribution, which is incompatible with the presence of multiple, subjective probability distributions in human thought.\n\n**Correct Answer:** B) The expected utility hypothesis and Savage's Sure-Thing Principle are limited by the assumption of a fixed and known probability distribution, which is incompatible with the presence of contextual risk in real-life decisions.\n\n**Explanation:** The correct answer is B) because the Ellsberg paradox highlights the failure of expected utility theory to predict human decisions under ambiguity, which is precisely the situation where contextual risk occurs. The contextual risk approach, as described in the paper, recognizes that situations involving ambiguity cannot be modeled by classical probabilistic structures, but rather require a non-Kolmogorovian framework with a quantum-like structure. This is precisely what option B states, highlighting the limitation of expected utility theory in capturing the complexities of human decision-making under ambiguity."}, "20": {"documentation": {"title": "Theory of gravitation theories: a no-progress report", "source": "Thomas P Sotiriou, Valerio Faraoni, Stefano Liberati", "docs_id": "0707.2748", "section": ["gr-qc", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theory of gravitation theories: a no-progress report. Already in the 1970s there where attempts to present a set of ground rules, sometimes referred to as a theory of gravitation theories, which theories of gravity should satisfy in order to be considered viable in principle and, therefore, interesting enough to deserve further investigation. From this perspective, an alternative title of the present paper could be ``why are we still unable to write a guide on how to propose viable alternatives to general relativity?''. Attempting to answer this question, it is argued here that earlier efforts to turn qualitative statements, such as the Einstein Equivalence Principle, into quantitative ones, such as the metric postulates, stand on rather shaky grounds -- probably contrary to popular belief -- as they appear to depend strongly on particular representations of the theory. This includes ambiguities in the identification of matter and gravitational fields, dependence of frequently used definitions, such as those of the stress-energy tensor or classical vacuum, on the choice of variables, etc. Various examples are discussed and possible approaches to this problem are pointed out. In the course of this study, several common misconceptions related to the various forms of the Equivalence Principle, the use of conformal frames and equivalence between theories are clarified."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary criticism of earlier attempts to turn qualitative statements, such as the Einstein Equivalence Principle, into quantitative ones, such as the metric postulates, and how does this criticism impact our understanding of viable alternatives to general relativity?\n\nA) These attempts were successful in providing a clear and concise framework for testing the Equivalence Principle, but were hindered by the complexity of the mathematical formulations. \nB) These attempts were unsuccessful due to the strong dependence on particular representations of the theory, which led to ambiguities in the identification of matter and gravitational fields. \nC) These attempts were successful, but were limited by the lack of a clear and consistent definition of the stress-energy tensor and classical vacuum. \nD) These attempts were unnecessary, as the Equivalence Principle is a fundamental concept that does not require quantitative formulation.\n\nCorrect Answer: B) These attempts were unsuccessful due to the strong dependence on particular representations of the theory, which led to ambiguities in the identification of matter and gravitational fields.\n\nExplanation: The correct answer, B, is supported by the text, which states that earlier efforts to turn qualitative statements into quantitative ones \"stand on rather shaky grounds\" and that these attempts \"appear to depend strongly on particular representations of the theory\". This suggests that the primary criticism is that these attempts are not robust or independent of the specific representation used. The other options are incorrect because they either downplay the significance of the criticism (A), introduce a new issue (C), or imply that the criticism is unnecessary (D)."}, "21": {"documentation": {"title": "Orbital eccentricity of WASP-12 and WASP-14 from new radial-velocity\n  monitoring with SOPHIE", "source": "Nawal Husnoo, Frederic Pont, Guillaume Hebrard, Elaine Simpson, Tsevi\n  Mazeh, Francois Bouchy, Claire Moutou, Luc Arnold, Isabelle Boisse, Rodrigo\n  Diaz, Anne Eggenberger and Avi Shporer", "docs_id": "1004.1809", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Orbital eccentricity of WASP-12 and WASP-14 from new radial-velocity\n  monitoring with SOPHIE. As part of the long-term radial velocity monitoring of known transiting planets -- designed to measure orbital eccentricities, spin-orbit alignments and further planetary companions -- we have acquired radial velocity data for the two transiting systems WASP-12 and WASP-14, each harbouring gas giants on close orbits (orbital period of 1.09 and 2.24 days respectively). In both cases, the initial orbital solution suggested a significant orbital eccentricity, 0.049+-0.015 for WASP-12 and 0.091+-0.003 for WASP-14. Since then, measurements of the secondary eclipse of WASP-12 in the infrared have indicated that one projection of the eccentricity (e cos w) was very close to zero, casting doubt on the eccentricity from the initial radial velocity orbit. Our measurements confirm that the initial eccentricity detection could be spurious, and show that the radial velocity data is compatible with a circular orbit. A MCMC analysis taking into account the presence of correlated systematic noise in both the radial velocity and photometric data gives e=0.017 (+0.015-0.011). By contrast, we confirm the orbital eccentricity of WASP-14, and refine its value to e=0.088+-0.003. WASP-14 is thus the closest presently known planet with a confirmed eccentric orbit."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the most likely explanation for the discrepancy between the initial orbital eccentricity detection and the confirmed eccentricity of WASP-12, and how does this affect our understanding of the planet's orbit?\n\nA) The initial eccentricity detection was due to a systematic error in the radial velocity measurements, which was later corrected by the MCMC analysis.\nB) The discrepancy is due to the presence of correlated systematic noise in the photometric data, which affected the initial eccentricity detection but not the confirmed eccentricity.\nC) The initial eccentricity detection was spurious, and the confirmed eccentricity is consistent with a circular orbit, but the MCMC analysis did not account for the effects of the planet's spin-orbit alignment.\nD) The discrepancy is due to the fact that the initial eccentricity detection was based on a limited dataset, while the confirmed eccentricity is based on a more comprehensive dataset.\n\nCorrect Answer: B) The discrepancy is due to the presence of correlated systematic noise in the photometric data, which affected the initial eccentricity detection but not the confirmed eccentricity.\n\nExplanation: The correct answer is B) because the text states that the MCMC analysis \"taking into account the presence of correlated systematic noise in both the radial velocity and photometric data\" gives a confirmed eccentricity of e=0.017, which is consistent with a circular orbit. This suggests that the discrepancy between the initial and confirmed eccentricities is due to the presence of correlated systematic noise in the photometric data, which affected the initial eccentricity detection but not the confirmed eccentricity."}, "22": {"documentation": {"title": "Temporal Markov Processes for Transport in Porous Media: Random Lattice\n  Networks", "source": "Amir H. Delgoshaie, Patrick Jenny, Hamdi A. Tchelepi", "docs_id": "1708.04173", "section": ["physics.comp-ph", "cs.CE", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal Markov Processes for Transport in Porous Media: Random Lattice\n  Networks. Monte Carlo (MC) simulations of transport in random porous networks indicate that for high variances of the log-normal permeability distribution, the transport of a passive tracer is non-Fickian. Here we model this non-Fickian dispersion in random porous networks using discrete temporal Markov models. We show that such temporal models capture the spreading behavior accurately. This is true despite the fact that the slow velocities are strongly correlated in time, and some studies have suggested that the persistence of low velocities would render the temporal Markovian model inapplicable. Compared to previously proposed temporal stochastic differential equations with case specific drift and diffusion terms, the models presented here require fewer modeling assumptions. Moreover, we show that discrete temporal Markov models can be used to represent dispersion in unstructured networks, which are widely used to model porous media. A new method is proposed to extend the state space of temporal Markov models to improve the model predictions in the presence of extremely low velocities in particle trajectories and extend the applicability of the model to higher temporal resolutions. Finally, it is shown that by combining multiple transitions, temporal models are more efficient for computing particle evolution compared to correlated CTRW with spatial increments that are equal to the lengths of the links in the network."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using discrete temporal Markov models to model non-Fickian dispersion in random porous networks, as proposed in the paper \"Temporal Markov Processes for Transport in Porous Media: Random Lattice Networks\"?\n\n**A)** They require fewer modeling assumptions compared to temporal stochastic differential equations.\n**B)** They can accurately capture the spreading behavior of a passive tracer in the presence of strongly correlated slow velocities.\n**C)** They can be used to represent dispersion in unstructured networks, which are widely used to model porous media.\n**D)** They are more efficient for computing particle evolution compared to correlated Continuous-Time Random Walks (CTRW) with spatial increments equal to the lengths of the links in the network.\n\n**Correct Answer:** C) They can be used to represent dispersion in unstructured networks, which are widely used to model porous media.\n\n**Explanation:** The paper highlights the ability of discrete temporal Markov models to represent dispersion in unstructured networks, which are commonly used to model porous media. This is a key advantage of the proposed model, as it allows for a more realistic representation of complex porous media systems. While the other options are also mentioned in the paper, they are not the primary advantage of the discrete temporal Markov model. Option A is a benefit, but not the primary one. Option B is a characteristic of the model, but not its primary advantage. Option D is a related concept, but not the main benefit of the model."}, "23": {"documentation": {"title": "Dynamics of domain-wall Dirac fermions on a topological insulator: a\n  chiral fermion beam splitter", "source": "Ren\\'e Hammer and Walter P\\\"otz", "docs_id": "1306.6139", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of domain-wall Dirac fermions on a topological insulator: a\n  chiral fermion beam splitter. The intersection of two ferromagnetic domain walls placed on the surface of topological insulators provides a one-way beam splitter for domain-wall Dirac fermions. Based on an analytic expression for a static two-soliton magnetic texture we perform a systematic numerical study of the propagation of Dirac wave packets along such intersections. A single-cone staggered-grid finite difference lattice scheme is employed in the numerical analysis. It is shown that the angle of intersection plays a decisive role in determining the splitting ratio of the fermion beam. For a non-rectangular intersection, the width and, to a lesser extent, the type of domain walls, e.g. Bloch or N{\\'e}el, determine the properties of the splitter. As the ratio between domain-wall width and transverse localization length of the Dirac fermion is increased its propagation behavior changes from quantum-mechanical (wave-like) to classical ballistic (particle-like). An electric gate placed near the intersection offers a dynamic external control knob for adjusting the splitting ratio."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the role of the angle of intersection in determining the splitting ratio of the fermion beam in the context of the intersection of two ferromagnetic domain walls on a topological insulator. How does the width and type of domain walls affect the properties of the splitter?\n\n**A)** The angle of intersection plays a minor role in determining the splitting ratio, and the width and type of domain walls have a negligible impact on the splitter's properties.\n\n**B)** The angle of intersection is a decisive factor in determining the splitting ratio, and the width and type of domain walls have a significant impact on the splitter's properties, with non-rectangular intersections leading to changes in the splitter's behavior.\n\n**C)** The angle of intersection has no effect on the splitting ratio, and the width and type of domain walls are irrelevant to the splitter's properties.\n\n**D)** The angle of intersection is a secondary factor in determining the splitting ratio, and the width and type of domain walls have a significant impact on the splitter's properties, but only in the case of rectangular intersections.\n\n**Correct Answer:** B) The angle of intersection is a decisive factor in determining the splitting ratio, and the width and type of domain walls have a significant impact on the splitter's properties, with non-rectangular intersections leading to changes in the splitter's behavior.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the angle of intersection plays a decisive role in determining the splitting ratio of the fermion beam.\" Additionally, the documentation states that \"for a non-rectangular intersection, the width and, to a lesser extent, the type of domain walls, e.g. Bloch or N{\\'e}el, determine the properties of the splitter.\" This suggests that the angle of intersection is a primary factor, and the width and type of domain walls have a significant impact on the splitter's properties, particularly in non-rectangular intersections."}, "24": {"documentation": {"title": "Robust transformations of firing patterns for neural networks", "source": "Karlis Kanders, Tom Lorimer, Yoko Uwate, Willi-Hans Steeb and Ruedi\n  Stoop", "docs_id": "1708.04168", "section": ["q-bio.NC", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust transformations of firing patterns for neural networks. As a promising computational paradigm, occurrence of critical states in artificial and biological neural networks has attracted wide-spread attention. An often-made explicit or implicit assumption is that one single critical state is responsible for two separate notions of criticality (avalanche criticality and dynamical edge of chaos criticality). Previously, we provided an isolated counter-example for co-occurrence. Here, we reveal a persistent paradigm of structural transitions that such networks undergo, as the overall connectivity strength is varied over its biologically meaningful range. Among these transitions, only one avalanche critical point emerges, with edge of chaos failing to co-occur. Our observations are based on ensembles of networks obtained from variations of network configuration and their neurons. This suggests that not only non-coincidence of criticality, but also the persistent paradigm of network structural changes in function of the overall connectivity strength, could be generic features of a large class of biological neural networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main finding of the study on \"Robust transformations of firing patterns for neural networks\" regarding the co-occurrence of critical states in artificial and biological neural networks?\n\n**A)** The study reveals that both avalanche criticality and dynamical edge of chaos criticality co-occur in a significant proportion of networks.\n**B)** The study demonstrates that only one type of critical state, avalanche criticality, is responsible for the emergence of both avalanche criticality and dynamical edge of chaos criticality.\n**C)** The study shows that the co-occurrence of critical states in artificial and biological neural networks is a rare event, and that most networks exhibit only one type of criticality.\n**D)** The study reveals that the overall connectivity strength of a network has no effect on the co-occurrence of critical states in artificial and biological neural networks.\n\n**Correct Answer:** C) The study shows that the co-occurrence of critical states in artificial and biological neural networks is a rare event, and that most networks exhibit only one type of criticality.\n\n**Explanation:** The study reveals that the co-occurrence of critical states in artificial and biological neural networks is a rare event, and that most networks exhibit only one type of criticality. This is evident from the fact that the study provides a counter-example for co-occurrence, and that only one avalanche critical point emerges, while the edge of chaos fails to co-occur. This suggests that the co-occurrence of critical states is not a generic feature of biological neural networks, but rather a rare event."}, "25": {"documentation": {"title": "Dynamically generated inflationary two-field potential via\n  non-Riemannian volume forms", "source": "David Benisty, Eduardo I. Guendelman, Emil Nissimov, Svetlana Pacheva", "docs_id": "1907.07625", "section": ["astro-ph.CO", "gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamically generated inflationary two-field potential via\n  non-Riemannian volume forms. We consider a simple model of modified gravity interacting with a single scalar field $\\varphi$ with weakly coupled exponential potential within the framework of non-Riemannian spacetime volume-form formalism. The specific form of the action is fixed by the requirement of invariance under global Weyl-scale symmetry. Upon passing to the physical Einstein frame we show how the non-Riemannian volume elements create a second canonical scalar field $u$ and dynamically generate a non-trivial two-scalar-field potential $U_{\\rm eff}(u,\\varphi)$ with two remarkable features: (i) it possesses a large flat region for large $u$ describing a slow-roll inflation; (ii) it has a stable low-lying minimum w.r.t. $(u,\\varphi)$ representing the dark energy density in the \"late universe\". We study the corresponding two-field slow-roll inflation and show that the pertinent slow-roll inflationary curve $\\varphi = \\varphi(u)$ in the two-field space $(u,\\varphi)$ has a very small curvature, i.e., $\\varphi$ changes very little during the inflationary evolution of $u$ on the flat region of $U_{\\rm eff}(u,\\varphi)$. Explicit expressions are found for the slow-roll parameters which differ from those in the single-field inflationary counterpart. Numerical solutions for the scalar spectral index and the tensor-to-scalar ratio are derived agreeing with the observational data."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of the Dynamically generated inflationary two-field potential via non-Riemannian volume forms, what is the relationship between the effective two-scalar-field potential $U_{\\rm eff}(u,\\varphi)$ and the physical Einstein frame, and how does it lead to a stable low-lying minimum representing the dark energy density in the \"late universe\"?\n\nA) The effective potential $U_{\\rm eff}(u,\\varphi)$ is invariant under the Weyl-scale symmetry in the non-Riemannian spacetime volume-form formalism, and its stable minimum corresponds to the dark energy density in the physical Einstein frame.\n\nB) The effective potential $U_{\\rm eff}(u,\\varphi)$ is dynamically generated through the non-Riemannian volume forms, and its stable minimum represents the dark energy density in the physical Einstein frame, which is invariant under the Weyl-scale symmetry.\n\nC) The effective potential $U_{\\rm eff}(u,\\varphi)$ is a function of the physical Einstein frame, and its stable minimum corresponds to the dark energy density in the physical Einstein frame, which is invariant under the Weyl-scale symmetry.\n\nD) The effective potential $U_{\\rm eff}(u,\\varphi)$ is invariant under the Weyl-scale symmetry in the non-Riemannian spacetime volume-form formalism, and its stable minimum represents the dark energy density in the physical Einstein frame, which is dynamically generated through the non-Riemannian volume forms.\n\nCorrect Answer: B) The effective potential $U_{\\rm eff}(u,\\varphi)$ is dynamically generated through the non-Riemannian volume forms, and its stable minimum represents the dark energy density in the physical Einstein frame, which is invariant under the Weyl-scale symmetry.\n\nExplanation: The correct answer is B) because the effective potential $U_{\\rm eff}(u,\\varphi)$ is dynamically generated through the non-Riemannian volume forms, and its stable minimum represents the dark energy density in the physical Einstein frame, which is invariant under the Weyl-scale symmetry. This is stated in the original Arxiv documentation: \"Upon passing to the physical Einstein frame we show how the non-Riemannian volume elements create a second canonical scalar field $u$ and dynamically generate a non-trivial two-scalar-field potential $U_{\\rm eff}(u,\\varphi)$...\""}, "26": {"documentation": {"title": "Active Sensing for Search and Tracking: A Review", "source": "Luca Varotto, Angelo Cenedese, and Andrea Cavallaro", "docs_id": "2112.02381", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active Sensing for Search and Tracking: A Review. Active Position Estimation (APE) is the task of localizing one or more targets using one or more sensing platforms. APE is a key task for search and rescue missions, wildlife monitoring, source term estimation, and collaborative mobile robotics. Success in APE depends on the level of cooperation of the sensing platforms, their number, their degrees of freedom and the quality of the information gathered. APE control laws enable active sensing by satisfying either pure-exploitative or pure-explorative criteria. The former minimizes the uncertainty on position estimation; whereas the latter drives the platform closer to its task completion. In this paper, we define the main elements of APE to systematically classify and critically discuss the state of the art in this domain. We also propose a reference framework as a formalism to classify APE-related solutions. Overall, this survey explores the principal challenges and envisages the main research directions in the field of autonomous perception systems for localization tasks. It is also beneficial to promote the development of robust active sensing methods for search and tracking applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of Active Position Estimation (APE) control laws in active sensing, and how do they differ from each other?\n\n**A)** APE control laws aim to minimize the uncertainty on position estimation and drive the platform closer to its task completion, which is a trade-off between exploiting known information and exploring unknown regions.\n\n**B)** APE control laws are designed to minimize the uncertainty on position estimation, whereas the platform's movement is driven solely by the need to complete the task, without considering the level of uncertainty.\n\n**C)** APE control laws are used to drive the platform closer to its task completion, whereas the primary objective is to minimize the uncertainty on position estimation, which is a secondary consideration.\n\n**D)** APE control laws are a type of machine learning algorithm used to optimize the platform's movement, but they do not have a primary objective of minimizing uncertainty on position estimation.\n\n**Correct Answer:** A) APE control laws aim to minimize the uncertainty on position estimation and drive the platform closer to its task completion, which is a trade-off between exploiting known information and exploring unknown regions.\n\n**Explanation:** The correct answer is A) because APE control laws are designed to balance two competing objectives: minimizing the uncertainty on position estimation (exploitative criterion) and driving the platform closer to its task completion (exploratory criterion). The correct answer requires an understanding of the trade-offs involved in APE control laws and the ability to distinguish between the two objectives."}, "27": {"documentation": {"title": "Classical versus quantum views of intense laser pulse propagation in\n  gases", "source": "S.A. Berman, C. Chandre, J. Dubois, M. Perin, and T. Uzer", "docs_id": "1901.06354", "section": ["physics.atom-ph", "nlin.CD", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Classical versus quantum views of intense laser pulse propagation in\n  gases. We study the behavior of reduced models for the propagation of intense laser pulses in atomic gases. The models we consider incorporate ionization, blueshifting, and other nonlinear propagation effects in an ab initio manner, by explicitly taking into account the microscopic electron dynamics. Numerical simulations of the propagation of ultrashort linearly-polarized and elliptically-polarized laser pulses over experimentally-relevant propagation distances are presented. We compare the behavior of models where the electrons are treated classically with those where they are treated quantum-mechanically. A classical equivalent to the ground state is found, which maximizes the agreement between the quantum and classical predictions of the single-atom ionization probability as a function of laser intensity. We show that this translates into quantitative agreement between the quantum and classical models for the laser field evolution during propagation through gases of ground-state atoms. This agreement is exploited to provide a classical perspective on low- and high-order harmonic generation in linearly-polarized fields. In addition, we demonstrate the stability of the polarization of a nearly-linearly-polarized pulse using a two-dimensional model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary difference between the classical and quantum views of intense laser pulse propagation in atomic gases, as studied in the given Arxiv documentation?\n\nA) The classical model assumes a fixed electron position, while the quantum model assumes a fixed electron momentum.\nB) The classical model incorporates ionization and blueshifting effects, while the quantum model does not.\nC) The classical model treats electrons as particles, while the quantum model treats electrons as waves.\nD) The classical model uses a reduced model that explicitly takes into account the microscopic electron dynamics, similar to the quantum model.\n\n**Correct Answer:** D) The classical model uses a reduced model that explicitly takes into account the microscopic electron dynamics, similar to the quantum model.\n\n**Explanation:** The correct answer is D) because the documentation states that both the classical and quantum models incorporate ionization, blueshifting, and other nonlinear propagation effects in an ab initio manner, by explicitly taking into account the microscopic electron dynamics. This suggests that the primary difference between the two models is not in the inclusion of certain effects, but rather in the underlying assumptions about the behavior of electrons. The classical model assumes a fixed electron position, while the quantum model assumes a wave function that describes the electron's probability distribution. However, the documentation does not explicitly state this difference, making option D the correct answer."}, "28": {"documentation": {"title": "POD-Galerkin reduced order models and physics-informed neural networks\n  for solving inverse problems for the Navier-Stokes equations", "source": "Saddam Hijazi, Melina Freitag and Niels Landwehr", "docs_id": "2112.11950", "section": ["physics.flu-dyn", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "POD-Galerkin reduced order models and physics-informed neural networks\n  for solving inverse problems for the Navier-Stokes equations. We present a Reduced Order Model (ROM) which exploits recent developments in Physics Informed Neural Networks (PINNs) for solving inverse problems for the Navier--Stokes equations (NSE). In the proposed approach, the presence of simulated data for the fluid dynamics fields is assumed. A POD-Galerkin ROM is then constructed by applying POD on the snapshots matrices of the fluid fields and performing a Galerkin projection of the NSE (or the modified equations in case of turbulence modeling) onto the POD reduced basis. A $\\textit{POD-Galerkin PINN ROM}$ is then derived by introducing deep neural networks which approximate the reduced outputs with the input being time and/or parameters of the model. The neural networks incorporate the physical equations (the POD-Galerkin reduced equations) into their structure as part of the loss function. Using this approach, the reduced model is able to approximate unknown parameters such as physical constants or the boundary conditions. A demonstration of the applicability of the proposed ROM is illustrated by two cases which are the steady flow around a backward step and the unsteady turbulent flow around a surface mounted cubic obstacle."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a POD-Galerkin reduced order model (ROM) in conjunction with Physics-Informed Neural Networks (PINNs) for solving inverse problems for the Navier-Stokes equations?\n\n**A)** The ROM allows for more accurate approximations of the fluid dynamics fields, while the PINNs provide a more efficient way to solve the inverse problem.\n\n**B)** The ROM enables the use of deep neural networks to approximate the reduced outputs, while the PINNs provide a more robust way to incorporate physical equations into the model.\n\n**C)** The ROM provides a more efficient way to solve the inverse problem, while the PINNs allow for more accurate approximations of the unknown parameters.\n\n**D)** The ROM enables the use of PINNs to approximate the reduced outputs, while the PINNs provide a more efficient way to solve the inverse problem.\n\n**Correct Answer:** B) The ROM enables the use of deep neural networks to approximate the reduced outputs, while the PINNs provide a more robust way to incorporate physical equations into the model.\n\n**Explanation:** The correct answer is B) because the POD-Galerkin ROM is constructed by applying POD on the snapshots matrices of the fluid fields and performing a Galerkin projection of the NSE onto the POD reduced basis. This is then combined with the use of deep neural networks, which approximate the reduced outputs, as part of the PINN approach. The PINNs incorporate the physical equations (the POD-Galerkin reduced equations) into their structure as part of the loss function, providing a more robust way to incorporate physical equations into the model."}, "29": {"documentation": {"title": "On Interpretability and Similarity in Concept-Based Machine Learning", "source": "L\\'eonard Kwuida and Dmitry I. Ignatov", "docs_id": "2102.12723", "section": ["cs.LG", "cs.AI", "cs.DM", "math.CO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Interpretability and Similarity in Concept-Based Machine Learning. Machine Learning (ML) provides important techniques for classification and predictions. Most of these are black-box models for users and do not provide decision-makers with an explanation. For the sake of transparency or more validity of decisions, the need to develop explainable/interpretable ML-methods is gaining more and more importance. Certain questions need to be addressed: How does an ML procedure derive the class for a particular entity? Why does a particular clustering emerge from a particular unsupervised ML procedure? What can we do if the number of attributes is very large? What are the possible reasons for the mistakes for concrete cases and models? For binary attributes, Formal Concept Analysis (FCA) offers techniques in terms of intents of formal concepts, and thus provides plausible reasons for model prediction. However, from the interpretable machine learning viewpoint, we still need to provide decision-makers with the importance of individual attributes to the classification of a particular object, which may facilitate explanations by experts in various domains with high-cost errors like medicine or finance. We discuss how notions from cooperative game theory can be used to assess the contribution of individual attributes in classification and clustering processes in concept-based machine learning. To address the 3rd question, we present some ideas on how to reduce the number of attributes using similarities in large contexts."}, "answer": "Here is a difficult multi-form question for an exam based on the provided documentation:\n\n**Question:** In the context of concept-based machine learning, what are the primary limitations of traditional machine learning models, and how do notions from cooperative game theory address these limitations?\n\n**A)** Traditional machine learning models are limited by their inability to provide transparent explanations for their predictions, and notions from cooperative game theory can help identify the most influential attributes in classification processes.\n\n**B)** Traditional machine learning models are limited by their inability to handle large numbers of attributes, and notions from cooperative game theory can help reduce the number of attributes by identifying redundant or irrelevant features.\n\n**C)** Traditional machine learning models are limited by their inability to provide decision-makers with the importance of individual attributes in classification processes, and notions from cooperative game theory can help assess the contribution of individual attributes in classification and clustering processes.\n\n**D)** Traditional machine learning models are limited by their inability to provide transparent explanations for their predictions, and notions from cooperative game theory can help provide plausible reasons for model predictions using Formal Concept Analysis (FCA).\n\n**Correct Answer:** C) Traditional machine learning models are limited by their inability to provide decision-makers with the importance of individual attributes in classification processes, and notions from cooperative game theory can help assess the contribution of individual attributes in classification and clustering processes.\n\n**Explanation:** This question requires the test-taker to understand the limitations of traditional machine learning models and how notions from cooperative game theory can address these limitations. The correct answer, option C, highlights the importance of providing decision-makers with the importance of individual attributes in classification processes, which is a key aspect of interpretable machine learning. The other options are incorrect because they either focus on the limitations of traditional machine learning models (options A and B) or provide a partial explanation (option D)."}, "30": {"documentation": {"title": "On the role of extrinsic noise in microRNA-mediated bimodal gene\n  expression", "source": "Marco Del Giudice, Stefano Bo, Silvia Grigolon and Carla Bosia", "docs_id": "1611.00666", "section": ["q-bio.MN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the role of extrinsic noise in microRNA-mediated bimodal gene\n  expression. Several studies highlighted the relevance of extrinsic noise in shaping cell decision making and differentiation in molecular networks. Experimental evidences of phenotypic differentiation are given by the presence of bimodal distributions of gene expression levels, where the modes of the distribution often correspond to different physiological states of the system. We theoretically address the presence of bimodal phenotypes in the context of microRNA (miRNA)-mediated regulation. MiRNAs are small noncoding RNA molecules that downregulate the expression of their target mRNAs. The nature of this interaction is titrative and induces a threshold effect: below a given target transcription rate no mRNAs are free and available for translation. We investigate the effect of extrinsic noise on the system by introducing a fluctuating miRNA-transcription rate. We find that the presence of extrinsic noise favours the presence of bimodal target distributions which can be observed for a wider range of parameters compared to the case with intrinsic noise only and for lower miRNA-target interaction strength. Our results suggest that combining threshold-inducing interactions with extrinsic noise provides a simple and robust mechanism for obtaining bimodal populations not requiring fine tuning. We furthermore characterise the protein distributions dependence on protein half-life."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of microRNA-mediated regulation, what is the effect of extrinsic noise on the system, and how does it compare to the case with intrinsic noise only?\n\n**A)** The presence of extrinsic noise favours the presence of unimodal target distributions, which can be observed for a wider range of parameters compared to the case with intrinsic noise only.\n\n**B)** The presence of extrinsic noise favours the presence of bimodal target distributions, which can be observed for a wider range of parameters compared to the case with intrinsic noise only, and this effect is more pronounced for lower miRNA-target interaction strength.\n\n**C)** The presence of extrinsic noise has no effect on the system, and the case with intrinsic noise only remains the same.\n\n**D)** The presence of extrinsic noise only favours the presence of unimodal target distributions, which can be observed for a wider range of parameters compared to the case with intrinsic noise only.\n\n**Correct Answer:** B) The presence of extrinsic noise favours the presence of bimodal target distributions, which can be observed for a wider range of parameters compared to the case with intrinsic noise only, and this effect is more pronounced for lower miRNA-target interaction strength.\n\n**Explanation:** The correct answer is B) because the documentation states that the presence of extrinsic noise \"favours the presence of bimodal target distributions\" and that this effect is more pronounced for lower miRNA-target interaction strength. The other options are incorrect because they either misstate the effect of extrinsic noise or imply that it has no effect on the system."}, "31": {"documentation": {"title": "The Need to Reconcile Concepts that Characterize Systems Withstanding\n  Threats", "source": "Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, Igor Linkov", "docs_id": "1912.10457", "section": ["physics.soc-ph", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Need to Reconcile Concepts that Characterize Systems Withstanding\n  Threats. Desirable system performance in the face of threats and disruptions has been characterized by various management concepts. Through semi-structured interviews with editors of journals in the fields of emergency response and systems management, a literature review, and professional judgment, we identified nine related and often interchangeably-used system performance concepts: adaptability, agility, reliability, resilience, resistance, robustness, safety, security, and sustainability. We analysed expert responses and reviewed the linguistic definitions and mathematical framing of these concepts to understand their applications. We found a lack of consensus on their usage between interview subjects, but using a mathematical framing enriched the linguistic definitions and enabled formulating comparative visualizations and system-specific definitions for the concepts. We propose a conceptual framing to relate the concepts for management purposes. A better understanding of these concepts will allow system planners to pursue management strategies best suited to their unique system dynamics and specific objectives of 'goodness' that all these concepts bring."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the Arxiv paper \"The Need to Reconcile Concepts that Characterize Systems Withstanding Threats\" identified nine system performance concepts: adaptability, agility, reliability, resilience, resistance, robustness, safety, security, and sustainability. However, they found a lack of consensus on the usage of these concepts among experts. Using the mathematical framing of these concepts, which of the following statements can be made about the relationship between adaptability and robustness?\n\nA) Adaptability is a necessary condition for robustness, but not sufficient.\nB) Adaptability is a sufficient condition for robustness, but not necessary.\nC) Adaptability and robustness are equivalent concepts.\nD) The relationship between adaptability and robustness cannot be determined using mathematical framing.\n\n**Correct Answer:** A) Adaptability is a necessary condition for robustness, but not sufficient.\n\n**Explanation:** The authors found that using mathematical framing enriched the linguistic definitions of the concepts, enabling formulating comparative visualizations and system-specific definitions. This suggests that the concepts are related, but not identical. Adaptability implies the ability to adjust to changing conditions, while robustness implies the ability to withstand disruptions. Therefore, adaptability is a necessary condition for robustness, but not sufficient, as a system can be adaptable but not robust.\n\n**Candidate A:** This answer is incorrect because it implies that adaptability is both necessary and sufficient for robustness, which is not supported by the mathematical framing.\n\n**Candidate B:** This answer is incorrect because it implies that adaptability is sufficient for robustness, which is not supported by the mathematical framing.\n\n**Candidate C:** This answer is incorrect because it implies that adaptability and robustness are equivalent concepts, which is not supported by the mathematical framing.\n\n**Candidate D:** This answer is incorrect because it implies that the relationship between adaptability and robustness cannot be determined using mathematical framing, which is not supported by the text."}, "32": {"documentation": {"title": "The proton radius (puzzle?) and its relatives", "source": "Clara Peset, Antonio Pineda, and Oleksandr Tomalak", "docs_id": "2106.00695", "section": ["hep-ph", "nucl-th", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The proton radius (puzzle?) and its relatives. We review determinations of the electric proton charge radius from a diverse set of low-energy observables. We explore under which conditions it can be related to Wilson coefficients of appropriate effective field theories. This discussion is generalized to other low-energy constants. This provides us with a unified framework to deal with a set of low-energy constants of the proton associated with its electromagnetic interactions. Unambiguous definitions of these objects are given, as well as their relation with expectation values of QCD operators. We show that the proton radius obtained from spectroscopy and lepton-proton scattering (when both the lepton and proton move with nonrelativistic velocities) is related to the same object of the underlying field theory with ${\\cal O}(\\alpha)$ precision. The model dependence of these analyses is discussed. The prospects of constructing effective field theories valid for the kinematic configuration of present, or near-future, lepton-proton scattering experiments are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the proton radius obtained from spectroscopy and lepton-proton scattering, and the underlying field theory, in terms of precision and model dependence?\n\nA) The proton radius is related to the underlying field theory with ${\\cal O}(\\alpha)$ precision, but model dependence is negligible.\nB) The proton radius is related to the underlying field theory with ${\\cal O}(\\alpha)$ precision, but model dependence is significant and must be taken into account.\nC) The proton radius is not related to the underlying field theory with ${\\cal O}(\\alpha)$ precision, and model dependence is not a concern.\nD) The proton radius is related to the underlying field theory with ${\\cal O}(\\alpha)$ precision, but the relationship is only valid for specific kinematic configurations.\n\nCorrect Answer: B) The proton radius is related to the underlying field theory with ${\\cal O}(\\alpha)$ precision, but model dependence is significant and must be taken into account.\n\nExplanation: The correct answer is B) because the documentation states that the proton radius obtained from spectroscopy and lepton-proton scattering is related to the underlying field theory with ${\\cal O}(\\alpha)$ precision. However, it also mentions that the model dependence of these analyses is discussed, implying that the relationship is not without limitations and that model dependence is a significant consideration."}, "33": {"documentation": {"title": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report)", "source": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "docs_id": "1811.00694", "section": ["cs.SE", "cs.FL", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report). Improving patient care safety is an ultimate objective for medical cyber-physical systems. A recent study shows that the patients' death rate can be significantly reduced by computerizing medical best practice guidelines. To facilitate the development of computerized medical best practice guidelines, statecharts are often used as a modeling tool because of their high resemblances to disease and treatment models and their capabilities to provide rapid prototyping and simulation for clinical validations. However, some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics which makes it difficult to model certain functionalities that are essential in modeling medical guidelines, such as two-way communications and configurable execution orders. Rather than introducing new statechart elements or changing the statechart implementation's underline semantics, we use existing basic statechart elements to design model patterns for the commonly occurring issues. In particular, we show the design of model patterns for two-way communications and configurable execution orders and formally prove the correctness of these model patterns. We further use a simplified airway laser surgery scenario as a case study to demonstrate how the developed model patterns address the two-way communication and configurable execution order issues and their impact on validation and verification of medical safety properties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Design a statechart model pattern to facilitate the modeling of two-way communications in medical best practice guidelines, considering the limitations of priority-based statecharts with synchronous execution semantics.\n\n**A)** Use a combination of internal and external transitions to model two-way communications, where internal transitions represent the execution of medical procedures and external transitions represent the communication with external systems.\n\n**B)** Introduce a new statechart element, \"Communication Channel\", to model two-way communications, and use a priority-based approach to handle concurrent communications.\n\n**C)** Utilize a hierarchical statechart structure, where the top-level state represents the overall medical procedure and sub-states represent the different stages of communication with external systems.\n\n**D)** Employ a \"Request-Response\" pattern, where each statechart element represents a request or response to a medical procedure, and use a synchronous execution semantics to ensure that communications are processed in a specific order.\n\n**Correct Answer:** A) Use a combination of internal and external transitions to model two-way communications, where internal transitions represent the execution of medical procedures and external transitions represent the communication with external systems.\n\n**Explanation:**\n\nThe correct answer, A, is based on the idea of using a combination of internal and external transitions to model two-way communications in medical best practice guidelines. This approach allows for the representation of both the execution of medical procedures (internal transitions) and the communication with external systems (external transitions). This is in line with the goal of designing model patterns that can facilitate the modeling of complex medical scenarios, such as two-way communications, without introducing new statechart elements or changing the underlying semantics of the statechart implementation.\n\nThe other options are incorrect because:\n\n* B) Introducing a new statechart element, \"Communication Channel\", may not be necessary, and using a priority-based approach may not be sufficient to handle concurrent communications.\n* C) A hierarchical statechart structure may not be the most effective way to model two-way communications, as it may not capture the complexity of the interactions between different medical procedures and external systems.\n* D) Using a synchronous execution semantics may not be suitable for modeling two-way communications, as it may not allow for the representation of concurrent communications or asynchronous responses."}, "34": {"documentation": {"title": "CoCoLasso for High-dimensional Error-in-variables Regression", "source": "Abhirup Datta and Hui Zou", "docs_id": "1510.07123", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CoCoLasso for High-dimensional Error-in-variables Regression. Much theoretical and applied work has been devoted to high-dimensional regression with clean data. However, we often face corrupted data in many applications where missing data and measurement errors cannot be ignored. Loh and Wainwright (2012) proposed a non-convex modification of the Lasso for doing high-dimensional regression with noisy and missing data. It is generally agreed that the virtues of convexity contribute fundamentally the success and popularity of the Lasso. In light of this, we propose a new method named CoCoLasso that is convex and can handle a general class of corrupted datasets including the cases of additive measurement error and random missing data. We establish the estimation error bounds of CoCoLasso and its asymptotic sign-consistent selection property. We further elucidate how the standard cross validation techniques can be misleading in presence of measurement error and develop a novel corrected cross-validation technique by using the basic idea in CoCoLasso. The corrected cross-validation has its own importance. We demonstrate the superior performance of our method over the non-convex approach by simulation studies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of CoCoLasso, a new method for high-dimensional error-in-variables regression, and how does it differ from the non-convex Lasso approach proposed by Loh and Wainwright (2012)?\n\n**A)** CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with clean data, and it is a non-convex modification of the Lasso.\n\n**B)** CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with corrupted data, and it is a convex modification of the Lasso.\n\n**C)** CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with clean data, but it is a non-convex approach that relies on the virtues of convexity.\n\n**D)** CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with corrupted data, but it is a convex approach that does not rely on the virtues of convexity.\n\n**Correct Answer:** B) CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with corrupted data, and it is a convex modification of the Lasso.\n\n**Explanation:** The correct answer is B) because the question states that CoCoLasso was developed to address the limitations of the Lasso in handling high-dimensional data with corrupted data, and it is a convex modification of the Lasso. This is in contrast to the non-convex Lasso approach proposed by Loh and Wainwright (2012), which is mentioned in the question as a non-convex modification of the Lasso. The virtues of convexity are also mentioned in the question as a key factor in the success and popularity of the Lasso, which is why CoCoLasso is a convex modification of the Lasso."}, "35": {"documentation": {"title": "Predictions on the alpha decay half lives of Superheavy nuclei with Z =\n  113 in the range 255 l.e. A l.e. 314", "source": "K. P. Santhosh, A. Augustine, C. Nithya and B. Priyanka", "docs_id": "1604.00296", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predictions on the alpha decay half lives of Superheavy nuclei with Z =\n  113 in the range 255 l.e. A l.e. 314. An intense study of the alpha decay properties of the isotopes of superheavy element Z=113 have been performed within the Coulomb and proximity potential model for deformed nuclei (CPPMDN) within the wide range 255 l.e. A l.e. 314. The predicted alpha decay half lives of $^{278}$113 and $^{282}$113 and the alpha half lives of their decay products are in good agreement with the experimental data. 6{\\alpha) chains and 4{\\alpha} chains predicted respectively for $^{278}$113 and $^{282}$113 are in agreement with the experimental observation. Our study shows that the isotopes in the mass range 278 l.e. A l.e. 286 will survive fission and can be synthesized and detected in the laboratory via alpha decay. In our study, we have predicted 6{\\alpha} chains from $^{279}$113, 4{\\alpha} chains from $^{286}$113, 3{\\alpha} chains from $^{280,281,283}$113, 2{\\alpha} chains from $^{284}$113 and 1{\\alpha} chain from $^{285}$113. We hope that these predictions will be a guideline for future experimental investigations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the predicted number of \u03b1 chains for the isotopes of superheavy element Z=113 in the mass range 278 \u2264 A \u2264 286, according to the study published on Arxiv?\n\n**A)** 2 \u03b1 chains\n**B)** 3 \u03b1 chains\n**C)** 4 \u03b1 chains\n**D)** 6 \u03b1 chains\n\n**Correct Answer:** D) 6 \u03b1 chains\n\n**Explanation:** According to the study, the predicted number of \u03b1 chains for the isotopes of superheavy element Z=113 in the mass range 278 \u2264 A \u2264 286 are: 6 \u03b1 chains from $^{279}$113, 4 \u03b1 chains from $^{286}$113, 3 \u03b1 chains from $^{280,281,283}$113, 2 \u03b1 chains from $^{284}$113, and 1 \u03b1 chain from $^{285}$113. Therefore, the correct answer is 6 \u03b1 chains."}, "36": {"documentation": {"title": "Bayesian Projected Calibration of Computer Models", "source": "Fangzheng Xie, Yanxun Xu", "docs_id": "1803.01231", "section": ["stat.ME", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Projected Calibration of Computer Models. We develop a Bayesian approach called Bayesian projected calibration to address the problem of calibrating an imperfect computer model using observational data from a complex physical system. The calibration parameter and the physical system are parametrized in an identifiable fashion via $L_2$-projection. The physical process is assigned a Gaussian process prior, which naturally induces a prior distribution on the calibration parameter through the $L_2$-projection constraint. The calibration parameter is estimated through its posterior distribution, which provides a natural and non-asymptotic way for the uncertainty quantification. We provide a rigorous large sample justification for the proposed approach by establishing the asymptotic normality of the posterior of the calibration parameter with the efficient covariance matrix. In addition, two efficient computational algorithms based on stochastic approximation are designed with theoretical guarantees. Through extensive simulation studies and two real-world datasets analyses, we show that the Bayesian projected calibration can accurately estimate the calibration parameters, appropriately calibrate the computer models, and compare favorably to alternative approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Bayesian Projected Calibration of Computer Models, the calibration parameter and physical system are parametrized via $L_2$-projection. What is the prior distribution on the calibration parameter induced by the Gaussian process prior on the physical process, and how does it relate to the $L_2$-projection constraint?\n\n**A)** The prior distribution on the calibration parameter is a normal distribution with mean 0 and variance equal to the $L_2$-norm of the physical system's parameters. This distribution is induced by the $L_2$-projection constraint, which ensures that the calibration parameter is orthogonal to the physical system's parameters.\n\n**B)** The prior distribution on the calibration parameter is a uniform distribution over the range of possible values for the calibration parameter. This distribution is induced by the $L_2$-projection constraint, which ensures that the calibration parameter is constrained to be within a certain range.\n\n**C)** The prior distribution on the calibration parameter is a Gaussian process prior with mean 0 and covariance function equal to the $L_2$-norm of the physical system's parameters. This distribution is induced by the $L_2$-projection constraint, which ensures that the calibration parameter is orthogonal to the physical system's parameters.\n\n**D)** The prior distribution on the calibration parameter is a discrete distribution with a single possible value, which is the $L_2$-projection of the physical system's parameters onto the calibration parameter space. This distribution is induced by the $L_2$-projection constraint, which ensures that the calibration parameter is constrained to be within a certain range.\n\n**Correct Answer:** C) The prior distribution on the calibration parameter is a Gaussian process prior with mean 0 and covariance function equal to the $L_2$-norm of the physical system's parameters. This distribution is induced by the $L_2$-projection constraint, which ensures that the calibration parameter is orthogonal to the physical system's parameters.\n\n**Explanation:** The $L_2$-projection constraint ensures that the calibration parameter is orthogonal to the physical system's parameters, which means that the dot product between the calibration parameter and the physical system's parameters is zero. This constraint induces a prior distribution on the calibration parameter that is a Gaussian process prior with mean 0 and covariance function equal to the $L_2$-norm of the physical system's parameters. This prior distribution provides a natural and non-asymptotic way for the uncertainty quantification of the calibration parameter."}, "37": {"documentation": {"title": "Ab initio electronic structure and prospects for the formation of\n  ultracold calcium--alkali-metal-atom molecular ions", "source": "Wissem Zrafi, Hela Ladjimi, Halima Said, Hamid Berriche, Micha{\\l}\n  Tomza", "docs_id": "2003.02813", "section": ["physics.atom-ph", "cond-mat.quant-gas", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ab initio electronic structure and prospects for the formation of\n  ultracold calcium--alkali-metal-atom molecular ions. Experiments with cold ion-atom mixtures have recently opened the way for the production and application of ultracold molecular ions. Here, in a comparative study, we theoretically investigate ground and several excited electronic states and prospects for the formation of molecular ions composed of a calcium ion and an alkali-metal atom: CaAlk$^{+}$ (Alk=Li, Na, K, Rb, Cs). We use a quantum chemistry approach based on non-empirical pseudopotential, operatorial core-valence correlation, large Gaussian basis sets, and full configuration interaction method for valence electrons. Adiabatic potential energy curves, spectroscopic constants, and transition and permanent electric dipole moments are determined and analyzed for the ground and excited electronic states. We examine the prospects for ion-neutral reactive processes and the production of molecular ions via spontaneous radiative association and laser-induced photoassociation. After that, spontaneous and stimulated blackbody radiation transition rates are calculated and used to obtain radiative lifetimes of vibrational states of the ground and first-excited electronic states. The present results pave the way for the formation and spectroscopy of calcium--alkali-metal-atom molecular ions in modern experiments with cold ion-atom mixtures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary approach used in the study to investigate the formation of ultracold calcium-alkali-metal-atom molecular ions, and what are the key features of this approach?\n\nA) Density Functional Theory (DFT) with empirical pseudopotentials and small basis sets\nB) Quantum chemistry approach based on non-empirical pseudopotential, operatorial core-valence correlation, large Gaussian basis sets, and full configuration interaction method\nC) Semi-empirical method with a limited basis set and no core-valence correlation\nD) Molecular Mechanics with classical trajectories and a simple potential energy surface\n\nCorrect Answer: B) Quantum chemistry approach based on non-empirical pseudopotential, operatorial core-valence correlation, large Gaussian basis sets, and full configuration interaction method\n\nExplanation: The correct answer is B) because the study uses a quantum chemistry approach, which is a theoretical method that uses wave functions to describe the electronic structure of molecules. The specific features of this approach mentioned in the study are non-empirical pseudopotential, operatorial core-valence correlation, large Gaussian basis sets, and full configuration interaction method. These features are not mentioned in the other options, which are incorrect."}, "38": {"documentation": {"title": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study", "source": "Artur Strzelecki", "docs_id": "2003.10998", "section": ["cs.CY", "cs.IR", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study. The recent emergence of a new coronavirus, COVID-19, has gained extensive coverage in public media and global news. As of 24 March 2020, the virus has caused viral pneumonia in tens of thousands of people in Wuhan, China, and thousands of cases in 184 other countries and territories. This study explores the potential use of Google Trends (GT) to monitor worldwide interest in this COVID-19 epidemic. GT was chosen as a source of reverse engineering data, given the interest in the topic. Current data on COVID-19 is retrieved from (GT) using one main search topic: Coronavirus. Geographical settings for GT are worldwide, China, South Korea, Italy and Iran. The reported period is 15 January 2020 to 24 March 2020. The results show that the highest worldwide peak in the first wave of demand for information was on 31 January 2020. After the first peak, the number of new cases reported daily rose for 6 days. A second wave started on 21 February 2020 after the outbreaks were reported in Italy, with the highest peak on 16 March 2020. The second wave is six times as big as the first wave. The number of new cases reported daily is rising day by day. This short communication gives a brief introduction to how the demand for information on coronavirus epidemic is reported through GT."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the Google Trends data presented in the study on the COVID-19 epidemic. What is the significance of the second wave of interest in the epidemic, and how does it compare to the first wave in terms of its peak and duration?\n\n**A)** The second wave of interest in the epidemic was caused by the outbreak in South Korea, and it peaked on 16 March 2020, with a duration of 6 days.\n\n**B)** The second wave of interest in the epidemic was six times as big as the first wave, with a peak on 31 January 2020, and a duration of 10 days.\n\n**C)** The second wave of interest in the epidemic was caused by the outbreak in Italy, and it peaked on 16 March 2020, with a duration of 6 days, and was six times as big as the first wave.\n\n**D)** The second wave of interest in the epidemic was caused by the outbreak in Iran, and it peaked on 16 March 2020, with a duration of 10 days, and was six times as big as the first wave.\n\n**Correct Answer:** C) The second wave of interest in the epidemic was caused by the outbreak in Italy, and it peaked on 16 March 2020, with a duration of 6 days, and was six times as big as the first wave.\n\n**Explanation:** The correct answer is C) because the study states that the second wave of interest in the epidemic started on 21 February 2020, after the outbreaks were reported in Italy, and the highest peak was on 16 March 2020. Additionally, the study mentions that the second wave is six times as big as the first wave."}, "39": {"documentation": {"title": "Finite-Sample Concentration of the Multinomial in Relative Entropy", "source": "Rohit Agrawal", "docs_id": "1904.02291", "section": ["cs.IT", "math.IT", "math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finite-Sample Concentration of the Multinomial in Relative Entropy. We show that the moment generating function of the Kullback-Leibler divergence (relative entropy) between the empirical distribution of $n$ independent samples from a distribution $P$ over a finite alphabet of size $k$ (i.e. a multinomial distribution) and $P$ itself is no more than that of a gamma distribution with shape $k - 1$ and rate $n$. The resulting exponential concentration inequality becomes meaningful (less than 1) when the divergence $\\varepsilon$ is larger than $(k-1)/n$, whereas the standard method of types bound requires $\\varepsilon > \\frac{1}{n} \\cdot \\log{\\binom{n+k-1}{k-1}} \\geq (k-1)/n \\cdot \\log(1 + n/(k-1))$, thus saving a factor of order $\\log(n/k)$ in the standard regime of parameters where $n\\gg k$. As a consequence, we also obtain finite-sample bounds on all the moments of the empirical divergence (equivalently, the discrete likelihood-ratio statistic), which are within constant factors (depending on the moment) of their asymptotic values. Our proof proceeds via a simple reduction to the case $k = 2$ of a binary alphabet (i.e. a binomial distribution), and has the property that improvements in the case of $k = 2$ directly translate to improvements for general $k$. In particular, we conjecture a bound on the binomial moment generating function that would almost close the quadratic gap between our finite-sample bound and the asymptotic moment generating function bound from Wilks' theorem (which does not hold for finite samples)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Prove that the moment generating function of the Kullback-Leibler divergence (relative entropy) between the empirical distribution of $n$ independent samples from a distribution $P$ over a finite alphabet of size $k$ and $P$ itself is no more than that of a gamma distribution with shape $k - 1$ and rate $n$.\n\n**A)** The moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$ because the empirical distribution is a convex combination of the empirical distribution and $P$.\n\n**B)** The moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$ because the empirical distribution is a discrete approximation of the continuous gamma distribution.\n\n**C)** The moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$ because the empirical distribution is a finite-sample approximation of the asymptotic distribution of the Kullback-Leibler divergence.\n\n**D)** The moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$ because the empirical distribution is a binomial distribution with parameters $n$ and $p = 1/k$.\n\n**Correct Answer:** C) The moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$ because the empirical distribution is a finite-sample approximation of the asymptotic distribution of the Kullback-Leibler divergence.\n\n**Explanation:** The proof in the documentation reduces the problem to the case $k = 2$ of a binary alphabet (i.e. a binomial distribution), and shows that the moment generating function of the Kullback-Leibler divergence is bounded by the moment generating function of a gamma distribution with shape $k - 1$ and rate $n$. This result is then generalized to the case of a finite alphabet of size $k$. The correct answer, C, follows from this generalization."}, "40": {"documentation": {"title": "Mori-Zwanzig reduced models for uncertainty quantification II: Initial\n  condition uncertainty", "source": "Panagiotis Stinis", "docs_id": "1212.6360", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mori-Zwanzig reduced models for uncertainty quantification II: Initial\n  condition uncertainty. In a recent preprint (arXiv:1211.4285v1) we addressed the problem of constructing reduced models for time-dependent systems described by differential equations which involve uncertain parameters. In the current work, we focus on the construction of reduced models for systems of differential equations when the initial condition is uncertain. While for both cases the reduced models are constructed through the Mori-Zwanzig formalism, the necessary estimation of the memory parameters is quite different. For the case of uncertain initial conditions we present an algorithm which allows to estimate on the fly the parameters appearing in the reduced model. The first part of the algorithm evolves the full system until the estimation of the parameters for the reduced model has converged. At the time instant that this happens, the algorithm switches to the evolution of only the reduced model with the estimated parameter values from the first part of the algorithm. The viscous Burgers equation with uncertain initial condition is used to illustrate the construction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Mori-Zwanzig reduced models for uncertainty quantification, what is the key difference between estimating memory parameters for time-dependent systems with uncertain parameters versus systems with uncertain initial conditions?\n\nA) The estimation of memory parameters is more computationally intensive for systems with uncertain initial conditions.\nB) The estimation of memory parameters is less sensitive to the initial condition for systems with uncertain parameters.\nC) The estimation of memory parameters is performed on the fly for systems with uncertain initial conditions, whereas it is not possible for systems with uncertain parameters.\nD) The estimation of memory parameters is only necessary for systems with uncertain initial conditions, whereas it is not necessary for systems with uncertain parameters.\n\n**Correct Answer:** C) The estimation of memory parameters is performed on the fly for systems with uncertain initial conditions, whereas it is not possible for systems with uncertain parameters.\n\n**Explanation:** The correct answer is C) because the documentation states that for systems with uncertain initial conditions, an algorithm is presented that allows for the estimation of memory parameters \"on the fly\" during the evolution of the full system. This is in contrast to systems with uncertain parameters, where the estimation of memory parameters is not performed in the same way."}, "41": {"documentation": {"title": "Mixed Precision DNN Qunatization for Overlapped Speech Separation and\n  Recognition", "source": "Junhao Xu, Jianwei Yu, Xunying Liu, Helen Meng", "docs_id": "2111.14479", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mixed Precision DNN Qunatization for Overlapped Speech Separation and\n  Recognition. Recognition of overlapped speech has been a highly challenging task to date. State-of-the-art multi-channel speech separation system are becoming increasingly complex and expensive for practical applications. To this end, low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. However, current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different model components to quantization errors. In this paper, novel mixed precision DNN quantization methods are proposed by applying locally variable bit-widths to individual TCN components of a TF masking based multi-channel speech separation system. The optimal local precision settings are automatically learned using three techniques. The first two approaches utilize quantization sensitivity metrics based on either the mean square error (MSE) loss function curvature, or the KL-divergence measured between full precision and quantized separation models. The third approach is based on mixed precision neural architecture search. Experiments conducted on the LRS3-TED corpus simulated overlapped speech data suggest that the proposed mixed precision quantization techniques consistently outperform the uniform precision baseline speech separation systems of comparable bit-widths in terms of SI-SNR and PESQ scores as well as word error rate (WER) reductions up to 2.88% absolute (8% relative)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of mixed precision DNN quantization methods for overlapped speech separation and recognition, and how do these methods address the limitations of current uniform precision quantization methods?\n\nA) To reduce the computational complexity of neural networks while maintaining their accuracy, mixed precision DNN quantization methods aim to adaptively adjust the bit-width of individual components to minimize quantization errors. This approach is particularly effective for speech separation systems, where the performance sensitivity to quantization errors varies across different model components.\n\nB) To improve the robustness of neural networks to adversarial attacks, mixed precision DNN quantization methods employ locally variable bit-widths to reduce the impact of quantization errors on the model's performance. This approach is particularly useful for speech recognition systems, where the accuracy of the model is sensitive to the quality of the input data.\n\nC) To enhance the interpretability of neural networks, mixed precision DNN quantization methods use a combination of quantization sensitivity metrics and mixed precision neural architecture search to optimize the local precision settings. This approach is particularly effective for speech separation systems, where the performance sensitivity to quantization errors varies across different model components.\n\nD) To reduce the memory requirements of neural networks, mixed precision DNN quantization methods employ locally variable bit-widths to reduce the storage needs of the model. This approach is particularly useful for speech recognition systems, where the model size needs to be reduced to improve the system's performance on limited resources.\n\nCorrect Answer: A) To reduce the computational complexity of neural networks while maintaining their accuracy, mixed precision DNN quantization methods aim to adaptively adjust the bit-width of individual components to minimize quantization errors. This approach is particularly effective for speech separation systems, where the performance sensitivity to quantization errors varies across different model components."}, "42": {"documentation": {"title": "Contextual First-Price Auctions with Budgets", "source": "Santiago Balseiro, Christian Kroer, Rachitesh Kumar", "docs_id": "2102.10476", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contextual First-Price Auctions with Budgets. The internet advertising market is a multi-billion dollar industry, in which advertisers buy thousands of ad placements every day by repeatedly participating in auctions. In recent years, the industry has shifted to first-price auctions as the preferred paradigm for selling advertising slots. Another important and ubiquitous feature of these auctions is the presence of campaign budgets, which specify the maximum amount the advertisers are willing to pay over a specified time period. In this paper, we present a new model to study the equilibrium bidding strategies in first-price auctions for advertisers who satisfy budget constraints on average. Our model dispenses with the common, yet unrealistic assumption that advertisers' values are independent and instead assumes a contextual model in which advertisers determine their values using a common feature vector. We show the existence of a natural value-pacing-based Bayes-Nash equilibrium under very mild assumptions, and study its structural properties. Furthermore, we generalize the existence result to standard auctions and prove a revenue equivalence showing that all standard auctions yield the same revenue even in the presence of budget constraints."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of first-price auctions with budgets, what is the key assumption made by the authors of the paper \"Contextual First-Price Auctions with Budgets\" that differs from the common assumption in the literature?\n\nA) Advertisers' values are independent and identically distributed.\nB) Advertisers' values are correlated with the contextual feature vector.\nC) Advertisers' values are determined by a common feature vector, but are not necessarily correlated.\nD) Advertisers' values are not relevant in the presence of budget constraints.\n\n**Correct Answer:** C) Advertisers' values are determined by a common feature vector, but are not necessarily correlated.\n\n**Explanation:** The authors of the paper assume a contextual model in which advertisers determine their values using a common feature vector, but do not assume that the values are independent or correlated. This is a key departure from the common assumption in the literature, which assumes that advertisers' values are independent and identically distributed. The authors' assumption allows for a more nuanced understanding of how advertisers' values are influenced by the contextual feature vector.\n\n**Additional Question:** What is the main result of the authors' analysis, and what does it imply for the revenue generated by standard auctions in the presence of budget constraints?\n\nA) The authors show that the revenue generated by first-price auctions with budgets is always lower than that of standard auctions.\nB) The authors prove that all standard auctions yield the same revenue even in the presence of budget constraints.\nC) The authors demonstrate that the revenue generated by first-price auctions with budgets is always higher than that of standard auctions.\nD) The authors show that the revenue generated by standard auctions is independent of the budget constraints.\n\n**Correct Answer:** B) The authors prove that all standard auctions yield the same revenue even in the presence of budget constraints.\n\n**Explanation:** The authors' main result is that all standard auctions yield the same revenue even in the presence of budget constraints. This implies that the revenue generated by standard auctions is not affected by the budget constraints, and that the optimal bidding strategy for advertisers is the same regardless of the auction mechanism."}, "43": {"documentation": {"title": "Aspects of Jamming in Two-Dimensional Frictionless Systems", "source": "C. Reichhardt and C.J. Olson Reichhardt", "docs_id": "1312.5367", "section": ["cond-mat.soft", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Aspects of Jamming in Two-Dimensional Frictionless Systems. In this work we provide an overview of jamming transitions in two dimensional systems focusing on the limit of frictionless particle interactions in the absence of thermal fluctuations. We first discuss jamming in systems with short range repulsive interactions, where the onset of jamming occurs at a critical packing density and where certain quantities show a divergence indicative of critical behavior. We describe how aspects of the dynamics change as the jamming density is approached and how these dynamics can be explored using externally driven probes. Different particle shapes can produce jamming densities much lower than those observed for disk-shaped particles, and we show how jamming exhibits fragility for some shapes while for other shapes this is absent. Next we describe the effects of long range interactions and jamming behavior in systems such as charged colloids, vortices in type-II superconductors, and dislocations. We consider the effect of adding obstacles to frictionless jamming systems and discuss connections between this type of jamming and systems that exhibit depinning transitions. Finally, we discuss open questions such as whether the jamming transition in all these different systems can be described by the same or a small subset of universal behaviors, as well as future directions for studies of jamming transitions in two dimensional systems, such as jamming in self-driven or active matter systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary difference in jamming behavior between systems with short-range repulsive interactions and those with long-range interactions, as described in the context of two-dimensional frictionless systems?\n\nA) The onset of jamming occurs at a lower critical packing density for long-range interactions.\nB) The onset of jamming occurs at a higher critical packing density for long-range interactions.\nC) The onset of jamming is independent of the range of interactions.\nD) The onset of jamming is only dependent on the type of particle shape.\n\nCorrect Answer: B) The onset of jamming occurs at a higher critical packing density for long-range interactions.\n\nExplanation: According to the documentation, the authors describe how aspects of the dynamics change as the jamming density is approached, and how these dynamics can be explored using externally driven probes. Specifically, they mention that \"Different particle shapes can produce jamming densities much lower than those observed for disk-shaped particles\". This suggests that long-range interactions lead to a higher critical packing density, making option B the correct answer."}, "44": {"documentation": {"title": "Threshold learning dynamics in social networks", "source": "J. C. Gonz\\'alez-Avella, V. M. Egu\\'iluz, M. Marsili, F. Vega-Redondo\n  and M. San Miguel", "docs_id": "1008.3083", "section": ["physics.soc-ph", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Threshold learning dynamics in social networks. Social learning is defined as the ability of a population to aggregate information, a process which must crucially depend on the mechanisms of social interaction. Consumers choosing which product to buy, or voters deciding which option to take respect to an important issues, typically confront external signals to the information gathered from their contacts. Received economic models typically predict that correct social learning occurs in large populations unless some individuals display unbounded influence. We challenge this conclusion by showing that an intuitive threshold process of individual adjustment does not always lead to such social learning. We find, specifically, that three generic regimes exist. And only in one of them, where the threshold is within a suitable intermediate range, the population learns the correct information. In the other two, where the threshold is either too high or too low, the system either freezes or enters into persistent flux, respectively. These regimes are generally observed in different social networks (both complex or regular), but limited interaction is found to promote correct learning by enlarging the parameter region where it occurs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of social learning in social networks, what are the three generic regimes that emerge when considering the threshold process of individual adjustment, and how do they affect the population's ability to learn correct information?\n\n**A)** The three regimes are: (i) a regime where the threshold is very high, leading to persistent flux; (ii) a regime where the threshold is very low, leading to freezing; and (iii) a regime where the threshold is within a suitable intermediate range, leading to correct learning. However, this regime only occurs in regular social networks.\n\n**B)** The three regimes are: (i) a regime where the threshold is very high, leading to correct learning; (ii) a regime where the threshold is very low, leading to persistent flux; and (iii) a regime where the threshold is within a suitable intermediate range, leading to freezing. However, this regime only occurs in complex social networks.\n\n**C)** The three regimes are: (i) a regime where the threshold is very high, leading to correct learning; (ii) a regime where the threshold is very low, leading to persistent flux; and (iii) a regime where the threshold is within a suitable intermediate range, leading to correct learning. However, this regime only occurs in social networks with limited interaction.\n\n**D)** The three regimes are: (i) a regime where the threshold is very high, leading to persistent flux; (ii) a regime where the threshold is very low, leading to correct learning; and (iii) a regime where the threshold is within a suitable intermediate range, leading to correct learning. This regime is observed in both complex and regular social networks.\n\n**Correct Answer:** D) The three regimes are: (i) a regime where the threshold is very high, leading to persistent flux; (ii) a regime where the threshold is very low, leading to correct learning; and (iii) a regime where the threshold is within a suitable intermediate range, leading to correct learning. This regime is observed in both complex and regular social networks.\n\n**Explanation:** The correct answer is D) because it accurately describes the three generic regimes that emerge when considering the threshold process of individual adjustment. The regime with a high threshold leads to persistent flux, the regime with a low threshold leads to correct learning, and the regime with an intermediate threshold leads to correct learning in both complex and regular social networks. The other options are incorrect because they either misdescribe the regimes or limit the regimes to specific types of social networks."}, "45": {"documentation": {"title": "Dynamics of contentment", "source": "Alexey A. Burluka", "docs_id": "2101.05655", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of contentment. A continuous variable changing between 0 and 1 is introduced to characterise contentment, or satisfaction with life, of an individual and an equation governing its evolution is postulated from analysis of several factors likely to affect the contentment. As contentment is strongly affected by material well-being, a similar equation is formulated for wealth of an individual and from these two equations derived an evolution equation for the joint distribution of individuals' wealth and contentment within a society. The equation so obtained is used to compute evolution of this joint distribution in a society with initially low variation of wealth and contentment over a long period time. As illustration of this model capabilities, effects of the wealth tax rate are simulated and it is shown that a higher taxation in the longer run may lead to a wealthier and more content society. It is also shown that lower rates of the wealth tax lead to pronounced stratification of the society in terms of both wealth and contentment and that there is no direct relationship between the average values of these two variables."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Dynamics of contentment\" propose a model to describe the evolution of an individual's contentment (a continuous variable between 0 and 1) and wealth over time. According to the paper, what is the expected outcome of a higher wealth tax rate on the joint distribution of individuals' wealth and contentment in a society?\n\n**A)** A lower contentment level and a more unequal distribution of wealth.\n**B)** A higher contentment level and a more equal distribution of wealth.\n**C)** No significant change in either contentment level or wealth distribution.\n**D)** A lower wealth level and a more stratified society.\n\n**Correct Answer:** B) A higher contentment level and a more equal distribution of wealth.\n\n**Explanation:** The paper states that a higher taxation rate in the long run may lead to a wealthier and more content society. This suggests that a higher wealth tax rate can lead to a more equal distribution of wealth, as the tax revenue is redistributed to reduce income inequality. Additionally, the paper shows that a higher taxation rate can lead to a higher contentment level, as the tax revenue is used to improve the overall well-being of the society. Therefore, option B is the correct answer."}, "46": {"documentation": {"title": "Cellular buckling in stiffened plates", "source": "M. Ahmer Wadee, Maryam Farsi", "docs_id": "1405.0216", "section": ["cond-mat.mtrl-sci", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cellular buckling in stiffened plates. An analytical model based on variational principles for a thin-walled stiffened plate subjected to axial compression is presented. A system of nonlinear differential and integral equations is derived and solved using numerical continuation. The results show that the system is susceptible to highly unstable local--global mode interaction after an initial instability is triggered. Moreover, snap-backs in the response showing sequential destabilization and restabilization, known as cellular buckling or snaking, arise. The analytical model is compared to static finite element models for joint conditions between the stiffener and the main plate that have significant rotational restraint. However, it is known from previous studies that the behaviour, where the same joint is insignificantly restrained rotationally, is captured better by an analytical approach than by standard finite element methods; the latter being unable to capture cellular buckling behaviour even though the phenomenon is clearly observed in laboratory experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the behavior of a stiffened plate subjected to axial compression, as described in the study \"Cellular buckling in stiffened plates\". What is the primary mechanism underlying the highly unstable local-global mode interaction observed in the system, and how does it relate to the phenomenon of cellular buckling or snaking?\n\n**A)** The primary mechanism is the nonlinear interaction between the stiffener and the main plate, leading to a self-sustaining oscillation that amplifies instability. This is due to the rotational restraint at the joint between the stiffener and the main plate, which creates a feedback loop that enhances the instability.\n\n**B)** The primary mechanism is the sequential destabilization and restabilization of the plate's response, driven by the nonlinear differential and integral equations that govern the system's behavior. This leads to a \"snaking\" or cellular buckling pattern, where the plate undergoes repeated cycles of instability and recovery.\n\n**C)** The primary mechanism is the failure of standard finite element methods to capture the cellular buckling behavior, which is instead accurately represented by an analytical approach that accounts for the joint conditions between the stiffener and the main plate. This is due to the limitations of finite element methods in modeling nonlinear interactions and rotational restraint.\n\n**D)** The primary mechanism is the triggering of an initial instability, which then leads to a highly unstable local-global mode interaction due to the nonlinear effects of the stiffener and the main plate. This interaction is characterized by a complex interplay between the plate's geometry, material properties, and boundary conditions.\n\n**Correct Answer:** B) The primary mechanism is the sequential destabilization and restabilization of the plate's response, driven by the nonlinear differential and integral equations that govern the system's behavior. This leads to a \"snaking\" or cellular buckling pattern, where the plate undergoes repeated cycles of instability and recovery.\n\n**Explanation:** The correct answer is B) because the study describes the phenomenon of cellular buckling or snaking, which is characterized by sequential destabilization and restabilization of the plate's response. This is driven by the nonlinear differential and integral equations that govern the system's behavior, which lead to a complex interplay between the plate's geometry, material properties, and boundary conditions. The other options are incorrect because they either oversimplify the mechanism (A) or misrepresent the relationship between the analytical model and finite element methods (C), or fail to capture the essence of the phenomenon (D)."}, "47": {"documentation": {"title": "The Cepheid mass discrepancy and pulsation-driven mass loss", "source": "Hilding R. Neilson, Matteo Cantiello, Norbert Langer (AIfA)", "docs_id": "1104.1638", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Cepheid mass discrepancy and pulsation-driven mass loss. Context. A longstanding challenge for understanding classical Cepheids is the Cepheid mass discrepancy, where theoretical mass estimates using stellar evolution and stellar pulsation calculations have been found to differ by approximately 10 - 20%. Aims. We study the role of pulsation-driven mass loss during the Cepheid stage of evolution as a possible solution to this mass discrepancy. Methods. We computed stellar evolution models with a Cepheid mass-loss prescription and various amounts of convective core overshooting. The contribution of mass loss towards the mass discrepancy is determined using these models, Results. Pulsation-driven mass loss is found to trap Cepheid evolution on the instability strip, allowing them to lose about 5 - 10% of their total mass when moderate convective core overshooting, an amount consistent with observations of other stars, is included in the stellar models. Conclusions. We find that the combination of moderate convective core overshooting and pulsation-driven mass loss can solve the Cepheid mass discrepancy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism proposed to resolve the Cepheid mass discrepancy, and what is the estimated amount of mass loss associated with this process?\n\nA) Convective core overshooting alone, resulting in a 5-10% mass loss\nB) Pulsation-driven mass loss, resulting in a 5-10% mass loss\nC) Stellar evolution models without mass loss, resulting in a 10-20% mass discrepancy\nD) Stellar evolution models with excessive mass loss, resulting in a 20-30% mass discrepancy\n\nCorrect Answer: B) Pulsation-driven mass loss, resulting in a 5-10% mass loss\n\nExplanation: The question requires the test-taker to understand the main idea of the paper, which is that pulsation-driven mass loss can resolve the Cepheid mass discrepancy. The correct answer is supported by the text, which states that pulsation-driven mass loss allows Cepheid evolution to be trapped on the instability strip, resulting in a 5-10% mass loss. The other options are incorrect because they either focus on convective core overshooting alone (A), do not mention mass loss (C), or propose excessive mass loss (D)."}, "48": {"documentation": {"title": "Bridging Physics-based and Data-driven modeling for Learning Dynamical\n  Systems", "source": "Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang, Rose Yu", "docs_id": "2011.10616", "section": ["cs.LG", "physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bridging Physics-based and Data-driven modeling for Learning Dynamical\n  Systems. How can we learn a dynamical system to make forecasts, when some variables are unobserved? For instance, in COVID-19, we want to forecast the number of infected and death cases but we do not know the count of susceptible and exposed people. While mechanics compartment models are widely used in epidemic modeling, data-driven models are emerging for disease forecasting. We first formalize the learning of physics-based models as AutoODE, which leverages automatic differentiation to estimate the model parameters. Through a benchmark study on COVID-19 forecasting, we notice that physics-based mechanistic models significantly outperform deep learning. Our method obtains a 57.4% reduction in mean absolute errors for 7-day ahead COVID-19 forecasting compared with the best deep learning competitor. Such performance differences highlight the generalization problem in dynamical system learning due to distribution shift. We identify two scenarios where distribution shift can occur: changes in data domain and changes in parameter domain (system dynamics). Through systematic experiments on several dynamical systems, we found that deep learning models fail to forecast well under both scenarios. While much research on distribution shift has focused on changes in the data domain, our work calls attention to rethink generalization for learning dynamical systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of deep learning models in forecasting dynamical systems, as identified in the study \"Bridging Physics-based and Data-driven modeling for Learning Dynamical Systems\"?\n\nA) They are prone to overfitting due to the complexity of the data.\nB) They fail to generalize well due to distribution shift in the data domain.\nC) They are computationally expensive to train due to the need for large amounts of data.\nD) They are unable to capture the underlying physics of the system.\n\n**Correct Answer:** B) They fail to generalize well due to distribution shift in the data domain.\n\n**Explanation:** The study highlights that deep learning models fail to forecast well under two scenarios: changes in data domain and changes in parameter domain (system dynamics). The correct answer, B, specifically mentions distribution shift in the data domain as a primary limitation of deep learning models. The other options are incorrect because they do not accurately reflect the main limitation identified in the study. Option A is incorrect because overfitting is not mentioned as a limitation in the study. Option C is incorrect because computational expense is not mentioned as a limitation. Option D is incorrect because the study does not suggest that deep learning models are unable to capture the underlying physics of the system."}, "49": {"documentation": {"title": "Hadron Collider Tests of Neutrino Mass-Generating Mechanisms", "source": "Richard E. Ruiz", "docs_id": "1509.06375", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hadron Collider Tests of Neutrino Mass-Generating Mechanisms. The Standard Model of particle physics (SM) is presently the best description of nature at small distances and high energies. However, with tiny but nonzero neutrino masses, a Higgs boson mass unstable under radiative corrections, and little guidance on understanding the hierarchy of fermion masses, the SM remains an unsatisfactory description of nature. Well-motivated scenarios that resolve these issues exist but also predict extended gauge (e.g., Left-Right Symmetric Models), scalar (e.g., Supersymmetry), and/or fermion sectors (e.g., Seesaw Models). Hence, discovering such new states would have far-reaching implications. After reviewing basic tenets of the SM and collider physics, several beyond the SM (BSM) scenarios that alleviate these shortcomings are investigated. Emphasis is placed on the production of a heavy Majorana neutrinos at hadron colliders in the context of low-energy, effective theories that simultaneously explain the origin of neutrino masses and their smallness compared to other elementary fermions, the so-called Seesaw Mechanisms. As probes of new physics, rare top quark decays to Higgs bosons in the context of the SM, the Types I and II Two Higgs Doublet Model (2HDM), and the semi-model independent framework of Effective Field Theory (EFT) have also been investigated. Observation prospects and discovery potentials of these models at current and future collider experiments are quantified."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the main motivations for extending the Standard Model of particle physics, and what are some of the potential implications of discovering new states that could alleviate the shortcomings of the SM?\n\n**A)** The SM is insufficient because it fails to explain the origin of neutrino masses and their smallness compared to other elementary fermions, and discovering new states could provide a solution to these issues.\n\n**B)** The SM is insufficient because it predicts a Higgs boson mass unstable under radiative corrections, and discovering new states could provide a solution to this issue.\n\n**C)** The SM is insufficient because it fails to explain the hierarchy of fermion masses, and discovering new states could provide a solution to this issue.\n\n**D)** The SM is insufficient because it predicts a Higgs boson mass unstable under radiative corrections and fails to explain the origin of neutrino masses and their smallness compared to other elementary fermions.\n\n**Correct Answer:** D) The SM is insufficient because it predicts a Higgs boson mass unstable under radiative corrections and fails to explain the origin of neutrino masses and their smallness compared to other elementary fermions.\n\n**Explanation:** This question requires the test-taker to understand the motivations for extending the Standard Model of particle physics, as stated in the provided documentation. The correct answer, D, is the only one that accurately reflects the main motivations for extending the SM, which include the prediction of a Higgs boson mass unstable under radiative corrections and the failure to explain the origin of neutrino masses and their smallness compared to other elementary fermions. The other options, while partially correct, do not fully capture the main motivations for extending the SM."}, "50": {"documentation": {"title": "CP Factor Model for Dynamic Tensors", "source": "Yuefeng Han, Cun-Hui Zhang and Rong Chen", "docs_id": "2110.15517", "section": ["stat.ME", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CP Factor Model for Dynamic Tensors. Observations in various applications are frequently represented as a time series of multidimensional arrays, called tensor time series, preserving the inherent multidimensional structure. In this paper, we present a factor model approach, in a form similar to tensor CP decomposition, to the analysis of high-dimensional dynamic tensor time series. As the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on Tucker-type tensor decomposition. The model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes, making it much more convenient to study the underlying dynamics of the time series. A new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures commonly used in Tucker-type tensor factor model and general tensor CP decomposition procedures. Theoretical investigation provides statistical error bounds for the proposed methods, which shows the significant advantage of utilizing the special model structure. Simulation study is conducted to further demonstrate the finite sample properties of the estimators. Real data application is used to illustrate the model and its interpretations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed CP Factor Model for Dynamic Tensors over existing tensor factor models based on Tucker-type tensor decomposition?\n\n**A)** The model allows for orthogonal loading vectors, making it easier to interpret the results.\n**B)** The model enables the estimation of high-order interactions between variables, which is not possible in Tucker-type models.\n**C)** The model provides a more convenient framework for studying the underlying dynamics of time series data, as it allows for uncorrelated one-dimensional latent dynamic factor processes.\n**D)** The model is more computationally efficient than Tucker-type models, requiring fewer iterations to converge.\n\n**Correct Answer:** C) The model provides a more convenient framework for studying the underlying dynamics of time series data, as it allows for uncorrelated one-dimensional latent dynamic factor processes.\n\n**Explanation:** The correct answer, C, highlights the key advantage of the proposed CP Factor Model for Dynamic Tensors. By allowing for uncorrelated one-dimensional latent dynamic factor processes, the model provides a more interpretable and convenient framework for studying the underlying dynamics of time series data. This is in contrast to existing tensor factor models based on Tucker-type tensor decomposition, which may require more complex and orthogonal loading vectors."}, "51": {"documentation": {"title": "Bare Action and Regularized Functional Integral of Asymptotically Safe\n  Quantum Gravity", "source": "Elisa Manrique, Martin Reuter", "docs_id": "0811.3888", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bare Action and Regularized Functional Integral of Asymptotically Safe\n  Quantum Gravity. Investigations of Quantum Einstein Gravity (QEG) based upon the effective average action employ a flow equation which does not contain any ultraviolet (UV) regulator. Its renormalization group trajectories emanating from a non-Gaussian fixed point define asymptotically safe quantum field theories. A priori these theories are, somewhat unusually, given in terms of their effective rather than bare action. In this paper we construct a functional integral representation of these theories. We fix a regularized measure and show that every trajectory of effective average actions, depending on an IR cutoff only, induces an associated trajectory of bare actions which depend on a UV cutoff. Together with the regularized measure these bare actions give rise to a functional integral which reproduces the prescribed effective action when the UV cutoff is removed. In this way we are able to reconstruct the underlying microscopic (\"classical\") system and identify its fundamental degrees of freedom and interactions. The bare action of the Einstein-Hilbert truncation is computed and its flow is analyzed as an example. Various conceptual issues related to the completion of the asymptotic safety program are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the authors in constructing a functional integral representation of asymptotically safe quantum field theories, and how does it relate to the completion of the asymptotic safety program?\n\nA) To derive a new UV regulator for the flow equation, which would allow for a more precise analysis of the renormalization group trajectories.\nB) To reconstruct the underlying microscopic system and identify its fundamental degrees of freedom and interactions, by relating the effective action to the bare action through a functional integral.\nC) To investigate the non-Gaussian fixed point and its implications for the renormalization group trajectories, without considering the functional integral representation.\nD) To develop a new truncation of the Einstein-Hilbert action, which would provide a more complete understanding of the asymptotic safety program.\n\nCorrect Answer: B) To reconstruct the underlying microscopic system and identify its fundamental degrees of freedom and interactions, by relating the effective action to the bare action through a functional integral.\n\nExplanation: The authors aim to reconstruct the underlying microscopic system by relating the effective action to the bare action through a functional integral. This allows them to identify the fundamental degrees of freedom and interactions of the system, which is a key goal of the asymptotic safety program. The correct answer highlights this primary goal, while the incorrect options focus on related but distinct aspects of the topic."}, "52": {"documentation": {"title": "Search for Anomalous Couplings in Top Decay at Hadron Colliders", "source": "S. Tsuno, I. Nakano, Y. Sumino, R. Tanaka", "docs_id": "hep-ex/0512037", "section": ["hep-ex", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for Anomalous Couplings in Top Decay at Hadron Colliders. We present a quantitative study on sensitivities to the top-decay anomalous couplings, taking into account realistic experimental conditions expected at Tevatron and LHC. A double angular distribution of W and charged lepton in the top decay is analyzed, using ttbar events in the lepton+jets channel. In order to improve sensitivities to the anomalous couplings, we apply two techniques: (1) We use a likelihood fitting method for full kinematical reconstruction of each top event. (2) We develop a new effective spin reconstruction method for leptonically-decayed top quarks; this method does not require spin information of the antitop side. For simplicity, we neglect couplings of right-handed bottom quark as well as CP violating couplings. The 95% C.L. estimated bound on a ratio of anomalous couplings reads -0.81 < f_2/f_1 < -0.70, -0.12<f_2/f_1<0.14 using 1000 reconstructed top events at Tevatron, while -0.74<f_2/f_1<-0.72, -0.01<f_2/f_1<0.01 is expected with 100k reconstructed top events at LHC, where only statistical errors are taken into account. A two-fold ambiguity in the allowed range remains when the number of events exceeds a few hundred."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A study on the search for anomalous couplings in top decay at hadron colliders uses a likelihood fitting method to reconstruct top events at the Tevatron and LHC. Assuming the experimental conditions are realistic, what is the estimated bound on the ratio of anomalous couplings, f2/f1, at the 95% confidence level for the following scenarios?\n\nA) 100 reconstructed top events at Tevatron with only statistical errors considered\nB) 100k reconstructed top events at LHC with only statistical errors considered\nC) 100 reconstructed top events at Tevatron with both statistical and systematic errors considered\nD) 100k reconstructed top events at LHC with both statistical and systematic errors considered\n\nCorrect Answer: B) -0.74<f2/f1<-0.72, -0.01<f2/f1<0.01\n\nExplanation: The correct answer is B) 100k reconstructed top events at LHC with only statistical errors considered. This is because the text states that for 100k reconstructed top events at LHC, the estimated bound on the ratio of anomalous couplings is -0.74<f2/f1<-0.72, -0.01<f2/f1<0.01, with only statistical errors taken into account."}, "53": {"documentation": {"title": "Group Secret Key Generation Algorithms", "source": "Chunxuan Ye and Alex Reznik", "docs_id": "cs/0701124", "section": ["cs.IT", "cs.CR", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Group Secret Key Generation Algorithms. We consider a pair-wise independent network where every pair of terminals in the network observes a common pair-wise source that is independent of all the sources accessible to the other pairs. We propose a method for secret key agreement in such a network that is based on well-established point-to-point techniques and repeated application of the one-time pad. Three specific problems are investigated. 1) Each terminal's observations are correlated only with the observations of a central terminal. All these terminals wish to generate a common secret key. 2) In a pair-wise independent network, two designated terminals wish to generate a secret key with the help of other terminals. 3) All the terminals in a pair-wise independent network wish to generate a common secret key. A separate protocol for each of these problems is proposed. Furthermore, we show that the protocols for the first two problems are optimal and the protocol for the third problem is efficient, in terms of the resulting secret key rates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In a pair-wise independent network, a secret key agreement protocol is proposed to generate a common secret key among terminals. The protocol is based on well-established point-to-point techniques and repeated application of the one-time pad. Investigate the three specific problems investigated in the protocol and explain the advantages and disadvantages of each protocol.\n\n**A)** The protocol for problem 1, where each terminal's observations are correlated only with the observations of a central terminal, is optimal because it ensures that the secret key is generated with the highest possible rate. However, it may not be efficient in terms of the number of terminals required to generate the secret key.\n\n**B)** The protocol for problem 2, where two designated terminals wish to generate a secret key with the help of other terminals, is optimal because it allows for the generation of a secret key with the highest possible rate. However, it may not be efficient in terms of the number of terminals required to generate the secret key.\n\n**C)** The protocol for problem 3, where all the terminals in a pair-wise independent network wish to generate a common secret key, is efficient because it allows for the generation of a secret key with a high rate. However, it may not be optimal because it may not ensure the highest possible rate of secret key generation.\n\n**D)** The protocol for problem 1, where each terminal's observations are correlated only with the observations of a central terminal, is optimal because it ensures that the secret key is generated with the highest possible rate and is efficient in terms of the number of terminals required to generate the secret key.\n\n**Correct Answer:** D) The protocol for problem 1, where each terminal's observations are correlated only with the observations of a central terminal, is optimal because it ensures that the secret key is generated with the highest possible rate and is efficient in terms of the number of terminals required to generate the secret key.\n\n**Explanation:** The correct answer is D) because the protocol for problem 1 is optimal in terms of the secret key rate and efficiency. This is because the protocol ensures that each terminal's observations are correlated only with the observations of a central terminal, which allows for the generation of a secret key with the highest possible rate. Additionally, the protocol is efficient in terms of the number of terminals required to generate the secret key, as only one central terminal is required to generate the secret key."}, "54": {"documentation": {"title": "Path Integral Renormalization of Flow through Random Porous Media", "source": "Umut C. \\\"Ozer, Peter R. King, Dimitri D. Vvedensky", "docs_id": "1911.11218", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Path Integral Renormalization of Flow through Random Porous Media. The path integral for Darcy's law with a stochastic conductivity, which characterizes flow through random porous media, is used as a basis for Wilson renormalization-group (RG) calculations in momentum space. A coarse graining procedure is implemented by integrating over infinitesimal shells of large momenta corresponding to the elimination of the small scale modes of the theory. The resulting one-loop $\\beta$-functions are solved exactly to obtain an effective conductivity in a coarse grained theory over successively larger length scales. We first carry out a calculation with uncorrelated Gaussian conductivity fluctuations to illustrate the RG procedure before considering the effect of a finite correlation length of conductivity fluctuations. We conclude by discussing applications and extensions of our calculations, including comparisons with the numerical evaluation of path integrals, non-Gaussian fluctuations, and multiphase flow, for which the path integral formulation should prove particularly useful."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Path Integral Renormalization of Flow through Random Porous Media, what is the primary goal of the coarse graining procedure implemented in the Wilson renormalization-group (RG) calculations?\n\nA) To eliminate small-scale fluctuations in the conductivity and obtain a more accurate description of the flow through the porous media.\nB) To integrate over infinitesimal shells of large momenta to obtain an effective conductivity in a coarse-grained theory over successively larger length scales.\nC) To solve the one-loop \u03b2-functions exactly to obtain a closed-form expression for the effective conductivity.\nD) To compare the path integral formulation with numerical evaluations of path integrals for non-Gaussian fluctuations.\n\nCorrect Answer: B) To integrate over infinitesimal shells of large momenta to obtain an effective conductivity in a coarse-grained theory over successively larger length scales.\n\nExplanation: The correct answer is B) because the coarse graining procedure is implemented by integrating over infinitesimal shells of large momenta to eliminate small-scale modes of the theory and obtain an effective conductivity in a coarse-grained theory over successively larger length scales. This is the primary goal of the coarse graining procedure in the context of the Path Integral Renormalization of Flow through Random Porous Media.\n\nCandidate A is incorrect because while eliminating small-scale fluctuations is a consequence of the coarse graining procedure, it is not the primary goal. Candidate C is incorrect because solving the one-loop \u03b2-functions exactly is a step in the RG procedure, but not the primary goal of the coarse graining procedure. Candidate D is incorrect because comparing the path integral formulation with numerical evaluations of path integrals is a potential application of the work, but not the primary goal of the coarse graining procedure."}, "55": {"documentation": {"title": "Supernova bounds on axion-like particles coupled with nucleons and\n  electrons", "source": "Francesca Calore, Pierluca Carenza, Maurizio Giannotti, Joerg Jaeckel,\n  Giuseppe Lucente, Alessandro Mirizzi", "docs_id": "2107.02186", "section": ["hep-ph", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supernova bounds on axion-like particles coupled with nucleons and\n  electrons. We investigate the potential of type II supernovae (SNe) to constrain axion-like particles (ALPs) coupled simultaneously to nucleons and electrons. ALPs coupled to nucleons can be efficiently produced in the SN core via nucleon-nucleon bremsstrahlung and, for a wide range of parameters, leave the SN unhindered, producing a large ALP flux. For masses exceeding 1 MeV, these ALPs would decay into electron-positron pairs, generating a positron flux. In the case of Galactic SNe, the annihilation of the created positrons with the electrons present in the Galaxy would contribute to the 511 keV annihilation line. Using the SPI (SPectrometer on INTEGRAL) observation of this line, allows us to exclude a wide range of the axion-electron coupling, $10^{-19} \\lesssim g_{ae} \\lesssim 10^{-11}$, for $g_{ap}\\sim 10^{-9}$. Additionally, ALPs from extra-galactic SNe decaying into electron-positron pairs would yield a contribution to the cosmic X-ray background. In this case, we constrain the ALP-electron coupling down to $g_{ae} \\sim 10^{-20}$."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** An axion-like particle (ALP) is produced in the core of a type II supernova (SN) via nucleon-nucleon bremsstrahlung. The ALP decays into electron-positron pairs for masses exceeding 1 MeV. Assuming the annihilation of the created positrons with the electrons present in the Galaxy contributes to the 511 keV annihilation line, and using the SPI observation of this line, what is the range of values for the axion-electron coupling constant $g_{ae}$ that can be excluded?\n\nA) $10^{-20} \\lesssim g_{ae} \\lesssim 10^{-15}$\nB) $10^{-19} \\lesssim g_{ae} \\lesssim 10^{-11}$\nC) $10^{-18} \\lesssim g_{ae} \\lesssim 10^{-12}$\nD) $10^{-17} \\lesssim g_{ae} \\lesssim 10^{-14}$\n\n**Correct Answer:** B) $10^{-19} \\lesssim g_{ae} \\lesssim 10^{-11}$\n\n**Explanation:** According to the provided Arxiv documentation, the correct range of values for the axion-electron coupling constant $g_{ae}$ that can be excluded is $10^{-19} \\lesssim g_{ae} \\lesssim 10^{-11}$, assuming $g_{ap} \\sim 10^{-9}$. This range is derived from the SPI observation of the 511 keV annihilation line in the case of Galactic SNe."}, "56": {"documentation": {"title": "Dynamical Formation of Scalarized Black Holes and Neutron Stars through\n  Stellar Core Collapse", "source": "Hao-Jui Kuan, Daniela D. Doneva, Stoytcho S. Yazadjiev", "docs_id": "2103.11999", "section": ["gr-qc", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Formation of Scalarized Black Holes and Neutron Stars through\n  Stellar Core Collapse. In a certain class of scalar-Gauss-Bonnet gravity, the black holes and the neutron stars can undergo spontaneous scalarization -- a strong gravity phase transition triggered by a tachyonic instability due to the nonminimal coupling between the scalar field and the spacetime curvature. Studies of this phenomenon have, so far, been restricted mainly to the study of the tachyonic instability and stationary scalarized black holes and neutron stars. To date, no realistic physical mechanism for the formation of isolated scalarized black holes and neutron stars has been proposed. We study, for the first time, the spherically symmetric fully nonlinear stellar core collapse to a black hole and a neutron star in scalar-Gauss-Bonnet theories allowing for a spontaneous scalarization. We show that the core collapse can produce scalarized black holes and scalarized neutron stars starting with a nonscalarized progenitor star. The possible paths to reach the end (non)scalarized state are quite rich leading to interesting possibilities for observational manifestations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which scalarized black holes and neutron stars can form in scalar-Gauss-Bonnet gravity, and what is the key feature of this process that distinguishes it from previous studies?\n\nA) The nonminimal coupling between the scalar field and the spacetime curvature triggers a tachyonic instability, leading to a strong gravity phase transition.\nB) The core collapse of a nonscalarized progenitor star can produce a scalarized black hole or neutron star through a spontaneous scalarization process.\nC) The scalar field's nonminimal coupling with the spacetime curvature leads to a phase transition that causes the black hole or neutron star to become scalarized.\nD) The tachyonic instability is the primary driver of the scalarization process, which is triggered by the nonminimal coupling between the scalar field and the spacetime curvature.\n\nCorrect Answer: B) The core collapse of a nonscalarized progenitor star can produce a scalarized black hole or neutron star through a spontaneous scalarization process.\n\nExplanation: The correct answer is B) because the question asks about the primary mechanism by which scalarized black holes and neutron stars can form, and the documentation states that the core collapse of a nonscalarized progenitor star can produce scalarized black holes and neutron stars through a spontaneous scalarization process. The other options are incorrect because they either describe the tachyonic instability (A and D) or the nonminimal coupling (C), but do not accurately capture the primary mechanism of formation."}, "57": {"documentation": {"title": "Quark production, Bose-Einstein condensates and thermalization of the\n  quark-gluon plasma", "source": "Jean-Paul Blaizot, Bin Wu, and Li Yan", "docs_id": "1402.5049", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quark production, Bose-Einstein condensates and thermalization of the\n  quark-gluon plasma. In this paper, we study the thermalization of gluons and N_f flavors of massless quarks and antiquarks in a spatially homogeneous system. First, two coupled transport equations for gluons and quarks (and antiquarks) are derived within the diffusion approximation of the Boltzmann equation, with only 2<-> 2 processes included in the collision term. Then, these transport equations are solved numerically in order to study the thermalization of the quark-gluon plasma. At initial time, we assume that no quarks or antiquarks are present and we choose the gluon distribution in the form f = f_0 theta (1-p/Q_s) with Q_s the saturation momentum and f_0 a constant. The subsequent evolution of systems may, or may not, lead to the formation of a (transient) Bose condensate, depending on the value of f_0. In fact, we observe, depending on the value of f_0, three different patterns: (a) thermalization without gluon Bose-Einstein condensate (BEC) for f_0 < f_{0t}, (b) thermalization with transient BEC for f_{0t} < f_0 < f_{0c}, and (c) thermalization with BEC for f_{0c} < f_0. The values of f_{0t} and f_{0c} depend on N_f. When f_0> 1 > f_{0c}, the onset of BEC occurs at a finite time t_c ~ 1/((alpha_s f_0)^2 Q_s). We also find that quark production slows down the thermalization process: the equilibration time for N_f = 3 is typically about 5 to 6 times longer than that for N_f = 0 at the same Q_s."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the quark-gluon plasma, what is the relationship between the onset of Bose-Einstein condensation (BEC) and the value of the constant $f_0$ in the gluon distribution function $f = f_0 \\theta (1-p/Q_s)$?\n\n**A)** The onset of BEC occurs at a finite time $t_c$ when $f_0 > 1$, regardless of the value of $Q_s$.\n\n**B)** The onset of BEC occurs at a finite time $t_c$ when $f_0 > 1$ and $Q_s > 1$, but not when $Q_s < 1$.\n\n**C)** The onset of BEC occurs at a finite time $t_c$ when $f_0 > 1$ and $Q_s > 1$, and the value of $t_c$ is given by $t_c \\sim 1/((\\alpha_s f_0)^2 Q_s)$.\n\n**D)** The onset of BEC does not occur at a finite time $t_c$ when $f_0 > 1$, regardless of the value of $Q_s$.\n\n**Correct Answer:** C) The onset of BEC occurs at a finite time $t_c$ when $f_0 > 1$ and $Q_s > 1$, and the value of $t_c$ is given by $t_c \\sim 1/((\\alpha_s f_0)^2 Q_s)$.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"When $f_0> 1 > f_{0c}$, the onset of BEC occurs at a finite time $t_c \\sim 1/((\\alpha_s f_0)^2 Q_s)$. This indicates that the onset of BEC is related to the value of $f_0$ and the saturation momentum $Q_s$, and that the value of $t_c$ is given by the expression provided."}, "58": {"documentation": {"title": "Nonlinear coupling of flow harmonics: Hexagonal flow and beyond", "source": "Giuliano Giacalone, Li Yan, and Jean-Yves Ollitrault", "docs_id": "1803.00253", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear coupling of flow harmonics: Hexagonal flow and beyond. Higher Fourier harmonics of anisotropic flow ($v_4$ and beyond) get large contributions induced by elliptic and triangular flow through nonlinear response. We present a general framework of nonlinear hydrodynamic response which encompasses the existing one, and allows to take into account the mutual correlation between the nonlinear couplings affecting Fourier harmonics of any order. Using Large Hadron Collider data on Pb+Pb collisions at ~$\\sqrt[]{s}=2.76$ TeV, we perform an application of our formalism to hexagonal flow, $v_6$, a coefficient affected by several nonlinear contributions which are of the same order of magnitude. We obtain the first experimental measure of the coefficient $\\chi_{624}$, which couples $v_6$ to $v_2$ and $v_4$. This is achieved by putting together the information from several analyses: event-plane correlations, symmetric cumulants, as well as new higher-order moments recently analyzed by the ALICE collaboration. The value of $\\chi_{624}$ extracted from data is in fair agreement with hydrodynamic calculations, although with large error bars, which would be dramatically reduced by a dedicated analysis. We argue that within our formalism the nonlinear structure of a given higher harmonic can be determined more accurately than the harmonic itself, and we emphasize potential applications to future measurements of $v_7$ and $v_8$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of nonlinear hydrodynamic response, what is the significance of the coefficient $\\chi_{624}$, and how does its extraction from Large Hadron Collider data relate to the potential applications of the formalism to higher-order harmonics such as $v_7$ and $v_8$?\n\nA) The coefficient $\\chi_{624}$ is a measure of the nonlinear response to elliptic and triangular flow, and its extraction from data provides a direct test of the formalism's ability to describe the nonlinear structure of higher harmonics.\n\nB) The coefficient $\\chi_{624}$ is a measure of the correlation between the nonlinear couplings affecting Fourier harmonics of any order, and its extraction from data provides a way to constrain the parameters of the formalism and improve its predictive power for higher-order harmonics.\n\nC) The coefficient $\\chi_{624}$ is a measure of the mutual correlation between the nonlinear couplings affecting hexagonal flow ($v_6$) and other harmonics, and its extraction from data provides a way to test the formalism's ability to describe the nonlinear structure of $v_6$ and its connections to other harmonics.\n\nD) The coefficient $\\chi_{624}$ is a measure of the nonlinear response to anisotropic flow ($v_4$ and beyond), and its extraction from data provides a way to constrain the parameters of the formalism and improve its predictive power for higher-order harmonics.\n\nCorrect Answer: C) The coefficient $\\chi_{624}$ is a measure of the mutual correlation between the nonlinear couplings affecting hexagonal flow ($v_6$) and other harmonics, and its extraction from data provides a way to test the formalism's ability to describe the nonlinear structure of $v_6$ and its connections to other harmonics.\n\nExplanation: The correct answer, C, highlights the significance of the coefficient $\\chi_{624}$ as a measure of the mutual correlation between the nonlinear couplings affecting hexagonal flow ($v_6$) and other harmonics. This is a key aspect of the formalism, as it allows for the description of the nonlinear structure of higher harmonics in a more accurate and detailed way. The extraction of $\\chi_{624}$ from data provides a way to test the formalism's ability to describe the nonlinear structure of $v_6$ and its connections to other harmonics, such as $v_2$ and $v_4$. This is in line with the statement in the documentation that \"within our formalism the nonlinear structure of a given higher harmonic can be determined more accurately than the harmonic itself\"."}, "59": {"documentation": {"title": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM", "source": "Yipu Zhao, Justin S. Smith, Patricio A. Vela", "docs_id": "2008.10123", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM. The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical characteristic of resource-limited applications. While hardware and algorithm advances have been significantly improved the cost-efficiency of VSLAM front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This paper describes a novel, rigorous method to improve the cost-efficiency of local BA in a BA-based VSLAM back-end. An efficient algorithm, called Good Graph, is developed to select size-reduced graphs optimized in local BA with condition preservation. To better suit BA-based VSLAM back-ends, the Good Graph predicts future estimation needs, dynamically assigns an appropriate size budget, and selects a condition-maximized subgraph for BA estimation. Evaluations are conducted on two scenarios: 1) VSLAM as standalone process, and 2) VSLAM as part of closed-loop navigation system. Results from the first scenario show Good Graph improves accuracy and robustness of VSLAM estimation, when computational limits exist. Results from the second scenario, indicate that Good Graph benefits the trajectory tracking performance of VSLAM-based closed-loop navigation systems, which is a primary application of VSLAM."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary bottleneck in the cost-efficiency of Visual SLAM (VSLAM) back-ends, according to the paper \"Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment in Visual SLAM\"?\n\n**A)** The cost-efficiency of VSLAM front-ends\n**B)** The computational complexity of local Bundle Adjustment (BA) algorithms\n**C)** The cost-efficiency of VSLAM back-ends remains a bottleneck\n**D)** The accuracy of VSLAM estimation is limited by the availability of computational resources\n\n**Correct Answer:** C) The cost-efficiency of VSLAM back-ends remains a bottleneck\n\n**Explanation:** The paper states that \"the cost-efficiency of VSLAM front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck.\" This indicates that the primary bottleneck is indeed the cost-efficiency of VSLAM back-ends, which is the focus of the paper's proposed solution, Good Graph.\n\n**Explanation of incorrect options:**\n\n* A) The cost-efficiency of VSLAM front-ends is mentioned as improved by hardware and algorithm advances, but it is not the primary bottleneck.\n* B) While computational complexity is a consideration in VSLAM back-ends, it is not explicitly stated as the primary bottleneck.\n* D) The accuracy of VSLAM estimation is not directly related to the cost-efficiency of VSLAM back-ends, and the paper does not suggest that it is limited by computational resources."}}