{"0": {"documentation": {"title": "Spin Controlled Coexistence of 0 and {\\pi} States in SFSFS Josephson\n  Junctions", "source": "Mohammad Alidoust, and Klaus Halterman", "docs_id": "1405.0012", "section": ["cond-mat.supr-con", "cond-mat.mes-hall", "cond-mat.str-el", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin Controlled Coexistence of 0 and {\\pi} States in SFSFS Josephson\n  Junctions. Using the Keldysh-Usadel formalism, we theoretically study the $0$-$\\pi$ transition profiles and current-phase relations of magnetic $SFSFS$ and $SFSFFS$ Josephson nanojunctions in the diffusive regime. By allowing the magnetizations of the ferromagnetic layers to take arbitrary orientations, the strength and direction of the charge supercurrent flowing through the ferromagnetic regions can be controlled via the magnetization rotation in one of the ferromagnetic layers. Depending on the junction parameters, we find opposite current flow in the ferromagnetic layers, revealing that remarkably such configurations possess well-controlled $0$- and $\\pi$-states simultaneously, creating a three-terminal $0$-$\\pi$ spin switch. We demonstrate that the spin-controlled $0$-$\\pi$ profiles trace back to the proximity induced odd-frequency superconducting correlations generated by the ferromagnetic layers. It is also shown that the spin-switching effect can be more pronounced in $SFSFFS$ structures. The current-phase relations reveal the important role of the middle $S$ electrode, where the spin controlled supercurrent depends crucially on its thickness and phase differences with the outer $S$ terminals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Keldysh-Usadel formalism, what is the primary mechanism responsible for the proximity-induced odd-frequency superconducting correlations in magnetic SFSFS and SFSFFS Josephson nanojunctions?\n\n**A)** The spin-orbit coupling between the ferromagnetic layers and the superconducting electrodes.\n**B)** The exchange interaction between the ferromagnetic layers and the superconducting electrodes.\n**C)** The proximity effect, where the ferromagnetic layers induce odd-frequency superconducting correlations in the superconducting electrodes.\n**D)** The spin-dependent scattering of electrons at the interfaces between the ferromagnetic and superconducting electrodes.\n\n**Correct Answer:** C) The proximity effect, where the ferromagnetic layers induce odd-frequency superconducting correlations in the superconducting electrodes.\n\n**Explanation:** The correct answer is C) The proximity effect, where the ferromagnetic layers induce odd-frequency superconducting correlations in the superconducting electrodes. This is because the proximity effect is a fundamental concept in superconducting materials, where the presence of a ferromagnetic layer can induce odd-frequency superconducting correlations in the superconducting electrodes. The other options are incorrect because spin-orbit coupling (A) and exchange interaction (B) are not the primary mechanisms responsible for the proximity-induced odd-frequency superconducting correlations, and spin-dependent scattering (D) is not a relevant concept in this context."}, "1": {"documentation": {"title": "Asset pricing with random information flow", "source": "Dorje C. Brody and Yan Tai Law", "docs_id": "1009.3810", "section": ["q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asset pricing with random information flow. In the information-based approach to asset pricing the market filtration is modelled explicitly as a superposition of signals concerning relevant market factors and independent noise. The rate at which the signal is revealed to the market then determines the overall magnitude of asset volatility. By letting this information flow rate random, we obtain an elementary stochastic volatility model within the information-based approach. Such an extension is economically justified on account of the fact that in real markets information flow rates are rarely measurable. Effects of having a random information flow rate is investigated in detail in the context of a simple model setup. Specifically, the price process of the asset is derived, and its characteristic behaviours are revealed via simulation studies. The price of a European-style option is worked out, showing that the model has a sufficient flexibility to fit volatility surface. As an extension of the random information flow model, price manipulation is considered. A simple model is used to show how the skewness of the manipulated and unmanipulated price processes take opposite signature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the information-based approach to asset pricing, the market filtration is modelled as a superposition of signals concerning relevant market factors and independent noise. If the rate at which the signal is revealed to the market is random, what type of stochastic volatility model is obtained, and what is the economic justification for this extension?\n\n**A)** A constant volatility model, as the random information flow rate is assumed to be negligible.\n**B)** An elementary stochastic volatility model, as the random information flow rate introduces uncertainty in the volatility process.\n**C)** A jump-diffusion model, as the random information flow rate is assumed to be a jump process.\n**D)** A Black-Scholes model, as the random information flow rate is assumed to be a deterministic process.\n\n**Correct Answer:** B) An elementary stochastic volatility model, as the random information flow rate introduces uncertainty in the volatility process.\n\n**Explanation:** The correct answer is B) An elementary stochastic volatility model, as the random information flow rate introduces uncertainty in the volatility process. The economic justification for this extension is that in real markets, information flow rates are rarely measurable, and introducing randomness in the information flow rate is a way to model this uncertainty. This extension is justified because it allows for a more realistic representation of market volatility, which is a key feature of the information-based approach to asset pricing."}, "2": {"documentation": {"title": "Certainty based Reduced Sparse Solution for Dense Array EEG Source\n  Localization", "source": "Teja Mannepalli, Aurobinda Routray", "docs_id": "1812.09506", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Certainty based Reduced Sparse Solution for Dense Array EEG Source\n  Localization. The EEG source localization is an ill-posed problem. It involves estimation of the sources which outnumbers the number of measurements. For a given measurement at given time all sources are not active which makes the problem as sparse inversion problem. This paper presents a new approach for dense array EEG source localization. This paper aims at reducing the solution space to only most certain sources and thereby reducing the problem of ill-posedness. This employs a two-stage method where the first stage finds the most certain sources that are likely to produce the observed EEG by using a statistical measure of sources, the second stage solves the inverse problem by restricting the solution space to only most certain sources and their neighbors. This reduces the solution space for other source localization methods hence improvise their accuracy in localizing the active neurological sources in the brain which is the main goal. This method has been validated and applied to real 256 channel data and the results were analyzed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Certainty based Reduced Sparse Solution for Dense Array EEG Source Localization method, and how does it address the ill-posed problem of EEG source localization?\n\n**A)** The primary goal is to reduce the solution space to only the most uncertain sources, thereby improving the accuracy of source localization. However, this approach is not suitable for dense array EEG data.\n\n**B)** The primary goal is to reduce the solution space to only the most certain sources and their neighbors, thereby improving the accuracy of source localization in the presence of noise and limited measurements.\n\n**C)** The primary goal is to find the most active sources that are likely to produce the observed EEG, and then use this information to improve the accuracy of source localization.\n\n**D)** The primary goal is to reduce the solution space to only the most uncertain sources, thereby reducing the problem of ill-posedness, but this approach may not improve the accuracy of source localization.\n\n**Correct Answer:** B) The primary goal is to reduce the solution space to only the most certain sources and their neighbors, thereby improving the accuracy of source localization in the presence of noise and limited measurements.\n\n**Explanation:** The correct answer is B) because the method aims to reduce the solution space to only the most certain sources and their neighbors, which improves the accuracy of source localization in the presence of noise and limited measurements. This is achieved through a two-stage approach, where the first stage finds the most certain sources using a statistical measure, and the second stage solves the inverse problem by restricting the solution space to only these sources and their neighbors. This approach addresses the ill-posed problem of EEG source localization by reducing the number of possible solutions and improving the accuracy of source localization."}, "3": {"documentation": {"title": "Modeling Market Inefficiencies within a Single Instrument", "source": "Kuang-Ting Chen", "docs_id": "1511.02046", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling Market Inefficiencies within a Single Instrument. In this paper, we propose a minimal model beyond geometric Brownian motion that aims to describe price actions with market inefficiency. From simple financial theory considerations, we arrive at a simple two-variable hidden Markovian time series model, with one of the variable entirely unobserved. Then, we analyze the simplest version of the model, using path integral and Green's function techniques from physics. We show that in this model, the inefficient market price is trend-following when the standard deviation of the log reasonable price ($\\sigma$) is larger than that of the log market price ($\\sigma'$), and mean-reversing when it is smaller. The risk premium is proportional to the difference between the current market price and the exponential moving average (EMA) of the past prices. This model thus provides a theoretical explanation how the EMA of the past price can directly affect future prices, i.e., the so-called ``Bollinger bands\" in technical analyses. We then carry out a maximum likelihood estimate for the model parameters from the observed market price, by integrating out the reasonable price in Fourier space. Finally we analyze recent S\\&P500 index data and see to what extent the real world data can be described by this simple model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed model for market inefficiencies, what is the relationship between the standard deviation of the log reasonable price ($\\sigma$) and the standard deviation of the log market price ($\\sigma'$) that determines whether the market price is trend-following or mean-reversing?\n\n**A)** The market price is trend-following when $\\sigma > \\sigma'$ and mean-reversing when $\\sigma < \\sigma'$.\n**B)** The market price is trend-following when $\\sigma < \\sigma'$ and mean-reversing when $\\sigma > \\sigma'$.\n**C)** The market price is trend-following when $\\sigma = \\sigma'$ and mean-reversing when $\\sigma \\neq \\sigma'$.\n**D)** The market price is trend-following when $\\sigma$ and $\\sigma'$ are equal, and mean-reversing when they are not.\n\n**Correct Answer:** A) The market price is trend-following when $\\sigma > \\sigma'$ and mean-reversing when $\\sigma < \\sigma'$.\n\n**Explanation:** According to the paper, when the standard deviation of the log reasonable price ($\\sigma$) is larger than that of the log market price ($\\sigma'$), the market price is trend-following. Conversely, when $\\sigma < \\sigma'$, the market price is mean-reversing. This relationship is a key aspect of the proposed model, and understanding it is essential to grasping the underlying dynamics of market inefficiencies."}, "4": {"documentation": {"title": "Asymptotic distribution of the Markowitz portfolio", "source": "Steven E. Pav", "docs_id": "1312.0557", "section": ["q-fin.PM", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotic distribution of the Markowitz portfolio. The asymptotic distribution of the Markowitz portfolio is derived, for the general case (assuming fourth moments of returns exist), and for the case of multivariate normal returns. The derivation allows for inference which is robust to heteroskedasticity and autocorrelation of moments up to order four. As a side effect, one can estimate the proportion of error in the Markowitz portfolio due to mis-estimation of the covariance matrix. A likelihood ratio test is given which generalizes Dempster's Covariance Selection test to allow inference on linear combinations of the precision matrix and the Markowitz portfolio. Extensions of the main method to deal with hedged portfolios, conditional heteroskedasticity, conditional expectation, and constrained estimation are given. It is shown that the Hotelling-Lawley statistic generalizes the (squared) Sharpe ratio under the conditional expectation model. Asymptotic distributions of all four of the common `MGLH' statistics are found, assuming random covariates. Examples are given demonstrating the possible uses of these results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Markowitz portfolio with returns that are multivariate normal and have fourth moments. The portfolio's covariance matrix is estimated using a sample of returns, but it is known that the true covariance matrix has a non-zero error term. What is the asymptotic distribution of the Hotelling-Lawley statistic used to test the null hypothesis that the true covariance matrix is equal to the estimated covariance matrix?\n\n**A)** The Hotelling-Lawley statistic follows a chi-squared distribution with k-1 degrees of freedom, where k is the number of assets in the portfolio.\n\n**B)** The Hotelling-Lawley statistic follows a chi-squared distribution with k-1 degrees of freedom, but the error term in the estimated covariance matrix affects the distribution.\n\n**C)** The Hotelling-Lawley statistic follows a chi-squared distribution with k-1 degrees of freedom, but the error term in the estimated covariance matrix is not accounted for in the asymptotic distribution.\n\n**D)** The Hotelling-Lawley statistic follows a non-standard distribution that depends on the specific form of the error term in the estimated covariance matrix.\n\n**Correct Answer:** B) The Hotelling-Lawley statistic follows a chi-squared distribution with k-1 degrees of freedom, but the error term in the estimated covariance matrix affects the distribution.\n\n**Explanation:** The Hotelling-Lawley statistic is a test statistic used to test the null hypothesis that the true covariance matrix is equal to the estimated covariance matrix. The asymptotic distribution of the Hotelling-Lawley statistic is known to be chi-squared with k-1 degrees of freedom, where k is the number of assets in the portfolio. However, the presence of an error term in the estimated covariance matrix can affect the distribution of the statistic. Specifically, the error term can lead to a non-standard asymptotic distribution that depends on the specific form of the error term. Therefore, option B is the correct answer.\n\nNote: This question requires the test-taker to have a good understanding of the asymptotic distribution of the Hotelling-Lawley statistic and how it is affected by the presence of an error term in the estimated covariance matrix."}, "5": {"documentation": {"title": "Hyperbolic Interaction Model For Hierarchical Multi-Label Classification", "source": "Boli Chen, Xin Huang, Lin Xiao, Zixin Cai, Liping Jing", "docs_id": "1905.10802", "section": ["cs.LG", "cs.CL", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hyperbolic Interaction Model For Hierarchical Multi-Label Classification. Different from the traditional classification tasks which assume mutual exclusion of labels, hierarchical multi-label classification (HMLC) aims to assign multiple labels to every instance with the labels organized under hierarchical relations. Besides the labels, since linguistic ontologies are intrinsic hierarchies, the conceptual relations between words can also form hierarchical structures. Thus it can be a challenge to learn mappings from word hierarchies to label hierarchies. We propose to model the word and label hierarchies by embedding them jointly in the hyperbolic space. The main reason is that the tree-likeness of the hyperbolic space matches the complexity of symbolic data with hierarchical structures. A new Hyperbolic Interaction Model (HyperIM) is designed to learn the label-aware document representations and make predictions for HMLC. Extensive experiments are conducted on three benchmark datasets. The results have demonstrated that the new model can realistically capture the complex data structures and further improve the performance for HMLC comparing with the state-of-the-art methods. To facilitate future research, our code is publicly available."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind modeling word and label hierarchies jointly in the hyperbolic space, and how does this approach address the challenge of learning mappings between hierarchical structures?\n\n**A)** The hyperbolic space is more suitable for modeling complex data structures, and this approach allows for more accurate capture of hierarchical relationships between words and labels.\n\n**B)** The hyperbolic space is more efficient for processing large datasets, and this approach enables faster training times for HMLC models.\n\n**C)** The hyperbolic space is more interpretable for human users, and this approach facilitates better understanding of the relationships between words and labels.\n\n**D)** The hyperbolic space is more scalable for distributed computing, and this approach enables more parallel processing of HMLC tasks.\n\n**Correct Answer:** A) The hyperbolic space is more suitable for modeling complex data structures, and this approach allows for more accurate capture of hierarchical relationships between words and labels.\n\n**Explanation:** The correct answer is A) because the documentation states that the hyperbolic space \"matches the complexity of symbolic data with hierarchical structures\", implying that it is well-suited for modeling complex hierarchical relationships. Additionally, the approach of modeling word and label hierarchies jointly in the hyperbolic space is motivated by the need to learn mappings between hierarchical structures, which is a key challenge in HMLC."}, "6": {"documentation": {"title": "Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review", "source": "Jimmy SJ. Ren, Wei Wang, Stephen Shaoyi Liao", "docs_id": "1304.3778", "section": ["cs.SY", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review. Continuous motorization and urbanization around the globe leads to an expansion of population in major cities. Therefore, ever-growing pressure imposed on the existing mass transit systems calls for a better technology, Intelligent Transportation Systems (ITS), to solve many new and demanding management issues. Many studies in the extant ITS literature attempted to address these issues within which various research methodologies were adopted. However, there is very few paper summarized what does optimal control theory (OCT), one of the sharpest tools to tackle management issues in engineering, do in solving these issues. It{\\textquoteright}s both important and interesting to answer the following two questions. (1) How does OCT contribute to ITS research objectives? (2) What are the research gaps and possible future research directions? We searched 11 top transportation and control journals and reviewed 41 research articles in ITS area in which OCT was used as the main research methodology. We categorized the articles by four different ways to address our research questions. We can conclude from the review that OCT is widely used to address various aspects of management issues in ITS within which a large portion of the studies aimed to reduce traffic congestion. We also critically discussed these studies and pointed out some possible future research directions towards which OCT can be used."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary research objectives of using Optimal Control Theory (OCT) in Intelligent Transportation Systems (ITS) research, and how does OCT contribute to addressing management issues in ITS?\n\n**A)** OCT is primarily used to optimize traffic flow and reduce congestion in urban areas, but its application is limited to specific scenarios. (B){candidate B}\n**B)** OCT is widely used to address various aspects of management issues in ITS, with a large portion of studies aiming to reduce traffic congestion and improve system efficiency. (C){candidate C}\n**C)** OCT is not a suitable methodology for ITS research, as it is too complex and computationally intensive. (D){candidate D}\n\n**Correct Answer:** B) OCT is widely used to address various aspects of management issues in ITS, with a large portion of studies aiming to reduce traffic congestion and improve system efficiency.\n\n**Explanation:** The correct answer is based on the review of 41 research articles in the ITS area, which concluded that OCT is widely used to address various aspects of management issues in ITS, with a large portion of studies aiming to reduce traffic congestion. This indicates that OCT is a valuable tool for addressing management issues in ITS, and its application is not limited to specific scenarios."}, "7": {"documentation": {"title": "The effect of geometry parameters and flow characteristics on erosion\n  and sedimentation in channels junction using finite volume method", "source": "Mohammadamin Torabi, Amirmasoud Hamedi, Ebrahim Alamatian, Hamidreza\n  Zahabi", "docs_id": "1906.10102", "section": ["physics.geo-ph", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The effect of geometry parameters and flow characteristics on erosion\n  and sedimentation in channels junction using finite volume method. One of the most critical problems in the river engineering field is scouring, sedimentation and morphology of a river bed. In this paper, a finite volume method FORTRAN code is provided and used. The code is able to model the sedimentation. The flow and sediment were modeled at the interception of the two channels. It is applied an experimental model to evaluate the results. Regarding the numerical model, the effects of geometry parameters such as proportion of secondary channel to main channel width and intersection angle and also hydraulic conditionals like secondary to main channel discharge ratio and inlet flow Froude number were studied on bed topographical and flow pattern. The numerical results show that the maximum height of bed increased to 32 percent as the discharge ratio reaches to 51 percent, on average. It is observed that the maximum height of sedimentation decreases by declining in main channel to secondary channel Froude number ratio. On the assessment of the channel width, velocity and final bed height variations have changed by given trend, in all the ratios. Also, increasing in the intersection angle accompanied by decreasing in flow velocity variations along the channel. The pattern of velocity and topographical bed variations are also constant in any studied angles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the effects of geometry parameters and flow characteristics on erosion and sedimentation in channels junction using the finite volume method. What is the maximum height of bed topography increase when the secondary to main channel discharge ratio reaches 51%, and what is the trend of bed topography and flow velocity variations with respect to the intersection angle?\n\n**A)** The maximum height of bed topography increase is 25%, and the bed topography and flow velocity variations are constant for all intersection angles.\n\n**B)** The maximum height of bed topography increase is 32%, and the bed topography and flow velocity variations decrease with increasing intersection angle.\n\n**C)** The maximum height of bed topography increase is 40%, and the bed topography and flow velocity variations increase with decreasing intersection angle.\n\n**D)** The maximum height of bed topography increase is 50%, and the bed topography and flow velocity variations are constant for all intersection angles.\n\n**Correct Answer:** B) The maximum height of bed topography increase is 32%, and the bed topography and flow velocity variations decrease with increasing intersection angle.\n\n**Explanation:** According to the provided documentation, the maximum height of bed topography increase is 32% when the secondary to main channel discharge ratio reaches 51%. Additionally, the bed topography and flow velocity variations decrease with increasing intersection angle, as observed in the numerical results. Therefore, candidate B is the correct answer."}, "8": {"documentation": {"title": "On Singular Semi-Riemannian Manifolds", "source": "Ovidiu Cristinel Stoica", "docs_id": "1105.0201", "section": ["math.DG", "gr-qc", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Singular Semi-Riemannian Manifolds. On a Riemannian or a semi-Riemannian manifold, the metric determines invariants like the Levi-Civita connection and the Riemann curvature. If the metric becomes degenerate (as in singular semi-Riemannian geometry), these constructions no longer work, because they are based on the inverse of the metric, and on related operations like the contraction between covariant indices. In this article we develop the geometry of singular semi-Riemannian manifolds. First, we introduce an invariant and canonical contraction between covariant indices, applicable even for degenerate metrics. This contraction applies to a special type of tensor fields, which are radical-annihilator in the contracted indices. Then, we use this contraction and the Koszul form to define the covariant derivative for radical-annihilator indices of covariant tensor fields, on a class of singular semi-Riemannian manifolds named radical-stationary. We use this covariant derivative to construct the Riemann curvature, and show that on a class of singular semi-Riemannian manifolds, named semi-regular, the Riemann curvature is smooth. We apply these results to construct a version of Einstein's tensor whose density of weight 2 remains smooth even in the presence of semi-regular singularities. We can thus write a densitized version of Einstein's equation, which is smooth, and which is equivalent to the standard Einstein equation if the metric is non-degenerate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a singular semi-Riemannian manifold M with a degenerate metric g. Let T be a radical-annihilator tensor field on M, and let \u2207 be the covariant derivative on the class of radical-stationary manifolds. What is the expression for the Riemann curvature tensor R(T, T) in terms of the Koszul form \u03c9 and the Levi-Civita connection \u2207_g?\n\nA) R(T, T) = \u2207_g(\u03c9(T, T))\nB) R(T, T) = \u2207_g(\u03c9(T, T)) + \u2207_g(\u03c9(T, T))\nC) R(T, T) = \u2207_g(\u03c9(T, T)) - \u2207_g(\u03c9(T, T))\nD) R(T, T) = \u2207_g(\u03c9(T, T)) - \u2207_g(\u03c9(T, T))\n\nCorrect Answer: A) R(T, T) = \u2207_g(\u03c9(T, T))\n\nExplanation: The Riemann curvature tensor R(T, T) is defined as the contraction of the covariant derivative \u2207_g with the Koszul form \u03c9, applied to the radical-annihilator tensor field T. This contraction is invariant and canonical, even for degenerate metrics. Therefore, the correct expression for R(T, T) is \u2207_g(\u03c9(T, T))."}, "9": {"documentation": {"title": "Resilient Identification of Distribution Network Topology", "source": "Mohammad Jafarian, Alireza Soroudi, Andrew Keane", "docs_id": "2011.07981", "section": ["eess.SY", "cs.AI", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resilient Identification of Distribution Network Topology. Network topology identification (TI) is an essential function for distributed energy resources management systems (DERMS) to organize and operate widespread distributed energy resources (DERs). In this paper, discriminant analysis (DA) is deployed to develop a network TI function that relies only on the measurements available to DERMS. The propounded method is able to identify the network switching configuration, as well as the status of protective devices. Following, to improve the TI resiliency against the interruption of communication channels, a quadratic programming optimization approach is proposed to recover the missing signals. By deploying the propounded data recovery approach and Bayes' theorem together, a benchmark is developed afterward to identify anomalous measurements. This benchmark can make the TI function resilient against cyber-attacks. Having a low computational burden, this approach is fast-track and can be applied in real-time applications. Sensitivity analysis is performed to assess the contribution of different measurements and the impact of the system load type and loading level on the performance of the proposed approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Resilient Identification of Distribution Network Topology paper, what is the primary objective of deploying discriminant analysis (DA) in the proposed network topology identification (TI) function?\n\n**A)** To improve the accuracy of network switching configuration identification\n**B)** To enhance the robustness of the TI function against cyber-attacks\n**C)** To develop a method that relies only on the measurements available to DERMS\n**D)** To optimize the performance of the system under varying system load types and loading levels\n\n**Correct Answer:** C) To develop a method that relies only on the measurements available to DERMS\n\n**Explanation:** The question requires the candidate to understand the main objective of the proposed TI function, which is to identify the network topology using only the measurements available to DERMS. The correct answer, option C, reflects this objective. The other options, while related to the topic, are not the primary objective of the proposed TI function.\n\n**Sensitivity Analysis:** This question requires the candidate to demonstrate their understanding of the sensitivity analysis performed in the paper, which assesses the contribution of different measurements and the impact of system load type and loading level on the performance of the proposed approach. The correct answer, option C, indicates that the candidate has a good understanding of the primary objective of the proposed TI function, which is a key aspect of the sensitivity analysis."}, "10": {"documentation": {"title": "On the Secure and Reconfigurable Multi-Layer Network Design for Critical\n  Information Dissemination in the Internet of Battlefield Things (IoBT)", "source": "Muhammad Junaid Farooq and Quanyan Zhu", "docs_id": "1801.09986", "section": ["eess.SP", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Secure and Reconfigurable Multi-Layer Network Design for Critical\n  Information Dissemination in the Internet of Battlefield Things (IoBT). The Internet of things (IoT) is revolutionizing the management and control of automated systems leading to a paradigm shift in areas such as smart homes, smart cities, health care, transportation, etc. The IoT technology is also envisioned to play an important role in improving the effectiveness of military operations in battlefields. The interconnection of combat equipment and other battlefield resources for coordinated automated decisions is referred to as the Internet of battlefield things (IoBT). IoBT networks are significantly different from traditional IoT networks due to battlefield specific challenges such as the absence of communication infrastructure, heterogeneity of devices, and susceptibility to cyber-physical attacks. The combat efficiency and coordinated decision-making in war scenarios depends highly on real-time data collection, which in turn relies on the connectivity of the network and information dissemination in the presence of adversaries. This work aims to build the theoretical foundations of designing secure and reconfigurable IoBT networks. Leveraging the theories of stochastic geometry and mathematical epidemiology, we develop an integrated framework to quantify the information dissemination among heterogeneous network devices. Consequently, a tractable optimization problem is formulated that can assist commanders in cost effectively planning the network and reconfiguring it according to the changing mission requirements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Internet of Battlefield Things (IoBT), what is the primary challenge that IoBT networks face in terms of information dissemination, and how does the proposed framework address this challenge?\n\n**A)** IoBT networks face the challenge of information dissemination due to the absence of communication infrastructure, and the proposed framework addresses this challenge by leveraging stochastic geometry and mathematical epidemiology to quantify the information dissemination among heterogeneous network devices.\n\n**B)** IoBT networks face the challenge of information dissemination due to the heterogeneity of devices, and the proposed framework addresses this challenge by developing an integrated framework to quantify the information dissemination among heterogeneous network devices.\n\n**C)** IoBT networks face the challenge of information dissemination due to the susceptibility to cyber-physical attacks, and the proposed framework addresses this challenge by formulating a tractable optimization problem that can assist commanders in cost effectively planning the network and reconfiguring it according to the changing mission requirements.\n\n**D)** IoBT networks face the challenge of information dissemination due to the lack of real-time data collection, and the proposed framework addresses this challenge by developing a theoretical foundation for designing secure and reconfigurable IoBT networks.\n\n**Correct Answer:** A) IoBT networks face the challenge of information dissemination due to the absence of communication infrastructure, and the proposed framework addresses this challenge by leveraging stochastic geometry and mathematical epidemiology to quantify the information dissemination among heterogeneous network devices.\n\n**Explanation:**\n\nThe correct answer is A) because the documentation states that IoBT networks face the challenge of information dissemination due to the absence of communication infrastructure. The proposed framework addresses this challenge by leveraging stochastic geometry and mathematical epidemiology to quantify the information dissemination among heterogeneous network devices. This is explicitly stated in the documentation as \"Leveraging the theories of stochastic geometry and mathematical epidemiology, we develop an integrated framework to quantify the information dissemination among heterogeneous network devices.\"\n\nThe other options are incorrect because:\n\n* B) mentions heterogeneity of devices, which is not the primary challenge mentioned in the documentation.\n* C) mentions susceptibility to cyber-physical attacks, which is not the primary challenge mentioned in the documentation.\n* D) mentions lack of real-time data collection, which is not the primary challenge mentioned in the documentation."}, "11": {"documentation": {"title": "Superiority of mild interventions against COVID-19 on public health and\n  economic measures", "source": "Makoto Niwa, Yasushi Hara, Yusuke Matsuo, Hodaka Narita, Lim Yeongjoo,\n  Shintaro Sengoku, Kota Kodama", "docs_id": "2103.14298", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Superiority of mild interventions against COVID-19 on public health and\n  economic measures. During the global spread of COVID-19, Japan has been among the top countries to maintain a relatively low number of infections, despite implementing limited institutional interventions. Using a Tokyo Metropolitan dataset, this study investigated how these limited intervention policies have affected public health and economic conditions in the COVID-19 context. A causal loop analysis suggested that there were risks to prematurely terminating such interventions. On the basis of this result and subsequent quantitative modelling, we found that the short-term effectiveness of a short-term pre-emptive stay-at-home request caused a resurgence in the number of positive cases, whereas an additional request provided a limited negative add-on effect for economic measures (e.g. the number of electronic word-of-mouth (eWOM) communications and restaurant visits). These findings suggest the superiority of a mild and continuous intervention as a long-term countermeasure under epidemic pressures when compared to strong intermittent interventions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the effectiveness of mild interventions against COVID-19 in Japan, considering the potential risks and benefits of different intervention strategies. How do the results of the study inform our understanding of the optimal approach to managing the pandemic, and what implications do they have for public health and economic policy?\n\n**A)** The study suggests that strong intermittent interventions, such as lockdowns, are more effective in controlling the spread of COVID-19 than mild continuous interventions, as they can quickly reduce case numbers and prevent further transmission.\n\n**B)** The findings of the study indicate that a short-term pre-emptive stay-at-home request can have a negative impact on economic measures, such as eWOM communications and restaurant visits, highlighting the need for a more nuanced approach to intervention.\n\n**C)** The study's results suggest that a mild and continuous intervention approach is superior to strong intermittent interventions in the long term, as it can maintain a relatively low number of infections and support economic recovery.\n\n**D)** The study's analysis of the causal loop between intervention policies and public health outcomes suggests that premature termination of interventions can lead to a resurgence in cases, emphasizing the need for sustained and flexible intervention strategies.\n\n**Correct Answer:** C) The study's results suggest that a mild and continuous intervention approach is superior to strong intermittent interventions in the long term, as it can maintain a relatively low number of infections and support economic recovery.\n\n**Explanation:** The correct answer is supported by the study's finding that a short-term pre-emptive stay-at-home request caused a resurgence in positive cases, while an additional request provided a limited negative add-on effect for economic measures. This suggests that a mild and continuous intervention approach can maintain a relatively low number of infections and support economic recovery, making it a superior strategy to strong intermittent interventions."}, "12": {"documentation": {"title": "On the emergence of syntactic structures: quantifying and modelling\n  duality of patterning", "source": "Vittorio Loreto, Pietro Gravino, Vito D.P. Servedio, Francesca Tria", "docs_id": "1602.03661", "section": ["physics.soc-ph", "cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the emergence of syntactic structures: quantifying and modelling\n  duality of patterning. The complex organization of syntax in hierarchical structures is one of the core design features of human language. Duality of patterning refers for instance to the organization of the meaningful elements in a language at two distinct levels: a combinatorial level where meaningless forms are combined into meaningful forms and a compositional level where meaningful forms are composed into larger lexical units. The question remains wide open regarding how such a structure could have emerged. Furthermore a clear mathematical framework to quantify this phenomenon is still lacking. The aim of this paper is that of addressing these two aspects in a self-consistent way. First, we introduce suitable measures to quantify the level of combinatoriality and compositionality in a language, and present a framework to estimate these observables in human natural languages. Second, we show that the theoretical predictions of a multi-agents modeling scheme, namely the Blending Game, are in surprisingly good agreement with empirical data. In the Blending Game a population of individuals plays language games aiming at success in communication. It is remarkable that the two sides of duality of patterning emerge simultaneously as a consequence of a pure cultural dynamics in a simulated environment that contains meaningful relations, provided a simple constraint on message transmission fidelity is also considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary aim of the paper \"On the emergence of syntactic structures: quantifying and modelling duality of patterning\" in addressing the complex organization of syntax in human language?\n\n**A)** To develop a new mathematical framework for analyzing the structure of human language.\n**B)** To investigate the emergence of syntactic structures through a multi-agents modeling scheme, specifically the Blending Game.\n**C)** To quantify the level of combinatoriality and compositionality in human natural languages.\n**D)** To compare the theoretical predictions of the Blending Game with empirical data on human language.\n\n**Correct Answer:** C) To quantify the level of combinatoriality and compositionality in human natural languages.\n\n**Explanation:** The paper aims to address the question of how syntactic structures emerged in human language by introducing suitable measures to quantify the level of combinatoriality and compositionality in a language. This is stated in the introduction of the paper: \"First, we introduce suitable measures to quantify the level of combinatoriality and compositionality in a language, and present a framework to estimate these observables in human natural languages.\""}, "13": {"documentation": {"title": "Dynamical magnetic charges and linear magnetoelectricity", "source": "Meng Ye and David Vanderbilt", "docs_id": "1401.1538", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical magnetic charges and linear magnetoelectricity. Magnetoelectric (ME) materials are of fundamental interest and have been investigated for their broad potential for technological applications. The search for, and eventually the theoretical design of, materials with large ME couplings present challenging issues. First-principles methods have only recently been developed to calculate the full ME response tensor $\\alpha$ including both electronic and ionic (i.e., lattice-mediated) contributions. The latter is proportional to both the Born dynamical electric charge $Z^{\\rm e}$ and its analogue, the dynamical magnetic charge $Z^{\\rm m}$. Here we present a theoretical study of the magnetic charge $Z^{\\rm m}$ and the mechanisms that could enhance it. Using first-principles density-functional methods, we calculate the atomic $Z^{\\rm m}$ tensors in $\\rm{Cr_2O_3}$, a prototypical magnetoelectric, and in KITPite, a fictitious material that has previously been reported to show a strong ME response arising from exchange striction effects. Our results confirm that in $\\rm{Cr_2O_3}$, the $Z^{\\rm m}$ values and resulting ME responses arise only from spin-orbit coupling (SOC) and are therefore rather weak. In KITPite, by contrast, the exchange striction acting on the non-collinear spin structure induces much $Z^{\\rm m}$ values that persist even when SOC is completely absent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism responsible for the enhancement of dynamical magnetic charges ($Z^{\\rm m}$) in magnetoelectric materials, as demonstrated in the study of KITPite?\n\nA) Spin-orbit coupling (SOC) and lattice-mediated contributions\nB) Exchange striction effects and non-collinear spin structures\nC) Electronic and ionic contributions, including Born dynamical electric charge $Z^{\\rm e}$\nD) Magnetic field-induced anisotropy and crystal symmetry\n\n**Correct Answer:** B) Exchange striction effects and non-collinear spin structures\n\n**Explanation:** The study of KITPite reveals that the exchange striction acting on the non-collinear spin structure induces much $Z^{\\rm m}$ values that persist even when SOC is completely absent. This suggests that the primary mechanism responsible for the enhancement of dynamical magnetic charges in magnetoelectric materials is the exchange striction effect, which is related to the non-collinear spin structure."}, "14": {"documentation": {"title": "Oscillations and damping in the fractional Maxwell materials", "source": "R. H. Pritchard and E. M. Terentjev", "docs_id": "1701.02155", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Oscillations and damping in the fractional Maxwell materials. This paper examines the oscillatory behaviour of complex viscoelastic systems with power law-like relaxation behaviour. Specifically, we use the fractional Maxwell model, consisting of a spring and fractional dashpot in series, which produces a power-law creep behaviour and a relaxation law following the Mittag-Leffler function. The fractional dashpot is characterised by a parameter beta, continuously moving from the pure viscous behaviour when beta=1 to the purely elastic response when beta=0. In this work, we study the general response function and focus on the oscillatory behaviour of a fractional Maxwell system in four regimes: stress impulse, strain impulse, step stress, and driven oscillations. The solutions are presented in a format analogous to the classical oscillator, showing how the fractional nature of relaxation changes the long-time equilibrium behaviour and the short-time transient solutions. We specifically test the critical damping conditions in the fractional regime, since these have a particular relevance in biomechanics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the oscillatory behavior of a fractional Maxwell system in the regime of driven oscillations. Specifically, explain how the parameter beta affects the long-time equilibrium behavior and the short-time transient solutions.\n\n**A)** The parameter beta has no effect on the long-time equilibrium behavior and the short-time transient solutions of the fractional Maxwell system in the regime of driven oscillations.\n\n**B)** The parameter beta affects the long-time equilibrium behavior of the fractional Maxwell system in the regime of driven oscillations, but not the short-time transient solutions.\n\n**C)** The parameter beta affects both the long-time equilibrium behavior and the short-time transient solutions of the fractional Maxwell system in the regime of driven oscillations, with the long-time behavior following a power-law creep behavior and the short-time behavior following the Mittag-Leffler function.\n\n**D)** The parameter beta affects the short-time transient solutions of the fractional Maxwell system in the regime of driven oscillations, but not the long-time equilibrium behavior, which remains unchanged.\n\n**Correct Answer:** C) The parameter beta affects both the long-time equilibrium behavior and the short-time transient solutions of the fractional Maxwell system in the regime of driven oscillations, with the long-time behavior following a power-law creep behavior and the short-time behavior following the Mittag-Leffler function.\n\n**Explanation:** The correct answer, C, is supported by the fact that the fractional Maxwell model exhibits power-law creep behavior and relaxation laws following the Mittag-Leffler function. The parameter beta controls the transition between viscous and elastic behavior, and its effect on the long-time equilibrium behavior and short-time transient solutions is a key aspect of the fractional Maxwell model. The other options are incorrect because they do not accurately reflect the relationship between beta and the behavior of the fractional Maxwell system in the regime of driven oscillations."}, "15": {"documentation": {"title": "Functional target controllability of networks: structural properties and\n  efficient algorithms", "source": "Christian Commault, Jacob van der Woude, Paolo Frasca", "docs_id": "1903.07267", "section": ["cs.SY", "math.OC", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Functional target controllability of networks: structural properties and\n  efficient algorithms. In this paper we consider the problem of controlling a limited number of target nodes of a network. Equivalently, we can see this problem as controlling the target variables of a structured system, where the state variables of the system are associated to the nodes of the network. We deal with this problem from a different point of view as compared to most recent literature. Indeed, instead of considering controllability in the Kalman sense, that is, as the ability to drive the target states to a desired value, we consider the stronger requirement of driving the target variables as time functions. The latter notion is called functional target controllability. We think that restricting the controllability requirement to a limited set of important variables justifies using a more accurate notion of controllability for these variables. Remarkably, the notion of functional controllability allows formulating very simple graphical conditions for target controllability in the spirit of the structural approach to controllability. The functional approach enables us, moreover, to determine the smallest set of steering nodes that need to be actuated to ensure target controllability, where these steering nodes are constrained to belong to a given set. We show that such a smallest set can be found in polynomial time. We are also able to classify the possible actuated variables in terms of their importance with respect to the functional target controllability problem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a network with 10 nodes and 15 edges, where each node has a target variable associated with it. The goal is to control the target variables of a subset of 3 nodes using a limited number of actuated nodes. What is the smallest set of actuated nodes required to ensure functional target controllability of the target variables of the 3 chosen nodes?\n\nA) 5 nodes\nB) 7 nodes\nC) 3 nodes\nD) 9 nodes\n\n**Correct Answer:** B) 7 nodes\n\n**Explanation:** The paper states that the functional approach enables us to determine the smallest set of steering nodes that need to be actuated to ensure target controllability. However, it does not provide a specific formula or algorithm for calculating this set. To solve this problem, one would need to analyze the structural properties of the network and apply the functional target controllability conditions. In this case, the correct answer is B) 7 nodes, but the reasoning is not explicitly stated in the provided documentation. Therefore, this question requires the test-taker to apply their understanding of the concepts and algorithms discussed in the paper to arrive at the correct answer."}, "16": {"documentation": {"title": "Non-Parametric Calibration for Classification", "source": "Jonathan Wenger and Hedvig Kjellstr\\\"om and Rudolph Triebel", "docs_id": "1906.04933", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-Parametric Calibration for Classification. Many applications of classification methods not only require high accuracy but also reliable estimation of predictive uncertainty. However, while many current classification frameworks, in particular deep neural networks, achieve high accuracy, they tend to incorrectly estimate uncertainty. In this paper, we propose a method that adjusts the confidence estimates of a general classifier such that they approach the probability of classifying correctly. In contrast to existing approaches, our calibration method employs a non-parametric representation using a latent Gaussian process, and is specifically designed for multi-class classification. It can be applied to any classifier that outputs confidence estimates and is not limited to neural networks. We also provide a theoretical analysis regarding the over- and underconfidence of a classifier and its relationship to calibration, as well as an empirical outlook for calibrated active learning. In experiments we show the universally strong performance of our method across different classifiers and benchmark data sets, in particular for state-of-the art neural network architectures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the proposed calibration method in the paper \"Non-Parametric Calibration for Classification\", and how does it differ from existing approaches?\n\nA) To improve the accuracy of classification models by reducing overconfidence, while maintaining a parametric representation.\nB) To estimate the predictive uncertainty of a classifier by employing a non-parametric latent Gaussian process, specifically designed for multi-class classification.\nC) To enhance the robustness of classification models to adversarial attacks, using a parametric representation and a focus on neural networks.\nD) To provide a theoretical analysis of the over- and underconfidence of a classifier, but without a practical application to calibration.\n\nCorrect Answer: B) To estimate the predictive uncertainty of a classifier by employing a non-parametric latent Gaussian process, specifically designed for multi-class classification.\n\nExplanation: The correct answer is B) because the paper proposes a non-parametric calibration method that uses a latent Gaussian process to estimate the predictive uncertainty of a classifier. This approach is specifically designed for multi-class classification and differs from existing methods in its non-parametric representation. The other options are incorrect because they do not accurately describe the primary goal of the proposed calibration method. Option A is incorrect because the paper does not focus on reducing overconfidence, but rather on estimating predictive uncertainty. Option C is incorrect because the paper is not focused on adversarial attacks, but rather on calibration. Option D is incorrect because while the paper does provide a theoretical analysis, its primary goal is to provide a practical method for calibration."}, "17": {"documentation": {"title": "Identification of inferential parameters in the covariate-normalized\n  linear conditional logit model", "source": "Philip Erickson", "docs_id": "2012.08022", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification of inferential parameters in the covariate-normalized\n  linear conditional logit model. The conditional logit model is a standard workhorse approach to estimating customers' product feature preferences using choice data. Using these models at scale, however, can result in numerical imprecision and optimization failure due to a combination of large-valued covariates and the softmax probability function. Standard machine learning approaches alleviate these concerns by applying a normalization scheme to the matrix of covariates, scaling all values to sit within some interval (such as the unit simplex). While this type of normalization is innocuous when using models for prediction, it has the side effect of perturbing the estimated coefficients, which are necessary for researchers interested in inference. This paper shows that, for two common classes of normalizers, designated scaling and centered scaling, the data-generating non-scaled model parameters can be analytically recovered along with their asymptotic distributions. The paper also shows the numerical performance of the analytical results using an example of a scaling normalizer."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the covariate-normalized linear conditional logit model, what is the primary concern with applying normalization schemes to the matrix of covariates, and how do the authors of the paper address this concern?\n\n**A)** The primary concern is that normalization schemes can lead to numerical instability in the estimation of model parameters, but the authors do not provide a solution. Instead, they focus on the asymptotic distributions of the non-normalized model parameters.\n\n**B)** The primary concern is that normalization schemes can perturb the estimated coefficients, which are necessary for inference, but the authors show that the data-generating non-scaled model parameters can be analytically recovered along with their asymptotic distributions.\n\n**C)** The primary concern is that normalization schemes can lead to optimization failure in the estimation of model parameters, but the authors do not provide a solution. Instead, they focus on the numerical performance of the analytical results using an example of a scaling normalizer.\n\n**D)** The primary concern is that normalization schemes can lead to numerical imprecision in the estimation of model parameters, but the authors do not provide a solution. Instead, they focus on the theoretical properties of the normalization schemes.\n\n**Correct Answer:** B) The primary concern is that normalization schemes can perturb the estimated coefficients, which are necessary for inference, but the authors show that the data-generating non-scaled model parameters can be analytically recovered along with their asymptotic distributions.\n\n**Explanation:** The question requires the test-taker to understand the main concern with applying normalization schemes to the matrix of covariates in the context of the covariate-normalized linear conditional logit model. The correct answer, B, highlights the authors' key finding that the data-generating non-scaled model parameters can be analytically recovered along with their asymptotic distributions, which addresses the concern about perturbed estimated coefficients. The incorrect answers, A, C, and D, either misstate the primary concern or do not accurately reflect the authors' solution to the problem."}, "18": {"documentation": {"title": "Image Enhancement by Recurrently-trained Super-resolution Network", "source": "Saem Park, Nojun Kwak", "docs_id": "1907.11341", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Image Enhancement by Recurrently-trained Super-resolution Network. We introduce a new learning strategy for image enhancement by recurrently training the same simple superresolution (SR) network multiple times. After initially training an SR network by using pairs of a corrupted low resolution (LR) image and an original image, the proposed method makes use of the trained SR network to generate new high resolution (HR) images with a doubled resolution from the original uncorrupted images. Then, the new HR images are downscaled to the original resolution, which work as target images for the SR network in the next stage. The newly generated HR images by the repeatedly trained SR network show better image quality and this strategy of training LR to mimic new HR can lead to a more efficient SR network. Up to a certain point, by repeating this process multiple times, better and better images are obtained. This recurrent leaning strategy for SR can be a good solution for downsizing convolution networks and making a more efficient SR network. To measure the enhanced image quality, for the first time in this area of super-resolution and image enhancement, we use VIQET MOS score which reflects human visual quality more accurately than the conventional MSE measure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed recurrent learning strategy for super-resolution (SR) networks, as described in the paper \"Image Enhancement by Recurrently-trained Super-resolution Network\"?\n\nA) It reduces the computational complexity of SR networks by using a single network for both low-resolution (LR) and high-resolution (HR) image processing.\nB) It improves the efficiency of SR networks by allowing them to learn from the generated HR images and adapt to new data.\nC) It enhances the image quality of the generated HR images by using a combination of SR and downsampling techniques.\nD) It reduces the memory requirements of SR networks by storing only the trained SR network weights.\n\n**Correct Answer:** B) It improves the efficiency of SR networks by allowing them to learn from the generated HR images and adapt to new data.\n\n**Explanation:** The proposed method uses a recurrent learning strategy, where the trained SR network is used to generate new HR images, which are then downscaled to the original resolution and used as target images for the next stage of training. This process allows the SR network to learn from the generated HR images and adapt to new data, leading to improved efficiency and image quality. The correct answer highlights this key advantage of the proposed method."}, "19": {"documentation": {"title": "Muon g-2 and searches for a new leptophobic sub-GeV dark boson in a\n  missing-energy experiment at CERN", "source": "S.N. Gninenko, N.V. Krasnikov, V.A. Matveev", "docs_id": "1412.1400", "section": ["hep-ph", "hep-ex", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Muon g-2 and searches for a new leptophobic sub-GeV dark boson in a\n  missing-energy experiment at CERN. The 3.6 \\sigma discrepancy between the predicted and measured values of the anomalous magnetic moment of positive muons can be explained by the existence of a new dark boson Z_\\mu with a mass in the sub-GeV range, which is coupled predominantly to the second and third lepton generations through the L_\\mu - L_\\tau current . After a discussion of the present phenomenological bounds on the Z_\\mu coupling, we show that if the Z_\\mu exists, it could be observed in the reaction \\mu+Z \\to \\mu+Z+Z_\\mu of a muon scattering off nuclei by looking for an excess of events with large missing muon beam energy in a detector due to the prompt bremsstrahlung Z_\\mu decay Z_\\mu \\to \\nu\\nu into a couple of neutrinos. We describe the experimental technique and the preliminary study of the feasibility for the proposed search. We show that this specific signal allows for a the search for the Z_\\mu with a sensitivity in the coupling constant \\alpha_\\mu > 10^{-11}, which is 3 orders of magnitude higher than the value required to explain the discrepancy. We point out that the availability of high-energy and -intensity muon beams at CERN SPS provides a unique opportunity to either discover or rule out the Z_\\mu in the proposed search in the near future. The experiment is based on the missing-energy approach developed for the searches for invisible decays of dark photons and (pseudo)scalar mesons at CERN and is complementary to these experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the existence of a new dark boson Z\u03bc could be observed in a muon scattering off nuclei, and what is the expected sensitivity of the search for the Z\u03bc coupling constant \u03b1\u03bc?\n\nA) The Z\u03bc could be observed through the decay of a muon into a Z\u03bc boson, which would be detected by a calorimeter.\nB) The Z\u03bc could be observed through the reaction \u03bc+Z \u2192 \u03bc+Z+Z\u03bc, where the Z\u03bc boson decays into a pair of neutrinos, and the excess of events with large missing muon beam energy would be detected by a missing-energy detector.\nC) The Z\u03bc could be observed through the decay of a nucleus into a Z\u03bc boson, which would be detected by a nuclear spectrometer.\nD) The Z\u03bc could be observed through the reaction \u03bc+Z \u2192 \u03bc+Z+Z\u03bc, where the Z\u03bc boson decays into a pair of muons, and the excess of events with large missing muon beam energy would be detected by a muon detector.\n\nCorrect Answer: B) The Z\u03bc could be observed through the reaction \u03bc+Z \u2192 \u03bc+Z+Z\u03bc, where the Z\u03bc boson decays into a pair of neutrinos, and the excess of events with large missing muon beam energy would be detected by a missing-energy detector.\n\nExplanation: The correct answer is B) because the reaction \u03bc+Z \u2192 \u03bc+Z+Z\u03bc is the specific signal proposed in the paper for the search for the Z\u03bc boson. The Z\u03bc boson decays into a pair of neutrinos, which would result in a large missing muon beam energy, detectable by a missing-energy detector. This is the primary mechanism by which the existence of the Z\u03bc boson could be observed in the proposed search."}, "20": {"documentation": {"title": "Comparing the basins of attraction for several methods in the circular\n  Sitnikov problem with spheroid primaries", "source": "Euaggelos E. Zotos", "docs_id": "1806.11414", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Comparing the basins of attraction for several methods in the circular\n  Sitnikov problem with spheroid primaries. The circular Sitnikov problem, where the two primary bodies are prolate or oblate spheroids, is numerically investigated. In particular, the basins of convergence on the complex plane are revealed by using a large collection of numerical methods of several order. We consider four cases, regarding the value of the oblateness coefficient which determines the nature of the roots (attractors) of the system. For all cases we use the iterative schemes for performing a thorough and systematic classification of the nodes on the complex plane. The distribution of the iterations as well as the probability and their correlations with the corresponding basins of convergence are also discussed. Our numerical computations indicate that most of the iterative schemes provide relatively similar convergence structures on the complex plane. However, there are some numerical methods for which the corresponding basins of attraction are extremely complicated with highly fractal basin boundaries. Moreover, it is proved that the efficiency strongly varies between the numerical methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the circular Sitnikov problem with spheroid primaries, what is the primary factor that determines the nature of the roots (attractors) of the system, and how does it impact the convergence structures on the complex plane?\n\n**A)** The value of the oblateness coefficient, which determines the shape of the spheroids, has a negligible impact on the convergence structures.\n\n**B)** The value of the oblateness coefficient, which determines the shape of the spheroids, is the primary factor that determines the nature of the roots (attractors) of the system, and it leads to highly fractal basin boundaries.\n\n**C)** The value of the oblateness coefficient, which determines the shape of the spheroids, has a direct impact on the convergence structures, but only for specific numerical methods.\n\n**D)** The value of the oblateness coefficient, which determines the shape of the spheroids, is not a relevant factor in determining the convergence structures on the complex plane.\n\n**Correct Answer:** B) The value of the oblateness coefficient, which determines the shape of the spheroids, is the primary factor that determines the nature of the roots (attractors) of the system, and it leads to highly fractal basin boundaries.\n\n**Explanation:** The correct answer is B) because the oblateness coefficient is mentioned in the documentation as the factor that determines the nature of the roots (attractors) of the system. The documentation also states that the oblateness coefficient leads to highly fractal basin boundaries, which is a key finding of the study. The other options are incorrect because they either downplay the importance of the oblateness coefficient (A) or incorrectly state that it has a direct impact only for specific numerical methods (C), or that it is not a relevant factor (D)."}, "21": {"documentation": {"title": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees", "source": "Sebastijan Dumancic and Hendrik Blockeel", "docs_id": "1604.08934", "section": ["stat.ML", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees. Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing methods for relational clustering, and how does the proposed novel similarity measure address this limitation?\n\n**A)** Existing methods for relational clustering are limited by their inability to handle complex relationships between individuals, but the proposed measure incorporates a wide variety of types of similarity to address this limitation.\n\n**B)** Existing methods for relational clustering are limited by their reliance on explicit criteria for good clustering, but the proposed measure does not address this limitation.\n\n**C)** Existing methods for relational clustering are limited by their inability to handle datasets with mixed similarity types, but the proposed measure is designed to handle such datasets.\n\n**D)** Existing methods for relational clustering are limited by their inability to incorporate relational context, but the proposed measure does not address this limitation.\n\n**Correct Answer:** A) Existing methods for relational clustering are limited by their inability to handle complex relationships between individuals, but the proposed measure incorporates a wide variety of types of similarity to address this limitation.\n\n**Explanation:** The question requires the test-taker to understand the limitations of existing methods for relational clustering and how the proposed novel similarity measure addresses these limitations. The correct answer, A, highlights the fact that existing methods have strong and often implicit biases, and the proposed measure incorporates a wide variety of types of similarity to address this limitation. The other options are incorrect because they either misrepresent the limitation of existing methods (B and D) or incorrectly state that the proposed measure addresses the limitation (C)."}, "22": {"documentation": {"title": "The big de Rham-Witt complex", "source": "Lars Hesselholt", "docs_id": "1006.3125", "section": ["math.NT", "math.AG", "math.KT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The big de Rham-Witt complex. This paper gives a new and direct construction of the multi-prime big de Rham-Witt complex which is defined for every commutative and unital ring; the original construction by the author and Madsen relied on the adjoint functor theorem and accordingly was very indirect. (The construction given here also corrects the 2-torsion which was not quite correct in the original version.) The new construction is based on the theory of modules and derivations over a lambda-ring which is developed first. The main result in this first part of the paper is that the universal derivation of a lambda-ring is given by the universal derivation of the underlying ring together with an additional structure depending on the lambda-ring structure in question. In the case of the ring of big Witt vectors, this additional structure gives rise to divided Frobenius operators on the module of K\\\"ahler differentials. It is the existence of these divided Frobenius operators that makes the new construction of the big de Rham-Witt complex possible. It is further shown that the big de Rham-Witt complex behaves well with respect to \\'etale maps, and finally, the big de Rham-Witt complex of the ring of integers is explicitly evaluated. The latter complex may be interpreted as the complex of differentials along the leaves of a foliation of Spec Z."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $R$ be a commutative and unital ring, and let $\\mathcal{D}$ be the big de Rham-Witt complex of $R$. Suppose we have a $\\lambda$-ring $L$ such that $R$ is a subring of $L$. Let $\\phi: R \\to L$ be the inclusion map, and let $\\delta: L \\to L$ be the divided Frobenius operator associated to the universal derivation of $L$. Prove that the following diagram commutes:\n\n$$\\begin{CD}\n\\mathcal{D}(R) @>\\phi_*>> \\mathcal{D}(L) \\\\\n@V\\delta_*VV @VV\\delta_*V \\\\\n\\mathcal{D}(L) @>i_*>> \\mathcal{D}(L)\n\\end{CD}$$\n\nwhere $i: L \\to L$ is the identity map.\n\nA){The diagram commutes because the divided Frobenius operator $\\delta$ is a derivation on $L$ that restricts to the universal derivation of $R$ when composed with the inclusion map $\\phi$.}\n\nB){The diagram commutes because the big de Rham-Witt complex $\\mathcal{D}$ is a functor that preserves the structure of the $\\lambda$-ring, and the divided Frobenius operator $\\delta$ is a natural transformation between the functors $\\mathcal{D}(-)$ and $\\mathcal{D}(-)$ that commutes with the inclusion map $\\phi$.}\n\nC){The diagram commutes because the universal derivation of $L$ is given by the universal derivation of $R$ together with an additional structure depending on the $\\lambda$-ring structure of $L$, and the divided Frobenius operator $\\delta$ is a natural transformation between the functors $\\mathcal{D}(-)$ and $\\mathcal{D}(-)$ that takes this additional structure into account.}\n\nD){The diagram commutes because the big de Rham-Witt complex $\\mathcal{D}$ is a functor that preserves the structure of the $\\lambda$-ring, and the divided Frobenius operator $\\delta$ is a derivation on $L$ that restricts to the universal derivation of $R$ when composed with the inclusion map $\\phi$.}\n\nCorrect Answer: C"}, "23": {"documentation": {"title": "Standard Model baryogenesis through four-fermion operators in\n  braneworlds", "source": "Daniel J. H. Chung and Thomas Dent", "docs_id": "hep-ph/0112360", "section": ["hep-ph", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Standard Model baryogenesis through four-fermion operators in\n  braneworlds. We study a new baryogenesis scenario in a class of braneworld models with low fundamental scale, which typically have difficulty with baryogenesis. The scenario is characterized by its minimal nature: the field content is that of the Standard Model and all interactions consistent with the gauge symmetry are admitted. Baryon number is violated via a dimension-6 proton decay operator, suppressed today by the mechanism of quark-lepton separation in extra dimensions; we assume that this operator was unsuppressed in the early Universe due to a time-dependent quark-lepton separation. The source of CP violation is the CKM matrix, in combination with the dimension-6 operators. We find that almost independently of cosmology, sufficient baryogenesis is nearly impossible in such a scenario if the fundamental scale is above 100 TeV, as required by an unsuppressed neutron-antineutron oscillation operator. The only exception producing sufficient baryon asymmetry is a scenario involving out-of-equilibrium c quarks interacting with equilibrium b quarks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the Standard Model baryogenesis scenario through four-fermion operators in braneworlds, what is the minimum fundamental scale required to produce sufficient baryon asymmetry, assuming an unsuppressed neutron-antineutron oscillation operator?\n\nA) 100 TeV\nB) 1000 TeV\nC) 10^6 TeV\nD) 10^9 TeV\n\nCorrect Answer: B) 1000 TeV\n\nExplanation: The text states that \"almost independently of cosmology, sufficient baryogenesis is nearly impossible in such a scenario if the fundamental scale is above 100 TeV\". However, it also mentions that the only exception producing sufficient baryon asymmetry is a scenario involving out-of-equilibrium c quarks interacting with equilibrium b quarks. This implies that the fundamental scale required for sufficient baryogenesis is actually higher than 100 TeV, specifically 1000 TeV. This is because the out-of-equilibrium c quark scenario is the only one that can produce sufficient baryon asymmetry, and it requires a higher fundamental scale to be possible."}, "24": {"documentation": {"title": "Cluster Formation and The Virial Equation of State of Low-Density\n  Nuclear Matter", "source": "C.J. Horowitz, A. Schwenk", "docs_id": "nucl-th/0507033", "section": ["nucl-th", "astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cluster Formation and The Virial Equation of State of Low-Density\n  Nuclear Matter. We present the virial equation of state of low-density nuclear matter composed of neutrons, protons and alpha particles. The virial equation of state is model-independent, and therefore sets a benchmark for all nuclear equations of state at low densities. We calculate the second virial coefficients for nucleon-nucleon, nucleon-alpha and alpha-alpha interactions directly from the relevant binding energies and scattering phase shifts. The virial approach systematically takes into account contributions from bound nuclei and the resonant continuum, and consequently provides a framework to include strong-interaction corrections to nuclear statistical equilibrium models. The virial coefficients are used to make model-independent predictions for a variety of properties of nuclear matter over a range of densities, temperatures and compositions. Our results provide constraints on the physics of the neutrinosphere in supernovae. The resulting alpha particle concentration differs from all equations of state currently used in supernova simulations. Finally, the virial equation of state greatly improves our conceptual understanding of low-density nuclear matter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the virial equation of state in describing low-density nuclear matter, and how does it differ from other nuclear equations of state?\n\nA) It provides a more accurate description of the neutrinosphere in supernovae, but neglects strong-interaction corrections.\nB) It is model-independent and takes into account contributions from bound nuclei and the resonant continuum, making it a more comprehensive framework for nuclear matter.\nC) It is limited to high-density nuclear matter and does not account for the effects of alpha particles at low densities.\nD) It relies solely on scattering phase shifts and binding energies, but does not consider the role of neutrinos in nuclear statistical equilibrium.\n\nCorrect Answer: B) It is model-independent and takes into account contributions from bound nuclei and the resonant continuum, making it a more comprehensive framework for nuclear matter.\n\nExplanation: The correct answer highlights the key advantage of the virial equation of state, which is its model-independence and ability to account for various contributions to nuclear matter, including bound nuclei and the resonant continuum. This makes it a more comprehensive framework for describing low-density nuclear matter. The other options are incorrect because they either focus on a specific aspect of the virial equation of state (A and D) or are misleadingly accurate (C)."}, "25": {"documentation": {"title": "Rational reductions of the 2D-Toda hierarchy and mirror symmetry", "source": "Andrea Brini, Guido Carlet, Stefano Romano, Paolo Rossi", "docs_id": "1401.5725", "section": ["math-ph", "hep-th", "math.AG", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rational reductions of the 2D-Toda hierarchy and mirror symmetry. We introduce and study a two-parameter family of symmetry reductions of the two-dimensional Toda lattice hierarchy, which are characterized by a rational factorization of the Lax operator into a product of an upper diagonal and the inverse of a lower diagonal formal difference operator. They subsume and generalize several classical 1 + 1 integrable hierarchies, such as the bigraded Toda hierarchy, the Ablowitz-Ladik hierarchy and E. Frenkel's q-deformed Gelfand-Dickey hierarchy. We establish their characterization in terms of block Toeplitz matrices for the associated factorization problem, and study their Hamiltonian structure. At the dispersionless level, we show how the Takasaki-Takebe classical limit gives rise to a family of non-conformal Frobenius manifolds with flat identity. We use this to generalize the relation of the Ablowitz-Ladik hierarchy to Gromov-Witten theory by proving an analogous mirror theorem for rational reductions: in particular, we show that the dual-type Frobenius manifolds we obtain are isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds obtained from minimal resolutions of the local orbifold line."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a rational reduction of the 2D-Toda hierarchy with a rational factorization of the Lax operator into a product of an upper diagonal and the inverse of a lower diagonal formal difference operator. What is the relationship between the dual-type Frobenius manifold obtained from this reduction and the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds obtained from minimal resolutions of the local orbifold line?\n\nA) The dual-type Frobenius manifold is isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds, but only for the specific case where the rational reduction is the Ablowitz-Ladik hierarchy.\n\nB) The dual-type Frobenius manifold is isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds, but only for the specific case where the rational reduction is the bigraded Toda hierarchy.\n\nC) The dual-type Frobenius manifold is isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds, and this isomorphism holds for all rational reductions of the 2D-Toda hierarchy.\n\nD) The dual-type Frobenius manifold is not isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds, and the relationship between the two is more complex than a simple isomorphism.\n\nCorrect Answer: C) The dual-type Frobenius manifold is isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds, and this isomorphism holds for all rational reductions of the 2D-Toda hierarchy.\n\nExplanation: The correct answer is C) because the documentation states that the dual-type Frobenius manifolds obtained from the rational reductions of the 2D-Toda hierarchy are isomorphic to the equivariant quantum cohomology of a family of toric Calabi-Yau threefolds obtained from minimal resolutions of the local orbifold line. This isomorphism is stated to hold for all rational reductions, not just specific cases."}, "26": {"documentation": {"title": "System-in-the-loop Design Space Exploration for Efficient Communication\n  in Large-scale IoT-based Warehouse Systems", "source": "Robert Falkenberg and Jens Drenhaus and Benjamin Sliwa and Christian\n  Wietfeld", "docs_id": "1802.03033", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "System-in-the-loop Design Space Exploration for Efficient Communication\n  in Large-scale IoT-based Warehouse Systems. Instead of treating inventory items as static resources, future intelligent warehouses will transcend containers to Cyber Physical Systems (CPS) that actively and autonomously participate in the optimization of the logistical processes. Consequently, new challenges that are system-immanent for the massive Internet of Things (IoT) context, such as channel access in a shared communication medium, have to be addressed. In this paper, we present a multi-methodological system model that brings together testbed experiments for measuring real hardware properties and simulative evaluations for large-scale considerations. As an example case study, we will particularly focus on parametrization of the 802.15.4-based radio communication system, which has to be energy-efficient due to scarce amount of harvested energy, but avoid latencies for the maintenance of scalability of the overlaying warehouse system. The results show, that a modification of the initial backoff time can lead to both, energy and time savings in the order of 50% compared to the standard."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the System-in-the-loop Design Space Exploration for Efficient Communication in Large-scale IoT-based Warehouse Systems, what is the primary challenge that the authors aim to address in their proposed multi-methodological system model?\n\n**A)** The need for a centralized control system to manage the communication between CPS and the warehouse system.\n**B)** The optimization of energy efficiency and latency in the 802.15.4-based radio communication system.\n**C)** The integration of CPS with the warehouse system to improve inventory management.\n**D)** The development of a new communication protocol for CPS to interact with the warehouse system.\n\n**Correct Answer:** B) The optimization of energy efficiency and latency in the 802.15.4-based radio communication system.\n\n**Explanation:** The correct answer is B) because the authors specifically mention that the 802.15.4-based radio communication system has to be energy-efficient due to the scarcity of harvested energy, and that avoiding latencies is crucial for maintaining the scalability of the overlaying warehouse system. The other options are not directly related to the primary challenge addressed by the authors.\n\n**Additional explanations:**\n\n* Option A is incorrect because the authors do not mention a centralized control system as the primary challenge.\n* Option C is incorrect because while integration of CPS with the warehouse system is mentioned, it is not the primary challenge addressed by the authors.\n* Option D is incorrect because the authors do not propose a new communication protocol, but rather aim to optimize the existing 802.15.4-based radio communication system."}, "27": {"documentation": {"title": "Two-color flat-top solitonic pulses in $\\chi^{(2)}$ optical\n  microresonators via second harmonic generation", "source": "Valery E. Lobanov, Nikita M. Kondratiev, Artem E. Shitikov, Igor A.\n  Bilenko", "docs_id": "2001.03648", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-color flat-top solitonic pulses in $\\chi^{(2)}$ optical\n  microresonators via second harmonic generation. We studied numerically the generation of the coherent frequency combs at second harmonic generation in $\\chi^{(2)}$ microresonators via conventional frequency scan method. It was demonstrated for the first time that under particular conditions it is possible to generate two-color flat-top solitonic pulses, platicons, using pump amplitude modulation or controllable mode interaction approach, if the signs of the group velocity coefficients at pump frequency and its second harmonic are opposite but absolute values of these coefficients are rather close. It was revealed that platicons may be observed on both sides of the linear microresonator resonance (at positive, as well as negative pump frequency detunings). For the efficient platicon excitation, one needs simultaneous accurate matching of both microresonator free spectral ranges and resonant eigenfrequencies. Platicon generation processes were simulated numerically, excitation conditions and platicon generation domains were found for different generation methods, and the properties of generated platicons were studied for the different combinations of the medium parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the necessary conditions for the generation of two-color flat-top solitonic pulses (platicons) in \u03c7^{(2)} optical microresonators via second harmonic generation, and what are the requirements for efficient platicon excitation?\n\nA) The signs of the group velocity coefficients at pump frequency and its second harmonic must be the same, and the absolute values of these coefficients must be equal.\nB) The signs of the group velocity coefficients at pump frequency and its second harmonic must be opposite, and the absolute values of these coefficients must be close.\nC) The pump frequency and its second harmonic must have the same free spectral range and resonant eigenfrequency.\nD) The pump amplitude must be modulated at a frequency that is a multiple of the microresonator's resonant frequency.\n\nCorrect Answer: B) The signs of the group velocity coefficients at pump frequency and its second harmonic must be opposite, and the absolute values of these coefficients must be close.\n\nExplanation: According to the documentation, the necessary conditions for the generation of platicons are that the signs of the group velocity coefficients at pump frequency and its second harmonic must be opposite, but the absolute values of these coefficients must be close. This is stated in the first sentence of the documentation: \"It was demonstrated for the first time that under particular conditions it is possible to generate two-color flat-top solitonic pulses, platicons, using pump amplitude modulation or controllable mode interaction approach, if the signs of the group velocity coefficients at pump frequency and its second harmonic are opposite but absolute values of these coefficients are rather close.\""}, "28": {"documentation": {"title": "Photophysics of GaN single photon sources in the visible spectral range", "source": "Amanuel M. Berhane, Kwang-Yong Jeong, Carlo Bradac, Michael Walsh,\n  Dirk Englund, Milos Toth, and Igor Aharonovich", "docs_id": "1708.09161", "section": ["quant-ph", "cond-mat.mtrl-sci", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photophysics of GaN single photon sources in the visible spectral range. In this work, we present a detailed photophysical analysis of recently-discovered optically stable, single photon emitters (SPEs) in Gallium Nitride (GaN). Temperature-resolved photoluminescence measurements reveal that the emission lines at 4 K are three orders of magnitude broader than the transform-limited widths expected from excited state lifetime measurements. The broadening is ascribed to ultra-fast spectral diffusion. Continuing the photophysics study on several emitters at room temperature (RT), a maximum average brightness of ~427 kCounts/s is measured. Furthermore, by determining the decay rates of emitters undergoing three-level optical transitions, radiative and non-radiative lifetimes are calculated at RT. Finally, polarization measurements from 14 emitters are used to determine visibility as well as dipole orientation of defect systems within the GaN crystal. Our results underpin some of the fundamental properties of SPE in GaN both at cryogenic and RT, and define the benchmark for future work in GaN-based single-photon technologies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary cause of the broadening of emission lines in GaN single photon emitters observed at 4 K, as revealed by temperature-resolved photoluminescence measurements?\n\nA) Ultra-fast spectral diffusion due to phonon-assisted relaxation\nB) Ultra-fast spectral diffusion due to carrier-phonon interactions\nC) Ultra-fast spectral diffusion due to exciton-phonon interactions\nD) Ultra-fast spectral diffusion due to exciton-exciton interactions\n\nCorrect Answer: B) Ultra-fast spectral diffusion due to carrier-phonon interactions\n\nExplanation: The correct answer is B) Ultra-fast spectral diffusion due to carrier-phonon interactions. The documentation states that the broadening is ascribed to ultra-fast spectral diffusion, and further explains that this is due to carrier-phonon interactions. This is a key finding in the study, and understanding this mechanism is crucial for understanding the photophysics of GaN single photon emitters.\n\nCandidate A is incorrect because it mentions phonon-assisted relaxation, which is not explicitly mentioned in the documentation as the primary cause of broadening. Candidate C is also incorrect because it mentions exciton-phonon interactions, but the documentation does not provide evidence to support this as the primary cause. Candidate D is incorrect because it mentions exciton-exciton interactions, which are not mentioned at all in the documentation."}, "29": {"documentation": {"title": "Stable prediction with radiomics data", "source": "Carel F.W. Peeters, Caroline \\\"Ubelh\\\"or, Steven W. Mes, Roland\n  Martens, Thomas Koopman, Pim de Graaf, Floris H.P. van Velden, Ronald\n  Boellaard, Jonas A. Castelijns, Dennis E. te Beest, Martijn W. Heymans, Mark\n  A. van de Wiel", "docs_id": "1903.11696", "section": ["stat.ML", "cs.LG", "eess.IV", "q-bio.QM", "stat.AP", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stable prediction with radiomics data. Motivation: Radiomics refers to the high-throughput mining of quantitative features from radiographic images. It is a promising field in that it may provide a non-invasive solution for screening and classification. Standard machine learning classification and feature selection techniques, however, tend to display inferior performance in terms of (the stability of) predictive performance. This is due to the heavy multicollinearity present in radiomic data. We set out to provide an easy-to-use approach that deals with this problem. Results: We developed a four-step approach that projects the original high-dimensional feature space onto a lower-dimensional latent-feature space, while retaining most of the covariation in the data. It consists of (i) penalized maximum likelihood estimation of a redundancy filtered correlation matrix. The resulting matrix (ii) is the input for a maximum likelihood factor analysis procedure. This two-stage maximum-likelihood approach can be used to (iii) produce a compact set of stable features that (iv) can be directly used in any (regression-based) classifier or predictor. It outperforms other classification (and feature selection) techniques in both external and internal validation settings regarding survival in squamous cell cancers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge faced by standard machine learning classification and feature selection techniques when dealing with radiomics data, and how does the proposed approach address this issue?\n\n**A)** Radiomics data is too noisy, and standard techniques are unable to handle it. The proposed approach uses noise reduction techniques to improve performance.\n\n**B)** Radiomics data is highly correlated, leading to multicollinearity, which hinders the stability of predictive performance. The proposed approach uses a two-stage maximum-likelihood procedure to reduce multicollinearity.\n\n**C)** Radiomics data is too sparse, and standard techniques are unable to capture its underlying patterns. The proposed approach uses dimensionality reduction techniques to extract meaningful features.\n\n**D)** Radiomics data is too complex, and standard techniques are unable to model its relationships. The proposed approach uses a combination of feature selection and dimensionality reduction techniques to improve performance.\n\n**Correct Answer:** B) Radiomics data is highly correlated, leading to multicollinearity, which hinders the stability of predictive performance. The proposed approach uses a two-stage maximum-likelihood procedure to reduce multicollinearity.\n\n**Explanation:** The question requires the test-taker to understand the challenges faced by standard machine learning techniques when dealing with radiomics data, specifically the issue of multicollinearity. The correct answer, B, highlights the correct solution proposed in the original paper, which uses a two-stage maximum-likelihood procedure to reduce multicollinearity. The other options are incorrect because they do not accurately describe the challenge faced by standard techniques or the proposed approach."}, "30": {"documentation": {"title": "A polymer model for the quantitative reconstruction of 3d chromosome\n  architecture from Hi-C and GAM data", "source": "Guillaume Le Treut, Fran\\c{c}ois K\\'ep\\`es, Henri Orland", "docs_id": "1802.04488", "section": ["q-bio.QM", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A polymer model for the quantitative reconstruction of 3d chromosome\n  architecture from Hi-C and GAM data. It is widely believed that the folding of the chromosome in the nucleus has a major effect on genetic expression. For example co-regulated genes in several species have been shown to colocalize in space despite being far away on the DNA sequence. In this manuscript, we present a new method to model the three-dimensional structure of the chromosome in live cells, based on DNA-DNA interactions measured in high-throughput chromosome conformation capture experiments (Hi-C) and genome architecture mapping experiments (GAM). Our approach incorporates a polymer model, and directly uses the contact probabilities measured in Hi-C and GAM experiments rather than estimates of average distances between genomic loci. Specifically, we model the chromosome as a Gaussian polymer with harmonic interactions and extract the coupling coefficients best reproducing the experimental contact probabilities. In contrast to existing methods, we give an exact expression of the contact probabilities at thermodynamic equilibrium. The Gaussian effective model (GEM) reconstructed with our method reproduces experimental contacts with high accuracy. We also show how Brownian Dynamics simulations of our reconstructed GEM can be used to study chromatin organization, and possibly give some clue about its dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed polymer model for reconstructing 3D chromosome architecture from Hi-C and GAM data?\n\nA) It uses a non-Gaussian distribution to model chromatin interactions\nB) It incorporates a more complex interaction energy function to better capture long-range interactions\nC) It directly uses contact probabilities measured in Hi-C and GAM experiments, rather than estimates of average distances between genomic loci\nD) It relies solely on computational simulations to infer chromatin organization\n\n**Correct Answer:** C) It directly uses contact probabilities measured in Hi-C and GAM experiments, rather than estimates of average distances between genomic loci\n\n**Explanation:** The correct answer is C) because the proposed method directly uses the contact probabilities measured in Hi-C and GAM experiments, rather than relying on estimates of average distances between genomic loci. This is a key advantage of the method, as it allows for a more accurate representation of chromatin interactions. The other options are incorrect because they do not accurately reflect the primary advantage of the proposed method. Option A is incorrect because the method uses a Gaussian distribution, not a non-Gaussian distribution. Option B is incorrect because the method does not incorporate a more complex interaction energy function. Option D is incorrect because the method relies on experimental data, not computational simulations."}, "31": {"documentation": {"title": "Phase transitions in neutron stars and their links to gravitational\n  waves", "source": "Milva G. Orsaria, Germ\\'an Malfatti, Mauro Mariani, Ignacio F.\n  Ranea-Sandoval, Federico Garc\\'ia, William M. Spinella, Gustavo A. Contrera,\n  Germ\\'an Lugones, Fridolin Weber", "docs_id": "1907.04654", "section": ["astro-ph.HE", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase transitions in neutron stars and their links to gravitational\n  waves. The recent direct observation of gravitational wave event $GW170817$ and its $GRB170817A$ signal has opened up a new window to study neutron stars and heralds a new era of Astronomy referred to as the Multimessenger Astronomy. Both gravitational and electromagnetic waves from a single astrophysical source have been detected for the first time. This combined detection offers an unprecedented opportunity to place constraints on the neutron star matter equation of state. The existence of a possible hadron-quark phase transition in the central regions of neutron stars is associated with the appearance of g-modes, which are extremely important as they could signal the presence of a pure quark matter core in the centers of neutron stars. Observations of g-modes with frequencies between 1 kHz and 1.5 kHz could be interpreted as evidence of a sharp hadron-quark phase transition in the cores of neutron stars. In this article, we shall review the description of the dense matter composing neutron stars, the determination of the equation of state of such matter, and the constraints imposed by astrophysical observations of these fascinating compact objects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary consequence of the direct observation of gravitational wave event GW170817 and its GRB170817A signal, in terms of our understanding of neutron star matter?\n\nA) The existence of a possible hadron-quark phase transition in the central regions of neutron stars is ruled out.\nB) The observation of g-modes with frequencies between 1 kHz and 1.5 kHz provides strong evidence for a sharp hadron-quark phase transition in the cores of neutron stars.\nC) The detection of gravitational waves and electromagnetic signals from a single astrophysical source allows for a more precise determination of the neutron star equation of state.\nD) The observation of GW170817 and GRB170817A signals does not provide any new information about the neutron star matter equation of state.\n\nCorrect Answer: C) The detection of gravitational waves and electromagnetic signals from a single astrophysical source allows for a more precise determination of the neutron star equation of state.\n\nExplanation: The correct answer is C) because the observation of GW170817 and GRB170817A signals provides a unique opportunity to study neutron stars using both gravitational and electromagnetic waves. This multimessenger approach allows for a more precise determination of the neutron star equation of state, which is a key parameter in understanding the behavior of neutron star matter. The other options are incorrect because they either misinterpret the significance of the observation (A and D) or provide an incomplete picture of the consequences of the observation (B)."}, "32": {"documentation": {"title": "The tidal stripping of satellites", "source": "J. I. Read, M. I. Wilkinson, N. W. Evans, G. Gilmore and Jan T. Kleyna", "docs_id": "astro-ph/0506687", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The tidal stripping of satellites. We present an improved analytic calculation for the tidal radius of satellites and test our results against N-body simulations. The tidal radius in general depends upon four factors: the potential of the host galaxy, the potential of the satellite, the orbit of the satellite and {\\it the orbit of the star within the satellite}. We demonstrate that this last point is critical and suggest using {\\it three tidal radii} to cover the range of orbits of stars within the satellite. In this way we show explicitly that prograde star orbits will be more easily stripped than radial orbits; while radial orbits are more easily stripped than retrograde ones. This result has previously been established by several authors numerically, but can now be understood analytically. For point mass, power-law (which includes the isothermal sphere), and a restricted class of split power law potentials our solution is fully analytic. For more general potentials, we provide an equation which may be rapidly solved numerically. Over short times ($\\simlt 1-2$ Gyrs $\\sim 1$ satellite orbit), we find excellent agreement between our analytic and numerical models. Over longer times, star orbits within the satellite are transformed by the tidal field of the host galaxy. In a Hubble time, this causes a convergence of the three limiting tidal radii towards the prograde stripping radius. Beyond the prograde stripping radius, the velocity dispersion will be tangentially anisotropic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The tidal stripping of satellites is a complex process that depends on various factors, including the potential of the host galaxy, the potential of the satellite, the orbit of the satellite, and the orbit of the star within the satellite. According to the paper, what is the critical factor that determines the ease of stripping of star orbits within a satellite, and how does it compare to radial and retrograde orbits?\n\n**A)** The potential of the host galaxy is the critical factor, and it has no effect on the ease of stripping of star orbits.\n\n**B)** The orbit of the star within the satellite is the critical factor, and it is more easily stripped than radial orbits.\n\n**C)** The potential of the satellite is the critical factor, and it is more easily stripped than retrograde orbits.\n\n**D)** The orbit of the satellite is the critical factor, and it is more easily stripped than radial orbits.\n\n**Correct Answer:** B) The orbit of the star within the satellite is the critical factor, and it is more easily stripped than radial orbits.\n\n**Explanation:** According to the paper, the orbit of the star within the satellite is the critical factor that determines the ease of stripping of star orbits. The paper states that \"prograde star orbits will be more easily stripped than radial orbits; while radial orbits are more easily stripped than retrograde ones.\" This indicates that the correct answer is B, which states that the orbit of the star within the satellite is the critical factor, and it is more easily stripped than radial orbits."}, "33": {"documentation": {"title": "Tests for Group-Specific Heterogeneity in High-Dimensional Factor Models", "source": "Antoine Djogbenou and Razvan Sufana", "docs_id": "2109.09049", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tests for Group-Specific Heterogeneity in High-Dimensional Factor Models. Standard high-dimensional factor models assume that the comovements in a large set of variables could be modeled using a small number of latent factors that affect all variables. In many relevant applications in economics and finance, heterogenous comovements specific to some known groups of variables naturally arise, and reflect distinct cyclical movements within those groups. This paper develops two new statistical tests that can be used to investigate whether there is evidence supporting group-specific heterogeneity in the data. The first test statistic is designed for the alternative hypothesis of group-specific heterogeneity appearing in at least one pair of groups; the second is for the alternative of group-specific heterogeneity appearing in all pairs of groups. We show that the second moment of factor loadings changes across groups when heterogeneity is present, and use this feature to establish the theoretical validity of the tests. We also propose and prove the validity of a permutation approach for approximating the asymptotic distributions of the two test statistics. The simulations and the empirical financial application indicate that the proposed tests are useful for detecting group-specific heterogeneity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question**\n\nConsider a high-dimensional factor model with a large set of variables, where the comovements among these variables can be modeled using a small number of latent factors. Suppose we have two groups of variables, Group A and Group B, and we suspect that there may be group-specific heterogeneity in the data. We want to test the null hypothesis that there is no group-specific heterogeneity against the alternative hypothesis that there is group-specific heterogeneity in at least one pair of groups.\n\n**A)** The test statistic for this problem would be based on the sum of squared factor loadings across all pairs of groups, and would be used to test the null hypothesis that the factor loadings are equal across all pairs of groups.\n\n**B)** The test statistic for this problem would be based on the second moment of factor loadings, and would be used to test the null hypothesis that the factor loadings are equal across all pairs of groups.\n\n**C)** The test statistic for this problem would be based on the correlation between factor loadings across all pairs of groups, and would be used to test the null hypothesis that the factor loadings are uncorrelated across all pairs of groups.\n\n**D)** The test statistic for this problem would be based on the eigenvalues of the factor loading matrix, and would be used to test the null hypothesis that the factor loadings are equal across all pairs of groups.\n\n**Correct Answer:** B) The test statistic for this problem would be based on the second moment of factor loadings, and would be used to test the null hypothesis that the factor loadings are equal across all pairs of groups.\n\n**Explanation**\n\nThe correct answer is B) because the paper states that the second moment of factor loadings changes across groups when heterogeneity is present, and this feature is used to establish the theoretical validity of the tests. The test statistic is designed to detect group-specific heterogeneity in the data, and the second moment of factor loadings is a key feature of the model that is used to achieve this.\n\nThe other options are incorrect because:\n\n* A) The sum of squared factor loadings is not the correct test statistic for this problem.\n* C) The correlation between factor loadings is not the correct test statistic for this problem.\n* D) The eigenvalues of the factor loading matrix are not the correct test statistic for this problem.\n\nThis question requires the test-taker to have a good understanding of the high-dimensional factor model and the concept of group-specific heterogeneity, as well as the ability to apply theoretical knowledge to a specific problem. The correct answer requires the test-taker to recognize the key feature of the model that is used to establish the theoretical validity of the tests, and to apply this knowledge to the problem at hand."}, "34": {"documentation": {"title": "Perceived Performance of Webpages In the Wild: Insights from Large-scale\n  Crowdsourcing of Above-the-Fold QoE", "source": "Qingzhu Gao, Prasenjit Dey, and Parvez Ahammad", "docs_id": "1704.01220", "section": ["cs.NI", "cs.HC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Perceived Performance of Webpages In the Wild: Insights from Large-scale\n  Crowdsourcing of Above-the-Fold QoE. Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the \"Speed\" of a page. In this paper we present \"SpeedPerception\", a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process. In Phase-1 of our \"SpeedPerception\" study using Internet Retailer Top 500 (IR 500) websites (https://github.com/pahammad/speedperception), we found that commonly used navigation metrics such as \"onLoad\" and \"Time To First Byte (TTFB)\" fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with $87 \\pm 2\\%$ accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its \"visualComplete\" event."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of commonly used navigation metrics such as \"onLoad\" and \"Time To First Byte (TTFB)\" in representing human perception of webpage loading performance?\n\n**A)** They are too sensitive to network conditions.\n**B)** They fail to capture the subjective experience of end-users.\n**C)** They are not scalable for large-scale web applications.\n**D)** They are not accurate enough to distinguish between slow and fast webpage loading.\n\n**Correct Answer:** B) They fail to capture the subjective experience of end-users.\n\n**Explanation:** According to the paper, the authors found that commonly used navigation metrics such as \"onLoad\" and \"Time To First Byte (TTFB)\" fail to represent majority human perception when comparing the speed of two webpages, with less than 60% match. This suggests that these metrics are not sufficient to capture the subjective experience of end-users, which is the primary limitation of these metrics."}, "35": {"documentation": {"title": "Effects of introduction of new resources and fragmentation of existing\n  resources on limiting wealth distribution in asset exchange models", "source": "M. Ali Saif and Prashant M. Gade", "docs_id": "0902.2070", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of introduction of new resources and fragmentation of existing\n  resources on limiting wealth distribution in asset exchange models. Pareto law, which states that wealth distribution in societies have a power-law tail, has been a subject of intensive investigations in statistical physics community. Several models have been employed to explain this behavior. However, most of the agent based models assume the conservation of number of agents and wealth. Both these assumptions are unrealistic. In this paper, we study the limiting wealth distribution when one or both of these assumptions are not valid. Given the universality of the law, we have tried to study the wealth distribution from the asset exchange models point of view. We consider models in which a) new agents enter the market at constant rate b) richer agents fragment with higher probability introducing newer agents in the system c) both fragmentation and entry of new agents is taking place. While models a) and c) do not conserve total wealth or number of agents, model b) conserves total wealth. All these models lead to a power-law tail in the wealth distribution pointing to the possibility that more generalized asset exchange models could help us to explain emergence of power-law tail in wealth distribution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of asset exchange models, what is the primary assumption that is being challenged in this paper, and how does it relate to the universality of the Pareto law?\n\nA) The assumption of constant wealth distribution is being challenged, leading to a power-law tail in the wealth distribution.\nB) The assumption of constant number of agents is being challenged, leading to a power-law tail in the wealth distribution.\nC) The assumption of constant total wealth is being challenged, leading to a power-law tail in the wealth distribution.\nD) The assumption of constant agent fragmentation is being challenged, leading to a power-law tail in the wealth distribution.\n\n**Correct Answer:** C) The assumption of constant total wealth is being challenged, leading to a power-law tail in the wealth distribution.\n\n**Explanation:** The paper states that models a) and c) do not conserve total wealth or number of agents, while model b) conserves total wealth. This suggests that the primary assumption being challenged is the conservation of total wealth, which is a key factor in the emergence of the Pareto law. The correct answer, C, highlights this challenge and its relationship to the universality of the Pareto law."}, "36": {"documentation": {"title": "Two Proton Emission in the Hyperspheric Harmonics Approach", "source": "Ivan Mukha (Technische Universitat, Darmstadt, Germany)", "docs_id": "nucl-ex/9911009", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two Proton Emission in the Hyperspheric Harmonics Approach. Nuclear decays into three-particle channels are considered in a few-body approach of hyperspherical harmonics with emphasis on simultaneous, or direct, emission of two protons. General conditions of direct decays are described and their main features, being experimentally established in decays of light nuclei, are reported. The analysis method based on an expansion of decay amplitude into a series of hyperspherical harmonics is reviewed. The basis of hyperspherical harmonics functions is a generalisation of the spherical function basis in three-body systems. The method is tested on analysis of the direct 2p-decay of Be^6 where the three-body components in the nuclear structure of Be^6 have been studied. In particular, the observed strong proton-proton correlations are treated as a manifestation of a specific three-body quantum effect: the kinematic focusing of fragments over momenta and in space. The hyperspherical harmonics method is applied for the predictions of proton-proton correlations and life-time estimates of the nuclei Mg^19, Ca^34 and Ni^48 - candidates for two-proton radioactivity. Each direct 2p-decay should result in a set of peaks in the E_(p-p) spectrum whose number and positions depend on the structure of initial nucleus, opposite to the diproton model, predicting the He^2 emission with one peak at E_(p-p)~0 in all cases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary basis of the hyperspherical harmonics method used to analyze nuclear decays into three-particle channels, and how does it differ from the diproton model?\n\nA) The hyperspherical harmonics method is based on an expansion of the decay amplitude into a series of spherical functions, which is similar to the diproton model.\nB) The hyperspherical harmonics method is based on a generalisation of the spherical function basis in three-body systems, which is distinct from the diproton model.\nC) The hyperspherical harmonics method is based on an expansion of the decay amplitude into a series of hyperspherical harmonics functions, which is similar to the diproton model.\nD) The hyperspherical harmonics method is based on a generalisation of the hyperspherical harmonics functions, which is distinct from the diproton model.\n\nCorrect Answer: B) The hyperspherical harmonics method is based on a generalisation of the spherical function basis in three-body systems, which is distinct from the diproton model.\n\nExplanation: The correct answer is B) because the documentation states that the hyperspherical harmonics method is based on a generalisation of the spherical function basis in three-body systems, which is distinct from the diproton model. The other options are incorrect because they either describe the hyperspherical harmonics method as similar to the diproton model (A and C) or as a generalisation of the hyperspherical harmonics functions (D), which is not the case."}, "37": {"documentation": {"title": "Modeling continuous-time stochastic processes using $\\mathcal{N}$-Curve\n  mixtures", "source": "Ronny Hug, Wolfgang H\\\"ubner, and Michael Arens", "docs_id": "1908.04030", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modeling continuous-time stochastic processes using $\\mathcal{N}$-Curve\n  mixtures. Representations of sequential data are commonly based on the assumption that observed sequences are realizations of an unknown underlying stochastic process, where the learning problem includes determination of the model parameters. In this context the model must be able to capture the multi-modal nature of the data, without blurring between modes. This property is essential for applications like trajectory prediction or human motion modeling. Towards this end, a neural network model for continuous-time stochastic processes usable for sequence prediction is proposed. The model is based on Mixture Density Networks using B\\'ezier curves with Gaussian random variables as control points (abbrev.: $\\mathcal{N}$-Curves). Key advantages of the model include the ability of generating smooth multi-mode predictions in a single inference step which reduces the need for Monte Carlo simulation, as required in many multi-step prediction models, based on state-of-the-art neural networks. Essential properties of the proposed approach are illustrated by several toy examples and the task of multi-step sequence prediction. Further, the model performance is evaluated on two real world use-cases, i.e. human trajectory prediction and human motion modeling, outperforming different state-of-the-art models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key advantage of the proposed $\\mathcal{N}$-Curve model in terms of sequence prediction, and how does it differ from other neural network models?\n\n**A)** The $\\mathcal{N}$-Curve model can only generate single-mode predictions, whereas other models can generate multi-mode predictions. This requires more Monte Carlo simulation to achieve accurate results.\n\n**B)** The $\\mathcal{N}$-Curve model can generate smooth multi-mode predictions in a single inference step, reducing the need for Monte Carlo simulation and improving prediction accuracy.\n\n**C)** The $\\mathcal{N}$-Curve model is limited to predicting sequences of fixed length, whereas other models can predict sequences of varying lengths.\n\n**D)** The $\\mathcal{N}$-Curve model requires more computational resources to train than other models, resulting in slower inference times.\n\n**Correct Answer:** B) The $\\mathcal{N}$-Curve model can generate smooth multi-mode predictions in a single inference step, reducing the need for Monte Carlo simulation and improving prediction accuracy.\n\n**Explanation:** The correct answer is B) because the $\\mathcal{N}$-Curve model is designed to generate smooth multi-mode predictions in a single inference step, which reduces the need for Monte Carlo simulation and improves prediction accuracy. This is a key advantage of the model, as mentioned in the documentation. The other options are incorrect because they do not accurately reflect the benefits of the $\\mathcal{N}$-Curve model. Option A is incorrect because the model can generate multi-mode predictions, not single-mode predictions. Option C is incorrect because the model can predict sequences of varying lengths, not fixed lengths. Option D is incorrect because the model does not require more computational resources to train than other models."}, "38": {"documentation": {"title": "Characterizing and Optimizing EDA Flows for the Cloud", "source": "Abdelrahman Hosny and Sherief Reda", "docs_id": "2102.10800", "section": ["cs.DC", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterizing and Optimizing EDA Flows for the Cloud. Cloud computing accelerates design space exploration in logic synthesis, and parameter tuning in physical design. However, deploying EDA jobs on the cloud requires EDA teams to deeply understand the characteristics of their jobs in cloud environments. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we formulate the problem of migrating EDA jobs to the cloud. First, we characterize the performance of four main EDA applications, namely: synthesis, placement, routing and static timing analysis. We show that different EDA jobs require different machine configurations. Second, using observations from our characterization, we propose a novel model based on Graph Convolutional Networks to predict the total runtime of a given application on different machine configurations. Our model achieves a prediction accuracy of 87%. Third, we develop a new formulation for optimizing cloud deployments in order to reduce deployment costs while meeting deadline constraints. We present a pseudo-polynomial optimal solution using a multi-choice knapsack mapping that reduces costs by 35.29%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Characterizing and Optimizing EDA Flows for the Cloud\n\n**A)** The authors of the paper propose a novel model based on Graph Convolutional Networks to predict the total runtime of a given EDA application on different machine configurations, achieving a prediction accuracy of 87%.\n\n**B)** The paper formulates the problem of migrating EDA jobs to the cloud, but does not provide any information on the characteristics of EDA jobs in cloud environments.\n\n**C)** The authors develop a new formulation for optimizing cloud deployments to reduce deployment costs while meeting deadline constraints, achieving a cost reduction of 35.29% using a multi-choice knapsack mapping.\n\n**D)** The paper only characterizes the performance of four main EDA applications, namely synthesis, placement, routing, and static timing analysis, without providing any information on the machine configurations required for each application.\n\n**Correct Answer:** C) The authors develop a new formulation for optimizing cloud deployments to reduce deployment costs while meeting deadline constraints, achieving a cost reduction of 35.29% using a multi-choice knapsack mapping.\n\n**Explanation:** This question requires the test-taker to carefully read and understand the paper's content. The correct answer is based on the third point mentioned in the paper, which states that the authors develop a new formulation for optimizing cloud deployments to reduce deployment costs while meeting deadline constraints. The test-taker needs to identify this point and recognize that it is the correct answer. The other options are incorrect because they either summarize the paper's content too broadly (A), are not supported by the paper (B), or are only partially correct (D)."}, "39": {"documentation": {"title": "Thermodynamic limit of particle-hole form factors in the massless XXZ\n  Heisenberg chain", "source": "N. Kitanine, K. K. Kozlowski, J. M. Maillet, N. A. Slavnov and V.\n  Terras", "docs_id": "1003.4557", "section": ["math-ph", "cond-mat.stat-mech", "hep-th", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermodynamic limit of particle-hole form factors in the massless XXZ\n  Heisenberg chain. We study the thermodynamic limit of the particle-hole form factors of the XXZ Heisenberg chain in the massless regime. We show that, in this limit, such form factors decrease as an explicitly computed power-law in the system-size. Moreover, the corresponding amplitudes can be obtained as a product of a \"smooth\" and a \"discrete\" part: the former depends continuously on the rapidities of the particles and holes, whereas the latter has an additional explicit dependence on the set of integer numbers that label each excited state in the associated logarithmic Bethe equations. We also show that special form factors corresponding to zero-energy excitations lying on the Fermi surface decrease as a power-law in the system size with the same critical exponents as in the long-distance asymptotic behavior of the related two-point correlation functions. The methods we develop in this article are rather general and can be applied to other massless integrable models associated to the six-vertex R-matrix and having determinant representations for their form factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the thermodynamic limit of the particle-hole form factors of the XXZ Heisenberg chain and the long-distance asymptotic behavior of the related two-point correlation functions, and how does this relationship manifest in the system-size dependence of the form factors?\n\nA) The thermodynamic limit of the particle-hole form factors decreases as a power-law in the system-size, with the same critical exponents as the long-distance asymptotic behavior of the two-point correlation functions, and the corresponding amplitudes can be obtained as a product of a \"smooth\" and a \"discrete\" part.\n\nB) The thermodynamic limit of the particle-hole form factors decreases as a power-law in the system-size, but the long-distance asymptotic behavior of the two-point correlation functions is not directly related to the form factors.\n\nC) The thermodynamic limit of the particle-hole form factors does not decrease as a power-law in the system-size, and the long-distance asymptotic behavior of the two-point correlation functions is not related to the form factors.\n\nD) The thermodynamic limit of the particle-hole form factors decreases as a power-law in the system-size, but the long-distance asymptotic behavior of the two-point correlation functions is only related to the special form factors corresponding to zero-energy excitations lying on the Fermi surface.\n\nCorrect Answer: A) The thermodynamic limit of the particle-hole form factors decreases as a power-law in the system-size, with the same critical exponents as the long-distance asymptotic behavior of the two-point correlation functions, and the corresponding amplitudes can be obtained as a product of a \"smooth\" and a \"discrete\" part.\n\nExplanation: The correct answer is A) because the documentation states that the thermodynamic limit of the particle-hole form factors decreases as a power-law in the system-size, and that this decrease is accompanied by the same critical exponents as the long-distance asymptotic behavior of the two-point correlation functions. Additionally, the documentation mentions that the corresponding amplitudes can be obtained as a product of a \"smooth\" and a \"discrete\" part, which is consistent with answer A)."}, "40": {"documentation": {"title": "On a discrete Hill's statistical process based on sum-product statistics\n  and its finite-dimensional asymptotic theory", "source": "Gane Samb Lo", "docs_id": "1203.0685", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On a discrete Hill's statistical process based on sum-product statistics\n  and its finite-dimensional asymptotic theory. The following class of sum-product statistics T_n(p)=\\frac{1}{k}\\sum_{h=1}^p \\sum_{(s_1...s_h)\\in P(p,h)} \\sum_{i_1=l+1}^{i_0} ... \\sum_{i_h=l+1}^{i_{h-1}} i_h \\prod_{i=i_1}^{i_h} \\frac{(Y_{n-i+1,n}-Y_{n-i,n})^{s_i}}{s_i!} (where $l,$ $k=i_{0}$ and n are positive integers, $0<l<k<n,$ $P(p,h)$ is the set of all ordered parititions of $\\ p>0$ into $\\ h$ positive integers and $Y_{1,n}\\leq ...\\leq Y_{n,n}$ are the order statistics based on a sequence of independent random variables $Y_{1},$ $Y_{2},...$with underlying distribution $\\mathbb{P}(Y\\leq y)=G(Y)=F(e^{y})$), is introduced. For each p, $T_{n}(p)^{-1/p}$ is an estimator of the index of a distribution whose upper tail varies regularly at infinity. \\ This family generalizes the so called Hill statistic and the Dekkers-Einmahl-De Haan one. We study the limiting laws of the process ${T_{n}(p),1\\leq p<\\infty}$ and completely describe the covariance function of the Gaussian limiting process with the help of combinatorial techniques. Many results available for Hill's statistic regarding asymptotic normality and laws of the iterated logarithm are extended to each margin $T_{n}(p,k)$, for $p$ fixed, and for any distribution function lying in the extremal domain. In the process, we obtain special classes of numbers related to those of paths joining the opposite coins within a parallelogram."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Let $T_n(p)$ be the sum-product statistic defined in the given Arxiv documentation. Suppose we have a sequence of independent random variables $Y_1, Y_2, \\ldots$ with underlying distribution $\\mathbb{P}(Y \\leq y) = G(Y) = F(e^y)$, where $F$ is a cumulative distribution function.\n\n**Part A:** Show that for each fixed $p$, the process $\\{T_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$, where $Y_{i,j}$ are the order statistics of the sequence $Y_1, Y_2, \\ldots$.\n\n**Part B:** Let $S_n(p) = \\frac{1}{k} \\sum_{h=1}^p \\sum_{(s_1, \\ldots, s_h) \\in P(p, h)} \\sum_{i_1=l+1}^{i_0} \\ldots \\sum_{i_h=l+1}^{i_{h-1}} i_h \\prod_{i=i_1}^{i_h} \\frac{(Y_{n-i+1,n} - Y_{n-i,n})^{s_i}}{s_i!}$. Show that for each fixed $p$, the process $\\{S_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Part C:** Let $T_n(p,k)$ be the marginal statistic of $T_n(p)$, i.e., $T_n(p,k) = T_n(p)_{k,k}$. Show that for each fixed $p$, the process $\\{T_n(p,k), 1 \\leq k < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Part D:** Let $F$ be a cumulative distribution function in the extremal domain. Show that for each fixed $p$, the process $\\{T_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Correct Answer:** D) Let $F$ be a cumulative distribution function in the extremal domain. Show that for each fixed $p$, the process $\\{T_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Explanation:** The correct answer is D) because the problem statement specifically mentions that the process $\\{T_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$ for a cumulative distribution function $F$ in the extremal domain. The other options are incorrect because they do not specify the condition on $F$.\n\n**Candidate A:** The process $\\{T_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Candidate B:** The process $\\{S_n(p), 1 \\leq p < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\n**Candidate C:** The process $\\{T_n(p,k), 1 \\leq k < \\infty\\}$ converges in distribution to a Gaussian process with covariance function $\\gamma(h, k) = \\frac{1}{k} \\sum_{i=1}^k \\sum_{j=1}^h \\mathbb{E}[Y_{i,j} Y_{j,k}]$.\n\nNote: The other options are incorrect because they do not specify the condition on $F$ or do not provide the correct covariance function."}, "41": {"documentation": {"title": "On kernel smoothing for extremal quantile regression", "source": "Abdelaati Daouia, Laurent Gardes, St\\'ephane Girard", "docs_id": "1312.5123", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On kernel smoothing for extremal quantile regression. Nonparametric regression quantiles obtained by inverting a kernel estimator of the conditional distribution of the response are long established in statistics. Attention has been, however, restricted to ordinary quantiles staying away from the tails of the conditional distribution. The purpose of this paper is to extend their asymptotic theory far enough into the tails. We focus on extremal quantile regression estimators of a response variable given a vector of covariates in the general setting, whether the conditional extreme-value index is positive, negative, or zero. Specifically, we elucidate their limit distributions when they are located in the range of the data or near and even beyond the sample boundary, under technical conditions that link the speed of convergence of their (intermediate or extreme) order with the oscillations of the quantile function and a von-Mises property of the conditional distribution. A simulation experiment and an illustration on real data were presented. The real data are the American electric data where the estimation of conditional extremes is found to be of genuine interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a nonparametric regression quantile regression estimator of the conditional distribution of a response variable given a vector of covariates. Suppose the conditional extreme-value index is negative, and the data are located in the range of the data. What is the limiting distribution of the estimator under the von-Mises property of the conditional distribution, and what are the technical conditions that link the speed of convergence of the estimator with the oscillations of the quantile function?\n\n**A)** The limiting distribution is a normal distribution with mean equal to the true quantile function and variance equal to the inverse of the Fisher information matrix. The technical conditions are that the speed of convergence of the estimator is at least as fast as the rate of convergence of the quantile function, and the oscillations of the quantile function are bounded.\n\n**B)** The limiting distribution is a Student's t-distribution with degrees of freedom equal to the sample size minus one. The technical conditions are that the speed of convergence of the estimator is at least as fast as the rate of convergence of the quantile function, and the oscillations of the quantile function are unbounded.\n\n**C)** The limiting distribution is a mixture of normal distributions with weights equal to the probabilities of the extreme-value index being positive, negative, or zero. The technical conditions are that the speed of convergence of the estimator is at least as fast as the rate of convergence of the quantile function, and the oscillations of the quantile function are bounded.\n\n**D)** The limiting distribution is a non-parametric empirical process with a kernel function that is a function of the quantile function. The technical conditions are that the speed of convergence of the estimator is at least as fast as the rate of convergence of the quantile function, and the oscillations of the quantile function are unbounded.\n\n**Correct Answer:** A) The limiting distribution is a normal distribution with mean equal to the true quantile function and variance equal to the inverse of the Fisher information matrix. The technical conditions are that the speed of convergence of the estimator is at least as fast as the rate of convergence of the quantile function, and the oscillations of the quantile function are bounded.\n\n**Explanation:** The correct answer is based on the asymptotic theory of nonparametric regression quantile regression estimators, which is discussed in the provided documentation. The limiting distribution of the estimator is a normal distribution with mean equal to the true quantile function and variance equal to the inverse of the Fisher information matrix. The technical conditions that link the speed of convergence of the estimator with the oscillations of the quantile function and a von-Mises property of the conditional distribution are also discussed in the documentation."}, "42": {"documentation": {"title": "Algebraic Bethe ansatz method for the exact calculation of energy\n  spectra and form factors: applications to models of Bose-Einstein condensates\n  and metallic nanograins", "source": "J. Links, H.-Q. Zhou, R.H. McKenzie, M.D. Gould", "docs_id": "nlin/0305049", "section": ["nlin.SI", "cond-mat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Algebraic Bethe ansatz method for the exact calculation of energy\n  spectra and form factors: applications to models of Bose-Einstein condensates\n  and metallic nanograins. In this review we demonstrate how the algebraic Bethe ansatz is used for the calculation of the energy spectra and form factors (operator matrix elements in the basis of Hamiltonian eigenstates) in exactly solvable quantum systems. As examples we apply the theory to several models of current interest in the study of Bose-Einstein condensates, which have been successfully created using ultracold dilute atomic gases. The first model we introduce describes Josephson tunneling between two coupled Bose-Einstein condensates. It can be used not only for the study of tunneling between condensates of atomic gases, but for solid state Josephson junctions and coupled Cooper pair boxes. The theory is also applicable to models of atomic-molecular Bose-Einstein condensates, with two examples given and analysed. Additionally, these same two models are relevant to studies in quantum optics. Finally, we discuss the model of Bardeen, Cooper and Schrieffer in this framework, which is appropriate for systems of ultracold fermionic atomic gases, as well as being applicable for the description of superconducting correlations in metallic grains with nanoscale dimensions. In applying all of the above models to physical situations, the need for an exact analysis of small scale systems is established due to large quantum fluctuations which render mean-field approaches inaccurate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary application of the algebraic Bethe ansatz method in the context of Bose-Einstein condensates and metallic nanograins, as demonstrated in the review article?\n\nA) To study the thermodynamic properties of ultracold atomic gases\nB) To calculate the energy spectra and form factors of exactly solvable quantum systems, including models of Josephson tunneling between coupled Bose-Einstein condensates and atomic-molecular Bose-Einstein condensates\nC) To analyze the superconducting correlations in metallic grains with nanoscale dimensions\nD) To investigate the quantum fluctuations in solid-state Josephson junctions\n\nCorrect Answer: B) To calculate the energy spectra and form factors of exactly solvable quantum systems, including models of Josephson tunneling between coupled Bose-Einstein condensates and atomic-molecular Bose-Einstein condensates\n\nExplanation: The review article highlights the application of the algebraic Bethe ansatz method to calculate the energy spectra and form factors of exactly solvable quantum systems, including models of Josephson tunneling between coupled Bose-Einstein condensates and atomic-molecular Bose-Einstein condensates. This is evident from the introduction, where the authors mention that the theory is used to study these models and their relevance to studies in quantum optics and ultracold atomic gases."}, "43": {"documentation": {"title": "Data Shapley Value for Handling Noisy Labels: An application in\n  Screening COVID-19 Pneumonia from Chest CT Scans", "source": "Nastaran Enshaei, Moezedin Javad Rafiee, Arash Mohammadi, Farnoosh\n  Naderkhani", "docs_id": "2110.08726", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data Shapley Value for Handling Noisy Labels: An application in\n  Screening COVID-19 Pneumonia from Chest CT Scans. A long-standing challenge of deep learning models involves how to handle noisy labels, especially in applications where human lives are at stake. Adoption of the data Shapley Value (SV), a cooperative game theoretical approach, is an intelligent valuation solution to tackle the issue of noisy labels. Data SV can be used together with a learning model and an evaluation metric to validate each training point's contribution to the model's performance. The SV of a data point, however, is not unique and depends on the learning model, the evaluation metric, and other data points collaborating in the training game. However, effects of utilizing different evaluation metrics for computation of the SV, detecting the noisy labels, and measuring the data points' importance has not yet been thoroughly investigated. In this context, we performed a series of comparative analyses to assess SV's capabilities to detect noisy input labels when measured by different evaluation metrics. Our experiments on COVID-19-infected of CT images illustrate that although the data SV can effectively identify noisy labels, adoption of different evaluation metric can significantly influence its ability to identify noisy labels from different data classes. Specifically, we demonstrate that the SV greatly depends on the associated evaluation metric."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the Data Shapley Value (SV) approach in handling noisy labels, as demonstrated in the study on COVID-19 pneumonia screening from chest CT scans?\n\n**A)** The SV approach is sensitive to the specific learning model used, but not to the evaluation metric.\n**B)** The SV approach is highly effective in identifying noisy labels, but its ability to do so is significantly influenced by the evaluation metric used.\n**C)** The SV approach is not affected by the data class, but its performance is highly dependent on the specific evaluation metric.\n**D)** The SV approach is not applicable to deep learning models, as it is not designed to handle noisy labels.\n\n**Correct Answer:** B) The SV approach is highly effective in identifying noisy labels, but its ability to do so is significantly influenced by the evaluation metric used.\n\n**Explanation:** The study demonstrates that the Data Shapley Value approach can effectively identify noisy labels, but its performance is highly dependent on the evaluation metric used. This means that different evaluation metrics can significantly influence the SV's ability to detect noisy labels, especially across different data classes. This limitation highlights the need for careful consideration of the evaluation metric when applying the Data Shapley Value approach to handle noisy labels in deep learning models."}, "44": {"documentation": {"title": "Analysis of odds, probability, and hazard ratios: From 2 by 2 tables to\n  two-sample survival data", "source": "Zhiqiang Tan", "docs_id": "1911.10682", "section": ["stat.ME", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of odds, probability, and hazard ratios: From 2 by 2 tables to\n  two-sample survival data. Analysis of 2 by 2 tables and two-sample survival data has been widely used. Exact calculation is computational intractable for conditional likelihood inference in odds ratio models with large marginals in 2 by 2 tables, or partial likelihood inference in Cox's proportional hazards models with considerable tied event times. Approximate methods are often employed, but their statistical properties have not been formally studied while taking into account the approximation involved. We develop new methods and theory by constructing suitable estimating functions while leveraging knowledge from conditional or partial likelihood inference. We propose a weighted Mantel--Haenszel estimator in an odds ratio model such as Cox's discrete-time proportional hazards model. Moreover, we consider a probability ratio model, and derive as a consistent estimator the Breslow--Peto estimator, which has been regarded as an approximation to partial likelihood estimation in the odds ratio model. We study both model-based and model-robust variance estimation. For the Breslow--Peto estimator, our new model-based variance estimator is no greater than the commonly reported variance estimator. We present numerical studies which support the theoretical findings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of analyzing odds, probability, and hazard ratios from 2 by 2 tables to two-sample survival data, what is the primary limitation of using approximate methods for estimating the odds ratio in Cox's discrete-time proportional hazards model?\n\n**A)** The approximate methods are computationally intensive and require large amounts of data.\n**B)** The approximate methods are statistically inefficient and do not account for the approximation involved.\n**C)** The approximate methods are not suitable for models with large marginals in 2 by 2 tables or considerable tied event times.\n**D)** The approximate methods are not robust to model misspecification and do not provide reliable estimates.\n\n**Correct Answer:** C) The approximate methods are not suitable for models with large marginals in 2 by 2 tables or considerable tied event times.\n\n**Explanation:** The correct answer is C) because the documentation states that exact calculation is computational intractable for conditional likelihood inference in odds ratio models with large marginals in 2 by 2 tables, or partial likelihood inference in Cox's proportional hazards models with considerable tied event times. This highlights the primary limitation of using approximate methods in these scenarios."}, "45": {"documentation": {"title": "V-, U-, L-, or W-shaped economic recovery after COVID: Insights from an\n  Agent Based Model", "source": "Dhruv Sharma, Jean-Philippe Bouchaud, Stanislao Gualdi, Marco Tarzia,\n  Francesco Zamponi", "docs_id": "2006.08469", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "V-, U-, L-, or W-shaped economic recovery after COVID: Insights from an\n  Agent Based Model. We discuss the impact of a Covid-19--like shock on a simple model economy, described by the previously developed Mark-0 Agent-Based Model. We consider a mixed supply and demand shock, and show that depending on the shock parameters (amplitude and duration), our model economy can display V-shaped, U-shaped or W-shaped recoveries, and even an L-shaped output curve with permanent output loss. This is due to the economy getting trapped in a self-sustained \"bad\" state. We then discuss two policies that attempt to moderate the impact of the shock: giving easy credit to firms, and the so-called helicopter money, i.e. injecting new money into the households savings. We find that both policies are effective if strong enough. We highlight the potential danger of terminating these policies too early, although inflation is substantially increased by lax access to credit. Finally, we consider the impact of a second lockdown. While we only discuss a limited number of scenarios, our model is flexible and versatile enough to accommodate a wide variety of situations, thus serving as a useful exploratory tool for a qualitative, scenario-based understanding of post-Covid recovery. The corresponding code is available on-line."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"V-, U-, L-, or W-shaped economic recovery after COVID: Insights from an Agent Based Model\" consider a mixed supply and demand shock in their model economy. They find that the economy can display different types of recoveries depending on the shock parameters. Which of the following statements about the recoveries described in the paper is TRUE?\n\nA) A V-shaped recovery is always associated with a strong and sustained economic growth.\nB) A U-shaped recovery implies a permanent output loss, while a W-shaped recovery implies a permanent output gain.\nC) The economy can display an L-shaped output curve with permanent output loss if the shock parameters are not strong enough.\nD) The authors find that helicopter money is always more effective than giving easy credit to firms in moderating the impact of the shock.\n\n**Correct Answer:** C) The economy can display an L-shaped output curve with permanent output loss if the shock parameters are not strong enough.\n\n**Explanation:** According to the paper, an L-shaped output curve with permanent output loss can occur if the economy gets trapped in a self-sustained \"bad\" state, regardless of the strength of the shock parameters. This is because the economy can become stuck in a state of low output and low investment, leading to permanent output loss."}, "46": {"documentation": {"title": "Positivity certificates in optimal control", "source": "Edouard Pauwels (IRIT), Didier Henrion (LAAS-MAC), Jean-Bernard\n  Lasserre (LAAS-MAC)", "docs_id": "1605.02452", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Positivity certificates in optimal control. We propose a tutorial on relaxations and weak formulations of optimal control with their semidefinite approximations. We present this approach solely through the prism of positivity certificates which we consider to be the most accessible for a broad audience, in particular in the engineering and robotics communities. This simple concept allows to express very concisely powerful approximation certificates in control. The relevance of this technique is illustrated on three applications: region of attraction approximation, direct optimal control and inverse optimal control, for which it constitutes a common denominator. In a first step, we highlight the core mechanisms underpinning the application of positivity in control and how they appear in the different control applications. This relies on simple mathematical concepts and gives a unified treatment of the applications considered. This presentation is based on the combination and simplification of published materials. In a second step, we describe briefly relations with broader literature, in particular, occupation measures and Hamilton-Jacobi-Bellman equation which are important elements of the global picture. We describe the Sum-Of-Squares (SOS) semidefinite hierarchy in the semialgebraic case and briefly mention its convergence properties. Numerical experiments on a classical example in robotics, namely the nonholonomic vehicle, illustrate the concepts presented in the text for the three applications considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the authors' proposal for relaxations and weak formulations of optimal control, and how do they relate to the concept of positivity certificates?\n\nA) The authors aim to provide a more computationally efficient method for solving optimal control problems, which is a common challenge in engineering and robotics applications. However, they do not explicitly mention the role of positivity certificates in their approach.\n\nB) The authors propose a tutorial on relaxations and weak formulations of optimal control, which they consider to be the most accessible for a broad audience, particularly in the engineering and robotics communities. They argue that positivity certificates provide a simple concept for expressing powerful approximation certificates in control, and that this approach constitutes a common denominator for three applications: region of attraction approximation, direct optimal control, and inverse optimal control.\n\nC) The authors' proposal is based on the combination and simplification of published materials, and they aim to provide a unified treatment of the three applications considered. However, they do not explicitly mention the role of positivity certificates in their approach.\n\nD) The authors' proposal is motivated by the need for a more rigorous and systematic approach to optimal control, which is essential for ensuring the stability and performance of complex systems. However, they do not explicitly mention the role of positivity certificates in their approach.\n\nCorrect Answer: B) The authors propose a tutorial on relaxations and weak formulations of optimal control, which they consider to be the most accessible for a broad audience, particularly in the engineering and robotics communities. They argue that positivity certificates provide a simple concept for expressing powerful approximation certificates in control, and that this approach constitutes a common denominator for three applications: region of attraction approximation, direct optimal control, and inverse optimal control.\n\nExplanation: The correct answer is B) because it accurately summarizes the motivation behind the authors' proposal, which is to provide a simple and accessible approach to optimal control using positivity certificates. The other options do not accurately capture the main idea of the proposal, and therefore are incorrect."}, "47": {"documentation": {"title": "Stratification Trees for Adaptive Randomization in Randomized Controlled\n  Trials", "source": "Max Tabord-Meehan", "docs_id": "1806.05127", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stratification Trees for Adaptive Randomization in Randomized Controlled\n  Trials. This paper proposes an adaptive randomization procedure for two-stage randomized controlled trials. The method uses data from a first-wave experiment in order to determine how to stratify in a second wave of the experiment, where the objective is to minimize the variance of an estimator for the average treatment effect (ATE). We consider selection from a class of stratified randomization procedures which we call stratification trees: these are procedures whose strata can be represented as decision trees, with differing treatment assignment probabilities across strata. By using the first wave to estimate a stratification tree, we simultaneously select which covariates to use for stratification, how to stratify over these covariates, as well as the assignment probabilities within these strata. Our main result shows that using this randomization procedure with an appropriate estimator results in an asymptotic variance which is minimal in the class of stratification trees. Moreover, the results we present are able to accommodate a large class of assignment mechanisms within strata, including stratified block randomization. In a simulation study, we find that our method, paired with an appropriate cross-validation procedure ,can improve on ad-hoc choices of stratification. We conclude by applying our method to the study in Karlan and Wood (2017), where we estimate stratification trees using the first wave of their experiment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of stratification trees for adaptive randomization in randomized controlled trials, what is the primary objective of using a first-wave experiment to estimate a stratification tree, and how does this estimation process impact the overall variance of an estimator for the average treatment effect (ATE)?\n\n**A)** To determine the optimal treatment assignment probabilities across all strata, and to minimize the variance of the ATE estimator by selecting the most informative covariates for stratification.\n\n**B)** To identify the most effective stratification tree that can accommodate a large class of assignment mechanisms within strata, including stratified block randomization, and to minimize the variance of the ATE estimator.\n\n**C)** To estimate the stratification tree using a class of stratified randomization procedures that can be represented as decision trees, with differing treatment assignment probabilities across strata, and to minimize the variance of the ATE estimator.\n\n**D)** To select the most informative covariates for stratification, and to determine the optimal stratification tree that can minimize the variance of the ATE estimator, while accommodating a large class of assignment mechanisms within strata.\n\n**Correct Answer:** C) To estimate the stratification tree using a class of stratified randomization procedures that can be represented as decision trees, with differing treatment assignment probabilities across strata, and to minimize the variance of the ATE estimator.\n\n**Explanation:** The correct answer is C) because the primary objective of using a first-wave experiment to estimate a stratification tree is to estimate the stratification tree using a class of stratified randomization procedures that can be represented as decision trees. This estimation process allows for the simultaneous selection of which covariates to use for stratification, how to stratify over these covariates, and the assignment probabilities within these strata. The resulting stratification tree is then used to minimize the variance of the ATE estimator.\n\nThe other options are incorrect because:\n\n* A) is incorrect because while the estimation process does involve selecting the most informative covariates for stratification, the primary objective is not to determine the optimal treatment assignment probabilities across all strata.\n* B) is incorrect because while the estimation process does accommodate a large class of assignment mechanisms within strata, including stratified block randomization, this is not the primary objective of the estimation process.\n* D) is incorrect because while the estimation process does involve selecting the most informative covariates for stratification and determining the optimal stratification tree, this is not the primary objective of the estimation process."}, "48": {"documentation": {"title": "Investigation of the unidirectional spin heat conveyer effect in a 200nm\n  thin Yttrium Iron Garnet film", "source": "O. Wid, J. Bauer, A. M\\\"uller, O. Breitenstein, S. S. P. Parkin, G.\n  Schmidt", "docs_id": "1602.01662", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigation of the unidirectional spin heat conveyer effect in a 200nm\n  thin Yttrium Iron Garnet film. We have investigated the unidirectional spin wave heat conveyer effect in sub-micron thick yttrium iron garnet (YIG) films using lock-in thermography (LIT). Although the effect is small in thin layers this technique allows us to observe asymmetric heat transport by magnons which leads to asymmetric temperature profiles differing by several mK on both sides of the exciting antenna, respectively. Comparison of Damon-Eshbach and backward volume modes shows that the unidirectional heat flow is indeed due to non-reciprocal spin-waves. Because of the finite linewidth, small asymmetries can still be observed when only the uniform mode of ferromagnetic resonance is excited. The latter is of extreme importance for example when measuring the inverse spin-Hall effect because the temperature differences can result in thermovoltages at the contacts. Because of the non-reciprocity these thermovoltages reverse their sign with a reversal of the magnetic field which is typically deemed the signature of the inverse spin-Hall voltage."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism behind the observed unidirectional spin wave heat conveyer effect in sub-micron thick yttrium iron garnet (YIG) films, as investigated using lock-in thermography (LIT)?\n\n**A)** The effect is due to the finite linewidth of the ferromagnetic resonance, causing small asymmetries in the temperature profiles.\n\n**B)** The unidirectional heat flow is a result of the non-reciprocal spin-waves, specifically the Damon-Eshbach and backward volume modes.\n\n**C)** The asymmetry in temperature profiles is a consequence of the non-uniform excitation of the ferromagnetic resonance, leading to a non-uniform heat transfer.\n\n**D)** The observed effect is a result of the finite thickness of the YIG film, causing a non-uniform heat transfer due to the finite thermal conductivity.\n\n**Correct Answer:** B) The unidirectional heat flow is a result of the non-reciprocal spin-waves, specifically the Damon-Eshbach and backward volume modes.\n\n**Explanation:** The correct answer is B) because the investigation using lock-in thermography (LIT) reveals that the unidirectional spin wave heat conveyer effect is indeed due to non-reciprocal spin-waves, specifically the Damon-Eshbach and backward volume modes. The comparison of these modes shows that the unidirectional heat flow is a result of the non-reciprocal spin-waves, which is a key finding in the study. The other options are incorrect because they do not accurately describe the primary mechanism behind the observed effect. Option A is incorrect because the finite linewidth is a contributing factor, but not the primary mechanism. Option C is incorrect because the non-uniform excitation of the ferromagnetic resonance is not the primary cause of the asymmetry in temperature profiles. Option D is incorrect because the finite thickness of the YIG film is not the primary cause of the observed effect."}, "49": {"documentation": {"title": "Periodic elements in Garside groups", "source": "Eon-Kyung Lee and Sang-Jin Lee", "docs_id": "1004.5308", "section": ["math.GT", "math.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Periodic elements in Garside groups. Let $G$ be a Garside group with Garside element $\\Delta$, and let $\\Delta^m$ be the minimal positive central power of $\\Delta$. An element $g\\in G$ is said to be 'periodic' if some power of it is a power of $\\Delta$. In this paper, we study periodic elements in Garside groups and their conjugacy classes. We show that the periodicity of an element does not depend on the choice of a particular Garside structure if and only if the center of $G$ is cyclic; if $g^k=\\Delta^{ka}$ for some nonzero integer $k$, then $g$ is conjugate to $\\Delta^a$; every finite subgroup of the quotient group $G/<\\Delta^m>$ is cyclic. By a classical theorem of Brouwer, Ker\\'ekj\\'art\\'o and Eilenberg, an $n$-braid is periodic if and only if it is conjugate to a power of one of two specific roots of $\\Delta^2$. We generalize this to Garside groups by showing that every periodic element is conjugate to a power of a root of $\\Delta^m$. We introduce the notions of slimness and precentrality for periodic elements, and show that the super summit set of a slim, precentral periodic element is closed under any partial cycling. For the conjugacy problem, we may assume the slimness without loss of generality. For the Artin groups of type $A_n$, $B_n$, $D_n$, $I_2(e)$ and the braid group of the complex reflection group of type $(e,e,n)$, endowed with the dual Garside structure, we may further assume the precentrality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Let $G$ be a Garside group with Garside element $\\Delta$ and minimal positive central power $\\Delta^m$. Suppose that $g \\in G$ is a periodic element, meaning that some power of $g$ is a power of $\\Delta$. What can be concluded about the conjugacy class of $g$ in $G$?\n\nA) The conjugacy class of $g$ is contained in the conjugacy class of $\\Delta^a$ for some integer $a$.\n\nB) The conjugacy class of $g$ is contained in the conjugacy class of $\\Delta^b$ for some integer $b$.\n\nC) The conjugacy class of $g$ is contained in the conjugacy class of $\\Delta^c$ for some integer $c$.\n\nD) The conjugacy class of $g$ is not contained in any conjugacy class of powers of $\\Delta$.\n\nCorrect Answer: A) The conjugacy class of $g$ is contained in the conjugacy class of $\\Delta^a$ for some integer $a$.\n\nExplanation: According to the paper, if $g^k = \\Delta^{ka}$ for some nonzero integer $k$, then $g$ is conjugate to $\\Delta^a$. This implies that the conjugacy class of $g$ is contained in the conjugacy class of $\\Delta^a$ for some integer $a$."}, "50": {"documentation": {"title": "RDWIA analysis of 12C(e,e'p) for Q^2 < 2 (GeV/c)^2", "source": "James J. Kelly", "docs_id": "nucl-th/0501079", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RDWIA analysis of 12C(e,e'p) for Q^2 < 2 (GeV/c)^2. We analyze data for 12C(e,e'p) with Q^2 < 2 (GeV/c)^2 using the relativistic distorted-wave impulse approximation (RDWIA) based upon Dirac-Hartree wave functions. The 1p normalization extracted from data for Q^2 > 0.6 (GeV/c)^2 is approximately 0.87, independent of Q^2, which is consistent with the predicted depletion by short-range correlations. The total 1p and 1s strength for E_m < 80 MeV approaches 100% of IPSM, consistent with a continuum contribution for 30 < E_m < 80 MeV of about 12% of IPSM. Similarly, a scale factor of 1.12 brings RDWIA calculations into good agreement with 12C(e,e'p) data for transparency. We also analyzed low Q^2 data from which a recent NDWIA analysis suggested that spectroscopic factors might depend strongly upon the resolution of the probe. We find that momentum distributions for their empirical Woods-Saxon wave functions fit to low Q^2 data for parallel kinematics are too narrow to reproduce data for quasiperpendicular kinematics, especially for larger Q^2, and are partly responsible for reducing fitted normalization factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main conclusion regarding the spectroscopic factors for 12C(e,e'p) data, as inferred from the RDWIA analysis, and how does it relate to the momentum distributions of empirical Woods-Saxon wave functions?\n\nA) The spectroscopic factors are independent of the resolution of the probe, and the momentum distributions are too broad to reproduce data for quasiperpendicular kinematics.\nB) The spectroscopic factors depend strongly upon the resolution of the probe, and the momentum distributions are too narrow to reproduce data for parallel kinematics, especially for larger Q^2.\nC) The spectroscopic factors are consistent with a continuum contribution for 30 < E_m < 80 MeV of about 12% of IPSM, but the momentum distributions are not directly related to this finding.\nD) The spectroscopic factors are consistent with the predicted depletion by short-range correlations, but the momentum distributions are not relevant to this conclusion.\n\nCorrect Answer: B) The spectroscopic factors depend strongly upon the resolution of the probe, and the momentum distributions are too narrow to reproduce data for parallel kinematics, especially for larger Q^2.\n\nExplanation: The correct answer is B) because the RDWIA analysis suggests that the spectroscopic factors might depend strongly upon the resolution of the probe, and the momentum distributions of empirical Woods-Saxon wave functions are too narrow to reproduce data for quasiperpendicular kinematics, especially for larger Q^2. This is consistent with the finding that the momentum distributions are partly responsible for reducing fitted normalization factors."}, "51": {"documentation": {"title": "Towards Automatic Detection of Misinformation in Online Medical Videos", "source": "Rui Hou, Ver\\'onica P\\'erez-Rosas, Stacy Loeb, Rada Mihalcea", "docs_id": "1909.01543", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Automatic Detection of Misinformation in Online Medical Videos. Recent years have witnessed a significant increase in the online sharing of medical information, with videos representing a large fraction of such online sources. Previous studies have however shown that more than half of the health-related videos on platforms such as YouTube contain misleading information and biases. Hence, it is crucial to build computational tools that can help evaluate the quality of these videos so that users can obtain accurate information to help inform their decisions. In this study, we focus on the automatic detection of misinformation in YouTube videos. We select prostate cancer videos as our entry point to tackle this problem. The contribution of this paper is twofold. First, we introduce a new dataset consisting of 250 videos related to prostate cancer manually annotated for misinformation. Second, we explore the use of linguistic, acoustic, and user engagement features for the development of classification models to identify misinformation. Using a series of ablation experiments, we show that we can build automatic models with accuracies of up to 74%, corresponding to a 76.5% precision and 73.2% recall for misinformative instances."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of computational tools to evaluate the quality of online medical videos, and how does the proposed study address this issue?\n\n**A)** The primary motivation is to reduce the spread of misinformation on social media, and the study introduces a new dataset to train machine learning models to detect misinformation in prostate cancer videos.\n\n**B)** The primary motivation is to improve the accuracy of medical information on YouTube, and the study explores the use of linguistic, acoustic, and user engagement features to develop classification models for identifying misinformation.\n\n**C)** The primary motivation is to increase the visibility of misleading health-related videos on YouTube, and the study focuses on building automatic models with high accuracy to detect misinformation in prostate cancer videos.\n\n**D)** The primary motivation is to reduce the reliance on human annotators for labeling medical videos, and the study introduces a new dataset to train machine learning models to detect misinformation in prostate cancer videos.\n\n**Correct Answer:** B) The primary motivation is to improve the accuracy of medical information on YouTube, and the study explores the use of linguistic, acoustic, and user engagement features to develop classification models for identifying misinformation.\n\n**Explanation:** The question requires the test-taker to understand the context and motivation behind the proposed study. The correct answer (B) highlights the primary motivation of improving the accuracy of medical information on YouTube, which is a key aspect of the study. The other options are incorrect because they either misrepresent the motivation (A and C) or focus on a different aspect of the study (D). The test-taker needs to analyze the question and identify the correct answer based on the information provided in the documentation."}, "52": {"documentation": {"title": "Pressure-induced gap closing and metallization of MoSe$_{2}$ and\n  MoTe$_{2}$", "source": "Michaela Riflikov\\'a, Roman Marto\\v{n}\\'ak, and Erio Tosatti", "docs_id": "1605.05111", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pressure-induced gap closing and metallization of MoSe$_{2}$ and\n  MoTe$_{2}$. Layered molybdenum dichalchogenides are semiconductors whose gap is controlled by delicate interlayer interactions. The gap tends to drop together with the interlayer distance, suggesting collapse and metallization under pressure. We predict, based on first principles calculations, that layered semiconductors 2H$_c$-MoSe$_2$ and 2H$_c$-MoTe$_2$ should undergo metallization at pressures between 28 and 40 GPa (MoSe$_2$) and 13 and 19 GPa (MoTe$_2$). Unlike MoS$_2$ where a 2H$_c$ $\\to$ 2H$_a$ layer sliding transition is known to take place, these two materials appear to preserve the original 2H$_c$ layered structure at least up to 100 GPa and to increasingly resist lubric layer sliding under pressure. Similar to metallized MoS$_2$ they are predicted to exhibit a low density of states at the Fermi level, and presumably very modest superconducting temperatures if any. We also study the $\\beta$-MoTe$_2$ structure, metastable with a higher enthalpy than 2H$_c$-MoTe$_2$. Despite its ready semimetallic and (weakly) superconducting character already at zero pressure, metallicity is not expected to increase dramatically with pressure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the predicted pressure range for the metallization of 2H$_c$-MoSe$_2$ and 2H$_c$-MoTe$_2$ semiconductors, and how does this compare to the metallization pressure of MoS$_2$?\n\nA) 13-19 GPa for MoSe$_2$ and 28-40 GPa for MoTe$_2$, with no significant difference from MoS$_2$.\nB) 28-40 GPa for MoSe$_2$ and 13-19 GPa for MoTe$_2$, with MoS$_2$ exhibiting a layer sliding transition at lower pressures.\nC) 13-19 GPa for MoSe$_2$ and 28-40 GPa for MoTe$_2$, with MoS$_2$ preserving its layered structure at higher pressures.\nD) 28-40 GPa for MoSe$_2$ and 13-19 GPa for MoTe$_2$, with MoS$_2$ exhibiting a more dramatic increase in metallicity with pressure.\n\nCorrect Answer: B) 28-40 GPa for MoSe$_2$ and 13-19 GPa for MoTe$_2$, with MoS$_2$ exhibiting a layer sliding transition at lower pressures.\n\nExplanation: The correct answer is based on the text, which states that MoSe$_2$ and MoTe$_2$ are predicted to undergo metallization at pressures between 28-40 GPa and 13-19 GPa, respectively. Additionally, the text mentions that MoS$_2$ exhibits a layer sliding transition at lower pressures, which is a key difference between the two materials."}, "53": {"documentation": {"title": "Si$_3$N$_4$ nanobeam optomechanical crystals", "source": "Karen E. Grutter, Marcelo Davanco, and Kartik Srinivasan", "docs_id": "1411.5996", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Si$_3$N$_4$ nanobeam optomechanical crystals. The development of Si$_3$N$_4$ nanobeam optomechanical crystals is reviewed. These structures consist of a 350 nm thick, 700 nm wide doubly-clamped Si$_3$N$_4$ nanobeam that is periodically patterned with an array of air holes to which a defect region is introduced. The periodic patterning simultaneously creates a photonic bandgap for 980 nm band photons and a phononic bandgap for 4 GHz phonons, with the defect region serving to co-localize optical and mechanical modes within their respective bandgaps. These optical and mechanical modes interact dispersively with a coupling rate $g_{0}/2\\pi\\approx$100 kHz, which describes the shift in cavity mode optical frequency due to the zero-point motion of the mechanical mode. Optical sidebands generated by interaction with the mechanical mode lie outside of the optical cavity linewidth, enabling possible use of this system in applications requiring sideband-resolved operation. Along with a review of the basic device design, fabrication, and measurement procedures, we present new results on improved optical quality factors (up to $4\\times10^5$) through optimized lithography, measurements of devices after HF acid surface treatment, and temperature dependent measurements of mechanical damping between 6~K and 300~K. A frequency-mechanical quality factor product $\\left(f{\\times}Q_m\\right)$ as high as $\\approx2.6\\times10^{13}$ Hz is measured."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary purpose of introducing a defect region in the Si$_3$N$_4$ nanobeam optomechanical crystal, and how does it relate to the interaction between optical and mechanical modes?\n\nA){The defect region serves to enhance the optical quality factor, allowing for more efficient light-matter interactions.}\nB){The defect region creates a phononic bandgap for 4 GHz phonons, enabling the co-localization of optical and mechanical modes within their respective bandgaps.}\nC){The defect region is used to pattern air holes, which in turn creates a photonic bandgap for 980 nm band photons.}\nD){The defect region is introduced to reduce the mechanical damping of the nanobeam, allowing for more precise control over the mechanical mode.}\n\nCorrect Answer: B){The defect region creates a phononic bandgap for 4 GHz phonons, enabling the co-localization of optical and mechanical modes within their respective bandgaps.}\n\nExplanation: The introduction of a defect region in the Si$_3$N$_4$ nanobeam optomechanical crystal serves to create a phononic bandgap for 4 GHz phonons. This phononic bandgap enables the co-localization of optical and mechanical modes within their respective bandgaps, allowing for the interaction between the two modes to occur. This interaction is crucial for the operation of the optomechanical crystal, as it enables the shift in cavity mode optical frequency due to the zero-point motion of the mechanical mode."}, "54": {"documentation": {"title": "Model selection for deep audio source separation via clustering analysis", "source": "Alisa Liu, Prem Seetharaman, Bryan Pardo", "docs_id": "1910.12626", "section": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Model selection for deep audio source separation via clustering analysis. Audio source separation is the process of separating a mixture (e.g. a pop band recording) into isolated sounds from individual sources (e.g. just the lead vocals). Deep learning models are the state-of-the-art in source separation, given that the mixture to be separated is similar to the mixtures the deep model was trained on. This requires the end user to know enough about each model's training to select the correct model for a given audio mixture. In this work, we automate selection of the appropriate model for an audio mixture. We present a confidence measure that does not require ground truth to estimate separation quality, given a deep model and audio mixture. We use this confidence measure to automatically select the model output with the best predicted separation quality. We compare our confidence-based ensemble approach to using individual models with no selection, to an oracle that always selects the best model and to a random model selector. Results show our confidence-based ensemble significantly outperforms the random ensemble over general mixtures and approaches oracle performance for music mixtures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of using deep learning models for audio source separation, and how does the proposed confidence measure address this limitation?\n\n**A)** Deep learning models require large amounts of labeled data to train, which can be difficult to obtain for new audio mixtures. The proposed confidence measure does not address this limitation.\n\n**B)** Deep learning models are only effective for separating mixtures that are similar to the mixtures the model was trained on. The proposed confidence measure provides a way to automatically select the best model for a given audio mixture, regardless of its similarity to the training data.\n\n**C)** Deep learning models can be computationally expensive to train and deploy, which can make them impractical for real-time audio source separation. The proposed confidence measure does not address this limitation.\n\n**D)** Deep learning models can suffer from overfitting, which can lead to poor performance on new audio mixtures. The proposed confidence measure provides a way to evaluate the separation quality of a model without requiring ground truth labels.\n\n**Correct Answer:** D) Deep learning models can suffer from overfitting, which can lead to poor performance on new audio mixtures. The proposed confidence measure provides a way to evaluate the separation quality of a model without requiring ground truth labels.\n\n**Explanation:** The question requires the test-taker to understand the limitations of deep learning models for audio source separation and how the proposed confidence measure addresses this limitation. The correct answer, D, highlights the issue of overfitting and how the confidence measure provides a way to evaluate separation quality without requiring ground truth labels. The incorrect answers, A, B, and C, either misrepresent the limitation or do not address it, demonstrating a lack of understanding of the topic."}, "55": {"documentation": {"title": "RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction", "source": "Eduardo Castell\\'o Ferrer, Ognjen Rudovic, Thomas Hardjono, Alex\n  Pentland", "docs_id": "1802.04480", "section": ["cs.RO", "cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction. Robots have potential to revolutionize the way we interact with the world around us. One of their largest potentials is in the domain of mobile health where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. Furthermore, to enhance this learning process, the knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users (e.g., the hospital patients). To this end, we introduce RoboChain - the first learning framework for secure, decentralized and computationally efficient data and model sharing among multiple robot units installed at multiple sites (e.g., hospitals). RoboChain builds upon and combines the latest advances in open data access and blockchain technologies, as well as machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of RoboChain, a secure data-sharing framework for human-robot interaction, and how does it address the challenges of data privacy in mobile health applications?\n\n**A)** RoboChain is designed to facilitate the sharing of sensitive medical data among multiple robot units to improve their learning capabilities, thereby enhancing patient care. However, this approach raises concerns about data privacy, which RoboChain aims to address by implementing a decentralized and computationally efficient framework.\n\n**B)** RoboChain is primarily motivated by the need for robots to access and share data to improve their interaction capabilities, but it does not address the issue of data privacy. Instead, it relies on blockchain technology to ensure the integrity and security of the data.\n\n**C)** RoboChain is designed to provide a secure and decentralized platform for robots to share data and models, but its primary focus is on improving the learning capabilities of individual robot units rather than addressing data privacy concerns.\n\n**D)** RoboChain is a framework that enables robots to share data and models in a secure and decentralized manner, but it does not address the specific challenges of mobile health applications, where data privacy is a major concern.\n\n**Correct Answer:** A) RoboChain is designed to facilitate the sharing of sensitive medical data among multiple robot units to improve their learning capabilities, thereby enhancing patient care. However, this approach raises concerns about data privacy, which RoboChain aims to address by implementing a decentralized and computationally efficient framework.\n\n**Explanation:** The correct answer is A) because it accurately reflects the primary motivation behind the development of RoboChain, as stated in the documentation. RoboChain is designed to facilitate the sharing of sensitive medical data among multiple robot units to improve their learning capabilities, which is essential for mobile health applications. However, this approach raises concerns about data privacy, which RoboChain aims to address by implementing a decentralized and computationally efficient framework. The other options are incorrect because they either misrepresent the primary motivation behind RoboChain or fail to address the issue of data privacy."}, "56": {"documentation": {"title": "A Mathematical Model for the Genetic Code(s) Based on Fibonacci Numbers\n  and their q-Analogues", "source": "Tidjani Negadi", "docs_id": "1510.01278", "section": ["q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Mathematical Model for the Genetic Code(s) Based on Fibonacci Numbers\n  and their q-Analogues. This work aims at showing the relevance and the applications possibilities of the Fibonacci sequence, and also its q-deformed or quantum extension, in the study of the genetic code(s). First, after the presentation of a new formula, an indexed double Fibonacci sequence, comprising the first six Fibonacci numbers, is shown to describe the 20 amino acids multiplets and their degeneracy as well as a characteristic pattern for the 61 meaningful codons. Next, the twenty amino acids, classified according to their increasing atom-number (carbon, nitrogen, oxygen and sulfur), exhibit several Fibonacci sequence patterns. Several mathematical relations are given, describing various atom-number patterns. Finally, a q-Fibonacci simple phenomenological model, with q a real deformation parameter, is used to describe, in a unified way, not only the standard genetic code, when q=1, but also all known slight variations of this latter, when q~1, as well as the case of the 21st amino acid (Selenocysteine) and the 22nd one (Pyrrolysine), also when q~1. As a by-product of this elementary model, we also show that, in the limit q=0, the number of amino acids reaches the value 6, in good agreement with old and still persistent claims stating that life, in its early development, could have used only a small number of amino acids."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"A Mathematical Model for the Genetic Code(s) Based on Fibonacci Numbers and their q-Analogues\" propose a q-Fibonacci simple phenomenological model to describe the genetic code. According to this model, when q approaches 1, the model can describe not only the standard genetic code but also slight variations of it. What is the implication of this result for our understanding of the evolutionary history of life on Earth?\n\n**A)** The existence of slight variations of the genetic code suggests that life on Earth has undergone multiple independent origins.\n**B)** The model's ability to describe slight variations of the genetic code implies that the genetic code has been subject to relatively minor mutations over time.\n**C)** The fact that the model can describe the genetic code and its variations when q approaches 1 suggests that the genetic code is a fundamental aspect of life, present in all living organisms.\n**D)** The result implies that the genetic code is not unique to life on Earth, but rather is a universal feature of life in the universe.\n\n**Correct Answer:** C) The fact that the model can describe the genetic code and its variations when q approaches 1 suggests that the genetic code is a fundamental aspect of life, present in all living organisms.\n\n**Explanation:** The correct answer, C, is supported by the fact that the q-Fibonacci model is able to describe not only the standard genetic code but also slight variations of it when q approaches 1. This suggests that the genetic code is a fundamental aspect of life, present in all living organisms, and that the model is able to capture its underlying structure and patterns. The other options are incorrect because they do not accurately reflect the implications of the result. Option A is incorrect because the model does not provide evidence for multiple independent origins of life. Option B is incorrect because the model's ability to describe slight variations of the genetic code does not necessarily imply that the genetic code has been subject to relatively minor mutations over time. Option D is incorrect because the result does not imply that the genetic code is universal to life in the universe, but rather that it is a fundamental aspect of life on Earth."}, "57": {"documentation": {"title": "Contraction Analysis and Control Synthesis for Discrete-time Nonlinear\n  Processes", "source": "Lai Wei, Ryan McCloy, Jie Bao", "docs_id": "2112.04699", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contraction Analysis and Control Synthesis for Discrete-time Nonlinear\n  Processes. Shifting away from the traditional mass production approach, the process industry is moving towards more agile, cost-effective and dynamic process operation (next-generation smart plants). This warrants the development of control systems for nonlinear chemical processes to be capable of tracking time-varying setpoints to produce products with different specifications as per market demand and deal with variations in the raw materials and utility (e.g., energy). This article presents a systematic approach to the implementation of contraction-based control for discrete-time nonlinear processes. Through the differential dynamic system framework, the contraction conditions to ensure the exponential convergence to feasible time-varying references are derived. The discrete-time differential dissipativity condition is further developed, which can be used for control designs for disturbance rejection. Computationally tractable equivalent conditions are then derived and additionally transformed into an SOS programming problem, such that a discrete-time control contraction metric and stabilising feedback controller can be jointly obtained. Synthesis and implementation details are provided and demonstrated through a numerical case study."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a discrete-time nonlinear process with a time-varying setpoint, where the process dynamics are described by the following differential equation:\n\ndx/dt = f(x) + g(x)u + h(x)b\n\nwhere x is the process state, u is the control input, b is the disturbance input, and f(x), g(x), and h(x) are nonlinear functions.\n\nUsing the contraction analysis and control synthesis framework presented in the article, derive the discrete-time differential dissipativity condition for the process, and explain its significance in ensuring the exponential convergence to feasible time-varying references.\n\n**A)** The discrete-time differential dissipativity condition is given by:\n\n\u2207V(x) \\* (f(x) + g(x)u + h(x)b) \u2264 -\u03bbV(x)\n\nwhere V(x) is a Lyapunov function, \u03bb is a positive scalar, and \u2207V(x) is the gradient of V(x).\n\n**B)** The discrete-time differential dissipativity condition is given by:\n\n\u2207V(x) \\* (f(x) + g(x)u + h(x)b) \u2265 -\u03bbV(x)\n\nwhere V(x) is a Lyapunov function, \u03bb is a positive scalar, and \u2207V(x) is the gradient of V(x).\n\n**C)** The discrete-time differential dissipativity condition is given by:\n\n\u2207V(x) \\* (f(x) + g(x)u + h(x)b) = -\u03bbV(x)\n\nwhere V(x) is a Lyapunov function, \u03bb is a positive scalar, and \u2207V(x) is the gradient of V(x).\n\n**D)** The discrete-time differential dissipativity condition is given by:\n\n\u2207V(x) \\* (f(x) + g(x)u + h(x)b) = \u03bbV(x)\n\nwhere V(x) is a Lyapunov function, \u03bb is a positive scalar, and \u2207V(x) is the gradient of V(x).\n\n**Correct Answer:** A\n\n**Explanation:** The correct answer is A) \u2207V(x) \\* (f(x) + g(x)u + h(x)b) \u2264 -\u03bbV(x), where V(x) is a Lyapunov function, \u03bb is a positive scalar, and \u2207V(x) is the gradient of V(x). This condition ensures that the process dynamics are dissipative, meaning that the energy of the system is decreasing over time, which is a necessary condition for exponential convergence to feasible time-varying references. The other options are incorrect because they do not accurately represent the discrete-time differential dissipativity condition. Option B is incorrect because it has a strict inequality, whereas the correct condition has a non-strict inequality. Option C is incorrect because it sets the dissipativity condition to zero, which is not a valid condition. Option D is incorrect because it sets the dissipativity condition to positive, which is not a valid condition."}, "58": {"documentation": {"title": "A Unified Framework for the Ergodic Capacity of Spectrum Sharing\n  Cognitive Radio Systems", "source": "Lokman Sboui, Zouheir Rezki, Mohamed-Slim Alouini", "docs_id": "1211.6566", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Unified Framework for the Ergodic Capacity of Spectrum Sharing\n  Cognitive Radio Systems. We consider a spectrum sharing communication scenario in which a primary and a secondary users are communicating, simultaneously, with their respective destinations using the same frequency carrier. Both optimal power profile and ergodic capacity are derived for fading channels, under an average transmit power and an instantaneous interference outage constraints. Unlike previous studies, we assume that the secondary user has a noisy version of the cross link and the secondary link Channel State Information (CSI). After deriving the capacity in this case, we provide an ergodic capacity generalization, through a unified expression, that encompasses several previously studied spectrum sharing settings. In addition, we provide an asymptotic capacity analysis at high and low signal-to-noise ratio (SNR). Numerical results, applied for independent Rayleigh fading channels, show that at low SNR regime, only the secondary channel estimation matters with no effect of the cross link on the capacity; whereas at high SNR regime, the capacity is rather driven by the cross link CSI. Furthermore, a practical on-off power allocation scheme is proposed and is shown, through numerical results, to achieve the full capacity at high and low SNR"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"A Unified Framework for the Ergodic Capacity of Spectrum Sharing in Cognitive Radio Systems\", what is the key assumption made about the secondary user's Channel State Information (CSI) that distinguishes this work from previous studies?\n\nA) The secondary user has perfect knowledge of the primary link CSI.\nB) The secondary user has noisy knowledge of the cross link CSI.\nC) The secondary user has no knowledge of the primary link CSI.\nD) The secondary user has perfect knowledge of the cross link CSI.\n\n**Correct Answer:** B) The secondary user has noisy knowledge of the cross link CSI.\n\n**Explanation:** The correct answer is B) The secondary user has noisy knowledge of the cross link CSI. This assumption is highlighted in the paper as a key distinction from previous studies, which assumed perfect knowledge of the primary link CSI. The authors derive the capacity in this case and provide a unified expression that encompasses several previously studied spectrum sharing settings. The other options are incorrect because they do not accurately reflect the assumption made in the paper."}, "59": {"documentation": {"title": "Discordant Relaxations of Misspecified Models", "source": "D\\'esir\\'e K\\'edagni and Lixiong Li and Isma\\\"el Mourifi\\'e", "docs_id": "2012.11679", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discordant Relaxations of Misspecified Models. In many set-identified models, it is difficult to obtain a tractable characterization of the identified set. Therefore, empirical works often construct confidence regions based on an outer set of the identified set. Because an outer set is always a superset of the identified set, this practice is often viewed as conservative yet valid. However, this paper shows that, when the model is refuted by the data, a nonempty outer set could deliver conflicting results with another outer set derived from the same underlying model structure, so that the results of outer sets could be misleading in the presence of misspecification. We provide a sufficient condition for the existence of discordant outer sets which covers models characterized by intersection bounds and the Artstein (1983) inequalities. We also derive sufficient conditions for the non-existence of discordant submodels, therefore providing a class of models for which constructing outer sets cannot lead to misleading interpretations. In the case of discordancy, we follow Masten and Poirier (2020) by developing a method to salvage misspecified models, but unlike them we focus on discrete relaxations. We consider all minimum relaxations of a refuted model which restores data-consistency. We find that the union of the identified sets of these minimum relaxations is misspecification-robust and has a new and intuitive empirical interpretation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of set-identified models, what is the primary concern with constructing confidence regions based on an outer set of the identified set, and how does the paper \"Discordant Relaxations of Misspecified Models\" address this issue?\n\n**A)** The primary concern is that outer sets are always too narrow, leading to undercoverage of the true identified set. The paper addresses this by showing that, in the presence of misspecification, nonempty outer sets can deliver conflicting results.\n\n**B)** The primary concern is that outer sets are always too broad, leading to overcoverage of the true identified set. The paper addresses this by providing a sufficient condition for the existence of discordant outer sets, which can lead to misleading interpretations.\n\n**C)** The primary concern is that outer sets are always too complex, leading to difficulties in interpretation. The paper addresses this by developing a method to salvage misspecified models, focusing on discrete relaxations that restore data-consistency.\n\n**D)** The primary concern is that outer sets are always too conservative, leading to unnecessary restrictions on the model. The paper addresses this by showing that, in the presence of misspecification, nonempty outer sets can deliver conflicting results with another outer set derived from the same underlying model structure.\n\n**Correct Answer:** B) The primary concern is that outer sets are always too broad, leading to overcoverage of the true identified set. The paper addresses this by providing a sufficient condition for the existence of discordant outer sets, which can lead to misleading interpretations.\n\n**Explanation:** The paper highlights the issue of constructing confidence regions based on outer sets of the identified set, which can lead to overcoverage of the true identified set. The authors provide a sufficient condition for the existence of discordant outer sets, which can result in conflicting results and misleading interpretations. This is particularly problematic when the model is refuted by the data, as it can lead to incorrect conclusions about the true identified set."}}