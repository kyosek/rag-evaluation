{"0": {"documentation": {"title": "Particle production in field theories coupled to strong external sources\n  I. Formalism and main results", "source": "F. Gelis, R. Venugopalan", "docs_id": "hep-ph/0601209", "section": ["hep-ph", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Particle production in field theories coupled to strong external sources\n  I. Formalism and main results. We develop a formalism for particle production in a field theory coupled to a strong time-dependent external source. An example of such a theory is the Color Glass Condensate. We derive a formula, in terms of cut vacuum-vacuum Feynman graphs, for the probability of producing a given number of particles. This formula is valid to all orders in the coupling constant. The distribution of multiplicities is non--Poissonian, even in the classical approximation. We investigate an alternative method of calculating the mean multiplicity. At leading order, the average multiplicity can be expressed in terms of retarded solutions of classical equations of motion. We demonstrate that the average multiplicity at {\\it next-to-leading order} can be formulated as an initial value problem by solving equations of motion for small fluctuation fields with retarded boundary conditions. The variance of the distribution can be calculated in a similar fashion. Our formalism therefore provides a framework to compute from first principles particle production in proton-nucleus and nucleus-nucleus collisions beyond leading order in the coupling constant and to all orders in the source density. We also provide a transparent interpretation (in conventional field theory language) of the well known Abramovsky-Gribov-Kancheli (AGK) cancellations. Explicit connections are made between the framework for multi-particle production developed here and the framework of Reggeon field theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of particle production in field theories coupled to strong external sources, what is the main result of the formalism developed in the paper, and how does it relate to the calculation of the mean multiplicity at next-to-leading order?\n\nA) The formalism provides a formula for the probability of producing a given number of particles, which is valid to all orders in the coupling constant, and the mean multiplicity can be expressed in terms of retarded solutions of classical equations of motion at leading order.\n\nB) The formalism provides a framework to compute particle production in proton-nucleus and nucleus-nucleus collisions beyond leading order in the coupling constant and to all orders in the source density, and the mean multiplicity can be formulated as an initial value problem by solving equations of motion for small fluctuation fields with retarded boundary conditions at next-to-leading order.\n\nC) The formalism provides a transparent interpretation of the well-known Abramovsky-Gribov-Kancheli (AGK) cancellations, and the mean multiplicity can be calculated in a similar fashion to the variance of the distribution at leading order.\n\nD) The formalism provides a formula for the probability of producing a given number of particles, which is valid to all orders in the coupling constant, and the mean multiplicity can be expressed in terms of classical equations of motion at next-to-leading order, but not as an initial value problem.\n\nCorrect Answer: B) The formalism provides a framework to compute particle production in proton-nucleus and nucleus-nucleus collisions beyond leading order in the coupling constant and to all orders in the source density, and the mean multiplicity can be formulated as an initial value problem by solving equations of motion for small fluctuation fields with retarded boundary conditions at next-to-leading order.\n\nExplanation: The correct answer is B) because the formalism developed in the paper provides a framework to compute particle production in proton-nucleus and nucleus-nucleus collisions beyond leading order in the coupling constant and to all orders in the source density. Additionally, the mean multiplicity can be formulated as an initial value problem by solving equations of motion for small fluctuation fields with retarded boundary conditions at next-to-leading order, as stated in the paper. The other options are incorrect because they either omit important details or misrepresent the main result of the formalism."}, "1": {"documentation": {"title": "User-Relatedness and Community Structure in Social Interaction Networks", "source": "Folke Mitzlaff and Martin Atzmueller and Dominik Benz and Andreas\n  Hotho and Gerd Stumme", "docs_id": "1309.3888", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "User-Relatedness and Community Structure in Social Interaction Networks. With social media and the according social and ubiquitous applications finding their way into everyday life, there is a rapidly growing amount of user generated content yielding explicit and implicit network structures. We consider social activities and phenomena as proxies for user relatedness. Such activities are represented in so-called social interaction networks or evidence networks, with different degrees of explicitness. We focus on evidence networks containing relations on users, which are represented by connections between individual nodes. Explicit interaction networks are then created by specific user actions, for example, when building a friend network. On the other hand, more implicit networks capture user traces or evidences of user actions as observed in Web portals, blogs, resource sharing systems, and many other social services. These implicit networks can be applied for a broad range of analysis methods instead of using expensive gold-standard information. In this paper, we analyze different properties of a set of networks in social media. We show that there are dependencies and correlations between the networks. These allow for drawing reciprocal conclusions concerning pairs of networks, based on the assessment of structural correlations and ranking interchangeability. Additionally, we show how these inter-network correlations can be used for assessing the results of structural analysis techniques, e.g., community mining methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the concept of \"user-relatedness\" in the context of social interaction networks, and explain how the authors of the paper \"User-Relatedness and Community Structure in Social Interaction Networks\" utilize implicit networks to assess structural correlations and ranking interchangeability.\n\n**A)** The authors of the paper define user-relatedness as the degree to which two users are connected through a common social activity or phenomenon, and use implicit networks to identify patterns of user behavior that can be used to infer user relationships.\n\n**B)** The authors argue that explicit interaction networks are more reliable than implicit networks for assessing user-relatedness, and therefore focus on explicit networks containing relations on users.\n\n**C)** The authors demonstrate that implicit networks can be used to identify dependencies and correlations between different social media networks, and that these correlations can be used to assess the results of community mining methods.\n\n**D)** The authors propose a new method for measuring user-relatedness based on the analysis of user-generated content and social media interactions, and show that this method can be used to identify clusters of users with similar interests and behaviors.\n\n**Correct Answer:** C) The authors demonstrate that implicit networks can be used to identify dependencies and correlations between different social media networks, and that these correlations can be used to assess the results of community mining methods.\n\n**Explanation:** The correct answer is C) because the paper specifically discusses the use of implicit networks to analyze dependencies and correlations between different social media networks, and to assess the results of community mining methods. The authors show that these correlations can be used to draw reciprocal conclusions about pairs of networks, and to rank interchangeability. This requires a deep understanding of the concepts of user-relatedness, social interaction networks, and community mining methods, as well as the ability to analyze complex relationships between different types of networks."}, "2": {"documentation": {"title": "Improved limits for violations of local position invariance from atomic\n  clock comparisons", "source": "R. Lange, N. Huntemann, J. M. Rahm, C. Sanner, H. Shao, B. Lipphardt,\n  Chr. Tamm, S. Weyers, and E. Peik", "docs_id": "2010.06620", "section": ["physics.atom-ph", "gr-qc", "hep-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improved limits for violations of local position invariance from atomic\n  clock comparisons. We compare two optical clocks based on the $^2$S$_{1/2}(F=0)\\to {}^2$D$_{3/2}(F=2)$ electric quadrupole (E2) and the $^2$S$_{1/2}(F=0)\\to {}^2$F$_{7/2}(F=3)$ electric octupole (E3) transition of $^{171}$Yb$^{+}$ and measure the frequency ratio $\\nu_{\\mathrm{E3}}/\\nu_{\\mathrm{E2}}=0.932\\,829\\,404\\,530\\,965\\,376(32)$. We determine the transition frequency $\\nu_{E3}=642\\,121\\,496\\,772\\,645.10(8)$ Hz using two caesium fountain clocks. Repeated measurements of both quantities over several years are analyzed for potential violations of local position invariance. We improve by factors of about 20 and 2 the limits for fractional temporal variations of the fine structure constant $\\alpha$ to $1.0(1.1)\\times10^{-18}/\\mathrm{yr}$ and of the proton-to-electron mass ratio $\\mu$ to $-8(36)\\times10^{-18}/\\mathrm{yr}$. Using the annual variation of the Sun's gravitational potential at Earth $\\Phi$, we improve limits for a potential coupling of both constants to gravity, $(c^2/\\alpha) (d\\alpha/d\\Phi)=14(11)\\times 10^{-9}$ and $(c^2/\\mu) (d\\mu/d\\Phi)=7(45)\\times 10^{-8}$."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the paper report improved limits on the fractional temporal variations of the fine structure constant \u03b1 and the proton-to-electron mass ratio \u03bc. Using the annual variation of the Sun's gravitational potential at Earth \u03a6, they also set limits on the coupling of these constants to gravity.\n\n**A)** The authors' results imply that the fine structure constant \u03b1 is not constant over time, but the proton-to-electron mass ratio \u03bc is constant. This is consistent with the Standard Model of particle physics.\n\n**B)** The authors' results are consistent with the idea that the fundamental constants of physics are not constant over time, but the variation of these constants is related to the variation of the gravitational potential at Earth.\n\n**C)** The authors' results are limited by the precision of the caesium fountain clocks used to measure the transition frequencies. A more precise measurement of the transition frequencies would lead to tighter limits on the variations of \u03b1 and \u03bc.\n\n**D)** The authors' results are consistent with the idea that the variation of the fine structure constant \u03b1 is related to the variation of the proton-to-electron mass ratio \u03bc, and both variations are related to the variation of the gravitational potential at Earth.\n\n**Correct Answer:** D) The authors' results are consistent with the idea that the variation of the fine structure constant \u03b1 is related to the variation of the proton-to-electron mass ratio \u03bc, and both variations are related to the variation of the gravitational potential at Earth.\n\n**Explanation:** The correct answer is D) because the authors report limits on the coupling of \u03b1 and \u03bc to gravity, which is a key aspect of the paper. The coupling of \u03b1 and \u03bc to gravity is related to the variation of the gravitational potential at Earth, which is a fundamental aspect of general relativity. The other options are incorrect because they do not accurately reflect the results of the paper. Option A is incorrect because the authors do not report limits on \u03bc being constant over time. Option B is incorrect because the authors do not discuss the idea that the fundamental constants of physics are not constant over time. Option C is incorrect because the authors do not discuss the precision of the caesium fountain clocks used to measure the transition frequencies."}, "3": {"documentation": {"title": "Anomalous reaction-diffusion equations for linear reactions", "source": "Sean D Lawley", "docs_id": "2008.11579", "section": ["cond-mat.stat-mech", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anomalous reaction-diffusion equations for linear reactions. Deriving evolution equations accounting for both anomalous diffusion and reactions is notoriously difficult, even in the simplest cases. In contrast to normal diffusion, reaction kinetics cannot be incorporated into evolution equations modeling subdiffusion by merely adding reaction terms to the equations describing spatial movement. A series of previous works derived fractional reaction-diffusion equations for the spatiotemporal evolution of particles undergoing subdiffusion in one space dimension with linear reactions between a finite number of discrete states. In this paper, we first give a short and elementary proof of these previous results. We then show how this argument gives the evolution equations for more general cases, including subdiffusion following any fractional Fokker-Planck equation in an arbitrary $d$-dimensional spatial domain with time-dependent reactions between infinitely many discrete states. In contrast to previous works which employed a variety of technical mathematical methods, our analysis reveals that the evolution equations follow from (i) the probabilistic independence of the stochastic spatial and discrete processes describing a single particle and (ii) the linearity of the integro-differential operators describing spatial movement. We also apply our results to systems combining reactions with superdiffusion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Derive the evolution equation for subdiffusion with time-dependent reactions between infinitely many discrete states in an arbitrary $d$-dimensional spatial domain, assuming that the stochastic spatial and discrete processes describing a single particle are probabilistically independent.\n\n**A)** $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\frac{\\partial}{\\partial t} \\left[ \\frac{\\partial u}{\\partial x} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx$\n\n**B)** $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx$\n\n**C)** $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx$\n\n**D)** $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx$\n\n**Correct Answer:** **C)** $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left[ \\frac{\\partial u}{\\partial x} \\right] \\left[ \\frac{\\partial u}{\\partial t} \\right] dx$\n\nExplanation: The correct answer is obtained by applying the probabilistic independence of the stochastic spatial and discrete processes describing a single particle, and the linearity of the integro-differential operators describing spatial movement. The correct equation is derived by considering the evolution of the probability density function $u(x,t)$ of a particle undergoing subdiffusion with time-dependent reactions between infinitely many discrete states. The equation is obtained by combining the effects of spatial movement and discrete reactions, and is consistent with the assumptions of probabilistic independence and linearity."}, "4": {"documentation": {"title": "C/O abundance ratios, iron depletions, and infrared dust features in\n  Galactic planetary nebulae", "source": "Gloria Delgado-Inglada (1,2) and M\\'onica Rodr\\'iguez (1) ((1)\n  Instituto Nacional de Astrof\\'isica, \\'Optica y Electr\\'onica (INAOE),\n  Mexico, (2) Instituto de Astronom\\'ia, UNAM, Mexico)", "docs_id": "1402.6376", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "C/O abundance ratios, iron depletions, and infrared dust features in\n  Galactic planetary nebulae. We study the dust present in 56 Galactic planetary nebulae (PNe) through their iron depletion factors, their C/O abundance ratios (in 51 objects), and the dust features that appear in their infrared spectra (for 33 objects). Our sample objects have deep optical spectra of good quality, and most of them also have ultraviolet observations. We use these observations to derive the iron abundances and the C/O abundance ratios in a homogeneous way for all the objects. We compile detections of infrared dust features from the literature and we analyze the available Spitzer/IRS spectra. Most of the PNe have C/O ratios below one and show crystalline silicates in their infrared spectra. The PNe with silicates have C/O < 1, with the exception of Cn 1-5. Most of the PNe with dust features related to C-rich environments (SiC or the 30 {\\mu}m feature usually associated to MgS) have C/O $\\gtrsim$ 0.8. PAHs are detected over the full range of C/O values, including 6 objects that also show silicates. Iron abundances are low in all the objects, implying that more than 90% of their iron atoms are deposited into dust grains. The range of iron depletions in the sample covers about two orders of magnitude, and we find that the highest depletion factors are found in C-rich objects with SiC or the 30 {\\mu}m feature in their infrared spectra, whereas some of the O-rich objects with silicates show the lowest depletion factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between the C/O abundance ratio and the presence of dust features in Galactic planetary nebulae. How do the findings of this study relate to the overall picture of dust formation and iron depletion in these objects?\n\n**A)** The C/O abundance ratio is directly correlated with the presence of crystalline silicates in the infrared spectra of PNe, with C-rich objects showing silicates and O-rich objects not showing silicates.\n\n**B)** The study found that PNe with C/O ratios below 1 show crystalline silicates in their infrared spectra, while PNe with C/O ratios above 1 do not show silicates. However, the presence of silicates is not directly related to the C/O abundance ratio.\n\n**C)** The C/O abundance ratio is inversely related to the iron depletion factor in PNe, with C-rich objects showing higher iron depletion factors and O-rich objects showing lower iron depletion factors.\n\n**D)** The study found that PNe with C/O ratios above 0.8 show dust features related to C-rich environments, such as SiC or the 30 \u03bcm feature associated with MgS, while PNe with C/O ratios below 0.8 do not show these features.\n\n**Correct Answer:** D) The study found that PNe with C/O ratios above 0.8 show dust features related to C-rich environments, such as SiC or the 30 \u03bcm feature associated with MgS, while PNe with C/O ratios below 0.8 do not show these features.\n\n**Explanation:** This question requires the test-taker to carefully read and analyze the findings of the study, paying close attention to the relationships between the C/O abundance ratio, dust features, and iron depletion factor. The correct answer is supported by the text, which states that \"Most of the PNe with dust features related to C-rich environments (SiC or the 30 {\\mu}m feature usually associated to MgS) have C/O $\\gtrsim$ 0.8.\" The incorrect answers are plausible but not supported by the text, requiring the test-taker to carefully evaluate the evidence and make an informed decision."}, "5": {"documentation": {"title": "Online Bagging for Anytime Transfer Learning", "source": "Guokun Chi and Min Jiang and Xing Gao and Weizhen Hu and Shihui Guo\n  and Kay Chen Tan", "docs_id": "1910.08945", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Online Bagging for Anytime Transfer Learning. Transfer learning techniques have been widely used in the reality that it is difficult to obtain sufficient labeled data in the target domain, but a large amount of auxiliary data can be obtained in the relevant source domain. But most of the existing methods are based on offline data. In practical applications, it is often necessary to face online learning problems in which the data samples are achieved sequentially. In this paper, We are committed to applying the ensemble approach to solving the problem of online transfer learning so that it can be used in anytime setting. More specifically, we propose a novel online transfer learning framework, which applies the idea of online bagging methods to anytime transfer learning problems, and constructs strong classifiers through online iterations of the usefulness of multiple weak classifiers. Further, our algorithm also provides two extension schemes to reduce the impact of negative transfer. Experiments on three real data sets show that the effectiveness of our proposed algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of online transfer learning methods, and how does the proposed framework in the paper address this challenge?\n\n**A)** The primary motivation is to improve the accuracy of models on the target domain, and the proposed framework addresses this by using online bagging methods to construct strong classifiers.\n\n**B)** The primary motivation is to reduce the impact of negative transfer, and the proposed framework addresses this by providing two extension schemes to mitigate this effect.\n\n**C)** The primary motivation is to overcome the limitation of offline data, and the proposed framework addresses this by applying the idea of online bagging methods to anytime transfer learning problems.\n\n**D)** The primary motivation is to improve the efficiency of model training, and the proposed framework addresses this by using online iterations of multiple weak classifiers to construct strong classifiers.\n\n**Correct Answer:** C) The primary motivation is to overcome the limitation of offline data, and the proposed framework addresses this by applying the idea of online bagging methods to anytime transfer learning problems.\n\n**Explanation:** The paper highlights the challenge of obtaining sufficient labeled data in the target domain, but having a large amount of auxiliary data in the relevant source domain. The proposed framework addresses this challenge by applying the idea of online bagging methods to anytime transfer learning problems, which enables the construction of strong classifiers through online iterations of the usefulness of multiple weak classifiers. This approach is specifically designed to overcome the limitation of offline data, making it a key motivation behind the development of online transfer learning methods."}, "6": {"documentation": {"title": "Gravitational Electric-Magnetic Duality, Gauge Invariance and Twisted\n  Self-Duality", "source": "Claudio Bunster, Marc Henneaux and Sergio H\\\"ortner", "docs_id": "1207.1840", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gravitational Electric-Magnetic Duality, Gauge Invariance and Twisted\n  Self-Duality. The concept of electric-magnetic duality can be extended to linearized gravity. It has indeed been established that in four dimensions, the Pauli-Fierz action (quadratic part of the Einstein-Hilbert action) can be cast in a form that is manifestly invariant under duality rotations in the internal 2-plane of the spacetime curvature and its dual. In order to achieve this manifestly duality-invariant form, it is necessary to introduce two \"prepotentials\", which form a duality multiplet. These prepotentials enjoy interesting gauge invariance symmetries, which are, for each, linearized diffeomorphisms and linearized Weyl rescalings. The purpose of this note is twofold: (i) To rewrite the manifestly-duality invariant action obtained in previous work in a way that makes its gauge invariances also manifest. (ii) To explicitly show that the equations of motion derived from that action can be interpreted as twisted self-duality conditions on the curvature tensors of the two metrics obtained from the two prepotentials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Pauli-Fierz action in four dimensions, which can be cast in a form manifestly invariant under duality rotations in the internal 2-plane of the spacetime curvature and its dual. The action is given by:\n\nS = \u222b d^4x \u221a-g (R + 2\u2202_\u03bcA_\u03bc + 2\u2202_\u03bcA_\u03bc\u0304)\n\nwhere g is the determinant of the metric tensor, R is the Ricci scalar, and A_\u03bc and A_\u03bc\u0304 are the two prepotentials. The gauge invariances of the action are given by:\n\n\u2202_\u03bcA_\u03bc = 0\n\u2202_\u03bcA_\u03bc\u0304 = 0\n\nShow that the equations of motion derived from the action can be interpreted as twisted self-duality conditions on the curvature tensors of the two metrics obtained from the two prepotentials.\n\nA) The equations of motion are given by \u2202_\u03bcF_\u03bc\u03bd = 0, where F_\u03bc\u03bd = dA_\u03bc + A_\u03bc\u0304 \u2227 A_\u03bd.\n\nB) The equations of motion are given by \u2202_\u03bcG_\u03bc\u03bd = 0, where G_\u03bc\u03bd = dA_\u03bc + A_\u03bc\u0304 \u2227 A_\u03bd and G_\u03bc\u03bd\u0304 = dA_\u03bc\u0304 + A_\u03bc \u2227 A_\u03bd\u0304.\n\nC) The equations of motion are given by \u2202_\u03bcH_\u03bc\u03bd = 0, where H_\u03bc\u03bd = dA_\u03bc + A_\u03bc\u0304 \u2227 A_\u03bd and H_\u03bc\u03bd\u0304 = dA_\u03bc\u0304 + A_\u03bc \u2227 A_\u03bd\u0304.\n\nD) The equations of motion are given by \u2202_\u03bcK_\u03bc\u03bd = 0, where K_\u03bc\u03bd = dA_\u03bc + A_\u03bc\u0304 \u2227 A_\u03bd and K_\u03bc\u03bd\u0304 = dA_\u03bc\u0304 + A_\u03bc \u2227 A_\u03bd\u0304.\n\nCorrect Answer: B) The equations of motion are given by \u2202_\u03bcG_\u03bc\u03bd = 0, where G_\u03bc\u03bd = dA_\u03bc + A_\u03bc\u0304 \u2227 A_\u03bd and G_\u03bc\u03bd\u0304 = dA_\u03bc\u0304 + A_\u03bc \u2227 A_\u03bd\u0304.\n\nExplanation: The Pauli-Fierz action is given by S = \u222b d^4x \u221a-g (R + 2\u2202_\u03bcA_\u03bc + 2\u2202_\u03bcA_\u03bc\u0304). The equations of motion are obtained by varying the action with respect to the prepotentials A_\u03bc and A_\u03bc\u0304. Using the definition of the curvature tensors G_\u03bc\u03bd and G_\u03bc\u03bd\u0304, we can rewrite the equations of motion as \u2202_\u03bcG_\u03bc\u03bd = 0 and \u2202_\u03bcG_\u03bc\u03bd\u0304 = 0. These equations can be interpreted as twisted self-duality conditions on the curvature tensors of the two metrics obtained from the two prepotentials."}, "7": {"documentation": {"title": "Pay Attention to the cough: Early Diagnosis of COVID-19 using\n  Interpretable Symptoms Embeddings with Cough Sound Signal Processing", "source": "Ankit Pal, Malaikannan Sankarasubbu", "docs_id": "2010.02417", "section": ["cs.LG", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pay Attention to the cough: Early Diagnosis of COVID-19 using\n  Interpretable Symptoms Embeddings with Cough Sound Signal Processing. COVID-19 (coronavirus disease 2019) pandemic caused by SARS-CoV-2 has led to a treacherous and devastating catastrophe for humanity. At the time of writing, no specific antivirus drugs or vaccines are recommended to control infection transmission and spread. The current diagnosis of COVID-19 is done by Reverse-Transcription Polymer Chain Reaction (RT-PCR) testing. However, this method is expensive, time-consuming, and not easily available in straitened regions. An interpretable and COVID-19 diagnosis AI framework is devised and developed based on the cough sounds features and symptoms metadata to overcome these limitations. The proposed framework's performance was evaluated using a medical dataset containing Symptoms and Demographic data of 30000 audio segments, 328 cough sounds from 150 patients with four cough classes ( COVID-19, Asthma, Bronchitis, and Healthy). Experiments' results show that the model captures the better and robust feature embedding to distinguish between COVID-19 patient coughs and several types of non-COVID-19 coughs with higher specificity and accuracy of 95.04 $\\pm$ 0.18% and 96.83$\\pm$ 0.18% respectively, all the while maintaining interpretability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The proposed framework for early diagnosis of COVID-19 using interpretable symptoms embeddings with cough sound signal processing relies on the following assumptions:\n\nA) The model is trained on a dataset that only includes symptoms metadata, without any audio features.\nB) The cough sound signal processing is used to extract features that are independent of the symptoms metadata.\nC) The model is evaluated on a dataset that includes 328 cough sounds from 150 patients, with four cough classes (COVID-19, Asthma, Bronchitis, and Healthy).\nD) The model's performance is evaluated using a medical dataset containing Symptoms and Demographic data of 30000 audio segments, with a specificity and accuracy of 95.04% and 96.83% respectively.\n\n**Correct Answer:** D) The model's performance is evaluated using a medical dataset containing Symptoms and Demographic data of 30000 audio segments, with a specificity and accuracy of 95.04% and 96.83% respectively.\n\n**Explanation:** The correct answer is D) because the documentation states that the model's performance was evaluated using a medical dataset containing Symptoms and Demographic data of 30000 audio segments, with a specificity and accuracy of 95.04% and 96.83% respectively. This is the only option that accurately reflects the evaluation method described in the documentation. Options A and B are incorrect because the documentation states that the model is trained on a dataset that includes both symptoms metadata and audio features, and that the cough sound signal processing is used to extract features that are dependent on the symptoms metadata. Option C is partially correct, but it only mentions the number of cough sounds and patients, without mentioning the dataset size or evaluation metrics."}, "8": {"documentation": {"title": "Phase transitions in optical turbulence", "source": "Natalia Vladimirova, Stanislav Derevyanko, Gregory Falkovich", "docs_id": "1108.1541", "section": ["nlin.CD", "cond-mat.stat-mech", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase transitions in optical turbulence. We consider turbulence in the Gross-Pitaevsky model and study the creation of a coherent condensate via an inverse cascade originated at small scales. The growth of the condensate leads to a spontaneous breakdown of symmetries of small-scale over-condensate fluctuations: first, statistical isotropy is broken, then series of phase transitions mark the change of symmetry from the two-fold to three-fold to four-fold. At the highest condensate level reached, we observe a short-range positional and long-range orientational order (similar to a hexatic phase in the condensed matter physics). In other words, the longer one pumps the system the more ordered it becomes. We show that these phase transitions happen when the driving term corresponds to an instability (i.e. it is multiplicative in the k-space) but not when the system is pumped by a random force. Thus we demonstrate for the first time non-universality of the inverse-cascade turbulence. We also describe anisotropic spectral flux flows in k-space, anomalous correlations of fluctuations and collective oscillations of turbulence-condensate system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Gross-Pitaevsky model, what is the relationship between the growth of the condensate and the breakdown of symmetries in the system, and how does this relate to the concept of non-universality in inverse-cascade turbulence?\n\n**A)** The growth of the condensate leads to a spontaneous breakdown of symmetries, resulting in a series of phase transitions that mark the change of symmetry from two-fold to three-fold to four-fold. This process is universal and occurs regardless of the driving term.\n\n**B)** The growth of the condensate leads to a spontaneous breakdown of symmetries, resulting in a series of phase transitions that mark the change of symmetry from two-fold to three-fold to four-fold. This process is non-universal and occurs only when the driving term corresponds to an instability.\n\n**C)** The growth of the condensate leads to a spontaneous breakdown of symmetries, resulting in a series of phase transitions that mark the change of symmetry from two-fold to three-fold to four-fold. This process is universal and occurs when the system is pumped by a random force.\n\n**D)** The growth of the condensate leads to a spontaneous breakdown of symmetries, resulting in a series of phase transitions that mark the change of symmetry from two-fold to three-fold to four-fold. This process is non-universal and occurs when the driving term is multiplicative in k-space.\n\n**Correct Answer:** D) The growth of the condensate leads to a spontaneous breakdown of symmetries, resulting in a series of phase transitions that mark the change of symmetry from two-fold to three-fold to four-fold. This process is non-universal and occurs when the driving term is multiplicative in k-space.\n\n**Explanation:** The correct answer is D) because the documentation states that the phase transitions occur when the driving term corresponds to an instability, i.e., it is multiplicative in k-space. This is a key point in the paper, as it highlights the non-universality of the inverse-cascade turbulence. The other options are incorrect because they either describe a universal process (A) or a process that occurs when the system is pumped by a random force (C), rather than an instability."}, "9": {"documentation": {"title": "The Gessel Correspondence and the Partial $\\gamma$-Positivity of the\n  Eulerian Polynomials on Multiset Stirling Permutations", "source": "William Y.C. Chen, Amy M. Fu, Sherry H.F. Yan", "docs_id": "2111.04098", "section": ["math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Gessel Correspondence and the Partial $\\gamma$-Positivity of the\n  Eulerian Polynomials on Multiset Stirling Permutations. Pondering upon the grammatical labeling of 0-1-2 increasing plane trees, we came to the realization that the grammatical labels play a role as records of chopped off leaves of the original increasing binary trees. While such an understanding is purely psychological, it does give rise to an efficient apparatus to tackle the partial $\\gamma$-positivity of the Eulearian polynomials on multiset Stirling permutations, as long as we bear in mind the combinatorial meanings of the labels $x$ and $y$ in the Gessel representation of a $k$-Stirling permutation by means of an increasing $(k+1)$-ary tree. More precisely, we introduce a Foata-Strehl action on the Gessel trees resulting in an interpretation of the partial $\\gamma$-coefficients of the aforementioned Eulerian polynomials, different from the ones found by Lin-Ma-Zhang and Yan-Huang-Yang. In particular, our strategy can be adapted to deal with the partial $\\gamma$-coefficients of the second order Eulerian polynomials, which in turn can be readily converted to the combinatorial formulation due to Ma-Ma-Yeh in connection with certain statistics of Stirling permutations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a multiset Stirling permutation of degree $k$ represented as an increasing $(k+1)$-ary tree, where the Gessel representation is given by a $k$-Stirling permutation. Let $x$ and $y$ be the labels of the two leaves of the tree, and let $\\gamma$ be the partial $\\gamma$-positivity of the Eulerian polynomials on this multiset Stirling permutation.\n\nA) The Foata-Strehl action on the Gessel trees results in an interpretation of the partial $\\gamma$-coefficients of the Eulerian polynomials that is equivalent to the one found by Lin-Ma-Zhang and Yan-Huang-Yang.\n\nB) The partial $\\gamma$-coefficients of the Eulerian polynomials on multiset Stirling permutations can be computed using the combinatorial formulation due to Ma-Ma-Yeh, which involves certain statistics of Stirling permutations.\n\nC) The Gessel representation of a $k$-Stirling permutation by means of an increasing $(k+1)$-ary tree provides a new interpretation of the partial $\\gamma$-positivity of the Eulerian polynomials on multiset Stirling permutations.\n\nD) The Foata-Strehl action on the Gessel trees results in an interpretation of the partial $\\gamma$-coefficients of the Eulerian polynomials that is different from the one found by Lin-Ma-Zhang and Yan-Huang-Yang, and can be adapted to deal with the partial $\\gamma$-coefficients of the second order Eulerian polynomials.\n\nCorrect Answer: D) The Foata-Strehl action on the Gessel trees results in an interpretation of the partial $\\gamma$-coefficients of the Eulerian polynomials that is different from the one found by Lin-Ma-Zhang and Yan-Huang-Yang, and can be adapted to deal with the partial $\\gamma$-coefficients of the second order Eulerian polynomials.\n\nExplanation: The correct answer is D) because the documentation states that the Foata-Strehl action on the Gessel trees results in an interpretation of the partial $\\gamma$-coefficients of the Eulerian polynomials that is different from the one found by Lin-Ma-Zhang and Yan-Huang-Yang. Additionally, the documentation mentions that this strategy can be adapted to deal with the partial $\\gamma$-coefficients of the second order Eulerian polynomials, which is option D. Options A, B, and C are incorrect because they either state that the Foata-Strehl action is equivalent to the one found by Lin-Ma-Zhang and Yan-Huang-Yang (A), or that the partial $\\gamma$-coefficients can be computed using the combinatorial formulation due to Ma-Ma-Yeh (B and C), which is not the case."}, "10": {"documentation": {"title": "The Araucaria Project. Bright Variable Stars in NGC 6822 from a\n  Wide-Field Imaging Survey", "source": "R.E. Mennickent, W. Gieren, I. Soszynski, G. Pietrzynski", "docs_id": "astro-ph/0601395", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Araucaria Project. Bright Variable Stars in NGC 6822 from a\n  Wide-Field Imaging Survey. We have performed a search for variable stars in the dwarf irregular galaxy NGC 6822 using wide-field multi-epoch VI photometry down to a limiting magnitude $V$ $\\sim$ 22. Apart from the Cepheid variables in this galaxy already reported in an earlier paper by Pietrzynski et al. (2004), we have found 1019 \"non-periodic\" variable stars, 50 periodically variable stars with periods ranging from 0.12 to 66 days and 146 probably periodic variables. Twelve of these stars are eclipsing binaries and fifteen are likely new, low-amplitude Cepheids. Interestingly, seven of these Cepheid candidates have periods longer than 100 days, have very low amplitudes (less than 0.2 mag in $I$), and are very red. They could be young, massive Cepheids still embedded in dusty envelopes. The other objects span a huge range in colours and represent a mixture of different types of luminous variables. Many of the variables classified as non-periodic in the present study may turn out to be {\\it periodic} variables once a much longer time baseline will be available to study them. We provide the catalogue of photometric parameters and show the atlas of light curves for the new variable stars. Our present catalogue is complementary to the one of Baldacci et al. (2005) which has focussed on very short-period and fainter variables in a subfield in NGC 6822."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the nature of the seven Cepheid candidates with periods longer than 100 days, low amplitudes, and high redness?\n\nA) They are likely to be old, low-mass Cepheids that have exhausted their fuel.\nB) They are probably young, massive Cepheids still embedded in dusty envelopes.\nC) They are probably contact binaries, where the two stars are in close physical contact.\nD) They are likely to be RR Lyrae variables, a type of Cepheid with a different pulsation period.\n\nCorrect Answer: B) They are probably young, massive Cepheids still embedded in dusty envelopes.\n\nExplanation: The text states that these seven Cepheid candidates have \"very low amplitudes (less than 0.2 mag in I)\" and are \"very red\", which suggests that they are still embedded in dusty envelopes. Additionally, their long periods (over 100 days) and low amplitudes are consistent with the characteristics of young, massive Cepheids. This inference requires the test-taker to analyze the characteristics of the Cepheid candidates and make a logical connection between them."}, "11": {"documentation": {"title": "Measurement of the neutron lifetime using an asymmetric magneto-\n  gravitational trap and in situ detection", "source": "R. W. Pattie Jr., N. B. Callahan, C. Cude-Woods, E. R. Adamek, L. J.\n  Broussard, S. M. Clayton, S. A. Currie, E. B. Dees, X. Ding, E. M. Engel, D.\n  E. Fellers, W. Fox, K. P. Hickerson, M. A. Hoffbauer, A. T. Holley, A.\n  Komives, C.-Y. Liu, S. W. T. MacDonald, M. Makela, C. L. Morris, J. D. Ortiz,\n  J. Ramsey, D. J. Salvat, A. Saunders, S. J. Seestrom, E. I. Sharapov, S. K.\n  Sjue, Z. Tang, J. Vanderwerp, B. Vogelaar, P. L. Walstrom, Z. Wang, W. Wei,\n  H. L. Weaver, J. W. Wexler, T. L. Womack, A. R. Young, B. A. Zeck", "docs_id": "1707.01817", "section": ["nucl-ex", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of the neutron lifetime using an asymmetric magneto-\n  gravitational trap and in situ detection. The precise value of the mean neutron lifetime, $\\tau_n$, plays an important role in nuclear and particle physics and cosmology. It is a key input for predicting the ratio of protons to helium atoms in the primordial universe and is used to search for new physics beyond the Standard Model of particle physics. There is a 3.9 standard deviation discrepancy between $\\tau_n$ measured by counting the decay rate of free neutrons in a beam (887.7 $\\pm$ 2.2 s) and by counting surviving ultracold neutrons stored for different storage times in a material trap (878.5$\\pm$0.8 s). The experiment described here eliminates loss mechanisms present in previous trap experiments by levitating polarized ultracold neutrons above the surface of an asymmetric storage trap using a repulsive magnetic field gradient so that the stored neutrons do not interact with material trap walls and neutrons in quasi-stable orbits rapidly exit the trap. As a result of this approach and the use of a new in situ neutron detector, the lifetime reported here (877.7 $\\pm$ 0.7 (stat) +0.4/-0.2 (sys) s) is the first modern measurement of $\\tau_n$ that does not require corrections larger than the quoted uncertainties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the experiment described in the paper \"Measurement of the neutron lifetime using an asymmetric magneto-gravitational trap and in situ detection\" aim to measure the mean neutron lifetime, $\\tau_n$, with high precision. What is the main advantage of using an asymmetric storage trap in this experiment, and how does it differ from previous trap experiments?\n\n**A)** The asymmetric storage trap allows for the elimination of loss mechanisms by levitating neutrons above the surface, reducing the interaction with material trap walls. This approach enables the measurement of $\\tau_n$ with higher precision.\n\n**B)** The asymmetric storage trap uses a repulsive magnetic field gradient to confine neutrons in quasi-stable orbits, which leads to a more efficient detection of neutron decay.\n\n**C)** The asymmetric storage trap is designed to minimize the effect of neutron interactions with the material trap walls, allowing for a more accurate measurement of $\\tau_n$.\n\n**D)** The asymmetric storage trap uses a new in situ neutron detector to measure the decay rate of neutrons, which enables the measurement of $\\tau_n$ with higher precision.\n\n**Correct Answer:** A) The asymmetric storage trap allows for the elimination of loss mechanisms by levitating neutrons above the surface, reducing the interaction with material trap walls. This approach enables the measurement of $\\tau_n$ with higher precision.\n\n**Explanation:** The correct answer is A) because the asymmetric storage trap is designed to eliminate loss mechanisms present in previous trap experiments by levitating polarized ultracold neutrons above the surface of the trap. This approach reduces the interaction between neutrons and the material trap walls, allowing for a more accurate measurement of $\\tau_n$. The other options are incorrect because they either describe a different aspect of the experiment (B and D) or are incomplete (C)."}, "12": {"documentation": {"title": "Nonequilibrium Statistical Physics in Ecology: Vegetation Patterns,\n  Animal Mobility and Temporal Fluctuations", "source": "Ricardo Martinez-Garcia", "docs_id": "1701.02908", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonequilibrium Statistical Physics in Ecology: Vegetation Patterns,\n  Animal Mobility and Temporal Fluctuations. This thesis focuses on the applications of mathematical tools and concepts brought from nonequilibrium statistical physics to the modeling of ecological problems. The first part provides a short introduction where the theoretical concepts and mathematical tools that are going to be used in subsequent chapters are presented. Firstly, the different levels of description usually employed in the models are explained. Secondly, the mathematical relationships among them are presented. Finally, the notation and terminology that will be used later on are explained. The second part is devoted to studying vegetation pattern formation in regions where precipitations are not frequent and resources for plant growth are scarce. This part comprises two chapters. The third part of the thesis develops a series of mathematical models describing the collective movement and behavior of some animal species. Its primary objective is to investigate the effect that communication among foragers has on searching times and the formation of groups. It consists of two chapters. The fourth part covers the effect of stochastic temporal disorder, mimicking climate and environmental variability, on systems formed by many interacting particles. These models may serve as an example of ecosystems. The thesis ends with a summary and devising future research lines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of nonequilibrium statistical physics applied to ecology, what is the primary objective of the mathematical models developed in Chapter 3 of the thesis, which focus on the collective movement and behavior of animal species?\n\n**A)** To investigate the effect of resource availability on animal migration patterns\n**B)** To study the impact of communication among foragers on searching times and group formation\n**C)** To analyze the role of environmental variability on ecosystem stability\n**D)** To model the effects of climate change on vegetation patterns\n\n**Correct Answer:** B) To study the impact of communication among foragers on searching times and group formation\n\n**Explanation:** The correct answer is B) because the third part of the thesis explicitly states that the primary objective of the mathematical models in Chapter 3 is to investigate the effect that communication among foragers has on searching times and the formation of groups. This requires an understanding of the application of nonequilibrium statistical physics to ecological problems, specifically the modeling of animal behavior and collective movement.\n\nThe other options are incorrect because:\n\n* A) is not mentioned as a primary objective of the models in Chapter 3.\n* C) is mentioned in the fourth part of the thesis, which focuses on the effect of stochastic temporal disorder on systems formed by many interacting particles.\n* D) is not mentioned at all in the provided documentation."}, "13": {"documentation": {"title": "Systemic Risk: Conditional Distortion Risk Measures", "source": "Jan Dhaene, Roger J. A. Laeven, Yiying Zhang", "docs_id": "1901.04689", "section": ["q-fin.RM", "stat.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Systemic Risk: Conditional Distortion Risk Measures. In this paper, we introduce the rich classes of conditional distortion (CoD) risk measures and distortion risk contribution ($\\Delta$CoD) measures as measures of systemic risk and analyze their properties and representations. The classes include the well-known conditional Value-at-Risk, conditional Expected Shortfall, and risk contribution measures in terms of the VaR and ES as special cases. Sufficient conditions are presented for two random vectors to be ordered by the proposed CoD-risk measures and distortion risk contribution measures. These conditions are expressed using the conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions. Numerical examples are provided to illustrate our theoretical findings. This paper is the second in a triplet of papers on systemic risk by the same authors. In \\cite{DLZorder2018a}, we introduce and analyze some new stochastic orders related to systemic risk. In a third (forthcoming) paper, we attribute systemic risk to the different participants in a given risky environment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the sufficient condition for two random vectors to be ordered by the proposed CoD-risk measures and distortion risk contribution measures, and how are these conditions expressed using conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions?\n\n**A)** The sufficient condition is that the random vectors are stochastically dominated by each other, and the conditions are expressed using the increasing convex/concave order of the marginals and canonical positive/negative stochastic dependence notions.\n\n**B)** The sufficient condition is that the random vectors are dispersive and have excess wealth in the same direction, and the conditions are expressed using the conventional stochastic dominance and increasing convex/concave orders of the marginals and canonical positive/negative stochastic dependence notions.\n\n**C)** The sufficient condition is that the random vectors are ordered by the VaR and ES as special cases, and the conditions are expressed using the conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions.\n\n**D)** The sufficient condition is that the random vectors are ordered by the CoD-risk measures and distortion risk contribution measures, and the conditions are expressed using the conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions.\n\n**Correct Answer:** C) The sufficient condition is that the random vectors are ordered by the VaR and ES as special cases, and the conditions are expressed using the conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions.\n\n**Explanation:** The correct answer is C) because the paper introduces the CoD-risk measures and distortion risk contribution measures as measures of systemic risk, and analyzes their properties and representations. The sufficient conditions for two random vectors to be ordered by these measures are expressed using the conventional stochastic dominance, increasing convex/concave, dispersive, and excess wealth orders of the marginals and canonical positive/negative stochastic dependence notions. The VaR and ES are special cases of the CoD-risk measures, and the conditions for ordering by these measures are expressed in terms of these special cases."}, "14": {"documentation": {"title": "Quantifying the relative effects of environmental and direct\n  transmission of norovirus", "source": "Sherry Towers, Jun Chen, Carlos Cruz, Steven Madler, Juan Melendez,\n  Jennifer Rodriguez, Armando Salinas, Fan Yu, Yun Kang", "docs_id": "1705.10441", "section": ["q-bio.QM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantifying the relative effects of environmental and direct\n  transmission of norovirus. Background: Norovirus is a common cause of outbreaks of acute gastroenteritis in health- and child-care settings, with serial outbreaks also frequently observed aboard cruise ships. The relative contributions of environmental and direct person-to-person transmission of norovirus has hitherto not been quantified. Objective: We employ a novel mathematical model of norovirus transmission, and fit the model to daily incidence data from a major norovirus outbreak on a cruise ship, and examine the relative efficacy of potential control strategies aimed at reducing environmental and/or direct transmission. Results: The reproduction number for environmental and direct transmission combined is Rtot = 11.0 [9.4,15.6], and of environmental transmission alone is Renviron = 0.85 [0.18,2.04]. Direct transmission is overwhelmingly due to 0 passenger-to-passenger contacts, but crew can act as a reservoir of infection from cruise-to-cruise. Implications: This is the first quantification of the relative roles of environmental and direct transmission of norovirus. While environmental transmission has the potential to maintain a sustained series of outbreaks aboard a cruise ship in the absence of strict sanitation practices, direct transmission dominates. Quarantine of ill passengers and cleaning are likely to have little impact on final outbreak size, but intensive promotion of good hand washing practices can prevent outbreaks."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A cruise ship is experiencing a norovirus outbreak, with a total of 500 passengers and 200 crew members on board. The reproduction number for environmental transmission alone is estimated to be 0.85 [0.18,2.04]. If the ship's sanitation practices are not improved, and the outbreak continues to spread, what is the expected number of secondary cases of norovirus that will occur among the passengers and crew, assuming an average transmission rate of 0.5 per contact?\n\n**A)** 250\n**B)** 375\n**C)** 500\n**D)** 750\n\n**Correct Answer:** B) 375\n\n**Explanation:** To solve this problem, we need to use the reproduction number (Renviron = 0.85) to estimate the expected number of secondary cases. Since the transmission rate is 0.5 per contact, we can use the formula:\n\nExpected number of secondary cases = Renviron x (number of contacts per person) x (number of people)\n\nAssuming an average transmission rate of 0.5 per contact, and using the estimated Renviron value, we can calculate the expected number of secondary cases:\n\nExpected number of secondary cases = 0.85 x 0.5 x (500 + 200) = 375\n\nTherefore, the correct answer is B) 375."}, "15": {"documentation": {"title": "The Binary Dwarf Carbon Star SDSS J125017.90+252427.6", "source": "Bruce Margon, Thomas Kupfer, Kevin Burdge, Thomas A. Prince, Shrinivas\n  R. Kulkarni, and David L. Shupe", "docs_id": "1803.01052", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Binary Dwarf Carbon Star SDSS J125017.90+252427.6. Although dwarf carbon (dC) stars are thought universally to be binaries to explain the presence of $C_2$ in their spectra while still near main sequence luminosity, direct observational evidence for binarity is remarkably scarce. Here we report the detection of a 2.92 d periodicity in both photometry and radial velocity of SDSS J125017.90+252427.6, an $r=16.4$ dC star. This is the first photometric binary dC, and only the second dC spectroscopic binary. The relative phase of the photometric period to the spectroscopic observations suggests that the photometric variations are a reflection effect due to heating from an unseen companion. The observed radial velocity amplitude of the dC component ($K = 98.8\\pm10.7$ km/s) is consistent with a white dwarf companion, presumably the evolved star that earlier donated the carbon to the dC, although substantial orbital evolution must have occurred. Large synoptic photometric surveys such as the Palomar Transient Factory, used for this work, may prove useful for identifying binaries among the shorter period dC stars."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the nature of the companion star to SDSS J125017.90+252427.6, based on the observed radial velocity amplitude of the dC component?\n\nA) The companion star is a red giant, causing the dC component to be in a state of tidal disruption.\nB) The companion star is a white dwarf, consistent with the observed radial velocity amplitude of the dC component.\nC) The companion star is a neutron star, causing the dC component to be in a state of gravitational tidal heating.\nD) The companion star is a black hole, causing the dC component to be in a state of accretion disk formation.\n\nCorrect Answer: B) The companion star is a white dwarf, consistent with the observed radial velocity amplitude of the dC component.\n\nExplanation: The observed radial velocity amplitude of the dC component (K = 98.8 \u00b1 10.7 km/s) is consistent with a white dwarf companion, as suggested by the text. This is because the amplitude of the radial velocity curve is typically related to the mass of the companion star, and a white dwarf would have a relatively small mass compared to other types of companions. The text also mentions that the companion star is presumably the evolved star that earlier donated the carbon to the dC, which further supports the idea that the companion is a white dwarf."}, "16": {"documentation": {"title": "Entropy production estimation with optimal current", "source": "Tan Van Vu and Van Tuan Vo and Yoshihiko Hasegawa", "docs_id": "2001.07131", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entropy production estimation with optimal current. Entropy production characterizes the thermodynamic irreversibility and reflects the amount of heat dissipated into the environment and free energy lost in nonequilibrium systems. According to the thermodynamic uncertainty relation, we propose a deterministic method to estimate the entropy production from a single trajectory of system states. We explicitly and approximately compute an optimal current that yields the tightest lower bound using predetermined basis currents. Notably, the obtained tightest lower bound is intimately related to the multidimensional thermodynamic uncertainty relation. By proving the saturation of the thermodynamic uncertainty relation in the short-time limit, the exact estimate of the entropy production can be obtained for overdamped Langevin systems, irrespective of the underlying dynamics. For Markov jump processes, because the attainability of the thermodynamic uncertainty relation is not theoretically ensured, the proposed method provides the tightest lower bound for the entropy production. When entropy production is the optimal current, a more accurate estimate can be further obtained using the integral fluctuation theorem. We illustrate the proposed method using three systems: a four-state Markov chain, a periodically driven particle, and a multiple bead-spring model. The estimated results in all examples empirically verify the effectiveness and efficiency of the proposed method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed method for estimating entropy production, and how does it relate to the thermodynamic uncertainty relation?\n\nA) The method provides an exact estimate of entropy production for all systems, regardless of the underlying dynamics.\nB) The method provides a tight lower bound on entropy production, which is intimately related to the multidimensional thermodynamic uncertainty relation.\nC) The method is only applicable to overdamped Langevin systems and does not account for Markov jump processes.\nD) The method is computationally intensive and requires a large number of basis currents to achieve accurate results.\n\n**Correct Answer:** B) The method provides a tight lower bound on entropy production, which is intimately related to the multidimensional thermodynamic uncertainty relation.\n\n**Explanation:** The proposed method provides a tight lower bound on entropy production, which is closely related to the multidimensional thermodynamic uncertainty relation. This relationship allows for the estimation of entropy production from a single trajectory of system states, making the method a valuable tool for understanding nonequilibrium systems. The correct answer highlights the key advantage of the proposed method, which is its ability to provide a tight lower bound on entropy production."}, "17": {"documentation": {"title": "A potential scenario for the Majorana neutrino detection at future\n  lepton colliders", "source": "Yang Zhang, Bin Zhang", "docs_id": "1805.09520", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A potential scenario for the Majorana neutrino detection at future\n  lepton colliders. The existence of Majorana neutrinos must lead to lepton-number violating processes, and the Majorana nature of neutrinos can only be experimentally verified via lepton-number violating processes. We propose a new approach to search for Majorana neutrinos at future electron-positron colliders by exploiting this feature. We investigate the $\\Delta L = 2$ like-sign dilepton production and find that lepton colliders with different center-of-mass energies have comparative advantages in resonant production of a Majorana neutrino in either light neutrino mass range or heavy mass range. At the future Circular Electron-Positron Collider (CEPC), with 250 GeV center-of-mass energy and 5 ab$^{-1}$ integrated luminosity, we find that there could be more significant sensitivity for resonant production of a Majorana neutrino in the mass range of 5-80 GeV than previous results at LEP2 or LHC. At the 1 TeV ILC with 1 ab$^{-1}$ integrated luminosity, it has better sensitivity than the similar process at LHC while the neutrino mass is larger than 250 GeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed approach to search for Majorana neutrinos at future electron-positron colliders, and how does the center-of-mass energy of the collider impact the sensitivity to resonant production of a Majorana neutrino?\n\nA) The proposed approach exploits the lepton-number violating processes that arise from the existence of Majorana neutrinos, and the center-of-mass energy of the collider affects the production of dilepton pairs in the $\\Delta L = 2$ channel.\n\nB) The proposed approach relies on the detection of Majorana neutrinos through their decay into a lepton and a neutrino, and the center-of-mass energy of the collider determines the mass range of the neutrino that can be produced.\n\nC) The proposed approach involves the use of a new detector technology that can distinguish between Majorana and non-Majorana neutrinos, and the center-of-mass energy of the collider affects the energy spectrum of the dilepton pairs produced.\n\nD) The proposed approach is based on the idea that Majorana neutrinos can be produced through the annihilation of a lepton and an antilepton, and the center-of-mass energy of the collider determines the energy of the annihilation products.\n\nCorrect Answer: A) The proposed approach exploits the lepton-number violating processes that arise from the existence of Majorana neutrinos, and the center-of-mass energy of the collider affects the production of dilepton pairs in the $\\Delta L = 2$ channel.\n\nExplanation: The correct answer is A) because the proposed approach to search for Majorana neutrinos at future electron-positron colliders exploits the lepton-number violating processes that arise from the existence of Majorana neutrinos. The center-of-mass energy of the collider affects the production of dilepton pairs in the $\\Delta L = 2$ channel, which is a key process for detecting Majorana neutrinos. The other options are incorrect because they do not accurately describe the proposed approach or the relationship between the center-of-mass energy and the sensitivity to resonant production of a Majorana neutrino."}, "18": {"documentation": {"title": "TUJU21: NNLO nuclear parton distribution functions with\n  electroweak-boson production data from the LHC", "source": "Ilkka Helenius, Marina Walt, Werner Vogelsang", "docs_id": "2112.11904", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TUJU21: NNLO nuclear parton distribution functions with\n  electroweak-boson production data from the LHC. We present new sets of nuclear parton distribution functions (nPDFs) at next-to-leading order and next-to-next-to-leading order in perturbative QCD. Our analyses are based on deeply inelastic scattering data with charged-lepton and neutrino beams on nuclear targets, and experimental data from measurements of $W^{\\pm},\\,Z$ boson production in p+Pb collisions at the LHC. In addition, a set of proton baseline PDFs is fitted within the same framework and with the same theoretical assumptions. The results of our global QCD analysis are compared to existing nPDF sets and to the previous nPDF set TUJU19 which was based on DIS data only. Our work is performed using an open-source tool, xFitter, and the required extensions of the code are discussed as well. We find good agreement with the data included in the fit and a lower value for $\\chi^2/N_{\\mathrm{dp}}$ when performing the fit at next-to-next-to-leading order. We apply the resulting nuclear PDFs to electroweak-boson production in Pb+Pb collisions at the LHC and compare the results to the most recent data from ATLAS and CMS."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the development of new nuclear parton distribution functions (nPDFs) at next-to-leading order and next-to-next-to-leading order in perturbative QCD, as presented in the TUJU21 paper?\n\nA) To improve the accuracy of proton baseline PDFs\nB) To enhance the description of electroweak-boson production in p+Pb collisions at the LHC\nC) To provide a more comprehensive framework for global QCD analysis, incorporating both deeply inelastic scattering data and experimental data from LHC measurements\nD) To develop a new open-source tool for fitting nPDFs, xFitter\n\nCorrect Answer: C) To provide a more comprehensive framework for global QCD analysis, incorporating both deeply inelastic scattering data and experimental data from LHC measurements\n\nExplanation: The TUJU21 paper presents new sets of nPDFs at next-to-leading order and next-to-next-to-leading order, which are developed within a global QCD analysis framework that incorporates both deeply inelastic scattering data and experimental data from LHC measurements. This framework aims to provide a more comprehensive description of nuclear parton distributions and their implications for electroweak-boson production in p+Pb collisions."}, "19": {"documentation": {"title": "Spectrum Cartography via Coupled Block-Term Tensor Decomposition", "source": "Guoyong Zhang, Xiao Fu, Jun Wang, Xi-Le Zhao, Mingyi Hong", "docs_id": "1911.12468", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectrum Cartography via Coupled Block-Term Tensor Decomposition. Spectrum cartography aims at estimating power propagation patterns over a geographical region across multiple frequency bands (i.e., a radio map)---from limited samples taken sparsely over the region. Classic cartography methods are mostly concerned with recovering the aggregate radio frequency (RF) information while ignoring the constituents of the radio map---but fine-grained emitter-level RF information is of great interest. In addition, many existing cartography methods work explicitly or implicitly assume random spatial sampling schemes that may be difficult to implement, due to legal/privacy/security issues. The theoretical aspects (e.g., identifiability of the radio map) of many existing methods are also unclear. In this work, we propose a joint radio map recovery and disaggregation method that is based on coupled block-term tensor decomposition. Our method guarantees identifiability of the individual radio map of \\textit{each emitter} (thereby that of the aggregate radio map as well), under realistic conditions. The identifiability result holds under a large variety of geographical sampling patterns, including a number of pragmatic systematic sampling strategies. We also propose effective optimization algorithms to carry out the formulated radio map disaggregation problems. Extensive simulations are employed to showcase the effectiveness of the proposed approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Spectrum Cartography via Coupled Block-Term Tensor Decomposition\n\n**A)** The proposed method in this work guarantees identifiability of the individual radio map of each emitter, but only under idealized conditions of uniform spatial sampling.\n\n**B)** The identifiability result of the proposed method holds under a large variety of geographical sampling patterns, including a number of pragmatic systematic sampling strategies.\n\n**C)** The proposed method is based on a classic cartography approach that ignores the constituents of the radio map and focuses on recovering aggregate RF information.\n\n**D)** The theoretical aspects of the proposed method are unclear, and many existing methods work explicitly or implicitly assume random spatial sampling schemes that may be difficult to implement.\n\n**Correct Answer:** B) The identifiability result of the proposed method holds under a large variety of geographical sampling patterns, including a number of pragmatic systematic sampling strategies.\n\n**Explanation:** The correct answer is B) because the documentation states that the identifiability result of the proposed method \"holds under a large variety of geographical sampling patterns, including a number of pragmatic systematic sampling strategies.\" This indicates that the method is robust and can handle different types of sampling schemes, making option B the correct answer. Options A, C, and D are incorrect because they either misrepresent the method's assumptions or its theoretical aspects."}, "20": {"documentation": {"title": "Optimising experimental design in neutron reflectometry", "source": "James H. Durant, Lucas Wilkins and Joshaniel F. K. Cooper", "docs_id": "2108.05605", "section": ["physics.data-an", "cond-mat.soft", "physics.comp-ph", "physics.med-ph", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimising experimental design in neutron reflectometry. Using the Fisher information (FI), the design of neutron reflectometry experiments can be optimised, leading to greater confidence in parameters of interest and better use of experimental time [Durant, Wilkins, Butler, & Cooper (2021). J. Appl. Cryst. 54, 1100-1110]. In this work, the FI is utilised in optimising the design of a wide range of reflectometry experiments. Two lipid bilayer systems are investigated to determine the optimal choice of measurement angles and liquid contrasts, in addition to the ratio of the total counting time that should be spent measuring each condition. The reduction in parameter uncertainties with the addition of underlayers to these systems is then quantified, using the FI, and validated through the use of experiment simulation and Bayesian sampling methods. For a \"one-shot\" measurement of a degrading lipid monolayer, it is shown that the common practice of measuring null-reflecting water is indeed optimal, but that the optimal measurement angle is dependent on the deuteration state of the monolayer. Finally, the framework is used to demonstrate the feasibility of measuring magnetic signals as small as $0.01\\mu_{B}/\\text{atom}$ in layers only $20\\r{A}$ thick, given the appropriate experimental design, and that time to reach a given level of confidence in the small magnetic moment is quantifiable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Optimising experimental design in neutron reflectometry\" use the Fisher information (FI) to optimise the design of neutron reflectometry experiments. What is the primary advantage of using the FI in this context, and how does it relate to the reduction in parameter uncertainties?\n\n**A)** The FI allows for the optimal allocation of experimental time, leading to a reduction in parameter uncertainties. However, this is only true for systems with a large number of parameters to be estimated.\n\n**B)** The FI enables the optimisation of measurement angles and liquid contrasts, resulting in a more efficient use of experimental resources and a reduction in parameter uncertainties. Additionally, the FI can be used to quantify the reduction in parameter uncertainties.\n\n**C)** The FI is used to determine the optimal ratio of total counting time for each condition, but it does not directly impact the reduction in parameter uncertainties. The reduction in parameter uncertainties is primarily due to the addition of underlayers to the systems.\n\n**D)** The FI is a measure of the information gained from an experiment, but it does not provide a direct method for optimising experimental design. The reduction in parameter uncertainties is a result of the experimental design, not the FI itself.\n\n**Correct Answer:** B) The FI enables the optimisation of measurement angles and liquid contrasts, resulting in a more efficient use of experimental resources and a reduction in parameter uncertainties. Additionally, the FI can be used to quantify the reduction in parameter uncertainties.\n\n**Explanation:** The correct answer is B) because the FI is used to optimise the design of neutron reflectometry experiments, including the measurement angles and liquid contrasts. This optimisation leads to a more efficient use of experimental resources and a reduction in parameter uncertainties. Additionally, the FI can be used to quantify the reduction in parameter uncertainties, which is a key aspect of the paper. The other options are incorrect because they either oversimplify the role of the FI or misrepresent its relationship to the reduction in parameter uncertainties."}, "21": {"documentation": {"title": "Reconstruction of financial network for robust estimation of systemic\n  risk", "source": "Iacopo Mastromatteo, Elia Zarinelli, Matteo Marsili", "docs_id": "1109.6210", "section": ["q-fin.RM", "cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconstruction of financial network for robust estimation of systemic\n  risk. In this paper we estimate the propagation of liquidity shocks through interbank markets when the information about the underlying credit network is incomplete. We show that techniques such as Maximum Entropy currently used to reconstruct credit networks severely underestimate the risk of contagion by assuming a trivial (fully connected) topology, a type of network structure which can be very different from the one empirically observed. We propose an efficient message-passing algorithm to explore the space of possible network structures, and show that a correct estimation of the network degree of connectedness leads to more reliable estimations for systemic risk. Such algorithm is also able to produce maximally fragile structures, providing a practical upper bound for the risk of contagion when the actual network structure is unknown. We test our algorithm on ensembles of synthetic data encoding some features of real financial networks (sparsity and heterogeneity), finding that more accurate estimations of risk can be achieved. Finally we find that this algorithm can be used to control the amount of information regulators need to require from banks in order to sufficiently constrain the reconstruction of financial networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Reconstruction of financial network for robust estimation of systemic risk\", what is the primary limitation of Maximum Entropy techniques used to reconstruct credit networks, and how does the proposed algorithm address this limitation?\n\n**A)** Maximum Entropy techniques assume a fully connected topology, which can lead to underestimation of contagion risk, and the proposed algorithm uses a more realistic network structure to estimate systemic risk.\n\n**B)** Maximum Entropy techniques assume a fully connected topology, which can lead to overestimation of contagion risk, and the proposed algorithm uses a more realistic network structure to estimate systemic risk.\n\n**C)** Maximum Entropy techniques assume a fully connected topology, which can lead to underestimation of contagion risk, and the proposed algorithm uses a more realistic network structure to estimate systemic risk, but only for sparse networks.\n\n**D)** Maximum Entropy techniques assume a fully connected topology, which can lead to overestimation of contagion risk, and the proposed algorithm uses a more realistic network structure to estimate systemic risk, but only for heterogeneous networks.\n\n**Correct Answer:** A) Maximum Entropy techniques assume a fully connected topology, which can lead to underestimation of contagion risk, and the proposed algorithm uses a more realistic network structure to estimate systemic risk.\n\n**Explanation:** The correct answer is A) because the paper states that Maximum Entropy techniques \"severely underestimate the risk of contagion by assuming a trivial (fully connected) topology, a type of network structure which can be very different from the one empirically observed.\" The proposed algorithm addresses this limitation by using a more realistic network structure to estimate systemic risk."}, "22": {"documentation": {"title": "The VERITAS Trigger System", "source": "A. Weinstein (for the VERITAS Collaboration)", "docs_id": "0709.4438", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The VERITAS Trigger System. The VERITAS gamma-ray observatory, situated in southern Arizona, is an array of four 12m diameter imaging Cherenkov telescopes, each with a 499-pixel photomultiplier-tube camera. The instrument is designed to detect astrophysical gamma rays at energies above 100 GeV. At the low end of the VERITAS energy range, fluctuations in the night sky background light and single muons from cosmic-ray showers constitute significant backgrounds. VERITAS employs a three-tier trigger system to reduce the rate of these background events: an initial trigger which acts at the single pixel level, a pattern trigger which acts on the relative timing and pixel level, a pattern trigger which acts on the relative timing and distribution of pixel-level triggers within a single telescope camera, and an array-level trigger which requires simultaneous observation of an air-shower event in multiple telescopes. This final coincidence requirement significantly reduces the rate of background events, particularly those due to single muons. In this paper, the implementation of all levels of the VERITAS trigger system is discussed and their joint performance is characterized."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary purpose of the three-tier trigger system employed by the VERITAS gamma-ray observatory to reduce background events, and how does the array-level trigger contribute to this reduction?\n\nA) To increase the sensitivity of the instrument to high-energy gamma rays, while reducing the rate of background events due to single muons and fluctuations in the night sky background light.\nB) To improve the timing accuracy of the instrument, while reducing the rate of background events due to single muons and fluctuations in the night sky background light.\nC) To enhance the spatial resolution of the instrument, while reducing the rate of background events due to single muons and fluctuations in the night sky background light.\nD) To increase the coincidence requirement for background events, thereby reducing the rate of background events due to single muons and fluctuations in the night sky background light.\n\nCorrect Answer: A) To increase the sensitivity of the instrument to high-energy gamma rays, while reducing the rate of background events due to single muons and fluctuations in the night sky background light.\n\nExplanation: The three-tier trigger system is designed to reduce the rate of background events, which are significant at the low end of the VERITAS energy range. The initial trigger acts at the single pixel level, the pattern trigger acts on the relative timing and pixel level, and the pattern trigger acts on the relative timing and distribution of pixel-level triggers within a single telescope camera. The array-level trigger requires simultaneous observation of an air-shower event in multiple telescopes, which significantly reduces the rate of background events, particularly those due to single muons. Therefore, the primary purpose of the three-tier trigger system is to increase the sensitivity of the instrument to high-energy gamma rays while reducing the rate of background events."}, "23": {"documentation": {"title": "Manifestation of {\\alpha}-clustering in $^{10}$Be via {\\alpha}-knockout\n  reaction", "source": "Mengjiao Lyu, Kazuki Yoshida, Yoshiko Kanada-En'yo and Kazuyuki Ogata", "docs_id": "1712.09753", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Manifestation of {\\alpha}-clustering in $^{10}$Be via {\\alpha}-knockout\n  reaction. Background: Proton-induced {\\alpha}-knockout reactions empower direct experimental manifestations of {\\alpha}-clustering in nuclei. This is obtained by relating the theoretical descriptions of clustering states with experimental reaction observables. It is desired to introduce microscopic structure models into the theoretical frameworks for {\\alpha}-knockout reactions. Purpose: Our goal is to probe the {\\alpha}-clustering in $^{10}$Be nucleus by proton-induced {\\alpha}-knockout reaction observables. Method: We adopt an extended version of the Tohsaki-Horiuchi-Schuck-R\\\"opke (THSR) wave function of $^{10}$Be and integrate it with the distorted wave impulse approximation (DWIA) framework for the calculation of (p,p{\\alpha}) knockout reactions. Results: We make the first calculation for the $^{10}$Be(p,p{\\alpha})$^{6}$He reaction at 250 MeV implementing a microscopic {\\alpha}-cluster wave function and predict the triple differential cross sections (TDX). Furthermore, by constructing artificial states of the target nucleus $^{10}$Be with compact or dilute spatial distributions, the TDX is found to be highly sensitive to the extent of clustering in the target nuclei. Conclusions: These results provide reliable manifestation of the {\\alpha}-clustering in $^{10}$Be."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the authors in their study on proton-induced \u03b1-knockout reactions to probe \u03b1-clustering in the 10Be nucleus?\n\nA) To investigate the nuclear stability of 10Be\nB) To calculate the reaction cross-sections for the 10Be(p,p\u03b1)6He reaction\nC) To introduce microscopic structure models into the theoretical frameworks for \u03b1-knockout reactions\nD) To determine the energy levels of the 10Be nucleus\n\nCorrect Answer: C) To introduce microscopic structure models into the theoretical frameworks for \u03b1-knockout reactions\n\nExplanation: The question requires the test-taker to understand the purpose of the study, which is to introduce microscopic structure models into the theoretical frameworks for \u03b1-knockout reactions. This is stated in the introduction of the document: \"Our goal is to probe the \u03b1-clustering in 10Be nucleus by proton-induced \u03b1-knockout reaction observables.\" The correct answer, C, is the one that aligns with this purpose. The other options are incorrect because they are either secondary goals or unrelated to the primary objective of the study. \n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the document to arrive at the correct answer. \n\nHere are the other options with explanations:\n\nA) To investigate the nuclear stability of 10Be: This is a related but secondary goal, as the study does investigate the nuclear stability of 10Be, but it is not the primary goal.\n\nB) To calculate the reaction cross-sections for the 10Be(p,p\u03b1)6He reaction: This is a method used in the study, but it is not the primary goal.\n\nD) To determine the energy levels of the 10Be nucleus: This is not mentioned in the document as a goal of the study."}, "24": {"documentation": {"title": "Cooperative Data Exchange with Unreliable Clients", "source": "Anoosheh Heidarzadeh and Alex Sprintson", "docs_id": "1508.03871", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cooperative Data Exchange with Unreliable Clients. Consider a set of clients in a broadcast network, each of which holds a subset of packets in the ground set X. In the (coded) cooperative data exchange problem, the clients need to recover all packets in X by exchanging coded packets over a lossless broadcast channel. Several previous works analyzed this problem under the assumption that each client initially holds a random subset of packets in X. In this paper we consider a generalization of this problem for settings in which an unknown (but of a certain size) subset of clients are unreliable and their packet transmissions are subject to arbitrary erasures. For the special case of one unreliable client, we derive a closed-form expression for the minimum number of transmissions required for each reliable client to obtain all packets held by other reliable clients (with probability approaching 1 as the number of packets tends to infinity). Furthermore, for the cases with more than one unreliable client, we provide an approximation solution in which the number of transmissions per packet is within an arbitrarily small additive factor from the value of the optimal solution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the cooperative data exchange problem with unreliable clients, consider a broadcast network where each client holds a subset of packets in the ground set X. Suppose there are two unreliable clients, each holding a subset of packets, and the reliable clients need to recover all packets in X by exchanging coded packets over a lossless broadcast channel. If the unreliable clients are randomly selected from the set of all clients, and the probability of erasure for each unreliable client is 0.1, what is the minimum expected number of transmissions required for each reliable client to obtain all packets held by the other reliable clients, assuming that the number of packets in X tends to infinity?\n\n**A)** 2.5\n**B)** 3.2\n**C)** 4.1\n**D)** 5.0\n\n**Correct Answer:** B) 3.2\n\n**Explanation:** To solve this problem, we need to consider the expected number of transmissions required for each reliable client to obtain all packets held by the other reliable clients. Let's denote the number of packets in X as n. Since there are two unreliable clients, each with a probability of 0.1 of erasure, the expected number of transmissions required for each reliable client to obtain all packets held by the other reliable clients can be calculated using the formula:\n\nE[T] = (1 - p) / (1 - p^2)\n\nwhere p is the probability of erasure for each unreliable client. Plugging in p = 0.1, we get:\n\nE[T] = (1 - 0.1) / (1 - 0.1^2) = 0.9 / 0.99 = 3.2\n\nTherefore, the minimum expected number of transmissions required for each reliable client to obtain all packets held by the other reliable clients is 3.2."}, "25": {"documentation": {"title": "Attention Based Real Image Restoration", "source": "Saeed Anwar, Nick Barnes, and Lars Petersson", "docs_id": "2004.13524", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Attention Based Real Image Restoration. Deep convolutional neural networks perform better on images containing spatially invariant degradations, also known as synthetic degradations; however, their performance is limited on real-degraded photographs and requires multiple-stage network modeling. To advance the practicability of restoration algorithms, this paper proposes a novel single-stage blind real image restoration network (R$^2$Net) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality for four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal, and JPEG Compression on 11 real degraded datasets against more than 30 state-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We also present the comparison on three synthetically generated degraded datasets for denoising to showcase the capability of our method on synthetics denoising. The codes, trained models, and results are available on https://github.com/saeed-anwar/R2Net."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed R$^2$Net architecture in blind real image restoration, and how does it differ from existing deep convolutional neural networks?\n\nA) R$^2$Net uses a multi-stage network modeling approach, which is more computationally efficient than existing methods.\nB) R$^2$Net employs a residual on the residual structure to ease the flow of low-frequency information, allowing for more efficient processing of real-degraded photographs.\nC) R$^2$Net uses a feature attention mechanism to exploit channel dependencies, but does not address the issue of spatially invariant degradations.\nD) R$^2$Net is a multi-stage network modeling approach that requires multiple stages to achieve good performance on real-degraded photographs.\n\nCorrect Answer: B) R$^2$Net employs a residual on the residual structure to ease the flow of low-frequency information, allowing for more efficient processing of real-degraded photographs.\n\nExplanation: The correct answer is B) because the documentation states that the proposed R$^2$Net architecture employs a residual on the residual structure to ease the flow of low-frequency information, which is a key advantage in processing real-degraded photographs. This is in contrast to existing deep convolutional neural networks, which perform better on images containing spatially invariant degradations (synthetic degradations) but struggle with real-degraded photographs."}, "26": {"documentation": {"title": "Neutrino Transfer in Three Dimensions for Core-Collapse Supernovae. I.\n  Static Configurations", "source": "Kohsuke Sumiyoshi, Shoichi Yamada", "docs_id": "1201.2244", "section": ["astro-ph.HE", "nucl-th", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutrino Transfer in Three Dimensions for Core-Collapse Supernovae. I.\n  Static Configurations. We develop a numerical code to calculate the neutrino transfer with multi-energy and multi-angle in three dimensions (3D) for the study of core-collapse supernovae. The numerical code solves the Boltzmann equations for neutrino distributions by the discrete-ordinate (S_n) method with a fully implicit differencing for time advance. The Boltzmann equations are formulated in the inertial frame with collision terms being evaluated to the zeroth order of v/c. A basic set of neutrino reactions for three neutrino species is implemented together with a realistic equation of state of dense matter. The pair process is included approximately in order to keep the system linear. We present numerical results for a set of test problems to demonstrate the ability of the code. The numerical treatments of advection and collision terms are validated first in the diffusion and free streaming limits. Then we compute steady neutrino distributions for a background extracted from a spherically symmetric, general relativistic simulation of 15Msun star and compare them with the results in the latter computation. We also demonstrate multi-D capabilities of the 3D code solving neutrino transfers for artificially deformed supernova cores in 2D and 3D. Formal solutions along neutrino paths are utilized as exact solutions. We plan to apply this code to the 3D neutrino-radiation hydrodynamics simulations of supernovae. This is the first article in a series of reports on the development."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary method used to solve the Boltzmann equations for neutrino distributions in the numerical code developed for studying core-collapse supernovae?\n\nA) The finite difference method with a fully implicit differencing for time advance\nB) The finite element method with a fully implicit differencing for time advance\nC) The discrete-ordinate (S_n) method with a fully implicit differencing for time advance\nD) The finite volume method with a fully implicit differencing for time advance\n\nCorrect Answer: C) The discrete-ordinate (S_n) method with a fully implicit differencing for time advance\n\nExplanation: The correct answer is C) The discrete-ordinate (S_n) method with a fully implicit differencing for time advance. The documentation states that the numerical code solves the Boltzmann equations for neutrino distributions using the discrete-ordinate (S_n) method with a fully implicit differencing for time advance. This method is specifically mentioned in the introduction of the article as the approach used to calculate the neutrino transfer with multi-energy and multi-angle in three dimensions."}, "27": {"documentation": {"title": "UGC-VIDEO: perceptual quality assessment of user-generated videos", "source": "Yang Li, Shengbin Meng, Xinfeng Zhang, Shiqi Wang, Yue Wang, Siwei Ma", "docs_id": "1908.11517", "section": ["cs.MM", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UGC-VIDEO: perceptual quality assessment of user-generated videos. Recent years have witnessed an ever-expandingvolume of user-generated content (UGC) videos available on the Internet. Nevertheless, progress on perceptual quality assessmentof UGC videos still remains quite limited. There are many distinguished characteristics of UGC videos in the complete video production and delivery chain, and one important property closely relevant to video quality is that there does not exist the pristine source after they are uploaded to the hosting platform,such that they often undergo multiple compression stages before ultimately viewed. To facilitate the UGC video quality assessment,we created a UGC video perceptual quality assessment database. It contains 50 source videos collected from TikTok with diverse content, along with multiple distortion versions generated bythe compression with different quantization levels and coding standards. Subjective quality assessment was conducted to evaluate the video quality. Furthermore, we benchmark the database using existing quality assessment algorithms, and potential roomis observed to future improve the accuracy of UGC video quality measures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in perceptual quality assessment of user-generated videos (UGC) on the Internet, and how does the proposed UGC video perceptual quality assessment database address this challenge?\n\n**A)** The primary challenge is the lack of pristine source videos, but the database addresses this by providing multiple distortion versions generated by compression stages.\n\n**B)** The primary challenge is the subjective nature of video quality assessment, but the database addresses this by providing a large dataset of videos with diverse content.\n\n**C)** The primary challenge is the limited availability of quality assessment algorithms, but the database addresses this by benchmarking existing algorithms and identifying potential areas for improvement.\n\n**D)** The primary challenge is the high volume of UGC videos, but the database addresses this by providing a large dataset of 50 source videos collected from TikTok.\n\n**Correct Answer:** C) The primary challenge is the limited progress on perceptual quality assessment of UGC videos, but the database addresses this by benchmarking existing quality assessment algorithms, and potential room is observed to future improve the accuracy of UGC video quality measures.\n\n**Explanation:** The correct answer is C) because the documentation states that \"progress on perceptual quality assessment of UGC videos still remains quite limited\" and that the database \"benchmark the database using existing quality assessment algorithms, and potential room is observed to future improve the accuracy of UGC video quality measures.\" This indicates that the primary challenge is the limited progress on perceptual quality assessment, and that the database addresses this challenge by benchmarking existing algorithms and identifying areas for improvement."}, "28": {"documentation": {"title": "Fast Distributionally Robust Learning with Variance Reduced Min-Max\n  Optimization", "source": "Yaodong Yu, Tianyi Lin, Eric Mazumdar, Michael I. Jordan", "docs_id": "2104.13326", "section": ["cs.LG", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast Distributionally Robust Learning with Variance Reduced Min-Max\n  Optimization. Distributionally robust supervised learning (DRSL) is emerging as a key paradigm for building reliable machine learning systems for real-world applications -- reflecting the need for classifiers and predictive models that are robust to the distribution shifts that arise from phenomena such as selection bias or nonstationarity. Existing algorithms for solving Wasserstein DRSL -- one of the most popular DRSL frameworks based around robustness to perturbations in the Wasserstein distance -- involve solving complex subproblems or fail to make use of stochastic gradients, limiting their use in large-scale machine learning problems. We revisit Wasserstein DRSL through the lens of min-max optimization and derive scalable and efficiently implementable stochastic extra-gradient algorithms which provably achieve faster convergence rates than existing approaches. We demonstrate their effectiveness on synthetic and real data when compared to existing DRSL approaches. Key to our results is the use of variance reduction and random reshuffling to accelerate stochastic min-max optimization, the analysis of which may be of independent interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of Distributionally Robust Supervised Learning (DRSL) frameworks, and how do existing algorithms for solving Wasserstein DRSL address this need?\n\n**A)** DRSL is motivated by the need for machine learning models to be robust to changes in the underlying data distribution, and existing algorithms for solving Wasserstein DRSL aim to improve the convergence rates of stochastic gradient descent methods.\n\n**B)** DRSL is motivated by the need for machine learning models to be robust to changes in the underlying data distribution, and existing algorithms for solving Wasserstein DRSL aim to improve the robustness of models to perturbations in the Wasserstein distance.\n\n**C)** DRSL is motivated by the need for machine learning models to be robust to changes in the underlying data distribution, and existing algorithms for solving Wasserstein DRSL aim to improve the scalability of models for large-scale machine learning problems.\n\n**D)** DRSL is motivated by the need for machine learning models to be robust to changes in the underlying data distribution, and existing algorithms for solving Wasserstein DRSL aim to improve the interpretability of models.\n\n**Correct Answer:** B) DRSL is motivated by the need for machine learning models to be robust to changes in the underlying data distribution, and existing algorithms for solving Wasserstein DRSL aim to improve the robustness of models to perturbations in the Wasserstein distance.\n\n**Explanation:** The correct answer is B) because the motivation behind DRSL is to build machine learning systems that are robust to distribution shifts, such as selection bias or nonstationarity. Existing algorithms for solving Wasserstein DRSL aim to improve the robustness of models to perturbations in the Wasserstein distance, which is a key aspect of DRSL. The other options are incorrect because they do not accurately capture the primary motivation behind DRSL or the focus of existing algorithms for solving Wasserstein DRSL."}, "29": {"documentation": {"title": "Non-congruence of the nuclear liquid-gas and deconfinement phase\n  transitions", "source": "Matthias Hempel, Veronica Dexheimer, Stefan Schramm and Igor\n  Iosilevskiy", "docs_id": "1302.2835", "section": ["nucl-th", "astro-ph.HE", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-congruence of the nuclear liquid-gas and deconfinement phase\n  transitions. First order phase transitions (PTs) with more than one globally conserved charge, so-called non-congruent PTs, have characteristic differences compared to congruent PTs (e.g., dimensionality of phase diagrams, location and properties of critical points and endpoints). In the present article we investigate the non-congruence of the nuclear liquid-gas PT at sub-saturation densities and the deconfinement PT at high densities and/or temperatures in Coulomb-less models, relevant for heavy-ion collisions and neutron stars. For the first PT, we use the FSUgold relativistic mean-field model and for the second one the relativistic chiral SU(3) model. The chiral SU(3) model is one of the few models for the deconfinement PT, which contains quarks and hadrons in arbitrary proportions (i.e. a \"solution\") and gives a continuous transition from pure hadronic to pure quark matter above a critical point. The study shows the universality of the applied concept of non-congruence for the two PTs with an upper critical point, and illustrates the different typical scales involved. In addition, we find a principle difference between the liquid-gas and the deconfinement PTs: in contrast to the ordinary Van-der-Waals-like PT, the phase coexistence line of the deconfinement PT has a negative slope in the pressure-temperature plane. As another qualitative difference we find that the non-congruent features of the deconfinement PT become vanishingly small around the critical point."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary difference in the phase coexistence line of the nuclear liquid-gas phase transition and the deconfinement phase transition, as identified in the study?\n\nA) The liquid-gas phase transition has a negative slope in the pressure-temperature plane, while the deconfinement phase transition has a positive slope.\nB) The liquid-gas phase transition has a positive slope in the pressure-temperature plane, while the deconfinement phase transition has a negative slope.\nC) The liquid-gas phase transition has a negative slope in the pressure-temperature plane, while the deconfinement phase transition has a positive slope, but only at high densities and/or temperatures.\nD) The liquid-gas phase transition has a positive slope in the pressure-temperature plane, while the deconfinement phase transition has a negative slope, but only around the critical point.\n\nCorrect Answer: B) The liquid-gas phase transition has a positive slope in the pressure-temperature plane, while the deconfinement phase transition has a negative slope.\n\nExplanation: The study identifies the deconfinement phase transition as having a negative slope in the pressure-temperature plane, in contrast to the ordinary Van-der-Waals-like phase transition of the nuclear liquid-gas phase transition, which has a positive slope. This difference is a key finding of the study and highlights the distinct characteristics of the two phase transitions."}, "30": {"documentation": {"title": "Floquet-Mode Solutions of Space-Time Modulated Huygens' Metasurfaces", "source": "Shulabh Gupta, Tom. J. Smy and Scott A. Stewart", "docs_id": "1701.05271", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Floquet-Mode Solutions of Space-Time Modulated Huygens' Metasurfaces. A rigorous Floquet mode analysis is proposed for a zero thickness space-time modulated Huygens' metasurface to model and determine the strengths of the new harmonic components of the scattered fields. The proposed method is based on Generalized Sheet Transition Conditions (GSTCs) treating a metasurface as a spatial discontinuity. The metasurface is described in terms of Lorentzian electric and magnetic surface susceptibilities, $\\chi_\\text{ee}$ and $\\chi_\\text{mm}$, respectively, and its resonant frequencies are periodically modulated in both space and time. The unknown scattered fields are then expressed in terms of Floquet modes, which when used with the GSTCs, lead to a system of field matrix equations. The resulting set of linear equations are then solved numerically to determine the total scattered fields. Using a finite-difference time domain (FDTD) solver, the proposed method is validated and confirmed for several examples of modulation depths ($\\Delta_p$) and frequencies ($\\omega_p$). Finally, the computed steady-state scattered fields are Fourier propagated analytically, for visualization of refracted harmonics. The proposed method is simple and versatile and able to determine the steady-state response of a space-time modulated Huygen's metasurface, for arbitrary modulation frequencies and depths."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A space-time modulated Huygens' metasurface is described by Lorentzian electric and magnetic surface susceptibilities, $\\chi_\\text{ee}$ and $\\chi_\\text{mm}$, respectively. The metasurface's resonant frequencies are periodically modulated in both space and time, with a modulation depth of $\\Delta_p = 0.5$ and a modulation frequency of $\\omega_p = 2\\pi \\times 10^9$ rad/s. Using the proposed method, determine the steady-state scattered fields for a metasurface with the following parameters:\n\n$\\chi_\\text{ee} = 1 + 0.2 \\sin(2\\pi \\times 10^9 t)$ and $\\chi_\\text{mm} = 1 + 0.3 \\sin(2\\pi \\times 10^9 t)$\n\nwhere $t$ is time in seconds.\n\n**A)** The steady-state scattered fields can be expressed as a linear combination of Floquet modes, with coefficients determined by solving a system of field matrix equations.\n\n**B)** The proposed method requires a finite-difference time domain (FDTD) solver to numerically solve the system of field matrix equations.\n\n**C)** The steady-state scattered fields can be Fourier propagated analytically to visualize refracted harmonics.\n\n**D)** The modulation depth and frequency are arbitrary and can be chosen as needed.\n\n**Correct Answer:** A) The steady-state scattered fields can be expressed as a linear combination of Floquet modes, with coefficients determined by solving a system of field matrix equations.\n\n**Explanation:**\n\nThe proposed method involves expressing the unknown scattered fields in terms of Floquet modes, which are then used with the Generalized Sheet Transition Conditions (GSTCs) to form a system of field matrix equations. This system of equations is then solved numerically using a finite-difference time domain (FDTD) solver. The resulting coefficients of the Floquet modes determine the steady-state scattered fields.\n\nThe correct answer, A, highlights the key aspect of the proposed method, which is the use of Floquet modes to express the scattered fields. The other options, B, C, and D, are incorrect because they either describe a different aspect of the method (FDTD solver) or imply that the modulation depth and frequency are arbitrary, which is not the case.\n\n**Explanation of incorrect options:**\n\n* B) The FDTD solver is used to numerically solve the system of field matrix equations, but it is not the primary method for determining the steady-state scattered fields.\n* C) Fourier propagation is used to visualize refracted harmonics, but it is not a key aspect of the proposed method for determining the steady-state scattered fields.\n* D) The modulation depth and frequency are not arbitrary and must be specified as $\\Delta_p = 0.5$ and $\\omega_p = 2\\pi \\times 10^9$ rad/s, respectively."}, "31": {"documentation": {"title": "The Indirect Effects of FDI on Trade: A Network Perspective", "source": "Paolo Sgrignoli, Rodolfo Metulini, Zhen Zhu, Massimo Riccaboni", "docs_id": "1705.02187", "section": ["q-fin.GN", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Indirect Effects of FDI on Trade: A Network Perspective. The relationship between international trade and foreign direct investment (FDI) is one of the main features of globalization. In this paper we investigate the effects of FDI on trade from a network perspective, since FDI takes not only direct but also indirect channels from origin to destination countries because of firms' incentive to reduce tax burden, to minimize coordination costs, and to break barriers to market entry. We use a unique data set of international corporate control as a measure of stock FDI to construct a corporate control network (CCN) where the nodes are the countries and the edges are the corporate control relationships. Based on the CCN, the network measures, i.e., the shortest path length and the communicability, are computed to capture the indirect channel of FDI. Empirically we find that corporate control has a positive effect on trade both directly and indirectly. The result is robust with different specifications and estimation strategies. Hence, our paper provides strong empirical evidence of the indirect effects of FDI on trade. Moreover, we identify a number of interplaying factors such as regional trade agreements and the region of Asia. We also find that the indirect effects are more pronounced for manufacturing sectors than for primary sectors such as oil extraction and agriculture."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the indirect effects of Foreign Direct Investment (FDI) on trade, according to the paper \"The Indirect Effects of FDI on Trade: A Network Perspective\"?\n\nA) To increase market share and reduce competition\nB) To minimize coordination costs and reduce tax burden\nC) To promote regional trade agreements and increase economic integration\nD) To increase exports and reduce imports\n\n**Correct Answer:** B) To minimize coordination costs and reduce tax burden\n\n**Explanation:** The paper states that FDI takes not only direct but also indirect channels from origin to destination countries, driven by firms' incentives to reduce tax burden, minimize coordination costs, and break barriers to market entry. This suggests that the primary motivation behind the indirect effects of FDI on trade is to minimize coordination costs and reduce tax burden.\n\n**Additional Questions:**\n\n**Question 2:** What type of data set is used to construct the Corporate Control Network (CCN) in the paper?\n\nA) International trade data\nB) Corporate control data\nC) Economic indicators data\nD) Financial data\n\n**Correct Answer:** B) Corporate control data\n\n**Explanation:** The paper states that a unique data set of international corporate control is used to construct the CCN, where the nodes are the countries and the edges are the corporate control relationships.\n\n**Question 3:** According to the paper, what is the result of the empirical analysis on the effect of corporate control on trade?\n\nA) Corporate control has a negative effect on trade\nB) Corporate control has a positive effect on trade both directly and indirectly\nC) Corporate control has no effect on trade\nD) Corporate control only affects manufacturing sectors\n\n**Correct Answer:** B) Corporate control has a positive effect on trade both directly and indirectly\n\n**Explanation:** The paper states that empirically, corporate control has a positive effect on trade both directly and indirectly, and the result is robust with different specifications and estimation strategies."}, "32": {"documentation": {"title": "Novel Distances for Dollo Data", "source": "Michael Woodhams, Dorothy A. Steane, Rebecca C. Jones, Dean Nicolle,\n  Vincent Moulton, Barbara R. Holland", "docs_id": "1203.0072", "section": ["q-bio.QM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Novel Distances for Dollo Data. We investigate distances on binary (presence/absence) data in the context of a Dollo process, where a trait can only arise once on a phylogenetic tree but may be lost many times. We introduce a novel distance, the Additive Dollo Distance (ADD), which is consistent for data generated under a Dollo model, and show that it has some useful theoretical properties including an intriguing link to the LogDet distance. Simulations of Dollo data are used to compare a number of binary distances including ADD, LogDet, Nei Li and some simple, but to our knowledge previously unstudied, variations on common binary distances. The simulations suggest that ADD outperforms other distances on Dollo data. Interestingly, we found that the LogDet distance performs poorly in the context of a Dollo process, which may have implications for its use in connection with conditioned genome reconstruction. We apply the ADD to two Diversity Arrays Technology (DArT) datasets, one that broadly covers Eucalyptus species and one that focuses on the Eucalyptus series Adnataria. We also reanalyse gene family presence/absence data on bacteria from the COG database and compare the results to previous phylogenies estimated using the conditioned genome reconstruction approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Additive Dollo Distance (ADD) in the context of Dollo data, and how does it differ from the LogDet distance?\n\nA) ADD is more computationally efficient than LogDet, but has a higher risk of overestimating genetic distances. \nB) ADD is more robust to noise in the data than LogDet, but has a lower theoretical consistency with the Dollo model. \nC) ADD is more consistent with the Dollo model than LogDet, and has a lower risk of overestimating genetic distances. \nD) ADD is more sensitive to outliers in the data than LogDet, but has a higher theoretical consistency with the Dollo model.\n\n**Correct answer:** C) ADD is more consistent with the Dollo model than LogDet, and has a lower risk of overestimating genetic distances.\n\n**Explanation:** The correct answer is C) because the documentation states that the ADD is \"consistent for data generated under a Dollo model\", which means it is more consistent with the Dollo process. Additionally, the ADD is shown to outperform other distances on Dollo data, suggesting that it has a lower risk of overestimating genetic distances. The other options are incorrect because they either misrepresent the relationship between ADD and LogDet, or introduce new information not present in the documentation."}, "33": {"documentation": {"title": "Accelerating Polaritons with External Electric and Magnetic Fields", "source": "Thibault Chervy, Patrick Kn\\\"uppel, Hadis Abbaspour, Mirko Lupatini,\n  Stefan F\\\"alt, Werner Wegscheider, Martin Kroner and Atac Imamo\\v{g}lu", "docs_id": "1911.06405", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accelerating Polaritons with External Electric and Magnetic Fields. It is widely assumed that photons cannot be manipulated using electric or magnetic fields. Even though hybridization of photons with electronic polarization to form exciton-polaritons has paved the way to a number of ground-breaking experiments in semiconductor microcavities, the neutral bosonic nature of these quasiparticles has severely limited their response to external gauge fields. Here, we demonstrate polariton acceleration by external electric and magnetic fields in the presence of nonperturbative coupling between polaritons and itinerant electrons, leading to formation of new quasiparticles termed polaron-polaritons. We identify the generation of electron density gradients by the applied fields to be primarily responsible for inducing a gradient in polariton energy, which in turn leads to acceleration along a direction determined by the applied fields. Remarkably, we also observe that different polarization components of the polaritons can be accelerated in opposite directions when the electrons are in $\\nu = 1$ integer quantum Hall state."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Accelerating Polaritons with External Electric and Magnetic Fields\", what is the primary mechanism by which the applied electric and magnetic fields induce a gradient in polariton energy, leading to acceleration?\n\n**A)** The generation of electron density gradients by the applied fields leads to a transfer of momentum from the electrons to the polaritons, resulting in acceleration.\n\n**B)** The nonperturbative coupling between polaritons and itinerant electrons causes the polaritons to experience a force proportional to the applied electric and magnetic fields, leading to acceleration.\n\n**C)** The formation of polaron-polaritons, which are quasiparticles resulting from the hybridization of photons with electronic polarization, leads to a change in the effective mass of the polaritons, causing them to accelerate.\n\n**D)** The applied electric and magnetic fields induce a gradient in the dielectric constant of the semiconductor material, which in turn affects the refractive index of the polaritons, leading to acceleration.\n\n**Correct Answer:** A) The generation of electron density gradients by the applied fields leads to a transfer of momentum from the electrons to the polaritons, resulting in acceleration.\n\n**Explanation:** The correct answer is A) because the paper states that the generation of electron density gradients by the applied fields is \"primarily responsible for inducing a gradient in polariton energy, which in turn leads to acceleration along a direction determined by the applied fields.\" This indicates that the transfer of momentum from the electrons to the polaritons is the primary mechanism by which the applied fields induce acceleration in the polaritons."}, "34": {"documentation": {"title": "Detection of an ancient principle and an elegant solution to the protein\n  classification problem", "source": "Ashok Palaniappan", "docs_id": "0708.2121", "section": ["q-bio.GN", "q-bio.BM", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detection of an ancient principle and an elegant solution to the protein\n  classification problem. This work is concerned with the development of a well-founded, theoretically justified, and least complicated metric for the classification of proteins with reference to enzymes. As the signature of an enzyme family, a catalytic domain is easily fingerprinted. Given that the classification problem has so far seemed intractable, a classification schema derived from the catalytic domain would be satisfying. Here I show that there exists a natural ab initio if nonobvious basis to theorize that the catalytic domain of an enzyme is uniquely informative about its regulation. This annotates its function. Based on this hypothesis, a method that correctly classifies potassium ion channels into their respective subfamilies is described. To put the principle on firmer ground, extra validation was sought and obtained through co-evolutionary analyses. The co-evolutionary analyses reveal a departure from the notion that potassium ion channel proteins are functionally modular. This finding is discussed in light of the prevailing notion of domain. These studies establish that significant co-evolution of the catalytic domain of a gene with its conjoint domain is a specialized, necessary process following fusion and swapping events in evolution. Instances of this discovery are likely to be found pervasive in protein science."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study propose that the catalytic domain of an enzyme is uniquely informative about its regulation and function. What is the primary basis for this hypothesis, and how does it relate to the classification of proteins?\n\n**A)** The catalytic domain is a conserved sequence motif that is present in all enzymes, and its presence is indicative of the enzyme's regulatory properties.\n\n**B)** The catalytic domain is a structural motif that is essential for the enzyme's catalytic activity, and its evolution is closely tied to the evolution of the enzyme's regulatory mechanisms.\n\n**C)** The catalytic domain is a functional motif that is involved in the binding of substrates and co-factors, and its evolution is closely tied to the evolution of the enzyme's regulatory mechanisms.\n\n**D)** The catalytic domain is a genetic element that is co-evolved with the enzyme's regulatory genes, and its presence is indicative of the enzyme's regulatory properties.\n\n**Correct Answer:** B) The catalytic domain is a structural motif that is essential for the enzyme's catalytic activity, and its evolution is closely tied to the evolution of the enzyme's regulatory mechanisms.\n\n**Explanation:** The correct answer is based on the idea that the catalytic domain is a structural motif that is essential for the enzyme's catalytic activity, and its evolution is closely tied to the evolution of the enzyme's regulatory mechanisms. This is supported by the text, which states that \"significant co-evolution of the catalytic domain of a gene with its conjoint domain is a specialized, necessary process following fusion and swapping events in evolution.\" This suggests that the evolution of the catalytic domain is closely tied to the evolution of the enzyme's regulatory mechanisms, and that the catalytic domain is a key factor in determining the enzyme's regulatory properties."}, "35": {"documentation": {"title": "Exploring the Interconnectedness of Cryptocurrencies using Correlation\n  Networks", "source": "Andrew Burnie", "docs_id": "1806.06632", "section": ["q-fin.CP", "cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring the Interconnectedness of Cryptocurrencies using Correlation\n  Networks. Correlation networks were used to detect characteristics which, although fixed over time, have an important influence on the evolution of prices over time. Potentially important features were identified using the websites and whitepapers of cryptocurrencies with the largest userbases. These were assessed using two datasets to enhance robustness: one with fourteen cryptocurrencies beginning from 9 November 2017, and a subset with nine cryptocurrencies starting 9 September 2016, both ending 6 March 2018. Separately analysing the subset of cryptocurrencies raised the number of data points from 115 to 537, and improved robustness to changes in relationships over time. Excluding USD Tether, the results showed a positive association between different cryptocurrencies that was statistically significant. Robust, strong positive associations were observed for six cryptocurrencies where one was a fork of the other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the existence of a group of cryptocurrencies particularly associated with Cardano, and a separate group correlated with Ethereum. The data was not consistent with a token's functionality or creation mechanism being the dominant determinants of the evolution of prices over time but did suggest that factors other than speculation contributed to the price."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the findings of the study \"Exploring the Interconnectedness of Cryptocurrencies using Correlation Networks\" regarding the relationships between different cryptocurrencies. What can be inferred about the factors that contribute to the evolution of prices over time, based on the results of the study?\n\n**A)** The study suggests that token functionality and creation mechanisms are the primary drivers of price evolution, and that speculation plays a minor role.\n\n**B)** The study reveals a positive association between different cryptocurrencies, indicating that their prices tend to move together, and that factors other than speculation contribute to price evolution.\n\n**C)** The study found no correlation between cryptocurrencies, suggesting that their prices are independent and unrelated, and that no underlying factors contribute to price evolution.\n\n**D)** The study's results are inconclusive, and further research is needed to fully understand the relationships between cryptocurrencies and their price evolution.\n\n**Correct Answer:** B) The study reveals a positive association between different cryptocurrencies, indicating that their prices tend to move together, and that factors other than speculation contribute to price evolution.\n\n**Explanation:** The study found statistically significant positive associations between different cryptocurrencies, including six pairs where one was a fork of the other. This suggests that the prices of these cryptocurrencies tend to move together, and that factors other than speculation contribute to price evolution. The study also found evidence of distinct groups of cryptocurrencies correlated with Cardano and Ethereum, further supporting the idea that factors other than speculation play a role in price evolution."}, "36": {"documentation": {"title": "A kinetic study of the gas-phase C(3P) + CH3CN reaction at low\n  temperature. Rate constants, H-atom product yields and astrochemical\n  implications", "source": "Kevin M. Hickson, Jean-Christophe Loison and Valentine Wakelam", "docs_id": "2103.13670", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A kinetic study of the gas-phase C(3P) + CH3CN reaction at low\n  temperature. Rate constants, H-atom product yields and astrochemical\n  implications. Rate constants have been measured for the C(3P) + CH3CN reaction between 50 K and 296 K using a continuous-flow supersonic reactor. C(3P) atoms were created by the in-situ pulsed laser photolysis of CBr4 at 266 nm, while the kinetics of C(3P) atom loss were followed by direct vacuum ultra-violet laser induced fluorescence at 115.8 nm. Secondary measurements of product H(2S) atom formation were also made, allowing absolute H-atom yields to be obtained by comparison with those obtained for the C(3P) + C2H4 reference reaction. In parallel, quantum chemical calculations were performed to obtain the various complexes, adducts and transition states relevant to the title reaction over the triplet potential energy surface, allowing us to better understand the preferred reaction pathways. The reaction is seen to be very fast, with measured rate constants in the range (3-4) x 10-10 cm3 s-1 with little or no observed temperature dependence. As the C + CH3CN reaction is not considered in current astrochemical networks, we test its influence on interstellar methyl cyanide abundances using a gas-grain dense interstellar cloud model. Its inclusion leads to predicted CH3CN abundances that are significantly lower than the observed ones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the study of the gas-phase C(3P) + CH3CN reaction at low temperature, and how does it impact our understanding of interstellar chemistry?\n\nA) The reaction is slow and temperature-dependent, leading to predicted CH3CN abundances that are higher than observed.\nB) The reaction is very fast, with little or no observed temperature dependence, and its inclusion in astrochemical networks leads to predicted CH3CN abundances that are significantly lower than observed.\nC) The reaction is highly exothermic, resulting in the formation of complex adducts and transition states that are not well-represented in current quantum chemical calculations.\nD) The reaction is catalyzed by the presence of CBr4, leading to a significant increase in the rate constant at low temperatures.\n\nCorrect Answer: B) The reaction is very fast, with little or no observed temperature dependence, and its inclusion in astrochemical networks leads to predicted CH3CN abundances that are significantly lower than observed.\n\nExplanation: The study concludes that the C(3P) + CH3CN reaction is very fast, with measured rate constants in the range (3-4) x 10^-10 cm^3 s^-1, and little or no observed temperature dependence. This conclusion is based on the experimental measurements and quantum chemical calculations performed in the study. The inclusion of this reaction in astrochemical networks leads to predicted CH3CN abundances that are significantly lower than observed, indicating that the reaction plays a significant role in regulating the abundance of methyl cyanide in interstellar clouds."}, "37": {"documentation": {"title": "Using machine-learning modelling to understand macroscopic dynamics in a\n  system of coupled maps", "source": "Francesco Borra, Marco Baldovin", "docs_id": "2011.05803", "section": ["physics.data-an", "cond-mat.stat-mech", "cs.LG", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using machine-learning modelling to understand macroscopic dynamics in a\n  system of coupled maps. Machine learning techniques not only offer efficient tools for modelling dynamical systems from data, but can also be employed as frontline investigative instruments for the underlying physics. Nontrivial information about the original dynamics, which would otherwise require sophisticated ad-hoc techniques, can be obtained by a careful usage of such methods. To illustrate this point, we consider as a case study the macroscopic motion emerging from a system of globally coupled maps. We build a coarse-grained Markov process for the macroscopic dynamics both with a machine learning approach and with a direct numerical computation of the transition probability of the coarse-grained process, and we compare the outcomes of the two analyses. Our purpose is twofold: on the one hand, we want to test the ability of the stochastic machine learning approach to describe nontrivial evolution laws, as the one considered in our study; on the other hand, we aim at gaining some insight into the physics of the macroscopic dynamics by modulating the information available to the network, we are able to infer important information about the effective dimension of the attractor, the persistence of memory effects and the multi-scale structure of the dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the study of macroscopic dynamics in a system of coupled maps, what is the primary advantage of employing machine learning techniques, and how do these methods contribute to the understanding of the underlying physics?\n\n**A)** Machine learning techniques offer a more efficient way to model complex systems, but they do not provide any new insights into the underlying physics. They are simply a tool for data analysis.\n\n**B)** Machine learning techniques allow for the identification of nontrivial evolution laws in the system, which would otherwise require sophisticated ad-hoc techniques. This is the primary advantage of using machine learning in this context.\n\n**C)** Machine learning techniques are limited to analyzing data and do not provide any physical insights into the system. They are only useful for making predictions based on existing data.\n\n**D)** Machine learning techniques are not necessary for understanding macroscopic dynamics in a system of coupled maps, as traditional numerical methods can already provide sufficient information about the system.\n\n**Correct Answer:** B) Machine learning techniques allow for the identification of nontrivial evolution laws in the system, which would otherwise require sophisticated ad-hoc techniques. This is the primary advantage of using machine learning in this context.\n\n**Explanation:** The correct answer is B) because the study highlights the ability of machine learning techniques to identify nontrivial evolution laws in the system, which would otherwise require sophisticated ad-hoc techniques. This is the primary advantage of using machine learning in this context, as it allows for a more efficient and effective understanding of the underlying physics. The other options are incorrect because they do not accurately reflect the benefits of machine learning techniques in this study."}, "38": {"documentation": {"title": "Production of $\\Omega NN$ and $\\Omega\\Omega N$ in ultra-relativistic\n  heavy ion collisions", "source": "Liang Zhang, Song Zhang, Yu-Gang Ma", "docs_id": "2112.02766", "section": ["hep-ph", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Production of $\\Omega NN$ and $\\Omega\\Omega N$ in ultra-relativistic\n  heavy ion collisions. Even though lots of $\\Lambda$-hypernuclei have been found and measured, multi-strangeness hypernuclei consisting of $\\Omega$ are not yet discovered. The studies of multi-strangeness hypernuclei help us further understand the interaction between hyperons and nucleons. Recently the $\\Omega N$ and $\\Omega\\Omega$ interactions as well as binding energies were calculated by the HAL-QCD's lattice Quantum Chromo-Dynamics (LQCD) simulations and production rates of $\\Omega$-dibaryon in Au + Au collisions at RHIC and Pb + Pb collisions at LHC energies were estimated by a coalescence model. This work discusses the production of more exotic triple-baryons including $\\Omega$, namely $\\Omega NN$ and $\\Omega\\Omega N$ as well as their decay channels. A variational method is used in calculations of bound states and binding energy of $\\Omega NN$ and $\\Omega\\Omega N$ with the potentials from the HAL-QCD's results. The productions of $\\Omega NN$ and $\\Omega\\Omega N$ are predicted by using a blast-wave model plus coalescence model in ultra-relativistic heavy ion collisions at $\\sqrt{s_{NN}} = 200$ GeV and $2.76$ TeV. Furthermore, plots for baryon number dependent yields of different baryons ($N$ and $\\Omega$), their dibaryons and hypernuclei are made and the production rate of a more exotic tetra-baryon ($\\Omega\\Omega NN$) is extrapolated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the study of multi-strangeness hypernuclei, specifically the production of $\\Omega NN$ and $\\Omega\\Omega N$ in ultra-relativistic heavy ion collisions?\n\nA) To investigate the properties of hyperons and nucleons in extreme conditions\nB) To search for evidence of the existence of exotic tetra-baryons\nC) To understand the interaction between hyperons and nucleons in the context of nuclear physics\nD) To predict the production rates of $\\Omega$-dibaryons in heavy ion collisions\n\nCorrect Answer: C) To understand the interaction between hyperons and nucleons in the context of nuclear physics\n\nExplanation: The study of multi-strangeness hypernuclei, including the production of $\\Omega NN$ and $\\Omega\\Omega N$, aims to gain insights into the interaction between hyperons and nucleons. This knowledge is crucial for understanding the behavior of hyperons in nuclear environments, which is essential for advancing our understanding of nuclear physics. The correct answer, option C, reflects this primary motivation. The other options, while related to the topic, are not the primary motivation behind the study. Option A is too broad, option B is too specific, and option D is incorrect as it refers to a different aspect of the study."}, "39": {"documentation": {"title": "Dynamics of Strategy Distribution in a One-Dimensional Continuous Trait\n  Space with a Bi-linear and Quadratic Payoff Functions", "source": "Georgiy Karev", "docs_id": "2003.06349", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamics of Strategy Distribution in a One-Dimensional Continuous Trait\n  Space with a Bi-linear and Quadratic Payoff Functions. Evolution of distribution of strategies in game theory is an interesting question that has been studied only for specific cases. Here I develop a general method to extend analysis of the evolution of continuous strategy distributions given bi-linear and quadratic payoff functions for any initial distribution to answer the following question: given the initial distribution of strategies in a game, how will it evolve over time? I look at several specific examples, including normal distribution on the entire line, normal truncated distribution, as well as exponential, uniform and Gamma distributions. I show that the class of exponential distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions. I show also that the class of normal distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions. The developed method can now be applied to a broad class of questions pertaining to evolution of strategies in games with different payoff functions and different initial distributions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Dynamics of Strategy Distribution in a One-Dimensional Continuous Trait Space with bi-linear and quadratic payoff functions, what can be concluded about the evolution of strategy distributions in games with different payoff functions and initial distributions?\n\n**A)** The class of normal distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions, but the class of exponential distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions.\n\n**B)** The class of exponential distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions, but the class of normal distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions.\n\n**C)** The class of normal distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions, and the class of exponential distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions.\n\n**D)** The class of normal distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions, but the class of exponential distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions.\n\n**Correct Answer:** C) The class of normal distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions, and the class of exponential distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions.\n\n**Explanation:** The correct answer is based on the conclusion drawn from the provided documentation, which states that the class of exponential distributions is invariant with respect to replicator dynamics in games with bi-linear payoff functions, and the class of normal distributions is invariant with respect to replicator dynamics in games with quadratic payoff functions. This conclusion is a key finding of the study and has implications for the evolution of strategy distributions in games with different payoff functions and initial distributions."}, "40": {"documentation": {"title": "G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation", "source": "Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang", "docs_id": "2106.11920", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. Analyzing the structure of proteins is a key part of understanding their functions and thus their role in biology at the molecular level. In addition, design new proteins in a methodical way is a major engineering challenge. In this work, we introduce a joint geometric-neural networks approach for comparing, deforming and generating 3D protein structures. Viewing protein structures as 3D open curves, we adopt the Square Root Velocity Function (SRVF) representation and leverage its suitable geometric properties along with Deep Residual Networks (ResNets) for a joint registration and comparison. Our ResNets handle better large protein deformations while being more computationally efficient. On top of the mathematical framework, we further design a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps original, previously unseen structures, into a low-dimensional (latent) hyper-sphere. Motivated by the spherical structure of the pre-shape space, we naturally adopt the von Mises-Fisher (vMF) distribution to model our hidden variables. We test the effectiveness of our models by generating novel protein structures and predicting completions of corrupted protein structures. Experimental results show that our method is able to generate plausible structures, different from the structures in the training data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind adopting the Square Root Velocity Function (SRVF) representation in the G-VAE framework for protein structure analysis?\n\n**A)** To improve the computational efficiency of the model\n**B)** To leverage the geometric properties of the SRVF representation for joint registration and comparison\n**C)** To enable the model to handle large protein deformations more effectively\n**D)** To simplify the mathematical framework of the model\n\n**Correct Answer:** B) To leverage the geometric properties of the SRVF representation for joint registration and comparison\n\n**Explanation:** The correct answer is B) because the documentation states that the SRVF representation is adopted to leverage its suitable geometric properties along with Deep Residual Networks (ResNets) for a joint registration and comparison. This is a key aspect of the G-VAE framework, and the SRVF representation is chosen for its geometric properties that facilitate this process.\n\nThe other options are incorrect because:\n\n* A) While the SRVF representation may have some benefits in terms of computational efficiency, this is not the primary motivation behind its adoption.\n* C) The SRVF representation is not specifically chosen to handle large protein deformations, but rather to facilitate joint registration and comparison.\n* D) The SRVF representation is not simply a simplification of the mathematical framework, but rather a deliberate choice to leverage its geometric properties."}, "41": {"documentation": {"title": "Crossover from a fission-evaporation scenario towards multifragmentation\n  in spallation reactions", "source": "P. Napolitani", "docs_id": "nucl-ex/0610041", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Crossover from a fission-evaporation scenario towards multifragmentation\n  in spallation reactions. Mostly for the purpose of applications for the energy and the environment and for the design of sources of neutrons or exotic nuclides, intense research has been dedicated to spallation, induced by protons or light projectiles at incident energies of around 1 GeV. In this energy range, while multifragmentation has still a minor share in the total reaction cross section, it was observed to have, together with fission, a prominent role in the production and the kinematics of intermediate-mass fragments, so as to condition the whole production of light and heavy nuclides. The experimental observables we dispose of attribute rather elusive properties to the intermediate-mass fragments and do not allow to classify them within one exclusive picture which is either multifragmentation or fission. Indeed, these two decay mechanisms, driven by different kinds of instabilities, exhibit behaviours which are closely comparable. High-resolution measurements of the reaction kinematics trace the way for probing finer features of the reaction kinematics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What can be inferred from the experimental observables of intermediate-mass fragments in spallation reactions, and how do these findings relate to the classification of these fragments into multifragmentation or fission?\n\nA) The experimental observables suggest that intermediate-mass fragments are exclusively the result of multifragmentation, and therefore cannot be classified as fission products.\n\nB) The experimental observables indicate that intermediate-mass fragments exhibit behaviors that are comparable to both multifragmentation and fission, making it challenging to classify them into one exclusive picture.\n\nC) The experimental observables reveal that intermediate-mass fragments are primarily the result of fission, and therefore multifragmentation can be ruled out as a contributing mechanism.\n\nD) The experimental observables attribute properties to intermediate-mass fragments that are consistent with a single, dominant decay mechanism, either multifragmentation or fission.\n\n**Correct Answer:** B) The experimental observables indicate that intermediate-mass fragments exhibit behaviors that are comparable to both multifragmentation and fission, making it challenging to classify them into one exclusive picture.\n\n**Explanation:** The correct answer is B because the documentation states that the experimental observables \"attribute rather elusive properties to the intermediate-mass fragments and do not allow to classify them within one exclusive picture which is either multifragmentation or fission.\" This suggests that the fragments exhibit behaviors that are comparable to both multifragmentation and fission, making it challenging to classify them into one exclusive picture."}, "42": {"documentation": {"title": "A general framework for inference on algorithm-agnostic variable\n  importance", "source": "Brian D. Williamson, Peter B. Gilbert, Noah R. Simon, Marco Carone", "docs_id": "2004.03683", "section": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A general framework for inference on algorithm-agnostic variable\n  importance. In many applications, it is of interest to assess the relative contribution of features (or subsets of features) toward the goal of predicting a response -- in other words, to gauge the variable importance of features. Most recent work on variable importance assessment has focused on describing the importance of features within the confines of a given prediction algorithm. However, such assessment does not necessarily characterize the prediction potential of features, and may provide a misleading reflection of the intrinsic value of these features. To address this limitation, we propose a general framework for nonparametric inference on interpretable algorithm-agnostic variable importance. We define variable importance as a population-level contrast between the oracle predictiveness of all available features versus all features except those under consideration. We propose a nonparametric efficient estimation procedure that allows the construction of valid confidence intervals, even when machine learning techniques are used. We also outline a valid strategy for testing the null importance hypothesis. Through simulations, we show that our proposal has good operating characteristics, and we illustrate its use with data from a study of an antibody against HIV-1 infection."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of existing variable importance assessment methods, and how does the proposed framework address this limitation?\n\n**A)** Existing methods focus on describing the importance of features within the confines of a given prediction algorithm, which may not capture the intrinsic value of features. The proposed framework addresses this by providing a population-level contrast between the oracle predictiveness of all available features versus all features except those under consideration.\n\n**B)** Existing methods are limited to assessing the importance of features within a specific algorithm, whereas the proposed framework provides a general framework for nonparametric inference on interpretable algorithm-agnostic variable importance.\n\n**C)** Existing methods may provide a misleading reflection of the intrinsic value of features, whereas the proposed framework provides a valid strategy for testing the null importance hypothesis.\n\n**D)** Existing methods are limited to assessing the importance of features within a specific dataset, whereas the proposed framework allows for the construction of valid confidence intervals even when machine learning techniques are used.\n\n**Correct Answer:** A) Existing methods focus on describing the importance of features within the confines of a given prediction algorithm, which may not capture the intrinsic value of features. The proposed framework addresses this by providing a population-level contrast between the oracle predictiveness of all available features versus all features except those under consideration.\n\n**Explanation:** The correct answer is A) because the question asks about the primary limitation of existing variable importance assessment methods and how the proposed framework addresses this limitation. The proposed framework addresses this limitation by providing a population-level contrast between the oracle predictiveness of all available features versus all features except those under consideration, which is a key aspect of the framework's design. The other options are incorrect because they either partially address the limitation or focus on a different aspect of the framework."}, "43": {"documentation": {"title": "Gift Contagion in Online Groups: Evidence From WeChat Red Packets", "source": "Yuan Yuan, Tracy Liu, Chenhao Tan, Qian Chen, Alex Pentland, Jie Tang", "docs_id": "1906.09698", "section": ["econ.GN", "cs.HC", "cs.SI", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gift Contagion in Online Groups: Evidence From WeChat Red Packets. Gifts are important instruments for forming bonds in interpersonal relationships. Our study analyzes the phenomenon of gift contagion in online groups. Gift contagion encourages social bonds of prompting further gifts; it may also promote group interaction and solidarity. Using data on 36 million online red packet gifts on China's social site WeChat, we leverage a natural experimental design to identify the social contagion of gift giving in online groups. Our natural experiment is enabled by the randomization of the gift amount allocation algorithm on WeChat, which addresses the common challenge of causal identifications in observational data. Our study provides evidence of gift contagion: on average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours. Decomposing this effect, we find that it is mainly driven by the extensive margin -- more recipients are triggered to send red packets. Moreover, we find that this effect is stronger for \"luckiest draw\" recipients, suggesting the presence of a group norm regarding the next red packet sender. Finally, we investigate the moderating effects of group- and individual-level social network characteristics on gift contagion as well as the causal impact of receiving gifts on group network structure. Our study has implications for promoting group dynamics and designing marketing strategies for product adoption."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on gift contagion in online groups, specifically the impact of receiving gifts on the behavior of recipients. What is the primary mechanism driving the effect of receiving a gift on the recipient's behavior, and what are the implications of this finding for understanding group dynamics?\n\n**A)** The primary mechanism driving the effect of receiving a gift on the recipient's behavior is the extensive margin, where more recipients are triggered to send red packets. This finding suggests that the study's results are robust to changes in the number of recipients, but not to changes in the amount of gifts received.\n\n**B)** The primary mechanism driving the effect of receiving a gift on the recipient's behavior is the extensive margin, where more recipients are triggered to send red packets. This finding suggests that the study's results are robust to changes in the number of recipients, but not to changes in the amount of gifts received. Furthermore, the study's results imply that the presence of a group norm regarding the next red packet sender is a key factor in driving gift contagion.\n\n**C)** The primary mechanism driving the effect of receiving a gift on the recipient's behavior is the intensive margin, where the amount of gifts received by the recipient increases. This finding suggests that the study's results are robust to changes in the amount of gifts received, but not to changes in the number of recipients.\n\n**D)** The primary mechanism driving the effect of receiving a gift on the recipient's behavior is the intensive margin, where the amount of gifts received by the recipient increases. This finding suggests that the study's results are robust to changes in the amount of gifts received, but not to changes in the number of recipients. Furthermore, the study's results imply that the presence of a group norm regarding the next red packet sender is a key factor in driving gift contagion.\n\n**Correct Answer:** B) The primary mechanism driving the effect of receiving a gift on the recipient's behavior is the extensive margin, where more recipients are triggered to send red packets. This finding suggests that the study's results are robust to changes in the number of recipients, but not to changes in the amount of gifts received. Furthermore, the study's results imply that the presence of a group norm regarding the next red packet sender is a key factor in driving gift contagion.\n\n**Explanation:** The correct answer is B) because the study finds that the effect of receiving a gift on the recipient's behavior is mainly driven by the extensive margin, where more recipients are triggered to send red packets. This suggests that the study's results are robust to changes in the number of recipients, but not to changes in the amount of gifts received. Additionally, the study's results imply that the presence of a group norm regarding the next red packet sender is a key factor in driving gift contagion, which is supported by the finding that the effect is stronger for \"luckiest draw\" recipients."}, "44": {"documentation": {"title": "Globally coherent short duration magnetic field transients and their\n  effect on ground based gravitational-wave detectors", "source": "Izabela Kowalska-Leszczynska, Marie-Anne Bizouard, Tomasz Bulik,\n  Nelson Christensen, Michael Coughlin, Mark Go{\\l}kowski, Jerzy Kubisz,\n  Andrzej Kulak, Janusz Mlynarczyk, Florent Robinet, Maximilian Rohde", "docs_id": "1612.01102", "section": ["astro-ph.IM", "gr-qc", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Globally coherent short duration magnetic field transients and their\n  effect on ground based gravitational-wave detectors. It has been recognized that the magnetic fields from the Schumann resonances could affect the search for a stochastic gravitational-wave background by LIGO and Virgo. Presented here are the observations of short duration magnetic field transients that are coincident in the magnetometers at the LIGO and Virgo sites. Data from low-noise magnetometers in Poland and Colorado, USA, are also used and show short duration magnetic transients of global extent. We measure at least 2.3 coincident (between Poland and Colorado) magnetic transient events per day where one of the pulses exceeds 200 pT. Given the recently measured values of the magnetic coupling to differential arm motion for Advanced LIGO, there would be a few events per day that would appear simultaneously at the gravitational-wave detector sites and could move the test masses of order $10^{-18}$ m. We confirm that in the advanced detector era short duration transient gravitational-wave searches must account for correlated magnetic field noise in the global detector network."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the potential impact of globally coherent short duration magnetic field transients on the search for a stochastic gravitational-wave background by LIGO and Virgo. Consider the following statements:\n\nA) The observed magnetic field transients in Poland and Colorado, USA, are likely to be caused by local sources, such as thunderstorms or human activity.\n\nB) The magnetic coupling to differential arm motion for Advanced LIGO is sufficient to cause test mass movements of order $10^{-18}$ m, which could be mistaken for gravitational-wave signals.\n\nC) The global extent of the magnetic field transients suggests that they are not correlated with the gravitational-wave background, and therefore do not pose a significant threat to the search.\n\nD) The observed coincident magnetic transient events per day, with at least 2.3 events exceeding 200 pT, indicate that the magnetic field transients are a significant source of noise that must be accounted for in gravitational-wave searches.\n\n**Correct Answer:** D) The observed coincident magnetic transient events per day, with at least 2.3 events exceeding 200 pT, indicate that the magnetic field transients are a significant source of noise that must be accounted for in gravitational-wave searches.\n\n**Explanation:**\n\nThe correct answer is D) because the observed coincident magnetic transient events per day, with at least 2.3 events exceeding 200 pT, suggest that the magnetic field transients are a significant source of noise that must be accounted for in gravitational-wave searches. This is because the magnetic coupling to differential arm motion for Advanced LIGO is sufficient to cause test mass movements of order $10^{-18}$ m, which could be mistaken for gravitational-wave signals. The fact that the magnetic field transients are globally coherent and coincident at multiple detector sites further supports the idea that they are a significant source of noise that must be accounted for in gravitational-wave searches.\n\nThe other options are incorrect because:\n\nA) The observed magnetic field transients in Poland and Colorado, USA, are likely to be caused by global sources, such as the Schumann resonances, rather than local sources.\n\nB) While the magnetic coupling to differential arm motion for Advanced LIGO is sufficient to cause test mass movements, this does not necessarily mean that the magnetic field transients are a significant source of noise.\n\nC) The global extent of the magnetic field transients suggests that they are correlated with the gravitational-wave background, and therefore pose a significant threat to the search."}, "45": {"documentation": {"title": "Collective Matrix Completion", "source": "Mokhtar Z. Alaya (MODAL'X, Univ Paris Nanterre) and Olga Klopp (ESSEC\n  Business School and CREST)", "docs_id": "1807.09010", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Collective Matrix Completion. Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries. Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system. However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one. In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution. Then, we relax the assumption of exponential family distribution for the noise and we investigate the distribution-free case. In this setting, we do not assume any specific model for the observations. The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Collective Matrix Completion, what is the primary assumption made about the distribution of the matrix entries in the setting where the matrices are sampled from an exponential family distribution?\n\n**A)** The entries are independently and identically distributed (i.i.d.) from an exponential family distribution.\n**B)** The entries are sampled from an exponential family distribution, but the noise is assumed to be normally distributed.\n**C)** The entries are sampled from an exponential family distribution, but the noise is assumed to be from a distribution that belongs to the exponential family.\n**D)** The entries are not sampled from any specific distribution, and the noise is assumed to be from a distribution that does not belong to the exponential family.\n\n**Correct Answer:** C) The entries are sampled from an exponential family distribution, but the noise is assumed to be from a distribution that belongs to the exponential family.\n\n**Explanation:** The correct answer is C) because the documentation states that the matrix entries are sampled from an exponential family distribution, but it does not assume any specific distribution for the noise. The correct answer is not A) because the documentation does not mention i.i.d. distribution. The correct answer is not B) because the documentation does not assume normal distribution for the noise. The correct answer is not D) because the documentation does assume a specific distribution for the noise, namely one that belongs to the exponential family."}, "46": {"documentation": {"title": "Channel Hardening in Massive MIMO: Model Parameters and Experimental\n  Assessment", "source": "Sara Gunnarsson, Jos\\'e Flordelis, Liesbet Van der Perre, Fredrik\n  Tufvesson", "docs_id": "2004.06772", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Channel Hardening in Massive MIMO: Model Parameters and Experimental\n  Assessment. Reliability is becoming increasingly important for many applications envisioned for future wireless systems. A technology that could improve reliability in these systems is massive MIMO (Multiple-Input Multiple-Output). One reason for this is a phenomenon called channel hardening, which means that as the number of antennas in the system increases, the variations of channel gain decrease in both the time- and frequency domain. Our analysis of channel hardening is based on a joint comparison of theory, measurements and simulations. Data from measurement campaigns including both indoor and outdoor scenarios, as well as cylindrical and planar base station arrays, are analyzed. The simulation analysis includes a comparison with the COST 2100 channel model with its massive MIMO extension. The conclusion is that the COST 2100 model is well suited to represent real scenarios, and provides a reasonable match to actual measurements up to the uncertainty of antenna patterns and user interaction. Also, the channel hardening effect in practical massive MIMO channels is less pronounced than in complex independent and identically distributed (i.i.d.) Gaussian channels, which are often considered in theoretical work."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary reason for the reduced channel hardening effect in practical massive MIMO channels, as compared to complex independent and identically distributed (i.i.d.) Gaussian channels?\n\n**A)** The COST 2100 model is less accurate in representing real-world scenarios.\n**B)** The number of antennas in the system is not sufficient to achieve significant channel hardening.\n**C)** The channel hardening effect is less pronounced due to the presence of antenna patterns and user interaction.\n**D)** The COST 2100 model is not well-suited to represent the time- and frequency domain variations of channel gain.\n\n**Correct Answer:** C) The channel hardening effect is less pronounced due to the presence of antenna patterns and user interaction.\n\n**Explanation:** According to the documentation, the channel hardening effect in practical massive MIMO channels is less pronounced than in complex i.i.d. Gaussian channels. This is because the COST 2100 model, which includes a massive MIMO extension, provides a reasonable match to actual measurements up to the uncertainty of antenna patterns and user interaction. Therefore, the correct answer is C) The channel hardening effect is less pronounced due to the presence of antenna patterns and user interaction."}, "47": {"documentation": {"title": "Economics of Innovation and Perceptions of Renewed Education and\n  Curriculum Design in Bangladesh", "source": "Shifa Taslim Chowdhury, Mohammad Nur Nobi and Anm Moinul Islam", "docs_id": "2112.13842", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Economics of Innovation and Perceptions of Renewed Education and\n  Curriculum Design in Bangladesh. The creative Education system is one of the effective education systems in many countries like Finland, Denmark, and South Korea. Bangladesh Government has also launched the creative curriculum system in 2009 in both primary and secondary levels, where changes have been made in educational contents and exam question patterns. These changes in the previous curriculum aimed to avoid memorization and less creativity and increase the students' level of understanding and critical thinking. Though the Government has taken these steps, the quality of the educational system in Bangladesh is still deteriorating. Since the curriculum has been changed recently, this policy issue got massive attention of the people because the problem of a substandard education system has arisen. Many students have poor performances in examinations, including entrance hall exams in universities and board examinations. This deteriorating situation is mostly for leakage of question paper, inadequate equipment and materials, and insufficient training. As a result, the existing education system has failed to provide the standard level of education. This research will discuss and find why this creative educational system is getting impacted by these factors. It will be qualitative research. A systematic questionnaire will interview different school teachers, parents, experts, and students."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary reasons for the deterioration of the educational system in Bangladesh, despite the implementation of the creative curriculum system in 2009?\n\n**A)** Lack of funding for educational institutions\n**B)** Insufficient training for teachers and inadequate equipment\n**C)** Leakage of question papers and poor examination management\n**D)** Inadequate parental involvement and lack of community support\n\n**Correct Answer:** C) Leakage of question papers and poor examination management\n\n**Explanation:** The question requires the test-taker to analyze the situation described in the documentation and identify the primary reasons for the deterioration of the educational system in Bangladesh. The correct answer, C) Leakage of question papers and poor examination management, is supported by the documentation, which states that \"the problem of a substandard education system has arisen due to leakage of question paper, inadequate equipment and materials, and insufficient training.\" The other options, while related to the educational system, are not the primary reasons for the deterioration described in the documentation. \n\n**Note:** The test-taker needs to have a good understanding of the documentation and be able to analyze the situation to arrive at the correct answer. The question requires critical thinking and the ability to identify the most relevant information."}, "48": {"documentation": {"title": "Helioseismic Travel-Time Definitions and Sensitivity to Horizontal Flows\n  Obtained From Simulations of Solar Convection", "source": "S. Couvidat (1), A.C. Birch (2) ((1) W.W. Hansen Experimental Physics\n  Laboratory, Stanford University, (2) NorthWest Research Associates, CoRA\n  Division)", "docs_id": "0904.2025", "section": ["astro-ph.SR", "astro-ph.CO", "astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Helioseismic Travel-Time Definitions and Sensitivity to Horizontal Flows\n  Obtained From Simulations of Solar Convection. We study the sensitivity of wave travel times to steady and spatially homogeneous horizontal flows added to a realistic simulation of the solar convection performed by Robert F. Stein, Ake Nordlund, Dali Georgobiani, and David Benson. Three commonly used definitions of travel times are compared. We show that the relationship between travel-time difference and flow amplitude exhibits a non-linearity depending on the travel distance, the travel-time definition considered, and the details of the time-distance analysis (in particular, the impact of the phase-speed filter width). For times measured using a Gabor wavelet fit, the travel-time differences become nonlinear in the flow strength for flows of about 300 m/s, and this non-linearity reaches almost 60% at 1200 m/s (relative difference between actual travel time and expected time for a linear behaviour). We show that for travel distances greater than about 17 Mm, the ray approximation predicts the sensitivity of travel-time shifts to uniform flows. For smaller distances, the ray approximation can be inaccurate by more than a factor of three."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Compare the sensitivity of travel-time differences to horizontal flows in the solar convection simulation using three commonly used definitions of travel times. How do the non-linear relationships between travel-time difference and flow amplitude vary depending on the travel distance, travel-time definition, and time-distance analysis?\n\n**A)** The non-linear relationships between travel-time difference and flow amplitude are independent of travel distance and travel-time definition, and only depend on the phase-speed filter width.\n\n**B)** The ray approximation predicts the sensitivity of travel-time shifts to uniform flows for all travel distances, and the non-linear relationships are only significant for flows greater than 300 m/s.\n\n**C)** The non-linear relationships between travel-time difference and flow amplitude are most pronounced for travel distances greater than 17 Mm, and decrease significantly for smaller distances.\n\n**D)** The Gabor wavelet fit predicts the most accurate results for travel-time differences, and the non-linear relationships are only significant for flows greater than 1200 m/s.\n\n**Correct Answer:** C) The non-linear relationships between travel-time difference and flow amplitude are most pronounced for travel distances greater than 17 Mm, and decrease significantly for smaller distances.\n\n**Explanation:** The correct answer is based on the statement in the Arxiv documentation that \"For smaller distances, the ray approximation can be inaccurate by more than a factor of three\" and that \"For times measured using a Gabor wavelet fit, the travel-time differences become nonlinear in the flow strength for flows of about 300 m/s, and this non-linearity reaches almost 60% at 1200 m/s\". This indicates that the non-linear relationships are most pronounced for travel distances greater than 17 Mm, and decrease significantly for smaller distances."}, "49": {"documentation": {"title": "Transport catastrophe analysis as an alternative to a fractal\n  description: theory and application to financial crisis time series", "source": "Sergey A. Kamenshchikov", "docs_id": "1405.6990", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transport catastrophe analysis as an alternative to a fractal\n  description: theory and application to financial crisis time series. The goal of this investigation was to overcome limitations of a persistency analysis, introduced by Benoit Mandelbrot for fractal Brownian processes: nondifferentiability, Brownian nature of process and a linear memory measure. We have extended a sense of a Hurst factor by consideration of a phase diffusion power law. It was shown that pre-catastrophic stabilization as an indicator of bifurcation leads to a new minimum of momentary phase diffusion, while bifurcation causes an increase of the momentary transport. Basic conclusions of a diffusive analysis have been compared to the Lyapunov stability model. An extended Reynolds parameter has been introduces as an indicator of phase transition. A combination of diffusive and Reynolds analysis has been applied for a description of a time series of Dow Jones Industrial weekly prices for a world financial crisis of 2007-2009. Diffusive and Reynolds parameters shown an extreme values in October 2008 when a mortgage crisis was fixed. A combined R/D description allowed distinguishing of short-memory and long memory shifts of a market evolution. It was stated that a systematic large scale failure of a financial system has begun in October 2008 and started fading in February 2009."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the concept of \"transport catastrophe\" in the context of financial crisis time series, as introduced in the paper \"Transport catastrophe analysis as an alternative to a fractal\". How does the proposed method of diffusive analysis and Reynolds parameter differ from traditional fractal analysis, and what insights does it provide into the dynamics of financial markets?\n\n**A)** The proposed method of diffusive analysis and Reynolds parameter is an extension of traditional fractal analysis, which incorporates the concept of phase diffusion power law to overcome the limitations of fractal Brownian processes. This approach allows for a more nuanced understanding of the dynamics of financial markets, particularly in the context of phase transitions and bifurcations.\n\n**B)** The paper proposes a new method of analysis that combines diffusive and Reynolds parameters to describe the behavior of financial time series. This approach is distinct from traditional fractal analysis, which focuses on the persistence and memory properties of the time series. The proposed method provides a more comprehensive understanding of the dynamics of financial markets, including the role of phase transitions and bifurcations.\n\n**C)** The concept of \"transport catastrophe\" is a new framework for analyzing financial crisis time series, which differs from traditional fractal analysis in its focus on the transport of information and the role of phase transitions in financial markets. The proposed method of diffusive analysis and Reynolds parameter provides a more detailed understanding of the dynamics of financial markets, particularly in the context of extreme events and phase transitions.\n\n**D)** The paper proposes a new method of analysis that combines diffusive and Reynolds parameters to describe the behavior of financial time series. This approach is distinct from traditional fractal analysis, which focuses on the persistence and memory properties of the time series. The proposed method provides a more comprehensive understanding of the dynamics of financial markets, including the role of phase transitions and bifurcations, but does not necessarily imply a new framework for analyzing financial crisis time series.\n\n**Correct Answer:** B) The paper proposes a new method of analysis that combines diffusive and Reynolds parameters to describe the behavior of financial time series. This approach is distinct from traditional fractal analysis, which focuses on the persistence and memory properties of the time series. The proposed method provides a more comprehensive understanding of the dynamics of financial markets, including the role of phase transitions and bifurcations."}, "50": {"documentation": {"title": "Quantum sets and Gelfand spectra (Ortho-sets and Gelfand spectra)", "source": "Chun Ding, Chi-Keung Ng", "docs_id": "2106.01697", "section": ["math-ph", "math.MP", "math.OA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum sets and Gelfand spectra (Ortho-sets and Gelfand spectra). Motivated by quantum states with zero transition probability, we introduce the notion of ortho-set which is a set equipped with a relation $\\neq_\\mathrm{q}$ satisfying: $x\\neq_\\mathrm{q} y$ implies both $x\\neq y$ and $y \\neq_\\mathrm{q} x$. For an ortho-set, a canonical complete ortholattice is constructed. Conversely, every complete ortholattice comes from an ortho-set in this way. Hence, the theory of ortho-sets captures almost everything about quantum logics. For a quantum system modeled by the self-adjoint part $B_\\mathrm{sa}$ of a $C^*$-algebra $B$, we also introduce a \"semi-classical object\" called the Gelfand spectrum. It is the ortho-set, $P(B)$, of pure states of $B$ equipped with an \"ortho-topology\", which is a collection of subsets of $P(B)$, defined via a hull-kernel construction with respects to closed left ideals of $B$. We establish a generalization of the Gelfand theorem by showing that a bijection between the Gelfand spectra of two quantum systems that preserves the respective ortho-topologies is induced by a Jordan isomorphism between the self-adjoint parts of the underlying $C^*$-algebras (i.e. an isomorphism of the quantum systems), when the underlying $C^*$-algebras satisfy a mild condition."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of quantum logics and Gelfand spectra, what is the relationship between the ortho-set of pure states and the Gelfand spectrum of a quantum system, and how does this relationship relate to the preservation of the ortho-topology under a bijection between Gelfand spectra?\n\n**A)** The ortho-set of pure states is a complete ortholattice that captures almost everything about quantum logics, and the Gelfand spectrum is a semi-classical object that is the ortho-set equipped with an ortho-topology. A bijection between Gelfand spectra preserves the ortho-topology if and only if the underlying C*-algebras are isomorphic.\n\n**B)** The Gelfand spectrum is a complete ortholattice that is the ortho-set of pure states equipped with an ortho-topology, and the ortho-set is a semi-classical object that is the Gelfand spectrum. A bijection between Gelfand spectra preserves the ortho-topology if and only if the underlying C*-algebras are isomorphic.\n\n**C)** The ortho-set of pure states is a semi-classical object that is the Gelfand spectrum equipped with an ortho-topology, and the Gelfand spectrum is a complete ortholattice that is the ortho-set. A bijection between Gelfand spectra preserves the ortho-topology if and only if the underlying C*-algebras are isomorphic.\n\n**D)** The Gelfand spectrum is a complete ortholattice that is the ortho-set of pure states equipped with an ortho-topology, and the ortho-set is a semi-classical object that is the Gelfand spectrum. A bijection between Gelfand spectra preserves the ortho-topology if and only if the underlying C*-algebras are isomorphic.\n\n**Correct Answer:** A\n\n**Explanation:**\n\nThe correct answer is A, as it accurately reflects the relationship between the ortho-set of pure states and the Gelfand spectrum, as well as the condition for a bijection between Gelfand spectra to preserve the ortho-topology.\n\nThe ortho-set of pure states is a complete ortholattice that captures almost everything about quantum logics, and the Gelfand spectrum is a semi-classical object that is the ortho-set equipped with an ortho-topology. A bijection between Gelfand spectra preserves the ortho-topology if and only if the underlying C*-algebras are isomorphic, as stated in the documentation.\n\nThe other options are incorrect because they either reverse the relationship between the ortho-set and the Gelfand spectrum, or incorrectly state the condition for a bijection to preserve the ortho-topology."}, "51": {"documentation": {"title": "Quantum Bose liquids with logarithmic nonlinearity: Self-sustainability\n  and emergence of spatial extent", "source": "Alexander V. Avdeenkov and Konstantin G. Zloshchastiev", "docs_id": "1108.0847", "section": ["cond-mat.quant-gas", "astro-ph.SR", "hep-th", "nucl-th", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum Bose liquids with logarithmic nonlinearity: Self-sustainability\n  and emergence of spatial extent. The Gross-Pitaevskii (GP) equation is a long-wavelength approach widely used to describe the dilute Bose-Einstein condensates (BEC). However, in many physical situations, such as higher densities, this approximation unlikely suffices hence one might need models which would account for long-range correlations and multi-body interactions. We show that the Bose liquid described by the logarithmic wave equation has a number of drastic differences from the GP one. It possesses the self-sustainability property: while the free GP condensate tends to spill all over the available volume the logarithmic one tends to form a Gaussian-type droplet - even in the absence of an external trapping potential. The quasi-particle modes of the logarithmic BEC are shown to acquire a finite size despite the bare particles being assumed point-like, i.e., the spatial extent emerges here as a result of quantum many-body correlations. Finally, we study the elementary excitations and demonstrate that the background density changes the topological structure of their momentum space which, in turn, affects their dispersion relations. Depending on the density the latter can be of the massive relativistic, massless relativistic, tachyonic and quaternionic type."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the logarithmic wave equation, what is the primary difference between the self-sustainability property of the Bose liquid and the free Gross-Pitaevskii (GP) condensate?\n\nA) The logarithmic BEC tends to form a Gaussian-type droplet in the presence of an external trapping potential, whereas the GP condensate does not.\nB) The logarithmic BEC tends to form a Gaussian-type droplet in the absence of an external trapping potential, whereas the GP condensate tends to spill all over the available volume.\nC) The logarithmic BEC has a self-sustainability property that is independent of the external trapping potential, whereas the GP condensate does not.\nD) The logarithmic BEC has a self-sustainability property that is dependent on the external trapping potential, whereas the GP condensate does not.\n\n**Correct Answer:** B) The logarithmic BEC tends to form a Gaussian-type droplet in the absence of an external trapping potential, whereas the GP condensate tends to spill all over the available volume.\n\n**Explanation:** The correct answer is B) because the logarithmic wave equation describes a Bose liquid that has a self-sustainability property, meaning it tends to form a Gaussian-type droplet even in the absence of an external trapping potential. In contrast, the free GP condensate tends to spill all over the available volume. This difference in behavior is a key result of the logarithmic wave equation, which accounts for long-range correlations and multi-body interactions that are not captured by the GP equation."}, "52": {"documentation": {"title": "The Approximate Capacity of the Gaussian N-Relay Diamond Network", "source": "Urs Niesen, Suhas Diggavi", "docs_id": "1008.3813", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Approximate Capacity of the Gaussian N-Relay Diamond Network. We consider the Gaussian \"diamond\" or parallel relay network, in which a source node transmits a message to a destination node with the help of N relays. Even for the symmetric setting, in which the channel gains to the relays are identical and the channel gains from the relays are identical, the capacity of this channel is unknown in general. The best known capacity approximation is up to an additive gap of order N bits and up to a multiplicative gap of order N^2, with both gaps independent of the channel gains. In this paper, we approximate the capacity of the symmetric Gaussian N-relay diamond network up to an additive gap of 1.8 bits and up to a multiplicative gap of a factor 14. Both gaps are independent of the channel gains and, unlike the best previously known result, are also independent of the number of relays N in the network. Achievability is based on bursty amplify-and-forward, showing that this simple scheme is uniformly approximately optimal, both in the low-rate as well as in the high-rate regimes. The upper bound on capacity is based on a careful evaluation of the cut-set bound. We also present approximation results for the asymmetric Gaussian N-relay diamond network. In particular, we show that bursty amplify-and-forward combined with optimal relay selection achieves a rate within a factor O(log^4(N)) of capacity with pre-constant in the order notation independent of the channel gains."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the approximate capacity of the Gaussian N-relay diamond network, and what is the significance of the additive and multiplicative gaps in this approximation?\n\nA) The capacity is approximately 1.8 bits with an additive gap of order N and a multiplicative gap of order N^2, which is independent of the channel gains and the number of relays N.\n\nB) The capacity is approximately 1.8 bits with an additive gap of 1.8 bits and a multiplicative gap of a factor 14, which is independent of the channel gains and the number of relays N.\n\nC) The capacity is approximately 1.8 bits with an additive gap of order N and a multiplicative gap of order N^2, which is independent of the channel gains but dependent on the number of relays N.\n\nD) The capacity is approximately 1.8 bits with an additive gap of 1.8 bits and a multiplicative gap of a factor 14, which is dependent on the channel gains and the number of relays N.\n\nCorrect Answer: B) The capacity is approximately 1.8 bits with an additive gap of 1.8 bits and a multiplicative gap of a factor 14, which is independent of the channel gains and the number of relays N.\n\nExplanation: The correct answer is B) because the documentation states that the approximate capacity of the Gaussian N-relay diamond network is up to an additive gap of 1.8 bits and up to a multiplicative gap of a factor 14, with both gaps being independent of the channel gains and the number of relays N. This matches the statement in option B. Options A and C are incorrect because they incorrectly state that the additive gap is of order N, and option D is incorrect because it states that the capacity is dependent on the channel gains and the number of relays N, which is not the case."}, "53": {"documentation": {"title": "Beam Spin Asymmetry in Electroproduction of Pseudoscalar or Scalar Meson\n  Production off the Scalar Target", "source": "Chueng-Ryong Ji, Ho-Meoyng Choi, Andrew Lundeen, Bernard L. G. Bakker", "docs_id": "1806.01379", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beam Spin Asymmetry in Electroproduction of Pseudoscalar or Scalar Meson\n  Production off the Scalar Target. We discuss the electroproduction of pseudoscalar ($0^{-+}$) or scalar ($0^{++}$) meson production off the scalar target. The most general formulation of the differential cross section for the $0^{-+}$ or $0^{++}$ meson production process involves only one or two hadronic form factors, respectively, on a scalar target. The Rosenbluth-type separation of the differential cross section provides the explicit relation between the hadronic form factors and the different parts of the differential cross section in a completely model-independent manner. The absence of the beam spin asymmetry for the pseudoscalar meson production provides a benchmark for the experimental data analysis. The measurement of the beam spin asymmetry for the scalar meson production may also provide a unique opportunity not only to explore the imaginary part of the hadronic amplitude in the general formulation but also to examine the significance of the chiral-odd generalized parton distribution (GPD) contribution in the leading-twist GPD formulation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary consequence of the Rosenbluth-type separation of the differential cross section for the electroproduction of pseudoscalar or scalar mesons off a scalar target?\n\nA) It provides a direct measurement of the imaginary part of the hadronic amplitude.\nB) It allows for the explicit relation between the hadronic form factors and the different parts of the differential cross section in a completely model-independent manner.\nC) It enables the separation of the real and imaginary parts of the hadronic amplitude.\nD) It provides a unique opportunity to examine the significance of the chiral-odd generalized parton distribution (GPD) contribution in the leading-twist GPD formulation.\n\nCorrect Answer: B) It allows for the explicit relation between the hadronic form factors and the different parts of the differential cross section in a completely model-independent manner.\n\nExplanation: The Rosenbluth-type separation of the differential cross section is a method used to separate the real and imaginary parts of the hadronic amplitude. However, the primary consequence of this separation is that it provides an explicit relation between the hadronic form factors and the different parts of the differential cross section in a completely model-independent manner. This allows for a more detailed understanding of the underlying physics of the process. The other options are incorrect because they either describe a secondary consequence of the Rosenbluth-type separation (A and C) or a related concept (D)."}, "54": {"documentation": {"title": "Quark Coalescence for Charmed Mesons in Ultrarelativistic Heavy-Ion\n  Collisions", "source": "V. Greco, C. M. Ko, and R. Rapp", "docs_id": "nucl-th/0312100", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quark Coalescence for Charmed Mesons in Ultrarelativistic Heavy-Ion\n  Collisions. We investigate effects of charm-quark interactions in a Quark-Gluon Plasma on the production of $D$ and $J/\\psi$ mesons in high-energy heavy-ion collisions. Employing a previously constructed coalescence model that successfully reproduces the transverse momentum ($p_T$) spectra and elliptic flow ($v_2(p_T)$) of light hadrons at RHIC from underlying light-quark distributions at the phase transition temperature $T_c$, $D$-meson and $J/\\psi$ $p_T$ spectra are evaluated. For the charm-quark distributions, we consider two limiting scenarios: (i) {\\em no} rescattering, corresponding to perturbative QCD spectra and (ii) {\\em complete} thermalization including transverse expansion. With the $D$-meson spectra acquiring a minimal $v_2$ inherited from their light-quark content, the corresponding semileptonic decay spectra of single electrons are found to practically preserve the $v_2$ of the parent particles, exhibiting marked differences between the pQCD and thermal scenarios for $p_T\\ge 1$ GeV. Likewise, the $p_T$-spectra and yields of $J/\\psi$'s differ appreciably in the two scenarios."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of Quark-Gluon Plasma and heavy-ion collisions, compare the effects of charm-quark interactions on the production of $D$ and $J/\\psi$ mesons in ultrarelativistic heavy-ion collisions. How do the $p_T$ spectra and yields of these mesons differ between the two limiting scenarios: (i) no rescattering (perturbative QCD) and (ii) complete thermalization including transverse expansion?\n\n**A)** The $p_T$ spectra of $D$ and $J/\\psi$ mesons are identical in both scenarios, with no differences in their yields.\n\n**B)** The $p_T$ spectra of $D$ and $J/\\psi$ mesons differ significantly between the two scenarios, with the thermal scenario exhibiting a stronger $v_2$ and higher yields for $p_T \\ge 1$ GeV.\n\n**C)** The $p_T$ spectra of $D$ and $J/\\psi$ mesons are only affected by the charm-quark interactions in the thermal scenario, with no differences in their yields between the two scenarios.\n\n**D)** The $p_T$ spectra of $D$ and $J/\\psi$ mesons are identical in both scenarios, with the thermal scenario exhibiting a weaker $v_2$ and lower yields for $p_T \\ge 1$ GeV.\n\n**Correct Answer:** B) The $p_T$ spectra of $D$ and $J/\\psi$ mesons differ significantly between the two scenarios, with the thermal scenario exhibiting a stronger $v_2$ and higher yields for $p_T \\ge 1$ GeV.\n\n**Explanation:** The correct answer is B) because the text states that the $p_T$ spectra of $D$ and $J/\\psi$ mesons differ appreciably between the two scenarios, with the thermal scenario exhibiting a stronger $v_2$ and higher yields for $p_T \\ge 1$ GeV. This is supported by the fact that the $D$-meson and $J/\\psi$ $p_T$ spectra acquire a minimal $v_2$ inherited from their light-quark content, and that the semileptonic decay spectra of single electrons preserve the $v_2$ of the parent particles."}, "55": {"documentation": {"title": "Multiwavelength Study of Equatorial Coronal-Hole Jets", "source": "Pankaj Kumar, Judith T. Karpen, Spiro K. Antiochos, Peter F. Wyper, C.\n  Richard DeVore, Craig E. DeForest", "docs_id": "1902.00922", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiwavelength Study of Equatorial Coronal-Hole Jets. Jets (transient/collimated plasma ejections) occur frequently throughout the solar corona and contribute mass/energy to the corona and solar wind. By combining numerical simulations and high-resolution observations, we have made substantial progress recently on determining the energy buildup and release processes in these jets. Here we describe a study of 27 equatorial coronal-hole jets using Solar Dynamics Observatory/AIA and HMI observations on 2013 June 27-28 and 2014 January 8-10. Out of 27 jets, 18 (67%) are associated with mini-filament ejections; the other 9 (33%) do not show mini-filament eruptions but do exhibit mini-flare arcades and other eruptive signatures. This indicates that every jet in our sample involved a filament-channel eruption. From the complete set of events, 6 jets (22%) are apparently associated with tiny flux-cancellation events at the polarity inversion line, and 2 jets (7%) are associated with sympathetic eruptions of filaments from neighboring bright points. Potential-field extrapolations of the source-region photospheric magnetic fields reveal that all jets originated in the fan-spine topology of an embedded bipole associated with an extreme ultraviolet coronal bright point. Hence, all our jets are in agreement with the breakout model of solar eruptions. We present selected examples and discuss the implications for the jet energy build-up and initiation mechanisms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism that explains the origin of all 27 equatorial coronal-hole jets studied in the paper, according to the potential-field extrapolations of the source-region photospheric magnetic fields?\n\nA) The breakout model of solar eruptions, which suggests that the jets originated from an embedded bipole associated with an extreme ultraviolet coronal bright point.\nB) The magnetic reconnection model, which proposes that the jets were caused by the interaction between the photospheric magnetic field and the coronal magnetic field.\nC) The convective zone model, which suggests that the jets were driven by the convective motion of the solar interior.\nD) The radiative diffusion model, which proposes that the jets were caused by the radiative diffusion of energy from the photosphere to the corona.\n\nCorrect Answer: A) The breakout model of solar eruptions, which suggests that the jets originated from an embedded bipole associated with an extreme ultraviolet coronal bright point.\n\nExplanation: The potential-field extrapolations of the source-region photospheric magnetic fields reveal that all jets originated in the fan-spine topology of an embedded bipole associated with an extreme ultraviolet coronal bright point, which is consistent with the breakout model of solar eruptions. This model suggests that the jets were caused by the buildup and release of magnetic energy in the corona, which is supported by the observation that all jets involved a filament-channel eruption."}, "56": {"documentation": {"title": "Hydrodynamic Predictions for Mixed Harmonic Correlations in 200 GeV\n  Au+Au Collisions", "source": "Fernando G. Gardim, Frederique Grassi, Matthew Luzum, and Jacquelyn\n  Noronha-Hostler", "docs_id": "1608.02982", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hydrodynamic Predictions for Mixed Harmonic Correlations in 200 GeV\n  Au+Au Collisions. Recent measurements at the LHC involve the correlation of different azimuthal flow harmonics $v_n$. These new observables add constraints to theoretical models and probe aspects of the system that are independent of the traditional single-harmonic measurements such as 2- and multi-particle cumulants $v_n\\{m\\}$. Many of these new observables have not yet been measured at RHIC, leaving an opportunity to make predictions as a test of models across energies. We make predictions using NeXSPheRIO, a hydrodynamical model which has accurately reproduced a large set of single-harmonic correlations in a large range of transverse momenta and centralities at RHIC. Our predictions thus provide an important baseline for comparison to correlations of flow harmonics, which contain non-trivial information about the initial state as well as QGP transport properties. We also point out significant biases that can appear when using wide centrality bins and non-trivial event weighting, necessitating care in performing experimental analyses and in comparing theoretical calculations to these measurements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What are the key implications of using NeXSPheRIO, a hydrodynamical model, to make predictions for mixed harmonic correlations in 200 GeV Au+Au collisions, and how do these predictions relate to experimental measurements of flow harmonics at the LHC and RHIC?\n\nA) The predictions provide a baseline for comparison to correlations of flow harmonics, which contain non-trivial information about the initial state and QGP transport properties, but do not account for biases in wide centrality bins and non-trivial event weighting.\n\nB) The predictions are limited to a small range of transverse momenta and centralities, and do not accurately reproduce the correlations of flow harmonics at the LHC and RHIC.\n\nC) The predictions are not sensitive to the initial state and QGP transport properties, and only provide a rough estimate of the correlations of flow harmonics at the LHC and RHIC.\n\nD) The predictions are not affected by biases in wide centrality bins and non-trivial event weighting, and provide a precise estimate of the correlations of flow harmonics at the LHC and RHIC.\n\nCorrect Answer: A) The predictions provide a baseline for comparison to correlations of flow harmonics, which contain non-trivial information about the initial state and QGP transport properties, but do not account for biases in wide centrality bins and non-trivial event weighting.\n\nExplanation: The correct answer is A) because the documentation states that the predictions made using NeXSPheRIO provide a baseline for comparison to correlations of flow harmonics, which contain non-trivial information about the initial state and QGP transport properties. However, it also notes that these predictions do not account for biases that can appear when using wide centrality bins and non-trivial event weighting, which is a crucial consideration for experimental analyses and theoretical calculations."}, "57": {"documentation": {"title": "Long time asymptotics for the defocusing mKdV equation with finite\n  density initial data in different solitonic regions", "source": "Taiyang Xu, Zechuan Zhang, Engui Fan", "docs_id": "2108.06284", "section": ["math.AP", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Long time asymptotics for the defocusing mKdV equation with finite\n  density initial data in different solitonic regions. We investigate the long time asymptotics for the Cauchy problem of the defocusing modified Kortweg-de Vries (mKdV) equation with finite density initial data in different solitonic regions \\begin{align*} &q_t(x,t)-6q^2(x,t)q_{x}(x,t)+q_{xxx}(x,t)=0, \\quad (x,t)\\in\\mathbb{R}\\times \\mathbb{R}^{+}, &q(x,0)=q_{0}(x), \\quad \\lim_{x\\rightarrow\\pm\\infty}q_{0}(x)=\\pm 1, \\end{align*} where $q_0\\mp 1\\in H^{4,4}(\\mathbb{R})$.Based on the spectral analysis of the Lax pair, we express the solution of the mKdV equation in terms of a Riemann-Hilbert problem. In our previous article, we have obtained long time asymptotics and soliton resolutions for the mKdV equation in the solitonic region $\\xi\\in(-6,-2)$ with $\\xi=\\frac{x}{t}$.In this paper, we calculate the asymptotic expansion of the solution $q(x,t)$ for the solitonic region $\\xi\\in(-\\varpi,-6)\\cup(-2,\\varpi)$ with $ 6 < \\varpi<\\infty$ being an arbitrary constant.For $-\\varpi<\\xi<-6$, there exist four stationary phase points on jump contour, and the asymptotic approximations can be characterized with an $N$-soliton on discrete spectrums and a leading order term $\\mathcal{O}(t^{-1/2})$ on continuous spectrum up to a residual error order $\\mathcal{O}(t^{-3/4})$. For $-2<\\xi<\\varpi$, the leading term of asymptotic expansion is described by the soliton solution and the error order $\\mathcal{O}(t^{-1})$ comes from a $\\bar{\\partial}$-problem. Additionally, asymptotic stability can be obtained."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the defocusing modified Kortweg-de Vries (mKdV) equation with finite density initial data in different solitonic regions. What is the leading term of the asymptotic expansion of the solution $q(x,t)$ for the solitonic region $\\xi\\in(-\\varpi,-6)\\cup(-2,\\varpi)$ with $6 < \\varpi<\\infty$ being an arbitrary constant?\n\nA) An $N$-soliton on discrete spectrums with a leading order term $\\mathcal{O}(t^{-1/2})$ on continuous spectrum up to a residual error order $\\mathcal{O}(t^{-3/4})$.\n\nB) A soliton solution with a leading order term $\\mathcal{O}(t^{-1})$ and an error order $\\mathcal{O}(t^{-1})$ from a $\\bar{\\partial}$-problem.\n\nC) A stationary phase solution with a leading order term $\\mathcal{O}(t^{-1/2})$ and an error order $\\mathcal{O}(t^{-3/4})$.\n\nD) A shock wave solution with a leading order term $\\mathcal{O}(t^{-1})$ and an error order $\\mathcal{O}(t^{-1})$.\n\nCorrect Answer: A) An $N$-soliton on discrete spectrums with a leading order term $\\mathcal{O}(t^{-1/2})$ on continuous spectrum up to a residual error order $\\mathcal{O}(t^{-3/4})$.\n\nExplanation: The correct answer is A) because the documentation states that for $-\\varpi<\\xi<-6$, there exist four stationary phase points on jump contour, and the asymptotic approximations can be characterized with an $N$-soliton on discrete spectrums and a leading order term $\\mathcal{O}(t^{-1/2})$ on continuous spectrum up to a residual error order $\\mathcal{O}(t^{-3/4})$. The other options are incorrect because they do not accurately describe the leading term of the asymptotic expansion for the solitonic region $\\xi\\in(-\\varpi,-6)\\cup(-2,\\varpi)$."}, "58": {"documentation": {"title": "Locally embedded presages of global network bursts", "source": "Satohiro Tajima, Takeshi Mita, Douglas J. Bakkum, Hirokazu Takahashi,\n  Taro Toyoizumi", "docs_id": "1703.04176", "section": ["q-bio.NC", "math.DS", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Locally embedded presages of global network bursts. Spontaneous, synchronous bursting of neural population is a widely observed phenomenon in nervous networks, which is considered important for functions and dysfunctions of the brain. However, how the global synchrony across a large number of neurons emerges from an initially non-bursting network state is not fully understood. In this study, we develop a new state-space reconstruction method combined with high-resolution recordings of cultured neurons. This method extracts deterministic signatures of upcoming global bursts in \"local\" dynamics of individual neurons during non-bursting periods. We find that local information within a single-cell time series can compare with or even outperform the global mean field activity for predicting future global bursts. Moreover, the inter-cell variability in the burst predictability is found to reflect the network structure realized in the non-bursting periods. These findings demonstrate the deterministic mechanisms underlying the locally concentrated early-warnings of the global state transition in self-organized networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary finding of the study on locally embedded presages of global network bursts, and how does it relate to the emergence of global synchrony in neural networks?\n\nA) The study found that local information within a single-cell time series is insufficient to predict global bursts, and that global mean field activity is more effective.\nB) The study demonstrated that local dynamics of individual neurons during non-bursting periods can predict future global bursts with comparable accuracy to global mean field activity.\nC) The study showed that the inter-cell variability in burst predictability is independent of the network structure realized in non-bursting periods.\nD) The study found that the emergence of global synchrony in neural networks is solely determined by the local dynamics of individual neurons.\n\n**Correct Answer:** B) The study demonstrated that local dynamics of individual neurons during non-bursting periods can predict future global bursts with comparable accuracy to global mean field activity.\n\n**Explanation:** The study found that local information within a single-cell time series can compare with or even outperform the global mean field activity for predicting future global bursts. This suggests that the local dynamics of individual neurons during non-bursting periods can provide valuable insights into the emergence of global synchrony in neural networks. The correct answer highlights this key finding, while the incorrect options (A, C, and D) either contradict or misrepresent the study's results."}, "59": {"documentation": {"title": "Achieving translational symmetry in trapped cold ion rings", "source": "Hao-Kun Li, Erik Urban, Crystal Noel, Alexander Chuang, Yang Xia,\n  Anthony Ransford, Boerge Hemmerling, Yuan Wang, Tongcang Li, Hartmut\n  Haeffner, Xiang Zhang", "docs_id": "1605.02143", "section": ["quant-ph", "cond-mat.mes-hall", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Achieving translational symmetry in trapped cold ion rings. Spontaneous symmetry breaking is a universal concept throughout science. For instance, the Landau-Ginzburg paradigm of translational symmetry breaking underlies the classification of nearly all quantum phases of matter and explains the emergence of crystals, insulators, and superconductors. Usually, the consequences of translational invariance are studied in large systems to suppress edge effects which cause undesired symmetry breaking. While this approach works for investigating global properties, studies of local observables and their correlations require access and control of the individual constituents. Periodic boundary conditions, on the other hand, could allow for translational symmetry in small systems where single particle control is achievable. Here, we crystallize up to fifteen 40Ca+ ions in a microscopic ring with inherent periodic boundary conditions. We show the ring's translational symmetry is preserved at millikelvin temperatures by delocalizing the Doppler laser cooled ions. This establishes an upper bound for undesired symmetry breaking at a level where quantum control becomes feasible. These findings pave the way towards studying quantum many-body physics with translational symmetry at the single particle level in a variety of disciplines from simulation of Hawking radiation to exploration of quantum phase transitions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using periodic boundary conditions in small systems, and how does this approach enable the study of local observables and their correlations in the context of translational symmetry?\n\n**A)** Periodic boundary conditions allow for the suppression of edge effects, enabling the study of global properties in small systems.\n**B)** Periodic boundary conditions enable the delocalization of individual constituents, preserving translational symmetry and allowing for the study of local observables and their correlations.\n**C)** Periodic boundary conditions are necessary for the simulation of Hawking radiation, but they do not necessarily preserve translational symmetry in small systems.\n**D)** Periodic boundary conditions are a limitation in studying quantum many-body physics, as they can lead to undesired symmetry breaking.\n\n**Correct Answer:** B) Periodic boundary conditions enable the delocalization of individual constituents, preserving translational symmetry and allowing for the study of local observables and their correlations.\n\n**Explanation:** The correct answer, B, highlights the key advantage of using periodic boundary conditions in small systems. By delocalizing individual constituents, these conditions preserve translational symmetry, enabling the study of local observables and their correlations. This approach is particularly useful in the context of trapped cold ion rings, where single-particle control is achievable. The other options are incorrect because they either misrepresent the primary advantage of periodic boundary conditions (A) or incorrectly associate them with limitations or unrelated applications (C and D)."}}