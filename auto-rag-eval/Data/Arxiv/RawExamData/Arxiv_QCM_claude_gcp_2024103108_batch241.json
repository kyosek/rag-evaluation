{"0": {"documentation": {"title": "Preserving general physical properties in model reduction of dynamical\n  systems via constrained-optimization projection", "source": "A. Schein, K. T. Carlberg, M. J. Zahr", "docs_id": "2011.13998", "section": ["cs.CE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Preserving general physical properties in model reduction of dynamical\n  systems via constrained-optimization projection. Model-reduction techniques aim to reduce the computational complexity of simulating dynamical systems by applying a (Petrov-)Galerkin projection process that enforces the dynamics to evolve in a low-dimensional subspace of the original state space. Frequently, the resulting reduced-order model (ROM) violates intrinsic physical properties of the original full-order model (FOM) (e.g., global conservation, Lagrangian structure, state-variable bounds) because the projection process does not generally ensure preservation of these properties. However, in many applications, ensuring the ROM preserves such intrinsic properties can enable the ROM to retain physical meaning and lead to improved accuracy and stability properties. In this work, we present a general constrained-optimization formulation for projection-based model reduction that can be used as a template to enforce the ROM to satisfy specific properties on the kinematics and dynamics. We introduce constrained-optimization formulations at both the time-continuous (i.e., ODE) level, which leads to a constrained Galerkin projection, and at the time-discrete level, which leads to a least-squares Petrov-Galerkin (LSPG) projection, in the context of linear multistep schemes. We demonstrate the ability of the proposed formulations to equip ROMs with desired properties such as global energy conservation and bounds on the total variation."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: Which of the following statements best describes the primary challenge addressed by the constrained-optimization formulation for projection-based model reduction, as presented in the given text?\n\nA) It aims to increase the computational complexity of simulating dynamical systems.\nB) It seeks to preserve intrinsic physical properties of the original full-order model that are often lost in traditional reduced-order models.\nC) It focuses on expanding the dimensionality of the original state space.\nD) It attempts to eliminate the need for (Petrov-)Galerkin projection in model reduction.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The text explicitly states that the constrained-optimization formulation for projection-based model reduction is developed to address the issue of reduced-order models (ROMs) violating intrinsic physical properties of the original full-order model (FOM). This is evident from the statement: \"Frequently, the resulting reduced-order model (ROM) violates intrinsic physical properties of the original full-order model (FOM) (e.g., global conservation, Lagrangian structure, state-variable bounds) because the projection process does not generally ensure preservation of these properties.\"\n\nOption A is incorrect because the goal of model reduction is to reduce, not increase, computational complexity.\n\nOption C is incorrect because model reduction aims to reduce dimensionality, not expand it.\n\nOption D is incorrect because the approach still uses (Petrov-)Galerkin projection, but with added constraints to preserve physical properties.\n\nThe question tests the reader's understanding of the core problem addressed by the proposed method and requires careful reading of the text to distinguish between the main goal and other aspects of model reduction techniques."}, "1": {"documentation": {"title": "Enhancing the Robustness of Deep Neural Networks by Boundary Conditional\n  GAN", "source": "Ke Sun, Zhanxing Zhu, Zhouchen Lin", "docs_id": "1902.11029", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enhancing the Robustness of Deep Neural Networks by Boundary Conditional\n  GAN. Deep neural networks have been widely deployed in various machine learning tasks. However, recent works have demonstrated that they are vulnerable to adversarial examples: carefully crafted small perturbations to cause misclassification by the network. In this work, we propose a novel defense mechanism called Boundary Conditional GAN to enhance the robustness of deep neural networks against adversarial examples. Boundary Conditional GAN, a modified version of Conditional GAN, can generate boundary samples with true labels near the decision boundary of a pre-trained classifier. These boundary samples are fed to the pre-trained classifier as data augmentation to make the decision boundary more robust. We empirically show that the model improved by our approach consistently defenses against various types of adversarial attacks successfully. Further quantitative investigations about the improvement of robustness and visualization of decision boundaries are also provided to justify the effectiveness of our strategy. This new defense mechanism that uses boundary samples to enhance the robustness of networks opens up a new way to defense adversarial attacks consistently."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary purpose and mechanism of the Boundary Conditional GAN approach in enhancing the robustness of deep neural networks against adversarial attacks?\n\nA) It generates adversarial examples to train the network to resist attacks\nB) It modifies the architecture of the neural network to create impenetrable layers\nC) It generates boundary samples near the decision boundary to augment training data and improve robustness\nD) It uses a conditional GAN to filter out potential adversarial inputs before they reach the classifier\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The Boundary Conditional GAN approach, as described in the documentation, generates boundary samples with true labels near the decision boundary of a pre-trained classifier. These samples are then used as data augmentation to make the decision boundary more robust. This process directly addresses the vulnerability of deep neural networks to adversarial examples by strengthening the areas where the network is most susceptible to misclassification.\n\nOption A is incorrect because the approach generates boundary samples, not adversarial examples. While adversarial training is a known defense mechanism, it's not the method described here.\n\nOption B is incorrect as the approach doesn't modify the network architecture itself, but rather augments the training data to improve the existing network's robustness.\n\nOption D is incorrect because the Boundary Conditional GAN is not used as a filter for inputs, but as a generator of additional training samples to improve the classifier's robustness.\n\nThis question tests understanding of the novel defense mechanism proposed in the paper, requiring candidates to distinguish between different approaches to improving neural network robustness and to identify the specific mechanism employed by the Boundary Conditional GAN method."}, "2": {"documentation": {"title": "A Class of Maximal-Rate, Low-PAPR, Non-square Complex Orthogonal Designs", "source": "Smarajit Das, B. Sundar Rajan", "docs_id": "0808.1400", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Class of Maximal-Rate, Low-PAPR, Non-square Complex Orthogonal Designs. Space-time block codes (STBCs) from non-square complex orthogonal designs are bandwidth efficient when compared with those from square real/complex orthogonal designs. Though there exists rate-1 ROD for any number of transmit antennas, rate-1 complex orthogonal designs (COD) does not exist for more than 2 transmit antennas. Liang (IEEE Trans. Inform. Theory, 2003) and Lu et al (IEEE Trans. Inform. Theory, 2005) have constructed a class of maximal rate non-square CODs where the rate is ${1/2}+\\frac{1}{n}$ if number of transmit antennas $n$ is even and ${1/2}+\\frac{1}{n+1}$ if $n$ is odd. In this paper, we present a simple construction for maximal rate non-square CODs obtained from square CODs which resembles the construction of rate-1 non-square RODs from square RODs. These designs are shown to be amenable for construction of a class of generalized CODs (called Coordinate-Interleaved Scaled CODs) with low peak-to-average power ratio (PAPR) having the same parameters as the maximal rate codes. Simulation results indicate that these codes perform better than the existing maximal rate codes under peak power constraint while performing the same under average power constraint."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Consider a space-time block code (STBC) using a non-square complex orthogonal design (COD) for a system with 6 transmit antennas. Which of the following statements is correct regarding the maximal achievable rate and peak-to-average power ratio (PAPR) characteristics?\n\nA) The maximal achievable rate is 2/3, and the design cannot be modified to achieve low PAPR without changing this rate.\n\nB) The maximal achievable rate is 5/7, and a Coordinate-Interleaved Scaled COD can be constructed to achieve low PAPR while maintaining this rate.\n\nC) The maximal achievable rate is 3/4, and the design inherently has low PAPR without any modifications.\n\nD) The maximal achievable rate is 7/12, and a Coordinate-Interleaved Scaled COD can be constructed to achieve low PAPR while maintaining this rate.\n\nCorrect Answer: B\n\nExplanation: For a system with 6 transmit antennas (n = 6, which is even), the maximal achievable rate for a non-square COD is 1/2 + 1/n = 1/2 + 1/6 = 5/7. The paper introduces a construction method for maximal rate non-square CODs that can be used to create Coordinate-Interleaved Scaled CODs. These designs maintain the same parameters (including the maximal rate) while achieving low PAPR. Therefore, option B is correct. \n\nOption A is incorrect because the rate is miscalculated and it wrongly states that low PAPR cannot be achieved. Option C is incorrect because the rate is wrong and CODs do not inherently have low PAPR. Option D is incorrect because the rate is miscalculated, although it correctly mentions the possibility of constructing a low PAPR design."}, "3": {"documentation": {"title": "The overdoped end of the cuprate phase diagram", "source": "Thomas A. Maier, Seher Karakuzu, and Douglas J. Scalapino", "docs_id": "2004.13120", "section": ["cond-mat.supr-con", "cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The overdoped end of the cuprate phase diagram. Studying the disappearance of superconductivity at the end of the overdoped region of the cuprate phase diagram offers a different approach for investigating the interaction which is responsible for pairing in these materials. In the underdoped region this question is complicated by the presence of charge and stripe ordered phases as well as the pseudogap. In the overdoped region the situation appears simpler with only a normal phase, a superconducting phase and impurity scattering. Here, for the overdoped region, we report the results of a combined dynamic cluster approximation (DCA) and a weak Born impurity scattering calculation for a $t-t'-U$ Hubbard model. We find that a decrease in the $d$-wave pairing strength of the two-particle scattering vertex is closely coupled to changes in the momentum and frequency structure of the magnetic spin fluctuations as the system is overdoped. Treating the impurity scattering within a disordered BCS $d$-wave approximation, we see how the combined effects of the decreasing $d$-wave pairing strength and weak impurity scattering lead to the end of the $T_c$ dome."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of the overdoped end of the cuprate phase diagram, which combination of factors is reported to lead to the end of the Tc dome according to the combined dynamic cluster approximation (DCA) and weak Born impurity scattering calculation?\n\nA) Increasing d-wave pairing strength and strong impurity scattering\nB) Decreasing d-wave pairing strength and weak impurity scattering\nC) Constant d-wave pairing strength and increasing charge ordering\nD) Increasing pseudogap effects and stripe ordered phases\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that \"the combined effects of the decreasing d-wave pairing strength and weak impurity scattering lead to the end of the Tc dome.\" This directly corresponds to option B.\n\nOption A is incorrect because the pairing strength is decreasing, not increasing, and the impurity scattering is described as weak, not strong.\n\nOption C is incorrect because the d-wave pairing strength is not constant, but decreasing. Additionally, charge ordering is not mentioned as a factor in the overdoped region.\n\nOption D is incorrect because the pseudogap and stripe ordered phases are specifically mentioned as complications in the underdoped region, not the overdoped region being discussed in this study.\n\nThis question tests the student's ability to carefully read and interpret complex scientific information, distinguishing between factors relevant to different regions of the cuprate phase diagram and identifying the specific combination of effects leading to the end of superconductivity in the overdoped region."}, "4": {"documentation": {"title": "Robustness of the international oil trade network under targeted attacks\n  to economies", "source": "N. Wei, W.-J. Xie, W.-X. Zhou", "docs_id": "2101.10679", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robustness of the international oil trade network under targeted attacks\n  to economies. In the international oil trade network (iOTN), trade shocks triggered by extreme events may spread over the entire network along the trade links of the central economies and even lead to the collapse of the whole system. In this study, we focus on the concept of \"too central to fail\" and use traditional centrality indicators as strategic indicators for simulating attacks on economic nodes, and simulates various situations in which the structure and function of the global oil trade network are lost when the economies suffer extreme trade shocks. The simulation results show that the global oil trade system has become more vulnerable in recent years. The regional aggregation of oil trade is an essential source of iOTN's vulnerability. Maintaining global oil trade stability and security requires a focus on economies with greater influence within the network module of the iOTN. International organizations such as OPEC and OECD established more trade links around the world, but their influence on the iOTN is declining. We improve the framework of oil security and trade risk assessment based on the topological index of iOTN, and provide a reference for finding methods to maintain network robustness and trade stability."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the vulnerability of the international oil trade network (iOTN) and the implications for maintaining global oil trade stability, according to the study?\n\nA) The iOTN has become more resilient in recent years, with international organizations like OPEC and OECD playing an increasingly influential role in stabilizing the network.\n\nB) Regional diversification of oil trade has decreased the vulnerability of the iOTN, suggesting that a focus on global trade links is more important than regional influences.\n\nC) The iOTN has become more vulnerable in recent years, with regional aggregation of oil trade being a key source of vulnerability, and maintaining stability requires focusing on economies with greater influence within network modules.\n\nD) The concept of \"too central to fail\" is outdated, and traditional centrality indicators are no longer useful for simulating attacks on economic nodes in the iOTN.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately reflects the key findings and implications of the study. The passage states that \"the global oil trade system has become more vulnerable in recent years\" and that \"regional aggregation of oil trade is an essential source of iOTN's vulnerability.\" Furthermore, it emphasizes that \"maintaining global oil trade stability and security requires a focus on economies with greater influence within the network module of the iOTN.\"\n\nOption A is incorrect because it contradicts the study's findings about increased vulnerability and the declining influence of international organizations like OPEC and OECD.\n\nOption B is incorrect because it misrepresents the impact of regional aggregation, which the study identifies as a source of vulnerability rather than a stabilizing factor.\n\nOption D is incorrect because the study explicitly uses the concept of \"too central to fail\" and traditional centrality indicators as part of its methodology for simulating attacks on economic nodes."}, "5": {"documentation": {"title": "Ariadne: PyTorch Library for Particle Track Reconstruction Using Deep\n  Learning", "source": "Pavel Goncharov, Egor Schavelev, Anastasia Nikolskaya, Gennady Ososkov", "docs_id": "2109.08982", "section": ["physics.data-an", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ariadne: PyTorch Library for Particle Track Reconstruction Using Deep\n  Learning. Particle tracking is a fundamental part of the event analysis in high energy and nuclear physics. Events multiplicity increases each year along with the drastic growth of the experimental data which modern HENP detectors produce, so the classical tracking algorithms such as the well-known Kalman filter cannot satisfy speed and scaling requirements. At the same time, breakthroughs in the study of deep learning open an opportunity for the application of high-performance deep neural networks for solving tracking problems in a dense environment of experiments with heavy ions. However, there are no well-documented software libraries for deep learning track reconstruction yet. We introduce Ariadne, the first open-source library for particle tracking based on the PyTorch deep learning framework. The goal of our library is to provide a simple interface that allows one to prepare train and test datasets and to train and evaluate one of the deep tracking models implemented in the library on the data from your specific experiment. The user experience is greatly facilitated because of the system of gin-configurations. The modular structure of the library and abstract classes let the user develop his data processing pipeline and deep tracking model easily. The proposed library is open-source to facilitate academic research in the field of particle tracking based on deep learning."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the primary motivation behind the development of the Ariadne library?\n\nA) To replace classical tracking algorithms entirely in all high energy and nuclear physics experiments\nB) To provide a closed-source solution for particle tracking using traditional methods\nC) To offer an open-source library for deep learning-based particle tracking that addresses speed and scaling issues in dense environments\nD) To create a standalone deep learning framework specifically for particle physics\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. The Ariadne library was developed to offer an open-source solution for particle tracking using deep learning techniques. This addresses the limitations of classical tracking algorithms like the Kalman filter, which struggle with speed and scaling in the increasingly dense environments of modern high energy and nuclear physics experiments.\n\nOption A is incorrect because the library aims to complement, not entirely replace, classical algorithms.\n\nOption B is incorrect on two counts: Ariadne is open-source, not closed-source, and it focuses on deep learning methods rather than traditional ones.\n\nOption D is incorrect because Ariadne is built on top of the existing PyTorch deep learning framework, rather than creating a new standalone framework.\n\nThe question tests understanding of the library's purpose, its open-source nature, and its relationship to both classical methods and existing deep learning frameworks."}, "6": {"documentation": {"title": "Analysis of two-point statistics of cosmic shear: II. Optimizing the\n  survey geometry", "source": "Martin Kilbinger and Peter Schneider (Institut f. Astrophysik, Univ.\n  Bonn)", "docs_id": "astro-ph/0308119", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of two-point statistics of cosmic shear: II. Optimizing the\n  survey geometry. We present simulations of a cosmic shear survey and show how the survey geometry influences the accuracy of determination of cosmological parameters. We numerically calculate the full covariance matrices Cov of two-point statistics of cosmic shear, based on the expressions derived in the first paper of this series. The individual terms are compared for two survey geometries with large and small cosmic variance. We use analyses based on maximum likelihood of Cov and the Fisher information matrix in order to derive expected constraints on cosmological parameters. As an illustrative example, we simulate various survey geometries consisting of 300 individual fields of 13'x13' size, placed (semi-)randomly into patches which are assumed to be widely separated on the sky and therefore uncorrelated. Using the aperture mass statistics, the optimum survey consists of 10 patches with 30 images in each patch. If \\Omega_m, \\sigma_8 and \\Gamma are supposed to be extracted from the data, the minimum variance bounds on these three parameters are 0.17, 0.25 and 0.04 respectively. These variances raise slightly when the initial power spectrum index n_s is also to be determined from the data. The cosmological constant is only poorly constrained."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A cosmic shear survey is being planned to optimize the determination of cosmological parameters. The survey will consist of 300 individual fields, each 13'x13' in size. Based on the information provided, which of the following survey designs would likely yield the most accurate results for determining \u03a9m, \u03c38, and \u0393?\n\nA) 30 patches with 10 images in each patch\nB) 15 patches with 20 images in each patch\nC) 5 patches with 60 images in each patch\nD) 10 patches with 30 images in each patch\n\nCorrect Answer: D\n\nExplanation: The documentation states that \"the optimum survey consists of 10 patches with 30 images in each patch\" when using the aperture mass statistics. This configuration provides the best balance between reducing cosmic variance and maintaining statistical power. The study simulated various survey geometries and found this particular arrangement to be optimal for constraining the cosmological parameters \u03a9m, \u03c38, and \u0393, with minimum variance bounds of 0.17, 0.25, and 0.04 respectively. The other options either have too many patches (increasing cosmic variance) or too few (reducing statistical power across different regions of the sky)."}, "7": {"documentation": {"title": "Constraints on Compact Hyperbolic Spaces from COBE", "source": "J. Richard Bond, Dmitry Pogosyan and Tarun Souradeep", "docs_id": "astro-ph/9702212", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraints on Compact Hyperbolic Spaces from COBE. The (large angle) COBE DMR data can be used to probe the global topology of our universe on scales comparable to and just beyond the present ``horizon''. For compact topologies, the two main effects on the CMB are: [1] the breaking of statistical isotropy in characteristic patterns determined by the photon geodesic structure of the manifold and [2] an infrared cutoff in the power spectrum of perturbations imposed by the finite spatial extent. To make a detailed confrontation of these effects with the COBE maps requires the computation of the pixel-pixel temperature correlation function for each topology and for each orientation of it relative to the sky. We present a general technique using the method of images for doing this in compact hyperbolic (CH) topologies which does not require spatial eigenmode decomposition. We demonstrate that strong constraints on compactness follow from [2] and that these limits can be improved by exploiting the details of the geodesic structure for each individual topology ([1]), as we show for the flat 3-torus and selected CH models."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In the context of probing the global topology of the universe using COBE DMR data, which of the following statements is NOT true regarding the effects on the Cosmic Microwave Background (CMB) for compact topologies?\n\nA) The breaking of statistical isotropy occurs in characteristic patterns determined by the photon geodesic structure of the manifold.\n\nB) An ultraviolet cutoff in the power spectrum of perturbations is imposed by the finite spatial extent.\n\nC) Detailed analysis requires computation of the pixel-pixel temperature correlation function for each topology and its orientation relative to the sky.\n\nD) The method of images can be used for computing correlations in compact hyperbolic topologies without requiring spatial eigenmode decomposition.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because the documentation states that there is an \"infrared cutoff\" in the power spectrum of perturbations, not an ultraviolet cutoff. An infrared cutoff affects large scales (low frequencies), while an ultraviolet cutoff would affect small scales (high frequencies).\n\nOption A is correct as it directly corresponds to effect [1] mentioned in the text.\n\nOption C is accurate, as the documentation explicitly states that detailed confrontation with COBE maps requires this computation.\n\nOption D is also correct, as the text mentions a general technique using the method of images for compact hyperbolic topologies that doesn't require spatial eigenmode decomposition."}, "8": {"documentation": {"title": "Non-linear Dynamics and Mass Function of Cosmic Structures: I Analytical\n  Results", "source": "Edouard Audit (1), Romain Teyssier (2) & Jean-Michel Alimi (1) ((1):\n  LAEC, Observatoire de Meudon, France (2): Service d'Astrophysique CEA-SACLAY,\n  France)", "docs_id": "astro-ph/9704023", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-linear Dynamics and Mass Function of Cosmic Structures: I Analytical\n  Results. We investigate some modifications to the Press & Schechter (1974) (PS) prescription resulting from shear and tidal effects. These modifications rely on more realistic treatments of the collapse process than the standard approach based on the spherical model. First, we show that the mass function resulting from a new approximate Lagrangian dynamic (Audit & Alimi 96), contains more objects at high mass, than the classical PS mass function and is well fitted by a PS-like function with a threshold density of $\\delta_c \\simeq 1.4$. However, such a Lagrangian description can underestimate the epoch of structure formation since it defines it as the collapse of the first principal axis. We therefore suggest some analytical prescriptions, for computing the collapse time along the second and third principal axes, and we deduce the corresponding mass functions. The collapse along the third axis is delayed by the shear and the number of objects of high mass then decreases. Finally, we show that the shear also strongly affects the formation of low-mass halos. This dynamical effect implies a modification of the low-mass slope of the mass function and allows the reproduction of the observed luminosity function of field galaxies. In a companion paper, we present results of numerical simulations which complete this work."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements accurately reflects the findings of the study regarding modifications to the Press & Schechter (1974) mass function?\n\nA) The modified mass function shows fewer high-mass objects and is best fitted by a PS-like function with a threshold density of \u03b4c \u2248 1.6\n\nB) The Lagrangian description tends to overestimate the epoch of structure formation by defining it as the collapse of the first principal axis\n\nC) The collapse along the third axis is accelerated by shear, leading to an increase in the number of high-mass objects\n\nD) Shear effects modify the low-mass slope of the mass function, potentially explaining the observed luminosity function of field galaxies\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The document states that \"the shear also strongly affects the formation of low-mass halos. This dynamical effect implies a modification of the low-mass slope of the mass function and allows the reproduction of the observed luminosity function of field galaxies.\"\n\nOption A is incorrect because the study found more objects at high mass, not fewer, and the threshold density was \u03b4c \u2248 1.4, not 1.6.\n\nOption B is incorrect because the Lagrangian description is said to underestimate, not overestimate, the epoch of structure formation.\n\nOption C is incorrect because the collapse along the third axis is delayed by shear, not accelerated, and this leads to a decrease in the number of high-mass objects, not an increase."}, "9": {"documentation": {"title": "Fast scrambling in holographic Einstein-Podolsky-Rosen pair", "source": "Keiju Murata", "docs_id": "1708.09493", "section": ["hep-th", "cond-mat.dis-nn", "gr-qc", "nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast scrambling in holographic Einstein-Podolsky-Rosen pair. We demonstrate that a holographic model of the Einstein-Podolsky-Rosen pair exhibits fast scrambling. Strongly entangled quark and antiquark in $\\mathcal{N}=4$ super Yang-Mills theory are considered. Their gravity dual is a fundamental string whose endpoints are uniformly accelerated in opposite direction. We slightly increase the acceleration of the endpoint and show that it quickly destroys the correlation between the quark and antiquark. The proper time scale of the destruction is $\\tau_\\ast\\sim \\beta \\ln S$ where $\\beta$ is the inverse Unruh temperature and $S$ is the entropy of the accelerating quark. We also evaluate the Lyapunov exponent from correlation function as $\\lambda_L=2\\pi/\\beta$, which saturates the Lyapunov bound. Our results suggest that the fast scrambling or saturation of the Lyapunov bound do not directly imply the existence of an Einstein dual. When we slightly decrease the acceleration, the quark and antiquark are causally connected and an \"one-way traversable wormhole\" is created on the worldsheet. It causes the divergence of the correlation function between the quark and antiquark."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: In the holographic model of the Einstein-Podolsky-Rosen pair described, what happens when the acceleration of the endpoint is slightly increased, and how does this relate to the concept of fast scrambling?\n\nA) The correlation between quark and antiquark is gradually weakened over a long timescale, with no relation to fast scrambling.\n\nB) The correlation between quark and antiquark is quickly destroyed, with a proper time scale of destruction \u03c4_* ~ \u03b2 ln S, demonstrating fast scrambling.\n\nC) The correlation between quark and antiquark remains unchanged, showing that the system is resistant to fast scrambling.\n\nD) The correlation between quark and antiquark is enhanced, contradicting the concept of fast scrambling in this model.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The documentation states that when the acceleration of the endpoint is slightly increased, it quickly destroys the correlation between the quark and antiquark. This rapid destruction of correlation is a hallmark of fast scrambling. The proper time scale of the destruction is given as \u03c4_* ~ \u03b2 ln S, where \u03b2 is the inverse Unruh temperature and S is the entropy of the accelerating quark. This logarithmic dependence on the entropy is characteristic of fast scrambling systems. \n\nAnswer A is incorrect because the destruction of correlation happens quickly, not gradually over a long timescale. \n\nAnswer C is incorrect because the correlation is not unchanged but is quickly destroyed. \n\nAnswer D is incorrect because the correlation is destroyed, not enhanced, and this behavior is consistent with, not contradictory to, fast scrambling.\n\nAdditionally, the question relates to the concept of fast scrambling by demonstrating how quickly information (in this case, the correlation between quark and antiquark) is spread or \"scrambled\" throughout the system when a small perturbation (the slight increase in acceleration) is introduced."}, "10": {"documentation": {"title": "Description of $^{93}$Nb stellar electron-capture rates by the Projected\n  Shell Model", "source": "Long-Jun Wang, Liang Tan, Zhipan Li, Bingshui Gao, Yang Sun", "docs_id": "2111.14288", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Description of $^{93}$Nb stellar electron-capture rates by the Projected\n  Shell Model. Capture of electrons by nuclei is an important process in stellar environments where excited nuclear states are thermally populated. However, accurate treatment for excited configurations in electron capture (EC) rates has been an unsolved problem for medium-heavy and heavy nuclei. In this work, we take the $^{93}$Nb $\\rightarrow$ $^{93}$Zr EC rates as the example to introduce the Projected-Shell-Model (PSM) in which excited configurations are explicitly included as multi-quasiparticle states. Applying the prevalent assumption that the parent nucleus always stays in its ground state in stellar conditions, we critically compare the obtained PSM results with the recently-measured Gamow-Teller transition data, and with the previous calculations by the conventional shell model and the quasiparticle random-phase approximation. We discuss important ingredients that are required in theoretical models used for stellar EC calculations, and demonstrate effects of the explicit inclusion of excited nuclear states in EC rate calculations, especially when both electron density and environment temperature are high."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of stellar electron-capture rates for $^{93}$Nb, which of the following statements best describes the advantages of using the Projected Shell Model (PSM) over conventional methods?\n\nA) PSM only considers the ground state of the parent nucleus, leading to more accurate results.\nB) PSM exclusively uses the quasiparticle random-phase approximation for calculations.\nC) PSM explicitly includes excited configurations as multi-quasiparticle states, allowing for a more comprehensive treatment of nuclear states.\nD) PSM is less computationally intensive than conventional shell model calculations.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The Projected Shell Model (PSM) explicitly includes excited configurations as multi-quasiparticle states, which allows for a more comprehensive treatment of nuclear states in electron capture rate calculations. This is a key advantage of PSM over conventional methods.\n\nAnswer A is incorrect because the document states that assuming the parent nucleus always stays in its ground state is a \"prevalent assumption,\" not a feature of PSM. In fact, PSM's ability to include excited states is highlighted as an important feature.\n\nAnswer B is incorrect because PSM is presented as an alternative to the quasiparticle random-phase approximation, not a method that exclusively uses it.\n\nAnswer D is incorrect because the document doesn't discuss the computational intensity of PSM compared to conventional methods. Moreover, including more states (as PSM does) typically increases computational complexity rather than reducing it.\n\nThe correct answer highlights PSM's ability to explicitly include excited configurations, which is crucial for accurate treatment of electron capture rates, especially in high temperature and high electron density environments."}, "11": {"documentation": {"title": "Fourier spectra from exoplanets with polar caps and ocean glint", "source": "P.M. Visser and F.J. van de Bult", "docs_id": "1503.05059", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fourier spectra from exoplanets with polar caps and ocean glint. The weak orbital-phase dependent reflection signal of an exoplanet contains information on the planet surface, such as the distribution of continents and oceans on terrestrial planets. This light curve is usually studied in the time domain, but because the signal from a stationary surface is (quasi)periodic, analysis of the Fourier series may provide an alternative, complementary approach. We study Fourier spectra from reflected light curves for geometrically simple configurations. Depending on its atmospheric properties, a rotating planet in the habitable zone could have circular polar ice caps. Tidally locked planets, on the other hand, may have symmetric circular oceans facing the star. These cases are interesting because the high-albedo contrast at the sharp edges of the ice-sheets and the glint from the host star in the ocean may produce recognizable light curves with orbital periodicity, which could also be interpreted in the Fourier domain. We derive a simple general expression for the Fourier coefficients of a quasiperiodic light curve in terms of the albedo map of a Lambertian planet surface. Analytic expressions for light curves and their spectra are calculated for idealized situations, and dependence of spectral peaks on the key parameters inclination, obliquity, and cap size is studied."}, "answer": "Question: An exoplanet with polar ice caps is observed over multiple orbital periods. Which of the following statements about analyzing its light curve is most accurate?\n\nA) The Fourier spectrum of the light curve will show no peaks related to the ice caps, as their reflection is constant throughout the orbit.\n\nB) Time domain analysis is the only reliable method to detect the presence of polar ice caps from the light curve.\n\nC) The Fourier spectrum will display peaks at frequencies related to the planet's rotation period, with amplitudes influenced by the size of the ice caps and the system's inclination.\n\nD) Glint from oceans on a tidally locked planet would produce the same Fourier spectrum as polar ice caps on a rotating planet.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that the reflection signal from an exoplanet with polar ice caps is (quasi)periodic, making it suitable for Fourier analysis. The Fourier spectrum would indeed show peaks related to the planet's rotation period, as this determines the frequency at which the ice caps come into and out of view. The amplitudes of these peaks would be influenced by the size of the ice caps (affecting the albedo contrast) and the system's inclination (affecting how much of the polar regions are visible).\n\nAnswer A is incorrect because the ice caps do create a periodic signal as the planet rotates.\n\nAnswer B is false because the document explicitly states that Fourier analysis can provide a complementary approach to time domain analysis.\n\nAnswer D is incorrect because tidally locked planets with oceans would have a different light curve pattern compared to rotating planets with polar caps, resulting in different Fourier spectra."}, "12": {"documentation": {"title": "Correlation energy within exact-exchange ACFD theory: systematic\n  development and simple approximations", "source": "Nicola Colonna, Maria Hellgren, Stefano de Gironcoli", "docs_id": "1409.0354", "section": ["cond-mat.str-el", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlation energy within exact-exchange ACFD theory: systematic\n  development and simple approximations. We have calculated the correlation energy of the homogeneous electron gas (HEG) and the dissociation energy curves of molecules with covalent bonds from a novel implementation of the adiabatic connection fluctuation dissipation (ACFD) expression including the exact exchange (EXX) kernel. The EXX kernel is defined from first order perturbation theory and used in the Dyson equation of time-dependent density functional theory. Within this approximation (RPAx), the correlation energies of the HEG are significantly improved with respect to the RPA up to densities of the order of $r_s \\approx 10$. However, beyond this value, the RPAx response function exhibits an unphysical divergence and the approximation breaks down. Total energies of molecules at equilibrium are also highly accurate but we find a similar instability at stretched geometries. Staying within an exact first order approximation to the response function we use an alternative resummation of the higher order terms. This slight redefinition of RPAx fixes the instability in total energy calculations without compromising the overall accuracy of the approach."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of the adiabatic connection fluctuation dissipation (ACFD) theory with exact exchange (EXX) kernel, which of the following statements is correct regarding the performance of the RPAx approximation for the homogeneous electron gas (HEG)?\n\nA) RPAx improves correlation energies for all densities compared to RPA\nB) RPAx shows significant improvement over RPA for densities up to rs \u2248 10, but exhibits an unphysical divergence beyond this value\nC) RPAx performs worse than RPA for all densities of the HEG\nD) RPAx shows uniform improvement over RPA for all densities, including very low density regimes\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. According to the documentation, the RPAx approximation significantly improves correlation energies of the homogeneous electron gas (HEG) compared to the RPA (Random Phase Approximation) for densities up to rs \u2248 10. However, beyond this density value, the RPAx response function exhibits an unphysical divergence, causing the approximation to break down.\n\nOption A is incorrect because the improvement is not for all densities, but only up to rs \u2248 10. \nOption C is wrong as RPAx does show improvement over RPA for a range of densities. \nOption D is incorrect because the improvement is not uniform for all densities, and the approximation breaks down for very low densities (high rs values).\n\nThis question tests the understanding of the limitations and strengths of the RPAx approximation within the ACFD framework, particularly its behavior for different density regimes of the homogeneous electron gas."}, "13": {"documentation": {"title": "Mismatched Data Detection in Massive MU-MIMO", "source": "Charles Jeon, Arian Maleki, and Christoph Studer", "docs_id": "2007.06491", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mismatched Data Detection in Massive MU-MIMO. We investigate mismatched data detection for massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems in which the prior distribution of the transmit signal used in the data detector differs from the true prior. In order to minimize the performance loss caused by the prior mismatch, we include a tuning stage into the recently proposed large-MIMO approximate message passing (LAMA) algorithm, which enables the development of data detectors with optimal as well as sub-optimal parameter tuning. We show that carefully-selected priors enable the design of simpler and computationally more efficient data detection algorithms compared to LAMA that uses the optimal prior, while achieving near-optimal error-rate performance. In particular, we demonstrate that a hardware-friendly approximation of the exact prior enables the design of low-complexity data detectors that achieve near individually-optimal performance. Furthermore, for Gaussian priors and uniform priors within a hypercube covering the quadrature amplitude modulation (QAM) constellation, our performance analysis recovers classical and recent results on linear and non-linear massive MU-MIMO data detection, respectively."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of mismatched data detection for massive MU-MIMO systems, which of the following statements is most accurate regarding the use of carefully-selected priors?\n\nA) They always result in optimal error-rate performance compared to the LAMA algorithm with the optimal prior.\n\nB) They lead to more complex data detection algorithms but with improved error-rate performance.\n\nC) They enable the design of simpler and more computationally efficient data detectors while achieving near-optimal error-rate performance.\n\nD) They are primarily useful for Gaussian priors but not for uniform priors within a hypercube covering QAM constellations.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation states that \"carefully-selected priors enable the design of simpler and computationally more efficient data detection algorithms compared to LAMA that uses the optimal prior, while achieving near-optimal error-rate performance.\" This directly supports the statement in option C.\n\nOption A is incorrect because the text mentions \"near-optimal\" performance, not always optimal. Option B is incorrect as it contradicts the documentation, which states that carefully-selected priors lead to simpler and more efficient algorithms, not more complex ones. Option D is incorrect because the documentation indicates that the approach works for both Gaussian priors and uniform priors within a hypercube covering QAM constellations, not primarily for Gaussian priors."}, "14": {"documentation": {"title": "Was there a COVID-19 harvesting effect in Northern Italy?", "source": "Augusto Cerqua, Roberta Di Stefano, Marco Letta, Sara Miccoli", "docs_id": "2103.01812", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Was there a COVID-19 harvesting effect in Northern Italy?. We investigate the possibility of a harvesting effect, i.e. a temporary forward shift in mortality, associated with the COVID-19 pandemic by looking at the excess mortality trends of an area that registered one of the highest death tolls in the world during the first wave, Northern Italy. We do not find any evidence of a sizable COVID-19 harvesting effect, neither in the summer months after the slowdown of the first wave nor at the beginning of the second wave. According to our estimates, only a minor share of the total excess deaths detected in Northern Italian municipalities over the entire period under scrutiny (February - November 2020) can be attributed to an anticipatory role of COVID-19. A slightly higher share is detected for the most severely affected areas (the provinces of Bergamo and Brescia, in particular), but even in these territories, the harvesting effect can only account for less than 20% of excess deaths. Furthermore, the lower mortality rates observed in these areas at the beginning of the second wave may be due to several factors other than a harvesting effect, including behavioral change and some degree of temporary herd immunity. The very limited presence of short-run mortality displacement restates the case for containment policies aimed at minimizing the health impacts of the pandemic."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Based on the study of excess mortality trends in Northern Italy during the COVID-19 pandemic, which of the following conclusions is most accurately supported by the research?\n\nA) The harvesting effect accounted for approximately 50% of excess deaths in the most severely affected areas.\n\nB) There was strong evidence of a significant COVID-19 harvesting effect across all of Northern Italy.\n\nC) The lower mortality rates at the beginning of the second wave were primarily due to the harvesting effect.\n\nD) Only a small proportion of excess deaths could be attributed to a COVID-19 harvesting effect, even in the hardest-hit regions.\n\nCorrect Answer: D\n\nExplanation: The study found no evidence of a sizable COVID-19 harvesting effect in Northern Italy. Even in the most severely affected areas (Bergamo and Brescia provinces), the harvesting effect accounted for less than 20% of excess deaths, not 50% as stated in option A. Option B is incorrect because the research explicitly states that they did not find evidence of a sizable harvesting effect. Option C is also incorrect, as the study suggests that lower mortality rates at the beginning of the second wave could be due to several factors other than a harvesting effect, such as behavioral changes and temporary herd immunity. Option D is the most accurate representation of the study's findings, as it correctly states that only a small proportion of excess deaths could be attributed to a COVID-19 harvesting effect, even in the hardest-hit regions."}, "15": {"documentation": {"title": "Nonet symmetry in \\eta, \\eta^{\\prime} and B\\to K\\eta,K\\eta^{\\prime}\n  decays", "source": "T. N. Pham", "docs_id": "0710.2412", "section": ["hep-ph", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonet symmetry in \\eta, \\eta^{\\prime} and B\\to K\\eta,K\\eta^{\\prime}\n  decays. The nonet symmetry scheme seems to describe rather well the masses and $\\eta-\\eta^{\\prime}$ mixing angle of the ground state pseudo-scalar mesons. It is expected that nonet symmetry should also be valid for the matrix elements of the pseudo-scalar densitty operators which play an important role in charmless two-body B decays with $\\eta$ or $\\eta^{\\prime}$ in the final state. Starting from the divergences of the SU(3) octet and singlet axial vector currents, we show that nonet symmetry for the pseudo-scalar mass term implies nonet symmetry for the pseudo-scalar density operators. In this nonet symmetry scheme, we find that the branching ratio $B\\to PP,PV$, with $\\eta$ in the final state agrees well with data, while those with $\\eta'$ are underestimated, but by increasing the $B\\to \\eta'$ form factor by $40-50%$, one could explain the tree-dominated $B^{-}\\to \\pi^{-}\\eta'$ and $B^{-}\\to \\rho^{-}\\eta'$ measured branching ratios. With this increased form factor and with only a moderate annihilation contribution, we are able to obtain $62\\times 10^{-6}$ for the penguin-dominated $B^{-}\\to K^{-}\\eta'$ branching ratios, quite close to the measured value. This supports the predicted value for the $B\\to \\eta'$ form factor in PQCD and light-cone sum rules approach. A possible increase by 15% of $<0|\\bar{s} i\\gamma_5 s|s\\bar{s}>$ for $\\eta_{0} $ would bring the predicted $B^{-}\\to K^{-}\\eta'$ branching ratio to $69.375\\times 10^{-6}$, very close to experiment."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the findings and implications of the nonet symmetry scheme in relation to B meson decays involving \u03b7 and \u03b7' mesons?\n\nA) The nonet symmetry scheme perfectly predicts all branching ratios for B\u2192PP and B\u2192PV decays involving \u03b7 and \u03b7', without any need for adjustments to form factors or matrix elements.\n\nB) The scheme underestimates branching ratios for decays involving \u03b7', but increasing the B\u2192\u03b7' form factor by 40-50% allows for explanation of tree-dominated decays, while penguin-dominated decays require significant annihilation contributions.\n\nC) Nonet symmetry accurately predicts branching ratios for decays with \u03b7, underestimates those with \u03b7', but increasing the B\u2192\u03b7' form factor by 40-50% explains tree-dominated decays and allows for close prediction of penguin-dominated B\u207b\u2192K\u207b\u03b7' with only moderate annihilation contribution.\n\nD) The scheme overestimates branching ratios for decays with \u03b7' and requires a decrease in the B\u2192\u03b7' form factor to match experimental data for both tree-dominated and penguin-dominated decays.\n\nCorrect Answer: C\n\nExplanation: This answer most accurately reflects the findings presented in the text. It captures the key points that:\n1) The nonet symmetry scheme works well for decays involving \u03b7.\n2) It underestimates branching ratios for decays with \u03b7'.\n3) Increasing the B\u2192\u03b7' form factor by 40-50% helps explain tree-dominated decays like B\u207b\u2192\u03c0\u207b\u03b7' and B\u207b\u2192\u03c1\u207b\u03b7'.\n4) With this increased form factor and only moderate annihilation contribution, the scheme can predict the penguin-dominated B\u207b\u2192K\u207b\u03b7' branching ratio close to the measured value.\n5) This supports predictions from PQCD and light-cone sum rules approaches for the B\u2192\u03b7' form factor.\n\nThe other options either misrepresent the findings (A and D) or omit crucial details about the success in predicting penguin-dominated decays with moderate adjustments (B)."}, "16": {"documentation": {"title": "Supersymmetric Color Superconductivity", "source": "Roni Harnik, Daniel T. Larson (UC Berkeley/LBNL) and Hitoshi Murayama\n  (IAS)", "docs_id": "hep-ph/0309224", "section": ["hep-ph", "hep-th", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supersymmetric Color Superconductivity. Recent interest in novel phases in high density QCD motivates the study of high density supersymmetric QCD (SQCD), where powerful exact results for supersymmetric gauge theories can be brought to bear in the strongly coupled regime. We begin by describing how a chemical potential can be incorporated into a supersymmetric theory as a spurion vector superfield. We then study supersymmetric SU(N_c) gauge theories with N_f flavors of quarks in the presence of a baryon chemical potential mu, and describe the global symmetry breaking patterns at low energy. Our analysis requires mu < Lambda and is thus complementary to the variational approach that has been successful for mu << Lambda. We find that for N_f < N_c a modified U(1)_B symmetry is preserved, analogous to the non-supersymmetric 2SC phase, whereas for N_f=N_c there is a critical chemical potential above which the U(1)_B is broken, as it is in the non-supersymmetric CFL phase. We further analyze the cases with N_c+1 <= N_f < 3/2 N_c and find that baryon number is broken dynamically for mu > mu_c. We also give a qualitative description of the phases in the `conformal window', 3/2 N_c < N_f < 3N_c, at finite density."}, "answer": "Here's a difficult multi-form question based on the given Arxiv documentation:\n\nQuestion: In supersymmetric SU(N_c) gauge theories with N_f flavors of quarks in the presence of a baryon chemical potential \u03bc, how does the behavior of the U(1)_B symmetry differ between the cases where N_f < N_c and N_f = N_c?\n\nA) For N_f < N_c, U(1)_B is always broken, while for N_f = N_c, it is always preserved.\n\nB) For N_f < N_c, a modified U(1)_B is preserved, while for N_f = N_c, U(1)_B is broken above a critical chemical potential.\n\nC) For both N_f < N_c and N_f = N_c, U(1)_B is always preserved regardless of the chemical potential.\n\nD) For N_f < N_c, U(1)_B is broken above a critical chemical potential, while for N_f = N_c, a modified U(1)_B is always preserved.\n\nCorrect Answer: B\n\nExplanation: According to the documentation, for N_f < N_c, a modified U(1)_B symmetry is preserved, which is analogous to the non-supersymmetric 2SC phase. However, for N_f = N_c, there exists a critical chemical potential above which the U(1)_B symmetry is broken, similar to the non-supersymmetric CFL phase. This difference in behavior between the two cases is correctly captured in option B.\n\nOption A is incorrect because it reverses the behaviors for the two cases. Option C is incorrect because it doesn't account for the symmetry breaking in the N_f = N_c case. Option D is incorrect because it swaps the behaviors described in the document for the two cases."}, "17": {"documentation": {"title": "Prompt and afterglow early X-ray phases in the comoving frame. Evidence\n  for Universal properties?", "source": "G. Chincarini, A. Moretti, P. Romano, S. Covino, G. Tagliaferri, S.\n  Campana, M. Goad, S. Kobayashi, B. Zhang, L. Angelini, P. Banat, S.\n  Barthelmy, A.P. Beardmore, P.T. Boyd, A. Breeveld, D.N. Burrows, M. Capalbi,\n  M.M. Chester, G. Cusumano, E.E. Fenimore, N. Gehrels, P. Giommi, J.E. Hill,\n  D. Hinshaw, S.T. Holland, J.A. Kennea, H.A. Krimm, V. La Parola, V. Mangano,\n  F.E. Marshall, K.O. Mason, J.A. Nousek, P.T. O'Brien, J.P. Osborne, M. Perri,\n  P. Meszaros, P.W.A. Roming, T. Sakamoto, P. Schady, M. Still, A.A. Wells", "docs_id": "astro-ph/0506453", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prompt and afterglow early X-ray phases in the comoving frame. Evidence\n  for Universal properties?. We analyze the Swift XRT light curves and spectra of the gamma-ray bursts (GRBs) for which the redshift has been measured. The sample consists of seven GRBs. The soft X-ray light curves of all these GRBs are separated into at least two morphological classes: 1) those starting off with a very steep light curve decay and 2) those showing a rather mild initial decay. This initial decay is followed by a flattening and by a further steepening. During these transitions the soft X-ray spectrum of these GRBs remains constant within the observational errors (except for one case, GRB050319). For the first time we are able to exploit the early light curve of GRB afterglows in the comoving frame. Besides the temporal and spectral behavior we find that the energy of the afterglow emitted in the (rest frame) time interval 20-200 s and 1300-12600 s after the trigger correlates with the mean energy of the prompt emission, hinting at a close link between the two. Averaging over the bursts that have a rather well sampled light curve and starting immediately after the prompt phase, the energy emitted in the soft X-rays is about 3% of the energy emitted during the prompt phase, except for GRB050315, where the soft X-ray emission is as high as 14% of the prompt emission."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: Based on the analysis of Swift XRT light curves and spectra of gamma-ray bursts (GRBs) with measured redshifts, which of the following statements is NOT supported by the findings described in the text?\n\nA) The soft X-ray light curves of all analyzed GRBs can be categorized into at least two distinct morphological classes.\n\nB) The energy emitted in soft X-rays during the afterglow phase is consistently about 3% of the energy emitted during the prompt phase for all GRBs in the sample.\n\nC) The soft X-ray spectrum of the GRBs remains constant within observational errors during the transitions between different phases of the light curve decay, with one exception.\n\nD) There is evidence of a correlation between the energy of the afterglow emitted in specific rest frame time intervals and the mean energy of the prompt emission.\n\nCorrect Answer: B\n\nExplanation: Statement B is not supported by the findings described in the text. The document states that for most of the well-sampled light curves, the energy emitted in soft X-rays is about 3% of the energy emitted during the prompt phase. However, it specifically mentions an exception: GRB050315, where the soft X-ray emission is as high as 14% of the prompt emission. This contradicts the claim that the 3% ratio is consistent for all GRBs in the sample.\n\nStatements A, C, and D are all supported by the text:\nA) The text mentions two morphological classes of soft X-ray light curves.\nC) The document states that the soft X-ray spectrum remains constant during transitions, except for one case (GRB050319).\nD) The text indicates a correlation between afterglow energy in specific time intervals and the mean energy of the prompt emission."}, "18": {"documentation": {"title": "Interactions of the solitons in periodic driven-dissipative systems\n  supporting quasi-BIC states", "source": "D. Dolinina and A. Yulin", "docs_id": "2109.04190", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interactions of the solitons in periodic driven-dissipative systems\n  supporting quasi-BIC states. The paper is devoted to the dynamics of dissipative gap solitons in the periodically corrugated optical waveguides whose spectrum of linear excitations contains a mode that can be referred to as a quasi-Bound State in the Continuum. These systems can support a large variety of stable bright and dark dissipative solitons that can interact with each other and with the inhomogeneities of the pump. One of the focus points of this work is the influence of slow variations of the pump on the behavior of the solitons. It is shown that for the fixed sets of parameters the effect of pump inhomogeneities on the solitons is not the same for the solitons of different kinds. The second main goal of the paper is systematic studies of the interaction between the solitons of the same or of different kinds. It is demonstrated that various scenarios of inter-soliton interactions can occur: the solitons can repulse each other or get attracted. In the latter case, the solitons can annihilate, fuse in a single soliton or form a new bound state depending on the kinds of the interacting solitons and on the system parameters."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: A study on dissipative gap solitons in periodically corrugated optical waveguides reveals that:\n\nA) Pump inhomogeneities always have the same effect on all types of solitons\nB) Soliton interactions invariably result in annihilation\nC) The system only supports bright dissipative solitons\nD) Soliton interactions can lead to repulsion, attraction, annihilation, fusion, or formation of new bound states\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"various scenarios of inter-soliton interactions can occur: the solitons can repulse each other or get attracted. In the latter case, the solitons can annihilate, fuse in a single soliton or form a new bound state depending on the kinds of the interacting solitons and on the system parameters.\"\n\nOption A is incorrect because the text specifically mentions that \"the effect of pump inhomogeneities on the solitons is not the same for the solitons of different kinds.\"\n\nOption B is incorrect as the documentation describes multiple possible outcomes of soliton interactions, not just annihilation.\n\nOption C is false because the text states that the system can support \"a large variety of stable bright and dark dissipative solitons,\" not just bright ones.\n\nOption D correctly summarizes the various possible outcomes of soliton interactions described in the document, making it the most comprehensive and accurate answer."}, "19": {"documentation": {"title": "Evaluation of techniques for predicting seizure Build up", "source": "Amira Hajjeji and Nawel Jmail and Abir Hadriche and Amal Ncibi and\n  Chokri Ben Amar", "docs_id": "1911.07081", "section": ["eess.SP", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation of techniques for predicting seizure Build up. The analysis of electrophysiological signal of scalp: EEG (electroencephalography), MEG (magnetoencephalography) and depth (intracerebral EEG) IEEG is a way to delimit epileptogenic zone (EZ). These epileptic signals present two different activities (oscillations and spikes) which can be overlapped in the time frequency plane. Automatic recognition of epileptic seizure occurrence needs several preprocessing steps. In this study, we evaluated two filtering techniques: the stationary wavelet transforms (SWT) and the Despikifying in order to extract pre ictal gamma oscillations (bio markers of seizure build up). Then, we used a temporal basis set of Jmail et al 2017 as a preprocessing step to evaluate the performance of both technique. Moreover, we used time-frequency and spatio-temporal mapping of simulated and real data for both techniques in order to predict seizure build up (in time and space). We concluded that SWT can detect the oscillations, but Despikyfying is more robust than SWT in reconstructing pure pre ictal gamma oscillations and hence in predicting seizure build up."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements most accurately describes the findings of the study regarding the effectiveness of Stationary Wavelet Transforms (SWT) and Despikifying techniques in predicting seizure build-up?\n\nA) SWT was found to be more effective than Despikifying in reconstructing pure pre-ictal gamma oscillations.\n\nB) Both SWT and Despikifying were equally effective in predicting seizure build-up in time and space.\n\nC) Despikifying was determined to be more robust than SWT in reconstructing pure pre-ictal gamma oscillations and predicting seizure build-up.\n\nD) SWT was superior in detecting oscillations, while Despikifying was better at identifying epileptogenic zones.\n\nCorrect Answer: C\n\nExplanation: The question tests the student's understanding of the study's conclusions regarding the two filtering techniques evaluated. The correct answer is C because the documentation explicitly states: \"We concluded that SWT can detect the oscillations, but Despikyfying is more robust than SWT in reconstructing pure pre ictal gamma oscillations and hence in predicting seizure build up.\" This directly supports the statement in option C.\n\nOption A is incorrect as it contradicts the study's findings. Option B is incorrect because the study did not conclude that both techniques were equally effective. Option D, while partially correct about SWT detecting oscillations, misrepresents the main conclusion about Despikifying's superiority in predicting seizure build-up."}, "20": {"documentation": {"title": "Weak Decay of Hypernuclei", "source": "W.M.Alberico and G.Garbarino", "docs_id": "nucl-th/0410059", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Weak Decay of Hypernuclei. The focus of these Lectures is on the weak decay modes of hypernuclei, with special attention to Lambda-hypernuclei. The subject involves many fields of modern theoretical and experimental physics, from nuclear structure to the fundamental constituents of matter and their interactions. The various weak decay modes of Lambda-hypernuclei are described: the mesonic mode and the non-mesonic ones. The latter are the dominant decay channels of medium--heavy hypernuclei, where, on the contrary, the mesonic decay is disfavoured by Pauli blocking effect on the outgoing nucleon. In particular, one can distinguish between one-body and two-body induced decays. Theoretical models employed to evaluate the (partial and total) decay widths of hypernuclei are illustrated, and their results compared with existing experimental data. Open problems and recent achievements are extensively discussed, in particular the determination of the ratio Gamma_n/Gamma_p, possible tests of the Delta I=1/2 rule in non-mesonic decays and the puzzling asymmetric non-mesonic decay of polarized hypernuclei."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements accurately describes the weak decay modes of medium to heavy Lambda-hypernuclei?\n\nA) Mesonic decay is the dominant mode due to enhanced Pauli blocking effect on outgoing nucleons.\n\nB) Non-mesonic decay channels are suppressed compared to mesonic decay in heavier hypernuclei.\n\nC) One-body and two-body induced decays are the primary non-mesonic decay channels, with mesonic decay being less favored due to Pauli blocking.\n\nD) The ratio of Gamma_n/Gamma_p is well-established and consistent with the Delta I=1/2 rule in non-mesonic decays.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the documentation, non-mesonic decay modes are dominant in medium to heavy Lambda-hypernuclei. This is because the mesonic decay is disfavored by the Pauli blocking effect on the outgoing nucleon in these larger nuclear systems. The non-mesonic decay channels can be further categorized into one-body and two-body induced decays. \n\nOption A is incorrect because it reverses the relationship between mesonic decay and Pauli blocking. Option B is also incorrect as it contradicts the dominance of non-mesonic decay in heavier hypernuclei. Option D is incorrect because the ratio Gamma_n/Gamma_p and its relationship to the Delta I=1/2 rule in non-mesonic decays are described as open problems, not well-established facts."}, "21": {"documentation": {"title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding", "source": "Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, Samy Bengio", "docs_id": "2106.02795", "section": ["cs.LG", "cs.AI", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding. Attentional mechanisms are order-invariant. Positional encoding is a crucial component to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where $L_2$ distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the advantages of the learnable Fourier features for multi-dimensional spatial positional encoding as proposed in the paper?\n\nA) It allows for faster training of Transformer models without improving accuracy\nB) It hard-codes each position as a token or vector, improving spatial representation\nC) It outperforms existing methods by improving accuracy and allowing faster convergence, particularly for multi-dimensional spatial positions\nD) It eliminates the need for positional encoding in attention-based deep model architectures\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The paper proposes a novel positional encoding method based on learnable Fourier features, which represents each position as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. This approach is particularly advantageous for spatial multi-dimensional positions, such as pixel positions on an image. The text explicitly states that their experiments show this method outperforms existing methods by both improving accuracy and allowing faster convergence.\n\nOption A is incorrect because while the method does allow for faster convergence, it also improves accuracy, not just training speed.\n\nOption B is incorrect because the method does not hard-code positions as tokens or vectors. In fact, it does the opposite by making the encoding learnable and flexible.\n\nOption D is incorrect because the paper emphasizes that positional encoding remains crucial for attention-based models like Transformers to address sequences or images where position matters. The proposed method enhances positional encoding rather than eliminating its need."}, "22": {"documentation": {"title": "Resonant states of neutron-rich $\\Lambda$ hypernucleus $^7_{\\Lambda}$He", "source": "E. Hiyama, M. Isaka, M. Kamimura, T. Myo, and T. Motoba", "docs_id": "1504.07735", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resonant states of neutron-rich $\\Lambda$ hypernucleus $^7_{\\Lambda}$He. The structure of neutron-rich $\\Lambda$ hypernucleus, $^7_{\\Lambda}$He is studied within the framework of an $\\alpha +\\Lambda +n+n$ four-body cluster model. We predict second $3/2^+$ and $5/2^+$ states, corresponding to a $0s$ $\\Lambda$ coupled to the second $2^+$ state of $^6$He, as narrow resonant states with widths $\\Gamma \\sim 1$ MeV to be at 0.03 MeV and 0.07 MeV respect to the $\\alpha +\\Lambda +n+n$ threshold. From an estimation of the differential cross section for the $^7{\\rm Li} (\\gamma,K^+) ^7_{\\Lambda}$He reaction, there is a possibility to observe these state at JLab in the future. We also calculate the second $2^+$ state of $^6$He as resonant state within the framework of an $\\alpha +n+n$ three-body cluster model. Our result is $2.81$ MeV with $\\Gamma =$4.63 MeV with respect to the $\\alpha +n+n$ threshold. This energy position is $\\sim 1$ MeV higher, and with a much broader decay width, than the recent SPIRAL data. It is suggested that an experiment at JLab to search for the second $3/2^+$ and $5/2^+$ states of $^7_{\\Lambda}$He would provide an opportunity to confirm the second $2^+$ state of the core nucleus $^6$He."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the study of the neutron-rich \u039b hypernucleus \u2077\u039bHe using an \u03b1+\u039b+n+n four-body cluster model, which of the following statements is correct regarding the predicted second 3/2\u207a and 5/2\u207a states?\n\nA) They are broad resonant states with widths \u0393 ~ 10 MeV\nB) They correspond to a 0s \u039b coupled to the first 2\u207a state of \u2076He\nC) They are predicted to be at 0.03 MeV and 0.07 MeV below the \u03b1+\u039b+n+n threshold\nD) They are narrow resonant states with widths \u0393 ~ 1 MeV, located at 0.03 MeV and 0.07 MeV above the \u03b1+\u039b+n+n threshold\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that the second 3/2\u207a and 5/2\u207a states are predicted as narrow resonant states with widths \u0393 ~ 1 MeV, to be at 0.03 MeV and 0.07 MeV with respect to the \u03b1+\u039b+n+n threshold. The use of \"respect to\" in this context implies that these energies are above the threshold.\n\nOption A is incorrect because the states are described as narrow, not broad, and the width is given as ~1 MeV, not 10 MeV.\n\nOption B is incorrect because the states correspond to a 0s \u039b coupled to the second 2\u207a state of \u2076He, not the first.\n\nOption C is incorrect because the states are above the threshold, not below it.\n\nThis question tests the student's ability to carefully read and interpret scientific information, particularly regarding energy levels and resonant states in nuclear physics."}, "23": {"documentation": {"title": "Efficient nonmyopic active search with applications in drug and\n  materials discovery", "source": "Shali Jiang, Gustavo Malkomes, Benjamin Moseley, Roman Garnett", "docs_id": "1811.08871", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient nonmyopic active search with applications in drug and\n  materials discovery. Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In this paper, we approach this problem in Bayesian decision framework. We first derive the Bayesian optimal policy under a natural utility, and establish a theoretical hardness of active search, proving that the optimal policy can not be approximated for any constant ratio. We also study the batch setting for the first time, where a batch of $b>1$ points can be queried at each iteration. We give an asymptotic lower bound, linear in batch size, on the adaptivity gap: how much we could lose if we query $b$ points at a time for $t$ iterations, instead of one point at a time for $bt$ iterations. We then introduce a novel approach to nonmyopic approximations of the optimal policy that admits efficient computation. Our proposed policy can automatically trade off exploration and exploitation, without relying on any tuning parameters. We also generalize our policy to batch setting, and propose two approaches to tackle the combinatorial search challenge. We evaluate our proposed policies on a large database of drug discovery and materials science. Results demonstrate the superior performance of our proposed policy in both sequential and batch setting; the nonmyopic behavior is also illustrated in various aspects."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of active search for scientific discovery, which of the following statements is NOT true according to the research described?\n\nA) The Bayesian optimal policy for active search cannot be approximated for any constant ratio.\n\nB) The adaptivity gap in batch setting is asymptotically lower bounded by a value linear in batch size.\n\nC) The proposed nonmyopic policy requires careful tuning of parameters to balance exploration and exploitation.\n\nD) The study introduces approaches to address combinatorial search challenges in batch settings.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation specifically states that the novel approach to nonmyopic approximations of the optimal policy \"can automatically trade off exploration and exploitation, without relying on any tuning parameters.\" This contradicts the statement in option C.\n\nOption A is true according to the documentation, which states that they \"establish a theoretical hardness of active search, proving that the optimal policy can not be approximated for any constant ratio.\"\n\nOption B is also correct, as the text mentions \"We give an asymptotic lower bound, linear in batch size, on the adaptivity gap.\"\n\nOption D is true as well, with the documentation stating \"We also generalize our policy to batch setting, and propose two approaches to tackle the combinatorial search challenge.\"\n\nThis question tests the reader's understanding of the key points in the research, particularly focusing on the characteristics of the proposed policy and the theoretical findings of the study."}, "24": {"documentation": {"title": "Spin Wave Based Approximate 4:2 Compressor", "source": "Abdulqader Mahmoud, Frederic Vanderveken, Florin Ciubotaru, Christoph\n  Adelmann, Said Hamdioui, and Sorin Cotofana", "docs_id": "2109.09554", "section": ["cond-mat.mes-hall", "physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin Wave Based Approximate 4:2 Compressor. In this paper, we propose an energy efficient SW based approximate 4:2 compressor comprising a 3-input and a 5-input Majority gate. We validate our proposal by means of micromagnetic simulations, and assess and compare its performance with one of the state-of-the-art SW, 45nm CMOS, and Spin-CMOS counterparts. The evaluation results indicate that the proposed compressor consumes 31.5\\% less energy in comparison with its accurate SW design version. Furthermore, it has the same energy consumption and error rate as the approximate compressor with Directional Coupler (DC), but it exhibits 3x lower delay. In addition, it consumes 14% less energy, while having 17% lower average error rate than the approximate 45nm CMOS counterpart. When compared with the other emerging technologies, the proposed compressor outperforms approximate Spin-CMOS based compressor by 3 orders of magnitude in term of energy consumption while providing the same error rate. Finally, the proposed compressor requires the smallest chip real-estate measured in terms of devices."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements accurately reflects the performance advantages of the proposed Spin Wave (SW) based approximate 4:2 compressor as described in the paper?\n\nA) It consumes 31.5% less energy than its accurate SW design version and has a 3x lower delay compared to the approximate compressor with Directional Coupler.\n\nB) It outperforms the approximate Spin-CMOS based compressor by 3 orders of magnitude in energy consumption while having a 17% lower average error rate.\n\nC) It exhibits the same energy consumption and error rate as the approximate compressor with Directional Coupler, while consuming 14% less energy than the 45nm CMOS counterpart.\n\nD) It requires the smallest chip real-estate and consumes 31.5% less energy than the 45nm CMOS counterpart, while having a 3x lower delay than the Spin-CMOS version.\n\nCorrect Answer: A\n\nExplanation: The correct answer is A because it accurately combines two key advantages of the proposed compressor mentioned in the text. The paper states that the compressor \"consumes 31.5% less energy in comparison with its accurate SW design version\" and \"it has the same energy consumption and error rate as the approximate compressor with Directional Coupler (DC), but it exhibits 3x lower delay.\"\n\nOption B is incorrect because while it does outperform the Spin-CMOS compressor by 3 orders of magnitude in energy consumption, the 17% lower average error rate is compared to the 45nm CMOS counterpart, not the Spin-CMOS version.\n\nOption C mixes correct and incorrect information. While it correctly states the comparison with the DC compressor, it incorrectly associates the 14% less energy consumption with the 45nm CMOS counterpart.\n\nOption D combines information from different comparisons incorrectly and introduces an unsupported claim about the delay compared to the Spin-CMOS version."}, "25": {"documentation": {"title": "Thin limit of the 6D Cascading DGP model", "source": "Fulvio Sbis\\`a", "docs_id": "1710.00437", "section": ["hep-th", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thin limit of the 6D Cascading DGP model. A thin limit description of the 6D Cascading DGP model is derived, starting from a configuration where both the codimension-1 and the codimension-2 branes are thick. Postulating that the thicknesses of the two branes obey a hierarchic relation, the thin limit is executed in two steps. First the thin limit of the codimension-1 brane is executed, obtaining a system where a \"ribbon\" codimension-2 brane is embedded inside a thin codimension-1 brane with induced gravity, and then the thin limit of the ribbon brane is considered. By proposing a geometric ansatz on the limit configuration, the junction conditions which are to hold at the thin codimension-2 brane are derived. The latters are fully non-perturbative and covariant and, together with the Israel junction conditions at the codimension-1 brane and the Einstein equations in the bulk, constitute the looked-for thin limit formulation of the 6D Cascading DGP model. It is commented on how wide is the class of thin source configurations which can be placed on the thin codimension-2 brane."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the thin limit description of the 6D Cascading DGP model, what is the correct sequence of steps and resulting configuration?\n\nA) First, execute the thin limit of the codimension-2 brane, then the codimension-1 brane, resulting in a thin codimension-2 brane embedded in a thick codimension-1 brane.\n\nB) Execute the thin limits of both branes simultaneously, resulting in two intersecting thin branes of different codimensions.\n\nC) First, execute the thin limit of the codimension-1 brane, then the codimension-2 brane, resulting in a thin codimension-2 brane embedded in a thin codimension-1 brane with induced gravity.\n\nD) Execute the thin limit of the codimension-1 brane only, leaving a \"ribbon\" codimension-2 brane embedded inside a thin codimension-1 brane with induced gravity.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation describes a two-step process for executing the thin limit. First, the thin limit of the codimension-1 brane is executed, resulting in a \"ribbon\" codimension-2 brane embedded inside a thin codimension-1 brane with induced gravity. Then, the thin limit of the ribbon (codimension-2) brane is considered. This process results in a thin codimension-2 brane embedded in a thin codimension-1 brane with induced gravity.\n\nOption A is incorrect because it reverses the order of the thin limit execution and incorrectly states that the codimension-1 brane remains thick.\n\nOption B is incorrect because the thin limits are not executed simultaneously, but in a specific sequence.\n\nOption D is partially correct but incomplete, as it only describes the first step of the process and doesn't include the final thin limit of the codimension-2 brane."}, "26": {"documentation": {"title": "Adversarial Example Detection and Classification With Asymmetrical\n  Adversarial Training", "source": "Xuwang Yin, Soheil Kolouri, Gustavo K. Rohde", "docs_id": "1905.11475", "section": ["cs.LG", "cs.CR", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adversarial Example Detection and Classification With Asymmetrical\n  Adversarial Training. The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we first present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using asymmetrical adversarial training (AAT). The novel AAT objective presents a minimax problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We first demonstrate that the minimax problem could be reasonably solved by PGD attack, and then use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the key innovation and purpose of the Asymmetrical Adversarial Training (AAT) method presented in the paper?\n\nA) It focuses solely on improving the accuracy of deep neural networks for normal inputs.\nB) It creates a detection method that is effective only against known adversarial attacks.\nC) It presents a minimax problem similar to GANs, allowing for the learning of class conditional distributions and providing performance guarantees against norm-constrained adversaries.\nD) It eliminates the need for adversarial example detection by making neural networks completely impervious to all types of attacks.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The paper introduces Asymmetrical Adversarial Training (AAT) as a novel method that presents a minimax problem similar to Generative Adversarial Networks (GANs). This approach allows for learning class conditional distributions and provides performance guarantees against norm-constrained adversaries.\n\nOption A is incorrect because the method is not focused solely on improving accuracy for normal inputs, but rather on detecting and classifying adversarial examples.\n\nOption B is incorrect because the method is not limited to known attacks. It provides performance guarantees against norm-constrained adversaries in general.\n\nOption D is incorrect because the paper does not claim to make networks completely impervious to all attacks. Instead, it focuses on detection and classification of adversarial examples with certain guarantees.\n\nThe key innovation lies in the AAT objective, which supports the learning of class conditional distributions and allows for the development of more robust and interpretable generative detection/classification models."}, "27": {"documentation": {"title": "Prosecutor Politics: The Impact of Election Cycles on Criminal\n  Sentencing in the Era of Rising Incarceration", "source": "Chika O. Okafor", "docs_id": "2110.09169", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prosecutor Politics: The Impact of Election Cycles on Criminal\n  Sentencing in the Era of Rising Incarceration. I investigate how political incentives affect the behavior of district attorneys (DAs). I develop a theoretical model that predicts DAs will increase sentencing intensity in an election period compared to the period prior. To empirically test this prediction, I compile one of the most comprehensive datasets to date on the political careers of all district attorneys in office during the steepest rise in incarceration in U.S. history (roughly 1986-2006). Using quasi-experimental methods, I find causal evidence that being in a DA election year increases total admissions per capita and total months sentenced per capita. I estimate that the election year effects on admissions are akin to moving 0.85 standard deviations along the distribution of DA behavior within state (e.g., going from the 50th to 80th percentile in sentencing intensity). I find evidence that election effects are larger (1) when DA elections are contested, (2) in Republican counties, and (3) in the southern United States--all these factors are consistent with the perspective that election effects arise from political incentives influencing DAs. Further, I find that district attorney election effects decline over the period 1986-2006, in tandem with U.S. public opinion softening regarding criminal punishment. These findings suggest DA behavior may respond to voter preferences--in particular to public sentiment regarding the harshness of the court system."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: According to the study, which of the following combinations of factors is most likely to result in the largest election year effects on criminal sentencing by district attorneys?\n\nA) Uncontested elections in Democratic counties in the northeastern United States\nB) Contested elections in Republican counties in the southern United States\nC) Contested elections in Democratic counties in the western United States\nD) Uncontested elections in Republican counties in the midwestern United States\n\nCorrect Answer: B\n\nExplanation: The study finds that election year effects on criminal sentencing are larger under specific conditions. These include:\n1) When DA elections are contested\n2) In Republican counties\n3) In the southern United States\n\nOption B combines all these factors, making it the most likely scenario to result in the largest election year effects on criminal sentencing by district attorneys. The other options either lack one or more of these factors or include elements not mentioned as contributing to larger effects in the study."}, "28": {"documentation": {"title": "Universal Statistical Properties of Inertial-particle Trajectories in\n  Three-dimensional, Homogeneous, Isotropic, Fluid Turbulence", "source": "Akshay Bhatnagar, Anupam Gupta, Dhrubaditya Mitra, Prasad Perlekar,\n  and Rahul Pandit", "docs_id": "1412.2686", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Universal Statistical Properties of Inertial-particle Trajectories in\n  Three-dimensional, Homogeneous, Isotropic, Fluid Turbulence. We uncover universal statistical properties of the trajectories of heavy inertial particles in three-dimensional, statistically steady, homogeneous, and isotropic turbulent flows by extensive direct numerical simulations. We show that the probability distribution functions (PDFs) $P(\\phi)$, of the angle $\\phi$ between the Eulerian velocity ${\\bf u}$ and the particle velocity ${\\bf v}$, at this point and time, shows a power-law region in which $P(\\phi) \\sim \\phi^{-\\gamma}$, with a new universal exponent $\\gamma \\simeq 4$. Furthermore, the PDFs of the trajectory curvature $\\kappa$ and modulus $\\theta$ of the torsion $\\vartheta$ have power-law tails that scale, respectively, as $P(\\kappa) \\sim \\kappa^{-h_\\kappa}$, as $\\kappa \\to \\infty$, and $P(\\theta) \\sim \\theta^{-h_\\theta}$, as $\\theta \\to \\infty$, with exponents $h_\\kappa \\simeq 2.5$ and $h_\\theta \\simeq 3$ that are universal to the extent that they do not depend on the Stokes number ${\\rm St}$ (given our error bars). We also show that $\\gamma$, $h_\\kappa$ and $h_\\theta$ can be obtained by using simple stochastic models. We characterize the complexity of heavy-particle trajectories by the number $N_{\\rm I}(t,{\\rm St})$ of points (up until time $t$) at which $\\vartheta$ changes sign. We show that $n_{\\rm I}({\\rm St}) \\equiv \\lim_{t\\to\\infty} \\frac{N_{\\rm I}(t,{\\rm St})}{t} \\sim {\\rm St}^{-\\Delta}$, with $\\Delta \\simeq 0.4$ a universal exponent."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: In a study of heavy inertial particles in three-dimensional, homogeneous, isotropic fluid turbulence, several universal statistical properties were observed. Which of the following statements correctly describes one of these properties and its corresponding mathematical representation?\n\nA) The probability distribution function (PDF) of the angle between Eulerian velocity and particle velocity follows a power-law with exponent \u03b3 \u2248 2, represented as P(\u03c6) ~ \u03c6^(-2).\n\nB) The PDF of trajectory curvature \u03ba has a power-law tail that scales as P(\u03ba) ~ \u03ba^(-2.5) as \u03ba \u2192 \u221e, with the exponent being dependent on the Stokes number.\n\nC) The PDF of the modulus of torsion \u03b8 has a power-law tail that scales as P(\u03b8) ~ \u03b8^(-3) as \u03b8 \u2192 \u221e, with the exponent being independent of the Stokes number.\n\nD) The number of points at which torsion changes sign, normalized by time, scales with Stokes number as n_I(St) ~ St^(-0.6) in the long-time limit.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. According to the given information, the PDF of the modulus \u03b8 of the torsion \u03d1 has a power-law tail that scales as P(\u03b8) ~ \u03b8^(-h_\u03b8) as \u03b8 \u2192 \u221e, with h_\u03b8 \u2248 3. This exponent is stated to be universal to the extent that it does not depend on the Stokes number, within the error bars of the study.\n\nOption A is incorrect because the exponent for the angle PDF is given as \u03b3 \u2248 4, not 2.\n\nOption B is incorrect because while the scaling P(\u03ba) ~ \u03ba^(-2.5) is correct, the exponent is stated to be independent of the Stokes number, not dependent on it.\n\nOption D is incorrect because the exponent in the scaling relation for n_I(St) is given as \u0394 \u2248 0.4, not 0.6."}, "29": {"documentation": {"title": "Information spreading during emergencies and anomalous events", "source": "James P. Bagrow", "docs_id": "1703.07362", "section": ["cs.SI", "cs.CY", "physics.data-an", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information spreading during emergencies and anomalous events. The most critical time for information to spread is in the aftermath of a serious emergency, crisis, or disaster. Individuals affected by such situations can now turn to an array of communication channels, from mobile phone calls and text messages to social media posts, when alerting social ties. These channels drastically improve the speed of information in a time-sensitive event, and provide extant records of human dynamics during and afterward the event. Retrospective analysis of such anomalous events provides researchers with a class of \"found experiments\" that may be used to better understand social spreading. In this chapter, we study information spreading due to a number of emergency events, including the Boston Marathon Bombing and a plane crash at a western European airport. We also contrast the different information which may be gleaned by social media data compared with mobile phone data and we estimate the rate of anomalous events in a mobile phone dataset using a proposed anomaly detection method."}, "answer": "Here's a difficult multi-form question based on the provided text:\n\nQuestion: Which of the following statements best describes the relationship between communication channels and information spreading during emergencies, as discussed in the given text?\n\nA) Mobile phone calls and text messages are the only reliable means of spreading information during emergencies.\n\nB) Social media posts are more effective than traditional communication methods in alerting social ties during a crisis.\n\nC) The array of modern communication channels has significantly decreased the speed of information spread during time-sensitive events.\n\nD) Multiple communication channels, including mobile phones and social media, have improved the speed and documentation of information spreading during emergencies.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The text explicitly states that \"Individuals affected by such situations can now turn to an array of communication channels, from mobile phone calls and text messages to social media posts, when alerting social ties. These channels drastically improve the speed of information in a time-sensitive event, and provide extant records of human dynamics during and afterward the event.\" This indicates that multiple channels, including both mobile phones and social media, contribute to faster information spread and better documentation during emergencies.\n\nOption A is incorrect because it limits the channels to only mobile phone calls and text messages, whereas the text mentions multiple channels including social media.\n\nOption B is not supported by the text, as it doesn't explicitly state that social media is more effective than traditional methods.\n\nOption C is incorrect because it contradicts the text, which states that these channels \"drastically improve the speed of information\" rather than decreasing it."}, "30": {"documentation": {"title": "Electronic states of Mn4+ ions in p-type GaN", "source": "B. Han, B. W. Wessels, and M. P. Ulmer", "docs_id": "cond-mat/0406228", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electronic states of Mn4+ ions in p-type GaN. The electronic states of manganese in p-type GaN are investigated using photoluminescence (PL) and photoluminescence excitation (PLE) spectroscopies. A series of sharp PL lines at 1.0 eV is observed in codoped GaN and attributed to the intra d-shell transition 4T2(F)-4T1(F) of Mn4+ ions. PLE spectrum of the Mn4+ [4T2(F)-4T1(F)] luminescence reveals intra-center excitation processes via the excited states of Mn4+ ions. PLE peaks observed at 1.79 and 2.33 eV are attributed to the intra d-shell 4T1(P)-4T1(F) and 4A2(F)-4T1(F) transitions of Mn4+, respectively. In addition to the intra-shell excitation processes, a broad PLE band involving charge-transfer transition of the Mn4+/3+ deep level is observed, which is well described by the Lucovsky model. As determined from the onset of this PLE band, the position of the Mn4+/3+ deep level is 1.11 eV above the valence band maximum, which is consistent with prior theory using ab initio calculations. Our work indicates 4+ is the predominant oxidation state of Mn ions in p-type GaN:Mn when the Fermi energy is lower than 1.11 eV above the valence band maximum."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Based on the photoluminescence excitation (PLE) spectroscopy results for Mn4+ ions in p-type GaN, which of the following statements is correct?\n\nA) The PLE peak at 2.33 eV corresponds to the 4T1(P)-4T1(F) transition of Mn4+\nB) The Mn4+/3+ deep level is positioned 1.11 eV below the conduction band minimum\nC) The broad PLE band involving charge-transfer transition of the Mn4+/3+ deep level is best described by the Franck-Condon principle\nD) The onset of the charge-transfer PLE band indicates that the Mn4+/3+ deep level is 1.11 eV above the valence band maximum\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"As determined from the onset of this PLE band, the position of the Mn4+/3+ deep level is 1.11 eV above the valence band maximum.\" This directly corresponds to option D.\n\nOption A is incorrect because the PLE peak at 2.33 eV is attributed to the 4A2(F)-4T1(F) transition, not the 4T1(P)-4T1(F) transition (which is at 1.79 eV).\n\nOption B is incorrect because the deep level is described as being above the valence band maximum, not below the conduction band minimum.\n\nOption C is incorrect because the broad PLE band is described as being well explained by the Lucovsky model, not the Franck-Condon principle.\n\nThis question tests the student's ability to carefully read and interpret spectroscopic data and results, as well as their understanding of electronic transitions and energy levels in doped semiconductors."}, "31": {"documentation": {"title": "Special Lagrangian submanifolds with isolated conical singularities. I.\n  Regularity", "source": "Dominic Joyce", "docs_id": "math/0211294", "section": ["math.DG", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Special Lagrangian submanifolds with isolated conical singularities. I.\n  Regularity. This is the first in a series of five papers math.DG/0211295, math.DG/0302355, math.DG/0302356, math.DG/0303272 studying special Lagrangian submanifolds (SL m-folds) X in (almost) Calabi-Yau m-folds M with singularities x_1,...,x_n locally modelled on special Lagrangian cones C_1,...,C_n in C^m with isolated singularities at 0. Readers are advised to begin with the final paper math.DG/0303272, which surveys the series, gives examples, and applies the results to prove some conjectures. This first paper lays the foundations for the series, giving definitions and proving auxiliary results in symplectic geometry and asymptotic analysis that will be needed later. We also prove results on the regularity of X near its singular points. We show that X converges to the cone C_i near x_i with all its derivatives, at rates determined by the eigenvalues of the Laplacian on the intersection of C_i with the unit sphere. We show that if X is a special Lagrangian integral current with a tangent cone C at x satisfying some conditions, then X has an isolated conical singularity at x in our sense. We also prove analogues of many of our results for Asymptotically Conical SL m-folds in C^m. The sequel math.DG/0211295 studies the deformation theory of compact SL m-folds X in M with conical singularities. The third and fourth papers math.DG/0302355, math.DG/0302356 construct desingularizations of X, realizing X as a limit of a family N^t of compact, nonsingular SL m-folds in M."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Consider a special Lagrangian submanifold (SL m-fold) X in an (almost) Calabi-Yau m-fold M with isolated conical singularities. Which of the following statements is true regarding the regularity of X near its singular points?\n\nA) X converges to the cone C_i near x_i with only its first-order derivatives.\n\nB) The rate of convergence of X to C_i is independent of the eigenvalues of the Laplacian on the intersection of C_i with the unit sphere.\n\nC) X converges to the cone C_i near x_i with all its derivatives, at rates determined by the eigenvalues of the Laplacian on the intersection of C_i with the unit sphere.\n\nD) The regularity of X near its singular points is not addressed in the paper.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The documentation explicitly states: \"We show that X converges to the cone C_i near x_i with all its derivatives, at rates determined by the eigenvalues of the Laplacian on the intersection of C_i with the unit sphere.\" \n\nOption A is incorrect because it mentions only first-order derivatives, while the paper states that X converges with all its derivatives. \n\nOption B is incorrect because it contradicts the statement that the rates of convergence are determined by the eigenvalues of the Laplacian on the intersection of C_i with the unit sphere. \n\nOption D is incorrect because the paper does address the regularity of X near its singular points, as evidenced by the quoted statement.\n\nThis question tests the understanding of the key results regarding the regularity of special Lagrangian submanifolds near their singular points, as presented in the first paper of the series."}, "32": {"documentation": {"title": "AHA! an 'Artificial Hippocampal Algorithm' for Episodic Machine Learning", "source": "Gideon Kowadlo, Abdelrahman Ahmed, and David Rawlinson", "docs_id": "1909.10340", "section": ["cs.NE", "cs.LG", "q-bio.NC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AHA! an 'Artificial Hippocampal Algorithm' for Episodic Machine Learning. The majority of ML research concerns slow, statistical learning of i.i.d. samples from large, labelled datasets. Animals do not learn this way. An enviable characteristic of animal learning is `episodic' learning - the ability to memorise a specific experience as a composition of existing concepts, after just one experience, without provided labels. The new knowledge can then be used to distinguish between similar experiences, to generalise between classes, and to selectively consolidate to long-term memory. The Hippocampus is known to be vital to these abilities. AHA is a biologically-plausible computational model of the Hippocampus. Unlike most machine learning models, AHA is trained without external labels and uses only local credit assignment. We demonstrate AHA in a superset of the Omniglot one-shot classification benchmark. The extended benchmark covers a wider range of known hippocampal functions by testing pattern separation, completion, and recall of original input. These functions are all performed within a single configuration of the computational model. Despite these constraints, image classification results are comparable to conventional deep convolutional ANNs."}, "answer": "Here's a multi-form question based on the given information:\n\nQuestion: The Artificial Hippocampal Algorithm (AHA) is designed to mimic certain learning capabilities of animals. Which of the following combinations best describes the key features and abilities of AHA?\n\nA) Statistical learning, label-dependent, slow accumulation of knowledge, pattern completion\nB) One-shot learning, label-free, local credit assignment, pattern separation\nC) Supervised learning, external labeling, global credit assignment, image classification\nD) Reinforcement learning, reward-based, iterative training, pattern recognition\n\nCorrect Answer: B\n\nExplanation: \nThe correct answer is B because it accurately represents the key features of AHA as described in the documentation. Let's break down why B is correct and the others are not:\n\nB) One-shot learning: AHA can memorize a specific experience after just one exposure, which is a hallmark of episodic learning.\nLabel-free: The model is trained without external labels, unlike conventional machine learning approaches.\nLocal credit assignment: AHA uses only local credit assignment, which is more biologically plausible.\nPattern separation: This is one of the hippocampal functions that AHA is designed to perform.\n\nA is incorrect because it mentions statistical learning and slow accumulation of knowledge, which are characteristics of conventional machine learning, not AHA.\n\nC is incorrect because it mentions supervised learning and external labeling, which are explicitly stated as not being part of AHA's approach.\n\nD is incorrect because it describes reinforcement learning, which is not mentioned in the context of AHA. Additionally, iterative training is not a feature of AHA's one-shot learning capability.\n\nThe correct answer encompasses the unique aspects of AHA that set it apart from traditional machine learning approaches and align it more closely with animal learning processes."}, "33": {"documentation": {"title": "Ensemble analysis of open cluster transit surveys: upper limits on the\n  frequency of short-period planets consistent with the field", "source": "Jennifer L. van Saders and B. Scott Gaudi", "docs_id": "1009.3013", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ensemble analysis of open cluster transit surveys: upper limits on the\n  frequency of short-period planets consistent with the field. Several photometric surveys for short-period transiting giant planets have targeted a number of open clusters, but no convincing detections have been made. Although each individual survey typically targeted an insufficient number of stars to expect a detection assuming the frequency of short-period giant planets found in surveys of field stars, we ask whether the lack of detections from the ensemble of open cluster surveys is inconsistent with expectations from the field planet population. We select a subset of existing transit surveys with well-defined selection criteria and quantified detection efficiencies, and statistically combine their null results to show that the upper limit on the planet fraction is 5.5% and 1.4% for 1.0 $R_{J}$ and 1.5 $R_{J}$ planets, respectively in the $3<P<5$ day period range. For the period range of $1<P<3$ days we find upper limits of 1.4% and 0.31% for 1.0 $R_{J}$ and 1.5 $R_{J}$, respectively. Comparing these results to the frequency of short-period giant planets around field stars in both radial velocity and transit surveys, we conclude that there is no evidence to suggest that open clusters support a fundamentally different planet population than field stars given the available data."}, "answer": "Here's a difficult multi-form question based on the provided information:\n\nQuestion: Based on the ensemble analysis of open cluster transit surveys, what can be concluded about the frequency of short-period giant planets in open clusters compared to field stars?\n\nA) Open clusters have a significantly higher frequency of short-period giant planets than field stars.\nB) Open clusters have a significantly lower frequency of short-period giant planets than field stars.\nC) The upper limits derived from open cluster surveys are inconsistent with the frequency of short-period giant planets found in field star surveys.\nD) The lack of detections in open cluster surveys is consistent with expectations based on the frequency of short-period giant planets around field stars.\n\nCorrect Answer: D\n\nExplanation: The question tests understanding of the main conclusion from the ensemble analysis. The correct answer is D because the passage states: \"we conclude that there is no evidence to suggest that open clusters support a fundamentally different planet population than field stars given the available data.\" This means that the lack of detections in open cluster surveys is consistent with what we would expect based on the frequency of such planets around field stars.\n\nOption A is incorrect because the passage does not suggest a higher frequency in open clusters. Option B is also incorrect for the same reason - there's no evidence of a lower frequency. Option C is incorrect because the upper limits derived are actually consistent with, not inconsistent with, the field star planet population.\n\nThis question requires careful reading and interpretation of the study's conclusions, making it suitable for a difficult exam question."}, "34": {"documentation": {"title": "Multi-Epoch Multiwavelength Spectra and Models for Blazar 3C~279", "source": "R. C. Hartman, M. Boettcher, et al", "docs_id": "astro-ph/0102127", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Epoch Multiwavelength Spectra and Models for Blazar 3C~279. Of the blazars detected by EGRET in GeV gamma rays, 3C 279 is not only the best-observed by EGRET, but also one of the best-monitored at lower frequencies. We have assembled eleven spectra, from GHz radio through GeV gamma rays, from the time intervals of EGRET observations. Although some of the data have appeared in previous publications, most are new, including data taken during the high states in early 1999 and early 2000. All of the spectra show substantial gamma-ray contribution to the total luminosity of the object; in a high state, the gamma-ray luminosity dominates over that at all other frequencies by a factor of more than 10. There is no clear pattern of time correlation; different bands do not always rise and fall together, even in the optical, X-ray, and gamma-ray bands. The spectra are modeled using a leptonic jet, with combined synchrotron self-Compton + external Compton gamma-ray production. Spectral variability of 3C 279 is consistent with variations of the bulk Lorentz factor of the jet, accompanied by changes in the spectral shape of the electron distribution. Our modeling results are consistent with the UV spectrum of 3C 279 being dominated by accretion disk radiation during times of low gamma-ray intensity."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Based on the spectral analysis and modeling of blazar 3C 279, which combination of factors best explains its observed spectral variability?\n\nA) Changes in accretion disk radiation and variations in the jet's magnetic field strength\nB) Fluctuations in the external photon field and alterations in the jet's viewing angle\nC) Variations in the bulk Lorentz factor of the jet and changes in the electron distribution's spectral shape\nD) Modifications in the synchrotron self-Compton process and variations in the gamma-ray production efficiency\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the documentation explicitly states that \"Spectral variability of 3C 279 is consistent with variations of the bulk Lorentz factor of the jet, accompanied by changes in the spectral shape of the electron distribution.\" This combination of factors best explains the observed spectral variability according to the modeling results presented in the study.\n\nOption A is incorrect because while accretion disk radiation is mentioned as dominating the UV spectrum during low gamma-ray intensity periods, it is not cited as a primary factor in spectral variability. Changes in magnetic field strength are not mentioned in the given text.\n\nOption B is plausible but incorrect. Although external Compton processes are mentioned in the gamma-ray production model, fluctuations in the external photon field are not specifically identified as a key factor in spectral variability. Changes in viewing angle are not discussed in the provided information.\n\nOption D contains elements related to the described model (synchrotron self-Compton and gamma-ray production) but does not accurately represent the factors explicitly stated as responsible for spectral variability in the documentation."}, "35": {"documentation": {"title": "Smile from the Past: A general option pricing framework with multiple\n  volatility and leverage components", "source": "Adam Aleksander Majewski, Giacomo Bormetti, Fulvio Corsi", "docs_id": "1404.3555", "section": ["q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Smile from the Past: A general option pricing framework with multiple\n  volatility and leverage components. In the current literature, the analytical tractability of discrete time option pricing models is guaranteed only for rather specific types of models and pricing kernels. We propose a very general and fully analytical option pricing framework, encompassing a wide class of discrete time models featuring multiple-component structure in both volatility and leverage, and a flexible pricing kernel with multiple risk premia. Although the proposed framework is general enough to include either GARCH-type volatility, Realized Volatility or a combination of the two, in this paper we focus on realized volatility option pricing models by extending the Heterogeneous Autoregressive Gamma (HARG) model of Corsi, Fusari, La Vecchia (2012) to incorporate heterogeneous leverage structures with multiple components, while preserving closed-form solutions for option prices. Applying our analytically tractable asymmetric HARG model to a large sample of S&P 500 index options, we demonstrate its superior ability to price out-of-the-money options compared to existing benchmarks."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: The Heterogeneous Autoregressive Gamma (HARG) model for option pricing is extended in this framework to include:\n\nA) Only GARCH-type volatility components\nB) Only Realized Volatility components\nC) Heterogeneous leverage structures with multiple components, while preserving closed-form solutions for option prices\nD) A combination of GARCH-type and Realized Volatility components, but loses analytical tractability\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the key innovation described in the document. The correct answer is C because the passage states: \"We focus on realized volatility option pricing models by extending the Heterogeneous Autoregressive Gamma (HARG) model of Corsi, Fusari, La Vecchia (2012) to incorporate heterogeneous leverage structures with multiple components, while preserving closed-form solutions for option prices.\"\n\nOption A is incorrect because the framework is not limited to GARCH-type volatility. \nOption B is partially correct but doesn't capture the key extension of heterogeneous leverage structures.\nOption D is incorrect because while the framework can potentially include both GARCH-type and Realized Volatility, the focus in this paper is on Realized Volatility models, and importantly, it maintains analytical tractability.\n\nThis question requires careful reading and understanding of the specific contributions of the paper within the broader context of option pricing models."}, "36": {"documentation": {"title": "Evaluation, Modeling and Optimization of Coverage Enhancement Methods of\n  NB-IoT", "source": "Sahithya Ravi, Pouria Zand, Mohieddine El Soussi, and Majid Nabi", "docs_id": "1902.09455", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluation, Modeling and Optimization of Coverage Enhancement Methods of\n  NB-IoT. Narrowband Internet of Things (NB-IoT) is a new Low Power Wide Area Network (LPWAN) technology released by 3GPP. The primary goals of NB-IoT are improved coverage, massive capacity, low cost, and long battery life. In order to improve coverage, NB-IoT has promising solutions, such as increasing transmission repetitions, decreasing bandwidth, and adapting the Modulation and Coding Scheme (MCS). In this paper, we present an implementation of coverage enhancement features of NB-IoT in NS-3, an end-to-end network simulator. The resource allocation and link adaptation in NS-3 are modified to comply with the new features of NB-IoT. Using the developed simulation framework, the influence of the new features on network reliability and latency is evaluated. Furthermore, an optimal hybrid link adaptation strategy based on all three features is proposed. To achieve this, we formulate an optimization problem that has an objective function based on latency, and constraint based on the Signal to Noise Ratio (SNR). Then, we propose several algorithms to minimize latency and compare them with respect to accuracy and speed. The best hybrid solution is chosen and implemented in the NS-3 simulator by which the latency formulation is verified. The numerical results show that the proposed optimization algorithm for hybrid link adaptation is eight times faster than the exhaustive search approach and yields similar latency."}, "answer": "Here's a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of NB-IoT coverage enhancement, which combination of strategies would likely result in the most optimal balance between improved coverage and minimized latency?\n\nA) Increasing transmission repetitions and widening bandwidth\nB) Decreasing bandwidth and using a more robust Modulation and Coding Scheme (MCS)\nC) Increasing transmission repetitions, decreasing bandwidth, and adapting the MCS based on SNR constraints\nD) Only adapting the MCS without modifying bandwidth or transmission repetitions\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it combines all three coverage enhancement methods mentioned in the document: increasing transmission repetitions, decreasing bandwidth, and adapting the Modulation and Coding Scheme (MCS). The paper describes an optimal hybrid link adaptation strategy that takes into account all these features. This approach aims to minimize latency while considering Signal to Noise Ratio (SNR) constraints, which is crucial for balancing coverage improvement and performance.\n\nOption A is incorrect because widening bandwidth is not mentioned as a coverage enhancement method and would likely decrease coverage.\n\nOption B, while partially correct, doesn't include the important aspect of increasing transmission repetitions, which is a key feature for improving coverage in NB-IoT.\n\nOption D is inadequate as it only uses one of the three mentioned enhancement methods, which would likely result in suboptimal coverage improvement and latency reduction.\n\nThe document emphasizes that the best hybrid solution, which incorporates all three methods, was implemented in the NS-3 simulator and verified to minimize latency effectively."}, "37": {"documentation": {"title": "How to Learn from Others: Transfer Machine Learning with Additive\n  Regression Models to Improve Sales Forecasting", "source": "Robin Hirt, Niklas K\\\"uhl, Yusuf Peker, Gerhard Satzger", "docs_id": "2005.10698", "section": ["cs.LG", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How to Learn from Others: Transfer Machine Learning with Additive\n  Regression Models to Improve Sales Forecasting. In a variety of business situations, the introduction or improvement of machine learning approaches is impaired as these cannot draw on existing analytical models. However, in many cases similar problems may have already been solved elsewhere-but the accumulated analytical knowledge cannot be tapped to solve a new problem, e.g., because of privacy barriers. For the particular purpose of sales forecasting for similar entities, we propose a transfer machine learning approach based on additive regression models that lets new entities benefit from models of existing entities. We evaluate the approach on a rich, multi-year dataset of multiple restaurant branches. We differentiate the options to simply transfer models from one branch to another (\"zero shot\") or to transfer and adapt them. We analyze feasibility and performance against several forecasting benchmarks. The results show the potential of the approach to exploit the collectively available analytical knowledge. Thus, we contribute an approach that is generalizable beyond sales forecasting and the specific use case in particular. In addition, we demonstrate its feasibility for a typical use case as well as the potential for improving forecasting quality. These results should inform academia, as they help to leverage knowledge across various entities, and have immediate practical application in industry."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of transfer machine learning for sales forecasting, what is the primary advantage of the \"zero shot\" approach compared to the \"transfer and adapt\" method?\n\nA) It allows for more accurate predictions by fine-tuning the model to the new entity's specific characteristics\nB) It requires less computational resources and can be implemented instantaneously\nC) It combines the strengths of multiple existing models to create a more robust forecast\nD) It enables the creation of entirely new models without relying on pre-existing data\n\nCorrect Answer: B\n\nExplanation: The \"zero shot\" approach refers to simply transferring models from one branch to another without modification. This method's primary advantage is that it requires minimal computational resources and can be implemented immediately, as it doesn't involve any adaptation or fine-tuning process. While it may not provide the most accurate results for the new entity, it offers a quick and resource-efficient solution, especially when time or data for the new entity are limited.\n\nOption A is incorrect because it describes the \"transfer and adapt\" method, not the \"zero shot\" approach. Option C is not mentioned in the given context and might be conflating different machine learning concepts. Option D is the opposite of what transfer learning aims to achieve, which is leveraging existing models rather than creating entirely new ones."}, "38": {"documentation": {"title": "Genealogies in Expanding Populations", "source": "Rick Durrett and Wai-Tong Louis Fan", "docs_id": "1507.00918", "section": ["math.PR", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Genealogies in Expanding Populations. The goal of this paper is to prove rigorous results for the behavior of genealogies in a one-dimensional long range biased voter model introduced by Hallatschek and Nelson [25]. The first step, which is easily accomplished using results of Mueller and Tribe [38], is to show that when space and time are rescaled correctly, our biased voter model converges to a Wright-Fisher SPDE. A simple extension of a result of Durrett and Restrepo [18] then shows that the dual branching coalescing random walk converges to a branching Brownian motion in which particles coalesce after an exponentially distributed amount of intersection local time. Brunet et al. [8] have conjectured that genealogies in models of this type are described by the Bolthausen-Sznitman coalescent, see [39]. However, in the model we study there are no simultaneous coalescences. Our third and most significant result concerns \"tracer dynamics\" in which some of the initial particles in the biased voter model are labeled. We show that the joint distribution of the labeled and unlabeled particles converges to the solution of a system of stochastic partial differential equations. A new duality equation that generalizes the one Shiga [44] developed for the Wright-Fisher SPDE is the key to the proof of that result."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: In the context of the biased voter model described in the paper, which of the following statements is correct regarding the convergence of the model and its implications for genealogies?\n\nA) The biased voter model converges to a Wright-Fisher SPDE under appropriate scaling, and its dual process converges to a branching Brownian motion with coalescence based on intersection local time, supporting the Bolthausen-Sznitman coalescent conjecture.\n\nB) The model converges to a Wright-Fisher SPDE, but the dual process converges to a standard coalescent model without branching, contradicting the Bolthausen-Sznitman coalescent conjecture.\n\nC) The biased voter model converges to a Wright-Fisher SPDE under appropriate scaling, and its dual process converges to a branching Brownian motion with coalescence based on intersection local time, but this model does not support simultaneous coalescences, differing from the Bolthausen-Sznitman coalescent.\n\nD) The model fails to converge to a Wright-Fisher SPDE, but the dual process still exhibits properties of a branching Brownian motion, partially supporting the Bolthausen-Sznitman coalescent conjecture.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately captures the key points from the documentation. The paper shows that under appropriate scaling, the biased voter model converges to a Wright-Fisher SPDE. Additionally, the dual branching coalescing random walk converges to a branching Brownian motion where particles coalesce after an exponentially distributed amount of intersection local time. However, the crucial point is that in this model, there are no simultaneous coalescences, which is a key feature of the Bolthausen-Sznitman coalescent. This makes the model different from what was conjectured by Brunet et al. for similar systems. Options A and B are incorrect because they either support or contradict the Bolthausen-Sznitman conjecture, which the paper doesn't do. Option D is incorrect because it states the model fails to converge to a Wright-Fisher SPDE, which contradicts the paper's findings."}, "39": {"documentation": {"title": "The resolved structure of the extragalactic supernova remnant SNR 4449-1", "source": "M. Mezcua, A.P. Lobanov, I. Mart\\'i-Vidal", "docs_id": "1309.4013", "section": ["astro-ph.HE", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The resolved structure of the extragalactic supernova remnant SNR 4449-1. We present very long baseline interferometry (VLBI) observations of the milliarcsecond-scale radio structure of the supernova remnant SNR 4449$-$1 in the galaxy NGC 4449. This young and superluminous remnant was observed at 1.6 GHz ($\\lambda = 18$\\,cm) with the European VLBI Network. The observations confirm earlier identifications of this object with a supernova remnant (SNR) while revealing a somewhat different morphology compared with the structure reported by Bietenholz et al. from VLBI observations at 1.4 GHz. This difference is discussed here in the context of structural sensitivity of both observations. The 1.6 GHz image yields accurate estimates of the size (0.0422 arcsec $\\times$ 0.0285 arcsec and 0.8 $\\times$ 0.5 pc) and age ($\\sim$55 yr) of SNR 4449$-$1. With a total flux of 6.1 $\\pm$ 0.6 mJy measured in the VLBI image, the historical lightcurve of the source can be well represented by a power-law decay with a power index of $-$1.19 $\\pm$ 0.07. The SNR exhibits a decline rate of the radio emission of 2.2$%$ $\\pm$ 0.1$%$ yr$^{-1}$ and a radio luminosity of 1.74 $\\times$ 10$^{35}$ erg s$^{-1}$."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Based on the VLBI observations of SNR 4449-1 at 1.6 GHz, which of the following statements is correct regarding the supernova remnant's characteristics and evolution?\n\nA) The remnant's size is approximately 0.0422 arcsec \u00d7 0.0285 arcsec, corresponding to 0.8 \u00d7 0.5 ly, and its age is estimated to be around 55 years.\n\nB) The total flux measured in the VLBI image is 6.1 \u00b1 0.6 mJy, and the historical lightcurve follows a power-law decay with an index of -1.19 \u00b1 0.07.\n\nC) The radio emission of the SNR is increasing at a rate of 2.2% \u00b1 0.1% per year, with a current radio luminosity of 1.74 \u00d7 10^35 erg s^-1.\n\nD) The remnant's morphology at 1.6 GHz is identical to that reported by Bietenholz et al. from 1.4 GHz observations, confirming consistent structural features across frequencies.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The question tests the candidate's ability to accurately interpret and recall multiple pieces of information from the given text. Let's break down why B is correct and the others are incorrect:\n\nA) This is incorrect because while the size in arcseconds is correct, the conversion to physical units is wrong. The text states 0.8 \u00d7 0.5 pc (parsecs), not light-years.\n\nB) This is correct. The text explicitly states that the total flux measured in the VLBI image is 6.1 \u00b1 0.6 mJy and that the historical lightcurve follows a power-law decay with an index of -1.19 \u00b1 0.07.\n\nC) This is incorrect. The text states that the SNR exhibits a decline rate of 2.2% \u00b1 0.1% per year, not an increase. The radio luminosity value is correct, but this doesn't make the entire statement true.\n\nD) This is incorrect. The text mentions that the 1.6 GHz observations reveal \"a somewhat different morphology compared with the structure reported by Bietenholz et al. from VLBI observations at 1.4 GHz.\"\n\nThis question requires careful reading and integration of multiple pieces of information, making it suitable for a difficult exam question."}, "40": {"documentation": {"title": "Influence network in Chinese stock market", "source": "Ya-Chun Gao, Yong Zeng, Shi-Min Cai", "docs_id": "1503.00823", "section": ["physics.soc-ph", "physics.data-an", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Influence network in Chinese stock market. In a stock market, the price fluctuations are interactive, that is, one listed company can influence others. In this paper, we seek to study the influence relationships among listed companies by constructing a directed network on the basis of Chinese stock market. This influence network shows distinct topological properties, particularly, a few large companies that can lead the tendency of stock market are recognized. Furthermore, by analyzing the subnetworks of listed companies distributed in several significant economic sectors, it is found that the influence relationships are totally different from one economic sector to another, of which three types of connectivity as well as hub-like listed companies are identified. In addition, the rankings of listed companies obtained from the centrality metrics of influence network are compared with that according to the assets, which gives inspiration to uncover and understand the importance of listed companies in the stock market. These empirical results are meaningful in providing these topological properties of Chinese stock market and economic sectors as well as revealing the interactively influence relationships among listed companies."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements best describes the findings of the study on the influence network in the Chinese stock market?\n\nA) The influence network shows uniform topological properties across all economic sectors, with consistent connectivity patterns.\n\nB) The study found that all listed companies have equal influence on the stock market, regardless of their size or economic sector.\n\nC) The influence network reveals distinct topological properties, with a few large companies leading market tendencies and different connectivity patterns across economic sectors.\n\nD) The rankings of listed companies based on centrality metrics of the influence network were identical to rankings based on company assets.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because it accurately summarizes the key findings of the study. The paper reports that the influence network shows distinct topological properties, with a few large companies recognized as being able to lead the tendency of the stock market. Additionally, the study found that influence relationships differ significantly between economic sectors, identifying three types of connectivity patterns. \n\nAnswer A is incorrect because the study explicitly states that influence relationships are \"totally different from one economic sector to another,\" not uniform across all sectors.\n\nAnswer B is wrong because the study identifies that a few large companies have more influence on leading market tendencies, contradicting the idea of equal influence.\n\nAnswer D is incorrect because the study compares rankings based on centrality metrics with rankings based on assets, implying that these rankings are different and provide new insights into company importance in the stock market."}, "41": {"documentation": {"title": "Vertical sediment concentration distribution revisited with\n  shear-induced diffusivity: An explicit series solution based on homotopy\n  analysis method", "source": "Punit Jain, Manotosh Kumbhakar and Koeli Ghoshal", "docs_id": "2008.07137", "section": ["physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vertical sediment concentration distribution revisited with\n  shear-induced diffusivity: An explicit series solution based on homotopy\n  analysis method. The present study revisits the vertical distribution of suspended sediment concentration in an open channel flow with a special attention to sediment diffusion coefficient. If turbulent diffusivity is considered to follow a parabolic-type profile, the diffusivity coefficient is zero at the bed and very small near the bed; so alone it may not be enough to diffuse the particles from bed-load layer to suspension region. Leighton & Acrivos (J. Fluid Mech., vol. 181, 1987, pp. 415-439) introduced the idea of shear-induced diffusion that arises due to the hydrodynamic interactions between solid particles. This work considers the Hunt diffusion equation incorporating the concept of shear-induced diffusion and reinvestigates the vertical sediment concentration profile. Analytical solution is derived using a non-perturbation approach, namely Homotopy Analysis Method (HAM), and is verified with numerical solution as well as compared with available experimental data. The behaviour of the shear-induced diffusion coefficient with vertical distance and varying particle diameters have been interpreted physically. In addition, the effects of important turbulent factors such as inverse of Schmidt number, hindered settling velocity on concentration profile, have been investigated considering relevant sets of experimental data."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the role of shear-induced diffusion in the vertical distribution of suspended sediment concentration, as presented in the study?\n\nA) It replaces turbulent diffusivity entirely, providing a more accurate model for sediment transport.\n\nB) It is only significant for large particle diameters and can be neglected for fine sediments.\n\nC) It complements turbulent diffusivity, especially near the bed where turbulent diffusivity alone may be insufficient to explain particle suspension.\n\nD) It is a theoretical concept that has no practical impact on sediment concentration profiles in open channel flows.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study introduces the concept of shear-induced diffusion to address limitations in the traditional turbulent diffusivity model, especially near the bed. The documentation states that \"turbulent diffusivity is considered to follow a parabolic-type profile, the diffusivity coefficient is zero at the bed and very small near the bed; so alone it may not be enough to diffuse the particles from bed-load layer to suspension region.\" The shear-induced diffusion, introduced by Leighton & Acrivos, arises due to hydrodynamic interactions between solid particles and complements the turbulent diffusivity, particularly in regions where turbulent diffusivity is insufficient to explain observed sediment suspension.\n\nOption A is incorrect because shear-induced diffusion complements rather than replaces turbulent diffusivity. Option B is not supported by the given information and misrepresents the significance of shear-induced diffusion. Option D contradicts the study's findings, which demonstrate the practical impact of shear-induced diffusion on sediment concentration profiles."}, "42": {"documentation": {"title": "CoinTossX: An open-source low-latency high-throughput matching engine", "source": "Ivan Jericevich and Dharmesh Sing and Tim Gebbie", "docs_id": "2102.10925", "section": ["cs.DC", "cs.MA", "q-fin.CP", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CoinTossX: An open-source low-latency high-throughput matching engine. We deploy and demonstrate the CoinTossX low-latency, high-throughput, open-source matching engine with orders sent using the Julia and Python languages. We show how this can be deployed for small-scale local desk-top testing and discuss a larger scale, but local hosting, with multiple traded instruments managed concurrently and managed by multiple clients. We then demonstrate a cloud based deployment using Microsoft Azure, with large-scale industrial and simulation research use cases in mind. The system is exposed and interacted with via sockets using UDP SBE message protocols and can be monitored using a simple web browser interface using HTTP. We give examples showing how orders can be be sent to the system and market data feeds monitored using the Julia and Python languages. The system is developed in Java with orders submitted as binary encodings (SBE) via UDP protocols using the Aeron Media Driver as the low-latency, high throughput message transport. The system separates the order-generation and simulation environments e.g. agent-based model simulation, from the matching of orders, data-feeds and various modularised components of the order-book system. This ensures a more natural and realistic asynchronicity between events generating orders, and the events associated with order-book dynamics and market data-feeds. We promote the use of Julia as the preferred order submission and simulation environment."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which combination of technologies and features best describes the CoinTossX matching engine's architecture and capabilities?\n\nA) Java-based core with Python as the preferred order submission language, using TCP for communication and JSON for message encoding\n\nB) C++-based core with Julia as the preferred order submission language, using UDP for communication and Protocol Buffers for message encoding\n\nC) Java-based core with Julia as the preferred order submission language, using UDP for communication and SBE for message encoding\n\nD) Rust-based core with Python as the preferred order submission language, using gRPC for communication and Avro for message encoding\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because:\n1. The documentation states that CoinTossX is \"developed in Java.\"\n2. It mentions that \"We promote the use of Julia as the preferred order submission and simulation environment.\"\n3. The system uses \"UDP protocols\" for communication.\n4. Orders are \"submitted as binary encodings (SBE) via UDP protocols.\"\n\nOption A is incorrect because it mentions Python (not the preferred language) and TCP/JSON (not used).\nOption B is incorrect because it mentions C++ (not Java) and Protocol Buffers (not SBE).\nOption D is incorrect because it mentions Rust (not Java), Python (not the preferred language), and gRPC/Avro (not used).\n\nThis question tests the understanding of the core technologies and design choices of the CoinTossX system, requiring careful attention to the details provided in the documentation."}, "43": {"documentation": {"title": "Nuclei Segmentation in Histopathology Images using Deep Learning with\n  Local and Global Views", "source": "Mahdi Arab Loodaricheh, Nader Karimi, Shadrokh Samavi", "docs_id": "2112.03998", "section": ["eess.IV", "cs.CV", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nuclei Segmentation in Histopathology Images using Deep Learning with\n  Local and Global Views. Digital pathology is one of the most significant developments in modern medicine. Pathological examinations are the gold standard of medical protocols and play a fundamental role in diagnosis. Recently, with the advent of digital scanners, tissue histopathology slides can now be digitized and stored as digital images. As a result, digitized histopathological tissues can be used in computer-aided image analysis programs and machine learning techniques. Detection and segmentation of nuclei are some of the essential steps in the diagnosis of cancers. Recently, deep learning has been used for nuclei segmentation. However, one of the problems in deep learning methods for nuclei segmentation is the lack of information from out of the patches. This paper proposes a deep learning-based approach for nuclei segmentation, which addresses the problem of misprediction in patch border areas. We use both local and global patches to predict the final segmentation map. Experimental results on the Multi-organ histopathology dataset demonstrate that our method outperforms the baseline nuclei segmentation and popular segmentation models."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the innovative approach proposed in this paper for nuclei segmentation in histopathology images?\n\nA) It uses a combination of traditional image processing techniques and deep learning algorithms\nB) It incorporates both local and global patch information to improve segmentation accuracy, especially at patch borders\nC) It relies solely on global view patches to overcome the limitations of local patch-based methods\nD) It introduces a new deep learning architecture specifically designed for multi-organ histopathology datasets\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper proposes a deep learning-based approach that addresses the problem of misprediction in patch border areas by using both local and global patches to predict the final segmentation map. This innovative method combines information from different scales to improve overall segmentation accuracy.\n\nAnswer A is incorrect because the paper focuses on a deep learning approach and doesn't mention traditional image processing techniques.\n\nAnswer C is incorrect because the method uses both local and global views, not just global views.\n\nAnswer D is plausible but incorrect. While the method is tested on a multi-organ dataset, the paper doesn't claim to introduce a new architecture specifically for multi-organ histopathology. Instead, it focuses on the local and global view combination to improve segmentation across different types of tissues."}, "44": {"documentation": {"title": "Learning to compress and search visual data in large-scale systems", "source": "Sohrab Ferdowsi", "docs_id": "1901.08437", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to compress and search visual data in large-scale systems. The problem of high-dimensional and large-scale representation of visual data is addressed from an unsupervised learning perspective. The emphasis is put on discrete representations, where the description length can be measured in bits and hence the model capacity can be controlled. The algorithmic infrastructure is developed based on the synthesis and analysis prior models whose rate-distortion properties, as well as capacity vs. sample complexity trade-offs are carefully optimized. These models are then extended to multi-layers, namely the RRQ and the ML-STC frameworks, where the latter is further evolved as a powerful deep neural network architecture with fast and sample-efficient training and discrete representations. For the developed algorithms, three important applications are developed. First, the problem of large-scale similarity search in retrieval systems is addressed, where a double-stage solution is proposed leading to faster query times and shorter database storage. Second, the problem of learned image compression is targeted, where the proposed models can capture more redundancies from the training images than the conventional compression codecs. Finally, the proposed algorithms are used to solve ill-posed inverse problems. In particular, the problems of image denoising and compressive sensing are addressed with promising results."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: Which of the following best describes the ML-STC framework as mentioned in the document?\n\nA) A single-layer compression algorithm optimized for visual data storage\nB) A deep neural network architecture with continuous representations\nC) A multi-layer framework evolved into a deep neural network with fast training and discrete representations\nD) A conventional compression codec for capturing image redundancies\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The document states that \"ML-STC frameworks, where the latter is further evolved as a powerful deep neural network architecture with fast and sample-efficient training and discrete representations.\" This clearly indicates that the ML-STC framework is a multi-layer approach that has been developed into a deep neural network with discrete representations and efficient training.\n\nOption A is incorrect because ML-STC is described as multi-layer, not single-layer.\nOption B is incorrect because the framework uses discrete representations, not continuous ones.\nOption D is incorrect because ML-STC is described as a learned approach, not a conventional compression codec."}, "45": {"documentation": {"title": "Impact of Metal ns2 Lone Pair on Luminescence Efficiency in\n  Low-Dimensional Halide Perovskites", "source": "Hongliang Shi, Dan Han, Shiyou Chen, and Mao-Hua Du", "docs_id": "1902.04700", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Impact of Metal ns2 Lone Pair on Luminescence Efficiency in\n  Low-Dimensional Halide Perovskites. Based on first-principles calculations, we show that chemically active metal ns2 lone pairs play an important role in exciton relaxation and dissociation in low-dimensional halide perovskites. We studied excited-state properties of several recently discovered luminescent all-inorganic and hybrid organic-inorganic zero-dimensional (0D) Sn and Pb halides. The results show that, despite the similarity in ground-state electronic structure between Sn and Pb halide perovskites, the chemically more active Sn2+ lone pair leads to stronger excited-state structural distortion and larger Stokes shift in Sn halides. The enhanced Stokes shift hinders excitation energy transport, which reduces energy loss to defects and increases the photoluminescence quantum efficiency (PLQE). The presence of the ns2 metal cations in the 0D halide perovskites also promotes the exciton dissociation into electron and hole polarons especially in all-inorganic compounds, in which the coupling between metal-halide clusters is significant."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements correctly describes the impact of metal ns2 lone pairs on luminescence efficiency in low-dimensional halide perovskites, according to the research findings?\n\nA) The chemically active Sn2+ lone pair leads to weaker excited-state structural distortion and smaller Stokes shift compared to Pb halides.\n\nB) The enhanced Stokes shift in Sn halides promotes excitation energy transport, increasing energy loss to defects and decreasing photoluminescence quantum efficiency (PLQE).\n\nC) The presence of ns2 metal cations in 0D halide perovskites inhibits exciton dissociation into electron and hole polarons, particularly in all-inorganic compounds.\n\nD) The chemically more active Sn2+ lone pair results in stronger excited-state structural distortion and larger Stokes shift, which hinders excitation energy transport and increases PLQE.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The documentation states that \"the chemically more active Sn2+ lone pair leads to stronger excited-state structural distortion and larger Stokes shift in Sn halides.\" It also mentions that \"The enhanced Stokes shift hinders excitation energy transport, which reduces energy loss to defects and increases the photoluminescence quantum efficiency (PLQE).\" This directly supports the statement in option D.\n\nOption A is incorrect because it contradicts the findings, stating weaker distortion and smaller Stokes shift for Sn2+, which is the opposite of what the research shows.\n\nOption B is incorrect because it misinterprets the effect of the enhanced Stokes shift. The research indicates that it hinders excitation energy transport, reducing energy loss to defects and increasing PLQE, not decreasing it.\n\nOption C is incorrect because it contradicts the research findings. The documentation states that the presence of ns2 metal cations \"promotes the exciton dissociation into electron and hole polarons especially in all-inorganic compounds,\" not inhibits it."}, "46": {"documentation": {"title": "Bayesian Estimation of Economic Simulation Models using Neural Networks", "source": "Donovan Platt", "docs_id": "1906.04522", "section": ["econ.GN", "q-fin.CP", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian Estimation of Economic Simulation Models using Neural Networks. Recent advances in computing power and the potential to make more realistic assumptions due to increased flexibility have led to the increased prevalence of simulation models in economics. While models of this class, and particularly agent-based models, are able to replicate a number of empirically-observed stylised facts not easily recovered by more traditional alternatives, such models remain notoriously difficult to estimate due to their lack of tractable likelihood functions. While the estimation literature continues to grow, existing attempts have approached the problem primarily from a frequentist perspective, with the Bayesian estimation literature remaining comparatively less developed. For this reason, we introduce a Bayesian estimation protocol that makes use of deep neural networks to construct an approximation to the likelihood, which we then benchmark against a prominent alternative from the existing literature. Overall, we find that our proposed methodology consistently results in more accurate estimates in a variety of settings, including the estimation of financial heterogeneous agent models and the identification of changes in dynamics occurring in models incorporating structural breaks."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements best describes the primary innovation and advantage of the Bayesian estimation protocol introduced in this research?\n\nA) It uses deep neural networks to create a tractable likelihood function for traditional economic models.\n\nB) It outperforms frequentist approaches in estimating agent-based models with structural breaks.\n\nC) It employs deep neural networks to approximate the likelihood function, enabling more accurate Bayesian estimation of simulation models.\n\nD) It provides a method for replicating empirically-observed stylized facts in economic simulation models.\n\nCorrect Answer: C\n\nExplanation: \nThe correct answer is C. The key innovation described in the text is the use of deep neural networks to construct an approximation to the likelihood function, which is then used in a Bayesian estimation protocol. This approach addresses the challenge of estimating simulation models (including agent-based models) that lack tractable likelihood functions.\n\nAnswer A is incorrect because while the method does use deep neural networks, it's not creating a tractable likelihood function for traditional models, but rather approximating the likelihood for simulation models.\n\nAnswer B, while partially true (the method does outperform alternatives), is not the primary innovation. The comparison to frequentist approaches is mentioned as context, not as the main contribution.\n\nAnswer D is incorrect because replicating stylized facts is mentioned as a capability of simulation models in general, not as the primary innovation of this estimation method.\n\nThe text explicitly states that this Bayesian estimation protocol using neural networks \"consistently results in more accurate estimates in a variety of settings,\" which aligns with the correct answer C."}, "47": {"documentation": {"title": "Using social network and semantic analysis to analyze online travel\n  forums and forecast tourism demand", "source": "A Fronzetti Colladon, B Guardabascio, R Innarella", "docs_id": "2105.07727", "section": ["econ.EM", "cs.CL", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using social network and semantic analysis to analyze online travel\n  forums and forecast tourism demand. Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network, i.e. the presence of eminent contributors, were the variables that contributed more to the forecasting of international airport arrivals."}, "answer": "Here's a multi-form question based on the provided information:\n\nQuestion: Which of the following statements best describes the key findings and methodology of the research on forecasting tourism demand using social network and semantic analysis of online travel forums?\n\nA) The study analyzed Google Trends data for 7 European cities over 5 years, finding that user demographics were the most important factor in predicting tourism demand.\n\nB) The research examined TripAdvisor forums for 10 major global cities, concluding that the number of posts per user was the best predictor of international airport arrivals.\n\nC) The study analyzed TripAdvisor forums for 7 European capital cities over 10 years, finding that forum language complexity and network centralization were the most significant variables in forecasting international airport arrivals.\n\nD) The research focused on Facebook travel groups for 5 European destinations, determining that sentiment analysis of user comments was the most effective method for predicting tourism trends.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The research analyzed TripAdvisor forums for 7 major European capital cities over a 10-year period, collecting more than 2,660,000 posts from about 147,000 users. The study found that forum language complexity and the centralization of the communication network (presence of eminent contributors) were the variables that contributed most to forecasting international airport arrivals. The research implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables, which often outperformed univariate models and models based on Google Trend data. This approach represents a new methodology for analyzing tourism-related big data and introduces variables that could be integrated into traditional forecasting models."}, "48": {"documentation": {"title": "The Bitcoin price formation: Beyond the fundamental sources", "source": "Jamal Bouoiyour (1), Refk Selmi (1) ((1) CATT)", "docs_id": "1707.01284", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Bitcoin price formation: Beyond the fundamental sources. Much significant research has been done to investigate various facets of the link between Bitcoin price and its fundamental sources. This study goes beyond by looking into least to most influential factors-across the fundamental, macroeconomic, financial, speculative and technical determinants as well as the 2016 events-which drove the value of Bitcoin in times of economic and geopolitical chaos. We use a Bayesian quantile regression to inspect how the structure of dependence of Bitcoin price and its determinants varies across the entire conditional distribution of Bitcoin price movements. In doing so, three groups of determinants were derived. The use of Bitcoin in trade and the uncertainty surrounding China's deepening slowdown, Brexit and India's demonetization were found to be the most potential contributors of Bitcoin price when the market is improving. The intense anxiety over Donald Trump being the president of United States was shown to be a positive determinant pushing up the price of Bitcoin when the market is functioning around the normal mode. The velocity of bitcoins in circulation, the gold price, the Venezuelan currency demonetization and the hash rate were found to be the fundamentals influencing the Bitcoin price when the market is heading into decline."}, "answer": "Here's a multi-form question based on the provided documentation:\n\nQuestion: According to the study on Bitcoin price formation, which of the following factors was found to be most influential when the Bitcoin market is heading into decline?\n\nA) The use of Bitcoin in trade\nB) Uncertainty surrounding Brexit\nC) The velocity of bitcoins in circulation\nD) Donald Trump's presidency\n\nCorrect Answer: C\n\nExplanation: The study used Bayesian quantile regression to examine how various factors influence Bitcoin price across different market conditions. When the market is heading into decline, the research found that \"the velocity of bitcoins in circulation, the gold price, the Venezuelan currency demonetization and the hash rate were the fundamentals influencing the Bitcoin price.\" Among the options provided, only the velocity of bitcoins in circulation (option C) is mentioned as a factor influencing Bitcoin price during market decline.\n\nOption A (use of Bitcoin in trade) was actually found to be influential when the market is improving, not declining. Option B (uncertainty surrounding Brexit) was also associated with improving market conditions. Option D (Donald Trump's presidency) was found to be a positive determinant when the market is functioning around the normal mode, not during decline.\n\nThis question tests the student's ability to carefully read and interpret research findings, distinguishing between factors influencing Bitcoin price under different market conditions."}, "49": {"documentation": {"title": "Fast matrix computations for functional additive models", "source": "Simon Barthelme", "docs_id": "1402.4984", "section": ["stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast matrix computations for functional additive models. It is common in functional data analysis to look at a set of related functions: a set of learning curves, a set of brain signals, a set of spatial maps, etc. One way to express relatedness is through an additive model, whereby each individual function $g_{i}\\left(x\\right)$ is assumed to be a variation around some shared mean $f(x)$. Gaussian processes provide an elegant way of constructing such additive models, but suffer from computational difficulties arising from the matrix operations that need to be performed. Recently Heersink & Furrer have shown that functional additive model give rise to covariance matrices that have a specific form they called quasi-Kronecker (QK), whose inverses are relatively tractable. We show that under additional assumptions the two-level additive model leads to a class of matrices we call restricted quasi-Kronecker, which enjoy many interesting properties. In particular, we formulate matrix factorisations whose complexity scales only linearly in the number of functions in latent field, an enormous improvement over the cubic scaling of na\\\"ive approaches. We describe how to leverage the properties of rQK matrices for inference in Latent Gaussian Models."}, "answer": "Question: Consider a functional additive model where individual functions g_i(x) are variations around a shared mean f(x). Which of the following statements is true regarding the computational efficiency of matrix operations in this context?\n\nA) Gaussian processes provide a computationally efficient method for constructing such additive models without any matrix operation difficulties.\n\nB) The quasi-Kronecker (QK) structure of covariance matrices in these models always leads to cubic scaling complexity in the number of functions.\n\nC) Under additional assumptions, the two-level additive model leads to restricted quasi-Kronecker (rQK) matrices, allowing for matrix factorizations with linear scaling in the number of functions.\n\nD) The computational complexity of matrix operations in functional additive models is always intractable, regardless of the covariance structure.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The passage states that under additional assumptions, the two-level additive model leads to a class of matrices called restricted quasi-Kronecker (rQK), which have special properties. Specifically, it mentions that these rQK matrices allow for matrix factorizations whose complexity scales linearly with the number of functions in the latent field, which is a significant improvement over the cubic scaling of naive approaches.\n\nAnswer A is incorrect because while Gaussian processes provide an elegant way to construct additive models, they still suffer from computational difficulties due to matrix operations.\n\nAnswer B is incorrect because it contradicts the information given. The passage indicates that QK structures, and especially rQK structures, can lead to more efficient computations, not always cubic scaling.\n\nAnswer D is too extreme and contradicts the main point of the passage, which discusses methods to improve the tractability of these computations."}, "50": {"documentation": {"title": "Multivariate risk measures: a constructive approach based on selections", "source": "Ignacio Cascos and Ilya Molchanov", "docs_id": "1301.1496", "section": ["q-fin.RM", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multivariate risk measures: a constructive approach based on selections. Since risky positions in multivariate portfolios can be offset by various choices of capital requirements that depend on the exchange rules and related transaction costs, it is natural to assume that the risk measures of random vectors are set-valued. Furthermore, it is reasonable to include the exchange rules in the argument of the risk measure and so consider risk measures of set-valued portfolios. This situation includes the classical Kabanov's transaction costs model, where the set-valued portfolio is given by the sum of a random vector and an exchange cone, but also a number of further cases of additional liquidity constraints. We suggest a definition of the risk measure based on calling a set-valued portfolio acceptable if it possesses a selection with all individually acceptable marginals. The obtained selection risk measure is coherent (or convex), law invariant and has values being upper convex closed sets. We describe the dual representation of the selection risk measure and suggest efficient ways of approximating it from below and from above. In case of Kabanov's exchange cone model, it is shown how the selection risk measure relates to the set-valued risk measures considered by Kulikov (2008), Hamel and Heyde (2010), and Hamel, Heyde and Rudloff (2013)."}, "answer": "Question: In the context of multivariate risk measures based on selections, which of the following statements is NOT true?\n\nA) The selection risk measure is always non-coherent and law-dependent.\n\nB) Risk measures of random vectors are set-valued due to the possibility of offsetting risky positions through various capital requirements.\n\nC) The selection risk measure considers a set-valued portfolio acceptable if it has a selection with all individually acceptable marginals.\n\nD) The selection risk measure can be efficiently approximated from both below and above.\n\nCorrect Answer: A\n\nExplanation:\nA) This statement is incorrect. According to the documentation, the selection risk measure is coherent (or convex) and law invariant, contradicting this option.\n\nB) This is correct. The documentation states that \"it is natural to assume that the risk measures of random vectors are set-valued\" due to the possibility of offsetting risky positions through various capital requirements depending on exchange rules and transaction costs.\n\nC) This is accurate. The definition provided in the text states that a set-valued portfolio is considered acceptable if it \"possesses a selection with all individually acceptable marginals.\"\n\nD) This is true. The documentation mentions that efficient ways of approximating the selection risk measure from both below and above are suggested.\n\nThe correct answer is A because it contradicts the properties of the selection risk measure described in the text, while the other options accurately reflect information provided in the documentation."}, "51": {"documentation": {"title": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos", "source": "Pia Bideau, Erik Learned-Miller", "docs_id": "1604.00136", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following statements best describes the novel approach and contribution of the research presented in the document?\n\nA) The research focuses solely on improving computational speed for motion detection to match human instantaneous capabilities.\n\nB) The study introduces a new benchmark dataset of camouflaged animals without proposing any new segmentation techniques.\n\nC) The paper presents a probabilistic model that combines the angle and magnitude of optical flow to assess 3D motion direction, resulting in a superior motion segmentation algorithm.\n\nD) The research primarily concentrates on replicating human ability to detect motion in the presence of complex background geometry, without introducing new mathematical models.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the document explicitly states that the researchers \"derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object.\" This likelihood function uses \"a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects.\" This approach led to the development of a motion segmentation algorithm that outperforms current state-of-the-art methods.\n\nOption A is incorrect because while the document mentions human instantaneous motion detection ability, the research doesn't focus solely on improving computational speed.\n\nOption B is partially correct in mentioning the new dataset of camouflaged animals, but it's not the main contribution of the research. The study does propose new segmentation techniques.\n\nOption D is incorrect because although the research acknowledges human abilities in motion detection under complex conditions, it does introduce new mathematical models rather than just trying to replicate human abilities."}, "52": {"documentation": {"title": "MU-MIMO Communications with MIMO Radar: From Co-existence to Joint\n  Transmission", "source": "Fan Liu, Christos Masouros, Ang Li, Huafei Sun, Lajos Hanzo", "docs_id": "1707.00519", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MU-MIMO Communications with MIMO Radar: From Co-existence to Joint\n  Transmission. Beamforming techniques are proposed for a joint multi-input-multi-output (MIMO) radar-communication (RadCom) system, where a single device acts both as a radar and a communication base station (BS) by simultaneously communicating with downlink users and detecting radar targets. Two operational options are considered, where we first split the antennas into two groups, one for radar and the other for communication. Under this deployment, the radar signal is designed to fall into the null-space of the downlink channel. The communication beamformer is optimized such that the beampattern obtained matches the radar's beampattern while satisfying the communication performance requirements. To reduce the optimizations' constraints, we consider a second operational option, where all the antennas transmit a joint waveform that is shared by both radar and communications. In this case, we formulate an appropriate probing beampattern, while guaranteeing the performance of the downlink communications. By incorporating the SINR constraints into objective functions as penalty terms, we further simplify the original beamforming designs to weighted optimizations, and solve them by efficient manifold algorithms. Numerical results show that the shared deployment outperforms the separated case significantly, and the proposed weighted optimizations achieve a similar performance to the original optimizations, despite their significantly lower computational complexity."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the joint MIMO radar-communication system described, which of the following statements best characterizes the shared deployment approach compared to the separated antenna approach?\n\nA) It utilizes a split antenna configuration with separate radar and communication signals\nB) It achieves lower performance but with higher computational efficiency\nC) It uses all antennas to transmit a joint waveform and outperforms the separated case significantly\nD) It requires more complex optimization constraints than the separated approach\n\nCorrect Answer: C\n\nExplanation: The question tests understanding of the two operational options described in the document. The correct answer is C because the document states that in the second operational option (shared deployment), \"all the antennas transmit a joint waveform that is shared by both radar and communications.\" Furthermore, it explicitly mentions that \"Numerical results show that the shared deployment outperforms the separated case significantly.\"\n\nOption A is incorrect as it describes the separated approach, not the shared deployment. \nOption B is wrong on both counts - the shared deployment actually achieves higher performance and aims to reduce optimization constraints. \nOption D is incorrect because the document mentions that the shared deployment aims \"To reduce the optimizations' constraints,\" not increase them."}, "53": {"documentation": {"title": "Spatially Aware Dictionary Learning and Coding for Fossil Pollen\n  Identification", "source": "Shu Kong, Surangi Punyasena, Charless Fowlkes", "docs_id": "1605.00775", "section": ["cs.CV", "q-bio.PE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatially Aware Dictionary Learning and Coding for Fossil Pollen\n  Identification. We propose a robust approach for performing automatic species-level recognition of fossil pollen grains in microscopy images that exploits both global shape and local texture characteristics in a patch-based matching methodology. We introduce a novel criteria for selecting meaningful and discriminative exemplar patches. We optimize this function during training using a greedy submodular function optimization framework that gives a near-optimal solution with bounded approximation error. We use these selected exemplars as a dictionary basis and propose a spatially-aware sparse coding method to match testing images for identification while maintaining global shape correspondence. To accelerate the coding process for fast matching, we introduce a relaxed form that uses spatially-aware soft-thresholding during coding. Finally, we carry out an experimental study that demonstrates the effectiveness and efficiency of our exemplar selection and classification mechanisms, achieving $86.13\\%$ accuracy on a difficult fine-grained species classification task distinguishing three types of fossil spruce pollen."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Which of the following best describes the novel approach introduced in the paper for selecting exemplar patches in fossil pollen identification?\n\nA) Random selection of patches from training images\nB) Optimization using a greedy submodular function framework\nC) Manual selection by expert paleontologists\nD) Deep learning-based feature extraction\n\nCorrect Answer: B\n\nExplanation: The correct answer is B. The paper introduces \"a novel criteria for selecting meaningful and discriminative exemplar patches\" and states that they \"optimize this function during training using a greedy submodular function optimization framework that gives a near-optimal solution with bounded approximation error.\" This approach is unique and central to their methodology.\n\nOption A is incorrect because random selection would not yield meaningful and discriminative exemplars. Option C is not mentioned in the text and would not be an automatic approach. Option D, while plausible in image recognition tasks, is not mentioned in the given text as part of their exemplar selection process.\n\nThis question tests the reader's understanding of the key methodological innovation presented in the paper, requiring careful reading and comprehension of the technical approach described."}, "54": {"documentation": {"title": "Formal characterization and efficient verification of a biological\n  robustness property", "source": "Lucia Nasti, Roberta Gori, Paolo Milazzo", "docs_id": "2104.13831", "section": ["cs.FL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Formal characterization and efficient verification of a biological\n  robustness property. Robustness is an observable property for which a chemical reaction network (CRN) can maintain its functionalities despite the influence of different perturbations. In general, to verify whether a network is robust, it is necessary to consider all the possible parameter configurations. This is a process that can entail a massive computational effort. In the work of Rizk et al., the authors propose a definition of robustness in linear temporal logic (LTL) through which, on the basis of multiple numerical timed traces obtained by considering different parameter configurations, they verify the robustness of a reaction network. In this paper, we focus on a notion of initial concentration robustness ($\\alpha$-robustness), which is related to the influence of the perturbation of the initial concentration of one species (i.e., the input) on the concentration of another species (i.e., the output) at the steady state. We characterize this notion of robustness in the framework proposed by Rizk et al., and we show that, for monotonic reaction networks, this allows us to drastically reduce the number of traces necessary to verify robustness of the CRN."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of chemical reaction networks (CRNs), which of the following statements most accurately describes the concept of \u03b1-robustness and its verification process as proposed in the paper?\n\nA) \u03b1-robustness refers to a CRN's ability to maintain its functionality despite perturbations in all possible parameter configurations, and requires exhaustive computational analysis of every parameter combination.\n\nB) \u03b1-robustness is a measure of how the perturbation of the initial concentration of one species affects the steady-state concentration of another species, and can be efficiently verified for monotonic reaction networks using fewer traces than traditional methods.\n\nC) \u03b1-robustness is defined using linear temporal logic (LTL) and requires the analysis of multiple numerical timed traces obtained from different parameter configurations to verify the robustness of a reaction network.\n\nD) \u03b1-robustness is a property that can only be verified by considering all possible parameter configurations, making it computationally infeasible for complex chemical reaction networks.\n\nCorrect Answer: B\n\nExplanation: The correct answer is B because it accurately captures the essence of \u03b1-robustness as described in the paper. \u03b1-robustness, or initial concentration robustness, specifically relates to how the perturbation of the initial concentration of one species (the input) influences the concentration of another species (the output) at the steady state. The paper introduces a method to characterize this robustness within the framework proposed by Rizk et al., which allows for more efficient verification, especially for monotonic reaction networks, by reducing the number of traces needed.\n\nOption A is incorrect because it describes robustness in general terms and suggests exhaustive analysis of all parameter combinations, which the paper aims to avoid.\n\nOption C is partially correct in mentioning LTL and multiple traces, but it describes the approach by Rizk et al. for general robustness, not the specific \u03b1-robustness concept introduced in this paper.\n\nOption D is incorrect because it contradicts the paper's main contribution, which is to provide a method for more efficient verification of \u03b1-robustness without needing to consider all possible parameter configurations."}, "55": {"documentation": {"title": "The GALAH survey: A census of lithium-rich giant stars", "source": "Sarah Martell, Jeffrey Simpson, Adithya Balasubramaniam, Sven Buder,\n  Sanjib Sharma, Marc Hon, Dennis Stello, Yuan-Sen Ting, Martin Asplund, Joss\n  Bland-Hawthorn, Gayandhi De Silva, Ken Freeman, Michael Hayden, Janez Kos,\n  Geraint Lewis, Karin Lind, Daniel Zucker, Tomaz Zwitter, Simon Campbell,\n  Klemen Cotar, Jonathan Horner, Benjamin Montet, and Rob Wittenmyer", "docs_id": "2006.02106", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The GALAH survey: A census of lithium-rich giant stars. We investigate the properties of 1262 red giant stars with high photospheric abundances of lithium observed by the GALAH and \\Ktwo-HERMES surveys, and discuss them in the context of proposed mechanisms for lithium enrichment and re-depletion in giant stars. We confirm that Li-rich giants are rare, making up only 1.2 per cent of our giant star sample. We use stellar parameters from the third public data release from the GALAH survey and a Bayesian isochrone analysis to divide the sample into first-ascent red giant branch and red clump stars, and confirm these classifications using asteroseismic data from \\Ktwo. We find that red clump stars are 2.5 times as likely to be lithium-rich as red giant branch stars, in agreement with other recent work. The probability for a star to be lithium-rich is affected by a number of factors, though the causality in those correlations is not entirely clear. We show for the first time that primary and secondary red clump stars have distinctly different lithium enrichment patterns. The data set discussed here is large and heterogeneous in terms of evolutionary phase, metallicity, rotation rate and mass. We expect that if the various mechanisms that have been proposed for lithium enrichment in evolved stars are in fact active, they should all contribute to this sample of lithium-rich giants at some level."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Based on the GALAH and K2-HERMES surveys, which of the following statements about lithium-rich giant stars is most accurate?\n\nA) Lithium-rich giants are equally distributed among red giant branch and red clump stars.\nB) Red clump stars are 2.5 times less likely to be lithium-rich compared to red giant branch stars.\nC) Primary and secondary red clump stars show identical lithium enrichment patterns.\nD) Red clump stars are 2.5 times more likely to be lithium-rich than red giant branch stars, and primary and secondary red clump stars exhibit distinct lithium enrichment patterns.\n\nCorrect Answer: D\n\nExplanation: The question tests the reader's understanding of the key findings from the GALAH and K2-HERMES surveys regarding lithium-rich giant stars. Option D is correct because it accurately combines two important observations from the study:\n\n1. The passage states, \"We find that red clump stars are 2.5 times as likely to be lithium-rich as red giant branch stars, in agreement with other recent work.\"\n\n2. It also mentions, \"We show for the first time that primary and secondary red clump stars have distinctly different lithium enrichment patterns.\"\n\nOption A is incorrect because the study clearly shows an unequal distribution of lithium-rich giants between red giant branch and red clump stars. Option B is the opposite of what the study found, making it incorrect. Option C contradicts the study's finding about the distinct patterns in primary and secondary red clump stars. Only option D correctly combines both key findings, making it the most accurate statement based on the given information."}, "56": {"documentation": {"title": "AMYTISS: Parallelized Automated Controller Synthesis for Large-Scale\n  Stochastic Systems", "source": "Abolfazl Lavaei, Mahmoud Khaled, Sadegh Soudjani, and Majid Zamani", "docs_id": "2005.06191", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AMYTISS: Parallelized Automated Controller Synthesis for Large-Scale\n  Stochastic Systems. In this paper, we propose a software tool, called AMYTISS, implemented in C++/OpenCL, for designing correct-by-construction controllers for large-scale discrete-time stochastic systems. This tool is employed to (i) build finite Markov decision processes (MDPs) as finite abstractions of given original systems, and (ii) synthesize controllers for the constructed finite MDPs satisfying bounded-time high-level properties including safety, reachability and reach-avoid specifications. In AMYTISS, scalable parallel algorithms are designed such that they support the parallel execution within CPUs, GPUs and hardware accelerators (HWAs). Unlike all existing tools for stochastic systems, AMYTISS can utilize high-performance computing (HPC) platforms and cloud-computing services to mitigate the effects of the state-explosion problem, which is always present in analyzing large-scale stochastic systems. We benchmark AMYTISS against the most recent tools in the literature using several physical case studies including robot examples, room temperature and road traffic networks. We also apply our algorithms to a 3-dimensional autonomous vehicle and 7-dimensional nonlinear model of a BMW 320i car by synthesizing an autonomous parking controller."}, "answer": "Question: Which of the following statements best describes the key innovation and functionality of AMYTISS as presented in the documentation?\n\nA) It is a software tool for simulating stochastic systems using machine learning algorithms.\n\nB) It is a parallelized tool for automated controller synthesis of large-scale stochastic systems, utilizing HPC platforms to mitigate state-explosion problems.\n\nC) It is a tool specifically designed for autonomous vehicle navigation in complex urban environments.\n\nD) It is a software package for analyzing the energy efficiency of robotic systems in industrial settings.\n\nCorrect Answer: B\n\nExplanation: Option B correctly captures the main features and innovations of AMYTISS as described in the documentation. AMYTISS is indeed a software tool for automated controller synthesis for large-scale stochastic systems. Its key innovations include:\n\n1. Parallelized algorithms that can run on CPUs, GPUs, and hardware accelerators.\n2. Utilization of high-performance computing (HPC) platforms and cloud-computing services.\n3. Ability to mitigate the state-explosion problem common in large-scale stochastic systems.\n4. Functionality to build finite Markov decision processes (MDPs) and synthesize controllers for bounded-time high-level properties.\n\nOption A is incorrect as it doesn't mention the controller synthesis aspect and incorrectly states it uses machine learning algorithms. Option C is too narrow, focusing only on autonomous vehicles, which is just one application example. Option D is incorrect as it mischaracterizes the tool's purpose, focusing on energy efficiency analysis rather than controller synthesis."}, "57": {"documentation": {"title": "Non-degenerate shell-model effective interactions from the\n  Okamoto-Suzuki and Krenciglowa-Kuo iteration methods", "source": "Huan Dong, T.T.S.Kuo and J.W.Holt", "docs_id": "1011.1487", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-degenerate shell-model effective interactions from the\n  Okamoto-Suzuki and Krenciglowa-Kuo iteration methods. We present calculations of shell-model effective interactions for both degenerate and non-degenerate model spaces using the Krenciglowa-Kuo (KK) and the extended Krenciglowa-Kuo iteration method recently developed by Okamoto, Suzuki {\\it et al.} (EKKO). The starting point is the low-momentum nucleon-nucleon interaction $V_{low-k}$ obtained from the N$^3$LO chiral two-nucleon interaction. The model spaces spanned by the $sd$ and $sdpf$ shells are both considered. With a solvable model, we show that both the KK and EKKO methods are convenient for deriving the effective interactions for non-degenerate model spaces. The EKKO method is especially desirable in this situation since the vertex function $\\hat Z$-box employed therein is well behaved while the corresponding vertex function $\\hat Q$-box employed in the Lee-Suzuki (LS) and KK methods may have singularities. The converged shell-model effective interactions given by the EKKO and KK methods are equivalent, although the former method is considerably more efficient. The degenerate $sd$-shell effective interactions given by the LS method are practically identical to those from the EKKO and KK methods. Results of the $sd$ one-shell and $sdpf$ two-shell calculations for $^{18}$O, $^{18}$F, $^{19}$O and $^{19}$F using the EKKO effective interactions are compared, and the importance of the shell-model three-nucleon forces is discussed."}, "answer": "Here's a difficult multi-form question based on the given documentation:\n\nQuestion: Which of the following statements about the Okamoto-Suzuki and Krenciglowa-Kuo iteration methods for calculating shell-model effective interactions is NOT correct?\n\nA) The EKKO method employs a well-behaved vertex function \u1e90-box, making it more suitable for non-degenerate model spaces.\n\nB) The KK and EKKO methods produce equivalent converged shell-model effective interactions, but EKKO is more computationally efficient.\n\nC) The Lee-Suzuki method produces significantly different results compared to EKKO and KK methods for degenerate sd-shell effective interactions.\n\nD) Both the KK and EKKO methods use the low-momentum nucleon-nucleon interaction V_low-k derived from the N\u00b3LO chiral two-nucleon interaction as a starting point.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C because the document states that \"The degenerate sd-shell effective interactions given by the LS method are practically identical to those from the EKKO and KK methods.\" This contradicts the statement in option C, which claims significant differences between the methods.\n\nOption A is correct according to the text, which mentions that the EKKO method's \u1e90-box is well-behaved, while the Q\u0302-box used in LS and KK methods may have singularities.\n\nOption B is supported by the document, which states that the EKKO and KK methods give equivalent results, but EKKO is more efficient.\n\nOption D is also correct, as the text explicitly mentions that the starting point for both methods is the low-momentum nucleon-nucleon interaction V_low-k obtained from the N\u00b3LO chiral two-nucleon interaction."}, "58": {"documentation": {"title": "Sparse Matrix Inversion with Scaled Lasso", "source": "Tingni Sun and Cun-Hui Zhang", "docs_id": "1202.2723", "section": ["math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparse Matrix Inversion with Scaled Lasso. We propose a new method of learning a sparse nonnegative-definite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm first estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other $\\ell_1$ regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the $\\ell_1$ and spectrum norms of the target inverse matrix diverges to infinity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso."}, "answer": "Here's a difficult multi-form question based on the given text:\n\nQuestion: Which of the following statements best describes the advantages of the proposed sparse matrix inversion method using scaled Lasso, as compared to existing \u21131 regularized algorithms?\n\nA) It requires cross-validation for determining the penalty level of the scaled Lasso for each column.\nB) It guarantees a slower rate of convergence in the spectrum norm under weaker conditions.\nC) It provides faster guaranteed rate of convergence when the ratio of the \u21131 and spectrum norms of the target inverse matrix approaches zero.\nD) It guarantees the fastest proven rate of convergence in the spectrum norm under weaker conditions and potentially faster convergence for certain matrix properties.\n\nCorrect Answer: D\n\nExplanation: The correct answer is D. The text states that the proposed method \"guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other \u21131 regularized algorithms.\" Additionally, it mentions that the method \"has faster guaranteed rate of convergence when the ratio of the \u21131 and spectrum norms of the target inverse matrix diverges to infinity.\" This corresponds to the information provided in option D.\n\nOption A is incorrect because the text explicitly states that the penalty level is \"completely determined by data via convex minimization, without using cross-validation.\"\n\nOption B is incorrect as it contradicts the information given; the method guarantees a faster, not slower, rate of convergence.\n\nOption C is incorrect because it states the opposite of what's given in the text. The faster convergence occurs when the ratio diverges to infinity, not when it approaches zero."}, "59": {"documentation": {"title": "Structuring of counterions around dna double helix: a molecular dynamics\n  study", "source": "O.O. Liubysh, A.V. Vlasiuk, S.M. Perepelytsya", "docs_id": "1503.05334", "section": ["q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Structuring of counterions around dna double helix: a molecular dynamics\n  study. Structuring of DNA counterions around the double helix has been studied by the molecular dynamics method. A DNA dodecamer d(CGCGAATTCGCG) in water solution with the alkali metal counterions Na$^{+}$, K$^{+}$, and Cs$^{+}$ has been simulated. The systems have been considered in the regimes without excess salt and with different salts (0.5 M of NaCl, KCl or CsCl) added. The results have showed that the Na$^{+}$ counterions interact with the phosphate groups directly from outside of the double helix and via water molecules at the top edge of DNA minor groove. The potassium ions are mostly localized in the grooves of the double helix, and the cesium ions penetrate deeply inside the minor groove being bonded directly to the atoms of nucleic bases. Due to the electrostatic repulsion the chlorine ions tend to be localized at large distances from the DNA polyanion, but some Cl$^{-}$ anions have been detected near atomic groups of the double helix forming electrically neutral pairs with counterions already condensed on DNA. The DNA sites, where counterions are incorporated, are characterized by local changes of double helix structure. The lifetime of Na$^{+}$ and K$^{+}$ in complex with DNA atomic groups is less than 0.5 ns, while in the case of the cesium ions it may reach several nanoseconds. In this time scale, the Cs$^{+}$ counterions form a structured system of charges in the DNA minor groove that can be considered as ionic lattice."}, "answer": "Here's a difficult multi-form question based on the given information:\n\nQuestion: Which of the following statements accurately describes the behavior of different alkali metal counterions around DNA, as observed in the molecular dynamics study?\n\nA) Na+ ions primarily interact with the phosphate groups from inside the double helix and rarely form water-mediated interactions in the minor groove.\n\nB) K+ ions show a strong preference for the major groove of the DNA double helix and rarely enter the minor groove.\n\nC) Cs+ ions penetrate deeply into the minor groove, forming direct bonds with nucleic base atoms and creating a structured system of charges that can be considered an ionic lattice.\n\nD) Cl- anions consistently form a tight shell around the DNA polyanion due to electrostatic attraction, never forming neutral pairs with condensed counterions.\n\nCorrect Answer: C\n\nExplanation: The correct answer is C. The study found that Cs+ ions penetrate deeply into the minor groove of DNA, forming direct bonds with atoms of nucleic bases. Furthermore, Cs+ ions were observed to have longer residence times (up to several nanoseconds) in complex with DNA atomic groups, allowing them to form a structured system of charges in the minor groove that can be considered as an ionic lattice.\n\nAnswer A is incorrect because Na+ ions interact with phosphate groups from outside the double helix, not inside, and they do form water-mediated interactions at the top edge of the minor groove.\n\nAnswer B is incorrect as K+ ions were found to be mostly localized in both grooves of the double helix, not showing a strong preference for the major groove.\n\nAnswer D is incorrect because Cl- anions tend to be localized at large distances from the DNA polyanion due to electrostatic repulsion, not forming a tight shell. Some Cl- anions were detected near atomic groups of the double helix, forming electrically neutral pairs with condensed counterions."}}