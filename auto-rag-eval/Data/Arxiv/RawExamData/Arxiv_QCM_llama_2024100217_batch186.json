{"0": {"documentation": {"title": "Learning Efficient Structured Sparse Models", "source": "Alex Bronstein (Tel Aviv University), Pablo Sprechmann (University of\n  Minnesota), Guillermo Sapiro (University of Minnesota)", "docs_id": "1206.4649", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Efficient Structured Sparse Models. We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed framework in \"Learning Efficient Structured Sparse Models\" in terms of its ability to approximate structured sparse codes?\n\n**A)** The framework introduces a new dictionary learning method that can learn sparse codes more efficiently than existing methods.\n\n**B)** The framework develops a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, allowing for the approximation of exact structured sparse codes with reduced complexity.\n\n**C)** The framework proposes a new type of sparse encoder that can learn to model complex patterns in data, rather than just approximating existing sparse codes.\n\n**D)** The framework presents a new optimization algorithm that can solve sparse coding problems in parallel, leading to significant speedups.\n\n**Correct Answer:** B) The framework develops a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, allowing for the approximation of exact structured sparse codes with reduced complexity.\n\n**Explanation:** The correct answer is B because the documentation states that the framework \"develops a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration.\" This indicates that the primary contribution of the framework is the development of a new method for approximating structured sparse codes with reduced complexity."}, "1": {"documentation": {"title": "Simulating the Emission and Outflows from Accretion Disks", "source": "Scott C. Noble, Po Kin Leung, Charles F. Gammie, Laura G. Book", "docs_id": "astro-ph/0701778", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simulating the Emission and Outflows from Accretion Disks. The radio source Sagittarius A* (Sgr A*) is believed to be a hot, inhomogeneous, magnetized plasma flowing near the event horizon of the 3 million solar mass black hole at the galactic center. At a distance of 8000 parsecs the black hole would be among the largest black holes as judged by angular size. Recent observations are consistent with the idea that the millimeter and sub-millimeter photons are dominated by optically thin, thermal synchrotron emission. Anticipating future Very Long Baseline Interferometry (VLBI) observations of Sgr A* at these wavelengths, we present here the first dynamically self-consistent models of millimeter and sub-millimeter emission from Sgr A* based on general relativistic numerical simulations of the accretion flow. Angle-dependent spectra are calculated assuming a thermal distribution of electrons at the baryonic temperature dictated by the simulation and the accretion rate, which acts as a free parameter in our model. The effects of varying model parameters (black hole spin and inclination of the spin to the line of sight) and source variability on the spectrum are shown. We find that the accretion rate value needed to match our calculated millimeter flux to the observed flux is consistent with constraints on the accretion rate inferred from detections of the rotation measure. We also describe the relativistic jet that is launched along the black hole spin axis by the accretion disk and evolves to scales of 1000 gravitational radii."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nSimulating the Emission and Outflows from Accretion Disks\n\nWhat is the primary assumption made about the millimeter and sub-millimeter photons emitted by the radio source Sagittarius A* (Sgr A*)?\n\nA) They are dominated by optically thick, non-thermal emission.\nB) They are dominated by optically thin, thermal synchrotron emission.\nC) They are dominated by relativistic jets launched by the accretion disk.\nD) They are dominated by emission from the black hole's event horizon.\n\nCorrect Answer: B) They are dominated by optically thin, thermal synchrotron emission.\n\nExplanation: The question requires the test-taker to recall the main assumption made about the millimeter and sub-millimeter photons emitted by Sgr A*. The correct answer, B, is supported by the text, which states that \"Recent observations are consistent with the idea that the millimeter and sub-millimeter photons are dominated by optically thin, thermal synchrotron emission.\" The other options are incorrect because they do not accurately reflect the assumption made about the emission mechanism. Option A is incorrect because the text states that the emission is optically thin, not optically thick. Option C is incorrect because the text does not mention the primary assumption about the emission mechanism, but rather describes the relativistic jet launched by the accretion disk. Option D is incorrect because the event horizon is not directly related to the emission mechanism."}, "2": {"documentation": {"title": "LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm", "source": "Sri Jamiya S, Esther Rani P", "docs_id": "2011.05940", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm. Vehicle detection in real-time is a challenging and important task. The existing real-time vehicle detection lacks accuracy and speed. Real-time systems must detect and locate vehicles during criminal activities like theft of vehicle and road traffic violations with high accuracy. Detection of vehicles in complex scenes with occlusion is also extremely difficult. In this study, a lightweight model of deep neural network LittleYOLO-SPP based on the YOLOv3-tiny network is proposed to detect vehicles effectively in real-time. The YOLOv3-tiny object detection network is improved by modifying its feature extraction network to increase the speed and accuracy of vehicle detection. The proposed network incorporated Spatial pyramid pooling into the network, which consists of different scales of pooling layers for concatenation of features to enhance network learning capability. The Mean square error (MSE) and Generalized IoU (GIoU) loss function for bounding box regression is used to increase the performance of the network. The network training includes vehicle-based classes from PASCAL VOC 2007,2012 and MS COCO 2014 datasets such as car, bus, and truck. LittleYOLO-SPP network detects the vehicle in real-time with high accuracy regardless of video frame and weather conditions. The improved network achieves a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP on MS COCO datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the LittleYOLO-SPP network in improving the accuracy and speed of real-time vehicle detection, and how does it address the challenges of occlusion in complex scenes?\n\n**A)** The LittleYOLO-SPP network improves the accuracy and speed of real-time vehicle detection by incorporating a new feature extraction network that increases the number of layers, resulting in a higher mAP of 83.21% on PASCAL VOC and 58.12% mAP on MS COCO datasets.\n\n**B)** The LittleYOLO-SPP network addresses the challenges of occlusion in complex scenes by using a spatial pyramid pooling layer that concatenates features from different scales, resulting in a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP on MS COCO datasets.\n\n**C)** The LittleYOLO-SPP network improves the accuracy and speed of real-time vehicle detection by using a new loss function that combines Mean square error (MSE) and Generalized IoU (GIoU), resulting in a higher mAP of 80.12% on PASCAL VOC and 60.45% mAP on MS COCO datasets.\n\n**D)** The LittleYOLO-SPP network addresses the challenges of occlusion in complex scenes by using a lightweight model of deep neural network that reduces the number of parameters, resulting in a higher mAP of 75.67% on PASCAL VOC and 50.21% mAP on MS COCO datasets.\n\n**Correct Answer:** B) The LittleYOLO-SPP network addresses the challenges of occlusion in complex scenes by using a spatial pyramid pooling layer that concatenates features from different scales, resulting in a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP on MS COCO datasets.\n\n**Explanation:** The correct answer is B) because the LittleYOLO-SPP network incorporates spatial pyramid pooling into the network, which consists of different scales of pooling layers for concatenation of features to enhance network learning capability. This allows the network to effectively detect vehicles in complex scenes with occlusion, resulting in a higher mAP on both PASCAL VOC and MS COCO datasets."}, "3": {"documentation": {"title": "Spreading dynamics on small-world networks with connectivity\n  fluctuations and correlations", "source": "Alexei Vazquez", "docs_id": "q-bio/0603010", "section": ["q-bio.PE", "cond-mat.dis-nn", "physics.soc-ph", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spreading dynamics on small-world networks with connectivity\n  fluctuations and correlations. Infectious diseases and computer malwares spread among humans and computers through the network of contacts among them. These networks are characterized by wide connectivity fluctuations, connectivity correlations and the small-world property. In a previous work [A. Vazquez, Phys. Rev. Lett. 96, 038702 (2006)] I have shown that the connectivity fluctuations together with the small-world property lead to a novel spreading law, characterized by an initial power law growth with an exponent determined by the average node distance on the network. Here I extend these results to consider the influence of connectivity correlations which are generally observed in real networks. I show that assortative and disassortative connectivity correlations enhance and diminish, respectively, the range of validity of this spreading law. As a corollary I obtain the region of connectivity fluctuations and degree correlations characterized by the absence of an epidemic threshold. These results are relevant for the spreading of infectious diseases, rumors, and information among humans and the spreading of computer viruses, email worms and hoaxes among computer users."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of spreading dynamics on small-world networks with connectivity fluctuations and correlations, what is the effect of assortative and disassortative connectivity correlations on the range of validity of the spreading law characterized by an initial power law growth?\n\n**A)** Both assortative and disassortative correlations enhance the range of validity of the spreading law.\n**B)** Disassortative correlations enhance the range of validity, while assortative correlations diminish it.\n**C)** The effect of assortative and disassortative correlations on the range of validity is the same.\n**D)** Neither assortative nor disassortative correlations affect the range of validity of the spreading law.\n\n**Correct Answer:** B) Disassortative correlations enhance the range of validity, while assortative correlations diminish it.\n\n**Explanation:** According to the documentation, the author shows that assortative and disassortative connectivity correlations enhance and diminish, respectively, the range of validity of the spreading law. This means that disassortative correlations increase the range of validity, while assortative correlations decrease it. Therefore, option B is the correct answer."}, "4": {"documentation": {"title": "Phase Imaging with Computational Specificity (PICS) for measuring dry\n  mass changes in sub-cellular compartments", "source": "Mikhail E. Kandel (1 and 2), Yuchen R. He (1 and 2), Young Jae Lee (1\n  and 3), Taylor Hsuan-Yu Chen (1 and 4), Kathryn Michele Sullivan (4), Onur\n  Aydin (5), M Taher A. Saif (4 and 5), Hyunjoon Kong (1, 4, 6 and 7), Nahil\n  Sobh (1), Gabriel Popescu (1, 2, and 4) ((1) Beckman Institute, University of\n  Illinois at Urbana-Champaign, Urbana, IL, USA, (2) Department of Electrical\n  and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana,\n  IL, USA, (3) Neuroscience Program, University of Illinois at\n  Urbana-Champaign, Urbana, IL, USA, (4) Department of Bioengineering,\n  University of Illinois at Urbana-Champaign, Urbana, IL, USA, (5) Department\n  of Mechanical Science and Engineering, University of Illinois at\n  Urbana-Champaign, Urbana, IL, USA, (6) Chemical and Biomolecular Engineering,\n  University of Illinois at Urbana-Champaign, Urbana, IL, USA, (7) Carl Woese\n  Institute for Genomic Biology, University of Illinois at Urbana-Champaign,\n  Urbana, IL, USA)", "docs_id": "2002.08361", "section": ["eess.IV", "physics.bio-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phase Imaging with Computational Specificity (PICS) for measuring dry\n  mass changes in sub-cellular compartments. Due to its specificity, fluorescence microscopy (FM) has become a quintessential imaging tool in cell biology. However, photobleaching, phototoxicity, and related artifacts continue to limit FM's utility. Recently, it has been shown that artificial intelligence (AI) can transform one form of contrast into another. We present PICS, a combination of quantitative phase imaging and AI, which provides information about unlabeled live cells with high specificity. Our imaging system allows for automatic training, while inference is built into the acquisition software and runs in real-time. Applying the computed fluorescence maps back to the QPI data, we measured the growth of both nuclei and cytoplasm independently, over many days, without loss of viability. Using a QPI method that suppresses multiple scattering, we measured the dry mass content of individual cell nuclei within spheroids. In its current implementation, PICS offers a versatile quantitative technique for continuous simultaneous monitoring of individual cellular components in biological applications where long-term label-free imaging is desirable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of fluorescence microscopy (FM) in cell biology, and how does the PICS technique address this limitation?\n\nA) Photobleaching and phototoxicity limit FM's utility due to the need for repeated labeling and fixation, which can alter cellular structures. PICS addresses this by using AI to transform one form of contrast into another, allowing for continuous label-free imaging.\n\nB) FM is limited by the need for specialized equipment and expertise, whereas PICS is limited by its reliance on complex AI algorithms. PICS addresses this by incorporating automatic training and real-time inference into the acquisition software.\n\nC) FM is limited by the need for multiple labeling and fixation steps, which can damage cellular structures. PICS addresses this by using quantitative phase imaging to measure dry mass changes in sub-cellular compartments without the need for labeling.\n\nD) FM is limited by the need for high-powered lasers, which can be hazardous to researchers. PICS addresses this by using a QPI method that suppresses multiple scattering, allowing for safe and efficient imaging.\n\nCorrect Answer: C) FM is limited by the need for multiple labeling and fixation steps, which can damage cellular structures. PICS addresses this by using quantitative phase imaging to measure dry mass changes in sub-cellular compartments without the need for labeling."}, "5": {"documentation": {"title": "Time series analysis via mechanistic models", "source": "Carles Bret\\'o, Daihai He, Edward L. Ionides, Aaron A. King", "docs_id": "0802.0021", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time series analysis via mechanistic models. The purpose of time series analysis via mechanistic models is to reconcile the known or hypothesized structure of a dynamical system with observations collected over time. We develop a framework for constructing nonlinear mechanistic models and carrying out inference. Our framework permits the consideration of implicit dynamic models, meaning statistical models for stochastic dynamical systems which are specified by a simulation algorithm to generate sample paths. Inference procedures that operate on implicit models are said to have the plug-and-play property. Our work builds on recently developed plug-and-play inference methodology for partially observed Markov models. We introduce a class of implicitly specified Markov chains with stochastic transition rates, and we demonstrate its applicability to open problems in statistical inference for biological systems. As one example, these models are shown to give a fresh perspective on measles transmission dynamics. As a second example, we present a mechanistic analysis of cholera incidence data, involving interaction between two competing strains of the pathogen Vibrio cholerae."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of time series analysis via mechanistic models, and how does it differ from traditional statistical models?\n\nA) To estimate the parameters of a linear dynamical system using maximum likelihood estimation, and to account for non-linear relationships between variables.\nB) To reconcile the known or hypothesized structure of a dynamical system with observations collected over time, and to consider implicit dynamic models that are specified by a simulation algorithm.\nC) To develop a framework for constructing nonlinear mechanistic models and carrying out inference, and to apply this framework to biological systems.\nD) To introduce a class of implicitly specified Markov chains with stochastic transition rates, and to demonstrate its applicability to open problems in statistical inference.\n\n**Correct Answer:** B) To reconcile the known or hypothesized structure of a dynamical system with observations collected over time, and to consider implicit dynamic models that are specified by a simulation algorithm.\n\n**Explanation:** The correct answer, B, accurately reflects the primary purpose of time series analysis via mechanistic models, as stated in the documentation. This approach aims to reconcile the known or hypothesized structure of a dynamical system with observations collected over time, and to consider implicit dynamic models that are specified by a simulation algorithm. The other options, while related to the topic, do not accurately capture the primary purpose of this approach.\n\n**Candidate A:** This option is incorrect because it describes a traditional statistical model, whereas the documentation emphasizes the use of mechanistic models and implicit dynamic models.\n\n**Candidate B:** This option is incorrect because it is a repetition of the correct answer, and does not provide additional information.\n\n**Candidate C:** This option is incorrect because it is too broad, and does not specifically address the primary purpose of time series analysis via mechanistic models.\n\n**Candidate D:** This option is incorrect because it introduces a new concept (implicitly specified Markov chains) that is not the primary focus of the documentation."}, "6": {"documentation": {"title": "One Dollar Each Eliminates Envy", "source": "Johannes Brustle and Jack Dippel and Vishnu V. Narayan and Mashbat\n  Suzuki and Adrian Vetta", "docs_id": "1912.02797", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "One Dollar Each Eliminates Envy. We study the fair division of a collection of $m$ indivisible goods amongst a set of $n$ agents. Whilst envy-free allocations typically do not exist in the indivisible goods setting, envy-freeness can be achieved if some amount of a divisible good (money) is introduced. Specifically, Halpern and Shah (SAGT 2019, pp.374-389) showed that, given additive valuation functions where the marginal value of each item is at most one dollar for each agent, there always exists an envy-free allocation requiring a subsidy of at most $(n-1)\\cdot m$ dollars. The authors also conjectured that a subsidy of $n-1$ dollars is sufficient for additive valuations. We prove this conjecture. In fact, a subsidy of at most one dollar per agent is sufficient to guarantee the existence of an envy-free allocation. Further, we prove that for general monotonic valuation functions an envy-free allocation always exists with a subsidy of at most $2(n-1)$ dollars per agent. In particular, the total subsidy required for monotonic valuations is independent of the number of items."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of fair division of indivisible goods amongst a set of agents, what is the minimum amount of subsidy required to guarantee the existence of an envy-free allocation, assuming a general monotonic valuation function?\n\nA) $(n-1)\\cdot m$ dollars\nB) $n-1$ dollars\nC) $2(n-1)$ dollars\nD) $m$ dollars\n\n**Correct Answer:** C) $2(n-1)$ dollars\n\n**Explanation:** The correct answer is C) $2(n-1)$ dollars. The problem statement mentions that for general monotonic valuation functions, an envy-free allocation always exists with a subsidy of at most $2(n-1)$ dollars per agent. This is because the subsidy is used to \"buy off\" agents who would otherwise be envious of others' allocations. The factor of 2 is likely due to the fact that each agent may have multiple items to envy, and the subsidy must be sufficient to cover the maximum possible envy.\n\n**Candidate A (n-1)\u22c5m dollars is incorrect because it is the result for additive valuations, not general monotonic valuations.**\n\n**Candidate B (n-1 dollars) is incorrect because it is the result for additive valuations, and the problem statement specifically mentions that a subsidy of at most one dollar per agent is sufficient for additive valuations.**\n\n**Candidate D (m dollars) is incorrect because it is not related to the subsidy required for envy-free allocations.**"}, "7": {"documentation": {"title": "Kinetic theory and Brazilian income distribution", "source": "Igor D. S. Siciliani and Marcelo H. R. Tragtenberg", "docs_id": "1709.06480", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kinetic theory and Brazilian income distribution. We investigate the Brazilian personal income distribution using data from National Household Sample Survey (PNAD), an annual research available by the Brazilian Institute of Geography and Statistics (IBGE). It provides general characteristics of the country's population. Using PNAD data background we also confirm the effectiveness of a semi-empirical model that reconciles Pareto power-law for high-income people and Boltzmann- Gibbs distribution for the rest of population. We use three measures of income inequality: the Pareto index, the average income and the crossover income. In order to cope with many dimensions of the income inequality, we calculate these three indices and also the Gini coefficient for the general population as well as for two kinds of population dichotomies: black / indigenous / mixed race versus white / yellow; and men versus women. We also followed the time series of these indices for the period 2001-2014. The results suggest a decreasing of Brazilian income inequality over the selected period. Another important result is that historically-disadvantaged subgroups (Women and black / indigenous / mixed race),that are the majority of the population, have a more equalitarian income distribution. These groups have also a smaller monthly income than the others and this social structure remained virtually unchanged in the period of time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on Brazilian income distribution using the data from the National Household Sample Survey (PNAD). What is the main conclusion regarding the trend of income inequality in Brazil over the period 2001-2014?\n\n**A)** The study suggests that Brazilian income inequality has increased significantly over the period 2001-2014, with historically-disadvantaged subgroups experiencing a widening gap with the rest of the population.\n\n**B)** The study confirms that Brazilian income inequality has decreased over the period 2001-2014, with historically-disadvantaged subgroups such as women and black/indigenous/mixed-race individuals experiencing a more equalitarian income distribution.\n\n**C)** The study reveals that the income distribution in Brazil is characterized by a power-law Pareto distribution for high-income individuals and a Boltzmann-Gibbs distribution for the rest of the population, but does not provide any information on the trend of income inequality.\n\n**D)** The study suggests that the income inequality in Brazil is influenced by demographic factors such as ethnicity and gender, but does not provide any information on the trend of income inequality over time.\n\n**Correct Answer:** B) The study confirms that Brazilian income inequality has decreased over the period 2001-2014, with historically-disadvantaged subgroups such as women and black/indigenous/mixed-race individuals experiencing a more equalitarian income distribution.\n\n**Explanation:** The correct answer is B) because the study specifically states that the results suggest a decreasing of Brazilian income inequality over the selected period. Additionally, the study highlights that historically-disadvantaged subgroups, such as women and black/indigenous/mixed-race individuals, have a more equalitarian income distribution, which is consistent with the conclusion that income inequality has decreased."}, "8": {"documentation": {"title": "Plasma instabilities and magnetic-field growth in clusters of galaxies", "source": "A.A.Schekochihin (Cambridge), S.C.Cowley (UCLA & Imperial),\n  R.M.Kulsrud (Princeton), G.W.Hammett (PPPL), P.Sharma (PPPL)", "docs_id": "astro-ph/0501362", "section": ["astro-ph", "nlin.CD", "physics.plasm-ph", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Plasma instabilities and magnetic-field growth in clusters of galaxies. We show that under very general conditions, cluster plasmas threaded by weak magnetic fields are subject to very fast growing plasma instabilities driven by the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field. Such an anisotropy will naturally arise in any weakly magnetized plasma that has low collisionality and is subject to stirring. The magnetic field must be sufficiently weak for the instabilities to occur, viz., beta>Re^{1/2}. The instabilities are captured by the extended MHD model with Braginskii viscosity. However, their growth rates are proportional to the wavenumber down to the ion gyroscale, so MHD equations with Braginskii viscosity are not well posed and a fully kinetic treatment is necessary. The instabilities can lead to magnetic fields in clusters being amplified from seed strength of ~10^{-18} G to dynamically important strengths of ~10 microG on cosmologically trivial time scales (~10^8 yr). The fields produced during the amplification stage are at scales much smaller than observed. Predicting the saturated field scale and structure will require a kinetic theory of magnetized cluster turbulence."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism driving the growth of magnetic fields in clusters of galaxies, as described in the Arxiv documentation?\n\nA) The anisotropy of the plasma pressure with respect to the local direction of the magnetic field, driven by the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field.\n\nB) The amplification of magnetic fields through the action of Alfv\u00e9n waves, which are generated by the stirring of the plasma.\n\nC) The dissipation of magnetic energy through the action of resistive diffusion, which is driven by the collisionality of the plasma.\n\nD) The generation of magnetic fields through the action of turbulent fluctuations, which are driven by the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field.\n\nCorrect Answer: D) The generation of magnetic fields through the action of turbulent fluctuations, which are driven by the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field.\n\nExplanation: The correct answer is D) because the Arxiv documentation states that the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field is the primary mechanism driving the growth of magnetic fields in clusters of galaxies. This is supported by the fact that the instabilities are driven by the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field, and that the magnetic field must be sufficiently weak for the instabilities to occur.\n\nCandidate A is incorrect because it is a description of the anisotropy of the plasma pressure (viscous stress) with respect to the local direction of the magnetic field, but it is not the primary mechanism driving the growth of magnetic fields.\n\nCandidate B is incorrect because it is a description of the amplification of magnetic fields through the action of Alfv\u00e9n waves, but it is not the primary mechanism driving the growth of magnetic fields in clusters of galaxies.\n\nCandidate C is incorrect because it is a description of the dissipation of magnetic energy through the action of resistive diffusion, but it is not the primary mechanism driving the growth of magnetic fields in clusters of galaxies."}, "9": {"documentation": {"title": "Particles, conformal invariance and criticality in pure and disordered\n  systems", "source": "Gesualdo Delfino", "docs_id": "2010.12275", "section": ["cond-mat.stat-mech", "cond-mat.dis-nn", "hep-th", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Particles, conformal invariance and criticality in pure and disordered\n  systems. The two-dimensional case occupies a special position in the theory of critical phenomena due to the exact results provided by lattice solutions and, directly in the continuum, by the infinite-dimensional character of the conformal algebra. However, some sectors of the theory, and most notably criticality in systems with quenched disorder and short range interactions, have appeared out of reach of exact methods and lacked the insight coming from analytical solutions. In this article we review recent progress achieved implementing conformal invariance within the particle description of field theory. The formalism yields exact unitarity equations whose solutions classify critical points with a given symmetry. It provides new insight in the case of pure systems, as well as the first exact access to criticality in presence of short range quenched disorder. Analytical mechanisms emerge that in the random case allow the superuniversality of some critical exponents and make explicit the softening of first order transitions by disorder."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the significance of the two-dimensional case in the theory of critical phenomena, and how does the implementation of conformal invariance within the particle description of field theory address the limitations of exact methods in systems with quenched disorder and short-range interactions?\n\nA) The two-dimensional case is significant due to the exact results provided by lattice solutions, but the implementation of conformal invariance does not address the limitations of exact methods.\nB) The two-dimensional case is significant due to the infinite-dimensional character of the conformal algebra, but the implementation of conformal invariance does not provide new insight into criticality in systems with quenched disorder.\nC) The implementation of conformal invariance within the particle description of field theory provides exact unitarity equations that classify critical points with a given symmetry, and this approach addresses the limitations of exact methods in systems with quenched disorder and short-range interactions.\nD) The two-dimensional case is significant due to the exact results provided by lattice solutions, but the implementation of conformal invariance does not make explicit the softening of first-order transitions by disorder.\n\nCorrect Answer: C) The implementation of conformal invariance within the particle description of field theory provides exact unitarity equations that classify critical points with a given symmetry, and this approach addresses the limitations of exact methods in systems with quenched disorder and short-range interactions.\n\nExplanation: The correct answer highlights the key contribution of the article, which is the implementation of conformal invariance within the particle description of field theory. This approach provides exact unitarity equations that classify critical points with a given symmetry, and it addresses the limitations of exact methods in systems with quenched disorder and short-range interactions. The other options are incorrect because they either downplay the significance of the two-dimensional case or misrepresent the main contribution of the article."}, "10": {"documentation": {"title": "Interplay between collective effects and non-standard interactions of\n  supernova neutrinos", "source": "A. Esteban-Pretel, R. Tomas, and J. W. F. Valle", "docs_id": "0909.2196", "section": ["hep-ph", "astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interplay between collective effects and non-standard interactions of\n  supernova neutrinos. We consider the effect of non-standard neutrino interactions (NSI, for short) on the propagation of neutrinos through the supernova (SN) envelope within a three-neutrino framework and taking into account the presence of a neutrino background. We find that for given NSI parameters, with strength generically denoted by $\\varepsilon_{ij}$, neutrino evolution exhibits a significant time dependence. For $|\\varepsilon_{\\tau\\tau}|\\gtrsim$ $10^{-3}$ the neutrino survival probability may become sensitive to the $\\theta_{23}$ octant and the sign of $\\varepsilon_{\\tau\\tau}$. In particular, if $\\varepsilon_{\\tau\\tau}\\gtrsim 10^{-2}$ an internal $I$-resonance may arise independently of the matter density. For typical values found in SN simulations this takes place in the same dense-neutrino region above the neutrinosphere where collective effects occur, in particular during the synchronization regime. This resonance may lead to an exchange of the neutrino fluxes entering the bipolar regime. The main consequences are (i) bipolar conversion taking place for normal neutrino mass hierarchy and (ii) a transformation of the flux of low-energy $\\nu_e$, instead of the usual spectral swap."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of supernova neutrinos, what is the expected outcome of the internal $I$-resonance that may arise due to non-standard neutrino interactions (NSI) with strength $\\varepsilon_{\\tau\\tau} \\gtrsim 10^{-2}$?\n\nA) The neutrino fluxes entering the bipolar regime will be swapped, leading to a spectral swap of low-energy $\\nu_e$.\nB) The bipolar conversion will occur for normal neutrino mass hierarchy, but the flux of low-energy $\\nu_e$ will not be transformed.\nC) The internal $I$-resonance will lead to an exchange of the neutrino fluxes entering the bipolar regime, resulting in bipolar conversion for normal neutrino mass hierarchy, and a transformation of the flux of low-energy $\\nu_e$.\nD) The internal $I$-resonance will have no effect on the neutrino fluxes entering the bipolar regime.\n\n**Correct Answer:** C) The internal $I$-resonance will lead to an exchange of the neutrino fluxes entering the bipolar regime, resulting in bipolar conversion for normal neutrino mass hierarchy, and a transformation of the flux of low-energy $\\nu_e$.\n\n**Explanation:** The internal $I$-resonance is a consequence of non-standard neutrino interactions with strength $\\varepsilon_{\\tau\\tau} \\gtrsim 10^{-2}$. This resonance can lead to an exchange of the neutrino fluxes entering the bipolar regime, resulting in bipolar conversion for normal neutrino mass hierarchy. Additionally, the transformation of the flux of low-energy $\\nu_e$ is expected to occur, rather than the usual spectral swap. This outcome is a direct consequence of the internal $I$-resonance and the non-standard neutrino interactions."}, "11": {"documentation": {"title": "Equivalent extensions of Hamilton-Jacobi-Bellman equations on\n  hypersurfaces", "source": "Lindsay Martin and Richard Tsai", "docs_id": "1903.11173", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Equivalent extensions of Hamilton-Jacobi-Bellman equations on\n  hypersurfaces. We present a new formulation for the computation of solutions of a class of Hamilton Jacobi Bellman (HJB) equations on closed smooth surfaces of co-dimension one. For the class of equations considered in this paper, the viscosity solution of the HJB equation is equivalent to the value function of a corresponding optimal control problem. In this work, we extend the optimal control problem given on the surface to an equivalent one defined in a sufficiently thin narrow band of the co-dimensional one surface. The extension is done appropriately so that the corresponding HJB equation, in the narrow band, has a unique viscosity solution which is identical to the constant normal extension of the value function of the original optimal control problem. With this framework, one can easily use existing (high order) numerical methods developed on Cartesian grids to solve HJB equations on surfaces, with a computational cost that scales with the dimension of the surfaces. This framework also provides a systematic way for solving HJB equations on the unstructured point clouds that are sampled from the surface."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a Hamilton-Jacobi-Bellman (HJB) equation on a closed smooth surface of co-dimension one, where the viscosity solution is equivalent to the value function of a corresponding optimal control problem. Suppose we want to extend the optimal control problem to an equivalent one defined in a sufficiently thin narrow band of the co-dimensional one surface. What is the key requirement for the extension to ensure that the corresponding HJB equation, in the narrow band, has a unique viscosity solution which is identical to the constant normal extension of the value function of the original optimal control problem?\n\nA) The narrow band must be parallel to the surface.\nB) The narrow band must be perpendicular to the surface.\nC) The narrow band must be of uniform thickness.\nD) The narrow band must be of a finite width.\n\nCorrect Answer: B) The narrow band must be perpendicular to the surface.\n\nExplanation: The correct answer is B) The narrow band must be perpendicular to the surface. This is because the constant normal extension of the value function requires that the narrow band is perpendicular to the surface, ensuring that the viscosity solution is identical to the value function of the original optimal control problem. The other options are incorrect because a parallel narrow band would not provide the necessary perpendicularity, a uniform thickness is not a requirement, and a finite width is not sufficient to guarantee the uniqueness of the viscosity solution."}, "12": {"documentation": {"title": "Secular increase of the Astronomical Unit and perihelion precessions as\n  tests of the Dvali-Gabadadze-Porrati multi-dimensional braneworld scenario", "source": "Lorenzo Iorio", "docs_id": "gr-qc/0508047", "section": ["gr-qc", "astro-ph", "hep-ph", "hep-th", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secular increase of the Astronomical Unit and perihelion precessions as\n  tests of the Dvali-Gabadadze-Porrati multi-dimensional braneworld scenario. An unexpected secular increase of the Astronomical Unit, the length scale of the Solar System, has recently been reported by three different research groups (Krasinsky and Brumberg, Pitjeva, Standish). The latest JPL measurements amount to 7+-2 m cy^-1. At present, there are no explanations able to accommodate such an observed phenomenon, neither in the realm of classical physics nor in the usual four-dimensional framework of the Einsteinian General Relativity. The Dvali-Gabadadze-Porrati braneworld scenario, which is a multi-dimensional model of gravity aimed to the explanation of the observed cosmic acceleration without dark energy, predicts, among other things, a perihelion secular shift, due to Lue and Starkman, of 5 10^-4 arcsec cy^-1 for all the planets of the Solar System. It yields a variation of about 6 m cy^-1 for the Earth-Sun distance which is compatible at 1-sigma level with the observed rate of the Astronomical Unit. The recently measured corrections to the secular motions of the perihelia of the inner planets of the Solar System are in agreement, at 1-sigma level, with the predicted value of the Lue-Starkman effect for Mercury and Mars and at 2-sigma level for the Earth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Dvali-Gabadadze-Porrati braneworld scenario predicts a perihelion secular shift of 5 \u00d7 10^-4 arcsec cy^-1 for all planets in the Solar System. However, recent measurements of the Astronomical Unit's secular increase suggest a value of 7 \u00b1 2 m cy^-1. Which of the following statements best describes the relationship between the predicted perihelion secular shift and the observed secular increase of the Astronomical Unit?\n\nA) The predicted perihelion secular shift is consistent with the observed secular increase of the Astronomical Unit, but the latter is not a direct consequence of the former.\nB) The predicted perihelion secular shift is not consistent with the observed secular increase of the Astronomical Unit, and the latter is a direct consequence of the former.\nC) The predicted perihelion secular shift is consistent with the observed secular increase of the Astronomical Unit, and the latter is a direct consequence of the former.\nD) The predicted perihelion secular shift is not related to the observed secular increase of the Astronomical Unit.\n\n**Correct Answer:** C) The predicted perihelion secular shift is consistent with the observed secular increase of the Astronomical Unit, and the latter is a direct consequence of the former.\n\n**Explanation:** The correct answer is C) because the predicted perihelion secular shift of 5 \u00d7 10^-4 arcsec cy^-1 is consistent with the observed secular increase of the Astronomical Unit of 7 \u00b1 2 m cy^-1. The latter is a direct consequence of the former, as the predicted perihelion secular shift is a result of the Dvali-Gabadadze-Porrati braneworld scenario, which is aimed at explaining the observed cosmic acceleration without dark energy. The observed secular increase of the Astronomical Unit is a manifestation of this effect, and the predicted perihelion secular shift is a key component of this explanation."}, "13": {"documentation": {"title": "Event-Based Dynamic Banking Network Exploration for Economic Anomaly\n  Detection", "source": "Andry Alamsyah, Dian Puteri Ramadhani, Farida Titik Kristanti", "docs_id": "2103.03120", "section": ["econ.GN", "cs.CY", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Event-Based Dynamic Banking Network Exploration for Economic Anomaly\n  Detection. The instability of financial system issues might trigger a bank failure, evoke spillovers, and generate contagion effects which negatively impacted the financial system, ultimately on the economy. This phenomenon is the result of the highly interconnected banking transaction. The banking transactions network is considered as a financial architecture backbone. The strong interconnectedness between banks escalates contagion disruption spreading over the banking network and trigger the entire system collapse. This far, the financial instability is generally detected using macro approach mainly the uncontrolled transaction deficits amount and unpaid foreign debt. This research proposes financial instability detection in another point of view, through the macro view where the banking network structure are explored globally and micro view where focuses on the detailed network patterns called motif. Network triadic motif patterns used as a denomination to detect financial instability. The most related network triadic motif changes related to the instability period are determined as a detector. We explore the banking network behavior under financial instability phenomenon along with the major religious event in Indonesia, Eid al-Fitr. We discover one motif pattern as the financial instability underlying detector. This research helps to support the financial system stability supervision."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach of the proposed research in detecting financial instability, and how does it differ from the traditional macro approach?\n\nA) The proposed research uses a micro approach to focus on the detailed network patterns called motif, whereas the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt.\nB) The proposed research uses a macro approach to explore the banking network structure globally, whereas the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt.\nC) The proposed research uses a micro approach to focus on the detailed network patterns called motif, but the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt.\nD) The proposed research uses a macro approach to explore the banking network structure globally, but the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt.\n\n**Correct Answer:** A) The proposed research uses a micro approach to focus on the detailed network patterns called motif, whereas the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt.\n\n**Explanation:** The proposed research uses a micro approach to focus on the detailed network patterns called motif, which is a key aspect of detecting financial instability. In contrast, the traditional macro approach relies on uncontrolled transaction deficits and unpaid foreign debt, which is a more general and macro-level approach. The correct answer highlights the distinction between the proposed research's micro approach and the traditional macro approach."}, "14": {"documentation": {"title": "Exact Coherent Structures and Chaotic Dynamics in a Model of Cardiac\n  Tissue", "source": "Greg Byrne, Christopher D. Marcotte and Roman O. Grigoriev", "docs_id": "1404.2316", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact Coherent Structures and Chaotic Dynamics in a Model of Cardiac\n  Tissue. Unstable nonchaotic solutions embedded in the chaotic attractor can provide significant new insight into chaotic dynamics of both low- and high-dimensional systems. In particular, in turbulent fluid flows, such unstable solutions are referred to as exact coherent structures (ECS) and play an important role in both initiating and sustaining turbulence. The nature of ECS and their role in organizing spatiotemporally chaotic dynamics, however, is reasonably well understood only for systems on relatively small spatial domains lacking continuous Euclidean symmetries. Construction of ECS on large domains and in the presence of continuous translational and/or rotational symmetries remains a challenge. This is especially true for models of excitable media which display spiral turbulence and for which the standard approach to computing ECS completely breaks down. This paper uses the Karma model of cardiac tissue to illustrate a potential approach that could allow computing a new class of ECS on large domains of arbitrary shape by decomposing them into a patchwork of solutions on smaller domains, or tiles, which retain Euclidean symmetries locally."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in computing exact coherent structures (ECS) in models of excitable media, such as cardiac tissue, and how does the proposed approach in this paper address this challenge?\n\nA) The primary challenge is the lack of computational power to simulate large domains, and the proposed approach uses a patchwork of solutions on smaller domains to overcome this limitation.\nB) The primary challenge is the presence of continuous translational and/or rotational symmetries, which break down the standard approach to computing ECS, and the proposed approach uses a decomposition method to retain Euclidean symmetries locally.\nC) The primary challenge is the difficulty in identifying unstable nonchaotic solutions embedded in the chaotic attractor, and the proposed approach uses a machine learning algorithm to detect these solutions.\nD) The primary challenge is the need for a more sophisticated mathematical framework to describe the dynamics of ECS, and the proposed approach uses a novel mathematical formulation to address this need.\n\nCorrect Answer: B) The primary challenge is the presence of continuous translational and/or rotational symmetries, which break down the standard approach to computing ECS, and the proposed approach uses a decomposition method to retain Euclidean symmetries locally.\n\nExplanation: The correct answer is B) because the documentation states that the standard approach to computing ECS \"completely breaks down\" in the presence of continuous translational and/or rotational symmetries, which is a key challenge in models of excitable media. The proposed approach in this paper addresses this challenge by using a decomposition method to retain Euclidean symmetries locally, allowing for the computation of ECS on large domains of arbitrary shape."}, "15": {"documentation": {"title": "Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference\n  Learning", "source": "Stephen N. Pallone, Peter I. Frazier, and Shane G. Henderson", "docs_id": "1702.07694", "section": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference\n  Learning. We analyze the problem of learning a single user's preferences in an active learning setting, sequentially and adaptively querying the user over a finite time horizon. Learning is conducted via choice-based queries, where the user selects her preferred option among a small subset of offered alternatives. These queries have been shown to be a robust and efficient way to learn an individual's preferences. We take a parametric approach and model the user's preferences through a linear classifier, using a Bayesian prior to encode our current knowledge of this classifier. The rate at which we learn depends on the alternatives offered at every time epoch. Under certain noise assumptions, we show that the Bayes-optimal policy for maximally reducing entropy of the posterior distribution of this linear classifier is a greedy policy, and that this policy achieves a linear lower bound when alternatives can be constructed from the continuum. Further, we analyze a different metric called misclassification error, proving that the performance of the optimal policy that minimizes misclassification error is bounded below by a linear function of differential entropy. Lastly, we numerically compare the greedy entropy reduction policy with a knowledge gradient policy under a number of scenarios, examining their performance under both differential entropy and misclassification error."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference Learning, what is the relationship between the rate of learning and the alternatives offered at every time epoch, assuming a linear classifier and a Bayesian prior?\n\n**A)** The rate of learning is directly proportional to the number of alternatives offered at every time epoch.\n**B)** The rate of learning is inversely proportional to the number of alternatives offered at every time epoch.\n**C)** The rate of learning is independent of the number of alternatives offered at every time epoch.\n**D)** The rate of learning is bounded by a linear function of the number of alternatives offered at every time epoch.\n\n**Correct Answer:** D) The rate of learning is bounded by a linear function of the number of alternatives offered at every time epoch.\n\n**Explanation:** According to the documentation, the rate at which we learn depends on the alternatives offered at every time epoch. Under certain noise assumptions, the Bayes-optimal policy for maximally reducing entropy of the posterior distribution of this linear classifier is a greedy policy, and that this policy achieves a linear lower bound when alternatives can be constructed from the continuum. This implies that the rate of learning is bounded by a linear function of the number of alternatives offered at every time epoch."}, "16": {"documentation": {"title": "On the formation of shocks of electromagnetic plane waves in non-linear\n  crystals", "source": "Demetrios Christodoulou and Daniel Raoul Perez", "docs_id": "1505.04101", "section": ["math.AP", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the formation of shocks of electromagnetic plane waves in non-linear\n  crystals. An influential result of F. John states that no genuinely non-linear strictly hyperbolic quasi-linear first order system of partial differential equations in two variables has a global $C^2$-solution for small enough initial data. Inspired by recent work of D. Christodoulou, we revisit John's original proof and extract a more precise description of the behaviour of solutions at the time of shock. We show that John's singular first order quantity, when expressed in characteristic coordinates, remains bounded until the final time, which is then characterised by an inverse density of characteristics tending to zero in one point. Moreover, we study the derivatives of second order, showing again their boundedness when expressed in appropriate coordinates. We also recover John's upper bound for the time of shock formation and complement it with a lower bound. Finally, we apply these results to electromagnetic plane waves in a crystal with no magnetic properties and cubic electric non-linearity in the energy density, assuming no dispersion."}, "answer": "Here is the multi-form question:\n\nWhat is the key finding of the study on the formation of shocks of electromagnetic plane waves in non-linear crystals, and how does it relate to John's original proof?\n\nA) The study shows that the time of shock formation is bounded by a constant, and that the derivatives of second order remain bounded until the final time.\nB) The study reveals that the inverse density of characteristics tending to zero in one point characterizes the final time of shock formation, and that the derivatives of second order remain bounded in characteristic coordinates.\nC) The study demonstrates that the global $C^2$-solution for small enough initial data is not possible, and that the time of shock formation is characterized by an inverse density of characteristics tending to zero in one point.\nD) The study shows that the time of shock formation is bounded by an inverse density of characteristics tending to zero in one point, and that the derivatives of second order remain bounded in characteristic coordinates.\n\nCorrect Answer: B) The study reveals that the inverse density of characteristics tending to zero in one point characterizes the final time of shock formation, and that the derivatives of second order remain bounded in characteristic coordinates.\n\nExplanation: The correct answer is B) because the study extracts a more precise description of the behavior of solutions at the time of shock, showing that the inverse density of characteristics tending to zero in one point characterizes the final time of shock formation. Additionally, the study shows that the derivatives of second order remain bounded when expressed in appropriate coordinates, which is a key finding of the study."}, "17": {"documentation": {"title": "Generalized Covariance Estimator", "source": "Christian Gourieroux, Joann Jasiak", "docs_id": "2107.06979", "section": ["econ.EM", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalized Covariance Estimator. We consider a class of semi-parametric dynamic models with strong white noise errors. This class of processes includes the standard Vector Autoregressive (VAR) model, the nonfundamental structural VAR, the mixed causal-noncausal models, as well as nonlinear dynamic models such as the (multivariate) ARCH-M model. For estimation of processes in this class, we propose the Generalized Covariance (GCov) estimator, which is obtained by minimizing a residual-based multivariate portmanteau statistic as an alternative to the Generalized Method of Moments. We derive the asymptotic properties of the GCov estimator and of the associated residual-based portmanteau statistic. Moreover, we show that the GCov estimators are semi-parametrically efficient and the residual-based portmanteau statistics are asymptotically chi-square distributed. The finite sample performance of the GCov estimator is illustrated in a simulation study. The estimator is also applied to a dynamic model of cryptocurrency prices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a semi-parametric dynamic model with strong white noise errors, which includes a Vector Autoregressive (VAR) model, a nonfundamental structural VAR, mixed causal-noncausal models, and nonlinear dynamic models such as the ARCH-M model. The Generalized Covariance (GCov) estimator is proposed as an alternative to the Generalized Method of Moments (GMM) for estimation of these processes. Derive the asymptotic distribution of the GCov estimator under the assumption of strong white noise errors.\n\n**A)** The GCov estimator is asymptotically normal with mean zero and variance equal to the inverse of the information matrix.\n\n**B)** The GCov estimator is asymptotically chi-square distributed with degrees of freedom equal to the number of parameters estimated.\n\n**C)** The GCov estimator is asymptotically efficient and has a finite sample size distribution that is a mixture of normal and chi-square distributions.\n\n**D)** The GCov estimator is asymptotically efficient and has a finite sample size distribution that is a normal distribution with mean equal to the true parameter value and variance equal to the inverse of the information matrix.\n\n**Correct Answer:** D) The GCov estimator is asymptotically efficient and has a finite sample size distribution that is a normal distribution with mean equal to the true parameter value and variance equal to the inverse of the information matrix.\n\n**Explanation:** The correct answer is D) because the documentation states that the GCov estimator is asymptotically efficient and has a finite sample size distribution that is a normal distribution with mean equal to the true parameter value and variance equal to the inverse of the information matrix. This is a key result of the asymptotic theory of the GCov estimator, and it is essential to understand this result to evaluate the performance of the GCov estimator in practice."}, "18": {"documentation": {"title": "Ubiquitous power law scaling in nonlinear self-excited Hawkes processes", "source": "Kiyoshi Kanazawa and Didier Sornette", "docs_id": "2102.00242", "section": ["cond-mat.stat-mech", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ubiquitous power law scaling in nonlinear self-excited Hawkes processes. The origin(s) of the ubiquity of probability distribution functions (PDF) with power law tails is still a matter of fascination and investigation in many scientific fields from linguistic, social, economic, computer sciences to essentially all natural sciences. In parallel, self-excited dynamics is a prevalent characteristic of many systems, from the physics of shot noise and intermittent processes, to seismicity, financial and social systems. Motivated by activation processes of the Arrhenius form, we bring the two threads together by introducing a general class of nonlinear self-excited point processes with fast-accelerating intensities as a function of \"tension\". Solving the corresponding master equations, we find that a wide class of such nonlinear Hawkes processes have the PDF of their intensities described by a power law on the condition that (i) the intensity is a fast-accelerating function of tension, (ii) the distribution of marks is two-sided with non-positive mean, and (iii) it has fast-decaying tails. In particular, Zipf's scaling is obtained in the limit where the average mark is vanishing. This unearths a novel mechanism for power laws including Zipf's law, providing a new understanding of their ubiquity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mechanism that explains the ubiquity of power law distribution functions (PDFs) with power law tails, as observed in various scientific fields, and how does it relate to the properties of nonlinear self-excited Hawkes processes?\n\nA) The ubiquity of power laws is due to the presence of long-range correlations in the underlying systems, which lead to the emergence of power law distributions in the intensities of the processes.\n\nB) The ubiquity of power laws is due to the fact that nonlinear self-excited Hawkes processes have fast-accelerating intensities as a function of \"tension\", which leads to the emergence of power law distributions in the intensities of the processes.\n\nC) The ubiquity of power laws is due to the presence of a \"critical point\" in the systems, where the distribution of marks is two-sided with non-positive mean, and the tails of the distribution decay rapidly.\n\nD) The ubiquity of power laws is due to the fact that the Arrhenius activation process is a fundamental mechanism that drives the emergence of power law distributions in various systems.\n\nCorrect Answer: B) The ubiquity of power laws is due to the fact that nonlinear self-excited Hawkes processes have fast-accelerating intensities as a function of \"tension\", which leads to the emergence of power law distributions in the intensities of the processes.\n\nExplanation: The correct answer is B) because the documentation states that the ubiquity of power law distribution functions (PDFs) with power law tails is explained by the fact that nonlinear self-excited Hawkes processes have fast-accelerating intensities as a function of \"tension\". This is a key finding of the study, and it provides a novel mechanism for understanding the ubiquity of power laws. The other options are incorrect because they do not accurately summarize the underlying mechanism explained in the documentation. Option A is incorrect because it mentions long-range correlations, which are not mentioned in the documentation. Option C is incorrect because it mentions a \"critical point\", which is not explicitly mentioned in the documentation. Option D is incorrect because it mentions the Arrhenius activation process, but it is not the underlying mechanism that explains the ubiquity of power laws."}, "19": {"documentation": {"title": "Inside the Mind of a Stock Market Crash", "source": "Stefano Giglio, Matteo Maggiori, Johannes Stroebel, Stephen Utkus", "docs_id": "2004.01831", "section": ["econ.GN", "q-fin.EC", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inside the Mind of a Stock Market Crash. We analyze how investor expectations about economic growth and stock returns changed during the February-March 2020 stock market crash induced by the COVID-19 pandemic, as well as during the subsequent partial stock market recovery. We surveyed retail investors who are clients of Vanguard at three points in time: (i) on February 11-12, around the all-time stock market high, (ii) on March 11-12, after the stock market had collapsed by over 20\\%, and (iii) on April 16-17, after the market had rallied 25\\% from its lowest point. Following the crash, the average investor turned more pessimistic about the short-run performance of both the stock market and the real economy. Investors also perceived higher probabilities of both further extreme stock market declines and large declines in short-run real economic activity. In contrast, investor expectations about long-run (10-year) economic and stock market outcomes remained largely unchanged, and, if anything, improved. Disagreement among investors about economic and stock market outcomes also increased substantially following the stock market crash, with the disagreement persisting through the partial market recovery. Those respondents who were the most optimistic in February saw the largest decline in expectations, and sold the most equity. Those respondents who were the most pessimistic in February largely left their portfolios unchanged during and after the crash."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the changes in investor expectations about economic growth and stock returns during the February-March 2020 stock market crash and the subsequent partial recovery, as described in the study \"Inside the Mind of a Stock Market Crash\". How did the level of disagreement among investors about economic and stock market outcomes change over time, and what were the implications for investor behavior?\n\n**A)** The level of disagreement among investors increased substantially following the stock market crash, but decreased over time as investors became more optimistic about long-run economic and stock market outcomes. This led to a decrease in the number of investors who sold their equity during the crash.\n\n**B)** The level of disagreement among investors remained largely unchanged during the crash and the subsequent recovery, but increased among those who were the most optimistic in February. This led to a decrease in the number of investors who sold their equity during the crash.\n\n**C)** The level of disagreement among investors increased substantially following the stock market crash, and persisted through the partial market recovery. This led to a decrease in the number of investors who sold their equity during the crash, as those who were the most pessimistic in February largely left their portfolios unchanged.\n\n**D)** The level of disagreement among investors decreased substantially following the stock market crash, as investors became more optimistic about long-run economic and stock market outcomes. This led to an increase in the number of investors who sold their equity during the crash.\n\n**Correct Answer:** C) The level of disagreement among investors increased substantially following the stock market crash, and persisted through the partial market recovery. This led to a decrease in the number of investors who sold their equity during the crash, as those who were the most pessimistic in February largely left their portfolios unchanged.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"Disagreement among investors about economic and stock market outcomes also increased substantially following the stock market crash, with the disagreement persisting through the partial market recovery.\" Additionally, the documentation states that \"Those respondents who were the most optimistic in February saw the largest decline in expectations, and sold the most equity.\" This suggests that the level of disagreement among investors increased following the crash, and that those who were the most optimistic in February were more likely to sell their equity. The fact that those who were the most pessimistic in February largely left their portfolios unchanged also supports the idea that the level of disagreement persisted through the recovery."}, "20": {"documentation": {"title": "Estimation of Building Rooftop Temperature from High Spatial Resolution\n  Aerial Thermal Images", "source": "Atma Bharathi Mani, Ramanathan Sugumaran", "docs_id": "2001.09191", "section": ["eess.IV", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimation of Building Rooftop Temperature from High Spatial Resolution\n  Aerial Thermal Images. This letter presents a novel technique to calculate temperatures of building rooftops and other impervious surfaces from high spatial resolution aerial thermal images. In this study, we collected aerial radiance images of 30cm spatial resolution using a FLIR Phoenix imager in long-wave and mid-wave infrared wavelengths for the city of Cedar Falls, USA to estimate building roof temperature loss. Simultaneous ground temperature measurements were made at pre-selected ground targets and roofs using 9 Fluke 561r infrared thermometers. Atmospheric correction of aerial images was performed by Empirical Line Calibration (ELC) method. The resulting ground-leaving radiances were corrected for the emissivity of different roof types and the true kinetic temperature of the building roofs was calculated. The ELC model was observed to perform better when only impervious surface targets were used for the regression. With an R2=0.71 for ELC, the method produced a root mean squared error of 0.74{\\deg}C for asphalt roofs. Further, we observed that the microclimate plays a significant role while synchronizing aerial and ground measurements."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the Empirical Line Calibration (ELC) method used for atmospheric correction of aerial thermal images in estimating building rooftop temperatures?\n\nA) It requires high spatial resolution images to accurately estimate building roof temperatures.\nB) It is only effective for estimating temperatures of impervious surfaces, but not for vegetation-covered surfaces.\nC) It performs better when only impervious surface targets are used for the regression, but not when vegetation-covered surfaces are used.\nD) It is not suitable for estimating temperatures in areas with high levels of atmospheric pollution.\n\n**Correct Answer:** C) It performs better when only impervious surface targets are used for the regression, but not when vegetation-covered surfaces are used.\n\n**Explanation:** The correct answer is C) because the study states that the ELC model \"was observed to perform better when only impervious surface targets were used for the regression\". This suggests that the method is not effective for estimating temperatures of vegetation-covered surfaces, which is a limitation of the technique. The other options are incorrect because they do not accurately reflect the limitations of the ELC method as described in the study."}, "21": {"documentation": {"title": "Regularization matrices for discrete ill-posed problems in several\n  space-dimensions", "source": "Laura Dykes, Guangxin Huang, Silvia Noschese, Lothar Reichel", "docs_id": "1705.06489", "section": ["math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Regularization matrices for discrete ill-posed problems in several\n  space-dimensions. Many applications in science and engineering require the solution of large linear discrete ill-posed problems that are obtained by the discretization of a Fredholm integral equation of the first kind in several space-dimensions. The matrix that defines these problems is very ill-conditioned and generally numerically singular, and the right-hand side, which represents measured data, typically is contaminated by measurement error. Straightforward solution of these problems generally is not meaningful due to severe error propagation. Tikhonov regularization seeks to alleviate this difficulty by replacing the given linear discrete ill-posed problem by a penalized least-squares problem, whose solution is less sensitive to the error in the right-hand side and to round-off errors introduced during the computations. This paper discusses the construction of penalty terms that are determined by solving a matrix-nearness problem. These penalty terms allow partial transformation to standard form of Tikhonov regularization problems that stem from the discretization of integral equations on a cube in several space-dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a linear discrete ill-posed problem obtained by discretizing a Fredholm integral equation of the first kind in three space-dimensions. The matrix defining this problem is ill-conditioned and numerically singular, and the right-hand side is contaminated by measurement error. A Tikhonov regularization approach is used to alleviate this difficulty.\n\n**A)** The penalty term for this problem can be constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the condition number of the original matrix.\n\n**B)** The penalty term for this problem can be constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the Frobenius norm of the original matrix.\n\n**C)** The penalty term for this problem can be constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the spectral radius of the original matrix.\n\n**D)** The penalty term for this problem can be constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the determinant of the original matrix.\n\n**Correct Answer:** A) The penalty term for this problem can be constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the condition number of the original matrix.\n\n**Explanation:** The correct answer is A) because the penalty term for Tikhonov regularization problems is constructed by solving a matrix-nearness problem, where the goal is to find a matrix that minimizes the condition number of the original matrix. This is stated in the provided documentation: \"These penalty terms allow partial transformation to standard form of Tikhonov regularization problems that stem from the discretization of integral equations on a cube in several space-dimensions.\" The condition number of a matrix is a measure of its ill-conditioning, and minimizing it is a key step in the construction of the penalty term."}, "22": {"documentation": {"title": "The spatial meaning of Pareto's scaling exponent of city-size\n  distribution", "source": "Yanguang Chen", "docs_id": "1309.4862", "section": ["physics.soc-ph", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The spatial meaning of Pareto's scaling exponent of city-size\n  distribution. The scaling exponent of a hierarchy of cities used to be regarded as a fractal parameter. The Pareto exponent was treated as the fractal dimension of size distribution of cities, while the Zipf exponent was treated as the reciprocal of the fractal dimension. However, this viewpoint is not exact. In this paper, I will present a new interpretation of the scaling exponent of rank-size distributions. The ideas from fractal measure relation and the principle of dimension consistency are employed to explore the essence of Pareto's and Zipf's scaling exponents. The Pareto exponent proved to be a ratio of the fractal dimension of a network of cities to the average dimension of city population. Accordingly, the Zipf exponent is the reciprocal of this dimension ratio. On a digital map, the Pareto exponent can be defined by the scaling relation between a map scale and the corresponding number of cities based on this scale. The cities of the United States of America in 1900, 1940, 1960, and 1980 and Indian cities in 1981, 1991, and 2001 are utilized to illustrate the geographical spatial meaning of Pareto's exponent. The results suggest that the Pareto exponent of city-size distribution is not a fractal dimension, but a ratio of the urban network dimension to the city population dimension. This conclusion is revealing for scientists to understand Zipf's law and fractal structure of hierarchy of cities."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the correct interpretation of Pareto's scaling exponent of city-size distribution, and how does it relate to the fractal dimension of a network of cities?\n\nA) It is a fractal dimension of the size distribution of cities, and its reciprocal is the Zipf exponent.\nB) It is a ratio of the fractal dimension of a network of cities to the average dimension of city population.\nC) It is a measure of the average city size, and its value is inversely proportional to the number of cities.\nD) It is a fractal dimension of the city population, and its value is directly proportional to the number of cities.\n\nCorrect Answer: B) It is a ratio of the fractal dimension of a network of cities to the average dimension of city population.\n\nExplanation: According to the paper, Pareto's scaling exponent is not a fractal dimension, but a ratio of the urban network dimension to the city population dimension. This means that the correct answer is option B, which accurately reflects this interpretation. Options A, C, and D are incorrect because they either misinterpret Pareto's exponent or confuse it with other concepts."}, "23": {"documentation": {"title": "Adaptive virtual element methods with equilibrated fluxes", "source": "Franco Dassi, Joscha Gedicke, Lorenzo Mascotto", "docs_id": "2004.11220", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive virtual element methods with equilibrated fluxes. We present an hp-adaptive virtual element method (VEM) based on the hypercircle method of Prager and Synge for the approximation of solutions to diffusion problems. We introduce a reliable and efficient a posteriori error estimator, which is computed by solving an auxiliary global mixed problem. We show that the mixed VEM satisfies a discrete inf-sup condition, with inf-sup constant independent of the discretization parameters. Furthermore, we construct a stabilization for the mixed VEM, with explicit bounds in terms of the local degree of accuracy of the method. The theoretical results are supported by several numerical experiments, including a comparison with the residual a posteriori error estimator. The numerics exhibit the p-robustness of the proposed error estimator. In addition, we provide a first step towards the localized flux reconstruction in the virtual element framework, which leads to an additional reliable a posteriori error estimator that is computed by solving local (cheap-to-solve and parallelizable) mixed problems. We provide theoretical and numerical evidence that the proposed local error estimator suffers from a lack of efficiency."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the proposed stabilization for the mixed Virtual Element Method (VEM) in the context of diffusion problems?\n\nA) It provides a direct bound on the error in terms of the local degree of accuracy of the method.\nB) It ensures the stability of the mixed VEM without any additional computational cost.\nC) It allows for the efficient computation of the a posteriori error estimator using local mixed problems.\nD) It guarantees the p-robustness of the proposed error estimator, independent of the discretization parameters.\n\nCorrect Answer: C) It allows for the efficient computation of the a posteriori error estimator using local mixed problems.\n\nExplanation: The correct answer is C) because the proposed stabilization enables the efficient computation of the a posteriori error estimator using local mixed problems, which is a key advantage of the method. The other options are incorrect because:\n\n* A) The stabilization does not provide a direct bound on the error in terms of the local degree of accuracy of the method.\n* B) The stabilization does not ensure the stability of the mixed VEM without any additional computational cost.\n* D) While the proposed error estimator is p-robust, this is not the primary advantage of the stabilization."}, "24": {"documentation": {"title": "Approximation algorithms for nonbinary agreement forests", "source": "Leo van Iersel, Steven Kelk, Nela Leki\\'c and Leen Stougie", "docs_id": "1210.3211", "section": ["math.CO", "cs.DM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approximation algorithms for nonbinary agreement forests. Given two rooted phylogenetic trees on the same set of taxa X, the Maximum Agreement Forest problem (MAF) asks to find a forest that is, in a certain sense, common to both trees and has a minimum number of components. The Maximum Acyclic Agreement Forest problem (MAAF) has the additional restriction that the components of the forest cannot have conflicting ancestral relations in the input trees. There has been considerable interest in the special cases of these problems in which the input trees are required to be binary. However, in practice, phylogenetic trees are rarely binary, due to uncertainty about the precise order of speciation events. Here, we show that the general, nonbinary version of MAF has a polynomial-time 4-approximation and a fixed-parameter tractable (exact) algorithm that runs in O(4^k poly(n)) time, where n = |X| and k is the number of components of the agreement forest minus one. Moreover, we show that a c-approximation algorithm for nonbinary MAF and a d-approximation algorithm for the classical problem Directed Feedback Vertex Set (DFVS) can be combined to yield a d(c+3)-approximation for nonbinary MAAF. The algorithms for MAF have been implemented and made publicly available."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the time complexity of the fixed-parameter tractable (exact) algorithm for the Maximum Acyclic Agreement Forest problem (MAAF) on nonbinary phylogenetic trees, where n = |X| and k is the number of components of the agreement forest minus one?\n\n**A)** O(2^k poly(n))\n**B)** O(4^k poly(n))\n**C)** O(k^2 poly(n))\n**D)** O(n^3 poly(k))\n\n**Correct Answer:** B) O(4^k poly(n))\n\n**Explanation:** The problem statement mentions that the fixed-parameter tractable (exact) algorithm for MAF has a time complexity of O(4^k poly(n)), where n = |X| and k is the number of components of the agreement forest minus one. This is because the algorithm is designed to handle the additional restriction of nonbinary trees, which requires a more complex approach to find the optimal agreement forest. The 4^k term represents the exponential growth of the number of possible agreement forests, while poly(n) represents the polynomial growth of the input size n.\n\nThe other options are incorrect because:\n\n* A) O(2^k poly(n)) is too optimistic, as the algorithm needs to handle the additional complexity of nonbinary trees.\n* C) O(k^2 poly(n)) is incorrect, as the time complexity is not quadratic in k.\n* D) O(n^3 poly(k)) is incorrect, as the time complexity is not cubic in n."}, "25": {"documentation": {"title": "An Artificial Neural Network Approach to the Solution of Molecular\n  Chemical Equilibrium", "source": "A. Asensio Ramos (1), H. Socas-Navarro (2) ((1) INAF-Osservatorio\n  Astrofisico di Arcetri, (2) High Altitude Observatory, NCAR)", "docs_id": "astro-ph/0505322", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Artificial Neural Network Approach to the Solution of Molecular\n  Chemical Equilibrium. A novel approach is presented for the solution of instantaneous chemical equilibrium problems. The chemical equilibrium can be considered, due to its intrinsically local character, as a mapping of the three-dimensional parameter space spanned by the temperature, hydrogen density and electron density into many one-dimensional spaces representing the number density of each species. We take advantage of the ability of artificial neural networks to approximate non-linear functions and construct neural networks for the fast and efficient solution of the chemical equilibrium problem in typical stellar atmosphere physical conditions. The neural network approach has the advantage of providing an analytic function, which can be rapidly evaluated. The networks are trained with a learning set (that covers the entire parameter space) until a relative error below 1% is reached. It has been verified that the networks are not overtrained by using an additional verification set. The networks are then applied to a snapshot of realistic three-dimensional convection simulations of the solar atmosphere showing good generalization properties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** An Artificial Neural Network Approach to the Solution of Molecular Chemical Equilibrium\n\n**A)** The authors of the paper propose using artificial neural networks to solve chemical equilibrium problems in stellar atmospheres because of their ability to approximate non-linear functions.\n\n**B)** The neural network approach is limited to solving chemical equilibrium problems in one-dimensional spaces, whereas the authors' approach maps the three-dimensional parameter space into many one-dimensional spaces.\n\n**C)** The neural networks are trained on a learning set that covers the entire parameter space, and the networks are verified to be not overtrained using an additional verification set.\n\n**D)** The authors claim that the neural network approach provides an analytic function that can be rapidly evaluated, but this approach is not suitable for solving chemical equilibrium problems in complex systems.\n\n**Correct Answer:** C) The neural networks are trained on a learning set that covers the entire parameter space, and the networks are verified to be not overtrained using an additional verification set.\n\n**Explanation:** This question requires the test-taker to carefully read the documentation and understand the key points of the paper. The correct answer is supported by the text, which states that the neural networks are trained on a learning set that covers the entire parameter space and are verified to be not overtrained using an additional verification set. The other options are incorrect because they either oversimplify or misrepresent the authors' approach. Option A is too vague, option B is incorrect because the authors' approach maps the three-dimensional parameter space into many one-dimensional spaces, and option D is incorrect because the authors do claim that the neural network approach provides an analytic function that can be rapidly evaluated."}, "26": {"documentation": {"title": "A Probabilistic Approach to Knowledge Translation", "source": "Shangpu Jiang, Daniel Lowd, Dejing Dou", "docs_id": "1507.03181", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Probabilistic Approach to Knowledge Translation. In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \"knowledge translation\" (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of knowledge translation (KT), what is the primary goal of using standard learning and inference algorithms for probabilistic graphical models in the KT framework?\n\n**A)** To minimize the Kullback-Leibler divergence between the implicit distribution and the target schema's knowledge.\n\n**B)** To find an explicit probability distribution in the target schema that maximizes the Kullback-Leibler divergence from the implicit distribution.\n\n**C)** To represent the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema as probability distributions, specifically using Markov random fields and Markov logic networks.\n\n**D)** To obtain a compact probabilistic model that respects the uncertainty in both the source knowledge and the mapping, while minimizing the Kullback-Leibler divergence.\n\n**Correct Answer:** D) To obtain a compact probabilistic model that respects the uncertainty in both the source knowledge and the mapping, while minimizing the Kullback-Leibler divergence.\n\n**Explanation:** The correct answer is D) because the KT framework aims to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution, while also respecting the uncertainty in both the source knowledge and the mapping. This is achieved by using standard learning and inference algorithms for probabilistic graphical models. The other options are incorrect because they either focus on the representation of knowledge as probability distributions (C), or incorrectly state the goal of minimizing the Kullback-Leibler divergence (A and B)."}, "27": {"documentation": {"title": "Entanglement Entropy for 2D Gauge Theories with Matters", "source": "Sinya Aoki, Norihiro Iizuka, Kotaro Tamaoka, Tsuyoshi Yokoya", "docs_id": "1705.01549", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entanglement Entropy for 2D Gauge Theories with Matters. We investigate the entanglement entropy in 1+1-dimensional $SU(N)$ gauge theories with various matter fields using the lattice regularization. Here we use extended Hilbert space definition for entanglement entropy, which contains three contributions; (1) classical Shannon entropy associated with superselection sector distribution, where sectors are labelled by irreducible representations of boundary penetrating fluxes, (2) logarithm of the dimensions of their representations, which is associated with \"color entanglement\", and (3) EPR Bell pairs, which give \"genuine\" entanglement. We explicitly show that entanglement entropies (1) and (2) above indeed appear for various multiple \"meson\" states in gauge theories with matter fields. Furthermore, we employ transfer matrix formalism for gauge theory with fundamental matter field and analyze its ground state using hopping parameter expansion (HPE), where the hopping parameter $K$ is roughly the inverse square of the mass for the matter. We evaluate the entanglement entropy for the ground state and show that all (1), (2), (3) above appear in the HPE, though the Bell pair part (3) appears in higher order than (1) and (2) do. With these results, we discuss how the ground state entanglement entropy in the continuum limit can be understood from the lattice ground state obtained in the HPE."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of 1+1-dimensional $SU(N)$ gauge theories with matter fields, what is the relationship between the entanglement entropy and the dimensions of the irreducible representations of boundary penetrating fluxes, and how does this relationship manifest in the lattice regularization formalism?\n\nA) The entanglement entropy is directly proportional to the logarithm of the dimensions of the irreducible representations, and this relationship is a fundamental aspect of the lattice regularization formalism.\n\nB) The entanglement entropy is inversely proportional to the logarithm of the dimensions of the irreducible representations, and this relationship is a consequence of the transfer matrix formalism.\n\nC) The entanglement entropy is a measure of the \"color entanglement\" between the gauge fields and the matter fields, and it is related to the dimensions of the irreducible representations of boundary penetrating fluxes through the EPR Bell pairs.\n\nD) The entanglement entropy is a measure of the \"classical Shannon entropy\" associated with the superselection sector distribution, and it is related to the dimensions of the irreducible representations of boundary penetrating fluxes through the lattice regularization formalism.\n\nCorrect Answer: D) The entanglement entropy is a measure of the \"classical Shannon entropy\" associated with the superselection sector distribution, and it is related to the dimensions of the irreducible representations of boundary penetrating fluxes through the lattice regularization formalism.\n\nExplanation: The correct answer is D) because the entanglement entropy is indeed a measure of the classical Shannon entropy associated with the superselection sector distribution, and it is related to the dimensions of the irreducible representations of boundary penetrating fluxes through the lattice regularization formalism. The other options are incorrect because they either misrepresent the relationship between the entanglement entropy and the dimensions of the irreducible representations, or they introduce concepts that are not relevant to the lattice regularization formalism."}, "28": {"documentation": {"title": "Detection of mixed-culture growth in the total biomass data by wavelet\n  transforms", "source": "H.C. Rosu, J.S. Murguia, V. Ibarra-Junquera", "docs_id": "1012.1894", "section": ["q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detection of mixed-culture growth in the total biomass data by wavelet\n  transforms. We have shown elsewhere that the presence of mixed-culture growth of microbial species in fermentation processes can be detected with high accuracy by employing the wavelet transform. This is achieved because the crosses in the different growth processes contributing to the total biomass signal appear as singularities that are very well evidenced through their singularity cones in the wavelet transform. However, we used very simple two-species cases. In this work, we extend the wavelet method to a more complicated illustrative fermentation case of three microbial species for which we employ several wavelets of different number of vanishing moments in order to eliminate possible numerical artifacts. Working in this way allows to filter in a more precise way the numerical values of the H\\\"older exponents. Therefore, we were able to determine the characteristic H\\\"older exponents for the corresponding crossing singularities of the microbial growth processes and their stability logarithmic scale ranges up to the first decimal in the value of the characteristic exponents. Since calibrating the mixed microbial growth by means of their H\\\"older exponents could have potential industrial applications, the dependence of the H\\\"older exponents on the kinetic and physical parameters of the growth models remains as a future experimental task"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Detection of mixed-culture growth in the total biomass data by wavelet transforms\" employed wavelet transforms to detect mixed-culture growth in fermentation processes. What is the primary advantage of using wavelet transforms in this context, and how does it relate to the detection of singularities in the growth processes?\n\n**A)** Wavelet transforms are more sensitive to noise than traditional methods, allowing for more accurate detection of singularities.\n**B)** Wavelet transforms can filter out numerical artifacts, enabling the precise determination of H\\\"older exponents.\n**C)** Wavelet transforms are more computationally efficient than other methods, allowing for faster analysis of large datasets.\n**D)** Wavelet transforms are more interpretable than other methods, providing a clearer understanding of the underlying growth processes.\n\n**Correct answer:** B) Wavelet transforms can filter out numerical artifacts, enabling the precise determination of H\\\"older exponents.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that using wavelets with different numbers of vanishing moments allows for a more precise filtering of numerical values, which is essential for determining the characteristic H\\\"older exponents. This is a key advantage of using wavelet transforms in this context. The other options are incorrect because they do not accurately reflect the primary advantage of using wavelet transforms in this specific application."}, "29": {"documentation": {"title": "Cross-Cultural and Cultural-Specific Production and Perception of Facial\n  Expressions of Emotion in the Wild", "source": "Ramprakash Srinivasan, Aleix M. Martinez", "docs_id": "1808.04399", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross-Cultural and Cultural-Specific Production and Perception of Facial\n  Expressions of Emotion in the Wild. Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different, e.g., 17 expressions transmit happiness, but only 1 is used to convey disgust."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** According to the study on Cross-Cultural and Cultural-Specific Production and Perception of Facial Expressions of Emotion in the Wild, what is the main finding regarding the number of facial expressions used to communicate each emotion across cultures?\n\n**A)** The number of expressions used to communicate each emotion is consistent across all cultures.\n**B)** The number of expressions used to communicate each emotion varies significantly across cultures.\n**C)** Only 8 facial expressions are used to communicate each emotion across a smaller number of cultures.\n**D)** Only 35 facial expressions are used to transmit emotive information across all cultures.\n\n**Correct Answer:** D) Only 35 facial expressions are used to transmit emotive information across all cultures.\n\n**Explanation:** The study found that out of 16,384 possible facial configurations, only 35 are successfully used to transmit emotive information across cultures. This suggests that the number of expressions used to communicate each emotion varies significantly across cultures, and not all emotions are conveyed using the same number of expressions."}, "30": {"documentation": {"title": "Measurements of the induced polarization in the quasi-elastic\n  $A(e,e'\\vec p\\,)$ process in non-coplanar kinematics", "source": "Sebouh J. Paul, Tim Kolar, Tilen Brecelj, Patrick Achenbach, Hartmuth\n  Arenh\\\"ovel, Adi Ashkenazi, Jure Beri\\v{c}i\\v{c}, Ralph B\\\"ohm, Damir Bosnar,\n  Ethan Cline, Erez O. Cohen, Luka Debenjak, Michael O. Distler, Anselm Esser,\n  Ivica Fri\\v{s}\\v{c}i\\'c, Ronald Gilman, Carlotta Giusti, Matthias Heilig,\n  Matthias Hoek, David Izraeli, Simon Kegel, Pascal Klag, Yvonne Kohl, Igor\n  Korover, Jechiel Lichtenstadt, Israel Mardor, Harald Merkel, Duncan G.\n  Middleton, Miha Mihovilovi\\v{c}, Julian M\\\"uller, Ulrich M\\\"uller, Mor\n  Olivenboim, Eliezer Piasetzky, Josef Pochodzalla, Guy Ron, Bj\\\"orn S.\n  Schlimme, Matthias Schoth, Florian Schulz, Concettina Sfienti, Simon\n  \\v{S}irca, Rouven Spreckels, Samo \\v{S}tajner, Steffen Strauch, Michaela\n  Thiel, Alexey Tyukin, Adrian Weber, Israel Yaron", "docs_id": "2008.04209", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurements of the induced polarization in the quasi-elastic\n  $A(e,e'\\vec p\\,)$ process in non-coplanar kinematics. We report measurements of the induced polarization $\\vec P$ of protons knocked out from $^2$H and $^{12}$C via the $A(e,e'\\vec p\\,)$ reaction. We have studied the dependence of $\\vec P$ on two kinematic variables: the missing momentum $p_{\\rm miss}$ and the \"off-coplanarity\" angle $\\phi_{pq}$ between the scattering and reaction planes. For the full 360$\\degree$ range in $\\phi_{pq}$, both the normal ($P_y$) and, for the first time, the transverse ($P_x$) components of the induced polarization were measured with respect to the coordinate system associated with the scattering plane. $P_x$ vanishes in coplanar kinematics, however in non-coplanar kinematics, it is on the same scale as $P_y$. We find that the dependence on $\\phi_{pq}$ is sine-like for $P_x$ and cosine-like for $P_y$. For carbon, the magnitude of the induced polarization is especially large when protons are knocked out from the $p_{3/2}$ shell at very small $p_{\\rm miss}$. For the deuteron, the induced polarization is near zero at small $|p_{\\rm miss}|$, and its magnitude increases with $|p_{\\rm miss}|$. For both nuclei such behavior is reproduced qualitatively by theoretical results, driven largely by the spin-orbit part of the final-state interactions. However, for both nuclei, sizeable discrepancies exist between experiment and theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The measurements of the induced polarization in the quasi-elastic $A(e,e'\\vec p\\,)$ process in non-coplanar kinematics reveal a dependence on two kinematic variables: the missing momentum $p_{\\rm miss}$ and the \"off-coplanarity\" angle $\\phi_{pq}$. Analyze the results for the deuteron and carbon nuclei, focusing on the behavior of the induced polarization components $P_x$ and $P_y$.\n\n**A)** The experimental results for the deuteron show that the magnitude of the induced polarization increases with $|p_{\\rm miss}|$, while for carbon, the magnitude is especially large when protons are knocked out from the $p_{3/2}$ shell at very small $p_{\\rm miss}$. This suggests that the spin-orbit part of the final-state interactions plays a significant role in the observed behavior.\n\n**B)** The measurements of the induced polarization components $P_x$ and $P_y$ reveal a sine-like dependence on $\\phi_{pq}$ for $P_x$ and a cosine-like dependence for $P_y$ for both the deuteron and carbon nuclei. This indicates that the reaction dynamics are influenced by the off-coplanarity angle.\n\n**C)** The experimental results for the deuteron and carbon nuclei show that the magnitude of the induced polarization is near zero at small $|p_{\\rm miss}|$, and increases with $|p_{\\rm miss}|$ for carbon. This suggests that the reaction is dominated by the spin-orbit part of the final-state interactions.\n\n**D)** The measurements of the induced polarization components $P_x$ and $P_y$ reveal a discrepancy between experiment and theory for both the deuteron and carbon nuclei, with sizeable discrepancies existing for both $P_x$ and $P_y$.\n\n**Correct Answer:** B) The measurements of the induced polarization components $P_x$ and $P_y$ reveal a sine-like dependence on $\\phi_{pq}$ for $P_x$ and a cosine-like dependence for $P_y$ for both the deuteron and carbon nuclei. This indicates that the reaction dynamics are influenced by the off-coplanarity angle.\n\n**Explanation:** The correct answer is B) because the measurements of the induced polarization components $P_x$ and $P_y$ reveal a sine-like dependence on $\\phi_{pq}$ for $P_x$ and a cosine-like dependence for $P_y$ for both the deuteron and carbon nuclei. This indicates that the reaction dynamics are influenced by the off-coplanarity angle, which is a key aspect of the non-coplanar kinematics. The other options do not accurately capture the experimental results and theoretical implications."}, "31": {"documentation": {"title": "Temporal Averaging LSTM-based Channel Estimation Scheme for IEEE 802.11p\n  Standard", "source": "Abdul Karim Gizzini, Marwa Chafii, Shahab Ehsanfar, Raed M. Shubair", "docs_id": "2106.04829", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal Averaging LSTM-based Channel Estimation Scheme for IEEE 802.11p\n  Standard. In vehicular communications, reliable channel estimation is critical for the system performance due to the doubly-dispersive nature of vehicular channels. IEEE 802.11p standard allocates insufficient pilots for accurate channel tracking. Consequently, conventional IEEE 802.11p estimators suffer from a considerable performance degradation, especially in high mobility scenarios. Recently, deep learning (DL) techniques have been employed for IEEE 802.11p channel estimation. Nevertheless, these methods suffer either from performance degradation in very high mobility scenarios or from large computational complexity. In this paper, these limitations are solved using a long short term memory (LSTM)-based estimation. The proposed estimator employs an LSTM unit to estimate the channel, followed by temporal averaging (TA) processing as a noise alleviation technique. Moreover, the noise mitigation ratio is determined analytically, thus validating the TA processing ability in improving the overall performance. Simulation results reveal the performance superiority of the proposed schemes compared to recently proposed DL-based estimators, while recording a significant reduction in the computational complexity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of conventional IEEE 802.11p estimators in vehicular communications, and how does the proposed LSTM-based estimation scheme address this limitation?\n\n**A)** Conventional estimators suffer from insufficient pilot allocation, leading to performance degradation in high mobility scenarios. The proposed scheme addresses this by employing a noise alleviation technique, but does not alleviate the pilot allocation issue.\n\n**B)** Conventional estimators suffer from a considerable performance degradation in high mobility scenarios due to the doubly-dispersive nature of vehicular channels. The proposed scheme addresses this by employing an LSTM unit to estimate the channel, which improves the overall performance.\n\n**C)** Conventional estimators suffer from large computational complexity, while the proposed scheme addresses this by employing a noise alleviation technique, but does not alleviate the pilot allocation issue.\n\n**D)** Conventional estimators suffer from insufficient pilot allocation, leading to performance degradation in high mobility scenarios. The proposed scheme addresses this by employing a noise alleviation technique, which improves the overall performance and reduces computational complexity.\n\n**Correct Answer:** D) Conventional estimators suffer from insufficient pilot allocation, leading to performance degradation in high mobility scenarios. The proposed scheme addresses this by employing a noise alleviation technique, which improves the overall performance and reduces computational complexity.\n\n**Explanation:** The correct answer is D) because the documentation states that conventional IEEE 802.11p estimators suffer from insufficient pilots for accurate channel tracking, leading to performance degradation in high mobility scenarios. The proposed LSTM-based estimation scheme addresses this limitation by employing a noise alleviation technique, which improves the overall performance and reduces computational complexity."}, "32": {"documentation": {"title": "A new spin on optimal portfolios and ecological equilibria", "source": "Jerome Garnier-Brun, Michael Benzaquen, Stefano Ciliberti,\n  Jean-Philippe Bouchaud", "docs_id": "2104.00668", "section": ["cond-mat.stat-mech", "cond-mat.dis-nn", "q-bio.PE", "q-fin.PM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new spin on optimal portfolios and ecological equilibria. We consider the classical problem of optimal portfolio construction with the constraint that no short position is allowed, or equivalently the valid equilibria of multispecies Lotka-Volterra equations with self-regulation in the special case where the interaction matrix is of unit rank, corresponding to species competing for a common resource. We compute the average number of solutions and show that its logarithm grows as $N^\\alpha$, where $N$ is the number of assets or species and $\\alpha \\leq 2/3$ depends on the interaction matrix distribution. We conjecture that the most likely number of solutions is much smaller and related to the typical sparsity $m(N)$ of the solutions, which we compute explicitly. We also find that the solution landscape is similar to that of spin-glasses, i.e. very different configurations are quasi-degenerate. Correspondingly, \"disorder chaos\" is also present in our problem. We discuss the consequence of such a property for portfolio construction and ecologies, and question the meaning of rational decisions when there is a very large number \"satisficing\" solutions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a portfolio construction problem with N assets, where the interaction matrix is of unit rank, representing species competing for a common resource. The average number of solutions to the multispecies Lotka-Volterra equations grows as $N^\\alpha$, where $\\alpha \\leq 2/3$. What is the most likely number of solutions, and how does it relate to the typical sparsity $m(N)$ of the solutions?\n\nA) The most likely number of solutions is $N^{2/3}$, and it is equal to the typical sparsity $m(N)$.\nB) The most likely number of solutions is $N^{1/3}$, and it is related to the typical sparsity $m(N)$ by a factor of 2.\nC) The most likely number of solutions is $N^{1/2}$, and it is equal to the typical sparsity $m(N)$.\nD) The most likely number of solutions is $N^{3/4}$, and it is related to the typical sparsity $m(N)$ by a factor of 3.\n\nCorrect Answer: A) The most likely number of solutions is $N^{2/3}$, and it is equal to the typical sparsity $m(N)$.\n\nExplanation: The correct answer is A) because the problem states that the average number of solutions grows as $N^\\alpha$, where $\\alpha \\leq 2/3$. Since the most likely number of solutions is related to the typical sparsity $m(N)$, and the problem mentions that the solution landscape is similar to that of spin-glasses, it is reasonable to assume that the most likely number of solutions is also related to the typical sparsity $m(N)$. The correct answer is also supported by the fact that the problem mentions that the most likely number of solutions is much smaller than the average number of solutions."}, "33": {"documentation": {"title": "Lyapunov analysis of multiscale dynamics: The slow bundle of the\n  two-scale Lorenz 96 model", "source": "Mallory Carlu, Francesco Ginelli, Valerio Lucarini and Antonio Politi", "docs_id": "1809.05065", "section": ["nlin.CD", "physics.ao-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lyapunov analysis of multiscale dynamics: The slow bundle of the\n  two-scale Lorenz 96 model. We investigate the geometrical structure of instabilities in the two-scales Lorenz 96 model through the prism of Lyapunov analysis. Our detailed study of the full spectrum of covariant Lyapunov vectors reveals the presence of a slow bundle in tangent space, composed by a set of vectors with a significant projection on the slow degrees of freedom; they correspond to the smallest (in absolute value) Lyapunov exponents and thereby to the longer time scales. We show that the dimension of the slow bundle is extensive in the number of both slow and fast degrees of freedom, and discuss its relationship with the results of a finite-size analysis of instabilities, supporting the conjecture that the slow-variable behavior is effectively determined by a non-trivial subset of degrees of freedom. More precisely, we show that the slow bundle corresponds to the Lyapunov spectrum region where fast and slow instability rates overlap, \"mixing\" their evolution into a set of vectors which simultaneously carry information on both scales. We suggest these results may pave the way for future applications to ensemble forecasting and data assimilations in weather and climate models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion of the study on the two-scale Lorenz 96 model, as revealed by the analysis of covariant Lyapunov vectors?\n\nA) The slow bundle in tangent space is composed of vectors with a significant projection on the fast degrees of freedom, corresponding to the largest Lyapunov exponents and the shortest time scales.\n\nB) The dimension of the slow bundle is extensive in the number of both slow and fast degrees of freedom, and it corresponds to the Lyapunov spectrum region where fast and slow instability rates overlap.\n\nC) The slow bundle in tangent space is a set of vectors with a significant projection on the slow degrees of freedom, corresponding to the smallest Lyapunov exponents and the longest time scales.\n\nD) The study reveals that the slow-variable behavior is effectively determined by a trivial subset of degrees of freedom, and the slow bundle has no significant impact on the overall dynamics.\n\nCorrect Answer: C) The slow bundle in tangent space is a set of vectors with a significant projection on the slow degrees of freedom, corresponding to the smallest Lyapunov exponents and the longest time scales.\n\nExplanation: The correct answer is C) because the study reveals that the slow bundle is composed of vectors with a significant projection on the slow degrees of freedom, which corresponds to the smallest Lyapunov exponents and the longest time scales. This is stated in the abstract of the study: \"We show that the dimension of the slow bundle is extensive in the number of both slow and fast degrees of freedom, and... they correspond to the smallest (in absolute value) Lyapunov exponents and thereby to the longer time scales.\""}, "34": {"documentation": {"title": "Dark Matter Decay to a Photon and a Neutrino: the Double Monochromatic\n  Smoking Gun Scenario", "source": "Cha\\\"imae El Aisati, Michael Gustafsson, Thomas Hambye, Tiziana Scarna", "docs_id": "1510.05008", "section": ["hep-ph", "astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dark Matter Decay to a Photon and a Neutrino: the Double Monochromatic\n  Smoking Gun Scenario. In the energy range from few TeV to 25 TeV, upper bounds on the dark matter decay rate into high energy monochromatic neutrinos have recently become comparable to those on monochromatic gamma-ray lines. This implies clear possibilities of a future double \"smoking-gun\" evidence for the dark matter particle, from the observation of both a gamma and a neutrino line at the same energy. In particular, we show that a scenario where both lines are induced from the same dark matter particle decay leads to correlations that can already be tested. We study this \"double monochromatic\" scenario by considering the complete list of lowest dimensional effective operators that could induce such a decay. Furthermore, we argue that, on top of lines from decays into two-body final states, three-body final states can also be highly relevant. In addition to producing a distinct hard photon spectrum, three-body final states also produce a line-like feature in the neutrino spectrum that can be searched for by neutrino telescopes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is a key implication of the recent upper bounds on the dark matter decay rate into high-energy monochromatic neutrinos being comparable to those on monochromatic gamma-ray lines in the energy range from few TeV to 25 TeV?\n\nA) The existence of dark matter is ruled out.\nB) The detection of dark matter decay products is more challenging.\nC) The possibility of a future double \"smoking-gun\" evidence for dark matter particles is enhanced.\nD) The energy range of dark matter decay products is limited to below 1 TeV.\n\nCorrect Answer: C) The possibility of a future double \"smoking-gun\" evidence for dark matter particles is enhanced.\n\nExplanation: The question requires the test-taker to understand the implications of the recent upper bounds on the dark matter decay rate into high-energy monochromatic neutrinos being comparable to those on monochromatic gamma-ray lines. The correct answer, C, is supported by the text, which states that \"this implies clear possibilities of a future double 'smoking-gun' evidence for the dark matter particle, from the observation of both a gamma and a neutrino line at the same energy.\" The other options are incorrect because they do not accurately reflect the implications of the upper bounds on the dark matter decay rate. Option A is incorrect because the existence of dark matter is not ruled out by the upper bounds. Option B is incorrect because the detection of dark matter decay products is not more challenging, but rather, the possibility of a future double \"smoking-gun\" evidence is enhanced. Option D is incorrect because the energy range of dark matter decay products is not limited to below 1 TeV, but rather, it is in the energy range from few TeV to 25 TeV."}, "35": {"documentation": {"title": "Sparse Channel Estimation and Hybrid Precoding Using Deep Learning for\n  Millimeter Wave Massive MIMO", "source": "Wenyan Ma, Chenhao Qi, Zaichen Zhang, Julian Cheng", "docs_id": "2002.06380", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sparse Channel Estimation and Hybrid Precoding Using Deep Learning for\n  Millimeter Wave Massive MIMO. Channel estimation and hybrid precoding are considered for multi-user millimeter wave massive multi-input multi-output system. A deep learning compressed sensing (DLCS) channel estimation scheme is proposed. The channel estimation neural network for the DLCS scheme is trained offline using simulated environments to predict the beamspace channel amplitude. Then the channel is reconstructed based on the obtained indices of dominant beamspace channel entries. A deep learning quantized phase (DLQP) hybrid precoder design method is developed after channel estimation. The training hybrid precoding neural network for the DLQP method is obtained offline considering the approximate phase quantization. Then the deployment hybrid precoding neural network (DHPNN) is obtained by replacing the approximate phase quantization with ideal phase quantization and the output of the DHPNN is the analog precoding vector. Finally, the analog precoding matrix is obtained by stacking the analog precoding vectors and the digital precoding matrix is calculated by zero-forcing. Simulation results demonstrate that the DLCS channel estimation scheme outperforms the existing schemes in terms of the normalized mean-squared error and the spectral efficiency, while the DLQP hybrid precoder design method has better spectral efficiency performance than other methods with low phase shifter resolution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Sparse Channel Estimation and Hybrid Precoding Using Deep Learning for Millimeter Wave Massive MIMO\", what is the primary advantage of using the Deep Learning Compressed Sensing (DLCS) channel estimation scheme over existing schemes?\n\n**A)** The DLCS scheme requires less computational resources to train the neural network.\n**B)** The DLCS scheme outperforms existing schemes in terms of the normalized mean-squared error and spectral efficiency.\n**C)** The DLCS scheme is more suitable for systems with limited phase shifter resolution.\n**D)** The DLCS scheme is only applicable to single-user systems.\n\n**Correct Answer:** B) The DLCS scheme outperforms existing schemes in terms of the normalized mean-squared error and spectral efficiency.\n\n**Explanation:** The correct answer is B) because the paper states that the DLCS channel estimation scheme \"outperforms the existing schemes in terms of the normalized mean-squared error and the spectral efficiency\". This indicates that the DLCS scheme has a significant advantage over existing schemes in terms of performance metrics.\n\nThe other options are incorrect because:\n\n* A) The paper does not mention the computational resources required to train the neural network, so this is not a valid advantage of the DLCS scheme.\n* C) The paper does not mention the suitability of the DLCS scheme for systems with limited phase shifter resolution, so this is not a valid advantage.\n* D) The paper does not limit the DLCS scheme to single-user systems, so this is not a valid restriction."}, "36": {"documentation": {"title": "Time your hedge with Deep Reinforcement Learning", "source": "Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay", "docs_id": "2009.14136", "section": ["q-fin.PM", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Time your hedge with Deep Reinforcement Learning. Can an asset manager plan the optimal timing for her/his hedging strategies given market conditions? The standard approach based on Markowitz or other more or less sophisticated financial rules aims to find the best portfolio allocation thanks to forecasted expected returns and risk but fails to fully relate market conditions to hedging strategies decision. In contrast, Deep Reinforcement Learning (DRL) can tackle this challenge by creating a dynamic dependency between market information and hedging strategies allocation decisions. In this paper, we present a realistic and augmented DRL framework that: (i) uses additional contextual information to decide an action, (ii) has a one period lag between observations and actions to account for one day lag turnover of common asset managers to rebalance their hedge, (iii) is fully tested in terms of stability and robustness thanks to a repetitive train test method called anchored walk forward training, similar in spirit to k fold cross validation for time series and (iv) allows managing leverage of our hedging strategy. Our experiment for an augmented asset manager interested in sizing and timing his hedges shows that our approach achieves superior returns and lower risk."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A financial institution is considering implementing a Deep Reinforcement Learning (DRL) framework to optimize its hedging strategies. The framework should take into account the following requirements:\n\nA) The DRL framework should be able to learn from historical market data and make decisions based on the current market conditions.\nB) The framework should be able to manage leverage in the hedging strategy, allowing the institution to take on more or less risk as needed.\nC) The framework should have a one-period lag between observations and actions to account for the daily rebalancing of the hedge portfolio.\nD) The framework should use additional contextual information to decide on the hedging strategy allocation, but not take into account any external factors beyond market conditions.\n\n**Correct Answer:** C) The framework should have a one-period lag between observations and actions to account for the daily rebalancing of the hedge portfolio.\n\n**Explanation:** The correct answer is C) because the paper states that the proposed DRL framework has a \"one period lag between observations and actions to account for one day lag turnover of common asset managers to rebalance their hedge\". This requirement is crucial in ensuring that the framework is realistic and aligned with the actual practices of asset managers. The other options are incorrect because they either omit or misstate important requirements of the framework. Option A is incorrect because the framework should not simply learn from historical data, but rather use current market conditions to inform its decisions. Option B is incorrect because while the framework should be able to manage leverage, this is not a requirement stated in the paper. Option D is incorrect because the framework should indeed take into account additional contextual information to inform its decisions, as stated in the paper."}, "37": {"documentation": {"title": "The Peculiar Radial Distribution of Multiple Populations in the massive\n  globular cluster M80", "source": "E. Dalessandro, M. Cadelano, E. Vesperini, M. Salaris, F. R. Ferraro,\n  B. Lanzoni, S. Raso, J. Hong, J. J. Webb, A. Zocchi", "docs_id": "1804.03222", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Peculiar Radial Distribution of Multiple Populations in the massive\n  globular cluster M80. We present a detailed analysis of the radial distribution of light-element multiple populations (LE-MPs) in the massive and dense globular cluster M80 based on the combination of UV and optical Hubble Space Telescope data. Surprisingly, we find that first generation stars (FG) are significantly more centrally concentrated than extreme second generation ones (SG) out to $\\sim 2.5 r_h$ from the cluster center. To understand the origin of such a peculiar behavior, we used a set of $N$-body simulations following the long-term dynamical evolution of LE-MPs. We find that, given the advanced dynamical state of the cluster, the observed difference does not depend on the primordial relative distributions of FG and SG stars. On the contrary, a difference of $\\sim 0.05-0.10 M_{\\odot}$ between the average masses of the two sub-populations is needed to account for the observed radial distributions. We argue that such a mass difference might be the result of the higher He abundance of SG stars (of the order of $\\Delta Y\\sim 0.05-0.06$) with respect to FG. Interestingly, we find that a similar He variation is necessary to reproduce the horizontal branch morphology of M80. These results demonstrate that differences in mass among LE-MPs, due to different He content, should be properly taken into account for a correct interpretation of their radial distribution, at least in dynamically evolved systems."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that explains the observed difference in the radial distribution of first generation (FG) and extreme second generation (SG) stars in the massive globular cluster M80?\n\nA) The primordial relative distributions of FG and SG stars\nB) The difference in He abundance between FG and SG stars\nC) The mass difference between FG and SG stars\nD) The dynamical state of the cluster\n\nCorrect Answer: C) The mass difference between FG and SG stars\n\nExplanation: The study found that a difference of $\\sim 0.05-0.10 M_{\\odot}$ between the average masses of the two sub-populations is needed to account for the observed radial distributions. This suggests that the mass difference between FG and SG stars is the primary factor explaining the observed difference in their radial distribution.\n\nNote: The other options are incorrect because:\n\n* A) The primordial relative distributions of FG and SG stars are not the primary factor, as the study found that the observed difference does not depend on these distributions.\n* B) While the difference in He abundance between FG and SG stars is related to the mass difference, it is not the primary factor explaining the observed difference in radial distribution.\n* D) The dynamical state of the cluster is not the primary factor, as the study found that the observed difference is independent of the cluster's dynamical state."}, "38": {"documentation": {"title": "Dragoon: Advanced Modelling of IP Geolocation by use of Latency\n  Measurements", "source": "Peter Hillmann and Lars Stiemert and Gabi Dreo Rodosek and Oliver Rose", "docs_id": "2006.16895", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dragoon: Advanced Modelling of IP Geolocation by use of Latency\n  Measurements. IP Geolocation is a key enabler for many areas of application like determination of an attack origin, targeted advertisement, and Content Delivery Networks. Although IP Geolocation is an ongoing field of research for over one decade, it is still a challenging task, whereas good results are only achieved by the use of active latency measurements. Nevertheless, an increased accuracy is needed to improve service quality. This paper presents an novel approach to find optimized Landmark positions which are used for active probing. Since a reasonable Landmark selection is important for a highly accurate localization service, the goal is to find Landmarks close to the target with respect to the infrastructure and hop count. Furthermore, we introduce a new approach of an adaptable and more accurate mathematical modelling of an improved geographical location estimation process. Current techniques provide less information about solving the Landmark problem as well as are using imprecise models. We demonstrate the usability of our approach in a real-world environment and analyse Geolocation for the first time in Europe. The combination of an optimized Landmark selection and advanced modulation results in an improved accuracy of IP Geolocation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed approach in the paper \"Dragoon: Advanced Modelling of IP Geolocation by use of Latency Measurements\"?\n\nA) To improve the accuracy of IP Geolocation using active latency measurements\nB) To develop a new method for determining the origin of attacks\nC) To optimize Landmark positions for active probing and improve geographical location estimation\nD) To analyze Geolocation in the United States\n\n**Correct Answer:** C) To optimize Landmark positions for active probing and improve geographical location estimation\n\n**Explanation:** The correct answer is C) To optimize Landmark positions for active probing and improve geographical location estimation. The paper states that the goal is to find optimized Landmark positions that are close to the target with respect to the infrastructure and hop count, which is essential for a highly accurate localization service. The proposed approach aims to improve the accuracy of IP Geolocation by optimizing Landmark selection and using advanced mathematical modeling.\n\n**Candidate A (Incorrect)**: While the paper does mention the importance of active latency measurements for achieving good results in IP Geolocation, the primary goal of the proposed approach is not to improve the accuracy of IP Geolocation using active latency measurements, but rather to optimize Landmark positions.\n\n**Candidate B (Incorrect)**: The paper does not mention determining the origin of attacks as a primary goal, although it does mention that IP Geolocation is a key enabler for this task.\n\n**Candidate D (Incorrect)**: The paper does not analyze Geolocation in the United States, but rather in Europe."}, "39": {"documentation": {"title": "System-size dependence of the viscous attenuation of anisotropic flow in\n  p+Pb and Pb+Pb collisions at LHC energies", "source": "Peifeng Liu (1,2) and Roy A. Lacey (1,2) ((1) Chemistry Dept. Stony\n  Brook University (2) Physics Dept., Stony Brook University, Stony Brook NY,\n  USA)", "docs_id": "1804.04618", "section": ["nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "System-size dependence of the viscous attenuation of anisotropic flow in\n  p+Pb and Pb+Pb collisions at LHC energies. The elliptic and triangular flow coefficients ($\\mathrm{v_n, \\, n=2,3}$) measured in Pb+Pb ($\\sqrt{s_{_{\\rm NN}}} = 2.76$ TeV) and p+Pb ($\\sqrt{s_{_{\\rm NN}}} = 5.02$ TeV) collisions, are studied as a function of initial-state eccentricity ($\\varepsilon_n$), and dimensionless size characterized by the cube root of the mid-rapidity charged hadron multiplicity density $\\mathrm{\\left< N_{ch} \\right>^{1/3}}$. The results indicate that the influence of eccentricity ($\\mathrm{v_n} \\propto \\varepsilon_n$) observed for large $\\mathrm{\\left< N_{ch} \\right>}$, is superseded by the effects of viscous attenuation for small $\\mathrm{\\left< N_{ch} \\right>}$, irrespective of the colliding species. Strikingly similar acoustic scaling patterns of exponential viscous modulation, with a damping rate proportional to $\\mathrm{n^2}$ and inversely proportional to the dimensionless size, are observed for the eccentricity-scaled coefficients for the two sets of colliding species. The resulting scaling parameters suggest that, contrary to current predilections, the patterns of viscous attenuation, as well as the specific shear viscosity $\\left<\\frac{\\eta}{s}(\\text{T})\\right>$ for the matter created in p+Pb and Pb+Pb collisions, are comparable."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: The authors of the study on the viscous attenuation of anisotropic flow in p+Pb and Pb+Pb collisions at LHC energies observe that the influence of initial-state eccentricity on the flow coefficients ($\\mathrm{v_n}$) is superseded by the effects of viscous attenuation for small dimensionless size ($\\mathrm{\\left< N_{ch} \\right>^{1/3}}$). However, the scaling patterns of exponential viscous modulation are strikingly similar for both colliding species.\n\nA) The authors conclude that the specific shear viscosity $\\left<\\frac{\\eta}{s}(\\text{T})\\right>$ for the matter created in p+Pb and Pb+Pb collisions is comparable, but the viscous attenuation patterns differ between the two species.\n\nB) The authors find that the dimensionless size ($\\mathrm{\\left< N_{ch} \\right>^{1/3}}$) is the dominant factor in determining the viscous attenuation, and that the influence of initial-state eccentricity is negligible for small size.\n\nC) The authors observe that the damping rate of viscous modulation is proportional to $\\mathrm{n^2}$, but the scaling parameters suggest that the viscous attenuation patterns are similar for both colliding species, regardless of the dimensionless size.\n\nD) The authors conclude that the patterns of viscous attenuation, as well as the specific shear viscosity $\\left<\\frac{\\eta}{s}(\\text{T})\\right>$ for the matter created in p+Pb and Pb+Pb collisions, are comparable, and that the influence of initial-state eccentricity is superseded by the effects of viscous attenuation for small dimensionless size.\n\nCorrect Answer: D) The authors conclude that the patterns of viscous attenuation, as well as the specific shear viscosity $\\left<\\frac{\\eta}{s}(\\text{T})\\right>$ for the matter created in p+Pb and Pb+Pb collisions, are comparable, and that the influence of initial-state eccentricity is superseded by the effects of viscous attenuation for small dimensionless size.\n\nExplanation: The correct answer is D) because the authors explicitly state that the patterns of viscous attenuation, as well as the specific shear viscosity $\\left<\\frac{\\eta}{s}(\\text{T})\\right>$ for the matter created in p+Pb and Pb+Pb collisions, are comparable. Additionally, the text states that the influence of initial-state eccentricity is superseded by the effects of viscous attenuation for small dimensionless size, which is also a key point in the correct answer."}, "40": {"documentation": {"title": "k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography", "source": "Eunju Cha, Eung Yeop Kim, and Jong Chul Ye", "docs_id": "1806.00806", "section": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography. Time-resolved angiography with interleaved stochastic trajectories (TWIST) has been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve highly accelerated acquisitions, TWIST combines the periphery of the k-space data from several adjacent frames to reconstruct one temporal frame. However, this view-sharing scheme limits the true temporal resolution of TWIST. Moreover, the k-space sampling patterns have been specially designed for a specific generalized autocalibrating partial parallel acquisition (GRAPPA) factor so that it is not possible to reduce the number of view-sharing once the k-data is acquired. To address these issues, this paper proposes a novel k-space deep learning approach for parallel MRI. In particular, we have designed our neural network so that accurate k-space interpolations are performed simultaneously for multiple coils by exploiting the redundancies along the coils and images. Reconstruction results using in vivo TWIST data set confirm that the proposed method can immediately generate high-quality reconstruction results with various choices of view- sharing, allowing us to exploit the trade-off between spatial and temporal resolution in time-resolved MR angiography."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of the TWIST (Time-resolved Angiography with Interleaved Stochastic Trajectories) method in achieving highly accelerated acquisitions for dynamic contrast enhanced MRI (DCE-MRI), and how does the proposed k-space deep learning approach address this limitation?\n\nA) TWIST's view-sharing scheme limits the true temporal resolution of TWIST due to the need for specialized k-space sampling patterns designed for GRAPPA factors, making it impossible to reduce the number of view-sharing once the k-data is acquired.\n\nB) TWIST's view-sharing scheme limits the true spatial resolution of TWIST due to the need for specialized k-space sampling patterns designed for GRAPPA factors, making it impossible to reduce the number of view-sharing once the k-data is acquired.\n\nC) TWIST's view-sharing scheme limits the true temporal resolution of TWIST due to the need for accurate k-space interpolations, which are performed simultaneously for multiple coils by exploiting the redundancies along the coils and images.\n\nD) TWIST's view-sharing scheme limits the true spatial resolution of TWIST due to the need for accurate k-space interpolations, which are performed simultaneously for multiple coils by exploiting the redundancies along the coils and images.\n\nCorrect Answer: A) TWIST's view-sharing scheme limits the true temporal resolution of TWIST due to the need for specialized k-space sampling patterns designed for GRAPPA factors, making it impossible to reduce the number of view-sharing once the k-data is acquired.\n\nExplanation: The correct answer is A) because the documentation states that the TWIST method combines the periphery of the k-space data from several adjacent frames to reconstruct one temporal frame, but this view-sharing scheme limits the true temporal resolution of TWIST. Additionally, the k-space sampling patterns have been specially designed for a specific GRAPPA factor, making it impossible to reduce the number of view-sharing once the k-data is acquired. The proposed k-space deep learning approach addresses this limitation by performing accurate k-space interpolations simultaneously for multiple coils, exploiting the redundancies along the coils and images."}, "41": {"documentation": {"title": "Vibrational dynamics of solid poly(ethylene oxide)", "source": "M. Krishnan, S. Balasubramanian (Jawaharlal Nehru Centre for Advanced\n  Scientific Research, India)", "docs_id": "cond-mat/0303102", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vibrational dynamics of solid poly(ethylene oxide). Molecular dynamics (MD) simulations of crystalline poly(ethylene oxide) (PEO) have been carried out in order to study its vibrational properties. The vibrational density of states has been calculated using a normal mode analysis (NMA) and also through the velocity autocorrelation function of the atoms. Results agree well with experimental spectroscopic data. System size effects in the crystalline state, studied through a comparison between results for 16 unit cells and that for one unit cell has shown important differences in the features below 100 cm^-1. Effects of interchain interactions are examined by a comparison of the spectra in the condensed state to that obtained for an isolated oligomer of ethylene oxide. Calculations of the local character of the modes indicate the presence of collective excitations for frequencies lower than 100 cm^-1, in which around 8 to 12 successive atoms of the polymer backbone participate. The backbone twisting of helical chains about their long axes is dominant in these low frequency modes."}, "answer": "Here is the difficult multi-form question:\n\n**Question:** What can be inferred from the vibrational density of states calculations using normal mode analysis (NMA) and velocity autocorrelation function of the atoms in the molecular dynamics (MD) simulations of crystalline poly(ethylene oxide) (PEO)?\n\nA) The vibrational density of states is only dependent on the interchain interactions.\nB) The low-frequency modes (below 100 cm^-1) are localized around the polymer backbone and involve collective excitations of around 8 to 12 successive atoms.\nC) The system size effects are negligible in the crystalline state.\nD) The vibrational density of states is identical to the velocity autocorrelation function.\n\n**Correct Answer:** B) The low-frequency modes (below 100 cm^-1) are localized around the polymer backbone and involve collective excitations of around 8 to 12 successive atoms.\n\n**Explanation:** The question requires the test-taker to analyze the results of the vibrational density of states calculations and infer the correct answer. The correct answer is supported by the text, which states that \"Calculations of the local character of the modes indicate the presence of collective excitations for frequencies lower than 100 cm^-1, in which around 8 to 12 successive atoms of the polymer backbone participate.\" This indicates that the low-frequency modes are indeed localized around the polymer backbone and involve collective excitations of around 8 to 12 successive atoms. The other options are incorrect because they do not accurately reflect the results of the calculations. Option A is incorrect because the text does not suggest that the vibrational density of states is only dependent on interchain interactions. Option C is incorrect because the text states that system size effects are important, particularly below 100 cm^-1. Option D is incorrect because the text states that the vibrational density of states is calculated using NMA and velocity autocorrelation function, but it is not identical to the velocity autocorrelation function."}, "42": {"documentation": {"title": "Distributed Average Tracking for Double-integrator Multi-agent Systems\n  with Reduced Requirement on Velocity Measurements", "source": "Sheida Ghapani, Wei Ren, Fei Chen, Yongduan Song", "docs_id": "1507.04780", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributed Average Tracking for Double-integrator Multi-agent Systems\n  with Reduced Requirement on Velocity Measurements. This paper addresses distributed average tracking for a group of physical double-integrator agents under an undirected graph with reduced requirement on velocity measurements. The idea is that multiple agents track the average of multiple time-varying input signals, each of which is available to only one agent, under local interaction with neighbors. We consider two cases. First, a distributed discontinuous algorithm and filter are proposed, where each agent needs the relative positions between itself and its neighbors and its neighbors' filter outputs obtained through communication but the requirement for either absolute or relative velocity measurements is removed. The agents' positions and velocities must be initialized correctly, but the algorithm can deal with a wide class of input signals with bounded acceleration deviations. Second, a distributed discontinuous algorithm and filter are proposed to remove the requirement for communication and accurate initialization. Here each agent needs to measure the relative position between itself and its neighbors and its own velocity but the requirement for relative velocity measurements between itself and its neighbors is removed. The algorithm can deal with the case where the input signals and their velocities and accelerations are all bounded. Numerical simulations are also presented to illustrate the theoretical results."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a group of double-integrator agents under an undirected graph, where each agent tracks the average of multiple time-varying input signals. The agents communicate with their neighbors to obtain the relative positions and filter outputs, but the requirement for velocity measurements is reduced. In this scenario, what is the primary advantage of the proposed distributed discontinuous algorithm and filter, as presented in the paper?\n\n**A)** It eliminates the need for accurate initialization of agent positions and velocities.\n**B)** It allows the algorithm to track the average of multiple time-varying input signals with reduced requirement on velocity measurements.\n**C)** It enables the agents to track the average of multiple time-varying input signals without any communication with their neighbors.\n**D)** It provides a more robust tracking performance in the presence of bounded acceleration deviations.\n\n**Correct Answer:** B) It allows the algorithm to track the average of multiple time-varying input signals with reduced requirement on velocity measurements.\n\n**Explanation:** The correct answer is B) because the paper specifically addresses the reduction of the requirement on velocity measurements, allowing the algorithm to track the average of multiple time-varying input signals with reduced velocity requirements. The other options are incorrect because they either misrepresent the primary advantage of the algorithm (A and C) or are not supported by the paper (D)."}, "43": {"documentation": {"title": "The relative efficiency of time-to-progression and continuous measures\n  of cognition in pre-symptomatic Alzheimer's", "source": "Dan Li, Samuel Iddi, Paul S. Aisen, Wesley K. Thompson, Michael C.\n  Donohue", "docs_id": "1902.02026", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The relative efficiency of time-to-progression and continuous measures\n  of cognition in pre-symptomatic Alzheimer's. Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker evidence of fibrillar amyloid beta pathology in the absence of clinical symptoms. Clinical trials in this early phase of disease are challenging due to the slow rate of disease progression as measured by periodic cognitive performance tests or by transition to a diagnosis of Mild Cognitive Impairment. In a multisite study, experts provide diagnoses by central chart review without the benefit of in-person assessment. We use a simulation study to demonstrate that models of repeated cognitive assessments detect treatment effects more efficiently compared to models of time-to-progression to an endpoint such as change in diagnosis. Multivariate continuous data are simulated from a Bayesian joint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging Initiative. Simulated progression events are algorithmically derived from the continuous assessments using a random forest model fit to the same data. We find that power is approximately doubled with models of repeated continuous outcomes compared to the time-to-progression analysis. The simulations also demonstrate that a plausible informative missing data pattern can induce a bias which inflates treatment effects, yet 5% Type I error is maintained."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A researcher is designing a clinical trial for pre-symptomatic Alzheimer's disease. The trial aims to detect treatment effects on cognitive performance using repeated assessments. However, the disease progression is slow, and the trial is challenging due to the need to account for missing data.\n\n**A)** The researcher should use a time-to-progression analysis, which focuses on the time to reach a specific endpoint (e.g., diagnosis of Mild Cognitive Impairment), to detect treatment effects.\n\n**B)** The researcher should use a model of repeated continuous outcomes, which captures the changes in cognitive performance over time, to detect treatment effects. This approach is more efficient than time-to-progression analysis, but may be affected by informative missing data patterns.\n\n**C)** The researcher should use a Bayesian joint mixed effects model to simulate the data and estimate the treatment effects, as this approach can account for the complex relationships between cognitive performance and disease progression.\n\n**D)** The researcher should use a random forest model to predict the progression events, as this approach can handle non-linear relationships and interactions between variables.\n\n**Correct Answer:** B) The researcher should use a model of repeated continuous outcomes, which captures the changes in cognitive performance over time, to detect treatment effects. This approach is more efficient than time-to-progression analysis, but may be affected by informative missing data patterns.\n\n**Explanation:** The correct answer is B) because the question highlights the challenge of detecting treatment effects in pre-symptomatic Alzheimer's disease, where the disease progression is slow. The use of repeated continuous outcomes is more efficient than time-to-progression analysis, as it can capture the changes in cognitive performance over time. However, the question also notes that informative missing data patterns can inflate treatment effects, which is a potential limitation of this approach. The other options are incorrect because they do not address the specific challenge of detecting treatment effects in pre-symptomatic Alzheimer's disease. Option A is incorrect because time-to-progression analysis is not the most efficient approach in this scenario. Option C is incorrect because the Bayesian joint mixed effects model is not directly relevant to the question. Option D is incorrect because the random forest model is not the best approach for predicting progression events in this context."}, "44": {"documentation": {"title": "Volatility Depend on Market Trades and Macro Theory", "source": "Victor Olkhov", "docs_id": "2008.07907", "section": ["q-fin.ST", "q-fin.PM", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Volatility Depend on Market Trades and Macro Theory. This paper presents probability distributions for price and returns random processes for averaging time interval {\\Delta}. These probabilities determine properties of price and returns volatility. We define statistical moments for price and returns random processes as functions of the costs and the volumes of market trades aggregated during interval {\\Delta}. These sets of statistical moments determine characteristic functionals for price and returns probability distributions. Volatilities are described by first two statistical moments. Second statistical moments are described by functions of second degree of the cost and the volumes of market trades aggregated during interval {\\Delta}. We present price and returns volatilities as functions of number of trades and second degree costs and volumes of market trades aggregated during interval {\\Delta}. These expressions support numerous results on correlations between returns volatility, number of trades and the volume of market transactions. Forecasting the price and returns volatilities depend on modeling the second degree of the costs and the volumes of market trades aggregated during interval {\\Delta}. Second degree market trades impact second degree of macro variables and expectations. Description of the second degree market trades, macro variables and expectations doubles the complexity of the current macroeconomic and financial theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary relationship between the second statistical moments of price and returns random processes and the costs and volumes of market trades aggregated during interval \u0394?\n\nA) The second statistical moments are directly proportional to the costs and volumes of market trades.\nB) The second statistical moments are inversely proportional to the costs and volumes of market trades.\nC) The second statistical moments are functions of the second degree of the costs and the volumes of market trades aggregated during interval \u0394.\nD) The second statistical moments are independent of the costs and volumes of market trades aggregated during interval \u0394.\n\n**Correct Answer:** C) The second statistical moments are functions of the second degree of the costs and the volumes of market trades aggregated during interval \u0394.\n\n**Explanation:** According to the paper, the second statistical moments of price and returns random processes are described by functions of the second degree of the costs and the volumes of market trades aggregated during interval \u0394. This means that the relationship between the second statistical moments and the costs and volumes of market trades is not a simple linear or inverse relationship, but rather a more complex function that depends on the second degree of these variables."}, "45": {"documentation": {"title": "Fixation Probability for Competing Selective Sweeps", "source": "Feng Yu, Alison Etheridge, Charles Cuthbertson", "docs_id": "0812.0104", "section": ["math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fixation Probability for Competing Selective Sweeps. We consider a biological population in which a beneficial mutation is undergoing a selective sweep when a second beneficial mutation arises at a linked locus and we investigate the probability that both mutations will eventually fix in the population. Previous work has dealt with the case where the second mutation to arise confers a smaller benefit than the first. In that case population size plays almost no role. Here we consider the opposite case and observe that, by contrast, the probability of both mutations fixing can be heavily dependent on population size. Indeed the key parameter is $\\rho N$, the product of the population size and the recombination rate between the two selected loci. If $\\rho N$ is small, the probability that both mutations fix can be reduced through interference to almost zero while for large $\\rho N$ the mutations barely influence one another. The main rigorous result is a method for calculating the fixation probability of a double mutant in the large population limit."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Fixation Probability for Competing Selective Sweeps, what is the key parameter that determines the probability of both beneficial mutations fixing in a population, and how does it relate to the population size and recombination rate?\n\nA) $\\rho N$ is the product of the population size and the mutation rate between the two selected loci.\nB) $\\rho N$ is the product of the population size and the recombination rate between the two selected loci.\nC) $\\rho N$ is the product of the population size and the selection coefficient of the first mutation.\nD) $\\rho N$ is the product of the population size and the fitness of the second mutation.\n\n**Correct Answer:** B) $\\rho N$ is the product of the population size and the recombination rate between the two selected loci.\n\n**Explanation:** The correct answer is B) $\\rho N$ because the documentation states that the key parameter is $\\rho N$, which is the product of the population size and the recombination rate between the two selected loci. This parameter determines the probability of both mutations fixing in the population, and its value can significantly impact the outcome.\n\n**Explanation of incorrect options:**\n\nA) This option is incorrect because the mutation rate is not the key parameter in this context. The documentation mentions recombination rate, not mutation rate.\n\nC) This option is incorrect because the selection coefficient of the first mutation is not the key parameter. The documentation focuses on the recombination rate and population size.\n\nD) This option is incorrect because the fitness of the second mutation is not the key parameter. The documentation emphasizes the recombination rate and population size as the critical factors."}, "46": {"documentation": {"title": "Panel semiparametric quantile regression neural network for electricity\n  consumption forecasting", "source": "Xingcai Zhou and Jiangyan Wang", "docs_id": "2103.00711", "section": ["stat.ML", "cs.LG", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Panel semiparametric quantile regression neural network for electricity\n  consumption forecasting. China has made great achievements in electric power industry during the long-term deepening of reform and opening up. However, the complex regional economic, social and natural conditions, electricity resources are not evenly distributed, which accounts for the electricity deficiency in some regions of China. It is desirable to develop a robust electricity forecasting model. Motivated by which, we propose a Panel Semiparametric Quantile Regression Neural Network (PSQRNN) by utilizing the artificial neural network and semiparametric quantile regression. The PSQRNN can explore a potential linear and nonlinear relationships among the variables, interpret the unobserved provincial heterogeneity, and maintain the interpretability of parametric models simultaneously. And the PSQRNN is trained by combining the penalized quantile regression with LASSO, ridge regression and backpropagation algorithm. To evaluate the prediction accuracy, an empirical analysis is conducted to analyze the provincial electricity consumption from 1999 to 2018 in China based on three scenarios. From which, one finds that the PSQRNN model performs better for electricity consumption forecasting by considering the economic and climatic factors. Finally, the provincial electricity consumptions of the next $5$ years (2019-2023) in China are reported by forecasting."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the Panel Semiparametric Quantile Regression Neural Network (PSQRNN) for electricity consumption forecasting in China?\n\n**A)** To improve the interpretability of parametric models by incorporating artificial neural networks.\n**B)** To explore the potential linear and nonlinear relationships among variables and maintain the interpretability of parametric models simultaneously.\n**C)** To develop a robust electricity forecasting model that can handle complex regional economic, social, and natural conditions.\n**D)** To evaluate the prediction accuracy of electricity consumption forecasting models using empirical analysis.\n\n**Correct Answer:** B) To explore the potential linear and nonlinear relationships among variables and maintain the interpretability of parametric models simultaneously.\n\n**Explanation:** The correct answer is B) because the PSQRNN is motivated by the need to explore the potential linear and nonlinear relationships among variables, as well as to maintain the interpretability of parametric models simultaneously. This is stated in the introduction of the paper: \"Motivated by which, we propose a Panel Semiparametric Quantile Regression Neural Network (PSQRNN) by utilizing the artificial neural network and semiparametric quantile regression.\""}, "47": {"documentation": {"title": "Transformation of envelope solitons on a bottom step", "source": "G. Ducrozet, A.V. Slunyaev, Y.A. Stepanyants", "docs_id": "2104.11432", "section": ["nlin.PS", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transformation of envelope solitons on a bottom step. In this paper we study the transformation of surface envelope solitons travelling over a bottom step in water of a finite depth. Using the transformation coefficients earlier derived in the linear approximation, we find the parameters of transmitted pulses and subsequent evolution of the pulses in the course of propagation. Relying on the weakly nonlinear theory, the analytic formulae are derived which describe the maximum attainable wave amplitude in the neighbourhood of the step and in the far zone. Solitary waves may be greatly amplified (within the weakly nonlinear theory formally even without a limit) when propagating from relatively shallow water to the deeper domain due to the constructive interference between the newly emerging envelope solitons and the residual quasi-linear waves. The theoretical results are in a good agreement with the data of direct numerical modelling of soliton transformation. In particular, more than double wave amplification is demonstrated in the performed simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the transformation of surface envelope solitons on a bottom step, what is the primary mechanism responsible for the significant amplification of solitary waves when propagating from relatively shallow water to a deeper domain?\n\n**A)** Residual quasi-linear waves\n**B)** Constructive interference between newly emerging envelope solitons and residual quasi-linear waves\n**C)** Nonlinear effects due to the interaction with the bottom step\n**D)** The finite depth of the water itself\n\n**Correct Answer:** B) Constructive interference between newly emerging envelope solitons and residual quasi-linear waves\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"solitary waves may be greatly amplified (within the weakly nonlinear theory formally even without a limit) when propagating from relatively shallow water to the deeper domain due to the constructive interference between the newly emerging envelope solitons and the residual quasi-linear waves.\" This indicates that the amplification is a result of the interaction between the newly emerging envelope solitons and the residual quasi-linear waves, which is a key concept in the paper."}, "48": {"documentation": {"title": "Exact partial information decompositions for Gaussian systems based on\n  dependency constraints", "source": "James W. Kay and Robin A. A. Ince", "docs_id": "1803.02030", "section": ["cond-mat.stat-mech", "cs.IT", "math.IT", "physics.data-an", "q-bio.QM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exact partial information decompositions for Gaussian systems based on\n  dependency constraints. The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a theoretical framework to characterize and quantify the structure of multivariate information sharing. A new method (Idep) has recently been proposed for computing a two-predictor PID over discrete spaces. [arXiv:1709.06653] A lattice of maximum entropy probability models is constructed based on marginal dependency constraints, and the unique information that a particular predictor has about the target is defined as the minimum increase in joint predictor-target mutual information when that particular predictor-target marginal dependency is constrained. Here, we apply the Idep approach to Gaussian systems, for which the marginally constrained maximum entropy models are Gaussian graphical models. Closed form solutions for the Idep PID are derived for both univariate and multivariate Gaussian systems. Numerical and graphical illustrations are provided, together with practical and theoretical comparisons of the Idep PID with the minimum mutual information PID (Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method generally produces larger estimates of redundancy and synergy than does the Idep method. In discussion of the practical examples, the PIDs are complemented by the use of deviance tests for the comparison of Gaussian graphical models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key difference between the Partial Information Decomposition (PID) method and the Idep (Information Decomposition for Gaussian systems) method, and how does this difference impact the estimation of redundancy and synergy in Gaussian systems?\n\n**A)** The Idep method uses a lattice of maximum entropy probability models, while the PID method relies on a theoretical framework to characterize and quantify multivariate information sharing. The Idep method generally produces smaller estimates of redundancy and synergy than the PID method.\n\n**B)** The Idep method is based on marginal dependency constraints, while the PID method is based on a theoretical framework to characterize and quantify multivariate information sharing. The Idep method generally produces larger estimates of redundancy and synergy than the PID method.\n\n**C)** The Idep method is a numerical approach, while the PID method is a theoretical framework. The Idep method is more suitable for practical applications, while the PID method is more suitable for theoretical comparisons.\n\n**D)** The Idep method is a closed-form solution, while the PID method is a numerical approach. The Idep method is more computationally efficient than the PID method.\n\n**Correct Answer:** B) The Idep method is based on marginal dependency constraints, while the PID method is based on a theoretical framework to characterize and quantify multivariate information sharing. The Idep method generally produces larger estimates of redundancy and synergy than the PID method.\n\n**Explanation:** The correct answer is B) because the Idep method is based on marginal dependency constraints, which are used to construct a lattice of maximum entropy probability models. In contrast, the PID method is based on a theoretical framework to characterize and quantify multivariate information sharing. The Idep method generally produces larger estimates of redundancy and synergy than the PID method, as shown in the provided documentation."}, "49": {"documentation": {"title": "Chandra Survey of Radio-quiet, High-redshift Quasars", "source": "Jill Bechtold (University of Arizona), Aneta Siemiginowska (CFA),\n  Joseph Shields (Ohio University), Bozena Czerny, Agnieszka Janiuk (Copernicus\n  Center), Fred Hamann (University of Florida), Thomas L. Aldcroft, Martin\n  Elvis, Adam Dobrzycki (CFA)", "docs_id": "astro-ph/0204462", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chandra Survey of Radio-quiet, High-redshift Quasars. We observed 17 optically-selected, radio-quiet high-redshift quasars with the Chandra Observatory ACIS, and detected 16 of them. The quasars have redshift between 3.70 and 6.28 and include the highest redshift quasars known. When compared to low-redshift quasars observed with ROSAT, these high redshift quasars are significantly more X-ray quiet. We also find that the X-ray spectral index of the high redshift objects is flatter than the average at lower redshift. These trends confirm the predictions of models where the accretion flow is described by a cold, optically-thick accretion disk surrounded by a hot, optically thin corona, provided the viscosity parameter alpha >= 0.02. The high redshift quasars have supermassive black holes with masses ~10^{10} M_{sun}, and are accreting material at ~0.1 the Eddington limit. We detect 10 X-ray photons from the z=6.28 quasar SDS 1030+0524, which may have a Gunn-Peterson trough and be near the redshift of reionization of the intergalactic medium. The X-ray data place an upper limit on the optical depth of the intergalactic medium tau(IGM) < 10^6, compared to the lower limit from the spectrum of Lyalpha and Lybeta, which implies tau(IGM) > 20."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the accretion flow in high-redshift quasars, based on the observed X-ray spectral index and the predictions of models?\n\nA) The accretion flow is described by a hot, optically-thick disk surrounded by a cold, optically-thin corona.\nB) The accretion flow is described by a cold, optically-thick accretion disk surrounded by a hot, optically-thin corona, provided the viscosity parameter alpha >= 0.02.\nC) The accretion flow is described by a hot, optically-thin disk surrounded by a cold, optically-thick corona.\nD) The accretion flow is not well described by any model.\n\nCorrect Answer: B) The accretion flow is described by a cold, optically-thick accretion disk surrounded by a hot, optically-thin corona, provided the viscosity parameter alpha >= 0.02.\n\nExplanation: The question requires the test-taker to analyze the information provided in the Arxiv documentation and make an inference about the accretion flow in high-redshift quasars. The correct answer is supported by the text, which states that the X-ray spectral index of the high redshift objects is flatter than the average at lower redshift, and that models with a cold, optically-thick accretion disk surrounded by a hot, optically-thin corona provide a good description of these trends, provided the viscosity parameter alpha >= 0.02. The other options are incorrect because they either contradict the information provided in the text or are not supported by the text."}, "50": {"documentation": {"title": "Deep neural networks can predict mortality from 12-lead\n  electrocardiogram voltage data", "source": "Sushravya Raghunath, Alvaro E. Ulloa Cerna, Linyuan Jing, David P.\n  vanMaanen, Joshua Stough, Dustin N. Hartzel, Joseph B. Leader, H. Lester\n  Kirchner, Christopher W. Good, Aalpen A. Patel, Brian P. Delisle, Amro\n  Alsaid, Dominik Beer, Christopher M. Haggerty, Brandon K. Fornwalt", "docs_id": "1904.07032", "section": ["q-bio.QM", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep neural networks can predict mortality from 12-lead\n  electrocardiogram voltage data. The electrocardiogram (ECG) is a widely-used medical test, typically consisting of 12 voltage versus time traces collected from surface recordings over the heart. Here we hypothesize that a deep neural network can predict an important future clinical event (one-year all-cause mortality) from ECG voltage-time traces. We show good performance for predicting one-year mortality with an average AUC of 0.85 from a model cross-validated on 1,775,926 12-lead resting ECGs, that were collected over a 34-year period in a large regional health system. Even within the large subset of ECGs interpreted as 'normal' by a physician (n=297,548), the model performance to predict one-year mortality remained high (AUC=0.84), and Cox Proportional Hazard model revealed a hazard ratio of 6.6 (p<0.005) for the two predicted groups (dead vs alive one year after ECG) over a 30-year follow-up period. A blinded survey of three cardiologists suggested that the patterns captured by the model were generally not visually apparent to cardiologists even after being shown 240 paired examples of labeled true positives (dead) and true negatives (alive). In summary, deep learning can add significant prognostic information to the interpretation of 12-lead resting ECGs, even in cases that are interpreted as 'normal' by physicians."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A deep neural network is trained to predict one-year all-cause mortality from 12-lead electrocardiogram voltage-time traces. The model is cross-validated on a dataset of 1,775,926 ECGs collected over a 34-year period. The average AUC of the model is 0.85. However, when the model is applied to a subset of ECGs interpreted as 'normal' by a physician (n=297,548), the AUC drops to 0.84. What can be inferred about the relationship between the model's performance and the interpretation of ECGs by physicians?\n\nA) The model's performance is not affected by the interpretation of ECGs by physicians.\nB) The model's performance is improved when ECGs are interpreted as 'normal' by physicians.\nC) The model's performance is similar to the interpretation of ECGs by physicians, suggesting that the model is not adding significant prognostic information.\nD) The model's performance is significantly improved when ECGs are interpreted as 'normal' by physicians, indicating that the model is able to capture subtle patterns in the data that are not apparent to physicians.\n\n**Correct Answer:** C) The model's performance is similar to the interpretation of ECGs by physicians, suggesting that the model is not adding significant prognostic information.\n\n**Explanation:** The fact that the model's AUC drops to 0.84 when applied to a subset of ECGs interpreted as 'normal' by physicians suggests that the model is not adding significant prognostic information to the interpretation of ECGs by physicians. This is because the model's performance is similar to the interpretation of ECGs by physicians, indicating that the model is not capturing any subtle patterns in the data that are not apparent to physicians. This is further supported by the fact that cardiologists were unable to visually identify the patterns captured by the model, even after being shown examples of labeled true positives and true negatives."}, "51": {"documentation": {"title": "Deep Smoothing of the Implied Volatility Surface", "source": "Damien Ackerer, Natasa Tagasovska, Thibault Vatter", "docs_id": "1906.05065", "section": ["q-fin.PR", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Smoothing of the Implied Volatility Surface. We present a neural network (NN) approach to fit and predict implied volatility surfaces (IVSs). Atypically to standard NN applications, financial industry practitioners use such models equally to replicate market prices and to value other financial instruments. In other words, low training losses are as important as generalization capabilities. Importantly, IVS models need to generate realistic arbitrage-free option prices, meaning that no portfolio can lead to risk-free profits. We propose an approach guaranteeing the absence of arbitrage opportunities by penalizing the loss using soft constraints. Furthermore, our method can be combined with standard IVS models in quantitative finance, thus providing a NN-based correction when such models fail at replicating observed market prices. This lets practitioners use our approach as a plug-in on top of classical methods. Empirical results show that this approach is particularly useful when only sparse or erroneous data are available. We also quantify the uncertainty of the model predictions in regions with few or no observations. We further explore how deeper NNs improve over shallower ones, as well as other properties of the network architecture. We benchmark our method against standard IVS models. By evaluating our method on both training sets, and testing sets, namely, we highlight both their capacity to reproduce observed prices and predict new ones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed neural network approach to fit and predict implied volatility surfaces (IVSs) in the context of financial modeling, and how does it address the issue of arbitrage opportunities?\n\n**A)** The primary goal is to minimize training losses and maximize generalization capabilities, while also ensuring that the model generates realistic arbitrage-free option prices.\n\n**B)** The primary goal is to replicate market prices and value other financial instruments, while also penalizing the loss using soft constraints to guarantee the absence of arbitrage opportunities.\n\n**C)** The primary goal is to improve the performance of standard IVS models by combining them with neural networks, while also exploring the properties of the network architecture.\n\n**D)** The primary goal is to quantify the uncertainty of the model predictions in regions with few or no observations, while also benchmarking the method against standard IVS models.\n\n**Correct Answer:** B) The primary goal is to replicate market prices and value other financial instruments, while also penalizing the loss using soft constraints to guarantee the absence of arbitrage opportunities.\n\n**Explanation:** The correct answer is B) because the question specifically asks about the primary goal of the proposed approach, which is to replicate market prices and value other financial instruments, while also ensuring that the model generates realistic arbitrage-free option prices by penalizing the loss using soft constraints. This is stated in the introduction of the paper: \"Importantly, IVS models need to generate realistic arbitrage-free option prices, meaning that no portfolio can lead to risk-free profits.\""}, "52": {"documentation": {"title": "Simple and collective twisted symmetries", "source": "Giuseppe Gaeta", "docs_id": "1410.7982", "section": ["math-ph", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simple and collective twisted symmetries. After the introduction of $\\lambda$-symmetries by Muriel and Romero, several other types of so called \"twisted symmetries\" have been considered in the literature (their name refers to the fact they are defined through a deformation of the familiar prolongation operation); they are as useful as standard symmetries for what concerns symmetry reduction of ODEs or determination of special (invariant) solutions for PDEs and have thus attracted attention. The geometrical relation of twisted symmetries to standard ones has already been noted: for some type of twisted symmetries (in particular, $\\lambda$ and $\\mu$-symmetries), this amounts to a certain kind of gauge transformation. In a previous review paper [G. Gaeta, \"Twisted symmetries of differential equations\", {\\it J. Nonlin. Math. Phys.}, {\\bf 16-S} (2009), 107-136] we have surveyed the first part of the developments of this theory; in the present paper we review recent developments. In particular, we provide a unifying geometrical description of the different types of twisted symmetries; this is based on the classical Frobenius reduction applied to distribution generated by Lie-point (local) symmetries."}, "answer": "Here is the generated question:\n\nQuestion: What is the geometrical relation between twisted symmetries and standard symmetries, and how does it relate to gauge transformations, according to the review paper \"Simple and collective twisted symmetries\"?\n\nA) Twisted symmetries are a type of gauge transformation that can be used to reduce the order of differential equations, but they do not have a direct geometrical relation to standard symmetries.\n\nB) The geometrical relation between twisted symmetries and standard symmetries is based on the classical Frobenius reduction, which can be applied to distributions generated by Lie-point (local) symmetries, and this relation amounts to a certain kind of gauge transformation.\n\nC) Twisted symmetries are a type of symmetry that can be used to determine special solutions for partial differential equations, but they do not have a geometrical relation to standard symmetries.\n\nD) The geometrical relation between twisted symmetries and standard symmetries is based on a deformation of the prolongation operation, but it does not amount to a gauge transformation.\n\nCorrect Answer: B) The geometrical relation between twisted symmetries and standard symmetries is based on the classical Frobenius reduction, which can be applied to distributions generated by Lie-point (local) symmetries, and this relation amounts to a certain kind of gauge transformation.\n\nExplanation: The review paper \"Simple and collective twisted symmetries\" discusses the geometrical relation between twisted symmetries and standard symmetries, and how it relates to gauge transformations. The correct answer, B, states that this relation is based on the classical Frobenius reduction, which can be applied to distributions generated by Lie-point (local) symmetries, and this relation amounts to a certain kind of gauge transformation. This is in line with the paper's discussion of the geometrical relation between twisted symmetries and standard symmetries, and how it relates to gauge transformations."}, "53": {"documentation": {"title": "The Millennial Boom, the Baby Bust, and the Housing Market", "source": "Marijn A. Bolhuis and Judd N. L. Cramer", "docs_id": "2003.11565", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Millennial Boom, the Baby Bust, and the Housing Market. As baby boomers have begun to downsize and retire, their preferences now overlap with millennials' predilection for urban amenities and smaller living spaces. This confluence in tastes between the two largest age segments of the U.S. population has meaningfully changed the evolution of home prices in the United States. Utilizing a Bartik shift-share instrument for demography-driven demand shocks, we show that from 2000 to 2018 (i) the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, (ii) within local labor markets, the relative home prices in baby boomer-rich zip codes have declined compared with millennial-rich neighborhoods, and (iii) the zip codes with the largest relative share of smaller homes have grown fastest. These patterns have become more pronounced during the latest economic cycle. We show that the effects are concentrated in areas where housing supply is most inelastic. If this pattern in the housing market persists or expands, the approximately 16.5 trillion in real estate wealth held by households headed by those aged 55 or older will be significantly affected. We find little evidence that these upcoming changes have been incorporated into current prices."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the impact of the confluence of baby boomer and millennial preferences on the evolution of home prices in the United States. How do the findings of the study suggest that housing market patterns may change in the future, and what are the potential implications for real estate wealth held by households headed by those aged 55 or older?\n\n**A)** The study suggests that the confluence of baby boomer and millennial preferences has led to a decline in home prices in areas with high concentrations of baby boomers, as younger generations prefer smaller living spaces. This trend is expected to continue, leading to a significant increase in real estate wealth held by millennials.\n\n**B)** The study finds that the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, indicating a shift towards smaller living spaces. However, the study does not provide evidence that this trend has been incorporated into current prices, suggesting that the impact on real estate wealth held by households headed by those aged 55 or older may be limited.\n\n**C)** The study suggests that the effects of the confluence of baby boomer and millennial preferences are concentrated in areas where housing supply is most inelastic, leading to a decline in home prices in areas with high concentrations of baby boomers. This trend is expected to persist, leading to a significant increase in real estate wealth held by millennials.\n\n**D)** The study finds that the zip codes with the largest relative share of smaller homes have grown fastest, indicating a shift towards smaller living spaces. However, the study does not provide evidence that this trend has been incorporated into current prices, suggesting that the impact on real estate wealth held by households headed by those aged 55 or older may be limited.\n\n**Correct Answer:** B) The study finds that the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, indicating a shift towards smaller living spaces. However, the study does not provide evidence that this trend has been incorporated into current prices, suggesting that the impact on real estate wealth held by households headed by those aged 55 or older may be limited.\n\n**Explanation:** The correct answer is B) because the study specifically states that the price growth of four- and five-bedroom houses has lagged the prices of one- and two-bedroom homes, indicating a shift towards smaller living spaces. Additionally, the study notes that there is little evidence that these upcoming changes have been incorporated into current prices, suggesting that the impact on real estate wealth held by households headed by those aged 55 or older may be limited."}, "54": {"documentation": {"title": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification", "source": "Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir", "docs_id": "2004.13455", "section": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification. Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence only roughly aims at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by 3.11%, 2.41%, respectively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed Decision Tree-based Co-Attention model (DTCA) in explainable claim verification, and how does it address the limitations of existing methods?\n\n**A)** DTCA aims to improve the interpretability of claim verification results by providing a more detailed explanation of the evidence used to support the verification outcome. However, it does not address the issue of false parts in claims.\n\n**B)** DTCA seeks to discover evidence for explainable claim verification in a transparent and interpretable way, and its Co-attention Self-attention networks (CaSa) enable the selected evidence to interact with claims to identify false parts.\n\n**C)** DTCA aims to improve the performance of claim verification by using a combination of Decision Tree-based Evidence model (DTE) and Co-attention Self-attention networks (CaSa), but it does not provide explanations for the results of claim verification.\n\n**D)** DTCA is designed to provide explanations for the results of claim verification, but its primary goal is to improve the performance of claim verification by using a combination of Decision Tree-based Evidence model (DTE) and Co-attention Self-attention networks (CaSa).\n\n**Correct Answer:** B) DTCA seeks to discover evidence for explainable claim verification in a transparent and interpretable way, and its Co-attention Self-attention networks (CaSa) enable the selected evidence to interact with claims to identify false parts.\n\n**Explanation:** The correct answer is B) because the paper proposes DTCA as a solution to the limitations of existing methods, which are non-transparent and unexplained. The Co-attention Self-attention networks (CaSa) in DTCA enable the selected evidence to interact with claims, which is crucial for identifying false parts in claims. The other options are incorrect because they do not accurately capture the primary goal of DTCA or its relationship with existing methods."}, "55": {"documentation": {"title": "Optimal Power Control for Transmitting Correlated Sources with Energy\n  Harvesting Constraints", "source": "Yunquan Dong, Zhi Chen, Jian Wang, Byonghyo Shim", "docs_id": "1706.02033", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Power Control for Transmitting Correlated Sources with Energy\n  Harvesting Constraints. We investigate the weighted-sum distortion minimization problem in transmitting two correlated Gaussian sources over Gaussian channels using two energy harvesting nodes. To this end, we develop offline and online power control policies to optimize the transmit power of the two nodes. In the offline case, we cast the problem as a convex optimization and investigate the structure of the optimal solution. We also develop a generalized water-filling based power allocation algorithm to obtain the optimal solution efficiently. For the online case, we quantify the distortion of the system using a cost function and show that the expected cost equals the expected weighted-sum distortion. Based on Banach's fixed point theorem, we further propose a geometrically converging algorithm to find the minimum cost via simple iterations. Simulation results show that our online power control outperforms the greedy power control where each node uses all the available energy in each slot and performs close to that of the proposed offline power control. Moreover, the performance of our offline power control almost coincides with the performance limit of the system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Optimal Power Control for Transmitting Correlated Sources with Energy Harvesting Constraints paper, what is the main difference between the offline and online power control policies proposed in the study?\n\nA) The offline policy uses a greedy approach, while the online policy uses a water-filling algorithm.\nB) The offline policy is based on a convex optimization problem, while the online policy is based on a geometrically converging algorithm.\nC) The offline policy is designed for a single energy harvesting node, while the online policy is designed for two energy harvesting nodes.\nD) The offline policy is used for a Gaussian channel, while the online policy is used for a non-Gaussian channel.\n\n**Correct Answer:** B) The offline policy is based on a convex optimization problem, while the online policy is based on a geometrically converging algorithm.\n\n**Explanation:** The question requires the test-taker to understand the main difference between the offline and online power control policies proposed in the study. The correct answer, B, highlights the key distinction between the two approaches: the offline policy is based on a convex optimization problem, which is a mathematical framework for solving optimization problems, while the online policy is based on a geometrically converging algorithm, which is a numerical method for finding the minimum cost. The other options are incorrect because they do not accurately describe the main difference between the offline and online policies."}, "56": {"documentation": {"title": "Entanglement and edge effects in superpositions of many-body Fock states\n  with spatial constraints", "source": "Ioannis Kleftogiannis, Ilias Amanatidis", "docs_id": "1707.07840", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entanglement and edge effects in superpositions of many-body Fock states\n  with spatial constraints. We investigate how entangled states can be created by considering collections of point-particles arranged at different spatial configurations, i.e., Fock states with spatial constraints. This type of states can be realized in Hubbard chains of spinless hard-core bosons, at different fillings, which have gapped energy spectrum with a highly degenerate ground state. We calculate the bipartite entanglement entropy for superpositions of such Fock states and show that their entanglement can be controlled via the spatial freedom of the particles, determined by the system filling. In addition we study the effect of confinement/boundary conditions on the Fock states and show that edge modes appear at the ends of the system, when open boundary conditions are considered. Our result is an example of entangled many-body states in 1D systems of strongly interacting particles, without requiring the spin, long-range microscopic interactions or external fields. Instead, the entanglement can be tuned by the empty space in the system, which determines the spatial freedom of the interacting particles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which entanglement is controlled in superpositions of many-body Fock states with spatial constraints, as investigated in the study \"Entanglement and edge effects in superpositions of many-body Fock states\"?\n\nA) The filling of the system determines the number of particles, which in turn affects the entanglement.\nB) The spatial configuration of the particles, determined by the system's filling, controls the entanglement.\nC) The type of boundary conditions imposed on the system affects the entanglement, with open boundaries leading to edge modes.\nD) The presence of external fields or long-range microscopic interactions is necessary to create entangled states.\n\nCorrect Answer: B) The spatial configuration of the particles, determined by the system's filling, controls the entanglement.\n\nExplanation: The study investigates how entangled states can be created by considering collections of point-particles arranged at different spatial configurations, i.e., Fock states with spatial constraints. The correct answer, B, reflects the main finding of the study, which is that the entanglement can be controlled via the spatial freedom of the particles, determined by the system filling. The other options are incorrect because they either misrepresent the study's findings (A and D) or only partially capture the relationship between spatial configuration and entanglement (C)."}, "57": {"documentation": {"title": "Stationary peaks in a multivariable reaction--diffusion system: Foliated\n  snaking due to subcritical Turing instability", "source": "Edgar Knobloch and Arik Yochelis", "docs_id": "2008.00094", "section": ["nlin.PS", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stationary peaks in a multivariable reaction--diffusion system: Foliated\n  snaking due to subcritical Turing instability. An activator-inhibitor-substrate model of side-branching used in the context of pulmonary vascular and lung development is considered on the supposition that spatially localized concentrations of the activator trigger local side-branching. The model consists of four coupled reaction-diffusion equations and its steady localized solutions therefore obey an eight-dimensional spatial dynamical system in one dimension (1D). Stationary localized structures within the model are found to be associated with a subcritical Turing instability and organized within a distinct type of foliated snaking bifurcation structure. This behavior is in turn associated with the presence of an exchange point in parameter space at which the complex leading spatial eigenvalues of the uniform concentration state are overtaken by a pair of real eigenvalues; this point plays the role of a Belyakov-Devaney point in this system. The primary foliated snaking structure consists of periodic spike or peak trains with $N$ identical equidistant peaks, $N=1,2,\\dots \\,$, together with cross-links consisting of nonidentical, nonequidistant peaks. The structure is complicated by a multitude of multipulse states, some of which are also computed, and spans the parameter range from the primary Turing bifurcation all the way to the fold of the $N=1$ state. These states form a complex template from which localized physical structures develop in the transverse direction in 2D."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the activator-inhibitor-substrate model of side-branching in pulmonary vascular and lung development, what is the role of the Belyakov-Devaney point in the parameter space of the system?\n\nA) It marks the onset of the primary Turing bifurcation, where the system undergoes a sudden transition to a stable, uniform state.\nB) It represents the exchange point where the complex leading spatial eigenvalues of the uniform concentration state are overtaken by a pair of real eigenvalues, leading to the emergence of foliated snaking bifurcation structures.\nC) It signifies the collapse of the foliated snaking structure into a stable, periodic spike or peak train with N=1 identical equidistant peaks.\nD) It denotes the fold of the N=1 state, where the system undergoes a bifurcation that leads to the emergence of a new, stable structure.\n\n**Correct Answer:** B) It represents the exchange point where the complex leading spatial eigenvalues of the uniform concentration state are overtaken by a pair of real eigenvalues, leading to the emergence of foliated snaking bifurcation structures.\n\n**Explanation:** The Belyakov-Devaney point is a critical parameter point in the system where the complex leading spatial eigenvalues of the uniform concentration state are overtaken by a pair of real eigenvalues. This exchange point is responsible for the emergence of foliated snaking bifurcation structures, which are characterized by periodic spike or peak trains with N identical equidistant peaks, as well as cross-links consisting of nonidentical, nonequidistant peaks. The correct answer, B, accurately describes the role of the Belyakov-Devaney point in the system."}, "58": {"documentation": {"title": "Thermodynamic geometry of static and rotating regular black holes in\n  conformal massive gravity", "source": "Saheb Soroushfar, Reza Saffari, Amare Abebe, Haidar Sheikhahmadi", "docs_id": "2109.03176", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermodynamic geometry of static and rotating regular black holes in\n  conformal massive gravity. A version of massive gravity, namely conformal massive gravity, is employed to study the behavior of thermodynamic geometry for both the static and the rotating regular black holes. Whereas in thermodynamic geometry singularity can be interpreted as the second phase transition, seeing such phenomena as heat capacity behavior for both cases is investigated in detail. In addition, other thermodynamic quantities like the entropy and the temperature are analyzed as well. Another important property of the black holes is the so-called stability, in which utilizing the first phase transition of the heat capacity is detailed, for both cases, say the static and the spinning black holes. It is also interestingly figured out that, the behavior of temperature of the uncharged black holes in the presence of hair parameter, $\\lambda$, can mimic the charged ones. {The effects of scalar charge, $Q$, and hair parameters with both positive and negative signs and how it affects key parameters in the study of black holes are investigated as well.} To investigate the properties of the black holes both regular thermodynamics and thermodynamic geometry approaches are considered. Then one can observe that aiming to realize the physics of the black holes, many complicated phenomena can be analyzed dramatically easier by considering the latter approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of conformal massive gravity, what is the relationship between the hair parameter \u03bb and the temperature of the uncharged black holes, and how does it compare to the behavior of charged black holes?\n\n**A)** The hair parameter \u03bb has no effect on the temperature of uncharged black holes, and the temperature of charged black holes is solely determined by their charge Q.\n\n**B)** The hair parameter \u03bb can mimic the behavior of charged black holes, and the temperature of uncharged black holes is directly proportional to \u03bb.\n\n**C)** The hair parameter \u03bb affects the temperature of uncharged black holes in a way that is analogous to the effect of charge Q on charged black holes, but with a different proportionality constant.\n\n**D)** The hair parameter \u03bb has no effect on the temperature of uncharged black holes, and the temperature of charged black holes is solely determined by their charge Q, but with a different proportionality constant.\n\n**Correct Answer:** C) The hair parameter \u03bb affects the temperature of uncharged black holes in a way that is analogous to the effect of charge Q on charged black holes, but with a different proportionality constant.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"the behavior of temperature of the uncharged black holes in the presence of hair parameter, \u03bb, can mimic the charged ones.\" This suggests that the hair parameter \u03bb has an effect on the temperature of uncharged black holes, but the exact nature of this effect is not specified. The correct answer C) suggests that the effect of \u03bb on the temperature of uncharged black holes is analogous to the effect of charge Q on charged black holes, but with a different proportionality constant. This is consistent with the idea that the hair parameter \u03bb is a measure of the \"hair\" or \"structure\" of the black hole, and that it affects its thermodynamic properties in a way that is similar to the effect of charge Q on charged black holes."}, "59": {"documentation": {"title": "Synchronized molecular-dynamics simulation for the thermal lubrication\n  of a polymeric liquid between parallel plates", "source": "Shugo Yasuda and Ryoichi Yamamoto", "docs_id": "1503.07289", "section": ["physics.flu-dyn", "cond-mat.soft", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synchronized molecular-dynamics simulation for the thermal lubrication\n  of a polymeric liquid between parallel plates. The Synchronized Molecular-Dynamics simulation which was recently proposed by authors [Phys. Rev. X {\\bf 4}, 041011 (2014)] is applied to the analysis of polymer lubrication between parallel plates. The rheological properties, conformational change of polymer chains, and temperature rise due to the viscous heating are investigated with changing the values of thermal conductivity of the polymeric liquid. It is found that at a small applied shear stress on the plate, the temperature of polymeric liquid only slightly increases in inverse proportion to the thermal conductivity and the apparent viscosity of polymeric liquid is not much affected by changing the thermal conductivity. However, at a large shear stress, the transitional behaviors of the polymeric liquid occur due to the interplay of the shear deformation and viscous heating by changing the thermal conductivity. This transition is characterized by the Nahme-Griffith number $Na$ which is defined as the ratio of the viscous heating to the thermal conduction at a characteristic temperature. When the Nahme-Griffith number exceeds the unity, the temperature of polymeric liquid increases rapidly and the apparent viscosity also exponentially decreases as the thermal conductivity decreases. The conformation of polymer chains is stretched and aligned by the shear flow for $Na<1$, but the coherent structure becomes disturbed by the thermal motion of molecules for $Na>1$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Synchronized Molecular-Dynamics simulation for the thermal lubrication of a polymeric liquid between parallel plates, what is the significance of the Nahme-Griffith number ($Na$) and how does it relate to the thermal conductivity of the polymeric liquid?\n\nA) The Nahme-Griffith number is a measure of the thermal conductivity of the polymeric liquid, and it is directly proportional to the temperature rise of the liquid.\n\nB) The Nahme-Griffith number is a dimensionless quantity that characterizes the transition from laminar to turbulent flow in the polymeric liquid, and it is inversely proportional to the thermal conductivity.\n\nC) The Nahme-Griffith number is a ratio of the viscous heating to the thermal conduction at a characteristic temperature, and it determines the stability of the polymeric liquid's conformation under shear flow.\n\nD) The Nahme-Griffith number is a measure of the apparent viscosity of the polymeric liquid, and it is directly proportional to the thermal conductivity.\n\n**Correct Answer:** C) The Nahme-Griffith number is a ratio of the viscous heating to the thermal conduction at a characteristic temperature, and it determines the stability of the polymeric liquid's conformation under shear flow.\n\n**Explanation:** The Nahme-Griffith number ($Na$) is defined as the ratio of the viscous heating to the thermal conduction at a characteristic temperature. It is a dimensionless quantity that characterizes the transition from laminar to turbulent flow in the polymeric liquid. When $Na < 1$, the conformation of polymer chains is stretched and aligned by the shear flow, but the coherent structure becomes disturbed by the thermal motion of molecules for $Na > 1$. This indicates that the Nahme-Griffith number is a critical parameter in determining the stability of the polymeric liquid's conformation under shear flow."}}