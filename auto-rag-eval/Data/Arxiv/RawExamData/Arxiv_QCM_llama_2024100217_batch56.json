{"0": {"documentation": {"title": "Rapid calculation of side chain packing and free energy with\n  applications to protein molecular dynamics", "source": "John M. Jumper, Karl F. Freed, Tobin R. Sosnick", "docs_id": "1610.07277", "section": ["q-bio.BM", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rapid calculation of side chain packing and free energy with\n  applications to protein molecular dynamics. To address the large gap between time scales that can be easily reached by molecular simulations and those required to understand protein dynamics, we propose a rapid self-consistent approximation of the side chain free energy at every integration step. In analogy with the adiabatic Born-Oppenheimer approximation for electronic structure, the protein backbone dynamics are simulated as preceding according to the dictates of the free energy of an instantaneously-equilibrated side chain potential. The side chain free energy is computed on the fly, allowing the protein backbone dynamics to traverse a greatly smoothed energetic landscape. This results in extremely rapid equilibration and sampling of the Boltzmann distribution. Because our method employs a reduced model involving single-bead side chains, we also provide a novel, maximum-likelihood method to parameterize the side chain model using input data from high resolution protein crystal structures. We demonstrate state-of-the-art accuracy for predicting $\\chi_1$ rotamer states while consuming only milliseconds of CPU time. We also show that the resulting free energies of side chains is sufficiently accurate for de novo folding of some proteins."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed method for simulating protein molecular dynamics, and how does it enable rapid equilibration and sampling of the Boltzmann distribution?\n\nA) The method uses a reduced model involving single-bead side chains, which allows for faster computation of side chain free energies.\nB) The method employs an adiabatic Born-Oppenheimer approximation for electronic structure, which enables the protein backbone dynamics to be simulated independently of the side chain potential.\nC) The method provides a novel, maximum-likelihood method to parameterize the side chain model using input data from high-resolution protein crystal structures, which enables accurate prediction of side chain free energies.\nD) The method uses a self-consistent approximation of the side chain free energy at every integration step, which enables the protein backbone dynamics to traverse a greatly smoothed energetic landscape.\n\nCorrect Answer: D) The method uses a self-consistent approximation of the side chain free energy at every integration step, which enables the protein backbone dynamics to traverse a greatly smoothed energetic landscape.\n\nExplanation: The correct answer is D) because the method's self-consistent approximation of the side chain free energy at every integration step enables the protein backbone dynamics to traverse a greatly smoothed energetic landscape, resulting in extremely rapid equilibration and sampling of the Boltzmann distribution. This is the primary advantage of the proposed method, as stated in the documentation. The other options are incorrect because they either describe a secondary aspect of the method (A and C) or a different concept altogether (B)."}, "1": {"documentation": {"title": "An empirical behavioral model of liquidity and volatility", "source": "Szabolcs Mike, J. Doyne Farmer", "docs_id": "0709.0159", "section": ["q-fin.ST", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An empirical behavioral model of liquidity and volatility. We develop a behavioral model for liquidity and volatility based on empirical regularities in trading order flow in the London Stock Exchange. This can be viewed as a very simple agent based model in which all components of the model are validated against real data. Our empirical studies of order flow uncover several interesting regularities in the way trading orders are placed and cancelled. The resulting simple model of order flow is used to simulate price formation under a continuous double auction, and the statistical properties of the resulting simulated sequence of prices are compared to those of real data. The model is constructed using one stock (AZN) and tested on 24 other stocks. For low volatility, small tick size stocks (called Group I) the predictions are very good, but for stocks outside Group I they are not good. For Group I, the model predicts the correct magnitude and functional form of the distribution of the volatility and the bid-ask spread, without adjusting any parameters based on prices. This suggests that at least for Group I stocks, the volatility and heavy tails of prices are related to market microstructure effects, and supports the hypothesis that, at least on short time scales, the large fluctuations of absolute returns are well described by a power law with an exponent that varies from stock to stock."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** An empirical behavioral model of liquidity and volatility, as described in the Arxiv paper, suggests that the volatility and heavy tails of prices are related to market microstructure effects for low-volatility, small-tick-size stocks (Group I). However, the model's predictions are not good for stocks outside Group I. What is the primary implication of this finding for our understanding of price formation under a continuous double auction?\n\nA) The model suggests that market microstructure effects are the primary driver of price volatility for all stocks, regardless of their volatility or size.\n\nB) The model implies that the relationship between volatility and market microstructure effects is specific to low-volatility, small-tick-size stocks (Group I), and may not apply to other stocks.\n\nC) The model's poor performance for stocks outside Group I suggests that the continuous double auction mechanism is not well-suited for modeling price formation in all types of markets.\n\nD) The model's findings support the hypothesis that large fluctuations of absolute returns are well described by a power law with an exponent that varies from stock to stock, regardless of their volatility or size.\n\n**Correct Answer:** B) The model implies that the relationship between volatility and market microstructure effects is specific to low-volatility, small-tick-size stocks (Group I), and may not apply to other stocks.\n\n**Explanation:** The correct answer is B because the paper states that \"For Group I, the model predicts the correct magnitude and functional form of the distribution of the volatility and the bid-ask spread, without adjusting any parameters based on prices.\" This suggests that the model's relationship between volatility and market microstructure effects is specific to low-volatility, small-tick-size stocks (Group I), and may not apply to other stocks. The other options are incorrect because they either overgeneralize the model's findings (A), misinterpret the model's results (C), or misrepresent the model's implications (D)."}, "2": {"documentation": {"title": "KIC 10001893: A pulsating sdB star with multiple trapped modes", "source": "M. Uzundag, A.S. Baran, R.H. {\\O}stensen, M.D. Reed, J.H. Telting and\n  B.K. Quick", "docs_id": "1812.05675", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "KIC 10001893: A pulsating sdB star with multiple trapped modes. KIC 10001893 is a V1093 Her type pulsating subdwarf-B star, which was observed extensively by the Kepler spacecraft. It was a part of the survey searching for compact pulsators in the Kepler field of view. An amplitude spectrum of the star demonstrates a rich content of g-modes between 102 and 496 {\\mu}Hz as well as a few p-modes above 2000 {\\mu}Hz. In total, we found 110 frequencies. The g-mode region contains 104 frequencies, while the p-mode region contains just six, altogether indicating the hybrid nature of KIC 10001893. The main goal of our analysis was to identify the detected modes and to find some features, which will significantly help modeling of the star. We found no multiplets, which points at a pole-on orientation, however, we defined modal degrees and relative radial orders using asymptotic period spacing. Eventually, we assigned 32 dipole l = 1 and 18 quadrupole l = 2 modes. The most remarkable feature we found are trapped modes, which are clearly seen in a reduce period diagram. It is the first time that three trapped modes are detected in one pulsating sdB star. Since the more trapped modes we find, the better sounding of the stellar interior we can do, this feature provides important constraints on the physical structure of the star. Mode trapping is likely caused by the He-H transition region and therefore it provides crucial constraints for making realistic theoretical models of hot subdwarfs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the detection of trapped modes in the pulsating subdwarf-B star KIC 10001893, and how does this feature impact our understanding of the star's physical structure?\n\nA) The detection of trapped modes indicates that the star's interior is too hot to support stable oscillations, leading to a more compact and dense core.\nB) The presence of trapped modes suggests that the star's helium-helium transition region is causing a significant perturbation in the star's oscillations, providing valuable constraints for modeling the star's interior.\nC) The observation of trapped modes implies that the star's magnetic field is too strong, causing the star's oscillations to become unstable and leading to a more turbulent interior.\nD) The detection of trapped modes indicates that the star's core is too cool, resulting in a less dense and less compact core.\n\nCorrect Answer: B) The presence of trapped modes suggests that the star's helium-helium transition region is causing a significant perturbation in the star's oscillations, providing valuable constraints for modeling the star's interior.\n\nExplanation: The correct answer is B) because the detection of trapped modes is a significant feature of the star's oscillations, and the text explicitly states that mode trapping is likely caused by the He-H transition region. This feature provides crucial constraints for making realistic theoretical models of hot subdwarfs, as it allows for a more detailed understanding of the star's interior structure. The other options are incorrect because they do not accurately reflect the primary conclusion drawn from the detection of trapped modes."}, "3": {"documentation": {"title": "Testing Finite Moment Conditions for the Consistency and the Root-N\n  Asymptotic Normality of the GMM and M Estimators", "source": "Yuya Sasaki and Yulong Wang", "docs_id": "2006.02541", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Testing Finite Moment Conditions for the Consistency and the Root-N\n  Asymptotic Normality of the GMM and M Estimators. Common approaches to inference for structural and reduced-form parameters in empirical economic analysis are based on the consistency and the root-n asymptotic normality of the GMM and M estimators. The canonical consistency (respectively, root-n asymptotic normality) for these classes of estimators requires at least the first (respectively, second) moment of the score to be finite. In this article, we present a method of testing these conditions for the consistency and the root-n asymptotic normality of the GMM and M estimators. The proposed test controls size nearly uniformly over the set of data generating processes that are compatible with the null hypothesis. Simulation studies support this theoretical result. Applying the proposed test to the market share data from the Dominick's Finer Foods retail chain, we find that a common \\textit{ad hoc} procedure to deal with zero market shares in analysis of differentiated products markets results in a failure to satisfy the conditions for both the consistency and the root-n asymptotic normality."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Consider a structural model of a differentiated products market, where the market share of each product is a function of its price and a vector of exogenous variables. The GMM estimator is used to estimate the structural parameters of the model. The data generating process is such that the score function has a finite first moment, but the second moment is infinite.\n\n**Which of the following statements is true?**\n\nA) The GMM estimator is consistent and asymptotically normal.\nB) The GMM estimator is consistent, but not asymptotically normal.\nC) The GMM estimator is not consistent, but has finite asymptotic variance.\nD) The GMM estimator is not consistent and does not have finite asymptotic variance.\n\n**Correct Answer:** B) The GMM estimator is consistent, but not asymptotically normal.\n\n**Explanation:** The canonical consistency of the GMM estimator requires the first moment of the score function to be finite, which is the case here. However, the root-n asymptotic normality of the GMM estimator requires the second moment of the score function to be finite, which is not the case here. Therefore, the GMM estimator is consistent, but not asymptotically normal.\n\n**Note:** This question requires the test-taker to understand the conditions for consistency and asymptotic normality of the GMM estimator, as well as the implications of infinite moments on these properties. It also requires the test-taker to apply this knowledge to a specific scenario, making it a challenging multi-form question."}, "4": {"documentation": {"title": "Reconceptualising equilibrium in Boltzmannian statistical mechanics and\n  characterising its existence", "source": "Charlotte Werndl and Roman Frigg", "docs_id": "1510.02260", "section": ["cond-mat.stat-mech", "math-ph", "math.DS", "math.MP", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconceptualising equilibrium in Boltzmannian statistical mechanics and\n  characterising its existence. In Boltzmannian statistical mechanics macro-states supervene on micro-states. This leads to a partitioning of the state space of a system into regions of macroscopically indistinguishable micro-states. The largest of these regions is singled out as the equilibrium region of the system. What justifies this association? We review currently available answers to this question and find them wanting both for conceptual and for technical reasons. We propose a new conception of equilibrium and prove a mathematical theorem which establishes in full generality -- i.e. without making any assumptions about the system's dynamics or the nature of the interactions between its components -- that the equilibrium macro-region is the largest macro-region. We then turn to the question of the approach to equilibrium, of which there exists no satisfactory general answer so far. In our account, this question is replaced by the question when an equilibrium state exists. We prove another -- again fully general -- theorem providing necessary and sufficient conditions for the existence of an equilibrium state. This theorem changes the way in which the question of the approach to equilibrium should be discussed: rather than launching a search for a crucial factor (such as ergodicity or typicality), the focus should be on finding triplets of macro-variables, dynamical conditions, and effective state spaces that satisfy the conditions of the theorem."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of Boltzmannian statistical mechanics, what are the necessary and sufficient conditions for the existence of an equilibrium state, as proposed by the authors, and how do these conditions relate to the concept of macro-states supervening on micro-states?\n\n**A)** The existence of an equilibrium state is guaranteed by the presence of a single, unique macro-state that is macroscopically indistinguishable from all other micro-states.\n\n**B)** The existence of an equilibrium state is determined by the existence of a triplet of macro-variables, dynamical conditions, and effective state spaces that satisfy the following conditions: (i) the macro-variables are independent, (ii) the dynamical conditions are time-reversible, and (iii) the effective state spaces are ergodic.\n\n**C)** The existence of an equilibrium state is ensured by the partitioning of the state space into regions of macroscopically indistinguishable micro-states, with the largest region being singled out as the equilibrium region.\n\n**D)** The existence of an equilibrium state is contingent upon the presence of a \"critical\" macro-state that is neither macroscopically indistinguishable from other micro-states nor ergodic.\n\n**Correct Answer:** B) The existence of an equilibrium state is determined by the existence of a triplet of macro-variables, dynamical conditions, and effective state spaces that satisfy the following conditions: (i) the macro-variables are independent, (ii) the dynamical conditions are time-reversible, and (iii) the effective state spaces are ergodic.\n\n**Explanation:** The authors propose a new conception of equilibrium that focuses on the existence of a triplet of macro-variables, dynamical conditions, and effective state spaces that satisfy specific conditions. These conditions ensure that the equilibrium state is not only macroscopically indistinguishable from other micro-states but also ergodic, meaning that the system is in a state of maximum entropy. The correct answer highlights the importance of these conditions in determining the existence of an equilibrium state."}, "5": {"documentation": {"title": "Angular Clustering of Millimeter-Wave Propagation Channels with\n  Watershed Transformation", "source": "Pengfei Lyu, Aziz Benlarbi-Dela\\\"i, Zhuoxiang Ren and Julien Sarrazin", "docs_id": "2009.01375", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Angular Clustering of Millimeter-Wave Propagation Channels with\n  Watershed Transformation. An angular clustering method based on image processing is proposed in this paper. It is used to identify clusters in 2D representations of propagation channels. The approach uses operations such as watershed segmentation and is particularly well suited for clustering directional channels obtained by beam-steering at millimeter-wave. This situation occurs for instance with electronic beam-steering using analog antenna arrays during beam training process or during channel modeling measurements using either electronic or mechanical beam-steering. In particular, the proposed technique is used here to cluster two-dimensional power angular spectrum maps. The proposed clustering is unsupervised and is well suited to preserve the shape of clusters by considering the angular connection between neighbor samples, which is useful to obtain more accurate descriptions of channel angular properties. The approach is found to outperform approaches based on K-Power- Means in terms of accuracy as well as computational resource . The technique is assessed in simulation using IEEE 802.11ad channel model and in measurement using experiments conducted at 60 GHz in an indoor environment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed angular clustering method in the paper \"Angular Clustering of Millimeter-Wave Propagation Channels with Watershed Transformation\" over approaches based on K-Power Means?\n\n**A)** It is more computationally intensive.\n**B)** It is less accurate in preserving the shape of clusters.\n**C)** It is better suited for clustering directional channels obtained by beam-steering at millimeter-wave.\n**D)** It is more prone to noise and interference.\n\n**Correct Answer:** C) It is better suited for clustering directional channels obtained by beam-steering at millimeter-wave.\n\n**Explanation:** The correct answer is C) because the paper states that the proposed technique is \"particularly well suited for clustering directional channels obtained by beam-steering at millimeter-wave\". This is a key advantage of the method, as it is specifically designed to handle the directional nature of millimeter-wave channels. The other options are incorrect because they do not accurately reflect the advantages of the proposed method. Option A is incorrect because the paper does not mention computational intensity as a disadvantage of the method. Option B is incorrect because the paper states that the method preserves the shape of clusters, which is a desirable property. Option D is incorrect because the paper does not mention noise and interference as a concern."}, "6": {"documentation": {"title": "Matched Illumination Waveforms using Multi-Tone Sinusoidal Frequency\n  Modulation", "source": "Kaushallya Adhikari and David A. Hague", "docs_id": "2105.11517", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Matched Illumination Waveforms using Multi-Tone Sinusoidal Frequency\n  Modulation. This paper explores the design of constant modulus Matched-Illumination (MI) waveforms using the Multi-Tone Sinusoidal Frequency Modulation (MTSFM) waveform model. MI waveforms are optimized for detecting targets in known noise and clutter Power Spectral Densities (PSDs). There exist well-defined information theoretic methods that describe the design of MI waveforms for a myriad of target/noise/clutter models. However, these methods generally only produce the magnitude square of the MI waveform's spectrum. Additionally, the waveform's time-series is not guaranteed to be constant modulus. The MTSFM is a constant modulus waveform model with a discrete set of design coefficients. The coefficients are adjusted to synthesize constant modulus waveforms that approximate the ideal MI waveform's spectrum. Simulations demonstrate that the MTSFM's detection performance closely approximates an ideal MI waveform spectrum and generally outperforms flat spectrum waveforms across a range of transmit energies when the noise and clutter PSDs vary greatly across the operational band."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using the Multi-Tone Sinusoidal Frequency Modulation (MTSFM) waveform model in designing constant modulus Matched-Illumination (MI) waveforms?\n\nA) MTSFM waveforms can only be used for detecting targets in known noise and clutter Power Spectral Densities (PSDs).\nB) MTSFM waveforms can synthesize constant modulus waveforms that approximate the ideal MI waveform's spectrum, outperforming flat spectrum waveforms in detection performance.\nC) MTSFM waveforms are limited to a discrete set of design coefficients, restricting their flexibility in waveform design.\nD) MTSFM waveforms are only suitable for detecting targets in clutter-free environments.\n\n**Correct Answer:** B) MTSFM waveforms can synthesize constant modulus waveforms that approximate the ideal MI waveform's spectrum, outperforming flat spectrum waveforms in detection performance.\n\n**Explanation:** The correct answer, B, highlights the primary advantage of using MTSFM waveforms, which is their ability to synthesize constant modulus waveforms that approximate the ideal MI waveform's spectrum. This allows for improved detection performance, especially in environments with varying noise and clutter PSDs. The other options are incorrect because they either misrepresent the capabilities of MTSFM waveforms (A and C) or are too restrictive in their application (D)."}, "7": {"documentation": {"title": "Trade-offs and Guarantees of Adversarial Representation Learning for\n  Information Obfuscation", "source": "Han Zhao, Jianfeng Chi, Yuan Tian, Geoffrey J. Gordon", "docs_id": "1906.07902", "section": ["cs.LG", "cs.CR", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Trade-offs and Guarantees of Adversarial Representation Learning for\n  Information Obfuscation. Crowdsourced data used in machine learning services might carry sensitive information about attributes that users do not want to share. Various methods have been proposed to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy. However, little is known about the theory behind these methods. In light of this gap, we develop a novel theoretical framework for attribute obfuscation. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its inference guarantees against worst-case adversaries. Meanwhile, it is clear that in general there is a tension between minimizing information leakage and maximizing task accuracy. To understand this, we prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage. We conduct experiments on two real-world datasets to corroborate the inference guarantees and validate this trade-off. Our results indicate that, among several alternatives, the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed minimax optimization formulation in the context of attribute obfuscation, and what does it aim to achieve?\n\nA) To minimize the potential information leakage of sensitive attributes while maximizing the task accuracy, and to analyze its inference guarantees against worst-case adversaries.\nB) To maximize the task accuracy while minimizing the potential information leakage of sensitive attributes, and to prove an information-theoretic lower bound to characterize the fundamental trade-off between accuracy and information leakage.\nC) To prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage, and to conduct experiments to corroborate the inference guarantees.\nD) To develop a novel theoretical framework for attribute obfuscation, and to propose a minimax optimization formulation to protect the given attribute.\n\n**Correct Answer:** A) To minimize the potential information leakage of sensitive attributes while maximizing the task accuracy, and to analyze its inference guarantees against worst-case adversaries.\n\n**Explanation:**\n\nThe correct answer is A) because the question asks about the primary goal of the proposed minimax optimization formulation. The formulation aims to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy, which is the primary goal of attribute obfuscation. Additionally, the formulation analyzes its inference guarantees against worst-case adversaries, which is a key aspect of the proposed approach.\n\nThe other options are incorrect because:\n\n* B) is incorrect because it mentions maximizing task accuracy while minimizing information leakage, but the primary goal of the formulation is to minimize information leakage while maximizing accuracy.\n* C) is incorrect because it mentions proving an information-theoretic lower bound, but this is not the primary goal of the formulation.\n* D) is incorrect because it mentions developing a novel theoretical framework, but this is a broader goal of the paper, not the specific goal of the minimax optimization formulation."}, "8": {"documentation": {"title": "Thermodynamics of Heisenberg ferromagnets with arbitrary spin in a\n  magnetic field", "source": "I. Juh\\'asz Junger (1), D. Ihle (1), L. Bogacz (1 and 2), and W. Janke\n  (1 and 3) ((1) Institut f\\\"ur Theoretische Physik, Universit\\\"at Leipzig,\n  Leipzig, Germany, (2) Department of Information Technologies, Faculty of\n  Physics, Astronomy and Applied Informatics, Jagellonian University, Krak\\'ow,\n  Poland, (3) Centre for Theoretical Sciences (NTZ), Universit\\\"at Leipzig,\n  Leipzig, Germany)", "docs_id": "0802.3395", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermodynamics of Heisenberg ferromagnets with arbitrary spin in a\n  magnetic field. The thermodynamic properties (magnetization, magnetic susceptibility, transverse and longitudinal correlation lengths, specific heat) of one- and two-dimensional ferromagnets with arbitrary spin S in a magnetic field are investigated by a second-order Green-function theory. In addition, quantum Monte Carlo simulations for S= 1/2 and S=1 are performed using the stochastic series expansion method. A good agreement between the results of both approaches is found. The field dependence of the position of the maximum in the temperature dependence of the susceptibility fits well to a power law at low fields and to a linear increase at high fields. The maximum height decreases according to a power law in the whole field region. The longitudinal correlation length may show an anomalous temperature dependence: a minimum followed by a maximum with increasing temperature. Considering the specific heat in one dimension and at low magnetic fields, two maxima in its temperature dependence for both the S= 1/2 and S = 1 ferromagnets are found. For S>1 only one maximum occurs, as in the two-dimensional ferromagnets. Relating the theory to experiments on the S= 1/2 quasi-one-dimensional copper salt TMCuC [(CH_3)_4NCuCl_3], a fit to the magnetization as a function of the magnetic field yields the value of the exchange energy which is used to make predictions for the occurrence of two maxima in the temperature dependence of the specific heat."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The thermodynamic properties of Heisenberg ferromagnets with arbitrary spin S in a magnetic field are investigated using a second-order Green-function theory and quantum Monte Carlo simulations. Analyze the field dependence of the position of the maximum in the temperature dependence of the susceptibility.\n\n**A)** The maximum height of the susceptibility decreases according to a power law in the whole field region, while the position of the maximum follows a power law at low fields and a linear increase at high fields.\n\n**B)** The maximum height of the susceptibility decreases according to a power law in the whole field region, while the position of the maximum follows a linear increase at low fields and a power law at high fields.\n\n**C)** The maximum height of the susceptibility increases according to a power law in the whole field region, while the position of the maximum follows a power law at low fields and a linear increase at high fields.\n\n**D)** The maximum height of the susceptibility decreases according to a power law in the whole field region, while the position of the maximum follows a linear increase at low fields and a power law at high fields.\n\n**Correct Answer:** A) The maximum height of the susceptibility decreases according to a power law in the whole field region, while the position of the maximum follows a power law at low fields and a linear increase at high fields.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"The field dependence of the position of the maximum in the temperature dependence of the susceptibility fits well to a power law at low fields and to a linear increase at high fields.\" Additionally, it is stated that \"The maximum height decreases according to a power law in the whole field region.\" Therefore, option A accurately summarizes the field dependence of the position and height of the maximum in the susceptibility."}, "9": {"documentation": {"title": "Expected performance of the ASTRI-SST-2M telescope prototype", "source": "C. Bigongiari, F. Di Pierro, C. Morello, A. Stamerra, P. Vallania, G.\n  Agnetta, L.A. Antonelli, D. Bastieri, G. Bellassai, M. Belluso, S. Billotta,\n  B. Biondo, G. Bonanno, G. Bonnoli, P. Bruno, A. Bulgarelli, R. Canestrari, M.\n  Capalbi, P. Caraveo, A. Carosi, E. Cascone, O. Catalano, M. Cereda, P.\n  Conconi, V. Conforti, G. Cusumano, V. De Caprio, A. De Luca, A. Di Paola, D.\n  Fantinel, M. Fiorini, D. Fugazza, D. Gardiol, M. Ghigo, F. Gianotti, S.\n  Giarrusso, E. Giro, A. Grillo, D. Impiombato, S. Incorvaia, A. La Barbera, N.\n  La Palombara, V. La Parola, G. La Rosa, L. Lessio, G. Leto, S. Lombardi, F.\n  Lucarelli, M.C. Maccarone, G. Malaguti, G. Malaspina, V. Mangano, D. Marano,\n  E. Martinetti, R. Millul, T. Mineo, A. Mist\\`O, G. Morlino, M.R. Panzera, G.\n  Pareschi, G. Rodeghiero, P. Romano, F. Russo, B. Sacco, N. Sartore, J.\n  Schwarz, A. Segreto, G. Sironi, G. Sottile, E. Strazzeri, L. Stringhetti, G.\n  Tagliaferri, V. Testa, M.C. Timpanaro, G. Toso, G. Tosti, M. Trifoglio, S.\n  Vercellone and V. Zitelli (The Astri Collaboration), L. Arrabito, K.\n  Bernlohr, G. Maier, N. Komin and The Cta Consortium", "docs_id": "1307.5006", "section": ["astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Expected performance of the ASTRI-SST-2M telescope prototype. ASTRI (Astrofisica con Specchi a Tecnologia Replicante Italiana) is an Italian flagship project pursued by INAF (Istituto Nazionale di Astrofisica) strictly linked to the development of the Cherenkov Telescope Array, CTA. Primary goal of the ASTRI program is the design and production of an end-to-end prototype of a Small Size Telescope for the CTA sub-array devoted to the highest gamma-ray energy region. The prototype, named ASTRI SST-2M, will be tested on field in Italy during 2014. This telescope will be the first Cherenkov telescope adopting the double reflection layout in a Schwarzschild-Couder configuration with a tessellated primary mirror and a monolithic secondary mirror. The collected light will be focused on a compact and light-weight camera based on silicon photo-multipliers covering a 9.6 deg full field of view. Detailed Monte Carlo simulations have been performed to estimate the performance of the planned telescope. The results regarding its energy threshold, sensitivity and angular resolution are shown and discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the ASTRI program, and how does it relate to the development of the Cherenkov Telescope Array (CTA)?\n\nA) To design and produce a Small Size Telescope for the CTA sub-array devoted to the highest gamma-ray energy region, which is the primary goal of the ASTRI program.\nB) To develop a new type of camera for the CTA, which is not related to the primary goal of the ASTRI program.\nC) To test the performance of a Cherenkov telescope in Italy during 2014, which is a specific activity of the ASTRI program.\nD) To improve the sensitivity of the CTA sub-array, which is not the primary goal of the ASTRI program.\n\n**Correct Answer:** A) To design and produce a Small Size Telescope for the CTA sub-array devoted to the highest gamma-ray energy region, which is the primary goal of the ASTRI program.\n\n**Explanation:** The primary goal of the ASTRI program is to design and produce an end-to-end prototype of a Small Size Telescope for the CTA sub-array devoted to the highest gamma-ray energy region. This is explicitly stated in the introduction of the documentation. The other options are incorrect because they either describe a specific activity of the ASTRI program (C) or a secondary goal (B), or they are unrelated to the primary goal of the ASTRI program (D)."}, "10": {"documentation": {"title": "Coherence and incoherence collective behavior in financial market", "source": "Shangmei Zhao, Qiuchao Xie, Qing Lu, Xin Jiang and Wei Chen", "docs_id": "1605.02283", "section": ["q-fin.ST", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coherence and incoherence collective behavior in financial market. Financial markets have been extensively studied as highly complex evolving systems. In this paper, we quantify financial price fluctuations through a coupled dynamical system composed of phase oscillators. We find a Financial Coherence and Incoherence (FCI) coexistence collective behavior emerges as the system evolves into the stable state, in which the stocks split into two groups: one is represented by coherent, phase-locked oscillators, the other is composed of incoherent, drifting oscillators. It is demonstrated that the size of the coherent stock groups fluctuates during the economic periods according to real-world financial instabilities or shocks. Further, we introduce the coherent characteristic matrix to characterize the involvement dynamics of stocks in the coherent groups. Clustering results on the matrix provides a novel manifestation of the correlations among stocks in the economic periods. Our analysis for components of the groups is consistent with the Global Industry Classification Standard (GICS) classification and can also figure out features for newly developed industries. These results can provide potentially implications on characterizing inner dynamical structure of financial markets and making optimal investment tragedies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Financial Coherence and Incoherence (FCI) model, what is the significance of the coherent characteristic matrix in characterizing the involvement dynamics of stocks in the coherent groups?\n\n**A)** It provides a measure of the overall market volatility.\n**B)** It characterizes the correlations among stocks within the coherent groups and can be used to identify clusters of highly correlated stocks.\n**C)** It is used to predict the future price movements of individual stocks.\n**D)** It is a tool for identifying the most influential stocks in the market.\n\n**Correct Answer:** B) It characterizes the correlations among stocks within the coherent groups and can be used to identify clusters of highly correlated stocks.\n\n**Explanation:** The coherent characteristic matrix is introduced in the paper as a tool to characterize the involvement dynamics of stocks in the coherent groups. By analyzing the clustering results on the matrix, researchers can identify correlations among stocks within the economic periods, which can provide insights into the inner dynamical structure of financial markets. This is a key finding of the paper, and the correct answer reflects this.\n\nNote: The other options are incorrect because they do not accurately reflect the significance of the coherent characteristic matrix in the context of the FCI model. Option A is too broad and does not specifically relate to the coherent characteristic matrix. Option C is incorrect because predicting future price movements is not the primary purpose of the coherent characteristic matrix. Option D is also incorrect because the coherent characteristic matrix is not used to identify the most influential stocks in the market."}, "11": {"documentation": {"title": "Incentive-Compatible Diffusion Auctions", "source": "Bin Li, Dong Hao, Dengji Zhao", "docs_id": "2001.06975", "section": ["cs.GT", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Incentive-Compatible Diffusion Auctions. Diffusion auction is a new model in auction design. It can incentivize the buyers who have already joined in the auction to further diffuse the sale information to others via social relations, whereby both the seller's revenue and the social welfare can be improved. Diffusion auctions are essentially non-typical multidimensional mechanism design problems and agents' social relations are complicatedly involved with their bids. In such auctions, incentive-compatibility (IC) means it is best for every agent to honestly report her valuation and fully diffuse the sale information to all her neighbors. Existing work identified some specific mechanisms for diffusion auctions, while a general theory characterizing all incentive-compatible diffusion auctions is still missing. In this work, we identify a sufficient and necessary condition for all dominant-strategy incentive-compatible (DSIC) diffusion auctions. We formulate the monotonic allocation policies in such multidimensional problems and show that any monotonic allocation policy can be implemented in a DSIC diffusion auction mechanism. Moreover, given any monotonic allocation policy, we obtain the optimal payment policy to maximize the seller's revenue."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Incentive-Compatible Diffusion Auctions\n\n**A)** A diffusion auction is a type of auction where buyers who have already joined the auction are incentivized to further diffuse the sale information to others via social relations, but the seller's revenue and social welfare are not necessarily improved.\n\n**B)** A general theory characterizing all incentive-compatible diffusion auctions is still missing, and existing work has identified specific mechanisms for diffusion auctions.\n\n**C)** Incentive-compatibility (IC) in diffusion auctions means that every agent should honestly report her valuation and fully diffuse the sale information to all her neighbors, but it does not necessarily imply that the seller's revenue is maximized.\n\n**D)** A sufficient and necessary condition for all dominant-strategy incentive-compatible (DSIC) diffusion auctions is that the allocation policy is monotonic, and any monotonic allocation policy can be implemented in a DSIC diffusion auction mechanism.\n\n**Correct Answer:** D) A sufficient and necessary condition for all dominant-strategy incentive-compatible (DSIC) diffusion auctions is that the allocation policy is monotonic, and any monotonic allocation policy can be implemented in a DSIC diffusion auction mechanism.\n\n**Explanation:**\n\nThis question requires the test-taker to understand the key concepts and findings in the paper \"Incentive-Compatible Diffusion Auctions\". The correct answer, D, is based on the statement in the paper that \"we identify a sufficient and necessary condition for all dominant-strategy incentive-compatible (DSIC) diffusion auctions. We formulate the monotonic allocation policies in such multidimensional problems and show that any monotonic allocation policy can be implemented in a DSIC diffusion auction mechanism.\"\n\nThe incorrect answers, A, B, and C, are designed to test the test-taker's understanding of the paper's main points. Answer A is incorrect because the paper states that diffusion auctions can incentivize buyers to further diffuse the sale information, which improves both the seller's revenue and social welfare. Answer B is incorrect because the paper states that a general theory characterizing all incentive-compatible diffusion auctions is still missing, and existing work has identified specific mechanisms for diffusion auctions. Answer C is incorrect because incentive-compatibility (IC) in diffusion auctions means that every agent should honestly report her valuation and fully diffuse the sale information to all her neighbors, which implies that the seller's revenue is maximized."}, "12": {"documentation": {"title": "Critical phenomena at the complex tensor ordering phase transition", "source": "Igor Boettcher, Igor F. Herbut", "docs_id": "1712.03981", "section": ["cond-mat.str-el", "cond-mat.stat-mech", "cond-mat.supr-con", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Critical phenomena at the complex tensor ordering phase transition. We investigate the critical properties of the phase transition towards complex tensor order that has been proposed to occur in spin-orbit coupled superconductors. For this purpose we formulate the bosonic field theory for fluctuations of the complex irreducible second-rank tensor order parameter close to the transition. We then determine the scale dependence of the couplings of the theory by means of the perturbative Renormalization Group (RG). For the isotropic system we generically detect a fluctuation-induced first-order phase transition. The initial values for the running couplings are determined by the underlying microscopic model for the tensorial order. As an example we study three-dimensional Luttinger semimetals with electrons at a quadratic band touching point. Whereas the strong-coupling transition of the model receives substantial fluctuation corrections, the weak-coupling transition at low temperatures is rendered only weakly first-order due to the presence of a fixed point in the vicinity of the RG trajectory. If the number of fluctuating complex components of the order parameter is reduced by cubic anisotropy, the theory maps onto the field theory for frustrated magnetism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the expected outcome of the Renormalization Group (RG) analysis for the complex tensor ordering phase transition in spin-orbit coupled superconductors, and how does the presence of a fixed point in the RG trajectory affect the nature of the phase transition?\n\nA) The RG analysis predicts a second-order phase transition, unaffected by the presence of a fixed point.\nB) The RG analysis detects a fluctuation-induced first-order phase transition, which is weakened by the presence of a fixed point.\nC) The RG analysis reveals a strong-coupling transition, which is rendered first-order due to the presence of a fixed point.\nD) The RG analysis indicates a phase transition that is unaffected by the number of fluctuating complex components of the order parameter.\n\nCorrect Answer: B) The RG analysis detects a fluctuation-induced first-order phase transition, which is weakened by the presence of a fixed point.\n\nExplanation: The correct answer is B) because the documentation states that the RG analysis detects a fluctuation-induced first-order phase transition, but notes that the weak-coupling transition at low temperatures is rendered only weakly first-order due to the presence of a fixed point in the vicinity of the RG trajectory. This indicates that the presence of a fixed point weakens the first-order nature of the phase transition."}, "13": {"documentation": {"title": "Complete NLO QCD study of single- and double-quarkonium hadroproduction\n  in the colour-evaporation model at the Tevatron and the LHC", "source": "Jean-Philippe Lansberg, Hua-Sheng Shao, Nodoka Yamanaka, Yu-Jie Zhang\n  and Camille No\\^us", "docs_id": "2004.14345", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complete NLO QCD study of single- and double-quarkonium hadroproduction\n  in the colour-evaporation model at the Tevatron and the LHC. We study the Single-Parton-Scattering (SPS) production of double quarkonia (J/psi+J/psi, J/psi+Upsilon, and Upsilon+Upsilon) in pp and pp(bar) collisions at the LHC and the Tevatron as measured by the CMS, ATLAS, LHCb, and D0 experiments in the Colour-Evaporation Model (CEM), based on the quark-hadron-duality, including Next-to-Leading Order (NLO) QCD corrections up to alpha_s^5. To do so, we also perform the first true NLO --up to alpha_s^4-- study of the p_T-differential cross section for single-quarkonium production. This allows us to fix the non-perturbative CEM parameters at NLO accuracy in the region where quarkonium-pair data are measured. Our results show that the CEM at NLO in general significantly undershoots these experimental data and, in view of the other existing SPS studies, confirm the need for Double Parton Scattering (DPS) to account for the data. Our NLO study of single-quarkonium production at mid and large p_T also confirms the difficulty of the approach to account for the measured p_T spectra; this is reminiscent of the impossibility to fit single-quarkonium data with the sole 3S18 NRQCD contribution from gluon fragmentation. We stress that the discrepancy occurs in a kinematical region where the new features of the improved CEM are not relevant."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the study of single- and double-quarkonium hadroproduction in the Colour-Evaporation Model (CEM) at the Tevatron and the LHC, and how does it relate to the need for Double Parton Scattering (DPS)?\n\nA) The CEM at NLO significantly overestimates the experimental data, and DPS is not necessary to account for the discrepancy.\nB) The CEM at NLO generally significantly undershoots the experimental data, and DPS is necessary to account for the discrepancy.\nC) The CEM at NLO accurately reproduces the experimental data, and DPS is not necessary.\nD) The CEM at NLO is unable to distinguish between single- and double-quarkonium production, and DPS is necessary to resolve this ambiguity.\n\nCorrect Answer: B) The CEM at NLO generally significantly undershoots the experimental data, and DPS is necessary to account for the discrepancy.\n\nExplanation: The study concludes that the CEM at NLO significantly undershoots the experimental data, particularly in the region where quarkonium-pair data are measured. This suggests that the CEM is not sufficient to accurately describe the production of quarkonia, and that Double Parton Scattering (DPS) is necessary to account for the discrepancy."}, "14": {"documentation": {"title": "Convective Dynamo Simulation with a Grand Minimum", "source": "Kyle Augustson, Sacha Brun, Mark Miesch, and Juri Toomre", "docs_id": "1503.04225", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convective Dynamo Simulation with a Grand Minimum. The global-scale dynamo action achieved in a simulation of a Sun-like star rotating at thrice the solar rate is assessed. The 3-D MHD Anelastic Spherical Harmonic (ASH) code, augmented with a viscosity minimization scheme, is employed to capture convection and dynamo processes in this G-type star. The simulation is carried out in a spherical shell that encompasses 3.8 density scale heights of the solar convection zone. It is found that dynamo action with a high degree of time variation occurs, with many periodic polarity reversals occurring roughly every 6.2 years. The magnetic energy also rises and falls with a regular period. The magnetic energy cycles arise from a Lorentz-force feedback on the differential rotation, whereas the processes leading to polarity reversals are more complex, appearing to arise from the interaction of convection with the mean toroidal fields. Moreover, an equatorial migration of toroidal field is found, which is linked to the changing differential rotation, and potentially to a nonlinear dynamo wave. This simulation also enters a grand minimum lasting roughly 20~years, after which the dynamo recovers its regular polarity cycles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism driving the periodic polarity reversals observed in the convective dynamo simulation of a Sun-like star?\n\nA) Lorentz-force feedback on the differential rotation\nB) Interaction of convection with the mean toroidal fields\nC) Nonlinear dynamo wave\nD) Viscosity minimization scheme\n\n**Correct Answer:** B) Interaction of convection with the mean toroidal fields\n\n**Explanation:** The correct answer is B) Interaction of convection with the mean toroidal fields. The text states that \"the processes leading to polarity reversals are more complex, appearing to arise from the interaction of convection with the mean toroidal fields.\" This suggests that the interaction between convection and the mean toroidal fields is the primary mechanism driving the periodic polarity reversals.\n\n**Explanation of incorrect options:**\n\nA) Lorentz-force feedback on the differential rotation: While the text mentions that the magnetic energy cycles arise from a Lorentz-force feedback on the differential rotation, this is not the primary mechanism driving the periodic polarity reversals.\n\nC) Nonlinear dynamo wave: The text does not mention a nonlinear dynamo wave as a mechanism driving the periodic polarity reversals.\n\nD) Viscosity minimization scheme: The text mentions that the 3-D MHD Anelastic Spherical Harmonic (ASH) code is augmented with a viscosity minimization scheme, but this is not related to the mechanism driving the periodic polarity reversals."}, "15": {"documentation": {"title": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition", "source": "Jos\\'e Henrique de Morais Goulart, Pierre Comon", "docs_id": "1712.05742", "section": ["math.NA", "cs.NA", "math.AG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition. Under the action of the general linear group with tensor structure, the ranks of matrices $A$ and $B$ forming an $m \\times n$ pencil $A + \\lambda B$ can change, but in a restricted manner. Specifically, with every pencil one can associate a pair of minimal ranks, which is unique up to a permutation. This notion can be defined for matrix pencils and, more generally, also for matrix polynomials of arbitrary degree. In this paper, we provide a formal definition of the minimal ranks, discuss its properties and the natural hierarchy it induces in a pencil space. Then, we show how the minimal ranks of a pencil can be determined from its Kronecker canonical form. For illustration, we classify the orbits according to their minimal ranks (under the action of the general linear group) in the case of real pencils with $m, n \\le 4$. Subsequently, we show that real regular $2k \\times 2k$ pencils having only complex-valued eigenvalues, which form an open positive-volume set, do not admit a best approximation (in the norm topology) on the set of real pencils whose minimal ranks are bounded by $2k-1$. Our results can be interpreted from a tensor viewpoint, where the minimal ranks of a degree-$(d-1)$ matrix polynomial characterize the minimal ranks of matrices constituting a block-term decomposition of an $m \\times n \\times d$ tensor into a sum of matrix-vector tensor products."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a real matrix pencil $A + \\lambda B$ of size $m \\times n$, where $A$ and $B$ are $m \\times n$ matrices. Suppose that the minimal ranks of $A$ and $B$ are $r_A$ and $r_B$, respectively. What can be concluded about the minimal ranks of the pencil $A + \\lambda B$ under the action of the general linear group with tensor structure?\n\nA) The minimal ranks of $A + \\lambda B$ are uniquely determined by the minimal ranks of $A$ and $B$.\n\nB) The minimal ranks of $A + \\lambda B$ are equal to the minimal ranks of $A$ and $B$.\n\nC) The minimal ranks of $A + \\lambda B$ are equal to the minimal ranks of $A$ and $B$ up to a permutation.\n\nD) The minimal ranks of $A + \\lambda B$ are not necessarily related to the minimal ranks of $A$ and $B$.\n\nCorrect Answer: C) The minimal ranks of $A + \\lambda B$ are equal to the minimal ranks of $A$ and $B$ up to a permutation.\n\nExplanation: The documentation states that the minimal ranks of a matrix pencil $A + \\lambda B$ can be associated with a pair of minimal ranks, which is unique up to a permutation. This means that the minimal ranks of $A + \\lambda B$ are equal to the minimal ranks of $A$ and $B$ up to a permutation."}, "16": {"documentation": {"title": "Topological phonons in an inhomogeneously strained silicon-4: Large spin\n  dependent thermoelectric response and thermal spin transfer torque due to\n  topological electronic magnetism of phonons", "source": "Ravindra G Bhardwaj, Anand Katailiha, Paul C. Lou, Ward P. Beyermann\n  and Sandeep Kumar", "docs_id": "2110.04939", "section": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological phonons in an inhomogeneously strained silicon-4: Large spin\n  dependent thermoelectric response and thermal spin transfer torque due to\n  topological electronic magnetism of phonons. The superposition of flexoelectronic doping and topological phonons give rise to topological electronic magnetism of phonon in an inhomogeneously strained Si in the bilayer structure with metal. In case of ferromagnetic metal and Si bilayer structure, the flexoelectronic doping will also give rise to larger spin current, which will lead to large spin to charge conversion due to topological electronic magnetism of phonon. By applying a temperature difference to ferromagnetic metal/Si bilayer structure under an applied strain gradient, a large thermoelectric response can be generated. In this experimental study, we report a large spin dependent thermoelectric response at Ni80Fe20/Si bilayer structure. The spin dependent response is found to be an order of magnitude larger than that in Pt thin films and similar to topological insulators surface states in spite of negligible intrinsic spin-orbit coupling of Si. This large response is attributed to the flexoelectronic doping and topological electronic magnetism of phonons, which was uncovered using topological Nernst effect measurement. This alternative and novel approach of using inhomogeneous strain engineering to address both spin current density and spin to charge conversion can open a new window to the realization of spintronics and spin-caloritronics devices using metal and doped-semiconductor layered materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism responsible for the large spin-dependent thermoelectric response observed in the Ni80Fe20/Si bilayer structure, and how does it differ from the response in Pt thin films?\n\nA) Topological electronic magnetism of phonons and flexoelectronic doping\nB) Topological electronic magnetism of phonons and intrinsic spin-orbit coupling of Si\nC) Flexoelectronic doping and topological electronic magnetism of phonons, but without strain gradient\nD) Intrinsic spin-orbit coupling of Si and topological electronic magnetism of phonons\n\nCorrect Answer: A) Topological electronic magnetism of phonons and flexoelectronic doping\n\nExplanation: The correct answer is A) Topological electronic magnetism of phonons and flexoelectronic doping. The documentation states that the large spin-dependent thermoelectric response is attributed to the flexoelectronic doping and topological electronic magnetism of phonons, which was uncovered using topological Nernst effect measurement. This is in contrast to the response in Pt thin films, which is not mentioned as being affected by these mechanisms."}, "17": {"documentation": {"title": "Deep Learning for Virtual Screening: Five Reasons to Use ROC Cost\n  Functions", "source": "Vladimir Golkov, Alexander Becker, Daniel T. Plop, Daniel\n  \\v{C}uturilo, Neda Davoudi, Jeffrey Mendenhall, Rocco Moretti, Jens Meiler,\n  Daniel Cremers", "docs_id": "2007.07029", "section": ["q-bio.BM", "cs.LG", "q-bio.QM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Learning for Virtual Screening: Five Reasons to Use ROC Cost\n  Functions. Computer-aided drug discovery is an essential component of modern drug development. Therein, deep learning has become an important tool for rapid screening of billions of molecules in silico for potential hits containing desired chemical features. Despite its importance, substantial challenges persist in training these models, such as severe class imbalance, high decision thresholds, and lack of ground truth labels in some datasets. In this work we argue in favor of directly optimizing the receiver operating characteristic (ROC) in such cases, due to its robustness to class imbalance, its ability to compromise over different decision thresholds, certain freedom to influence the relative weights in this compromise, fidelity to typical benchmarking measures, and equivalence to positive/unlabeled learning. We also propose new training schemes (coherent mini-batch arrangement, and usage of out-of-batch samples) for cost functions based on the ROC, as well as a cost function based on the logAUC metric that facilitates early enrichment (i.e. improves performance at high decision thresholds, as often desired when synthesizing predicted hit compounds). We demonstrate that these approaches outperform standard deep learning approaches on a series of PubChem high-throughput screening datasets that represent realistic and diverse drug discovery campaigns on major drug target families."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the key advantages of directly optimizing the receiver operating characteristic (ROC) in deep learning for virtual screening, and how do these advantages address the challenges of class imbalance, decision thresholds, and ground truth labels in silico screening?\n\n**A)** ROC optimization allows for the use of class weights, enables the adjustment of decision thresholds, and provides a more robust measure of model performance, particularly in cases with severe class imbalance.\n\n**B)** ROC optimization is robust to class imbalance, allows for compromise over different decision thresholds, and provides a measure of model performance that is equivalent to positive/unlabeled learning, making it an attractive approach for virtual screening.\n\n**C)** ROC optimization enables the use of ensemble methods, allows for the adjustment of decision thresholds, and provides a measure of model performance that is sensitive to the relative weights of different classes, making it a useful approach for virtual screening.\n\n**D)** ROC optimization is a variant of cost-sensitive learning, allows for the adjustment of decision thresholds, and provides a measure of model performance that is equivalent to the area under the ROC curve, making it a useful approach for virtual screening.\n\n**Correct Answer:** B) ROC optimization is robust to class imbalance, allows for compromise over different decision thresholds, and provides a measure of model performance that is equivalent to positive/unlabeled learning, making it an attractive approach for virtual screening.\n\n**Explanation:** The correct answer highlights the key advantages of ROC optimization in deep learning for virtual screening, including its robustness to class imbalance, ability to compromise over different decision thresholds, and equivalence to positive/unlabeled learning. These advantages address the challenges of class imbalance, decision thresholds, and ground truth labels in silico screening, making ROC optimization a useful approach for virtual screening."}, "18": {"documentation": {"title": "Efficient and accurate extraction of in vivo calcium signals from\n  microendoscopic video data", "source": "Pengcheng Zhou, Shanna L. Resendez, Jose Rodriguez-Romaguera, Jessica\n  C. Jimenez, Shay Q. Neufeld, Garret D. Stuber, Rene Hen, Mazen A. Kheirbek,\n  Bernardo L. Sabatini, Robert E. Kass, Liam Paninski", "docs_id": "1605.07266", "section": ["q-bio.NC", "q-bio.QM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient and accurate extraction of in vivo calcium signals from\n  microendoscopic video data. In vivo calcium imaging through microscopes has enabled deep brain imaging of previously inaccessible neuronal populations within the brains of freely moving subjects. However, microendoscopic data suffer from high levels of background fluorescence as well as an increased potential for overlapping neuronal signals. Previous methods fail in identifying neurons and demixing their temporal activity because the cellular signals are often submerged in the large fluctuating background. Here we develop an efficient method to extract cellular signals with minimal influence from the background. We model the background with two realistic components: (1) one models the constant baseline and slow trends of each pixel, and (2) the other models the fast fluctuations from out-of-focus signals and is therefore constrained to have low spatial-frequency structure. This decomposition avoids cellular signals being absorbed into the background term. After subtracting the background approximated with this model, we use Constrained Nonnegative Matrix Factorization (CNMF, Pnevmatikakis et al. (2016)) to better demix neural signals and get their denoised and deconvolved temporal activity. We validate our method on simulated and experimental data, where it shows fast, reliable, and high quality signal extraction under a wide variety of imaging parameters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed method for extracting cellular signals from microendoscopic video data, and how does it address the challenges of background fluorescence and overlapping neuronal signals?\n\n**A)** The primary goal is to model the background fluorescence and use it to enhance the visibility of cellular signals. However, this approach would likely amplify the noise and make it more difficult to demix neuronal signals.\n\n**B)** The primary goal is to use Constrained Nonnegative Matrix Factorization (CNMF) to demix neural signals and get their denoised and deconvolved temporal activity, but this approach does not address the issue of background fluorescence.\n\n**C)** The primary goal is to model the background with two realistic components: one for the constant baseline and slow trends, and the other for the fast fluctuations from out-of-focus signals, which allows for the separation of cellular signals from the background.\n\n**D)** The primary goal is to use a traditional filtering approach to remove the background fluorescence, but this approach would likely be ineffective in removing the fast fluctuations from out-of-focus signals.\n\n**Correct Answer:** C) The primary goal is to model the background with two realistic components: one for the constant baseline and slow trends, and the other for the fast fluctuations from out-of-focus signals, which allows for the separation of cellular signals from the background.\n\n**Explanation:** The correct answer is C) because the proposed method specifically addresses the challenges of background fluorescence and overlapping neuronal signals by modeling the background with two realistic components. This approach allows for the separation of cellular signals from the background, enabling the extraction of high-quality temporal activity from the microendoscopic video data. The other options are incorrect because they either do not address the challenges of background fluorescence (B) or propose an ineffective approach (A and D)."}, "19": {"documentation": {"title": "Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding", "source": "Vardan Papyan, Jeremias Sulam and Michael Elad", "docs_id": "1607.02005", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding. The celebrated sparse representation model has led to remarkable results in various signal processing tasks in the last decade. However, despite its initial purpose of serving as a global prior for entire signals, it has been commonly used for modeling low dimensional patches due to the computational constraints it entails when deployed with learned dictionaries. A way around this problem has been proposed recently, adopting a convolutional sparse representation model. This approach assumes that the global dictionary is a concatenation of banded Circulant matrices. Although several works have presented algorithmic solutions to the global pursuit problem under this new model, very few truly-effective guarantees are known for the success of such methods. In the first of this two-part work, we address the theoretical aspects of the sparse convolutional model, providing the first meaningful answers to corresponding questions of uniqueness of solutions and success of pursuit algorithms. To this end, we generalize mathematical quantities, such as the $\\ell_0$ norm, the mutual coherence and the Spark, to their counterparts in the convolutional setting, which intrinsically capture local measures of the global model. In a companion paper, we extend the analysis to a noisy regime, addressing the stability of the sparsest solutions and pursuit algorithms, and demonstrate practical approaches for solving the global pursuit problem via simple local processing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the traditional sparse representation model when deployed with learned dictionaries, and how does the convolutional sparse representation model address this issue?\n\n**A)** The traditional sparse representation model is limited by its inability to handle large-scale signals, whereas the convolutional sparse representation model addresses this issue by using a global dictionary that is a concatenation of banded Circulant matrices.\n\n**B)** The traditional sparse representation model is limited by its computational complexity, whereas the convolutional sparse representation model addresses this issue by using a local processing approach that simplifies the global pursuit problem.\n\n**C)** The traditional sparse representation model is limited by its inability to capture local measures of the global model, whereas the convolutional sparse representation model addresses this issue by generalizing mathematical quantities such as the $\\ell_0$ norm and mutual coherence to their counterparts in the convolutional setting.\n\n**D)** The traditional sparse representation model is limited by its inability to handle non-linear signals, whereas the convolutional sparse representation model addresses this issue by using a non-linear dictionary that captures local patterns in the signal.\n\n**Correct Answer:** C) The traditional sparse representation model is limited by its inability to capture local measures of the global model, whereas the convolutional sparse representation model addresses this issue by generalizing mathematical quantities such as the $\\ell_0$ norm and mutual coherence to their counterparts in the convolutional setting.\n\n**Explanation:** The correct answer is C) because the traditional sparse representation model is limited by its inability to capture local measures of the global model, whereas the convolutional sparse representation model addresses this issue by generalizing mathematical quantities such as the $\\ell_0$ norm and mutual coherence to their counterparts in the convolutional setting. This is stated in the introduction of the paper: \"Although several works have presented algorithmic solutions to the global pursuit problem under this new model, very few truly-effective guarantees are known for the success of such methods. In the first of this two-part work, we address the theoretical aspects of the sparse convolutional model, providing the first meaningful answers to corresponding questions of uniqueness of solutions and success of pursuit algorithms.\""}, "20": {"documentation": {"title": "Parametric Instabilities in Resonantly-Driven Bose-Einstein Condensates", "source": "S. Lellouch and N. Goldman", "docs_id": "1711.08832", "section": ["cond-mat.quant-gas", "cond-mat.mes-hall", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parametric Instabilities in Resonantly-Driven Bose-Einstein Condensates. Shaking optical lattices in a resonant manner offers an efficient and versatile method to devise artificial gauge fields and topological band structures for ultracold atomic gases. This was recently demonstrated through the experimental realization of the Harper-Hofstadter model, which combined optical superlattices and resonant time-modulations. Adding inter-particle interactions to these engineered band systems is expected to lead to strongly-correlated states with topological features, such as fractional Chern insulators. However, the interplay between interactions and external time-periodic drives typically triggers violent instabilities and uncontrollable heating, hence potentially ruling out the possibility of accessing such intriguing states of matter in experiments. In this work, we study the early-stage parametric instabilities that occur in systems of resonantly-driven Bose-Einstein condensates in optical lattices. We apply and extend an approach based on Bogoliubov theory [PRX 7, 021015 (2017)] to a variety of resonantly-driven band models, from a simple shaken Wannier-Stark ladder to the more intriguing driven-induced Harper-Hofstadter model. In particular, we provide ab initio numerical and analytical predictions for the stability properties of these topical models. This work sheds light on general features that could guide current experiments to stable regimes of operation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in creating strongly-correlated states with topological features in ultracold atomic gases, and how does the study of parametric instabilities in resonantly-driven Bose-Einstein condensates address this challenge?\n\n**A)** The primary challenge is the difficulty in achieving a stable equilibrium between inter-particle interactions and external time-periodic drives, which can be addressed by studying the stability properties of driven-induced band models.\n\n**B)** The primary challenge is the need for a more efficient method to devise artificial gauge fields and topological band structures, which can be addressed by exploring the effects of inter-particle interactions on these engineered band systems.\n\n**C)** The primary challenge is the violent instabilities and uncontrollable heating that occur when inter-particle interactions are added to engineered band systems, which can be addressed by developing an approach based on Bogoliubov theory to study the early-stage parametric instabilities in resonantly-driven Bose-Einstein condensates.\n\n**D)** The primary challenge is the lack of understanding of the interplay between interactions and external time-periodic drives, which can be addressed by providing ab initio numerical and analytical predictions for the stability properties of driven-induced band models.\n\n**Correct Answer:** C) The primary challenge is the violent instabilities and uncontrollable heating that occur when inter-particle interactions are added to engineered band systems, which can be addressed by developing an approach based on Bogoliubov theory to study the early-stage parametric instabilities in resonantly-driven Bose-Einstein condensates.\n\n**Explanation:** The correct answer is C) because the study of parametric instabilities in resonantly-driven Bose-Einstein condensates is specifically aimed at addressing the challenge of violent instabilities and uncontrollable heating that occur when inter-particle interactions are added to engineered band systems. The approach based on Bogoliubov theory is designed to provide a better understanding of these instabilities and to shed light on general features that could guide current experiments to stable regimes of operation."}, "21": {"documentation": {"title": "Asymmetric response to PMI announcements in China's stock returns", "source": "Yingli Wang and Xiaoguang Yang", "docs_id": "1806.04347", "section": ["q-fin.ST", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric response to PMI announcements in China's stock returns. Considered an important macroeconomic indicator, the Purchasing Managers' Index (PMI) on Manufacturing generally assumes that PMI announcements will produce an impact on stock markets. International experience suggests that stock markets react to negative PMI news. In this research, we empirically investigate the stock market reaction towards PMI in China. The asymmetric effects of PMI announcements on the stock market are observed: no market reaction is generated towards negative PMI announcements, while a positive reaction is generally generated for positive PMI news. We further find that the positive reaction towards the positive PMI news occurs 1 day before the announcement and lasts for nearly 3 days, and the positive reaction is observed in the context of expanding economic conditions. By contrast, the negative reaction towards negative PMI news is prevalent during downward economic conditions for stocks with low market value, low institutional shareholding ratios or high price earnings. Our study implies that China's stock market favors risk to a certain extent given the vast number of individual investors in the country, and there may exist information leakage in the market."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on asymmetric response to PMI announcements in China's stock returns suggests that the stock market reacts differently to positive and negative PMI news. What are the two conditions under which a negative reaction towards negative PMI news is observed, according to the study?\n\nA) During upward economic conditions and high institutional shareholding ratios\nB) During downward economic conditions and low market value\nC) During expanding economic conditions and low price earnings\nD) During contracting economic conditions and high institutional shareholding ratios\n\n**Correct Answer:** B) During downward economic conditions and low market value\n\n**Explanation:** According to the study, the negative reaction towards negative PMI news is observed during downward economic conditions, particularly in stocks with low market value. This suggests that the study found a specific set of conditions under which investors are more likely to react negatively to bad news, such as declining economic conditions and low market value."}, "22": {"documentation": {"title": "Mixed Bino-Wino-Higgsino Dark Matter in Gauge Messenger Models", "source": "Kyu Jung Bae, Radovan Dermisek, Hyung Do Kim and Ian-Woo Kim", "docs_id": "hep-ph/0702041", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mixed Bino-Wino-Higgsino Dark Matter in Gauge Messenger Models. Almost degenerate bino and wino masses at the weak scale is one of unique features of gauge messenger models. The lightest neutralino is a mixture of bino, wino and higgsino and can produce the correct amount of the dark matter density if it is the lightest supersymmetric particle. Furthermore, as a result of squeezed spectrum of superpartners which is typical for gauge messenger models, various co-annihilation and resonance regions overlap and very often the correct amount of the neutralino relic density is generated as an interplay of several processes. This feature makes the explanation of the observed amount of the dark matter density much less sensitive to fundamental parameters. We calculate the neutralino relic density assuming thermal history and present both spin independent and spin dependent cross sections for the direct detection. We also discuss phenomenological constraints from b to s gamma and muon g-2 and compare results of gauge messenger models to well known results of the mSUGRA scenario."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In gauge messenger models, the lightest neutralino is a mixture of bino, wino, and higgsino components. Assuming a thermal history, calculate the neutralino relic density and compare it to the observed value of the dark matter density. Which of the following scenarios is most likely to generate the correct amount of neutralino relic density, and explain your reasoning?\n\nA) The lightest supersymmetric particle is the bino, with a mass of 100 GeV.\nB) The lightest supersymmetric particle is a mixture of bino and wino, with masses of 50 GeV and 150 GeV, respectively.\nC) The lightest supersymmetric particle is a mixture of bino, wino, and higgsino, with masses of 20 GeV, 50 GeV, and 100 GeV, respectively.\nD) The lightest supersymmetric particle is the higgsino, with a mass of 200 GeV.\n\nCorrect Answer: C) The lightest supersymmetric particle is a mixture of bino, wino, and higgsino, with masses of 20 GeV, 50 GeV, and 100 GeV, respectively.\n\nExplanation: The correct answer is C) because the problem statement mentions that the lightest neutralino is a mixture of bino, wino, and higgsino, and that the correct amount of neutralino relic density is generated as an interplay of several processes. The scenario in C) takes into account the squeezed spectrum of superpartners, which is typical for gauge messenger models, and the overlap of co-annihilation and resonance regions, which can lead to the correct amount of neutralino relic density. The other options do not take into account these features, and therefore are not correct."}, "23": {"documentation": {"title": "Causal inference with misspecified exposure mappings", "source": "Fredrik S\\\"avje", "docs_id": "2103.06471", "section": ["math.ST", "econ.EM", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal inference with misspecified exposure mappings. Exposure mappings facilitate investigations of complex causal effects when units interact in experiments. Current methods assume that the exposures are correctly specified, but such an assumption cannot be verified, and its validity is often questionable. This paper describes conditions under which one can draw inferences about exposure effects when the exposures are misspecified. The main result is a proof of consistency under mild conditions on the errors introduced by the misspecification. The rate of convergence is determined by the dependence between units' specification errors, and consistency is achieved even if the errors are large as long as they are sufficiently weakly dependent. In other words, exposure effects can be precisely estimated also under misspecification as long as the units' exposures are not misspecified in the same way. The limiting distribution of the estimator is discussed. Asymptotic normality is achieved under stronger conditions than those needed for consistency. Similar conditions also facilitate conservative variance estimation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a causal inference problem where the exposure mappings are misspecified, but the units' exposures are correctly specified. Suppose we have a sample of n units, each with a binary exposure variable X_i and an outcome variable Y_i. The exposure mappings are misspecified, but the errors introduced by the misspecification are weakly dependent. What is the rate of convergence of the estimator for the exposure effect under these conditions?\n\n**A)** O(1/n)\n**B)** O(1/n^2)\n**C)** O(1/n^3)\n**D)** O(1/n^4)\n\n**Correct Answer:** B) O(1/n^2)\n\n**Explanation:** The main result of the paper states that the rate of convergence of the estimator for the exposure effect is determined by the dependence between units' specification errors. Since the errors are weakly dependent, the rate of convergence is O(1/n^2), where n is the sample size. This is because the dependence between errors leads to a slower rate of convergence compared to the case where errors are independent.\n\nNote: The other options are incorrect because:\n\n* A) O(1/n) is the rate of convergence for the case where errors are independent, not weakly dependent.\n* C) O(1/n^3) is too slow a rate of convergence, and is not supported by the paper.\n* D) O(1/n^4) is also too slow a rate of convergence, and is not supported by the paper."}, "24": {"documentation": {"title": "Carbon isotope fractionation and depletion in TMC1", "source": "H. S. Liszt, L. M. Ziurys", "docs_id": "1201.0696", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Carbon isotope fractionation and depletion in TMC1. 12C/13C isotopologue abundance anomalies have long been predicted for gas-phase chemistry in molecules other than CO and have recently been observed in the Taurus molecular cloud in several species hosting more than one carbon atom, i.e. CCH, CCS, CCCS and HC$_3$N. Here we work to ascertain whether these isotopologic anomalies actually result from the predicted depletion of the 13C+ ion in an oxygen-rich optically-shielded dense gas, or from some other more particular mechanism or mechanisms. We observed $\\lambda$3mm emission from carbon, sulfur and nitrogen-bearing isotopologues of HNC, CS and \\HH CS at three positions in Taurus(TMC1, L1527 and the ammonia peak) using the ARO 12m telescope. We saw no evidence of 12C/13C anomalies in our observations. Although the pool of C+ is likely to be depleted in 13C 13C is not depleted in the general pool of carbon outside CO, which probably exists mostly in the form of C^0. The observed isotopologic abundance anomalies are peculiar to those species in which they are found."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism behind the observed 12C/13C isotopologue abundance anomalies in carbon-bearing molecules in the Taurus molecular cloud?\n\nA) The depletion of the 13C+ ion in an oxygen-rich optically-shielded dense gas.\nB) The presence of a specific catalyst that enhances the formation of 13C-bearing molecules.\nC) The influence of external radiation on the chemical composition of the cloud.\nD) The inherent properties of the carbon atom itself, which leads to a preferential formation of 12C-bearing molecules.\n\nCorrect Answer: A) The depletion of the 13C+ ion in an oxygen-rich optically-shielded dense gas.\n\nExplanation: The question requires the test-taker to understand the context of the Arxiv documentation and the specific mechanism proposed to explain the observed isotopologue abundance anomalies. The correct answer is based on the text, which states that the anomalies are likely to result from the predicted depletion of the 13C+ ion in an oxygen-rich optically-shielded dense gas. The other options are incorrect because they are not supported by the text and are not plausible mechanisms for explaining the observed anomalies. \n\nNote: The question requires the test-taker to analyze the text, identify the key information, and make a logical connection between the information and the correct answer. This requires critical thinking and analytical skills, making it a challenging question for an exam."}, "25": {"documentation": {"title": "Double Robust Mass-Imputation with Matching Estimators", "source": "Ali Furkan Kalay", "docs_id": "2110.09275", "section": ["stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Double Robust Mass-Imputation with Matching Estimators. This paper proposes using a method named Double Score Matching (DSM) to do mass-imputation and presents an application to make inferences with a nonprobability sample. DSM is a $k$-Nearest Neighbors algorithm that uses two balance scores instead of covariates to reduce the dimension of the distance metric and thus to achieve a faster convergence rate. DSM mass-imputation and population inference are consistent if one of two balance score models is correctly specified. Simulation results show that the DSM performs better than recently developed double robust estimators when the data generating process has nonlinear confounders. The nonlinearity of the DGP is a major concern because it cannot be tested, and it leads to a violation of the assumptions required to achieve consistency. Even if the consistency of the DSM relies on the two modeling assumptions, it prevents bias from inflating under such cases because DSM is a semiparametric estimator. The confidence intervals are constructed using a wild bootstrapping approach. The proposed bootstrapping method generates valid confidence intervals as long as DSM is consistent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Double Robust Mass-Imputation with Matching Estimators, what is the primary advantage of using the Double Score Matching (DSM) algorithm, and how does it address the challenge of nonlinearity in the data generating process?\n\n**A)** DSM is a parametric estimator that assumes linearity in the confounding variables, allowing for efficient estimation of the mass-imputation model. However, this assumption can be violated in practice, leading to inconsistent estimates.\n\n**B)** DSM is a semiparametric estimator that uses two balance scores to reduce the dimension of the distance metric, achieving a faster convergence rate and preventing bias from inflating under nonlinearity. This approach allows for consistent estimation even when the data generating process has nonlinear confounders.\n\n**C)** DSM is a k-Nearest Neighbors algorithm that relies on the correct specification of a balance score model to achieve consistency. However, the nonlinearity of the data generating process can lead to a violation of this assumption, resulting in inconsistent estimates.\n\n**D)** DSM is a robust estimator that uses a wild bootstrapping approach to construct confidence intervals, which are valid as long as the DSM is consistent. However, this approach does not address the challenge of nonlinearity in the data generating process.\n\n**Correct Answer:** B) DSM is a semiparametric estimator that uses two balance scores to reduce the dimension of the distance metric, achieving a faster convergence rate and preventing bias from inflating under nonlinearity. This approach allows for consistent estimation even when the data generating process has nonlinear confounders.\n\n**Explanation:** The correct answer highlights the key advantage of DSM, which is its ability to address the challenge of nonlinearity in the data generating process. By using two balance scores, DSM reduces the dimension of the distance metric, leading to a faster convergence rate and preventing bias from inflating under nonlinearity. This approach allows for consistent estimation even when the data generating process has nonlinear confounders, making it a valuable tool for mass-imputation and population inference."}, "26": {"documentation": {"title": "Estimating the causal effect of an intervention in a time series\n  setting: the C-ARIMA approach", "source": "Fiammetta Menchetti, Fabrizio Cipollini, Fabrizia Mealli", "docs_id": "2103.06740", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating the causal effect of an intervention in a time series\n  setting: the C-ARIMA approach. The Rubin Causal Model (RCM) is a framework that allows to define the causal effect of an intervention as a contrast of potential outcomes. In recent years, several methods have been developed under the RCM to estimate causal effects in time series settings. None of these makes use of ARIMA models, which are instead very common in the econometrics literature. In this paper, we propose a novel approach, C-ARIMA, to define and estimate the causal effect of an intervention in a time series setting under the RCM. We first formalize the assumptions enabling the definition, the estimation and the attribution of the effect to the intervention; we then check the validity of the proposed method with an extensive simulation study, comparing its performance against a standard intervention analysis approach. In the empirical application, we use C-ARIMA to assess the causal effect of a permanent price reduction on supermarket sales. The CausalArima R package provides an implementation of our proposed approach."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the C-ARIMA approach to estimating the causal effect of an intervention in a time series setting, what is the primary assumption that enables the definition and estimation of the causal effect under the Rubin Causal Model (RCM)?\n\n**A)** The intervention is a deterministic shock that affects the time series in a linear fashion.\n**B)** The time series is stationary and the intervention is a random shock that affects the series in a non-linear fashion.\n**C)** The time series is non-stationary and the intervention is a permanent change that affects the series in a linear fashion.\n**D)** The time series is stationary and the intervention is a permanent change that affects the series in a non-linear fashion.\n\n**Correct Answer:** C) The time series is non-stationary and the intervention is a permanent change that affects the series in a linear fashion.\n\n**Explanation:** The correct answer is C) because the C-ARIMA approach assumes that the time series is non-stationary and the intervention is a permanent change that affects the series in a linear fashion. This assumption is necessary to define and estimate the causal effect under the RCM. The other options are incorrect because they do not accurately reflect the assumptions of the C-ARIMA approach. Option A is incorrect because the intervention is not necessarily deterministic or linear. Option B is incorrect because the time series is assumed to be non-stationary, not stationary. Option D is incorrect because the intervention is assumed to be permanent and linear, not non-linear."}, "27": {"documentation": {"title": "Capturing Model Risk and Rating Momentum in the Estimation of\n  Probabilities of Default and Credit Rating Migrations", "source": "Marius Pfeuffer, Goncalo dos Reis, Greig smith", "docs_id": "1809.09889", "section": ["q-fin.RM", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Capturing Model Risk and Rating Momentum in the Estimation of\n  Probabilities of Default and Credit Rating Migrations. We present two methodologies on the estimation of rating transition probabilities within Markov and non-Markov frameworks. We first estimate a continuous-time Markov chain using discrete (missing) data and derive a simpler expression for the Fisher information matrix, reducing the computational time needed for the Wald confidence interval by a factor of a half. We provide an efficient procedure for transferring such uncertainties from the generator matrix of the Markov chain to the corresponding rating migration probabilities and, crucially, default probabilities. For our second contribution, we assume access to the full (continuous) data set and propose a tractable and parsimonious self-exciting marked point processes model able to capture the non-Markovian effect of rating momentum. Compared to the Markov model, the non-Markov model yields higher probabilities of default in the investment grades, but also lower default probabilities in some speculative grades. Both findings agree with empirical observations and have clear practical implications. We illustrate all methods using data from Moody's proprietary corporate credit ratings data set. Implementations are available in the R package ctmcd."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the non-Markov model proposed in the paper \"Capturing Model Risk and Rating Momentum in the Estimation of Probabilities of Default and Credit Rating Migrations\" over the Markov model?\n\n**A)** It provides more accurate default probabilities for investment grades.\n**B)** It captures the non-Markovian effect of rating momentum more effectively.\n**C)** It requires less computational time for estimating the Fisher information matrix.\n**D)** It is more parsimonious and tractable than the Markov model.\n\n**Correct Answer:** B) It captures the non-Markovian effect of rating momentum more effectively.\n\n**Explanation:** The paper highlights the limitations of the Markov model in capturing the non-Markovian effect of rating momentum, which is a key factor in credit rating migrations. The non-Markov model proposed in the paper addresses this limitation by incorporating the non-Markovian effect into a self-exciting marked point processes model. This allows for a more accurate estimation of rating transition probabilities and default probabilities, particularly in investment grades."}, "28": {"documentation": {"title": "Theoretical study of projectile fragmentation in the reactions\n  $^{112}$Sn + $^{112}$Sn and $^{124}$Sn + $^{124}$Sn at 1 GeV/nucleon", "source": "H. Imal, A. Ergun, N. Buyukcizmeci, R. Ogul, A.S. Botvina, W.\n  Trautmann", "docs_id": "1403.4786", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical study of projectile fragmentation in the reactions\n  $^{112}$Sn + $^{112}$Sn and $^{124}$Sn + $^{124}$Sn at 1 GeV/nucleon. We analyze the production cross sections and isotopic distributions of projectile-like residues in the reactions $^{112}$Sn + $^{112}$Sn and $^{124}$Sn + $^{124}$Sn at an incident beam energy of 1 GeV/nucleon measured with the FRS fragment separator at the GSI laboratory. Calculations within the statistical multifragmentation model (SMM) for an ensemble of excited sources were performed with ensemble parameters determined previously for similar reactions at 600 MeV/nucleon. The obtained good agreement with the experiment establishes the universal properties of the excited spectator systems produced during the dynamical stage of the reaction. It is furthermore confirmed that a significant reduction of the symmetry-energy term at the freeze-out stage of reduced density and high temperature is necessary to reproduce the experimental isotope distributions. A trend of decreasing symmetry energy for large neutron-rich fragments of low excitation energy is interpreted as a nuclear-structure effect."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred from the experimental results and the calculations within the statistical multifragmentation model (SMM) regarding the freeze-out stage of the reaction?\n\nA) The symmetry-energy term remains unchanged at the freeze-out stage, leading to a good agreement with the experimental isotope distributions.\nB) A significant reduction of the symmetry-energy term at the freeze-out stage is necessary to reproduce the experimental isotope distributions, indicating a change in the nuclear-structure properties.\nC) The freeze-out stage occurs at a fixed temperature and density, independent of the incident beam energy.\nD) The experimental isotope distributions are consistent with the predictions of the SMM, but the freeze-out stage is not well understood.\n\nCorrect Answer: B) A significant reduction of the symmetry-energy term at the freeze-out stage is necessary to reproduce the experimental isotope distributions, indicating a change in the nuclear-structure properties.\n\nExplanation: The correct answer is B) because the documentation states that \"a significant reduction of the symmetry-energy term at the freeze-out stage of reduced density and high temperature is necessary to reproduce the experimental isotope distributions.\" This indicates that the freeze-out stage is affected by the symmetry-energy term, and a change in this term is necessary to explain the experimental results. The other options are incorrect because they do not accurately reflect the information provided in the documentation. Option A is incorrect because the symmetry-energy term is reduced at the freeze-out stage, not unchanged. Option C is incorrect because the freeze-out stage is not fixed at a specific temperature and density. Option D is incorrect because the experimental results are consistent with the predictions of the SMM, but the freeze-out stage is not well understood, and the reduction of the symmetry-energy term is necessary to explain the results."}, "29": {"documentation": {"title": "Stock market volatility: An approach based on Tsallis entropy", "source": "Sonia R. Bentes, Rui Menezes, Diana A. Mendes", "docs_id": "0809.4570", "section": ["q-fin.ST", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stock market volatility: An approach based on Tsallis entropy. One of the major issues studied in finance that has always intrigued, both scholars and practitioners, and to which no unified theory has yet been discovered, is the reason why prices move over time. Since there are several well-known traditional techniques in the literature to measure stock market volatility, a central point in this debate that constitutes the actual scope of this paper is to compare this common approach in which we discuss such popular techniques as the standard deviation and an innovative methodology based on Econophysics. In our study, we use the concept of Tsallis entropy to capture the nature of volatility. More precisely, what we want to find out is if Tsallis entropy is able to detect volatility in stock market indexes and to compare its values with the ones obtained from the standard deviation. Also, we shall mention that one of the advantages of this new methodology is its ability to capture nonlinear dynamics. For our purpose, we shall basically focus on the behaviour of stock market indexes and consider the CAC 40, MIB 30, NIKKEI 225, PSI 20, IBEX 35, FTSE 100 and SP 500 for a comparative analysis between the approaches mentioned above."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the study on stock market volatility presented in the Arxiv paper, and how does it differ from traditional approaches to measuring volatility?\n\n**A)** The study aims to develop a new mathematical model to predict stock prices, and it differs from traditional approaches in its use of linear dynamics.\n\n**B)** The study seeks to compare the performance of different volatility measures, including Tsallis entropy and standard deviation, in capturing the nonlinear dynamics of stock market indexes.\n\n**C)** The study aims to investigate the relationship between stock market indexes and economic indicators, and it differs from traditional approaches in its use of econophysics.\n\n**D)** The study seeks to develop a new algorithm for trading stocks, and it differs from traditional approaches in its use of machine learning techniques.\n\n**Correct Answer:** B) The study seeks to compare the performance of different volatility measures, including Tsallis entropy and standard deviation, in capturing the nonlinear dynamics of stock market indexes.\n\n**Explanation:** The correct answer is B) because the study aims to compare the performance of Tsallis entropy and standard deviation in capturing the nonlinear dynamics of stock market indexes, which is the primary objective of the study. The other options are incorrect because they do not accurately reflect the main objective of the study. Option A is incorrect because the study does not aim to develop a new mathematical model to predict stock prices. Option C is incorrect because the study does not investigate the relationship between stock market indexes and economic indicators. Option D is incorrect because the study does not aim to develop a new algorithm for trading stocks."}, "30": {"documentation": {"title": "A data-driven robust optimization approach to scenario-based stochastic\n  model predictive control", "source": "Chao Shang and Fengqi You", "docs_id": "1807.05146", "section": ["math.OC", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A data-driven robust optimization approach to scenario-based stochastic\n  model predictive control. Stochastic model predictive control (SMPC) has been a promising solution to complex control problems under uncertain disturbances. However, traditional SMPC approaches either require exact knowledge of probabilistic distributions, or rely on massive scenarios that are generated to represent uncertainties. In this paper, a novel scenario-based SMPC approach is proposed by actively learning a data-driven uncertainty set from available data with machine learning techniques. A systematical procedure is then proposed to further calibrate the uncertainty set, which gives appropriate probabilistic guarantee. The resulting data-driven uncertainty set is more compact than traditional norm-based sets, and can help reducing conservatism of control actions. Meanwhile, the proposed method requires less data samples than traditional scenario-based SMPC approaches, thereby enhancing the practicability of SMPC. Finally the optimal control problem is cast as a single-stage robust optimization problem, which can be solved efficiently by deriving the robust counterpart problem. The feasibility and stability issue is also discussed in detail. The efficacy of the proposed approach is demonstrated through a two-mass-spring system and a building energy control problem under uncertain disturbances."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed data-driven robust optimization approach to scenario-based stochastic model predictive control, as described in the paper?\n\nA) It requires massive scenarios to represent uncertainties, unlike traditional SMPC approaches.\nB) It can be solved efficiently by deriving the robust counterpart problem, unlike traditional SMPC approaches.\nC) It actively learns a data-driven uncertainty set from available data, reducing the need for massive scenarios.\nD) It provides a more compact uncertainty set than traditional norm-based sets, reducing conservatism in control actions.\n\n**Correct Answer:** C) It actively learns a data-driven uncertainty set from available data, reducing the need for massive scenarios.\n\n**Explanation:** The correct answer is C) because the paper proposes a novel scenario-based SMPC approach that actively learns a data-driven uncertainty set from available data, which reduces the need for massive scenarios. This is a key advantage of the proposed approach, as it enhances the practicability of SMPC. The other options are incorrect because they either describe traditional SMPC approaches (A and B) or describe a different aspect of the proposed approach (D)."}, "31": {"documentation": {"title": "Agent-based model with multi-level herding for complex financial systems", "source": "Jun-Jie Chen, Lei Tan, Bo Zheng", "docs_id": "1504.01811", "section": ["q-fin.GN", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Agent-based model with multi-level herding for complex financial systems. In complex financial systems, the sector structure and volatility clustering are respectively important features of the spatial and temporal correlations. However, the microscopic generation mechanism of the sector structure is not yet understood. Especially, how to produce these two features in one model remains challenging. We introduce a novel interaction mechanism, i.e., the multi-level herding, in constructing an agent-based model to investigate the sector structure combined with volatility clustering. According to the previous market performance, agents trade in groups, and their herding behavior comprises the herding at stock, sector and market levels. Further, we propose methods to determine the key model parameters from historical market data, rather than from statistical fitting of the results. From the simulation, we obtain the sector structure and volatility clustering, as well as the eigenvalue distribution of the cross-correlation matrix, for the New York and Hong Kong stock exchanges. These properties are in agreement with the empirical ones. Our results quantitatively reveal that the multi-level herding is the microscopic generation mechanism of the sector structure, and provide new insight into the spatio-temporal interactions in financial systems at the microscopic level."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of introducing a novel interaction mechanism, multi-level herding, in constructing an agent-based model to investigate the sector structure combined with volatility clustering in complex financial systems?\n\nA) To improve the accuracy of statistical fitting of the results\nB) To determine the key model parameters from historical market data\nC) To produce the sector structure and volatility clustering in one model\nD) To analyze the eigenvalue distribution of the cross-correlation matrix\n\n**Correct Answer:** C) To produce the sector structure and volatility clustering in one model\n\n**Explanation:** The correct answer is C) To produce the sector structure and volatility clustering in one model. The question states that the primary goal of introducing the multi-level herding mechanism is to investigate the sector structure combined with volatility clustering, implying that the model aims to produce both features in a single model. This requires the test-taker to understand the context and purpose of the multi-level herding mechanism.\n\n**Candidate A:** This option is incorrect because improving the accuracy of statistical fitting of the results is not the primary goal of introducing the multi-level herding mechanism.\n\n**Candidate B:** This option is incorrect because determining the key model parameters from historical market data is a method proposed in the paper, but not the primary goal of introducing the multi-level herding mechanism.\n\n**Candidate C:** This is the correct answer, as explained above.\n\n**Candidate D:** This option is incorrect because analyzing the eigenvalue distribution of the cross-correlation matrix is a result obtained from the simulation, but not the primary goal of introducing the multi-level herding mechanism."}, "32": {"documentation": {"title": "Realistic Image Normalization for Multi-Domain Segmentation", "source": "Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers and\n  Herve Lombaert", "docs_id": "2009.14024", "section": ["cs.LG", "eess.IV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Realistic Image Normalization for Multi-Domain Segmentation. Image normalization is a building block in medical image analysis. Conventional approaches are customarily utilized on a per-dataset basis. This strategy, however, prevents the current normalization algorithms from fully exploiting the complex joint information available across multiple datasets. Consequently, ignoring such joint information has a direct impact on the performance of segmentation algorithms. This paper proposes to revisit the conventional image normalization approach by instead learning a common normalizing function across multiple datasets. Jointly normalizing multiple datasets is shown to yield consistent normalized images as well as an improved image segmentation. To do so, a fully automated adversarial and task-driven normalization approach is employed as it facilitates the training of realistic and interpretable images while keeping performance on-par with the state-of-the-art. The adversarial training of our network aims at finding the optimal transfer function to improve both the segmentation accuracy and the generation of realistic images. We evaluated the performance of our normalizer on both infant and adult brains images from the iSEG, MRBrainS and ABIDE datasets. Results reveal the potential of our normalization approach for segmentation, with Dice improvements of up to 57.5% over our baseline. Our method can also enhance data availability by increasing the number of samples available when learning from multiple imaging domains."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of conventional image normalization approaches in medical image analysis, and how does the proposed approach address this limitation?\n\nA) Conventional approaches are limited to a single normalization algorithm, whereas the proposed approach learns a common normalizing function across multiple datasets, thereby exploiting joint information.\n\nB) Conventional approaches are computationally expensive, whereas the proposed approach is faster and more efficient, resulting in improved segmentation accuracy.\n\nC) Conventional approaches are not interpretable, whereas the proposed approach facilitates the training of realistic and interpretable images, enabling better understanding of the underlying data.\n\nD) Conventional approaches are not scalable, whereas the proposed approach can handle large datasets and multiple imaging domains, increasing data availability and improving segmentation performance.\n\nCorrect Answer: A) Conventional approaches are limited to a single normalization algorithm, whereas the proposed approach learns a common normalizing function across multiple datasets, thereby exploiting joint information."}, "33": {"documentation": {"title": "Self-Organization and Fractality in a Metabolic Process of the Krebs\n  Cycle", "source": "V.I. Grytsay, I.V. Musatenko", "docs_id": "1710.09252", "section": ["q-bio.MN", "nlin.AO", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Self-Organization and Fractality in a Metabolic Process of the Krebs\n  Cycle. With the help of a mathematical model, the metabolic process of the Krebs cycle is studied. The autocatalytic processes resulting in both the formation of the self-organization in the Krebs cycle and the appearance of a cyclicity of its dynamics are determined. Some structural-functional connections creating the synchronism of an autoperiodic functioning at the transport in the respiratory chain and the oxidative phosphorylation are investigated. The conditions for breaking the synchronization of processes, increasing the multiplicity of a cyclicity, and for the appearance of chaotic modes are analyzed. The phase-parametric diagram of a cascade of bifurcations showing the transition to a chaotic mode by the Feigenbaum scenario is obtained. The fractal nature of the revealed cascade of bifurcations is demonstrated. The strange attractors formed as a result of the folding are obtained. The results obtained give the idea of structural-functional connections, due to which the self-organization appears in the metabolic process running in a cell. The constructed mathematical model can be applied to the study of the toxic and allergic effects of drugs and various substances on the metabolism of a cell."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which the self-organization in the Krebs cycle is achieved, according to the mathematical model presented in the study?\n\nA) Through the formation of a stable equilibrium state\nB) Through the autocatalytic processes that create a cascade of bifurcations\nC) Through the synchronization of autoperiodic functioning in the respiratory chain and oxidative phosphorylation\nD) Through the introduction of external noise or perturbations\n\nCorrect Answer: B) Through the autocatalytic processes that create a cascade of bifurcations\n\nExplanation: The study suggests that the autocatalytic processes resulting from the metabolic reactions in the Krebs cycle lead to the formation of self-organization and cyclicity in the dynamics of the process. This is achieved through the creation of a cascade of bifurcations, which is a key concept in chaos theory. The correct answer, B, reflects this understanding. The other options are incorrect because they do not accurately describe the primary mechanism of self-organization in the Krebs cycle according to the study. Option A is incorrect because the study does not suggest that a stable equilibrium state is the primary mechanism of self-organization. Option C is incorrect because while synchronization of autoperiodic functioning is investigated in the study, it is not the primary mechanism of self-organization. Option D is incorrect because the study does not suggest that external noise or perturbations are responsible for self-organization in the Krebs cycle."}, "34": {"documentation": {"title": "Causal Inference in Higher Education: Building Better Curriculums", "source": "Prableen Kaur, Agoritsa Polyzou and George Karypis", "docs_id": "1906.04698", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal Inference in Higher Education: Building Better Curriculums. Higher educational institutions constantly look for ways to meet students' needs and support them through graduation. Recent work in the field of learning analytics have developed methods for grade prediction and course recommendations. Although these methods work well, they often fail to discover causal relationships between courses, which may not be evident through correlation-based methods. In this work, we aim at understanding the causal relationships between courses to aid universities in designing better academic pathways for students and to help them make better choices. Our methodology employs methods of causal inference to study these relationships using historical student performance data. We make use of a doubly-robust method of matching and regression in order to obtain the casual relationship between a pair of courses. The results were validated by the existing prerequisite structure and by cross-validation of the regression model. Further, our approach was also tested for robustness and sensitivity to certain hyperparameters. This methodology shows promising results and is a step forward towards building better academic pathways for students."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of causal inference in higher education, what is the primary limitation of correlation-based methods in discovering causal relationships between courses, and how does the proposed methodology address this limitation?\n\n**A)** Correlation-based methods fail to discover causal relationships between courses because they are not sensitive to the underlying causal structure of the data. The proposed methodology addresses this limitation by using a doubly-robust method of matching and regression to estimate causal relationships.\n\n**B)** Correlation-based methods are limited by their inability to account for confounding variables that may affect student performance. The proposed methodology addresses this limitation by using a doubly-robust method of matching and regression to control for confounding variables.\n\n**C)** Correlation-based methods are not robust to changes in the data distribution, which can lead to biased estimates of causal relationships. The proposed methodology addresses this limitation by using a doubly-robust method of matching and regression to estimate causal relationships.\n\n**D)** Correlation-based methods are not sensitive to the specific characteristics of individual students, which can lead to inaccurate predictions of student performance. The proposed methodology addresses this limitation by using a doubly-robust method of matching and regression to estimate causal relationships.\n\n**Correct Answer:** A) Correlation-based methods fail to discover causal relationships between courses because they are not sensitive to the underlying causal structure of the data. The proposed methodology addresses this limitation by using a doubly-robust method of matching and regression to estimate causal relationships.\n\n**Explanation:** The question requires the test-taker to understand the limitations of correlation-based methods in discovering causal relationships between courses, and how the proposed methodology addresses this limitation. The correct answer, A, highlights the primary limitation of correlation-based methods (lack of sensitivity to the underlying causal structure of the data) and how the proposed methodology (doubly-robust method of matching and regression) addresses this limitation. The other options, while related to the topic, do not accurately capture the primary limitation of correlation-based methods and the proposed methodology."}, "35": {"documentation": {"title": "Retrofitted Natural Supersymmetry from a U(1)", "source": "Edward Hardy, John March-Russell", "docs_id": "1302.5423", "section": ["hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Retrofitted Natural Supersymmetry from a U(1). We propose that a single, spontaneously broken, U(1) gauge symmetry may be responsible for suppressing both the first two generation Yukawa couplings, and also, in a correlated manner, parameters in the dynamical supersymmetry (SUSY) breaking sector by the mechanism of retrofitting. In the dynamical SUSY breaking sector, these small parameters are typically required in order to introduce R-symmetry breaking in a controlled manner and obtain phenomenologically viable meta-stable vacua. The heavy U(1) multiplet mediates a dominant contribution to the first two generation MSSM sfermion soft masses, while gauge mediation provides a parametrically suppressed soft term contribution to the stop and most other states, so realising a natural SUSY spectrum in a fashion consistent with SUSY unification. In explicit models the spectra obtained can be such that current LHC limits are evaded, and predictions of flavour changing processes are consistent with observation. We examine both implementations with low scale mediation, and string-motivated examples where the U(1) is anomalous before the inclusion of a generalised Green-Schwarz mechanism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Retrofitted Natural Supersymmetry from a U(1), what is the primary mechanism by which the U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings, and how does this mechanism relate to the dynamical supersymmetry (SUSY) breaking sector?\n\nA) The U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings through a mechanism of R-symmetry breaking, which in turn leads to a correlated suppression of parameters in the SUSY breaking sector.\n\nB) The U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings through a mechanism of gauge mediation, which provides a parametrically suppressed soft term contribution to the stop and most other states.\n\nC) The U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings through a mechanism of retrofitting, which introduces a heavy U(1) multiplet that mediates a dominant contribution to the first two generation MSSM sfermion soft masses.\n\nD) The U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings through a mechanism of spontaneous symmetry breaking, which leads to a correlated suppression of parameters in the SUSY breaking sector.\n\nCorrect Answer: C) The U(1) gauge symmetry is responsible for suppressing the first two generation Yukawa couplings through a mechanism of retrofitting, which introduces a heavy U(1) multiplet that mediates a dominant contribution to the first two generation MSSM sfermion soft masses.\n\nExplanation: The correct answer is C) because the documentation states that the heavy U(1) multiplet mediates a dominant contribution to the first two generation MSSM sfermion soft masses, which is a key aspect of the retrofitting mechanism. The other options are incorrect because they either misdescribe the mechanism of suppression (A and D) or attribute it to gauge mediation (B), which is not the primary mechanism described in the documentation."}, "36": {"documentation": {"title": "Auto-DSP: Learning to Optimize Acoustic Echo Cancellers", "source": "Jonah Casebeer, Nicholas J. Bryan, Paris Smaragdis", "docs_id": "2110.04284", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Auto-DSP: Learning to Optimize Acoustic Echo Cancellers. Adaptive filtering algorithms are commonplace in signal processing and have wide-ranging applications from single-channel denoising to multi-channel acoustic echo cancellation and adaptive beamforming. Such algorithms typically operate via specialized online, iterative optimization methods and have achieved tremendous success, but require expert knowledge, are slow to develop, and are difficult to customize. In our work, we present a new method to automatically learn adaptive filtering update rules directly from data. To do so, we frame adaptive filtering as a differentiable operator and train a learned optimizer to output a gradient descent-based update rule from data via backpropagation through time. We demonstrate our general approach on an acoustic echo cancellation task (single-talk with noise) and show that we can learn high-performing adaptive filters for a variety of common linear and non-linear multidelayed block frequency domain filter architectures. We also find that our learned update rules exhibit fast convergence, can optimize in the presence of nonlinearities, and are robust to acoustic scene changes despite never encountering any during training."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of traditional adaptive filtering algorithms, according to the authors of the Auto-DSP paper?\n\nA) They are too complex and require significant computational resources.\nB) They are too slow to develop and require expert knowledge.\nC) They are too sensitive to noise and require frequent retraining.\nD) They are too limited in their ability to handle non-linearities.\n\n**Correct Answer:** B) They are too slow to develop and require expert knowledge.\n\n**Explanation:** The authors of the Auto-DSP paper mention that traditional adaptive filtering algorithms are \"slow to develop\" and require \"expert knowledge\", indicating that they are difficult to learn and require specialized knowledge to implement effectively. This limitation is a key problem that the authors aim to address with their new method, which learns adaptive filtering update rules directly from data."}, "37": {"documentation": {"title": "Towards physical principles of biological evolution", "source": "Mikhail I. Katsnelson, Yuri I. Wolf, Eugene V. Koonin", "docs_id": "1709.00284", "section": ["q-bio.OT", "cond-mat.stat-mech", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards physical principles of biological evolution. Biological systems reach organizational complexity that far exceeds the complexity of any known inanimate objects. Biological entities undoubtedly obey the laws of quantum physics and statistical mechanics. However, is modern physics sufficient to adequately describe, model and explain the evolution of biological complexity? Detailed parallels have been drawn between statistical thermodynamics and the population-genetic theory of biological evolution. Based on these parallels, we outline new perspectives on biological innovation and major transitions in evolution, and introduce a biological equivalent of thermodynamic potential that reflects the innovation propensity of an evolving population. Deep analogies have been suggested to also exist between the properties of biological entities and processes, and those of frustrated states in physics, such as glasses. We extend such analogies by examining frustration-type phenomena, such as conflicts between different levels of selection, in biological evolution. We further address evolution in multidimensional fitness landscapes from the point of view of percolation theory and suggest that percolation at level above the critical threshold dictates the tree-like evolution of complex organisms. Taken together, these multiple connections between fundamental processes in physics and biology imply that construction of a meaningful physical theory of biological evolution might not be a futile effort."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nQuestion: What theoretical framework is proposed to describe the evolution of biological complexity, and what fundamental processes in physics are suggested to be connected to biological evolution?\n\nA) The theory of evolution by natural selection, with a focus on genetic drift and mutation.\nB) Thermodynamic potential, percolation theory, and frustrated states in physics, with a focus on the evolution of complex organisms.\nC) The concept of self-organization, with a focus on the emergence of complex systems from simple interactions.\nD) The idea of epigenetics, with a focus on the role of gene expression in evolution.\n\nCorrect Answer: B) Thermodynamic potential, percolation theory, and frustrated states in physics, with a focus on the evolution of complex organisms.\n\nExplanation: The question requires the test-taker to understand the main idea of the text, which is that biological systems obey the laws of quantum physics and statistical mechanics, and that modern physics can be used to describe and explain the evolution of biological complexity. The correct answer, B, highlights the connection between thermodynamic potential, percolation theory, and frustrated states in physics, which are all mentioned in the text as relevant to the evolution of complex organisms. The other options are incorrect because they do not accurately reflect the main idea of the text. Option A is too narrow, focusing only on natural selection, while options C and D are unrelated to the text's main idea."}, "38": {"documentation": {"title": "The Quotient of Normal Random Variables And Application to Asset Price\n  Fat Tails", "source": "Carey Caginalp and Gunduz Caginalp", "docs_id": "1802.04778", "section": ["q-fin.MF", "math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Quotient of Normal Random Variables And Application to Asset Price\n  Fat Tails. The quotient of random variables with normal distributions is examined and proven to have have power law decay, with density $f\\left( x\\right) \\simeq f_{0}x^{-2}$, with the coefficient depending on the means and variances of the numerator and denominator and their correlation. We also obtain the conditional probability densities for each of the four quadrants given by the signs of the numerator and denominator for arbitrary correlation $\\rho \\in\\lbrack-1,1).$ For $\\rho=-1$ we obtain a particularly simple closed form solution for all $x\\in$ $\\mathbb{R}$. The results are applied to a basic issue in economics and finance, namely the density of relative price changes. Classical finance stipulates a normal distribution of relative price changes, though empirical studies suggest a power law at the tail end. By considering the supply and demand in a basic price change model, we prove that the relative price change has density that decays with an $x^{-2}$ power law. Various parameter limits are established."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider two normal random variables, X and Y, with means \u03bcX and \u03bcY, variances \u03c3X^2 and \u03c3Y^2, and correlation coefficient \u03c1. The quotient of these two random variables is given by Z = X/Y. Find the density function of Z, and determine the conditions under which it decays with a power law tail.\n\n**A)** The density function of Z is given by fZ(x) = (1/\u03c3X^2) \\* (1/\u03c3Y^2) \\* (1/\u221a(2\u03c0)) \\* exp(-x^2/(2\u03c3^2)), where \u03c3^2 = \u03c3X^2 + \u03c3Y^2 - 2\u03c1\u03c3X\u03c3Y.\n\n**B)** The density function of Z is given by fZ(x) = (1/\u03c3X^2) \\* (1/\u03c3Y^2) \\* (1/\u221a(2\u03c0)) \\* exp(-x^2/(2\u03c3^2)) \\* x^2, where \u03c3^2 = \u03c3X^2 + \u03c3Y^2 - 2\u03c1\u03c3X\u03c3Y.\n\n**C)** The density function of Z is given by fZ(x) = (1/\u03c3X^2) \\* (1/\u03c3Y^2) \\* (1/\u221a(2\u03c0)) \\* exp(-x^2/(2\u03c3^2)) \\* x^(-2), where \u03c3^2 = \u03c3X^2 + \u03c3Y^2 - 2\u03c1\u03c3X\u03c3Y.\n\n**D)** The density function of Z is given by fZ(x) = (1/\u03c3X^2) \\* (1/\u03c3Y^2) \\* (1/\u221a(2\u03c0)) \\* exp(-x^2/(2\u03c3^2)) \\* x^2 \\* (1 - \u03c1^2), where \u03c3^2 = \u03c3X^2 + \u03c3Y^2 - 2\u03c1\u03c3X\u03c3Y.\n\n**Correct Answer:** C) The density function of Z is given by fZ(x) = (1/\u03c3X^2) \\* (1/\u03c3Y^2) \\* (1/\u221a(2\u03c0)) \\* exp(-x^2/(2\u03c3^2)) \\* x^(-2), where \u03c3^2 = \u03c3X^2 + \u03c3Y^2 - 2\u03c1\u03c3X\u03c3Y.\n\n**Explanation:** The correct answer is C) because the quotient of two normal random variables with means \u03bcX and \u03bcY, variances \u03c3X^2 and \u03c3Y^2, and correlation coefficient \u03c1, has a density function that decays with a power law tail, specifically x^(-2). This result is proven in the original paper, and it is a key application of the theory to asset price modeling. The other options are incorrect because they do not accurately represent the density function of the quotient of two normal random variables."}, "39": {"documentation": {"title": "Nonlinear GARCH model and 1/f noise", "source": "Aleksejus Kononovicius, Julius Ruseckas", "docs_id": "1412.6244", "section": ["q-fin.ST", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear GARCH model and 1/f noise. Auto-regressive conditionally heteroskedastic (ARCH) family models are still used, by practitioners in business and economic policy making, as a conditional volatility forecasting models. Furthermore ARCH models still are attracting an interest of the researchers. In this contribution we consider the well known GARCH(1,1) process and its nonlinear modifications, reminiscent of NGARCH model. We investigate the possibility to reproduce power law statistics, probability density function and power spectral density, using ARCH family models. For this purpose we derive stochastic differential equations from the GARCH processes in consideration. We find the obtained equations to be similar to a general class of stochastic differential equations known to reproduce power law statistics. We show that linear GARCH(1,1) process has power law distribution, but its power spectral density is Brownian noise-like. However, the nonlinear modifications exhibit both power law distribution and power spectral density of the power law form, including 1/f noise."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider the GARCH(1,1) process and its nonlinear modifications, reminiscent of the Nonlinear GARCH model. Investigate the possibility of reproducing power law statistics, probability density function, and power spectral density using ARCH family models.\n\n**A)** The GARCH(1,1) process with nonlinear modifications exhibits a power law distribution, but its power spectral density is Brownian noise-like.\n\n**B)** The linear GARCH(1,1) process has a power law distribution, but its power spectral density is of the power law form, including 1/f noise.\n\n**C)** The Nonlinear GARCH model can reproduce power law statistics, probability density function, and power spectral density, including 1/f noise, but only for specific values of the model parameters.\n\n**D)** The GARCH(1,1) process with nonlinear modifications does not exhibit power law statistics, probability density function, or power spectral density, and is therefore not suitable for modeling financial time series.\n\n**Correct Answer:** B) The linear GARCH(1,1) process has a power law distribution, but its power spectral density is of the power law form, including 1/f noise.\n\n**Explanation:** The correct answer is B) because the documentation states that the linear GARCH(1,1) process has a power law distribution, but its power spectral density is Brownian noise-like. However, the nonlinear modifications of the GARCH(1,1) process exhibit both power law distribution and power spectral density of the power law form, including 1/f noise. Therefore, option B is the correct answer."}, "40": {"documentation": {"title": "Second Order Topological Insulator State in Hexagonal Lattices and its\n  Abundant Material Candidates", "source": "Shifeng Qian, Cheng-Cheng Liu, Yugui Yao", "docs_id": "2108.09511", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Second Order Topological Insulator State in Hexagonal Lattices and its\n  Abundant Material Candidates. We propose two mechanisms to realize the second order topological insulator (SOTI) state in spinless hexagonal lattices, viz., chemical modification and anti-Kekul\\'e/Kekul\\'e distortion of hexagonal lattice. Correspondingly, we construct two models and demonstrate the nontrivial band topology of the SOTI state characterized by the second Stiefel-Whitney class $w_2$ in the presence of inversion symmetry ($\\textit{P}$) and time-reversal symmetry ($\\textit{T}$). Based on the two mechanisms and using first-principles calculations and symmetry analysis, we predict three categories of real light element material candidates, i.e., hydrogenated and halogenated 2D hexagonal group IV materials XY (X=C, Si, Ge, Sn, Y=H, F, Cl), 2D hexagonal group V materials (blue phosphorene, blue arsenene, and black phosphorene, black arsenene), and the recent experimentally synthesized anti-Kekul\\'e/Kekul\\'e order graphenes and the counterparts of silicene/germanene/stanene. We explicitly demonstrate the nontrivial topological invariants and existence of the protected corner states with fractional charge for these candidates with giant bulk band gap (up to 3.5 eV), which could facilitate the experimental verification by STM. Our approaches and proposed abundant real material candidates will greatly enrich 2D SOTIs and promote their intriguing physics research."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism proposed to realize the second-order topological insulator (SOTI) state in spinless hexagonal lattices, according to the study?\n\nA) Chemical modification of the lattice structure\nB) Anti-Kekul\u00e9/Kekul\u00e9 distortion of the hexagonal lattice\nC) Introducing a non-zero magnetic moment to the lattice\nD) Applying a periodic electric field to the lattice\n\nCorrect Answer: B) Anti-Kekul\u00e9/Kekul\u00e9 distortion of the hexagonal lattice\n\nExplanation: The study proposes two mechanisms to realize the SOTI state in spinless hexagonal lattices, namely chemical modification and anti-Kekul\u00e9/Kekul\u00e9 distortion of the hexagonal lattice. The correct answer is B) Anti-Kekul\u00e9/Kekul\u00e9 distortion of the hexagonal lattice, as it is explicitly mentioned in the study as one of the proposed mechanisms. The other options are incorrect as they are not mentioned in the study as mechanisms for realizing the SOTI state."}, "41": {"documentation": {"title": "Reorientation kinetics of superparamagnetic nanostructured rods", "source": "J. Fresnais and J.-F. Berret, B. Frka-Petesic, O. Sandre and R.\n  Perzynski", "docs_id": "0809.4288", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reorientation kinetics of superparamagnetic nanostructured rods. The attractive interactions between oppositely charged species (colloids, macromolecules etc) dispersed in water are strong, and the direct mixing of solutions containing such species generally yields to a precipitation, or to a phase separation. We have recently developed means to control the electrostatically-driven attractions between nanoparticles and polymers in water, and at the same time to preserve the stability of the dispersions. We give here an account of the formation of supracolloidal aggregates obtained by co-assembly of 7 nm particles with copolymers. Nanostructured rods of length comprised between 5 and 50 microns and diameter 500 nm were investigated. By application of a magnetic field, the rods were found to reorient along with the magnetic field lines. The kinetics of reorientation was investigated using step changes of the magnetic field of amplitude 90 degrees. From the various results obtained, among which an exponential decay of the tangent of the angle made between the rod and the field, we concluded that the rods are superparamagnetic."}, "answer": "Here is the difficult multi-form question:\n\n**Question:** What is the primary mechanism underlying the reorientation of nanostructured rods in a magnetic field, as described in the study on \"Reorientation kinetics of superparamagnetic nanostructured rods\"?\n\nA) The rods undergo Brownian motion, causing them to reorient randomly in response to the magnetic field.\nB) The rods are driven by electrostatic interactions between the nanoparticles and the copolymers, which are aligned by the magnetic field.\nC) The rods are superparamagnetic, meaning they are able to rotate freely in response to the magnetic field due to the alignment of their magnetic moments.\nD) The rods undergo a phase transition, where the magnetic field induces a change in the rod's structure that causes them to reorient.\n\n**A)** Brownian motion is a random process that occurs due to thermal fluctuations, and it is not directly related to the reorientation of the rods in response to a magnetic field.\n\n**B)** While electrostatic interactions between the nanoparticles and copolymers are important in the study, they are not the primary mechanism underlying the reorientation of the rods in response to the magnetic field.\n\n**C)** This is the correct answer. The study concludes that the rods are superparamagnetic, meaning they are able to rotate freely in response to the magnetic field due to the alignment of their magnetic moments.\n\n**D)** Phase transitions are changes in the structure of a system in response to a change in temperature, pressure, or other external factors. While the study does describe the reorientation of the rods, it does not suggest that a phase transition is occurring.\n\n**Correct Answer:** C"}, "42": {"documentation": {"title": "Search for dark matter decay of the free neutron from the UCNA\n  experiment: n $\\rightarrow \\chi + e^+e^-$", "source": "X. Sun, E. Adamek, B. Allgeier, M. Blatnik, T. J. Bowles, L. J.\n  Broussard, M. A.-P. Brown, R. Carr, S. Clayton, C. Cude-Woods, S. Currie, E.\n  B. Dees, X. Ding, B. W. Filippone, A. Garc\\'ia, P. Geltenbort, S. Hasan, K.\n  P. Hickerson, J. Hoagland, R. Hong, G. E. Hogan, A. T. Holley, T. M. Ito, A.\n  Knecht, C.-Y. Liu, J. Liu, M. Makela, R. Mammei, J. W. Martin, D. Melconian,\n  M. P. Mendenhall, S. D. Moore, C. L. Morris, S. Nepal, N. Nouri, R. W.\n  Pattie, Jr., A. P\\'erez Galv\\'an, D. G. Phillips II, R. Picker, M. L. Pitt,\n  B. Plaster, J. C. Ramsey, R. Rios, D. J. Salvat, A. Saunders, W. Sondheim, S.\n  Sjue, S. Slutsky, C. Swank, G. Swift, E. Tatar, R. B. Vogelaar, B. VornDick,\n  Z. Wang, W. Wei, J. Wexler, T. Womack, C. Wrede, A. R. Young, and B. A. Zeck", "docs_id": "1803.10890", "section": ["nucl-ex", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for dark matter decay of the free neutron from the UCNA\n  experiment: n $\\rightarrow \\chi + e^+e^-$. It has been proposed recently that a previously unobserved neutron decay branch to a dark matter particle ($\\chi$) could account for the discrepancy in the neutron lifetime observed in experiments that use two different measurement techniques. One of the possible final states discussed includes a single $\\chi$ along with an $e^{+}e^{-}$ pair. We use data from the UCNA (Ultracold Neutron Asymmetry) experiment to set limits on this decay channel. Coincident electron-like events are detected with $\\sim 4\\pi$ acceptance using a pair of detectors that observe a volume of stored Ultracold Neutrons (UCNs). The summed kinetic energy ($E_{e^{+}e^{-}}$) from such events is used to set limits, as a function of the $\\chi$ mass, on the branching fraction for this decay channel. For $\\chi$ masses consistent with resolving the neutron lifetime discrepancy, we exclude this as the dominant dark matter decay channel at $\\gg~5\\sigma$ level for $100~\\text{keV} < E_{e^{+}e^{-}} < 644~\\text{keV}$. If the $\\chi+e^{+}e^{-}$ final state is not the only one, we set limits on its branching fraction of $< 10^{-4}$ for the above $E_{e^{+}e^{-}}$ range at $> 90\\%$ confidence level."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: The UCNA experiment searches for the decay of the free neutron into a dark matter particle (\u03c7) and an electron-positron pair (e+e-). Assuming the \u03c7+e+e- final state is the only possible decay channel, set limits on the branching fraction of this decay channel for \u03c7 masses consistent with resolving the neutron lifetime discrepancy.\n\nA) For \u03c7 masses between 100 keV and 644 keV, the branching fraction is less than 10^-4 at a 90% confidence level.\n\nB) For \u03c7 masses between 100 keV and 644 keV, the branching fraction is greater than 10^-4 at a 90% confidence level.\n\nC) For \u03c7 masses between 100 keV and 644 keV, the branching fraction is less than 10^-5 at a 5\u03c3 level.\n\nD) For \u03c7 masses between 100 keV and 644 keV, the branching fraction is greater than 10^-3 at a 5\u03c3 level.\n\nCorrect Answer: A) For \u03c7 masses between 100 keV and 644 keV, the branching fraction is less than 10^-4 at a 90% confidence level.\n\nExplanation: According to the text, the UCNA experiment sets limits on the branching fraction of the \u03c7+e+e- final state at a 90% confidence level, with a value less than 10^-4 for \u03c7 masses between 100 keV and 644 keV. This corresponds to option A. Options B, C, and D are incorrect because they either state the branching fraction is greater than 10^-4 (B and D) or less than 10^-5 (C), which is not supported by the text."}, "43": {"documentation": {"title": "The competition between surface adsorption and folding of fibril-forming\n  polypeptides", "source": "Ran Ni, J. Mieke Kleijn, Sanne Abeln, Martien A. Cohen Stuart, Peter\n  G. Bolhuis", "docs_id": "1502.01864", "section": ["cond-mat.soft", "physics.bio-ph", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The competition between surface adsorption and folding of fibril-forming\n  polypeptides. Self-assembly of polypeptides into fibrillar structures can be initiated by planar surfaces that interact favorably with certain residues. Using a coarse grained model, we systematically studied the folding and adsorption behavior of a $\\beta$-roll forming polypeptide. We find that there are two different folding pathways depending on the temperature: (i) at low temperature, the polypeptide folds in solution into a $\\beta$-roll before adsorbing onto the attractive surface, (ii) at higher temperature, the polypeptide first adsorbs in a disordered state, and folds while on the surface. The folding temperature increases with increasing attraction, as the folded $\\beta$-roll is stabilized by the surface. Surprisingly, further increasing the attraction lowers the folding temperature again, as strong attraction also stabilizes the adsorbed disordered state, which competes with folding of the polypeptide. Our results suggest that to enhance the folding, one should use a weakly attractive surface. They also explain the recent experimental observation of the non-monotonic effect of charge on the fibril formation on an oppositely charged surface [C. Charbonneau, et al., ACS Nano, 8, 2328 (2014)]."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The competition between surface adsorption and folding of fibril-forming polypeptides can be influenced by the temperature and the strength of the attractive surface. What is the relationship between the folding temperature and the attraction between the polypeptide and the surface, according to the study?\n\nA) The folding temperature increases with increasing attraction, as the folded \u03b2-roll is stabilized by the surface.\nB) The folding temperature decreases with increasing attraction, as the strong attraction stabilizes the adsorbed disordered state.\nC) The folding temperature remains constant regardless of the attraction between the polypeptide and the surface.\nD) The folding temperature increases with increasing attraction, but only up to a certain point, after which it decreases.\n\n**Correct Answer:** A) The folding temperature increases with increasing attraction, as the folded \u03b2-roll is stabilized by the surface.\n\n**Explanation:** According to the study, the folding temperature increases with increasing attraction between the polypeptide and the surface. This is because the folded \u03b2-roll is stabilized by the surface, making it more energetically favorable for the polypeptide to fold into a \u03b2-roll. The study also notes that further increasing the attraction lowers the folding temperature again, as strong attraction also stabilizes the adsorbed disordered state, which competes with folding of the polypeptide."}, "44": {"documentation": {"title": "Causal Non-Linear Financial Networks", "source": "Pawe{\\l} Fiedor", "docs_id": "1407.5020", "section": ["q-fin.ST", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal Non-Linear Financial Networks. In our previous study we have presented an approach to studying lead--lag effect in financial markets using information and network theories. Methodology presented there, as well as previous studies using Pearson's correlation for the same purpose, approached the concept of lead--lag effect in a naive way. In this paper we further investigate the lead--lag effect in financial markets, this time treating them as causal effects. To incorporate causality in a manner consistent with our previous study, that is including non-linear interdependencies, we base this study on a generalisation of Granger causality in the form of transfer entropy, or equivalently a special case of conditional (partial) mutual information. This way we are able to produce networks of stocks, where directed links represent causal relationships for a specific time lag. We apply this procedure to stocks belonging to the NYSE 100 index for various time lags, to investigate the short-term causality on this market, and to comment on the resulting Bonferroni networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the study \"Causal Non-Linear Financial Networks\", the authors generalize Granger causality to incorporate non-linear interdependencies by using transfer entropy and conditional (partial) mutual information. What is the primary advantage of using this approach over traditional methods such as Pearson's correlation, and how does it enable the construction of directed networks representing causal relationships between stocks?\n\n**A)** This approach allows for the detection of non-linear relationships between stocks, which may not be captured by traditional correlation methods. Additionally, it enables the construction of directed networks, which can provide insights into the causal relationships between stocks.\n\n**B)** The use of transfer entropy and conditional (partial) mutual information enables the identification of causal relationships between stocks, which can be used to construct networks that reflect the underlying dynamics of the financial market. This approach is particularly useful for detecting short-term causality.\n\n**C)** The authors' approach provides a more nuanced understanding of the lead-lag effect in financial markets, as it takes into account the non-linear interdependencies between stocks. This allows for the construction of networks that capture the complex relationships between stocks.\n\n**D)** The use of transfer entropy and conditional (partial) mutual information enables the detection of causal relationships between stocks, which can be used to construct networks that reflect the underlying dynamics of the financial market. This approach is particularly useful for detecting short-term causality and providing insights into the lead-lag effect.\n\n**Correct Answer:** B) The use of transfer entropy and conditional (partial) mutual information enables the identification of causal relationships between stocks, which can be used to construct networks that reflect the underlying dynamics of the financial market. This approach is particularly useful for detecting short-term causality.\n\n**Explanation:** The correct answer, B, highlights the key advantage of the authors' approach, which is the ability to detect causal relationships between stocks using transfer entropy and conditional (partial) mutual information. This approach enables the construction of directed networks that reflect the underlying dynamics of the financial market, particularly in detecting short-term causality. The other options, while partially correct, do not fully capture the primary advantage of the authors' approach."}, "45": {"documentation": {"title": "Should the Endless Frontier of Federal Science be Expanded?", "source": "David Baltimore, Robert Conn, William H Press, Thomas Rosenbaum, David\n  N Spergel, Shirley M Tilghman, and Harold Varmus", "docs_id": "2103.09614", "section": ["physics.soc-ph", "astro-ph.IM", "econ.GN", "physics.hist-ph", "q-bio.OT", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Should the Endless Frontier of Federal Science be Expanded?. Scientific research in the United States could receive a large increase in federal funding--up to 100 billion dollars over five years -- if proposed legislation entitled the Endless Frontiers Act becomes law. This bipartisan and bicameral bill, introduced in May 2020 by Senators Chuck Schumer (D-NY) and Todd Young (R-IN) and Congressmen Ro Khanna (D-CA) and Mike Gallagher (R-WI), is intended to expand the funding of the physical sciences, engineering, and technology at the National Science Foundation (NSF) and create a new Technology Directorate focused on use-inspired research. In addition to provisions to protect the NSF's current missions, a minimum of 15\\% of the newly appropriated funds would be used to enhance NSF's basic science portfolio. The Endless Frontier Act offers a rare opportunity to enhance the breadth and financial support of the American research enterprise. In this essay, we consider the benefits and the liabilities of the proposed legislation and recommend changes that would further strengthen it."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\nShould the Endless Frontier of Federal Science be Expanded?\n\nA) The proposed legislation would likely lead to a decrease in the quality of research in the United States, as the increased funding would create a culture of mediocrity among scientists.\n\nB) The Endless Frontier Act would be a step in the right direction, as it would provide a significant boost to the American research enterprise and help to address the country's growing technological gap with other nations.\n\nC) The bill's focus on use-inspired research would undermine the fundamental principles of basic scientific inquiry, leading to a loss of scientific integrity and a shift towards applied science that prioritizes practical applications over theoretical understanding.\n\nD) The proposed legislation would have a negligible impact on the overall direction of federal science funding, and the benefits of increased funding would be largely offset by the costs of implementing the new Technology Directorate.\n\nCorrect Answer: B) The Endless Frontier Act would be a step in the right direction, as it would provide a significant boost to the American research enterprise and help to address the country's growing technological gap with other nations.\n\nExplanation: The correct answer is B) because the Endless Frontier Act is intended to expand federal funding for the physical sciences, engineering, and technology, which would provide a significant boost to the American research enterprise. This would help to address the country's growing technological gap with other nations, making option B the most plausible answer. Options A, C, and D are incorrect because they either misrepresent the purpose of the Endless Frontier Act or overstate its potential impact."}, "46": {"documentation": {"title": "Iterated function systems, moments, and transformations of infinite\n  matrices", "source": "Palle Jorgensen (Univerisity of Iowa), Keri Kornelson (University of\n  Oklahoma), Karen Shuman (Grinnell College)", "docs_id": "0809.2124", "section": ["math.CA", "math.FA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Iterated function systems, moments, and transformations of infinite\n  matrices. We study the moments of equilibrium measures for iterated function systems (IFSs) and draw connections to operator theory. Our main object of study is the infinite matrix which encodes all the moment data of a Borel measure on R^d or C. To encode the salient features of a given IFS into precise moment data, we establish an interdependence between IFS equilibrium measures, the encoding of the sequence of moments of these measures into operators, and a new correspondence between the IFS moments and this family of operators in Hilbert space. For a given IFS, our aim is to establish a functorial correspondence in such a way that the geometric transformations of the IFS turn into transformations of moment matrices, or rather transformations of the operators that are associated with them. We first examine the classical existence problem for moments, culminating in a new proof of the existence of a Borel measure on R or C with a specified list of moments. Next, we consider moment problems associated with affine and non-affine IFSs. Our main goal is to determine conditions under which an intertwining relation is satisfied by the moment matrix of an equilibrium measure of an IFS. Finally, using the famous Hilbert matrix as our prototypical example, we study boundedness and spectral properties of moment matrices viewed as Kato-Friedrichs operators on weighted l^2 spaces."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider an iterated function system (IFS) with a Borel measure on R^d or C, and let M be the infinite matrix that encodes the moments of the equilibrium measure. Suppose that the IFS has a non-affine transformation, and let T be the moment matrix associated with the equilibrium measure. What are the necessary and sufficient conditions for the moment matrix T to be a Kato-Friedrichs operator on a weighted l^2 space, and what are the implications of these conditions for the boundedness and spectral properties of T?\n\nA) The moment matrix T is a Kato-Friedrichs operator if and only if the IFS has a finite number of iterations, and the equilibrium measure is a probability measure.\n\nB) The moment matrix T is a Kato-Friedrichs operator if and only if the IFS has a non-affine transformation, and the equilibrium measure has a finite number of moments.\n\nC) The moment matrix T is a Kato-Friedrichs operator if and only if the IFS has a finite number of iterations, and the equilibrium measure has a finite number of moments.\n\nD) The moment matrix T is a Kato-Friedrichs operator if and only if the IFS has a non-affine transformation, and the equilibrium measure has a finite number of moments.\n\nCorrect Answer: B) The moment matrix T is a Kato-Friedrichs operator if and only if the IFS has a non-affine transformation, and the equilibrium measure has a finite number of moments.\n\nExplanation: The correct answer is B) because the problem statement mentions that the IFS has a non-affine transformation, and the equilibrium measure has a finite number of moments. The other options are incorrect because they do not take into account the non-affine transformation and the finite number of moments of the equilibrium measure. \n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the problem statement and the implications of the conditions for the moment matrix T to be a Kato-Friedrichs operator."}, "47": {"documentation": {"title": "Localized Features in Non-Gaussianity from Heavy Physics", "source": "Ryo Saito and Yu-ichi Takamizu", "docs_id": "1303.3839", "section": ["astro-ph.CO", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Localized Features in Non-Gaussianity from Heavy Physics. We discuss the possibility that we could obtain some hints of the heavy physics during inflation by analyzing local features of the primordial bispectrum. A heavy scalar field could leave large signatures in the primordial spectra through the parametric resonance between its background oscillation and the fluctuations. Since the duration of the heavy-mode oscillations is finite, the effect of the resonance is localized in momentum space. In this paper, we show that the bispectrum is amplified when such a resonance occurs, and that the peak amplitude of the feature can be O(10^{1-2}), or as large as O(10^5) depending on the type of interactions. In particular, the resonance can give large contributions in finitely squeezed configurations, while the bispectrum cannot be large in the exact squeezed limit. We also find that there is a relation between the scales at which the features appear in the bispectrum and the power spectrum, and that the feature in the bispectrum can be much larger than that in the power spectrum. If correlated features are observed at characteristic scales in the primordial spectra, it will indicate the presence of heavy degrees of freedom. By analyzing these features, we may be able to obtain some information on the physics behind inflation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the implications of parametric resonance between a heavy scalar field and the fluctuations during inflation on the primordial bispectrum. What are the conditions under which the resonance leads to large signatures in the bispectrum, and how do these signatures relate to the power spectrum?\n\nA) The resonance leads to large signatures in the bispectrum only in the exact squeezed limit, where the bispectrum is not amplified. The feature in the bispectrum is much smaller than that in the power spectrum.\n\nB) The resonance amplifies the bispectrum in finitely squeezed configurations, but the feature in the bispectrum is O(10^1-2) times smaller than that in the power spectrum. The exact squeezed limit is not a relevant regime for large signatures.\n\nC) The resonance leads to large signatures in the bispectrum in both finitely squeezed and exact squeezed limits, with the peak amplitude of the feature being O(10^1-2) or O(10^5) depending on the type of interactions. The feature in the bispectrum can be much larger than that in the power spectrum.\n\nD) The resonance does not lead to large signatures in the bispectrum, as the effect of the resonance is localized in momentum space and the duration of the heavy-mode oscillations is finite.\n\nCorrect Answer: C) The resonance leads to large signatures in the bispectrum in both finitely squeezed and exact squeezed limits, with the peak amplitude of the feature being O(10^1-2) or O(10^5) depending on the type of interactions. The feature in the bispectrum can be much larger than that in the power spectrum.\n\nExplanation: The correct answer is based on the text, which states that the resonance amplifies the bispectrum in finitely squeezed configurations and that the peak amplitude of the feature can be O(10^1-2) or O(10^5) depending on the type of interactions. Additionally, the text mentions that the feature in the bispectrum can be much larger than that in the power spectrum. Candidate C accurately summarizes these points."}, "48": {"documentation": {"title": "Ion-temperature-gradient stability near the magnetic axis of\n  quasisymmetric stellarators", "source": "R. Jorge, M. Landreman", "docs_id": "2102.12390", "section": ["physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ion-temperature-gradient stability near the magnetic axis of\n  quasisymmetric stellarators. The stability of the ion-temperature gradient mode in quasisymmetric stellarators is assessed. This is performed using a set of analytical estimates together with linear gyrokinetic simulations. The peak growth rates, their corresponding real frequencies and wave-vectors are identified. A comparison is made between a first-order near-axis expansion model and eleven realistic designs obtained using numerical optimization methods. It is found that while the near-axis expansion is able to replicate the growth rates, real frequencies and perpendicular wave-vector at the inner core (both using simplified dispersion relations and first-principle gyrokinetic simulations), it leads to an overestimation of the growth rate at larger radii. An approximate analytic solution of the ITG dispersion relation for the non-resonant limit suggests growth rates could be systematically higher in quasi-axisymmetric (QA) configurations compared to quasi-helically (QH) symmetric ones. However except for very close to the axis, linear gyrokinetic simulations do not show systematic differences between QA and QH configurations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary finding regarding the stability of the ion-temperature gradient mode in quasisymmetric stellarators, as assessed through a combination of analytical estimates and linear gyrokinetic simulations?\n\nA) The near-axis expansion model overestimates the growth rate at larger radii, but accurately replicates the real frequencies and perpendicular wave-vector at the inner core.\nB) The linear gyrokinetic simulations reveal systematic differences in growth rates and real frequencies between quasi-axisymmetric (QA) and quasi-helically (QH) symmetric stellarator configurations.\nC) The analytical estimates suggest that the growth rates of the ion-temperature gradient mode are systematically higher in quasi-axisymmetric (QA) configurations compared to quasi-helically (QH) symmetric ones, except for very close to the axis.\nD) The near-axis expansion model accurately replicates the growth rates, real frequencies, and perpendicular wave-vector at the inner core, but overestimates the growth rate at larger radii.\n\nCorrect Answer: C) The analytical estimates suggest that the growth rates of the ion-temperature gradient mode are systematically higher in quasi-axisymmetric (QA) configurations compared to quasi-helically (QH) symmetric ones, except for very close to the axis.\n\nExplanation: The correct answer is based on the statement in the documentation that \"An approximate analytic solution of the ITG dispersion relation for the non-resonant limit suggests growth rates could be systematically higher in quasi-axisymmetric (QA) configurations compared to quasi-helically (QH) symmetric ones.\" This statement is the primary finding regarding the stability of the ion-temperature gradient mode in quasisymmetric stellarators. The other options are incorrect because they either misrepresent the findings of the study or are not supported by the documentation."}, "49": {"documentation": {"title": "Natural Selection as an Inhibitor of Genetic Diversity: Multiplicative\n  Weights Updates Algorithm and a Conjecture of Haploid Genetics", "source": "Ruta Mehta and Ioannis Panageas and Georgios Piliouras", "docs_id": "1408.6270", "section": ["math.DS", "cs.CE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Natural Selection as an Inhibitor of Genetic Diversity: Multiplicative\n  Weights Updates Algorithm and a Conjecture of Haploid Genetics. In a recent series of papers a surprisingly strong connection was discovered between standard models of evolution in mathematical biology and Multiplicative Weights Updates Algorithm, a ubiquitous model of online learning and optimization. These papers establish that mathematical models of biological evolution are tantamount to applying discrete Multiplicative Weights Updates Algorithm, a close variant of MWUA, on coordination games. This connection allows for introducing insights from the study of game theoretic dynamics into the field of mathematical biology. Using these results as a stepping stone, we show that mathematical models of haploid evolution imply the extinction of genetic diversity in the long term limit, a widely believed conjecture in genetics. In game theoretic terms we show that in the case of coordination games, under minimal genericity assumptions, discrete MWUA converges to pure Nash equilibria for all but a zero measure of initial conditions. This result holds despite the fact that mixed Nash equilibria can be exponentially (or even uncountably) many, completely dominating in number the set of pure Nash equilibria. Thus, in haploid organisms the long term preservation of genetic diversity needs to be safeguarded by other evolutionary mechanisms such as mutations and speciation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of haploid evolution, what is the long-term consequence of applying the Multiplicative Weights Updates Algorithm, and how does this relate to the preservation of genetic diversity?\n\nA) The algorithm leads to the extinction of genetic diversity in the long term, as it converges to pure Nash equilibria, which are stable and resistant to genetic variation.\n\nB) The algorithm promotes the preservation of genetic diversity in the long term, as it converges to mixed Nash equilibria, which are diverse and adaptable to changing environments.\n\nC) The algorithm has no significant impact on the preservation of genetic diversity in the long term, as it is a neutral process that does not favor or disfavor genetic variation.\n\nD) The algorithm leads to the emergence of new genetic variants in the long term, as it converges to a state of maximum genetic diversity, which is beneficial for the survival of the species.\n\n**Correct Answer:** A) The algorithm leads to the extinction of genetic diversity in the long term, as it converges to pure Nash equilibria, which are stable and resistant to genetic variation.\n\n**Explanation:** The correct answer is based on the idea that the Multiplicative Weights Updates Algorithm, when applied to coordination games, converges to pure Nash equilibria in the long term limit. Pure Nash equilibria are stable states where no player can improve their payoff by unilaterally changing their strategy, and they are resistant to genetic variation. In the context of haploid evolution, this means that the algorithm leads to the extinction of genetic diversity, as the population converges to a state where genetic variation is no longer beneficial. This is in contrast to the other options, which either promote the preservation of genetic diversity (B), have no impact on it (C), or lead to the emergence of new genetic variants (D)."}, "50": {"documentation": {"title": "Nonlinear mean field Fokker-Planck equations. Application to the\n  chemotaxis of biological populations", "source": "Pierre-Henri Chavanis", "docs_id": "0709.1829", "section": ["cond-mat.stat-mech", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear mean field Fokker-Planck equations. Application to the\n  chemotaxis of biological populations. We study a general class of nonlinear mean field Fokker-Planck equations in relation with an effective generalized thermodynamical formalism. We show that these equations describe several physical systems such as: chemotaxis of bacterial populations, Bose-Einstein condensation in the canonical ensemble, porous media, generalized Cahn-Hilliard equations, Kuramoto model, BMF model, Burgers equation, Smoluchowski-Poisson system for self-gravitating Brownian particles, Debye-Huckel theory of electrolytes, two-dimensional turbulence... In particular, we show that nonlinear mean field Fokker-Planck equations can provide generalized Keller-Segel models describing the chemotaxis of biological populations. As an example, we introduce a new model of chemotaxis incorporating both effects of anomalous diffusion and exclusion principle (volume filling). Therefore, the notion of generalized thermodynamics can have applications for concrete physical systems. We also consider nonlinear mean field Fokker-Planck equations in phase space and show the passage from the generalized Kramers equation to the generalized Smoluchowski equation in a strong friction limit. Our formalism is simple and illustrated by several explicit examples corresponding to Boltzmann, Tsallis and Fermi-Dirac entropies among others."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary application of nonlinear mean field Fokker-Planck equations, as described in the given Arxiv documentation, and how do these equations relate to the Keller-Segel model in the context of chemotaxis of biological populations?\n\nA) Nonlinear mean field Fokker-Planck equations describe the behavior of porous media and are used to model the chemotaxis of bacterial populations, but they do not relate to the Keller-Segel model.\n\nB) Nonlinear mean field Fokker-Planck equations are used to model the chemotaxis of biological populations and can provide generalized Keller-Segel models, but they do not describe the behavior of porous media.\n\nC) Nonlinear mean field Fokker-Planck equations describe the behavior of porous media and can provide generalized Keller-Segel models for the chemotaxis of biological populations, but they do not relate to the Debye-Huckel theory of electrolytes.\n\nD) Nonlinear mean field Fokker-Planck equations are used to model the chemotaxis of biological populations, can provide generalized Keller-Segel models, and describe the behavior of porous media, including the Debye-Huckel theory of electrolytes.\n\nCorrect Answer: D) Nonlinear mean field Fokker-Planck equations are used to model the chemotaxis of biological populations, can provide generalized Keller-Segel models, and describe the behavior of porous media, including the Debye-Huckel theory of electrolytes.\n\nExplanation: The correct answer is D) because the documentation states that nonlinear mean field Fokker-Planck equations describe several physical systems, including chemotaxis of bacterial populations, Bose-Einstein condensation, porous media, and the Debye-Huckel theory of electrolytes. Additionally, the documentation mentions that these equations can provide generalized Keller-Segel models for the chemotaxis of biological populations. Therefore, option D is the correct answer because it accurately summarizes the primary application of nonlinear mean field Fokker-Planck equations and their relation to the Keller-Segel model."}, "51": {"documentation": {"title": "Prophylaxis of Epidemic Spreading with Transient Dynamics", "source": "Geraldine Bouveret and Antoine Mandel", "docs_id": "2007.07580", "section": ["econ.TH", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Prophylaxis of Epidemic Spreading with Transient Dynamics. We investigate the containment of epidemic spreading in networks from a normative point of view. We consider a susceptible/infected model in which agents can invest in order to reduce the contagiousness of network links. In this setting, we study the relationships between social efficiency, individual behaviours and network structure. First, we exhibit an upper bound on the Price of Anarchy and prove that the level of inefficiency can scale up to linearly with the number of agents. Second, we prove that policies of uniform reduction of interactions satisfy some optimality conditions in a vast range of networks. In setting where no central authority can enforce such stringent policies, we consider as a type of second-best policy the shift from a local to a global game by allowing agents to subsidise investments in contagiousness reduction in the global rather than in the local network. We then characterise the scope for Pareto improvement opened by such policies through a notion of Price of Autarky, measuring the ratio between social welfare at a global and a local equilibrium. Overall, our results show that individual behaviours can be extremely inefficient in the face of epidemic propagation but that policy can take advantage of the network structure to design efficient containment policies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Prophylaxis of Epidemic Spreading with Transient Dynamics paper, what is the relationship between the Price of Anarchy (PoA) and the number of agents in the network, as investigated in the study?\n\n**A)** The PoA is independent of the number of agents and remains constant across all network sizes.\n**B)** The PoA increases linearly with the number of agents, indicating that the level of inefficiency can scale up to linearly with the number of agents.\n**C)** The PoA decreases exponentially with the number of agents, suggesting that larger networks are more efficient in terms of social welfare.\n**D)** The PoA remains constant regardless of the number of agents, as the social efficiency is solely determined by individual behaviors.\n\n**Correct Answer:** B) The PoA increases linearly with the number of agents, indicating that the level of inefficiency can scale up to linearly with the number of agents.\n\n**Explanation:** The study investigates the containment of epidemic spreading in networks from a normative point of view and finds that the level of inefficiency can scale up to linearly with the number of agents. This means that as the number of agents in the network increases, the level of inefficiency in the system also increases, indicating that larger networks are more prone to inefficiencies in the face of epidemic propagation."}, "52": {"documentation": {"title": "Fine-Grained Stochastic Architecture Search", "source": "Shraman Ray Chaudhuri, Elad Eban, Hanhan Li, Max Moroz, Yair\n  Movshovitz-Attias", "docs_id": "2006.09581", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fine-Grained Stochastic Architecture Search. State-of-the-art deep networks are often too large to deploy on mobile devices and embedded systems. Mobile neural architecture search (NAS) methods automate the design of small models but state-of-the-art NAS methods are expensive to run. Differentiable neural architecture search (DNAS) methods reduce the search cost but explore a limited subspace of candidate architectures. In this paper, we introduce Fine-Grained Stochastic Architecture Search (FiGS), a differentiable search method that searches over a much larger set of candidate architectures. FiGS simultaneously selects and modifies operators in the search space by applying a structured sparse regularization penalty based on the Logistic-Sigmoid distribution. We show results across 3 existing search spaces, matching or outperforming the original search algorithms and producing state-of-the-art parameter-efficient models on ImageNet (e.g., 75.4% top-1 with 2.6M params). Using our architectures as backbones for object detection with SSDLite, we achieve significantly higher mAP on COCO (e.g., 25.8 with 3.0M params) than MobileNetV3 and MnasNet."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of state-of-the-art neural architecture search (NAS) methods, according to the paper \"Fine-Grained Stochastic Architecture Search\"?\n\nA) They are too computationally expensive to run.\nB) They are limited to searching over a very small set of candidate architectures.\nC) They are unable to handle large models due to memory constraints.\nD) They are too slow to converge to optimal solutions.\n\nCorrect Answer: A) They are too computationally expensive to run.\n\nExplanation: The paper states that \"state-of-the-art NAS methods are expensive to run\", indicating that the primary limitation of these methods is their high computational cost. This is in contrast to the proposed FiGS method, which reduces the search cost."}, "53": {"documentation": {"title": "A Machine Learning Strategy to Identity Exonic Splice Enhancers in Human\n  Protein-coding Sequence", "source": "Thomas A. Down, Bernard Leong, Tim J.P. Hubbard", "docs_id": "q-bio/0403024", "section": ["q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Machine Learning Strategy to Identity Exonic Splice Enhancers in Human\n  Protein-coding Sequence. Background: Exonic splice enhancers are sequences embedded within exons which promote and regulate the splicing of the transcript in which they are located. A class of exonic splice enhancers are the SR proteins, which are thought to mediate interactions between splicing factors bound to the 5' and 3' splice sites. Method and results: We present a novel strategy for analysing protein-coding sequence by first randomizing the codons used at each position within the coding sequence, then applying a motif-based machine learning algorithm to compare the true and randomized sequences. This strategy identified a collection of motifs which can successfully discriminate between real and randomized coding sequence, including -- but not restricted to -- several previously reported splice enhancer elements. As well as successfully distinguishing coding exons from randomized sequences, we show that our model is able to recognize non-coding exons. Conclusions: Our strategy succeeded in detecting signals in coding exons which seem to be orthogonal to the sequences' primary function of coding for proteins. We believe that many of the motifs detected here may represent binding sites for previously unrecognized proteins which influence RNA splicing. We hope that this development will lead to improved knowledge of exonic splice enhancers, and new developments in the field of computational gene prediction."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary function of the machine learning algorithm used in the study to analyze protein-coding sequences, and how does it contribute to the identification of exonic splice enhancers?\n\n**A)** The algorithm is used to predict the 3D structure of proteins, and its primary function is to identify potential binding sites for splicing factors. This contributes to the identification of exonic splice enhancers by highlighting regions of the coding sequence that may interact with splicing factors.\n\n**B)** The algorithm is used to classify protein-coding sequences as either coding or non-coding, and its primary function is to identify motifs that distinguish coding exons from non-coding exons. This contributes to the identification of exonic splice enhancers by highlighting regions of the coding sequence that may regulate splicing.\n\n**C)** The algorithm is used to predict the function of protein-coding sequences, and its primary function is to identify potential binding sites for previously unrecognized proteins. This contributes to the identification of exonic splice enhancers by highlighting regions of the coding sequence that may interact with these proteins.\n\n**D)** The algorithm is used to analyze the secondary structure of RNA molecules, and its primary function is to identify regions of the coding sequence that may interact with splicing factors. This contributes to the identification of exonic splice enhancers by highlighting regions of the coding sequence that may regulate splicing.\n\n**Correct Answer:** B) The algorithm is used to classify protein-coding sequences as either coding or non-coding, and its primary function is to identify motifs that distinguish coding exons from non-coding exons. This contributes to the identification of exonic splice enhancers by highlighting regions of the coding sequence that may regulate splicing.\n\n**Explanation:** The study uses a machine learning algorithm to analyze protein-coding sequences by first randomizing the codons used at each position within the coding sequence, then applying a motif-based machine learning algorithm to compare the true and randomized sequences. The algorithm successfully identifies a collection of motifs that can discriminate between real and randomized coding sequences, including several previously reported splice enhancer elements. The primary function of the algorithm is to classify protein-coding sequences as either coding or non-coding, and its ability to identify motifs that distinguish coding exons from non-coding exons contributes to the identification of exonic splice enhancers."}, "54": {"documentation": {"title": "A Game-Theoretic Algorithm for Link Prediction", "source": "Mateusz Tarkowski, Tomasz Michalak, Michael Wooldridge", "docs_id": "1912.12846", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Game-Theoretic Algorithm for Link Prediction. Predicting edges in networks is a key problem in social network analysis and involves reasoning about the relationships between nodes based on the structural properties of a network. In particular, link prediction can be used to analyse how a network will develop or - given incomplete information about relationships - to discover \"missing\" links. Our approach to this problem is rooted in cooperative game theory, where we propose a new, quasi-local approach (i.e., one which considers nodes within some radius k) that combines generalised group closeness centrality and semivalue interaction indices. We develop fast algorithms for computing our measure and evaluate it on a number of real-world networks, where it outperforms a selection of other state-of-the-art methods from the literature. Importantly, choosing the optimal radius k for quasi-local methods is difficult, and there is no assurance that the choice is optimal. Additionally, when compared to other quasi-local methods, ours achieves very good results even when given a suboptimal radius k as a parameter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of link prediction in social network analysis, what is the primary challenge associated with quasi-local methods, such as the proposed algorithm in \"A Game-Theoretic Algorithm for Link Prediction\"?\n\n**A)** Choosing the optimal radius k for quasi-local methods is trivial and can be determined through exhaustive search.\n**B)** The choice of radius k has no significant impact on the performance of quasi-local methods.\n**C)** Choosing the optimal radius k is difficult, and there is no assurance that the choice is optimal, even when compared to other quasi-local methods.\n**D)** Quasi-local methods are inherently more accurate than non-quasi-local methods due to their ability to consider local structural properties.\n\n**Correct Answer:** C) Choosing the optimal radius k is difficult, and there is no assurance that the choice is optimal, even when compared to other quasi-local methods.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"choosing the optimal radius k for quasi-local methods is difficult, and there is no assurance that the choice is optimal.\" This highlights the challenge of selecting the optimal radius k for quasi-local methods, which is a key aspect of the proposed algorithm. The other options are incorrect because they either downplay the importance of choosing the optimal radius k (A and B) or make an unsubstantiated claim about the inherent accuracy of quasi-local methods (D)."}, "55": {"documentation": {"title": "Detecting chirality in molecules by linearly polarized laser fields", "source": "Andrey Yachmenev and Sergei N. Yurchenko", "docs_id": "1602.08946", "section": ["physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detecting chirality in molecules by linearly polarized laser fields. A new scheme for enantiomer differentiation of chiral molecules using a pair of linearly polarized intense ultrashort laser pulses with skewed mutual polarization is presented. The technique relies on the fact that the off-diagonal anisotropic contributions to the electric polarizability tensor for two enantiomers have different signs. Exploiting this property, we are able to excite a coherent unidirectional rotation of two enantiomers with a {\\pi} phase difference in the molecular electric dipole moment. The approach is robust and suitable for relatively high temperatures of molecular samples, making it applicable for selective chiral analysis of mixtures, and to chiral molecules with low barriers between enantiomers. As an illustration, we present nanosecond laser-driven dynamics of a tetratomic non-rigid chiral molecule with short-lived chirality. The ultrafast time scale of the proposed technique is well suited to study parity violation in molecular systems in short-lived chiral states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the fundamental principle behind the proposed technique for detecting chirality in molecules using linearly polarized laser fields?\n\nA) The technique relies on the fact that the diagonal elements of the electric polarizability tensor for two enantiomers have the same sign.\nB) The technique exploits the difference in the off-diagonal anisotropic contributions to the electric polarizability tensor for two enantiomers, which have opposite signs.\nC) The technique is based on the principle of parity conservation in molecular systems.\nD) The technique relies on the fact that the molecular electric dipole moment is always perpendicular to the direction of the laser field.\n\n**Correct Answer:** B) The technique exploits the difference in the off-diagonal anisotropic contributions to the electric polarizability tensor for two enantiomers, which have opposite signs.\n\n**Explanation:** The correct answer is based on the fact that the technique relies on the difference in the off-diagonal anisotropic contributions to the electric polarizability tensor for two enantiomers, which have opposite signs. This difference in signs is the key to exciting a coherent unidirectional rotation of the molecular electric dipole moment, allowing for the detection of chirality. The other options are incorrect because they do not accurately describe the fundamental principle behind the proposed technique."}, "56": {"documentation": {"title": "Not even wrong: Reply to Wagg et al", "source": "Pradeep Pillai and Tarik C. Gouhier", "docs_id": "1910.13670", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Not even wrong: Reply to Wagg et al. We demonstrate that the issues described in the Wagg et al. (2019) Comment on our paper (Pillai and Gouhier, 2019) are all due to misunderstandings about the implications of pairwise effects, the nature of the null baseline in both our framework and in the Loreau-Hector (LH) partitioning scheme (i.e., the midpoint of the monocultures), and the impact of nonlinearity on the LH partitioning results. Specifically, we show that (i) pairwise effects can be computed over any time horizon and thus do not imply stable coexistence, (ii) the midpoint of the monocultures corresponds to a neutral community so coexistence was always part of the LH baseline, and (iii) contrary to what Wagg et al. suggested, generalized diversity-interaction models do not account for (and may in fact exacerbate) the problem of nonlinearity in monocultures, which inflates the LH net biodiversity effect and generates incorrect estimates of selection and complementarity. Hence, all of our original claims about the triviality inherent in biodiversity-ecosystem functioning research and the issues with the LH partitioning scheme hold."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: According to the authors of the paper, what is the primary reason for the issues described in Wagg et al. (2019) Comment on our paper (Pillai and Gouhier, 2019)?\n\nA) Misunderstandings about the implications of pairwise effects\nB) Misunderstandings about the nature of the null baseline in the Loreau-Hector (LH) partitioning scheme\nC) Misunderstandings about the impact of nonlinearity on the LH partitioning results\nD) Misunderstandings about the triviality inherent in biodiversity-ecosystem functioning research\n\nCorrect Answer: A) Misunderstandings about the implications of pairwise effects\n\nExplanation: The authors explicitly state that the issues described in Wagg et al. (2019) are due to misunderstandings about the implications of pairwise effects, specifically that pairwise effects can be computed over any time horizon and do not imply stable coexistence. This is a key point in the authors' response to the comment, and it is the primary reason for the issues described in Wagg et al. (2019)."}, "57": {"documentation": {"title": "Distance to G14.33-0.64 in the Sagittarius Spiral Arm: H2O Maser\n  Trigonometric Parallax with VERA", "source": "Mayumi Sato, Tomoya Hirota, Mark J. Reid, Mareki Honma, Hideyuki\n  Kobayashi, Kenzaburo Iwadate, Takeshi Miyaji, Katsunori M. Shibata", "docs_id": "1001.2565", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distance to G14.33-0.64 in the Sagittarius Spiral Arm: H2O Maser\n  Trigonometric Parallax with VERA. We report on trigonometric parallax measurements for the Galactic star forming region G14.33-0.64 toward the Sagittarius spiral arm. We conducted multi-epoch phase-referencing observations of an H2O maser source in G14.33-0.64 with the Japanese VLBI array VERA. We successfully detected a parallax of 0.893+/-0.101 mas, corresponding to a source distance of 1.12+/-0.13 kpc, which is less than half of the kinematic distance for G14.33-0.64. Our new distance measurement demonstrates that the Sagittarius arm lies at a closer distance of ~1 kpc, instead of previously assumed ~2-3 kpc from kinematic distances. The previously suggested deviation of the Sagittarius arm toward the Galactic center from the symmetrically fitted model (Taylor & Cordes 1993) is likely due to large errors of kinematic distances at low galactic longitudes. G14.33-0.64 most likely traces the near side of the Sagittarius arm. We attempted fitting the pitch angle of the arm with other parallax measurements along the arm, which yielded two possible pitch angles of i=34.7+/-2.7 degrees and i=11.2+/-10.5 degrees. Our proper motion measurements suggest G14.33-0.64 has no significant peculiar motion relative to the differential rotation of the Galaxy (assumed to be in a circular orbit), indicating that the source motion is in good agreement with the Galactic rotation."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the distance to the Galactic star forming region G14.33-0.64 based on the trigonometric parallax measurement?\n\nA) The distance to G14.33-0.64 is likely to be greater than 2 kpc.\nB) The distance to G14.33-0.64 is likely to be less than 1 kpc.\nC) The distance to G14.33-0.64 is consistent with a distance of 2-3 kpc.\nD) The distance to G14.33-0.64 is uncertain and cannot be determined.\n\nCorrect Answer: B) The distance to G14.33-0.64 is likely to be less than 1 kpc.\n\nExplanation: The text states that the trigonometric parallax measurement yields a source distance of 1.12+0.13 kpc, which is less than half of the previously assumed kinematic distance for G14.33-0.64. This suggests that the distance to G14.33-0.64 is likely to be less than 1 kpc."}, "58": {"documentation": {"title": "Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots", "source": "Bo Zhou, Ramon van den Akker and Bas J.M. Werker", "docs_id": "1806.09304", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiparametrically Point-Optimal Hybrid Rank Tests for Unit Roots. We propose a new class of unit root tests that exploits invariance properties in the Locally Asymptotically Brownian Functional limit experiment associated to the unit root model. The invariance structures naturally suggest tests that are based on the ranks of the increments of the observations, their average, and an assumed reference density for the innovations. The tests are semiparametric in the sense that they are valid, i.e., have the correct (asymptotic) size, irrespective of the true innovation density. For a correctly specified reference density, our test is point-optimal and nearly efficient. For arbitrary reference densities, we establish a Chernoff-Savage type result, i.e., our test performs as well as commonly used tests under Gaussian innovations but has improved power under other, e.g., fat-tailed or skewed, innovation distributions. To avoid nonparametric estimation, we propose a simplified version of our test that exhibits the same asymptotic properties, except for the Chernoff-Savage result that we are only able to demonstrate by means of simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a unit root test that exploits invariance properties in the Locally Asymptotically Brownian Functional limit experiment associated with the unit root model. Suppose the test is semiparametric and based on the ranks of the increments of the observations, their average, and an assumed reference density for the innovations. Under what conditions does the test have the correct (asymptotic) size, irrespective of the true innovation density?\n\n**A)** The test is valid if the reference density is correctly specified and the sample size is sufficiently large.\n**B)** The test is valid if the reference density is correctly specified and the sample size is finite.\n**C)** The test is valid if the reference density is correctly specified and the sample size is infinite.\n**D)** The test is valid if the reference density is incorrectly specified and the sample size is finite.\n\n**Correct Answer:** A) The test is valid if the reference density is correctly specified and the sample size is sufficiently large.\n\n**Explanation:** According to the documentation, the test is semiparametric and valid, i.e., has the correct (asymptotic) size, irrespective of the true innovation density, for a correctly specified reference density. The documentation does not specify a finite sample size condition, but rather implies that the test is valid for sufficiently large sample sizes. Therefore, option A is the correct answer."}, "59": {"documentation": {"title": "Fast and Robust Registration of Aerial Images and LiDAR data Based on\n  Structrual Features and 3D Phase Correlation", "source": "Bai Zhu, Yuanxin Ye, Chao Yang, Liang Zhou, Huiyu Liu, Yungang Cao", "docs_id": "2004.09811", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast and Robust Registration of Aerial Images and LiDAR data Based on\n  Structrual Features and 3D Phase Correlation. Co-Registration of aerial imagery and Light Detection and Ranging (LiDAR) data is quilt challenging because the different imaging mechanism causes significant geometric and radiometric distortions between such data. To tackle the problem, this paper proposes an automatic registration method based on structural features and three-dimension (3D) phase correlation. In the proposed method, the LiDAR point cloud data is first transformed into the intensity map, which is used as the reference image. Then, we employ the Fast operator to extract uniformly distributed interest points in the aerial image by a partition strategy and perform a local geometric correction by using the collinearity equation to eliminate scale and rotation difference between images. Subsequently, a robust structural feature descriptor is build based on dense gradient features, and the 3D phase correlation is used to detect control points (CPs) between aerial images and LiDAR data in the frequency domain, where the image matching is accelerated by the 3D Fast Fourier Transform (FFT). Finally, the obtained CPs are employed to correct the exterior orientation elements, which is used to achieve co-registration of aerial images and LiDAR data. Experiments with two datasets of aerial images and LiDAR data show that the proposed method is much faster and more robust than state of the art methods"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach used in the proposed method for co-registering aerial images and LiDAR data, and how does it differ from traditional methods?\n\n**A)** The proposed method uses traditional feature extraction techniques, such as SIFT or SURF, to detect control points between images.\n\n**B)** The proposed method employs a combination of structural features and 3D phase correlation to detect control points, which is faster and more robust than traditional methods.\n\n**C)** The proposed method uses a traditional geometric correction approach, such as the collinearity equation, to eliminate scale and rotation differences between images.\n\n**D)** The proposed method relies solely on 3D phase correlation to detect control points, without using structural features or geometric correction.\n\n**Correct Answer:** B) The proposed method employs a combination of structural features and 3D phase correlation to detect control points, which is faster and more robust than traditional methods.\n\n**Explanation:** The correct answer is B) because the proposed method uses a combination of structural features and 3D phase correlation to detect control points, which is a key innovation in the paper. This approach is faster and more robust than traditional methods, which typically rely on feature extraction techniques or geometric correction approaches. The other options are incorrect because they do not accurately describe the primary approach used in the proposed method. Option A is incorrect because the proposed method uses a novel approach that differs from traditional feature extraction techniques. Option C is incorrect because the proposed method uses 3D phase correlation, not traditional geometric correction. Option D is incorrect because the proposed method uses both structural features and 3D phase correlation to detect control points."}}