{"0": {"documentation": {"title": "Analytic estimate of the maximum Lyapunov exponent in coupled-map\n  lattices", "source": "F. Cecconi and A. Politi", "docs_id": "chao-dyn/9901014", "section": ["nlin.CD", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analytic estimate of the maximum Lyapunov exponent in coupled-map\n  lattices. In this work we present a theoretical and numerical study of the behaviour of the maximum Lyapunov exponent for a generic coupled-map-lattice in the weak-coupling regime. We explain the observed results by introducing a suitable continuous-time formulation of the tangent dynamics. The first general result is that the deviation of the Lyapunov exponent from the uncoupled-limit limit is function of a single scaling parameter which, in the case of strictly positive multipliers, is the ratio of the coupling strength with the variance of local multipliers. Moreover, we find an approximate analytic expression for the Lyapunov exponent by mapping the problem onto the evolution of a chain of nonlinear Langevin equations, which are eventually reduced to a single stochastic equation. The probability distribution of this dynamical equation provides an excellent description for the behaviour of the Lyapunov exponent. Furthermore, multipliers with random signs are considered as well, finding that the Lyapunov exponent still depends on a single scaling parameter, which however has a different expression."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the relationship between the maximum Lyapunov exponent and the coupling strength in a generic coupled-map-lattice in the weak-coupling regime. How does the deviation of the Lyapunov exponent from the uncoupled-limit limit depend on the coupling strength, and what is the role of the variance of local multipliers in this context?\n\nA) The maximum Lyapunov exponent is directly proportional to the coupling strength, and the deviation from the uncoupled-limit limit is zero when the coupling strength is zero.\n\nB) The maximum Lyapunov exponent is inversely proportional to the coupling strength, and the deviation from the uncoupled-limit limit is a function of the ratio of the coupling strength to the variance of local multipliers.\n\nC) The maximum Lyapunov exponent is independent of the coupling strength, and the deviation from the uncoupled-limit limit is a function of the variance of local multipliers alone.\n\nD) The maximum Lyapunov exponent is a function of the coupling strength and the variance of local multipliers, but the exact relationship is not explicitly stated in the provided documentation.\n\nCorrect Answer: B) The maximum Lyapunov exponent is inversely proportional to the coupling strength, and the deviation from the uncoupled-limit limit is a function of the ratio of the coupling strength to the variance of local multipliers.\n\nExplanation: According to the provided documentation, the deviation of the Lyapunov exponent from the uncoupled-limit limit is a function of a single scaling parameter, which is the ratio of the coupling strength to the variance of local multipliers. This is stated in the first general result of the work. Therefore, option B is the correct answer."}, "1": {"documentation": {"title": "Reexamining the relation between the binding energy of finite nuclei and\n  the equation of state of infinite nuclear matter", "source": "M. C. Atkinson, W. H. Dickhoff, M. Piarulli, A. Rios, and R. B.\n  Wiringa", "docs_id": "2001.07231", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reexamining the relation between the binding energy of finite nuclei and\n  the equation of state of infinite nuclear matter. The energy density is calculated in coordinate space for $^{12}$C, $^{40}$Ca, $^{48}$Ca, and $^{208}$Pb using a dispersive optical model constrained by all relevant data including the corresponding energy of the ground state. The energy density of $^{8}$Be is also calculated using the Green's function Monte-Carlo method employing the Argonne/Urbana two and three-body interactions. The nuclear interior minimally contributes to the total binding energy due to the 4$\\pi r^2$ phase space factor. Thus, the volume contribution to the energy in the interior is not well constrained. The dispersive-optical-model energy densities are in good agreement with \\textit{ab initio} self-consistent Green's function calculations of infinite nuclear matter restricted to treat only short-range and tensor correlations. These results call into question the degree to which the equation of state for nuclear matter is constrained by the empirical mass formula. In particular, the results in this paper indicate that saturated nuclear matter does not require the canonical value of 16 MeV binding per particle but only about 13-14 MeV when the interior of $^{208}$Pb is considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Reexamining the relation between the binding energy of finite nuclei and the equation of state of infinite nuclear matter, what is the approximate binding energy per particle required for saturated nuclear matter, considering the interior of $^{208}$Pb?\n\nA) 16 MeV\nB) 13-14 MeV\nC) 18 MeV\nD) 20 MeV\n\nCorrect Answer: B) 13-14 MeV\n\nExplanation: According to the text, the results indicate that saturated nuclear matter does not require the canonical value of 16 MeV binding per particle, but rather about 13-14 MeV when the interior of $^{208}$Pb is considered. This suggests that the correct answer is B) 13-14 MeV."}, "2": {"documentation": {"title": "Emergence of time-horizon invariant correlation structure in financial\n  returns by subtraction of the market mode", "source": "Christian Borghesi, Matteo Marsili, Salvatore Miccich\\`e", "docs_id": "physics/0702106", "section": ["physics.soc-ph", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergence of time-horizon invariant correlation structure in financial\n  returns by subtraction of the market mode. We investigate the emergence of a structure in the correlation matrix of assets' returns as the time-horizon over which returns are computed increases from the minutes to the daily scale. We analyze data from different stock markets (New York, Paris, London, Milano) and with different methods. Result crucially depends on whether the data is restricted to the ``internal'' dynamics of the market, where the ``center of mass'' motion (the market mode) is removed or not. If the market mode is not removed, we find that the structure emerges, as the time-horizon increases, from splitting a single large cluster. In NYSE we find that when the market mode is removed, the structure of correlation at the daily scale is already well defined at the 5 minutes time-horizon, and this structure accounts for 80 % of the classification of stocks in economic sectors. Similar results, though less sharp, are found for the other markets. We also find that the structure of correlations in the overnight returns is markedly different from that of intraday activity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study \"Emergence of time-horizon invariant correlation structure in financial returns by subtraction of the market mode\" investigates the emergence of a structure in the correlation matrix of assets' returns as the time-horizon increases. What is the crucial factor that determines the result of the study, and how does it impact the analysis of the data?\n\n**A)** The type of market data used (e.g. NYSE, Paris, London, Milano) has a significant impact on the result.\n**B)** The method used to remove the market mode has a significant impact on the result.\n**C)** Whether the data is restricted to the \"internal\" dynamics of the market or not has a significant impact on the result.\n**D)** The time-horizon over which returns are computed has a significant impact on the result.\n\n**Correct Answer:** C) Whether the data is restricted to the \"internal\" dynamics of the market or not has a significant impact on the result.\n\n**Explanation:** The study finds that the result crucially depends on whether the data is restricted to the \"internal\" dynamics of the market, where the \"center of mass\" motion (the market mode) is removed or not. If the market mode is not removed, the structure emerges as the time-horizon increases, from splitting a single large cluster. This suggests that the restriction to internal dynamics is a crucial factor in determining the result of the study."}, "3": {"documentation": {"title": "Evolving useful delusions: Subjectively rational selfishness leads to\n  objectively irrational cooperation", "source": "Artem Kaznatcheev, Marcel Montrey, Thomas R. Shultz", "docs_id": "1405.0041", "section": ["q-bio.PE", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolving useful delusions: Subjectively rational selfishness leads to\n  objectively irrational cooperation. We introduce a framework within evolutionary game theory for studying the distinction between objective and subjective rationality and apply it to the evolution of cooperation on 3-regular random graphs. In our simulations, agents evolve misrepresentations of objective reality that help them cooperate and maintain higher social welfare in the Prisoner's dilemma. These agents act rationally on their subjective representations of the world, but irrationally from the perspective of an external observer. We model misrepresentations as subjective perceptions of payoffs and quasi-magical thinking as an inferential bias, finding that the former is more conducive to cooperation. This highlights the importance of internal representations, not just observed behavior, in evolutionary thought. Our results provide support for the interface theory of perception and suggest that the individual's interface can serve not only the individual's aims, but also society as a whole, offering insight into social phenomena such as religion."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of the paper \"Evolving useful delusions: Subjectively rational selfishness leads to objectively irrational cooperation\", what is the primary finding regarding the relationship between subjective rationality and cooperation in evolutionary game theory?\n\nA) Subjective rationality is a necessary condition for cooperation to emerge.\nB) Subjective rationality is sufficient for cooperation to emerge, but not necessary.\nC) Subjective rationality is a more important factor in cooperation than objective rationality.\nD) Subjective rationality is a necessary and sufficient condition for cooperation to emerge.\n\nCorrect Answer: C) Subjective rationality is a more important factor in cooperation than objective rationality.\n\nExplanation: The paper suggests that subjective rationality, which involves agents having misrepresentations of objective reality that help them cooperate, is more conducive to cooperation than objective rationality. This is because subjective rationality allows agents to act rationally on their subjective representations of the world, even if they are irrational from an external observer's perspective. The paper models misrepresentations as subjective perceptions of payoffs and quasi-magical thinking as an inferential bias, finding that the former is more conducive to cooperation. Therefore, option C is the correct answer.\n\nNote: The other options are incorrect because:\n\n* A) Subjective rationality is not a necessary condition for cooperation to emerge, as the paper shows that cooperation can emerge even if agents have irrational subjective representations of reality.\n* B) Subjective rationality is not sufficient for cooperation to emerge, as the paper suggests that objective rationality is also necessary for cooperation to emerge.\n* D) Subjective rationality is not a necessary and sufficient condition for cooperation to emerge, as the paper shows that cooperation can emerge even if agents have irrational subjective representations of reality."}, "4": {"documentation": {"title": "Partial Weight Adaptation for Robust DNN Inference", "source": "Xiufeng Xie, Kyu-Han Kim", "docs_id": "2003.06131", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Partial Weight Adaptation for Robust DNN Inference. Mainstream video analytics uses a pre-trained DNN model with an assumption that inference input and training data follow the same probability distribution. However, this assumption does not always hold in the wild: autonomous vehicles may capture video with varying brightness; unstable wireless bandwidth calls for adaptive bitrate streaming of video; and, inference servers may serve inputs from heterogeneous IoT devices/cameras. In such situations, the level of input distortion changes rapidly, thus reshaping the probability distribution of the input. We present GearNN, an adaptive inference architecture that accommodates heterogeneous DNN inputs. GearNN employs an optimization algorithm to identify a small set of \"distortion-sensitive\" DNN parameters, given a memory budget. Based on the distortion level of the input, GearNN then adapts only the distortion-sensitive parameters, while reusing the rest of constant parameters across all input qualities. In our evaluation of DNN inference with dynamic input distortions, GearNN improves the accuracy (mIoU) by an average of 18.12% over a DNN trained with the undistorted dataset and 4.84% over stability training from Google, with only 1.8% extra memory overhead."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption made by mainstream video analytics when using a pre-trained DNN model, and how does GearNN address this limitation?\n\n**A)** The primary assumption is that the inference input and training data follow the same probability distribution, and GearNN addresses this by retraining the model for each new input. (Incorrect)\n**B)** The primary assumption is that the inference input and training data follow the same probability distribution, and GearNN addresses this by adapting only the distortion-sensitive parameters of the pre-trained model. (Correct)\n**C)** The primary assumption is that the inference input and training data follow different probability distributions, and GearNN addresses this by using a different DNN architecture for each input. (Incorrect)\n**D)** The primary assumption is that the inference input and training data follow the same probability distribution, and GearNN addresses this by using a fixed set of parameters for all inputs. (Incorrect)\n\n**Explanation:** The correct answer is B) The primary assumption is that the inference input and training data follow the same probability distribution, and GearNN addresses this by adapting only the distortion-sensitive parameters of the pre-trained model. This is because GearNN is designed to accommodate heterogeneous DNN inputs by identifying a small set of \"distortion-sensitive\" parameters and adapting them based on the input distortion level, while reusing the rest of the constant parameters across all input qualities."}, "5": {"documentation": {"title": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms", "source": "Karl Michel Koerich, Mohammad Esmaeilpour, Sajjad Abdoli, Alceu de\n  Souza Britto Jr., Alessandro Lameiras Koerich", "docs_id": "1910.10106", "section": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross-Representation Transferability of Adversarial Attacks: From\n  Spectrograms to Audio Waveforms. This paper shows the susceptibility of spectrogram-based audio classifiers to adversarial attacks and the transferability of such attacks to audio waveforms. Some commonly used adversarial attacks to images have been applied to Mel-frequency and short-time Fourier transform spectrograms, and such perturbed spectrograms are able to fool a 2D convolutional neural network (CNN). Such attacks produce perturbed spectrograms that are visually imperceptible by humans. Furthermore, the audio waveforms reconstructed from the perturbed spectrograms are also able to fool a 1D CNN trained on the original audio. Experimental results on a dataset of western music have shown that the 2D CNN achieves up to 81.87% of mean accuracy on legitimate examples and such performance drops to 12.09% on adversarial examples. Likewise, the 1D CNN achieves up to 78.29% of mean accuracy on original audio samples and such performance drops to 27.91% on adversarial audio waveforms reconstructed from the perturbed spectrograms."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary finding of the study \"Cross-Representation Transferability of Adversarial Attacks: From Spectrograms to Audio Waveforms\" regarding the susceptibility of audio classifiers to adversarial attacks?\n\n**A)** The study shows that adversarial attacks on spectrograms are ineffective against 1D CNNs trained on audio waveforms.\n**B)** The study demonstrates that 2D CNNs are more robust to adversarial attacks on spectrograms than 1D CNNs trained on audio waveforms.\n**C)** The study reveals that adversarial attacks on spectrograms can fool both 2D CNNs and 1D CNNs trained on audio waveforms, with significant drops in accuracy.\n**D)** The study finds that the performance of audio classifiers on legitimate examples is unaffected by adversarial attacks on spectrograms.\n\n**Correct Answer:** C) The study reveals that adversarial attacks on spectrograms can fool both 2D CNNs and 1D CNNs trained on audio waveforms, with significant drops in accuracy.\n\n**Explanation:** The study shows that adversarial attacks on spectrograms can fool both 2D CNNs and 1D CNNs trained on audio waveforms, with significant drops in accuracy. Specifically, the 2D CNN achieves up to 81.87% accuracy on legitimate examples and drops to 12.09% on adversarial examples, while the 1D CNN achieves up to 78.29% accuracy on original audio samples and drops to 27.91% on adversarial audio waveforms reconstructed from the perturbed spectrograms. This demonstrates the transferability of adversarial attacks from spectrograms to audio waveforms."}, "6": {"documentation": {"title": "Semi-discrete optimal transport", "source": "Gershon Wolansky", "docs_id": "1911.04348", "section": ["math.OC", "cs.IT", "econ.TH", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semi-discrete optimal transport. In the current book I suggest an off-road path to the subject of optimal transport. I tried to avoid prior knowledge of analysis, PDE theory and functional analysis, as much as possible. Thus I concentrate on discrete and semi-discrete cases, and always assume compactness for the underlying spaces. However, some fundamental knowledge of measure theory and convexity is unavoidable. In order to make it as self-contained as possible I included an appendix with some basic definitions and results. I believe that any graduate student in mathematics, as well as advanced undergraduate students, can read and understand this book. Some chapters (in particular in Parts II\\&III ) can also be interesting for experts. Starting with the the most fundamental, fully discrete problem I attempted to place optimal transport as a particular case of the celebrated stable marriage problem. From there we proceed to the partition problem, which can be formulated as a transport from a continuous space to a discrete one. Applications to information theory and game theory (cooperative and non-cooperative) are introduced as well. Finally, the general case of transport between two compact measure spaces is introduced as a coupling between two semi-discrete transports."}, "answer": "Here is a difficult multi-form question for an exam based on the provided documentation:\n\nQuestion: Consider a semi-discrete optimal transport problem between two compact measure spaces, X and Y, with respect to a cost function c(x, y) that is convex and lower semicontinuous. Suppose we have two transport plans, \u03c0 and \u03c3, that satisfy the following conditions:\n\n\u03c0 is a semi-discrete transport plan that maps a finite set of points in X to a finite set of points in Y, and satisfies the following properties:\n\n* \u03c0 is a probability measure on X\n* \u03c0 is a transport plan, i.e., for any measurable set A \u2286 X, \u222b_{A} \u03c0(dx) = \u222b_{A} \u03c0(dy)\n* \u03c0 is semi-discrete, i.e., there exists a finite set of points {x_i} in X such that \u03c0(x) = \u03b4_{x_i}(x) for all x in X\n\n\u03c3 is a semi-discrete transport plan that maps a finite set of points in X to a finite set of points in Y, and satisfies the following properties:\n\n* \u03c3 is a probability measure on X\n* \u03c3 is a transport plan, i.e., for any measurable set A \u2286 X, \u222b_{A} \u03c3(dx) = \u222b_{A} \u03c3(dy)\n* \u03c3 is semi-discrete, i.e., there exists a finite set of points {x_i} in X such that \u03c3(x) = \u03b4_{x_i}(x) for all x in X\n\nThe cost function c(x, y) is given by:\n\nc(x, y) = |x - y|\n\nShow that if \u03c0 and \u03c3 are two semi-discrete transport plans that satisfy the above conditions, then there exists a coupling between \u03c0 and \u03c3, i.e., a probability measure \u03bc on X \u00d7 Y such that:\n\n* \u03bc is a transport plan, i.e., for any measurable set A \u00d7 B \u2286 X \u00d7 Y, \u222b_{A \u00d7 B} \u03bc(dx, dy) = \u222b_{A} \u03c0(dx) \u222b_{B} \u03c3(dy)\n* \u03bc is semi-discrete, i.e., there exists a finite set of points {(x_i, y_j)} in X \u00d7 Y such that \u03bc((x, y)) = \u03b4_{(x_i, y_j)}((x, y)) for all (x, y) in X \u00d7 Y\n\nA){The correct answer is that there exists a coupling between \u03c0 and \u03c3, and the coupling can be constructed using the following formula: \u03bc((x, y)) = \u03c0(x)\u03c3(y) for all (x, y) in X \u00d7 Y. This follows from the fact that \u03c0 and \u03c3 are both semi-discrete transport plans, and the cost function c(x, y) = |x - y| is convex and lower semicontinuous.}\n\nB){The correct answer is that there does not exist a coupling between \u03c0 and \u03c3. This follows from the fact that \u03c0 and \u03c3 are two different semi-discrete transport plans that satisfy the same conditions, but have different finite sets of points {x_i} and {y_j} that they map to.}\n\nC){The correct answer is that the coupling between \u03c0 and \u03c3 can be constructed using the following formula: \u03bc((x, y)) = \u03c0(x)\u03c3(y) for all (x, y) in X \u00d7 Y, but this formula is not valid for all (x, y) in X \u00d7 Y. The correct formula is \u03bc((x, y)) = \u03c0(x)\u03c3(y) for all (x, y) in X \u00d7 Y, but only if x = x_i and y = y_j for some i and j.}\n\nD){The correct answer is that the coupling between \u03c0 and \u03c3 can be constructed using the following formula: \u03bc((x, y)) = \u03c0(x)\u03c3(y) for all (x, y) in X \u00d7 Y, but this formula is not valid for all (x, y) in X \u00d7 Y. The correct formula is \u03bc((x, y)) = \u03c0(x)\u03c3(y) for all (x, y) in X \u00d7 Y, but only if x = x_i and y = y_j for some i and j, and the cost function c(x, y) = |x - y| is convex and lower semicontinuous.}\n\nCorrect Answer: A"}, "7": {"documentation": {"title": "30 GHz Zeno-based Graphene Electro-optic Modulator", "source": "Christopher T. Phare, Yoon-Ho Daniel Lee, Jaime Cardenas, and Michal\n  Lipson", "docs_id": "1411.2053", "section": ["physics.optics", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "30 GHz Zeno-based Graphene Electro-optic Modulator. Graphene has generated exceptional interest as an optoelectronic material because its high carrier mobility and broadband absorption promise to make extremely fast and broadband electro-optic devices possible. Electro-optic graphene modulators reported to date, however, have been limited in bandwidth to a few GHz because of the large capacitance required to achieve reasonable voltage swings. Here we demonstrate a graphene electro-optic modulator based on the classical Zeno effect that shows drastically increased speed and efficiency. Our device operates with a 30 GHz bandwidth, over an order of magnitude faster than prior work, and a state-of-the-art modulation efficiency of 1.5 dB/V. We also show the first high-speed large-signal operation in a graphene modulator, paving the way for fast digital communications using this platform. The modulator uniquely uses silicon nitride waveguides, an otherwise completely passive material platform, with promising applications for ultra-low-loss broadband structures and nonlinear optics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of electro-optic graphene modulators reported to date, and how does the proposed Zeno-based graphene electro-optic modulator address this limitation?\n\nA) The high carrier mobility of graphene is not sufficient to achieve reasonable voltage swings, resulting in limited bandwidth.\nB) The large capacitance required to achieve reasonable voltage swings in graphene modulators limits their bandwidth to a few GHz.\nC) The high absorption of graphene is not sufficient to achieve fast and broadband electro-optic devices.\nD) The use of silicon nitride waveguides in the proposed modulator does not address the limitation of large capacitance.\n\nCorrect Answer: B) The large capacitance required to achieve reasonable voltage swings in graphene modulators limits their bandwidth to a few GHz.\n\nExplanation: The question requires the test-taker to understand the limitations of existing graphene electro-optic modulators and how the proposed Zeno-based graphene electro-optic modulator addresses this limitation. The correct answer, B, is supported by the text, which states that electro-optic graphene modulators have been limited in bandwidth to a few GHz due to the large capacitance required to achieve reasonable voltage swings. The other options are incorrect because they do not accurately reflect the limitation of existing graphene modulators or the proposed solution."}, "8": {"documentation": {"title": "Stability Analysis of Convection in the Intracluster Medium", "source": "Himanshu Gupta, Shailendra K. Rathor, Martin E. Pessah, and Sagar\n  Chakraborty", "docs_id": "1605.09591", "section": ["physics.flu-dyn", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability Analysis of Convection in the Intracluster Medium. We use the machinery usually employed for studying the onset of Rayleigh--B\\'enard convection in hydro- and magnetohydro-dynamic settings to address the onset of convection induced by the magnetothermal instability and the heat-flux-buoyancy-driven-instability in the weakly-collisional magnetized plasma permeating the intracluster medium. Since most of the related numerical simulations consider the plasma being bounded between two `plates' on which boundary conditions are specified, our strategy provides a framework that could enable a more direct connection between analytical and numerical studies. We derive the conditions for the onset of these instabilities considering the effects of induced magnetic tension resulting from a finite plasma beta. We provide expressions for the Rayleigh number in terms of the wave vector associated with a given mode, which allow us to characterize the modes that are first to become unstable. For both the heat-flux-buoyancy-driven-instability and the magnetothermal instability, oscillatory marginal stable states are possible."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the study on the stability analysis of convection in the intracluster medium, and how does it differ from previous numerical simulations?\n\nA) To investigate the onset of convection in a bounded plasma between two plates with specified boundary conditions, similar to previous numerical simulations.\nB) To derive conditions for the onset of instabilities in a weakly-collisional magnetized plasma permeating the intracluster medium, considering the effects of induced magnetic tension.\nC) To develop a new numerical method for simulating convection in the intracluster medium, with a focus on the heat-flux-buoyancy-driven-instability.\nD) To compare the stability of different modes of convection in the intracluster medium, using a combination of analytical and numerical methods.\n\n**Correct answer:** B) To derive conditions for the onset of instabilities in a weakly-collisional magnetized plasma permeating the intracluster medium, considering the effects of induced magnetic tension.\n\n**Explanation:** The correct answer is B) because the study aims to provide a framework for a more direct connection between analytical and numerical studies of convection in the intracluster medium. The authors derive conditions for the onset of instabilities, considering the effects of induced magnetic tension resulting from a finite plasma beta. This approach differs from previous numerical simulations, which typically consider the plasma being bounded between two plates with specified boundary conditions."}, "9": {"documentation": {"title": "Isospin Breaking in Heavy-Meson Decay Constants", "source": "Wolfgang Lucha, Dmitri Melikhov, Silvano Simula", "docs_id": "1711.07899", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isospin Breaking in Heavy-Meson Decay Constants. Evaluation of Borelized QCD sum rules in the so-called local-duality limit of infinitely large Borel mass parameter provides an alternate route for extraction of the dependence of the decay constants of heavy-light mesons on the mass $m_q$ of the involved light quark $q$: For appropriate choices of the two-point correlation functions of currents interpolating the hadrons under study, the local-duality limit forces all nonperturbative contributions parametrized by vacuum condensates to such kind of correlator to vanish. As a consequence, the sought $m_q$ dependence of the heavy-light meson decay constants proves to be controlled primarily by the correlator contributions from perturbative QCD. Our knowledge of the analytic behaviour of the latter as functions of $m_q$ enables us to derive the $m_q$ dependence of the decay constants of both pseudoscalar and vector heavy-light mesons, for which we estimate strong isospin breaking to be of the order of 1 MeV for both charm and beauty sectors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Evaluate the implications of the local-duality limit on the extraction of the dependence of heavy-light meson decay constants on the mass $m_q$ of the involved light quark $q$.\n\nA) The local-duality limit leads to a vanishing of all nonperturbative contributions to the two-point correlation functions, resulting in a complete dependence of the decay constants on perturbative QCD.\n\nB) The local-duality limit forces the two-point correlation functions to vanish, but only for the non-perturbative contributions parametrized by vacuum condensates, leaving the perturbative QCD contributions unaffected.\n\nC) The local-duality limit provides an alternate route for extracting the dependence of heavy-light meson decay constants on $m_q$, but only for the pseudoscalar heavy-light mesons, and not for the vector heavy-light mesons.\n\nD) The local-duality limit leads to a strong isospin breaking of the order of 1 MeV for both charm and beauty sectors, due to the vanishing of the non-perturbative contributions to the two-point correlation functions.\n\nCorrect Answer: B) The local-duality limit forces the two-point correlation functions to vanish, but only for the non-perturbative contributions parametrized by vacuum condensates, leaving the perturbative QCD contributions unaffected.\n\nExplanation: The correct answer is B) because the local-duality limit is described in the documentation as a scenario where all nonperturbative contributions parametrized by vacuum condensates to the two-point correlation functions vanish, leaving only the perturbative QCD contributions to control the dependence of the heavy-light meson decay constants on $m_q$. This is in line with the statement in the documentation: \"For appropriate choices of the two-point correlation functions of currents interpolating the hadrons under study, the local-duality limit forces all nonperturbative contributions parametrized by vacuum condensates to such kind of correlator to vanish.\""}, "10": {"documentation": {"title": "Scaling laws in turbulence", "source": "Yves Pomeau and Martine Le Berre", "docs_id": "1912.12866", "section": ["physics.flu-dyn", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scaling laws in turbulence. Following the idea that dissipation in turbulence at high Reynolds number is by events singular in space-time and described by solutions of the inviscid Euler equations, we draw the conclusion that in such flows scaling laws should depend only on quantities appearing in the Euler equations. This excludes viscosity or a turbulent length as scaling parameters and constrains drastically possible analytical pictures of this limit. We focus on the law of drag by Newton for a projectile moving quickly in a fluid at rest. Inspired by the Newton's drag force law (proportional to the square of the speed of the moving object in the limit of large Reynolds numbers), which is well verified in experiments when the location of the detachment of the boundary layer is defined, we propose an explicit relationship between Reynolds's stress in the turbulent wake and quantities depending on the velocity field (averaged in time but depending on space), in the form of an integro-differential equation for the velocity which is solved for a Poiseuille flow in a circular pipe."}, "answer": "Here is the generated question:\n\nQuestion: Consider a turbulent flow in a circular pipe, where the Reynolds number is high and the flow is dominated by the inviscid Euler equations. The drag force on a projectile moving quickly in the fluid is proportional to the square of the speed of the projectile. Derive an integro-differential equation for the velocity field in the pipe, which relates the Reynolds's stress in the turbulent wake to quantities depending on the velocity field.\n\nA) \u2202u/\u2202t + u\u2207u = -1/\u03c1 \u2207p + \u03bd \u2207\u00b2u + F(t)\nB) \u2202u/\u2202t + u\u2207u = -1/\u03c1 \u2207p + \u03bd \u2207\u00b2u + \u222b\u222b\u222b(u\u00b2 - u\u2080\u00b2) dx dy dz\nC) \u2202u/\u2202t + u\u2207u = -1/\u03c1 \u2207p + \u03bd \u2207\u00b2u + \u222b\u222b\u222b(u\u00b2 - u\u2080\u00b2) dx dy dz + \u2202/\u2202x (u\u00b2)\nD) \u2202u/\u2202t + u\u2207u = -1/\u03c1 \u2207p + \u03bd \u2207\u00b2u + \u222b\u222b\u222b(u\u00b2 - u\u2080\u00b2) dx dy dz + \u2202/\u2202x (u\u00b2) + \u2202/\u2202y (u\u00b2)\n\nCorrect Answer: B) \u2202u/\u2202t + u\u2207u = -1/\u03c1 \u2207p + \u03bd \u2207\u00b2u + \u222b\u222b\u222b(u\u00b2 - u\u2080\u00b2) dx dy dz\n\nExplanation: The correct answer is based on the idea that the drag force on a projectile moving quickly in the fluid is proportional to the square of the speed of the projectile, which is a well-verified experimental result. This implies that the Reynolds's stress in the turbulent wake is related to the velocity field in a way that depends on the velocity field itself, rather than on viscosity or a turbulent length scale. The integro-differential equation proposed in the question is a mathematical representation of this idea, and it is consistent with the assumptions of the problem. The other options are incorrect because they either introduce additional terms that are not supported by the problem statement, or they do not capture the relationship between the Reynolds's stress and the velocity field that is implied by the problem."}, "11": {"documentation": {"title": "Reduced hierarchical equations of motion in real and imaginary time:\n  Correlated initial states and thermodynamic quantities", "source": "Yoshitaka Tanimura", "docs_id": "1407.1811", "section": ["cond-mat.stat-mech", "q-bio.QM", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reduced hierarchical equations of motion in real and imaginary time:\n  Correlated initial states and thermodynamic quantities. For a system strongly coupled to a heat bath, the quantum coherence of the system and the heat bath plays an important role in the system dynamics. This is particularly true in the case of non-Markovian noise. We rigorously investigate the influence of system-bath coherence by deriving the reduced hierarchal equations of motion (HEOM), not only in real time, but also in imaginary time, which represents an inverse temperature. It is shown that the HEOM in real time obtained when we include the system-bath coherence of the initial thermal equilibrium state possess the same form as those obtained from a factorized initial state. We find that the difference in behavior of systems treated in these two manners results from the difference in initial conditions of the HEOM elements, which are defined in path integral form. We also derive HEOM along the imaginary time path to obtain the thermal equilibrium state of a system strongly coupled to a non-Markovian bath. Then, we show that the steady state hierarchy elements calculated from the real-time HEOM can be expressed in terms of the hierarchy elements calculated from the imaginary-time HEOM. Moreover, we find that the imaginary-time HEOM allow us to evaluate a number of thermodynamic variables, including the free energy, entropy, internal energy, heat capacity, and susceptibility. The expectation values of the system energy and system-bath interaction energy in the thermal equilibrium state are also evaluated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Reduced Hierarchical Equations of Motion (HEOM) for a system strongly coupled to a non-Markovian heat bath, what is the relationship between the steady-state hierarchy elements calculated from the real-time HEOM and those calculated from the imaginary-time HEOM?\n\n**A)** The steady-state hierarchy elements from the real-time HEOM are identical to those from the imaginary-time HEOM.\n\n**B)** The steady-state hierarchy elements from the real-time HEOM are proportional to those from the imaginary-time HEOM.\n\n**C)** The steady-state hierarchy elements from the real-time HEOM are equal to the negative of those from the imaginary-time HEOM.\n\n**D)** The steady-state hierarchy elements from the real-time HEOM are independent of those from the imaginary-time HEOM.\n\n**Correct Answer:** C) The steady-state hierarchy elements from the real-time HEOM are equal to the negative of those from the imaginary-time HEOM.\n\n**Explanation:** According to the documentation, the steady-state hierarchy elements calculated from the real-time HEOM can be expressed in terms of the hierarchy elements calculated from the imaginary-time HEOM. This is because the real-time HEOM elements are defined in path integral form, which leads to a difference in initial conditions compared to the imaginary-time HEOM. Specifically, the real-time HEOM elements are equal to the negative of the imaginary-time HEOM elements, as shown in the documentation."}, "12": {"documentation": {"title": "Complexity of Scott Sentences", "source": "Rachael Alvir, Charles McCoy, and Julia Knight", "docs_id": "1807.02715", "section": ["math.LO", "math.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity of Scott Sentences. We give effective versions of some results on Scott sentences. We show that if $\\mathcal{A}$ has a computable $\\Pi_\\alpha$ Scott sentence, then the orbits of all tuples are defined by formulas that are computable $\\Sigma_\\beta$ for some $\\beta <\\alpha$. (This is an effective version of a result of Montalb\\'{a}n.) We show that if a countable structure $\\mathcal{A}$ has a computable $\\Sigma_\\alpha$ Scott sentence and one that is computable $\\Pi_\\alpha$, then it has one that is computable $d$-$\\Sigma_\\beta$ for some $\\beta < \\alpha$. (This is an effective version of a result of A. Miller.) We also give an effective version of a result of D. Miller. Using the non-effective results of Montalb\\'{a}n and A. Miller, we show that a finitely generated group has a $d$-$\\Sigma_2$ Scott sentence iff the orbit of some (or every) generating tuple is defined by a $\\Pi_1$ formula. Using our effective results, we show that for a computable finitely generated group, there is a computable $d$-$\\Sigma_2$ Scott sentence iff the orbit of some (every) generating tuple is defined by a computable $\\Pi_1$ formula."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the relationship between the computability of Scott sentences and the definition of orbits in finitely generated groups, as described in the effective version of Montalb\u00e1n's and Miller's results?\n\nA) If a finitely generated group has a computable $d$-$\\Sigma_2$ Scott sentence, then the orbit of every generating tuple is defined by a $\\Pi_1$ formula.\n\nB) If a finitely generated group has a computable $\\Pi_1$ formula defining the orbit of some generating tuple, then it has a computable $d$-$\\Sigma_2$ Scott sentence.\n\nC) If a finitely generated group has a computable $d$-$\\Sigma_2$ Scott sentence, then the orbit of every generating tuple is defined by a computable $\\Pi_1$ formula.\n\nD) If a finitely generated group has a computable $\\Pi_1$ formula defining the orbit of some generating tuple, then it has a computable $d$-$\\Sigma_2$ Scott sentence for every generating tuple.\n\nCorrect Answer: C) If a finitely generated group has a computable $d$-$\\Sigma_2$ Scott sentence, then the orbit of every generating tuple is defined by a computable $\\Pi_1$ formula.\n\nExplanation: This question requires the test-taker to understand the relationship between the computability of Scott sentences and the definition of orbits in finitely generated groups, as described in the effective version of Montalb\u00e1n's and Miller's results. The correct answer, C, states that if a finitely generated group has a computable $d$-$\\Sigma_2$ Scott sentence, then the orbit of every generating tuple is defined by a computable $\\Pi_1$ formula. This requires the test-taker to apply the results from the documentation to a specific scenario and to reason about the implications of the computability of Scott sentences on the definition of orbits."}, "13": {"documentation": {"title": "Area-preserving mapping of 3D ultrasound carotid artery images using\n  density-equalizing reference map", "source": "Gary P. T. Choi, Bernard Chiu, Chris H. Rycroft", "docs_id": "1812.03434", "section": ["cs.CG", "cs.CV", "math.NA", "physics.med-ph", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Area-preserving mapping of 3D ultrasound carotid artery images using\n  density-equalizing reference map. Carotid atherosclerosis is a focal disease at the bifurcations of the carotid artery. To quantitatively monitor the local changes in the vessel-wall-plus-plaque thickness (VWT) and compare the VWT distributions for different patients or for the same patients at different ultrasound scanning sessions, a mapping technique is required to adjust for the geometric variability of different carotid artery models. In this work, we propose a novel method called density-equalizing reference map (DERM) for mapping 3D carotid surfaces to a standardized 2D carotid template, with an emphasis on preserving the local geometry of the carotid surface by minimizing the local area distortion. The initial map was generated by a previously described arc-length scaling (ALS) mapping method, which projects a 3D carotid surface onto a 2D non-convex L-shaped domain. A smooth and area-preserving flattened map was subsequently constructed by deforming the ALS map using the proposed algorithm that combines the density-equalizing map and the reference map techniques. This combination allows, for the first time, one-to-one mapping from a 3D surface to a standardized non-convex planar domain in an area-preserving manner. Evaluations using 20 carotid surface models show that the proposed method reduced the area distortion of the flattening maps by over 80% as compared to the ALS mapping method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed density-equalizing reference map (DERM) method in mapping 3D carotid surfaces to a standardized 2D carotid template, and how does it differ from the arc-length scaling (ALS) mapping method?\n\n**A)** To reduce the complexity of the 3D carotid surface models and improve computational efficiency, while preserving the local geometry of the carotid surface.\n\n**B)** To create a one-to-one mapping from a 3D surface to a standardized non-convex planar domain in an area-preserving manner, while minimizing local area distortion.\n\n**C)** To enhance the visualization of carotid atherosclerosis by projecting 3D images onto a 2D plane, while maintaining the original shape and size of the carotid surface.\n\n**D)** To develop a novel method for segmenting carotid artery images and detecting plaque buildup, while preserving the local geometry of the carotid surface.\n\n**Correct Answer:** B) To create a one-to-one mapping from a 3D surface to a standardized non-convex planar domain in an area-preserving manner, while minimizing local area distortion.\n\n**Explanation:** The proposed DERM method aims to map 3D carotid surfaces to a standardized 2D carotid template while preserving the local geometry of the carotid surface. This is achieved by combining density-equalizing map and reference map techniques, which allows for an area-preserving flattening of the 3D surface onto a non-convex planar domain. In contrast, the ALS mapping method projects the 3D surface onto a 2D non-convex L-shaped domain, but does not necessarily preserve the local geometry of the carotid surface."}, "14": {"documentation": {"title": "Dynamical Analysis of Spatial Interaction Models", "source": "James Wilkinson, Theodore Emms, Tim S. Evans", "docs_id": "1911.10123", "section": ["physics.soc-ph", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Analysis of Spatial Interaction Models. We develop a novel dynamical method to examine spatial interaction models (SIMs). For each SIM, we use our dynamical framework to model emigration patterns. We look at the resulting population distributions to see if they are realistic or not. We use the US census data from 2010 and various spatial statistics to access the success or failure of each model. While we looked at over eighty different SIMs, we will focus here on two examples: the production constrained gravity model and the Radiation model. The results suggest that all these models fail to produce realistic population distributions and we identify the flaws within existing models. This leads us to suggest that we should define site attractiveness in terms of a second short range SIM leading to a new spatial interaction model - the Two-Trip model - which offers significant improvements when examined via our method. We also note that our Two-Trip adaptation can be used in any spatial modelling contexts, not just emigration."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Dynamical Analysis of Spatial Interaction Models\" propose a novel dynamical method to examine spatial interaction models (SIMs). What is the primary limitation of the existing SIMs, according to the authors, and how do they address this limitation in their proposed Two-Trip model?\n\nA) The existing SIMs fail to account for the effects of non-linear interactions between sites, which leads to unrealistic population distributions. The Two-Trip model addresses this limitation by incorporating a second short-range SIM.\n\nB) The existing SIMs are unable to capture the dynamics of emigration patterns, which leads to unrealistic population distributions. The Two-Trip model addresses this limitation by modeling emigration patterns using a dynamical framework.\n\nC) The existing SIMs are too simplistic and do not account for the complexities of spatial interactions, which leads to unrealistic population distributions. The Two-Trip model addresses this limitation by incorporating a more nuanced understanding of spatial interactions.\n\nD) The existing SIMs are unable to account for the effects of external factors, such as climate change, which leads to unrealistic population distributions. The Two-Trip model addresses this limitation by incorporating external factors into the model.\n\n**Correct Answer:** B) The existing SIMs are unable to capture the dynamics of emigration patterns, which leads to unrealistic population distributions. The Two-Trip model addresses this limitation by modeling emigration patterns using a dynamical framework.\n\n**Explanation:** The authors of the paper identify that the existing SIMs fail to produce realistic population distributions, which they attribute to the inability of these models to capture the dynamics of emigration patterns. In response, they propose the Two-Trip model, which uses a dynamical framework to model emigration patterns. This allows the model to better capture the complexities of spatial interactions and produce more realistic population distributions."}, "15": {"documentation": {"title": "On stochastic gradient Langevin dynamics with dependent data streams in\n  the logconcave case", "source": "M. Barkhagen, N. H. Chau, \\'E. Moulines, M. R\\'asonyi, S. Sabanis, Y.\n  Zhang", "docs_id": "1812.02709", "section": ["math.ST", "math.PR", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On stochastic gradient Langevin dynamics with dependent data streams in\n  the logconcave case. We study the problem of sampling from a probability distribution $\\pi$ on $\\rset^d$ which has a density \\wrt\\ the Lebesgue measure known up to a normalization factor $x \\mapsto \\rme^{-U(x)} / \\int_{\\rset^d} \\rme^{-U(y)} \\rmd y$. We analyze a sampling method based on the Euler discretization of the Langevin stochastic differential equations under the assumptions that the potential $U$ is continuously differentiable, $\\nabla U$ is Lipschitz, and $U$ is strongly concave. We focus on the case where the gradient of the log-density cannot be directly computed but unbiased estimates of the gradient from possibly dependent observations are available. This setting can be seen as a combination of a stochastic approximation (here stochastic gradient) type algorithms with discretized Langevin dynamics. We obtain an upper bound of the Wasserstein-2 distance between the law of the iterates of this algorithm and the target distribution $\\pi$ with constants depending explicitly on the Lipschitz and strong convexity constants of the potential and the dimension of the space. Finally, under weaker assumptions on $U$ and its gradient but in the presence of independent observations, we obtain analogous results in Wasserstein-2 distance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a probability distribution $\\pi$ on $\\mathbb{R}^d$ with a density function $f(x) = \\exp(-U(x)) / \\int_{\\mathbb{R}^d} \\exp(-U(y)) dy$, where $U$ is a continuously differentiable potential function. Suppose we have a stochastic gradient Langevin dynamics algorithm that samples from $\\pi$ using Euler discretization of the Langevin stochastic differential equations. The algorithm uses unbiased estimates of the gradient of the log-density, $\\nabla \\log f(x)$, which are available but possibly dependent on the observations.\n\n**Assumptions:**\n\n* The potential function $U$ is strongly concave.\n* The gradient of the potential function, $\\nabla U$, is Lipschitz continuous.\n* The dimension of the space, $d$, is finite.\n\n**Question:** What is the upper bound on the Wasserstein-2 distance between the law of the iterates of the stochastic gradient Langevin dynamics algorithm and the target distribution $\\pi$?\n\n**A)** $\\sqrt{2 \\log (1 + \\epsilon)}$\n**B)** $\\sqrt{2 \\log (1 + \\epsilon / C)}$\n**C)** $\\sqrt{2 \\log (1 + \\epsilon / C^2)}$\n**D)** $\\sqrt{2 \\log (1 + \\epsilon / C^3)}$\n\n**Correct Answer:** B) $\\sqrt{2 \\log (1 + \\epsilon / C)}$\n\n**Explanation:** The correct answer can be obtained by analyzing the assumptions and the assumptions of the stochastic gradient Langevin dynamics algorithm. The algorithm uses unbiased estimates of the gradient of the log-density, which are available but possibly dependent on the observations. The upper bound on the Wasserstein-2 distance between the law of the iterates of the algorithm and the target distribution $\\pi$ can be obtained by applying the Hoeffding inequality and the properties of the stochastic gradient Langevin dynamics algorithm. The correct answer is $\\sqrt{2 \\log (1 + \\epsilon / C)}$, where $C$ is a constant that depends on the Lipschitz and strong convexity constants of the potential function $U$ and the dimension of the space $d$."}, "16": {"documentation": {"title": "Interfacial thermal conduction and negative temperature jump in\n  one-dimensional lattices", "source": "Xiaodong Cao and Dahai He", "docs_id": "1507.05607", "section": ["cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interfacial thermal conduction and negative temperature jump in\n  one-dimensional lattices. We study the thermal boundary conduction in one-dimensional harmonic and $\\phi^{4}$ lattices, both of which consist of two segments coupled by a harmonic interaction. For the ballistic interfacial heat transport through the harmonic lattice, we use both theoretical calculation and molecular dynamics simulation to study the heat flux and temperature jump at the interface as to gain insights of the Kapitza resistance at the atomic scale. In the weak coupling regime, the heat current is proportional to the square of the coupling strength for the harmonic model as well as anharmonic models. Interestingly, there exists a negative temperature jump between the interfacial particles in particular parameter regimes. A nonlinear response of the boundary temperature jump to the externally applied temperature difference in the $\\phi^{4}$ lattice is observed. To understand the anomalous result, we then extend our studies to a model in which the interface is represented by a relatively small segment with gradually changing spring constants, and find that the negative temperature jump still exist. Finally, we show that the local velocity distribution at the interface is so close to the Gaussian distribution that the existence/absence of local equilibrium state seems unable to determine by numerics in this way."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the study of thermal boundary conduction in one-dimensional harmonic and \u03c64 lattices, and how does it relate to the concept of Kapitza resistance at the atomic scale?\n\nA) The heat current is proportional to the square of the coupling strength, and the Kapitza resistance is directly related to the temperature jump at the interface.\nB) The heat current is proportional to the square of the coupling strength, and the Kapitza resistance is inversely related to the temperature jump at the interface.\nC) The heat current is proportional to the square of the coupling strength, and the Kapitza resistance is independent of the temperature jump at the interface.\nD) The heat current is inversely proportional to the square of the coupling strength, and the Kapitza resistance is directly related to the temperature jump at the interface.\n\nCorrect Answer: B) The heat current is proportional to the square of the coupling strength, and the Kapitza resistance is inversely related to the temperature jump at the interface.\n\nExplanation: The study found that in the weak coupling regime, the heat current is proportional to the square of the coupling strength for both harmonic and anharmonic models. Additionally, the study observed a negative temperature jump between the interfacial particles in certain parameter regimes. This suggests that the Kapitza resistance, which is related to the temperature jump at the interface, is inversely related to the temperature jump. Therefore, option B is the correct answer."}, "17": {"documentation": {"title": "Variable Selection in Macroeconomic Forecasting with Many Predictors", "source": "Zhenzhong Wang, Zhengyuan Zhu, Cindy Yu", "docs_id": "2007.10160", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variable Selection in Macroeconomic Forecasting with Many Predictors. In the data-rich environment, using many economic predictors to forecast a few key variables has become a new trend in econometrics. The commonly used approach is factor augment (FA) approach. In this paper, we pursue another direction, variable selection (VS) approach, to handle high-dimensional predictors. VS is an active topic in statistics and computer science. However, it does not receive as much attention as FA in economics. This paper introduces several cutting-edge VS methods to economic forecasting, which includes: (1) classical greedy procedures; (2) l1 regularization; (3) gradient descent with sparsification and (4) meta-heuristic algorithms. Comprehensive simulation studies are conducted to compare their variable selection accuracy and prediction performance under different scenarios. Among the reviewed methods, a meta-heuristic algorithm called sequential Monte Carlo algorithm performs the best. Surprisingly the classical forward selection is comparable to it and better than other more sophisticated algorithms. In addition, we apply these VS methods on economic forecasting and compare with the popular FA approach. It turns out for employment rate and CPI inflation, some VS methods can achieve considerable improvement over FA, and the selected predictors can be well explained by economic theories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of macroeconomic forecasting, what are the key differences between the factor augmentation (FA) approach and the variable selection (VS) approach, and how do the VS methods introduced in the paper perform in comparison to the FA approach?\n\n**A)** The FA approach is more suitable for high-dimensional predictors, while the VS approach is more suitable for low-dimensional predictors. The VS methods introduced in the paper perform similarly to the FA approach in terms of variable selection accuracy and prediction performance.\n\n**B)** The FA approach is more computationally intensive than the VS approach, which is more suitable for large datasets. The VS methods introduced in the paper outperform the FA approach in terms of variable selection accuracy and prediction performance, especially for employment rate and CPI inflation forecasting.\n\n**C)** The FA approach is more interpretable than the VS approach, which is more suitable for complex economic systems. The VS methods introduced in the paper are comparable to the FA approach in terms of variable selection accuracy and prediction performance, but may require more computational resources.\n\n**D)** The FA approach is more prone to overfitting than the VS approach, which is more suitable for datasets with many redundant predictors. The VS methods introduced in the paper perform better than the FA approach in terms of variable selection accuracy and prediction performance, especially when using meta-heuristic algorithms.\n\n**Correct Answer:** B) The VS methods introduced in the paper outperform the FA approach in terms of variable selection accuracy and prediction performance, especially for employment rate and CPI inflation forecasting."}, "18": {"documentation": {"title": "Energy-Enstrophy Stability of beta-plane Kolmogorov Flow with Drag", "source": "Yue-Kin Tsang, William R. Young", "docs_id": "0803.0558", "section": ["physics.flu-dyn", "nlin.CD", "physics.ao-ph", "physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy-Enstrophy Stability of beta-plane Kolmogorov Flow with Drag. We develop a new nonlinear stability method, the Energy-Enstrophy (EZ) method, that is specialized to two-dimensional hydrodynamics; the method is applied to a beta-plane flow driven by a sinusoidal body force, and retarded by drag with damping time-scale mu^{-1}. The standard energy method (Fukuta and Murakami, J. Phys. Soc. Japan, 64, 1995, pp 3725) shows that the laminar solution is monotonically and globally stable in a certain portion of the (mu,beta)-parameter space. The EZ method proves nonlinear stability in a larger portion of the (mu,beta)-parameter space. And by penalizing high wavenumbers, the EZ method identifies a most strongly amplifying disturbance that is more physically realistic than that delivered by the energy method. Linear instability calculations are used to determine the region of the (mu,beta)-parameter space where the flow is unstable to infinitesimal perturbations. There is only a small gap between the linearly unstable region and the nonlinearly stable region, and full numerical solutions show only small transient amplification in that gap."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Energy-Enstrophy (EZ) method over the standard energy method in the context of nonlinear stability of two-dimensional hydrodynamics?\n\nA) The EZ method is more computationally efficient.\nB) The EZ method is more physically realistic in identifying the most strongly amplifying disturbance.\nC) The EZ method is only applicable to specific types of body forces.\nD) The EZ method is only applicable to flows with a certain range of beta values.\n\nCorrect Answer: B) The EZ method is more physically realistic in identifying the most strongly amplifying disturbance.\n\nExplanation: The EZ method is able to identify a more physically realistic most strongly amplifying disturbance compared to the energy method, which is a key advantage of the EZ method. This is stated in the documentation as \"And by penalizing high wavenumbers, the EZ method identifies a most strongly amplifying disturbance that is more physically realistic than that delivered by the energy method.\""}, "19": {"documentation": {"title": "A comparative study of two-dimensional vocal tract acoustic modeling\n  based on Finite-Difference Time-Domain methods", "source": "Debasish Ray Mohapatra, Victor Zappi, Sidney Fels", "docs_id": "2102.04588", "section": ["cs.SD", "cs.CL", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A comparative study of two-dimensional vocal tract acoustic modeling\n  based on Finite-Difference Time-Domain methods. The two-dimensional (2D) numerical approaches for vocal tract (VT) modelling can afford a better balance between the low computational cost and accurate rendering of acoustic wave propagation. However, they require a high spatio-temporal resolution in the numerical scheme for a precise estimation of acoustic formants at the simulation run-time expense. We have recently proposed a new VT acoustic modelling technique, known as the 2.5D Finite-Difference Time-Domain (2.5D FDTD), which extends the existing 2D FDTD approach by adding tube depth to its acoustic wave solver. In this work, first, the simulated acoustic outputs of our new model are shown to be comparable with the 2D FDTD and a realistic 3D FEM VT model at a low spatio-temporal resolution. Next, a radiation model is developed by including a circular baffle around the VT as head geometry. The transfer functions of the radiation model are analyzed using five different vocal tract shapes for vowel sounds /a/, /e/, /i/, /o/ and /u/."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the advantages and limitations of the 2.5D Finite-Difference Time-Domain (2.5D FDTD) approach for two-dimensional vocal tract acoustic modeling with the existing 2D FDTD approach and a realistic 3D FEM VT model.\n\n**A)** The 2.5D FDTD approach offers better accuracy in simulating acoustic wave propagation at low spatio-temporal resolution compared to the 2D FDTD approach, but it requires more computational resources.\n\n**B)** The 2.5D FDTD approach provides a better balance between computational cost and accuracy in simulating acoustic wave propagation compared to the 3D FEM VT model, which requires high spatio-temporal resolution.\n\n**C)** The 2.5D FDTD approach is more computationally expensive than the 2D FDTD approach and the 3D FEM VT model, but it offers better accuracy in simulating acoustic wave propagation at low spatio-temporal resolution.\n\n**D)** The 2.5D FDTD approach is more accurate than the 2D FDTD approach and the 3D FEM VT model in simulating acoustic wave propagation at low spatio-temporal resolution, but it requires more computational resources.\n\n**Correct Answer:** C) The 2.5D FDTD approach is more computationally expensive than the 2D FDTD approach and the 3D FEM VT model, but it offers better accuracy in simulating acoustic wave propagation at low spatio-temporal resolution.\n\n**Explanation:** The correct answer is C) because the 2.5D FDTD approach is more computationally expensive than the 2D FDTD approach and the 3D FEM VT model, but it offers better accuracy in simulating acoustic wave propagation at low spatio-temporal resolution. This is stated in the provided documentation: \"We have recently proposed a new VT acoustic modelling technique, known as the 2.5D Finite-Difference Time-Domain (2.5D FDTD), which extends the existing 2D FDTD approach by adding tube depth to its acoustic wave solver.\" The documentation also states that the 2.5D FDTD approach requires a high spatio-temporal resolution in the numerical scheme for a precise estimation of acoustic formants, but it offers better accuracy in simulating acoustic wave propagation at low spatio-temporal resolution."}, "20": {"documentation": {"title": "Fractional Dynamical Systems", "source": "Mark Edelman", "docs_id": "1401.0048", "section": ["nlin.CD", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fractional Dynamical Systems. In this paper the author presents the results of the preliminary investigation of fractional dynamical systems based on the results of numerical simulations of fractional maps. Fractional maps are equivalent to fractional differential equations describing systems experiencing periodic kicks. Their properties depend on the value of two parameters: the non-linearity parameter, which arises from the corresponding regular dynamical systems; and the memory parameter which is the order of the fractional derivative in the corresponding non-linear fractional differential equations. The examples of the fractional Standard and Logistic maps demonstrate that phase space of non-linear fractional dynamical systems may contain periodic sinks, attracting slow diverging trajectories, attracting accelerator mode trajectories, chaotic attractors, and cascade of bifurcations type trajectories whose properties are different from properties of attractors in regular dynamical systems. The author argues that discovered properties should be evident in the natural (biological, psychological, physical, etc.) and engineering systems with power-law memory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the properties of non-linear fractional dynamical systems, specifically the Standard and Logistic maps, and explain how the memory parameter affects the behavior of these systems.\n\nA) The memory parameter determines the stability of the system, with higher values leading to more stable trajectories and lower values leading to more unstable trajectories.\n\nB) The memory parameter influences the type of attractor that emerges in the system, with higher values favoring periodic sinks and lower values favoring chaotic attractors.\n\nC) The memory parameter affects the non-linearity of the system, with higher values leading to more non-linear behavior and lower values leading to more linear behavior.\n\nD) The memory parameter determines the rate at which the system converges to its equilibrium state, with higher values leading to faster convergence and lower values leading to slower convergence.\n\nCorrect Answer: B) The memory parameter influences the type of attractor that emerges in the system, with higher values favoring periodic sinks and lower values favoring chaotic attractors.\n\nExplanation: The correct answer is B) because the memory parameter is the order of the fractional derivative in the corresponding non-linear fractional differential equations, and it influences the type of attractor that emerges in the system. Higher values of the memory parameter favor periodic sinks, while lower values favor chaotic attractors. This is evident from the examples of the fractional Standard and Logistic maps, which demonstrate the emergence of periodic sinks, attracting slow diverging trajectories, and chaotic attractors, depending on the value of the memory parameter."}, "21": {"documentation": {"title": "A Direct Measurement of the IGM Opacity to HI Ionizing Photons", "source": "J. Xavier Prochaska (1), Gabor Worseck (1), John M. O'Meara (2) ((1)\n  IMPS, UCO/Lick Observatory, UCSC; (2) St. Michael's College)", "docs_id": "0910.0009", "section": ["astro-ph.CO", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Direct Measurement of the IGM Opacity to HI Ionizing Photons. We present a new method to directly measure the opacity from HI Lyman limit (LL) absorption k_LL along quasar sightlines by the intergalactic medium (IGM). The approach analyzes the average (``stacked'') spectrum of an ensemble of quasars at a common redshift to infer the mean free path (MFP) to ionizing radiation. We apply this technique to 1800 quasars at z=3.50-4.34 drawn from the Sloan Digital Sky Survey (SDSS), giving the most precise measurements on k_LL at any redshift. From z=3.6 to 4.3, the opacity increases steadily as expected and is well parameterized by MFP = (48.4 +/- 2.1) - (38.0 +/- 5.3)*(z-3.6) h^-1 Mpc (proper distance). The relatively high MFP values indicate that the incidence of systems which dominate k_LL evolves less strongly at z>3 than that of the Lya forest. We infer a mean free path three times higher than some previous estimates, a result which has important implications for the photo-ionization rate derived from the emissivity of star forming galaxies and quasars. Finally, our analysis reveals a previously unreported, systematic bias in the SDSS quasar sample related to the survey's color targeting criteria. This bias potentially affects all z~3 IGM studies using the SDSS database."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the main conclusion drawn from the analysis of the opacity to HI ionizing photons in the intergalactic medium (IGM) using the Sloan Digital Sky Survey (SDSS) quasar sample, and what implications does this have for the photo-ionization rate derived from the emissivity of star forming galaxies and quasars?\n\nA) The mean free path to ionizing radiation is lower than previously estimated, indicating a stronger evolution of k_LL at z>3, which has significant implications for the photo-ionization rate.\n\nB) The opacity to HI ionizing photons increases steadily from z=3.6 to 4.3, and the mean free path is three times higher than some previous estimates, which has important implications for the photo-ionization rate derived from the emissivity of star forming galaxies and quasars.\n\nC) The incidence of systems that dominate k_LL evolves more strongly at z>3 than that of the Lya forest, leading to a lower mean free path and reduced photo-ionization rate.\n\nD) The analysis reveals a systematic bias in the SDSS quasar sample related to the survey's color targeting criteria, but does not provide conclusive evidence for changes in the opacity to HI ionizing photons.\n\nCorrect Answer: B) The opacity to HI ionizing photons increases steadily from z=3.6 to 4.3, and the mean free path is three times higher than some previous estimates, which has important implications for the photo-ionization rate derived from the emissivity of star forming galaxies and quasars.\n\nExplanation: The correct answer is B) because the analysis reveals a steady increase in opacity from z=3.6 to 4.3, and the mean free path is indeed three times higher than some previous estimates. This has significant implications for the photo-ionization rate derived from the emissivity of star forming galaxies and quasars. The other options are incorrect because they either misrepresent the main conclusion of the analysis (A and C) or fail to mention the implications for the photo-ionization rate (D)."}, "22": {"documentation": {"title": "Role of the V-V dimerization in insulator-metal transition and optical\n  transmittance of pure and doped VO2 thin films", "source": "S. S. Majid (1), S. R. Sahu (2), A. Ahad (1), K. Dey (2), K. Gautam\n  (2), F. Rahman (1), P. Behera (2), U. Deshpande (2), V. G. Sathe (2), D. K.\n  Shukla (2) ((1) Department of Physics, Aligarh Muslim University,\n  Aligarh-202002, India (2) UGC-DAE Consortium for Scientific Research,\n  Indore-452001, India)", "docs_id": "1907.13042", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Role of the V-V dimerization in insulator-metal transition and optical\n  transmittance of pure and doped VO2 thin films. Insulator to metal (IMT) transition (T$_t$ $\\sim$ 341 K) in the VO2 accompanies transition from an infrared (IR) transparent to IR opaque phase. Tailoring of the IMT and associated IR switching behavior can offer potential thermochromic applications. Here we report on effects of the W and the Tb doping on the IMT and associated structural, electronic structure and optical properties of the VO2 thin film. Our results show that the W doping significantly lowers IMT temperature ($\\sim$ 292 K to $\\sim$ 247 K for 1.3\\% W to 3.7\\% W) by stabilizing the metallic rutile, $\\it{R}$, phase while Tb doping does not alter the IMT temperature much and retains the insulating monoclinic, $\\it{M1}$, phase at room temperature. It is observed that the W doping albeit significantly reduces the IR switching temperature but is detrimental to the solar modulation ability, contrary to the Tb doping effects where higher IR switching temperature and solar modulation ability is observed. The IMT behavior, electrical conductivity and IR switching behavior in the W and the Tb doped thin films are found to be directly associated with the spectral changes in the V 3$\\it{d_{\\|}}$ states."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary effect of W doping on the Insulator-Metal Transition (IMT) temperature of VO2 thin films, as reported in the study?\n\nA) W doping increases the IMT temperature, stabilizing the monoclinic M1 phase.\nB) W doping decreases the IMT temperature, stabilizing the metallic rutile R phase.\nC) W doping has no significant effect on the IMT temperature, retaining the insulating M1 phase.\nD) W doping increases the IMT temperature, but also reduces the IR switching temperature.\n\n**Correct Answer:** B) W doping decreases the IMT temperature, stabilizing the metallic rutile R phase.\n\n**Explanation:** The study reports that W doping significantly lowers the IMT temperature (from ~341 K to ~247 K) by stabilizing the metallic rutile R phase, while Tb doping has little to no effect on the IMT temperature. This suggests that W doping has a direct impact on the electronic structure and phase stability of VO2 thin films, leading to a decrease in the IMT temperature."}, "23": {"documentation": {"title": "Nonlinear Tensor Ring Network", "source": "Xiao Peng Li, Qi Liu and Hing Cheung So", "docs_id": "2111.06532", "section": ["cs.LG", "cs.AI", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear Tensor Ring Network. The state-of-the-art deep neural networks (DNNs) have been widely applied for various real-world applications, and achieved significant performance for cognitive problems. However, the increment of DNNs' width and depth in architecture results in a huge amount of parameters to challenge the storage and memory cost, limiting to the usage of DNNs on resource-constrained platforms, such as portable devices. By converting redundant models into compact ones, compression technique appears to be a practical solution to reducing the storage and memory consumption. In this paper, we develop a nonlinear tensor ring network (NTRN) in which both fullyconnected and convolutional layers are compressed via tensor ring decomposition. Furthermore, to mitigate the accuracy loss caused by compression, a nonlinear activation function is embedded into the tensor contraction and convolution operations inside the compressed layer. Experimental results demonstrate the effectiveness and superiority of the proposed NTRN for image classification using two basic neural networks, LeNet-5 and VGG-11 on three datasets, viz. MNIST, Fashion MNIST and Cifar-10."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the Nonlinear Tensor Ring Network (NTRN) proposed in the paper, and how does it address the limitations of traditional deep neural networks (DNNs) in resource-constrained platforms?\n\n**A)** The NTRN is designed to improve the accuracy of DNNs on resource-constrained platforms by reducing the number of parameters, thereby decreasing the storage and memory consumption. However, this approach may lead to a loss of accuracy, which is mitigated by the use of a nonlinear activation function.\n\n**B)** The NTRN is primarily motivated by the need to reduce the computational complexity of DNNs, which is achieved through the use of tensor ring decomposition. This approach enables the compression of both fully connected and convolutional layers, resulting in a more efficient model that can be deployed on resource-constrained platforms.\n\n**C)** The NTRN is designed to address the issue of overfitting in DNNs, which is achieved through the use of a nonlinear activation function that is embedded into the tensor contraction and convolution operations inside the compressed layer. This approach helps to maintain the accuracy of the model while reducing the number of parameters.\n\n**D)** The NTRN is primarily motivated by the need to improve the interpretability of DNNs, which is achieved through the use of tensor ring decomposition. This approach enables the compression of both fully connected and convolutional layers, resulting in a more interpretable model that can be deployed on resource-constrained platforms.\n\n**Correct Answer:** B) The NTRN is primarily motivated by the need to reduce the computational complexity of DNNs, which is achieved through the use of tensor ring decomposition. This approach enables the compression of both fully connected and convolutional layers, resulting in a more efficient model that can be deployed on resource-constrained platforms.\n\n**Explanation:** The correct answer is B) because the paper states that the increment of DNNs' width and depth in architecture results in a huge amount of parameters to challenge the storage and memory cost, limiting to the usage of DNNs on resource-constrained platforms. The NTRN is developed to mitigate this issue by converting redundant models into compact ones through tensor ring decomposition. The other options are incorrect because they do not accurately reflect the motivation behind the development of the NTRN. Option A is incorrect because while the NTRN does address the issue of accuracy loss caused by compression, it is not the primary motivation. Option C is incorrect because the NTRN is not primarily motivated by the need to improve the interpretability of DNNs. Option D is incorrect because the NTRN is not primarily motivated by the need to improve the interpretability of DNNs."}, "24": {"documentation": {"title": "Polyphonic Music Generation with Sequence Generative Adversarial\n  Networks", "source": "Sang-gil Lee, Uiwon Hwang, Seonwoo Min, Sungroh Yoon", "docs_id": "1710.11418", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Polyphonic Music Generation with Sequence Generative Adversarial\n  Networks. We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using sequence generative adversarial networks (SeqGAN) for creating polyphonic musical sequences, as proposed in the given Arxiv documentation?\n\n**A)** SeqGAN allows for the generation of monophonic melodies with dynamic timings.\n**B)** SeqGAN enables the simultaneous capture of chords and melodies with dynamic timings in a single word vector representation.\n**C)** SeqGAN stabilizes the training of GANs using the least squares method to the discriminator.\n**D)** SeqGAN requires careful optimization of reinforcement learning signals for general application.\n\n**Correct Answer:** B) SeqGAN enables the simultaneous capture of chords and melodies with dynamic timings in a single word vector representation.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed method \"condenses duration, octaves, and keys of both melodies and chords into a single word vector representation\", which allows for the simultaneous capture of chords and melodies with dynamic timings. This is a key advantage of using SeqGAN for creating polyphonic musical sequences. The other options are incorrect because they either describe a limitation (A), a related but distinct aspect of SeqGAN (C), or a separate consideration for general application (D)."}, "25": {"documentation": {"title": "How effective delays shape oscillatory dynamics in neuronal networks", "source": "Alex Roxin, Ernest Montbrio", "docs_id": "0905.0701", "section": ["q-bio.NC", "nlin.PS", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How effective delays shape oscillatory dynamics in neuronal networks. Synaptic, dendritic and single-cell kinetics generate significant time delays that shape the dynamics of large networks of spiking neurons. Previous work has shown that such effective delays can be taken into account with a rate model through the addition of an explicit, fixed delay [Roxin et al. PRL 238103 (2005)]. Here we extend this work to account for arbitrary symmetric patterns of synaptic connectivity and generic nonlinear transfer functions. Specifically, we conduct a weakly nonlinear analysis of the dynamical states arising via primary instabilities of the asynchronous state. In this way we determine analytically how the nature and stability of these states depend on the choice of transfer function and connectivity. We arrive at two general observations of physiological relevance that could not be explained in previous works. These are: 1 - Fast oscillations are always supercritical for realistic transfer functions. 2 - Traveling waves are preferred over standing waves given plausible patterns of local connectivity. We finally demonstrate that these results show a good agreement with those obtained performing numerical simulations of a network of Hodgkin-Huxley neurons."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** How do the authors of the study \"How effective delays shape oscillatory dynamics in neuronal networks\" address the issue of accounting for arbitrary symmetric patterns of synaptic connectivity and generic nonlinear transfer functions in their analysis of the dynamics of large networks of spiking neurons?\n\n**A)** The authors use a rate model with an explicit, fixed delay to account for the effects of synaptic and dendritic kinetics, as previously shown by Roxin et al. (2005).\n\n**B)** The authors conduct a weakly nonlinear analysis of the dynamical states arising via primary instabilities of the asynchronous state, taking into account the specific patterns of synaptic connectivity and nonlinear transfer functions.\n\n**C)** The authors use a numerical simulation approach to model the behavior of a network of Hodgkin-Huxley neurons, and compare the results with those obtained from the weakly nonlinear analysis.\n\n**D)** The authors assume a fixed, symmetric pattern of synaptic connectivity and a linear transfer function, and use this simplification to derive general observations about the nature and stability of the dynamical states.\n\n**Correct Answer:** B) The authors conduct a weakly nonlinear analysis of the dynamical states arising via primary instabilities of the asynchronous state, taking into account the specific patterns of synaptic connectivity and nonlinear transfer functions.\n\n**Explanation:** The correct answer is B) because the study explicitly states that the authors conduct a weakly nonlinear analysis of the dynamical states arising via primary instabilities of the asynchronous state, taking into account the specific patterns of synaptic connectivity and nonlinear transfer functions. This is in contrast to the other options, which either oversimplify the analysis (A and D) or focus on a different aspect of the study (C)."}, "26": {"documentation": {"title": "Current conservation, screening and the magnetic moment of the $\\Delta$\n  resonance. -- 1. Formulation without quark degrees of freedom", "source": "A. I. Machavariani and Amand Faessler", "docs_id": "0804.1322", "section": ["nucl-th", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Current conservation, screening and the magnetic moment of the $\\Delta$\n  resonance. -- 1. Formulation without quark degrees of freedom. The pion-nucleon bremsstrahlung $\\pi+N\\Longrightarrow\\gamma'+\\pi'+N'$ is studied in a new form of current conservation. According to this condition, the internal and external particle radiation parts of the $\\pi N$ radiation amplitude have opposite signs, i.e., they contain terms which must cancel each other. Therefore, one has a screening of the internal and external particle radiation in the $\\pi N$ bremsstrahlung. In particular, it is shown that the double $\\Delta$ exchange diagram with the $\\Delta-\\gamma' \\Delta'$ vertex cancel against the appropriate longitudinal part of the external particle radiation diagrams. Consequently, a model independent relation between the magnetic dipole moments of the $\\Delta^+$ and $\\Delta^{++}$ resonances and the anomalous magnetic moment of the proton $\\mu_p$ is obtained, where $\\mu_{\\Delta}$ is expressed by $\\mu_p$ as $\\mu_{\\Delta^+}={{M_{\\Delta}}\\over {m_p}} \\mu_p$ and $\\mu_{\\Delta^{++}}={3\\over 2}\\mu_{\\Delta^+}$ in agreement with the values extracted from the fit for the experimental cross section of the $\\pi^+ p\\to\\gamma'\\pi^+ p$ reaction."}, "answer": "Here is the exam question:\n\nQuestion: In the context of the pion-nucleon bremsstrahlung $\\pi+N\\Longrightarrow\\gamma'+\\pi'+N'$, what is the consequence of the current conservation formulation without quark degrees of freedom on the magnetic moment of the $\\Delta$ resonance?\n\nA) The magnetic moment of the $\\Delta$ resonance is proportional to the anomalous magnetic moment of the proton $\\mu_p$.\nB) The magnetic moment of the $\\Delta$ resonance is independent of the anomalous magnetic moment of the proton $\\mu_p$.\nC) The magnetic moment of the $\\Delta$ resonance is related to the anomalous magnetic moment of the proton $\\mu_p$ by the equation $\\mu_{\\Delta^+}={{M_{\\Delta}}\\over {m_p}} \\mu_p$ and $\\mu_{\\Delta^{++}}={3\\over 2}\\mu_{\\Delta^+}$.\nD) The magnetic moment of the $\\Delta$ resonance is equal to the anomalous magnetic moment of the proton $\\mu_p$.\n\nCorrect Answer: C) The magnetic moment of the $\\Delta$ resonance is related to the anomalous magnetic moment of the proton $\\mu_p$ by the equation $\\mu_{\\Delta^+}={{M_{\\Delta}}\\over {m_p}} \\mu_p$ and $\\mu_{\\Delta^{++}}={3\\over 2}\\mu_{\\Delta^+}$.\n\nExplanation: The correct answer is based on the formulation of current conservation without quark degrees of freedom, which leads to a screening of the internal and external particle radiation in the $\\pi N$ bremsstrahlung. This screening results in a model-independent relation between the magnetic dipole moments of the $\\Delta^+$ and $\\Delta^{++}$ resonances and the anomalous magnetic moment of the proton $\\mu_p$. The correct equations are $\\mu_{\\Delta^+}={{M_{\\Delta}}\\over {m_p}} \\mu_p$ and $\\mu_{\\Delta^{++}}={3\\over 2}\\mu_{\\Delta^+}$, which are derived from the double $\\Delta$ exchange diagram with the $\\Delta-\\gamma' \\Delta'$ vertex."}, "27": {"documentation": {"title": "Endogenous Formation of Limit Order Books: Dynamics Between Trades", "source": "Roman Gayduk and Sergey Nadtochiy", "docs_id": "1605.09720", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Endogenous Formation of Limit Order Books: Dynamics Between Trades. In this work, we present a continuous-time large-population game for modeling market microstructure betweentwo consecutive trades. The proposed modeling framework is inspired by our previous work [23]. In this framework, the Limit Order Book (LOB) arises as an outcome of an equilibrium between multiple agents who have different beliefs about the future demand for the asset. The agents' beliefs may change according to the information they observe, triggering changes in their behavior. We present an example illustrating how the proposed models can be used to quantify the consequences of changes in relevant information signals. If these signals, themselves, depend on the LOB, then, our approach allows one to model the \"indirect\" market impact (as opposed to the \"direct\" impact that a market order makes on the LOB, by eliminating certain limit orders). On the mathematical side, we formulate the proposed modeling framework as a continuum-player control-stopping game. We manage to split the equilibrium problem into two parts. The first one is described by a two-dimensional system of Reflected Backward Stochastic Differential Equations (RBSDEs), whose solution components reflect against each other. The second one leads to an infinite-dimensional fixed-point problem for a discontinuous mapping. Both problems are non-standard, and we prove the existence of their solutions in the paper."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Endogenous Formation of Limit Order Books: Dynamics Between Trades paper, what is the mathematical formulation of the proposed modeling framework, and what type of game is it?\n\n**A)** A finite-player zero-sum game, where agents' beliefs are modeled as a Markov chain, and the Limit Order Book (LOB) is a Nash equilibrium.\n\n**B)** A continuum-player control-stopping game, where agents' beliefs change according to observed information, and the LOB arises as an equilibrium between multiple agents with different beliefs.\n\n**C)** A dynamic programming problem, where agents' behavior is modeled as a sequence of optimal decisions, and the LOB is a function of the agents' beliefs and the market conditions.\n\n**D)** A game-theoretic model, where agents' actions are modeled as a set of strategies, and the LOB is a function of the agents' payoffs and the market conditions.\n\n**Correct Answer:** B) A continuum-player control-stopping game, where agents' beliefs change according to observed information, and the LOB arises as an equilibrium between multiple agents with different beliefs.\n\n**Explanation:** The correct answer is B) because the paper formulates the proposed modeling framework as a continuum-player control-stopping game, where agents' beliefs change according to observed information, and the LOB arises as an equilibrium between multiple agents with different beliefs. This is evident from the statement \"We manage to split the equilibrium problem into two parts. The first one is described by a two-dimensional system of Reflected Backward Stochastic Differential Equations (RBSDEs), whose solution components reflect against each other.\" This indicates that the game is a control-stopping game, where agents' behavior is controlled by their beliefs, which change according to observed information."}, "28": {"documentation": {"title": "On the charged Riemannian Penrose inequality with charged matter", "source": "Stephen McCormick", "docs_id": "1907.07967", "section": ["gr-qc", "math.DG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the charged Riemannian Penrose inequality with charged matter. Throughout the literature on the charged Riemannian Penrose inequality, it is generally assumed that there is no charged matter present; that is, the electric field is divergence-free. The aim of this article is to clarify when the charged Riemannian Penrose inequality holds in the presence of charged matter, and when it does not. First we revisit Jang's proof of the charged Riemannian Penrose inequality to show that under suitable conditions on the charged matter, this argument still carries though. In particular, a charged Riemannian Penrose inequality is obtained from this argument when charged matter is present provided that the charge density does not change sign. Moreover, we show that such hypotheses on the sign of the charge are in fact required by constructing counterexamples to the charged Riemannian Penrose inequality when these conditions are violated. We conclude by comparing this counterexample to another version of the Riemannian Penrose inequality with charged matter existing in the literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a charged Riemannian manifold (M,g) with a charge density \u03c1. Suppose that the electric field E is divergence-free, i.e., \u2207\u22c5E = 0. What are the necessary and sufficient conditions on the charge density \u03c1 for the charged Riemannian Penrose inequality to hold?\n\nA) \u03c1 is non-negative everywhere on M\nB) \u03c1 is non-positive everywhere on M\nC) \u03c1 is non-zero everywhere on M\nD) \u03c1 is zero on the boundary of M\n\nCorrect Answer: A) \u03c1 is non-negative everywhere on M\n\nExplanation: According to the article, the charged Riemannian Penrose inequality holds when the charge density \u03c1 does not change sign. This implies that \u03c1 must be non-negative everywhere on M, as a negative charge density would lead to a change in sign. The other options are incorrect because they do not guarantee that the charge density does not change sign. Option B is incorrect because a non-positive charge density would also lead to a change in sign. Option C is incorrect because a zero charge density would not provide any information about the electric field. Option D is incorrect because the boundary of M is not relevant to the charged Riemannian Penrose inequality."}, "29": {"documentation": {"title": "MammoGANesis: Controlled Generation of High-Resolution Mammograms for\n  Radiology Education", "source": "Cyril Zakka, Ghida Saheb, Elie Najem, Ghina Berjawi", "docs_id": "2010.05177", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MammoGANesis: Controlled Generation of High-Resolution Mammograms for\n  Radiology Education. During their formative years, radiology trainees are required to interpret hundreds of mammograms per month, with the objective of becoming apt at discerning the subtle patterns differentiating benign from malignant lesions. Unfortunately, medico-legal and technical hurdles make it difficult to access and query medical images for training. In this paper we train a generative adversarial network (GAN) to synthesize 512 x 512 high-resolution mammograms. The resulting model leads to the unsupervised separation of high-level features (e.g. the standard mammography views and the nature of the breast lesions), with stochastic variation in the generated images (e.g. breast adipose tissue, calcification), enabling user-controlled global and local attribute-editing of the synthesized images. We demonstrate the model's ability to generate anatomically and medically relevant mammograms by achieving an average AUC of 0.54 in a double-blind study on four expert mammography radiologists to distinguish between generated and real images, ascribing to the high visual quality of the synthesized and edited mammograms, and to their potential use in advancing and facilitating medical education."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the primary motivation behind the development of the MammoGANesis model, and how does it address the challenges faced by radiology trainees in interpreting mammograms?\n\nA) To improve the accuracy of mammogram interpretation by providing a large dataset of labeled images, but ultimately failing to address the need for controlled generation of high-resolution mammograms.\nB) To enable radiology trainees to practice interpreting mammograms in a controlled and safe environment, by generating high-quality, anatomically and medically relevant images that can be edited and queried.\nC) To develop a new algorithm for detecting breast cancer, but neglecting the importance of high-resolution mammogram generation for radiology education.\nD) To create a tool for radiologists to analyze and compare mammograms, but not addressing the need for controlled generation of images for training purposes.\n\nCorrect Answer: B) To enable radiology trainees to practice interpreting mammograms in a controlled and safe environment, by generating high-quality, anatomically and medically relevant images that can be edited and queried."}, "30": {"documentation": {"title": "Echoes in correlated neural systems", "source": "Moritz Helias, Tom Tetzlaff, Markus Diesmann", "docs_id": "1207.0298", "section": ["q-bio.NC", "cond-mat.dis-nn", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Echoes in correlated neural systems. Correlations are employed in modern physics to explain microscopic and macroscopic phenomena, like the fractional quantum Hall effect and the Mott insulator state in high temperature superconductors and ultracold atoms. Simultaneously probed neurons in the intact brain reveal correlations between their activity, an important measure to study information processing in the brain that also influences macroscopic signals of neural activity, like the electro encephalogram (EEG). Networks of spiking neurons differ from most physical systems: The interaction between elements is directed, time delayed, mediated by short pulses, and each neuron receives events from thousands of neurons. Even the stationary state of the network cannot be described by equilibrium statistical mechanics. Here we develop a quantitative theory of pairwise correlations in finite sized random networks of spiking neurons. We derive explicit analytic expressions for the population averaged cross correlation functions. Our theory explains why the intuitive mean field description fails, how the echo of single action potentials causes an apparent lag of inhibition with respect to excitation, and how the size of the network can be scaled while maintaining its dynamical state. Finally, we derive a new criterion for the emergence of collective oscillations from the spectrum of the time-evolution propagator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of correlated neural systems, what is the primary limitation of the mean field description of networks of spiking neurons, and how does the theory developed in the paper \"Echoes in correlated neural systems\" address this limitation?\n\nA) The mean field description fails to account for the non-equilibrium nature of the network's stationary state, which is influenced by the directed and time-delayed interactions between neurons.\nB) The mean field description is unable to capture the effects of short pulses and the echo of single action potentials on the network's dynamics.\nC) The mean field description is insufficient to describe the emergence of collective oscillations from the spectrum of the time-evolution propagator, which is a key feature of correlated neural systems.\nD) The mean field description is unable to account for the fact that each neuron receives events from thousands of neurons, leading to a complex and non-trivial network structure.\n\nCorrect Answer: B) The mean field description is unable to capture the effects of short pulses and the echo of single action potentials on the network's dynamics.\n\nExplanation: The paper \"Echoes in correlated neural systems\" develops a quantitative theory of pairwise correlations in finite-sized random networks of spiking neurons. The theory explains why the intuitive mean field description fails to accurately describe the network's dynamics, and it attributes this failure to the effects of short pulses and the echo of single action potentials. These effects cause an apparent lag of inhibition with respect to excitation, which is a key feature of correlated neural systems. Therefore, option B is the correct answer."}, "31": {"documentation": {"title": "Freeze-out of baryon number in low-scale leptogenesis", "source": "S. Eijima, M. Shaposhnikov, I. Timiryasov", "docs_id": "1709.07834", "section": ["hep-ph", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Freeze-out of baryon number in low-scale leptogenesis. Low-scale leptogenesis provides an economic and testable description of the origin of the baryon asymmetry of the Universe. In this scenario, the baryon asymmetry of the Universe is reprocessed from the lepton asymmetry by electroweak sphaleron processes. Provided that sphalerons are fast enough to maintain equilibrium, the values of the baryon and lepton asymmetries are related to each other. Usually, this relation is used to find the value of the baryon asymmetry at the time of the sphaleron freeze-out. To put in other words, the formula which is valid only when the sphalerons are fast, is applied at the moment when they are actually switched off. In this paper, we examine the validity of such a treatment. To this end, we solve the full system of kinetic equations for low-scale leptogenesis. This system includes equations describing the production of the lepton asymmetry in oscillations of right-handed neutrinos, as well as a separate kinetic equation for the baryon asymmetry. We show that for some values of the model parameters, the corrections to the standard approach are sizeable. We also present a feasible improvement to the ordinary procedure, which accounts for these corrections."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of low-scale leptogenesis, what is the assumption made when applying the formula that relates baryon and lepton asymmetries to find the value of the baryon asymmetry at the time of sphaleron freeze-out?\n\nA) That the sphalerons are always fast enough to maintain equilibrium.\nB) That the formula is valid even when the sphalerons are switched off.\nC) That the formula is only applicable when the lepton asymmetry is produced in oscillations of right-handed neutrinos.\nD) That the formula is a direct result of the electroweak sphaleron processes.\n\n**Correct Answer:** B) That the formula is valid even when the sphalerons are switched off.\n\n**Explanation:** The question requires the test-taker to understand the context of low-scale leptogenesis and the assumption made when applying the formula that relates baryon and lepton asymmetries. The correct answer, B, indicates that the formula is applied at the moment when the sphalerons are actually switched off, despite the fact that the formula is only valid when the sphalerons are fast enough to maintain equilibrium. This requires the test-taker to analyze the situation and identify the correct assumption.\n\n**Candidate A:** This option is incorrect because it states that the sphalerons are always fast enough to maintain equilibrium, which is not the assumption made in the question.\n\n**Candidate B:** This option is incorrect because it states that the formula is valid even when the sphalerons are switched off, which is the opposite of the correct assumption.\n\n**Candidate C:** This option is incorrect because it states that the formula is only applicable when the lepton asymmetry is produced in oscillations of right-handed neutrinos, which is not the assumption made in the question.\n\n**Candidate D:** This option is incorrect because it states that the formula is a direct result of the electroweak sphaleron processes, which is not the assumption made in the question."}, "32": {"documentation": {"title": "Understanding the Impacts of Dark Pools on Price Discovery", "source": "Linlin Ye", "docs_id": "1612.08486", "section": ["q-fin.GN", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the Impacts of Dark Pools on Price Discovery. This paper investigates the impact of dark pools on price discovery (the efficiency of prices on stock exchanges to aggregate information). Assets are traded in either an exchange or a dark pool, with the dark pool offering better prices but lower execution rates. Informed traders receive noisy and heterogeneous signals about an asset's fundamental. We find that informed traders use dark pools to mitigate their information risk and there is a sorting effect: in equilibrium, traders with strong signals trade in exchanges, traders with moderate signals trade in dark pools, and traders with weak signals do not trade. As a result, dark pools have an amplification effect on price discovery. That is, when information precision is high (information risk is low), the majority of informed traders trade in the exchange hence adding a dark pool enhances price discovery, whereas when information precision is low (information risk is high), the majority of the informed traders trade in the dark pool hence adding a dark pool impairs price discovery. The paper reconciles the conflicting empirical evidence and produces novel empirical predictions. The paper also provides regulatory suggestions with dark pools on current equity markets and in emerging markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the paper \"Understanding the Impacts of Dark Pools on Price Discovery\", what is the primary effect of dark pools on price discovery, and under what conditions does this effect occur?\n\n**A)** Dark pools have a neutral effect on price discovery, and the effect of dark pools on price discovery depends on the specific asset being traded.\n**B)** Dark pools have an amplification effect on price discovery when information precision is high, and an impairment effect on price discovery when information precision is low.\n**C)** Dark pools have a sorting effect on price discovery, where traders with strong signals trade in exchanges, traders with moderate signals trade in dark pools, and traders with weak signals do not trade.\n**D)** Dark pools have a diminishing effect on price discovery, regardless of the information precision, and the effect of dark pools on price discovery is always negative.\n\n**Correct Answer:** B) Dark pools have an amplification effect on price discovery when information precision is high, and an impairment effect on price discovery when information precision is low.\n\n**Explanation:** The paper states that \"when information precision is high (information risk is low), the majority of informed traders trade in the exchange hence adding a dark pool enhances price discovery, whereas when information precision is low (information risk is high), the majority of the informed traders trade in the dark pool hence adding a dark pool impairs price discovery.\" This indicates that dark pools have an amplification effect on price discovery when information precision is high, and an impairment effect when information precision is low."}, "33": {"documentation": {"title": "Conduct Risk - distribution models with very thin Tails", "source": "Peter Mitic", "docs_id": "1705.06868", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conduct Risk - distribution models with very thin Tails. Regulatory requirements dictate that financial institutions must calculate risk capital (funds that must be retained to cover future losses) at least annually. Procedures for doing this have been well-established for many years, but recent developments in the treatment of conduct risk (the risk of loss due to the relationship between a financial institution and its customers) have cast doubt on 'standard' procedures. Regulations require that operational risk losses should be aggregated by originating event. The effect is that a large number of small and medium-sized losses are aggregated into a small number of very large losses, such that a risk capital calculation produces a hugely inflated result. To solve this problem, a novel distribution based on a one-parameter probability density with an exponential of a fourth power is proposed, where the parameter is to be estimated. Symbolic computation is used to derive the necessary analytical expressions with which to formulate the problem, and is followed by numeric calculations in R. Goodness-of-fit and parameter estimation are both determined by using a novel method developed specifically for use with probability distribution functions. The results compare favourably with an existing model that used a LogGamma Mixture density, for which it was necessary to limit the frequency and severity of the losses. No such limits were needed using the proposed exponential density."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A financial institution is required to calculate risk capital at least annually to cover future losses due to conduct risk. The current procedure aggregates operational risk losses by originating event, resulting in a large number of small and medium-sized losses being aggregated into a small number of very large losses. A novel distribution model is proposed to address this issue, which is based on a one-parameter probability density function with an exponential of a fourth power. The parameter of this distribution is to be estimated.\n\n**A)** The proposed distribution model is suitable for modeling conduct risk losses that are normally distributed with a mean of 100 and a standard deviation of 50.\n\n**B)** The parameter of the proposed distribution model should be estimated using a maximum likelihood estimation method, as it is a one-parameter probability density function.\n\n**C)** The proposed distribution model is expected to produce a more accurate risk capital calculation than the existing LogGamma Mixture density model, which requires limiting the frequency and severity of losses.\n\n**D)** The proposed distribution model is only suitable for financial institutions with a large number of customers, as it is designed to aggregate small and medium-sized losses into a small number of very large losses.\n\n**Correct Answer:** C) The proposed distribution model is expected to produce a more accurate risk capital calculation than the existing LogGamma Mixture density model, which requires limiting the frequency and severity of losses.\n\n**Explanation:** The correct answer is C) because the documentation states that the proposed exponential density model produces a more accurate risk capital calculation than the existing LogGamma Mixture density model, which requires limiting the frequency and severity of losses. This is because the proposed model is able to accurately capture the distribution of conduct risk losses without the need for such limitations."}, "34": {"documentation": {"title": "Gluon and valence quark distributions for the pion and kaon in nuclear\n  matter", "source": "Parada T.P. Hutauruk, Seung-il Nam", "docs_id": "2112.05435", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gluon and valence quark distributions for the pion and kaon in nuclear\n  matter. In this paper we study the gluon and valence quark distributions in the pion and kaon in nuclear medium for various nuclear densities as well as in vacuum within the Nambu--Jona-Lasinio (NJL) model with the help of the proper-time regularization scheme which simulates a confinement of QCD. The nuclear medium effect is also determined in the same model for the symmetric nuclear matter. We then analyze the gluon and valence quark distributions for the bound pion and kaon in symmetric nuclear matter as well as those in vacuum. We find that the valence quark and gluon distributions in vacuum have relatively good agreements with the experimental data, the lattice QCD simulations, and the JAM Monte-Carlo (MC) global fit QCD analysis. Evolving to the higher factorization scale $Q = $ 4 GeV, the in-medium gluon and valence-quark distributions of the pion for various nuclear densities are turned out to be almost unchanged in comparison to the vacuum cases. On the contrary, for the kaon, they increase significantly with respect to the densities. Finally, we find that the vacuum gluon distribution for the kaon is smaller than that for the pion, which is consistent with other theoretical predictions. This feature holds for the in-medium gluon distribution in the nuclear density up to the saturation density."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Nambu-Jona-Lasinio (NJL) model, what is the expected behavior of the gluon and valence quark distributions for the pion and kaon in nuclear medium, particularly in comparison to their behavior in vacuum, and what implications does this have for theoretical predictions?\n\n**A)** The gluon and valence quark distributions for the pion and kaon in nuclear medium are expected to be significantly larger than those in vacuum, leading to a stronger nuclear medium effect.\n\n**B)** The gluon and valence quark distributions for the pion and kaon in nuclear medium are expected to be relatively unchanged compared to those in vacuum, with the kaon exhibiting a stronger nuclear medium effect.\n\n**C)** The gluon and valence quark distributions for the pion and kaon in nuclear medium are expected to be smaller than those in vacuum, with the pion exhibiting a weaker nuclear medium effect.\n\n**D)** The gluon and valence quark distributions for the pion and kaon in nuclear medium are expected to be similar to those in vacuum, with no significant nuclear medium effect.\n\n**Correct Answer:** B) The gluon and valence quark distributions for the pion and kaon in nuclear medium are expected to be relatively unchanged compared to those in vacuum, with the kaon exhibiting a stronger nuclear medium effect.\n\n**Explanation:** The correct answer is based on the text, which states that \"Evolving to the higher factorization scale $Q = $ 4 GeV, the in-medium gluon and valence-quark distributions of the pion for various nuclear densities are turned out to be almost unchanged in comparison to the vacuum cases.\" This suggests that the gluon and valence quark distributions for the pion in nuclear medium are relatively unchanged compared to those in vacuum. Additionally, the text states that \"for the kaon, they increase significantly with respect to the densities\", indicating that the kaon exhibits a stronger nuclear medium effect."}, "35": {"documentation": {"title": "Considering non-locality in the optical potentials within eikonal models", "source": "Chlo\\\"e Hebborn and Filomena M. Nunes", "docs_id": "2105.09844", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Considering non-locality in the optical potentials within eikonal models. Background: For its simplicity, the eikonal method is the tool of choice to analyze nuclear reactions at high energies ($E>100$ MeV/nucleon), including knockout reactions. However, so far, the effective interactions used in this method are assumed to be fully local. Purpose: Given the recent studies on non-local optical potentials, in this work we assess whether non-locality in the optical potentials is expected to impact reactions at high energies and then explore different avenues for extending the eikonal method to include non-local interactions. Method: We compare angular distributions obtained for non-local interactions (using the exact R-matrix approach for elastic scattering and the adiabatic distorted wave approximation for transfer) with those obtained using their local-equivalent interactions. Results: Our results show that transfer observables are significantly impacted by non-locality in the high-energy regime. Because knockout reactions are dominated by stripping (transfer to inelastic channels), non-locality is expected to have a large effect on knockout observables too. Three approaches are explored for extending the eikonal method to non-local interactions, including an iterative method and a perturbation theory. Conclusions: None of the derived extensions of the eikonal model provide a good description of elastic scattering. This work suggests that non-locality removes the formal simplicity associated with the eikonal model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion of the study on non-locality in optical potentials within eikonal models, and how does it impact the application of the eikonal method to high-energy nuclear reactions?\n\nA) The study concludes that non-locality has a negligible effect on high-energy nuclear reactions, and the eikonal method remains a suitable tool for analyzing these reactions. \nB) The study finds that non-locality significantly impacts transfer observables in high-energy nuclear reactions, but has a minimal effect on elastic scattering. \nC) The study concludes that the eikonal method is not suitable for analyzing high-energy nuclear reactions due to the limitations of its local-equivalent interactions. \nD) The study suggests that non-locality removes the formal simplicity associated with the eikonal model, but does not provide a clear conclusion on its impact on high-energy nuclear reactions.\n\nCorrect Answer: B) The study finds that non-locality significantly impacts transfer observables in high-energy nuclear reactions, but has a minimal effect on elastic scattering. \n\nExplanation: The study concludes that non-locality has a significant impact on transfer observables, which are dominant in knockout reactions. However, the impact on elastic scattering is minimal, as the study notes that none of the derived extensions of the eikonal model provide a good description of elastic scattering. This requires the test-taker to carefully read the study's conclusions and identify the correct impact of non-locality on high-energy nuclear reactions."}, "36": {"documentation": {"title": "The market nanostructure origin of asset price time reversal asymmetry", "source": "Marcus Cordi, Damien Challet, Serge Kassibrakis", "docs_id": "1901.00834", "section": ["q-fin.ST", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The market nanostructure origin of asset price time reversal asymmetry. We introduce a framework to infer lead-lag networks between the states of elements of complex systems, determined at different timescales. As such networks encode the causal structure of a system, infering lead-lag networks for many pairs of timescales provides a global picture of the mutual influence between timescales. We apply our method to two trader-resolved FX data sets and document strong and complex asymmetric influence of timescales on the structure of lead-lag networks. Expectedly, this asymmetry extends to trader activity: for institutional clients in our dataset, past activity on timescales longer than 3 hours is more correlated with future activity at shorter timescales than the opposite (Zumbach effect), while a reverse Zumbach effect is found for past timescales shorter than 3 hours; retail clients have a totally different, and much more intricate, structure of asymmetric timescale influence. The causality structures are clearly caused by markedly different behaviors of the two types of traders. Hence, market nanostructure, i.e., market dynamics at the individual trader level, provides an unprecedented insight into the causality structure of financial markets, which is much more complex than previously thought."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"The market nanostructure origin of asset price time reversal asymmetry\" propose a framework to infer lead-lag networks between the states of elements of complex systems at different timescales. What is the primary implication of this framework, according to the authors?\n\nA) That lead-lag networks are only present in financial markets during times of high volatility.\nB) That the causal structure of a system is determined by the interactions between its constituent elements at different timescales.\nC) That the influence of timescales on the structure of lead-lag networks is symmetric, meaning that past activity at one timescale is equally correlated with future activity at another timescale.\nD) That the asymmetry in the influence of timescales on lead-lag networks is a result of the different behaviors of institutional and retail traders.\n\n**Correct answer:** B) That the causal structure of a system is determined by the interactions between its constituent elements at different timescales.\n\n**Explanation:** The correct answer is B) because the authors' framework is designed to infer lead-lag networks that encode the causal structure of a system, which is determined by the interactions between its constituent elements at different timescales. The other options are incorrect because they either misrepresent the authors' findings (A and C) or are too narrow in scope (D). Option B is the most comprehensive and accurate summary of the authors' proposal."}, "37": {"documentation": {"title": "Wide band spectroscopic response of monocrystallines to low dose neutron\n  and gamma radiation", "source": "Yossi Mosbacher, Micha Weiss, Hagar Landsman, Nadav Priel, Ilan\n  Eliyahu, Arik Kreisel, Offir Ozeri, David Hershkovich, Ori Cheshnovsky, Ranny\n  Budnik", "docs_id": "1902.10668", "section": ["physics.ins-det", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wide band spectroscopic response of monocrystallines to low dose neutron\n  and gamma radiation. We identify a number of crystalline structures with promising characteristics to serve as a detection medium for a novel Dark Matter (DM) detector with a low threshold energy. A detector of this kind can be specifically useful in application requiring the detection of nuclear recoils, such as in direct detection of low mass DM, coherent neutrino scattering and neutrons. We describe a broad band, high sensitivity optical setup designed and constructed for the purpose of this search and future investigations of specific crystals. We report on the fluorescent signals produced from exposure to low doses of neutrons and $\\gamma$ rays and find potential targets in Quartz, Sapphire, LiF, CaF$_{2}$ and BaF$_{2}$. These crystals and specific signals will be the subject of further study to establish the various traits relevant for a full scale DM detector. In this paper we identify the most interesting signals that will be promoted to significantly more detailed studies, including their production mechanism."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat type of radiation is most likely to produce a fluorescent signal in the crystals identified as potential targets for a Dark Matter detector, and what is the characteristic energy threshold for this type of radiation?\n\nA) Gamma rays with an energy threshold of 10 keV\nB) Neutrons with an energy threshold of 1 MeV\nC) Alpha particles with an energy threshold of 100 keV\nD) X-rays with an energy threshold of 50 keV\n\nCorrect Answer: A) Gamma rays with an energy threshold of 10 keV\n\nExplanation: The paper states that the authors \"exposure to low doses of neutrons and \u03b3 rays\" produced fluorescent signals, and that the crystals identified as potential targets for a Dark Matter detector are Quartz, Sapphire, LiF, CaF2, and BaF2. The paper also mentions that the detector is designed to detect \"nuclear recoils\", which is a characteristic of interactions between gamma rays and matter. Therefore, it is likely that gamma rays are the type of radiation most likely to produce a fluorescent signal in these crystals, and that the characteristic energy threshold for this type of radiation is around 10 keV, which is a typical energy range for gamma rays used in spectroscopic applications.\n\nNote: The other options are incorrect because:\n\n* B) Neutrons are not typically associated with fluorescent signals, and the energy threshold for neutron-induced reactions is typically much higher than 1 MeV.\n* C) Alpha particles are not mentioned in the paper as a type of radiation that produces fluorescent signals in the identified crystals.\n* D) X-rays are not mentioned in the paper as a type of radiation that produces fluorescent signals in the identified crystals, and the energy threshold for X-rays is typically much lower than 50 keV."}, "38": {"documentation": {"title": "Casimir Self-Entropy of an Electromagnetic Thin Sheet", "source": "Yang Li, K. A. Milton, Pushpa Kalauni, and Prachi Parashar", "docs_id": "1607.07900", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Casimir Self-Entropy of an Electromagnetic Thin Sheet. Casimir entropies due to quantum fluctuations in the interaction between electrical bodies can often be negative, either caused by dissipation or by geometry. Although generally such entropies vanish at zero temperature, consistent with the third law of thermodynamics (the Nernst heat theorem), there is a region in the space of temperature and separation between the bodies where negative entropy occurs, while positive interaction entropies arise for large distances or temperatures. Systematic studies on this phenomenon in the Casimir-Polder interaction between a polarizable nanoparticle or atom and a conducting plate in the dipole approximation have been given recently. Since the total entropy should be positive according to the second law of thermodynamics, we expect that the self-entropy of the bodies would be sufficiently positive as to overwhelm the negative interaction entropy. This expectation, however, has not been explicitly verified. Here we compute the self-entropy of an electromagnetic $\\delta$-function plate, which corresponds to a perfectly conducting sheet in the strong coupling limit. The transverse electric contribution to the self-entropy is negative, while the transverse magnetic contribution is larger and positive, so the total self-entropy is positive. However, this self-entropy vanishes in the strong-coupling limit. In that case, it is the self-entropy of the nanoparticle that is just sufficient to result in a nonnegative total entropy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a polarizable nanoparticle interacting with a perfectly conducting electromagnetic sheet in the strong coupling limit. What is the expected behavior of the total entropy of the system, and what is the role of the self-entropy of the nanoparticle in this scenario?\n\nA) The total entropy is negative, and the self-entropy of the nanoparticle is sufficient to result in a nonnegative total entropy.\nB) The total entropy is positive, and the self-entropy of the nanoparticle is negative, leading to a negative total entropy.\nC) The total entropy is zero, and the self-entropy of the nanoparticle is equal to the negative interaction entropy.\nD) The total entropy is positive, and the self-entropy of the nanoparticle is positive, but the self-entropy vanishes in the strong-coupling limit.\n\nCorrect Answer: D) The total entropy is positive, and the self-entropy of the nanoparticle is positive, but the self-entropy vanishes in the strong-coupling limit.\n\nExplanation: According to the text, the total entropy of the system is expected to be positive, as it should be according to the second law of thermodynamics. The self-entropy of the nanoparticle is positive, but the self-entropy of the electromagnetic sheet is negative, leading to a positive total entropy. However, in the strong-coupling limit, the self-entropy of the electromagnetic sheet vanishes, leaving the self-entropy of the nanoparticle as the dominant contributor to the total entropy, which is positive."}, "39": {"documentation": {"title": "Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler", "source": "Ding Shi, Wenjin Wang, Li You, Xiaohang Song, Yi Hong, Xiqi Gao,\n  Gerhard Fettweis", "docs_id": "2105.09628", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler. Although the combination of the orthogonal time frequency space (OTFS) modulation and the massive multiple-input multiple-output (MIMO) technology can make communication systems perform better in high-mobility scenarios, there are still many challenges in downlink channel estimation owing to inaccurate modeling and high pilot overhead in practical systems. In this paper, we propose a channel state information (CSI) acquisition scheme for downlink massive MIMO-OTFS in presence of the fractional Doppler, including deterministic pilot design and channel estimation algorithm. First, we analyze the input-output relationship of the single-input single-output (SISO) OTFS based on the orthogonal frequency division multiplexing (OFDM) modem and extend it to massive MIMO-OTFS. Moreover, we formulate an accurate model for the practical system in which the fractional Doppler is considered and the influence of subpaths is revealed. A deterministic pilot design is then proposed based on the model and the structure of the pilot matrix to reduce pilot overhead and save memory consumption. Since channel geometry changes very slowly relative to the communication timescale, we put forward a modified sensing matrix based channel estimation (MSMCE) algorithm to acquire the downlink CSI. Simulation results demonstrate that the proposed downlink CSI acquisition scheme has significant advantages over traditional algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of downlink massive MIMO-OTFS systems in presence of fractional Doppler, what is the primary advantage of the proposed deterministic pilot design and channel estimation algorithm, MSMCE?\n\n**A)** Reduced pilot overhead and memory consumption, but with increased computational complexity.\n**B)** Improved channel estimation accuracy, but at the cost of increased pilot overhead.\n**C)** Enhanced robustness to fractional Doppler, but with reduced pilot design flexibility.\n**D)** Simplified pilot design and estimation, but with decreased channel estimation accuracy.\n\n**Correct Answer:** A) Reduced pilot overhead and memory consumption, but with increased computational complexity.\n\n**Explanation:** The proposed deterministic pilot design and channel estimation algorithm, MSMCE, aims to reduce pilot overhead and memory consumption by designing a pilot matrix that takes into account the slow-changing channel geometry. However, this approach also increases the computational complexity of the channel estimation process. The correct answer highlights the trade-off between these two competing design goals."}, "40": {"documentation": {"title": "New ALMA and Fermi/LAT Observations of the large-scale jet of PKS\n  0637-752 Strengthen the Case Against the IC/CMB Model", "source": "Eileen T. Meyer, Peter Breiding, Markos Georganopoulos, Ivan Oteo,\n  Martin A. Zwaan, Robert Laing, Leith Godfrey, R. J. Ivison", "docs_id": "1702.00015", "section": ["astro-ph.HE", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New ALMA and Fermi/LAT Observations of the large-scale jet of PKS\n  0637-752 Strengthen the Case Against the IC/CMB Model. The Chandra X-ray observatory has discovered several dozen anomalously X-ray-bright jets associated with powerful quasars. A popular explanation for the X-ray flux from the knots in these jets is that relativistic synchrotron-emitting electrons inverse-Compton scatter Cosmic Microwave Background (CMB) photons to X-ray energies (the IC/CMB model). This model predicts a high gamma-ray flux which should be detectable by the Fermi Large Area Telescope (LAT) for many sources. GeV-band upper limits from Fermi/LAT for the well-known anomalous X-ray jet in PKS 0637-752 were previously shown in Meyer et al., (2015) to violate the predictions of the IC/CMB model. Previously, measurements of the jet synchrotron spectrum, important for accurately predicting the gamma-ray flux level, were lacking between radio and infrared wavelengths. Here we present new Atacama Large Millimeter/submillimeter Array (ALMA) observations of the large-scale jet at 100, 233, and 319 GHz which further constrain the synchrotron spectrum, supporting the previously published empirical model. We also present updated limits from the Fermi/LAT using the new `Pass 8' calibration and approximately 30% more time on source. With these deeper limits we rule out the IC/CMB model at the 8.7 sigma level. Finally, we demonstrate that complete knowledge of the synchrotron SED is critical in evaluating the IC/CMB model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the new ALMA and Fermi/LAT observations of the large-scale jet of PKS 0637-752, and how does it relate to the IC/CMB model?\n\nA) The observations confirm the IC/CMB model, providing strong evidence for inverse-Compton scattering of CMB photons to X-ray energies.\nB) The observations rule out the IC/CMB model, demonstrating that the X-ray flux from the knots in the jet cannot be explained by inverse-Compton scattering of CMB photons.\nC) The observations provide mixed results, with some evidence supporting the IC/CMB model and other evidence contradicting it.\nD) The observations are inconclusive, and further data are needed to determine the validity of the IC/CMB model.\n\nCorrect Answer: B) The observations rule out the IC/CMB model, demonstrating that the X-ray flux from the knots in the jet cannot be explained by inverse-Compton scattering of CMB photons.\n\nExplanation: The correct answer is B) because the text states that the new Fermi/LAT limits \"rule out the IC/CMB model at the 8.7 sigma level\", indicating that the observations provide strong evidence against the IC/CMB model. The other options are incorrect because they do not accurately reflect the conclusion drawn from the observations. Option A is incorrect because the observations actually contradict the IC/CMB model, not confirm it. Option C is incorrect because the text states that the observations \"support the previously published empirical model\", indicating a clear direction in the evidence. Option D is incorrect because the text states that the observations provide strong evidence against the IC/CMB model, indicating that the conclusion is clear and well-supported."}, "41": {"documentation": {"title": "The interconnected wealth of nations: Shock propagation on global\n  trade-investment multiplex networks", "source": "Michele Starnini, Mari\\'an Bogu\\~n\\'a, and M. \\'Angeles Serrano", "docs_id": "1901.01976", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The interconnected wealth of nations: Shock propagation on global\n  trade-investment multiplex networks. The increasing integration of world economies, which organize in complex multilayer networks of interactions, is one of the critical factors for the global propagation of economic crises. We adopt the network science approach to quantify shock propagation on the global trade-investment multiplex network. To this aim, we propose a model that couples a Susceptible-Infected-Recovered epidemic spreading dynamics, describing how economic distress propagates between connected countries, with an internal contagion mechanism, describing the spreading of such economic distress within a given country. At the local level, we find that the interplay between trade and financial interactions influences the vulnerabilities of countries to shocks. At the large scale, we find a simple linear relation between the relative magnitude of a shock in a country and its global impact on the whole economic system, albeit the strength of internal contagion is country-dependent and the intercountry propagation dynamics is non-linear. Interestingly, this systemic impact can be predicted on the basis of intra-layer and inter-layer scale factors that we name network multipliers, that are independent of the magnitude of the initial shock. Our model sets-up a quantitative framework to stress-test the robustness of individual countries and of the world economy to propagating crashes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"The interconnected wealth of nations: Shock propagation on global trade-investment multiplex networks\" propose a model that couples a Susceptible-Infected-Recovered (SIR) epidemic spreading dynamics with an internal contagion mechanism to quantify shock propagation on the global trade-investment multiplex network. What is the primary finding of the study regarding the relationship between the relative magnitude of a shock in a country and its global impact on the whole economic system?\n\nA) The strength of internal contagion is directly proportional to the magnitude of the initial shock.\nB) The intercountry propagation dynamics is linear, and the global impact is directly proportional to the magnitude of the initial shock.\nC) The global impact is independent of the magnitude of the initial shock, and can be predicted using network multipliers that are independent of the initial shock.\nD) The interplay between trade and financial interactions influences the vulnerabilities of countries to shocks, but does not affect the global impact of a shock.\n\n**Correct Answer:** C) The global impact is independent of the magnitude of the initial shock, and can be predicted using network multipliers that are independent of the initial shock.\n\n**Explanation:** The study finds that the global impact of a shock in a country is independent of the magnitude of the initial shock, and can be predicted using network multipliers that are independent of the initial shock. This means that even if the initial shock is small, its global impact can be predicted using the network structure and the relationships between countries. This is a key finding of the study, as it suggests that the global economy is more resilient to shocks than previously thought."}, "42": {"documentation": {"title": "Nucleon Polarisabilities from Compton Scattering on the Deuteron", "source": "Harald W. Griesshammer (TU M\"unchen), Gautam Rupak (TRIUMF)", "docs_id": "nucl-th/0012096", "section": ["nucl-th", "hep-ph", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nucleon Polarisabilities from Compton Scattering on the Deuteron. An analytic calculation of the differential cross section for elastic Compton scattering on the deuteron at photon energies \\omega in the range of 25-50 MeV is presented to next-to-next-to-leading order, i.e. to an accuracy of \\sim 3%. The calculation is model-independent and performed in the low energy nuclear Effective Field Theory without dynamical pions. The iso-scalar, scalar electric and magnetic nucleon polarisabilities \\alpha_0 and \\beta_0 enter as free parameters with a theoretical uncertainty of about 20%. Using data at $\\omega_{Lab}=49 MeV$ we find $\\alpha_0=8.4\\pm 3.0(exp)\\pm 1.7(theor)$, $\\beta_0=8.9\\pm 3.9(exp)\\pm 1.8(theor)$, each in units of $10^{-4} fm^3$. With the experimental constraint for the iso-scalar Baldin sum rule, $\\alpha_0=7.2\\pm 2.1(exp)\\pm 1.6(theor)$, $\\beta_0=6.9\\mp 2.1(exp)\\mp 1.6(theor)$. A more accurate result can be achieved by: (i) better experimental data, and (ii) a higher order theoretical calculation including contributions from a couple of so far undetermined four-nucleon-two-photon operators."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: The authors of the paper \"Nucleon Polarisabilities from Compton Scattering on the Deuteron\" present an analytic calculation of the differential cross section for elastic Compton scattering on the deuteron at photon energies \u03c9 in the range of 25-50 MeV. The calculation is performed in the low energy nuclear Effective Field Theory without dynamical pions.\n\nA) The authors assume that the iso-scalar, scalar electric, and magnetic nucleon polarisabilities \u03b10 and \u03b20 are equal to their experimental values, i.e., \u03b10 = 8.4 \u00d7 10^(-4) fm^3 and \u03b20 = 8.9 \u00d7 10^(-4) fm^3.\n\nB) The authors use a higher order theoretical calculation including contributions from a couple of so far undetermined four-nucleon-two-photon operators to achieve a more accurate result.\n\nC) The authors find that the experimental constraint for the iso-scalar Baldin sum rule leads to a significant reduction in the uncertainty of \u03b10 and \u03b20, resulting in \u03b10 = 7.2 \u00d7 10^(-4) fm^3 and \u03b20 = 6.9 \u00d7 10^(-4) fm^3.\n\nD) The authors conclude that the theoretical uncertainty of \u03b10 and \u03b20 is dominated by the experimental uncertainty, and that a more accurate result can be achieved by improving the experimental data.\n\nCorrect Answer: C) The authors find that the experimental constraint for the iso-scalar Baldin sum rule leads to a significant reduction in the uncertainty of \u03b10 and \u03b20, resulting in \u03b10 = 7.2 \u00d7 10^(-4) fm^3 and \u03b20 = 6.9 \u00d7 10^(-4) fm^3.\n\nExplanation: This question requires the student to carefully read the text and identify the correct statement. Option C is the correct answer because it accurately summarizes the result of the experimental constraint for the iso-scalar Baldin sum rule, which leads to a reduction in the uncertainty of \u03b10 and \u03b20. Option A is incorrect because it assumes that the authors use the experimental values of \u03b10 and \u03b20, which is not stated in the text. Option B is incorrect because it mentions a higher order theoretical calculation, but the text does not provide any information about this. Option D is incorrect because it implies that the experimental uncertainty is the only source of uncertainty, which is not the case according to the text."}, "43": {"documentation": {"title": "Expanding versus non expanding universe", "source": "Antonio Alfonso-Faus", "docs_id": "1207.0997", "section": ["physics.gen-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Expanding versus non expanding universe. In cosmology the number of scientists using the framework of an expanding universe is very high. This model, the big-bang, is now overwhelmingly present in almost all aspects of society. It is the main stream cosmology of today. A small number of scientists are researching on the possibility of a non-expanding universe. The existence of these two groups, one very large and the other very small, is a good proof of the use of the scientific method: it does not drive to an absolute certainty. All models have to be permanently validated, falsified. Ockham's razor, a powerful philosophical tool, will probably change the amount of scientists working in each of these groups. We present here a model where a big-bang is unnecessary. It ends, in a finite time, in a second INFLATION, or a disaggregation to infinity. We also discuss the possibilities of a non-expanding universe model. Only a few references will be cited, mainly concerned with our own work in the past, thus purposely avoiding citing the many thousands of professionals working in this field."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The author of the given Arxiv documentation presents a model that challenges the mainstream cosmology of an expanding universe. According to the author, what is the primary consequence of using Ockham's razor in the context of cosmology?\n\nA) It leads to an absolute certainty about the existence of a non-expanding universe.\nB) It favors the big-bang model over the non-expanding universe model.\nC) It reduces the number of scientists working on the non-expanding universe model.\nD) It eliminates the possibility of a finite-time end to the universe.\n\n**Correct Answer:** C) It reduces the number of scientists working on the non-expanding universe model.\n\n**Explanation:** The author states that Ockham's razor, a powerful philosophical tool, will \"probably change the amount of scientists working in each of these groups.\" This implies that the application of Ockham's razor will lead to a reduction in the number of scientists working on the non-expanding universe model, as it is considered a less parsimonious explanation. The other options are incorrect because the author does not claim that Ockham's razor leads to absolute certainty, favors the big-bang model, or eliminates the possibility of a finite-time end to the universe."}, "44": {"documentation": {"title": "Higher order approximation of call option prices under stochastic\n  volatility models", "source": "Archil Gulisashvili, Ra\\'ul Merino, Marc Lagunas and Josep Vives", "docs_id": "1905.06315", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Higher order approximation of call option prices under stochastic\n  volatility models. In the present paper, a decomposition formula for the call price due to Al\\`{o}s is transformed into a Taylor type formula containing an infinite series with stochastic terms. The new decomposition may be considered as an alternative to the decomposition of the call price found in a recent paper of Al\\`{o}s, Gatheral and Radoi\\v{c}i\\'{c}. We use the new decomposition to obtain various approximations to the call price in the Heston model with sharper estimates of the error term than in the previously known approximations. One of the formulas obtained in the present paper has five significant terms and an error estimate of the form $O(\\nu^{3}(\\left|\\rho\\right|+\\nu))$, where $\\nu$ is the vol-vol parameter, and $\\rho$ is the correlation coefficient between the price and the volatility in the Heston model. Another approximation formula contains seven more terms and the error estimate is of the form $O(\\nu^4(1+|\\rho|)$. For the uncorrelated Hestom model ($\\rho=0$), we obtain a formula with four significant terms and an error estimate $O(\\nu^6)$. Numerical experiments show that the new approximations to the call price perform especially well in the high volatility mode."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Heston model, derive the Taylor type formula for the call price using the new decomposition formula, and identify the error estimate for the approximation formula with five significant terms.\n\nA) The Taylor type formula for the call price is given by: $C(S,t) = S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + \\frac{\\nu}{2} \\cdot S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + O(\\nu^3(\\left|\\rho\\right|+\\nu))$\n\nB) The Taylor type formula for the call price is given by: $C(S,t) = S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + \\frac{\\nu}{2} \\cdot S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + O(\\nu^4(1+|\\rho|))$\n\nC) The Taylor type formula for the call price is given by: $C(S,t) = S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + \\frac{\\nu}{2} \\cdot S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + O(\\nu^6)$\n\nD) The Taylor type formula for the call price is given by: $C(S,t) = S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + \\frac{\\nu}{2} \\cdot S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + O(\\nu^3(\\left|\\rho\\right|+\\nu))$\n\n**Correct Answer:** D) The Taylor type formula for the call price is given by: $C(S,t) = S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + \\frac{\\nu}{2} \\cdot S \\cdot e^{-r(T-t)} \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) \\cdot \\left(1 - \\frac{\\nu^2}{2} \\cdot \\frac{\\partial^2 \\ln S}{\\partial \\sigma^2} \\right) + O(\\nu^3(\\left|\\rho\\right|+\\nu))$\n\nExplanation: The correct answer is D) because it matches the error estimate given in the original documentation, which is $O(\\nu^3(\\left|\\rho\\right|+\\nu))$. The other options do not match the error estimate, and therefore are incorrect."}, "45": {"documentation": {"title": "Bayesian optimisation of large-scale photonic reservoir computers", "source": "Piotr Antonik, Nicolas Marsal, Daniel Brunner, Damien Rontani", "docs_id": "2004.02535", "section": ["cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian optimisation of large-scale photonic reservoir computers. Introduction. Reservoir computing is a growing paradigm for simplified training of recurrent neural networks, with a high potential for hardware implementations. Numerous experiments in optics and electronics yield comparable performance to digital state-of-the-art algorithms. Many of the most recent works in the field focus on large-scale photonic systems, with tens of thousands of physical nodes and arbitrary interconnections. While this trend significantly expands the potential applications of photonic reservoir computing, it also complicates the optimisation of the high number of hyper-parameters of the system. Methods. In this work, we propose the use of Bayesian optimisation for efficient exploration of the hyper-parameter space in a minimum number of iteration. Results. We test this approach on a previously reported large-scale experimental system, compare it to the commonly used grid search, and report notable improvements in performance and the number of experimental iterations required to optimise the hyper-parameters. Conclusion. Bayesian optimisation thus has the potential to become the standard method for tuning the hyper-parameters in photonic reservoir computing."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary challenge in optimizing the hyper-parameters of large-scale photonic reservoir computers, and how does Bayesian optimization address this challenge?\n\n**A)** The primary challenge is the high computational complexity of the system, and Bayesian optimization addresses this by using a more efficient algorithm. However, this is not the main challenge.\n\n**B)** The primary challenge is the high number of physical nodes and arbitrary interconnections in the system, which complicates the optimization of hyper-parameters. Bayesian optimization addresses this by using a probabilistic approach to explore the hyper-parameter space.\n\n**C)** The primary challenge is the lack of computational resources, and Bayesian optimization addresses this by using a more efficient algorithm that requires fewer iterations. However, this is not the main challenge.\n\n**D)** The primary challenge is the high cost of experimental iterations, and Bayesian optimization addresses this by using a more efficient algorithm that requires fewer iterations. However, this is not the main challenge.\n\n**Correct Answer:** B) The primary challenge is the high number of physical nodes and arbitrary interconnections in the system, which complicates the optimization of hyper-parameters. Bayesian optimization addresses this by using a probabilistic approach to explore the hyper-parameter space.\n\n**Explanation:** The correct answer is B) because the text states that \"many of the most recent works in the field focus on large-scale photonic systems, with tens of thousands of physical nodes and arbitrary interconnections\" and that this trend \"significantly expands the potential applications of photonic reservoir computing, but also complicates the optimisation of the high number of hyper-parameters of the system\". Bayesian optimization is proposed as a solution to this challenge by using a probabilistic approach to explore the hyper-parameter space."}, "46": {"documentation": {"title": "Nonlinear wave dynamics near phase transition in\n  $\\mathcal{PT}$-symmetric localized potentials", "source": "Sean Nixon and Jianke Yang", "docs_id": "1506.04445", "section": ["nlin.PS", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear wave dynamics near phase transition in\n  $\\mathcal{PT}$-symmetric localized potentials. Nonlinear wave propagation in parity-time ($\\mathcal{PT}$) symmetric localized potentials is investigated analytically near a phase-transition point where a pair of real eigenvalues of the potential coalesce and bifurcate into the complex plane. Necessary conditions for phase transition to occur are derived based on a generalization of the Krein signature. Using multi-scale perturbation analysis, a reduced nonlinear ODE model is derived for the amplitude of localized solutions near phase transition. Above phase transition, this ODE model predicts a family of stable solitons not bifurcating from linear (infinitesimal) modes under a certain sign of nonlinearity. In addition, it predicts periodically-oscillating nonlinear modes away from solitons. Under the opposite sign of nonlinearity, it predicts unbounded growth of solutions. Below phase transition, solution dynamics is predicted as well. All analytical results are compared to direct computations of the full system and good agreement is observed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of nonlinear wave dynamics near a phase transition in $\\mathcal{PT}$-symmetric localized potentials, what is the necessary condition for a phase transition to occur, as derived based on a generalization of the Krein signature?\n\nA) The potential must be strictly convex.\nB) The potential must have a pair of real eigenvalues that coalesce and bifurcate into the complex plane.\nC) The nonlinearity must be strictly positive.\nD) The nonlinearity must be strictly negative.\n\nCorrect Answer: B) The potential must have a pair of real eigenvalues that coalesce and bifurcate into the complex plane.\n\nExplanation: The correct answer is B) because the documentation states that \"necessary conditions for phase transition to occur are derived based on a generalization of the Krein signature\" and that this generalization is related to the coalescence and bifurcation of real eigenvalues into the complex plane. The other options are incorrect because they do not accurately reflect the necessary condition for a phase transition to occur in this context. Option A is incorrect because convexity is not mentioned in the documentation as a necessary condition for a phase transition. Option C is incorrect because the documentation does not specify a sign of nonlinearity as a necessary condition for a phase transition. Option D is incorrect because the documentation does not specify a sign of nonlinearity as a necessary condition for a phase transition."}, "47": {"documentation": {"title": "VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for\n  Robot Swarms", "source": "Ting-Kuei Hu, Fernando Gama, Tianlong Chen, Zhangyang Wang, Alejandro\n  Ribeiro, Brian M. Sadler", "docs_id": "2002.02308", "section": ["eess.SY", "cs.CV", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for\n  Robot Swarms. Decentralized coordination of a robot swarm requires addressing the tension between local perceptions and actions, and the accomplishment of a global objective. In this work, we propose to learn decentralized controllers based on solely raw visual inputs. For the first time, that integrates the learning of two key components: communication and visual perception, in one end-to-end framework. More specifically, we consider that each robot has access to a visual perception of the immediate surroundings, and communication capabilities to transmit and receive messages from other neighboring robots. Our proposed learning framework combines a convolutional neural network (CNN) for each robot to extract messages from the visual inputs, and a graph neural network (GNN) over the entire swarm to transmit, receive and process these messages in order to decide on actions. The use of a GNN and locally-run CNNs results naturally in a decentralized controller. We jointly train the CNNs and the GNN so that each robot learns to extract messages from the images that are adequate for the team as a whole. Our experiments demonstrate the proposed architecture in the problem of drone flocking and show its promising performance and scalability, e.g., achieving successful decentralized flocking for large-sized swarms consisting of up to 75 drones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed learning framework in the paper \"VGAI: End-to-End Learning of Vision-Based Decentralized Controllers for Robot Swarms\"?\n\nA) It introduces a new type of graph neural network (GNN) specifically designed for decentralized control.\nB) It demonstrates the effectiveness of using convolutional neural networks (CNNs) for visual perception in decentralized control.\nC) It proposes a novel approach to integrate communication and visual perception in a single end-to-end framework, enabling decentralized coordination of robot swarms.\nD) It shows that decentralized control can be achieved through the use of reinforcement learning algorithms.\n\n**Correct Answer:** C) It proposes a novel approach to integrate communication and visual perception in a single end-to-end framework, enabling decentralized coordination of robot swarms.\n\n**Explanation:** The correct answer is C because the paper proposes a novel approach that integrates the learning of two key components: communication and visual perception, in one end-to-end framework. This is the primary contribution of the paper, as it enables decentralized coordination of robot swarms. The other options are incorrect because they either focus on specific components of the framework (A and B) or propose alternative approaches (D)."}, "48": {"documentation": {"title": "Chiral phase transition and thermal Hall effect in an anisotropic spin\n  model on the kagome lattice", "source": "F. A. G\\'omez Albarrac\\'in, H. D. Rosales and Pierre Pujol", "docs_id": "2009.03941", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chiral phase transition and thermal Hall effect in an anisotropic spin\n  model on the kagome lattice. We present a study of the thermal Hall effect in the extended Heisenberg model with $XXZ$ anisotropy in the kagome lattice. This model has the particularity that, in the classical case, and for a broad region in parameter space, an external magnetic field induces a chiral symmetry breaking: the ground state is a doubly degenerate $q=0$ order with either positive or negative net chirality. Here, we focus on the effect of this chiral phase transition in the thermal Hall conductivity using Linear-Spin-Waves theory. We explore the topology and calculate the Chern numbers of the magnonic bands, obtaining a variety of topological phase transitions. We also compute the magnonic effect to the critical temperature associated with the chiral phase transition ($T_c^{SW}$). Our main result is that, the thermal Hall conductivity, which is null for $T>T_c^{SW}$, becomes non-zero as a consequence of the spontaneous chiral symmetry breaking at low temperatures. Therefore, we present a simple model where it is possible to \"switch\" on/off the thermal transport properties introducing a magnetic field and heating or cooling the system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the extended Heisenberg model with $XXZ$ anisotropy on the kagome lattice, what is the relationship between the thermal Hall conductivity and the spontaneous chiral symmetry breaking induced by an external magnetic field?\n\n**A)** The thermal Hall conductivity is directly proportional to the strength of the external magnetic field.\n\n**B)** The thermal Hall conductivity is null for all temperatures, regardless of the presence of an external magnetic field.\n\n**C)** The thermal Hall conductivity becomes non-zero as a consequence of the spontaneous chiral symmetry breaking at low temperatures, and its value is determined by the critical temperature associated with the chiral phase transition ($T_c^{SW}$).\n\n**D)** The thermal Hall conductivity is only affected by the temperature of the system, and not by the presence of an external magnetic field.\n\n**Correct Answer:** C) The thermal Hall conductivity becomes non-zero as a consequence of the spontaneous chiral symmetry breaking at low temperatures, and its value is determined by the critical temperature associated with the chiral phase transition ($T_c^{SW}$).\n\n**Explanation:** The correct answer is based on the main result of the study, which states that the thermal Hall conductivity becomes non-zero as a consequence of the spontaneous chiral symmetry breaking at low temperatures. This is a consequence of the chiral phase transition induced by the external magnetic field, and the value of the thermal Hall conductivity is determined by the critical temperature associated with this phase transition ($T_c^{SW}$). The other options are incorrect because they do not accurately reflect the relationship between the thermal Hall conductivity and the spontaneous chiral symmetry breaking."}, "49": {"documentation": {"title": "Quantum Corrections to Fidelity Decay in Chaotic Systems", "source": "Boris Gutkin, Daniel Waltner, Martha Gutierrez, Jack Kuipers, Klaus\n  Richter", "docs_id": "0911.0823", "section": ["cond-mat.mes-hall", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum Corrections to Fidelity Decay in Chaotic Systems. By considering correlations between classical orbits we derive semiclassical expressions for the decay of the quantum fidelity amplitude for classically chaotic quantum systems, as well as for its squared modulus, the fidelity or Loschmidt echo. Our semiclassical results for the fidelity amplitude agree with random matrix theory (RMT) and supersymmetry predictions in the universal Fermi golden rule regime. The calculated quantum corrections can be viewed as arising from a static random perturbation acting on nearly self-retracing interfering paths, and hence will be suppressed for time-varying perturbations. Moreover, using trajectory-based methods we show a relation, recently obtained in RMT, between the fidelity amplitude and the cross-form factor for parametric level correlations. Beyond RMT, we compute Ehrenfest-time effects on the fidelity amplitude. Furthermore our semiclassical approach allows for a unified treatment of the fidelity, both in the Fermi golden rule and Lyapunov regimes, demonstrating that quantum corrections are suppressed in the latter."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Quantum Corrections to Fidelity Decay in Chaotic Systems, what is the relationship between the fidelity amplitude and the cross-form factor for parametric level correlations, as recently obtained in Random Matrix Theory (RMT)?\n\nA) The fidelity amplitude is directly proportional to the cross-form factor.\nB) The fidelity amplitude is inversely proportional to the cross-form factor.\nC) The fidelity amplitude is equal to the cross-form factor.\nD) The fidelity amplitude is zero when the cross-form factor is non-zero.\n\nCorrect Answer: B) The fidelity amplitude is inversely proportional to the cross-form factor.\n\nExplanation: The correct answer is B) The fidelity amplitude is inversely proportional to the cross-form factor. This relationship is a key finding in the paper, which demonstrates a connection between the fidelity amplitude and the cross-form factor for parametric level correlations. The other options are incorrect, as they do not accurately reflect the relationship described in the paper. \n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the text to arrive at the correct answer."}, "50": {"documentation": {"title": "Kernel Additive Principal Components", "source": "Xin Lu Tan, Andreas Buja, and Zongming Ma", "docs_id": "1511.06821", "section": ["stat.ME", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kernel Additive Principal Components. Additive principal components (APCs for short) are a nonlinear generalization of linear principal components. We focus on smallest APCs to describe additive nonlinear constraints that are approximately satisfied by the data. Thus APCs fit data with implicit equations that treat the variables symmetrically, as opposed to regression analyses which fit data with explicit equations that treat the data asymmetrically by singling out a response variable. We propose a regularized data-analytic procedure for APC estimation using kernel methods. In contrast to existing approaches to APCs that are based on regularization through subspace restriction, kernel methods achieve regularization through shrinkage and therefore grant distinctive flexibility in APC estimation by allowing the use of infinite-dimensional functions spaces for searching APC transformation while retaining computational feasibility. To connect population APCs and kernelized finite-sample APCs, we study kernelized population APCs and their associated eigenproblems, which eventually lead to the establishment of consistency of the estimated APCs. Lastly, we discuss an iterative algorithm for computing kernelized finite-sample APCs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using kernel methods for estimating additive principal components (APCs) in contrast to existing approaches that rely on regularization through subspace restriction?\n\n**A)** Kernel methods allow for the use of infinite-dimensional functions spaces, enabling the estimation of APCs with higher dimensionality.\n**B)** Kernel methods achieve regularization through shrinkage, granting flexibility in APC estimation and allowing for the use of infinite-dimensional functions spaces.\n**C)** Kernel methods are computationally more efficient than existing approaches, making them suitable for large-scale data analysis.\n**D)** Kernel methods are limited to linear APCs, whereas existing approaches can handle nonlinear APCs.\n\n**Correct Answer:** B) Kernel methods achieve regularization through shrinkage, granting flexibility in APC estimation and allowing for the use of infinite-dimensional functions spaces.\n\n**Explanation:** The correct answer, B, highlights the key benefit of using kernel methods for APC estimation. By achieving regularization through shrinkage, kernel methods provide flexibility in APC estimation, allowing for the use of infinite-dimensional functions spaces to search for APC transformations. This is in contrast to existing approaches that rely on regularization through subspace restriction, which can limit the flexibility of APC estimation."}, "51": {"documentation": {"title": "Physical Layer Security in a SISO Communication using Frequency-Domain\n  Time-Reversal OFDM Precoding and Artificial Noise Injection", "source": "Sidney Jonathan Golstein, Fran\\c{c}ois Rottenberg, Fran\\c{c}ois\n  Horlin, Philippe De Doncker, Julien Sarrazin", "docs_id": "2011.06840", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Physical Layer Security in a SISO Communication using Frequency-Domain\n  Time-Reversal OFDM Precoding and Artificial Noise Injection. A frequency domain (FD) time-reversal (TR) precoder is proposed to perform physical layer security (PLS) in single-input single-output (SISO) systems using orthogonal frequency-division multiplexing (OFDM) and artificial noise (AN) signal injection. The AN signal does not corrupt the data transmission to the legitimate receiver but degrades the decoding performance of the eavesdropper. This scheme guarantees the secrecy of a communication towards a legitimate user when the transmitter knows the instantaneous channel state information (CSI) of the legitimate link thanks to the channel reciprocity in time division duplex (TDD) systems, but does not know the instantaneous CSI of a potential eavesdropper. Three optimal decoding structures at the eavesdropper are considered in a fast fading (FF) environment depending on the handshake procedure between Alice and Bob. Closed-form approximations of the AN energy to inject in order to maximize the SR of the communication are derived. In addition, the required conditions at the legitimate receiver's end to guarantee a given SR are determined when Eve's signal-to-noise ratio (SNR) is infinite. Furthermore, a waterfilling power allocation strategy is presented to further enhance the secrecy of the scheme. Simulation results are presented to demonstrate the security performance of the proposed secure system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the proposed frequency domain (FD) time-reversal (TR) precoder for physical layer security (PLS) in SISO systems using OFDM and artificial noise (AN) signal injection, what is the primary purpose of injecting AN signal?\n\nA) To corrupt the data transmission to the legitimate receiver\nB) To degrade the decoding performance of the eavesdropper\nC) To enhance the secrecy of the communication by making it difficult for the eavesdropper to decode the signal\nD) To increase the signal-to-noise ratio (SNR) of the legitimate receiver\n\n**Correct Answer:** B) To degrade the decoding performance of the eavesdropper\n\n**Explanation:** The correct answer is B) To degrade the decoding performance of the eavesdropper. According to the documentation, the AN signal does not corrupt the data transmission to the legitimate receiver but degrades the decoding performance of the eavesdropper, thereby enhancing the secrecy of the communication.\n\n**Explanation of incorrect options:**\n\nA) To corrupt the data transmission to the legitimate receiver: This is incorrect because the AN signal does not corrupt the data transmission to the legitimate receiver.\n\nC) To enhance the secrecy of the communication by making it difficult for the eavesdropper to decode the signal: While this is a related concept, it is not the primary purpose of injecting AN signal.\n\nD) To increase the signal-to-noise ratio (SNR) of the legitimate receiver: This is incorrect because the AN signal is injected to degrade the decoding performance of the eavesdropper, not to improve the SNR of the legitimate receiver."}, "52": {"documentation": {"title": "Predicting the ancestral character changes in a tree is typically easier\n  than predicting the root state", "source": "Olivier Gascuel and Mike Steel", "docs_id": "1309.0926", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Predicting the ancestral character changes in a tree is typically easier\n  than predicting the root state. Predicting the ancestral sequences of a group of homologous sequences related by a phylogenetic tree has been the subject of many studies, and numerous methods have been proposed to this purpose. Theoretical results are available that show that when the mutation rate become too large, reconstructing the ancestral state at the tree root is no longer feasible. Here, we also study the reconstruction of the ancestral changes that occurred along the tree edges. We show that, depending on the tree and branch length distribution, reconstructing these changes (i.e. reconstructing the ancestral state of all internal nodes in the tree) may be easier or harder than reconstructing the ancestral root state. However, results from information theory indicate that for the standard Yule tree, the task of reconstructing internal node states remains feasible, even for very high substitution rates. Moreover, computer simulations demonstrate that for more complex trees and scenarios, this result still holds. For a large variety of counting, parsimony-based and likelihood-based methods, the predictive accuracy of a randomly selected internal node in the tree is indeed much higher than the accuracy of the same method when applied to the tree root. Moreover, parsimony- and likelihood-based methods appear to be remarkably robust to sampling bias and model mis-specification."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the main finding regarding the predictive accuracy of reconstructing ancestral states in a phylogenetic tree, and how does it compare to reconstructing the root state?\n\n**A)** The predictive accuracy of reconstructing ancestral states is always higher than reconstructing the root state, regardless of the tree topology or branch length distribution.\n\n**B)** For the standard Yule tree, reconstructing internal node states remains feasible even for very high substitution rates, but this result may not hold for more complex trees.\n\n**C)** Parsimony- and likelihood-based methods are more accurate at reconstructing internal node states than at reconstructing the root state, and are robust to sampling bias and model mis-specification.\n\n**D)** The predictive accuracy of reconstructing ancestral states is lower than reconstructing the root state, especially for complex trees and high substitution rates.\n\n**Correct Answer:** C) Parsimony- and likelihood-based methods are more accurate at reconstructing internal node states than at reconstructing the root state, and are robust to sampling bias and model mis-specification.\n\n**Explanation:** The correct answer is based on the text, which states that \"For a large variety of counting, parsimony-based and likelihood-based methods, the predictive accuracy of a randomly selected internal node in the tree is indeed much higher than the accuracy of the same method when applied to the tree root.\" Additionally, the text notes that these methods are \"remarkably robust to sampling bias and model mis-specification\", which supports the correct answer."}, "53": {"documentation": {"title": "Systematic-free inference of the cosmic matter density field from\n  SDSS3-BOSS data", "source": "Guilhem Lavaux, Jens Jasche, Florent Leclercq", "docs_id": "1909.06396", "section": ["astro-ph.CO", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Systematic-free inference of the cosmic matter density field from\n  SDSS3-BOSS data. We perform an analysis of the three-dimensional cosmic matter density field traced by galaxies of the SDSS-III/BOSS galaxy sample. The systematic-free nature of this analysis is confirmed by two elements: the successful cross-correlation with the gravitational lensing observations derived from Planck 2018 data and the absence of bias at scales $k \\simeq 10^{-3}-10^{-2}h$ Mpc$^{-1}$ in the a posteriori power spectrum of recovered initial conditions. Our analysis builds upon our algorithm for Bayesian Origin Reconstruction from Galaxies (BORG) and uses a physical model of cosmic structure formation to infer physically meaningful cosmic structures and their corresponding dynamics from deep galaxy observations. Our approach accounts for redshift-space distortions and light-cone effects inherent to deep observations. We also apply detailed corrections to account for known and unknown foreground contaminations, selection effects and galaxy biases. We obtain maps of residual, so far unexplained, systematic effects in the spectroscopic data of SDSS-III/BOSS. Our results show that unbiased and physically plausible models of the cosmic large scale structure can be obtained from present and next-generation galaxy surveys."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the Bayesian Origin Reconstruction from Galaxies (BORG) algorithm used in the analysis of the SDSS-III/BOSS galaxy sample, as described in the paper \"Systematic-free inference of the cosmic matter density field from SDSS3-BOSS data\"?\n\nA) It allows for the estimation of galaxy biases at scales $k \\simeq 10^{-3}-10^{-2}h$ Mpc$^{-1}$.\nB) It enables the recovery of physically meaningful cosmic structures and their corresponding dynamics from deep galaxy observations.\nC) It provides a model-independent approach to analyzing galaxy distributions.\nD) It is a Markov chain Monte Carlo (MCMC) algorithm specifically designed for cosmological parameter estimation.\n\nCorrect Answer: B) It enables the recovery of physically meaningful cosmic structures and their corresponding dynamics from deep galaxy observations.\n\nExplanation: The correct answer is B) because the paper states that the analysis \"builds upon our algorithm for Bayesian Origin Reconstruction from Galaxies (BORG) and uses a physical model of cosmic structure formation to infer physically meaningful cosmic structures and their corresponding dynamics from deep galaxy observations.\" This indicates that the primary advantage of the BORG algorithm is its ability to recover physically meaningful cosmic structures and their dynamics, which is a key aspect of the analysis described in the paper."}, "54": {"documentation": {"title": "Evolutionary Turing in the Context of Evolutionary Machines", "source": "Mark Burgin and Eugene Eberbach", "docs_id": "1304.3762", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolutionary Turing in the Context of Evolutionary Machines. One of the roots of evolutionary computation was the idea of Turing about unorganized machines. The goal of this work is the development of foundations for evolutionary computations, connecting Turing's ideas and the contemporary state of art in evolutionary computations. To achieve this goal, we develop a general approach to evolutionary processes in the computational context, building mathematical models of computational systems, functioning of which is based on evolutionary processes, and studying properties of such systems. Operations with evolutionary machines are described and it is explored when definite classes of evolutionary machines are closed with respect to basic operations with these machines. We also study such properties as linguistic and functional equivalence of evolutionary machines and their classes, as well as computational power of evolutionary machines and their classes, comparing of evolutionary machines to conventional automata, such as finite automata or Turing machines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the work \"Evolutionary Turing in the Context of Evolutionary Machines\", and what type of systems are being studied in this research?\n\nA) To develop a new type of Turing machine that uses evolutionary processes to solve complex problems, and to study the properties of such machines.\nB) To establish a connection between Turing's ideas on unorganized machines and the contemporary state of art in evolutionary computations, with a focus on studying the computational power of evolutionary machines.\nC) To design a new type of evolutionary algorithm that can be used to solve optimization problems, and to compare its performance to conventional optimization algorithms.\nD) To investigate the linguistic and functional equivalence of evolutionary machines and their classes, and to study the properties of such systems.\n\n**Correct Answer:** B) To establish a connection between Turing's ideas on unorganized machines and the contemporary state of art in evolutionary computations, with a focus on studying the computational power of evolutionary machines.\n\n**Explanation:** The correct answer is B) because the introduction states that the goal of this work is to develop foundations for evolutionary computations, connecting Turing's ideas and the contemporary state of art in evolutionary computations. The research focuses on studying the properties of evolutionary machines and their classes, including their computational power, and comparing them to conventional automata."}, "55": {"documentation": {"title": "Isolation Distributional Kernel: A New Tool for Point & Group Anomaly\n  Detection", "source": "Kai Ming Ting, Bi-Cun Xu, Takashi Washio and Zhi-Hua Zhou", "docs_id": "2009.12196", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Isolation Distributional Kernel: A New Tool for Point & Group Anomaly\n  Detection. We introduce Isolation Distributional Kernel as a new way to measure the similarity between two distributions. Existing approaches based on kernel mean embedding, which convert a point kernel to a distributional kernel, have two key issues: the point kernel employed has a feature map with intractable dimensionality; and it is {\\em data independent}. This paper shows that Isolation Distributional Kernel (IDK), which is based on a {\\em data dependent} point kernel, addresses both key issues. We demonstrate IDK's efficacy and efficiency as a new tool for kernel based anomaly detection for both point and group anomalies. Without explicit learning, using IDK alone outperforms existing kernel based point anomaly detector OCSVM and other kernel mean embedding methods that rely on Gaussian kernel. For group anomaly detection,we introduce an IDK based detector called IDK$^2$. It reformulates the problem of group anomaly detection in input space into the problem of point anomaly detection in Hilbert space, without the need for learning. IDK$^2$ runs orders of magnitude faster than group anomaly detector OCSMM.We reveal for the first time that an effective kernel based anomaly detector based on kernel mean embedding must employ a characteristic kernel which is data dependent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the two key issues that existing approaches based on kernel mean embedding face, and how does Isolation Distributional Kernel (IDK) address them?\n\n**A)** IDK addresses the issue of high-dimensional feature maps by using a Gaussian kernel, and the issue of data independence by using a data-dependent point kernel.\n\n**B)** IDK addresses the issue of high-dimensional feature maps by using a data-dependent point kernel, and the issue of data independence by using a Gaussian kernel.\n\n**C)** IDK addresses the issue of high-dimensional feature maps by using a Gaussian kernel, and the issue of data independence by using a data-dependent point kernel, which is also data-independent.\n\n**D)** IDK addresses the issue of high-dimensional feature maps by using a data-dependent point kernel, and the issue of data independence by using a Gaussian kernel, which is also data-dependent.\n\n**Correct Answer:** B) IDK addresses the issue of high-dimensional feature maps by using a data-dependent point kernel, and the issue of data independence by using a Gaussian kernel.\n\n**Explanation:** The correct answer is B) because the documentation states that existing approaches based on kernel mean embedding face two key issues: the point kernel employed has a feature map with intractable dimensionality, and it is data-independent. IDK addresses these issues by using a data-dependent point kernel, which reduces the dimensionality of the feature map, and by being data-dependent, which makes it data-independent. The correct answer is B) because it accurately reflects the two key issues and how IDK addresses them."}, "56": {"documentation": {"title": "A Quaternion Based Quantum Chemical ab initio Treatment of Coherent and\n  Non-Coherent Electron Transport in Molecules", "source": "Augusto C. L. Moreira and Celso P. de Melo", "docs_id": "1201.3487", "section": ["cond-mat.mes-hall", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Quaternion Based Quantum Chemical ab initio Treatment of Coherent and\n  Non-Coherent Electron Transport in Molecules. We present a quaternion inspired formalism specifically developed to evaluate the intensity of the electrical current that traverses a single molecule connected to two semi-infinite electrodes as the applied external bias is varied. The self-adjustment of the molecular levels is fully described at a density functional ab initio quantum chemical level. Use of a quaternion approach allows for an integrated treatment of both coherent (ballistic) and non-coherent (co-tunneling) contributions to the effective charge transport, where the latter involve the existence of transient charged states of the corresponding molecular species. An expression for the net current is calculated by using second-order perturbation theory to take into account all possible transitions between states localized at the two different electrodes that involve intermediary levels in the so-called \"extended molecule\" complex that comprises the system of interest attached to two small metallic clusters. We show that by a judicious choice of the relevant molecular parameters, the formalism can be extended to describe the electronic transport both in conjugated as in saturated molecules, where localized orbitals are more likely to be found. In this manner, the method can be applied to the full range of coupling regimes, not only to the weak or strong cases, but also in intermediate situations, where ballistic and co-tunneling processes may coexist."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a quaternion approach in the ab initio treatment of electron transport in molecules, as described in the given Arxiv documentation?\n\nA) It allows for a more accurate description of localized orbitals in conjugated molecules.\nB) It enables the treatment of both coherent and non-coherent electron transport in a single formalism.\nC) It provides a more efficient method for calculating the intensity of electrical current in molecules.\nD) It offers a more intuitive understanding of the role of transient charged states in co-tunneling processes.\n\nCorrect Answer: B) It enables the treatment of both coherent and non-coherent electron transport in a single formalism.\n\nExplanation: The correct answer is B) because the quaternion approach allows for an integrated treatment of both coherent (ballistic) and non-coherent (co-tunneling) contributions to the effective charge transport in a single formalism. This is a key advantage of the method, as it enables the description of complex electron transport phenomena in molecules that involve both ballistic and co-tunneling processes. The other options are incorrect because they do not accurately capture the primary advantage of the quaternion approach. Option A is incorrect because the quaternion approach is not specifically designed to describe localized orbitals in conjugated molecules. Option C is incorrect because while the method does provide an expression for the net current, this is not the primary advantage of the quaternion approach. Option D is incorrect because while the method does describe the role of transient charged states in co-tunneling processes, this is not the primary advantage of the quaternion approach."}, "57": {"documentation": {"title": "Multivariate Spatiotemporal Hawkes Processes and Network Reconstruction", "source": "Baichuan Yuan, Hao Li, Andrea L. Bertozzi, P. Jeffrey Brantingham, and\n  Mason A. Porter", "docs_id": "1811.06321", "section": ["cs.SI", "eess.SP", "nlin.AO", "physics.soc-ph", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multivariate Spatiotemporal Hawkes Processes and Network Reconstruction. There is often latent network structure in spatial and temporal data and the tools of network analysis can yield fascinating insights into such data. In this paper, we develop a nonparametric method for network reconstruction from spatiotemporal data sets using multivariate Hawkes processes. In contrast to prior work on network reconstruction with point-process models, which has often focused on exclusively temporal information, our approach uses both temporal and spatial information and does not assume a specific parametric form of network dynamics. This leads to an effective way of recovering an underlying network. We illustrate our approach using both synthetic networks and networks constructed from real-world data sets (a location-based social media network, a narrative of crime events, and violent gang crimes). Our results demonstrate that, in comparison to using only temporal data, our spatiotemporal approach yields improved network reconstruction, providing a basis for meaningful subsequent analysis --- such as community structure and motif analysis --- of the reconstructed networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a multivariate Hawkes process for network reconstruction from spatiotemporal data sets, as opposed to traditional point-process models that focus exclusively on temporal information?\n\nA) It allows for the incorporation of spatial information in a more explicit manner.\nB) It enables the estimation of network dynamics with a specific parametric form.\nC) It provides a more efficient method for recovering an underlying network structure.\nD) It offers a more interpretable model for community structure and motif analysis.\n\nCorrect Answer: A) It allows for the incorporation of spatial information in a more explicit manner.\n\nExplanation: The correct answer is A) because the paper highlights that the multivariate Hawkes process approach uses both temporal and spatial information, which is a key advantage over traditional point-process models that focus exclusively on temporal information. This allows for a more comprehensive understanding of the underlying network structure. The other options are incorrect because they either misrepresent the advantage of the multivariate Hawkes process (B and C) or are not directly related to the primary advantage (D)."}, "58": {"documentation": {"title": "A simultaneous center-of-mass correction of nucleon density and momentum\n  distributions in nuclei", "source": "A. Shebeko, P. Grygorov", "docs_id": "0704.2147", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A simultaneous center-of-mass correction of nucleon density and momentum\n  distributions in nuclei. The approach exposed in the recent paper (A. Shebeko, P. Papakonstantinou, E. Mavrommatis, Eur. Phys. J. A 27, 143 (2006)) has been applied in studying center-of-mass motion effects on the nucleon density and momentum distributions in nuclei. We are focused upon effects due to the center-of-mass and short-range nucleon correlations embedded in translationally invariant ground-state wavefunctions. The latter are constructed in the so-called fixed center-of-mass approximation, starting with a Slater determinant wave function modified by some correlator (e.g., after Jastrow or Villars). It is shown how one can simplify evaluation of the corresponding expectation values that determine the distributions. The analytic expressions derived here involve the own \"Tassie-Barker\" factors for each distribution. As an illustration, numerical calculations have been carried out for the nucleus ^{4}He with the Slater determinant to describe the nucleon (1s)^4 configuration composed of single-particle orbitals which differ from harmonic oscillator ones at small distances. Such orbitals simulate somewhat short-range repulsion between nucleons. Special attention is paid to a simultaneous shrinking of the center--of--mass corrected density and momentum distributions compared to the purely (1s)^4 shell nontranslationally invariant ones."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the fixed center-of-mass approximation, what is the role of the \"Tassie-Barker\" factors in the evaluation of expectation values for nucleon density and momentum distributions in nuclei?\n\nA) They represent the short-range nucleon correlations embedded in the translationally invariant ground-state wavefunctions.\nB) They are used to calculate the center-of-mass correction for the nucleon density and momentum distributions.\nC) They are a type of correlator used to modify the Slater determinant wave function.\nD) They are a measure of the harmonic oscillator-like single-particle orbitals at small distances.\n\nCorrect Answer: A) They represent the short-range nucleon correlations embedded in the translationally invariant ground-state wavefunctions.\n\nExplanation: The \"Tassie-Barker\" factors are mentioned in the documentation as being involved in the analytic expressions derived for the expectation values of the nucleon density and momentum distributions. The correct answer, A, indicates that these factors represent the short-range nucleon correlations embedded in the translationally invariant ground-state wavefunctions, which is a key concept in the paper. The other options are incorrect because they either describe a different aspect of the calculation (B and C) or a different concept (D)."}, "59": {"documentation": {"title": "On few aspects of the dynamics of granular matter", "source": "P. Evesque", "docs_id": "cond-mat/0507261", "section": ["cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On few aspects of the dynamics of granular matter. This paper, in French, describes a series of completely different behaviours of the mechanics of granular matter, which are obtained experimentally using periodic forcing at different amplitude, frequency and orientation. It starts with the problem of granular dissipative gas which has been investigated in micro-gravity ; it is found that such a gas exists only at very low density ; it is shown also that clustering occurs at larger density. Is this a phase transition ? The problem of dissipative Sinai billiard is then investigated briefly ; is it ergodic ? An experiment on propagation of acoustic wave is studied, with peculiar attention paid to scattering and diffusion that occurs when acoustic wavelength is comparable to the grain size. A third experiment demonstrates that bulk convection can be induced by slow (quasi static) horizontal forcing ; this flow is related to the quasi static rheology of granular matter, but looks rather like convection occurring under \"dynamic vibration\". A fourth experiment describes pure inertial effect, making the sand behaving as a perfect (non viscous) fluid ; in particular, it is shown that a static swell 9that does not propagate) is enforced at the interface between liquid and sand by strong periodic horizontal forcing . Pacs # : 5.40 ; 45.70 ; 62.20 ; 83.70.Fn"}, "answer": "Here is the difficult multi-form question:\n\n**Question:** Analyze the experimental results presented in the paper \"On few aspects of the dynamics of granular matter\" and discuss the implications for our understanding of the behavior of granular matter under periodic forcing.\n\n**A)** The paper suggests that the behavior of granular matter under periodic forcing is characterized by a transition from a dissipative gas to a clustered state at higher densities. This transition is likely to be a phase transition, as evidenced by the changes in behavior at the critical density. However, the paper does not provide sufficient evidence to confirm this hypothesis.\n\n**B)** The author's conclusion that the dissipative Sinai billiard is ergodic is supported by the experimental results, which show that the system exhibits a high degree of disorder and randomness. The fact that the system does not exhibit any signs of clustering or phase separation at low densities further supports this conclusion.\n\n**C)** The paper's findings on the propagation of acoustic waves in granular matter are consistent with the idea that the system exhibits a non-ergodic behavior, characterized by the presence of scattering and diffusion. The fact that the acoustic wavelength is comparable to the grain size suggests that the system is in a state of \"dynamic vibration\", which is consistent with the observed behavior.\n\n**D)** The author's conclusion that the bulk convection in granular matter is related to the quasi-static rheology of the material is supported by the experimental results, which show that the flow is driven by slow (quasi-static) horizontal forcing. The fact that the flow is similar to convection occurring under \"dynamic vibration\" suggests that the system is in a state of non-ergodic behavior.\n\n**Correct Answer:** C) The paper's findings on the propagation of acoustic waves in granular matter are consistent with the idea that the system exhibits a non-ergodic behavior, characterized by the presence of scattering and diffusion. The fact that the acoustic wavelength is comparable to the grain size suggests that the system is in a state of \"dynamic vibration\", which is consistent with the observed behavior.\n\n**Explanation:** The correct answer is C) because the paper's findings on the propagation of acoustic waves in granular matter are consistent with the idea that the system exhibits a non-ergodic behavior, characterized by the presence of scattering and diffusion. The fact that the acoustic wavelength is comparable to the grain size suggests that the system is in a state of \"dynamic vibration\", which is consistent with the observed behavior. This is supported by the paper's discussion of the quasi-static rheology of granular matter and the similarity between the observed flow and convection occurring under \"dynamic vibration\"."}}