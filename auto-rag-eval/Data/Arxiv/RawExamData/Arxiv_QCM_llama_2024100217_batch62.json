{"0": {"documentation": {"title": "Critical slowing down associated with critical transition and risk of\n  collapse in cryptocurrency", "source": "Chengyi Tu, Paolo DOdorico, Samir Suweis", "docs_id": "1806.08386", "section": ["q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Critical slowing down associated with critical transition and risk of\n  collapse in cryptocurrency. The year 2017 saw the rise and fall of the crypto-currency market, followed by high variability in the price of all crypto-currencies. In this work, we study the abrupt transition in crypto-currency residuals, which is associated with the critical transition (the phenomenon of critical slowing down) or the stochastic transition phenomena. We find that, regardless of the specific crypto-currency or rolling window size, the autocorrelation always fluctuates around a high value, while the standard deviation increases monotonically. Therefore, while the autocorrelation does not display signals of critical slowing down, the standard deviation can be used to anticipate critical or stochastic transitions. In particular, we have detected two sudden jumps in the standard deviation, in the second quarter of 2017 and at the beginning of 2018, which could have served as early warning signals of two majors price collapses that have happened in the following periods. We finally propose a mean-field phenomenological model for the price of crypto-currency to show how the use of the standard deviation of the residuals is a better leading indicator of the collapse in price than the time series' autocorrelation. Our findings represent a first step towards a better diagnostic of the risk of critical transition in the price and/or volume of crypto-currencies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on critical slowing down associated with critical transition and risk of collapse in cryptocurrency observe that the standard deviation of the residuals increases monotonically over time. However, the autocorrelation of the residuals does not display signals of critical slowing down. What can be inferred from this observation, and how does it relate to the authors' proposal for a mean-field phenomenological model for the price of cryptocurrency?\n\n**A)** The authors conclude that the standard deviation is a reliable indicator of critical transitions, while the autocorrelation is not. However, this conclusion is based on a flawed assumption that the autocorrelation is not relevant to the model.\n\n**B)** The authors propose that the standard deviation is a better leading indicator of collapse in price than the time series' autocorrelation, but this is because the autocorrelation is not sensitive enough to detect the subtle changes in the residuals.\n\n**C)** The authors' observation suggests that the standard deviation is a more reliable indicator of critical transitions than the autocorrelation, because the autocorrelation is subject to noise and fluctuations that mask the underlying trends.\n\n**D)** The authors' proposal for a mean-field phenomenological model is based on the assumption that the standard deviation is a more reliable indicator of critical transitions than the autocorrelation, but this assumption is not supported by the data.\n\n**Correct Answer:** C) The authors' observation suggests that the standard deviation is a more reliable indicator of critical transitions than the autocorrelation, because the autocorrelation is subject to noise and fluctuations that mask the underlying trends.\n\n**Explanation:** The correct answer is C because the authors' observation that the standard deviation increases monotonically, while the autocorrelation does not display signals of critical slowing down, suggests that the standard deviation is a more reliable indicator of critical transitions. This is because the autocorrelation is subject to noise and fluctuations that mask the underlying trends, making it less sensitive to changes in the residuals. In contrast, the standard deviation is a more robust measure of the variability of the residuals, and its increase over time suggests a more significant change in the underlying dynamics of the system."}, "1": {"documentation": {"title": "Metastability and low lying spectra in reversible Markov chains", "source": "A. Bovier (WIAS), M. Eckhoff (Univ. of Potsdam), V. Gayrard (EPFL), M.\n  Klein (Univ. of Potsdam)", "docs_id": "math/0007160", "section": ["math.PR", "cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Metastability and low lying spectra in reversible Markov chains. We study a large class of reversible Markov chains with discrete state space and transition matrix $P_N$. We define the notion of a set of {\\it metastable points} as a subset of the state space $\\G_N$ such that (i) this set is reached from any point $x\\in \\G_N$ without return to x with probability at least $b_N$, while (ii) for any two point x,y in the metastable set, the probability $T^{-1}_{x,y}$ to reach y from x without return to x is smaller than $a_N^{-1}\\ll b_N$. Under some additional non-degeneracy assumption, we show that in such a situation: \\item{(i)} To each metastable point corresponds a metastable state, whose mean exit time can be computed precisely. \\item{(ii)} To each metastable point corresponds one simple eigenvalue of $1-P_N$ which is essentially equal to the inverse mean exit time from this state. The corresponding eigenfunctions are close to the indicator function of the support of the metastable state. Moreover, these results imply very sharp uniform control of the deviation of the probability distribution of metastable exit times from the exponential distribution."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Consider a reversible Markov chain with discrete state space and transition matrix $P_N$. Suppose that the chain has a set of metastable points $\\G_N$ such that:\n\n(i) any point $x\\in \\G_N$ can be reached from any other point $y\\in \\G_N$ without return to $x$ with probability at least $b_N$;\n(ii) for any two points $x,y\\in \\G_N$, the probability $T^{-1}_{x,y}$ to reach $y$ from $x$ without return to $x$ is smaller than $a_N^{-1}\\ll b_N$.\n\nUnder the additional non-degeneracy assumption, what can be said about the mean exit time from each metastable state, and how does it relate to the eigenvalues of $1-P_N$?\n\n**A)** The mean exit time from each metastable state is equal to the inverse of the largest eigenvalue of $1-P_N$.\n\n**B)** The mean exit time from each metastable state is equal to the inverse of the smallest eigenvalue of $1-P_N$.\n\n**C)** The mean exit time from each metastable state is equal to the inverse of the sum of the eigenvalues of $1-P_N$.\n\n**D)** The mean exit time from each metastable state is equal to the inverse of the largest eigenvalue of $1-P_N$, and the corresponding eigenfunction is close to the indicator function of the support of the metastable state.\n\n**Correct Answer:** D) The mean exit time from each metastable state is equal to the inverse of the largest eigenvalue of $1-P_N$, and the corresponding eigenfunction is close to the indicator function of the support of the metastable state.\n\n**Explanation:** The correct answer follows from the results stated in the Arxiv documentation. Specifically, it is shown that to each metastable point corresponds a metastable state, and the mean exit time from this state can be computed precisely. Furthermore, it is shown that the corresponding eigenvalue of $1-P_N$ is essentially equal to the inverse mean exit time from this state, and the corresponding eigenfunction is close to the indicator function of the support of the metastable state."}, "2": {"documentation": {"title": "Replicated Vector Approximate Message Passing For Resampling Problem", "source": "Takashi Takahashi, Yoshiyuki Kabashima", "docs_id": "1905.09545", "section": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Replicated Vector Approximate Message Passing For Resampling Problem. Resampling techniques are widely used in statistical inference and ensemble learning, in which estimators' statistical properties are essential. However, existing methods are computationally demanding, because repetitions of estimation/learning via numerical optimization/integral for each resampled data are required. In this study, we introduce a computationally efficient method to resolve such problem: replicated vector approximate message passing. This is based on a combination of the replica method of statistical physics and an accurate approximate inference algorithm, namely the vector approximate message passing of information theory. The method provides tractable densities without repeating estimation/learning, and the densities approximately offer an arbitrary degree of the estimators' moment in practical time. In the experiment, we apply the proposed method to the stability selection method, which is commonly used in variable selection problems. The numerical results show its fast convergence and high approximation accuracy for problems involving both synthetic and real-world datasets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of replicated vector approximate message passing, and how does it address the limitations of existing resampling techniques?\n\n**A)** Replicated vector approximate message passing is designed to improve the accuracy of estimators in ensemble learning, but it does not address the computational efficiency issue. Instead, it focuses on developing more complex inference algorithms.\n\n**B)** The primary motivation is to reduce the computational cost of resampling techniques, which are widely used in statistical inference and ensemble learning. Replicated vector approximate message passing achieves this by combining the replica method of statistical physics with an accurate approximate inference algorithm.\n\n**C)** The method is primarily used for variable selection problems, and its primary goal is to improve the stability of the stability selection method. While it does provide tractable densities, this is a secondary benefit.\n\n**D)** Replicated vector approximate message passing is designed to provide an arbitrary degree of approximation accuracy for estimators, but it does not address the computational efficiency issue. Instead, it focuses on developing more complex inference algorithms.\n\n**Correct Answer:** B) The primary motivation is to reduce the computational cost of resampling techniques, which are widely used in statistical inference and ensemble learning. Replicated vector approximate message passing achieves this by combining the replica method of statistical physics with an accurate approximate inference algorithm.\n\n**Explanation:** The correct answer is B) because the question asks about the primary motivation behind the development of replicated vector approximate message passing. The documentation states that existing methods are computationally demanding, and the proposed method is designed to resolve this problem. The correct answer highlights the main advantage of the method, which is its ability to reduce computational cost. The other options are incorrect because they either misrepresent the primary motivation (A and D) or focus on secondary benefits (C)."}, "3": {"documentation": {"title": "Analysis of Charge-spin-orbital Fluctuations by Ab Initio Calculation\n  and Random Phase Approximation: Application to Non-coplanar Antiferromagnet\n  Cd2Os2O7", "source": "Amane Uehara, Hiroshi Shinaoka, Yukitoshi Motome", "docs_id": "1506.06231", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of Charge-spin-orbital Fluctuations by Ab Initio Calculation\n  and Random Phase Approximation: Application to Non-coplanar Antiferromagnet\n  Cd2Os2O7. We present a systematic analysis on the basis of ab initio calculations and many-body perturbation theory for clarifying the dominant fluctuation in complex charge-spin-orbital coupled systems. For a tight-binding multiband model obtained from the maximally-localized Wannier function analysis of the band structure by the local density approximation, we take into account electron correlations at the level of random phase approximation. To identify the dominant fluctuation, we carry out the eigenmode analysis of the generalized susceptibility that includes all the multiple degrees of freedom: charge, spin, and orbital. We apply this method to the paramagnetic metallic phase of a pyrochlore oxide Cd$_2$Os$_2$O$_7$, which shows a metal-insulator transition accompanied by a peculiar noncoplanar antiferromagnetic order of all-in all-out type. We find that the corresponding spin fluctuation is dominantly enhanced by the on-site Coulomb repulsions in the presence of strong spin-orbit coupling and trigonal crystal field splitting. Our results indicate that the combined method offers an effective tool for the systematic analysis of potential instabilities in strongly correlated electron materials."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary factor that enhances the spin fluctuation in the paramagnetic metallic phase of Cd2Os2O7, according to the analysis presented in the paper?\n\nA) The presence of strong spin-orbit coupling and trigonal crystal field splitting\nB) The electron correlations at the level of random phase approximation\nC) The maximally-localized Wannier function analysis of the band structure\nD) The local density approximation\n\nCorrect Answer: A) The presence of strong spin-orbit coupling and trigonal crystal field splitting\n\nExplanation: The paper states that the spin fluctuation is \"dominantly enhanced by the on-site Coulomb repulsions in the presence of strong spin-orbit coupling and trigonal crystal field splitting.\" This indicates that the correct answer is A, as it specifically mentions the presence of strong spin-orbit coupling and trigonal crystal field splitting as the primary factor enhancing the spin fluctuation. The other options are incorrect because they do not specifically mention the enhancement of spin fluctuation, but rather are related to the methodological approach or the model used in the analysis."}, "4": {"documentation": {"title": "Dynamical Evidence For a Fifth Force Explanation of the ATOMKI Nuclear\n  Anomalies", "source": "Jonathan L. Feng, Tim M. P. Tait, Christopher B. Verhaaren", "docs_id": "2006.01151", "section": ["hep-ph", "hep-ex", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Evidence For a Fifth Force Explanation of the ATOMKI Nuclear\n  Anomalies. Recent anomalies in $^8$Be and $^4$He nuclear decays can be explained by postulating a fifth force mediated by a new boson $X$. The distributions of both transitions are consistent with the same $X$ mass, 17 MeV, providing kinematic evidence for a single new particle explanation. In this work, we examine whether the new results also provide dynamical evidence for a new particle explanation, that is, whether the observed decay rates of both anomalies can be described by a single hypothesis for the $X$ boson's interactions. We consider the observed $^8$Be and $^4$He excited nuclei, as well as a $^{12}$C excited nucleus; together these span the possible $J^P$ quantum numbers up to spin 1. For each transition, we determine whether scalar, pseudoscalar, vector, or axial vector $X$ particles can mediate the decay, and we construct the leading operators in a nuclear physics effective field theory that describes them. Assuming parity conservation, the scalar case is excluded and the pseudoscalar case is highly disfavored. Remarkably, however, the protophobic vector gauge boson, first proposed to explain only the $^8$Be anomaly, also explains the $^4$He anomaly within experimental uncertainties. We predict signal rates for other closely related nuclear measurements, which, if confirmed, will provide overwhelming evidence that a fifth force has been discovered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat type of boson is most likely to mediate the decay of $^8$Be and $^4$He nuclear decays, and what is the predicted signal rate for other closely related nuclear measurements?\n\nA) Pseudoscalar boson with a mass of 17 MeV\nB) Protophobic vector gauge boson with a mass of 17 MeV\nC) Axial vector boson with a mass of 17 MeV\nD) Scalar boson with a mass of 17 MeV\n\nCorrect Answer: B) Protophobic vector gauge boson with a mass of 17 MeV\n\nExplanation: According to the text, the protophobic vector gauge boson is the most likely candidate to mediate the decay of $^8$Be and $^4$He nuclear decays. This is because it is the only case that explains both anomalies within experimental uncertainties. Additionally, the text states that the predicted signal rates for other closely related nuclear measurements will provide overwhelming evidence that a fifth force has been discovered, which is consistent with the protophobic vector gauge boson explanation."}, "5": {"documentation": {"title": "Resonance spectra and a periodic orbit sum rule for bound chaotic\n  systems", "source": "Per Dahlqvist", "docs_id": "chao-dyn/9308005", "section": ["nlin.CD", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resonance spectra and a periodic orbit sum rule for bound chaotic\n  systems. We consider the spectrum of the evolution operator for bound chaotic systems by evaluating its trace. This trace is known to approach unity as $t \\rightarrow \\infty$ for bound systems. It is written as the Fourier transform of the logaritmic derivative of a zeta function whose zeros are identified with the eigenvalues of the operator. Zeta functions are expected to be entire only for very specific systems, like Axiom-A systems. For bound chaotic systems complications arise due to e.g. intermittency and non completeness of the symbolic dynamics. For bound intermittent system an approximation of the zeta function is derived. It is argued that bound systems with {\\em long time tails} have branch cuts in the zeta function and traces approaching unity as a powerlaw. Another feature of bound chaotic systems, relevant for the asymptotics of the trace, is that the dominant time scale can be much longer than the period of the shortest periodic orbit. This the case for e.g. the hyperbola billiard. Isolated zeros of the zeta function for the hyperbola billiard are evaluated by means of a cycle expansion. Crucial for the success of this approach is the identification of a sequence of periodic orbit responsible for a logarithmic branch cut in the zeta function. Semiclassical implications are discussed at the end."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a bound chaotic system with a long time tail, and evaluate the asymptotic behavior of its evolution operator's trace as $t \\rightarrow \\infty$. What is the expected form of the power-law decay of the trace, and what is the significance of the dominant time scale being much longer than the period of the shortest periodic orbit?\n\nA) The trace decays as $t^{-1/2}$, and the dominant time scale is equal to the period of the shortest periodic orbit.\nB) The trace decays as $t^{-3/4}$, and the dominant time scale is equal to the period of the shortest periodic orbit.\nC) The trace decays as $t^{-1/2}$, and the dominant time scale is much longer than the period of the shortest periodic orbit, with a power-law decay exponent of $1/2$.\nD) The trace decays as $t^{-3/4}$, and the dominant time scale is equal to the period of the shortest periodic orbit.\n\nCorrect Answer: C) The trace decays as $t^{-1/2}$, and the dominant time scale is much longer than the period of the shortest periodic orbit, with a power-law decay exponent of $1/2$.\n\nExplanation: The correct answer is based on the discussion in the documentation, which states that bound chaotic systems with long time tails have branch cuts in the zeta function and traces approaching unity as a power-law. The power-law decay exponent is expected to be $1/2$, and the dominant time scale is much longer than the period of the shortest periodic orbit. This is relevant to the hyperbola billiard, where isolated zeros of the zeta function are evaluated using a cycle expansion. The correct answer requires an understanding of the asymptotic behavior of the evolution operator's trace and the significance of the dominant time scale."}, "6": {"documentation": {"title": "Spectral narrowing and spin echo for localized carriers with\n  heavy-tailed Levy distribution of hopping times", "source": "Z. Yue, V. V. Mkhitaryan, and M. E. Raikh", "docs_id": "1602.00785", "section": ["cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral narrowing and spin echo for localized carriers with\n  heavy-tailed Levy distribution of hopping times. We study analytically the free induction decay and the spin echo decay originating from the localized carriers moving between the sites which host random magnetic fields. Due to disorder in the site positions and energies, the on-site residence times, \\tau, are widely spread according to the Levy distribution. The power-law tail \\propto \\tau^{-1-\\alpha} in the distribution of waiting times does not affect the conventional spectral narrowing for \\alpha >2, but leads to a dramatic acceleration of the free induction decay in the domain 2>\\alpha >1. The next abrupt acceleration of the decay takes place as the tail parameter, \\alpha, becomes smaller than 1. In the latter domain the decay does not follow a simple-exponent law. To capture the behavior of the average spin in this domain, we solve the evolution equation for the average spin using the approach different from the conventional approach based on the Laplace transform. Unlike the free induction decay, the tail in the distribution of the residence times leads to the slow decay of the spin echo. The echo is dominated by realizations of the carrier motion for which the number of sites, visited by the carrier, is minimal."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the effect of the heavy-tailed Levy distribution of hopping times on the free induction decay and spin echo decay in a system with localized carriers moving between sites hosting random magnetic fields. How does the power-law tail in the distribution of waiting times, specifically for 2 > \u03b1 > 1, influence the decay rates, and what is the underlying reason for the abrupt acceleration of the decay in this domain?\n\n**A)** The power-law tail in the distribution of waiting times leads to a simple-exponent law for the free induction decay, with no effect on the spin echo decay.\n\n**B)** The tail in the distribution of waiting times accelerates the free induction decay due to the increased mobility of carriers, but has no effect on the spin echo decay, which remains unaffected by the distribution of waiting times.\n\n**C)** The power-law tail in the distribution of waiting times leads to a dramatic acceleration of the free induction decay in the domain 2 > \u03b1 > 1, due to the increased number of carriers visiting sites with random magnetic fields, and also accelerates the spin echo decay, which is dominated by realizations of the carrier motion with minimal site visits.\n\n**D)** The tail in the distribution of waiting times leads to a slow decay of the spin echo due to the increased number of carriers visiting sites with random magnetic fields, but has no effect on the free induction decay, which remains unaffected by the distribution of waiting times.\n\n**Correct Answer:** C) The power-law tail in the distribution of waiting times leads to a dramatic acceleration of the free induction decay in the domain 2 > \u03b1 > 1, due to the increased number of carriers visiting sites with random magnetic fields, and also accelerates the spin echo decay, which is dominated by realizations of the carrier motion with minimal site visits.\n\n**Explanation:** The correct answer is C) because the power-law tail in the distribution of waiting times leads to a dramatic acceleration of the free induction decay in the domain 2 > \u03b1 > 1, due to the increased number of carriers visiting sites with random magnetic fields. Additionally, the tail in the distribution of waiting times also accelerates the spin echo decay, which is dominated by realizations of the carrier motion with minimal site visits. This is because the tail in the distribution of waiting times leads to a slow decay of the spin echo, and the echo is dominated by realizations of the carrier motion for which the number of sites visited by the carrier is minimal."}, "7": {"documentation": {"title": "Long range correlations and slow time scales in a boundary driven\n  granular model", "source": "Andrea Plati and Andrea Puglisi", "docs_id": "2101.09516", "section": ["cond-mat.stat-mech", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Long range correlations and slow time scales in a boundary driven\n  granular model. We consider a velocity field with linear viscous interactions defined on a one dimensional lattice. Brownian baths with different parameters can be coupled to the boundary sites and to the bulk sites, determining different kinds of non-equilibrium steady states or free-cooling dynamics. Analytical results for spatial and temporal correlations are provided by analytical diagonalisation of the system's equations in the infinite size limit. We demonstrate that spatial correlations are scale-free and time-scales become exceedingly long when the system is driven only at the boundaries. On the contrary, in the case a bath is coupled to the bulk sites too, an exponential correlation decay is found with a finite characteristic length. This is also true in the free cooling regime, but in this case the correlation length grows diffusively in time. We discuss the crucial role of boundary driving for long-range correlations and slow time-scales, proposing an analogy between this simplified dynamical model and dense vibro-fluidized granular materials. Several generalizations and connections with the statistical physics of active matter are also suggested."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the role of boundary driving in the formation of long-range correlations and slow time-scales in a boundary-driven granular model. How does the coupling of Brownian baths to boundary sites versus bulk sites affect the spatial and temporal correlations in the system?\n\n**A)** The coupling of Brownian baths to boundary sites leads to the formation of long-range correlations and slow time-scales, whereas the coupling to bulk sites results in exponential correlation decay with a finite characteristic length.\n\n**B)** The boundary driving is responsible for the formation of long-range correlations and slow time-scales, but the coupling of Brownian baths to bulk sites also plays a crucial role in determining the characteristic length scale of the correlations.\n\n**C)** The system exhibits scale-free spatial correlations and long time-scales when driven only at the boundaries, but the coupling of Brownian baths to bulk sites leads to a transition to exponential correlation decay with a finite characteristic length.\n\n**D)** The free-cooling regime exhibits long-range correlations and slow time-scales, but the coupling of Brownian baths to bulk sites leads to a transition to exponential correlation decay with a finite characteristic length, whereas the boundary driving is not a crucial factor in determining the characteristic length scale of the correlations.\n\n**Correct Answer:** B) The boundary driving is responsible for the formation of long-range correlations and slow time-scales, but the coupling of Brownian baths to bulk sites also plays a crucial role in determining the characteristic length scale of the correlations.\n\n**Explanation:** The correct answer is B) because the documentation states that the boundary driving is responsible for the formation of long-range correlations and slow time-scales, but the coupling of Brownian baths to bulk sites also plays a crucial role in determining the characteristic length scale of the correlations. The coupling to bulk sites leads to exponential correlation decay with a finite characteristic length, whereas the coupling to boundary sites leads to scale-free spatial correlations and long time-scales. The free-cooling regime also exhibits long-range correlations and slow time-scales, but the coupling to bulk sites leads to a transition to exponential correlation decay with a finite characteristic length."}, "8": {"documentation": {"title": "Normal edge-colorings of cubic graphs", "source": "Giuseppe Mazzuoccolo, Vahan Mkrtchyan", "docs_id": "1804.09449", "section": ["cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Normal edge-colorings of cubic graphs. A normal $k$-edge-coloring of a cubic graph is an edge-coloring with $k$ colors having the additional property that when looking at the set of colors assigned to any edge $e$ and the four edges adjacent it, we have either exactly five distinct colors or exactly three distinct colors. We denote by $\\chi'_{N}(G)$ the smallest $k$, for which $G$ admits a normal $k$-edge-coloring. Normal $k$-edge-colorings were introduced by Jaeger in order to study his well-known Petersen Coloring Conjecture. More precisely, it is known that proving $\\chi'_{N}(G)\\leq 5$ for every bridgeless cubic graph is equivalent to proving Petersen Coloring Conjecture and then, among others, Cycle Double Cover Conjecture and Berge-Fulkerson Conjecture. Considering the larger class of all simple cubic graphs (not necessarily bridgeless), some interesting questions naturally arise. For instance, there exist simple cubic graphs, not bridgeless, with $\\chi'_{N}(G)=7$. On the other hand, the known best general upper bound for $\\chi'_{N}(G)$ was $9$. Here, we improve it by proving that $\\chi'_{N}(G)\\leq7$ for any simple cubic graph $G$, which is best possible. We obtain this result by proving the existence of specific no-where zero $\\mathbb{Z}_2^2$-flows in $4$-edge-connected graphs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the smallest number of colors required for a normal edge-coloring of a simple cubic graph, and what is the significance of this number in the context of graph coloring conjectures?\n\nA) 5, which is the best known general upper bound for the number of colors required for a normal edge-coloring of a bridgeless cubic graph.\n\nB) 7, which is the smallest number of colors required for a normal edge-coloring of any simple cubic graph, and is equivalent to proving the Cycle Double Cover Conjecture.\n\nC) 9, which is the best known general upper bound for the number of colors required for a normal edge-coloring of any simple cubic graph.\n\nD) 6, which is the smallest number of colors required for a normal edge-coloring of any simple cubic graph, and is equivalent to proving the Berge-Fulkerson Conjecture.\n\nCorrect Answer: B) 7, which is the smallest number of colors required for a normal edge-coloring of any simple cubic graph, and is equivalent to proving the Cycle Double Cover Conjecture.\n\nExplanation: The correct answer is B) 7, because the documentation states that the authors prove that \u03c7'_{N}(G) \u2264 7 for any simple cubic graph G, which is the smallest number of colors required for a normal edge-coloring of any simple cubic graph. Additionally, the documentation mentions that this result is equivalent to proving the Cycle Double Cover Conjecture, which is one of the graph coloring conjectures mentioned in the text."}, "9": {"documentation": {"title": "Individual dipole toroidal states: main features and search in (e,e')\n  reaction", "source": "V.O. Nesterenko, A. Repko, J. Kvasil, and P.-G. Reinhard", "docs_id": "1904.08302", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Individual dipole toroidal states: main features and search in (e,e')\n  reaction. Individual low-energy E1 toroidal and compressional states (TS and CS) produced by the convective nuclear current ${\\bf j}_{\\rm c}$ were recently predicted for $^{24}$Mg in the framework of quasiparticle random-phase-approximation (QRPA) with Skyrme forces. In the present QRPA study with Skyrme parametrization SLy6, we explore in more detail properties of these states (toroidal and compressional responses, current distributions, and transitions probabilities $B(E1K, 0^+0 \\to 1^-K),\\; B(E3K, 0^+0 \\to 3^-K)$, $B(M2K, 0^+0 \\to 2^-K)$ with $K=$0 and 1) and analyze the possibility to discriminate and identify TS in inelastic electron scattering to back angles. The interplay of the convective ${\\bf j}_{\\rm c}$ and magnetization ${\\bf j}_{\\rm m}$ nuclear currents is thoroughly scrutinized. A two-step scheme for identification of TS in $(e,e')$ reaction is proposed. The key element of the scheme is the strong interference of the orbital and spin contributions, resulting in specific features of E1 and M2 transversal form factors."}, "answer": "Here is the exam question:\n\nWhat is the key element of the proposed two-step scheme for identification of toroidal states (TS) in the $(e,e')$ reaction, which involves a strong interference of the orbital and spin contributions?\n\nA) The use of Skyrme forces in the QRPA study\nB) The analysis of current distributions and transitions probabilities\nC) The strong interference of the orbital and spin contributions resulting in specific features of E1 and M2 transversal form factors\nD) The interplay of the convective ${\\bf j}_{\\rm c}$ and magnetization ${\\bf j}_{\\rm m}$ nuclear currents\n\nCorrect Answer: C) The strong interference of the orbital and spin contributions resulting in specific features of E1 and M2 transversal form factors\n\nExplanation: The correct answer is C) because the question specifically asks about the key element of the proposed scheme, which is the strong interference of the orbital and spin contributions. This is mentioned in the introduction of the document as the \"key element\" of the scheme. The other options are related to the topic but are not the key element of the scheme. Option A is a relevant background information, option B is a related aspect of the study, and option D is a related concept, but not the key element of the scheme."}, "10": {"documentation": {"title": "Lagrangian submanifolds of the complex hyperbolic quadric", "source": "Joeri Van der Veken and Anne Wijffels", "docs_id": "2002.10314", "section": ["math.DG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lagrangian submanifolds of the complex hyperbolic quadric. We consider the complex hyperbolic quadric ${Q^*}^n$ as a complex hypersurface of complex anti-de Sitter space. Shape operators of this submanifold give rise to a family of local almost product structures on ${Q^*}^n$, which are then used to define local angle functions on any Lagrangian submanifold of ${Q^*}^n$. We prove that a Lagrangian immersion into ${Q^*}^n$ can be seen as the Gauss map of a spacelike hypersurface of (real) anti-de Sitter space and relate the angle functions to the principal curvatures of this hypersurface. We also give a formula relating the mean curvature of the Lagrangian immersion to these principal curvatures. The theorems are illustrated with several examples of spacelike hypersurfaces of anti-de Sitter space and their Gauss maps. Finally, we classify some families of minimal Lagrangian submanifolds of ${Q^*}^n$: those with parallel second fundamental form and those for which the induced sectional curvature is constant. In both cases, the Lagrangian submanifold is forced to be totally geodesic."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a Lagrangian immersion $\\phi: M \\to Q^*^n$ into the complex hyperbolic quadric ${Q^*}^n$, where $M$ is a Riemannian manifold. Let $\\nabla$ be the Levi-Civita connection on $M$ and $\\tilde{\\nabla}$ be the connection on the complex anti-de Sitter space $\\tilde{M}$ induced by $\\nabla$. Suppose that the Lagrangian immersion $\\phi$ is the Gauss map of a spacelike hypersurface $\\Sigma$ of $\\tilde{M}$.\n\nA) The mean curvature $H$ of $\\phi(M)$ is related to the principal curvatures $K_i$ of $\\Sigma$ by the formula $H = \\frac{1}{n-1} \\sum_{i=1}^{n-1} K_i$.\n\nB) The sectional curvature $K$ of $\\phi(M)$ is constant and equal to the sectional curvature $K_\\Sigma$ of $\\Sigma$.\n\nC) The induced sectional curvature $K_\\phi$ of $\\phi(M)$ is equal to the sectional curvature $K_\\Sigma$ of $\\Sigma$ if and only if the Lagrangian immersion $\\phi$ is totally geodesic.\n\nD) The mean curvature $H$ of $\\phi(M)$ is related to the principal curvatures $K_i$ of $\\Sigma$ by the formula $H = \\frac{1}{n-1} \\sum_{i=1}^{n-1} K_i + \\frac{1}{n-1} K_n$, where $K_n$ is the principal curvature of $\\Sigma$ corresponding to the normal direction.\n\nCorrect Answer: D) The mean curvature $H$ of $\\phi(M)$ is related to the principal curvatures $K_i$ of $\\Sigma$ by the formula $H = \\frac{1}{n-1} \\sum_{i=1}^{n-1} K_i + \\frac{1}{n-1} K_n$, where $K_n$ is the principal curvature of $\\Sigma$ corresponding to the normal direction.\n\nExplanation: This question requires the application of the Gauss map theorem and the relationship between the mean curvature of a Lagrangian immersion and the principal curvatures of the underlying hypersurface. The correct answer follows from the formula given in the documentation, which relates the mean curvature of the Lagrangian immersion to the principal curvatures of the hypersurface. The other options are incorrect because they either omit the term corresponding to the normal direction or incorrectly relate the sectional curvature of the Lagrangian immersion to the sectional curvature of the hypersurface."}, "11": {"documentation": {"title": "Fast Converging Path Integrals for Time-Dependent Potentials II:\n  Generalization to Many-body Systems and Real-Time Formalism", "source": "Antun Balaz, Ivana Vidanovic, Aleksandar Bogojevic, Aleksandar Belic,\n  Axel Pelster", "docs_id": "1011.5185", "section": ["cond-mat.stat-mech", "cond-mat.other", "cond-mat.quant-gas", "hep-th", "physics.comp-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast Converging Path Integrals for Time-Dependent Potentials II:\n  Generalization to Many-body Systems and Real-Time Formalism. Based on a previously developed recursive approach for calculating the short-time expansion of the propagator for systems with time-independent potentials and its time-dependent generalization for simple single-particle systems, in this paper we present a full extension of this formalism to a general quantum system with many degrees of freedom in a time-dependent potential. Furthermore, we also present a recursive approach for the velocity-independent part of the effective potential, which is necessary for calculating diagonal amplitudes and partition functions, as well as an extension from the imaginary-time formalism to the real-time one, which enables to study the dynamical properties of quantum systems. The recursive approach developed here allows an analytic derivation of the short-time expansion to orders that have not been accessible before, using the implemented SPEEDUP symbolic calculation code. The analytically derived results are extensively numerically verified by treating several models in both imaginary and real time."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a quantum system with N degrees of freedom in a time-dependent potential, where the potential is given by V(r,t) = V0 + V1(t)cos(\u03c9t). Derive the short-time expansion of the propagator for this system using the recursive approach presented in the paper, and then use this expansion to calculate the velocity-independent part of the effective potential.\n\nA) Use the recursive approach to derive the short-time expansion of the propagator, and then use the SPEEDUP symbolic calculation code to obtain the velocity-independent part of the effective potential.\n\nB) Use the imaginary-time formalism to calculate the partition function and the velocity-independent part of the effective potential, and then use the real-time formalism to obtain the short-time expansion of the propagator.\n\nC) Use the recursive approach to derive the short-time expansion of the propagator, and then use the analytic results to calculate the velocity-independent part of the effective potential.\n\nD) Use the SPEEDUP symbolic calculation code to directly calculate the velocity-independent part of the effective potential, without deriving the short-time expansion of the propagator.\n\nCorrect Answer: C) Use the recursive approach to derive the short-time expansion of the propagator, and then use the analytic results to calculate the velocity-independent part of the effective potential.\n\nExplanation: The correct answer is C) because the recursive approach presented in the paper allows for the derivation of the short-time expansion of the propagator, which is then used to calculate the velocity-independent part of the effective potential. This approach is described in the paper as a way to extend the formalism to a general quantum system with many degrees of freedom in a time-dependent potential. The SPEEDUP symbolic calculation code is mentioned in the paper as a tool for obtaining the analytic results, but it is not necessary to use it to derive the short-time expansion of the propagator. Therefore, option C) is the correct answer."}, "12": {"documentation": {"title": "Inferring Economic Condition Uncertainty from Electricity Big Data", "source": "Zhengyu Shi, Libo Wu, Haoqi Qian and Yingjie Tian", "docs_id": "2107.11593", "section": ["econ.GN", "q-fin.EC", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inferring Economic Condition Uncertainty from Electricity Big Data. Inferring the uncertainties in economic conditions are of significant importance for both decision makers as well as market players. In this paper, we propose a novel method based on Hidden Markov Model (HMM) to construct the Economic Condition Uncertainty (ECU) index that can be used to infer the economic condition uncertainties. The ECU index is a dimensionless index ranges between zero and one, this makes it to be comparable among sectors, regions and periods. We use the daily electricity consumption data of nearly 20 thousand firms in Shanghai from 2018 to 2020 to construct the ECU indexes. Results show that all ECU indexes, no matter at sectoral level or regional level, successfully captured the negative impacts of COVID-19 on Shanghai's economic conditions. Besides, the ECU indexes also presented the heterogeneities in different districts as well as in different sectors. This reflects the facts that changes in uncertainties of economic conditions are mainly related to regional economic structures and targeted regulation policies faced by sectors. The ECU index can also be easily extended to measure uncertainties of economic conditions in different fields which has great potentials in the future."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Inferring Economic Condition Uncertainty from Electricity Big Data\" propose a novel method using Hidden Markov Model (HMM) to construct the Economic Condition Uncertainty (ECU) index. What is the primary advantage of using the ECU index, according to the authors?\n\nA) It allows for the comparison of economic conditions across different sectors and regions.\nB) It provides a dimensionless index that can be easily extended to measure uncertainties of economic conditions in different fields.\nC) It captures the negative impacts of external shocks, such as the COVID-19 pandemic, on economic conditions.\nD) It enables the identification of heterogeneities in economic conditions across different districts and sectors.\n\n**Correct Answer:** B) It provides a dimensionless index that can be easily extended to measure uncertainties of economic conditions in different fields.\n\n**Explanation:** The correct answer is B) because the ECU index is described as a \"dimensionless index ranges between zero and one\" that can be used to compare economic conditions across different sectors, regions, and periods. Additionally, the authors mention that the ECU index can be easily extended to measure uncertainties of economic conditions in different fields, which is a key advantage of the proposed method. The other options are incorrect because while the ECU index may capture the negative impacts of external shocks (C) and heterogeneities in economic conditions (D), these are not the primary advantages of the index. Option A is also incorrect because while the ECU index can be used to compare economic conditions, this is not the primary advantage of the index."}, "13": {"documentation": {"title": "Spontaneous Chiral Symmetry Breaking in a Random, Driven Chemical System", "source": "William D. Pi\\~neros and Tsvi Tlusty", "docs_id": "2108.02950", "section": ["physics.chem-ph", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spontaneous Chiral Symmetry Breaking in a Random, Driven Chemical System. Living systems have evolved to efficiently consume available energy sources using an elaborate circuitry of chemical reactions, which are puzzlingly restricted to specific chiral configurations. While autocatalysis is known to induce such chiral symmetry breaking, whether this might also arise in a more general class of non-autocatalytic chemical networks---by mere virtue of energy source exploitation---is a sensible yet underappreciated possibility. In this work, we examine this question within a model of randomly-generated complex chemical networks and show that chiral symmetry breaking may occur spontaneously and generically by harnessing energy sources from external environmental drives. Key to this transition are intrinsic fluctuations of achiral-to-chiral reactions and tight matching of system configurations to the environmental drive which, together, amplify and sustain diverged enantiomer distributions. The results thus demonstrate a generic mechanism in which energetic drives may give rise to homochirality in an otherwise totally symmetrical environment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which chiral symmetry breaking occurs in a randomly-driven chemical system, according to the study on Spontaneous Chiral Symmetry Breaking in a Random, Driven Chemical System?\n\nA) Through the introduction of autocatalytic reactions that amplify chiral configurations.\nB) By harnessing energy sources from external environmental drives, which amplify and sustain diverged enantiomer distributions.\nC) Due to the inherent asymmetry of the chemical reactions themselves, leading to a natural selection of chiral configurations.\nD) Through the formation of complex networks that self-organize into chiral configurations.\n\nCorrect Answer: B) By harnessing energy sources from external environmental drives, which amplify and sustain diverged enantiomer distributions.\n\nExplanation: The study suggests that the primary mechanism by which chiral symmetry breaking occurs is through the harnessing of energy sources from external environmental drives. This energy drives the system to favor the formation of chiral configurations, which are then amplified and sustained by the system's intrinsic fluctuations. This mechanism is key to understanding how chiral symmetry breaking can occur in a randomly-driven chemical system, even in the absence of autocatalysis."}, "14": {"documentation": {"title": "Inferring models of bacterial dynamics toward point sources", "source": "Hossein Jashnsaz, Tyler Nguyen, Horia I. Petrache, Steve Press\\'e", "docs_id": "1604.08916", "section": ["q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inferring models of bacterial dynamics toward point sources. Experiments have shown that bacteria can be sensitive to small variations in chemoattractant (CA) concentrations. Motivated by these findings, our focus here is on a regime rarely studied in experiments: bacteria tracking point CA sources (such as food patches or even prey). In tracking point sources, the CA detected by bacteria may show very large spatiotemporal fluctuations which vary with distance from the source. We present a general statistical model to describe how bacteria locate point sources of food on the basis of stochastic event detection, rather than CA gradient information. We show how all model parameters can be directly inferred from single cell tracking data even in the limit of high detection noise. Once parameterized, our model recapitulates bacterial behavior around point sources such as the \"volcano effect\". In addition, while the search by bacteria for point sources such as prey may appear random, our model identifies key statistical signatures of a targeted search for a point source given any arbitrary source configuration."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption underlying the proposed statistical model for bacterial dynamics in tracking point sources of chemoattractants, and how does it differ from previous models that rely on CA gradient information?\n\n**A)** The model assumes that bacteria detect CA concentrations at a fixed rate, regardless of distance from the source, and that this detection rate is the primary driver of bacterial movement.\n\n**B)** The model assumes that bacteria detect CA concentrations at a variable rate that is correlated with the distance from the source, and that this detection rate is the primary driver of bacterial movement.\n\n**C)** The model assumes that bacteria detect CA concentrations at a fixed rate, but that this detection rate is influenced by the bacteria's past experiences and learning mechanisms.\n\n**D)** The model assumes that bacteria detect CA concentrations at a variable rate that is correlated with the distance from the source, but that this detection rate is also influenced by the bacteria's past experiences and learning mechanisms.\n\n**Correct Answer:** B) The model assumes that bacteria detect CA concentrations at a variable rate that is correlated with the distance from the source, and that this detection rate is the primary driver of bacterial movement.\n\n**Explanation:** The correct answer is B) because the proposed model assumes that bacteria detect CA concentrations at a variable rate that is correlated with the distance from the source, which is a key aspect of the \"volcano effect\" observed in bacterial behavior around point sources. This assumption differs from previous models that rely on CA gradient information, which assume a fixed detection rate regardless of distance. The model's ability to recapitulate bacterial behavior around point sources and identify key statistical signatures of a targeted search for a point source, even in the presence of high detection noise, is a key strength of this approach."}, "15": {"documentation": {"title": "Ensemble Forecasting of Monthly Electricity Demand using Pattern\n  Similarity-based Methods", "source": "Pawe{\\l} Pe{\\l}ka, Grzegorz Dudek", "docs_id": "2004.00426", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ensemble Forecasting of Monthly Electricity Demand using Pattern\n  Similarity-based Methods. This work presents ensemble forecasting of monthly electricity demand using pattern similarity-based forecasting methods (PSFMs). PSFMs applied in this study include $k$-nearest neighbor model, fuzzy neighborhood model, kernel regression model, and general regression neural network. An integral part of PSFMs is a time series representation using patterns of time series sequences. Pattern representation ensures the input and output data unification through filtering a trend and equalizing variance. Two types of ensembles are created: heterogeneous and homogeneous. The former consists of different type base models, while the latter consists of a single-type base model. Five strategies are used for controlling a diversity of members in a homogeneous approach. The diversity is generated using different subsets of training data, different subsets of features, randomly disrupted input and output variables, and randomly disrupted model parameters. An empirical illustration applies the ensemble models as well as individual PSFMs for comparison to the monthly electricity demand forecasting for 35 European countries."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary purpose of using pattern similarity-based methods (PSFMs) in ensemble forecasting of monthly electricity demand, as presented in the study \"Ensemble Forecasting of Monthly Electricity Demand using Pattern Similarity-based Methods\"?\n\n**A)** To improve the accuracy of individual forecasting models by combining their predictions.\n**B)** To reduce the impact of noise and variability in time series data by identifying patterns and trends.\n**C)** To enhance the interpretability of forecasting models by providing a unified representation of input and output data.\n**D)** To increase the diversity of ensemble members by introducing different subsets of training data and features.\n\n**Correct Answer:** C) To enhance the interpretability of forecasting models by providing a unified representation of input and output data.\n\n**Explanation:** The study highlights the importance of pattern similarity-based methods in ensemble forecasting, which involves representing time series sequences using patterns. This representation ensures that the input and output data are unified, allowing for a more interpretable and comparable analysis of the forecasting models. By filtering out trends and equalizing variance, PSFMs provide a common ground for evaluating the performance of individual models and ensemble members."}, "16": {"documentation": {"title": "Backward Deep BSDE Methods and Applications to Nonlinear Problems", "source": "Yajie Yu, Bernhard Hientzsch, Narayan Ganesan", "docs_id": "2006.07635", "section": ["q-fin.CP", "q-fin.MF", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Backward Deep BSDE Methods and Applications to Nonlinear Problems. In this paper, we present a backward deep BSDE method applied to Forward Backward Stochastic Differential Equations (FBSDE) with given terminal condition at maturity that time-steps the BSDE backwards. We present an application of this method to a nonlinear pricing problem - the differential rates problem. To time-step the BSDE backward, one needs to solve a nonlinear problem. For the differential rates problem, we derive an exact solution of this time-step problem and a Taylor-based approximation. Previously backward deep BSDE methods only treated zero or linear generators. While a Taylor approach for nonlinear generators was previously mentioned, it had not been implemented or applied, while we apply our method to nonlinear generators and derive details and present results. Likewise, previously backward deep BSDE methods were presented for fixed initial risk factor values $X_0$ only, while we present a version with random $X_0$ and a version that learns portfolio values at intermediate times as well. The method is able to solve nonlinear FBSDE problems in high dimensions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a Forward Backward Stochastic Differential Equation (FBSDE) with a nonlinear generator, where the terminal condition is given at maturity. The FBSDE is time-stepped backwards using a backward deep BSDE method. What are the key features of the proposed method, and how does it differ from previously existing methods?\n\n**A)** The method only solves FBSDE problems with linear generators and fixed initial risk factor values. It does not provide a Taylor-based approximation for nonlinear generators.\n\n**B)** The method is only applicable to FBSDE problems with zero initial risk factor values and does not learn portfolio values at intermediate times.\n\n**C)** The method is able to solve nonlinear FBSDE problems in high dimensions, but it does not provide an exact solution for the time-step problem. It only provides a Taylor-based approximation.\n\n**D)** The method is able to solve nonlinear FBSDE problems in high dimensions, provides an exact solution for the time-step problem, and can learn portfolio values at intermediate times, including cases with random initial risk factor values.\n\n**Correct Answer:** D) The method is able to solve nonlinear FBSDE problems in high dimensions, provides an exact solution for the time-step problem, and can learn portfolio values at intermediate times, including cases with random initial risk factor values.\n\n**Explanation:** The correct answer is D) because the paper presents a backward deep BSDE method that can solve nonlinear FBSDE problems in high dimensions, provides an exact solution for the time-step problem, and can learn portfolio values at intermediate times, including cases with random initial risk factor values. The other options are incorrect because they do not accurately describe the key features of the proposed method."}, "17": {"documentation": {"title": "Rotating and non-rotating AdS black holes in $f({\\cal T})$ gravity\n  non-linear electrodynamics", "source": "Salvatore Capozziello and Gamal G.L. Nashed", "docs_id": "1908.07381", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rotating and non-rotating AdS black holes in $f({\\cal T})$ gravity\n  non-linear electrodynamics. We derive new exact charged $d$-dimensional black hole solutions for quadratic teleparallel equivalent gravity, $f({\\cal T})=a_0+a_1{\\cal T}+a_2{\\cal T}^2$, where $\\cal T$ is the torsion scalar, in the case of non-linear electrodynamics. We give a specific form of electromagnetic function and find out the form of the unknown functions that characterize the vielbeins in presence of the electromagnetic field. It is possible to show that the black holes behave asymptotically as AdS solutions and contain, in addition to the monopole and quadrupole terms, other higher order terms whose source is the non-linear electrodynamics field. We calculate the electromagnetic Maxwell field and show that our d-dimensional black hole solutions coincide with the previous obtained one \\cite{2017JHEP...07..136A}. The structure of the solutions show that there is a central singularity that is much mild in comparison with the respective one in General Relativity. Finally, the thermodynamical properties of the solutions are investigated by calculating the entropy, the Hawking temperature, the heat capacity, and other physical quantities. The most important result of thermodynamics is that the entropy is not proportional to the area of the black hole. This inanition points out that we must have a constrain on the quadrupole term to get a positive entropy otherwise we get a negative value."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of $f({\\cal T})$ gravity and non-linear electrodynamics, what is the constraint on the quadrupole term that is required to obtain a positive entropy for the rotating and non-rotating AdS black holes in $d$-dimensional spacetime?\n\nA) $a_2 = 0$\nB) $a_2 > 0$\nC) $a_2 < 0$\nD) $a_2 = \\frac{1}{2}$\n\nCorrect Answer: A) $a_2 = 0$\n\nExplanation: According to the documentation, the most important result of thermodynamics is that the entropy is not proportional to the area of the black hole. This implies that the quadrupole term must be constrained to be zero in order to obtain a positive entropy. This is because if the quadrupole term is not zero, the entropy would be negative, indicating a problem with the solution. Therefore, the correct answer is A) $a_2 = 0$."}, "18": {"documentation": {"title": "Topological based classification using graph convolutional networks", "source": "Roy Abel, Idan Benami, Yoram Louzoun", "docs_id": "1911.06892", "section": ["cs.SI", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological based classification using graph convolutional networks. In colored graphs, node classes are often associated with either their neighbors class or with information not incorporated in the graph associated with each node. We here propose that node classes are also associated with topological features of the nodes. We use this association to improve Graph machine learning in general and specifically, Graph Convolutional Networks (GCN). First, we show that even in the absence of any external information on nodes, a good accuracy can be obtained on the prediction of the node class using either topological features, or using the neighbors class as an input to a GCN. This accuracy is slightly less than the one that can be obtained using content based GCN. Secondly, we show that explicitly adding the topology as an input to the GCN does not improve the accuracy when combined with external information on nodes. However, adding an additional adjacency matrix with edges between distant nodes with similar topology to the GCN does significantly improve its accuracy, leading to results better than all state of the art methods in multiple datasets."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of Graph Convolutional Networks (GCNs), what is the primary advantage of incorporating topological features of nodes as an input to the GCN, as proposed in the paper \"Topological based classification using graph convolutional networks\"?\n\n**A)** The accuracy of node classification improves when topological features are used in addition to external information on nodes.\n**B)** The use of topological features does not improve the accuracy of node classification when combined with external information on nodes.\n**C)** The accuracy of node classification is slightly less than that of content-based GCNs when using topological features as an input.\n**D)** The use of topological features as an input to the GCN leads to results better than all state-of-the-art methods in multiple datasets.\n\n**Correct Answer:** D) The use of topological features as an input to the GCN leads to results better than all state-of-the-art methods in multiple datasets.\n\n**Explanation:** The correct answer is D) because the paper states that adding an additional adjacency matrix with edges between distant nodes with similar topology to the GCN \"significantly improves its accuracy, leading to results better than all state-of-the-art methods in multiple datasets.\" This suggests that incorporating topological features as an input to the GCN leads to improved performance, which is the primary advantage mentioned in the paper.\n\nThe other options are incorrect because:\n\n* A) is incorrect because the paper states that the accuracy is slightly less than that of content-based GCNs when using topological features as an input.\n* B) is incorrect because the paper states that explicitly adding the topology as an input to the GCN does not improve the accuracy when combined with external information on nodes.\n* C) is incorrect because the paper states that the accuracy is slightly less than that of content-based GCNs when using topological features as an input, but it does not say that the use of topological features leads to results better than all state-of-the-art methods."}, "19": {"documentation": {"title": "Comparison of numerical simulations of reactive transport and\n  chemostat-like models", "source": "Ihab Haidar (MISTEA), Fr\\'ed\\'eric G\\'erard (Eco\\&Sols), Alain\n  Rapaport (MISTEA, INRIA Sophia Antipolis)", "docs_id": "1108.1772", "section": ["math.DS", "physics.geo-ph", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Comparison of numerical simulations of reactive transport and\n  chemostat-like models. The objective of the paper is to evaluate the ability of reactive transport models and their numerical implementations (such as MIN3P) to simulate simple microbial transformations in conditions of chemostat or gradostat models, that are popular in microbial ecology and waste treatment ecosystems. To make this comparison, we first consider an abstract ecosystem composed of a single limiting resource and a single microbial species that are carried by advection. In a second stage, we consider another microbial species in competition for the same limiting resource. Comparing the numerical solutions of the two models, we found that the numerical accuracy of simulations of advective transport models performed with MIN3P depends on the evolution of the concentrations of the microbial species: when the state of the system is close to a non-hyperbolic equilibrium, we observe a numerical inaccuracy that may be due to the discretization method used in numerical approximations of reactive transport equations. Therefore, one has to be cautious about the predictions given by the models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the numerical accuracy of simulations of reactive transport models and chemostat-like models, considering the limitations of the discretization method used in numerical approximations of reactive transport equations.\n\n**A)** The numerical accuracy of simulations of reactive transport models is not affected by the discretization method used, as long as the model is well-posed and the numerical scheme is stable.\n\n**B)** The numerical accuracy of simulations of reactive transport models is highly dependent on the evolution of the concentrations of the microbial species, and may be compromised when the system is close to a non-hyperbolic equilibrium.\n\n**C)** The numerical accuracy of simulations of chemostat-like models is generally better than that of reactive transport models, due to the simpler mathematical structure of the chemostat model.\n\n**D)** The numerical accuracy of simulations of reactive transport models can be improved by using more advanced numerical schemes, such as finite element methods, which can better capture the complex dynamics of the system.\n\n**Correct Answer:** B) The numerical accuracy of simulations of reactive transport models is highly dependent on the evolution of the concentrations of the microbial species, and may be compromised when the system is close to a non-hyperbolic equilibrium.\n\n**Explanation:** The correct answer is based on the statement from the documentation that \"when the state of the system is close to a non-hyperbolic equilibrium, we observe a numerical inaccuracy that may be due to the discretization method used in numerical approximations of reactive transport equations.\" This suggests that the numerical accuracy of simulations of reactive transport models is highly dependent on the evolution of the concentrations of the microbial species, and may be compromised when the system is close to a non-hyperbolic equilibrium."}, "20": {"documentation": {"title": "How fine can fine-tuning be? Learning efficient language models", "source": "Evani Radiya-Dixit and Xin Wang", "docs_id": "2004.14129", "section": ["cs.CL", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How fine can fine-tuning be? Learning efficient language models. State-of-the-art performance on language understanding tasks is now achieved with increasingly large networks; the current record holder has billions of parameters. Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count. Does this mean that fine-tuning only introduces small differences from the pre-trained model in the parameter space? If so, can one avoid storing and computing an entire model for each task? In this work, we address these questions by using Bidirectional Encoder Representations from Transformers (BERT) as an example. As expected, we find that the fine-tuned models are close in parameter space to the pre-trained one, with the closeness varying from layer to layer. We show that it suffices to fine-tune only the most critical layers. Further, we find that there are surprisingly many good solutions in the set of sparsified versions of the pre-trained model. As a result, fine-tuning of huge language models can be achieved by simply setting a certain number of entries in certain layers of the pre-trained parameters to zero, saving both task-specific parameter storage and computational cost."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary finding of the study on fine-tuning language models, as demonstrated by the use of Bidirectional Encoder Representations from Transformers (BERT), regarding the relationship between fine-tuning and the parameter space of pre-trained models?\n\n**A)** Fine-tuning only introduces significant differences in the parameter space, requiring the storage and computation of an entire model for each task.\n\n**B)** Fine-tuning only introduces small differences in the parameter space, allowing for the storage and computation of a single model for multiple tasks.\n\n**C)** Fine-tuning does not affect the parameter space of pre-trained models, resulting in identical models for all tasks.\n\n**D)** Fine-tuning only affects the most critical layers of pre-trained models, while the remaining layers remain unchanged.\n\n**Correct Answer:** B) Fine-tuning only introduces small differences in the parameter space, allowing for the storage and computation of a single model for multiple tasks.\n\n**Explanation:** The study shows that fine-tuned models are close in parameter space to the pre-trained one, with the closeness varying from layer to layer. This suggests that fine-tuning only introduces small differences in the parameter space, allowing for the storage and computation of a single model for multiple tasks."}, "21": {"documentation": {"title": "Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle\n  Physics", "source": "Shun Zhou", "docs_id": "0706.0302", "section": ["hep-ph", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle\n  Physics. Following Georgi's unparticle scheme, we examine the effective couplings between neutrinos and unparticle operators. As an immediate consequence, neutrinos become unstable and can decay into the unparticle stuff. Assuming the dimension transmutation scale is around $\\Lambda^{}_{\\cal U} \\sim 1 ~{\\rm TeV}$, we implement the cosmological limit on the neutrino lifetime to constrain the neutrino-unparticle couplings for different scaling dimensions $d$. In addition, provided that the electron-unparticle coupling is restricted due to the precise measurement of the anomalous magnetic moment of electron, we calculate the unparticle contribution to the neutrino-electron elastic scattering. It is more important to jointly deal with the couplings of the unparticle to the standard model particles rather than separately. Taking into account both electron- and neutrino-unparticle couplings, we find that the scaling dimension of the scalar unparticle should lie in the narrow range $1 < d < 2$ by requiring the observables to be physically meaningful. However, there is no consistent range of $d$ for the vector unparticle operator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Georgi unparticle scheme, what is the range of scaling dimensions for the scalar unparticle operator that is required to ensure physically meaningful observables, considering both electron- and neutrino-unparticle couplings?\n\n**A)** $0 < d < 1$\n**B)** $1 < d < 2$\n**C)** $2 < d < 3$\n**D)** $3 < d < 4$\n\n**Correct Answer:** B) $1 < d < 2$\n\n**Explanation:** According to the documentation, the correct range of scaling dimensions for the scalar unparticle operator is $1 < d < 2$, as this range is required to ensure physically meaningful observables when considering both electron- and neutrino-unparticle couplings. This range is derived from the constraint on the neutrino lifetime and the precise measurement of the anomalous magnetic moment of the electron."}, "22": {"documentation": {"title": "Construction of Type-II Backlund Transformation for the mKdV Hierarchy", "source": "J.F. Gomes, A.L. Retore and A.H. Zimerman", "docs_id": "1505.01024", "section": ["nlin.SI", "hep-th", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Construction of Type-II Backlund Transformation for the mKdV Hierarchy. From an algebraic construction of the mKdV hierarchy we observe that the space component of the Lax operator play a role of an universal algebraic object. This fact induces the universality of a gauge transformation that relates two field configurations of a given member of the hierarchy. Such gauge transformation generates the Backlund transformation (BT). In this paper we propose a systematic construction of Backlund Transformation for the entire mKdV hierarchy form the known Type-II BT of the sinh-Gordon theory. We explicitly construct the BT of the first few integrable models associated to positive and negative grade-time evolutions. Solutions of these transformations for several cases describing the transition from vacuum-vacuum and the vacuum to one-soliton solutions which determines the value for the auxiliary field and the the Backlund parameter respectively, independently of the model. The same follows for the scattering of two one-soliton solutions. The resultant delay is determined by a condition independent of the model considered."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Construct the Backlund Transformation (BT) for the mKdV hierarchy using the Type-II BT of the sinh-Gordon theory, and explain the universality of the gauge transformation that relates two field configurations of a given member of the hierarchy.\n\n**A)** The BT for the mKdV hierarchy can be constructed by applying the Type-II BT of the sinh-Gordon theory to the space component of the Lax operator, resulting in a universal algebraic object that generates the BT.\n\n**B)** The BT for the mKdV hierarchy is a gauge transformation that relates two field configurations of a given member of the hierarchy, and its universality is induced by the algebraic construction of the mKdV hierarchy.\n\n**C)** The BT for the mKdV hierarchy can be explicitly constructed for the first few integrable models associated with positive and negative grade-time evolutions, and its solutions describe the transition from vacuum-vacuum and the vacuum to one-soliton solutions.\n\n**D)** The BT for the mKdV hierarchy generates the scattering of two one-soliton solutions, and the resultant delay is determined by a condition independent of the model considered.\n\n**Correct Answer:** B) The BT for the mKdV hierarchy is a gauge transformation that relates two field configurations of a given member of the hierarchy, and its universality is induced by the algebraic construction of the mKdV hierarchy.\n\n**Explanation:** The correct answer is B) because the question asks for the universality of the gauge transformation that relates two field configurations of a given member of the hierarchy, which is explicitly stated in the documentation as a consequence of the algebraic construction of the mKdV hierarchy. The other options are incorrect because they either describe a specific aspect of the BT (A and C) or a consequence of the BT (D), but do not address the universality of the gauge transformation."}, "23": {"documentation": {"title": "Resilient Active Information Acquisition with Teams of Robots", "source": "Brent Schlotfeldt, Vasileios Tzoumas, George J. Pappas", "docs_id": "2103.02733", "section": ["cs.RO", "cs.MA", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Resilient Active Information Acquisition with Teams of Robots. Emerging applications of collaborative autonomy, such as Multi-Target Tracking, Unknown Map Exploration, and Persistent Surveillance, require robots plan paths to navigate an environment while maximizing the information collected via on-board sensors. In this paper, we consider such information acquisition tasks but in adversarial environments, where attacks may temporarily disable the robots' sensors. We propose the first receding horizon algorithm, aiming for robust and adaptive multi-robot planning against any number of attacks, which we call Resilient Active Information acquisitioN (RAIN). RAIN calls, in an online fashion, a Robust Trajectory Planning (RTP) subroutine which plans attack-robust control inputs over a look-ahead planning horizon. We quantify RTP's performance by bounding its suboptimality. We base our theoretical analysis on notions of curvature introduced in combinatorial optimization. We evaluate RAIN in three information acquisition scenarios: Multi-Target Tracking, Occupancy Grid Mapping, and Persistent Surveillance. The scenarios are simulated in C++ and a Unity-based simulator. In all simulations, RAIN runs in real-time, and exhibits superior performance against a state-of-the-art baseline information acquisition algorithm, even in the presence of a high number of attacks. We also demonstrate RAIN's robustness and effectiveness against varying models of attacks (worst-case and random), as well as, varying replanning rates."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Resilient Active Information Acquisition (RAIN), what is the primary objective of the Robust Trajectory Planning (RTP) subroutine, and how does it contribute to the overall performance of the RAIN algorithm?\n\n**A)** The RTP subroutine aims to minimize the suboptimality of the control inputs over the planning horizon, while maximizing the information collected by the robots. This is achieved by iteratively replanning and adapting to the changing environment.\n\n**B)** The RTP subroutine is designed to optimize the robots' navigation paths to maximize the information collected via on-board sensors, while considering the potential attacks that may temporarily disable the robots' sensors.\n\n**C)** The RTP subroutine is a receding horizon algorithm that plans attack-robust control inputs over a look-ahead planning horizon, aiming to maximize the information collected by the robots while minimizing the suboptimality of the control inputs.\n\n**D)** The RTP subroutine is a heuristic-based approach that uses machine learning algorithms to predict the optimal control inputs for the robots, taking into account the uncertainty of the environment and the potential attacks.\n\n**Correct Answer:** C) The RTP subroutine is a receding horizon algorithm that plans attack-robust control inputs over a look-ahead planning horizon, aiming to maximize the information collected by the robots while minimizing the suboptimality of the control inputs.\n\n**Explanation:** The correct answer is C) because the RTP subroutine is indeed a receding horizon algorithm that plans attack-robust control inputs over a look-ahead planning horizon. This is a key aspect of the RAIN algorithm, which aims to maximize the information collected by the robots while minimizing the suboptimality of the control inputs. The other options are incorrect because they either misrepresent the objective of the RTP subroutine (A and B) or describe a different approach (D)."}, "24": {"documentation": {"title": "Emergence of Brain Rhythms: Model Interpretation of EEG Data", "source": "Javier A. Galad\\'i and Joaqu\\'in J. Torres and J. Marro", "docs_id": "1903.04576", "section": ["q-bio.NC", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergence of Brain Rhythms: Model Interpretation of EEG Data. Electroencephalography (EEG) monitors ---by either intrusive or noninvasive electrodes--- time and frequency variations and spectral content of voltage fluctuations or waves, known as brain rhythms, which in some way uncover activity during both rest periods and specific events in which the subject is under stimulus. This is a useful tool to explore brain behavior, as it complements imaging techniques that have a poorer temporal resolution. We here approach the understanding of EEG data from first principles by studying a networked model of excitatory and inhibitory neurons which generates a variety of comparable waves. In fact, we thus reproduce $\\alpha$, $\\beta,$ $\\gamma$ and other rhythms as observed by EEG, and identify the details of the respectively involved complex phenomena, including a precise relationship between an input and the collective response to it. It ensues the potentiality of our model to better understand actual mind mechanisms and its possible disorders, and we also describe kind of stochastic resonance phenomena which locate main qualitative changes of mental behavior in (e.g.) humans. We also discuss the plausible use of these findings to design deep learning algorithms to detect the occurence of phase transitions in the brain and to analyse its consequences."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Emergence of Brain Rhythms: Model Interpretation of EEG Data\" propose a networked model of excitatory and inhibitory neurons to understand EEG data. What is the primary advantage of using this model over imaging techniques, and how does it relate to the authors' goal of understanding brain behavior?\n\n**A)** The model's ability to generate a variety of comparable waves allows for a more detailed understanding of brain activity, which complements imaging techniques' poorer temporal resolution. This is in line with the authors' goal of understanding brain behavior.\n\n**B)** The model's use of stochastic resonance phenomena enables the detection of phase transitions in the brain, which is a key aspect of understanding mental behavior. This is a consequence of the authors' findings, but not the primary advantage of the model.\n\n**C)** The model's ability to reproduce EEG rhythms such as \u03b1, \u03b2, and \u03b3 waves allows for a more precise understanding of the complex phenomena involved. This is a key aspect of the authors' research, but not the primary advantage of the model.\n\n**D)** The model's use of noninvasive electrodes allows for a more comfortable and less invasive measurement of brain activity, which is a practical advantage. This is not mentioned in the provided documentation as a primary advantage of the model.\n\n**Correct Answer:** A) The model's ability to generate a variety of comparable waves allows for a more detailed understanding of brain activity, which complements imaging techniques' poorer temporal resolution. This is in line with the authors' goal of understanding brain behavior.\n\n**Explanation:** The correct answer is A) because the documentation states that the model \"complements imaging techniques that have a poorer temporal resolution\", which means that the model's ability to generate comparable waves allows for a more detailed understanding of brain activity. This is in line with the authors' goal of understanding brain behavior. The other options are incorrect because they either focus on a specific aspect of the model (B and C) or a practical advantage (D) that is not mentioned in the documentation."}, "25": {"documentation": {"title": "Cosmological vector modes and quantum gravity effects", "source": "Martin Bojowald and Golam Mortuza Hossain", "docs_id": "0709.0872", "section": ["gr-qc", "astro-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cosmological vector modes and quantum gravity effects. In contrast to scalar and tensor modes, vector modes of linear perturbations around an expanding Friedmann--Robertson--Walker universe decay. This makes them largely irrelevant for late time cosmology, assuming that all modes started out at a similar magnitude at some early stage. By now, however, bouncing models are frequently considered which exhibit a collapsing phase. Before this phase reaches a minimum size and re-expands, vector modes grow. Such modes are thus relevant for the bounce and may even signal the breakdown of perturbation theory if the growth is too strong. Here, a gauge invariant formulation of vector mode perturbations in Hamiltonian cosmology is presented. This lays out a framework for studying possible canonical quantum gravity effects, such as those of loop quantum gravity, at an effective level. As an explicit example, typical quantum corrections, namely those coming from inverse densitized triad components and holonomies, are shown to increase the growth rate of vector perturbations in the contracting phase, but only slightly. Effects at the bounce of the background geometry can, however, be much stronger."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of bouncing models of cosmology, what is the expected effect of quantum gravity corrections on the growth rate of vector perturbations during the contracting phase, and how does this compare to the effects at the bounce of the background geometry?\n\nA) The quantum corrections increase the growth rate of vector perturbations during the contracting phase, but have no effect at the bounce.\nB) The quantum corrections have no effect on the growth rate of vector perturbations during the contracting phase, but increase it significantly at the bounce.\nC) The quantum corrections increase the growth rate of vector perturbations during the contracting phase, but only slightly, and have a negligible effect at the bounce.\nD) The quantum corrections decrease the growth rate of vector perturbations during the contracting phase, but increase it significantly at the bounce.\n\nCorrect Answer: C) The quantum corrections increase the growth rate of vector perturbations during the contracting phase, but only slightly, and have a negligible effect at the bounce.\n\nExplanation: According to the provided documentation, the quantum corrections from inverse densitized triad components and holonomies increase the growth rate of vector perturbations in the contracting phase, but only slightly. In contrast, the effects at the bounce of the background geometry can be much stronger. Therefore, option C is the correct answer."}, "26": {"documentation": {"title": "Statistical model selection methods applied to biological networks", "source": "M.P.H. Stumpf, P.J. Ingram, I. Nouvel, C. Wiuf", "docs_id": "q-bio/0506013", "section": ["q-bio.MN", "q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical model selection methods applied to biological networks. Many biological networks have been labelled scale-free as their degree distribution can be approximately described by a powerlaw distribution. While the degree distribution does not summarize all aspects of a network it has often been suggested that its functional form contains important clues as to underlying evolutionary processes that have shaped the network. Generally determining the appropriate functional form for the degree distribution has been fitted in an ad-hoc fashion. Here we apply formal statistical model selection methods to determine which functional form best describes degree distributions of protein interaction and metabolic networks. We interpret the degree distribution as belonging to a class of probability models and determine which of these models provides the best description for the empirical data using maximum likelihood inference, composite likelihood methods, the Akaike information criterion and goodness-of-fit tests. The whole data is used in order to determine the parameter that best explains the data under a given model (e.g. scale-free or random graph). As we will show, present protein interaction and metabolic network data from different organisms suggests that simple scale-free models do not provide an adequate description of real network data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat statistical model selection methods are applied to determine the functional form of the degree distribution in biological networks, and what are the advantages of using these methods?\n\nA) Maximum likelihood inference, composite likelihood methods, and the Akaike information criterion are applied to determine the functional form of the degree distribution in biological networks. These methods are advantageous because they provide a comprehensive evaluation of the model's fit to the data, allowing for the selection of the best model.\n\nB) Maximum likelihood inference, composite likelihood methods, and goodness-of-fit tests are applied to determine the functional form of the degree distribution in biological networks. These methods are advantageous because they provide a robust evaluation of the model's fit to the data, allowing for the selection of the best model.\n\nC) The Akaike information criterion and goodness-of-fit tests are applied to determine the functional form of the degree distribution in biological networks. These methods are advantageous because they provide a simple and efficient evaluation of the model's fit to the data, allowing for the selection of the best model.\n\nD) Maximum likelihood inference and composite likelihood methods are applied to determine the functional form of the degree distribution in biological networks. These methods are advantageous because they provide a detailed evaluation of the model's fit to the data, allowing for the selection of the best model.\n\nCorrect Answer: A) Maximum likelihood inference, composite likelihood methods, and the Akaike information criterion are applied to determine the functional form of the degree distribution in biological networks. These methods are advantageous because they provide a comprehensive evaluation of the model's fit to the data, allowing for the selection of the best model.\n\nExplanation: The correct answer is A) because it accurately lists the three statistical model selection methods applied to determine the functional form of the degree distribution in biological networks. Maximum likelihood inference, composite likelihood methods, and the Akaike information criterion are all used to evaluate the fit of different models to the data, and the Akaike information criterion is particularly useful for comparing the relative merits of different models. The other options are incorrect because they either omit one or more of the methods or provide an incomplete explanation of their advantages."}, "27": {"documentation": {"title": "Transport of Ionizing Radiation in Terrestrial-like Exoplanet\n  Atmospheres", "source": "David S. Smith (1), John Scalo (1), and J. Craig Wheeler (1) ((1)\n  Dept. of Astronomy, Univ. of Texas at Austin)", "docs_id": "astro-ph/0308311", "section": ["astro-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transport of Ionizing Radiation in Terrestrial-like Exoplanet\n  Atmospheres. (Abridged) The propagation of ionizing radiation through model atmospheres of terrestrial-like exoplanets is studied for a large range of column densities and incident photon energies using a Monte Carlo code we have developed to treat Compton scattering and photoabsorption. Incident spectra from parent star flares, supernovae, and gamma-ray bursts are modeled and compared to energetic particles in importance. We find that terrestrial-like exoplanets with atmospheres thinner than about 100 g cm^-2 transmit and reprocess a significant fraction of incident gamma-rays, producing a characteristic, flat surficial spectrum. Thick atmospheres (>~ 100 g cm^-2) efficiently block even gamma-rays, but nearly all incident energy is redistributed into diffuse UV and visible aurora-like emission, increasing the effective atmospheric transmission by many orders of magnitude. Depending on the presence of molecular UV absorbers and atmospheric thickness, up to 10% of the incident energy can reach the surface as UV reemission. For the Earth, between 2 x 10^-3 and 4 x 10^-2 of the incident flux reaches the ground in the biologically effective 200--320 nm range, depending on O_2/O_3 shielding. Finally, we suggest that transient atmospheric ionization layers can be frequently created at low altitudes. We conclude that these events can produce frequent fluctuations in atmospheric ionization levels and surficial UV fluxes on terrestrial-like planets."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary effect of a terrestrial-like exoplanet's atmosphere on the transmission of incident gamma-rays, and how does it compare to the transmission of UV and visible radiation?\n\n**A)** The atmosphere efficiently blocks gamma-rays, but allows a significant fraction of UV and visible radiation to pass through, resulting in a characteristic flat surficial spectrum.\n\n**B)** The atmosphere transmits a significant fraction of gamma-rays, but blocks most of the UV and visible radiation, resulting in a diffuse emission spectrum.\n\n**C)** The atmosphere transmits and reprocesses a significant fraction of incident gamma-rays, producing a characteristic flat surficial spectrum, but blocks most of the UV and visible radiation.\n\n**D)** The atmosphere transmits and reprocesses a significant fraction of incident gamma-rays, but does not affect the transmission of UV and visible radiation, resulting in a diffuse emission spectrum.\n\n**Correct Answer:** C) The atmosphere transmits and reprocesses a significant fraction of incident gamma-rays, producing a characteristic flat surficial spectrum, but blocks most of the UV and visible radiation.\n\n**Explanation:** The correct answer is C) because the text states that terrestrial-like exoplanets with atmospheres thinner than about 100 g cm^-2 transmit and reprocess a significant fraction of incident gamma-rays, producing a characteristic flat surficial spectrum. Additionally, the text states that thick atmospheres (>~ 100 g cm^-2) efficiently block even gamma-rays, but nearly all incident energy is redistributed into diffuse UV and visible aurora-like emission. This indicates that the atmosphere transmits and reprocesses gamma-rays, but blocks most of the UV and visible radiation."}, "28": {"documentation": {"title": "Efficient and Fair Collaborative Mobile Internet Access", "source": "George Iosifidis and Lin Gao and Jianwei Huang and Leandros Tassiulas", "docs_id": "1612.05129", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient and Fair Collaborative Mobile Internet Access. The surging global mobile data traffic challenges the economic viability of cellular networks and calls for innovative solutions to reduce the network congestion and improve user experience. In this context, user-provided networks (UPNs), where mobile users share their Internet access by exploiting their diverse network resources and needs, turn out to be very promising. Heterogeneous users with advanced handheld devices can form connections in a distributed fashion and unleash dormant network resources at the network edge. However, the success of such services heavily depends on users' willingness to contribute their resources, such as network access and device battery energy. In this paper, we introduce a general framework for UPN services and design a bargaining-based distributed incentive mechanism to ensure users participation. The proposed mechanism determines the resources that each user should contribute in order to maximise the aggregate data rate in UPN, and fairly allocate the benefit among the users. The numerical results verify that the service can always improve performance, and such improvement increases with the diversity of the users' resources. Quantitatively, it can reach an average 30% increase of the total served traffic for a typical scenario even with only 6 mobile users."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of user-provided networks (UPNs), what is the primary challenge that the proposed bargaining-based distributed incentive mechanism aims to address, and how does it ensure fair allocation of benefits among users?\n\n**A)** The primary challenge is the lack of standardization in mobile devices, and the mechanism ensures fair allocation by assigning resources based on individual user needs.\n**B)** The primary challenge is the economic viability of cellular networks, and the mechanism ensures fair allocation by maximizing aggregate data rate and allocating benefits proportionally to each user's contribution.\n**C)** The primary challenge is the heterogeneity of users' network resources, and the mechanism ensures fair allocation by determining the optimal resource contribution for each user to maximize aggregate data rate.\n**D)** The primary challenge is the security of user-provided networks, and the mechanism ensures fair allocation by implementing a secure and transparent bargaining process.\n\n**Correct Answer:** C) The primary challenge is the heterogeneity of users' network resources, and the mechanism ensures fair allocation by determining the optimal resource contribution for each user to maximize aggregate data rate.\n\n**Explanation:** The correct answer is C) because the proposed bargaining-based distributed incentive mechanism aims to address the challenge of heterogeneity in users' network resources, which is a key aspect of UPNs. The mechanism determines the optimal resource contribution for each user to maximize aggregate data rate, ensuring fair allocation of benefits among users. The other options are incorrect because they either misrepresent the primary challenge or the mechanism's objective."}, "29": {"documentation": {"title": "A model of inter-organizational network formation", "source": "Shweta Gaonkar and Angelo Mele", "docs_id": "2105.00458", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A model of inter-organizational network formation. How do inter-organizational networks emerge? Accounting for interdependence among ties while studying tie formation is one of the key challenges in this area of research. We address this challenge using an equilibrium framework where firms' decisions to form links with other firms are modeled as a strategic game. In this game, firms weigh the costs and benefits of establishing a relationship with other firms and form ties if their net payoffs are positive. We characterize the equilibrium networks as exponential random graphs (ERGM), and we estimate the firms' payoffs using a Bayesian approach. To demonstrate the usefulness of our approach, we apply the framework to a co-investment network of venture capital firms in the medical device industry. The equilibrium framework allows researchers to draw economic interpretation from parameter estimates of the ERGM Model. We learn that firms rely on their joint partners (transitivity) and prefer to form ties with firms similar to themselves (homophily). These results hold after controlling for the interdependence among ties. Another, critical advantage of a structural approach is that it allows us to simulate the effects of economic shocks or policy counterfactuals. We test two such policy shocks, namely, firm entry and regulatory change. We show how new firms' entry or a regulatory shock of minimum capital requirements increase the co-investment network's density and clustering."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in studying inter-organizational network formation, and how does the proposed equilibrium framework address this challenge?\n\nA) The challenge is that firms have limited resources, and the framework addresses this by modeling firms' decisions as a strategic game. However, it does not account for interdependence among ties.\nB) The challenge is that firms have different goals and objectives, and the framework addresses this by modeling firms' decisions as a strategic game, where firms weigh the costs and benefits of establishing relationships with other firms.\nC) The challenge is that firms have limited information about other firms, and the framework addresses this by modeling firms' decisions as a strategic game, where firms form ties if their net payoffs are positive.\nD) The challenge is that firms have different levels of expertise, and the framework addresses this by modeling firms' decisions as a strategic game, where firms form ties with firms similar to themselves.\n\n**Correct Answer:** B) The challenge is that firms have different goals and objectives, and the framework addresses this by modeling firms' decisions as a strategic game, where firms weigh the costs and benefits of establishing relationships with other firms.\n\n**Explanation:** The correct answer is B because the documentation states that \"Accounting for interdependence among ties while studying tie formation is one of the key challenges in this area of research.\" The proposed equilibrium framework addresses this challenge by modeling firms' decisions as a strategic game, where firms weigh the costs and benefits of establishing relationships with other firms. This allows researchers to account for the interdependence among ties and draw economic interpretation from parameter estimates of the ERGM model."}, "30": {"documentation": {"title": "A continuum treatment of growth in biological tissue: The coupling of\n  mass transport and mechanics", "source": "K. Garikipati, E. M. Arruda, K. Grosh, H. Narayanan, S. Calve", "docs_id": "q-bio/0312001", "section": ["q-bio.QM", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A continuum treatment of growth in biological tissue: The coupling of\n  mass transport and mechanics. Growth (and resorption) of biological tissue is formulated in the continuum setting. The treatment is macroscopic, rather than cellular or sub-cellular. Certain assumptions that are central to classical continuum mechanics are revisited, the theory is reformulated, and consequences for balance laws and constitutive relations are deduced. The treatment incorporates multiple species. Sources and fluxes of mass, and terms for momentum and energy transfer between species are introduced to enhance the classical balance laws. The transported species include: (\\romannumeral 1) a fluid phase, and (\\romannumeral 2) the precursors and byproducts of the reactions that create and break down tissue. A notable feature is that the full extent of coupling between mass transport and mechanics emerges from the thermodynamics. Contributions to fluxes from the concentration gradient, chemical potential gradient, stress gradient, body force and inertia have not emerged in a unified fashion from previous formulations of the problem. The present work demonstrates these effects via a physically-consistent treatment. The presence of multiple, interacting species requires that the formulation be consistent with mixture theory. This requirement has far-reaching consequences. A preliminary numerical example is included to demonstrate some aspects of the coupled formulation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the continuum treatment of growth in biological tissue, what is the primary assumption that is revisited and reformulated, and what are the consequences of this reformulation for balance laws and constitutive relations?\n\nA) The Navier-Stokes equations are revised to account for the effects of chemical reactions on fluid flow.\nB) The conservation of mass is reformulated to include multiple species, with sources and fluxes of mass, and terms for momentum and energy transfer between species.\nC) The constitutive relations are revised to incorporate the effects of thermodynamics on the coupling between mass transport and mechanics.\nD) The balance laws are reformulated to include the effects of body forces and inertia on the growth of biological tissue.\n\nCorrect Answer: B) The conservation of mass is reformulated to include multiple species, with sources and fluxes of mass, and terms for momentum and energy transfer between species.\n\nExplanation: The correct answer is B) because the documentation states that the treatment incorporates multiple species, with sources and fluxes of mass, and terms for momentum and energy transfer between species. This is a key aspect of the reformulation of the conservation of mass, and is a consequence of revisiting and reformulating certain assumptions central to classical continuum mechanics. The other options are incorrect because they do not accurately reflect the primary assumption that is revisited and reformulated, or the consequences of this reformulation."}, "31": {"documentation": {"title": "On the de Sitter swampland criterion", "source": "David Andriot", "docs_id": "1806.10999", "section": ["hep-th", "astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the de Sitter swampland criterion. A new swampland criterion has recently been proposed. As a consequence, it forbids the existence of de Sitter solutions in a low energy effective theory of a quantum gravity. However, there exist classical de Sitter solutions of ten-dimensional (10d) type II supergravities, even though they are unstable. This appears at first sight in contradiction with the criterion. Beyond possible doubts on the validity of these solutions, we propose two answers to this apparent puzzle. A first possibility is that the known 10d solutions always exhibit an energy scale of order or higher than a Kaluza-Klein scale, as we argue. A corresponding 4d low energy effective theory would then differ from the usual consistent truncations, and as we explain, would not admit a de Sitter solution. This would reconcile the existence of these 10d de Sitter solutions with the 4d criterion. A second, alternative possibility is to have a refined swampland criterion, that we propose. It forbids to have both the existence and the stability of a de Sitter solution, while unstable solutions are still allowed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What are the two possible resolutions to the apparent contradiction between the de Sitter swampland criterion and the existence of classical de Sitter solutions in 10d type II supergravities?\n\nA) The 10d solutions are stable and can be used to construct a consistent 4d low energy effective theory, which would admit a de Sitter solution.\n\nB) The 10d solutions are unstable, but the swampland criterion only forbids the existence of stable de Sitter solutions, allowing for the existence of unstable solutions.\n\nC) The 10d solutions always exhibit an energy scale of order or higher than a Kaluza-Klein scale, which would prevent the existence of de Sitter solutions in a corresponding 4d low energy effective theory.\n\nD) The swampland criterion is too restrictive and should be refined to allow for the existence of both stable and unstable de Sitter solutions in 10d type II supergravities.\n\nCorrect Answer: C) The 10d solutions always exhibit an energy scale of order or higher than a Kaluza-Klein scale, which would prevent the existence of de Sitter solutions in a corresponding 4d low energy effective theory.\n\nExplanation: The correct answer is C) because the authors propose that the 10d solutions always exhibit an energy scale of order or higher than a Kaluza-Klein scale, which would prevent the existence of de Sitter solutions in a corresponding 4d low energy effective theory. This is the first possibility mentioned in the documentation as a resolution to the apparent contradiction. The other options are incorrect because they either suggest that the 10d solutions are stable (A and B) or that the swampland criterion should be refined to allow for the existence of both stable and unstable de Sitter solutions (D)."}, "32": {"documentation": {"title": "Discovery of a giant radio fossil in the Ophiuchus galaxy cluster", "source": "S. Giacintucci, M. Markevitch, M. Johnston-Hollitt, D. R. Wik, Q. H.\n  S. Wang, T. E. Clarke", "docs_id": "2002.01291", "section": ["astro-ph.GA", "astro-ph.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discovery of a giant radio fossil in the Ophiuchus galaxy cluster. The Ophiuchus galaxy cluster exhibits a curious concave gas density discontinuity at the edge of its cool core. It was discovered in the Chandra X-ray image by Werner and collaborators, who considered a possibility of it being a boundary of an AGN-inflated bubble located outside the core, but discounted this possibility because it required much too powerful an AGN outburst. Using low-frequency (72-240 MHz) radio data from MWA GLEAM and GMRT, we found that the X-ray structure is, in fact, a giant cavity in the X-ray gas filled with diffuse radio emission with an extraordinarily steep radio spectrum. It thus appears to be a very aged fossil of the most powerful AGN outburst seen in any galaxy cluster ($pV\\sim 5\\times 10^{61}$ erg for this cavity). There is no apparent diametrically opposite counterpart either in X-ray or in the radio. It may have aged out of the observable radio band because of the cluster asymmetry. At present, the central AGN exhibits only a weak radio source, so it should have been much more powerful in the past to have produced such a bubble. The AGN is currently starved of accreting cool gas because the gas density peak is displaced by core sloshing. The sloshing itself could have been set off by this extraordinary explosion if it had occurred in an asymmetric gas core. This dinosaur may be an early example of a new class of sources to be uncovered by low-frequency surveys of galaxy clusters."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most likely explanation for the observed concave gas density discontinuity at the edge of the Ophiuchus galaxy cluster's cool core, and what implications does this have for our understanding of the cluster's evolution?\n\nA) The discontinuity is a result of the cluster's asymmetry, which has caused the gas to be displaced and create a cavity that has aged out of the observable radio band.\nB) The discontinuity is a boundary of an AGN-inflated bubble that was too powerful to have occurred in the past, and its current weak radio source is a result of the AGN being starved of accreting cool gas.\nC) The discontinuity is a result of the cluster's core sloshing, which was set off by an extraordinary explosion that created a giant cavity filled with diffuse radio emission.\nD) The discontinuity is a result of the cluster's merger history, which has caused the gas to be compressed and create a density peak that has displaced the AGN.\n\nCorrect Answer: C) The discontinuity is a result of the cluster's core sloshing, which was set off by an extraordinary explosion that created a giant cavity filled with diffuse radio emission.\n\nExplanation: The correct answer is C) because the text states that the AGN is currently starved of accreting cool gas because the gas density peak is displaced by core sloshing, and that the sloshing itself could have been set off by the extraordinary explosion that created the giant cavity. This suggests that the core sloshing was triggered by the explosion, which is consistent with the idea that the discontinuity is a result of the cluster's core sloshing. The other options are incorrect because they do not provide a clear explanation for the observed discontinuity, and the text does not suggest that the cluster's asymmetry (A) or merger history (D) are responsible for the discontinuity. Option B is also incorrect because it suggests that the AGN-inflated bubble is too powerful to have occurred in the past, but the text states that the bubble is a \"giant fossil\" that has aged out of the observable radio band, implying that it is a relic of a past event."}, "33": {"documentation": {"title": "Training CNNs faster with Dynamic Input and Kernel Downsampling", "source": "Zissis Poulos, Ali Nouri, Andreas Moshovos", "docs_id": "1910.06548", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Training CNNs faster with Dynamic Input and Kernel Downsampling. We reduce training time in convolutional networks (CNNs) with a method that, for some of the mini-batches: a) scales down the resolution of input images via downsampling, and b) reduces the forward pass operations via pooling on the convolution filters. Training is performed in an interleaved fashion; some batches undergo the regular forward and backpropagation passes with original network parameters, whereas others undergo a forward pass with pooled filters and downsampled inputs. Since pooling is differentiable, the gradients of the pooled filters propagate to the original network parameters for a standard parameter update. The latter phase requires fewer floating point operations and less storage due to the reduced spatial dimensions in feature maps and filters. The key idea is that this phase leads to smaller and approximate updates and thus slower learning, but at significantly reduced cost, followed by passes that use the original network parameters as a refinement stage. Deciding how often and for which batches the downsmapling occurs can be done either stochastically or deterministically, and can be defined as a training hyperparameter itself. Experiments on residual architectures show that we can achieve up to 23% reduction in training time with minimal loss in validation accuracy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using Dynamic Input and Kernel Downsampling in training CNNs, according to the paper \"Training CNNs faster with Dynamic Input and Kernel Downsampling\"?\n\nA) It reduces the number of parameters in the network, leading to faster convergence.\nB) It allows for the use of smaller models with fewer parameters, resulting in faster training times.\nC) It reduces the number of floating-point operations required for training, leading to significant reductions in training time.\nD) It enables the use of more complex models with larger filters, resulting in improved accuracy.\n\n**Correct Answer:** C) It reduces the number of floating-point operations required for training, leading to significant reductions in training time.\n\n**Explanation:** The correct answer is C) because the paper states that the latter phase of training, which involves pooling on the convolution filters and downsampled inputs, requires fewer floating-point operations and less storage due to the reduced spatial dimensions in feature maps and filters. This leads to smaller and approximate updates, which in turn results in slower learning, but at significantly reduced cost. The other options are incorrect because they do not accurately reflect the primary advantage of Dynamic Input and Kernel Downsampling as described in the paper."}, "34": {"documentation": {"title": "A continuous auction model with insiders and random time of information\n  release", "source": "Jos\\'e Manuel Corcuera, Giulia Di Nunno, Gergely Farkas, and Bernt\n  {\\O}ksendal", "docs_id": "1411.2835", "section": ["q-fin.PR", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A continuous auction model with insiders and random time of information\n  release. In a unified framework we study equilibrium in the presence of an insider having information on the signal of the firm value, which is naturally connected to the fundamental price of the firm related asset. The fundamental value itself is announced at a future random (stopping) time. We consider two cases. First when the release time of information is known to the insider and then when it is unknown also to her. Allowing for very general dynamics, we study the structure of the insider's optimal strategies in equilibrium and we discuss market efficiency. In particular, we show that in the case the insider knows the information release time, the market is fully efficient. In the case the insider does not know this random time, we see that there is an equilibrium with no full efficiency, but where the sensitivity of prices is decreasing in time according with the probability that the announcement time is greater than the current time. In other words, the prices become more and more stable as the announcement approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the continuous auction model with insiders and random time of information release, consider the case where the insider knows the information release time. What is the implication of this knowledge on market efficiency, and how does it affect the insider's optimal strategies in equilibrium?\n\n**A)** The insider's knowledge of the information release time leads to a decrease in market efficiency, as the insider can exploit this knowledge to gain an unfair advantage.\n\n**B)** The insider's knowledge of the information release time leads to a fully efficient market, as the insider's optimal strategies are aligned with the market's equilibrium.\n\n**C)** The insider's knowledge of the information release time leads to a decrease in the sensitivity of prices over time, as the insider can adjust their strategies to minimize the impact of the announcement.\n\n**D)** The insider's knowledge of the information release time leads to a decrease in the probability of the announcement time being greater than the current time, as the insider can use this knowledge to make more informed decisions.\n\n**Correct Answer:** B) The insider's knowledge of the information release time leads to a fully efficient market, as the insider's optimal strategies are aligned with the market's equilibrium.\n\n**Explanation:** According to the documentation, when the insider knows the information release time, the market is fully efficient. This is because the insider's optimal strategies are aligned with the market's equilibrium, meaning that the insider's actions do not disrupt the market's natural price discovery process. In other words, the insider's knowledge of the information release time allows them to make informed decisions that do not lead to any inefficiencies in the market."}, "35": {"documentation": {"title": "Low-energy spectrum of N = 4 super-Yang-Mills on T^3: flat connections,\n  bound states at threshold, and S-duality", "source": "Mans Henningson and Niclas Wyllard", "docs_id": "hep-th/0703172", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low-energy spectrum of N = 4 super-Yang-Mills on T^3: flat connections,\n  bound states at threshold, and S-duality. We study (3+1)-dimensional N=4 supersymmetric Yang-Mills theory on a spatial three-torus. The low energy spectrum consists of a number of continua of states of arbitrarily low energies. Although the theory has no mass-gap, it appears that the dimensions and discrete abelian magnetic and electric 't Hooft fluxes of the continua are computable in a semi-classical approximation. The wave-functions of the low-energy states are supported on submanifolds of the moduli space of flat connections, at which various subgroups of the gauge group are left unbroken. The field theory degrees of freedom transverse to such a submanifold are approximated by supersymmetric matrix quantum mechanics with 16 supercharges, based on the semi-simple part of this unbroken group. Conjectures about the number of normalizable bound states at threshold in the latter theory play a crucial role in our analysis. In this way, we compute the low-energy spectra in the cases where the simply connected cover of the gauge group is given by SU(n), Spin(2n+1) or Sp(2n). We then show that the constraints of S-duality are obeyed for unique values of the number of bound states in the matrix quantum mechanics. In the cases based on Spin(2n+1) and Sp(2n), the proof involves surprisingly subtle combinatorial identities, which hint at a rich underlying structure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the low-energy spectrum of N = 4 super-Yang-Mills theory on a spatial three-torus. What is the relationship between the number of normalizable bound states at threshold in the supersymmetric matrix quantum mechanics and the constraints of S-duality?\n\nA) The number of bound states is directly proportional to the number of supersymmetries.\nB) The number of bound states is inversely proportional to the number of supersymmetries.\nC) The number of bound states is equal to the number of supersymmetries.\nD) The number of bound states is uniquely determined by the simply connected cover of the gauge group.\n\nCorrect Answer: D) The number of bound states is uniquely determined by the simply connected cover of the gauge group.\n\nExplanation: The correct answer is D) because the problem statement mentions that the constraints of S-duality are obeyed for unique values of the number of bound states in the matrix quantum mechanics, and that this is related to the simply connected cover of the gauge group. The other options are incorrect because they do not accurately reflect the relationship between the number of bound states and the constraints of S-duality.\n\nNote: The other options are designed to be plausible but incorrect, requiring the test-taker to carefully read and understand the problem statement and the underlying physics."}, "36": {"documentation": {"title": "Bayesian gradient sensing in the presence of rotational diffusion", "source": "Maja Novak, Benjamin M. Friedrich", "docs_id": "2002.08138", "section": ["physics.bio-ph", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bayesian gradient sensing in the presence of rotational diffusion. Biological cells estimate concentration gradients of signaling molecules with a precision that is limited not only by sensing noise, but additionally by the cell's own stochastic motion. We ask for the theoretical limits of gradient estimation in the presence of both motility and sensing noise. We introduce a minimal model of a stationary chemotactic agent in the plane subject to rotational diffusion, which uses Bayesian estimation to optimally infer a gradient direction from noisy concentration measurements. Contrary to the known case of gradient sensing by temporal comparison, we show that for spatial comparison, the ultimate precision of gradient sensing scales not with the rotational diffusion time, but with its square-root. To achieve this precision, an individual agent needs to know its own rotational diffusion coefficient. This agent can accurately estimate the expected variability within an ensemble of agents. If an agent, however, does not account for its own motility noise, Bayesian estimation fails in a characteristic manner."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A chemotactic agent is moving in a plane subject to rotational diffusion, and it wants to estimate the direction of a concentration gradient with the highest precision. What is the key factor that determines the ultimate precision of gradient sensing in this scenario?\n\nA) The rotational diffusion time\nB) The rotational diffusion coefficient\nC) The sensing noise level\nD) The agent's motility speed\n\n**Correct Answer:** B) The rotational diffusion coefficient\n\n**Explanation:** According to the documentation, the ultimate precision of gradient sensing scales not with the rotational diffusion time, but with its square-root. This implies that the rotational diffusion coefficient is the key factor that determines the precision of gradient sensing. The agent needs to know its own rotational diffusion coefficient to accurately estimate the expected variability within an ensemble of agents.\n\n**Candidate A (Incorrect)**: The rotational diffusion time is related to the rotational diffusion coefficient, but it is not the key factor that determines the ultimate precision of gradient sensing.\n\n**Candidate B (Correct)**: The rotational diffusion coefficient is the key factor that determines the ultimate precision of gradient sensing, as it is the parameter that scales with the precision of gradient estimation.\n\n**Candidate C (Incorrect)**: The sensing noise level is an important factor in gradient sensing, but it is not the key factor that determines the ultimate precision of gradient sensing in the presence of rotational diffusion.\n\n**Candidate D (Incorrect)**: The agent's motility speed is related to its motility noise, but it is not the key factor that determines the ultimate precision of gradient sensing."}, "37": {"documentation": {"title": "Graph Generators: State of the Art and Open Challenges", "source": "Angela Bonifati, Irena Holubov\\'a, Arnau Prat-P\\'erez, Sherif Sakr", "docs_id": "2001.07906", "section": ["cs.DB", "cs.IR", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graph Generators: State of the Art and Open Challenges. The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties, or gauging the effectiveness of graph algorithms, techniques and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary goals of the Graph Generators: State of the Art and Open Challenges survey, and what type of data-intensive tasks are the survey's focus on?\n\nA) To provide a comprehensive overview of graph generators for specific applications, such as social networks and community detection, and to identify open challenges and missing requirements for current graph generators.\nB) To evaluate the effectiveness of graph algorithms and techniques for general graphs, and to discuss the practical usage of graph generators in various subfields.\nC) To classify graph generators across multiple subfields, such as Semantic Web and graph databases, and to analyze their functionalities and supported operations.\nD) To develop new graph generators for emerging fields, and to discuss the future extensions of current graph generators.\n\nCorrect Answer: A) To provide a comprehensive overview of graph generators for specific applications, such as social networks and community detection, and to identify open challenges and missing requirements for current graph generators.\n\nExplanation: The survey aims to provide a comprehensive overview of the state-of-the-art graph generators, focusing on those that are pertinent and suitable for several data-intensive tasks. This includes evaluating the effectiveness of graph generators for specific applications, such as social networks and community detection, and identifying open challenges and missing requirements for current graph generators."}, "38": {"documentation": {"title": "Navigating the Cryptocurrency Landscape: An Islamic Perspective", "source": "Hina Binte Haq, Syed Taha Ali", "docs_id": "1811.05935", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Navigating the Cryptocurrency Landscape: An Islamic Perspective. Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to generate headlines and intense debate. What started as an underground experiment by a rag tag group of programmers armed with a Libertarian manifesto has now resulted in a thriving $230 billion ecosystem, with constant on-going innovation. Scholars and researchers alike are realizing that cryptocurrencies are far more than mere technical innovation; they represent a distinct and revolutionary new economic paradigm tending towards decentralization. Unfortunately, this bold new universe is little explored from the perspective of Islamic economics and finance. Our work aims to address these deficiencies. Our paper makes the following distinct contributions We significantly expand the discussion on whether cryptocurrencies qualify as \"money\" from an Islamic perspective and we argue that this debate necessitates rethinking certain fundamental definitions. We conclude that the cryptocurrency phenomenon, with its radical new capabilities, may hold considerable opportunity which merits deeper investigation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary argument made by the authors of the paper \"Navigating the Cryptocurrency Landscape: An Islamic Perspective\" regarding the classification of cryptocurrencies as \"money\" from an Islamic perspective?\n\nA) Cryptocurrencies do not qualify as \"money\" from an Islamic perspective due to their decentralized nature.\nB) Cryptocurrencies do not qualify as \"money\" from an Islamic perspective due to their lack of physical presence.\nC) Cryptocurrencies qualify as \"money\" from an Islamic perspective due to their decentralized nature and radical new capabilities.\nD) Cryptocurrencies do not qualify as \"money\" from an Islamic perspective due to their reliance on programming and technical innovation.\n\nCorrect Answer: C) Cryptocurrencies qualify as \"money\" from an Islamic perspective due to their decentralized nature and radical new capabilities.\n\nExplanation: The authors argue that the debate over whether cryptocurrencies qualify as \"money\" from an Islamic perspective necessitates rethinking certain fundamental definitions. They conclude that the cryptocurrency phenomenon, with its radical new capabilities, may hold considerable opportunity that merits deeper investigation. This suggests that the authors believe cryptocurrencies do qualify as \"money\" from an Islamic perspective, due to their decentralized nature and innovative capabilities."}, "39": {"documentation": {"title": "Synthetic Interventions", "source": "Anish Agarwal, Devavrat Shah, Dennis Shen", "docs_id": "2006.07691", "section": ["econ.EM", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Synthetic Interventions. Consider a setting where there are $N$ heterogeneous units (e.g., individuals, sub-populations) and $D$ interventions (e.g., socio-economic policies). Our goal is to learn the potential outcome associated with every intervention on every unit (i.e., $N \\times D$ causal parameters). Towards this, we present a causal framework, synthetic interventions (SI), to infer these $N \\times D$ causal parameters while only observing each of the $N$ units under at most two interventions, independent of $D$. This can be significant as the number of interventions, i.e, level of personalization, grows. Importantly, our estimator also allows for latent confounders that determine how interventions are assigned. Theoretically, under a novel tensor factor model across units, measurements, and interventions, we formally establish an identification result for each of these $N \\times D$ causal parameters and establish finite-sample consistency and asymptotic normality of our estimator. The estimator is furnished with a data-driven test to verify its suitability. Empirically, we validate our framework through both experimental and observational case studies; namely, a large-scale A/B test performed on an e-commerce platform, and an evaluation of mobility restriction on morbidity outcomes due to COVID-19. We believe this has important implications for program evaluation and the design of data-efficient RCTs with heterogeneous units and multiple interventions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a setting where there are $N$ heterogeneous units (e.g., individuals, sub-populations) and $D$ interventions (e.g., socio-economic policies). Suppose we want to learn the potential outcome associated with every intervention on every unit (i.e., $N \\times D$ causal parameters). Assume that we only observe each of the $N$ units under at most two interventions, independent of $D$. What is the main advantage of the synthetic interventions (SI) framework presented in the paper, and how does it address the challenge of learning $N \\times D$ causal parameters with limited data?\n\n**A)** The SI framework allows for the estimation of $N \\times D$ causal parameters with high accuracy, even with limited data, by leveraging a tensor factor model across units, measurements, and interventions.\n\n**B)** The SI framework provides a data-driven test to verify the suitability of the estimator, which is essential for ensuring the accuracy of the estimated causal parameters.\n\n**C)** The SI framework enables the estimation of $N \\times D$ causal parameters while accounting for latent confounders that determine how interventions are assigned, which is crucial for program evaluation and RCT design.\n\n**D)** The SI framework is particularly useful for large-scale A/B tests and observational studies, where the number of interventions and units is high, and the data is limited.\n\n**Correct Answer:** C) The SI framework enables the estimation of $N \\times D$ causal parameters while accounting for latent confounders that determine how interventions are assigned, which is crucial for program evaluation and RCT design.\n\n**Explanation:** The correct answer highlights the key advantage of the SI framework, which is its ability to account for latent confounders that determine how interventions are assigned. This is a critical aspect of program evaluation and RCT design, as it allows for more accurate estimation of causal parameters and better decision-making. The other options, while related to the SI framework, do not capture the main advantage of the framework. Option A is incorrect because while the SI framework does use a tensor factor model, it is not the main advantage. Option B is incorrect because the data-driven test is a tool used to verify the estimator, not the main advantage. Option D is incorrect because while the SI framework is useful for large-scale A/B tests and observational studies, it is not the main advantage."}, "40": {"documentation": {"title": "Sequential sampling of junction trees for decomposable graphs", "source": "Jimmy Olsson, Tetyana Pavlenko and Felix L. Rios", "docs_id": "1806.00584", "section": ["math.ST", "cs.DM", "math.CO", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sequential sampling of junction trees for decomposable graphs. The junction-tree representation provides an attractive structural property for organizing a decomposable graph. In this study, we present two novel stochastic algorithms, which we call the junction-tree expander and junction-tree collapser for sequential sampling of junction trees for decomposable graphs. We show that recursive application of the junction-tree expander, expanding incrementally the underlying graph with one vertex at a time, has full support on the space of junction trees with any given number of underlying vertices. On the other hand, the junction-tree collapser provides a complementary operation for removing vertices in the underlying decomposable graph of a junction tree, while maintaining the junction tree property. A direct application of our suggested algorithms is demonstrated in a sequential-Monte-Carlo setting designed for sampling from distributions on spaces of decomposable graphs. Numerical studies illustrate the utility of the proposed algorithms for combinatorial computations on decomposable graphs and junction trees. All the methods proposed in the paper are implemented in the Python library trilearn."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the junction-tree expander algorithm over other methods for sequential sampling of junction trees for decomposable graphs?\n\nA) It provides a more efficient way to collapse vertices in the underlying graph.\nB) It allows for the recursive application of the algorithm to expand the underlying graph incrementally, providing full support on the space of junction trees with any given number of underlying vertices.\nC) It is more suitable for sampling from distributions on spaces of decomposable graphs with a large number of vertices.\nD) It is a more complex algorithm that requires more computational resources.\n\n**Correct Answer:** B) It allows for the recursive application of the algorithm to expand the underlying graph incrementally, providing full support on the space of junction trees with any given number of underlying vertices.\n\n**Explanation:** The junction-tree expander algorithm is a novel stochastic algorithm that allows for the recursive application of the algorithm to expand the underlying graph incrementally. This property provides full support on the space of junction trees with any given number of underlying vertices, making it a powerful tool for sequential sampling of junction trees for decomposable graphs. The other options are incorrect because they do not accurately describe the primary advantage of the junction-tree expander algorithm."}, "41": {"documentation": {"title": "Gas phase water in the surface layer of protoplanetary disks", "source": "C. Dominik, C. Ceccarelli, D. Hollenbach, M.Kaufman", "docs_id": "astro-ph/0510635", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gas phase water in the surface layer of protoplanetary disks. Recent observations of the ground state transition of HDO at 464 GHz towards the protoplanetary disk of DM Tau have detected the presence of water vapor in the regions just above the outer disk midplane (Ceccarelli et al 2005). In the absence of non-thermal desorption processes, water should be almost entirely frozen onto the grain mantles and HDO undetectable. In this Letter we present a chemical model that explores the possibility that the icy mantles are photo-desorbed by FUV (6eV < h nu < 13.6eV) photons. We show that the average Interstellar FUV field is enough to create a layer of water vapor above the disk midplane over the entire disk. Assuming a photo-desorption yield of 10^{-3}, the water abundance in this layer is predicted to be ~ 3 x 10^{-7} and the average H2O column density is ~ 1.6x 10^{15} cm^{-2}. The predictions are very weakly dependent on the details of the model, like the incident FUV radiation field, and the gas density in the disk. Based on this model, we predict a gaseous HDO/H2O ratio in DM Tau of ~1%. In addition, we predict the ground state transition of water at 557 GHz to be undetectable with ODIN and/or HSO-HIFI."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the predicted average H2O column density in the layer above the disk midplane in the protoplanetary disk of DM Tau, assuming a photo-desorption yield of 10^-3?\n\nA) 1.6 x 10^14 cm^-2\nB) 1.6 x 10^15 cm^-2\nC) 3.2 x 10^15 cm^-2\nD) 6.4 x 10^15 cm^-2\n\nCorrect Answer: B) 1.6 x 10^15 cm^-2\n\nExplanation: According to the text, the predicted average H2O column density is ~ 1.6 x 10^15 cm^-2, which corresponds to option B. The other options are incorrect, as they do not match the predicted value."}, "42": {"documentation": {"title": "Amplitude death and resurgence of oscillation in network of mobile\n  oscillators", "source": "Soumen Majhi and Dibakar Ghosh", "docs_id": "1707.01218", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Amplitude death and resurgence of oscillation in network of mobile\n  oscillators. The phenomenon of amplitude death has been explored using a variety of different coupling strategies in the last two decades. In most of the work, the basic coupling arrangement is considered to be static over time, although many realistic systems exhibit significant changes in the interaction pattern as time varies. In this article, we study the emergence of amplitude death in a dynamical network composed of time-varying interaction amidst a collection of random walkers in a finite region of three dimensional space. We consider an oscillator for each walker and demonstrate that depending upon the network parameters and hence the interaction between them, global oscillation in the network gets suppressed. In this framework, vision range of each oscillator decides the number of oscillators with which it interacts. In addition, with the use of an appropriate feedback parameter in the coupling strategy, we articulate how the suppressed oscillation can be resurrected in the systems' parameter space. The phenomenon of amplitude death and the resurgence of oscillation is investigated taking limit cycle and chaotic oscillators for broad ranges of parameters, like interaction strength k between the entities, vision range r and the speed of movement v."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of amplitude death and resurgence of oscillation in a network of mobile oscillators, what is the role of the \"vision range\" of each oscillator in determining the number of oscillators with which it interacts?\n\n**A)** The vision range determines the strength of the coupling between oscillators.\n**B)** The vision range determines the number of oscillators with which an oscillator interacts, with larger vision ranges leading to more interactions.\n**C)** The vision range determines the frequency of oscillation, with larger vision ranges leading to lower frequencies.\n**D)** The vision range has no effect on the number of oscillators with which an oscillator interacts.\n\n**Correct Answer:** B) The vision range determines the number of oscillators with which an oscillator interacts, with larger vision ranges leading to more interactions.\n\n**Explanation:** According to the documentation, the vision range of each oscillator \"decides the number of oscillators with which it interacts.\" This suggests that the vision range is a critical parameter in determining the number of oscillators with which an oscillator interacts. Therefore, option B is the correct answer.\n\n**Explanation for incorrect options:**\n\n* Option A is incorrect because the documentation does not mention the strength of the coupling between oscillators being determined by the vision range.\n* Option C is incorrect because the documentation does not mention the frequency of oscillation being determined by the vision range.\n* Option D is incorrect because the documentation explicitly states that the vision range determines the number of oscillators with which an oscillator interacts."}, "43": {"documentation": {"title": "Multifaceted nonlinear dynamics in $\\mathcal{PT}$-symmetric coupled\n  Li\\'{e}nard oscillators", "source": "Jyoti Prasad Deka, Arjunan Govindarajan, Manas Kulkarni and Amarendra\n  K. Sarma", "docs_id": "1812.10126", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multifaceted nonlinear dynamics in $\\mathcal{PT}$-symmetric coupled\n  Li\\'{e}nard oscillators. We propose a generalized parity-time ($\\mathcal{PT}$) -symmetric Li\\'enard oscillator with two different orders of nonlinear position-dependent dissipation. We study the stability of the stationary states by using the eigenvalues of Jacobian and evaluate the stability threshold thereafter. In the first order nonlinear damping model, we discover that the temporal evolution of both gain and lossy oscillators attains a complete convergence towards the stable stationary state leading to the emergence of oscillation and amplitude deaths. Also, the system displays a remarkable manifestation of transient chaos in the lossy oscillator while the gain counterpart exhibits blow-up dynamics for certain choice of initial conditions and control parameters. Employing an external driving force on the loss oscillator, we find that the blow-up dynamics can be controlled and a pure aperiodic state is achievable. On the other hand, the second order nonlinear damping model yields a completely different dynamics on contrary to the first order where the former reveals a conventional quasi-periodic route to chaos upon decreasing the natural frequency of both gain and loss oscillators. An electronic circuit scheme for the experimental realization of the proposed system has also been put forward."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the generalized parity-time ($\\mathcal{PT}$)-symmetric Li\u00e9nard oscillator with two different orders of nonlinear position-dependent dissipation, what is the primary difference in the dynamics exhibited by the system when the natural frequency of both gain and loss oscillators is decreased?\n\nA) The system exhibits a complete convergence towards the stable stationary state, leading to the emergence of oscillation and amplitude deaths.\nB) The system displays a conventional quasi-periodic route to chaos upon decreasing the natural frequency of both gain and loss oscillators.\nC) The system exhibits a transient chaos in the lossy oscillator, while the gain counterpart exhibits blow-up dynamics for certain choice of initial conditions and control parameters.\nD) The system displays a pure aperiodic state when an external driving force is applied to the loss oscillator.\n\n**Correct Answer:** B) The system displays a conventional quasi-periodic route to chaos upon decreasing the natural frequency of both gain and loss oscillators.\n\n**Explanation:**\n\nThe correct answer is B) because the second-order nonlinear damping model yields a completely different dynamics compared to the first-order nonlinear damping model. In the first-order model, the system exhibits a complete convergence towards the stable stationary state, leading to the emergence of oscillation and amplitude deaths. However, in the second-order model, the system displays a conventional quasi-periodic route to chaos upon decreasing the natural frequency of both gain and loss oscillators. This is a key difference between the two models, and the correct answer requires an understanding of the underlying dynamics and the effects of decreasing the natural frequency on the system's behavior.\n\n**Candidate A** is incorrect because it describes the dynamics of the first-order nonlinear damping model, not the second-order model.\n\n**Candidate C** is incorrect because it describes the dynamics of the first-order nonlinear damping model, not the second-order model.\n\n**Candidate D** is incorrect because it describes a specific scenario where an external driving force is applied to the loss oscillator, but it does not address the primary difference in the dynamics between the two models."}, "44": {"documentation": {"title": "Volatility: a hidden Markov process in financial time series", "source": "Zoltan Eisler, Josep Perello, Jaume Masoliver", "docs_id": "physics/0612084", "section": ["physics.soc-ph", "physics.data-an", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Volatility: a hidden Markov process in financial time series. The volatility characterizes the amplitude of price return fluctuations. It is a central magnitude in finance closely related to the risk of holding a certain asset. Despite its popularity on trading floors, the volatility is unobservable and only the price is known. Diffusion theory has many common points with the research on volatility, the key of the analogy being that volatility is the time-dependent diffusion coefficient of the random walk for the price return. We present a formal procedure to extract volatility from price data, by assuming that it is described by a hidden Markov process which together with the price form a two-dimensional diffusion process. We derive a maximum likelihood estimate valid for a wide class of two-dimensional diffusion processes. The choice of the exponential Ornstein-Uhlenbeck (expOU) stochastic volatility model performs remarkably well in inferring the hidden state of volatility. The formalism is applied to the Dow Jones index. The main results are: (i) the distribution of estimated volatility is lognormal, which is consistent with the expOU model; (ii) the estimated volatility is related to trading volume by a power law of the form $\\sigma \\propto V^{0.55}$; and (iii) future returns are proportional to the current volatility which suggests some degree of predictability for the size of future returns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Arxiv paper \"Volatility: a hidden Markov process in financial time series\", what is the primary assumption made about the relationship between volatility and price return fluctuations?\n\nA) Volatility is a constant parameter that is directly proportional to the price return.\nB) Volatility is a time-dependent diffusion coefficient that is related to the price return through a random walk process.\nC) Volatility is a hidden Markov process that is independent of the price return.\nD) Volatility is a deterministic function of the price return.\n\n**Correct Answer:** B) Volatility is a time-dependent diffusion coefficient that is related to the price return through a random walk process.\n\n**Explanation:** The paper states that \"volatility is the time-dependent diffusion coefficient of the random walk for the price return\", which implies that volatility is related to the price return through a stochastic process. This is a key assumption in the paper, and it is used to derive a maximum likelihood estimate of volatility.\n\n**Additional questions:**\n\n* What is the name of the stochastic volatility model that is chosen in the paper, and what is its significance in inferring the hidden state of volatility? (A) expOU, (B) lognormal, (C) Ornstein-Uhlenbeck, (D) Black-Scholes.\n**Correct Answer:** C) Ornstein-Uhlenbeck\n\n**Explanation:** The paper states that the \"choice of the exponential Ornstein-Uhlenbeck (expOU) stochastic volatility model performs remarkably well in inferring the hidden state of volatility\". This suggests that the expOU model is a good choice for modeling volatility.\n\n* What is the relationship between estimated volatility and trading volume, according to the paper? (A) Volatility is inversely proportional to trading volume, (B) Volatility is directly proportional to trading volume, (C) Volatility is proportional to trading volume to the power of 0.55, (D) Volatility is independent of trading volume.\n**Correct Answer:** C) Volatility is proportional to trading volume to the power of 0.55\n\n**Explanation:** The paper states that \"the estimated volatility is related to trading volume by a power law of the form \u03c3 \u221d V^{0.55}\", which implies that volatility is proportional to trading volume to the power of 0.55."}, "45": {"documentation": {"title": "Distinguishing $d_{xz}+i d_{yz}$ and $d_{x^2-y^2}$ pairing in\n  $Sr_2RuO_4$ by high magnetic field H-T phase diagrams", "source": "R. Gupta, S. Shallcross, J. Quintanilla, M. Gradhand, and J. Annett", "docs_id": "2111.00257", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distinguishing $d_{xz}+i d_{yz}$ and $d_{x^2-y^2}$ pairing in\n  $Sr_2RuO_4$ by high magnetic field H-T phase diagrams. Employing a realistic tight-binding model describing the Fermi surface in the normal state of $Sr_2RuO_4$ we map out magnetic field versus temperature phase diagrams for $d_{x^2-y^2} (B_{1g})$ and $d_{xz}+id_{yz} (E_g)$ pairing types. Both produce (i) a similar Knight shift suppression of $\\sim\\!80\\%$ and (ii) a bicritical point at $T=0.88$K separating low field second order phase transitions from high field Pauli limiting first order transitions. We find, however, strikingly different phase behaviour within the high field Pauli limiting region. For $d_{x^2-y^2}$ pairing symmetry an additional lower critical line of first order transitions is found (terminating in a critical point at $T=0.09-0.22$K depending on the choice of Hubbard U parameters) while for $d_{xz}+id_{yz}$ no such additional high field phase transitions are found for any choice of Hubbard U. In conjunction with our earlier finding [{\\it Physical Review B} {\\bf 102} (23), 235203] for $p$-wave helical pairing of a still different high field phase structure (a lower critical field line meeting the upper critical field line exactly at the bicritical point), we suggest high field Pauli limiting phase structure as a possible route to distinguish pairing symmetries in this material."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study on $Sr_2RuO_4$ propose that high-field Pauli limiting phase structure can be used to distinguish between different pairing symmetries in this material. What is the key difference in phase behavior between $d_{x^2-y^2}$ and $d_{xz}+id_{yz}$ pairing symmetries in the high-field Pauli limiting region?\n\nA) The presence of a lower critical line of first-order transitions for $d_{x^2-y^2}$ pairing symmetry.\nB) The absence of a lower critical line of first-order transitions for $d_{xz}+id_{yz}$ pairing symmetry.\nC) The presence of a bicritical point at $T=0.88$K for both pairing symmetries.\nD) The identical phase behavior within the high-field Pauli limiting region for both pairing symmetries.\n\n**Correct Answer:** A) The presence of a lower critical line of first-order transitions for $d_{x^2-y^2}$ pairing symmetry.\n\n**Explanation:** The study finds that for $d_{x^2-y^2}$ pairing symmetry, an additional lower critical line of first-order transitions is present, while for $d_{xz}+id_{yz}$ pairing symmetry, no such additional high-field phase transitions are found. This difference in phase behavior is a key distinguishing feature between the two pairing symmetries."}, "46": {"documentation": {"title": "Minimal length effects in black hole thermodynamics from tunneling\n  formalism", "source": "Sunandan Gangopadhyay", "docs_id": "1405.4229", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Minimal length effects in black hole thermodynamics from tunneling\n  formalism. The tunneling formalism in the Hamilton-Jacobi approach is adopted to study Hawking radiation of massless Dirac particles from spherically symmetric black hole spacetimes incorporating the effects of the generalized uncertainty principle. The Hawking temperature is found to contain corrections from the generalized uncertainty principle. Further, we show from this result that the ratio of the GUP corrected energy of the particle to the GUP corrected Hawking temperature is equal to the ratio of the corresponding uncorrected quantities. This result is then exploited to compute the Hawking temperature for more general forms of the uncertainty principle having infinite number of terms. Choosing the coefficients of the terms in the series in a specific way enables one to sum the infinite series exactly. This leads to a Hawking temperature for the Schwarzschild black hole that agrees with the result which accounts for the one loop back reaction effect. The entropy is finally computed and yields the area theorem upto logarithmic corrections."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of minimal length effects in black hole thermodynamics from tunneling formalism, what is the implication of the ratio of the GUP corrected energy of a massless Dirac particle to the GUP corrected Hawking temperature being equal to the ratio of the corresponding uncorrected quantities?\n\nA) The GUP corrections lead to a significant increase in the Hawking temperature.\nB) The GUP corrections have no effect on the Hawking temperature.\nC) The GUP corrections imply that the Hawking temperature is proportional to the energy of the particle.\nD) The GUP corrections imply that the Hawking temperature is independent of the energy of the particle.\n\nCorrect Answer: C) The GUP corrections imply that the Hawking temperature is proportional to the energy of the particle.\n\nExplanation: The correct answer is C) because the ratio of the GUP corrected energy of the particle to the GUP corrected Hawking temperature is equal to the ratio of the corresponding uncorrected quantities, implying that the Hawking temperature is proportional to the energy of the particle. This result is a key finding in the paper, and it has important implications for our understanding of black hole thermodynamics.\n\nCandidate A is incorrect because the GUP corrections do not necessarily lead to a significant increase in the Hawking temperature. Candidate B is incorrect because the GUP corrections do have an effect on the Hawking temperature. Candidate D is incorrect because the GUP corrections imply that the Hawking temperature is proportional to the energy of the particle, not independent of it."}, "47": {"documentation": {"title": "Elastic scattering measurements for the $^{10}$C + $^{208}$Pb system at\n  E$_{\\rm lab}$ = 66 MeV", "source": "R Linares, Mandira Sinha, E N Cardozo, V Guimaraes, G Rogachev, J\n  Hooker, E Koshchiy, T Ahn, C Hunt, H Jayatissa, S Upadhyayula, B Roeder, A\n  Saastomoinen, J Lubian, M Rodriguez-Gallardo, J Casal, KCC Pires, M Assuncao,\n  Y Penionzhkevich and S Lukyanov", "docs_id": "2106.05693", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Elastic scattering measurements for the $^{10}$C + $^{208}$Pb system at\n  E$_{\\rm lab}$ = 66 MeV. Background: The influence of halo structure of $^6$He, $^8$B, $^{11}$Be and $^{11}$Li nuclei in several mechanisms such as direct reactions and fusion is already established, although not completely understood. The influence of the $^{10}$C Brunnian structure is less known. Purpose: To investigate the influence of the cluster configuration of $^{10}$C on the elastic scattering at an energy close to the Coulomb barrier. Methods: We present experimental data for the elastic scattering of the $^{10}$C+$^{208}$Pb system at $E_{\\rm lab}$ = 66 MeV. The data are compared to the three- and the four-body continuum-discretized coupled-channels calculations assuming $^9$B+$p$, $^6$Be+$\\alpha$ and $^8$Be+$p$+$p$ configurations. Results: The experimental angular distribution of the cross sections shows the suppression of the Fresnel peak that is reasonably well reproduced by the continuum-discretized coupled-channels calculations. However, the calculations underestimate the cross sections at backward angles. Couplings to continuum states represent a small effect. Conclusions: The cluster configurations of $^{10}$C assumed in the present work are able to describe some of the features of the data. In order to explain the data at backward angles, experimental data for the breakup and an extension of theoretical formalism towards a four-body cluster seem to be in need to reproduce the measured angular distribution."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the cluster configuration of $^{10}$C from the experimental data and theoretical calculations presented in the study?\n\nA) The cluster configuration of $^{10}$C is not important for the elastic scattering of the $^{10}$C+$^{208}$Pb system.\n\nB) The cluster configurations of $^{10}$C, such as $^9$B+$p$, $^6$Be+$\\alpha$, and $^8$Be+$p$+$p$, are able to describe some features of the data, but do not fully explain the experimental results.\n\nC) The cluster configuration of $^{10}$C is not well understood and requires further experimental and theoretical investigation to be fully described.\n\nD) The cluster configuration of $^{10}$C is not relevant to the elastic scattering of the $^{10}$C+$^{208}$Pb system, and the data can be explained by other mechanisms.\n\nCorrect Answer: B) The cluster configurations of $^{10}$C, such as $^9$B+$p$, $^6$Be+$\\alpha$, and $^8$Be+$p$+$p$, are able to describe some features of the data, but do not fully explain the experimental results.\n\nExplanation: The study presents experimental data and theoretical calculations that suggest the cluster configurations of $^{10}$C are able to describe some features of the data, such as the suppression of the Fresnel peak. However, the calculations also underestimate the cross sections at backward angles, indicating that the cluster configurations are not fully explaining the experimental results. This requires further investigation to fully understand the role of the cluster configuration of $^{10}$C in the elastic scattering of the $^{10}$C+$^{208}$Pb system."}, "48": {"documentation": {"title": "The path and the multi-teaching issues in the coupled pendulum and\n  mass-spring experiments", "source": "Ilario Boscolo, Leonardo Gariboldi, Ruth Loewenstein", "docs_id": "0901.0994", "section": ["physics.ed-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The path and the multi-teaching issues in the coupled pendulum and\n  mass-spring experiments. The main task of an introductory laboratory course is to foster students' manual, conceptual and statistical ability to investigate physical phenomena. Needing very simple apparatus, pendulum experiments are an ideal starting point in our first-year laboratory course because they are rich in both physical content and data processing. These experiments allow many variations, e.g. pendulum bobs can have different shapes, threads can be tied to a hook at their edge or pass through their centre of mass, they can be hanged as simple or bifilar pendulums. In these many variations, they emphasize the difference between theory and practice in the passage from an idealized scheme to a real experimental asset, which becomes evident, for example, when the pendulum bob cannot be considered an idealized point mass. Moreover, they require careful observation of details such as the type of thread used and its rigidity or the bob initial slant, which leads to different behaviors. Their mathematical models require a wide range of fundamental topics in experimental data analysis: arithmetic and weighted mean, standard deviation, central limit theorem application, data distribution, and the significant difference between theory and practice. Setting the mass-spring experiment immediately after the pendulum highlights the question of resonance, revises the gap between theory and practice in another context, and provides another occasion to practice further techniques in data analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the coupled pendulum and mass-spring experiments, what is the primary advantage of using pendulum bobs with different shapes, threads, and initial slant angles, and how does this relate to the difference between theory and practice?\n\n**A)** The varied pendulum bobs allow for a more precise measurement of the pendulum's period, which is essential for understanding the effects of resonance. However, this precision comes at the cost of increased complexity in data analysis.\n\n**B)** The use of varied pendulum bobs enables students to investigate the impact of non-idealized assumptions on experimental results, highlighting the importance of considering real-world limitations in theoretical models.\n\n**C)** The different pendulum bobs and initial conditions allow students to explore the effects of thread rigidity and bob shape on the pendulum's behavior, providing a more nuanced understanding of the physical phenomena being studied.\n\n**D)** The varied pendulum bobs and initial conditions lead to a greater emphasis on statistical analysis, as students must account for the variability in experimental results to draw meaningful conclusions.\n\n**Correct Answer:** B) The use of varied pendulum bobs enables students to investigate the impact of non-idealized assumptions on experimental results, highlighting the importance of considering real-world limitations in theoretical models.\n\n**Explanation:** The correct answer highlights the primary advantage of using varied pendulum bobs, which is to illustrate the difference between theory and practice. By introducing non-idealized assumptions and varying experimental conditions, students can gain a deeper understanding of how theoretical models can be influenced by real-world limitations. This requires careful observation of details, such as thread rigidity and bob initial slant, and the application of statistical techniques to analyze the data. The correct answer emphasizes the importance of considering these real-world limitations in theoretical models, which is a key aspect of the experiment."}, "49": {"documentation": {"title": "Search for sub-solar mass ultracompact binaries in Advanced LIGO's\n  second observing run", "source": "The LIGO Scientific Collaboration and the Virgo Collaboration: B. P.\n  Abbott, R. Abbott, T. D. Abbott, S. Abraham, F. Acernese, K. Ackley, C.\n  Adams, R. X. Adhikari, V. B. Adya, C. Affeldt, M. Agathos, K. Agatsuma, N.\n  Aggarwal, O. D. Aguiar, L. Aiello, A. Ain, P. Ajith, G. Allen, A. Allocca, M.\n  A. Aloy, P. A. Altin, A. Amato, S. Anand, A. Ananyeva, S. B. Anderson, W. G.\n  Anderson, S. V. Angelova, S. Antier, S. Appert, K. Arai, M. C. Araya, J. S.\n  Areeda, M. Ar\\`ene, N. Arnaud, S. M. Aronson, K. G. Arun, S. Ascenzi, G.\n  Ashton, S. M. Aston, P. Astone, F. Aubin, P. Aufmuth, K. AultONeal, C.\n  Austin, V. Avendano, A. Avila-Alvarez, S. Babak, P. Bacon, F. Badaracco, M.\n  K. M. Bader, S. Bae, J. Baird, P. T. Baker, F. Baldaccini, G. Ballardin, S.\n  W. Ballmer, A. Bals, S. Banagiri, J. C. Barayoga, C. Barbieri, S. E. Barclay,\n  B. C. Barish, D. Barker, K. Barkett, S. Barnum, F. Barone, B. Barr, L.\n  Barsotti, M. Barsuglia, D. Barta, J. Bartlett, I. Bartos, R. Bassiri, A.\n  Basti, M. Bawaj, J. C. Bayley, M. Bazzan, B. B\\'ecsy, M. Bejger, I.\n  Belahcene, A. S. Bell, D. Beniwal, M. G. Benjamin, B. K. Berger, G. Bergmann,\n  S. Bernuzzi, C. P. L. Berry, D. Bersanetti, A. Bertolini, J. Betzwieser, R.\n  Bhandare, J. Bidler, E. Biggs, I. A. Bilenko, S. A. Bilgili, G. Billingsley,\n  R. Birney, O. Birnholtz, S. Biscans, M. Bischi, S. Biscoveanu, A. Bisht, M.\n  Bitossi, M. A. Bizouard, J. K. Blackburn, J. Blackman, C. D. Blair, D. G.\n  Blair, R. M. Blair, S. Bloemen, F. Bobba, N. Bode, M. Boer, Y. Boetzel, G.\n  Bogaert, F. Bondu, R. Bonnand, P. Booker, B. A. Boom, R. Bork, V. Boschi, S.\n  Bose, V. Bossilkov, J. Bosveld, Y. Bouffanais, A. Bozzi, C. Bradaschia, P. R.\n  Brady, A. Bramley, M. Branchesi, J. E. Brau, M. Breschi, T. Briant, J. H.\n  Briggs, F. Brighenti, A. Brillet, M. Brinkmann, P. Brockill, A. F. Brooks, J.\n  Brooks, D. D. Brown, S. Brunett, A. Buikema, T. Bulik, H. J. Bulten, A.\n  Buonanno, D. Buskulic, C. Buy, R. L. Byer, M. Cabero, L. Cadonati, G.\n  Cagnoli, C. Cahillane, J. Calder\\'on Bustillo, T. A. Callister, E. Calloni,\n  J. B. Camp, W. A. Campbell, K. C. Cannon, H. Cao, J. Cao, G. Carapella, F.\n  Carbognani, S. Caride, M. F. Carney, G. Carullo, J. Casanueva Diaz, C.\n  Casentini, S. Caudill, M. Cavagli\\`a, F. Cavalier, R. Cavalieri, G. Cella, P.\n  Cerd\\'a-Dur\\'an, E. Cesarini, O. Chaibi, K. Chakravarti, S. J. Chamberlin, M.\n  Chan, S. Chao, P. Charlton, E. A. Chase, E. Chassande-Mottin, D. Chatterjee,\n  M. Chaturvedi, K. Chatziioannou, B. D. Cheeseboro, H. Y. Chen, X. Chen, Y.\n  Chen, H.-P. Cheng, C. K. Cheong, H. Y. Chia, F. Chiadini, A. Chincarini, A.\n  Chiummo, G. Cho, H. S. Cho, M. Cho, N. Christensen, Q. Chu, S. Chua, K. W.\n  Chung, S. Chung, G. Ciani, M. Cie{\\'s}lar, A. A. Ciobanu, R. Ciolfi, F.\n  Cipriano, A. Cirone, F. Clara, J. A. Clark, P. Clearwater, F. Cleva, E.\n  Coccia, P.-F. Cohadon, D. Cohen, M. Colleoni, C. G. Collette, C. Collins, M.\n  Colpi, L. R. Cominsky, M. Constancio Jr., L. Conti, S. J. Cooper, P. Corban,\n  T. R. Corbitt, I. Cordero-Carri\\'on, S. Corezzi, K. R. Corley, N. Cornish, D.\n  Corre, A. Corsi, S. Cortese, C. A. Costa, R. Cotesta, M. W. Coughlin, S. B.\n  Coughlin, J.-P. Coulon, S. T. Countryman, P. Couvares, P. B. Covas, E. E.\n  Cowan, D. M. Coward, M. J. Cowart, D. C. Coyne, R. Coyne, J. D. E. Creighton,\n  T. D. Creighton, J. Cripe, M. Croquette, S. G. Crowder, T. J. Cullen, A.\n  Cumming, L. Cunningham, E. Cuoco, T. Dal Canton, G. D\\'alya, B. D'Angelo, S.\n  L. Danilishin, S. D'Antonio, K. Danzmann, A. Dasgupta, C. F. Da Silva Costa,\n  L. E. H. Datrier, V. Dattilo, I. Dave, M. Davier, D. Davis, E. J. Daw, D.\n  DeBra, M. Deenadayalan, J. Degallaix, M. De Laurentis, S. Del\\'eglise, W. Del\n  Pozzo, L. M. DeMarchi, N. Demos, T. Dent, R. De Pietri, R. De Rosa, C. De\n  Rossi, R. DeSalvo, O. de Varona, S. Dhurandhar, M. C. D\\'iaz, T. Dietrich, L.\n  Di Fiore, C. DiFronzo, C. Di Giorgio, F. Di Giovanni, M. Di Giovanni, T. Di\n  Girolamo, A. Di Lieto, B. Ding, S. Di Pace, I. Di Palma, F. Di Renzo, A. K.\n  Divakarla, A. Dmitriev, Z. Doctor, F. Donovan, K. L. Dooley, S. Doravari, I.\n  Dorrington, T. P. Downes, M. Drago, J. C. Driggers, Z. Du, J.-G. Ducoin, P.\n  Dupej, O. Durante, S. E. Dwyer, P. J. Easter, G. Eddolls, T. B. Edo, A.\n  Effler, P. Ehrens, J. Eichholz, S. S. Eikenberry, M. Eisenmann, R. A.\n  Eisenstein, L. Errico, R. C. Essick, H. Estelles, D. Estevez, Z. B. Etienne,\n  T. Etzel, M. Evans, T. M. Evans, V. Fafone, S. Fairhurst, X. Fan, S. Farinon,\n  B. Farr, W. M. Farr, E. J. Fauchon-Jones, M. Favata, M. Fays, M. Fazio, C.\n  Fee, J. Feicht, M. M. Fejer, F. Feng, A. Fernandez-Galiana, I. Ferrante, E.\n  C. Ferreira, T. A. Ferreira, F. Fidecaro, I. Fiori, D. Fiorucci, M. Fishbach,\n  R. P. Fisher, J. M. Fishner, R. Fittipaldi, M. Fitz-Axen, V. Fiumara, R.\n  Flaminio, M. Fletcher, E. Floden, E. Flynn, H. Fong, J. A. Font, P. W. F.\n  Forsyth, J.-D. Fournier, Francisco Hernandez Vivanco, S. Frasca, F. Frasconi,\n  Z. Frei, A. Freise, R. Frey, V. Frey, P. Fritschel, V. V. Frolov, G.\n  Fronz\\`e, P. Fulda, M. Fyffe, H. A. Gabbard, B. U. Gadre, S. M. Gaebel, J. R.\n  Gair, L. Gammaitoni, S. G. Gaonkar, C. Garc\\'ia-Quir\\'os, F. Garufi, B.\n  Gateley, S. Gaudio, G. Gaur, V. Gayathri, G. Gemme, E. Genin, A. Gennai, D.\n  George, J. George, L. Gergely, S. Ghonge, Abhirup Ghosh, Archisman Ghosh, S.\n  Ghosh, B. Giacomazzo, J. A. Giaime, K. D. Giardina, D. R. Gibson, K. Gill, L.\n  Glover, J. Gniesmer, P. Godwin, E. Goetz, R. Goetz, B. Goncharov, G.\n  Gonz\\'alez, J. M. Gonzalez Castro, A. Gopakumar, S. E. Gossan, M. Gosselin,\n  R. Gouaty, B. Grace, A. Grado, M. Granata, A. Grant, S. Gras, P. Grassia, C.\n  Gray, R. Gray, G. Greco, A. C. Green, R. Green, E. M. Gretarsson, A.\n  Grimaldi, S. J. Grimm, P. Groot, H. Grote, S. Grunewald, P. Gruning, G. M.\n  Guidi, H. K. Gulati, Y. Guo, A. Gupta, Anchal Gupta, P. Gupta, E. K.\n  Gustafson, R. Gustafson, L. Haegel, O. Halim, B. R. Hall, E. D. Hall, E. Z.\n  Hamilton, G. Hammond, M. Haney, M. M. Hanke, J. Hanks, C. Hanna, M. D.\n  Hannam, O. A. Hannuksela, T. J. Hansen, J. Hanson, T. Harder, T. Hardwick, K.\n  Haris, J. Harms, G. M. Harry, I. W. Harry, R. K. Hasskew, C. J. Haster, K.\n  Haughian, F. J. Hayes, J. Healy, A. Heidmann, M. C. Heintze, H. Heitmann, F.\n  Hellman, P. Hello, G. Hemming, M. Hendry, I. S. Heng, J. Hennig, M. Heurs, S.\n  Hild, T. Hinderer, S. Hochheim, D. Hofman, A. M. Holgado, N. A. Holland, K.\n  Holt, D. E. Holz, P. Hopkins, C. Horst, J. Hough, E. J. Howell, C. G. Hoy, Y.\n  Huang, M. T. H\\\"ubner, E. A. Huerta, D. Huet, B. Hughey, V. Hui, S. Husa, S.\n  H. Huttner, T. Huynh-Dinh, B. Idzkowski, A. Iess, H. Inchauspe, C. Ingram, R.\n  Inta, G. Intini, B. Irwin, H. N. Isa, J.-M. Isac, M. Isi, B. R. Iyer, T.\n  Jacqmin, S. J. Jadhav, K. Jani, N. N. Janthalur, P. Jaranowski, D. Jariwala,\n  A. C. Jenkins, J. Jiang, D. S. Johnson, A. W. Jones, D. I. Jones, J. D.\n  Jones, R. Jones, R. J. G. Jonker, L. Ju, J. Junker, C. V. Kalaghatgi, V.\n  Kalogera, B. Kamai, S. Kandhasamy, G. Kang, J. B. Kanner, S. J. Kapadia, S.\n  Karki, R. Kashyap, M. Kasprzack, S. Katsanevas, E. Katsavounidis, W. Katzman,\n  S. Kaufer, K. Kawabe, N. V. Keerthana, F. K\\'ef\\'elian, D. Keitel, R.\n  Kennedy, J. S. Key, F. Y. Khalili, I. Khan, S. Khan, E. A. Khazanov, N.\n  Khetan, M. Khursheed, N. Kijbunchoo, Chunglee Kim, J. C. Kim, K. Kim, W. Kim,\n  W. S. Kim, Y.-M. Kim, C. Kimball, P. J. King, M. Kinley-Hanlon, R. Kirchhoff,\n  J. S. Kissel, L. Kleybolte, J. H. Klika, S. Klimenko, T. D. Knowles, P. Koch,\n  S. M. Koehlenbeck, G. Koekoek, S. Koley, V. Kondrashov, A. Kontos, N. Koper,\n  M. Korobko, W. Z. Korth, M. Kovalam, D. B. Kozak, C. Kr\\\"amer, V. Kringel, N.\n  Krishnendu, A. Kr\\'olak, N. Krupinski, G. Kuehn, A. Kumar, P. Kumar, Rahul\n  Kumar, Rakesh Kumar, L. Kuo, A. Kutynia, S. Kwang, B. D. Lackey, D. Laghi, K.\n  H. Lai, T. L. Lam, M. Landry, B. B. Lane, R. N. Lang, J. Lange, B. Lantz, R.\n  K. Lanza, A. Lartaux-Vollard, P. D. Lasky, M. Laxen, A. Lazzarini, C.\n  Lazzaro, P. Leaci, S. Leavey, Y. K. Lecoeuche, C. H. Lee, H. K. Lee, H. M.\n  Lee, H. W. Lee, J. Lee, K. Lee, J. Lehmann, A. K. Lenon, N. Leroy, N.\n  Letendre, Y. Levin, A. Li, J. Li, K. J. L. Li, T. G. F. Li, X. Li, F. Lin, F.\n  Linde, S. D. Linker, T. B. Littenberg, J. Liu, X. Liu, M. Llorens-Monteagudo,\n  R. K. L. Lo, L. T. London, A. Longo, M. Lorenzini, V. Loriette, M. Lormand,\n  G. Losurdo, J. D. Lough, C. O. Lousto, G. Lovelace, M. E. Lower, H. L\\\"uck,\n  D. Lumaca, A. P. Lundgren, R. Lynch, Y. Ma, R. Macas, S. Macfoy, M. MacInnis,\n  D. M. Macleod, A. Macquet, I. Maga\\~na Hernandez, F. Maga\\~na-Sandoval, R. M.\n  Magee, E. Majorana, I. Maksimovic, A. Malik, N. Man, V. Mandic, V. Mangano,\n  G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, F. Marchesoni, F. Marion,\n  S. M\\'arka, Z. M\\'arka, C. Markakis, A. S. Markosyan, A. Markowitz, E. Maros,\n  A. Marquina, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, V. Martinez,\n  D. V. Martynov, H. Masalehdan, K. Mason, E. Massera, A. Masserot, T. J.\n  Massinger, M. Masso-Reid, S. Mastrogiovanni, A. Matas, F. Matichard, L.\n  Matone, N. Mavalvala, J. J. McCann, R. McCarthy, D. E. McClelland, P.\n  McClincy, S. McCormick, L. McCuller, S. C. McGuire, C. McIsaac, J. McIver, D.\n  J. McManus, T. McRae, S. T. McWilliams, D. Meacher, G. D. Meadors, M. Mehmet,\n  A. K. Mehta, J. Meidam, E. Mejuto Villa, A. Melatos, G. Mendell, R. A.\n  Mercer, L. Mereni, K. Merfeld, E. L. Merilh, M. Merzougui, S. Meshkov, C.\n  Messenger, C. Messick, F. Messina, R. Metzdorff, P. M. Meyers, F. Meylahn, A.\n  Miani, H. Miao, C. Michel, H. Middleton, L. Milano, A. L. Miller, M.\n  Millhouse, J. C. Mills, M. C. Milovich-Goff, O. Minazzoli, Y. Minenkov, A.\n  Mishkin, C. Mishra, T. Mistry, S. Mitra, V. P. Mitrofanov, G. Mitselmakher,\n  R. Mittleman, G. Mo, D. Moffa, K. Mogushi, S. R. P. Mohapatra, M.\n  Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, F. Morawski, G.\n  Moreno, S. Morisaki, B. Mours, C. M. Mow-Lowry, F. Muciaccia, Arunava\n  Mukherjee, D. Mukherjee, S. Mukherjee, Subroto Mukherjee, N. Mukund, A.\n  Mullavey, J. Munch, E. A. Mu\\~niz, M. Muratore, P. G. Murray, I. Nardecchia,\n  L. Naticchioni, R. K. Nayak, B. F. Neil, J. Neilson, G. Nelemans, T. J. N.\n  Nelson, M. Nery, A. Neunzert, L. Nevin, K. Y. Ng, S. Ng, C. Nguyen, P.\n  Nguyen, D. Nichols, S. A. Nichols, S. Nissanke, F. Nocera, C. North, L. K.\n  Nuttall, M. Obergaulinger, J. Oberling, B. D. O'Brien, G. Oganesyan, G. H.\n  Ogin, J. J. Oh, S. H. Oh, F. Ohme, H. Ohta, M. A. Okada, M. Oliver, P.\n  Oppermann, Richard J. Oram, B. O'Reilly, R. G. Ormiston, L. F. Ortega, R.\n  O'Shaughnessy, S. Ossokine, D. J. Ottaway, H. Overmier, B. J. Owen, A. E.\n  Pace, G. Pagano, M. A. Page, G. Pagliaroli, A. Pai, S. A. Pai, J. R. Palamos,\n  O. Palashov, C. Palomba, H. Pan, P. K. Panda, P. T. H. Pang, C. Pankow, F.\n  Pannarale, B. C. Pant, F. Paoletti, A. Paoli, A. Parida, W. Parker, D.\n  Pascucci, A. Pasqualetti, R. Passaquieti, D. Passuello, M. Patil, B.\n  Patricelli, E. Payne, B. L. Pearlstone, T. C. Pechsiri, A. J. Pedersen, M.\n  Pedraza, R. Pedurand, A. Pele, S. Penn, A. Perego, C. J. Perez, C.\n  P\\'erigois, A. Perreca, J. Petermann, H. P. Pfeiffer, M. Phelps, K. S.\n  Phukon, O. J. Piccinni, M. Pichot, F. Piergiovanni, V. Pierro, G. Pillant, L.\n  Pinard, I. M. Pinto, M. Pirello, M. Pitkin, W. Plastino, R. Poggiani, D. Y.\n  T. Pong, S. Ponrathnam, P. Popolizio, E. K. Porter, J. Powell, A. K.\n  Prajapati, J. Prasad, K. Prasai, R. Prasanna, G. Pratten, T. Prestegard, M.\n  Principe, G. A. Prodi, L. Prokhorov, M. Punturo, P. Puppo, M. P\\\"urrer, H.\n  Qi, V. Quetschke, P. J. Quinonez, F. J. Raab, G. Raaijmakers, H. Radkins, N.\n  Radulesco, P. Raffai, S. Raja, C. Rajan, B. Rajbhandari, M. Rakhmanov, K. E.\n  Ramirez, A. Ramos-Buades, Javed Rana, K. Rao, P. Rapagnani, V. Raymond, M.\n  Razzano, J. Read, T. Regimbau, L. Rei, S. Reid, D. H. Reitze, P. Rettegno, F.\n  Ricci, C. J. Richardson, J. W. Richardson, P. M. Ricker, G. Riemenschneider,\n  K. Riles, M. Rizzo, N. A. Robertson, F. Robinet, A. Rocchi, L. Rolland, J. G.\n  Rollins, V. J. Roma, M. Romanelli, R. Romano, C. L. Romel, J. H. Romie, C. A.\n  Rose, D. Rose, K. Rose, D. Rosi\\'nska, S. G. Rosofsky, M. P. Ross, S. Rowan,\n  A. R\\\"udiger, P. Ruggi, G. Rutins, K. Ryan, S. Sachdev, T. Sadecki, M.\n  Sakellariadou, O. S. Salafia, L. Salconi, M. Saleem, A. Samajdar, L. Sammut,\n  E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, K. A. Santiago,\n  E. Santos, N. Sarin, B. Sassolas, B. S. Sathyaprakash, O. Sauter, R. L.\n  Savage, P. Schale, M. Scheel, J. Scheuer, P. Schmidt, R. Schnabel, R. M. S.\n  Schofield, A. Sch\\\"onbeck, E. Schreiber, B. W. Schulte, B. F. Schutz, J.\n  Scott, S. M. Scott, E. Seidel, D. Sellers, A. S. Sengupta, N. Sennett, D.\n  Sentenac, V. Sequino, A. Sergeev, Y. Setyawati, D. A. Shaddock, T. Shaffer,\n  M. S. Shahriar, M. B. Shaner, A. Sharma, P. Sharma, P. Shawhan, H. Shen, R.\n  Shink, D. H. Shoemaker, D. M. Shoemaker, K. Shukla, S. ShyamSundar, K.\n  Siellez, M. Sieniawska, D. Sigg, L. P. Singer, D. Singh, N. Singh, A.\n  Singhal, A. M. Sintes, S. Sitmukhambetov, V. Skliris, B. J. J. Slagmolen, T.\n  J. Slaven-Blair, J. R. Smith, R. J. E. Smith, S. Somala, E. J. Son, S. Soni,\n  B. Sorazu, F. Sorrentino, T. Souradeep, E. Sowell, A. P. Spencer, M. Spera,\n  A. K. Srivastava, V. Srivastava, K. Staats, C. Stachie, M. Standke, D. A.\n  Steer, M. Steinke, J. Steinlechner, S. Steinlechner, D. Steinmeyer, S. P.\n  Stevenson, D. Stocks, R. Stone, D. J. Stops, K. A. Strain, G. Stratta, S. E.\n  Strigin, A. Strunk, R. Sturani, A. L. Stuver, V. Sudhir, T. Z. Summerscales,\n  L. Sun, S. Sunil, A. Sur, J. Suresh, P. J. Sutton, B. L. Swinkels, M. J.\n  Szczepa\\'nczyk, M. Tacca, S. C. Tait, C. Talbot, D. B. Tanner, D. Tao, M.\n  T\\'apai, A. Tapia, J. D. Tasson, R. Taylor, R. Tenorio, L. Terkowski, M.\n  Thomas, P. Thomas, S. R. Thondapu, K. A. Thorne, E. Thrane, Shubhanshu\n  Tiwari, Srishti Tiwari, V. Tiwari, K. Toland, M. Tonelli, Z. Tornasi, A.\n  Torres-Forn\\'e, C. I. Torrie, D. T\\\"oyr\\\"a, F. Travasso, G. Traylor, M. C.\n  Tringali, A. Tripathee, A. Trovato, L. Trozzo, K. W. Tsang, M. Tse, R. Tso,\n  L. Tsukada, D. Tsuna, T. Tsutsui, D. Tuyenbayev, K. Ueno, D. Ugolini, C. S.\n  Unnikrishnan, A. L. Urban, S. A. Usman, H. Vahlbruch, G. Vajente, G. Valdes,\n  M. Valentini, N. van Bakel, M. van Beuzekom, J. F. J. van den Brand, C. Van\n  Den Broeck, D. C. Vander-Hyde, L. van der Schaaf, J. V. VanHeijningen, A. A.\n  van Veggel, M. Vardaro, V. Varma, S. Vass, M. Vas\\'uth, A. Vecchio, G.\n  Vedovato, J. Veitch, P. J. Veitch, K. Venkateswara, G. Venugopalan, D.\n  Verkindt, F. Vetrano, A. Vicer\\'e, A. D. Viets, S. Vinciguerra, D. J. Vine,\n  J.-Y. Vinet, S. Vitale, T. Vo, H. Vocca, C. Vorvick, S. P. Vyatchanin, A. R.\n  Wade, L. E. Wade, M. Wade, R. Walet, M. Walker, L. Wallace, S. Walsh, H.\n  Wang, J. Z. Wang, S. Wang, W. H. Wang, Y. F. Wang, R. L. Ward, Z. A. Warden,\n  J. Warner, M. Was, J. Watchi, B. Weaver, L.-W. Wei, M. Weinert, A. J.\n  Weinstein, R. Weiss, F. Wellmann, L. Wen, E. K. Wessel, P. We{\\ss}els, J. W.\n  Westhouse, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, D. M. Wilken,\n  D. Williams, A. R. Williamson, J. L. Willis, B. Willke, W. Winkler, C. C.\n  Wipf, H. Wittel, G. Woan, J. Woehler, J. K. Wofford, J. L. Wright, D. S. Wu,\n  D. M. Wysocki, S. Xiao, R. Xu, H. Yamamoto, C. C. Yancey, L. Yang, Y. Yang,\n  Z. Yang, M. J. Yap, M. Yazback, D. W. Yeeles, Hang Yu, Haocun Yu, S. H. R.\n  Yuen, A. K. Zadro\\.zny, A. Zadro\\.zny, M. Zanolin, T. Zelenova, J.-P. Zendri,\n  M. Zevin, J. Zhang, L. Zhang, T. Zhang, C. Zhao, G. Zhao, M. Zhou, Z. Zhou,\n  X. J. Zhu, A. B. Zimmerman, M. E. Zucker, J. Zweizig, and S. Shandera", "docs_id": "1904.08976", "section": ["astro-ph.CO", "astro-ph.HE", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search for sub-solar mass ultracompact binaries in Advanced LIGO's\n  second observing run. We present an Advanced LIGO and Advanced Virgo search for sub-solar mass ultracompact objects in data obtained during Advanced LIGO's second observing run. In contrast to a previous search of Advanced LIGO data from the first observing run, this search includes the effects of component spin on the gravitational waveform. We identify no viable gravitational wave candidates consistent with sub-solar mass ultracompact binaries with at least one component between 0.2 - 1.0 solar masses. We use the null result to constrain the binary merger rate of (0.2 solar mass, 0.2 solar mass) binaries to be less than 3.7 x 10^5 Gpc^-3 yr^-1 and the binary merger rate of (1.0 solar mass, 1.0 solar mass) binaries to be less than 5.2 x 10^3 Gpc^-3 yr^-1. Sub-solar mass ultracompact objects are not expected to form via known stellar evolution channels, though it has been suggested that primordial density fluctuations or particle dark matter with cooling mechanisms and/or nuclear interactions could form black holes with sub-solar masses. Assuming a particular primordial black hole formation model, we constrain a population of merging 0.2 solar mass black holes to account for less than 16% of the dark matter density and a population of merging 1.0 solar mass black holes to account for less than 2% of the dark matter density. We discuss how constraints on the merger rate and dark matter fraction may be extended to arbitrary black hole population models that predict sub-solar mass binaries."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: The authors of the study on sub-solar mass ultracompact binaries in Advanced LIGO's second observing run constrain the binary merger rate of (0.2 solar mass, 0.2 solar mass) binaries to be less than 3.7 x 10^5 Gpc^-3 yr^-1. However, they also discuss the possibility of primordial black hole formation models that predict sub-solar mass binaries. Assuming a particular primordial black hole formation model, what is the maximum allowed fraction of dark matter density that can be accounted for by merging 0.2 solar mass black holes?\n\nA) 10%\nB) 16%\nC) 20%\nD) 25%\n\nCorrect Answer: B) 16%\n\nExplanation: According to the text, the authors constrain a population of merging 0.2 solar mass black holes to account for less than 16% of the dark matter density. This is the correct answer. The other options are incorrect because they are either too high (A, C, D) or not mentioned in the text (B is mentioned, but as an upper limit, not a maximum allowed fraction)."}, "50": {"documentation": {"title": "Conditional t-SNE: Complementary t-SNE embeddings through factoring out\n  prior information", "source": "Bo Kang, Dar\\'io Garc\\'ia Garc\\'ia, Jefrey Lijffijt, Ra\\'ul\n  Santos-Rodr\\'iguez, Tijl De Bie", "docs_id": "1905.10086", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conditional t-SNE: Complementary t-SNE embeddings through factoring out\n  prior information. Dimensionality reduction and manifold learning methods such as t-Distributed Stochastic Neighbor Embedding (t-SNE) are routinely used to map high-dimensional data into a 2-dimensional space to visualize and explore the data. However, two dimensions are typically insufficient to capture all structure in the data, the salient structure is often already known, and it is not obvious how to extract the remaining information in a similarly effective manner. To fill this gap, we introduce \\emph{conditional t-SNE} (ct-SNE), a generalization of t-SNE that discounts prior information from the embedding in the form of labels. To achieve this, we propose a conditioned version of the t-SNE objective, obtaining a single, integrated, and elegant method. ct-SNE has one extra parameter over t-SNE; we investigate its effects and show how to efficiently optimize the objective. Factoring out prior knowledge allows complementary structure to be captured in the embedding, providing new insights. Qualitative and quantitative empirical results on synthetic and (large) real data show ct-SNE is effective and achieves its goal."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the Conditional t-SNE (ct-SNE) method, and how does it differ from traditional t-SNE in terms of incorporating prior information?\n\nA) To reduce the dimensionality of data while preserving the global structure, and to discount prior information by ignoring labels.\nB) To reduce the dimensionality of data while preserving the local structure, and to incorporate prior information by using labels to guide the embedding.\nC) To reduce the dimensionality of data while preserving the global structure, and to discount prior information by factoring out known prior knowledge.\nD) To reduce the dimensionality of data while preserving the local structure, and to incorporate prior information by using labels to guide the embedding.\n\nCorrect Answer: C) To reduce the dimensionality of data while preserving the global structure, and to discount prior information by factoring out known prior knowledge.\n\nExplanation: The correct answer is C) because the question asks about the primary goal of ct-SNE, which is to reduce dimensionality while preserving the global structure of the data, and to discount prior information by factoring out known prior knowledge. The other options are incorrect because they either describe traditional t-SNE (A and B) or describe a different aspect of ct-SNE (D). \n\nNote: The question requires the test-taker to understand the main idea of the ct-SNE method and its differences from traditional t-SNE, which is a key concept in the provided documentation. The correct answer is not explicitly stated in the documentation, but can be inferred from the description of the method."}, "51": {"documentation": {"title": "Learning in Gated Neural Networks", "source": "Ashok Vardhan Makkuva, Sewoong Oh, Sreeram Kannan, Pramod Viswanath", "docs_id": "1906.02777", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning in Gated Neural Networks. Gating is a key feature in modern neural networks including LSTMs, GRUs and sparsely-gated deep neural networks. The backbone of such gated networks is a mixture-of-experts layer, where several experts make regression decisions and gating controls how to weigh the decisions in an input-dependent manner. Despite having such a prominent role in both modern and classical machine learning, very little is understood about parameter recovery of mixture-of-experts since gradient descent and EM algorithms are known to be stuck in local optima in such models. In this paper, we perform a careful analysis of the optimization landscape and show that with appropriately designed loss functions, gradient descent can indeed learn the parameters accurately. A key idea underpinning our results is the design of two {\\em distinct} loss functions, one for recovering the expert parameters and another for recovering the gating parameters. We demonstrate the first sample complexity results for parameter recovery in this model for any algorithm and demonstrate significant performance gains over standard loss functions in numerical experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in parameter recovery of mixture-of-experts layers in gated neural networks, and how does the proposed approach address this challenge?\n\nA) The primary challenge is the non-convexity of the optimization landscape, and the proposed approach uses a different initialization scheme to escape local optima. However, this approach may not be effective for all types of data.\n\nB) The primary challenge is the difficulty in recovering the gating parameters, and the proposed approach uses a two-stage optimization process to recover both the expert parameters and the gating parameters simultaneously. This approach is particularly effective for sparse data.\n\nC) The primary challenge is the instability of gradient descent in mixture-of-experts models, and the proposed approach uses a regularization technique to stabilize the optimization process. However, this approach may not be effective for large-scale datasets.\n\nD) The primary challenge is the lack of understanding of the optimization landscape, and the proposed approach uses a novel loss function design to recover both the expert parameters and the gating parameters accurately. This approach is particularly effective for a wide range of data distributions.\n\n**Correct Answer:** D) The primary challenge is the lack of understanding of the optimization landscape, and the proposed approach uses a novel loss function design to recover both the expert parameters and the gating parameters accurately. This approach is particularly effective for a wide range of data distributions.\n\n**Explanation:** The correct answer is D) because the paper states that \"very little is understood about parameter recovery of mixture-of-experts\" and that the proposed approach addresses this challenge by designing two distinct loss functions for recovering the expert parameters and the gating parameters. The paper also mentions that the approach is particularly effective for a wide range of data distributions, which is in line with the correct answer. The other options are incorrect because they either focus on specific aspects of the optimization process (A and C) or make an incorrect assumption about the primary challenge (B)."}, "52": {"documentation": {"title": "Integrating Holographic Vector Dominance to Hidden Local Symmetry for\n  the Nucleon Form Factor", "source": "Masayasu Harada and Mannque Rho", "docs_id": "1102.5489", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Integrating Holographic Vector Dominance to Hidden Local Symmetry for\n  the Nucleon Form Factor. We derive a two-parameter formula for the electromagnetic form factors of the nucleon described as an instanton by \"integrating out\" all KK modes other than the lowest mesons from the infinite-tower of vector mesons in holographic QCD while preserving hidden local symmetry for the resultant vector fields. With only two parameters, the proton Sachs form factors can be fit surprisingly well to the available experimental data for momentum transfers $Q^2\\lsim 0.5$ GeV$^2$ with $\\chi^2$/dof $\\lsim 2$. We interpret this agreement as indicating the importance of an infinite tower in the soliton structure of the nucleon. The prediction of the Sakai-Sugimoto holographic dual model is checked against the fit values to assess its accuracy in describing the proton structure. We find that the structure of the \"core\" of roughly 1/3 in the proton size indicated in experiments and commonly associated with an intrinsic quark-gluon structure in QCD is \"hidden\" in the infinite tower in the holographic model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary implication of the agreement between the two-parameter formula for the electromagnetic form factors of the nucleon and the available experimental data, as described in the Arxiv documentation?\n\nA) The infinite tower of vector mesons is not necessary for describing the nucleon structure.\nB) The Sakai-Sugimoto holographic dual model is not accurate in describing the proton structure.\nC) The \"core\" of the proton size is not hidden in the infinite tower of vector mesons.\nD) The agreement between the formula and experimental data indicates that the infinite tower of vector mesons is essential for describing the nucleon structure.\n\nCorrect Answer: D) The agreement between the formula and experimental data indicates that the infinite tower of vector mesons is essential for describing the nucleon structure.\n\nExplanation: The correct answer is D because the agreement between the two-parameter formula and the experimental data suggests that the infinite tower of vector mesons is crucial for describing the nucleon structure, as indicated by the fact that the proton Sachs form factors can be fit surprisingly well to the available experimental data. This implies that the \"core\" of the proton size, associated with an intrinsic quark-gluon structure in QCD, is \"hidden\" in the infinite tower of vector mesons in the holographic model."}, "53": {"documentation": {"title": "Higgs Production with a Central Jet Veto at NNLL+NNLO", "source": "Carola F. Berger, Claudio Marcantonini, Iain W. Stewart, Frank\n  J.Tackmann and Wouter J. Waalewijn", "docs_id": "1012.4480", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Higgs Production with a Central Jet Veto at NNLL+NNLO. A major ingredient in Higgs searches at the Tevatron and LHC is the elimination of backgrounds with jets. In current H -> WW -> lnulnu searches, jet algorithms are used to veto central jets to obtain a 0-jet sample, which is then analyzed to discover the Higgs signal. Imposing this tight jet veto induces large double logarithms which significantly modify the Higgs production cross section. These jet-veto logarithms are presently only accounted for at fixed order or with the leading-logarithmic summation from parton-shower Monte Carlos. Here we consider Higgs production with an inclusive event-shape variable for the jet veto, namely beam thrust Tau_cm, which has a close correspondence with a traditional p_T jet veto. Tau_cm allows us to systematically sum the large jet-veto logarithms to higher orders and to provide better estimates for theoretical uncertainties. We present results for the 0-jet Higgs production cross section from gluon fusion at next-to-next-to-leading-logarithmic order (NNLL), fully incorporating fixed-order results at next-to-next-to-leading order (NNLO). At this order the scale uncertainty is 15-20%, depending on the cut, implying that a larger scale uncertainty should be used in current Tevatron bounds on the Higgs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Higgs production with a central jet veto at NNLL+NNLO, what is the primary advantage of using the beam thrust Tau_cm as an inclusive event-shape variable for the jet veto, and how does this impact the theoretical uncertainties in the 0-jet Higgs production cross section?\n\nA) Tau_cm allows for a more precise definition of the jet veto, reducing theoretical uncertainties by 10%.\nB) Tau_cm enables the systematic summation of large jet-veto logarithms to higher orders, reducing theoretical uncertainties by 20%.\nC) Tau_cm introduces a new source of uncertainty in the jet veto, increasing theoretical uncertainties by 15%.\nD) Tau_cm is equivalent to a traditional p_T jet veto, with no impact on theoretical uncertainties.\n\nCorrect Answer: B) Tau_cm enables the systematic summation of large jet-veto logarithms to higher orders, reducing theoretical uncertainties by 20%.\n\nExplanation: The correct answer is B) Tau_cm enables the systematic summation of large jet-veto logarithms to higher orders, reducing theoretical uncertainties by 20%. This is because Tau_cm allows for the inclusion of higher-order corrections to the jet veto, which are currently only accounted for at fixed order or with leading-logarithmic summation from parton-shower Monte Carlos. By systematically summing these logarithms, Tau_cm provides a more accurate estimate of the theoretical uncertainties in the 0-jet Higgs production cross section."}, "54": {"documentation": {"title": "Optimal Tracking Portfolio with A Ratcheting Capital Benchmark", "source": "Lijun Bo, Huafu Liao, Xiang Yu", "docs_id": "2006.13661", "section": ["q-fin.PM", "math.OC", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Tracking Portfolio with A Ratcheting Capital Benchmark. This paper studies the finite horizon portfolio management by optimally tracking a ratcheting capital benchmark process. It is assumed that the fund manager can dynamically inject capital into the portfolio account such that the total capital dominates a non-decreasing benchmark floor process at each intermediate time. The tracking problem is formulated to minimize the cost of accumulated capital injection. We first transform the original problem with floor constraints into an unconstrained control problem, however, under a running maximum cost. By identifying a controlled state process with reflection, the problem is further shown to be equivalent to an auxiliary problem, which leads to a nonlinear Hamilton-Jacobi-Bellman (HJB) equation with a Neumann boundary condition. By employing the dual transform, the probabilistic representation and some stochastic flow analysis, the existence of the unique classical solution to the HJB equation is established. The verification theorem is carefully proved, which gives the complete characterization of the feedback optimal portfolio. The application to market index tracking is also discussed when the index process is modeled by a geometric Brownian motion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Optimal Tracking Portfolio with A Ratcheting Capital Benchmark paper, what is the underlying assumption about the fund manager's ability to dynamically inject capital into the portfolio account?\n\nA) The fund manager can only inject capital at the beginning of each period.\nB) The fund manager can inject capital at any time, but the total capital must always dominate the benchmark floor process.\nC) The fund manager can only inject capital when the benchmark process is increasing.\nD) The fund manager can inject capital at any time, but the total capital must always be equal to the benchmark process.\n\n**Correct Answer:** B) The fund manager can inject capital at any time, but the total capital must always dominate the benchmark floor process.\n\n**Explanation:** The paper states that the fund manager can dynamically inject capital into the portfolio account such that the total capital \"dominates a non-decreasing benchmark floor process at each intermediate time.\" This means that the fund manager can inject capital at any time, but the total capital must always be greater than or equal to the benchmark floor process. This assumption is crucial in formulating the tracking problem and solving the HJB equation.\n\n**Candidate A:** This option is incorrect because the paper does not restrict the fund manager's ability to inject capital to the beginning of each period.\n\n**Candidate B:** This is the correct answer. The paper assumes that the fund manager can inject capital at any time, but the total capital must always dominate the benchmark floor process.\n\n**Candidate C:** This option is incorrect because the paper does not restrict the fund manager's ability to inject capital to when the benchmark process is increasing.\n\n**Candidate D:** This option is incorrect because the paper assumes that the total capital must always dominate the benchmark floor process, not be equal to it."}, "55": {"documentation": {"title": "The Sun Diver: Combining solar sails with the Oberth effect", "source": "Coryn A.L. Bailer-Jones (Max Planck Institute for Astronomy,\n  Heidelberg)", "docs_id": "2009.12659", "section": ["physics.pop-ph", "astro-ph.IM", "physics.class-ph", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Sun Diver: Combining solar sails with the Oberth effect. A highly reflective sail provides a way to propel a spacecraft out of the solar system using solar radiation pressure. The closer the spacecraft is to the Sun when it starts its outward journey, the larger the radiation pressure and so the larger the final velocity. For a spacecraft starting on the Earth's orbit, closer proximity can be achieved via a retrograde impulse from a rocket engine. The sail is then deployed at the closest approach to the Sun. Employing the so-called Oberth effect, a second, prograde, impulse at closest approach will raise the final velocity further. Here I investigate how a fixed total impulse ({\\Delta}v) can best be distributed in this procedure to maximize the sail's velocity at infinity. Once {\\Delta}v exceeds a threshold that depends on the lightness number of the sail (a measure of its sun-induced acceleration), the best strategy is to use all of the {\\Delta}v in the retrograde impulse to dive as close as possible to the Sun. Below the threshold the best strategy is to use all of the {\\Delta}v in the prograde impulse and thus not to dive at all. Although larger velocities can be achieved with multi-stage impulsive transfers, this study shows some interesting and perhaps counter-intuitive consequences of combining impulses with solar sails."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A spacecraft is launched from Earth's orbit using a solar sail and the Oberth effect. The total impulse required to reach infinity is 10 km/s. The sail's lightness number is 0.5, which means that the sail's acceleration is limited to 0.5 km/s^2. If the spacecraft is to be propelled as close as possible to the Sun, what is the optimal distribution of the total impulse between the retrograde and prograde impulses?\n\nA) 5 km/s retrograde impulse and 5 km/s prograde impulse\nB) 10 km/s retrograde impulse and 0 km/s prograde impulse\nC) 0 km/s retrograde impulse and 10 km/s prograde impulse\nD) 0 km/s retrograde impulse and 5 km/s prograde impulse\n\n**Correct Answer:** B) 10 km/s retrograde impulse and 0 km/s prograde impulse\n\n**Explanation:** According to the documentation, if the total impulse exceeds a threshold that depends on the lightness number of the sail, the best strategy is to use all of the total impulse in the retrograde impulse to dive as close as possible to the Sun. Since the sail's lightness number is 0.5, the threshold is exceeded, and the optimal distribution of the total impulse is 10 km/s retrograde impulse and 0 km/s prograde impulse. This is because the retrograde impulse will provide the largest possible acceleration, while the prograde impulse will not be needed to achieve the required velocity at infinity."}, "56": {"documentation": {"title": "Computational Complexity of the Hylland-Zeckhauser Scheme for One-Sided\n  Matching Markets", "source": "Vijay V. Vazirani and Mihalis Yannakakis", "docs_id": "2004.01348", "section": ["cs.GT", "cs.CC", "econ.TH", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computational Complexity of the Hylland-Zeckhauser Scheme for One-Sided\n  Matching Markets. In 1979, Hylland and Zeckhauser \\cite{hylland} gave a simple and general scheme for implementing a one-sided matching market using the power of a pricing mechanism. Their method has nice properties -- it is incentive compatible in the large and produces an allocation that is Pareto optimal -- and hence it provides an attractive, off-the-shelf method for running an application involving such a market. With matching markets becoming ever more prevalant and impactful, it is imperative to finally settle the computational complexity of this scheme. We present the following partial resolution: 1. A combinatorial, strongly polynomial time algorithm for the special case of $0/1$ utilities. 2. An example that has only irrational equilibria, hence proving that this problem is not in PPAD. Furthermore, its equilibria are disconnected, hence showing that the problem does not admit a convex programming formulation. 3. A proof of membership of the problem in the class FIXP. We leave open the (difficult) question of determining if the problem is FIXP-hard. Settling the status of the special case when utilities are in the set $\\{0, {\\frac 1 2}, 1 \\}$ appears to be even more difficult."}, "answer": "Here is the exam question:\n\nQuestion: What is the computational complexity of the Hylland-Zeckhauser Scheme for One-Sided Matching Markets?\n\nA) P\nB) NP\nC) PPAD\nD) FIXP\n\nCorrect Answer: D) FIXP\n\nExplanation: The correct answer is FIXP (Fixed Point Computation) because the problem statement mentions that the special case of 0/1 utilities has a combinatorial, strongly polynomial time algorithm, and the problem is also shown to be in the class FIXP. The other options are incorrect because the problem is not in P (it's not known to be solvable in polynomial time), it's not in NP (it's not known to be in the polynomial hierarchy), and it's not in PPAD (the problem has irrational equilibria, which is not a characteristic of PPAD problems)."}, "57": {"documentation": {"title": "Using the Econometric Models for Identification of Risk Factors for\n  Albanian SMEs (Case study: SMEs of Gjirokastra region)", "source": "Lorenc Kociu and Kledian Kodra", "docs_id": "2101.03598", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using the Econometric Models for Identification of Risk Factors for\n  Albanian SMEs (Case study: SMEs of Gjirokastra region). Using the econometric models, this paper addresses the ability of Albanian Small and Medium-sized Enterprises (SMEs) to identify the risks they face. To write this paper, we studied SMEs operating in the Gjirokastra region. First, qualitative data gathered through a questionnaire was used. Next, the 5-level Likert scale was used to measure it. Finally, the data was processed through statistical software SPSS version 21, using the binary logistic regression model, which reveals the probability of occurrence of an event when all independent variables are included. Logistic regression is an integral part of a category of statistical models, which are called General Linear Models. Logistic regression is used to analyze problems in which one or more independent variables interfere, which influences the dichotomous dependent variable. In such cases, the latter is seen as the random variable and is dependent on them. To evaluate whether Albanian SMEs can identify risks, we analyzed the factors that SMEs perceive as directly affecting the risks they face. At the end of the paper, we conclude that Albanian SMEs can identify risk"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the study \"Using the Econometric Models for Identification of Risk Factors for Albanian SMEs (Case study: SMEs of Gjirokastra region)\", the authors used a 5-level Likert scale to measure the qualitative data gathered through a questionnaire. What type of statistical model was used to analyze the data, and what is its primary application in problems involving dichotomous dependent variables?\n\n**A)** General Linear Model (GLM) with Ordinary Least Squares (OLS)\n**B)** Binary Logistic Regression Model\n**C)** Generalized Linear Model (GLM) with Poisson Regression\n**D)** Non-Parametric Regression Model\n\n**Correct Answer:** B) Binary Logistic Regression Model\n\n**Explanation:** The correct answer is Binary Logistic Regression Model because the study mentions that the authors used the binary logistic regression model to analyze the data, which is a type of General Linear Model. Logistic regression is used to analyze problems in which one or more independent variables influence a dichotomous dependent variable, which is the case in this study. The other options are incorrect because they do not accurately describe the statistical model used in the study."}, "58": {"documentation": {"title": "Data Fitting with Signomial Programming Compatible Difference of Convex\n  Functions", "source": "Cody Karcher", "docs_id": "2110.12104", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data Fitting with Signomial Programming Compatible Difference of Convex\n  Functions. Signomial Programming (SP) has proven to be a powerful tool for engineering design optimization, striking a balance between the computational efficiency of Geometric Programming (GP) and the extensibility of more general optimization methods like Sequential Quadratic Programming (SQP). But when an existing engineering analysis tool is incompatible with the mathematics of the SP formulation, options are limited. Previous literature has suggested schemes for fitting GP compatible models to pre-computed data, but no methods have yet been proposed that take advantage of the increased modeling flexibility available in SP. This paper describes a new Soft Difference of Max Affine (SDMA) function class that is constructed from existing methods of GP compatible fitting and the theory of Difference of Convex (DC) functions. When a SDMA function is fit to data in log-log transformed space, it becomes either a signomial or a set of signomials upon inverse transformation. Three examples of fitting are presented here, including simple test cases in 2D and 3D, and a fit to the performance data of the NACA 24xx family of airfoils. In each case, RMS error is driven to less than 1%."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A new Soft Difference of Max Affine (SDMA) function class is proposed for engineering design optimization. The SDMA function is constructed from existing methods of Geometric Programming (GP) compatible fitting and the theory of Difference of Convex (DC) functions. When a SDMA function is fit to data in log-log transformed space, it becomes either a signomial or a set of signomials upon inverse transformation. What is the primary advantage of using the SDMA function class over traditional Geometric Programming (GP) methods for engineering design optimization?\n\n**A)** SDMA functions are more computationally efficient than GP functions.\n**B)** SDMA functions can handle more complex optimization problems than GP functions.\n**C)** SDMA functions can be transformed into signomials, allowing for more flexible modeling and easier implementation in existing engineering analysis tools.\n**D)** SDMA functions are more suitable for large-scale optimization problems than GP functions.\n\n**Correct Answer:** C) SDMA functions can be transformed into signomials, allowing for more flexible modeling and easier implementation in existing engineering analysis tools.\n\n**Explanation:** The correct answer is C) SDMA functions can be transformed into signomials, allowing for more flexible modeling and easier implementation in existing engineering analysis tools. This is because the SDMA function class can be transformed into signomials upon inverse transformation, which provides more flexibility in modeling and implementation compared to traditional GP methods. The other options are incorrect because they do not accurately reflect the primary advantage of using the SDMA function class. Option A is incorrect because SDMA functions may not necessarily be more computationally efficient than GP functions. Option B is incorrect because SDMA functions may not be able to handle more complex optimization problems than GP functions. Option D is incorrect because SDMA functions are not necessarily more suitable for large-scale optimization problems than GP functions."}, "59": {"documentation": {"title": "The Subleading Eikonal in Supergravity Theories", "source": "Arnau Koemans Collado, Paolo Di Vecchia, Rodolfo Russo, Steven Thomas", "docs_id": "1807.04588", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Subleading Eikonal in Supergravity Theories. In this paper we study the subleading contributions to eikonal scattering in (super)gravity theories with particular emphasis on the role of both elastic and inelastic scattering processes. For concreteness we focus on the scattering of various massless particles off a stack of D$p$-branes in type II supergravity in the limit of large impact parameter $b$. We analyse the relevant field theory Feynman diagrams which naturally give rise to both elastic and inelastic processes. We show that in the case analysed the leading and subleading eikonal only depend on elastic processes, while inelastic processes are captured by a pre-factor multiplying the exponentiated leading and subleading eikonal phase. In addition to the traditional Feynman diagram computations mentioned above, we also present a novel method for computing the amplitudes contributing to the leading and subleading eikonal phases, which, in the large $b$ limit, only involves knowledge of the onshell three and four-point vertices. The two methods are shown to give the same results. Furthermore we derive these results in yet another way, by computing various one-point amplitudes which allow us to extract the classical solution of the gravitational back reaction of the target D$p$-branes. Finally we show how our expressions for the leading and subleading eikonal agree with the calculation of the metric and corresponding deflection angle for massless states moving along geodesics in the relevant curved geometry."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the subleading eikonal in supergravity theories, what is the relationship between the leading and subleading eikonal phases, and how do inelastic processes affect the overall expression?\n\nA) The leading and subleading eikonal phases are independent of each other, and inelastic processes only affect the exponentiation of the leading eikonal phase.\n\nB) The leading and subleading eikonal phases are related by a factor that depends on the inelastic processes, and the subleading eikonal phase is captured by a pre-factor multiplying the exponentiated leading eikonal phase.\n\nC) The leading and subleading eikonal phases are identical, and inelastic processes only affect the classical solution of the gravitational back reaction of the target Dp-branes.\n\nD) The leading and subleading eikonal phases are related by a factor that depends on the large impact parameter b, and inelastic processes are not relevant in the large b limit.\n\nCorrect Answer: B) The leading and subleading eikonal phases are related by a factor that depends on the inelastic processes, and the subleading eikonal phase is captured by a pre-factor multiplying the exponentiated leading eikonal phase.\n\nExplanation: The correct answer is B) because the paper states that \"in the case analysed the leading and subleading eikonal only depend on elastic processes, while inelastic processes are captured by a pre-factor multiplying the exponentiated leading and subleading eikonal phase.\" This means that the subleading eikonal phase is related to the leading eikonal phase by a factor that depends on the inelastic processes, and not by an independent relationship."}}