{"0": {"documentation": {"title": "Sentiment Correlation in Financial News Networks and Associated Market\n  Movements", "source": "Xingchen Wan, Jie Yang, Slavi Marinov, Jan-Peter Calliess, Stefan\n  Zohren, Xiaowen Dong", "docs_id": "2011.06430", "section": ["cs.SI", "q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sentiment Correlation in Financial News Networks and Associated Market\n  Movements. In an increasingly connected global market, news sentiment towards one company may not only indicate its own market performance, but can also be associated with a broader movement on the sentiment and performance of other companies from the same or even different sectors. In this paper, we apply NLP techniques to understand news sentiment of 87 companies among the most reported on Reuters for a period of seven years. We investigate the propagation of such sentiment in company networks and evaluate the associated market movements in terms of stock price and volatility. Our results suggest that, in certain sectors, strong media sentiment towards one company may indicate a significant change in media sentiment towards related companies measured as neighbours in a financial network constructed from news co-occurrence. Furthermore, there exists a weak but statistically significant association between strong media sentiment and abnormal market return as well as volatility. Such an association is more significant at the level of individual companies, but nevertheless remains visible at the level of sectors or groups of companies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on Sentiment Correlation in Financial News Networks and Associated Market Movements. How do the results suggest that media sentiment towards one company can be associated with broader market movements, and what are the implications of this association for investors?\n\n**A)** The study suggests that media sentiment towards one company can be associated with broader market movements through the propagation of sentiment in company networks, which can lead to abnormal market returns and increased volatility. This association is significant at the level of individual companies, but also visible at the level of sectors or groups of companies.\n\n**B)** The study found no significant association between media sentiment and market movements, and instead suggests that media sentiment is a poor predictor of market performance. The propagation of sentiment in company networks is not a key factor in understanding market movements.\n\n**C)** The study suggests that media sentiment towards one company can be associated with broader market movements through the creation of \"sentiment bubbles\" that can lead to market crashes. This association is significant at the level of individual companies, but not visible at the level of sectors or groups of companies.\n\n**D)** The study found that media sentiment towards one company has no impact on market movements, and that market movements are instead driven by fundamental factors such as earnings reports and economic indicators.\n\n**Correct Answer:** A) The study suggests that media sentiment towards one company can be associated with broader market movements through the propagation of sentiment in company networks, which can lead to abnormal market returns and increased volatility. This association is significant at the level of individual companies, but also visible at the level of sectors or groups of companies.\n\n**Explanation:** The correct answer is A) because the study found a statistically significant association between strong media sentiment and abnormal market returns and volatility, both at the level of individual companies and at the level of sectors or groups of companies. The propagation of sentiment in company networks is a key factor in understanding this association, as it suggests that media sentiment can have a ripple effect on market movements."}, "1": {"documentation": {"title": "Inverted effective SUSY with combined Z' and gravity mediation, and muon\n  anomalous magnetic moment", "source": "Jihn E. Kim", "docs_id": "1208.5484", "section": ["hep-ph", "astro-ph.CO", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inverted effective SUSY with combined Z' and gravity mediation, and muon\n  anomalous magnetic moment. Effective supersymmetry(SUSY) where stop is the lightest squark may run into a two-loop tachyonic problem in some Z' mediation models. In addition, a large A term or/and a large stop mass are needed to have about a 126 GeV Higgs boson with three families of quarks and leptons. Thus, we suggest an inverted effective SUSY(IeffSUSY) where stop mass is larger compared to those of the first two families. In this case, it is possible to have a significant correction to the anomalous magnetic moment of muon. A three family IeffSUSY in a Z' mediation scenario is explicitly studied with the Z' quantum number related to B-L. Here, we adopt both the Z' mediation and gravity mediation where the Z' mediation is the dominant one for stop, while the gravity mediation is the dominant one for the muonic leptons and Higgs multiplets. We present a numerical study based on a specific anomaly free model, and show the existence of the parameter region where all the phenomenological conditions are satisfied."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Inverted Effective SUSY (IeffSUSY) with combined Z' and gravity mediation, what is the primary motivation for introducing a large stop mass, and how does it relate to the anomalous magnetic moment of the muon?\n\nA) To avoid a two-loop tachyonic problem in Z' mediation models, and to enhance the muon's anomalous magnetic moment.\nB) To satisfy the requirement for a 126 GeV Higgs boson with three families of quarks and leptons, and to mitigate the effects of a large A term.\nC) To address the issue of stop mass hierarchy in IeffSUSY models, and to provide a correction to the muon's anomalous magnetic moment.\nD) To reconcile the Z' mediation and gravity mediation scenarios, and to ensure the stability of the Higgs boson mass.\n\nCorrect Answer: B) To satisfy the requirement for a 126 GeV Higgs boson with three families of quarks and leptons, and to mitigate the effects of a large A term.\n\nExplanation: The correct answer is B) because the documentation states that a large A term or/and a large stop mass are needed to have a 126 GeV Higgs boson with three families of quarks and leptons. Additionally, the introduction of a large stop mass is motivated by the need to avoid a two-loop tachyonic problem in Z' mediation models. The other options are incorrect because they either misrepresent the motivation for introducing a large stop mass (A and C) or are unrelated to the specific phenomenological conditions (D)."}, "2": {"documentation": {"title": "Stable controllable giant vortex in a trapped Bose-Einstein condensate", "source": "S. K. Adhikari", "docs_id": "1906.11108", "section": ["cond-mat.quant-gas", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stable controllable giant vortex in a trapped Bose-Einstein condensate. In a harmonically-trapped rotating Bose-Einstein condensate (BEC), a vortex of large angular momentum decays to multiple vortices of unit angular momentum from an energetic consideration. We demonstrate the formation of a robust and dynamically stable giant vortex of large angular momentum in a harmonically trapped rotating BEC with a potential hill at the center, thus forming a Mexican hat like trapping potential. For a small inter-atomic interaction strength, a highly controllable stable giant vortex appears, whose angular momentum slowly increases as the angular frequency of rotation is increased. As the inter-atomic interaction strength is increased beyond a critical value, only vortices of unit angular momentum are formed, unless the strength of the potential hill at the center is also increased: for a stronger potential hill at the center a giant vortex is again formed. The dynamical stability of the giant vortex is demonstrated by real-time propagation numerically. These giant vortices of large angular momentum can be observed and studied experimentally in a highly controlled fashion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the effect of increasing the inter-atomic interaction strength on the formation of giant vortices in a harmonically-trapped rotating Bose-Einstein condensate?\n\nA) The giant vortex will remain stable and its angular momentum will increase.\nB) The giant vortex will decay into multiple vortices of unit angular momentum.\nC) The giant vortex will only form if the potential hill at the center is also increased.\nD) The giant vortex will disappear and only vortices of unit angular momentum will form.\n\n**Correct Answer:** B) The giant vortex will decay into multiple vortices of unit angular momentum.\n\n**Explanation:** According to the documentation, when the inter-atomic interaction strength is increased beyond a critical value, only vortices of unit angular momentum are formed, unless the strength of the potential hill at the center is also increased. This indicates that the giant vortex will decay into multiple vortices of unit angular momentum.\n\n**Explanation for incorrect options:**\n\nA) Incorrect because increasing the inter-atomic interaction strength does not necessarily lead to a stable giant vortex.\n\nC) Incorrect because the giant vortex will still form even if the potential hill at the center is not increased.\n\nD) Incorrect because the giant vortex will not disappear, but rather decay into multiple vortices of unit angular momentum."}, "3": {"documentation": {"title": "The effective QCD phase diagram and the critical end point", "source": "Alejandro Ayala, Adnan Bashir, J.J. Cobos-Martinez, Saul\n  Hernandez-Ortiz, Alfredo Raya", "docs_id": "1411.4953", "section": ["hep-ph", "hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The effective QCD phase diagram and the critical end point. We study the QCD phase diagram on the plane of temperature T and quark chemical potential mu, modelling the strong interactions with the linear sigma model coupled to quarks. The phase transition line is found from the effective potential at finite T and mu taking into accounts the plasma screening effects. We find the location of the critical end point (CEP) to be (mu^CEP/T_c,T^CEP/T_c) sim (1.2,0.8), where T_c is the (pseudo)critical temperature for the crossover phase transition at vanishing mu. This location lies within the region found by lattice inspired calculations. The results show that in the linear sigma model, the CEP's location in the phase diagram is expectedly determined solely through chiral symmetry breaking. The same is likely to be true for all other models which do not exhibit confinement, provided the proper treatment of the plasma infrared properties for the description of chiral symmetry restoration is implemented. Similarly, we also expect these corrections to be substantially relevant in the QCD phase diagram."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on the QCD phase diagram and the critical end point (CEP) propose that the CEP's location in the phase diagram is determined solely through chiral symmetry breaking. However, they also acknowledge that corrections to this result are likely to be relevant, particularly for models that do not exhibit confinement. What is the primary mechanism by which these corrections are expected to be relevant, according to the authors?\n\nA) Confinement of quarks\nB) Plasma infrared properties\nC) Chiral symmetry restoration\nD) Strong interactions between quarks and gluons\n\n**Correct Answer:** B) Plasma infrared properties\n\n**Explanation:** The authors mention that the corrections to the CEP's location are expected to be \"substantially relevant\" in the QCD phase diagram, and that they are likely to be relevant for models that do not exhibit confinement. This suggests that the corrections are related to the plasma infrared properties, which are a key aspect of the QCD phase diagram. The other options are not supported by the text, and are therefore incorrect."}, "4": {"documentation": {"title": "Future of work: ethics", "source": "David Pastor-Escuredo", "docs_id": "2104.02580", "section": ["cs.CY", "cs.AI", "cs.HC", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Future of work: ethics. Work must be reshaped in the upcoming new era characterized by new challenges and the presence of new technologies and computational tools. Over-automation seems to be the driver of the digitalization process. Substitution is the paradigm leading Artificial Intelligence and robotics development against human cognition. Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities. Digital technology is characterized also by scalability because of its easy and inexpensive deployment. Thus, automation can lead to the absence of jobs and scalable negative impact in human development and the performance of business. A look at digitalization from the lens of Sustainable Development Goals can tell us how digitalization impact in different sectors and areas considering society as a complex interconnected system. Here, reflections on how AI and Data impact future of work and sustainable development are provided grounded on an ethical core that comprises human-level principles and also systemic principles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the impact of digitalization on the future of work and sustainable development, considering the ethical implications of automation and the role of Artificial Intelligence and robotics in reshaping human cognition and capacities.\n\n**A)** Digitalization will lead to a significant increase in job opportunities, as automation will free humans from mundane tasks, allowing them to focus on creative and high-value work.\n\n**B)** The absence of jobs due to automation will have a negative impact on human development, as it will lead to increased poverty, inequality, and social unrest.\n\n**C)** Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities, but this should be achieved through a paradigm shift in the way we approach work and the economy, prioritizing human well-being and sustainable development.\n\n**D)** The scalability of digital technology is a major driver of automation, leading to a loss of jobs and a negative impact on human development, but this can be mitigated through the development of new industries and job opportunities that are created by digitalization.\n\n**Correct Answer:** C) Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities, but this should be achieved through a paradigm shift in the way we approach work and the economy, prioritizing human well-being and sustainable development.\n\n**Explanation:** This answer choice reflects the ethical core of the documentation, which emphasizes the need for digital technology to be designed to enhance human skills and capacities, rather than simply automating tasks. It also highlights the importance of prioritizing human well-being and sustainable development in the face of automation and digitalization. This answer choice requires the test-taker to think critically about the ethical implications of digitalization and its impact on the future of work and sustainable development."}, "5": {"documentation": {"title": "On the Human Control of a Multiple Quadcopters with a Cable-suspended\n  Payload System", "source": "Pratik Prajapati, Sagar Parekh, and Vineet Vashista", "docs_id": "2004.01841", "section": ["cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Human Control of a Multiple Quadcopters with a Cable-suspended\n  Payload System. A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a leader-follower architecture for controlling multiple quadcopters with a cable-suspended payload system, as presented in the paper \"On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System\"?\n\n**A)** To improve the stability of the system by reducing the number of control inputs required.\n**B)** To enable manual control for on-the-go maneuvering of the quadcopter-payload system in unknown environments.\n**C)** To increase the payload capacity of the quadcopters by adding more control inputs.\n**D)** To reduce the energy consumption of the quadcopters by optimizing their flight paths.\n\n**Correct Answer:** B) To enable manual control for on-the-go maneuvering of the quadcopter-payload system in unknown environments.\n\n**Explanation:** The correct answer is B) To enable manual control for on-the-go maneuvering of the quadcopter-payload system in unknown environments. The paper presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system, which motivates aerial transportation in the unknown environments. This is the primary motivation behind the development of the leader-follower architecture.\n\nThe other options are incorrect because:\n\n* A) The leader-follower architecture does not necessarily reduce the number of control inputs required.\n* C) The paper does not mention increasing the payload capacity of the quadcopters.\n* D) The paper does not discuss optimizing the flight paths to reduce energy consumption."}, "6": {"documentation": {"title": "One-loop weak corrections to Higgs production", "source": "Valentin Hirschi, Simone Lionetti, Armin Schweitzer", "docs_id": "1902.10167", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "One-loop weak corrections to Higgs production. We compute mixed QCD-weak corrections to inclusive Higgs production at the LHC from the partonic process $g g \\rightarrow H q \\bar{q}$. We start from the UV- and IR-finite one-loop weak amplitude and consider its interference with the corresponding one-loop QCD amplitude. This contribution is a $\\mathcal{O}(\\alpha_s\\alpha)$ correction to the leading-order gluon-fusion cross section, and was not numerically assessed in previous works. We also compute the cross section from the square of this weak amplitude, suppressed by $\\mathcal{O}(\\alpha^2)$. Finally, we consider contributions from the partonic process $g q \\rightarrow H q$, which are one order lower in $\\alpha_s$, as a reference for the size of terms which are not enhanced by the large gluon luminosity. We find that, given the magnitude of the uncertainties on current state-of-the-art predictions for Higgs production, all contributions computed in this work can be safely ignored, both fully inclusively and in the boosted Higgs regime. This result supports the approximate factorisation of QCD and weak corrections to that process."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of inclusive Higgs production at the LHC, what is the significance of the interference between the one-loop weak amplitude and the one-loop QCD amplitude, and how does this relate to the factorisation of QCD and weak corrections to the process?\n\nA) The interference term represents a $\\mathcal{O}(\\alpha_s\\alpha)$ correction to the leading-order gluon-fusion cross section, and its inclusion is crucial for accurate predictions.\n\nB) The interference term is negligible due to the large uncertainties in current state-of-the-art predictions for Higgs production, and its suppression by $\\mathcal{O}(\\alpha^2)$ is sufficient for approximating the cross section.\n\nC) The interference term is a $\\mathcal{O}(\\alpha_s\\alpha)$ correction to the cross section from the square of the weak amplitude, and its inclusion is necessary for a complete understanding of the process.\n\nD) The interference term is a $\\mathcal{O}(\\alpha_s\\alpha)$ correction to the cross section from the partonic process $g q \\rightarrow H q$, and its inclusion is sufficient for approximating the size of terms not enhanced by the large gluon luminosity.\n\nCorrect Answer: B) The interference term is negligible due to the large uncertainties in current state-of-the-art predictions for Higgs production, and its suppression by $\\mathcal{O}(\\alpha^2)$ is sufficient for approximating the cross section.\n\nExplanation: The correct answer is B) because the documentation states that \"given the magnitude of the uncertainties on current state-of-the-art predictions for Higgs production, all contributions computed in this work can be safely ignored, both fully inclusively and in the boosted Higgs regime.\" This implies that the interference term is negligible and its suppression by $\\mathcal{O}(\\alpha^2)$ is sufficient for approximating the cross section. The other options are incorrect because they either misrepresent the significance of the interference term or incorrectly state its relation to the factorisation of QCD and weak corrections."}, "7": {"documentation": {"title": "In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication", "source": "Takashi Nakamura, Valentin Goverdovsky, Danilo P. Mandic", "docs_id": "1705.03742", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication. The use of EEG as a biometrics modality has been investigated for about a decade, however its feasibility in real-world applications is not yet conclusively established, mainly due to the issues with collectability and reproducibility. To this end, we propose a readily deployable EEG biometrics system based on a `one-fits-all' viscoelastic generic in-ear EEG sensor (collectability), which does not require skilled assistance or cumbersome preparation. Unlike most existing studies, we consider data recorded over multiple recording days and for multiple subjects (reproducibility) while, for rigour, the training and test segments are not taken from the same recording days. A robust approach is considered based on the resting state with eyes closed paradigm, the use of both parametric (autoregressive model) and non-parametric (spectral) features, and supported by simple and fast cosine distance, linear discriminant analysis and support vector machine classifiers. Both the verification and identification forensics scenarios are considered and the achieved results are on par with the studies based on impractical on-scalp recordings. Comprehensive analysis over a number of subjects, setups, and analysis features demonstrates the feasibility of the proposed ear-EEG biometrics, and its potential in resolving the critical collectability, robustness, and reproducibility issues associated with current EEG biometrics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary challenges associated with the use of EEG as a biometrics modality, and how does the proposed system address these challenges?\n\n**A)** The primary challenges are the lack of standardization in EEG recordings and the need for skilled assistance to prepare the equipment. The proposed system addresses these challenges by using a generic in-ear EEG sensor and a simple preparation process.\n\n**B)** The primary challenges are the issues with collectability and reproducibility, and the proposed system addresses these challenges by using data recorded over multiple recording days and for multiple subjects, as well as a robust approach based on resting state with eyes closed paradigm.\n\n**C)** The primary challenges are the need for high-quality on-scalp recordings and the lack of robustness to noise and artifacts. The proposed system addresses these challenges by using a robust approach based on spectral features and simple classifiers.\n\n**D)** The primary challenges are the lack of standardization in EEG recordings and the need for specialized equipment. The proposed system addresses these challenges by using a generic in-ear EEG sensor and a simple preparation process.\n\n**Correct Answer:** B) The primary challenges are the issues with collectability and reproducibility, and the proposed system addresses these challenges by using data recorded over multiple recording days and for multiple subjects, as well as a robust approach based on resting state with eyes closed paradigm.\n\n**Explanation:** The correct answer is B) because the documentation states that the primary challenges associated with EEG biometrics are the issues with collectability and reproducibility. The proposed system addresses these challenges by using data recorded over multiple recording days and for multiple subjects, as well as a robust approach based on resting state with eyes closed paradigm. The other options are incorrect because they do not accurately summarize the challenges and solutions presented in the documentation."}, "8": {"documentation": {"title": "Effects of the direct light in the surface detectors (SD) of the Pierre\n  Auger Observatory and their change in time", "source": "Pedro Alfonso Valencia Esquipula, Karen Salom\\'e Caballero Mora", "docs_id": "1703.07422", "section": ["physics.ins-det", "astro-ph.HE", "astro-ph.IM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of the direct light in the surface detectors (SD) of the Pierre\n  Auger Observatory and their change in time. Cosmic Rays (CR) are particles which come to the earth from Universe. Their origin and production mechanisms are still unknown. The Pierre Auger Observatory is located in Mendoza, Argentina. It is dedicated to the study of CR. When CR arrive to the earth's atmosphere they produce a shower of secondary particles called \\textit{air shower}. The surface detector (SD) of the Pierre Auger Observatory consists of tanks full of pure water, where CR produce \\textit{Cherenkov radiation}, when going through them. This light is detected by three photomultiplier tubes (PMT) located on the top of each tank. Depending of the angle of arrival direction of the primary CR, each PMT is able to register different signal than the other. The goal of this study is to look at these effects of direct light on the PMT's to explore if they change in time. The obtained results may give information about the physical status of the tanks in order to monitor the work of the SD, and to estimate possible systematic effects on the measurements. The current results of this study are shown."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Pierre Auger Observatory's surface detector (SD) uses tanks filled with pure water to detect Cherenkov radiation produced by Cosmic Rays (CR) as they pass through. The detection system consists of three photomultiplier tubes (PMTs) located on the top of each tank, which register different signals depending on the angle of arrival direction of the primary CR. What is the primary purpose of monitoring the effects of direct light on the PMTs, as reported in the study mentioned in the documentation?\n\nA) To optimize the performance of the PMTs for better signal-to-noise ratio\nB) To estimate possible systematic effects on the measurements of CR energy and composition\nC) To investigate the physical status of the tanks and monitor the work of the SD\nD) To develop new algorithms for signal processing and data analysis\n\n**Correct Answer:** C) To investigate the physical status of the tanks and monitor the work of the SD\n\n**Explanation:** The study aims to explore the effects of direct light on the PMTs, which can provide information about the physical status of the tanks. This information can be used to monitor the work of the SD and estimate possible systematic effects on the measurements of CR energy and composition. The correct answer requires an understanding of the purpose of the study and the role of the PMTs in the detection system.\n\n**Additional explanations:**\n\n* Candidate A is incorrect because while optimizing the performance of the PMTs is an important aspect of the detection system, it is not the primary purpose of monitoring the effects of direct light.\n* Candidate B is incorrect because estimating systematic effects is a secondary goal of the study, but not the primary purpose.\n* Candidate D is incorrect because developing new algorithms for signal processing and data analysis is not related to the purpose of the study mentioned in the documentation."}, "9": {"documentation": {"title": "Performance Measurement for Deep Bayesian Neural Network", "source": "Yikuan Li and Yajie Zhu", "docs_id": "1903.08674", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Performance Measurement for Deep Bayesian Neural Network. Deep Bayesian neural network has aroused a great attention in recent years since it combines the benefits of deep neural network and probability theory. Because of this, the network can make predictions and quantify the uncertainty of the predictions at the same time, which is important in many life-threatening areas. However, most of the recent researches are mainly focusing on making the Bayesian neural network easier to train, and proposing methods to estimate the uncertainty. I notice there are very few works that properly discuss the ways to measure the performance of the Bayesian neural network. Although accuracy and average uncertainty are commonly used for now, they are too general to provide any insight information about the model. In this paper, we would like to introduce more specific criteria and propose several metrics to measure the model performance from different perspectives, which include model calibration measurement, data rejection ability and uncertainty divergence for samples from the same and different distributions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the limitations of using accuracy and average uncertainty as metrics to evaluate the performance of a Bayesian neural network, and what specific criteria and metrics are proposed in the paper to address these limitations?\n\n**A)** Accuracy and average uncertainty are sufficient metrics because they provide a general indication of the model's performance, and the proposed metrics are too complex to be practically applied.\n\n**B)** The paper proposes a new framework for evaluating the performance of Bayesian neural networks, which includes model calibration measurement, data rejection ability, and uncertainty divergence for samples from the same and different distributions.\n\n**C)** The limitations of accuracy and average uncertainty are that they are too general and do not provide any insight into the model's ability to quantify uncertainty, and the proposed metrics are too specific to be applicable to all types of Bayesian neural networks.\n\n**D)** The paper introduces a new metric, \"uncertainty divergence\", which measures the difference between the predicted uncertainty and the true uncertainty of the samples, and proposes a new framework for evaluating the performance of Bayesian neural networks.\n\n**Correct Answer:** C) The limitations of accuracy and average uncertainty are that they are too general and do not provide any insight into the model's ability to quantify uncertainty, and the proposed metrics are too specific to be applicable to all types of Bayesian neural networks.\n\n**Explanation:** The correct answer is C) because the paper highlights that accuracy and average uncertainty are too general to provide any insight into the model's performance, and that the proposed metrics are specific to certain aspects of Bayesian neural network performance, such as model calibration, data rejection ability, and uncertainty divergence. This requires the test-taker to understand the limitations of the current metrics and the new framework proposed in the paper."}, "10": {"documentation": {"title": "Hilbert's tenth problem, G\\\"odel's incompleteness, Halting problem, a\n  unifying perspective", "source": "Tarek Sayed Ahmed", "docs_id": "1812.00990", "section": ["math.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hilbert's tenth problem, G\\\"odel's incompleteness, Halting problem, a\n  unifying perspective. We formulate a property $P$ on a class of relations on the natural numbers, and formulate a general theorem on $P$, from which we get as corollaries the insolvability of Hilbert's tenth problem, G\\\"odel's incompleteness theorem, and Turing's halting problem. By slightly strengthening the property $P$, we get Tarski's definability theorem, namely that truth is not first order definable. The property $P$ together with a \"Cantor's diagonalization\" process emphasizes that all the above theorems are a variation on a theme, that of self reference and diagonalization combined. We relate our results to self referential paradoxes, including a formalisation of the Liar paradox, and fixed point theorems. We also discuss the property $P$ for arbitrary rings. We give a survey on Hilbert's tenth problem for quadratic rings and for the rationals pointing the way to ongoing research in main stream mathematics involving recursion theory, definability in model theory, algebraic geometry and number theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the common thread underlying Hilbert's tenth problem, G\\\"odel's incompleteness theorem, Turing's halting problem, and Tarski's definability theorem, and how do they relate to self-reference and diagonalization?\n\nA) They are all related to the concept of computability and the limits of formal systems, but they do not share a common underlying theme.\n\nB) They are all examples of undecidable problems, but they do not share a common underlying theme beyond their undecidability.\n\nC) They are all variations on the theme of self-reference and diagonalization, which is a fundamental concept in logic and mathematics that highlights the limitations of formal systems.\n\nD) They are all related to the concept of truth and definability, but they do not share a common underlying theme beyond their focus on truth and definability.\n\n**Correct Answer:** C) They are all variations on the theme of self-reference and diagonalization, which is a fundamental concept in logic and mathematics that highlights the limitations of formal systems.\n\n**Explanation:** The correct answer is C) because all four theorems mentioned in the question (Hilbert's tenth problem, G\\\"odel's incompleteness theorem, Turing's halting problem, and Tarski's definability theorem) can be seen as variations on the theme of self-reference and diagonalization. Self-reference refers to the ability of a system to refer to itself, while diagonalization refers to the process of constructing a new system that is designed to be inconsistent or incomplete. The theorems mentioned in the question all rely on self-reference and diagonalization in different ways, highlighting the limitations of formal systems and the importance of considering the limitations of logic and mathematics."}, "11": {"documentation": {"title": "Effective Description of Higher-Order Scalar-Tensor Theories", "source": "David Langlois, Michele Mancarella, Karim Noui, Filippo Vernizzi", "docs_id": "1703.03797", "section": ["hep-th", "astro-ph.CO", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effective Description of Higher-Order Scalar-Tensor Theories. Most existing theories of dark energy and/or modified gravity, involving a scalar degree of freedom, can be conveniently described within the framework of the Effective Theory of Dark Energy, based on the unitary gauge where the scalar field is uniform. We extend this effective approach by allowing the Lagrangian in unitary gauge to depend on the time derivative of the lapse function. Although this dependence generically signals the presence of an extra scalar degree of freedom, theories that contain only one propagating scalar degree of freedom, in addition to the usual tensor modes, can be constructed by requiring the initial Lagrangian to be degenerate. Starting from a general quadratic action, we derive the dispersion relations for the linear perturbations around Minkowski and a cosmological background. Our analysis directly applies to the recently introduced Degenerate Higher-Order Scalar-Tensor (DHOST) theories. For these theories, we find that one cannot recover a Poisson-like equation in the static linear regime except for the subclass that includes the Horndeski and so-called \"beyond Horndeski\" theories. We also discuss Lorentz-breaking models inspired by Horava gravity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary condition required for the linear perturbations around Minkowski and a cosmological background in Higher-Order Scalar-Tensor Theories to be described by a Poisson-like equation in the static linear regime?\n\nA) The Lagrangian must be degenerate.\nB) The Lagrangian must be non-degenerate.\nC) The scalar field must be uniform.\nD) The scalar field must be non-uniform.\n\nCorrect Answer: A) The Lagrangian must be degenerate.\n\nExplanation: According to the documentation, the primary condition required for the linear perturbations to be described by a Poisson-like equation is that the initial Lagrangian must be degenerate. This is because the presence of an extra scalar degree of freedom, which is allowed by the extension of the Effective Theory of Dark Energy, can be compensated by requiring the Lagrangian to be degenerate. This condition is specifically mentioned in the context of Higher-Order Scalar-Tensor Theories, including the Degenerate Higher-Order Scalar-Tensor (DHOST) theories."}, "12": {"documentation": {"title": "BARCHAN: Blob Alignment for Robust CHromatographic ANalysis", "source": "Camille Couprie, Laurent Duval, Maxime Moreaud, Sophie H\\'enon,\n  M\\'elinda Tebib, Vincent Souchon", "docs_id": "1702.07942", "section": ["cs.CV", "physics.data-an"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "BARCHAN: Blob Alignment for Robust CHromatographic ANalysis. Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role into the elucidation of complex samples. The automation of the identification of peak areas is of prime interest to obtain a fast and repeatable analysis of chromatograms. To determine the concentration of compounds or pseudo-compounds, templates of blobs are defined and superimposed on a reference chromatogram. The templates then need to be modified when different chromatograms are recorded. In this study, we present a chromatogram and template alignment method based on peak registration called BARCHAN. Peaks are identified using a robust mathematical morphology tool. The alignment is performed by a probabilistic estimation of a rigid transformation along the first dimension, and a non-rigid transformation in the second dimension, taking into account noise, outliers and missing peaks in a fully automated way. Resulting aligned chromatograms and masks are presented on two datasets. The proposed algorithm proves to be fast and reliable. It significantly reduces the time to results for GCxGC analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the BARCHAN algorithm in the context of comprehensive two-dimensional gas chromatography (GCxGC) analysis?\n\nA) To identify the compounds present in a sample by matching chromatograms\nB) To automate the identification of peak areas in chromatograms and determine the concentration of compounds or pseudo-compounds\nC) To apply a non-rigid transformation to the second dimension of chromatograms to account for noise and outliers\nD) To use a probabilistic estimation of a rigid transformation along the first dimension to align chromatograms\n\nCorrect Answer: B) To automate the identification of peak areas in chromatograms and determine the concentration of compounds or pseudo-compounds\n\nExplanation: The question requires the test-taker to understand the main objective of the BARCHAN algorithm, which is to automate the identification of peak areas in chromatograms and determine the concentration of compounds or pseudo-compounds. This requires an understanding of the context of GCxGC analysis and the role of the BARCHAN algorithm in this process.\n\nCandidate A is incorrect because while the BARCHAN algorithm does involve matching chromatograms, its primary goal is not to identify compounds present in a sample. Candidate C is incorrect because the non-rigid transformation is applied in the second dimension, not to account for noise and outliers. Candidate D is incorrect because the probabilistic estimation of a rigid transformation is applied along the first dimension, not to align chromatograms."}, "13": {"documentation": {"title": "Statistical properties of volatility return intervals of Chinese stocks", "source": "Fei Ren, Liang Guo, and Wei-Xing Zhou", "docs_id": "0807.1818", "section": ["q-fin.ST", "physics.data-an", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical properties of volatility return intervals of Chinese stocks. The statistical properties of the return intervals $\\tau_q$ between successive 1-min volatilities of 30 liquid Chinese stocks exceeding a certain threshold $q$ are carefully studied. The Kolmogorov-Smirnov (KS) test shows that 12 stocks exhibit scaling behaviors in the distributions of $\\tau_q$ for different thresholds $q$. Furthermore, the KS test and weighted KS test shows that the scaled return interval distributions of 6 stocks (out of the 12 stocks) can be nicely fitted by a stretched exponential function $f(\\tau/\\bar{\\tau})\\sim e^{- \\alpha (\\tau/\\bar{\\tau})^{\\gamma}}$ with $\\gamma\\approx0.31$ under the significance level of 5%, where $\\bar{\\tau}$ is the mean return interval. The investigation of the conditional probability distribution $P_q(\\tau | \\tau_0)$ and the mean conditional return interval $<\\tau| \\tau_0>$ demonstrates the existence of short-term correlation between successive return interval intervals. We further study the mean return interval $<\\tau| \\tau_0>$ after a cluster of $n$ intervals and the fluctuation $F(l)$ using detrended fluctuation analysis and find that long-term memory also exists in the volatility return intervals."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on the statistical properties of volatility return intervals of Chinese stocks reveals that 12 stocks exhibit scaling behaviors in the distributions of return intervals. However, the scaled return interval distributions of only 6 stocks can be fitted by a stretched exponential function. What can be inferred about the remaining 6 stocks that do not exhibit scaling behaviors?\n\nA) They have a different volatility profile compared to the scaling stocks.\nB) They have a higher threshold value q compared to the scaling stocks.\nC) They have a lower mean return interval compared to the scaling stocks.\nD) They have a different underlying mechanism driving their return intervals.\n\n**Correct Answer:** A) They have a different volatility profile compared to the scaling stocks.\n\n**Explanation:** The study shows that the scaled return interval distributions of 6 stocks can be fitted by a stretched exponential function, indicating that these stocks exhibit scaling behaviors. However, the remaining 6 stocks do not exhibit scaling behaviors, which suggests that they have a different volatility profile compared to the scaling stocks. This inference is based on the fact that the scaling behavior is a characteristic of the volatility profile, and the failure to exhibit scaling behavior implies a different underlying mechanism.\n\n**Additional Question:**\n\n**Question:** The study also investigates the conditional probability distribution Pq(\u03c4 | \u03c40) and the mean conditional return interval <\u03c4 | \u03c40>. What can be inferred about the existence of short-term correlation between successive return interval intervals?\n\nA) The study finds no evidence of short-term correlation.\nB) The study finds evidence of short-term correlation, but it is not statistically significant.\nC) The study finds evidence of short-term correlation, which is statistically significant.\nD) The study finds no evidence of short-term correlation, but it is statistically significant.\n\n**Correct Answer:** C) The study finds evidence of short-term correlation, which is statistically significant.\n\n**Explanation:** The study investigates the conditional probability distribution Pq(\u03c4 | \u03c40) and the mean conditional return interval <\u03c4 | \u03c40>, and finds that the mean conditional return interval is not equal to the mean return interval. This suggests that there is a short-term correlation between successive return interval intervals. Furthermore, the study finds that this correlation is statistically significant, indicating that the observed correlation is not due to chance."}, "14": {"documentation": {"title": "Uncovering fossils of the distant Milky Way with UNIONS: NGC 5466 and\n  its stellar stream", "source": "Jaclyn Jensen, Guillaume Thomas, Alan W. McConnachie, Else\n  Starkenburg, Khyati Malhan, Julio Navarro, Nicolas Martin, Benoit Famaey,\n  Rodrigo Ibata, Scott Chapman, Jean-Charles Cuillandre and Stephen Gwyn", "docs_id": "2108.04340", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uncovering fossils of the distant Milky Way with UNIONS: NGC 5466 and\n  its stellar stream. We examine the spatial clustering of blue horizontal branch (BHB) stars from the $\\textit{u}$-band of the Canada-France Imaging Survey (CFIS, a component of the Ultraviolet Near-Infrared Optical Northern Survey, or UNIONS). All major groupings of stars are associated with previously known satellites, and among these is NGC 5466, a distant (16 kpc) globular cluster. NGC 5466 reportedly possesses a long stellar stream, although no individual members of the stream have previously been identified. Using both BHBs and more numerous red giant branch stars cross-matched to $\\textit{Gaia}$ Data Release 2, we identify extended tidal tails from NGC 5466 that are both spatially and kinematically coherent. Interestingly, we find that this stream does not follow the same path as the previous detection at large distances from the cluster. We trace the stream across 31$^{\\circ}$ of sky and show that it exhibits a very strong distance gradient ranging from 10 $<$ R$_{helio}$ $<$ 30 kpc. We compare our observations to simple dynamical models of the stream and find that they are able to broadly reproduce the overall path and kinematics. The fact that NGC 5466 is so distant, traces a wide range of Galactic distances, has an identified progenitor, and appears to have recently had an interaction with the Galaxy's disk, makes it a unique test-case for dynamical modelling of the Milky Way."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the implications of the discovery of NGC 5466's stellar stream on our understanding of the Milky Way's structure and evolution.\n\n**A)** The discovery of NGC 5466's stellar stream suggests that the Milky Way's disk has undergone significant interactions with its satellite galaxies, leading to the formation of complex tidal tails.\n\n**B)** The fact that NGC 5466's stream exhibits a strong distance gradient implies that the stream is likely to be a remnant of a smaller satellite galaxy that was tidally disrupted by the Milky Way.\n\n**C)** The observation that NGC 5466's stream does not follow the same path as previous detections at large distances from the cluster suggests that the stream is likely to be a result of a recent interaction between the Milky Way and its disk.\n\n**D)** The identification of NGC 5466 as a unique test-case for dynamical modelling of the Milky Way highlights the importance of studying the properties of distant globular clusters in understanding the Galaxy's evolution.\n\n**Correct Answer:** D) The identification of NGC 5466 as a unique test-case for dynamical modelling of the Milky Way highlights the importance of studying the properties of distant globular clusters in understanding the Galaxy's evolution.\n\n**Explanation:** The correct answer is D) because the documentation highlights NGC 5466 as a unique case for studying the Milky Way's evolution due to its distance, identified progenitor, and recent interaction with the Galaxy's disk. This makes it an ideal candidate for dynamical modelling, allowing researchers to gain insights into the Galaxy's structure and evolution. The other options, while related to the topic, do not capture the essence of the discovery's implications as accurately as option D."}, "15": {"documentation": {"title": "Large-scale Sustainable Search on Unconventional Computing Hardware", "source": "Kirill P. Kalinin and Natalia G. Berloff", "docs_id": "2104.02553", "section": ["cond-mat.dis-nn", "cs.ET", "cs.IR", "physics.comp-ph", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Large-scale Sustainable Search on Unconventional Computing Hardware. Since the advent of the Internet, quantifying the relative importance of web pages is at the core of search engine methods. According to one algorithm, PageRank, the worldwide web structure is represented by the Google matrix, whose principal eigenvector components assign a numerical value to web pages for their ranking. Finding such a dominant eigenvector on an ever-growing number of web pages becomes a computationally intensive task incompatible with Moore's Law. We demonstrate that special-purpose optical machines such as networks of optical parametric oscillators, lasers, and gain-dissipative condensates, may aid in accelerating the reliable reconstruction of principal eigenvectors of real-life web graphs. We discuss the feasibility of simulating the PageRank algorithm on large Google matrices using such unconventional hardware. We offer alternative rankings based on the minimisation of spin Hamiltonians. Our estimates show that special-purpose optical machines may provide dramatic improvements in power consumption over classical computing architectures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the development of special-purpose optical machines for accelerating the reliable reconstruction of principal eigenvectors of real-life web graphs, and how do these machines compare to classical computing architectures in terms of power consumption?\n\nA) To improve the accuracy of PageRank algorithm by reducing computational complexity, but at the cost of increased power consumption.\nB) To minimize the computational complexity of the PageRank algorithm, thereby reducing the need for Moore's Law, and to achieve significant power consumption improvements over classical computing architectures.\nC) To enhance the scalability of web search engines by leveraging the parallel processing capabilities of optical parametric oscillators, but at the expense of increased power consumption.\nD) To develop alternative ranking methods based on spin Hamiltonians, which are not directly related to the PageRank algorithm or web graph reconstruction.\n\nCorrect Answer: B) To minimize the computational complexity of the PageRank algorithm, thereby reducing the need for Moore's Law, and to achieve significant power consumption improvements over classical computing architectures.\n\nExplanation: The correct answer is B because the question highlights the primary motivation behind the development of special-purpose optical machines, which is to accelerate the reliable reconstruction of principal eigenvectors of real-life web graphs. This is achieved by minimizing the computational complexity of the PageRank algorithm, thereby reducing the need for Moore's Law. Additionally, the question mentions that these machines provide \"dramatic improvements in power consumption\" over classical computing architectures, which is a key benefit of using special-purpose optical machines."}, "16": {"documentation": {"title": "A first look at browser-based Cryptojacking", "source": "Shayan Eskandari, Andreas Leoutsarakos, Troy Mursch, Jeremy Clark", "docs_id": "1803.02887", "section": ["cs.CR", "cs.CY", "cs.HC", "econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A first look at browser-based Cryptojacking. In this paper, we examine the recent trend towards in-browser mining of cryptocurrencies; in particular, the mining of Monero through Coinhive and similar code- bases. In this model, a user visiting a website will download a JavaScript code that executes client-side in her browser, mines a cryptocurrency, typically without her consent or knowledge, and pays out the seigniorage to the website. Websites may consciously employ this as an alternative or to supplement advertisement revenue, may offer premium content in exchange for mining, or may be unwittingly serving the code as a result of a breach (in which case the seigniorage is collected by the attacker). The cryptocurrency Monero is preferred seemingly for its unfriendliness to large-scale ASIC mining that would drive browser-based efforts out of the market, as well as for its purported privacy features. In this paper, we survey this landscape, conduct some measurements to establish its prevalence and profitability, outline an ethical framework for considering whether it should be classified as an attack or business opportunity, and make suggestions for the detection, mitigation and/or prevention of browser-based mining for non- consenting users."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is a primary motivation for websites to employ browser-based Cryptojacking, and how does this relate to the cryptocurrency Monero?\n\nA) To increase user engagement and encourage sharing on social media\nB) To supplement advertisement revenue and capitalize on the profitability of Monero mining\nC) To enhance the user experience by providing premium content in exchange for mining\nD) To drive users to download and install Monero mining software on their devices\n\nCorrect Answer: B) To supplement advertisement revenue and capitalize on the profitability of Monero mining\n\nExplanation: According to the paper, websites may employ browser-based Cryptojacking as an alternative or to supplement advertisement revenue, as it can be a profitable way to mine Monero. This is a key motivation for websites to engage in this practice, and it highlights the potential for Cryptojacking to be both a business opportunity and a security threat."}, "17": {"documentation": {"title": "Identification of Peer Effects with Miss-specified Peer Groups: Missing\n  Data and Group Uncertainty", "source": "Christiern Rose", "docs_id": "2104.10365", "section": ["econ.EM", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification of Peer Effects with Miss-specified Peer Groups: Missing\n  Data and Group Uncertainty. We consider identification of peer effects under peer group miss-specification. Our model of group miss-specification allows for missing data and peer group uncertainty. Missing data can take the form of some individuals being entirely absent from the data, and the researcher need not have any information on these individuals and may not even know that they are missing. We show that peer effects are nevertheless identifiable under mild restrictions on the probabilities of observing individuals, and propose a GMM estimator to estimate the peer effects. In practice this means that the researcher need only have access to an individual/household level sample with group identifiers. The researcher may also be uncertain as to what is the relevant peer group for the outcome under study. We show that peer effects are nevertheless identifiable provided that the candidate peer groups are nested within one another (e.g. classroom, grade, school) and propose a non-linear least squares estimator. We conduct a Monte-Carlo experiment to demonstrate our identification results and the performance of the proposed estimators in a setting tailored to real data (the Dartmouth room-mate data)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of identifying peer effects under peer group miss-specification, what is the key assumption that allows for the identification of peer effects even when there is missing data and uncertainty about the relevant peer group?\n\n**A)** The researcher must have access to a complete individual/household level sample with group identifiers.\n**B)** The candidate peer groups must be nested within one another (e.g. classroom, grade, school).\n**C)** The probabilities of observing individuals must be strictly positive.\n**D)** The researcher must have prior knowledge of the missing individuals and their characteristics.\n\n**Correct Answer:** B) The candidate peer groups must be nested within one another (e.g. classroom, grade, school).\n\n**Explanation:** The correct answer is B) because the key assumption is that the candidate peer groups are nested within one another, which allows for the identification of peer effects even when there is missing data and uncertainty about the relevant peer group. This assumption is crucial in ensuring that the peer effects are not confounded by the uncertainty about the peer group. The other options are incorrect because they do not capture the essence of the assumption required for identification under peer group miss-specification. Option A is incorrect because it is not a necessary assumption for identification. Option C is incorrect because it is a mild restriction on the probabilities of observing individuals, but it is not the key assumption required for identification. Option D is incorrect because it is not a relevant assumption for identification under peer group miss-specification."}, "18": {"documentation": {"title": "Measurement of the Decays $\\boldsymbol{B\\to\\eta\\ell\\nu_\\ell}$ and\n  $\\boldsymbol{B\\to\\eta^\\prime\\ell\\nu_\\ell}$ in Fully Reconstructed Events at\n  Belle", "source": "Belle Collaboration: C. Bele\\~no, J. Dingfelder, P. Urquijo, H.\n  Aihara, S. Al Said, D. M. Asner, T. Aushev, R. Ayad, V. Babu, I. Badhrees, A.\n  M. Bakich, V. Bansal, P. Behera, B. Bhuyan, J. Biswal, A. Bobrov, M.\n  Bra\\v{c}ko, T. E. Browder, D. \\v{C}ervenkov, A. Chen, B. G. Cheon, R.\n  Chistov, S.-K. Choi, Y. Choi, D. Cinabro, N. Dash, S. Di Carlo, Z.\n  Dole\\v{z}al, S. Eidelman, H. Farhat, J. E. Fast, T. Ferber, A. Frey, B. G.\n  Fulsom, V. Gaur, N. Gabyshev, A. Garmash, R. Gillard, P. Goldenzweig, T.\n  Hara, H. Hayashii, M. T. Hedges, W.-S. Hou, T. Iijima, K. Inami, G. Inguglia,\n  A. Ishikawa, R. Itoh, Y. Iwasaki, H. B. Jeon, Y. Jin, D. Joffe, K. K. Joo, K.\n  H. Kang, G. Karyan, D. Y. Kim, J. B. Kim, K. T. Kim, M. J. Kim, Y. J. Kim, K.\n  Kinoshita, P. Kody\\v{s}, S. Korpar, D. Kotchetkov, P. Kri\\v{z}an, R.\n  Kulasiri, I. S. Lee, Y. Li, L. Li Gioi, J. Libby, D. Liventsev, M. Lubej, T.\n  Luo, M. Masuda, T. Matsuda, D. Matvienko, K. Miyabayashi, H. Miyata, H. K.\n  Moon, T. Mori, E. Nakano, M. Nakao, T. Nanut, K. J. Nath, M. Nayak, S.\n  Nishida, S. Ogawa, S. Okuno, H. Ono, B. Pal, C.-S. Park, C. W. Park, H. Park,\n  T. K. Pedlar, R. Pestotnik, L. E. Piilonen, M. Ritter, Y. Sakai, M. Salehi,\n  S. Sandilya, T. Sanuki, O. Schneider, G. Schnell, C. Schwanda, Y. Seino, K.\n  Senyo, O. Seon, M. E. Sevior, V. Shebalin, T.-A. Shibata, J.-G. Shiu, F.\n  Simon, E. Solovieva, M. Stari\\v{c}, T. Sumiyoshi, M. Takizawa, U. Tamponi, K.\n  Tanida, F. Tenchini, M. Uchida, T. Uglov, Y. Unno, S. Uno, Y. Usov, C. Van\n  Hulse, G. Varner, K. E. Varvell, A. Vinokurova, V. Vorobyev, C. H. Wang,\n  M.-Z. Wang, P. Wang, Y. Watanabe, E. Widmann, E. Won, Y. Yamashita, H. Ye, J.\n  Yelton, Y. Yook, Z. P. Zhang, V. Zhilich, V. Zhukova, V. Zhulanov, A. Zupanc", "docs_id": "1703.10216", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of the Decays $\\boldsymbol{B\\to\\eta\\ell\\nu_\\ell}$ and\n  $\\boldsymbol{B\\to\\eta^\\prime\\ell\\nu_\\ell}$ in Fully Reconstructed Events at\n  Belle. We report branching fraction measurements of the decays $B^+\\to\\eta\\ell^+\\nu_\\ell$ and $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ based on 711~fb$^{-1}$ of data collected near the $\\Upsilon(4S)$ resonance with the Belle experiment at the KEKB asymmetric-energy $e^+e^-$ collider. This data sample contains 772 million $B\\bar B$~events. One of the two $B$~mesons is fully reconstructed in a hadronic decay mode. Among the remaining (\"signal-$B$\") daughters, we search for the $\\eta$~meson in two decay channels, $\\eta\\to\\gamma\\gamma$ and $\\eta\\to\\pi^+\\pi^-\\pi^0$, and reconstruct the $\\eta^{\\prime}$~meson in $\\eta^\\prime\\to\\eta\\pi^+\\pi^-$ with subsequent decay of the $\\eta$ into $\\gamma\\gamma$. Combining the two $\\eta$ modes and using an extended maximum likelihood, the $B^+\\to\\eta\\ell^+\\nu_\\ell$ branching fraction is measured to be $(4.2\\pm 1.1 (\\rm stat.)\\pm 0.3 (\\rm syst.))\\times 10^{-5}$. For $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$, we observe no significant signal and set an upper limit of $0.72\\times 10^{-4}$ at 90\\% confidence level."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of the Belle experiment's measurement of the decays $B^+\\to\\eta\\ell^+\\nu_\\ell$ and $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$, what is the significance of the observed absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay, and how does this impact the determination of the branching fraction for this process?\n\nA) The absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay indicates that the $\\eta^\\prime$ meson is not a viable candidate for the $\\ell\\nu_\\ell$ final state, and therefore the branching fraction is zero. This is because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which does not contain a lepton.\n\nB) The absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay indicates that the $\\eta^\\prime$ meson is not a viable candidate for the $\\ell\\nu_\\ell$ final state, and therefore the branching fraction is zero. This is because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which does not contain a lepton. Furthermore, the $\\eta$ meson decays into $\\gamma\\gamma$, which also does not contain a lepton, making it impossible for the $\\eta^\\prime$ meson to decay into a lepton.\n\nC) The absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay indicates that the $\\eta^\\prime$ meson is a viable candidate for the $\\ell\\nu_\\ell$ final state, and therefore the branching fraction is non-zero. This is because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which contains a lepton, and the $\\eta$ meson decays into $\\gamma\\gamma$, which also contains a lepton.\n\nD) The absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay indicates that the $\\eta^\\prime$ meson is not a viable candidate for the $\\ell\\nu_\\ell$ final state, and therefore the branching fraction is zero. This is because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which does not contain a lepton, and the $\\eta$ meson decays into $\\pi^+\\pi^-\\pi^0$, which also does not contain a lepton.\n\n**Correct Answer:** B) The absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay indicates that the $\\eta^\\prime$ meson is not a viable candidate for the $\\ell\\nu_\\ell$ final state, and therefore the branching fraction is zero. This is because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which does not contain a lepton. Furthermore, the $\\eta$ meson decays into $\\gamma\\gamma$, which also does not contain a lepton, making it impossible for the $\\eta^\\prime$ meson to decay into a lepton.\n\n**Explanation:** The correct answer is B) because the $\\eta^\\prime$ meson decays primarily into $\\eta\\pi^+\\pi^-$, which does not contain a lepton, and the $\\eta$ meson decays into $\\gamma\\gamma$, which also does not contain a lepton. This makes it impossible for the $\\eta^\\prime$ meson to decay into a lepton, resulting in an absence of a significant signal in the $B^+\\to\\eta^\\prime\\ell^+\\nu_\\ell$ decay."}, "19": {"documentation": {"title": "A Comparison of Hybrid and End-to-End Models for Syllable Recognition", "source": "Sebastian P. Bayerl and Korbinian Riedhammer", "docs_id": "1909.12232", "section": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Comparison of Hybrid and End-to-End Models for Syllable Recognition. This paper presents a comparison of a traditional hybrid speech recognition system (kaldi using WFST and TDNN with lattice-free MMI) and a lexicon-free end-to-end (TensorFlow implementation of multi-layer LSTM with CTC training) models for German syllable recognition on the Verbmobil corpus. The results show that explicitly modeling prior knowledge is still valuable in building recognition systems. With a strong language model (LM) based on syllables, the structured approach significantly outperforms the end-to-end model. The best word error rate (WER) regarding syllables was achieved using kaldi with a 4-gram LM, modeling all syllables observed in the training set. It achieved 10.0% WER w.r.t. the syllables, compared to the end-to-end approach where the best WER was 27.53%. The work presented here has implications for building future recognition systems that operate independent of a large vocabulary, as typically used in a tasks such as recognition of syllabic or agglutinative languages, out-of-vocabulary techniques, keyword search indexing and medical speech processing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare the performance of the hybrid speech recognition system using kaldi with a 4-gram LM versus the end-to-end model using multi-layer LSTM with CTC training for German syllable recognition on the Verbmobil corpus. Which approach achieves a better word error rate (WER) regarding syllables?\n\n**A)** The end-to-end model with multi-layer LSTM and CTC training achieves a WER of 10.0% for syllables, outperforming the hybrid system.\n\n**B)** The hybrid system using kaldi with a 4-gram LM achieves a WER of 10.0% for syllables, significantly outperforming the end-to-end model.\n\n**C)** The end-to-end model with multi-layer LSTM and CTC training achieves a WER of 10.0% for syllables, while the hybrid system achieves a WER of 27.53%.\n\n**D)** The hybrid system using kaldi with a 4-gram LM achieves a WER of 27.53% for syllables, while the end-to-end model achieves a WER of 10.0%.\n\n**Correct Answer:** B) The hybrid system using kaldi with a 4-gram LM achieves a WER of 10.0% for syllables, significantly outperforming the end-to-end model.\n\n**Explanation:** The correct answer is B) because the hybrid system using kaldi with a 4-gram LM achieves a WER of 10.0% for syllables, which is significantly better than the end-to-end model with multi-layer LSTM and CTC training, which achieves a WER of 27.53%. This indicates that explicitly modeling prior knowledge using a 4-gram LM is valuable in building recognition systems, and the structured approach significantly outperforms the end-to-end model."}, "20": {"documentation": {"title": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment", "source": "Abigail Copiaco, Christian Ritz, Stefano Fasciani, Nidhal Abdulaziz", "docs_id": "2104.13423", "section": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment. Access to informative databases is a crucial part of notable research developments. In the field of domestic audio classification, there have been significant advances in recent years. Although several audio databases exist, these can be limited in terms of the amount of information they provide, such as the exact location of the sound sources, and the associated noise levels. In this work, we detail our approach on generating an unbiased synthetic domestic audio database, consisting of sound scenes and events, emulated in both quiet and noisy environments. Data is carefully curated such that it reflects issues commonly faced in a dementia patients environment, and recreate scenarios that could occur in real-world settings. Similarly, the room impulse response generated is based on a typical one-bedroom apartment at Hebrew SeniorLife Facility. As a result, we present an 11-class database containing excerpts of clean and noisy signals at 5-seconds duration each, uniformly sampled at 16 kHz. Using our baseline model using Continues Wavelet Transform Scalograms and AlexNet, this yielded a weighted F1-score of 86.24 percent."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the creation of the DASEE synthetic database of domestic acoustic scenes and events in dementia patients' environments, and how does it address the limitations of existing audio databases?\n\n**A)** To provide a comprehensive dataset for audio classification research, focusing on the emotional and cognitive aspects of dementia patients' experiences.\n**B)** To develop a realistic and unbiased representation of domestic audio environments, emulating the complexities of real-world settings and addressing the lack of detailed information in existing databases.\n**C)** To create a large-scale audio database for speech recognition applications, prioritizing the accuracy of speech recognition models over other aspects of audio classification.\n**D)** To investigate the effects of noise levels on audio classification performance, using a controlled environment to isolate the impact of noise on model accuracy.\n\n**Correct Answer:** B) To develop a realistic and unbiased representation of domestic audio environments, emulating the complexities of real-world settings and addressing the lack of detailed information in existing databases.\n\n**Explanation:** The correct answer is B) because the documentation states that the DASEE database is designed to provide an unbiased synthetic domestic audio database, emulating real-world settings and addressing the limitations of existing databases. The database is curated to reflect issues commonly faced in dementia patients' environments and recreate scenarios that could occur in real-world settings. This motivation is in line with the goals of the DASEE project, as stated in the introduction of the documentation."}, "21": {"documentation": {"title": "Elastic turbulence in curvilinear flows of polymer solutions", "source": "Alexander Groisman and Victor Steinberg", "docs_id": "nlin/0401006", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Elastic turbulence in curvilinear flows of polymer solutions. Following our first report (A. Groisman and V. Steinberg, $\\sl Nature$ $\\bf 405$, 53 (2000)) we present an extended account of experimental observations of elasticity induced turbulence in three different systems: a swirling flow between two plates, a Couette-Taylor (CT) flow between two cylinders, and a flow in a curvilinear channel (Dean flow). All three set-ups had high ratio of width of the region available for flow to radius of curvature of the streamlines. The experiments were carried out with dilute solutions of high molecular weight polyacrylamide in concentrated sugar syrups. High polymer relaxation time and solution viscosity ensured prevalence of non-linear elastic effects over inertial non-linearity, and development of purely elastic instabilities at low Reynolds number (Re) in all three flows. Above the elastic instability threshold, flows in all three systems exhibit features of developed turbulence. Those include: (i)randomly fluctuating fluid motion excited in a broad range of spatial and temporal scales; (ii) significant increase in the rates of momentum and mass transfer (compared to those expected for a steady flow with a smooth velocity profile). Phenomenology, driving mechanisms, and parameter dependence of the elastic turbulence are compared with those of the conventional high Re hydrodynamic turbulence in Newtonian fluids."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary driving mechanism behind the development of elastic turbulence in curvilinear flows of polymer solutions, as observed in the experiments described in the paper?\n\nA) Inertial non-linearity due to high Reynolds number\nB) Non-linear elastic effects due to high polymer relaxation time and solution viscosity\nC) Viscous dissipation due to the presence of concentrated sugar syrups\nD) Thermal fluctuations due to the high molecular weight polyacrylamide\n\n**A)** Non-linear elastic effects due to high polymer relaxation time and solution viscosity\n**B)** Inertial non-linearity due to high Reynolds number\n**C)** Viscous dissipation due to the presence of concentrated sugar syrups\n**D)** Thermal fluctuations due to the high molecular weight polyacrylamide\n\n**Correct Answer:** **B)** Inertial non-linearity due to high Reynolds number\n\n**Explanation:** The paper states that \"high polymer relaxation time and solution viscosity ensured prevalence of non-linear elastic effects over inertial non-linearity\" in all three flows. This suggests that inertial non-linearity, which is typically associated with high Reynolds numbers, is not the primary driving mechanism behind the development of elastic turbulence in these systems. Instead, the non-linear elastic effects due to the high polymer relaxation time and solution viscosity are the dominant factor."}, "22": {"documentation": {"title": "Fermionic phases and their transitions induced by competing finite-range\n  interactions", "source": "Marcin Szyniszewski, Henning Schomerus", "docs_id": "1808.02715", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fermionic phases and their transitions induced by competing finite-range\n  interactions. We identify ground states of one-dimensional fermionic systems subject to competing repulsive interactions of finite range, and provide phenomenological and fundamental signatures of these phases and their transitions. Commensurable particle densities admit multiple competing charge-ordered insulating states with various periodicities and internal structure. Our reference point are systems with interaction range $p=2$, where phase transitions between these charge-ordered configurations are known to be mediated by liquid and bond-ordered phases. For increased interaction range $p=4$, we find that the phase transitions can also appear to be abrupt, as well as being mediated by re-emergent ordered phases that cross over into liquid behavior. These considerations are underpinned by a classification of the competing charge-ordered states in the atomic limit for varying interaction range at the principal commensurable particle densities. We also consider the effects of disorder, leading to fragmentization of the ordered phases and localization of the liquid phases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of one-dimensional fermionic systems with competing repulsive interactions of finite range, what is the primary mechanism by which phase transitions between competing charge-ordered insulating states occur, and how do these transitions change with increasing interaction range?\n\n**A)** The phase transitions are mediated by the emergence of re-emergent ordered phases that cross over into liquid behavior, and this mechanism is independent of the interaction range.\n\n**B)** The phase transitions are abrupt and mediated by liquid and bond-ordered phases, which are specific to systems with interaction range $p=2$.\n\n**C)** The phase transitions are mediated by the fragmentization of ordered phases due to disorder, and this mechanism is applicable to all interaction ranges.\n\n**D)** The phase transitions are mediated by the re-emergence of liquid phases, which is a universal mechanism that occurs for all interaction ranges.\n\n**Correct Answer:** B) The phase transitions are abrupt and mediated by liquid and bond-ordered phases, which are specific to systems with interaction range $p=2$.\n\n**Explanation:** The correct answer is B) because the documentation states that for interaction range $p=2$, phase transitions between competing charge-ordered configurations are known to be mediated by liquid and bond-ordered phases. However, for increased interaction range $p=4$, the phase transitions can also appear abrupt, and are mediated by re-emergent ordered phases that cross over into liquid behavior. This suggests that the mechanism of phase transitions changes with increasing interaction range, and that the correct answer is specific to the case of $p=2$."}, "23": {"documentation": {"title": "Interaction energy and itinerant ferromagnetism in a strongly\n  interacting Fermi gas in the absence of molecule formation", "source": "Lianyi He", "docs_id": "1405.5242", "section": ["cond-mat.quant-gas", "cond-mat.str-el", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interaction energy and itinerant ferromagnetism in a strongly\n  interacting Fermi gas in the absence of molecule formation. We investigate the interaction energy and the possibility of itinerant ferromagnetism in a strongly interacting Fermi gas at zero temperature in the absence of molecule formation. The interaction energy is obtained by summing the perturbative contributions of Galitskii-Feynman type to all orders in the gas parameter. It can be expressed by a simple phase space integral of an in-medium scattering phase shift. In both three and two dimensions (3D and 2D), the interaction energy shows a maximum before reaching the resonance from the Bose-Einstein condensate side, which provides a possible explanation of the experimental measurements of the interaction energy. This phenomenon can be theoretically explained by the qualitative change of the nature of the binary interaction in the medium. The appearance of an energy maximum has significant effects on the itinerant ferromagnetism. In 3D, the ferromagnetic transition is reentrant and itinerant ferromagnetism exists in a narrow window around the energy maximum. In 2D, the present theoretical approach suggests that itinerant ferromagnetism does not exist, which reflects the fact that the energy maximum becomes much lower than the energy of the fully polarized state."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the theoretical explanation for the appearance of an energy maximum in the interaction energy of a strongly interacting Fermi gas in the absence of molecule formation, and how does this affect the possibility of itinerant ferromagnetism in 3D and 2D systems?\n\nA) The energy maximum is due to the qualitative change in the nature of the binary interaction in the medium, which leads to a reentrant ferromagnetic transition in 3D and the absence of itinerant ferromagnetism in 2D.\n\nB) The energy maximum is a result of the Bose-Einstein condensate side, which provides a possible explanation for the experimental measurements of the interaction energy.\n\nC) The energy maximum is a consequence of the Galitskii-Feynman type perturbative contributions to the interaction energy, which can be expressed by a simple phase space integral of an in-medium scattering phase shift.\n\nD) The energy maximum is a result of the Fermi gas parameter, which determines the strength of the interaction between particles in the gas.\n\n**Correct Answer:** A) The energy maximum is due to the qualitative change in the nature of the binary interaction in the medium, which leads to a reentrant ferromagnetic transition in 3D and the absence of itinerant ferromagnetism in 2D.\n\n**Explanation:**\n\nThe correct answer is A) because the documentation states that the energy maximum is due to the qualitative change in the nature of the binary interaction in the medium, which leads to a reentrant ferromagnetic transition in 3D and the absence of itinerant ferromagnetism in 2D. This is a key point in the documentation, and the other options do not accurately summarize the explanation provided.\n\nOption B is incorrect because the energy maximum is not a result of the Bose-Einstein condensate side, but rather a consequence of the qualitative change in the binary interaction in the medium.\n\nOption C is incorrect because while the Galitskii-Feynman type perturbative contributions to the interaction energy are mentioned in the documentation, they are not the primary explanation for the energy maximum.\n\nOption D is incorrect because the Fermi gas parameter is not mentioned as a cause of the energy maximum, and the documentation does not suggest that it is the primary factor in determining the interaction energy."}, "24": {"documentation": {"title": "Longitudinal distribution of initial energy density and directed flow of\n  charged particles in relativistic heavy-ion collisions", "source": "Ze-Fang Jiang, Shanshan Cao, Xiang-Yu Wu, C. B. Yang and Ben-Wei Zhang", "docs_id": "2112.01916", "section": ["hep-ph", "hep-th", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal distribution of initial energy density and directed flow of\n  charged particles in relativistic heavy-ion collisions. We study the origin of the directed flow of charged particles produced in relativistic heavy-ion collisions. Three different initial conditions, Boz$\\dot{\\textrm{e}}$k-Wyskiel, CCNU and Shen-Alzhrani, of energy density distributions are coupled to the (3+1)-dimensional viscous hydrodynamic model CLVisc, and their effects on the development of the anisotropic medium geometry, pressure gradient and radial flow are systematically compared. By comparing to experimental data at both RHIC and LHC, we find that the directed flow provides a unique constraint on the tilt of the initial medium profile in the plane spanned by the impact parameter and space-time rapidity. Within mid-rapidity, the counter-clockwise tilt is shown to be a crucial source of the positive/negative force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, which drives a negative slope of the $x$ component of the medium flow velocity with respect to rapidity, and in the end the same feature of the charged particle directed flow."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of relativistic heavy-ion collisions, what is the primary effect of the counter-clockwise tilt of the initial medium profile on the pressure gradient and radial flow of the anisotropic medium geometry?\n\n**A)** The counter-clockwise tilt leads to a positive force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, resulting in a positive slope of the $x$ component of the medium flow velocity with respect to rapidity.\n\n**B)** The counter-clockwise tilt leads to a negative force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, resulting in a negative slope of the $x$ component of the medium flow velocity with respect to rapidity.\n\n**C)** The counter-clockwise tilt has no effect on the pressure gradient and radial flow of the anisotropic medium geometry.\n\n**D)** The counter-clockwise tilt leads to a positive force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, resulting in a positive slope of the $x$ component of the medium flow velocity with respect to rapidity, and a negative slope of the $x$ component of the medium flow velocity with respect to rapidity.\n\n**Correct Answer:** B) The counter-clockwise tilt leads to a negative force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, resulting in a negative slope of the $x$ component of the medium flow velocity with respect to rapidity.\n\n**Explanation:** According to the documentation, the counter-clockwise tilt of the initial medium profile is shown to be a crucial source of the positive/negative force by the pressure gradient along the impact parameter ($x$) direction at backward/forward rapidity, which drives a negative slope of the $x$ component of the medium flow velocity with respect to rapidity. Therefore, the correct answer is B."}, "25": {"documentation": {"title": "Distributed Value of Information in Feedback Control over Multi-hop\n  Networks", "source": "Precious Ugo Abara, Sandra Hirche", "docs_id": "2107.07822", "section": ["eess.SY", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributed Value of Information in Feedback Control over Multi-hop\n  Networks. Recent works in the domain of networked control systems have demonstrated that the joint design of medium access control strategies and control strategies for the closed-loop system is beneficial. However, several metrics introduced so far fail in either appropriately representing the network requirements or in capturing how valuable the data is. In this paper we propose a distributed value of information (dVoI) metric for the joint design of control and schedulers for medium access in a multi-loop system and multi-hop network. We start by providing conditions under certainty equivalent controller is optimal. Then we reformulate the joint control and communication problem as a Bellman-like equation. The corresponding dynamic programming problem is solved in a distributed fashion by the proposed VoI-based scheduling policies for the multi-loop multi-hop networked control system, which outperforms the well-known time-triggered periodic sampling policies. Additionally we show that the dVoI-based scheduling policies are independent of each other, both loop-wise and hop-wise. At last, we illustrate the results with a numerical example."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Distributed Value of Information in Feedback Control over Multi-hop Networks\", what is the primary contribution of the proposed Distributed Value of Information (dVoI) metric, and how does it address the limitations of existing metrics in joint control and scheduler design?\n\n**A)** The dVoI metric provides a new framework for evaluating the value of information in multi-hop networks, but it does not address the limitations of existing metrics. It is a standalone metric that can be used in isolation.\n\n**B)** The dVoI metric reformulates the joint control and communication problem as a Bellman-like equation, which allows for the solution of the dynamic programming problem in a distributed fashion. It addresses the limitations of existing metrics by providing a more comprehensive framework for evaluating the value of information.\n\n**C)** The dVoI metric is independent of the specific network topology and can be applied to any multi-hop network. It addresses the limitations of existing metrics by providing a more general framework for evaluating the value of information.\n\n**D)** The dVoI metric is a heuristic approach that relies on empirical evaluation of the value of information, rather than a rigorous mathematical formulation. It addresses the limitations of existing metrics by providing a more practical approach to joint control and scheduler design.\n\n**Correct Answer:** B) The dVoI metric reformulates the joint control and communication problem as a Bellman-like equation, which allows for the solution of the dynamic programming problem in a distributed fashion. It addresses the limitations of existing metrics by providing a more comprehensive framework for evaluating the value of information.\n\n**Explanation:** The correct answer is B) because the dVoI metric is a key contribution of the paper, and it addresses the limitations of existing metrics by providing a more comprehensive framework for evaluating the value of information. The dVoI metric reformulates the joint control and communication problem as a Bellman-like equation, which allows for the solution of the dynamic programming problem in a distributed fashion. This is a critical innovation that enables the development of more effective control and scheduler designs for multi-hop networks."}, "26": {"documentation": {"title": "A New Deep Learning Method for Image Deblurring in Optical Microscopic\n  Systems", "source": "Huangxuan Zhao, Ziwen Ke, Ningbo Chen, Ke Li, Lidai Wang, Xiaojing\n  Gong, Wei Zheng, Liang Song, Zhicheng Liu, Dong Liang, and Chengbo Liu", "docs_id": "1910.03928", "section": ["eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Deep Learning Method for Image Deblurring in Optical Microscopic\n  Systems. Deconvolution is the most commonly used image processing method to remove the blur caused by the point-spread-function (PSF) in optical imaging systems. While this method has been successful in deblurring, it suffers from several disadvantages including being slow, since it takes many iterations, suboptimal, in cases where experimental operator chosen to represent PSF is not optimal. In this paper, we are proposing a deep-learning-based deblurring method applicable to optical microscopic imaging systems. We tested the proposed method in database data, simulated data, and experimental data (include 2D optical microscopic data and 3D photoacoustic microscopic data), all of which showed much improved deblurred results compared to deconvolution. To quantify the improved performance, we compared our results against several deconvolution methods. Our results are better than conventional techniques and do not require multiple iterations or pre-determined experimental operator. Our method has the advantages of simple operation, short time to compute, good deblur results and wide application in all types of optical microscopic imaging systems. The deep learning approach opens up a new path for deblurring and can be applied in various biomedical imaging fields."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary disadvantages of the deconvolution method used in optical imaging systems, and how does the proposed deep-learning-based deblurring method address these limitations?\n\nA) Deconvolution is slow and suboptimal due to the need for multiple iterations and a pre-determined experimental operator, whereas the proposed method is simple to operate and requires no iterations.\n\nB) Deconvolution is computationally expensive and requires significant expertise to implement, whereas the proposed method is fast and can be applied to various biomedical imaging fields.\n\nC) Deconvolution suffers from suboptimal results due to the choice of experimental operator, whereas the proposed method produces better deblurred results without requiring multiple iterations or pre-determined operators.\n\nD) Deconvolution is limited to 2D optical microscopic data, whereas the proposed method can be applied to both 2D and 3D photoacoustic microscopic data.\n\nCorrect Answer: C) Deconvolution suffers from suboptimal results due to the choice of experimental operator, whereas the proposed method produces better deblurred results without requiring multiple iterations or pre-determined operators.\n\nExplanation: The documentation states that deconvolution suffers from suboptimal results in cases where the experimental operator chosen to represent the point-spread-function (PSF) is not optimal. In contrast, the proposed deep-learning-based deblurring method produces better deblurred results without requiring multiple iterations or pre-determined operators, making it a more reliable and efficient method for image deblurring in optical microscopic systems."}, "27": {"documentation": {"title": "Spectral Curves and Whitham Equations in Isomonodromic Problems of\n  Schlesinger Type", "source": "Kanehisa Takasaki (Kyoto University)", "docs_id": "solv-int/9704004", "section": ["nlin.SI", "hep-th", "math.QA", "nlin.SI", "math.QA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spectral Curves and Whitham Equations in Isomonodromic Problems of\n  Schlesinger Type. It has been known since the beginning of this century that isomonodromic problems --- typically the Painlev\\'e transcendents --- in a suitable asymptotic region look like a kind of ``modulation'' of isospectral problem. This connection between isomonodromic and isospectral problems is reconsidered here in the light of recent studies related to the Seiberg-Witten solutions of $N = 2$ supersymmetric gauge theories. A general machinary is illustrated in a typical isomonodromic problem, namely the Schlesinger equation, which is reformulated to include a small parameter $\\epsilon$. In the small-$\\epsilon$ limit, solutions of this isomonodromic problem are expected to behave as a slowly modulated finite-gap solution of an isospectral problem. The modulation is caused by slow deformations of the spectral curve of the finite-gap solution. A modulation equation of this slow dynamics is derived by a heuristic method. An inverse period map of Seiberg-Witten type turns out to give general solutions of this modulation equation. This construction of general solution also reveals the existence of deformations of Seiberg-Witten type on the same moduli space of spectral curves. A prepotential is also constructed in the same way as the prepotential of the Seiberg-Witten theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the isomonodromic problem of the Schlesinger equation, which can be reformulated to include a small parameter $\\epsilon$. In the small-$\\epsilon$ limit, solutions of this isomonodromic problem are expected to behave as a slowly modulated finite-gap solution of an isospectral problem. Derive the modulation equation of this slow dynamics using a heuristic method, and show that an inverse period map of Seiberg-Witten type provides general solutions to this modulation equation.\n\nA) The modulation equation is given by $\\frac{\\partial \\phi}{\\partial \\epsilon} = \\sum_{i=1}^n \\frac{\\partial \\phi}{\\partial t_i} \\frac{\\partial t_i}{\\partial \\epsilon}$, where $\\phi$ is the prepotential and $t_i$ are the spectral parameters.\n\nB) The modulation equation is given by $\\frac{\\partial \\phi}{\\partial \\epsilon} = \\sum_{i=1}^n \\frac{\\partial \\phi}{\\partial t_i} \\frac{\\partial t_i}{\\partial \\epsilon} + \\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial t_i^2} \\frac{\\partial t_i}{\\partial \\epsilon}$, where $\\phi$ is the prepotential and $t_i$ are the spectral parameters.\n\nC) The modulation equation is given by $\\frac{\\partial \\phi}{\\partial \\epsilon} = \\sum_{i=1}^n \\frac{\\partial \\phi}{\\partial t_i} \\frac{\\partial t_i}{\\partial \\epsilon} + \\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial t_i^2} \\frac{\\partial t_i}{\\partial \\epsilon} - \\sum_{i=1}^n \\frac{\\partial^3 \\phi}{\\partial t_i^3} \\frac{\\partial t_i}{\\partial \\epsilon}$, where $\\phi$ is the prepotential and $t_i$ are the spectral parameters.\n\nD) The modulation equation is given by $\\frac{\\partial \\phi}{\\partial \\epsilon} = \\sum_{i=1}^n \\frac{\\partial \\phi}{\\partial t_i} \\frac{\\partial t_i}{\\partial \\epsilon} + \\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial t_i^2} \\frac{\\partial t_i}{\\partial \\epsilon} - \\sum_{i=1}^n \\frac{\\partial^3 \\phi}{\\partial t_i^3} \\frac{\\partial t_i}{\\partial \\epsilon} + \\sum_{i=1}^n \\frac{\\partial^4 \\phi}{\\partial t_i^4} \\frac{\\partial t_i}{\\partial \\epsilon}$, where $\\phi$ is the prepotential and $t_i$ are the spectral parameters.\n\nCorrect Answer: B) The modulation equation is given by $\\frac{\\partial \\phi}{\\partial \\epsilon} = \\sum_{i=1}^n \\frac{\\partial \\phi}{\\partial t_i} \\frac{\\partial t_i}{\\partial \\epsilon} + \\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial t_i^2} \\frac{\\partial t_i}{\\partial \\epsilon}$, where $\\phi$ is the prepotential and $t_i$ are the spectral parameters.\n\nExplanation: The correct answer is B) because it correctly represents the modulation equation derived using a heuristic method. The modulation equation is a result of the slow deformation of the spectral curve of the finite-gap solution, and it involves the partial derivatives of the prepotential and spectral parameters with respect to the small parameter $\\epsilon$. The correct answer also correctly includes the term $\\sum_{i=1}^n \\frac{\\partial^2 \\phi}{\\partial t_i^2} \\frac{\\partial t_i}{\\partial \\epsilon}$, which represents the second-order deformation of the spectral curve. The other options are incorrect because they include additional terms that are not present in the correct modulation equation."}, "28": {"documentation": {"title": "The Impact of Digital Marketing on Sausage Manufacturing Companies in\n  the Altos of Jalisco", "source": "Guillermo Jose Navarro del Toro", "docs_id": "2101.06603", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Impact of Digital Marketing on Sausage Manufacturing Companies in\n  the Altos of Jalisco. One of the goals of any business, in addition to producing high-quality, community-accepted products, is to significantly increase sales. Unfortunately, there are regions where new marketing technologies that make it possible to reach a larger number of potential consumers, not only at the regional level, but also at the state and national level, are not yet used. This research, which included qualitative and quantitative methods, as well as interviews applied to owners, employees and clients of three sausage companies, seeks to measure the impact of digital marketing in the Altos of Jalisco, Mexico. Thus, in addition to inquiring about the degree of knowledge they have regarding information and communication technologies (ICT) to expand their markets to areas with higher population density, another goal is to know the opinion about their manufactured products, their quality and acceptance. It should not be forgotten that companies are moving to an increasingly connected world, which enables entrepreneurs to get their products to a greater number of consumers through the Internet and smart devices, such as cell phones, tablets and computers; and thus ensure the survival of the company and a longer stay in the market."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of digital marketing in the context of sausage manufacturing companies in the Altos of Jalisco, Mexico, according to the research?\n\n**A)** To increase brand awareness among local consumers\n**B)** To expand the market to areas with higher population density and increase sales\n**C)** To improve product quality and customer satisfaction\n**D)** To reduce production costs and increase efficiency\n\n**Correct Answer:** B) To expand the market to areas with higher population density and increase sales\n\n**Explanation:** The research aims to measure the impact of digital marketing on sausage manufacturing companies in the Altos of Jalisco, Mexico, with a focus on expanding their markets to areas with higher population density, both regionally, state-wide, and nationally. This goal is in line with the increasing trend of companies moving to an increasingly connected world, enabling them to reach a greater number of consumers through digital channels."}, "29": {"documentation": {"title": "Thermodynamics of the disordered Hubbard model studied via numerical\n  linked-cluster expansions", "source": "Jacob Park and Ehsan Khatami", "docs_id": "2101.12721", "section": ["cond-mat.str-el", "cond-mat.dis-nn", "cond-mat.quant-gas"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thermodynamics of the disordered Hubbard model studied via numerical\n  linked-cluster expansions. The interplay of disorder and strong correlations in quantum many-body systems remains an open question. That is despite much progress made in recent years with ultracold atoms in optical lattices to better understand phenomena such as many-body localization or the effect of disorder on Mott metal-insulator transitions. Here, we utilize the numerical linked-cluster expansion technique, extended to treat disordered quantum lattice models, and study exact thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries. We consider box distributions for the disorder in the onsite energy, the interaction strength, as well as the hopping amplitude and explore how energy, double occupancy, entropy, heat capacity and magnetic correlations of the system in the thermodynamic limit evolve as the strength of disorder changes. We compare our findings with those obtained from determinant quantum Monte Carlo simulations and discuss the relevance of our results to experiments with cold fermionic atoms in optical lattices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary goal of the study presented in the Arxiv documentation, and how does it relate to the current understanding of quantum many-body systems?\n\nA) To investigate the effects of disorder on the Mott metal-insulator transition in ultracold atoms in optical lattices.\nB) To explore the thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries in the presence of strong correlations.\nC) To develop a new numerical method for studying quantum many-body systems, specifically the linked-cluster expansion technique.\nD) To compare the results of numerical linked-cluster expansions with those obtained from determinant quantum Monte Carlo simulations.\n\nCorrect Answer: B) To explore the thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries in the presence of strong correlations.\n\nExplanation: The primary goal of the study is to investigate the thermodynamic properties of the disordered Fermi-Hubbard model on the square and cubic geometries, which is a key aspect of understanding quantum many-body systems. The study aims to explore how energy, double occupancy, entropy, heat capacity, and magnetic correlations of the system evolve as the strength of disorder changes. This is a crucial aspect of understanding the interplay of disorder and strong correlations in quantum many-body systems.\n\nThe other options are incorrect because:\n\nA) The study does not focus on the Mott metal-insulator transition in ultracold atoms in optical lattices, but rather on the disordered Fermi-Hubbard model.\n\nC) While the linked-cluster expansion technique is used in the study, it is not the primary goal of the study, but rather a tool used to explore the thermodynamic properties of the disordered Fermi-Hubbard model.\n\nD) The study does compare the results of numerical linked-cluster expansions with those obtained from determinant quantum Monte Carlo simulations, but this is not the primary goal of the study."}, "30": {"documentation": {"title": "Multi-step processes in heavy-ion induced single-nucleon transfer\n  reactions", "source": "N. Keeley, K. W. Kemper and K. Rusek", "docs_id": "2007.13370", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-step processes in heavy-ion induced single-nucleon transfer\n  reactions. It was first noted during the 1970s that finite-range distorted wave Born approximation (FR-DWBA) calculations were unable satisfactorily to describe the shape of the angular distributions of many single-proton (and some single-neutron) transfer reactions induced by heavy ions, with calculations shifted to larger angles by up to ~ 4 degrees compared with the data. These reactions exhibited a significant mismatch, either of the reaction Q value or the grazing angular momentum of the entrance and exit channels, and it was speculated that the inclusion of multi-step transfer paths via excited state(s) of the projectile and/or ejectile could compensate for the effect of this mismatch and yield good descriptions of the data by shifting the calculated peaks to smaller angles. However, to date this has not been explicitly demonstrated for many reactions. In this work we show that inclusion of the two-step transfer path via the 4.44-MeV 2+ excited state of the 12C projectile in coupled channel Born approximation calculations enables a good description of the 208Pb(12C,11B)209Bi single-proton stripping data at four incident energies which could not be described by the FR-DWBA. We also show that inclusion of a similar reaction path for the 208Pb(12C,13C)207Pb single-neutron pickup reaction has a relatively minor influence, slightly improving the already good description obtained with the FR-DWBA."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason why finite-range distorted wave Born approximation (FR-DWBA) calculations were unable to satisfactorily describe the shape of the angular distributions of many single-proton (and some single-neutron) transfer reactions induced by heavy ions, according to the 1970s observations?\n\nA) The FR-DWBA calculations did not account for the effects of nuclear pairing.\nB) The FR-DWBA calculations did not include the possibility of multi-step transfer paths via excited states of the projectile and/or ejectile.\nC) The FR-DWBA calculations were limited by the range of the nuclear potential used.\nD) The FR-DWBA calculations were unable to accurately describe the reaction Q value.\n\nCorrect Answer: B) The FR-DWBA calculations did not include the possibility of multi-step transfer paths via excited states of the projectile and/or ejectile."}, "31": {"documentation": {"title": "Probing the semi-macroscopic vacuum by higher-harmonic generation under\n  focused intense laser fields", "source": "Kensuke Homma, Dieter Habs, and Toshiki Tajima", "docs_id": "1103.1748", "section": ["hep-ph", "gr-qc", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Probing the semi-macroscopic vacuum by higher-harmonic generation under\n  focused intense laser fields. The invention of the laser immediately enabled the detection of nonlinear photon-matter interactions, as manifested for example by Franken et al.'s detection of second-harmonic generation. With the recent advancement in high-power, high-energy lasers and the examples of nonlinearity studies of the laser-matter interaction by virtue of properly arranging lasers and detectors, we envision the possibility of probing nonlinearities of the photon interaction in vacuum over substantial space-time scales, compared to the microscopic scale provided by high-energy accelerators. Specifically, we introduce the photon-photon interaction in a quasi-parallel colliding system and the detection of higher harmonics in that system. The method proposed should realize a far greater sensitivity of probing possible low-mass and weakly coupling fields that have been postulated. With the availability of a large number of coherent photons, we suggest a scheme for the detection of higher harmonics via the averaged resonant production and decay of these postulated fields within the uncertainty of the center-of-mass energy between incoming laser photons. The method carves out a substantial swath of new experimental parameter regimes on the coupling of these fields to photons, under appropriate laser technologies, even weaker than that of gravity in the mass range well below 1 eV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Probing the semi-macroscopic vacuum by higher-harmonic generation under focused intense laser fields involves the detection of nonlinear photon-matter interactions. What is the primary advantage of using a quasi-parallel colliding system to probe these interactions, and how does this compare to the microscopic scale provided by high-energy accelerators?\n\n**A)** The quasi-parallel colliding system allows for a greater sensitivity to low-mass and weakly coupling fields, enabling the detection of higher harmonics in a more controlled manner.\n\n**B)** The quasi-parallel colliding system enables the detection of higher harmonics in a more controlled manner, but does not necessarily provide a greater sensitivity to low-mass and weakly coupling fields.\n\n**C)** The quasi-parallel colliding system is limited to detecting higher harmonics in a specific energy range, whereas high-energy accelerators can probe a broader range of energies.\n\n**D)** The quasi-parallel colliding system is not a viable method for detecting higher harmonics, as it is not sensitive enough to detect the postulated low-mass and weakly coupling fields.\n\n**Correct Answer:** A) The quasi-parallel colliding system allows for a greater sensitivity to low-mass and weakly coupling fields, enabling the detection of higher harmonics in a more controlled manner.\n\n**Explanation:** The correct answer is A) because the documentation states that the method proposed should realize a far greater sensitivity of probing possible low-mass and weakly coupling fields. This is achieved through the use of a quasi-parallel colliding system, which allows for the detection of higher harmonics in a more controlled manner. This is in contrast to high-energy accelerators, which provide a microscopic scale of probing."}, "32": {"documentation": {"title": "Procurements with Bidder Asymmetry in Cost and Risk-Aversion", "source": "Gaurab Aryal, Hanna Charankevich, Seungwon Jeong, Dong-Hyuk Kim", "docs_id": "2111.04626", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Procurements with Bidder Asymmetry in Cost and Risk-Aversion. We propose an empirical method to analyze data from first-price procurements where bidders are asymmetric in their risk-aversion (CRRA) coefficients and distributions of private costs. Our Bayesian approach evaluates the likelihood by solving type-symmetric equilibria using the boundary-value method and integrates out unobserved heterogeneity through data augmentation. We study a new dataset from Russian government procurements focusing on the category of printing papers. We find that there is no unobserved heterogeneity (presumably because the job is routine), but bidders are highly asymmetric in their cost and risk-aversion. Our counterfactual study shows that choosing a type-specific cost-minimizing reserve price marginally reduces the procurement cost; however, inviting one more bidder substantially reduces the cost, by at least 5.5%. Furthermore, incorrectly imposing risk-neutrality would severely mislead inference and policy recommendations, but the bias from imposing homogeneity in risk-aversion is small."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Procurements with Bidder Asymmetry in Cost and Risk-Aversion paper, what is the primary finding regarding the distribution of private costs among bidders in the Russian government procurement dataset?\n\n**A)** The distribution of private costs is highly heterogeneous, leading to significant unobserved heterogeneity in the data.\n**B)** The distribution of private costs is highly symmetric, with no significant variation among bidders.\n**C)** The distribution of private costs is highly asymmetric, with bidders exhibiting significant variation in their cost and risk-aversion profiles.\n**D)** The distribution of private costs is not relevant to the analysis, as the focus is on risk-aversion and cost-minimizing reserve prices.\n\n**Correct Answer:** C) The distribution of private costs is highly asymmetric, with bidders exhibiting significant variation in their cost and risk-aversion profiles.\n\n**Explanation:** The paper states that \"bidders are highly asymmetric in their cost and risk-aversion\" and that \"the job is routine, presumably because there is no unobserved heterogeneity\". This suggests that the distribution of private costs among bidders is indeed highly asymmetric, leading to significant variation in their cost and risk-aversion profiles."}, "33": {"documentation": {"title": "Supersymmetric Regularization, Two-Loop QCD Amplitudes and Coupling\n  Shifts", "source": "Z. Bern, A. De Freitas, L. Dixon and H.L. Wong", "docs_id": "hep-ph/0202271", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Supersymmetric Regularization, Two-Loop QCD Amplitudes and Coupling\n  Shifts. We present a definition of the four-dimensional helicity (FDH) regularization scheme valid for two or more loops. This scheme was previously defined and utilized at one loop. It amounts to a variation on the standard 't Hooft-Veltman scheme and is designed to be compatible with the use of helicity states for \"observed\" particles. It is similar to dimensional reduction in that it maintains an equal number of bosonic and fermionic states, as required for preserving supersymmetry. Supersymmetry Ward identities relate different helicity amplitudes in supersymmetric theories. As a check that the FDH scheme preserves supersymmetry, at least through two loops, we explicitly verify a number of these identities for gluon-gluon scattering (gg to gg) in supersymmetric QCD. These results also cross-check recent non-trivial two-loop calculations in ordinary QCD. Finally, we compute the two-loop shift between the FDH coupling and the standard MS-bar coupling, alpha_s. The FDH shift is identical to the one for dimensional reduction. The two-loop coupling shifts are then used to obtain the three-loop QCD beta function in the FDH and dimensional reduction schemes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the four-dimensional helicity (FDH) regularization scheme, what is the relationship between the FDH coupling and the standard MS-bar coupling, \u03b1s, at two loops, and how does this relationship compare to the one in dimensional reduction?\n\nA) The FDH coupling is identical to the MS-bar coupling at two loops, and the FDH shift is zero.\nB) The FDH coupling is identical to the MS-bar coupling at two loops, and the FDH shift is equal to the one in dimensional reduction.\nC) The FDH coupling is not identical to the MS-bar coupling at two loops, and the FDH shift is equal to the one in dimensional reduction.\nD) The FDH coupling is identical to the MS-bar coupling at two loops, but the FDH shift is not equal to the one in dimensional reduction.\n\nCorrect Answer: B) The FDH coupling is identical to the MS-bar coupling at two loops, and the FDH shift is equal to the one in dimensional reduction.\n\nExplanation: The correct answer is B) because the documentation states that the FDH shift is identical to the one for dimensional reduction, and the FDH coupling is identical to the MS-bar coupling at two loops. This is a consequence of the fact that the FDH scheme is designed to be compatible with the use of helicity states for \"observed\" particles and maintains an equal number of bosonic and fermionic states, as required for preserving supersymmetry."}, "34": {"documentation": {"title": "Ptychographic X-ray Speckle Tracking", "source": "Andrew J. Morgan, Harry M. Quiney, Sa\\v{s}a Bajt, Henry N. Chapman", "docs_id": "2003.12686", "section": ["physics.optics", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ptychographic X-ray Speckle Tracking. We present a method for the measurement of the phase gradient of a wavefront by tracking the relative motion of speckles in projection holograms as a sample is scanned across the wavefront. By removing the need to obtain an un-distorted reference image of the sample, this method is suitable for the metrology of highly divergent wavefields. Such wavefields allow for large magnification factors, that, according to current imaging capabilities, will allow for nano-radian angular sensitivity and nano-scale sample projection imaging. Both the reconstruction algorithm and the imaging geometry are nearly identical to that of ptychography, except that the sample is placed downstream of the beam focus and that no coherent propagation is explicitly accounted for. Like other x-ray speckle tracking methods, it is robust to low-coherence x-ray sources making is suitable for lab based x-ray sources. Likewise it is robust to errors in the registered sample positions making it suitable for x-ray free-electron laser facilities, where beam pointing fluctuations can be problematic for wavefront metrology. We also present a modified form of the speckle tracking approximation, based on a second-order local expansion of the Fresnel integral. This result extends the validity of the speckle tracking approximation and may be useful for similar approaches in the field."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Ptychographic X-ray Speckle Tracking method over traditional wavefront metrology techniques, and how does it relate to the limitations of current imaging capabilities?\n\n**A)** The method eliminates the need for an un-distorted reference image, allowing for the measurement of phase gradients in highly divergent wavefields. This is particularly useful for achieving nano-radian angular sensitivity and nano-scale sample projection imaging.\n\n**B)** The method relies on coherent propagation, which is not explicitly accounted for, and is therefore limited to lab-based x-ray sources with low-coherence x-ray sources.\n\n**C)** The method is only suitable for x-ray free-electron laser facilities due to its sensitivity to errors in registered sample positions.\n\n**D)** The method is limited to reconstructing wavefronts with a fixed magnification factor, making it unsuitable for applications requiring large magnification factors.\n\n**Correct Answer:** A) The method eliminates the need for an un-distorted reference image, allowing for the measurement of phase gradients in highly divergent wavefields. This is particularly useful for achieving nano-radian angular sensitivity and nano-scale sample projection imaging.\n\n**Explanation:** The correct answer is A) because the Ptychographic X-ray Speckle Tracking method is designed to measure phase gradients without the need for an un-distorted reference image, making it suitable for highly divergent wavefields. This is a key advantage of the method, as it allows for the measurement of phase gradients in wavefields that would be difficult or impossible to image using traditional wavefront metrology techniques. The other options are incorrect because they either misrepresent the method's advantages (B) or its limitations (C and D)."}, "35": {"documentation": {"title": "Random Network Behaviour of Protein Structures", "source": "Brinda K.V., Saraswathi Vishveshwara and Smitha Vishveshwara", "docs_id": "0912.5406", "section": ["physics.bio-ph", "cond-mat.other", "q-bio.BM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Random Network Behaviour of Protein Structures. Geometric and structural constraints greatly restrict the selection of folds adapted by protein backbones, and yet, folded proteins show an astounding diversity in functionality. For structure to have any bearing on function, it is thus imperative that, apart from the protein backbone, other tunable degrees of freedom be accountable. Here, we focus on side-chain interactions, which non-covalently link amino acids in folded proteins to form a network structure. At a coarse-grained level, we show that the network conforms remarkably well to realizations of random graphs and displays associated percolation behavior. Thus, within the rigid framework of the protein backbone that restricts the structure space, the side-chain interactions exhibit an element of randomness, which account for the functional flexibility and diversity shown by proteins. However, at a finer level, the network exhibits deviations from these random graphs which, as we demonstrate for a few specific examples, reflect the intrinsic uniqueness in the structure and stability, and perhaps specificity in the functioning of biological proteins."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary conclusion drawn by the authors regarding the role of side-chain interactions in protein structure and function, and how do they reconcile this with the rigidity of the protein backbone?\n\nA) The side-chain interactions are solely responsible for the functional diversity of proteins, and the protein backbone is merely a passive framework.\nB) The side-chain interactions exhibit percolation behavior, but this is limited by the constraints imposed by the protein backbone, which allows for a degree of randomness and flexibility.\nC) The side-chain interactions are entirely random and unstructured, and the protein backbone plays no role in determining protein function.\nD) The side-chain interactions are deterministic and predictable, and the protein backbone provides a rigid framework that determines protein structure and function.\n\n**Correct Answer:** B) The side-chain interactions exhibit percolation behavior, but this is limited by the constraints imposed by the protein backbone, which allows for a degree of randomness and flexibility.\n\n**Explanation:** The authors suggest that the side-chain interactions conform to random graphs at a coarse-grained level, but exhibit deviations from these models at a finer level, reflecting the unique structure and stability of individual proteins. This implies that while the side-chain interactions are influenced by the protein backbone, they also exhibit a degree of randomness and flexibility that allows for functional diversity."}, "36": {"documentation": {"title": "Novel dual relation and constant in Hawking-Page phase transitions", "source": "Shao-Wen Wei, Yu-Xiao Liu, Robert B. Mann", "docs_id": "2006.11503", "section": ["gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Novel dual relation and constant in Hawking-Page phase transitions. Universal relations and constants have important applications in understanding a physical theory. In this article, we explore this issue for Hawking-Page phase transitions in Schwarzschild anti-de Sitter black holes. We find a novel exact dual relation between the minimum temperature of the ($d$+1)-dimensional black hole and the Hawking-Page phase transition temperature in $d$ dimensions, reminiscent of the holographic principle. Furthermore, we find that the normalized Ruppeiner scalar curvature is a universal constant at the Hawking-Page transition point. Since the Ruppeiner curvature can be treated as an indicator of the intensity of the interactions amongst black hole microstructures, we conjecture that this universal constant denotes an interaction threshold, beyond which a virtual black hole becomes a real one. This new dual relation and universal constant are fundamental in understanding Hawking-Page phase transitions, and might have new important applications in the black hole physics in the near future."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of Hawking-Page phase transitions in Schwarzschild anti-de Sitter black holes, what is the significance of the normalized Ruppeiner scalar curvature being a universal constant at the Hawking-Page transition point?\n\nA) It indicates the temperature of the black hole at the transition point\nB) It denotes the interaction threshold beyond which a virtual black hole becomes a real one\nC) It is a measure of the entropy of the black hole at the transition point\nD) It is a consequence of the holographic principle in d dimensions\n\nCorrect Answer: B) It denotes the interaction threshold beyond which a virtual black hole becomes a real one\n\nExplanation: The normalized Ruppeiner scalar curvature is treated as an indicator of the intensity of interactions amongst black hole microstructures. The fact that it is a universal constant at the Hawking-Page transition point suggests that it represents a threshold beyond which a virtual black hole becomes a real one. This is a key finding in the article and has implications for understanding Hawking-Page phase transitions and their applications in black hole physics."}, "37": {"documentation": {"title": "Topology of SO(5)-monopoles and three-dimensional, stable Dirac\n  semimetals", "source": "Alexander C. Tyner, Shouvik Sur, Danilo Puggioni, James M. Rondinelli,\n  and Pallab Goswami", "docs_id": "2012.12906", "section": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.str-el", "hep-lat", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topology of SO(5)-monopoles and three-dimensional, stable Dirac\n  semimetals. The band-touching points of stable, three-dimensional, Kramers-degenerate, Dirac semimetals are singularities of a five-component, unit vector field and non-Abelian, $SO(5)$-Berry's connections, whose topological classification is an important, open problem. We solve this problem by performing second homotopy classification of Berry's connections. Using Abelian projected connections, the generic planes, orthogonal to the direction of nodal separation, and lying between two Dirac points are shown to be higher-order topological insulators, which support quantized, chromo-magnetic flux or relative Chern number, and gapped, edge states. The Dirac points are identified as a pair of unit-strength, $SO(5)$- monopole and anti-monopole, where the relative Chern number jumps by $\\pm 1$. Using these bulk invariants, we determine the topological universality class of different types of Dirac semimetals. We also describe a universal recipe for computing quantized, non-Abelian flux for Dirac materials from the windings of spectra of planar Wilson loops, displaying $SO(5)$-gauge invariance. With non-perturbative, analytical solutions of surface-states, we show the absence of helical Fermi arcs, and predict the fermiology and the spin-orbital textures. We also discuss the similarities and important topological distinction between the surface-states Hamiltonian and the generator of Polyakov loop of Berry's connections."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the topological classification of the five-component, unit vector field and non-Abelian, $SO(5)$-Berry's connections in stable, three-dimensional, Kramers-degenerate, Dirac semimetals, and how does it relate to the quantized, chromo-magnetic flux or relative Chern number?\n\nA) The topological classification is a first homotopy classification, and the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals.\n\nB) The topological classification is a second homotopy classification, and the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals, which are higher-order topological insulators.\n\nC) The topological classification is a third homotopy classification, and the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals, which are non-Abelian, $SO(5)$-gauge invariant.\n\nD) The topological classification is a fourth homotopy classification, and the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals, which are non-Abelian, $SO(5)$-gauge invariant, and support quantized, chromo-magnetic flux.\n\nCorrect Answer: B) The topological classification is a second homotopy classification, and the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals, which are higher-order topological insulators.\n\nExplanation: The correct answer is B) because the documentation states that the authors perform a second homotopy classification of Berry's connections, and that the relative Chern number is a topological invariant that characterizes the topological universality class of different types of Dirac semimetals, which are higher-order topological insulators. This is supported by the fact that the authors show that the generic planes, orthogonal to the direction of nodal separation, and lying between two Dirac points are higher-order topological insulators, which support quantized, chromo-magnetic flux or relative Chern number, and gapped, edge states."}, "38": {"documentation": {"title": "Cointegration in functional autoregressive processes", "source": "Massimo Franchi and Paolo Paruolo", "docs_id": "1712.07522", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cointegration in functional autoregressive processes. This paper defines the class of $\\mathcal{H}$-valued autoregressive (AR) processes with a unit root of finite type, where $\\mathcal{H}$ is an infinite dimensional separable Hilbert space, and derives a generalization of the Granger-Johansen Representation Theorem valid for any integration order $d=1,2,\\dots$. An existence theorem shows that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer $d$ and displays a common trends representation with a finite number of common stochastic trends of the type of (cumulated) bilateral random walks and an infinite dimensional cointegrating space. A characterization theorem clarifies the connections between the structure of the AR operators and $(i)$ the order of integration, $(ii)$ the structure of the attractor space and the cointegrating space, $(iii)$ the expression of the cointegrating relations, and $(iv)$ the Triangular representation of the process. Except for the fact that the number of cointegrating relations that are integrated of order 0 is infinite, the representation of $\\mathcal{H}$-valued ARs with a unit root of finite type coincides with that of usual finite dimensional VARs, which corresponds to the special case $\\mathcal{H}=\\mathbb{R}^p$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider an $\\mathcal{H}$-valued autoregressive (AR) process with a unit root of finite type, where $\\mathcal{H}$ is an infinite dimensional separable Hilbert space. Suppose that the process is integrated of order $d$ and displays a common trends representation with a finite number of common stochastic trends of the type of (cumulated) bilateral random walks and an infinite dimensional cointegrating space.\n\n**A)** The Granger-Johansen Representation Theorem can be generalized to any integration order $d=1,2,\\dots$ for $\\mathcal{H}$-valued AR processes with a unit root of finite type.\n\n**B)** The number of cointegrating relations that are integrated of order 0 is finite.\n\n**C)** The representation of $\\mathcal{H}$-valued ARs with a unit root of finite type coincides with that of usual finite dimensional VARs, which corresponds to the special case $\\mathcal{H}=\\mathbb{R}^p$.\n\n**D)** The existence theorem implies that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer $d$.\n\n**Correct Answer:** D) The existence theorem implies that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer $d$.\n\n**Explanation:** The existence theorem states that the solution of an AR with a unit root of finite type is necessarily integrated of some finite integer $d$. This is a crucial result that establishes the connection between the structure of the AR process and its integration order. The correct answer is D) because it accurately reflects the content of the existence theorem.\n\nThe other options are incorrect because:\n\n* A) is incorrect because the Granger-Johansen Representation Theorem is not generalized to any integration order $d=1,2,\\dots$ for $\\mathcal{H}$-valued AR processes with a unit root of finite type.\n* B) is incorrect because the number of cointegrating relations that are integrated of order 0 is actually infinite.\n* C) is incorrect because the representation of $\\mathcal{H}$-valued ARs with a unit root of finite type does not coincide with that of usual finite dimensional VARs, although it does correspond to the special case $\\mathcal{H}=\\mathbb{R}^p$."}, "39": {"documentation": {"title": "Microscopic derivation of density functional theory for superfluid\n  systems based on effective action formalism", "source": "Takeru Yokota, Haruki Kasuya, Kenichi Yoshida, Teiji Kunihiro", "docs_id": "2008.05919", "section": ["nucl-th", "cond-mat.supr-con", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Microscopic derivation of density functional theory for superfluid\n  systems based on effective action formalism. Density-functional theory for superfluid systems is developed in the framework of the functional renormalization group based on the effective action formalism. We introduce the effective action for the particle-number and nonlocal pairing densities and demonstrate that the Hohenberg-Kohn theorem for superfluid systems is established in terms of the effective action. The flow equation for the effective action is then derived, where the flow parameter runs from $0$ to $1$, corresponding to the non-interacting and interacting systems. From the flow equation and the variational equation that the equilibrium density satisfies, we obtain the exact expression for the Kohn-Sham potential generalized to including the pairing potentials. The resultant Kohn-Sham potential has a nice feature that it expresses the microscopic formulae of the external, Hartree, pairing, and exchange-correlation terms, separately. It is shown that our Kohn-Sham potential gives the ground-state energy of the Hartree-Fock-Bogoliubov theory by neglecting the correlations. An advantage of our exact formalism lies in the fact that it provides ways to systematically improve the correlation part."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the effective action formalism for superfluid systems, what is the significance of the Hohenberg-Kohn theorem being established in terms of the effective action, and how does the flow equation for the effective action relate to the variational equation for the equilibrium density?\n\nA) The Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a direct consequence of the variational equation for the equilibrium density. This is a fundamental result in the development of density-functional theory for superfluid systems.\n\nB) The Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a necessary condition for the variational equation for the equilibrium density to hold. This is a key insight in the development of density-functional theory for superfluid systems.\n\nC) The Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a sufficient condition for the variational equation for the equilibrium density to hold. This is a crucial result in the development of density-functional theory for superfluid systems.\n\nD) The Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a consequence of the variational equation for the equilibrium density, but only in the limit where the flow parameter runs from 0 to 1. This is a subtle but important result in the development of density-functional theory for superfluid systems.\n\nCorrect Answer: B) The Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a necessary condition for the variational equation for the equilibrium density to hold. This is a key insight in the development of density-functional theory for superfluid systems.\n\nExplanation: The correct answer is B) because the Hohenberg-Kohn theorem is established in terms of the effective action, which implies that the flow equation for the effective action is a necessary condition for the variational equation for the equilibrium density to hold. This is a key insight in the development of density-functional theory for superfluid systems. The other options are incorrect because they either imply that the flow equation is a sufficient condition (C), a direct consequence (A), or a consequence only in the limit (D), which is not the case."}, "40": {"documentation": {"title": "Libra: Fair Order-Matching for Electronic Financial Exchanges", "source": "Vasilios Mavroudis, Hayden Melton", "docs_id": "1910.00321", "section": ["cs.CR", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Libra: Fair Order-Matching for Electronic Financial Exchanges. While historically, economists have been primarily occupied with analyzing the behaviour of the markets, electronic trading gave rise to a new class of unprecedented problems associated with market fairness, transparency and manipulation. These problems stem from technical shortcomings that are not accounted for in the simple conceptual models used for theoretical market analysis. They, thus, call for more pragmatic market design methodologies that consider the various infrastructure complexities and their potential impact on the market procedures. First, we formally define temporal fairness and then explain why it is very difficult for order-matching policies to ensure it in continuous markets. Subsequently, we introduce a list of system requirements and evaluate existing \"fair\" market designs in various practical and adversarial scenarios. We conclude that they fail to retain their properties in the presence of infrastructure inefficiencies and sophisticated technical manipulation attacks. Based on these findings, we then introduce Libra, a \"fair\" policy that is resilient to gaming and tolerant of technical complications. Our security analysis shows that it is significantly more robust than existing designs, while Libra's deployment (in a live foreign currency exchange) validated both its considerably low impact on the operation of the market and its ability to reduce speed-based predatory trading."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary motivation behind the development of Libra, a \"fair\" order-matching policy, in the context of electronic financial exchanges?\n\n**A)** To reduce transaction costs and increase market efficiency\n**B)** To prevent market manipulation and ensure temporal fairness in continuous markets\n**C)** To improve market transparency and facilitate faster trading\n**D)** To enhance the security of financial transactions and protect against cyber attacks\n\n**Correct Answer:** B) To prevent market manipulation and ensure temporal fairness in continuous markets\n\n**Explanation:** The question requires the test-taker to understand the context of electronic financial exchanges and the challenges associated with market fairness, transparency, and manipulation. The correct answer, B, is supported by the documentation, which states that Libra is designed to prevent market manipulation and ensure temporal fairness in continuous markets. The other options, while related to financial markets, are not the primary motivation behind the development of Libra.\n\n**Additional explanation:** The test-taker should be able to identify the pros and cons of different market design methodologies and understand the limitations of existing \"fair\" market designs. They should also be able to analyze the security analysis of Libra and its deployment in a live foreign currency exchange, as mentioned in the documentation. This requires a deep understanding of the subject matter and the ability to evaluate complex information."}, "41": {"documentation": {"title": "Quasi-simultaneous 43 and 86 GHz SiO Maser Observations and Potential\n  Bias in the BAaDE Survey Are Resolved", "source": "Michael C. Stroh, Ylva M. Pihlstr\\\"om, Lor\\`ant O. Sjouwerman, Mark J.\n  Claussen, Mark R. Morris, and Michael R. Rich", "docs_id": "1808.02899", "section": ["astro-ph.SR", "astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quasi-simultaneous 43 and 86 GHz SiO Maser Observations and Potential\n  Bias in the BAaDE Survey Are Resolved. We observed the 43 GHz v=1, 2, and 3 and 86 GHz v=1 SiO maser transitions quasi-simultaneously for a Mira-variable-dominated sample of over 80 sources from the Bulge Asymmetries and Dynamical Evolution (BAaDE) project, using ATCA, and statistically compared the relative line strengths. On average, the 43 GHz v=1 line is brighter than the 86 GHz v=1 line by a factor of 1.36+/-0.15. As a result, an 86 GHz v=1 observed sample can be observed to 85.9% of the distance of a 43 GHz v=1 observed sample using the same sensitivity. We discuss what impact this may have on the BAaDE Galactic plane survey using the VLA and ALMA. Despite fewer v=3 detections, specific trends are discerned or strengthened when the 43 GHz v=3 line is detected. In particular the 43 and 86 GHz v=1 lines are on average equal for sources with no detectable 43 GHz v=3 emission, but the 43 GHz v=1 line strength is on average about twice as bright as the 86 GHz v=1 line for sources with detectable 43 GHz v=3 emission. Some weak correlations are found between line strengths and Midcourse Space Experiment flux densities and colors, which are tightened when considering only sources with detectable 43 GHz v=3 emission. We discuss these trends in the context of a radiative pumping model to highlight how the 43 GHz v=3 line, when coupled with the v=1 and v=2 lines, can further our understanding of variable conditions like density in the circumstellar envelopes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the relative line strengths of the 43 GHz v=1 and 86 GHz v=1 SiO maser transitions, and how does this impact the BAaDE Galactic plane survey?\n\nA) The 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference has no significant impact on the survey's sensitivity.\nB) The 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference allows for the observation of 85.9% of the distance of a 43 GHz v=1 observed sample using the same sensitivity.\nC) The 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference is only significant for sources with detectable 43 GHz v=3 emission.\nD) The 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference is not statistically significant and should be ignored in the survey.\n\nCorrect Answer: B) The 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference allows for the observation of 85.9% of the distance of a 43 GHz v=1 observed sample using the same sensitivity.\n\nExplanation: The correct answer is B because the text states that the 43 GHz v=1 line is on average 1.36 times brighter than the 86 GHz v=1 line, and this difference allows for the observation of 85.9% of the distance of a 43 GHz v=1 observed sample using the same sensitivity. This is a key finding in the study, and it has significant implications for the BAaDE Galactic plane survey. The other options are incorrect because they either downplay or misrepresent the significance of the difference between the two line strengths."}, "42": {"documentation": {"title": "Business disruptions from social distancing", "source": "Mikl\\'os Koren, Rita Pet\\H{o}", "docs_id": "2003.13983", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Business disruptions from social distancing. Social distancing interventions can be effective against epidemics but are potentially detrimental for the economy. Businesses that rely heavily on face-to-face communication or close physical proximity when producing a product or providing a service are particularly vulnerable. There is, however, no systematic evidence about the role of human interactions across different lines of business and about which will be the most limited by social distancing. Here we provide theory-based measures of the reliance of U.S. businesses on human interaction, detailed by industry and geographic location. We find that 49 million workers work in occupations that rely heavily on face-to-face communication or require close physical proximity to other workers. Our model suggests that when businesses are forced to reduce worker contacts by half, they need a 12 percent wage subsidy to compensate for the disruption in communication. Retail, hotels and restaurants, arts and entertainment and schools are the most affected sectors. Our results can help target fiscal assistance to businesses that are most disrupted by social distancing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the potential economic impact of social distancing interventions on various industries in the United States. How might the findings of this study inform fiscal assistance policies aimed at mitigating the effects of social distancing on businesses?\n\n**A)** The study suggests that industries with high levels of human interaction, such as retail and hospitality, will require significant wage subsidies to compensate for the disruption in communication caused by social distancing. However, this may not be sufficient to fully offset the losses, and alternative forms of support, such as tax breaks or subsidies for essential services, may be necessary.\n\n**B)** The study's findings indicate that industries with low levels of human interaction, such as technology and manufacturing, will be less affected by social distancing and may even benefit from the reduced labor costs. Therefore, fiscal assistance policies should focus on supporting these industries rather than those with high levels of human interaction.\n\n**C)** The study's results suggest that a 12% wage subsidy is necessary to compensate for the disruption in communication caused by social distancing. However, this figure may vary depending on the specific industry and geographic location, and policymakers should consider these factors when designing fiscal assistance policies.\n\n**D)** The study's findings indicate that social distancing interventions will have a disproportionate impact on small businesses and startups, which rely heavily on face-to-face communication and close physical proximity to other workers. Therefore, fiscal assistance policies should prioritize support for these businesses to help them weather the economic disruption caused by social distancing.\n\n**Correct Answer:** C) The study's results suggest that a 12% wage subsidy is necessary to compensate for the disruption in communication caused by social distancing. However, this figure may vary depending on the specific industry and geographic location, and policymakers should consider these factors when designing fiscal assistance policies.\n\n**Explanation:** The correct answer is based on the study's finding that a 12% wage subsidy is necessary to compensate for the disruption in communication caused by social distancing. However, the study also notes that this figure may vary depending on the specific industry and geographic location, and policymakers should consider these factors when designing fiscal assistance policies. This requires the test-taker to analyze the study's findings and consider the complexities of the issue, rather than simply recalling a single figure or statistic."}, "43": {"documentation": {"title": "Optical Crystals and Light-Bullets in Kerr Resonators", "source": "M. Tlidi, S. S. Gopalakrishnan, M. Taki, and K. Panajotov", "docs_id": "2107.14489", "section": ["physics.optics", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optical Crystals and Light-Bullets in Kerr Resonators. Stable light bullets and clusters of them are presented in the monostable regime using the mean-field Lugiato-Lefever equation [Gopalakrishnan, Panajotov, Taki, and Tlidi, Phys. Rev. Lett. 126, 153902 (2021)]. It is shown that three-dimensional (3D) dissipative structures occur in a strongly nonlinear regime where modulational instability is subcritical. We provide a detailed analysis on the formation of optical 3D crystals in both the super- and sub-critical modulational instability regimes, and we highlight their link to the formation of light bullets in diffractive and dispersive Kerr resonators. We construct bifurcation diagrams associated with the formation of optical crystals in both monostable and bistable regimes. An analytical study has predicted the predominance of body-centered-cubic (bcc) crystals in the intracavity field over a large variety of other 3D solutions with less symmetry. These results have been obtained using a weakly nonlinear analysis but have never been checked numerically. We show numerically that indeed the most robust structures over other self-organized crystals are the bcc crystals. Finally, we show that light-bullets and clusters of them can occur also in a bistable regime."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary type of crystal structure that forms in the intracavity field of a Kerr resonator, according to the analytical study and numerical simulations presented in the paper?\n\nA) Face-centered-cubic (fcc) crystals\nB) Body-centered-cubic (bcc) crystals\nC) Hexagonal close-packed (hcp) crystals\nD) Diamond cubic crystals\n\nCorrect Answer: B) Body-centered-cubic (bcc) crystals\n\nExplanation: The paper states that an analytical study has predicted the predominance of body-centered-cubic (bcc) crystals in the intracavity field, and numerical simulations have confirmed this result. This suggests that bcc crystals are the most robust and stable structures that form in this regime.\n\nNote: The other options (A, C, and D) are incorrect because they are not mentioned as the primary type of crystal structure that forms in the paper."}, "44": {"documentation": {"title": "Increasing the Raw Key Rate in Energy-Time Entanglement Based Quantum\n  Key Distribution", "source": "Esmaeil Karimi, Emina Soljanin, and Philip Whiting", "docs_id": "2001.09049", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Increasing the Raw Key Rate in Energy-Time Entanglement Based Quantum\n  Key Distribution. A Quantum Key Distribution (QKD) protocol describes how two remote parties can establish a secret key by communicating over a quantum and a public classical channel that both can be accessed by an eavesdropper. QKD protocols using energy-time entangled photon pairs are of growing practical interest because of their potential to provide a higher secure key rate over long distances by carrying multiple bits per entangled photon pair. We consider a system where information can be extracted by measuring random times of a sequence of entangled photon arrivals. Our goal is to maximize the utility of each such pair. We propose a discrete time model for the photon arrival process, and establish a theoretical bound on the number of raw bits that can be generated under this model. We first analyse a well known simple binning encoding scheme, and show that it generates significantly lower information rate than what is theoretically possible. We then propose three adaptive schemes that increase the number of raw bits generated per photon, and compute and compare the information rates they offer. Moreover, the effect of public channel communication on the secret key rates of the proposed schemes is investigated."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Energy-Time Entanglement Based Quantum Key Distribution, what is the primary goal of the proposed discrete time model for the photon arrival process, and how does it relate to the theoretical bound on the number of raw bits that can be generated?\n\n**A)** To minimize the impact of public channel communication on the secret key rates, while maximizing the information rate per photon.\n**B)** To establish a theoretical bound on the number of raw bits that can be generated, while analyzing the performance of a simple binning encoding scheme.\n**C)** To increase the number of raw bits generated per photon, while investigating the effect of public channel communication on the secret key rates of adaptive schemes.\n**D)** To develop a discrete time model for the photon arrival process, which can be used to maximize the utility of each entangled photon pair.\n\n**Correct Answer:** D) To develop a discrete time model for the photon arrival process, which can be used to maximize the utility of each entangled photon pair.\n\n**Explanation:** The correct answer is D) because the primary goal of the proposed discrete time model is to develop a mathematical framework for understanding the photon arrival process, which can be used to maximize the utility of each entangled photon pair. This is stated in the introduction of the paper: \"Our goal is to maximize the utility of each such pair.\" The other options are incorrect because they either focus on a specific aspect of the problem (A and C) or are too narrow in scope (B)."}, "45": {"documentation": {"title": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies", "source": "Zhengyang Dong", "docs_id": "1805.12111", "section": ["q-fin.ST", "cs.CE", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies. Stock trend prediction is a challenging task due to the market's noise, and machine learning techniques have recently been successful in coping with this challenge. In this research, we create a novel framework for stock prediction, Dynamic Advisor-Based Ensemble (dynABE). dynABE explores domain-specific areas based on the companies of interest, diversifies the feature set by creating different \"advisors\" that each handles a different area, follows an effective model ensemble procedure for each advisor, and combines the advisors together in a second-level ensemble through an online update strategy we developed. dynABE is able to adapt to price pattern changes of the market during the active trading period robustly, without needing to retrain the entire model. We test dynABE on three cobalt-related companies, and it achieves the best-case misclassification error of 31.12% and an annualized absolute return of 359.55% with zero maximum drawdown. dynABE also consistently outperforms the baseline models of support vector machine, neural network, and random forest in all case studies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Dynamic Advisor-Based Ensemble (dynABE) framework in stock trend prediction, and how does it differ from traditional machine learning approaches?\n\n**A)** dynABE's ability to handle market noise is its primary advantage, and it differs from traditional approaches by using a second-level ensemble procedure to combine advisors.\n\n**B)** dynABE's ability to adapt to price pattern changes is its primary advantage, and it differs from traditional approaches by using online update strategies to retrain the model.\n\n**C)** dynABE's ability to diversify the feature set is its primary advantage, and it differs from traditional approaches by creating different \"advisors\" that handle different areas of interest.\n\n**D)** dynABE's ability to achieve high accuracy is its primary advantage, and it differs from traditional approaches by using a combination of machine learning techniques.\n\n**Correct Answer:** C) dynABE's ability to diversify the feature set is its primary advantage, and it differs from traditional approaches by creating different \"advisors\" that handle different areas of interest.\n\n**Explanation:** The correct answer is C) because the documentation states that dynABE \"diversifies the feature set by creating different 'advisors' that each handles a different area\" and that this is a key aspect of the framework. This allows dynABE to adapt to price pattern changes and achieve robust performance in stock trend prediction. The other options are incorrect because while dynABE does handle market noise and adapt to price pattern changes, these are not its primary advantages. Additionally, dynABE's ability to achieve high accuracy is not its primary advantage, but rather a result of its ability to diversify the feature set and adapt to market changes."}, "46": {"documentation": {"title": "Metasurface Freeform Nanophotonics", "source": "Alan Zhan, Shane Colburn, Christopher M. Dodson, Arka Majumdar", "docs_id": "1610.00019", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Metasurface Freeform Nanophotonics. Freeform optics aims to expand the toolkit of optical elements by allowing for more complex phase geometries beyond rotational symmetry. Complex, asymmetric curvatures are employed to enhance the performance of optical components while minimizing their weight and size. Unfortunately, these asymmetric forms are often difficult to manufacture at the nanoscale with current technologies. Metasurfaces are planar sub-wavelength structures that can control the phase, amplitude, and polarization of incident light, and can thereby mimic complex geometric curvatures on a flat, wavelength-scale thick surface. We present a methodology for designing analogues of freeform optics using a low contrast dielectric metasurface platform for operation at visible wavelengths. We demonstrate a cubic phase plate with a point spread function exhibiting enhanced depth of field over 300 {\\mu}m along the optical axis with potential for performing metasurface-based white light imaging, and an Alvarez lens with a tunable focal length range of over 2.5 mm with 100 {\\mu}m of total mechanical displacement. The adaptation of freeform optics to a sub-wavelength metasurface platform allows for the ultimate miniaturization of optical components and offers a scalable route toward implementing near-arbitrary geometric curvatures in nanophotonics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using metasurfaces in freeform optics, and how does it enable the miniaturization of optical components?\n\nA) Metasurfaces enable the creation of complex phase geometries, allowing for the manipulation of light at the nanoscale.\nB) Metasurfaces can control the phase, amplitude, and polarization of incident light, mimicking complex geometric curvatures on a flat surface.\nC) Metasurfaces are limited to operation at specific wavelengths, restricting their application in freeform optics.\nD) Metasurfaces are difficult to manufacture at the nanoscale, making them impractical for use in freeform optics.\n\nCorrect Answer: B) Metasurfaces can control the phase, amplitude, and polarization of incident light, mimicking complex geometric curvatures on a flat surface.\n\nExplanation: The correct answer highlights the key benefit of using metasurfaces in freeform optics, which is their ability to control the phase, amplitude, and polarization of incident light. This allows for the creation of complex phase geometries, enabling the manipulation of light at the nanoscale and the miniaturization of optical components. The other options are incorrect because they either oversimplify the advantage of metasurfaces (A) or introduce incorrect limitations (C and D)."}, "47": {"documentation": {"title": "Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right\n  Ventricle", "source": "Zhao-Yang Wang, Xiao-Yun Zhou, Peichao Li, and Celia Riga, and\n  Guang-Zhong Yang", "docs_id": "1909.08986", "section": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Instantiation-Net: 3D Mesh Reconstruction from Single 2D Image for Right\n  Ventricle. 3D shape instantiation which reconstructs the 3D shape of a target from limited 2D images or projections is an emerging technique for surgical intervention. It improves the currently less-informative and insufficient 2D navigation schemes for robot-assisted Minimally Invasive Surgery (MIS) to 3D navigation. Previously, a general and registration-free framework was proposed for 3D shape instantiation based on Kernel Partial Least Square Regression (KPLSR), requiring manually segmented anatomical structures as the pre-requisite. Two hyper-parameters including the Gaussian width and component number also need to be carefully adjusted. Deep Convolutional Neural Network (DCNN) based framework has also been proposed to reconstruct a 3D point cloud from a single 2D image, with end-to-end and fully automatic learning. In this paper, an Instantiation-Net is proposed to reconstruct the 3D mesh of a target from its a single 2D image, by using DCNN to extract features from the 2D image and Graph Convolutional Network (GCN) to reconstruct the 3D mesh, and using Fully Connected (FC) layers to connect the DCNN to GCN. Detailed validation was performed to demonstrate the practical strength of the method and its potential clinical use."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the previously proposed general framework for 3D shape instantiation, and how does the proposed Instantiation-Net address this limitation?\n\n**A)** The previously proposed framework requires manual segmentation of anatomical structures, which is time-consuming and prone to errors. Instantiation-Net addresses this limitation by using a registration-free approach.\n\n**B)** The previously proposed framework requires careful adjustment of two hyper-parameters, including the Gaussian width and component number, to achieve accurate results. Instantiation-Net addresses this limitation by using a deep convolutional neural network (DCNN) to extract features from the 2D image.\n\n**C)** The previously proposed framework is limited by its reliance on 2D navigation schemes, which are insufficient for accurate surgical intervention. Instantiation-Net addresses this limitation by reconstructing the 3D shape of the target from a single 2D image.\n\n**D)** The previously proposed framework is limited by its inability to handle complex anatomical structures, which are common in right ventricle surgery. Instantiation-Net addresses this limitation by using a graph convolutional network (GCN) to reconstruct the 3D mesh.\n\n**Correct Answer:** C) The previously proposed framework is limited by its reliance on 2D navigation schemes, which are insufficient for accurate surgical intervention. Instantiation-Net addresses this limitation by reconstructing the 3D shape of the target from a single 2D image.\n\n**Explanation:** The correct answer is C) because the previously proposed framework was limited by its reliance on 2D navigation schemes, which are insufficient for accurate surgical intervention. Instantiation-Net addresses this limitation by reconstructing the 3D shape of the target from a single 2D image, which is a significant improvement over the previous framework. The other options are incorrect because they do not accurately summarize the limitations of the previously proposed framework and the advantages of Instantiation-Net."}, "48": {"documentation": {"title": "Thin homotopy and the holonomy approach to gauge theories", "source": "Claudio Meneses", "docs_id": "1904.10822", "section": ["math-ph", "math.DG", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Thin homotopy and the holonomy approach to gauge theories. We survey several mathematical developments in the holonomy approach to gauge theory. A cornerstone of this approach is the introduction of group structures on spaces of based loops on a smooth manifold, relying on certain homotopy equivalence relations -- such as the so-called thin homotopy -- and the resulting interpretation of gauge fields as group homomorphisms to a Lie group $G$ satisfying a suitable smoothness condition, encoding the holonomy of a gauge orbit of smooth connections on a principal $G$-bundle. We also prove several structural results on thin homotopy, and in particular we clarify the difference between thin equivalence and retrace equivalence for piecewise-smooth based loops on a smooth manifold, which are often used interchangeably in the physics literature. We conclude by listing a set of questions on topological and functional analytic aspects of groups of based loops, which we consider to be fundamental to establish a rigorous differential geometric foundation of the holonomy formulation of gauge theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a principal $G$-bundle $P \\to M$ over a smooth manifold $M$, where $G$ is a Lie group. Suppose we have a smooth connection $\\nabla$ on $P$ and a based loop $\\gamma: [0,1] \\to M$ based at a point $p \\in M$. What is the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$, and how does it relate to the group structure on the space of based loops on $M$?\n\nA) The holonomy is a smooth map from the space of based loops on $M$ to the Lie group $G$, and it encodes the gauge transformation of the connection $\\nabla$ along the loop $\\gamma$.\n\nB) The holonomy is a Lie group homomorphism from the space of based loops on $M$ to the Lie group $G$, and it satisfies a smoothness condition that encodes the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$.\n\nC) The holonomy is a smooth map from the space of based loops on $M$ to the Lie group $G$, but it does not satisfy a smoothness condition that encodes the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$.\n\nD) The holonomy is a Lie group homomorphism from the space of based loops on $M$ to the Lie group $G$, but it is not a smooth map.\n\nCorrect Answer: B) The holonomy is a Lie group homomorphism from the space of based loops on $M$ to the Lie group $G$, and it satisfies a smoothness condition that encodes the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$.\n\nExplanation: The correct answer is B) because the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$ is indeed a Lie group homomorphism from the space of based loops on $M$ to the Lie group $G$. This homomorphism encodes the gauge transformation of the connection $\\nabla$ along the loop $\\gamma$, and it satisfies a smoothness condition that encodes the holonomy of the gauge orbit of $\\nabla$ along $\\gamma$. The correct answer requires an understanding of the holonomy approach to gauge theory and the group structure on the space of based loops on $M$."}, "49": {"documentation": {"title": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling", "source": "Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang", "docs_id": "1803.04687", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling. This paper proposes a new method called Multimodal RNNs for RGB-D scene semantic segmentation. It is optimized to classify image pixels given two input sources: RGB color channels and Depth maps. It simultaneously performs training of two recurrent neural networks (RNNs) that are crossly connected through information transfer layers, which are learnt to adaptively extract relevant cross-modality features. Each RNN model learns its representations from its own previous hidden states and transferred patterns from the other RNNs previous hidden states; thus, both model-specific and crossmodality features are retained. We exploit the structure of quad-directional 2D-RNNs to model the short and long range contextual information in the 2D input image. We carefully designed various baselines to efficiently examine our proposed model structure. We test our Multimodal RNNs method on popular RGB-D benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of using information transfer layers in the proposed Multimodal RNNs architecture for RGB-D scene semantic segmentation?\n\nA) They enable the RNNs to learn independent representations for each input source.\nB) They allow the RNNs to adaptively extract relevant cross-modality features and retain both model-specific and cross-modality features.\nC) They improve the efficiency of the RNNs by reducing the number of parameters.\nD) They enhance the robustness of the RNNs to variations in the input data.\n\nCorrect Answer: B) They allow the RNNs to adaptively extract relevant cross-modality features and retain both model-specific and cross-modality features.\n\nExplanation: The correct answer is B) because the documentation states that the information transfer layers are learnt to adaptively extract relevant cross-modality features, and that both model-specific and cross-modality features are retained. This is a key advantage of the proposed architecture, as it enables the RNNs to learn from both the RGB and depth maps and retain useful information from both sources. The other options are incorrect because they do not accurately reflect the primary advantage of the information transfer layers. Option A is incorrect because the RNNs do not learn independent representations for each input source. Option C is incorrect because the documentation does not mention anything about improving the efficiency of the RNNs. Option D is incorrect because the documentation does not mention anything about enhancing the robustness of the RNNs."}, "50": {"documentation": {"title": "Lateral Casimir Force between Two Sinusoidally Corrugated Eccentric\n  Cylinders Using Proximity Force Approximation", "source": "M. R. Setare, and A. Seyedzahedi", "docs_id": "1402.3652", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lateral Casimir Force between Two Sinusoidally Corrugated Eccentric\n  Cylinders Using Proximity Force Approximation. This paper is devoted to the presentation of the lateral Casimir force between two sinusoidally corrugated eccentric cylinders. Despite that applying scattering matrix method explains the problem exactly, procedure of applying this method is somehow complicated specially at nonzero temperature. Using the proximity force approximation (PFA) helps to achieve the lateral Casimir force in a truly explicit manner. We assume the cylinders to be slightly eccentric with similar radiuses and separations much smaller than corrugations' wave length for the validity of PFA. For such short distances the effect of finite conductivity would be non negligible. In addition to the effect of finite conductivity, we investigate thermal corrections of the lateral Casimir force to reduce the inaccuracy of the result obtained by PFA. Assuming the Casimir force density between two parallel plates, the normal Casimir force between two cylinders is obtained. With the aid of additive summation of the Casimir energy between cylinders without corrugation, we obtain the lateral Casimir force between corrugated cylinders."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary assumption made in the Proximity Force Approximation (PFA) for calculating the lateral Casimir force between two sinusoidally corrugated eccentric cylinders?\n\nA) The cylinders are assumed to be identical in terms of radius and separation.\nB) The cylinders are assumed to be slightly eccentric with similar radiuses and separations much smaller than corrugations' wavelength.\nC) The cylinders are assumed to be parallel to each other.\nD) The cylinders are assumed to be non-conductive.\n\nCorrect Answer: B) The cylinders are assumed to be slightly eccentric with similar radiuses and separations much smaller than corrugations' wavelength.\n\nExplanation: The correct answer is B) because the documentation states that the cylinders are assumed to be \"slightly eccentric with similar radiuses and separations much smaller than corrugations' wavelength\" for the validity of the PFA. This assumption is crucial in simplifying the calculation of the lateral Casimir force. The other options are incorrect because they do not accurately reflect the assumptions made in the PFA. Option A is incorrect because the cylinders are not assumed to be identical, but rather slightly eccentric. Option C is incorrect because the cylinders are not assumed to be parallel, but rather eccentric. Option D is incorrect because the cylinders are assumed to be conductive, not non-conductive."}, "51": {"documentation": {"title": "TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image\n  Enhancement", "source": "Xinxu Wei, Xianshi Zhang, Shisen Wang, Yanlin Huang, and Yongjie Li", "docs_id": "2110.02477", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image\n  Enhancement. Low-light image enhancement is a challenging low-level computer vision task because after we enhance the brightness of the image, we have to deal with amplified noise, color distortion, detail loss, blurred edges, shadow blocks and halo artifacts. In this paper, we propose a Two-Stage Network with Channel Attention (denoted as TSN-CA) to enhance the brightness of the low-light image and restore the enhanced images from various kinds of degradation. In the first stage, we enhance the brightness of the low-light image in HSV space and use the information of H and S channels to help the recovery of details in V channel. In the second stage, we integrate Channel Attention (CA) mechanism into the skip connection of U-Net in order to restore the brightness-enhanced image from severe kinds of degradation in RGB space. We train and evaluate the performance of our proposed model on the LOL real-world and synthetic datasets. In addition, we test our model on several other commonly used datasets without Ground-Truth. We conduct extensive experiments to demonstrate that our method achieves excellent effect on brightness enhancement as well as denoising, details preservation and halo artifacts elimination. Our method outperforms many other state-of-the-art methods qualitatively and quantitatively."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed Two-Stage Network with Channel Attention (TSN-CA) in the paper \"TSN-CA: A Two-Stage Network with Channel Attention for Low-Light Image Enhancement\"?\n\n**A)** To enhance the color balance of low-light images\n**B)** To restore the details and reduce noise in low-light images\n**C)** To improve the brightness and reduce halo artifacts in low-light images\n**D)** To denoise and enhance the texture of low-light images\n\n**Correct Answer:** C) To improve the brightness and reduce halo artifacts in low-light images\n\n**Explanation:** The paper states that low-light image enhancement is a challenging task that requires dealing with amplified noise, color distortion, detail loss, blurred edges, shadow blocks, and halo artifacts. The proposed TSN-CA model aims to enhance the brightness of low-light images and restore the enhanced images from various kinds of degradation. The first stage of the model enhances brightness in HSV space, and the second stage uses Channel Attention to restore the brightness-enhanced image from severe kinds of degradation in RGB space. Therefore, the primary goal of TSN-CA is to improve the brightness and reduce halo artifacts in low-light images."}, "52": {"documentation": {"title": "Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting", "source": "Michael L. Wagman and Martin J. Savage", "docs_id": "1704.07356", "section": ["hep-lat", "cond-mat.stat-mech", "cond-mat.str-el", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting. Path integrals describing quantum many-body systems can be calculated with Monte Carlo sampling techniques, but average quantities are often subject to signal-to-noise ratios that degrade exponentially with time. A phase-reweighting technique inspired by recent observations of random walk statistics in correlation functions is proposed that allows energy levels to be extracted from late-time correlation functions with time-independent signal-to-noise ratios. Phase reweighting effectively includes dynamical refinement of source magnitudes but introduces a bias associated with the phase. This bias can be removed by performing an extrapolation, but at the expense of re-introducing a signal-to-noise problem. Lattice Quantum Chromodynamics calculations of the $\\rho$ and nucleon masses and of the $\\Xi\\Xi$ binding energy show consistency between standard results obtained using earlier-time correlation functions and phase-reweighted results using late-time correlation functions inaccessible to standard statistical analysis methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of lattice QCD, what is the primary advantage of using phase reweighting to extract energy levels from late-time correlation functions, and what is the associated limitation?\n\n**A)** Phase reweighting allows for the extraction of energy levels with time-independent signal-to-noise ratios, but introduces a bias that can be removed through extrapolation. However, this bias may lead to a re-introduction of a signal-to-noise problem.\n\n**B)** Phase reweighting enables the calculation of average quantities with exponentially improving signal-to-noise ratios, but introduces a bias that can only be mitigated through the use of advanced statistical analysis techniques.\n\n**C)** Phase reweighting allows for the extraction of energy levels with exponentially improving signal-to-noise ratios, but introduces a bias that can only be removed through the use of early-time correlation functions.\n\n**D)** Phase reweighting enables the calculation of average quantities with exponentially improving signal-to-noise ratios, but introduces a bias that can be removed through extrapolation, allowing for the extraction of energy levels with time-independent signal-to-noise ratios.\n\n**Correct Answer:** D) Phase reweighting enables the calculation of average quantities with exponentially improving signal-to-noise ratios, but introduces a bias that can be removed through extrapolation, allowing for the extraction of energy levels with time-independent signal-to-noise ratios.\n\n**Explanation:** The correct answer, D, accurately reflects the primary advantage of phase reweighting, which is the ability to extract energy levels from late-time correlation functions with time-independent signal-to-noise ratios. However, it also acknowledges the associated limitation, which is the introduction of a bias that can be removed through extrapolation. The other options either misrepresent the primary advantage of phase reweighting or introduce incorrect limitations."}, "53": {"documentation": {"title": "A rate of convergence result for the largest eigenvalue of complex white\n  Wishart matrices", "source": "Noureddine El Karoui", "docs_id": "math/0409610", "section": ["math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A rate of convergence result for the largest eigenvalue of complex white\n  Wishart matrices. It has been recently shown that if $X$ is an $n\\times N$ matrix whose entries are i.i.d. standard complex Gaussian and $l_1$ is the largest eigenvalue of $X^*X$, there exist sequences $m_{n,N}$ and $s_{n,N}$ such that $(l_1-m_{n,N})/s_{n,N}$ converges in distribution to $W_2$, the Tracy--Widom law appearing in the study of the Gaussian unitary ensemble. This probability law has a density which is known and computable. The cumulative distribution function of $W_2$ is denoted $F_2$. In this paper we show that, under the assumption that $n/N\\to \\gamma\\in(0,\\infty)$, we can find a function $M$, continuous and nonincreasing, and sequences $\\tilde{\\mu}_{n,N}$ and $\\tilde{\\sigma}_{n,N}$ such that, for all real $s_0$, there exists an integer $N(s_0,\\gamma)$ for which, if $(n\\wedge N)\\geq N(s_0,\\gamma)$, we have, with $l_{n,N}=(l_1-\\tilde{\\mu}_{n,N})/\\tilde{\\sigma}_{n,N}$, \\[\\forall s\\geq s_0\\qquad (n\\wedge N)^{2/3}|P(l_{n,N}\\leq s)-F_2(s)|\\leq M(s_0)\\exp(-s).\\] The surprisingly good 2/3 rate and qualitative properties of the bounding function help explain the fact that the limiting distribution $W_2$ is a good approximation to the empirical distribution of $l_{n,N}$ in simulations, an important fact from the point of view of (e.g., statistical) applications."}, "answer": "Here is the exam question:\n\nWhat is the rate of convergence result for the largest eigenvalue of complex white Wishart matrices, and how does it relate to the Tracy-Widom law?\n\nA) The rate of convergence is 1/3, and the limiting distribution is a Gaussian process.\nB) The rate of convergence is 2/3, and the limiting distribution is the Tracy-Widom law, which has a density that is known and computable.\nC) The rate of convergence is 1/2, and the limiting distribution is a Poisson process.\nD) The rate of convergence is 3/4, and the limiting distribution is a Brownian motion.\n\nCorrect Answer: B) The rate of convergence is 2/3, and the limiting distribution is the Tracy-Widom law, which has a density that is known and computable.\n\nExplanation: The question requires the test-taker to understand the main result of the paper, which is that the rate of convergence of the largest eigenvalue of complex white Wishart matrices is 2/3, and the limiting distribution is the Tracy-Widom law. The correct answer is B) The rate of convergence is 2/3, and the limiting distribution is the Tracy-Widom law, which has a density that is known and computable. The other options are incorrect because they do not accurately describe the main result of the paper. Option A is incorrect because the rate of convergence is not 1/3, and the limiting distribution is not a Gaussian process. Option C is incorrect because the rate of convergence is not 1/2, and the limiting distribution is not a Poisson process. Option D is incorrect because the rate of convergence is not 3/4, and the limiting distribution is not a Brownian motion."}, "54": {"documentation": {"title": "An Improved Quadrature Voltage-Controlled Oscillator with\n  Through-Silicon-Via Inductor in Three-dimensional Integrated Circuits", "source": "Dawei Li, Madhava Sarma Vemuri, Umamaheswara Rao Tida", "docs_id": "2001.10678", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Improved Quadrature Voltage-Controlled Oscillator with\n  Through-Silicon-Via Inductor in Three-dimensional Integrated Circuits. Low-power quadrature voltage-controlled oscillator (QVCO) design utilizing transformer-feedback and current-reuse techniques with increased frequency range is proposed in this paper. With increasing demand for QVCOs in on-chip applications, the conventional spiral inductor based approaches for QVCOs has become a major bottleneck due to their large size. To address this concern, we propose to replace the conventional spiral inductor based approaches with through-silicon-via (TSV) inductor based approach in three-dimensional integrated circuits (3D ICs). In addition, the proposed QVCO circuit can provide higher frequency range of operation compared with conventional designs. Experimental results show by replacing conventional spiral transformers with TSV transformers, up to 3.9x reduction in metal resource consumption. The proposed QVCOs achieves a phase noise of -114 $dBc/Hz$@1 $MHz$ and -111.2 $dBc/Hz$@1 $MHz$ at the carrier of 2.5 $GHz$ for toroidal TSV transformed based-QVCO and vertical spiral transformer based-QVCO respectively. The power consumption is only 1.5 $mW$ and 1.7 $mW$ for toroidal TSV transformed based-QVCO and vertical spiral transformer based-QVCO respectively, under the supply voltage of 0.7 $V$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a Through-Silicon-Via (TSV) inductor based approach in three-dimensional integrated circuits (3D ICs) for quadrature voltage-controlled oscillator (QVCO) design, as proposed in the paper?\n\n**A)** Reduced power consumption\n**B)** Increased frequency range of operation\n**C)** Improved phase noise performance\n**D)** Reduced metal resource consumption\n\n**Correct Answer:** D) Reduced metal resource consumption\n\n**Explanation:** The paper states that replacing conventional spiral inductor based approaches with TSV inductor based approaches in 3D ICs results in \"up to 3.9x reduction in metal resource consumption\". This suggests that the primary advantage of using TSV inductors is the reduction in metal resource consumption, making option D the correct answer.\n\n**Additional explanations:**\n\n* Option A is incorrect because while power consumption is mentioned in the paper, it is not the primary advantage of using TSV inductors.\n* Option B is incorrect because the paper does not mention that the frequency range of operation is improved by using TSV inductors.\n* Option C is incorrect because the paper does not mention that phase noise performance is improved by using TSV inductors. In fact, the paper mentions that the phase noise performance is similar between the TSV and spiral transformer based-QVCOs."}, "55": {"documentation": {"title": "A network approach to cartel detection in public auction markets", "source": "Johannes Wachs, J\\'anos Kert\\'esz", "docs_id": "1906.08667", "section": ["physics.soc-ph", "cs.SI", "econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A network approach to cartel detection in public auction markets. Competing firms can increase profits by setting prices collectively, imposing significant costs on consumers. Such groups of firms are known as cartels and because this behavior is illegal, their operations are secretive and difficult to detect. Cartels feel a significant internal obstacle: members feel short-run incentives to cheat. Here we present a network-based framework to detect potential cartels in bidding markets based on the idea that the chance a group of firms can overcome this obstacle and sustain cooperation depends on the patterns of its interactions. We create a network of firms based on their co-bidding behavior, detect interacting groups, and measure their cohesion and exclusivity, two group-level features of their collective behavior. Applied to a market for school milk, our method detects a known cartel and calculates that it has high cohesion and exclusivity. In a comprehensive set of nearly 150,000 public contracts awarded by the Republic of Georgia from 2011 to 2016, detected groups with high cohesion and exclusivity are significantly more likely to display traditional markers of cartel behavior. We replicate this relationship between group topology and the emergence of cooperation in a simulation model. Our method presents a scalable, unsupervised method to find groups of firms in bidding markets ideally positioned to form lasting cartels."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** A network-based framework is proposed to detect potential cartels in bidding markets. The framework relies on the idea that the chance a group of firms can overcome internal short-run incentives to cheat and sustain cooperation depends on the patterns of their interactions. What two group-level features of collective behavior are measured to detect potential cartels, and how are they related to the emergence of cooperation?\n\n**A)** Cohesion and exclusivity are measured to detect potential cartels, and high cohesion and exclusivity are associated with the emergence of cooperation.\n\n**B)** Cohesion and exclusivity are not measured to detect potential cartels, and the framework relies on traditional markers of cartel behavior.\n\n**C)** Cohesion and exclusivity are not measured to detect potential cartels, and the framework relies on the patterns of firms' co-bidding behavior.\n\n**D)** Cohesion and exclusivity are not measured to detect potential cartels, and the framework relies on the firms' internal incentives to cheat.\n\n**Correct Answer:** A) Cohesion and exclusivity are measured to detect potential cartels, and high cohesion and exclusivity are associated with the emergence of cooperation.\n\n**Explanation:** According to the provided documentation, the network-based framework measures two group-level features of collective behavior: cohesion and exclusivity. The framework detects interacting groups and calculates their cohesion and exclusivity, which are then used to predict the emergence of cooperation. Specifically, the study finds that detected groups with high cohesion and exclusivity are significantly more likely to display traditional markers of cartel behavior. Therefore, option A is the correct answer."}, "56": {"documentation": {"title": "Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training", "source": "Jiatong Shi, Nan Huo, Qin Jin", "docs_id": "2008.08647", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training. Mispronunciation detection is an essential component of the Computer-Assisted Pronunciation Training (CAPT) systems. State-of-the-art mispronunciation detection models use Deep Neural Networks (DNN) for acoustic modeling, and a Goodness of Pronunciation (GOP) based algorithm for pronunciation scoring. However, GOP based scoring models have two major limitations: i.e., (i) They depend on forced alignment which splits the speech into phonetic segments and independently use them for scoring, which neglects the transitions between phonemes within the segment; (ii) They only focus on phonetic segments, which fails to consider the context effects across phonemes (such as liaison, omission, incomplete plosive sound, etc.). In this work, we propose the Context-aware Goodness of Pronunciation (CaGOP) scoring model. Particularly, two factors namely the transition factor and the duration factor are injected into CaGOP scoring. The transition factor identifies the transitions between phonemes and applies them to weight the frame-wise GOP. Moreover, a self-attention based phonetic duration modeling is proposed to introduce the duration factor into the scoring model. The proposed scoring model significantly outperforms baselines, achieving 20% and 12% relative improvement over the GOP model on the phoneme-level and sentence-level mispronunciation detection respectively."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What are the two major limitations of traditional Goodness of Pronunciation (GOP) based scoring models in mispronunciation detection, and how do the proposed Context-aware Goodness of Pronunciation (CaGOP) scoring model address these limitations?\n\n**A)** GOP models are limited by their inability to consider the context effects across phonemes, and they neglect the transitions between phonemes within the segment. The CaGOP model addresses this limitation by introducing a transition factor that identifies the transitions between phonemes and applies them to weight the frame-wise GOP.\n\n**B)** GOP models are limited by their dependence on forced alignment, which splits the speech into phonetic segments and independently uses them for scoring. The CaGOP model addresses this limitation by introducing a self-attention based phonetic duration modeling that introduces the duration factor into the scoring model.\n\n**C)** GOP models are limited by their inability to consider the context effects across phonemes, and they neglect the transitions between phonemes within the segment. The CaGOP model addresses this limitation by introducing a duration factor that is based on the phonetic duration modeling.\n\n**D)** GOP models are limited by their dependence on forced alignment, which splits the speech into phonetic segments and independently uses them for scoring. The CaGOP model addresses this limitation by introducing a transition factor that is based on the phonetic duration modeling.\n\n**Correct Answer:** A) GOP models are limited by their inability to consider the context effects across phonemes, and they neglect the transitions between phonemes within the segment. The CaGOP model addresses this limitation by introducing a transition factor that identifies the transitions between phonemes and applies them to weight the frame-wise GOP.\n\n**Explanation:** The correct answer is A) because it accurately describes the two major limitations of traditional GOP based scoring models, which are their inability to consider the context effects across phonemes and their neglect of the transitions between phonemes within the segment. The CaGOP model addresses these limitations by introducing a transition factor that identifies the transitions between phonemes and applies them to weight the frame-wise GOP."}, "57": {"documentation": {"title": "Photoproduction of $\\Lambda^*$ and $\\Sigma^*$ resonances with\n  $J^P=1/2^-$ off the proton", "source": "Sang-Ho Kim, K. P. Khemchandani, A. Martinez Torres, Seung-il Nam,\n  Atsushi Hosaka", "docs_id": "2101.08668", "section": ["hep-ph", "hep-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Photoproduction of $\\Lambda^*$ and $\\Sigma^*$ resonances with\n  $J^P=1/2^-$ off the proton. We study the photoproduction of the $\\Lambda(1405)$ and $\\Sigma(1400)$ hyperon resonances, the latter of which is not a well established state. We evaluate the $s$-, $t$- and $u$-channel diagrams in the Born approximation by employing the effective Lagrangians. A new ingredient is the inclusion of a nucleon resonance $N^*(1895)$ that is dynamically generated with predictions for its coupling to the $K\\Lambda(1405)$ and $K\\Sigma(1400)$ channels. To extend the applicability of the model to energies beyond the threshold region, we consider a Regge model for the $t$-channel $K$- and $K^*$-exchanges. Our results are in good agreement with the CLAS data available on $\\Lambda(1405)$, while for $\\Sigma(1400)$ we predict observables for its production. We also provide polarization observables for both hyperon productions, which can be useful in future experimental investigations. The present study provides new information on the nucleon resonance $N^*(1895)$ which can be an alternative source for generating the hyperon resonances $\\Lambda(1405)$ and $\\Sigma(1400)$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary motivation behind the inclusion of the nucleon resonance $N^*(1895)$ in the photoproduction of $\\Lambda^*$ and $\\Sigma^*$ resonances, and how does this relate to the generation of the hyperon resonances $\\Lambda(1405)$ and $\\Sigma(1400)$?\n\nA) To improve the model's agreement with CLAS data on $\\Lambda(1405)$ production, the authors include the $N^*(1895)$ resonance to dynamically generate its coupling to the $K\\Lambda(1405)$ and $K\\Sigma(1400)$ channels, providing an alternative source for generating the hyperon resonances.\n\nB) The authors use the Regge model for $t$-channel $K$- and $K^*$-exchanges to extend the applicability of the model to energies beyond the threshold region, but this has no relation to the inclusion of the $N^*(1895)$ resonance.\n\nC) The inclusion of the $N^*(1895)$ resonance is solely to improve the model's agreement with CLAS data on $\\Sigma(1400)$ production, and has no bearing on the generation of the hyperon resonances.\n\nD) The authors' use of effective Lagrangians to evaluate the $s$-, $t$- and $u$-channel diagrams is the primary motivation behind the inclusion of the $N^*(1895)$ resonance, but this is unrelated to the generation of the hyperon resonances.\n\nCorrect Answer: A) To improve the model's agreement with CLAS data on $\\Lambda(1405)$ production, the authors include the $N^*(1895)$ resonance to dynamically generate its coupling to the $K\\Lambda(1405)$ and $K\\Sigma(1400)$ channels, providing an alternative source for generating the hyperon resonances.\n\nExplanation: The correct answer is A) because the documentation states that the authors include the $N^*(1895)$ resonance to dynamically generate its coupling to the $K\\Lambda(1405)$ and $K\\Sigma(1400)$ channels, providing an alternative source for generating the hyperon resonances. This is the primary motivation behind the inclusion of the $N^*(1895)$ resonance. The other options are incorrect because they either misrepresent the motivation behind the inclusion of the $N^*(1895)$ resonance or are unrelated to the generation of the hyperon resonances."}, "58": {"documentation": {"title": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?", "source": "Marina Friedrich, S\\'ebastien Fries, Michael Pahle and Ottmar\n  Edenhofer", "docs_id": "1906.10572", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?. In 2018, allowance prices in the EU Emission Trading Scheme (EU ETS) experienced a run-up from persistently low levels in previous years. Regulators attribute this to a comprehensive reform in the same year, and are confident the new price level reflects an anticipated tighter supply of allowances. We ask if this is indeed the case, or if it is an overreaction of the market driven by speculation. We combine several econometric methods - time-varying coefficient regression, formal bubble detection as well as time stamping and crash odds prediction - to juxtapose the regulators' claim versus the concurrent explanation. We find evidence of a long period of explosive behaviour in allowance prices, starting in March 2018 when the reform was adopted. Our results suggest that the reform triggered market participants into speculation, and question regulators' confidence in its long-term outcome. This has implications for both the further development of the EU ETS, and the long lasting debate about taxes versus emission trading schemes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the findings of the study on the EU ETS price trend in 2018. What can be inferred about the role of speculation in the explosive trend in allowance prices, and what implications does this have for the EU ETS and the debate on taxes versus emission trading schemes?\n\n**A)** The study suggests that the EU ETS price trend in 2018 was primarily driven by a tightening supply of allowances, as regulators claimed. The findings indicate that market participants were overly optimistic about the long-term outcome of the reform.\n\n**B)** The study reveals that the EU ETS price trend in 2018 was characterized by a long period of explosive behavior, starting in March 2018, which was triggered by speculation among market participants. The findings question regulators' confidence in the long-term outcome of the reform.\n\n**C)** The study concludes that the EU ETS price trend in 2018 was an overreaction of the market driven by speculation, rather than a reflection of the anticipated tightening supply of allowances. The findings have significant implications for the further development of the EU ETS.\n\n**D)** The study suggests that the EU ETS price trend in 2018 was primarily driven by speculation, but the findings do not provide conclusive evidence of a long period of explosive behavior. The implications of the study are limited to the short-term behavior of market participants.\n\n**Correct Answer:** B) The study reveals that the EU ETS price trend in 2018 was characterized by a long period of explosive behavior, starting in March 2018, which was triggered by speculation among market participants. The findings question regulators' confidence in the long-term outcome of the reform.\n\n**Explanation:** The correct answer is B) because the study combines several econometric methods to analyze the EU ETS price trend in 2018, and the findings suggest that the reform triggered market participants into speculation, rather than a tightening supply of allowances. The study also questions regulators' confidence in the long-term outcome of the reform, which is a key implication of the findings."}, "59": {"documentation": {"title": "Non interactive simulation of correlated distributions is decidable", "source": "Anindya De and Elchanan Mossel and Joe Neeman", "docs_id": "1701.01485", "section": ["cs.CC", "cs.IT", "math.IT", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non interactive simulation of correlated distributions is decidable. A basic problem in information theory is the following: Let $\\mathbf{P} = (\\mathbf{X}, \\mathbf{Y})$ be an arbitrary distribution where the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob be two players where Alice gets samples $\\{x_i\\}_{i \\ge 1}$ and Bob gets samples $\\{y_i\\}_{i \\ge 1}$ and for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What joint distributions $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction? Classical works in information theory by G{\\'a}cs-K{\\\"o}rner and Wyner answer this question when at least one of $\\mathbf{P}$ or $\\mathbf{Q}$ is the distribution on $\\{0,1\\} \\times \\{0,1\\}$ where each marginal is unbiased and identical. However, other than this special case, the answer to this question is understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that this problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times \\{0,1\\}$. We extend their result to $\\mathbf{Q}$ supported on any finite alphabet. We rely on recent results in Gaussian geometry (by the authors) as well as a new \\emph{smoothing argument} inspired by the method of \\emph{boosting} from learning theory and potential function arguments from complexity theory and additive combinatorics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a joint distribution $\\mathbf{P} = (\\mathbf{X}, \\mathbf{Y})$ where the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ are correlated. Alice and Bob are two players who receive samples $\\{x_i\\}_{i \\ge 1}$ and $\\{y_i\\}_{i \\ge 1}$, respectively, such that for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What is the condition under which Alice and Bob can simulate a joint distribution $\\mathbf{Q}$ without any interaction, and what is the significance of this result in information theory?\n\n**A)** The condition is that the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ must be unbiased and identical, and the joint distribution $\\mathbf{Q}$ must be supported on $\\{0,1\\} \\times \\{0,1\\}$.\n\n**B)** The condition is that the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ must be correlated, and the joint distribution $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction if and only if $\\mathbf{Q}$ is supported on any finite alphabet.\n\n**C)** The condition is that the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ must be unbiased and identical, and the joint distribution $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction if and only if $\\mathbf{Q}$ is supported on $\\{0,1\\} \\times \\{0,1\\}$.\n\n**D)** The condition is that the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ must be correlated, and the joint distribution $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction if and only if $\\mathbf{Q}$ is supported on $\\{0,1\\} \\times \\{0,1\\}$.\n\n**Correct Answer:** B) The condition is that the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ must be correlated, and the joint distribution $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction if and only if $\\mathbf{Q}$ is supported on any finite alphabet.\n\n**Explanation:**\n\nThe correct answer is B) because the problem statement mentions that the result is understood in very few cases, and Ghazi, Kamath, and Sudan showed that the problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times \\{0,1\\}$. However, the problem statement also mentions that the result is extended to $\\mathbf{Q}$ supported on any finite alphabet, which is the correct condition.\n\nCandidate A is incorrect because it is a special case mentioned in the problem statement, but not the general condition.\n\nCandidate C is incorrect because it is the same as Candidate A, and the problem statement mentions that the result is extended to $\\mathbf{Q}$ supported on any finite alphabet, not just $\\{0,1\\} \\times \\{0,1\\}$.\n\nCandidate D is incorrect because it is a partial condition, and the problem statement mentions that the joint distribution $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction if and only if $\\mathbf{Q}$ is supported on any finite alphabet, not just $\\{0,1\\} \\times \\{0,1\\}$."}}