{"0": {"documentation": {"title": "New parton distributions in fixed flavour factorization scheme from\n  recent deep-inelastic-scattering data", "source": "H. Khanpour, Ali N. Khorramian, S. Atashbar Tehrani", "docs_id": "1205.5194", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New parton distributions in fixed flavour factorization scheme from\n  recent deep-inelastic-scattering data. We present our QCD analysis of the proton structure function $F_2^p(x,Q^2)$ to determine the parton distributions at the next-to-leading order (NLO). The heavy quark contributions to $F_2^i(x,Q^2)$, with $i$ = $c$, $b$ have been included in the framework of the `fixed flavour number scheme' (FFNS). The results obtained in the FFNS are compared with available results such as the general-mass variable-flavour-number scheme (GM-VFNS) and other prescriptions used in global fits of PDFs. In the present QCD analysis, we use a wide range of the inclusive neutral-current deep-inelastic-scattering (NC DIS) data, including the most recent data for charm $F_2^c$, bottom $F_2^b$, longitudinal $F_L$ structure functions and also the reduced DIS cross sections $\\sigma_{r,NC}^\\pm$ from HERA experiments. The most recent HERMES data for proton and deuteron structure functions are also added. We take into account ZEUS neutral current $e^ \\pm p$ DIS inclusive jet cross section data from HERA together with the recent Tevatron Run-II inclusive jet cross section data from CDF and D{\\O}. The impact of these recent DIS data on the PDFs extracted from the global fits are studied. We present two families of PDFs, {\\tt KKT12} and {\\tt KKT12C}, without and with HERA `combined' data sets on $e^{\\pm}p$ DIS. We find these are in good agreement with the available theoretical models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary purpose of including heavy quark contributions to the proton structure function $F_2^p(x,Q^2)$ in the framework of the `fixed flavour number scheme' (FFNS)?\n\nA) To improve the accuracy of the proton structure function at low $x$ values\nB) To account for the effects of heavy quark interactions on the proton's parton distribution\nC) To reduce the impact of charm and bottom quark contributions on the proton's structure function\nD) To enhance the proton's structure function at high $Q^2$ values\n\nCorrect Answer: B) To account for the effects of heavy quark interactions on the proton's parton distribution\n\nExplanation: The correct answer is B) because the inclusion of heavy quark contributions in the FFNS is intended to provide a more accurate description of the proton's parton distribution, particularly at low $x$ values. This is because heavy quarks (charm and bottom) are known to play a significant role in the proton's structure function at low $x$, and their contributions are not accounted for in the standard parton distribution functions. By including these contributions, the FFNS provides a more comprehensive and accurate description of the proton's structure function."}, "1": {"documentation": {"title": "Word-level Embeddings for Cross-Task Transfer Learning in Speech\n  Processing", "source": "Pierre Beckmann, Mikolaj Kegler, Milos Cernak", "docs_id": "1910.09909", "section": ["cs.CL", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Word-level Embeddings for Cross-Task Transfer Learning in Speech\n  Processing. Recent breakthroughs in deep learning often rely on representation learning and knowledge transfer. In recent years, unsupervised and self-supervised techniques for learning speech representation were developed to foster automatic speech recognition. Up to date, most of these approaches are task-specific and designed for within-task transfer learning between different datasets or setups of a particular task. In turn, learning task-independent representation of speech and cross-task applications of transfer learning remain less common. Here, we introduce an encoder capturing word-level representations of speech for cross-task transfer learning. We demonstrate the application of the pre-trained encoder in four distinct speech and audio processing tasks: (i) speech enhancement, (ii) language identification, (iii) speech, noise, and music classification, and (iv) speaker identification. In each task, we compare the performance of our cross-task transfer learning approach to task-specific baselines. Our results show that the speech representation captured by the encoder through the pre-training is transferable across distinct speech processing tasks and datasets. Notably, even simple applications of our pre-trained encoder outperformed task-specific methods, or were comparable, depending on the task."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of recent unsupervised and self-supervised techniques for learning speech representation, according to the paper \"Word-level Embeddings for Cross-Task Transfer Learning in Speech Processing\"?\n\nA) They are not effective for cross-task transfer learning\nB) They are not designed for within-task transfer learning\nC) They are not capable of learning task-independent representation of speech\nD) They are not applicable to speech and audio processing tasks\n\nCorrect Answer: C) They are not capable of learning task-independent representation of speech\n\nExplanation: The paper highlights that recent breakthroughs in deep learning often rely on representation learning and knowledge transfer, but most of these approaches are task-specific and designed for within-task transfer learning. This suggests that the primary limitation of these techniques is that they are not capable of learning task-independent representation of speech, which is the main goal of the proposed approach."}, "2": {"documentation": {"title": "System parameters of three short period cataclysmic variable stars", "source": "J. F. Wild, S. P. Littlefair, R. P. Ashley, E. Breedt, A. Brown, V. S.\n  Dhillon, M. J. Dyer, M. J. Green, P. Kerry, T. R. Marsh, S. G. Parsons, D. I.\n  Sahman", "docs_id": "2107.07400", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "System parameters of three short period cataclysmic variable stars. Using photometric ULTRACAM observations of three new short period cataclysmic variables, we model the primary eclipse lightcurves to extract the orbital separation, masses, and radii of their component stars. We find donor masses of 0.060 +/- 0.008 solar masses, 0.042 +/- 0.001 solar masses, and 0.042 +/- 0.004 solar masses, two being very low-mass sub-stellar donors, and one within 2 sigma of the hydrogen burning limit. All three of the new systems lie close to the modified, \"optimal\" model evolutionary sequence of Knigge et al. (2011). We briefly re-evaluate the long-standing discrepancy between observed donor mass and radius data, and theoretical CV evolutionary tracks. By looking at the difference in the observed period at each mass and the period predicted by the Knigge et al. (2011) evolutionary sequence, we qualitatively examine the form of excess angular momentum loss that is missing from the models below the period gap. We show indications that the excess angular momentum loss missing from CV models grows in importance relative to gravitational losses as the period decreases. Detailed CV evolutionary models are necessary to draw more quantitative conclusions in the future."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the donor masses of the three short period cataclysmic variable stars, and how do the observed masses compare to the theoretical CV evolutionary tracks?\n\nA) The donor masses are consistent with the theoretical CV evolutionary tracks, and the observed masses are within 1 sigma of the predicted values.\nB) The donor masses are significantly lower than the predicted values, indicating a discrepancy between observed and theoretical data.\nC) The donor masses are within 2 sigma of the hydrogen burning limit, and the observed masses are consistent with the modified, \"optimal\" model evolutionary sequence of Knigge et al. (2011).\nD) The donor masses are not significantly different from the predicted values, and the observed masses are consistent with the theoretical CV evolutionary tracks.\n\nCorrect Answer: C) The donor masses are within 2 sigma of the hydrogen burning limit, and the observed masses are consistent with the modified, \"optimal\" model evolutionary sequence of Knigge et al. (2011).\n\nExplanation: The correct answer is C) because the text states that \"All three of the new systems lie close to the modified, 'optimal' model evolutionary sequence of Knigge et al. (2011)\". This indicates that the observed donor masses are consistent with the modified evolutionary sequence, and that two of the systems have donor masses within 2 sigma of the hydrogen burning limit."}, "3": {"documentation": {"title": "Grazing-angle scattering of electromagnetic waves in gratings with\n  varying mean parameters: grating eigenmodes", "source": "D. K. Gramotnev, S. J. Goodman and T. A. Nieminen", "docs_id": "physics/0509029", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Grazing-angle scattering of electromagnetic waves in gratings with\n  varying mean parameters: grating eigenmodes. A highly unusual pattern of strong multiple resonances for bulk electromagnetic waves is predicted and analysed numerically in thick periodic holographic gratings in a slab with the mean permittivity that is larger than that of the surrounding media. This pattern is shown to exist in the geometry of grazing-angle scattering (GAS), that is when the scattered wave (+1 diffracted order) in the slab propagates almost parallel to the slab (grating) boundaries. The predicted resonances are demonstrated to be unrelated to resonant generation of the conventional guided modes of the slab. Their physical explanation is associated with resonant generation of a completely new type of eigenmodes in a thick slab with a periodic grating. These new slab eigenmodes are generically related to the grating; they do not exist if the grating amplitude is zero. The field structure of these eigenmodes and their dependence on structural and wave parameters is analysed. The results are extended to the case of GAS of guided modes in a slab with a periodic groove array of small corrugation amplitude and small variations in the mean thickness of the slab at the array boundaries."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary physical explanation for the predicted strong multiple resonances in grazing-angle scattering (GAS) of electromagnetic waves in thick periodic holographic gratings, and how do these resonances relate to the generation of new slab eigenmodes?\n\nA) The resonances are caused by the interaction between the grating and the surrounding media, resulting in the generation of conventional guided modes in the slab.\nB) The resonances are associated with the resonant generation of a completely new type of eigenmodes in a thick slab with a periodic grating, which are generically related to the grating and do not exist if the grating amplitude is zero.\nC) The resonances are a result of the diffraction of electromagnetic waves by the grating, leading to the formation of a new type of waveguide mode in the slab.\nD) The resonances are caused by the variation in the mean permittivity of the slab, which creates a standing wave pattern that enhances the scattering of electromagnetic waves.\n\nCorrect Answer: B) The resonances are associated with the resonant generation of a completely new type of eigenmodes in a thick slab with a periodic grating, which are generically related to the grating and do not exist if the grating amplitude is zero.\n\nExplanation: The correct answer is based on the text, which states that the predicted resonances are \"associated with resonant generation of a completely new type of eigenmodes in a thick slab with a periodic grating\". This is a key point in the paper, and the other options do not accurately summarize the physical explanation for the resonances. Option B is the only one that correctly captures the essence of the paper's findings."}, "4": {"documentation": {"title": "Testing for long-range dependence in non-stationary time series\n  time-varying regression", "source": "Lujia Bai and Weichi Wu", "docs_id": "2110.08089", "section": ["math.ST", "econ.EM", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Testing for long-range dependence in non-stationary time series\n  time-varying regression. We consider the problem of testing for long-range dependence for time-varying coefficient regression models. The covariates and errors are assumed to be locally stationary, which allows complex temporal dynamics and heteroscedasticity. We develop KPSS, R/S, V/S, and K/S-type statistics based on the nonparametric residuals, and propose bootstrap approaches equipped with a difference-based long-run covariance matrix estimator for practical implementation. Under the null hypothesis, the local alternatives as well as the fixed alternatives, we derive the limiting distributions of the test statistics, establish the uniform consistency of the difference-based long-run covariance estimator, and justify the bootstrap algorithms theoretically. In particular, the exact local asymptotic power of our testing procedure enjoys the order $O( \\log^{-1} n)$, the same as that of the classical KPSS test for long memory in strictly stationary series without covariates. We demonstrate the effectiveness of our tests by extensive simulation studies. The proposed tests are applied to a COVID-19 dataset in favor of long-range dependence in the cumulative confirmed series of COVID-19 in several countries, and to the Hong Kong circulatory and respiratory dataset, identifying a new type of 'spurious long memory'."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a time-varying coefficient regression model with locally stationary covariates and errors. Suppose we want to test for long-range dependence in the residuals of this model. Which of the following test statistics is most likely to be used in this context, and why?\n\nA) KPSS test\nB) R/S test\nC) V/S test\nD) K/S test\n\n**Correct Answer:** D) K/S test\n\n**Explanation:** The KPSS test is typically used for testing for long-range dependence in strictly stationary series without covariates. The R/S test and V/S test are also used for testing for long-range dependence, but they are not specifically designed for time-varying coefficient regression models with locally stationary covariates and errors. The K/S test, on the other hand, is a type of test statistic that is specifically designed for testing for long-range dependence in non-stationary time series, making it the most suitable choice for this context. The difference-based long-run covariance matrix estimator used in the K/S test is also well-suited for handling the complex temporal dynamics and heteroscedasticity present in locally stationary time series.\n\nNote: The other options are plausible but incorrect, and the correct answer requires a good understanding of the context and the specific test statistics mentioned in the documentation."}, "5": {"documentation": {"title": "Identifying the nature of the QCD transition in relativistic collision\n  of heavy nuclei with deep learning", "source": "Yi-Lun Du, Kai Zhou, Jan Steinheimer, Long-Gang Pang, Anton\n  Motornenko, Hong-Shi Zong, Xin-Nian Wang, Horst St\\\"ocker", "docs_id": "1910.11530", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identifying the nature of the QCD transition in relativistic collision\n  of heavy nuclei with deep learning. Using deep convolutional neural network (CNN), the nature of the QCD transition can be identified from the final-state pion spectra from hybrid model simulations of heavy-ion collisions that combines a viscous hydrodynamic model with a hadronic cascade \"after-burner\". Two different types of equations of state (EoS) of the medium are used in the hydrodynamic evolution. The resulting spectra in transverse momentum and azimuthal angle are used as the input data to train the neural network to distinguish different EoS. Different scenarios for the input data are studied and compared in a systematic way. A clear hierarchy is observed in the prediction accuracy when using the event-by-event, cascade-coarse-grained and event-fine-averaged spectra as input for the network, which are about 80%, 90% and 99%, respectively. A comparison with the prediction performance by deep neural network (DNN) with only the normalized pion transverse momentum spectra is also made. High-level features of pion spectra captured by a carefully-trained neural network were found to be able to distinguish the nature of the QCD transition even in a simulation scenario which is close to the experiments."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of identifying the nature of the QCD transition in relativistic heavy-ion collisions using deep learning, what is the primary advantage of using event-fine-averaged spectra as input for the neural network, compared to event-by-event and cascade-coarse-grained spectra?\n\n**A)** Event-fine-averaged spectra provide more robust predictions due to reduced noise and fluctuations.\n**B)** Event-fine-averaged spectra allow for better handling of rare events and outliers.\n**C)** Event-fine-averaged spectra enable the neural network to capture higher-level features in the pion spectra.\n**D)** Event-fine-averaged spectra result in faster training times and improved computational efficiency.\n\n**Correct Answer:** C) Event-fine-averaged spectra enable the neural network to capture higher-level features in the pion spectra.\n\n**Explanation:** The correct answer is C) because the documentation states that \"High-level features of pion spectra captured by a carefully-trained neural network were found to be able to distinguish the nature of the QCD transition even in a simulation scenario which is close to the experiments.\" This suggests that the event-fine-averaged spectra, which provide a more aggregated and averaged view of the data, are better suited for capturing higher-level features that are relevant for distinguishing the nature of the QCD transition."}, "6": {"documentation": {"title": "Unravelling the trading invariance hypothesis", "source": "Michael Benzaquen, Jonathan Donier, Jean-Philippe Bouchaud", "docs_id": "1602.03011", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Unravelling the trading invariance hypothesis. We confirm and substantially extend the recent empirical result of Andersen et al. \\cite{Andersen2015}, where it is shown that the amount of risk $W$ exchanged in the E-mini S\\&P futures market (i.e. price times volume times volatility) scales like the 3/2 power of the number of trades $N$. We show that this 3/2-law holds very precisely across 12 futures contracts and 300 single US stocks, and across a wide range of time scales. However, we find that the \"trading invariant\" $I=W/N^{3/2}$ proposed by Kyle and Obizhaeva is in fact quite different for different contracts, in particular between futures and single stocks. Our analysis suggests $I/{\\cal C}$ as a more natural candidate, where $\\cal C$ is the average spread cost of a trade, defined as the average of the trade size times the bid-ask spread. We also establish two more complex scaling laws for the volatility $\\sigma$ and the traded volume $V$ as a function of $N$, that reveal the existence of a characteristic number of trades $N_0$ above which the expected behaviour $\\sigma \\sim \\sqrt{N}$ and $V \\sim N$ hold, but below which strong deviations appear, induced by the size of the~tick."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The trading invariance hypothesis, proposed by Kyle and Obizhaeva, suggests that the trading invariant $I = W/N^{3/2}$ is a fundamental property of financial markets. However, the empirical result of Andersen et al. \\cite{Andersen2015} suggests that this invariant may not be universally applicable. What are the implications of the authors' finding that the trading invariant $I/{\\cal C}$ is a more natural candidate, where ${\\cal C}$ is the average spread cost of a trade?\n\nA) The trading invariance hypothesis is supported by the empirical result, but the invariant $I/{\\cal C}$ is a more nuanced and accurate representation of market behavior.\n\nB) The authors' finding suggests that the trading invariance hypothesis is not universally applicable, and that different markets may exhibit different scaling laws.\n\nC) The authors' finding implies that the trading invariant $I/{\\cal C}$ is a more fundamental property of financial markets than the invariant $I = W/N^{3/2}$.\n\nD) The authors' finding suggests that the trading invariance hypothesis is supported by the empirical result, but that the invariant $I/{\\cal C}$ is a more complex and multifaceted representation of market behavior.\n\n**Correct Answer:** B) The authors' finding suggests that the trading invariance hypothesis is not universally applicable, and that different markets may exhibit different scaling laws.\n\n**Explanation:** The correct answer is B because the authors' finding suggests that the trading invariant $I/{\\cal C}$ is a more nuanced and accurate representation of market behavior, and that it may not be universally applicable across different markets. This is in contrast to the trading invariance hypothesis, which suggests that the invariant $I = W/N^{3/2}$ is a fundamental property of financial markets. The authors' finding implies that different markets may exhibit different scaling laws, which is a key insight into the complexity of financial markets."}, "7": {"documentation": {"title": "Broad Learning System Based on Maximum Correntropy Criterion", "source": "Yunfei Zheng, Badong Chen, Senior Member, IEEE, Shiyuan Wang, Senior\n  Member, IEEE, and Weiqun Wang, Member, IEEE", "docs_id": "1912.11368", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Broad Learning System Based on Maximum Correntropy Criterion. As an effective and efficient discriminative learning method, Broad Learning System (BLS) has received increasing attention due to its outstanding performance in various regression and classification problems. However, the standard BLS is derived under the minimum mean square error (MMSE) criterion, which is, of course, not always a good choice due to its sensitivity to outliers. To enhance the robustness of BLS, we propose in this work to adopt the maximum correntropy criterion (MCC) to train the output weights, obtaining a correntropy based broad learning system (C-BLS). Thanks to the inherent superiorities of MCC, the proposed C-BLS is expected to achieve excellent robustness to outliers while maintaining the original performance of the standard BLS in Gaussian or noise-free environment. In addition, three alternative incremental learning algorithms, derived from a weighted regularized least-squares solution rather than pseudoinverse formula, for C-BLS are developed.With the incremental learning algorithms, the system can be updated quickly without the entire retraining process from the beginning, when some new samples arrive or the network deems to be expanded. Experiments on various regression and classification datasets are reported to demonstrate the desirable performance of the new methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind adopting the maximum correntropy criterion (MCC) to train the output weights in the proposed Correntropy-based Broad Learning System (C-BLS)?\n\n**A)** To improve the generalization performance of the system in noisy environments.\n**B)** To enhance the robustness of the system to outliers and maintain its original performance in Gaussian or noise-free environments.\n**C)** To reduce the computational complexity of the system by using a simpler optimization algorithm.\n**D)** To increase the capacity of the system by allowing for more complex non-linear relationships between inputs and outputs.\n\n**Correct Answer:** B) To enhance the robustness of the system to outliers and maintain its original performance in Gaussian or noise-free environments.\n\n**Explanation:** The correct answer is B) because the documentation states that the maximum correntropy criterion (MCC) is adopted to train the output weights in C-BLS to enhance the robustness of the system to outliers, while maintaining its original performance in Gaussian or noise-free environments. This is a key motivation behind the proposed method, as the standard BLS is sensitive to outliers and the MMSE criterion is not always a good choice."}, "8": {"documentation": {"title": "Two-loop corrections to Starobinsky-Higgs inflation", "source": "D. M. Ghilencea", "docs_id": "1807.06900", "section": ["hep-ph", "astro-ph.CO", "gr-qc", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-loop corrections to Starobinsky-Higgs inflation. Higgs inflation and $R^2$-inflation (Starobinsky model) are two limits of the same quantum model, hereafter called Starobinsky-Higgs. We analyse the two-loop action of the Higgs-like scalar $\\phi$ in the presence of: 1) non-minimal coupling ($\\xi$) and 2) quadratic curvature terms. The latter are generated at the quantum level with $\\phi$-dependent couplings ($\\tilde\\alpha$) even if their tree-level couplings ($\\alpha$) are tuned to zero. Therefore, the potential always depends on both Higgs field $\\phi$ and scalaron $\\rho$, hence multi-field inflation is a quantum consequence. The effects of the quantum (one- and two-loop) corrections on the potential $\\hat W(\\phi,\\rho)$ and on the spectral index are discussed, showing that the Starobinsky-Higgs model is in general stable in their presence. Two special cases are also considered: first, for a large $\\xi$ in the quantum action one can integrate $\\phi$ and generate a \"refined\" Starobinsky model which contains additional terms $\\xi^2 R^2\\ln^p (\\xi \\vert R\\vert/\\mu^2)$, $p=1,2$ ($\\mu$ is the subtraction scale). These generate corrections linear in the scalaron to the \"usual\" Starobinsky potential and a \"running\" scalaron mass. Second, for a small fixed Higgs field $\\phi^2 \\ll M_p^2/\\xi$ and a vanishing classical coefficient of the $R^2$-term, we show that the \"usual\" Starobinsky inflation is generated by the quantum corrections alone, for a suitable non-minimal coupling ($\\xi$)."}, "answer": "Here is a multi-form question based on the provided documentation:\n\nQuestion: In the Starobinsky-Higgs model, what is the effect of the non-minimal coupling (\u03be) on the potential \\hat W(\u03c6,\u03c1) and the spectral index, and how does it relate to the \"refined\" Starobinsky model?\n\nA) The non-minimal coupling (\u03be) generates a \"refined\" Starobinsky model with additional terms that depend on the subtraction scale \u03bc, but does not affect the spectral index.\n\nB) The non-minimal coupling (\u03be) generates a \"refined\" Starobinsky model with additional terms that depend on the scalaron mass, but does not affect the potential \\hat W(\u03c6,\u03c1).\n\nC) The non-minimal coupling (\u03be) generates a \"refined\" Starobinsky model with additional terms that depend on the subtraction scale \u03bc and the scalaron mass, and affects both the potential \\hat W(\u03c6,\u03c1) and the spectral index.\n\nD) The non-minimal coupling (\u03be) does not generate a \"refined\" Starobinsky model, and the effects on the potential \\hat W(\u03c6,\u03c1) and the spectral index are negligible.\n\nCorrect Answer: C) The non-minimal coupling (\u03be) generates a \"refined\" Starobinsky model with additional terms that depend on the subtraction scale \u03bc and the scalaron mass, and affects both the potential \\hat W(\u03c6,\u03c1) and the spectral index.\n\nExplanation: The correct answer is C) because the documentation states that for a large \u03be, one can integrate \u03c6 and generate a \"refined\" Starobinsky model with additional terms that depend on the subtraction scale \u03bc and the scalaron mass. Additionally, the documentation mentions that these terms generate corrections linear in the scalaron to the \"usual\" Starobinsky potential and a \"running\" scalaron mass, which affects the spectral index."}, "9": {"documentation": {"title": "Constraints on $s-\\bar s$ asymmetry of the proton in chiral effective\n  theory", "source": "X.G. Wang, Chueng-Ryong Ji, W. Melnitchouk, Y. Salamu, A.W. Thomas, P.\n  Wang", "docs_id": "1602.06646", "section": ["nucl-th", "hep-lat", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constraints on $s-\\bar s$ asymmetry of the proton in chiral effective\n  theory. We compute the $s-\\bar s$ asymmetry in the proton in chiral effective theory, using phenomenological constraints based upon existing data. Unlike previous meson cloud model calculations, which accounted for kaon loop contributions with on-shell intermediate states alone, this work includes off-shell terms and contact interactions, which impact the shape of the $s-\\bar s$ difference. We identify a valence-like component of $s(x)$ which is balanced by a $\\delta$-function contribution to $\\bar s(x)$ at $x=0$, so that the integrals of $s$ and $\\bar s$ over the experimentally accessible region $x > 0$ are not equal. Using a regularization procedure that preserves chiral symmetry and Lorentz invariance, we find that existing data limit the integrated value of the second moment of the asymmetry to the range $-0.07 \\times 10^{-3} \\leq \\langle x(s-\\bar s) \\rangle \\leq 1.12 \\times 10^{-3}$ at a scale of $Q^2=1 $GeV$^2$. This is too small to account for the NuTeV anomaly and of the wrong sign to enhance it."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nQuestion: In the context of chiral effective theory, what is the implication of the regularization procedure used to compute the $s-\\bar s$ asymmetry in the proton, and how does it relate to the preservation of chiral symmetry and Lorentz invariance?\n\nA) The regularization procedure introduces a non-zero value for the second moment of the asymmetry, violating Lorentz invariance.\nB) The regularization procedure preserves the chiral symmetry of the theory, but introduces a non-zero value for the second moment of the asymmetry, which is a consequence of the off-shell terms and contact interactions included in the calculation.\nC) The regularization procedure is equivalent to the on-shell intermediate states used in previous meson cloud model calculations, and does not impact the preservation of chiral symmetry and Lorentz invariance.\nD) The regularization procedure is not necessary to preserve chiral symmetry and Lorentz invariance, and can be omitted without affecting the results.\n\nCorrect Answer: B) The regularization procedure preserves the chiral symmetry of the theory, but introduces a non-zero value for the second moment of the asymmetry, which is a consequence of the off-shell terms and contact interactions included in the calculation.\n\nExplanation: The regularization procedure used in the calculation is designed to preserve chiral symmetry and Lorentz invariance. However, it also includes off-shell terms and contact interactions, which can introduce a non-zero value for the second moment of the asymmetry. This is a consequence of the more complete treatment of the $s-\\bar s$ asymmetry, which includes both on-shell and off-shell contributions. The correct answer, B, acknowledges this trade-off between preserving chiral symmetry and including more complete terms in the calculation."}, "10": {"documentation": {"title": "Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from Multi-Vendor X-ray Scans", "source": "Taimur Hassan, Salman H. Khan, Samet Akcay, Mohammed Bennamoun,\n  Naoufel Werghi", "docs_id": "1912.04251", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from Multi-Vendor X-ray Scans. In the last two decades, luggage scanning has globally become one of the prime aviation security concerns. Manual screening of the baggage items is a cumbersome, subjective and inefficient process. Hence, many researchers have developed Xray imagery-based autonomous systems to address these shortcomings. However, to the best of our knowledge, there is no framework, up to now, that can recognize heavily occluded and cluttered baggage items from multi-vendor X-ray scans. This paper presents a cascaded structure tensor framework which can automatically extract and recognize suspicious items irrespective of their position and orientation in the multi-vendor X-ray scans. The proposed framework is unique, as it intelligently extracts each object by iteratively picking contour based transitional information from different orientations and uses only a single feedforward convolutional neural network for the recognition. The proposed framework has been rigorously tested on publicly available GDXray and SIXray datasets containing a total of 1,067,381 X-ray scans where it significantly outperformed the state-of-the-art solutions by achieving the mean average precision score of 0.9343 and 0.9595 for extracting and recognizing suspicious items from GDXray and SIXray scans, respectively. Furthermore, the proposed framework has achieved 15.78% better time"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of X-ray imagery-based autonomous systems for luggage scanning, and how does the proposed cascaded structure tensor framework address this challenge?\n\n**A)** The primary motivation is to improve the accuracy of manual screening, and the proposed framework addresses this by using a single feedforward convolutional neural network for recognition. However, it does not explicitly address the challenge of heavily occluded and cluttered baggage items.\n\n**B)** The primary motivation is to reduce the time required for luggage scanning, and the proposed framework addresses this by achieving 15.78% better time compared to state-of-the-art solutions. However, it does not explicitly address the challenge of recognizing suspicious items from multi-vendor X-ray scans.\n\n**C)** The primary motivation is to improve the efficiency of luggage screening, and the proposed framework addresses this by intelligently extracting each object by iteratively picking contour-based transitional information from different orientations. However, it does not explicitly address the challenge of recognizing suspicious items from heavily occluded and cluttered baggage items.\n\n**D)** The primary motivation is to address the challenge of recognizing suspicious items from heavily occluded and cluttered baggage items, and the proposed framework addresses this by using a cascaded structure tensor framework that can automatically extract and recognize suspicious items irrespective of their position and orientation in multi-vendor X-ray scans.\n\n**Correct Answer:** D) The primary motivation is to address the challenge of recognizing suspicious items from heavily occluded and cluttered baggage items, and the proposed framework addresses this by using a cascaded structure tensor framework that can automatically extract and recognize suspicious items irrespective of their position and orientation in multi-vendor X-ray scans.\n\n**Explanation:** The correct answer is D) because it accurately summarizes the primary motivation behind the development of X-ray imagery-based autonomous systems for luggage scanning, which is to address the challenge of recognizing suspicious items from heavily occluded and cluttered baggage items. The proposed cascaded structure tensor framework is specifically designed to address this challenge by using a unique approach that can automatically extract and recognize suspicious items irrespective of their position and orientation in multi-vendor X-ray scans."}, "11": {"documentation": {"title": "A Comparative Review of Dimension Reduction Methods in Approximate\n  Bayesian Computation", "source": "M. G. B. Blum, M. A. Nunes, D. Prangle, S. A. Sisson", "docs_id": "1202.3819", "section": ["stat.ME", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Comparative Review of Dimension Reduction Methods in Approximate\n  Bayesian Computation. Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to overcome the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with minimal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challenging models and data sets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the primary challenges in implementing Approximate Bayesian Computation (ABC) methods, and how do dimension reduction techniques address these challenges?\n\nA) The primary challenge in ABC is the computational complexity of likelihood functions, and dimension reduction techniques address this by reducing the dimensionality of the data while preserving the most informative features.\n\nB) The primary challenge in ABC is the lack of information about the prior distribution, and dimension reduction techniques address this by using regularization procedures to incorporate prior knowledge into the analysis.\n\nC) The primary challenge in ABC is the need for efficient computation of summary statistics, and dimension reduction techniques address this by using projection techniques to reduce the dimensionality of the data while preserving the most informative features.\n\nD) The primary challenge in ABC is the difficulty in selecting the optimal subset of variables, and dimension reduction techniques address this by using best subset selection methods to identify the most informative variables.\n\nCorrect Answer: A) The primary challenge in ABC is the computational complexity of likelihood functions, and dimension reduction techniques address this by reducing the dimensionality of the data while preserving the most informative features.\n\nExplanation: The documentation states that ABC methods make use of comparisons between simulated and observed summary statistics to overcome the problem of computationally intractable likelihood functions. Therefore, the primary challenge in ABC is indeed the computational complexity of likelihood functions. Dimension reduction techniques address this challenge by reducing the dimensionality of the data while preserving the most informative features, which is option A. Options B, C, and D are incorrect because they do not accurately describe the primary challenge in ABC or the role of dimension reduction techniques in addressing it."}, "12": {"documentation": {"title": "Maximum Entropy Principle underlying the dynamics of automobile sales", "source": "A. Hernando, D. Villuendas, M. Sulc, R. Hernando, R. Seoane, A.\n  Plastino", "docs_id": "1705.03458", "section": ["physics.soc-ph", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Maximum Entropy Principle underlying the dynamics of automobile sales. We analyze an exhaustive data-set of new-cars monthly sales. The set refers to 10 years of Spanish sales of more than 6500 different car model configurations and a total of 10M sold cars, from January 2007 to January 2017. We find that for those model configurations with a monthly market-share higher than 0.1% the sales become scalable obeying Gibrat's law of proportional growth under logistic dynamics. Remarkably, the distribution of total sales follows the predictions of the Maximum Entropy Principle for systems subject to proportional growth in dynamical equilibrium. We also encounter that the associated dynamics are non-Markovian, i.e., the system has a decaying memory or inertia of about 5 years. Thus, car sales are predictable within a certain time-period. We show that the main characteristics of the dynamics can be described via a construct based upon the Langevin equation. This construct encompasses the fundamental principles that any predictive model on car sales should obey."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the underlying dynamics of automobile sales as described by the Maximum Entropy Principle. How do the findings of the study relate to the concept of Gibrat's law of proportional growth, and what implications do they have for predictive modeling of car sales?\n\n**A)** The study suggests that car sales follow a deterministic, non-proportional growth pattern, which is incompatible with the Maximum Entropy Principle. The findings are therefore irrelevant to predictive modeling.\n\n**B)** The data analysis reveals that car sales exhibit a decaying memory or inertia of approximately 5 years, which is a characteristic of non-Markovian systems. This implies that predictive models should account for this memory effect to accurately forecast sales.\n\n**C)** The study demonstrates that the distribution of total sales follows the predictions of the Maximum Entropy Principle, which suggests that the underlying dynamics are governed by a probabilistic, proportional growth process. This implies that predictive models should be based on a probabilistic framework to accurately capture the underlying mechanisms.\n\n**D)** The findings of the study indicate that car sales are predictable within a certain time-period, which is consistent with the concept of Gibrat's law of proportional growth. However, the study also highlights the importance of accounting for the decaying memory or inertia of the system, which is a key characteristic of non-Markovian systems.\n\n**Correct Answer:** D) The findings of the study indicate that car sales are predictable within a certain time-period, which is consistent with the concept of Gibrat's law of proportional growth. However, the study also highlights the importance of accounting for the decaying memory or inertia of the system, which is a key characteristic of non-Markovian systems.\n\n**Explanation:** The correct answer, D, acknowledges the key findings of the study, including the predictability of car sales within a certain time-period and the importance of accounting for the decaying memory or inertia of the system. This answer demonstrates an understanding of the underlying dynamics of automobile sales and the implications for predictive modeling. The incorrect answers, A, B, and C, either misinterpret or oversimplify the findings of the study, and do not demonstrate a thorough understanding of the underlying concepts."}, "13": {"documentation": {"title": "Fast T2 Mapping with Improved Accuracy Using Undersampled Spin-echo MRI\n  and Model-based Reconstructions with a Generating Function", "source": "Tilman J. Sumpf (1), Andreas Petrovic (2), Martin Uecker (3), Florian\n  Knoll (4), Jens Frahm (1) ((1) Biomedizinische NMR Forschungs GmbH am\n  Max-Planck-Institut f\\\"ur biophysikalische Chemie, G\\\"ottingen. (2) Ludwig\n  Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria, and\n  Institute for Medical Engineering, Graz University of Technology, Graz,\n  Austria. (3) Department of Electrical Engineering and Computer Sciences,\n  University of California, Berkeley, California. (4) Center for Biomedical\n  Imaging, New York University School of Medicine, New York.)", "docs_id": "1405.3574", "section": ["physics.med-ph", "cs.CE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast T2 Mapping with Improved Accuracy Using Undersampled Spin-echo MRI\n  and Model-based Reconstructions with a Generating Function. A model-based reconstruction technique for accelerated T2 mapping with improved accuracy is proposed using undersampled Cartesian spin-echo MRI data. The technique employs an advanced signal model for T2 relaxation that accounts for contributions from indirect echoes in a train of multiple spin echoes. An iterative solution of the nonlinear inverse reconstruction problem directly estimates spin-density and T2 maps from undersampled raw data. The algorithm is validated for simulated data as well as phantom and human brain MRI at 3 T. The performance of the advanced model is compared to conventional pixel-based fitting of echo-time images from fully sampled data. The proposed method yields more accurate T2 values than the mono-exponential model and allows for undersampling factors of at least 6. Although limitations are observed for very long T2 relaxation times, respective reconstruction problems may be overcome by a gradient dampening approach. The analytical gradient of the utilized cost function is included as Appendix."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed model-based reconstruction technique for accelerated T2 mapping using undersampled Cartesian spin-echo MRI data?\n\nA) It allows for undersampling factors of up to 4, resulting in significant reduction in acquisition time.\nB) It employs a mono-exponential model for T2 relaxation, which is more accurate than the advanced signal model proposed in the paper.\nC) It directly estimates spin-density and T2 maps from undersampled raw data, allowing for more accurate T2 values than conventional pixel-based fitting.\nD) It requires a fully sampled dataset to estimate T2 values, making it unsuitable for undersampled data.\n\n**Correct Answer:** C) It directly estimates spin-density and T2 maps from undersampled raw data, allowing for more accurate T2 values than conventional pixel-based fitting.\n\n**Explanation:** The correct answer is C) because the proposed technique employs an advanced signal model for T2 relaxation that accounts for contributions from indirect echoes in a train of multiple spin echoes. This allows for direct estimation of spin-density and T2 maps from undersampled raw data, resulting in more accurate T2 values than conventional pixel-based fitting. The other options are incorrect because they either underestimate the advantage of the proposed technique (A), misrepresent the model used (B), or incorrectly state the requirement for the technique (D)."}, "14": {"documentation": {"title": "Generalized roof duality and bisubmodular functions", "source": "Vladimir Kolmogorov", "docs_id": "1005.2305", "section": ["cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalized roof duality and bisubmodular functions. Consider a convex relaxation $\\hat f$ of a pseudo-boolean function $f$. We say that the relaxation is {\\em totally half-integral} if $\\hat f(x)$ is a polyhedral function with half-integral extreme points $x$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\\gamma$ where $\\gamma\\in\\{0, 1, 1/2}$ is a constant. A well-known example is the {\\em roof duality} relaxation for quadratic pseudo-boolean functions $f$. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $\\hat f$ by establishing a one-to-one correspondence with {\\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a convex relaxation $\\hat f$ of a pseudo-boolean function $f$ that is totally half-integral. What is the relationship between totally half-integral relaxations and bisubmodular functions, and how do they relate to each other?\n\n**A)** Totally half-integral relaxations are equivalent to bisubmodular functions, and this equivalence is preserved under the addition of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\\gamma$.\n\n**B)** Totally half-integral relaxations are a special case of bisubmodular functions, but not all bisubmodular functions are totally half-integral.\n\n**C)** Totally half-integral relaxations are a necessary but not sufficient condition for a function to be bisubmodular, and the relationship between the two is more complex.\n\n**D)** Totally half-integral relaxations are a type of bisubmodular function, but the converse is not necessarily true.\n\n**Correct Answer:** C) Totally half-integral relaxations are a necessary but not sufficient condition for a function to be bisubmodular, and the relationship between the two is more complex.\n\n**Explanation:** The correct answer is C) because the paper argues that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions, but it does not claim that total half-integrality is equivalent to bisubmodularity. The relationship between the two is more complex, and the paper provides a characterization of totally half-integral relaxations by establishing a one-to-one correspondence with bisubmodular functions, but also shows that not all bisubmodular functions are totally half-integral."}, "15": {"documentation": {"title": "Dynamical Chiral Symmetry Breaking on the Light Front I. DLCQ Approach", "source": "K. Itakura, S. Maedan", "docs_id": "hep-th/9907071", "section": ["hep-th", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamical Chiral Symmetry Breaking on the Light Front I. DLCQ Approach. Dynamical chiral symmetry breaking in the DLCQ method is investigated in detail using a chiral Yukawa model closely related to the Nambu-Jona-Lasinio model. By classically solving three constraints characteristic of the light-front formalism, we show that the chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent. A quantum analysis demonstrates that a nonperturbative mean-field solution to the ``zero-mode constraint'' for a scalar boson (sigma) can develop a nonzero condensate while a perturbative solution cannot. This description is due to our identification of the ``zero-mode constraint'' with the gap equation. The mean-field calculation clarifies unusual chiral transformation properties of fermionic field, which resolves a seemingly inconsistency between triviality of the null-plane chiral charge Q_5|0>=0 and nonzero condensate. We also calculate masses of scalar and pseudoscalar bosons for both symmetric and broken phases, and eventually derive the PCAC relation and nonconservation of Q_5 in the broken phase."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Dynamical Chiral Symmetry Breaking on the Light Front using the DLCQ approach, what is the relationship between the chiral transformation defined on the light front and the usual chiral transformation when bare mass is absent?\n\n**A)** The chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent, but only in the symmetric phase.\n\n**B)** The chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent, but only in the broken phase.\n\n**C)** The chiral transformation defined on the light front is not equivalent to the usual one when bare mass is absent, and the equivalence is lost in both the symmetric and broken phases.\n\n**D)** The chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent, but only in the perturbative solution.\n\n**Correct Answer:** A) The chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent, but only in the symmetric phase.\n\n**Explanation:** According to the documentation, the chiral transformation defined on the light front is equivalent to the usual one when bare mass is absent. This equivalence is demonstrated by classically solving three constraints characteristic of the light-front formalism. However, this equivalence is only valid in the symmetric phase, as stated in the documentation."}, "16": {"documentation": {"title": "Sensitivity of $\\beta$-decay rates to the radial dependence of the\n  nucleon effective mass", "source": "A. P. Severyukhin, J\\'er\\^ome Margueron (IPNL), I. N. Borzov, Nguyen\n  Van Giai (IPNO)", "docs_id": "1505.07559", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sensitivity of $\\beta$-decay rates to the radial dependence of the\n  nucleon effective mass. We analyze the sensitivity of $\\beta$-decay rates in 78 Ni and 100,132 Sn to a correction term in Skyrme energy-density functionals (EDF) which modifies the radial shape of the nucleon effective mass. This correction is added on top of several Skyrme parametrizations which are selected from their effective mass properties and predictions about the stability properties of 132 Sn. The impact of the correction on high-energy collective modes is shown to be moderate. From the comparison of the effects induced by the surface-peaked effective mass in the three doubly magic nuclei, it is found that 132 Sn is largely impacted by the correction, while 78 Ni and 100 Sn are only moderately affected. We conclude that $\\beta$-decay rates in these nuclei can be used as a test of different parts of the nuclear EDF: 78 Ni and 100 Sn are mostly sensitive to the particle-hole interaction through the B(GT) values, while 132 Sn is sensitive to the radial shape of the effective mass. Possible improvements of these different parts could therefore be better constrained in the future."}, "answer": "Here is the difficult multi-form question:\n\nQuestion: What can be concluded about the sensitivity of \u03b2-decay rates in 78 Ni, 100 Sn, and 132 Sn to different parts of the nuclear energy-density function (EDF) based on the analysis in the provided Arxiv documentation?\n\nA) The \u03b2-decay rates in these nuclei are equally sensitive to the particle-hole interaction and the radial shape of the effective mass.\nB) The \u03b2-decay rates in 78 Ni and 100 Sn are sensitive to the radial shape of the effective mass, while 132 Sn is sensitive to the particle-hole interaction.\nC) The \u03b2-decay rates in these nuclei are not sensitive to the different parts of the EDF, and therefore, no conclusions can be drawn.\nD) The \u03b2-decay rates in 132 Sn are sensitive to both the particle-hole interaction and the radial shape of the effective mass, while 78 Ni and 100 Sn are sensitive to only one of these.\n\nCorrect Answer: B) The \u03b2-decay rates in 78 Ni and 100 Sn are sensitive to the radial shape of the effective mass, while 132 Sn is sensitive to the particle-hole interaction."}, "17": {"documentation": {"title": "Diversity Evolution", "source": "Russell K. Standish", "docs_id": "nlin/0210026", "section": ["nlin.AO", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diversity Evolution. Bedau has developed a general set of evolutionary statistics that quantify the adaptive component of evolutionary processes. On the basis of these measures, he has proposed a set of 4 classes of evolutionary system. All artificial life sytems so far looked at fall into the first 3 classes, whereas the biosphere, and possibly the human economy belongs to the 4th class. The challenge to the artificial life community is to identify exactly what is difference between these natural evolutionary systems, and existing artificial life systems. At ALife VII, I presented a study using an artificial evolutionary ecology called \\EcoLab. Bedau's statistics captured the qualitative behaviour of the model. \\EcoLab{} exhibited behaviour from the first 3 classes, but not class 4, which is characterised by unbounded growth in diversity. \\EcoLab{} exhibits a critical surface given by an inverse relationship between connectivity and diversity, above which the model cannot tarry long. Thus in order to get unbounded diversity increase, there needs to be a corresponding connectivity reducing (or food web pruning) process. This paper reexamines this question in light of two possible processes that reduce ecosystem connectivity: a tendency for specialisation and increase in biogeographic zones through continental drift."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the paper, what is the key difference between the evolutionary systems of the biosphere, the human economy, and artificial life systems, and how does this relate to the concept of unbounded growth in diversity?\n\n**A)** The biosphere and human economy exhibit unbounded growth in diversity due to their ability to adapt and evolve rapidly, whereas artificial life systems are limited by their programming and lack of adaptability.\n\n**B)** The biosphere and human economy exhibit unbounded growth in diversity due to their ability to undergo processes such as continental drift and biogeographic zone formation, which reduce ecosystem connectivity and promote specialization, whereas artificial life systems are limited by their inability to undergo these processes.\n\n**C)** The biosphere and human economy exhibit unbounded growth in diversity due to their ability to exhibit complex and dynamic behavior, whereas artificial life systems are limited by their simplicity and lack of complexity.\n\n**D)** The biosphere and human economy exhibit unbounded growth in diversity due to their ability to evolve and adapt through mechanisms such as mutation and selection, whereas artificial life systems are limited by their lack of these mechanisms.\n\n**Correct Answer:** B) The biosphere and human economy exhibit unbounded growth in diversity due to their ability to undergo processes such as continental drift and biogeographic zone formation, which reduce ecosystem connectivity and promote specialization, whereas artificial life systems are limited by their inability to undergo these processes.\n\n**Explanation:** The correct answer is B) because the paper states that the biosphere and human economy exhibit unbounded growth in diversity due to their ability to undergo processes such as continental drift and biogeographic zone formation, which reduce ecosystem connectivity and promote specialization. This is in contrast to artificial life systems, which, as demonstrated by the EcoLab model, are limited by their inability to undergo these processes. The other options are incorrect because they do not accurately reflect the key difference between the evolutionary systems of the biosphere, the human economy, and artificial life systems."}, "18": {"documentation": {"title": "High Frequency Lead/lag Relationships - Empirical facts", "source": "Nicolas Huth, Fr\\'ed\\'eric Abergel", "docs_id": "1111.7103", "section": ["q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High Frequency Lead/lag Relationships - Empirical facts. Lead/lag relationships are an important stylized fact at high frequency. Some assets follow the path of others with a small time lag. We provide indicators to measure this phenomenon using tick-by-tick data. Strongly asymmetric cross-correlation functions are empirically observed, especially in the future/stock case. We confirm the intuition that the most liquid assets (short intertrade duration, narrow bid/ask spread, small volatility, high turnover) tend to lead smaller stocks. However, the most correlated stocks are those with similar levels of liquidity. This lead/lag phenomenon is not constant throughout the day, it shows an intraday seasonality with changes of behaviour at very specific times such as the announcement of macroeconomic figures and the US market opening. These lead/lag relationships become more and more pronounced as we zoom on significant events. We reach 60% of accuracy when forecasting the next midquote variation of the lagger using only the past information of the leader, which is significantly better than using the information of the lagger only. However, a naive strategy based on market orders cannot make any profit of this effect because of the bid/ask spread."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the lead/lag relationships observed in high-frequency financial data. How do the characteristics of the most liquid assets, such as short intertrade duration, narrow bid/ask spread, small volatility, and high turnover, affect their lead/lag relationships with smaller stocks?\n\n**A)** The most liquid assets tend to lag behind smaller stocks due to their lower volatility and narrower bid/ask spreads, making them less susceptible to market fluctuations.\n\n**B)** The most liquid assets tend to lead smaller stocks because they have higher turnover and are more responsive to market changes, allowing them to anticipate and react to changes in the market.\n\n**C)** The lead/lag relationships between the most liquid assets and smaller stocks are not affected by their liquidity characteristics, and instead are driven by other factors such as the announcement of macroeconomic figures and the US market opening.\n\n**D)** The most liquid assets tend to lead smaller stocks because they have similar levels of liquidity, allowing them to move in tandem with each other and respond to market changes in a coordinated manner.\n\n**Correct Answer:** D) The most liquid assets tend to lead smaller stocks because they have similar levels of liquidity, allowing them to move in tandem with each other and respond to market changes in a coordinated manner.\n\n**Explanation:** The correct answer is D) because the documentation states that \"the most correlated stocks are those with similar levels of liquidity\" and that \"the most liquid assets (short intertrade duration, narrow bid/ask spread, small volatility, high turnover) tend to lead smaller stocks\". This suggests that the lead/lag relationships between the most liquid assets and smaller stocks are driven by their similar liquidity characteristics, allowing them to move in tandem with each other and respond to market changes in a coordinated manner."}, "19": {"documentation": {"title": "Compressing deep neural networks by matrix product operators", "source": "Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao,\n  Zhong-Yi Lu, Tao Xiang", "docs_id": "1904.06194", "section": ["cs.LG", "cs.CV", "cs.NE", "physics.comp-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compressing deep neural networks by matrix product operators. A deep neural network is a parametrization of a multilayer mapping of signals in terms of many alternatively arranged linear and nonlinear transformations. The linear transformations, which are generally used in the fully connected as well as convolutional layers, contain most of the variational parameters that are trained and stored. Compressing a deep neural network to reduce its number of variational parameters but not its prediction power is an important but challenging problem toward the establishment of an optimized scheme in training efficiently these parameters and in lowering the risk of overfitting. Here we show that this problem can be effectively solved by representing linear transformations with matrix product operators (MPOs), which is a tensor network originally proposed in physics to characterize the short-range entanglement in one-dimensional quantum states. We have tested this approach in five typical neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO representation indeed sets up a faithful and efficient mapping between input and output signals, which can keep or even improve the prediction accuracy with a dramatically reduced number of parameters. Our method greatly simplifies the representations in deep learning, and opens a possible route toward establishing a framework of modern neural networks which might be simpler and cheaper, but more efficient."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind representing linear transformations in deep neural networks with matrix product operators (MPOs), and how does this approach impact the representation of neural networks?\n\n**A)** MPOs are used to reduce the computational complexity of neural networks, allowing for faster training times. However, this approach may compromise the accuracy of the network's predictions.\n\n**B)** MPOs are used to simplify the representations of neural networks, making it easier to train and optimize the parameters of the network. This approach can lead to improved prediction accuracy and reduced overfitting.\n\n**C)** MPOs are used to introduce non-linearity into the linear transformations of neural networks, allowing for more complex and expressive representations. However, this approach may increase the number of parameters in the network.\n\n**D)** MPOs are used to reduce the number of parameters in neural networks, making it easier to train and optimize the network. However, this approach may compromise the accuracy of the network's predictions.\n\n**Correct Answer:** B) MPOs are used to simplify the representations of neural networks, making it easier to train and optimize the parameters of the network. This approach can lead to improved prediction accuracy and reduced overfitting.\n\n**Explanation:** The correct answer is B) because the documentation states that the primary motivation behind representing linear transformations with MPOs is to simplify the representations of neural networks, making it easier to train and optimize the parameters of the network. This approach can lead to improved prediction accuracy and reduced overfitting, as stated in the abstract of the paper. The other options are incorrect because they either misrepresent the motivation behind MPOs or introduce incorrect assumptions about the impact of MPOs on neural networks."}, "20": {"documentation": {"title": "A cascaded multiple-speaker localization and tracking system", "source": "Xiaofei Li, Yutong Ban, Laurent Girin, Xavier Alameda-Pineda and Radu\n  Horaud", "docs_id": "1812.04417", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A cascaded multiple-speaker localization and tracking system. This paper presents an online multiple-speaker localization and tracking method, as the INRIA-Perception contribution to the LOCATA Challenge 2018. First, the recursive least-square method is used to adaptively estimate the direct-path relative transfer function as an interchannel localization feature. The feature is assumed to associate with a single speaker at each time-frequency bin. Second, a complex Gaussian mixture model (CGMM) is used as a generative model of the features. The weight of each CGMM component represents the probability that this component corresponds to an active speaker, and is adaptively estimated with an online optimization algorithm. Finally, taking the CGMM component weights as observations, a Bayesian multiple-speaker tracking method based on the variational expectation maximization algorithm is used. The tracker accounts for the variation of active speakers and the localization miss measurements, by introducing speaker birth and sleeping processes. The experiments carried out on the development dataset of the challenge are reported."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary purpose of using a complex Gaussian mixture model (CGMM) in the cascaded multiple-speaker localization and tracking system, and how does it relate to the adaptive estimation of the direct-path relative transfer function?\n\nA) To model the variability of speaker positions and orientations, and to adaptively estimate the direct-path relative transfer function as an interchannel localization feature.\nB) To represent the generative model of the features, where the weight of each component represents the probability that it corresponds to an active speaker, and is adaptively estimated with an online optimization algorithm.\nC) To introduce speaker birth and sleeping processes, and to account for the variation of active speakers and localization miss measurements in the Bayesian multiple-speaker tracking method.\nD) To optimize the weight of each CGMM component using the recursive least-square method, and to use the resulting weights as observations in the Bayesian multiple-speaker tracking method.\n\nCorrect Answer: B) To represent the generative model of the features, where the weight of each component represents the probability that it corresponds to an active speaker, and is adaptively estimated with an online optimization algorithm.\n\nExplanation: The CGMM is used as a generative model of the features, where the weight of each component represents the probability that it corresponds to an active speaker. This is stated in the documentation as \"The weight of each CGMM component represents the probability that this component corresponds to an active speaker, and is adaptively estimated with an online optimization algorithm.\" The correct answer highlights the role of the CGMM in representing the generative model of the features, and the adaptive estimation of the weights using an online optimization algorithm."}, "21": {"documentation": {"title": "Solutions of local and nonlocal equations reduced from the AKNS\n  hierarchy", "source": "Kui Chen, Xiao Deng, Senyue Lou, Da-jun Zhang", "docs_id": "1710.10479", "section": ["nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solutions of local and nonlocal equations reduced from the AKNS\n  hierarchy. In the paper possible local and nonlocal reductions of the Ablowitz-Kaup-Newell-Suger (AKNS) hierarchy are collected, including the Korteweg-de Vries (KdV) hierarchy, modified KdV hierarchy and their nonlocal versions, nonlinear Schr\\\"{o}dinger hierarchy and their nonlocal versions, sine-Gordon equation in nonpotential form and its nonlocal forms. A reduction technique for solutions is employed, by which exact solutions in double Wronskian form are obtained for these reduced equations from those double Wronskian solutions of the AKNS hierarchy. As examples of dynamics we illustrate new interaction of two-soliton solutions of the reverse-$t$ nonlinear Schr\\\"{o}dinger equation. Although as a single soliton it is always stationary, two solitons travel along completely symmetric trajectories in $\\{x,t\\}$ plane and their amplitudes are affected by phase parameters. Asymptotic analysis is given as demonstration. The approach and relation described in this paper are systematic and general and can be used to other nonlocal equations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary reduction technique employed in the paper \"Solutions of local and nonlocal equations reduced from the AKNS hierarchy\" to obtain exact solutions in double Wronskian form for the various reduced equations?\n\nA) The inverse scattering method\nB) The Darboux transformation\nC) The Weyl transformation\nD) The inverse Fourier transform\n\nCorrect Answer: B) The Darboux transformation\n\nExplanation: The Darboux transformation is a well-known method in the theory of soliton equations, which allows for the reduction of a soliton equation to a new equation with a different form of the potential. In the context of the AKNS hierarchy, the Darboux transformation is used to obtain exact solutions in double Wronskian form for the various reduced equations. This is demonstrated in the paper as a way to reduce the AKNS hierarchy to other soliton equations, such as the KdV hierarchy and the nonlinear Schr\u00f6dinger hierarchy.\n\nThe other options are incorrect because:\n\nA) The inverse scattering method is a different technique used to solve soliton equations, but it is not the primary reduction technique employed in the paper.\n\nC) The Weyl transformation is a method used in some soliton equation reductions, but it is not the primary technique used in the paper.\n\nD) The inverse Fourier transform is not a relevant technique in this context, as it is not used to reduce soliton equations."}, "22": {"documentation": {"title": "The DEEP2 Galaxy Redshift Survey: The Galaxy Luminosity Function to z ~\n  1", "source": "C. N. A. Willmer, S. M. Faber, D. C. Koo, B. J. Weiner, J. A. Newman,\n  A. L. Coil, A. J. Connolly, C. Conroy, M. C. Cooper, M. Davis, D. P.\n  Finkbeiner, B. F. Gerke, P. Guhathakurta, J. Harker, N. Kaiser, S. Kassin, N.\n  P. Konidaris, L. Lin, G. Luppino, D. S. Madgwick, K. G. Noeske, A. C.\n  Phillips and R. Yan", "docs_id": "astro-ph/0506041", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The DEEP2 Galaxy Redshift Survey: The Galaxy Luminosity Function to z ~\n  1. The evolution of the B-band galaxy luminosity function is measured using a sample of more than 11,000 galaxies with spectroscopic redshifts from the DEEP2 Redshift Survey. The rest-frame M_B versus U-B color-magnitude diagram of DEEP2 galaxies shows that the color-magnitude bi-modality seen in galaxies locally is still present at redshifts z > 1. Dividing the sample at the trough of this color bimodality into predominantly red and blue galaxies, we find that the luminosity function of each galaxy color type evolves differently. Blue counts tend to shift to brighter magnitudes at constant number density, while the red counts remain largely constant at a fixed absolute magnitude. Using Schechter functions with fixed faint-end slopes we find that M*_B for blue galaxies brightens by ~ 1.3 magnitudes per unit redshift, with no significant evolution in number density. For red galaxies M*_B brightens somewhat less with redshift, while the formal value of phi* declines. When the population of blue galaxies is subdivided into two halves using the rest-frame color as the criterion, the measured evolution of both blue subpopulations is very similar."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the evolution of the B-band galaxy luminosity function in the DEEP2 Redshift Survey. What is the primary difference in the luminosity function evolution between blue and red galaxies, and how does this difference impact our understanding of galaxy evolution?\n\n**A)** The luminosity function of blue galaxies brightens by ~ 1.3 magnitudes per unit redshift, while the luminosity function of red galaxies remains largely constant at a fixed absolute magnitude.\n\n**B)** The luminosity function of blue galaxies remains largely constant at a fixed absolute magnitude, while the luminosity function of red galaxies brightens by ~ 1.3 magnitudes per unit redshift.\n\n**C)** The luminosity function of both blue and red galaxies evolves similarly, with no significant differences in their evolution.\n\n**D)** The luminosity function of blue galaxies declines with redshift, while the luminosity function of red galaxies brightens by ~ 1.3 magnitudes per unit redshift.\n\n**Correct Answer:** A) The luminosity function of blue galaxies brightens by ~ 1.3 magnitudes per unit redshift, while the luminosity function of red galaxies remains largely constant at a fixed absolute magnitude.\n\n**Explanation:** The correct answer is A) because the text states that \"Blue counts tend to shift to brighter magnitudes at constant number density, while the red counts remain largely constant at a fixed absolute magnitude.\" This indicates that the luminosity function of blue galaxies brightens with redshift, while the luminosity function of red galaxies remains largely constant."}, "23": {"documentation": {"title": "Matching on What Matters: A Pseudo-Metric Learning Approach to Matching\n  Estimation in High Dimensions", "source": "Gentry Johnson, Brian Quistorff, Matt Goldman", "docs_id": "1905.12020", "section": ["econ.EM", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Matching on What Matters: A Pseudo-Metric Learning Approach to Matching\n  Estimation in High Dimensions. When pre-processing observational data via matching, we seek to approximate each unit with maximally similar peers that had an alternative treatment status--essentially replicating a randomized block design. However, as one considers a growing number of continuous features, a curse of dimensionality applies making asymptotically valid inference impossible (Abadie and Imbens, 2006). The alternative of ignoring plausibly relevant features is certainly no better, and the resulting trade-off substantially limits the application of matching methods to \"wide\" datasets. Instead, Li and Fu (2017) recasts the problem of matching in a metric learning framework that maps features to a low-dimensional space that facilitates \"closer matches\" while still capturing important aspects of unit-level heterogeneity. However, that method lacks key theoretical guarantees and can produce inconsistent estimates in cases of heterogeneous treatment effects. Motivated by straightforward extension of existing results in the matching literature, we present alternative techniques that learn latent matching features through either MLPs or through siamese neural networks trained on a carefully selected loss function. We benchmark the resulting alternative methods in simulations as well as against two experimental data sets--including the canonical NSW worker training program data set--and find superior performance of the neural-net-based methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of matching on what matters, what is the primary challenge that arises when dealing with high-dimensional datasets, and how do Li and Fu (2017) attempt to address this challenge?\n\n**A)** The primary challenge is the loss of information due to the curse of dimensionality, and Li and Fu (2017) address this by ignoring irrelevant features.\n\n**B)** The primary challenge is the difficulty in estimating treatment effects due to heterogeneous treatment effects, and Li and Fu (2017) address this by recasting the problem in a metric learning framework.\n\n**C)** The primary challenge is the need for a randomized block design, and Li and Fu (2017) address this by mapping features to a low-dimensional space using a neural network.\n\n**D)** The primary challenge is the lack of theoretical guarantees in existing matching methods, and Li and Fu (2017) address this by using a siamese neural network trained on a carefully selected loss function.\n\n**Correct Answer:** D) The primary challenge is the lack of theoretical guarantees in existing matching methods, and Li and Fu (2017) address this by using a siamese neural network trained on a carefully selected loss function.\n\n**Explanation:** The correct answer is D) because the documentation states that Li and Fu (2017) recast the problem of matching in a metric learning framework, but lacks key theoretical guarantees and can produce inconsistent estimates in cases of heterogeneous treatment effects. The alternative methods presented in the paper, such as using MLPs or siamese neural networks, address this challenge by providing theoretical guarantees and improving the performance of matching methods.\n\n**Explanation of incorrect options:**\n\n* A) is incorrect because Li and Fu (2017) do not ignore irrelevant features, but rather attempt to address the curse of dimensionality.\n* B) is incorrect because while Li and Fu (2017) do recast the problem in a metric learning framework, this is not the primary challenge they are addressing.\n* C) is incorrect because the primary challenge is not the need for a randomized block design, but rather the lack of theoretical guarantees in existing matching methods."}, "24": {"documentation": {"title": "Temperature-dependent non-covalent protein-protein interactions explain\n  normal and inverted solubility in a mutant of human gamma D-crystallin", "source": "Amir R. Khan, Susan James, Michelle K. Quinn, Irem Altan, Patrick\n  Charbonneau, Jennifer J. McManus", "docs_id": "1811.00477", "section": ["q-bio.BM", "cond-mat.soft", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temperature-dependent non-covalent protein-protein interactions explain\n  normal and inverted solubility in a mutant of human gamma D-crystallin. Protein crystal production is a major bottleneck for the structural characterisation of proteins. To advance beyond large-scale screening, rational strategies for protein crystallization are crucial. Understanding how chemical anisotropy (or patchiness) of the protein surface due to the variety of amino acid side chains in contact with solvent, contributes to protein protein contact formation in the crystal lattice is a major obstacle to predicting and optimising crystallization. The relative scarcity of sophisticated theoretical models that include sufficient detail to link collective behaviour, captured in protein phase diagrams, and molecular level details, determined from high-resolution structural information is a further barrier. Here we present two crystals structures for the P23TR36S mutant of gamma D-crystallin, each with opposite solubility behaviour, one melts when heated, the other when cooled. When combined with the protein phase diagram and a tailored patchy particle model we show that a single temperature dependent interaction is sufficient to stabilise the inverted solubility crystal. This contact, at the P23T substitution site, relates to a genetic cataract and reveals at a molecular level, the origin of the lowered and retrograde solubility of the protein. Our results show that the approach employed here may present an alternative strategy for the rationalization of protein crystallization."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary obstacle to predicting and optimizing protein crystallization, according to the study on temperature-dependent non-covalent protein-protein interactions in human gamma D-crystallin?\n\nA) The scarcity of high-resolution structural information\nB) The difficulty in understanding the role of chemical anisotropy on protein surface\nC) The lack of sophisticated theoretical models that link collective behavior and molecular level details\nD) The limited understanding of protein phase diagrams\n\n**Correct Answer:** C) The lack of sophisticated theoretical models that link collective behavior and molecular level details\n\n**Explanation:** The study highlights the scarcity of theoretical models that include sufficient detail to link collective behavior, captured in protein phase diagrams, and molecular level details, determined from high-resolution structural information. This gap in understanding is a major barrier to predicting and optimizing protein crystallization.\n\n**Candidate A:** The scarcity of high-resolution structural information is a related issue, but not the primary obstacle. High-resolution structural information is necessary for understanding protein behavior, but it is not the main challenge in predicting and optimizing protein crystallization.\n\n**Candidate B:** Chemical anisotropy on the protein surface is an important factor in protein-protein contact formation, but it is not the primary obstacle to predicting and optimizing protein crystallization.\n\n**Candidate C:** This is the correct answer. The lack of sophisticated theoretical models that link collective behavior and molecular level details is the primary obstacle to predicting and optimizing protein crystallization.\n\n**Candidate D:** Protein phase diagrams are an important tool for understanding protein behavior, but the lack of sophisticated theoretical models that link collective behavior and molecular level details is the primary obstacle to predicting and optimizing protein crystallization."}, "25": {"documentation": {"title": "Differential privacy and robust statistics in high dimensions", "source": "Xiyang Liu, Weihao Kong, Sewoong Oh", "docs_id": "2111.06578", "section": ["math.ST", "cs.CR", "cs.IT", "cs.LG", "math.IT", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Differential privacy and robust statistics in high dimensions. We introduce a universal framework for characterizing the statistical efficiency of a statistical estimation problem with differential privacy guarantees. Our framework, which we call High-dimensional Propose-Test-Release (HPTR), builds upon three crucial components: the exponential mechanism, robust statistics, and the Propose-Test-Release mechanism. Gluing all these together is the concept of resilience, which is central to robust statistical estimation. Resilience guides the design of the algorithm, the sensitivity analysis, and the success probability analysis of the test step in Propose-Test-Release. The key insight is that if we design an exponential mechanism that accesses the data only via one-dimensional robust statistics, then the resulting local sensitivity can be dramatically reduced. Using resilience, we can provide tight local sensitivity bounds. These tight bounds readily translate into near-optimal utility guarantees in several cases. We give a general recipe for applying HPTR to a given instance of a statistical estimation problem and demonstrate it on canonical problems of mean estimation, linear regression, covariance estimation, and principal component analysis. We introduce a general utility analysis technique that proves that HPTR nearly achieves the optimal sample complexity under several scenarios studied in the literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key insight behind the High-dimensional Propose-Test-Release (HPTR) framework, and how does it relate to the design of the algorithm, sensitivity analysis, and success probability analysis of the test step in Propose-Test-Release?\n\n**A)** The key insight is that the exponential mechanism should access the data via multiple-dimensional robust statistics to reduce local sensitivity. This allows for tighter bounds on the success probability of the test step.\n\n**B)** The key insight is that the exponential mechanism should access the data via one-dimensional robust statistics to reduce local sensitivity. This enables the design of an algorithm that can provide tight local sensitivity bounds.\n\n**C)** The key insight is that the Propose-Test-Release mechanism should be designed to access the data via multiple-dimensional robust statistics to increase the sample complexity of the algorithm.\n\n**D)** The key insight is that the resilience concept is not crucial for the design of the algorithm, and the exponential mechanism can be designed to access the data via multiple-dimensional robust statistics without any impact on local sensitivity.\n\n**Correct Answer:** B) The key insight is that the exponential mechanism should access the data via one-dimensional robust statistics to reduce local sensitivity. This enables the design of an algorithm that can provide tight local sensitivity bounds.\n\n**Explanation:** The correct answer is B) because the documentation states that \"if we design an exponential mechanism that accesses the data only via one-dimensional robust statistics, then the resulting local sensitivity can be dramatically reduced.\" This highlights the key insight behind the HPTR framework, which is that accessing the data via one-dimensional robust statistics can lead to tighter bounds on local sensitivity."}, "26": {"documentation": {"title": "An Interpretable Intensive Care Unit Mortality Risk Calculator", "source": "Eugene T. Y. Ang, Milashini Nambiar, Yong Sheng Soh, Vincent Y. F. Tan", "docs_id": "2101.07426", "section": ["stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Interpretable Intensive Care Unit Mortality Risk Calculator. Mortality risk is a major concern to patients have just been discharged from the intensive care unit (ICU). Many studies have been directed to construct machine learning models to predict such risk. Although these models are highly accurate, they are less amenable to interpretation and clinicians are typically unable to gain further insights into the patients' health conditions and the underlying factors that influence their mortality risk. In this paper, we use patients' profiles extracted from the MIMIC-III clinical database to construct risk calculators based on different machine learning techniques such as logistic regression, decision trees, random forests and multilayer perceptrons. We perform an extensive benchmarking study that compares the most salient features as predicted by various methods. We observe a high degree of agreement across the considered machine learning methods; in particular, the cardiac surgery recovery unit, age, and blood urea nitrogen levels are commonly predicted to be the most salient features for determining patients' mortality risks. Our work has the potential for clinicians to interpret risk predictions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the primary limitations of machine learning models used to predict mortality risk in patients discharged from the intensive care unit (ICU), and how does the proposed risk calculator in the paper address these limitations?\n\n**A)** Machine learning models are limited by their inability to account for the complex interactions between multiple variables, leading to inaccurate predictions. The proposed risk calculator addresses this limitation by using a combination of feature engineering techniques to extract relevant information from patients' profiles.\n\n**B)** Machine learning models are limited by their lack of interpretability, making it difficult for clinicians to understand the underlying factors that influence mortality risk. The proposed risk calculator addresses this limitation by providing a set of salient features that can be used to interpret the predictions.\n\n**C)** Machine learning models are limited by their reliance on large amounts of data, which can be difficult to obtain in clinical settings. The proposed risk calculator addresses this limitation by using a large dataset from the MIMIC-III clinical database to train the models.\n\n**D)** Machine learning models are limited by their inability to account for the temporal dynamics of patient health, leading to inaccurate predictions. The proposed risk calculator addresses this limitation by using a multilayer perceptron model that can capture temporal relationships between variables.\n\n**Correct Answer:** B) Machine learning models are limited by their lack of interpretability, making it difficult for clinicians to understand the underlying factors that influence mortality risk. The proposed risk calculator addresses this limitation by providing a set of salient features that can be used to interpret the predictions.\n\n**Explanation:** The paper highlights the limitation of machine learning models in predicting mortality risk due to their lack of interpretability. The proposed risk calculator addresses this limitation by identifying the most salient features that contribute to mortality risk, such as cardiac surgery recovery unit, age, and blood urea nitrogen levels. These features can be used by clinicians to gain insights into the underlying factors that influence mortality risk, making the risk predictions more interpretable and actionable."}, "27": {"documentation": {"title": "Genetic information, physical interpreters and thermodynamics; the\n  material-informatic basis of biosemiosis", "source": "Peter R. Wills", "docs_id": "1308.2107", "section": ["q-bio.OT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Genetic information, physical interpreters and thermodynamics; the\n  material-informatic basis of biosemiosis. The sequence of nucleotide bases occurring in an organism's DNA is often regarded as a codescript for its construction. However, information in a DNA sequence can only be regarded as a codescript relative to an operational biochemical machine, which the information constrains in such a way as to direct the process of construction. In reality, any biochemical machine for which a DNA codescript is efficacious is itself produced through the mechanical interpretation of an identical or very similar codescript. In these terms the origin of life can be described as a bootstrap process involving the simultaneous accumulation of genetic information and the generation of a machine that interprets it as instructions for its own construction. This problem is discussed within the theoretical frameworks of thermodynamics, informatics and self-reproducing automata, paying special attention to the physico-chemical origin of genetic coding and the conditions, both thermodynamic and informatic, which a system must fulfil in order for it to sustain semiosis. The origin of life is equated with biosemiosis"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the fundamental relationship between genetic information, physical interpreters, and thermodynamics in the context of biosemiosis, and how does this relationship impact our understanding of the origin of life?\n\nA) Genetic information is a direct result of thermodynamic processes, and physical interpreters are merely byproducts of these processes.\nB) The origin of life is a thermodynamic process that generates genetic information, which is then interpreted by physical interpreters to sustain semiosis.\nC) Physical interpreters are the primary drivers of genetic information, and thermodynamics plays a secondary role in the interpretation of this information.\nD) Genetic information is a self-sustaining system that can arise independently of thermodynamic processes, and physical interpreters are merely tools for its interpretation.\n\n**Correct Answer:** B) The origin of life is a thermodynamic process that generates genetic information, which is then interpreted by physical interpreters to sustain semiosis.\n\n**Explanation:**\n\nThe correct answer, B, reflects the idea that the origin of life is a bootstrap process involving the simultaneous accumulation of genetic information and the generation of a machine that interprets it as instructions for its own construction. This process is deeply rooted in thermodynamics, as it involves the mechanical interpretation of genetic information to sustain semiosis. The other options are incorrect because they either oversimplify the relationship between genetic information, physical interpreters, and thermodynamics (A and C), or imply that genetic information can arise independently of thermodynamic processes (D)."}, "28": {"documentation": {"title": "Nonlinear bang-bang eigenproblems and optimization of resonances in\n  layered cavities", "source": "Illya M. Karabash, Olga M. Logachova, Ievgen V. Verbytskyi", "docs_id": "1508.04706", "section": ["math.OC", "math.AP", "math.CA", "math.SP", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear bang-bang eigenproblems and optimization of resonances in\n  layered cavities. Quasi-normal-eigenvalue optimization is studied under constraints $b_1(x) \\le B(x) \\le b_2 (x)$ on structure functions $B$ of 2-side open optical or mechanical resonators. We prove existence of various optimizers and provide an example when different structures generate the same optimal quasi-(normal-)eigenvalue. To show that quasi-eigenvalues locally optimal in various senses are in the spectrum $\\Sigma^{nl}$ of the bang-bang eigenproblem $y\" = - \\omega^2 y [ b_1 + (b_2 - b_1) \\chi_{\\mathbb{C}_+} (y^2 ) ]$, where $\\chi_{\\mathbb{C}_+} (\\cdot)$ is the indicator function of the upper complex half-plane $\\mathbb{C}_+$, we obtain a variational characterization of the nonlinear spectrum $\\Sigma^{nl}$ in terms of quasi-eigenvalue perturbations. To address the minimization of the decay rate $| \\mathrm{Im} \\ \\omega |$, we study the bang-bang equation and explain how it excludes an unknown optimal $B$ from the optimization process. Computing one of minimal decay structures for 1-side open settings, we show that it resembles gradually size-modulated 1-D stack cavities introduced recently in Optical Engineering. In 2-side open symmetric settings, our example has an additional centered defect. Nonexistence of global decay rate minimizers is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a 2-side open optical resonator with structure functions $B(x)$ subject to the constraints $b_1(x) \\le B(x) \\le b_2 (x)$. The quasi-normal-eigenvalue optimization problem is to minimize the decay rate $| \\mathrm{Im} \\ \\omega |$.\n\nA) The bang-bang equation excludes the optimal structure function $B$ from the optimization process due to the presence of a centered defect in the resonator.\n\nB) The nonlinear spectrum $\\Sigma^{nl}$ of the bang-bang eigenproblem is a set of quasi-eigenvalues that locally optimize the decay rate $| \\mathrm{Im} \\ \\omega |$.\n\nC) The minimization of the decay rate $| \\mathrm{Im} \\ \\omega |$ is equivalent to finding the structure function $B(x)$ that minimizes the quasi-normal-eigenvalue perturbations.\n\nD) The quasi-normal-eigenvalue optimization problem is equivalent to finding the structure function $B(x)$ that minimizes the nonlinear spectrum $\\Sigma^{nl}$ of the bang-bang eigenproblem.\n\nCorrect Answer: B) The nonlinear spectrum $\\Sigma^{nl}$ of the bang-bang eigenproblem is a set of quasi-eigenvalues that locally optimize the decay rate $| \\mathrm{Im} \\ \\omega |$.\n\nExplanation: The correct answer is B) because the documentation states that the nonlinear spectrum $\\Sigma^{nl}$ of the bang-bang eigenproblem is a set of quasi-eigenvalues that locally optimize the decay rate $| \\mathrm{Im} \\ \\omega |$. This is a key result of the proof, which establishes a variational characterization of the nonlinear spectrum in terms of quasi-eigenvalue perturbations. The other options are incorrect because they either misinterpret the role of the bang-bang equation (A), incorrectly equate the minimization of the decay rate with the minimization of the nonlinear spectrum (C), or incorrectly equate the optimization problem with the nonlinear spectrum (D)."}, "29": {"documentation": {"title": "Contrastive Variational Autoencoder Enhances Salient Features", "source": "Abubakar Abid, James Zou", "docs_id": "1902.04601", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contrastive Variational Autoencoder Enhances Salient Features. Variational autoencoders are powerful algorithms for identifying dominant latent structure in a single dataset. In many applications, however, we are interested in modeling latent structure and variation that are enriched in a target dataset compared to some background---e.g. enriched in patients compared to the general population. Contrastive learning is a principled framework to capture such enriched variation between the target and background, but state-of-the-art contrastive methods are limited to linear models. In this paper, we introduce the contrastive variational autoencoder (cVAE), which combines the benefits of contrastive learning with the power of deep generative models. The cVAE is designed to identify and enhance salient latent features. The cVAE is trained on two related but unpaired datasets, one of which has minimal contribution from the salient latent features. The cVAE explicitly models latent features that are shared between the datasets, as well as those that are enriched in one dataset relative to the other, which allows the algorithm to isolate and enhance the salient latent features. The algorithm is straightforward to implement, has a similar run-time to the standard VAE, and is robust to noise and dataset purity. We conduct experiments across diverse types of data, including gene expression and facial images, showing that the cVAE effectively uncovers latent structure that is salient in a particular analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Contrastive Variational Autoencoder (cVAE) introduced in the paper \"Contrastive Variational Autoencoder Enhances Salient Features\"?\n\nA) To identify dominant latent structure in a single dataset\nB) To capture enriched variation between a target dataset and a background dataset\nC) To enhance salient latent features in a target dataset compared to a background dataset\nD) To model latent features that are shared between two unpaired datasets\n\n**Correct Answer:** C) To enhance salient latent features in a target dataset compared to a background dataset\n\n**Explanation:** The correct answer is C) To enhance salient latent features in a target dataset compared to a background dataset. The paper states that the cVAE is designed to \"identify and enhance salient latent features\" and that it is trained on two related but unpaired datasets, one of which has minimal contribution from the salient latent features. This suggests that the primary goal of the cVAE is to enhance the salient features in the target dataset compared to the background dataset.\n\n**Candidate A:** A) To identify dominant latent structure in a single dataset is a goal of traditional Variational Autoencoders (VAEs), but not the primary goal of the cVAE.\n\n**Candidate B:** B) To capture enriched variation between a target dataset and a background dataset is a related concept, but not the primary goal of the cVAE. The cVAE is designed to capture this variation, but its primary goal is to enhance the salient features.\n\n**Candidate D:** D) To model latent features that are shared between two unpaired datasets is a feature of the cVAE, but not its primary goal. The cVAE explicitly models both shared and enriched latent features, but its primary goal is to enhance the salient features."}, "30": {"documentation": {"title": "What Isn't Complexity?", "source": "Christopher R. Stephens", "docs_id": "1502.03199", "section": ["nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "What Isn't Complexity?. The question What is Complexity? has occupied a great deal of time and paper over the last 20 or so years. There are a myriad different perspectives and definitions but still no consensus. In this paper I take a phenomenological approach, identifying several factors that discriminate well between systems that would be consensually agreed to be simple versus others that would be consensually agreed to be complex - biological systems and human languages. I argue that a crucial component is that of structural building block hierarchies that, in the case of complex systems, correspond also to a functional hierarchy. I argue that complexity is an emergent property of this structural/functional hierarchy, induced by a property - fitness in the case of biological systems and meaning in the case of languages - that links the elements of this hierarchy across multiple scales. Additionally, I argue that non-complex systems \"are\" while complex systems \"do\" so that the latter, in distinction to physical systems, must be described not only in a space of states but also in a space of update rules (strategies) which we do not know how to specify. Further, the existence of structural/functional building block hierarchies allows for the functional specialisation of structural modules as amply observed in nature. Finally, we argue that there is at least one measuring apparatus capable of measuring complexity as characterised in the paper - the human brain itself."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the fundamental distinction between non-complex and complex systems, as argued by the author of the paper \"What Isn't Complexity?\", and how does this distinction relate to the concept of emergent properties?\n\nA) Non-complex systems are characterized by a lack of structural building block hierarchies, whereas complex systems exhibit a functional hierarchy that is induced by a property such as fitness or meaning.\n\nB) Non-complex systems are described solely in a space of states, whereas complex systems must be described in both a space of states and a space of update rules (strategies) that are not yet fully understood.\n\nC) Non-complex systems are characterized by a lack of functional specialization, whereas complex systems exhibit functional specialization of structural modules as observed in nature.\n\nD) Non-complex systems are not capable of measuring complexity, whereas complex systems can be measured by the human brain itself.\n\nCorrect Answer: A) Non-complex systems are characterized by a lack of structural building block hierarchies, whereas complex systems exhibit a functional hierarchy that is induced by a property such as fitness or meaning.\n\nExplanation: The author argues that a crucial component of complexity is the presence of structural building block hierarchies that correspond to a functional hierarchy. This hierarchical structure is what gives rise to emergent properties, such as fitness in biological systems or meaning in human languages. Non-complex systems, on the other hand, lack this hierarchical structure. Therefore, option A is the correct answer. Options B, C, and D are incorrect because they either misrepresent the author's argument or introduce concepts that are not relevant to the distinction between non-complex and complex systems."}, "31": {"documentation": {"title": "Multinomial logit processes and preference discovery: inside and outside\n  the black box", "source": "Simone Cerreia-Vioglio, Fabio Maccheroni, Massimo Marinacci, and Aldo\n  Rustichini", "docs_id": "2004.13376", "section": ["econ.TH", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multinomial logit processes and preference discovery: inside and outside\n  the black box. We provide two characterizations, one axiomatic and the other neuro-computational, of the dependence of choice probabilities on deadlines, within the widely used softmax representation \\[ p_{t}\\left( a,A\\right) =\\dfrac{e^{\\frac{u\\left( a\\right) }{\\lambda \\left( t\\right) }+\\alpha \\left( a\\right) }}{\\sum_{b\\in A}e^{\\frac{u\\left( b\\right) }{\\lambda \\left( t\\right) }+\\alpha \\left( b\\right) }}% \\] where $p_{t}\\left( a,A\\right) $ is the probability that alternative $a$ is selected from the set $A$ of feasible alternatives if $t$ is the time available to decide, $\\lambda$ is a time dependent noise parameter measuring the unit cost of information, $u$ is a time independent utility function, and $\\alpha$ is an alternative-specific bias that determines the initial choice probabilities reflecting prior information and memory anchoring. Our axiomatic analysis provides a behavioral foundation of softmax (also known as Multinomial Logit Model when $\\alpha$ is constant). Our neuro-computational derivation provides a biologically inspired algorithm that may explain the emergence of softmax in choice behavior. Jointly, the two approaches provide a thorough understanding of soft-maximization in terms of internal causes (neurophysiological mechanisms) and external effects (testable implications)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the multinomial logit process, what is the role of the parameter $\\alpha$ in the softmax representation, and how does it relate to the emergence of softmax in choice behavior?\n\n**A)** $\\alpha$ is a time-dependent parameter that measures the unit cost of information, and its value determines the initial choice probabilities reflecting prior information and memory anchoring.\n\n**B)** $\\alpha$ is a time-independent parameter that represents the strength of alternative-specific biases, and its value determines the shape of the softmax function.\n\n**C)** $\\alpha$ is a neurophysiological parameter that measures the neural activity associated with each alternative, and its value determines the emergence of softmax in choice behavior.\n\n**D)** $\\alpha$ is a parameter that is set to zero when the softmax function is used, and its value determines the initial choice probabilities reflecting prior information and memory anchoring.\n\n**Correct Answer:** B) $\\alpha$ is a time-independent parameter that represents the strength of alternative-specific biases, and its value determines the shape of the softmax function.\n\n**Explanation:** The parameter $\\alpha$ is an alternative-specific bias that determines the initial choice probabilities reflecting prior information and memory anchoring. In the softmax representation, $\\alpha$ is a time-independent parameter that represents the strength of alternative-specific biases. Its value determines the shape of the softmax function, which in turn determines the choice probabilities. The correct answer, B, highlights the role of $\\alpha$ in determining the shape of the softmax function, which is a key aspect of the multinomial logit process.\n\nNote: The other options are incorrect because:\n\n* A) $\\alpha$ is not a time-dependent parameter that measures the unit cost of information.\n* C) $\\alpha$ is not a neurophysiological parameter that measures neural activity associated with each alternative.\n* D) $\\alpha$ is not a parameter that is set to zero when the softmax function is used."}, "32": {"documentation": {"title": "Technology networks: the autocatalytic origins of innovation", "source": "Lorenzo Napolitano, Evangelos Evangelou, Emanuele Pugliese, Paolo\n  Zeppini, Graham Room", "docs_id": "1708.03511", "section": ["econ.GN", "cs.SI", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Technology networks: the autocatalytic origins of innovation. We analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. To this aim, we define a directed network of technological fields based on the International Patents Classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. We show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. We further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. Finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Technology networks: the autocatalytic origins of innovation\" propose a directed network of technological fields based on the International Patents Classification. What is the primary mechanism by which technological fields in the core of the autocatalytic set are said to benefit from being connected to one another?\n\nA) Positive spillovers occur when a technological field is connected to a receiver field that has a similar patenting activity profile.\nB) Positive reinforcement is achieved when a technological field is connected to a receiver field that has a higher patenting activity rate than the sender field.\nC) Core fields benefit from being connected to one another through the presence of autocatalytic structure, which enables positive spillovers and reinforcement.\nD) The connection between core fields is driven by the hierarchical classification of technological fields, which creates a natural grouping of related fields.\n\n**Correct Answer:** C) Core fields benefit from being connected to one another through the presence of autocatalytic structure, which enables positive spillovers and reinforcement.\n\n**Explanation:** The correct answer is based on the idea that the autocatalytic structure of the technological network enables positive spillovers and reinforcement, where technological fields in the core of the autocatalytic set tend to appear in a greater number of patents. This suggests that the connection between core fields is driven by the presence of autocatalytic structure, which enables the exchange of ideas and innovations between related fields. The other options are incorrect because they do not accurately capture the mechanism by which core fields benefit from being connected to one another."}, "33": {"documentation": {"title": "A pricing measure to explain the risk premium in power markets", "source": "Fred Espen Benth and Salvador Ortiz-Latorre", "docs_id": "1308.3378", "section": ["q-fin.PR", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A pricing measure to explain the risk premium in power markets. In electricity markets, it is sensible to use a two-factor model with mean reversion for spot prices. One of the factors is an Ornstein-Uhlenbeck (OU) process driven by a Brownian motion and accounts for the small variations. The other factor is an OU process driven by a pure jump L\\'evy process and models the characteristic spikes observed in such markets. When it comes to pricing, a popular choice of pricing measure is given by the Esscher transform that preserves the probabilistic structure of the driving L\\'evy processes, while changing the levels of mean reversion. Using this choice one can generate stochastic risk premiums (in geometric spot models) but with (deterministically) changing sign. In this paper we introduce a pricing change of measure, which is an extension of the Esscher transform. With this new change of measure we also can slow down the speed of mean reversion and generate stochastic risk premiums with stochastic non constant sign, even in arithmetic spot models. In particular, we can generate risk profiles with positive values in the short end of the forward curve and negative values in the long end. Finally, our pricing measure allows us to have a stationary spot dynamics while still having randomly fluctuating forward prices for contracts far from maturity."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Esscher transform and its extension for pricing change of measure in power markets, what is the primary advantage of using this pricing measure, and how does it differ from the traditional Esscher transform?\n\n**A)** The primary advantage is that it preserves the probabilistic structure of the driving L\u00e9vy processes, allowing for stochastic risk premiums with non-constant sign. However, it does not change the levels of mean reversion.\n\n**B)** The primary advantage is that it allows for stochastic risk premiums with non-constant sign, even in arithmetic spot models, and enables the generation of risk profiles with positive values in the short end of the forward curve and negative values in the long end.\n\n**C)** The primary advantage is that it slows down the speed of mean reversion, enabling the generation of stochastic risk premiums with non-constant sign, while preserving the probabilistic structure of the driving L\u00e9vy processes.\n\n**D)** The primary advantage is that it generates stationary spot dynamics while still having randomly fluctuating forward prices for contracts far from maturity, but it does not change the levels of mean reversion.\n\n**Correct Answer:** B) The primary advantage is that it allows for stochastic risk premiums with non-constant sign, even in arithmetic spot models, and enables the generation of risk profiles with positive values in the short end of the forward curve and negative values in the long end.\n\n**Explanation:** The correct answer highlights the key benefits of the pricing change of measure introduced in the paper, which include the ability to generate stochastic risk premiums with non-constant sign, even in arithmetic spot models, and the generation of risk profiles with positive values in the short end of the forward curve and negative values in the long end. This answer demonstrates an understanding of the primary advantage of the pricing measure and its ability to accommodate complex risk profiles in power markets."}, "34": {"documentation": {"title": "Confidence intervals for nonhomogeneous branching processes and\n  polymerase chain reactions", "source": "Didier Piau", "docs_id": "math/0503659", "section": ["math.PR", "q-bio.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Confidence intervals for nonhomogeneous branching processes and\n  polymerase chain reactions. We extend in two directions our previous results about the sampling and the empirical measures of immortal branching Markov processes. Direct applications to molecular biology are rigorous estimates of the mutation rates of polymerase chain reactions from uniform samples of the population after the reaction. First, we consider nonhomogeneous processes, which are more adapted to real reactions. Second, recalling that the first moment estimator is analytically known only in the infinite population limit, we provide rigorous confidence intervals for this estimator that are valid for any finite population. Our bounds are explicit, nonasymptotic and valid for a wide class of nonhomogeneous branching Markov processes that we describe in detail. In the setting of polymerase chain reactions, our results imply that enlarging the size of the sample becomes useless for surprisingly small sizes. Establishing confidence intervals requires precise estimates of the second moment of random samples. The proof of these estimates is more involved than the proofs that allowed us, in a previous paper, to deal with the first moment. On the other hand, our method uses various, seemingly new, monotonicity properties of the harmonic moments of sums of exchangeable random variables."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a nonhomogeneous branching Markov process with a finite population size. Suppose we have a uniform sample of the population after a polymerase chain reaction, and we want to estimate the mutation rate of the reaction. What is the main limitation of using a larger sample size in this context, according to the provided documentation?\n\n**A)** The mutation rate is more accurately estimated with a larger sample size.\n**B)** The mutation rate is less accurately estimated with a larger sample size, as the confidence intervals become useless for surprisingly small sizes.\n**C)** The mutation rate is not affected by the sample size, as the process is homogeneous.\n**D)** The mutation rate is only accurately estimated with an infinite population size.\n\n**Correct Answer:** B) The mutation rate is less accurately estimated with a larger sample size, as the confidence intervals become useless for surprisingly small sizes.\n\n**Explanation:** The documentation states that \"enlarging the size of the sample becomes useless for surprisingly small sizes\" in the context of polymerase chain reactions. This implies that using a larger sample size does not necessarily lead to more accurate estimates of the mutation rate, but rather that the confidence intervals become less useful for smaller sample sizes. This requires the test-taker to carefully read and understand the nuances of the documentation, and to apply the information to a specific scenario."}, "35": {"documentation": {"title": "Adaptive convolutional neural networks for k-space data interpolation in\n  fast magnetic resonance imaging", "source": "Tianming Du, Honggang Zhang, Yuemeng Li, Hee Kwon Song, Yong Fan", "docs_id": "2006.01385", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive convolutional neural networks for k-space data interpolation in\n  fast magnetic resonance imaging. Deep learning in k-space has demonstrated great potential for image reconstruction from undersampled k-space data in fast magnetic resonance imaging (MRI). However, existing deep learning-based image reconstruction methods typically apply weight-sharing convolutional neural networks (CNNs) to k-space data without taking into consideration the k-space data's spatial frequency properties, leading to ineffective learning of the image reconstruction models. Moreover, complementary information of spatially adjacent slices is often ignored in existing deep learning methods. To overcome such limitations, we develop a deep learning algorithm, referred to as adaptive convolutional neural networks for k-space data interpolation (ACNN-k-Space), which adopts a residual Encoder-Decoder network architecture to interpolate the undersampled k-space data by integrating spatially contiguous slices as multi-channel input, along with k-space data from multiple coils if available. The network is enhanced by self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels. We have evaluated our method on two public datasets and compared it with state-of-the-art existing methods. Ablation studies and experimental results demonstrate that our method effectively reconstructs images from undersampled k-space data and achieves significantly better image reconstruction performance than current state-of-the-art techniques."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of existing deep learning-based image reconstruction methods for undersampled k-space data in fast magnetic resonance imaging (MRI), and how does the proposed ACNN-k-Space algorithm address this limitation?\n\nA) Existing methods ignore the spatial frequency properties of k-space data, leading to ineffective learning of image reconstruction models. ACNN-k-Space addresses this limitation by incorporating self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels.\n\nB) Existing methods apply weight-sharing convolutional neural networks (CNNs) to k-space data without considering the spatial frequency properties of k-space data. ACNN-k-Space addresses this limitation by using a residual Encoder-Decoder network architecture to integrate spatially contiguous slices as multi-channel input.\n\nC) Existing methods ignore the complementary information of spatially adjacent slices, leading to ineffective image reconstruction. ACNN-k-Space addresses this limitation by incorporating self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels.\n\nD) Existing methods apply weight-sharing CNNs to k-space data without considering the k-space data's spatial frequency properties, leading to ineffective learning of the image reconstruction models. ACNN-k-Space addresses this limitation by using a residual Encoder-Decoder network architecture to integrate spatially contiguous slices as multi-channel input.\n\nCorrect Answer: A) Existing methods ignore the spatial frequency properties of k-space data, leading to ineffective learning of image reconstruction models. ACNN-k-Space addresses this limitation by incorporating self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels.\n\nExplanation: The correct answer is A) because the documentation states that existing deep learning-based image reconstruction methods \"typically apply weight-sharing convolutional neural networks (CNNs) to k-space data without taking into consideration the k-space data's spatial frequency properties, leading to ineffective learning of the image reconstruction models.\" The proposed ACNN-k-Space algorithm addresses this limitation by incorporating self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels."}, "36": {"documentation": {"title": "Energy Disaggregation using Variational Autoencoders", "source": "Antoine Langevin, Marc-Andr\\'e Carbonneau, Mohamed Cheriet, Ghyslain\n  Gagnon", "docs_id": "2103.12177", "section": ["cs.LG", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy Disaggregation using Variational Autoencoders. Non-intrusive load monitoring (NILM) is a technique that uses a single sensor to measure the total power consumption of a building. Using an energy disaggregation method, the consumption of individual appliances can be estimated from the aggregate measurement. Recent disaggregation algorithms have significantly improved the performance of NILM systems. However, the generalization capability of these methods to different houses as well as the disaggregation of multi-state appliances are still major challenges. In this paper we address these issues and propose an energy disaggregation approach based on the variational autoencoders framework. The probabilistic encoder makes this approach an efficient model for encoding information relevant to the reconstruction of the target appliance consumption. In particular, the proposed model accurately generates more complex load profiles, thus improving the power signal reconstruction of multi-state appliances. Moreover, its regularized latent space improves the generalization capabilities of the model across different houses. The proposed model is compared to state-of-the-art NILM approaches on the UK-DALE and REFIT datasets, and yields competitive results. The mean absolute error reduces by 18% on average across all appliances compared to the state-of-the-art. The F1-Score increases by more than 11%, showing improvements for the detection of the target appliance in the aggregate measurement."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a probabilistic encoder in the proposed energy disaggregation approach based on variational autoencoders, and how does it improve the performance of NILM systems?\n\n**A)** The probabilistic encoder allows for more accurate reconstruction of the target appliance consumption by reducing the impact of noise in the aggregate measurement.\n\n**B)** The probabilistic encoder enables the model to generate more complex load profiles, which improves the power signal reconstruction of multi-state appliances.\n\n**C)** The probabilistic encoder improves the generalization capabilities of the model across different houses by reducing the dimensionality of the latent space.\n\n**D)** The probabilistic encoder increases the computational efficiency of the model by reducing the number of parameters required for the encoder.\n\n**Correct Answer:** B) The probabilistic encoder enables the model to generate more complex load profiles, which improves the power signal reconstruction of multi-state appliances.\n\n**Explanation:** The correct answer is B) because the probabilistic encoder is specifically mentioned in the paper as a key component that allows the model to generate more complex load profiles, which is essential for improving the power signal reconstruction of multi-state appliances. This is a critical advantage of the proposed approach, as it enables the model to better capture the nuances of real-world appliance behavior. The other options are incorrect because they either misrepresent the role of the probabilistic encoder (A and C) or are unrelated to the specific advantage of the encoder (D)."}, "37": {"documentation": {"title": "Low-Rank Separated Representation Surrogates of High-Dimensional\n  Stochastic Functions: Application in Bayesian Inference", "source": "AbdoulAhad Validi", "docs_id": "1306.5374", "section": ["physics.data-an", "math-ph", "math.MP", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Low-Rank Separated Representation Surrogates of High-Dimensional\n  Stochastic Functions: Application in Bayesian Inference. This study introduces a non-intrusive approach in the context of low-rank separated representation to construct a surrogate of high-dimensional stochastic functions, e.g., PDEs/ODEs, in order to decrease the computational cost of Markov Chain Monte Carlo simulations in Bayesian inference. The surrogate model is constructed via a regularized alternative least-square regression with Tikhonov regularization using a roughening matrix computing the gradient of the solution, in conjunction with a perturbation-based error indicator to detect optimal model complexities. The model approximates a vector of a continuous solution at discrete values of a physical variable. The required number of random realizations to achieve a successful approximation linearly depends on the function dimensionality. The computational cost of the model construction is quadratic in the number of random inputs, which potentially tackles the curse of dimensionality in high-dimensional stochastic functions. Furthermore, this vector valued separated representation-based model, in comparison to the available scalar-valued case, leads to a significant reduction in the cost of approximation by an order of magnitude equal to the vector size. The performance of the method is studied through its application to three numerical examples including a 41-dimensional elliptic PDE and a 21-dimensional cavity flow."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study \"Low-Rank Separated Representation Surrogates of High-Dimensional Stochastic Functions: Application in Bayesian Inference\", what is the primary advantage of using a vector-valued separated representation-based model over the available scalar-valued case in terms of computational cost?\n\n**A)** The computational cost of the model construction is linear in the number of random inputs.\n**B)** The computational cost of the model construction is quadratic in the number of random inputs, which potentially tackles the curse of dimensionality in high-dimensional stochastic functions.\n**C)** The required number of random realizations to achieve a successful approximation linearly depends on the function dimensionality.\n**D)** The model approximates a vector of a continuous solution at discrete values of a physical variable, but the computational cost is reduced by a factor of the vector size.\n\n**Correct Answer:** D) The model approximates a vector of a continuous solution at discrete values of a physical variable, but the computational cost is reduced by a factor of the vector size.\n\n**Explanation:** The correct answer is D) because the study states that the vector-valued separated representation-based model leads to a significant reduction in the cost of approximation by an order of magnitude equal to the vector size. This means that the computational cost is reduced by a factor of the vector size, not the number of random inputs or the function dimensionality."}, "38": {"documentation": {"title": "Spin polarization in the Hubbard model with Rashba spin-orbit coupling\n  on a ladder", "source": "Jos\\'e A. Riera", "docs_id": "1303.3613", "section": ["cond-mat.str-el", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin polarization in the Hubbard model with Rashba spin-orbit coupling\n  on a ladder. The competition between on-site Coulomb repulsion and Rashba spin-orbit coupling (RSOC) is studied on two-leg ladders by numerical techniques. By studying persistent currents in closed rings by exact diagonalization, it is found that the contribution to the current due to the RSOC V_{SO}, for a fixed value of the Hubbard repulsion U reaches a maximum at intermediate values of V_{SO}. By increasing the repulsive Hubbard coupling U, this spin-flipping current is suppressed and eventually it becomes opposite to the spin-conserving current. The main result is that the spin accumulation defined as the relative spin polarization between the two legs of the ladder is enhanced by U. Similar results for this Hubbard-Rashba model are observed for a completely different setup in which two halves of the ladders are connected to a voltage bias and the ensuing time-dependent regime is studied by the density matrix-renormalization group technique. It is also interesting a combined effect between V_{SO} and U leading to a strong enhancement of antiferromagnetic order which in turn may explain the observed behavior of the spin-flipping current. The implications of this enhancement of the spin-Hall effect with electron correlations for spintronic devices is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the competition between on-site Coulomb repulsion and Rashba spin-orbit coupling (RSOC) in the Hubbard model on a two-leg ladder. How do the spin accumulation and spin-flipping current evolve with increasing Hubbard repulsion U, and what are the implications of this behavior for spintronic devices?\n\n**A)** The spin accumulation is enhanced by increasing Hubbard repulsion U, while the spin-flipping current is suppressed, leading to a stronger spin-Hall effect. However, this enhancement is accompanied by a strong antiferromagnetic order, which may explain the observed behavior of the spin-flipping current.\n\n**B)** The spin accumulation is suppressed by increasing Hubbard repulsion U, while the spin-flipping current is enhanced, leading to a weaker spin-Hall effect. This behavior is attributed to the combined effect of V_{SO} and U, which leads to a strong antiferromagnetic order.\n\n**C)** The spin accumulation and spin-flipping current are independent of each other and do not evolve with increasing Hubbard repulsion U. The behavior of the spin-flipping current is solely determined by the Rashba spin-orbit coupling (RSOC) V_{SO}.\n\n**D)** The spin accumulation is enhanced by increasing Hubbard repulsion U, while the spin-flipping current is suppressed, leading to a stronger spin-Hall effect. However, this enhancement is accompanied by a strong antiferromagnetic order, which may explain the observed behavior of the spin-flipping current.\n\n**Correct Answer:** A) The spin accumulation is enhanced by increasing Hubbard repulsion U, while the spin-flipping current is suppressed, leading to a stronger spin-Hall effect. However, this enhancement is accompanied by a strong antiferromagnetic order, which may explain the observed behavior of the spin-flipping current.\n\n**Explanation:** The correct answer is A) because the documentation states that the spin accumulation is enhanced by increasing Hubbard repulsion U, while the spin-flipping current is suppressed. This leads to a stronger spin-Hall effect. Additionally, the documentation mentions that the combined effect of V_{SO} and U leads to a strong antiferromagnetic order, which may explain the observed behavior of the spin-flipping current."}, "39": {"documentation": {"title": "Wealth disparities and economic flow: Assessment using an asset exchange\n  model with the surplus stock of the wealthy", "source": "Takeshi Kato, Yoshinori Hiroi", "docs_id": "2108.07888", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wealth disparities and economic flow: Assessment using an asset exchange\n  model with the surplus stock of the wealthy. How can we limit wealth disparities while stimulating economic flows in sustainable societies? To examine the link between these concepts, we propose an econophysics asset exchange model with the surplus stock of the wealthy. The wealthy are one of the two exchange agents and have more assets than the poor. Our simulation model converts the surplus contribution rate of the wealthy to a new variable parameter alongside the saving rate and introduces the total exchange (flow) and rank correlation coefficient (metabolism) as new evaluation indexes, adding to the Gini index (disparities), thereby assessing both wealth distribution and the relationships among the disparities, flow, and metabolism. We show that these result in a gamma-like wealth distribution, and our model reveals a trade-off between limiting disparities and vitalizing the market. To limit disparities and increase flow and metabolism, we also find the need to restrain savings and use the wealthy surplus stock. This relationship is explicitly expressed in the new equation introduced herein. The insights gained by uncovering the root of disparities may present a persuasive case for investments in social security measures or social businesses involving stock redistribution or sharing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the proposed econophysics asset exchange model, and how does it relate to limiting wealth disparities and stimulating economic flows in sustainable societies?\n\nA) To maximize the Gini index and minimize the surplus contribution rate of the wealthy\nB) To introduce a new variable parameter that converts the surplus contribution rate of the wealthy to a new variable alongside the saving rate\nC) To reveal a trade-off between limiting disparities and vitalizing the market, and to restrain savings and use the wealthy surplus stock\nD) To assess the relationships among the disparities, flow, and metabolism, and to show that the model results in a gamma-like wealth distribution\n\nCorrect Answer: C) To reveal a trade-off between limiting disparities and vitalizing the market, and to restrain savings and use the wealthy surplus stock\n\nExplanation: The correct answer is C) because the question asks about the primary goal of the model, which is to reveal the trade-off between limiting disparities and stimulating economic flows. The model achieves this by showing that there is a relationship between restraining savings and using the wealthy surplus stock. The other options are incorrect because they either focus on specific components of the model (A and B) or describe a different aspect of the model's findings (D)."}, "40": {"documentation": {"title": "Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search", "source": "Xianglong Liu, Lei Huang, Cheng Deng, Bo Lang, Dacheng Tao", "docs_id": "1904.08623", "section": ["cs.IR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search. Hash based nearest neighbor search has become attractive in many applications. However, the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides, for large-scale visual search, existing hashing methods cannot directly support the efficient search over the data with multiple sources, and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems, this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table, a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect, multiple hash tables are built for different data views as a joint index, over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.11% and 20.28% performance gains on single and multiple table search over state-of-the-art methods."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed query-adaptive hash code ranking approach in the paper \"Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual Search\"?\n\n**A)** To alleviate the quantization loss in hashing methods by exploiting the quality of hash functions and their complement for nearest neighbor search.\n**B)** To build multiple hash tables with multiple views and generate fine-grained ranking results at bitwise and tablewise levels.\n**C)** To develop a novel and generic approach to support efficient search over large-scale visual data with multiple sources.\n**D)** To improve the performance of existing hashing methods by incorporating complementary information from diverse sources or views.\n\n**Correct Answer:** A) To alleviate the quantization loss in hashing methods by exploiting the quality of hash functions and their complement for nearest neighbor search.\n\n**Explanation:** The correct answer is A) because the paper aims to address the problem of quantization loss in hashing methods, which degrades the discriminative power when using Hamming distance ranking. The proposed query-adaptive bitwise weighting introduces a new approach to alleviate this loss by simultaneously exploiting the quality of hash functions and their complement. This is the primary goal of the paper, as stated in the introduction."}, "41": {"documentation": {"title": "The X-Ray Point-Source Population of NGC 1365: The Puzzle of Two\n  Highly-Variable Ultraluminous X-ray Sources", "source": "Iskra V. Strateva (1), Stefanie Komossa (1) ((1) MPE Garching,\n  Germany)", "docs_id": "0810.3793", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The X-Ray Point-Source Population of NGC 1365: The Puzzle of Two\n  Highly-Variable Ultraluminous X-ray Sources. We present 26 point-sources discovered with Chandra within 200\" (~20kpc) of the center of the barred supergiant galaxy NGC 1365. The majority of these sources are high-mass X-ray binaries, containing a neutron star or a black hole accreting from a luminous companion at a sub-Eddington rate. Using repeat Chandra and XMM-Newton as well as optical observations, we discuss in detail the natures of two highly-variable ultraluminous X-ray sources (ULXs): NGC 1365 X1, one of the most luminous ULXs known since the ROSAT era, which is X-ray variable by a factor of 30, and NGC 1365 X2, a newly discovered transient ULX, variable by a factor of >90. Their maximum X-ray luminosities (3-5 x 10^40 erg/s, measured with Chandra) and multiwavelength properties suggest the presence of more exotic objects and accretion modes: accretion onto intermediate mass black holes (IMBHs) and beamed/super-Eddington accretion onto solar-mass compact remnants. We argue that these two sources have black-hole masses higher than those of the typical primaries found in X-ray binaries in our Galaxy (which have masses of <20 Msolar), with a likely black-hole mass of 40-60 Msolar in the case of NGC 1365 X1 with a beamed/super-Eddington accretion mode, and a possible IMBH in the case of NGC 1365 X2 with M=80-500Msolar."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat is the most likely explanation for the highly-variable ultraluminous X-ray sources (ULXs) NGC 1365 X1 and NGC 1365 X2, considering their maximum X-ray luminosities and multiwavelength properties?\n\nA) Accretion onto stellar-mass black holes with masses less than 20 Msolar.\nB) Accretion onto intermediate-mass black holes (IMBHs) with masses between 20-60 Msolar.\nC) Accretion onto solar-mass compact remnants with beamed/super-Eddington accretion modes.\nD) Accretion onto supermassive black holes at the centers of galaxies.\n\nCorrect Answer: B) Accretion onto intermediate-mass black holes (IMBHs) with masses between 20-60 Msolar.\n\nExplanation: The text suggests that the highly-variable ULXs NGC 1365 X1 and NGC 1365 X2 have black-hole masses higher than those of typical primaries found in X-ray binaries in our Galaxy, with a likely black-hole mass of 40-60 Msolar in the case of NGC 1365 X1 and a possible IMBH with M=80-500Msolar in the case of NGC 1365 X2. This implies that the most likely explanation for these sources is accretion onto IMBHs, which are intermediate-mass black holes with masses between 20-60 Msolar."}, "42": {"documentation": {"title": "Mechanical characterization of disordered and anisotropic cellular\n  monolayers", "source": "Alexander Nestor-Bergmann, Emma Johns, Sarah Woolner, Oliver E. Jensen", "docs_id": "1711.02909", "section": ["q-bio.CB", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mechanical characterization of disordered and anisotropic cellular\n  monolayers. We consider a cellular monolayer, described using a vertex-based model, for which cells form a spatially disordered array of convex polygons that tile the plane. Equilibrium cell configurations are assumed to minimize a global energy defined in terms of cell areas and perimeters; energy is dissipated via dynamic area and length changes, as well as cell neighbour exchanges. The model captures our observations of an epithelium from a Xenopus embryo showing that uniaxial stretching induces spatial ordering, with cells under net tension (compression) tending to align with (against) the direction of stretch, but with the stress remaining heterogeneous at the single-cell level. We use the vertex model to derive the linearized relation between tissue-level stress, strain and strain-rate about a deformed base state, which can be used to characterize the tissue's anisotropic mechanical properties; expressions for viscoelastic tissue moduli are given as direct sums over cells. When the base state is isotropic, the model predicts that tissue properties can be tuned to a regime with high elastic shear resistance but low resistance to area changes, or vice versa."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the vertex-based model of cellular monolayers, what is the relationship between tissue-level stress, strain, and strain-rate about a deformed base state, as derived from the model?\n\nA) Stress is directly proportional to strain, with no dependence on strain-rate.\nB) Stress is directly proportional to strain, but inversely proportional to strain-rate.\nC) Stress is directly proportional to strain, but directly proportional to strain-rate.\nD) Stress is inversely proportional to strain, but directly proportional to strain-rate.\n\n**Correct Answer:** B) Stress is directly proportional to strain, but inversely proportional to strain-rate.\n\n**Explanation:** The vertex-based model predicts that tissue properties can be tuned to a regime with high elastic shear resistance but low resistance to area changes, or vice versa. This implies that the relationship between stress, strain, and strain-rate is not straightforward, but rather involves a complex interplay between these quantities. Specifically, the model suggests that stress is directly proportional to strain, but inversely proportional to strain-rate, which is a characteristic of viscoelastic materials. This relationship allows the tissue to exhibit both elastic and viscous behavior, depending on the conditions."}, "43": {"documentation": {"title": "Parity Violating Measurements of Neutron Densities", "source": "C. J. Horowitz, S. J. Pollock, P. A. Souder, R. Michaels", "docs_id": "nucl-th/9912038", "section": ["nucl-th", "hep-ph", "nucl-ex", "physics.atom-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parity Violating Measurements of Neutron Densities. Parity violating electron nucleus scattering is a clean and powerful tool for measuring the spatial distributions of neutrons in nuclei with unprecedented accuracy. Parity violation arises from the interference of electromagnetic and weak neutral amplitudes, and the $Z^0$ of the Standard Model couples primarily to neutrons at low $Q^2$. The data can be interpreted with as much confidence as electromagnetic scattering. After briefly reviewing the present theoretical and experimental knowledge of neutron densities, we discuss possible parity violation measurements, their theoretical interpretation, and applications. The experiments are feasible at existing facilities. We show that theoretical corrections are either small or well understood, which makes the interpretation clean. The quantitative relationship to atomic parity nonconservation observables is examined, and we show that the electron scattering asymmetries can be directly applied to atomic PNC because the observables have approximately the same dependence on nuclear shape."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary mechanism by which the $Z^0$ boson couples to neutrons in nuclei, and how does this relate to the interpretation of parity-violating electron nucleus scattering measurements?\n\nA) The $Z^0$ boson couples to neutrons through the weak nuclear force, which is the primary mechanism for parity violation in these measurements.\nB) The $Z^0$ boson couples to neutrons through the electromagnetic force, which is the primary mechanism for parity violation in these measurements.\nC) The $Z^0$ boson couples to neutrons through the strong nuclear force, which is not relevant to parity-violating electron nucleus scattering measurements.\nD) The $Z^0$ boson couples to neutrons through the Higgs field, which is not relevant to parity-violating electron nucleus scattering measurements.\n\nCorrect Answer: B) The $Z^0$ boson couples to neutrons through the electromagnetic force, which is the primary mechanism for parity violation in these measurements.\n\nExplanation: According to the provided documentation, parity-violating electron nucleus scattering is a clean and powerful tool for measuring the spatial distributions of neutrons in nuclei, and the $Z^0$ of the Standard Model couples primarily to neutrons at low $Q^2$. This implies that the primary mechanism by which the $Z^0$ boson couples to neutrons is through the electromagnetic force, which is the primary mechanism for parity violation in these measurements."}, "44": {"documentation": {"title": "Vacuum phenomenology of the chiral partner of the nucleon in a linear\n  sigma model with vector mesons", "source": "Susanna Gallas, Francesco Giacosa, and Dirk H. Rischke", "docs_id": "0907.5084", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vacuum phenomenology of the chiral partner of the nucleon in a linear\n  sigma model with vector mesons. We investigate a linear sigma model with global chiral $U(2)_{R} \\times U(2)_{L}$ symmetry. The mesonic degrees of freedom are the standard scalar and pseudoscalar mesons and the vector and axial-vector mesons. The baryonic degrees of freedom are the nucleon, $N$, and its chiral partner, $N^{*}$, which is usually identified with N(1535). The chiral partner is incorporated in the so-called mirror assignment, where the nucleon mass is not solely generated by the chiral condensate but also by a chirally invariant mass term, $m_{0}$. The presence of (axial-) vector fields modifies the expressions for the axial coupling constants of the nucleon, $g_{A}^{N}$, and its partner, $g_{A}^{N^{*}}$. Using experimental data for the decays $N^{*} \\to N \\pi$ and $a_{1} \\to\\pi\\gamma$, as well as lattice results for $g_{A}^{N^{*}}$ we infer $m_{0}\\sim500$ MeV, i.e., an appreciable amount of the nucleon mass originates from sources other than the chiral condensate. We test our model by evaluating the decay $N^{*} \\to N \\eta$ and the s-wave nucleon-pion scattering lengths $a_{0}^{(\\pm)}$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the linear sigma model with vector mesons, the chiral partner of the nucleon, $N^{*}$, is incorporated via the mirror assignment, where the nucleon mass is generated by both the chiral condensate and a chirally invariant mass term, $m_{0}$. Using experimental data for the decays $N^{*} \\to N \\pi$ and $a_{1} \\to\\pi\\gamma$, as well as lattice results for $g_{A}^{N^{*}}$, what is the inferred value of $m_{0}$ in units of MeV?\n\nA) 200\nB) 500\nC) 1000\nD) 2500\n\n**Correct Answer:** B) 500\n\n**Explanation:** The correct answer is inferred from the text, which states that \"We infer $m_{0}\\sim500$ MeV\". This value is obtained by combining experimental data and lattice results, and is a key result of the study. The other options are incorrect, as they do not match the value inferred from the data."}, "45": {"documentation": {"title": "Rewards and the evolution of cooperation in public good games", "source": "Tatsuya Sasaki and Satoshi Uchida", "docs_id": "1310.5520", "section": ["physics.soc-ph", "cs.GT", "nlin.AO", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rewards and the evolution of cooperation in public good games. Properly coordinating cooperation is relevant for resolving public good problems such as clean energy and environmental protection. However, little is known about how individuals can coordinate themselves for a certain level of cooperation in large populations of strangers. In a typical situation, a consensus-building process hardly succeeds due to lack of face and standing. The evolution of cooperation in this type of situation is studied using threshold public good games in which cooperation prevails when it is initially sufficient, or otherwise, it perishes. While punishment is a powerful tool to shape human behaviours, institutional punishment is often too costly to start with only a few contributors, which is another coordination problem. Here we show that whatever the initial conditions, reward funds based on voluntary contribution can evolve. The voluntary reward paves the way for effectively overcoming the coordination problem and efficiently transforms freeloaders to cooperators with a perceived small risk of collective failure."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of threshold public good games, what is the primary challenge in achieving cooperation among individuals in large populations of strangers, and how can voluntary reward funds help overcome this challenge?\n\n**A)** The primary challenge is the lack of institutional punishment, which can be too costly to implement in the early stages of cooperation. Voluntary reward funds can help overcome this challenge by providing a perceived small risk of collective failure, thereby transforming freeloaders to cooperators.\n\n**B)** The primary challenge is the lack of face and standing, which makes it difficult to establish a consensus-building process. Voluntary reward funds can help overcome this challenge by providing a sense of community and shared responsibility among contributors.\n\n**C)** The primary challenge is the difficulty in coordinating individual contributions, which can lead to a free-rider problem. Voluntary reward funds can help overcome this challenge by providing a mechanism for individuals to contribute voluntarily and receive rewards for their efforts.\n\n**D)** The primary challenge is the lack of clear incentives for cooperation, which can lead to a lack of motivation among individuals. Voluntary reward funds can help overcome this challenge by providing a clear and tangible incentive for individuals to contribute to the public good.\n\n**Correct Answer:** A) The primary challenge is the lack of institutional punishment, which can be too costly to implement in the early stages of cooperation. Voluntary reward funds can help overcome this challenge by providing a perceived small risk of collective failure, thereby transforming freeloaders to cooperators.\n\n**Explanation:** This question requires the test-taker to understand the challenges of achieving cooperation in large populations of strangers, as discussed in the Arxiv documentation. The correct answer, A, highlights the primary challenge of the lack of institutional punishment and how voluntary reward funds can help overcome this challenge by providing a perceived small risk of collective failure. The other options, while related to the topic, do not accurately capture the primary challenge and the role of voluntary reward funds in overcoming it."}, "46": {"documentation": {"title": "Inverse cubic law of index fluctuation distribution in Indian markets", "source": "Raj Kumar Pan and Sitabhra Sinha", "docs_id": "physics/0607014", "section": ["physics.soc-ph", "cond-mat.other", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inverse cubic law of index fluctuation distribution in Indian markets. One of the principal statistical features characterizing the activity in financial markets is the distribution of fluctuations in market indicators such as the index. While the developed stock markets, e.g., the New York Stock Exchange (NYSE) have been found to show heavy-tailed return distribution with a characteristic power-law exponent, the universality of such behavior has been debated, particularly in regard to emerging markets. Here we investigate the distribution of several indices from the Indian financial market, one of the largest emerging markets in the world. We have used tick-by-tick data from the National Stock Exchange (NSE), as well as, daily closing data from both NSE and Bombay Stock Exchange (BSE). We find that the cumulative distributions of index returns have long tails consistent with a power-law having exponent \\alpha \\approx 3, at time-scales of both 1 min and 1 day. This ``inverse cubic law'' is quantitatively similar to what has been observed in developed markets, thereby providing strong evidence of universality in the behavior of market fluctuations."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The study on the distribution of index returns in the Indian financial market found that the cumulative distributions of index returns have long tails consistent with a power-law having exponent \u03b1 \u2248 3. What can be inferred about the universality of market fluctuations in emerging markets, given that this result is quantitatively similar to what has been observed in developed markets?\n\n**A)** The study provides strong evidence that emerging markets exhibit the same level of market volatility as developed markets.\n**B)** The universality of market fluctuations in emerging markets is still a topic of debate, and further research is needed to confirm the findings.\n**C)** The study suggests that the power-law exponent \u03b1 \u2248 3 is a universal characteristic of financial markets, regardless of their level of development.\n**D)** The findings of the study are inconsistent with the idea of universality in market fluctuations, and instead suggest that emerging markets exhibit distinct characteristics.\n\n**Correct Answer:** C) The study suggests that the power-law exponent \u03b1 \u2248 3 is a universal characteristic of financial markets, regardless of their level of development.\n\n**Explanation:** The study found that the cumulative distributions of index returns in the Indian financial market have long tails consistent with a power-law having exponent \u03b1 \u2248 3, which is similar to what has been observed in developed markets. This suggests that the power-law exponent \u03b1 \u2248 3 is a universal characteristic of financial markets, regardless of their level of development. This inference is supported by the fact that the study found a similar result in both developed and emerging markets, which suggests that the underlying mechanisms driving market fluctuations are similar across different market types."}, "47": {"documentation": {"title": "A Multirate Variational Approach to Nonlinear MPC", "source": "Yana Lishkova and Mark Cannon and Sina Ober-Bl\\\"obaum", "docs_id": "2111.04811", "section": ["math.OC", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Multirate Variational Approach to Nonlinear MPC. A nonlinear model predictive control (NMPC) approach is proposed based on a variational representation of the system model and the receding horizon optimal control problem. The proposed tube-based convex MPC approach provides improvements in model accuracy and computational efficiency, and allows for alternative means of computing linearization error bounds. To this end we investigate the use of single rate and multirate system representations derived from a discrete variational principle to obtain structure-preserving time-stepping schemes. We show empirically that the desirable conservation properties of the discrete time model are inherited by the optimal control problem. Model linearization is achieved either by direct Jacobian Linearization or by quadratic and linear Taylor series approximations of the Lagrangian and generalized forces respectively. These two linearization schemes are proved to be equivalent for a specific choice of approximation points. Using the multirate variational formulation we derive a novel multirate NMPC approach, and show that it can provide large computational savings for systems with dynamics or control inputs evolving on different time scales."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the proposed multirate variational approach to nonlinear model predictive control (NMPC), what is the primary advantage of using a multirate system representation derived from a discrete variational principle, and how does it relate to the computational efficiency of the NMPC approach?\n\n**A)** The multirate representation allows for a more accurate model of the system dynamics, leading to improved control performance. However, it does not necessarily result in computational savings.\n\n**B)** The multirate representation enables the use of alternative linearization schemes, such as quadratic and linear Taylor series approximations, which can provide more accurate error bounds. This leads to improved computational efficiency, particularly for systems with dynamics or control inputs evolving on different time scales.\n\n**C)** The multirate representation provides a more efficient way to compute the optimal control problem, allowing for faster convergence of the solution. However, it does not necessarily lead to improved model accuracy or computational efficiency.\n\n**D)** The multirate representation is only applicable to systems with a specific type of dynamics or control inputs, and does not provide any general benefits for NMPC approaches.\n\n**Correct Answer:** B) The multirate representation enables the use of alternative linearization schemes, such as quadratic and linear Taylor series approximations, which can provide more accurate error bounds. This leads to improved computational efficiency, particularly for systems with dynamics or control inputs evolving on different time scales.\n\n**Explanation:** The correct answer is B) because the multirate representation allows for the use of alternative linearization schemes, which can provide more accurate error bounds and lead to improved computational efficiency. This is a key advantage of the multirate approach, as it enables the use of more efficient linearization schemes that can capture the dynamics of the system more accurately. The other options are incorrect because they do not accurately capture the primary advantage of the multirate representation. Option A is incorrect because the multirate representation does not necessarily lead to improved control performance. Option C is incorrect because the multirate representation does not necessarily lead to faster convergence of the solution. Option D is incorrect because the multirate representation is applicable to a wide range of systems, not just those with specific dynamics or control inputs."}, "48": {"documentation": {"title": "Molecular beam depletion: a new approach", "source": "Manuel Dorado", "docs_id": "1404.2625", "section": ["physics.atm-clus", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Molecular beam depletion: a new approach. During the last years some interesting experimental results have been reported for experiments in N20, N0 , N0 dimer , H2 , Toluene and BaFCH3 cluster. The main result consists in the observation of molecular beam depletion when the molecules of a pulsed beam interact with a static electric or magnetic field and an oscillating field (RF). In these cases, and as a main difference, instead of using four fields as in the original technique developed by I.I. Rabi and others, only two fields, those which configure the resonant unit, are used. That is, without using the nonhomogeneous magnetic fields. The depletion explanation for I.I. Rabi and others is based in the interaction between the molecular electric or magnetic dipole moment and the non-homogeneous fields. But, obviously, the change in the molecules trajectories observed on these new experiments has to be explained without considering the force provided by the field gradient because it happens without using non-homogeneous fields. In this paper a theoretical way for the explanation of these new experimental results is presented. One important point emerges as a result of this development, namely, the existence of an until now unknown, spin-dependent force, which would be responsible of the aforementioned deviation of the molecules"}, "answer": "Here is the difficult multi-form question:\n\nWhat is the underlying mechanism responsible for the molecular beam depletion observed in the new experimental approach, and how does it differ from the original technique developed by I.I. Rabi and others?\n\nA) The interaction between the molecular electric or magnetic dipole moment and the non-homogeneous fields is responsible for the depletion, as it causes a change in the molecules' trajectories.\n\nB) The existence of an until now unknown, spin-dependent force is responsible for the depletion, which arises from the interaction between the molecular electric or magnetic dipole moment and the oscillating field.\n\nC) The depletion is caused by the force provided by the field gradient, which is absent in the new experimental approach.\n\nD) The molecular beam depletion is a result of the interaction between the molecules and the static electric or magnetic field, which causes a change in the molecules' trajectories.\n\nCorrect Answer: B) The existence of an until now unknown, spin-dependent force is responsible for the depletion, which arises from the interaction between the molecular electric or magnetic dipole moment and the oscillating field.\n\nExplanation: The correct answer is B) because the new experimental approach uses only two fields, the resonant unit fields, and does not involve non-homogeneous magnetic fields. The depletion explanation presented in the paper is based on the existence of an until now unknown, spin-dependent force, which is responsible for the deviation of the molecules' trajectories. This force arises from the interaction between the molecular electric or magnetic dipole moment and the oscillating field, which is absent in the original technique developed by I.I. Rabi and others."}, "49": {"documentation": {"title": "Outcome-guided Sparse K-means for Disease Subtype Discovery via\n  Integrating Phenotypic Data with High-dimensional Transcriptomic Data", "source": "Lingsong Meng, Dorina Avram, George Tseng, Zhiguang Huo", "docs_id": "2103.09974", "section": ["q-bio.QM", "q-bio.GN", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Outcome-guided Sparse K-means for Disease Subtype Discovery via\n  Integrating Phenotypic Data with High-dimensional Transcriptomic Data. The discovery of disease subtypes is an essential step for developing precision medicine, and disease subtyping via omics data has become a popular approach. While promising, subtypes obtained from current approaches are not necessarily associated with clinical outcomes. With the rich clinical data along with the omics data in modern epidemiology cohorts, it is urgent to develop an outcome-guided clustering algorithm to fully integrate the phenotypic data with the high-dimensional omics data. Hence, we extended a sparse K-means method to an outcome-guided sparse K-means (GuidedSparseKmeans) method, which incorporated a phenotypic variable from the clinical dataset to guide gene selections from the high-dimensional omics data. We demonstrated the superior performance of the GuidedSparseKmeans by comparing with existing clustering methods in simulations and applications of high-dimensional transcriptomic data of breast cancer and Alzheimer's disease. Our algorithm has been implemented into an R package, which is publicly available on GitHub (https://github.com/LingsongMeng/GuidedSparseKmeans)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the Outcome-guided Sparse K-means (GuidedSparseKmeans) algorithm, and how does it differ from existing clustering methods in disease subtype discovery?\n\n**A)** The primary motivation is to improve the accuracy of gene selection from high-dimensional omics data, and GuidedSparseKmeans differs from existing methods in its use of phenotypic variables to guide gene selection.\n\n**B)** The primary motivation is to develop an algorithm that can handle high-dimensional transcriptomic data, and GuidedSparseKmeans differs from existing methods in its use of sparse K-means clustering.\n\n**C)** The primary motivation is to integrate phenotypic data with high-dimensional omics data to improve disease subtype discovery, and GuidedSparseKmeans differs from existing methods in its use of outcome-guided clustering.\n\n**D)** The primary motivation is to develop an algorithm that can handle large-scale clinical datasets, and GuidedSparseKmeans differs from existing methods in its use of R programming language implementation.\n\n**Correct Answer:** C) The primary motivation is to integrate phenotypic data with high-dimensional omics data to improve disease subtype discovery, and GuidedSparseKmeans differs from existing methods in its use of outcome-guided clustering.\n\n**Explanation:** The correct answer is C) because the question asks about the primary motivation behind the development of GuidedSparseKmeans, which is to integrate phenotypic data with high-dimensional omics data to improve disease subtype discovery. The correct answer also highlights the key difference between GuidedSparseKmeans and existing clustering methods, which is the use of outcome-guided clustering to guide gene selections from high-dimensional omics data."}, "50": {"documentation": {"title": "Multiferroic heterostructures for spin filter application - an ab initio\n  study", "source": "Stephan Borek, J\\\"urgen Braun, Hubert Ebert, and J\\'an Min\\'ar", "docs_id": "1507.06413", "section": ["physics.comp-ph", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiferroic heterostructures for spin filter application - an ab initio\n  study. Novel imaging spin-filter techniques, which are based on low energy electron diffraction, are currently of high scientific interest. To improve the spin-detection efficiency a variety of new materials have been introduced in recent years. A new class of promising spin-filter materials are represented by multiferroic systems, as both magnetic and electric ordering exist in these materials. We have investigated Fe/BaTiO3(001), which defines a prominent candidate due to its moderate spontaneous polarization, for spin filter applications calculating diffraction patterns for spin polarized electrons incident on the Fe surface. Motivated by the fact that spin polarized low energy electron diffraction is a powerful method for the determination of the properties of surfaces we investigated the influence of switching the BaTiO3 polarization on the exchange and spin orbit scattering as well as on reflectivity and figure of merit. This system obviously offers the possibility to realize a multiferroic spin filter and manipulating the spin-orbit and exchange scattering by an external electric field. The calculations have been done for a large range of kinetic energies and polar angles of the diffracted electrons considering different numbers of Fe monolayers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using multiferroic systems, such as Fe/BaTiO3(001), for spin filter applications, and how does the switching of BaTiO3 polarization influence the exchange and spin-orbit scattering in this system?\n\nA) Multiferroic systems offer the possibility to manipulate spin-orbit and exchange scattering by an external electric field, which is not the case for non-multiferroic systems.\nB) The moderate spontaneous polarization of Fe/BaTiO3(001) makes it an ideal candidate for spin filter applications due to its ability to control the exchange and spin-orbit scattering.\nC) The use of multiferroic systems allows for the realization of a multiferroic spin filter, which can be manipulated by an external electric field to control the spin-detection efficiency.\nD) The switching of BaTiO3 polarization influences the reflectivity and figure of merit of the system, but not the exchange and spin-orbit scattering.\n\nCorrect Answer: C) The use of multiferroic systems allows for the realization of a multiferroic spin filter, which can be manipulated by an external electric field to control the spin-detection efficiency.\n\nExplanation: The correct answer is C) because the documentation states that the system \"obviously offers the possibility to realize a multiferroic spin filter and manipulating the spin-orbit and exchange scattering by an external electric field.\" This indicates that the primary advantage of using multiferroic systems is the ability to manipulate the spin-orbit and exchange scattering, which is a key feature of multiferroic spin filters."}, "51": {"documentation": {"title": "Coexistence of vector soliton Kerr combs in normal dispersion resonators", "source": "B. Kostet, Y. Soupart, K. Panajotov, M. Tlidi", "docs_id": "2107.13959", "section": ["nlin.PS", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coexistence of vector soliton Kerr combs in normal dispersion resonators. We investigate the formation of dark vector localized structures in the presence of nonlinear polarization mode coupling in optical resonators subject to a coherent optical injection in the normal dispersion regime. This simple device is described by coupled Lugiato-Lefever equations. The stabilization of localized structures is attributed to a front locking mechanism. We show that in a multistable homogeneous steady-state regime, two branches of dark localized structures can coexist for a fixed value of the system parameters. These coexisting solutions possess different polarization states and different power peaks in the microresonator. We characterize in-depth their formation by drawing their bifurcation diagrams in regimes close to modulational instability and far from it. It is shown that both branches of localized structures exhibit a heteroclinic collapse snaking type of behavior. The coexistence of two vectorial branches of dark localized states is not possible without taking into account polarization degrees of freedom."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Coexistence of vector soliton Kerr combs in normal dispersion resonators, what is the primary mechanism responsible for the stabilization of localized structures in the presence of nonlinear polarization mode coupling?\n\nA) Front locking mechanism\nB) Heteroclinic collapse snaking type of behavior\nC) Polarization degree of freedom\nD) Modulational instability\n\n**Correct Answer:** A) Front locking mechanism\n\n**Explanation:** The correct answer is A) Front locking mechanism, as stated in the documentation: \"The stabilization of localized structures is attributed to a front locking mechanism.\" This mechanism is responsible for the formation and maintenance of dark vector localized structures in the presence of nonlinear polarization mode coupling.\n\n**Candidate B) Heteroclinic collapse snaking type of behavior:** This is a characteristic of the bifurcation diagrams of the localized structures, but it is not the primary mechanism responsible for their stabilization.\n\n**Candidate C) Polarization degree of freedom:** While polarization degrees of freedom are important in the context of the problem, they are not the primary mechanism responsible for the stabilization of localized structures.\n\n**Candidate D) Modulational instability:** Modulational instability is a regime close to which the bifurcation diagrams of the localized structures are drawn, but it is not the primary mechanism responsible for their stabilization."}, "52": {"documentation": {"title": "ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by\n  Attack Re-generation", "source": "Naser Damer, Kiran Raja, Marius S\\\"u{\\ss}milch, Sushma Venkatesh, Fadi\n  Boutros, Meiling Fang, Florian Kirchbuchner, Raghavendra Ramachandra, Arjan\n  Kuijper", "docs_id": "2108.09130", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by\n  Attack Re-generation. Face morphing attacks aim at creating face images that are verifiable to be the face of multiple identities, which can lead to building faulty identity links in operations like border checks. While creating a morphed face detector (MFD), training on all possible attack types is essential to achieve good detection performance. Therefore, investigating new methods of creating morphing attacks drives the generalizability of MADs. Creating morphing attacks was performed on the image level, by landmark interpolation, or on the latent-space level, by manipulating latent vectors in a generative adversarial network. The earlier results in varying blending artifacts and the latter results in synthetic-like striping artifacts. This work presents the novel morphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using a GAN-based generation, as well as, eliminate the manipulation in the latent space, resulting in visibly realistic morphed images compared to previous works. The generated ReGenMorph appearance is compared to recent morphing approaches and evaluated for face recognition vulnerability and attack detectability, whether as known or unknown attacks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the ReGenMorph pipeline, and how does it differ from previous morphing approaches in terms of artifacts and realism?\n\n**A)** ReGenMorph aims to create morphing attacks that are undetectable by face recognition systems, and it eliminates blending artifacts by manipulating latent vectors in a generative adversarial network. However, it results in synthetic-like striping artifacts.\n\n**B)** ReGenMorph is designed to create morphing attacks that are visually realistic and eliminate blending artifacts by using a GAN-based generation. It also eliminates manipulation in the latent space, resulting in more realistic morphed images compared to previous works.\n\n**C)** ReGenMorph is a morphing attack that aims to create face images that are verifiable to be the face of multiple identities, but it results in blending artifacts and synthetic-like striping artifacts due to its reliance on landmark interpolation.\n\n**D)** ReGenMorph is a morphing pipeline that aims to create morphing attacks that are undetectable by face recognition systems, but it eliminates blending artifacts by using a GAN-based generation, resulting in visibly realistic morphed images.\n\n**Correct Answer:** B) ReGenMorph is designed to create morphing attacks that are visually realistic and eliminate blending artifacts by using a GAN-based generation. It also eliminates manipulation in the latent space, resulting in more realistic morphed images compared to previous works.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the primary goal of ReGenMorph, which is to create visually realistic morphing attacks that eliminate blending artifacts and manipulation in the latent space. The other options are incorrect because they either misrepresent the goal of ReGenMorph or incorrectly describe the artifacts and realism of the approach."}, "53": {"documentation": {"title": "Incorporating Data Uncertainty in Object Tracking Algorithms", "source": "Anish Muthali, Forrest Laine, Claire Tomlin", "docs_id": "2109.10521", "section": ["eess.SY", "cs.CV", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Incorporating Data Uncertainty in Object Tracking Algorithms. Methodologies for incorporating the uncertainties characteristic of data-driven object detectors into object tracking algorithms are explored. Object tracking methods rely on measurement error models, typically in the form of measurement noise, false positive rates, and missed detection rates. Each of these quantities, in general, can be dependent on object or measurement location. However, for detections generated from neural-network processed camera inputs, these measurement error statistics are not sufficient to represent the primary source of errors, namely a dissimilarity between run-time sensor input and the training data upon which the detector was trained. To this end, we investigate incorporating data uncertainty into object tracking methods such as to improve the ability to track objects, and particularly those which out-of-distribution w.r.t. training data. The proposed methodologies are validated on an object tracking benchmark as well on experiments with a real autonomous aircraft."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary source of errors in object tracking algorithms that rely on neural-network processed camera inputs, and how do the proposed methodologies address this issue?\n\nA) The primary source of errors is the dissimilarity between run-time sensor input and the training data upon which the detector was trained, and the proposed methodologies address this issue by incorporating measurement error models.\n\nB) The primary source of errors is the dissimilarity between run-time sensor input and the training data upon which the detector was trained, and the proposed methodologies address this issue by using more advanced machine learning models.\n\nC) The primary source of errors is the dissimilarity between run-time sensor input and the training data upon which the detector was trained, and the proposed methodologies address this issue by using data augmentation techniques.\n\nD) The primary source of errors is the dissimilarity between run-time sensor input and the training data upon which the detector was trained, and the proposed methodologies address this issue by using more robust measurement error models.\n\n**Correct Answer:** D) The primary source of errors is the dissimilarity between run-time sensor input and the training data upon which the detector was trained, and the proposed methodologies address this issue by using more robust measurement error models.\n\n**Explanation:** The correct answer is D) because the documentation states that the primary source of errors in object tracking algorithms is the dissimilarity between run-time sensor input and the training data upon which the detector was trained. The proposed methodologies address this issue by incorporating data uncertainty into the object tracking methods, which is achieved by using more robust measurement error models. This is evident from the statement \"To this end, we investigate incorporating data uncertainty into object tracking methods such as to improve the ability to track objects, and particularly those which out-of-distribution w.r.t. training data.\""}, "54": {"documentation": {"title": "Electronic doping of graphene by deposited transition metal atoms", "source": "Jaime E. Santos, Nuno M. R. Peres, Joao M. B. Lopes dos Santos and\n  Antonio H. Castro Neto", "docs_id": "1104.4729", "section": ["cond-mat.str-el", "cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electronic doping of graphene by deposited transition metal atoms. We perform a phenomenological analysis of the problem of the electronic doping of a graphene sheet by deposited transition metal atoms, which aggregate in clusters. The sample is placed in a capacitor device such that the electronic doping of graphene can be varied by the application of a gate voltage and such that transport measurements can be performed via the application of a (much smaller) voltage along the graphene sample, as reported in the work of Pi et al. [Phys. Rev. B 80, 075406 (2009)]. The analysis allows us to explain the thermodynamic properties of the device, such as the level of doping of graphene and the ionisation potential of the metal clusters in terms of the chemical interaction between graphene and the clusters. We are also able, by modelling the metallic clusters as perfect conducting spheres, to determine the scattering potential due to these clusters on the electronic carriers of graphene and hence the contribution of these clusters to the resistivity of the sample. The model presented is able to explain the measurements performed by Pi et al. on Pt-covered graphene samples at the lowest metallic coverages measured and we also present a theoretical argument based on the above model that explains why significant deviations from such a theory are observed at higher levels of coverage."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the primary mechanism by which the electronic doping of graphene is achieved in the experiment described in the Arxiv documentation?\n\nA) The application of a gate voltage causes the metal clusters to break apart and redistribute, altering the doping level of graphene.\nB) The chemical interaction between graphene and the metal clusters leads to the transfer of electrons from the clusters to the graphene sheet, resulting in doping.\nC) The metal clusters act as perfect conducting spheres, scattering electronic carriers and contributing to the resistivity of the sample, but do not directly dope the graphene.\nD) The gate voltage is used to heat the metal clusters, causing them to evaporate and deposit onto the graphene sheet, thereby doping it.\n\nCorrect Answer: B) The chemical interaction between graphene and the metal clusters leads to the transfer of electrons from the clusters to the graphene sheet, resulting in doping.\n\nExplanation: The correct answer is based on the understanding that the experiment involves the deposition of transition metal atoms onto a graphene sheet, which aggregate into clusters. The analysis presented in the documentation explains the thermodynamic properties of the device, including the level of doping of graphene and the ionisation potential of the metal clusters, in terms of the chemical interaction between graphene and the clusters. This suggests that the doping of graphene is achieved through the transfer of electrons from the metal clusters to the graphene sheet, which is a consequence of the chemical interaction between the two materials."}, "55": {"documentation": {"title": "Exploring the expansion dynamics of the universe from galaxy cluster\n  surveys", "source": "Deng Wang, Xin-He Meng", "docs_id": "1709.04134", "section": ["astro-ph.CO", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring the expansion dynamics of the universe from galaxy cluster\n  surveys. To understand the expansion dynamics of the universe from galaxy cluster scales, using the angular diameter distance (ADD) data from two different galaxy cluster surveys, we constrain four cosmological models to explore the underlying value of $H_0$ and employ the model-independent Gaussian Processes to investigate the evolution of the equation of state of dark energy. The ADD data in the X-ray bands consists of two samples covering the redshift ranges [0.023, 0.784] and [0.14, 0.89], respectively. We find that: (i) For these two samples, the obtained values of $H_0$ are more consistent with the recent local observation by Riess et al. than the global measurement by the Plank Collaboration, and the $\\Lambda$CDM model is still preferred utilizing the information criterions; (ii) For the first sample, there is no evidence of dynamical dark energy (DDE) at the $2\\sigma$ confidence level (CL); (iii) For the second one, the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior in the relatively low redshift range over the $2\\sigma$ CL, which gives a hint that the late-time universe may be actually dominated by the DDE from galaxy cluster scales; (iv) By adding a combination of Type Ia Supernovae, cosmic chronometers and Planck-2015 shift parameter and HII galaxy measurements into both ADD samples, the DDE exists evidently over the $2\\sigma$ CL."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study \"Exploring the expansion dynamics of the universe from galaxy cluster surveys\" constrain four cosmological models to investigate the expansion dynamics of the universe. However, they also employ model-independent Gaussian Processes to investigate the evolution of the equation of state of dark energy. Which of the following statements accurately describes the findings of the study regarding the existence of dynamical dark energy (DDE) in the universe?\n\nA) The study finds no evidence of DDE in either of the two ADD samples, and the $\\Lambda$CDM model is still preferred.\nB) The study finds evidence of DDE in the first sample, but not in the second sample, and the $\\Lambda$CDM model is still preferred.\nC) The study finds evidence of DDE in both samples, but only at the $1\\sigma$ confidence level, and the $\\Lambda$CDM model is not preferred.\nD) The study finds evidence of DDE in both samples, and the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior, indicating that DDE may dominate the universe at late times.\n\n**Correct Answer:** D) The study finds evidence of DDE in both samples, and the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior, indicating that DDE may dominate the universe at late times.\n\n**Explanation:**\n\n* The study finds evidence of DDE in both samples, as stated in the original text: \"For the second one, the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior in the relatively low redshift range over the $2\\sigma$ CL, which gives a hint that the late-time universe may be actually dominated by the DDE from galaxy cluster scales.\"\n* The study also finds that the reconstructed equation of state of dark energy exhibits a phantom-crossing behavior, which is a characteristic of DDE. This behavior is observed in the second sample, and it suggests that DDE may dominate the universe at late times.\n* The other options are incorrect because:\n\t+ Option A is incorrect because the study finds evidence of DDE in both samples, not just the first sample.\n\t+ Option B is incorrect because the study finds evidence of DDE in both samples, not just the first sample.\n\t+ Option C is incorrect because the study finds evidence of DDE in both samples at the $2\\sigma$ confidence level, not just at the $1\\sigma$ confidence level."}, "56": {"documentation": {"title": "Stochastic relaxational dynamics applied to finance: towards\n  non-equilibrium option pricing theory", "source": "Matthias Otto (Institute of Theoretical Physics, University of\n  Goettingen, Germany)", "docs_id": "cond-mat/9906196", "section": ["cond-mat.stat-mech", "q-fin.CP", "q-fin.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stochastic relaxational dynamics applied to finance: towards\n  non-equilibrium option pricing theory. Non-equilibrium phenomena occur not only in physical world, but also in finance. In this work, stochastic relaxational dynamics (together with path integrals) is applied to option pricing theory. A recently proposed model (by Ilinski et al.) considers fluctuations around this equilibrium state by introducing a relaxational dynamics with random noise for intermediate deviations called ``virtual'' arbitrage returns. In this work, the model is incorporated within a martingale pricing method for derivatives on securities (e.g. stocks) in incomplete markets using a mapping to option pricing theory with stochastic interest rates. Using a famous result by Merton and with some help from the path integral method, exact pricing formulas for European call and put options under the influence of virtual arbitrage returns (or intermediate deviations from economic equilibrium) are derived where only the final integration over initial arbitrage returns needs to be performed numerically. This result is complemented by a discussion of the hedging strategy associated to a derivative, which replicates the final payoff but turns out to be not self-financing in the real world, but self-financing {\\it when summed over the derivative's remaining life time}. Numerical examples are given which underline the fact that an additional positive risk premium (with respect to the Black-Scholes values) is found reflecting extra hedging costs due to intermediate deviations from economic equilibrium."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of stochastic relaxational dynamics applied to finance, what is the key insight provided by the model of Ilinski et al. regarding the incorporation of virtual arbitrage returns into option pricing theory?\n\nA) The model assumes that virtual arbitrage returns are independent of the underlying asset prices.\nB) The model introduces a relaxational dynamics with random noise to capture fluctuations around the equilibrium state, allowing for intermediate deviations from economic equilibrium.\nC) The model uses a martingale pricing method to derive exact pricing formulas for European call and put options under the influence of virtual arbitrage returns.\nD) The model assumes that the virtual arbitrage returns are deterministic and can be precisely predicted.\n\n**Correct Answer:** B) The model introduces a relaxational dynamics with random noise to capture fluctuations around the equilibrium state, allowing for intermediate deviations from economic equilibrium.\n\n**Explanation:** The correct answer is B) because the model of Ilinski et al. introduces a relaxational dynamics with random noise to capture fluctuations around the equilibrium state, which allows for intermediate deviations from economic equilibrium. This is a key insight in the paper, as it enables the derivation of exact pricing formulas for European call and put options under the influence of virtual arbitrage returns.\n\nThe other options are incorrect because:\n\nA) The model does not assume that virtual arbitrage returns are independent of the underlying asset prices.\n\nC) While the model does use a martingale pricing method, this is not the key insight regarding the incorporation of virtual arbitrage returns.\n\nD) The model does not assume that the virtual arbitrage returns are deterministic and can be precisely predicted."}, "57": {"documentation": {"title": "On the quantum stability of Q-balls", "source": "Anders Tranberg, David J. Weir", "docs_id": "1310.7487", "section": ["hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the quantum stability of Q-balls. We consider the evolution and decay of Q-balls under the influence of quantum fluctuations. We argue that the most important effect resulting from these fluctuations is the modification of the effective potential in which the Q-ball evolves. This is in addition to spontaneous decay into elementary particle excitations and fission into smaller Q-balls previously considered in the literature, which -- like most tunnelling processes -- are likely to be strongly suppressed. We illustrate the effect of quantum fluctuations in a particular model $\\phi^6$ potential, for which we implement the inhomogeneous Hartree approximation to quantum dynamics and solve for the evolution of Q-balls in 3+1 dimensions. We find that the stability range as a function of (field space) angular velocity $\\omega$ is modified significantly compared to the classical case, so that small-$\\omega$ Q-balls are less stable than in the classical limit, and large-$\\omega$ Q-balls are more stable. This can be understood qualitatively in a simple way."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the evolution and decay of Q-balls under the influence of quantum fluctuations, as described in the paper \"On the quantum stability of Q-balls\". In the context of the $\\phi^6$ potential, what is the qualitative effect of quantum fluctuations on the stability range of Q-balls as a function of (field space) angular velocity $\\omega$?\n\nA) The stability range is unaffected by quantum fluctuations, and Q-balls remain stable for all values of $\\omega$.\nB) The stability range is increased for small-$\\omega$ Q-balls and decreased for large-$\\omega$ Q-balls due to quantum fluctuations.\nC) The stability range is modified significantly, with small-$\\omega$ Q-balls becoming more stable and large-$\\omega$ Q-balls becoming less stable due to quantum fluctuations.\nD) The stability range is modified significantly, with small-$\\omega$ Q-balls becoming less stable and large-$\\omega$ Q-balls becoming more stable due to quantum fluctuations.\n\nCorrect Answer: C) The stability range is modified significantly, with small-$\\omega$ Q-balls becoming more stable and large-$\\omega$ Q-balls becoming less stable due to quantum fluctuations.\n\nExplanation: The correct answer is C) because the paper states that quantum fluctuations modify the effective potential, leading to a significant change in the stability range of Q-balls as a function of $\\omega$. Specifically, it is argued that small-$\\omega$ Q-balls are less stable than in the classical limit, while large-$\\omega$ Q-balls are more stable. This is a qualitative understanding of the effect of quantum fluctuations on the stability range of Q-balls."}, "58": {"documentation": {"title": "Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization", "source": "Megumi Nakao, Mitsuhiro Nakamura, Takashi Mizowaki, Tetsuya Matsuda", "docs_id": "1911.05439", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization. Respiratory motion and the associated deformations of abdominal organs and tumors are essential information in clinical applications. However, inter- and intra-patient multi-organ deformations are complex and have not been statistically formulated, whereas single organ deformations have been widely studied. In this paper, we introduce a multi-organ deformation library and its application to deformation reconstruction based on the shape features of multiple abdominal organs. Statistical multi-organ motion/deformation models of the stomach, liver, left and right kidneys, and duodenum were generated by shape matching their region labels defined on four-dimensional computed tomography images. A total of 250 volumes were measured from 25 pancreatic cancer patients. This paper also proposes a per-region-based deformation learning using the reproducing kernel to predict the displacement of pancreatic cancer for adaptive radiotherapy. The experimental results show that the proposed concept estimates deformations better than general per-patient-based learning models and achieves a clinically acceptable estimation error with a mean distance of 1.2 $\\pm$ 0.7 mm and a Hausdorff distance of 4.2 $\\pm$ 2.3 mm throughout the respiratory motion."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the proposed multi-organ deformation library in the paper \"Statistical Deformation Reconstruction Using Multi-organ Shape Features for Pancreatic Cancer Localization\"?\n\nA) To generate statistical models of single organ deformations\nB) To develop a per-region-based deformation learning model for adaptive radiotherapy\nC) To estimate the displacement of pancreatic cancer for adaptive radiotherapy using a per-patient-based learning model\nD) To create a multi-organ deformation library for statistical deformation reconstruction\n\n**Correct Answer:** D) To create a multi-organ deformation library for statistical deformation reconstruction\n\n**Explanation:** The correct answer is D) because the paper introduces a multi-organ deformation library to statistically formulate inter- and intra-patient multi-organ deformations, which is the primary goal of the study. The library is used to reconstruct deformations based on the shape features of multiple abdominal organs, as mentioned in the abstract.\n\n**Candidate A (Incorrect)**: This option is incorrect because the paper focuses on multi-organ deformations, not single organ deformations.\n\n**Candidate B (Partially Correct)**: This option is partially correct because the paper does propose a per-region-based deformation learning model for adaptive radiotherapy, but this is not the primary goal of the study.\n\n**Candidate C (Incorrect)**: This option is incorrect because the paper uses a per-region-based deformation learning model, but the primary goal is to create a multi-organ deformation library, not to estimate pancreatic cancer displacement using a per-patient-based learning model."}, "59": {"documentation": {"title": "Finite-size scaling in globally coupled phase oscillators with a general\n  coupling scheme", "source": "Isao Nishikawa, Koji Iwayama, Gouhei Tanaka, Takehiko Horita, and\n  Kazuyuki Aihara", "docs_id": "1211.4364", "section": ["cond-mat.stat-mech", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finite-size scaling in globally coupled phase oscillators with a general\n  coupling scheme. We investigate a critical exponent related to synchronization transition in globally coupled nonidentical phase oscillators. The critical exponents of susceptibility, correlation time, and correlation size are significant quantities to characterize fluctuations in coupled oscillator systems of large but finite size and understand a universal property of synchronization. These exponents have been identified for the sinusoidal coupling but not fully studied for other coupling schemes. Herein, for a general coupling function including a negative second harmonic term in addition to the sinusoidal term, we numerically estimate the critical exponent of the correlation size, denoted by $\\nu_+$, in a synchronized regime of the system by employing a non-conventional statistical quantity. First, we confirm that the estimated value of $\\nu_+$ is approximately 5/2 for the sinusoidal coupling case, which is consistent with the well-known theoretical result. Second, we show that the value of $\\nu_+$ increases with an increase in the strength of the second harmonic term. Our result implies that the critical exponent characterizing synchronization transition largely depends on the coupling function."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of finite-size scaling in globally coupled phase oscillators, what is the relationship between the strength of the second harmonic term in the coupling function and the critical exponent of the correlation size, denoted by $\\nu_+$?\n\nA) An increase in the strength of the second harmonic term leads to a decrease in the value of $\\nu_+$.\nB) The value of $\\nu_+$ remains constant regardless of the strength of the second harmonic term.\nC) An increase in the strength of the second harmonic term leads to an increase in the value of $\\nu_+$.\nD) The value of $\\nu_+$ is inversely proportional to the strength of the second harmonic term.\n\n**Correct Answer:** C) An increase in the strength of the second harmonic term leads to an increase in the value of $\\nu_+$.\n\n**Explanation:** According to the documentation, the value of $\\nu_+$ increases with an increase in the strength of the second harmonic term. This implies that the critical exponent characterizing synchronization transition largely depends on the coupling function, and the strength of the second harmonic term plays a significant role in determining the value of $\\nu_+$."}}