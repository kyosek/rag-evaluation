{"0": {"documentation": {"title": "Enabling Scientific Crowds: The Theory of Enablers for Crowd-Based\n  Scientific Investigation", "source": "Jorge Faleiro", "docs_id": "1809.07195", "section": ["q-fin.CP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enabling Scientific Crowds: The Theory of Enablers for Crowd-Based\n  Scientific Investigation. Evidence shows that in a significant number of cases the current methods of research do not allow for reproducible and falsifiable procedures of scientific investigation. As a consequence, the majority of critical decisions at all levels, from personal investment choices to overreaching global policies, rely on some variation of try-and-error and are mostly non-scientific by definition. We lack transparency for procedures and evidence, proper explanation of market events, predictability on effects, or identification of causes. There is no clear demarcation of what is inherently scientific, and as a consequence, the line between fake and genuine is blurred. This paper presents highlights of the Theory of Enablers for Crowd-Based Scientific Investigation, or Theory of Enablers for short. The Theory of Enablers assumes the use of a next-generation investigative approach leveraging forces of human diversity, micro-specialized crowds, and proper computer-assisted control methods associated with accessibility, reproducibility, communication, and collaboration. This paper defines the set of very specific cognitive and non-cognitive enablers for crowd-based scientific investigation: methods of proof, large-scale collaboration, and a domain-specific computational representation. These enablers allow the application of procedures of structured scientific investigation powered by crowds, a collective brain in which neurons are human collaborators"}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary goal of the Theory of Enablers for Crowd-Based Scientific Investigation, as described in the paper?\n\nA) To create a new paradigm for scientific research that relies solely on human intuition and expertise.\nB) To develop a framework for harnessing the collective intelligence of crowds to facilitate reproducible and falsifiable scientific investigation.\nC) To design a system for automating the scientific investigation process using artificial intelligence and machine learning algorithms.\nD) To establish a new standard for scientific methodology that prioritizes individual expertise over collective collaboration.\n\n**Correct Answer:** B) To develop a framework for harnessing the collective intelligence of crowds to facilitate reproducible and falsifiable scientific investigation.\n\n**Explanation:** The Theory of Enablers aims to leverage the forces of human diversity, micro-specialized crowds, and computer-assisted control methods to create a new approach to scientific investigation that is more transparent, reproducible, and collaborative. This approach recognizes the limitations of current scientific methods and seeks to harness the collective intelligence of crowds to facilitate structured scientific investigation. The correct answer, B, accurately reflects this goal. The other options are incorrect because they do not accurately capture the primary objective of the Theory of Enablers. Option A is incorrect because it suggests a reliance on human intuition and expertise, which is not the primary goal of the Theory. Option C is incorrect because it implies a focus on automation, whereas the Theory emphasizes the importance of human collaboration. Option D is incorrect because it suggests a prioritization of individual expertise over collective collaboration, which is not in line with the Theory's emphasis on harnessing collective intelligence."}, "1": {"documentation": {"title": "Evolution of X-ray and FUV Disk-Dispersing Radiation Fields", "source": "Laura Ingleby, Nuria Calvet, Jesus Hernandez, Cesar Briceno, Catherine\n  Espaillat, Jon Miller, Edwin Bergin, Lee Hartmann", "docs_id": "1102.0956", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolution of X-ray and FUV Disk-Dispersing Radiation Fields. We present new X-ray and Far Ultraviolet (FUV) observations of T Tauri stars covering the age range 1 to 10 Myr. Our goals are to observationally constrain the intensity of radiation fields responsible for evaporating gas from the circumstellar disk and to assess the feasibility of current photoevaporation models, focusing on X-ray and UV radiation. We greatly increase the number of 7-10 Myr old T Tauri stars observed in the X-rays by including observations of the well populated 25 Ori aggregate in the Orion OB1a subassociation. With these new 7-10 Myr objects, we confirm that X-ray emission remains constant from 1-10 Myr. We also show, for the first time, observational evidence for the evolution of FUV radiation fields with a sample of 56 accreting and non-accreting young stars spanning 1 Myr to 1 Gyr. We find that the FUV emission decreases on timescales consistent with the decline of accretion in classical T Tauri stars until reaching the chromospheric level in weak T Tauri stars and debris disks. Overall, we find that the observed strength of high energy radiation is consistent with that required by photoevaporation models to dissipate the disks in timescales of approximately 10 Myr. Finally, we find that the high energy fields that affect gas evolution are not similarly affecting dust evolution; in particular, we find that disks with inner clearings, the transitional disks, have similar levels of FUV emission as full disks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion regarding the evolution of X-ray and FUV radiation fields in T Tauri stars, as inferred from the observations presented in the study?\n\nA) The X-ray emission increases with age, indicating a stronger radiation field.\nB) The FUV emission decreases with age, consistent with the decline of accretion in classical T Tauri stars.\nC) The high energy radiation fields are responsible for dissipating the disks in timescales of approximately 10 Myr.\nD) The FUV emission remains constant from 1-10 Myr, with no significant evolution.\n\nCorrect Answer: D) The FUV emission remains constant from 1-10 Myr, with no significant evolution.\n\nExplanation: The study presents new observations of T Tauri stars covering the age range 1 to 10 Myr, and finds that the FUV emission remains constant from 1-10 Myr, with no significant evolution. This suggests that the radiation fields responsible for evaporating gas from the circumstellar disk do not change significantly over this age range."}, "2": {"documentation": {"title": "Pretext Tasks selection for multitask self-supervised speech\n  representation learning", "source": "Salah Zaiem, Titouan Parcollet and Slim Essid", "docs_id": "2107.00594", "section": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pretext Tasks selection for multitask self-supervised speech\n  representation learning. Through solving pretext tasks, self-supervised learning leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. In audio/speech signal processing, a wide range of features where engineered through decades of research efforts. As it turns out, learning to predict such features (a.k.a pseudo-labels) has proven to be a particularly relevant pretext task, leading to useful self-supervised representations which prove to be effective for downstream tasks. However, methods and common practices for combining such pretext tasks for better performance on the downstream task have not been explored and understood properly. In fact, the process relies almost exclusively on a computationally heavy experimental procedure, which becomes intractable with the increase of the number of pretext tasks. This paper introduces a method to select a group of pretext tasks among a set of candidates. The method we propose estimates calibrated weights for the partial losses corresponding to the considered pretext tasks during the self-supervised training process. The experiments conducted on automatic speech recognition, speaker and emotion recognition validate our approach, as the groups selected and weighted with our method perform better than classic baselines, thus facilitating the selection and combination of relevant pseudo-labels for self-supervised representation learning."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of traditional pretext task selection methods for self-supervised speech representation learning, and how does the proposed method address this limitation?\n\nA) Traditional methods rely on a single pretext task, whereas the proposed method allows for the selection of multiple tasks, but does not address the computational complexity of the experimental procedure.\nB) The proposed method estimates calibrated weights for partial losses, which enables the selection of a group of pretext tasks that can be combined to improve downstream task performance, but does not address the issue of feature engineering.\nC) The proposed method introduces a new experimental procedure that is computationally heavy and intractable with the increase of the number of pretext tasks, which is the primary limitation of traditional methods.\nD) The proposed method relies on a computationally heavy experimental procedure, but does not address the issue of feature engineering, which is a critical aspect of self-supervised representation learning.\n\nCorrect Answer: D) The proposed method relies on a computationally heavy experimental procedure, but does not address the issue of feature engineering, which is a critical aspect of self-supervised representation learning.\n\nExplanation: The correct answer is D) because the proposed method does not address the issue of feature engineering, which is a critical aspect of self-supervised representation learning. The paper states that \"a wide range of features where engineered through decades of research efforts\" and that \"learning to predict such features (a.k.a pseudo-labels) has proven to be a particularly relevant pretext task\", indicating that feature engineering is a crucial aspect of self-supervised representation learning. The proposed method only addresses the issue of selecting and weighting pretext tasks, but does not address the issue of feature engineering."}, "3": {"documentation": {"title": "Statistically Discriminative Sub-trajectory Mining", "source": "Vo Nguyen Le Duy, Takuto Sakuma, Taiju Ishiyama, Hiroki Toda, Kazuya\n  Nishi, Masayuki Karasuyama, Yuta Okubo, Masayuki Sunaga, Yasuo Tabei, Ichiro\n  Takeuchi", "docs_id": "1905.01788", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistically Discriminative Sub-trajectory Mining. We study the problem of discriminative sub-trajectory mining. Given two groups of trajectories, the goal of this problem is to extract moving patterns in the form of sub-trajectories which are more similar to sub-trajectories of one group and less similar to those of the other. We propose a new method called Statistically Discriminative Sub-trajectory Mining (SDSM) for this problem. An advantage of the SDSM method is that the statistical significance of the extracted sub-trajectories are properly controlled in the sense that the probability of finding a false positive sub-trajectory is smaller than a specified significance threshold alpha (e.g., 0.05), which is indispensable when the method is used in scientific or social studies under noisy environment. Finding such statistically discriminative sub-trajectories from massive trajectory dataset is both computationally and statistically challenging. In the SDSM method, we resolve the difficulties by introducing a tree representation among sub-trajectories and running an efficient permutation-based statistical inference method on the tree. To the best of our knowledge, SDSM is the first method that can efficiently extract statistically discriminative sub-trajectories from massive trajectory dataset. We illustrate the effectiveness and scalability of the SDSM method by applying it to a real-world dataset with 1,000,000 trajectories which contains 16,723,602,505 sub-trajectories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Statistically Discriminative Sub-trajectory Mining (SDSM) method in extracting moving patterns from massive trajectory datasets?\n\n**A)** It can handle large datasets with high computational efficiency.\n**B)** It can extract sub-trajectories with high statistical significance, controlling for false positives.\n**C)** It uses a tree representation among sub-trajectories to improve scalability.\n**D)** It relies solely on machine learning algorithms to identify patterns.\n\n**Correct Answer:** B) It can extract sub-trajectories with high statistical significance, controlling for false positives.\n\n**Explanation:** The SDSM method is designed to extract moving patterns from massive trajectory datasets while controlling for false positives. This is achieved through the use of a permutation-based statistical inference method on the tree representation of sub-trajectories. By doing so, the method ensures that the extracted sub-trajectories have a statistically significant difference between groups, making it a valuable tool for scientific and social studies under noisy environments."}, "4": {"documentation": {"title": "Active phases and flickering of a symbiotic recurrent nova T CrB", "source": "Krystian Ilkiewicz, Joanna Mikolajewska, Kiril Stoyanov, Antonios\n  Manousakis and Brent Miszalski", "docs_id": "1607.06804", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active phases and flickering of a symbiotic recurrent nova T CrB. T CrB is a symbiotic recurrent nova known to exhibit active phases, characterised by apparent increases in the hot component temperature and the appearance of flickering, i.e. changes in the observed flux on the time-scale of minutes. Historical UV observations have ruled out orbital variability as an explanation for flickering and instead suggest flickering is caused by variable mass transfer. We have analysed optical and X-ray observations to investigate the nature of the flickering as well as the active phases in T CrB. The spectroscopic and photometric observations confirm that the active phases follow two periods of ~1000d and ~5000d. Flickering in the X-rays is detected and follows an amplitude-flux relationship similar to that observed in the optical. The flickering is most prominent at harder X-ray energies, suggesting that it originates in the boundary layer between the accretion disc and the white dwarf. The X-ray radiation from the boundary layer is then reprocessed by a thick accretion disc or a nebula into UV radiation. A more detailed understanding of flickering would benefit from long-term simultaneous X-ray and optical monitoring of the phenomena in symbiotic recurrent novae and related systems such as Z And type symbiotic stars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most likely explanation for the flickering observed in the X-rays of T CrB, and how does it relate to the boundary layer between the accretion disc and the white dwarf?\n\nA) The flickering is caused by changes in the mass transfer rate from the companion star to the white dwarf, resulting in variations in the accretion disc's opacity.\n\nB) The flickering is caused by the reprocessing of X-ray radiation from the boundary layer by a thick accretion disc or a nebula, resulting in UV radiation that is then observed in the optical.\n\nC) The flickering is caused by the orbital variability of the system, resulting in changes in the observed flux on the time-scale of minutes.\n\nD) The flickering is caused by the thermal instability of the accretion disc, resulting in variations in the disc's temperature and emissivity.\n\nCorrect Answer: B) The flickering is caused by the reprocessing of X-ray radiation from the boundary layer by a thick accretion disc or a nebula, resulting in UV radiation that is then observed in the optical.\n\nExplanation: The correct answer is based on the text, which states that the flickering is most prominent at harder X-ray energies, suggesting that it originates in the boundary layer between the accretion disc and the white dwarf. The text also mentions that the X-ray radiation from the boundary layer is then reprocessed by a thick accretion disc or a nebula into UV radiation, which is observed in the optical. This supports the idea that the flickering is caused by the reprocessing of X-ray radiation by the accretion disc or nebula."}, "5": {"documentation": {"title": "Generalization of the Einstein-Plank-Richardson law for the photon\n  energy in medium resolves Abraham-Minkowski dilemma in the electromagnetic\n  field theory statement", "source": "Sergey G. Chefranov", "docs_id": "1202.0739", "section": ["physics.gen-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalization of the Einstein-Plank-Richardson law for the photon\n  energy in medium resolves Abraham-Minkowski dilemma in the electromagnetic\n  field theory statement. On the base of the Hamilton theory for the time-like photon in isotropic dielectric with refraction index n (S.Antoci, et.al, 2007), we suggest generalization of the Einstein-Plank-Richardson law for the value of the light energy quantum in medium:E=h*nu*n, where h is the Plank's constant, and nu is the light frequency. By use of this new quantum law, we resolve the famous contradiction between de Broglie and Einstein's theories, related with the old Abraham-Minkowski dilemma in the definition of the photon momentum value p (in the medium for n>1). We show that the same value p=p_a=E/(c*n)(c is the speed of light in vacuum) follows now from the both theories of de Broglie and Einstein, which complies with the theory of Abraham, but not with the theory of Minkowski (where p=p_m=E*n/c). Based on the corpuscular approach with p=p_a and E=h*nu*n^2, we give new inference for the Snellius refraction law and resolve more old corresponding corpuscular-wave Newton-Huygens dilemma. We show that even for n-1<<1 the Abraham and Minkowski theories (with different p=p_a and p=p_m)may lead to very different conclusions. Thus, only the theory with p=p_a, contrary to the theory with p=p_m, allows the conclusion about the Vavilov-Cherenkov radiation realization in the photon gas of the background cosmic radiation, for which n-1=10^(-42) in the current epoch."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Einstein-Plank-Richardson law for the photon energy in a medium is given by E = h*nu*n, where h is the Plank's constant, nu is the light frequency, and n is the refraction index of the medium. According to the theory, the photon momentum p is given by p = E/(c*n), where c is the speed of light in vacuum. However, this contradicts the theory of Minkowski, which gives p = E*n/c. Which of the following statements is true?\n\nA) The Abraham-Minkowski dilemma is resolved by the Einstein-Plank-Richardson law, but only for n > 1.\nB) The Einstein-Plank-Richardson law resolves the Abraham-Minkowski dilemma for all values of n, including n < 1.\nC) The Einstein-Plank-Richardson law resolves the Abraham-Minkowski dilemma, but only for the specific case of n = 1.\nD) The Einstein-Plank-Richardson law does not resolve the Abraham-Minkowski dilemma, and the two theories remain incompatible.\n\n**Correct Answer:** B) The Einstein-Plank-Richardson law resolves the Abraham-Minkowski dilemma for all values of n, including n < 1.\n\n**Explanation:** The Einstein-Plank-Richardson law resolves the Abraham-Minkowski dilemma for all values of n, including n < 1, because it gives the correct value of the photon momentum p = E/(c*n) for all n. This is in contrast to the theory of Minkowski, which gives p = E*n/c, which is only true for n > 1. Therefore, option B is the correct answer."}, "6": {"documentation": {"title": "General spin and pseudospin symmetries of the Dirac equation", "source": "P. Alberto, M. Malheiro, T. Frederico and A. de Castro", "docs_id": "1512.02552", "section": ["quant-ph", "cond-mat.str-el", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "General spin and pseudospin symmetries of the Dirac equation. In the 70's Smith and Tassie, and Bell and Ruegg independently found SU(2) symmetries of the Dirac equation with scalar and vector potentials. These symmetries, known as pseudospin and spin symmetries, have been extensively researched and applied to several physical systems. Twenty years after, in 1997, the pseudospin symmetry has been revealed by Ginocchio as a relativistic symmetry of the atomic nuclei when it is described by relativistic mean field hadronic models. The main feature of these symmetries is the suppression of the spin-orbit coupling either in the upper or lower components of the Dirac spinor, thereby turning the respective second-order equations into Schr\\\"odinger-like equations, i.e, without a matrix structure. In this paper we propose a generalization of these SU(2) symmetries for potentials in the Dirac equation with several Lorentz structures, which also allow for the suppression of the matrix structure of second-order equation equation of either the upper or lower components of the Dirac spinor. We derive the general properties of those potentials and list some possible candidates, which include the usual spin-pseudospin potentials, and also 2- and 1-dimensional potentials. An application for a particular physical system in two dimensions, electrons in graphene, is suggested."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main feature of the pseudospin symmetry in the Dirac equation, and how does it relate to the suppression of the spin-orbit coupling in the Dirac spinor?\n\nA) The pseudospin symmetry is a non-relativistic symmetry that leads to the suppression of the spin-orbit coupling in the lower components of the Dirac spinor.\n\nB) The pseudospin symmetry is a relativistic symmetry that suppresses the spin-orbit coupling in the upper components of the Dirac spinor, turning second-order equations into Schr\u00f6dinger-like equations.\n\nC) The pseudospin symmetry is a non-relativistic symmetry that leads to the suppression of the spin-orbit coupling in the upper components of the Dirac spinor, but only for 2-dimensional potentials.\n\nD) The pseudospin symmetry is a non-relativistic symmetry that suppresses the spin-orbit coupling in the lower components of the Dirac spinor, but only for 1-dimensional potentials.\n\n**Correct Answer:** B) The pseudospin symmetry is a relativistic symmetry that suppresses the spin-orbit coupling in the upper components of the Dirac spinor, turning second-order equations into Schr\u00f6dinger-like equations.\n\n**Explanation:** The pseudospin symmetry is a relativistic symmetry that arises in the Dirac equation with scalar and vector potentials. Its main feature is the suppression of the spin-orbit coupling in the upper components of the Dirac spinor, which leads to the transformation of second-order equations into Schr\u00f6dinger-like equations without a matrix structure. This is a key aspect of the pseudospin symmetry, and it has been extensively researched and applied to various physical systems."}, "7": {"documentation": {"title": "Multiview Based 3D Scene Understanding On Partial Point Sets", "source": "Ye Zhu and Sven Ewan Shepstone and Pablo Mart\\'inez-Nuevo and Miklas\n  Str{\\o}m Kristoffersen and Fabien Moutarde and Zhuang Fu", "docs_id": "1812.01712", "section": ["cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiview Based 3D Scene Understanding On Partial Point Sets. Deep learning within the context of point clouds has gained much research interest in recent years mostly due to the promising results that have been achieved on a number of challenging benchmarks, such as 3D shape recognition and scene semantic segmentation. In many realistic settings however, snapshots of the environment are often taken from a single view, which only contains a partial set of the scene due to the field of view restriction of commodity cameras. 3D scene semantic understanding on partial point clouds is considered as a challenging task. In this work, we propose a processing approach for 3D point cloud data based on a multiview representation of the existing 360{\\deg} point clouds. By fusing the original 360{\\deg} point clouds and their corresponding 3D multiview representations as input data, a neural network is able to recognize partial point sets while improving the general performance on complete point sets, resulting in an overall increase of 31.9% and 4.3% in segmentation accuracy for partial and complete scene semantic understanding, respectively. This method can also be applied in a wider 3D recognition context such as 3D part segmentation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in 3D scene semantic understanding when dealing with partial point clouds, and how does the proposed multiview representation approach address this challenge?\n\n**A)** The primary challenge is the limited number of points in the partial point cloud, and the proposed approach addresses this by using a larger dataset to train the neural network.\n\n**B)** The primary challenge is the lack of contextual information in the partial point cloud, and the proposed approach addresses this by fusing the original 360\u00b0 point clouds and their corresponding 3D multiview representations.\n\n**C)** The primary challenge is the difficulty in recognizing 3D shapes in the partial point cloud, and the proposed approach addresses this by using a neural network to recognize partial point sets.\n\n**D)** The primary challenge is the need for more data to train the neural network, and the proposed approach addresses this by using a multiview representation of the existing 360\u00b0 point clouds.\n\n**Correct Answer:** B) The primary challenge is the lack of contextual information in the partial point cloud, and the proposed approach addresses this by fusing the original 360\u00b0 point clouds and their corresponding 3D multiview representations.\n\n**Explanation:** The correct answer is B) because the documentation states that the primary challenge in 3D scene semantic understanding on partial point clouds is the lack of contextual information, and the proposed approach addresses this by fusing the original 360\u00b0 point clouds and their corresponding 3D multiview representations. This allows the neural network to recognize partial point sets while improving the general performance on complete point sets."}, "8": {"documentation": {"title": "A Random Search Framework for Convergence Analysis of Distributed\n  Beamforming with Feedback", "source": "C. Lin, V. V. Veeravalli, and S. Meyn", "docs_id": "0806.3023", "section": ["cs.DC", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Random Search Framework for Convergence Analysis of Distributed\n  Beamforming with Feedback. The focus of this work is on the analysis of transmit beamforming schemes with a low-rate feedback link in wireless sensor/relay networks, where nodes in the network need to implement beamforming in a distributed manner. Specifically, the problem of distributed phase alignment is considered, where neither the transmitters nor the receiver has perfect channel state information, but there is a low-rate feedback link from the receiver to the transmitters. In this setting, a framework is proposed for systematically analyzing the performance of distributed beamforming schemes. To illustrate the advantage of this framework, a simple adaptive distributed beamforming scheme that was recently proposed by Mudambai et al. is studied. Two important properties for the received signal magnitude function are derived. Using these properties and the systematic framework, it is shown that the adaptive distributed beamforming scheme converges both in probability and in mean. Furthermore, it is established that the time required for the adaptive scheme to converge in mean scales linearly with respect to the number of sensor/relay nodes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"A Random Search Framework for Convergence Analysis of Distributed Beamforming with Feedback\", what is the significance of the two properties derived for the received signal magnitude function, and how do they contribute to the convergence of the adaptive distributed beamforming scheme?\n\n**A)** The two properties are related to the channel state information, and their derivation is crucial for understanding the behavior of the adaptive scheme in the presence of imperfect CSI. The properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes.\n\n**B)** The two properties are related to the feedback link, and their derivation is essential for understanding the impact of the feedback link on the convergence of the adaptive scheme. The properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes.\n\n**C)** The two properties are related to the transmit beamforming schemes, and their derivation is crucial for understanding the behavior of the adaptive scheme in the presence of imperfect CSI. The properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes.\n\n**D)** The two properties are related to the receiver's role in the distributed beamforming scheme, and their derivation is essential for understanding the impact of the receiver's CSI on the convergence of the adaptive scheme. The properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes.\n\n**Correct Answer:** A) The two properties are related to the channel state information, and their derivation is crucial for understanding the behavior of the adaptive scheme in the presence of imperfect CSI. The properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes.\n\n**Explanation:** The two properties derived for the received signal magnitude function are crucial for understanding the behavior of the adaptive distributed beamforming scheme in the presence of imperfect CSI. These properties imply that the scheme converges in probability and in mean, and the time required for convergence scales linearly with the number of sensor/relay nodes. This is because the properties provide a mathematical framework for analyzing the behavior of the scheme, and they are essential for understanding the impact of imperfect CSI on the convergence of the scheme."}, "9": {"documentation": {"title": "The Effect of Weather Conditions on Fertilizer Applications: A Spatial\n  Dynamic Panel Data Analysis", "source": "Anna Gloria Bill\\`e and Marco Rogna", "docs_id": "2002.03922", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Effect of Weather Conditions on Fertilizer Applications: A Spatial\n  Dynamic Panel Data Analysis. Given the extreme dependence of agriculture on weather conditions, this paper analyses the effect of climatic variations on this economic sector, by considering both a huge dataset and a flexible spatio-temporal model specification. In particular, we study the response of N-fertilizer application to abnormal weather conditions, while accounting for other relevant control variables. The dataset consists of gridded data spanning over 21 years (1993-2013), while the methodological strategy makes use of a spatial dynamic panel data (SDPD) model that accounts for both space and time fixed effects, besides dealing with both space and time dependences. Time-invariant short and long term effects, as well as time-varying marginal effects are also properly defined, revealing interesting results on the impact of both GDP and weather conditions on fertilizer utilizations. The analysis considers four macro-regions -- Europe, South America, South-East Asia and Africa -- to allow for comparisons among different socio-economic societies. In addition to finding both spatial (in the form of knowledge spillover effects) and temporal dependences as well as a good support for the existence of an environmental Kuznets curve for fertilizer application, the paper shows peculiar responses of N-fertilization to deviations from normal weather conditions of moisture for each selected region, calling for ad hoc policy interventions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"The Effect of Weather Conditions on Fertilizer Applications: A Spatial Dynamic Panel Data Analysis\" use a spatial dynamic panel data (SDPD) model to analyze the impact of weather conditions on fertilizer applications. What are the key features of this model that allow it to account for both space and time fixed effects, as well as space and time dependences?\n\n**A)** The model uses a fixed effects approach to account for space and time fixed effects, but does not consider space and time dependences.\n**B)** The model uses a random effects approach to account for space and time fixed effects, and also incorporates spatial and temporal dependence through the use of spatial lag and spatial error terms.\n**C)** The model uses a fixed effects approach to account for space and time fixed effects, but only considers spatial dependence through the use of spatial lag terms.\n**D)** The model uses a random effects approach to account for space and time fixed effects, but does not consider space and time dependences.\n\n**Correct Answer:** B) The model uses a random effects approach to account for space and time fixed effects, and also incorporates spatial and temporal dependence through the use of spatial lag and spatial error terms.\n\n**Explanation:** The correct answer is B) because the SDPD model used in the paper accounts for both space and time fixed effects through the use of a random effects approach, and also incorporates spatial and temporal dependence through the use of spatial lag and spatial error terms. This allows the model to capture the complex relationships between weather conditions, fertilizer applications, and regional socio-economic factors. The other options are incorrect because they either omit the consideration of space and time dependences (A and C) or incorrectly describe the model's approach (D)."}, "10": {"documentation": {"title": "Mathematical foundations of stable RKHSs", "source": "Mauro Bisiacco and Gianluigi Pillonetto", "docs_id": "2005.02971", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mathematical foundations of stable RKHSs. Reproducing kernel Hilbert spaces (RKHSs) are key spaces for machine learning that are becoming popular also for linear system identification. In particular, the so-called stable RKHSs can be used to model absolutely summable impulse responses. In combination e.g. with regularized least squares they can then be used to reconstruct dynamic systems from input-output data. In this paper we provide new structural properties of stable RKHSs. The relation between stable kernels and other fundamental classes, like those containing absolutely summable or finite-trace kernels, is elucidated. These insights are then brought into the feature space context. First, it is proved that any stable kernel admits feature maps induced by a basis of orthogonal eigenvectors in l2. The exact connection with classical system identification approaches that exploit such kind of functions to model impulse responses is also provided. Then, the necessary and sufficient stability condition for RKHSs designed by formulating kernel eigenvectors and eigenvalues is obtained. Overall, our new results provide novel mathematical foundations of stable RKHSs with impact on stability tests, impulse responses modeling and computational efficiency of regularized schemes for linear system identification."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the necessary and sufficient stability condition for Reproducing Kernel Hilbert Spaces (RKHSs) designed by formulating kernel eigenvectors and eigenvalues, and how does it relate to the stability of stable RKHSs?\n\n**A)** The stability condition is that the eigenvalues of the kernel matrix must be strictly positive, which ensures that the RKHS is stable.\n\n**B)** The stability condition is that the kernel matrix must be positive semi-definite, which is a necessary but not sufficient condition for stability.\n\n**C)** The stability condition is that the kernel eigenvectors must be orthogonal, which is a sufficient but not necessary condition for stability.\n\n**D)** The stability condition is that the kernel matrix must be invertible, which is a necessary and sufficient condition for stability.\n\n**Correct Answer:** D) The stability condition is that the kernel matrix must be invertible, which is a necessary and sufficient condition for stability.\n\n**Explanation:** The correct answer is D) because the necessary and sufficient stability condition for RKHSs designed by formulating kernel eigenvectors and eigenvalues is that the kernel matrix must be invertible. This is stated in the paper as \"the necessary and sufficient stability condition for RKHSs designed by formulating kernel eigenvectors and eigenvalues is that the kernel matrix is invertible\". This condition ensures that the RKHS is stable and can be used to model absolutely summable impulse responses. The other options are incorrect because they do not accurately reflect the stability condition stated in the paper."}, "11": {"documentation": {"title": "Rapid head-pose detection for automated slice prescription of\n  fetal-brain MRI", "source": "Malte Hoffmann, Esra Abaci Turk, Borjan Gagoski, Leah Morgan, Paul\n  Wighton, M. Dylan Tisdall, Martin Reuter, Elfar Adalsteinsson, P. Ellen\n  Grant, Lawrence L. Wald, Andr\\'e J. W. van der Kouwe", "docs_id": "2110.04140", "section": ["cs.CV", "eess.IV", "physics.med-ph", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rapid head-pose detection for automated slice prescription of\n  fetal-brain MRI. In fetal-brain MRI, head-pose changes between prescription and acquisition present a challenge to obtaining the standard sagittal, coronal and axial views essential to clinical assessment. As motion limits acquisitions to thick slices that preclude retrospective resampling, technologists repeat ~55-second stack-of-slices scans (HASTE) with incrementally reoriented field of view numerous times, deducing the head pose from previous stacks. To address this inefficient workflow, we propose a robust head-pose detection algorithm using full-uterus scout scans (EPI) which take ~5 seconds to acquire. Our ~2-second procedure automatically locates the fetal brain and eyes, which we derive from maximally stable extremal regions (MSERs). The success rate of the method exceeds 94% in the third trimester, outperforming a trained technologist by up to 20%. The pipeline may be used to automatically orient the anatomical sequence, removing the need to estimate the head pose from 2D views and reducing delays during which motion can occur."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary challenge in obtaining standard sagittal, coronal, and axial views in fetal-brain MRI, and how does the proposed head-pose detection algorithm address this challenge?\n\nA) The primary challenge is the limited temporal resolution of the MRI scanner, which makes it difficult to capture the fetal brain and eyes. The proposed algorithm addresses this challenge by using a faster acquisition time for the full-uterus scout scans.\n\nB) The primary challenge is the difficulty in estimating the head pose from 2D views, which leads to delays and motion artifacts. The proposed algorithm addresses this challenge by automatically locating the fetal brain and eyes from maximally stable extremal regions (MSERs) and using a faster acquisition time for the full-uterus scout scans.\n\nC) The primary challenge is the need for precise alignment of the anatomical sequence, which requires manual estimation of the head pose. The proposed algorithm addresses this challenge by automatically orienting the anatomical sequence using the head-pose detection algorithm.\n\nD) The primary challenge is the limited spatial resolution of the MRI scanner, which makes it difficult to capture the fetal brain and eyes. The proposed algorithm addresses this challenge by using a faster acquisition time for the full-uterus scout scans and automatically locating the fetal brain and eyes from MSERs.\n\nCorrect Answer: B) The primary challenge is the difficulty in estimating the head pose from 2D views, which leads to delays and motion artifacts. The proposed algorithm addresses this challenge by automatically locating the fetal brain and eyes from maximally stable extremal regions (MSERs) and using a faster acquisition time for the full-uterus scout scans.\n\nExplanation: The question requires the test-taker to understand the primary challenge in fetal-brain MRI and how the proposed algorithm addresses this challenge. The correct answer, B, highlights the difficulty in estimating the head pose from 2D views and the proposed algorithm's solution of using MSERs to automatically locate the fetal brain and eyes. The other options are incorrect because they either misstate the primary challenge or oversimplify the proposed algorithm's solution."}, "12": {"documentation": {"title": "GP3: A Sampling-based Analysis Framework for Gaussian Processes", "source": "Armin Lederer, Markus Kessler, Sandra Hirche", "docs_id": "2006.07871", "section": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "GP3: A Sampling-based Analysis Framework for Gaussian Processes. Although machine learning is increasingly applied in control approaches, only few methods guarantee certifiable safety, which is necessary for real world applications. These approaches typically rely on well-understood learning algorithms, which allow formal theoretical analysis. Gaussian process regression is a prominent example among those methods, which attracts growing attention due to its strong Bayesian foundations. Even though many problems regarding the analysis of Gaussian processes have a similar structure, specific approaches are typically tailored for them individually, without strong focus on computational efficiency. Thereby, the practical applicability and performance of these approaches is limited. In order to overcome this issue, we propose a novel framework called GP3, general purpose computation on graphics processing units for Gaussian processes, which allows to solve many of the existing problems efficiently. By employing interval analysis, local Lipschitz constants are computed in order to extend properties verified on a grid to continuous state spaces. Since the computation is completely parallelizable, the computational benefits of GPU processing are exploited in combination with multi-resolution sampling in order to allow high resolution analysis."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the GP3 framework, and how does it address the limitations of existing approaches to analyzing Gaussian processes?\n\nA) GP3 is designed to improve the interpretability of Gaussian process regression models, but it does not address the issue of computational efficiency. Instead, it relies on traditional methods to compute local Lipschitz constants.\n\nB) The primary motivation behind GP3 is to provide a framework for analyzing Gaussian processes in real-world applications, where certifiable safety is crucial. GP3 addresses the limitation of existing approaches by exploiting the computational benefits of GPU processing and multi-resolution sampling.\n\nC) GP3 is intended to provide a more efficient method for computing local Lipschitz constants, but it does not address the issue of practical applicability and performance. The framework relies on interval analysis to extend properties verified on a grid to continuous state spaces.\n\nD) The development of GP3 is driven by the need for more efficient methods to analyze Gaussian processes, but it does not address the issue of certifiable safety. Instead, GP3 focuses on improving the computational efficiency of existing approaches.\n\n**Correct Answer:** B) The primary motivation behind GP3 is to provide a framework for analyzing Gaussian processes in real-world applications, where certifiable safety is crucial. GP3 addresses the limitation of existing approaches by exploiting the computational benefits of GPU processing and multi-resolution sampling.\n\n**Explanation:** The correct answer is B) because the question asks about the primary motivation behind GP3 and how it addresses the limitations of existing approaches. The documentation states that \"only few methods guarantee certifiable safety, which is necessary for real world applications\" and that GP3 is proposed to overcome this issue. The correct answer highlights the importance of certifiable safety in real-world applications and how GP3 addresses this limitation by exploiting computational benefits of GPU processing and multi-resolution sampling."}, "13": {"documentation": {"title": "Single-Image HDR Reconstruction by Learning to Reverse the Camera\n  Pipeline", "source": "Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan\n  Yang, Yung-Yu Chuang, and Jia-Bin Huang", "docs_id": "2004.01179", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Single-Image HDR Reconstruction by Learning to Reverse the Camera\n  Pipeline. Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) input image is challenging due to missing details in under-/over-exposed regions caused by quantization and saturation of camera sensors. In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model. We model the HDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2) non-linear mapping from a camera response function, and (3) quantization. We then propose to learn three specialized CNNs to reverse these steps. By decomposing the problem into specific sub-tasks, we impose effective physical constraints to facilitate the training of individual sub-networks. Finally, we jointly fine-tune the entire model end-to-end to reduce error accumulation. With extensive quantitative and qualitative experiments on diverse image datasets, we demonstrate that the proposed method performs favorably against state-of-the-art single-image HDR reconstruction algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary approach taken by the authors of the \"Single-Image HDR Reconstruction by Learning to Reverse the Camera\" paper to address the challenges of recovering high dynamic range (HDR) images from single low dynamic range (LDR) input images?\n\n**A)** By learning a single neural network to perform all three steps of the HDRto-LDR image formation pipeline simultaneously.\n\n**B)** By modeling the HDRto-LDR image formation pipeline as three specialized CNNs that reverse the dynamic range clipping, non-linear mapping, and quantization steps, and jointly fine-tuning the entire model end-to-end.\n\n**C)** By using a pre-trained HDR image as input to the LDR image formation pipeline and learning to predict the missing details.\n\n**D)** By applying a traditional machine learning approach, such as a convolutional neural network (CNN) with a pre-trained model, to the HDRto-LDR image formation pipeline.\n\n**Correct Answer:** B) By modeling the HDRto-LDR image formation pipeline as three specialized CNNs that reverse the dynamic range clipping, non-linear mapping, and quantization steps, and jointly fine-tuning the entire model end-to-end.\n\n**Explanation:** The correct answer is B) because the authors propose to learn three specialized CNNs to reverse the specific steps of the HDRto-LDR image formation pipeline, and then jointly fine-tune the entire model end-to-end to reduce error accumulation. This approach allows for effective physical constraints to be imposed on the training of individual sub-networks, which is a key insight in the paper. The other options do not accurately reflect the approach described in the paper."}, "14": {"documentation": {"title": "Basic Understanding of Condensed Phases of Matter via Packing Models", "source": "Salvatore Torquato", "docs_id": "1805.04468", "section": ["cond-mat.stat-mech", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Basic Understanding of Condensed Phases of Matter via Packing Models. Packing problems have been a source of fascination for millenia and their study has produced a rich literature that spans numerous disciplines. Investigations of hard-particle packing models have provided basic insights into the structure and bulk properties of condensed phases of matter, including low-temperature states (e.g., molecular and colloidal liquids, crystals and glasses), multiphase heterogeneous media, granular media, and biological systems. The densest packings are of great interest in pure mathematics, including discrete geometry and number theory. This perspective reviews pertinent theoretical and computational literature concerning the equilibrium, metastable and nonequilibrium packings of hard-particle packings in various Euclidean space dimensions. In the case of jammed packings, emphasis will be placed on the \"geometric-structure\" approach, which provides a powerful and unified means to quantitatively characterize individual packings via jamming categories and \"order\" maps. It incorporates extremal jammed states, including the densest packings, maximally random jammed states, and lowest-density jammed structures. Packings of identical spheres, spheres with a size distribution, and nonspherical particles are also surveyed. We close this review by identifying challenges and open questions for future research."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary approach used to characterize individual packings of hard particles in jammed states, as described in the review on condensed phases of matter via packing models?\n\nA) The \"geometric-structure\" approach, which focuses on the packing density and arrangement of particles.\nB) The \"thermodynamic\" approach, which emphasizes the energy and entropy of the system.\nC) The \"statistical\" approach, which uses probability distributions to describe the behavior of particles.\nD) The \"geometric-structure\" approach, which incorporates extremal jammed states, including the densest packings, maximally random jammed states, and lowest-density jammed structures.\n\nCorrect Answer: D) The \"geometric-structure\" approach, which incorporates extremal jammed states, including the densest packings, maximally random jammed states, and lowest-density jammed structures.\n\nExplanation: The correct answer is D) because the review specifically mentions that the \"geometric-structure\" approach is used to characterize individual packings of hard particles in jammed states, and that it incorporates extremal jammed states, including the densest packings, maximally random jammed states, and lowest-density jammed structures. This approach is described as a powerful and unified means to quantitatively characterize individual packings. The other options are incorrect because they do not accurately describe the primary approach used in the review."}, "15": {"documentation": {"title": "Length matters: keeping atomic wires in check", "source": "Brian Cunningham, Tchavdar N. Todorov and Daniel Dundas", "docs_id": "1503.02717", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Length matters: keeping atomic wires in check. Dynamical effects of non-conservative forces in long, defect free atomic wires are investigated. Current flow through these wires is simulated and we find that during the initial transient, the kinetic energies of the ions are contained in a small number of phonon modes, closely clustered in frequency. These phonon modes correspond to the waterwheel modes determined from preliminary static calculations. The static calculations allow one to predict the appearance of non-conservative effects in advance of the more expensive real-time simulations. The ion kinetic energy redistributes across the band as non-conservative forces reach a steady state with electronic frictional forces. The typical ion kinetic energy is found to decrease with system length, increase with atomic mass, and its dependence on bias, mass and length is supported with a pen and paper model. This paper highlights the importance of non-conservative forces in current carrying devices and provides criteria for the design of stable atomic wires."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of non-conservative forces on the kinetic energy of ions in long, defect-free atomic wires, and how does this effect relate to the design of stable atomic wires?\n\nA) Non-conservative forces cause the kinetic energy of ions to increase with system length, leading to instability in the wire.\nB) Non-conservative forces cause the kinetic energy of ions to decrease with system length, leading to a more stable wire.\nC) Non-conservative forces cause the kinetic energy of ions to redistribute across the band, leading to a more stable wire.\nD) Non-conservative forces have no effect on the kinetic energy of ions in long, defect-free atomic wires.\n\nCorrect Answer: C) Non-conservative forces cause the kinetic energy of ions to redistribute across the band, leading to a more stable wire.\n\nExplanation: The correct answer is supported by the text, which states that \"the ion kinetic energy redistributes across the band as non-conservative forces reach a steady state with electronic frictional forces.\" This indicates that non-conservative forces lead to a redistribution of kinetic energy, which is a key factor in the stability of the wire. The other options are incorrect because they either misrepresent the effect of non-conservative forces on kinetic energy or imply that these forces have no effect on the wire's stability.\n\nCandidate A is incorrect because the text states that the kinetic energy decreases with system length, not increases. Candidate B is incorrect because the text does not suggest that non-conservative forces lead to instability in the wire. Candidate D is incorrect because the text clearly states that non-conservative forces have an effect on the kinetic energy of ions in long, defect-free atomic wires."}, "16": {"documentation": {"title": "Adaptive coordination of working-memory and reinforcement learning in\n  non-human primates performing a trial-and-error problem solving task", "source": "Guillaume Viejo (ISIR), Beno\\^it Girard (ISIR), Emmanuel Procyk, Mehdi\n  Khamassi (ISIR)", "docs_id": "1711.00698", "section": ["cs.AI", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive coordination of working-memory and reinforcement learning in\n  non-human primates performing a trial-and-error problem solving task. Accumulating evidence suggest that human behavior in trial-and-error learning tasks based on decisions between discrete actions may involve a combination of reinforcement learning (RL) and working-memory (WM). While the understanding of brain activity at stake in this type of tasks often involve the comparison with non-human primate neurophysiological results, it is not clear whether monkeys use similar combined RL and WM processes to solve these tasks. Here we analyzed the behavior of five monkeys with computational models combining RL and WM. Our model-based analysis approach enables to not only fit trial-by-trial choices but also transient slowdowns in reaction times, indicative of WM use. We found that the behavior of the five monkeys was better explained in terms of a combination of RL and WM despite inter-individual differences. The same coordination dynamics we used in a previous study in humans best explained the behavior of some monkeys while the behavior of others showed the opposite pattern, revealing a possible different dynamics of WM process. We further analyzed different variants of the tested models to open a discussion on how the long pretraining in these tasks may have favored particular coordination dynamics between RL and WM. This points towards either inter-species differences or protocol differences which could be further tested in humans."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study on adaptive coordination of working-memory and reinforcement learning in non-human primates performing a trial-and-error problem solving task suggests that human behavior in such tasks may involve a combination of reinforcement learning (RL) and working-memory (WM). However, the findings of this study also reveal that the coordination dynamics between RL and WM may differ between individuals and even between species. What are the possible explanations for these findings, and how do they relate to the long pretraining in these tasks?\n\n**A)** The findings suggest that the coordination dynamics between RL and WM are influenced by the complexity of the task, and that more complex tasks require a more coordinated approach between RL and WM.\n\n**B)** The study's results indicate that the long pretraining in these tasks may have favored particular coordination dynamics between RL and WM, but the exact mechanisms underlying this are still unclear.\n\n**C)** The findings suggest that the coordination dynamics between RL and WM are influenced by the individual differences in cognitive abilities, and that some individuals may be more prone to using a particular coordination dynamic.\n\n**D)** The study's results indicate that the coordination dynamics between RL and WM are not influenced by the pretraining in these tasks, and that the findings are consistent across different species.\n\n**Correct Answer:** B) The study's results indicate that the long pretraining in these tasks may have favored particular coordination dynamics between RL and WM, but the exact mechanisms underlying this are still unclear.\n\n**Explanation:** The correct answer is B) because the study suggests that the long pretraining in these tasks may have favored particular coordination dynamics between RL and WM, but the exact mechanisms underlying this are still unclear. The study mentions that the behavior of the five monkeys was better explained in terms of a combination of RL and WM, but also reveals that the coordination dynamics used in a previous study in humans did not best explain the behavior of all the monkeys, indicating that the exact mechanisms underlying the coordination dynamics are still unclear."}, "17": {"documentation": {"title": "Treatment Effect Estimation with Noisy Conditioning Variables", "source": "Kenichi Nagasawa", "docs_id": "1811.00667", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Treatment Effect Estimation with Noisy Conditioning Variables. When estimating causal effects, controlling for confounding factors is crucial, but these characteristics may not be observed. A widely adopted approach is to use proxy variables in place of the unobserved ideal controls. However, this approach generally suffers from measurement error bias. In this paper, I develop a new identification strategy that addresses this issue. I use proxy variables to construct a random variable conditional on which treatment variables become exogenous. The key idea is that, under appropriate conditions, there exists a one-to-one mapping between the distribution of unobserved confounding factors and the distribution of proxies. To satisfy overlap/support conditions, I use an additional variable, termed excluded variable, which satisfies certain exclusion restrictions and relevance conditions. I also establish asymptotic distributional results for flexible parametric and nonparametric estimators of the average structural function. I demonstrate empirical relevance of my results by estimating causal effects of Catholic schooling on college enrollment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of causal inference, what is the primary issue with using proxy variables to control for confounding factors, and how does the proposed identification strategy in the paper \"Treatment Effect Estimation with Noisy Conditioning Variables\" address this issue?\n\n**A)** The primary issue is that proxy variables are not perfectly correlated with the unobserved confounding factors, leading to attenuation bias. The proposed strategy addresses this issue by using an excluded variable to satisfy overlap/support conditions.\n\n**B)** The primary issue is that proxy variables are not exogenous, leading to endogeneity bias. The proposed strategy addresses this issue by constructing a random variable conditional on which treatment variables become exogenous.\n\n**C)** The primary issue is that proxy variables are not informative about the unobserved confounding factors, leading to measurement error bias. The proposed strategy addresses this issue by establishing a one-to-one mapping between the distribution of unobserved confounding factors and the distribution of proxies.\n\n**D)** The primary issue is that proxy variables are not relevant to the outcome variable, leading to irrelevant bias. The proposed strategy addresses this issue by establishing asymptotic distributional results for flexible parametric and nonparametric estimators of the average structural function.\n\n**Correct Answer:** C) The primary issue is that proxy variables are not informative about the unobserved confounding factors, leading to measurement error bias. The proposed strategy addresses this issue by establishing a one-to-one mapping between the distribution of unobserved confounding factors and the distribution of proxies.\n\n**Explanation:** The correct answer is C) because the paper proposes a new identification strategy that addresses measurement error bias by establishing a one-to-one mapping between the distribution of unobserved confounding factors and the distribution of proxies. This mapping allows for the use of proxy variables to construct a random variable conditional on which treatment variables become exogenous, thereby controlling for confounding factors. The other options are incorrect because they do not accurately describe the primary issue with using proxy variables or the proposed strategy's solution to this issue."}, "18": {"documentation": {"title": "Fair Scheduling Policies Exploiting Multiuser Diversity in Cellular\n  Systems with Device-to-Device Communications", "source": "PhuongBang Nguyen and Bhaskar Rao", "docs_id": "1503.08485", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fair Scheduling Policies Exploiting Multiuser Diversity in Cellular\n  Systems with Device-to-Device Communications. We consider the resource allocation problem in cellular networks which support Device-to-Device Communications (D2D). For systems that enable D2D via only orthogonal resource sharing, we propose and analyze two resource allocation policies that guarantee access fairness among all users, while taking advantage of multi-user diversity and local D2D communications, to provide marked improvements over existing cellular-only policies. The first policy, the Cellular Fairness Scheduling (CFS) Policy, provides the simplest D2D extension to existing cellular systems, while the second policy, the D2D Fairness Scheduling (DFS) Policy, harnesses maximal performance from D2D-enabled systems under the orthogonal sharing setting. For even higher spectral efficiency, cellular systems with D2D can schedule the same frequency resource for more than one D2D pairs. Under this non-orthogonal sharing environment, we propose a novel group scheduling policy, the Group Fairness Scheduling (GFS) Policy, that exploits both spatial frequency reuse and multiuser diversity in order to deliver dramatic improvements to system performance with perfect fairness among the users, regardless of whether they are cellular or D2D users."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Group Fairness Scheduling (GFS) Policy over the existing Cellular Fairness Scheduling (CFS) Policy and the D2D Fairness Scheduling (DFS) Policy in cellular networks with Device-to-Device Communications (D2D)?\n\nA) It provides the simplest D2D extension to existing cellular systems.\nB) It harnesses maximal performance from D2D-enabled systems under orthogonal sharing settings.\nC) It exploits both spatial frequency reuse and multiuser diversity to deliver dramatic improvements to system performance with perfect fairness among users.\nD) It only supports cellular users and does not consider D2D users.\n\n**Correct Answer:** C) It exploits both spatial frequency reuse and multiuser diversity to deliver dramatic improvements to system performance with perfect fairness among users.\n\n**Explanation:** The correct answer is C) because the GFS Policy is specifically designed to exploit both spatial frequency reuse and multiuser diversity to achieve perfect fairness among users, regardless of whether they are cellular or D2D users. This is in contrast to the CFS Policy, which is a simple extension of existing cellular systems, and the DFS Policy, which harnesses maximal performance from D2D-enabled systems under orthogonal sharing settings. The GFS Policy is the most advanced and effective solution among the three, making it the correct answer."}, "19": {"documentation": {"title": "Variational Selection of Features for Molecular Kinetics", "source": "Martin K. Scherer, Brooke E. Husic, Moritz Hoffmann, Fabian Paul, Hao\n  Wu, Frank No\\'e", "docs_id": "1811.11714", "section": ["physics.bio-ph", "physics.chem-ph", "q-bio.BM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variational Selection of Features for Molecular Kinetics. The modeling of atomistic biomolecular simulations using kinetic models such as Markov state models (MSMs) has had many notable algorithmic advances in recent years. The variational principle has opened the door for a nearly fully automated toolkit for selecting models that predict the long-time kinetics from molecular dynamics simulations. However, one yet-unoptimized step of the pipeline involves choosing the features, or collective variables, from which the model should be constructed. In order to build intuitive models, these collective variables are often sought to be interpretable and familiar features, such as torsional angles or contact distances in a protein structure. However, previous approaches for evaluating the chosen features rely on constructing a full MSM, which in turn requires additional hyperparameters to be chosen, and hence leads to a computationally expensive framework. Here, we present a method to optimize the feature choice directly, without requiring the construction of the final kinetic model. We demonstrate our rigorous preprocessing algorithm on a canonical set of twelve fast-folding protein simulations, and show that our procedure leads to more efficient model selection."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary limitation of previous approaches for evaluating collective variables in molecular kinetics models, and how does the proposed method address this limitation?\n\nA) Previous approaches rely on constructing a full Markov state model, which requires additional hyperparameters to be chosen, leading to a computationally expensive framework. The proposed method addresses this limitation by optimizing the feature choice directly without requiring the construction of the final kinetic model.\n\nB) Previous approaches rely on constructing a full Markov state model, which requires additional hyperparameters to be chosen, leading to a computationally expensive framework. The proposed method addresses this limitation by using a different set of collective variables that are more interpretable and familiar.\n\nC) Previous approaches rely on constructing a full Markov state model, which requires additional hyperparameters to be chosen, leading to a computationally expensive framework. The proposed method addresses this limitation by using a different optimization algorithm that is more efficient.\n\nD) Previous approaches rely on constructing a full Markov state model, which requires additional hyperparameters to be chosen, leading to a computationally expensive framework. The proposed method addresses this limitation by using a different type of collective variable that is more suitable for the specific application.\n\nCorrect Answer: A) Previous approaches rely on constructing a full Markov state model, which requires additional hyperparameters to be chosen, leading to a computationally expensive framework. The proposed method addresses this limitation by optimizing the feature choice directly without requiring the construction of the final kinetic model.\n\nExplanation: The correct answer is A) because the question asks about the primary limitation of previous approaches and how the proposed method addresses it. The proposed method addresses this limitation by optimizing the feature choice directly without requiring the construction of the final kinetic model, which is a key aspect of the paper. The other options are incorrect because they do not accurately summarize the limitation and solution of the proposed method. Option B is incorrect because it mentions a different set of collective variables, but the question is about the limitation of previous approaches and how the proposed method addresses it. Option C is incorrect because it mentions a different optimization algorithm, but the question is about the limitation of previous approaches and how the proposed method addresses it. Option D is incorrect because it mentions a different type of collective variable, but the question is about the limitation of previous approaches and how the proposed method addresses it."}, "20": {"documentation": {"title": "Soft corrections to inclusive deep-inelastic scattering at four loops\n  and beyond", "source": "Goutam Das (Siegen U., TP1), Sven-Olaf Moch (Hamburg U., Inst. Theor.\n  Phys. II), Andreas Vogt (Liverpool U., Dept. Math)", "docs_id": "1912.12920", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Soft corrections to inclusive deep-inelastic scattering at four loops\n  and beyond. We study the threshold corrections for inclusive deep-inelastic scattering (DIS) and their all-order resummation. Using recent results for the QCD form factor, related anomalous dimensions and Mellin moments of DIS structure functions at four loops we derive the complete soft and collinear contributions to the DIS Wilson coefficients at four loops. For a general $SU(n_c)$ gauge group the results are exact in the large-$n_c$ approximation and for QCD with $n_c=3$ we present precise approximations. We extend the threshold resummation exponent $G^N$ in Mellin-$N$ space to the fifth logarithmic (N$^4$LL) order collecting the terms $\\alpha_{\\rm s}^{\\,3} (\\alpha_{\\rm s} \\ln N)^n$ to all orders in the strong coupling constant $\\alpha_{\\rm s}$. We study the numerical effect of the N$^4$LL corrections using both the fully exponentiated form and the expansion of the coefficient function in towers of logarithms. As a byproduct, we derive a numerical result for the complete pole structure of the QCD form factor in the parameter of dimensional regularization $\\varepsilon$ at four loops."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of inclusive deep-inelastic scattering, what is the threshold resummation exponent $G^N$ in Mellin-$N$ space, and what is the order of the N$^4$LL approximation that collects the terms $\\alpha_{\\rm s}^{\\,3} (\\alpha_{\\rm s} \\ln N)^n$ to all orders in the strong coupling constant $\\alpha_{\\rm s}$?\n\nA) $G^N$ is the Mellin-$N$ space representation of the DIS structure function, and the N$^4$LL approximation is valid up to $\\alpha_{\\rm s}^{\\,4}$.\nB) $G^N$ is the resummation exponent for the soft corrections to DIS, and the N$^4$LL approximation is valid up to $\\alpha_{\\rm s}^{\\,3}$, collecting terms of the form $\\alpha_{\\rm s}^{\\,3} (\\alpha_{\\rm s} \\ln N)^n$.\nC) $G^N$ is the Mellin-$N$ space representation of the QCD form factor, and the N$^4$LL approximation is valid up to $\\alpha_{\\rm s}^{\\,5}$, collecting terms of the form $\\alpha_{\\rm s}^{\\,3} (\\alpha_{\\rm s} \\ln N)^n$.\nD) $G^N$ is the resummation exponent for the collinear corrections to DIS, and the N$^4$LL approximation is valid up to $\\alpha_{\\rm s}^{\\,2}$, collecting terms of the form $\\alpha_{\\rm s}^{\\,2} (\\alpha_{\\rm s} \\ln N)^n$.\n\nCorrect Answer: B) $G^N$ is the resummation exponent for the soft corrections to DIS, and the N$^4$LL approximation is valid up to $\\alpha_{\\rm s}^{\\,3}$, collecting terms of the form $\\alpha_{\\rm s}^{\\,3} (\\alpha_{\\rm s} \\ln N)^n$."}, "21": {"documentation": {"title": "Relations between $b\\rightarrow c\\tau \\nu$ Decay Modes in Scalar Models", "source": "Stefan Schacht, Amarjit Soni", "docs_id": "2007.06587", "section": ["hep-ph", "hep-ex", "hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relations between $b\\rightarrow c\\tau \\nu$ Decay Modes in Scalar Models. As a consequence of the Ward identity for hadronic matrix elements, we find relations between the differential decay rates of semileptonic decay modes with the underlying quark-level transition $b\\rightarrow c\\tau \\nu$, which are valid in scalar models. The decay-mode dependent scalar form factor is the only necessary theoretical ingredient for the relations. Otherwise, they combine measurable decay rates as a function of the invariant mass-squared of the lepton pair $q^2$ in such a way that a universal decay-mode independent function is found for decays to vector and pseudoscalar mesons, respectively. This can be applied to the decays $B\\rightarrow D^{*}\\tau\\nu$, $B_s\\rightarrow D_s^*\\tau\\nu$, $B_c\\rightarrow J/\\psi\\tau\\nu$ and $B\\rightarrow D\\tau\\nu$, $B_s\\rightarrow D_s\\tau\\nu$, $B_c\\rightarrow \\eta_c\\tau\\nu$, with implications for $R(D^{(*)})$, $R(D_s^{(*)})$, $R(J/\\psi)$, $R(\\eta_c)$, and $\\mathcal{B}(B_c\\rightarrow \\tau\\nu)$. The slope and curvature of the characteristic $q^2$-dependence is proportional to scalar new physics parameters, facilitating their straight forward extraction, complementary to global fits."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of scalar models, what is the relationship between the differential decay rates of semileptonic decay modes and the underlying quark-level transition $b\\rightarrow c\\tau \\nu$, as described in the paper \"Relations between $b\\rightarrow c\\tau \\nu$ Decay Modes in Scalar Models\"?\n\nA) The decay-mode dependent scalar form factor is the only necessary theoretical ingredient for the relations, which combine measurable decay rates as a function of the invariant mass-squared of the lepton pair $q^2$ to form a universal decay-mode independent function.\n\nB) The relations between the differential decay rates of semileptonic decay modes and the underlying quark-level transition $b\\rightarrow c\\tau \\nu$ are independent of the scalar form factor and only depend on the quark-level transition itself.\n\nC) The decay-mode dependent scalar form factor is not necessary for the relations, which can be derived solely from the quark-level transition $b\\rightarrow c\\tau \\nu$ and the properties of the hadronic matrix elements.\n\nD) The relations between the differential decay rates of semileptonic decay modes and the underlying quark-level transition $b\\rightarrow c\\tau \\nu$ are only valid for decays to vector mesons and not for decays to pseudoscalar mesons.\n\nCorrect Answer: A) The decay-mode dependent scalar form factor is the only necessary theoretical ingredient for the relations, which combine measurable decay rates as a function of the invariant mass-squared of the lepton pair $q^2$ to form a universal decay-mode independent function.\n\nExplanation: The correct answer is A) because the paper states that the decay-mode dependent scalar form factor is the only necessary theoretical ingredient for the relations between the differential decay rates of semileptonic decay modes and the underlying quark-level transition $b\\rightarrow c\\tau \\nu$. The relations combine measurable decay rates as a function of the invariant mass-squared of the lepton pair $q^2$ to form a universal decay-mode independent function, which is a key result of the paper. The other options are incorrect because they either omit the role of the scalar form factor or limit the validity of the relations to specific types of mesons."}, "22": {"documentation": {"title": "Robust Inference Using Inverse Probability Weighting", "source": "Xinwei Ma and Jingshen Wang", "docs_id": "1810.11397", "section": ["econ.EM", "math.ST", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust Inference Using Inverse Probability Weighting. Inverse Probability Weighting (IPW) is widely used in empirical work in economics and other disciplines. As Gaussian approximations perform poorly in the presence of \"small denominators,\" trimming is routinely employed as a regularization strategy. However, ad hoc trimming of the observations renders usual inference procedures invalid for the target estimand, even in large samples. In this paper, we first show that the IPW estimator can have different (Gaussian or non-Gaussian) asymptotic distributions, depending on how \"close to zero\" the probability weights are and on how large the trimming threshold is. As a remedy, we propose an inference procedure that is robust not only to small probability weights entering the IPW estimator but also to a wide range of trimming threshold choices, by adapting to these different asymptotic distributions. This robustness is achieved by employing resampling techniques and by correcting a non-negligible trimming bias. We also propose an easy-to-implement method for choosing the trimming threshold by minimizing an empirical analogue of the asymptotic mean squared error. In addition, we show that our inference procedure remains valid with the use of a data-driven trimming threshold. We illustrate our method by revisiting a dataset from the National Supported Work program."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Inverse Probability Weighting (IPW) estimation, what is the primary concern with using ad hoc trimming as a regularization strategy, and how does the proposed inference procedure address this issue?\n\n**A)** Ad hoc trimming can lead to biased estimates due to the loss of information, and the proposed procedure addresses this by employing resampling techniques to correct the trimming bias.\n\n**B)** Ad hoc trimming can result in invalid inference procedures for the target estimand, even in large samples, and the proposed procedure addresses this by adapting to different asymptotic distributions through resampling and trimming correction.\n\n**C)** Ad hoc trimming can lead to non-negligible trimming bias, and the proposed procedure addresses this by employing a data-driven trimming threshold to minimize the asymptotic mean squared error.\n\n**D)** Ad hoc trimming can result in poor Gaussian approximations, and the proposed procedure addresses this by employing non-Gaussian approximations to account for the trimming threshold.\n\n**Correct Answer:** B) Ad hoc trimming can result in invalid inference procedures for the target estimand, even in large samples, and the proposed procedure addresses this by adapting to different asymptotic distributions through resampling and trimming correction.\n\n**Explanation:** The correct answer is B) because the documentation states that ad hoc trimming of the observations renders usual inference procedures invalid for the target estimand, even in large samples. The proposed procedure addresses this issue by adapting to different asymptotic distributions through resampling and trimming correction, as mentioned in the introduction of the paper. The other options are incorrect because they either focus on the wrong aspect of the problem (A and D) or provide an incomplete explanation (C)."}, "23": {"documentation": {"title": "A Search for the Progenitors of Two Type-Ia Supernovae in NGC 1316", "source": "Dan Maoz and Filippo Mannucci", "docs_id": "0801.2898", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Search for the Progenitors of Two Type-Ia Supernovae in NGC 1316. Recent evidence of a young progenitor population for many Type-Ia SNe (SNe-Ia) raises the possibility that evolved intermediate-mass progenitor stars may be detected in pre-explosion images. NGC 1316, a radio galaxy in the Fornax cluster, is a prolific producer of SNe-Ia, with four detected since 1980. We analyze Hubble Space Telescope (HST) pre-explosion images of the sites of two of the SNe-Ia that exploded in this galaxy, SN2006dd (a normal Type-Ia) and SN2006mr (likely a subluminous, 1991bg-like, SN-Ia). Astrometric positions are obtained from optical and near-IR ground-based images of the events. We find no candidate point sources at either location, and set upper limits on the flux in B, V, and I from any such progenitors. We also estimate the amount of extinction that could be present, based on analysis of the surface-brightness inhomogeneities in the HST images themselves. At the distance of NGC 1316, the limits correspond to absolute magnitudes of about -5.5, -5.4, and -6.0 mag in M_B, M_V, and M_I, respectively. Comparison to stellar evolution models argues against the presence at the SN sites, 3 years prior to the explosion, of normal stars with initial masses > 6 M_sun at the tip of their asymptotic-giant branch (AGB) evolution, young post-AGB stars that had initial masses > 4 M_sun, and post-red-giant stars of initial masses > 9 M_sun."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the progenitors of Type-Ia Supernovae (SNe-Ia) in NGC 1316, based on the analysis of Hubble Space Telescope pre-explosion images?\n\nA) The progenitors are likely to be young, post-AGB stars with initial masses > 4 M_sun.\nB) The progenitors are likely to be normal stars with initial masses > 6 M_sun at the tip of their asymptotic-giant branch (AGB) evolution.\nC) The progenitors are likely to be post-red-giant stars of initial masses > 9 M_sun.\nD) The progenitors are unlikely to be detected in pre-explosion images, given the estimated upper limits on their flux.\n\nCorrect Answer: D) The progenitors are unlikely to be detected in pre-explosion images, given the estimated upper limits on their flux.\n\nExplanation: The analysis of Hubble Space Telescope pre-explosion images found no candidate point sources at either location, and set upper limits on the flux in B, V, and I from any such progenitors. This suggests that the progenitors are unlikely to be detected in pre-explosion images, given the estimated upper limits on their flux. The other options are incorrect because the analysis did not provide evidence for the presence of young post-AGB stars, normal stars at the tip of their AGB evolution, or post-red-giant stars with initial masses > 9 M_sun."}, "24": {"documentation": {"title": "Lazy TSO Reachability", "source": "Ahmed Bouajjani, Georgel Calin, Egor Derevenetc, Roland Meyer", "docs_id": "1501.02683", "section": ["cs.PL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lazy TSO Reachability. We address the problem of checking state reachability for programs running under Total Store Order (TSO). The problem has been shown to be decidable but the cost is prohibitive, namely non-primitive recursive. We propose here to give up completeness. Our contribution is a new algorithm for TSO reachability: it uses the standard SC semantics and introduces the TSO semantics lazily and only where needed. At the heart of our algorithm is an iterative refinement of the program of interest. If the program's goal state is SC-reachable, we are done. If the goal state is not SC-reachable, this may be due to the fact that SC under-approximates TSO. We employ a second algorithm that determines TSO computations which are infeasible under SC, and hence likely to lead to new states. We enrich the program to emulate, under SC, these TSO computations. Altogether, this yields an iterative under-approximation that we prove sound and complete for bug hunting, i.e., a semi-decision procedure halting for positive cases of reachability. We have implemented the procedure as an extension to the tool Trencher and compared it to the Memorax and CBMC model checkers."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary contribution of the authors in the paper \"Lazy TSO Reachability\" regarding the Total Store Order (TSO) semantics, and how does it differ from previous approaches to checking state reachability for programs running under TSO?\n\nA) The authors propose a new algorithm that uses a brute-force approach to check all possible TSO computations, which is complete but computationally expensive.\nB) The authors introduce a new semantics, called \"Lazy TSO\", which only approximates the TSO semantics and is used to reduce the computational cost of reachability checking.\nC) The authors propose a new algorithm that uses an iterative refinement of the program of interest, but only checks for SC-reachability, and does not consider the limitations of SC in approximating TSO.\nD) The authors propose a new algorithm that uses a combination of SC and TSO semantics, but only checks for reachability in a limited subset of the program's states.\n\nCorrect Answer: C) The authors propose a new algorithm that uses an iterative refinement of the program of interest, but only checks for SC-reachability, and does not consider the limitations of SC in approximating TSO.\n\nExplanation: The correct answer is C) because the authors' contribution is to introduce a new algorithm that uses an iterative refinement of the program of interest, but only checks for SC-reachability. This approach is sound and complete for bug hunting, but does not attempt to address the limitations of SC in approximating TSO. The authors acknowledge that SC under-approximates TSO, but their algorithm does not try to overcome this limitation. This requires the test-taker to carefully read the introduction and understand the authors' approach to reachability checking."}, "25": {"documentation": {"title": "Statistical inference for time-changed L\\'{e}vy processes via composite\n  characteristic function estimation", "source": "Denis Belomestny", "docs_id": "1003.0275", "section": ["stat.ME", "math.ST", "stat.AP", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Statistical inference for time-changed L\\'{e}vy processes via composite\n  characteristic function estimation. In this article, the problem of semi-parametric inference on the parameters of a multidimensional L\\'{e}vy process $L_t$ with independent components based on the low-frequency observations of the corresponding time-changed L\\'{e}vy process $L_{\\mathcal{T}(t)}$, where $\\mathcal{T}$ is a nonnegative, nondecreasing real-valued process independent of $L_t$, is studied. We show that this problem is closely related to the problem of composite function estimation that has recently gotten much attention in statistical literature. Under suitable identifiability conditions, we propose a consistent estimate for the L\\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise convergence rates of the estimate proposed. Moreover, we prove that the rates obtained are optimal in a minimax sense over suitable classes of time-changed L\\'{e}vy models. Finally, we present a simulation study showing the performance of our estimation algorithm in the case of time-changed Normal Inverse Gaussian (NIG) L\\'{e}vy processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a time-changed L\u00e9vy process $L_{\\mathcal{T}(t)}$ with independent components, where $\\mathcal{T}$ is a nonnegative, nondecreasing real-valued process independent of $L_t$. Suppose we have a sample of low-frequency observations of $L_{\\mathcal{T}(t)}$, denoted by $\\{X_i\\}_{i=1}^n$, and we want to estimate the L\u00e9vy density of $L_t$ using composite characteristic function estimation.\n\n**A)** Under the assumption that $\\mathcal{T}(t)$ is a deterministic function of $t$, the composite characteristic function estimator is consistent and has a pointwise convergence rate of $O(n^{-1/2})$.\n\n**B)** If the time-changed L\u00e9vy process $L_{\\mathcal{T}(t)}$ has independent and identically distributed (i.i.d.) components, then the composite characteristic function estimator is asymptotically normal with a mean of 0 and a variance of $\\sigma^2$, where $\\sigma^2$ is the variance of the L\u00e9vy density of $L_t$.\n\n**C)** The composite characteristic function estimator is minimax optimal over the class of time-changed L\u00e9vy processes with independent components, with a minimax risk of $O(n^{-1/2})$.\n\n**D)** If the time-changed L\u00e9vy process $L_{\\mathcal{T}(t)}$ has a finite-dimensional distribution, then the composite characteristic function estimator is consistent and has a uniform convergence rate of $O(n^{-1/2})$.\n\n**Correct Answer:** C) The composite characteristic function estimator is minimax optimal over the class of time-changed L\u00e9vy processes with independent components, with a minimax risk of $O(n^{-1/2})$.\n\n**Explanation:** The correct answer is C) because the article states that the rates obtained are optimal in a minimax sense over suitable classes of time-changed L\u00e9vy models. This means that the composite characteristic function estimator has a minimax risk of $O(n^{-1/2})$, which is the optimal rate of convergence for this type of problem. The other options are incorrect because they do not accurately reflect the results of the article. Option A is incorrect because the article does not assume that $\\mathcal{T}(t)$ is deterministic. Option B is incorrect because the article does not assume that the components of the time-changed L\u00e9vy process are i.i.d. Option D is incorrect because the article does not assume that the time-changed L\u00e9vy process has a finite-dimensional distribution."}, "26": {"documentation": {"title": "Interactive Multi-level Stroke Control for Neural Style Transfer", "source": "Max Reimann and Benito Buchheim and Amir Semmo and J\\\"urgen D\\\"ollner\n  and Matthias Trapp", "docs_id": "2106.13787", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interactive Multi-level Stroke Control for Neural Style Transfer. We present StyleTune, a mobile app for interactive multi-level control of neural style transfers that facilitates creative adjustments of style elements and enables high output fidelity. In contrast to current mobile neural style transfer apps, StyleTune supports users to adjust both the size and orientation of style elements, such as brushstrokes and texture patches, on a global as well as local level. To this end, we propose a novel stroke-adaptive feed-forward style transfer network, that enables control over stroke size and intensity and allows a larger range of edits than current approaches. For additional level-of-control, we propose a network agnostic method for stroke-orientation adjustment by utilizing the rotation-variance of CNNs. To achieve high output fidelity, we further add a patch-based style transfer method that enables users to obtain output resolutions of more than 20 Megapixel. Our approach empowers users to create many novel results that are not possible with current mobile neural style transfer apps."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed stroke-adaptive feed-forward style transfer network in StyleTune, and how does it enable users to create novel results that are not possible with current mobile neural style transfer apps?\n\nA) It allows for global adjustments to style elements, such as brushstrokes and texture patches, on a local level.\nB) It enables control over stroke size and intensity, allowing for a larger range of edits than current approaches.\nC) It facilitates the adjustment of style elements on a global as well as local level, with a focus on stroke orientation.\nD) It enables users to obtain output resolutions of more than 20 Megapixel, but does not provide control over stroke size and intensity.\n\nCorrect Answer: B) It enables control over stroke size and intensity, allowing for a larger range of edits than current approaches.\n\nExplanation: The question requires the test-taker to understand the key contribution of the proposed stroke-adaptive feed-forward style transfer network in StyleTune. The correct answer highlights the network's ability to control stroke size and intensity, which is a novel feature that enables users to create more complex and nuanced edits. The incorrect options either focus on global adjustments or output resolutions, which are not the primary advantages of the proposed network."}, "27": {"documentation": {"title": "Neutron star matter equation of state including $d^*$-hexaquark degrees\n  of freedom", "source": "A. Mantziris and A. Pastore and I. Vida\\~na and D. P. Watts and M.\n  Bashkanov and A. M. Romero", "docs_id": "2002.06571", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Neutron star matter equation of state including $d^*$-hexaquark degrees\n  of freedom. We present an extension of a previous work where, assuming a simple free bosonic gas supplemented with a relativistic meand field model to describe the pure nucleonic part of the EoS, we studied the consequences that the first non-trivial hexaquark $d^*$(2380) could have on the properties of neutron stars. Compared to that exploratory work we employ a standard non-linear Walecka model including additional terms that describe the interaction of the $d^*(2380)$ di-baryon with the other particles of the system through the exchange of $\\sigma$- and $\\omega$-meson fields. Our results have show that the presence of the $d^*(2380)$ leads to maximum masses compatible with the recent observations of $\\sim 2$M$_\\odot$ millisecond pulsars if the interaction of the $d^*(2380)$ is slightly repulsive or the $d^*(2380)$ does not interacts at all. An attractive interaction makes the equation of state too soft to be able to support a $2$M$_\\odot$ neutron star whereas an extremely repulsive one induces the collapse of the neutron star into a black hole as soon as the $d^*(2380)$ appears."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study on neutron star matter equation of state including $d^*$-hexaquark degrees of freedom investigate the impact of the first non-trivial hexaquark $d^*(2380)$ on the properties of neutron stars. According to the results, what is the consequence of the interaction of the $d^*(2380)$ with other particles in the system?\n\n**A)** The presence of the $d^*(2380)$ leads to a significant increase in the maximum mass of neutron stars, making them even more massive than the observed $\\sim 2$M$_\\odot$ millisecond pulsars.\n\n**B)** The interaction of the $d^*(2380)$ with other particles results in a soft equation of state, making it impossible to support a $2$M$_\\odot$ neutron star.\n\n**C)** The $d^*(2380)$ does not interact with other particles in the system, leading to a stable neutron star with a maximum mass compatible with the observed $\\sim 2$M$_\\odot$ millisecond pulsars.\n\n**D)** An attractive interaction between the $d^*(2380)$ and other particles in the system leads to the collapse of the neutron star into a black hole as soon as the $d^*(2380)$ appears.\n\n**Correct Answer:** D) An attractive interaction between the $d^*(2380)$ and other particles in the system leads to the collapse of the neutron star into a black hole as soon as the $d^*(2380)$ appears.\n\n**Explanation:** The correct answer is D) because the text states that \"An attractive interaction makes the equation of state too soft to be able to support a $2$M$_\\odot$ neutron star whereas an extremely repulsive one induces the collapse of the neutron star into a black hole as soon as the $d^*(2380)$ appears.\" This indicates that an attractive interaction between the $d^*(2380)$ and other particles leads to the collapse of the neutron star, making option D the correct answer."}, "28": {"documentation": {"title": "Learning Latent Dynamics for Partially-Observed Chaotic Systems", "source": "Said Ouala, Duong Nguyen, Lucas Drumetz, Bertrand Chapron, Ananda\n  Pascual, Fabrice Collard, Lucile Gaultier and Ronan Fablet", "docs_id": "1907.02452", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Latent Dynamics for Partially-Observed Chaotic Systems. This paper addresses the data-driven identification of latent dynamical representations of partially-observed systems, i.e., dynamical systems for which some components are never observed, with an emphasis on forecasting applications, including long-term asymptotic patterns. Whereas state-of-the-art data-driven approaches rely on delay embeddings and linear decompositions of the underlying operators, we introduce a framework based on the data-driven identification of an augmented state-space model using a neural-network-based representation. For a given training dataset, it amounts to jointly learn an ODE (Ordinary Differential Equation) representation in the latent space and reconstructing latent states. Through numerical experiments, we demonstrate the relevance of the proposed framework w.r.t. state-of-the-art approaches in terms of short-term forecasting performance and long-term behaviour. We further discuss how the proposed framework relates to Koopman operator theory and Takens' embedding theorem."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary contribution of the proposed framework in the paper \"Learning Latent Dynamics for Partially-Observed Chaotic Systems\" in terms of its approach to data-driven identification of latent dynamical representations?\n\n**A)** The framework introduces a novel method for linear decompositions of the underlying operators, which is based on delay embeddings.\n\n**B)** The framework proposes a data-driven identification of an augmented state-space model using a neural-network-based representation, which jointly learns an ODE (Ordinary Differential Equation) representation in the latent space and reconstructs latent states.\n\n**C)** The framework relies on Koopman operator theory and Takens' embedding theorem to establish a connection between the proposed framework and the existing literature on dynamical systems.\n\n**D)** The framework is limited to short-term forecasting applications and does not consider long-term asymptotic patterns.\n\n**Correct Answer:** B) The framework proposes a data-driven identification of an augmented state-space model using a neural-network-based representation, which jointly learns an ODE (Ordinary Differential Equation) representation in the latent space and reconstructs latent states.\n\n**Explanation:** The correct answer is B) because the paper explicitly states that the proposed framework introduces a novel approach to data-driven identification of latent dynamical representations, which involves jointly learning an ODE representation in the latent space and reconstructing latent states. This is in contrast to state-of-the-art approaches, which rely on delay embeddings and linear decompositions of the underlying operators."}, "29": {"documentation": {"title": "FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven\n  Deep Reinforcement Learning in Quantitative Finance", "source": "Xiao-Yang Liu, Jingyang Rui, Jiechao Gao, Liuqing Yang, Hongyang Yang,\n  Zhaoran Wang, Christina Dan Wang, Jian Guo", "docs_id": "2112.06753", "section": ["q-fin.TR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven\n  Deep Reinforcement Learning in Quantitative Finance. Deep reinforcement learning (DRL) has shown huge potentials in building financial market simulators recently. However, due to the highly complex and dynamic nature of real-world markets, raw historical financial data often involve large noise and may not reflect the future of markets, degrading the fidelity of DRL-based market simulators. Moreover, the accuracy of DRL-based market simulators heavily relies on numerous and diverse DRL agents, which increases demand for a universe of market environments and imposes a challenge on simulation speed. In this paper, we present a FinRL-Meta framework that builds a universe of market environments for data-driven financial reinforcement learning. First, FinRL-Meta separates financial data processing from the design pipeline of DRL-based strategy and provides open-source data engineering tools for financial big data. Second, FinRL-Meta provides hundreds of market environments for various trading tasks. Third, FinRL-Meta enables multiprocessing simulation and training by exploiting thousands of GPU cores. Our codes are available online at https://github.com/AI4Finance-Foundation/FinRL-Meta."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge faced by deep reinforcement learning (DRL) in building financial market simulators, according to the paper \"FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven Deep Reinforcement Learning in Quantitative Finance\"?\n\n**A)** The complexity of real-world markets makes it difficult to design DRL agents that can generalize to new situations.\n**B)** The accuracy of DRL-based market simulators relies heavily on the quality of the historical financial data used to train the agents.\n**C)** The need for a universe of market environments and the requirement for numerous and diverse DRL agents impose a challenge on simulation speed and fidelity.\n**D)** The use of GPU cores for multiprocessing simulation and training is the primary challenge faced by DRL in financial markets.\n\n**Correct Answer:** C) The need for a universe of market environments and the requirement for numerous and diverse DRL agents impose a challenge on simulation speed and fidelity.\n\n**Explanation:** The paper highlights that the accuracy of DRL-based market simulators heavily relies on numerous and diverse DRL agents, which increases demand for a universe of market environments. This is a significant challenge because it requires a large number of simulations and training data, which can be time-consuming and computationally expensive. The correct answer, C, accurately reflects this challenge. The other options are incorrect because they do not accurately capture the primary challenge faced by DRL in building financial market simulators."}, "30": {"documentation": {"title": "Quantum motion and level quantization of a skyrmion in a pinning\n  potential in chiral magnets", "source": "Shi-Zeng Lin and Lev N. Bulaevskii", "docs_id": "1304.7322", "section": ["cond-mat.str-el", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantum motion and level quantization of a skyrmion in a pinning\n  potential in chiral magnets. A new topological excitation called skyrmion has been observed experimentally in chiral magnets without spatial inversion symmetry. The dynamics of a skyrmion is equivalent to an electron moving in a strong magnetic field. As a skyrmion involves large number of spins, it is not clear whether there exist observable quantum effects. In this work, we study the quantum dynamics of a skyrmion in a pinning potential. Without a pinning potential, the skyrmion performs cyclotron motion due to the strong emergent magnetic field originating from the Berry phase of spins, and all skyrmions occupy the lowest Landau level. Their wave functions are strongly localized in a region with radius less than $1\\ \\AA$ when no driving force is applied. Thus in most circumstances, the quantum effects of a skyrmion are weak. In the presence of a pinning potential, the lowest Landau level for skyrmions is split into quantized levels, characterized by the orbital moments. The transition between quantized levels can be observed experimentally by microwave absorption measurements in low temperature region. The quantum effects are more prominent for a skyrmion with a small size, which can be realized in magnets with a large Dzyaloshinskii-Moriya interaction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the quantum motion and level quantization of a skyrmion in a pinning potential in chiral magnets, what is the primary effect of the pinning potential on the lowest Landau level for skyrmions?\n\nA) It causes the skyrmion to occupy a higher Landau level.\nB) It splits the lowest Landau level into quantized levels, characterized by the orbital moments.\nC) It eliminates the cyclotron motion of the skyrmion due to the strong emergent magnetic field.\nD) It increases the radius of the skyrmion's wave function.\n\n**Correct Answer:** B) It splits the lowest Landau level into quantized levels, characterized by the orbital moments.\n\n**Explanation:** According to the documentation, the presence of a pinning potential splits the lowest Landau level for skyrmions into quantized levels, characterized by the orbital moments. This is a key finding in the study of quantum dynamics of skyrmions in chiral magnets. The other options are incorrect: the pinning potential does not cause the skyrmion to occupy a higher Landau level (A), eliminate cyclotron motion (C), or increase the radius of the skyrmion's wave function (D)."}, "31": {"documentation": {"title": "Love Thy Neighbor? Perceived Community Abidance and Private Compliance\n  to COVID-19 Norms in India", "source": "Upasak Das, Prasenjit Sarkhel, Sania Ashraf", "docs_id": "2010.12350", "section": ["econ.GN", "physics.soc-ph", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Love Thy Neighbor? Perceived Community Abidance and Private Compliance\n  to COVID-19 Norms in India. Compliance with measures like social distancing, hand-washing and wearing masks have emerged as the dominant strategy to combat health risk from the COVID-19 pandemic. These behaviors are often argued to be pro-social, where one must incur private cost to benefit or protect others. Using self-reported data across India (n=934) through online survey, we assess if changes in perceived community compliance can predict changes in individual compliance behavior, controlling for the potential confounders. We observe statistically significant and positive relationship between the two, even after accounting for omitted variable bias, plausibly allowing us to view the results from a plausible causal lens. Further, we find subsequent lockdowns such as the ones imposed in India, have a detrimental effect on individual compliance though the gains from higher perceived community compliance seems to offset this loss. We also find that sensitization through community can be particularly effective for people with pre-existing co-morbidities. Our findings underscore the need for multi-level behavioral interventions involving local actors and community institutions to sustain private compliance during the pandemic."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The study on \"Love Thy Neighbor? Perceived Community Abidance and Private Compliance to COVID-19 Norms in India\" suggests that perceived community compliance has a positive relationship with individual compliance behavior. However, the study also finds that subsequent lockdowns have a detrimental effect on individual compliance. What is the most plausible explanation for this seeming paradox, and how do the findings of the study inform the development of multi-level behavioral interventions to sustain private compliance during the pandemic?\n\n**A)** The positive relationship between perceived community compliance and individual compliance is due to social norms, where individuals are more likely to comply with norms when they perceive others around them to be doing so. However, lockdowns disrupt these social norms, leading to a decline in individual compliance.\n\n**B)** The positive relationship between perceived community compliance and individual compliance is due to the fact that individuals with pre-existing co-morbidities are more susceptible to the effects of lockdowns, and therefore, the gains from higher perceived community compliance are more pronounced for this group.\n\n**C)** The positive relationship between perceived community compliance and individual compliance is due to the fact that individuals are more likely to comply with norms when they perceive others around them to be doing so, but the detrimental effect of lockdowns on individual compliance is due to the fact that individuals are more likely to comply with norms when they feel a sense of community and social connection.\n\n**D)** The positive relationship between perceived community compliance and individual compliance is due to the fact that individuals are more likely to comply with norms when they perceive others around them to be doing so, but the detrimental effect of lockdowns on individual compliance is due to the fact that individuals are more likely to comply with norms when they feel a sense of community and social connection, and that this effect is particularly pronounced for individuals with pre-existing co-morbidities.\n\n**Correct Answer:** C) The positive relationship between perceived community compliance and individual compliance is due to the fact that individuals are more likely to comply with norms when they perceive others around them to be doing so, but the detrimental effect of lockdowns on individual compliance is due to the fact that individuals are more likely to comply with norms when they feel a sense of community and social connection.\n\n**Explanation:** This answer choice correctly captures the essence of the study's findings. The positive relationship between perceived community compliance and individual compliance is due to social norms, where individuals are more likely to comply with norms when they perceive others around them to be doing so. However, the detrimental effect of lockdowns on individual compliance is due to the fact that individuals are more likely to comply with norms when they feel a sense of community and social connection. This answer choice also correctly highlights the importance of community and social connection in sustaining private compliance during the pandemic."}, "32": {"documentation": {"title": "Hamilton--Jacobi meet M\\\"obius", "source": "Alon E. Faraggi and Marco Matone", "docs_id": "1503.01286", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hamilton--Jacobi meet M\\\"obius. Adaptation of the Hamilton--Jacobi formalism to quantum mechanics leads to a cocycle condition, which is invariant under $D$--dimensional M\\\"obius transformations with Euclidean or Minkowski metrics. In this paper we aim to provide a pedagogical presentation of the proof of the M\\\"obius symmetry underlying the cocycle condition. The M\\\"obius symmetry implies energy quantization and undefinability of quantum trajectories, without assigning any prior interpretation to the wave function. As such, the Hamilton--Jacobi formalism, augmented with the global M\\\"obius symmetry, provides an alternative starting point, to the axiomatic probability interpretation of the wave function, for the formulation of quantum mechanics and the quantum spacetime. The M\\\"obius symmetry can only be implemented consistently if spatial space is compact, and correspondingly if there exist a finite ultraviolet length scale. Evidence for non--trivial space topology may exist in the cosmic microwave background radiation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the Hamilton-Jacobi formalism adapted to quantum mechanics, which leads to a cocycle condition invariant under $D$-dimensional M\\\"obius transformations with Euclidean or Minkowski metrics. What is the underlying symmetry of this cocycle condition, and what implications does it have for the formulation of quantum mechanics and the quantum spacetime?\n\nA) The underlying symmetry is the Lorentz group, and it implies that the wave function is undefined for non-trivial space topology.\n\nB) The underlying symmetry is the M\\\"obius group, and it implies that energy quantization is a fundamental aspect of quantum mechanics.\n\nC) The underlying symmetry is the Poincar\u00e9 group, and it implies that the wave function is a local function of the spacetime coordinates.\n\nD) The underlying symmetry is the Galilean group, and it implies that the wave function is a global function of the spacetime coordinates.\n\nCorrect Answer: B) The underlying symmetry is the M\\\"obius group, and it implies that energy quantization is a fundamental aspect of quantum mechanics.\n\nExplanation: The correct answer is B) because the M\\\"obius symmetry is the underlying symmetry of the cocycle condition, which implies energy quantization and undefinability of quantum trajectories. The other options are incorrect because they do not accurately describe the underlying symmetry of the cocycle condition. Option A is incorrect because the Lorentz group is not the underlying symmetry of the cocycle condition, but rather a related group. Option C is incorrect because the Poincar\u00e9 group is not the underlying symmetry of the cocycle condition, but rather a group that includes the Lorentz group. Option D is incorrect because the Galilean group is not the underlying symmetry of the cocycle condition, and it does not imply energy quantization."}, "33": {"documentation": {"title": "Uniform bounds for invariant subspace perturbations", "source": "Anil Damle and Yuekai Sun", "docs_id": "1905.07865", "section": ["math.NA", "cs.NA", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniform bounds for invariant subspace perturbations. For a fixed symmetric matrix A and symmetric perturbation E we develop purely deterministic bounds on how invariant subspaces of A and A+E can differ when measured by a suitable \"row-wise\" metric rather than via traditional measures of subspace distance. Understanding perturbations of invariant subspaces with respect to such metrics is becoming increasingly important across a wide variety of applications and therefore necessitates new theoretical developments. Under minimal assumptions we develop new bounds on subspace perturbations under the two-to-infinity matrix norm and show in what settings these row-wise differences in the invariant subspaces can be significantly smaller than the analogous two or Frobenius norm differences. We also demonstrate that the constitutive pieces of our bounds are necessary absent additional assumptions and, therefore, our results provide a natural starting point for further analysis of specific problems. Lastly, we briefly discuss extensions of our bounds to scenarios where A and/or E are non-normal matrices."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a symmetric matrix A and a symmetric perturbation E. Suppose we want to develop bounds on the difference between the invariant subspaces of A and A+E, measured by the two-to-infinity matrix norm. What is the necessary condition for the row-wise metric to be a suitable choice for bounding the difference between the invariant subspaces of A and A+E?\n\nA) The matrix A must be invertible.\nB) The matrix A must be diagonalizable.\nC) The matrix A must have a specific eigenvalue structure.\nD) The matrix A must satisfy a certain spectral condition.\n\nCorrect Answer: D) The matrix A must satisfy a certain spectral condition.\n\nExplanation: The correct answer is D) The matrix A must satisfy a certain spectral condition. The documentation states that the constitutive pieces of the bounds are necessary absent additional assumptions, implying that the spectral condition of the matrix A is crucial in developing the bounds. The other options are not mentioned in the documentation as necessary conditions for the row-wise metric to be a suitable choice. \n\nNote: The documentation does not explicitly state the spectral condition, but it implies that it is a necessary condition for the row-wise metric to be a suitable choice. Therefore, the correct answer requires the test-taker to understand the underlying assumptions and implications of the documentation."}, "34": {"documentation": {"title": "Whirling skirts and rotating cones", "source": "Jemal Guven, J A Hanna, Martin Michael M\\\"uller", "docs_id": "1306.2619", "section": ["physics.class-ph", "cond-mat.soft", "math-ph", "math.MP", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Whirling skirts and rotating cones. Steady, dihedrally symmetric patterns with sharp peaks may be observed on a spinning skirt, lagging behind the material flow of the fabric. These qualitative features are captured with a minimal model of traveling waves on an inextensible, flexible, generalized-conical sheet rotating about a fixed axis. Conservation laws are used to reduce the dynamics to a quadrature describing a particle in a three-parameter family of potentials. One parameter is associated with the stress in the sheet, aNoether is the current associated with rotational invariance, and the third is a Rossby number which indicates the relative strength of Coriolis forces. Solutions are quantized by enforcing a topology appropriate to a skirt and a particular choice of dihedral symmetry. A perturbative analysis of nearly axisymmetric cones shows that Coriolis effects are essential in establishing skirt-like solutions. Fully non-linear solutions with three-fold symmetry are presented which bear a suggestive resemblance to the observed patterns."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a spinning, flexible, generalized-conical sheet rotating about a fixed axis. The sheet is subject to a three-parameter family of potentials, where one parameter represents the stress in the sheet, one parameter is associated with the current associated with rotational invariance, and the third parameter is the Rossby number indicating the relative strength of Coriolis forces.\n\nA) The Rossby number is a measure of the strength of the Coriolis force relative to the inertial force, and its value determines the stability of the sheet's rotation.\n\nB) The sheet's rotation is governed by a quadrature describing a particle in a three-parameter family of potentials, where the parameters represent the stress in the sheet, the current associated with rotational invariance, and the Rossby number.\n\nC) The sheet's rotation is a manifestation of the Noether current associated with rotational invariance, and the Rossby number determines the sheet's tendency to break into a skirt-like pattern.\n\nD) The sheet's rotation is a result of the interplay between the stress in the sheet, the current associated with rotational invariance, and the Rossby number, which determines the sheet's tendency to form a rotating cone.\n\nCorrect Answer: B) The sheet's rotation is governed by a quadrature describing a particle in a three-parameter family of potentials, where the parameters represent the stress in the sheet, the current associated with rotational invariance, and the Rossby number.\n\nExplanation: The correct answer is B) because the documentation states that the sheet's rotation is governed by a quadrature describing a particle in a three-parameter family of potentials, where the parameters represent the stress in the sheet, the current associated with rotational invariance, and the Rossby number. This is a direct quote from the documentation, and it accurately describes the governing equation for the sheet's rotation. The other options are incorrect because they either misrepresent the relationship between the parameters (A and D) or introduce new concepts not mentioned in the documentation (C)."}, "35": {"documentation": {"title": "Nested Pseudo Likelihood Estimation of Continuous-Time Dynamic Discrete\n  Games", "source": "Jason R. Blevins and Minhae Kim", "docs_id": "2108.02182", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nested Pseudo Likelihood Estimation of Continuous-Time Dynamic Discrete\n  Games. We introduce a sequential estimator for continuous time dynamic discrete choice models (single-agent models and games) by adapting the nested pseudo likelihood (NPL) estimator of Aguirregabiria and Mira (2002, 2007), developed for discrete time models with discrete time data, to the continuous time case with data sampled either discretely (i.e., uniformly-spaced snapshot data) or continuously. We establish conditions for consistency and asymptotic normality of the estimator, a local convergence condition, and, for single agent models, a zero Jacobian property assuring local convergence. We carry out a series of Monte Carlo experiments using an entry-exit game with five heterogeneous firms to confirm the large-sample properties and demonstrate finite-sample bias reduction via iteration. In our simulations we show that the convergence issues documented for the NPL estimator in discrete time models are less likely to affect comparable continuous-time models. We also show that there can be large bias in economically-relevant parameters, such as the competitive effect and entry cost, from estimating a misspecified discrete time model when in fact the data generating process is a continuous time model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a continuous-time dynamic discrete choice model with a single agent, where the agent's choice is determined by a continuous-time process with a time-varying probability of choosing one of two alternatives. The model is misspecified, with a discrete-time likelihood function that assumes a uniform distribution over the two alternatives. Under what conditions, if any, would the Nested Pseudo Likelihood (NPL) estimator be consistent and asymptotically normal for estimating the model's parameters, and what are the implications of this result for the estimation of economically-relevant parameters such as the competitive effect and entry cost?\n\n**A)** The NPL estimator would be consistent and asymptotically normal if the model's data is sampled continuously, and the misspecification is only in the discrete-time likelihood function. However, this would lead to large bias in the estimated parameters.\n\n**B)** The NPL estimator would be consistent and asymptotically normal if the model's data is sampled discretely, and the misspecification is only in the continuous-time process. However, this would lead to convergence issues.\n\n**C)** The NPL estimator would be consistent and asymptotically normal if the model's data is sampled continuously, and the misspecification is only in the continuous-time process. Additionally, the zero Jacobian property would ensure local convergence, and the local convergence condition would ensure that the estimator is consistent and asymptotically normal.\n\n**D)** The NPL estimator would be consistent and asymptotically normal if the model's data is sampled discretely, and the misspecification is only in the continuous-time process. However, this would lead to large bias in the estimated parameters.\n\n**Correct Answer:** C) The NPL estimator would be consistent and asymptotically normal if the model's data is sampled continuously, and the misspecification is only in the continuous-time process. Additionally, the zero Jacobian property would ensure local convergence, and the local convergence condition would ensure that the estimator is consistent and asymptotically normal.\n\n**Explanation:** The correct answer is based on the conditions established in the documentation for the NPL estimator. The zero Jacobian property ensures local convergence, and the local convergence condition ensures that the estimator is consistent and asymptotically normal. The documentation also states that the convergence issues documented for the NPL estimator in discrete time models are less likely to affect comparable continuous-time models. Therefore, the correct answer is C."}, "36": {"documentation": {"title": "A 20 Gbps Data Transmitting ASIC with PAM4 for Particle Physics\n  Experiments", "source": "Li Zhang (1,2), Datao Gong (1), Suen Hou (3), Guanming Huang (2), Xing\n  Huang (2), Chonghan Liu (1), Tiankuan Liu (1), Hanhan Sun (1,2), Quan Sun\n  (4), Xiangming Sun (2), Wei Zhang (1,2) and Jingbo Ye (1) ((1) Southern\n  Methodist University, (2) Central China Normal University, (3) Academia\n  Sinica, Nangang (4) Fermi National Accelerator Laboratory)", "docs_id": "2010.16064", "section": ["physics.ins-det", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A 20 Gbps Data Transmitting ASIC with PAM4 for Particle Physics\n  Experiments. We present the design principle and test results of a data transmitting ASIC, GBS20, for particle physics experiments. The goal of GBS20 will be an ASIC that employs two serializers each from the 10.24 Gbps lpGBT SerDes, sharing the PLL also from lpGBT. A PAM4 encoder plus a VCSEL driver will be implemented in the same die to use the same clock system, eliminating the need of CDRs in the PAM4 encoder. This way the transmitter module, GBT20, developed using the GBS20 ASIC, will have the exact lpGBT data interface and transmission protocol, with an output up to 20.48 Gbps over one fiber. With PAM4 embedded FPGAs at the receiving end, GBT20 will halve the fibers needed in a system and better use the input bandwidth of the FPGA. A prototype, GBS20v0 is fabricated using a commercial 65 nm CMOS technology. This prototype has two serializers and a PAM4 encoder sharing the lpGBT PLL, but no user data input. An internal PRBS generator provides data to the serializers. GBS20v0 is tested barely up to 20.48 Gbps. With lessons learned from this prototype, we are designing the second prototype, GBS20v1, that will have 16 user data input channels each at 1.28 Gbps. We present the design concept of the GBS20 ASIC and the GBT20 module, the preliminary test results, and lessons learned from GBS20v0 and the design of GBS20v1 which will be not only a test chip but also a user chip with 16 input data channels."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a PAM4 encoder and VCSEL driver on the same die as the serializers in the GBS20 ASIC, and how does this impact the overall system design?\n\n**A)** The PAM4 encoder and VCSEL driver reduce the power consumption of the ASIC, allowing for more efficient transmission over longer distances.\n**B)** The PAM4 encoder and VCSEL driver enable the use of the same clock system, eliminating the need for CDRs in the PAM4 encoder and reducing the overall system complexity.\n**C)** The PAM4 encoder and VCSEL driver increase the data transmission rate to 20.48 Gbps, allowing for faster data transfer between the transmitter and receiver modules.\n**D)** The PAM4 encoder and VCSEL driver reduce the number of fibers required in the system, but do not impact the overall system complexity.\n\n**Correct Answer:** B) The PAM4 encoder and VCSEL driver enable the use of the same clock system, eliminating the need for CDRs in the PAM4 encoder and reducing the overall system complexity.\n\n**Explanation:** The correct answer is B) because the text states that the PAM4 encoder and VCSEL driver will \"use the same clock system, eliminating the need of CDRs in the PAM4 encoder.\" This is a key advantage of using this design approach, as it simplifies the system and reduces complexity. The other options are incorrect because they do not accurately reflect the benefits of using a PAM4 encoder and VCSEL driver on the same die as the serializers."}, "37": {"documentation": {"title": "Boundary of Distribution Support Generator (BDSG): Sample Generation on\n  the Boundary", "source": "Nikolaos Dionelis", "docs_id": "2107.09950", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Boundary of Distribution Support Generator (BDSG): Sample Generation on\n  the Boundary. Generative models, such as Generative Adversarial Networks (GANs), have been used for unsupervised anomaly detection. While performance keeps improving, several limitations exist particularly attributed to difficulties at capturing multimodal supports and to the ability to approximate the underlying distribution closer to the tails, i.e. the boundary of the distribution's support. This paper proposes an approach that attempts to alleviate such shortcomings. We propose an invertible-residual-network-based model, the Boundary of Distribution Support Generator (BDSG). GANs generally do not guarantee the existence of a probability distribution and here, we use the recently developed Invertible Residual Network (IResNet) and Residual Flow (ResFlow), for density estimation. These models have not yet been used for anomaly detection. We leverage IResNet and ResFlow for Out-of-Distribution (OoD) sample detection and for sample generation on the boundary using a compound loss function that forces the samples to lie on the boundary. The BDSG addresses non-convex support, disjoint components, and multimodal distributions. Results on synthetic data and data from multimodal distributions, such as MNIST and CIFAR-10, demonstrate competitive performance compared to methods from the literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of Generative Adversarial Networks (GANs) in anomaly detection, and how does the proposed Boundary of Distribution Support Generator (BDSG) model address this limitation?\n\n**A)** GANs are unable to capture multimodal supports due to their inability to approximate the underlying distribution closer to the tails, and BDSG addresses this by using a compound loss function that forces samples to lie on the boundary.\n\n**B)** GANs are unable to approximate the underlying distribution due to their non-convex support, and BDSG addresses this by using an invertible-residual-network-based model.\n\n**C)** GANs are unable to capture disjoint components of multimodal distributions, and BDSG addresses this by using a compound loss function that forces samples to lie on the boundary.\n\n**D)** GANs are unable to approximate the underlying distribution due to their inability to handle out-of-distribution samples, and BDSG addresses this by using Residual Flow (ResFlow) for density estimation.\n\n**Correct Answer:** A) GANs are unable to capture multimodal supports due to their inability to approximate the underlying distribution closer to the tails, and BDSG addresses this by using a compound loss function that forces samples to lie on the boundary.\n\n**Explanation:** The correct answer is A) because the documentation states that GANs have difficulties capturing multimodal supports and approximating the underlying distribution closer to the tails, which is the boundary of the distribution's support. The BDSG model addresses this limitation by using a compound loss function that forces samples to lie on the boundary, as mentioned in the introduction of the paper."}, "38": {"documentation": {"title": "The Future of Nuclear Energy: Facts and Fiction Chapter IV: Energy from\n  Breeder Reactors and from Fusion?", "source": "Michael Dittmar (ETH Zurich)", "docs_id": "0911.2628", "section": ["physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Future of Nuclear Energy: Facts and Fiction Chapter IV: Energy from\n  Breeder Reactors and from Fusion?. The accumulated knowledge and the prospects for commercial energy production from fission breeder and fusion reactors are analyzed in this report. The publicly available data from past experimental breeder reactors indicate that a large number of unsolved technological problems exist and that the amount of \"created\" fissile material, either from the U238 --> Pu239 or from the Th232 --> U233 cycle, is still far below the breeder requirements and optimistic theoretical expectations. Thus huge efforts, including many basic research questions with an uncertain outcome, are needed before a large commercial breeder prototype can be designed. Even if such efforts are undertaken by the technologically most advanced countries, it will take several decades before such a prototype can be constructed. We conclude therefore, that ideas about near-future commercial fission breeder reactors are nothing but wishful thinking. We further conclude that, no matter how far into the future we may look, nuclear fusion as an energy source is even less probable than large-scale breeder reactors, for the accumulated knowledge on this subject is already sufficient to say that commercial fusion power will never become a reality."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the main conclusions drawn by the authors regarding the prospects for commercial energy production from fission breeder and fusion reactors, and what are the implications of these conclusions for the development of these technologies?\n\nA) The authors conclude that commercial fission breeder reactors are feasible in the near future, and that fusion power will become a reality within the next few decades. (Candidate A)\nB) The authors conclude that large-scale breeder reactors are unlikely to become a commercial reality due to significant technological challenges, and that fusion power is even less probable. (Candidate B)\nC) The authors conclude that the development of fission breeder reactors will be hindered by the lack of basic research, and that fusion power will become a reality due to advancements in materials science. (Candidate C)\nD) The authors conclude that commercial fission breeder reactors are feasible in the near future, but that fusion power will require significant breakthroughs in plasma physics. (Candidate D)\n\nCorrect Answer: B) The authors conclude that large-scale breeder reactors are unlikely to become a commercial reality due to significant technological challenges, and that fusion power is even less probable.\n\nExplanation: The correct answer is based on the text, which states that \"we conclude therefore, that ideas about near-future commercial fission breeder reactors are nothing but wishful thinking\" and that \"no matter how far into the future we may look, nuclear fusion as an energy source is even less probable than large-scale breeder reactors\". This indicates that the authors are skeptical about the feasibility of commercial fission breeder reactors and even more skeptical about the possibility of commercial fusion power."}, "39": {"documentation": {"title": "q-Viscous Burgers' Equation: Dynamical Symmetry, Shock Solitons and\n  q-Semiclassical Expansion", "source": "Sengul Nalci Tumer and Oktay K. Pashaev", "docs_id": "1707.01737", "section": ["nlin.SI", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "q-Viscous Burgers' Equation: Dynamical Symmetry, Shock Solitons and\n  q-Semiclassical Expansion. We propose new type of $q$-diffusive heat equation with nonsymmetric $q$-extension of the diffusion term. Written in relative gradient variables this system appears as the $q$- viscous Burgers' equation. Exact solutions of this equation in polynomial form as generalized Kampe de Feriet polynomials, corresponding dynamical symmetry and description in terms of Bell polynomials are derived. We found the generating function for these polynomials by application of dynamical symmetry and the Zassenhaus formula. We have constructed and analyzed shock solitons and their interactions with different $q$. We obtain modification of the soliton relative speeds depending on value of $q$.For $q< 1$ the soliton speed becomes bounded from above and as a result in addition to usual Burgers soliton process of fusion, we found a new phenomena, when soliton with higher amplitude but smaller velocity is fissing to two solitons. q-Semiclassical expansion of these equations are found in terms of Bernoulli polynomials in power of $\\ln q$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the q-viscous Burgers' equation with a nonsymmetric q-extension of the diffusion term. What is the relationship between the soliton speed and the value of q, and how does it affect the soliton's behavior for q < 1?\n\nA) The soliton speed is directly proportional to q, and for q < 1, the soliton speed becomes unbounded.\n\nB) The soliton speed is inversely proportional to q, and for q < 1, the soliton speed becomes bounded from above.\n\nC) The soliton speed is independent of q, and for q < 1, the soliton speed remains constant.\n\nD) The soliton speed is proportional to the square root of q, and for q < 1, the soliton speed becomes unbounded.\n\nCorrect Answer: B) The soliton speed is inversely proportional to q, and for q < 1, the soliton speed becomes bounded from above.\n\nExplanation: According to the documentation, for q < 1, the soliton speed becomes bounded from above, which means that as q decreases, the soliton speed decreases. This is because the soliton's amplitude increases while its velocity decreases, leading to a new phenomenon where a soliton with higher amplitude but smaller velocity \"fissions\" into two solitons."}, "40": {"documentation": {"title": "Limited intelligence and performance-based compensation: An agent-based\n  model of the hidden action problem", "source": "Patrick Reinwald, Stephan Leitner and Friederike Wall", "docs_id": "2107.03764", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Limited intelligence and performance-based compensation: An agent-based\n  model of the hidden action problem. Models of economic decision makers often include idealized assumptions, such as rationality, perfect foresight, and access to all relevant pieces of information. These assumptions often assure the models' internal validity, but, at the same time, might limit the models' power to explain empirical phenomena. This paper is particularly concerned with the model of the hidden action problem, which proposes an optimal performance-based sharing rule for situations in which a principal assigns a task to an agent, and the action taken to carry out this task is not observable by the principal. We follow the agentization approach and introduce an agent-based version of the hidden action problem, in which some of the idealized assumptions about the principal and the agent are relaxed so that they only have limited information access, are endowed with the ability to gain information, and store it in and retrieve it from their (limited) memory. We follow an evolutionary approach and analyze how the principal's and the agent's decisions affect the sharing rule, task performance, and their utility over time. The results indicate that the optimal sharing rule does not emerge. The principal's utility is relatively robust to variations in intelligence, while the agent's utility is highly sensitive to limitations in intelligence. The principal's behavior appears to be driven by opportunism, as she withholds a premium from the agent to assure the optimal utility for herself."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the hidden action problem, what is the primary finding regarding the optimal sharing rule, and how does it relate to the principal's and agent's decision-making processes?\n\n**A)** The optimal sharing rule emerges when both the principal and agent have unlimited information access and perfect foresight, leading to a more efficient allocation of resources.\n\n**B)** The principal's utility is highly sensitive to the agent's intelligence, while the agent's utility is relatively robust to variations in the principal's intelligence.\n\n**C)** The optimal sharing rule does not emerge when the principal and agent have limited information access and the ability to gain information, leading to a more nuanced understanding of their decision-making processes.\n\n**D)** The principal's behavior is driven by altruism, as she prioritizes the agent's utility over her own, resulting in a more equitable sharing rule.\n\n**Correct Answer:** C) The optimal sharing rule does not emerge when the principal and agent have limited information access and the ability to gain information, leading to a more nuanced understanding of their decision-making processes.\n\n**Explanation:** The correct answer is based on the findings of the paper, which states that the optimal sharing rule does not emerge when the principal and agent have limited information access and the ability to gain information. This is because the principal's utility is relatively robust to variations in intelligence, while the agent's utility is highly sensitive to limitations in intelligence. This suggests that the decision-making processes of the principal and agent are more complex and nuanced than previously assumed, and that the optimal sharing rule may not be achievable in practice."}, "41": {"documentation": {"title": "Scattering Experiments with Microwave Billiards at an Exceptional Point\n  under Broken Time Reversal Invariance", "source": "S.Bittner, B.Dietz, H.L.Harney, M.Miski-Oglu, A.Richter, and F.\n  Sch\\\"afer", "docs_id": "1402.3537", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Scattering Experiments with Microwave Billiards at an Exceptional Point\n  under Broken Time Reversal Invariance. Scattering experiments with microwave cavities were performed and the effects of broken time-reversal invariance (TRI), induced by means of a magnetized ferrite placed inside the cavity, on an isolated doublet of nearly degenerate resonances were investigated. All elements of the effective Hamiltonian of this two-level system were extracted. As a function of two experimental parameters, the doublet and also the associated eigenvectors could be tuned to coalesce at a so-called exceptional point (EP). The behavior of the eigenvalues and eigenvectors when encircling the EP in parameter space was studied, including the geometric amplitude that builds up in the case of broken TRI. A one-dimensional subspace of parameters was found where the differences of the eigenvalues are either real or purely imaginary. There, the Hamiltonians were found PT-invariant under the combined operation of parity (P) and time reversal (T) in a generalized sense. The EP is the point of transition between both regions. There a spontaneous breaking of PT occurs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Scattering Experiments with Microwave Billiards at an Exceptional Point under Broken Time Reversal Invariance, what is the significance of the one-dimensional subspace of parameters where the differences of the eigenvalues are either real or purely imaginary, and how does this relate to the PT-invariance of the Hamiltonians?\n\n**A)** This subspace represents the region where the system exhibits perfect time-reversal symmetry, and the Hamiltonians are invariant under PT.\n\n**B)** This subspace marks the boundary between the regions where the system exhibits broken time-reversal symmetry, and the Hamiltonians are PT-invariant.\n\n**C)** This subspace is a one-dimensional subspace of parameters where the system exhibits both time-reversal and parity symmetry, and the Hamiltonians are invariant under PT.\n\n**D)** This subspace represents the region where the system exhibits spontaneous breaking of PT, and the Hamiltonians are not invariant under PT.\n\n**Correct Answer:** B) This subspace marks the boundary between the regions where the system exhibits broken time-reversal symmetry, and the Hamiltonians are PT-invariant.\n\n**Explanation:** The correct answer is B) because the one-dimensional subspace of parameters where the differences of the eigenvalues are either real or purely imaginary marks the boundary between the regions where the system exhibits broken time-reversal symmetry (i.e., the system is not PT-invariant) and the regions where the system exhibits PT-invariance. The exceptional point (EP) is the point of transition between these two regions, where spontaneous breaking of PT occurs."}, "42": {"documentation": {"title": "Nighttime Light Intensity and Child Health Outcomes in Bangladesh", "source": "Mohammad Rafiqul Islam, Masud Alam, Munshi Naser \\.Ibne Afzal", "docs_id": "2108.00926", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nighttime Light Intensity and Child Health Outcomes in Bangladesh. This study examines the impact of nighttime light intensity on child health outcomes in Bangladesh. We use nighttime light intensity as a proxy measure of urbanization and argue that the higher intensity of nighttime light, the higher is the degree of urbanization, which positively affects child health outcomes. In econometric estimation, we employ a methodology that combines parametric and non-parametric approaches using the Gradient Boosting Machine (GBM), K-Nearest Neighbors (KNN), and Bootstrap Aggregating that originate from machine learning algorithms. Based on our benchmark estimates, findings show that one standard deviation increase of nighttime light intensity is associated with a 1.515 rise of Z-score of weight for age after controlling for several control variables. The maximum increase of weight for height and height for age score range from 5.35 to 7.18 units. To further understand our benchmark estimates, generalized additive models also provide a robust positive relationship between nighttime light intensity and children's health outcomes. Finally, we develop an economic model that supports the empirical findings of this study that the marginal effect of urbanization on children's nutritional outcomes is strictly positive."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The study \"Nighttime Light Intensity and Child Health Outcomes in Bangladesh\" examines the impact of nighttime light intensity on child health outcomes in Bangladesh. The authors argue that higher nighttime light intensity is associated with higher child health outcomes, but they also acknowledge that the relationship between nighttime light intensity and child health outcomes is complex and influenced by various factors. Which of the following statements best summarizes the authors' findings regarding the relationship between nighttime light intensity and child health outcomes?\n\nA) The authors found a significant negative correlation between nighttime light intensity and child health outcomes, indicating that higher nighttime light intensity is associated with poorer child health outcomes.\nB) The authors found a significant positive correlation between nighttime light intensity and child health outcomes, indicating that higher nighttime light intensity is associated with better child health outcomes.\nC) The authors found a non-significant relationship between nighttime light intensity and child health outcomes, indicating that the relationship between the two variables is unclear.\nD) The authors found that the relationship between nighttime light intensity and child health outcomes is strictly positive, but the magnitude of the effect is small and only significant at the 10% level.\n\n**Correct Answer:** B) The authors found a significant positive correlation between nighttime light intensity and child health outcomes, indicating that higher nighttime light intensity is associated with better child health outcomes.\n\n**Explanation:** The correct answer is B) because the study found a significant positive correlation between nighttime light intensity and child health outcomes, as indicated by the benchmark estimates and generalized additive models. The authors argue that higher nighttime light intensity is associated with higher child health outcomes, including a 1.515 rise in the Z-score of weight for age and increases in weight for height and height for age scores. This suggests a positive relationship between nighttime light intensity and child health outcomes."}, "43": {"documentation": {"title": "Convergence of Computed Dynamic Models with Unbounded Shock", "source": "Kenichiro McAlinn and Kosaku Takanashi", "docs_id": "2103.06483", "section": ["econ.EM", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Convergence of Computed Dynamic Models with Unbounded Shock. This paper studies the asymptotic convergence of computed dynamic models when the shock is unbounded. Most dynamic economic models lack a closed-form solution. As such, approximate solutions by numerical methods are utilized. Since the researcher cannot directly evaluate the exact policy function and the associated exact likelihood, it is imperative that the approximate likelihood asymptotically converges -- as well as to know the conditions of convergence -- to the exact likelihood, in order to justify and validate its usage. In this regard, Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006) show convergence of the likelihood, when the shock has compact support. However, compact support implies that the shock is bounded, which is not an assumption met in most dynamic economic models, e.g., with normally distributed shocks. This paper provides theoretical justification for most dynamic models used in the literature by showing the conditions for convergence of the approximate invariant measure obtained from numerical simulations to the exact invariant measure, thus providing the conditions for convergence of the likelihood."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of dynamic economic models, what is the primary concern regarding the convergence of approximate solutions obtained through numerical methods, and how does this concern relate to the justification and validation of the usage of approximate likelihood functions?\n\n**A)** The primary concern is that the approximate solutions may not capture the full range of possible outcomes, leading to biased estimates of the model's parameters. However, this concern is mitigated by the fact that most dynamic economic models have closed-form solutions, which can be used to validate the approximate solutions.\n\n**B)** The primary concern is that the approximate solutions may not converge to the exact solution as the sample size increases, leading to a loss of information and reduced accuracy. However, this concern is addressed by the work of Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006), which shows convergence of the likelihood function under certain conditions.\n\n**C)** The primary concern is that the approximate solutions may not capture the underlying dynamics of the model, leading to a failure to justify and validate the usage of approximate likelihood functions. This concern is particularly relevant in models with unbounded shocks, where the compact support assumption made by Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006) does not hold.\n\n**D)** The primary concern is that the approximate solutions may not be consistent with the exact policy function, leading to a failure to justify and validate the usage of approximate likelihood functions. However, this concern is addressed by the work of this paper, which provides theoretical justification for the convergence of the approximate invariant measure to the exact invariant measure.\n\n**Correct Answer:** C) The primary concern is that the approximate solutions may not capture the underlying dynamics of the model, leading to a failure to justify and validate the usage of approximate likelihood functions. This concern is particularly relevant in models with unbounded shocks, where the compact support assumption made by Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006) does not hold.\n\n**Explanation:** The correct answer, C, highlights the primary concern regarding the convergence of approximate solutions in dynamic economic models, particularly in models with unbounded shocks. The concern is that the approximate solutions may not capture the underlying dynamics of the model, leading to a failure to justify and validate the usage of approximate likelihood functions. This concern is particularly relevant in models with unbounded shocks, where the compact support assumption made by Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006) does not hold. The other options are incorrect because they either downplay the significance of the concern or provide incorrect information about the relationship between the convergence of approximate solutions and the justification and validation of approximate likelihood functions."}, "44": {"documentation": {"title": "Edge-Minimum Saturated k-Planar Drawings", "source": "Steven Chaplick, Fabian Klute, Irene Parada, Jonathan Rollin, Torsten\n  Ueckerdt", "docs_id": "2012.08631", "section": ["cs.CG", "cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Edge-Minimum Saturated k-Planar Drawings. For a class $\\mathcal{D}$ of drawings of loopless (multi-)graphs in the plane, a drawing $D \\in \\mathcal{D}$ is \\emph{saturated} when the addition of any edge to $D$ results in $D' \\notin \\mathcal{D}$ - this is analogous to saturated graphs in a graph class as introduced by Tur\\'an (1941) and Erd\\H{o}s, Hajnal, and Moon (1964). We focus on $k$-planar drawings, that is, graphs drawn in the plane where each edge is crossed at most $k$ times, and the classes $\\mathcal{D}$ of all $k$-planar drawings obeying a number of restrictions, such as having no crossing incident edges, no pair of edges crossing more than once, or no edge crossing itself. While saturated $k$-planar drawings are the focus of several prior works, tight bounds on how sparse these can be are not well understood. We establish a generic framework to determine the minimum number of edges among all $n$-vertex saturated $k$-planar drawings in many natural classes. For example, when incident crossings, multicrossings and selfcrossings are all allowed, the sparsest $n$-vertex saturated $k$-planar drawings have $\\frac{2}{k - (k \\bmod 2)} (n-1)$ edges for any $k \\geq 4$, while if all that is forbidden, the sparsest such drawings have $\\frac{2(k+1)}{k(k-1)}(n-1)$ edges for any $k \\geq 6$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Edge-Minimum Saturated k-Planar Drawings, what is the minimum number of edges among all n-vertex saturated k-planar drawings when incident crossings, multicrossings, and selfcrossings are all allowed?\n\nA) $\\frac{2}{k - (k \\bmod 2)} (n-1)$\nB) $\\frac{2(k+1)}{k(k-1)}(n-1)$\nC) $\\frac{2}{k} (n-1)$\nD) $\\frac{2}{k} (n-1)$\n\n**Correct Answer:** B) $\\frac{2(k+1)}{k(k-1)}(n-1)$\n\n**Explanation:** The correct answer is based on the result mentioned in the documentation, which states that when incident crossings, multicrossings, and selfcrossings are all allowed, the sparsest n-vertex saturated k-planar drawings have $\\frac{2(k+1)}{k(k-1)}(n-1)$ edges for any k \u2265 6. This result is a specific case of the general framework established in the paper, which provides tight bounds on the minimum number of edges among all n-vertex saturated k-planar drawings in many natural classes."}, "45": {"documentation": {"title": "Generative Adversarial Networks for Financial Trading Strategies\n  Fine-Tuning and Combination", "source": "Adriano Koshiyama and Nick Firoozye and Philip Treleaven", "docs_id": "1901.01751", "section": ["cs.LG", "q-fin.PM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generative Adversarial Networks for Financial Trading Strategies\n  Fine-Tuning and Combination. Systematic trading strategies are algorithmic procedures that allocate assets aiming to optimize a certain performance criterion. To obtain an edge in a highly competitive environment, the analyst needs to proper fine-tune its strategy, or discover how to combine weak signals in novel alpha creating manners. Both aspects, namely fine-tuning and combination, have been extensively researched using several methods, but emerging techniques such as Generative Adversarial Networks can have an impact into such aspects. Therefore, our work proposes the use of Conditional Generative Adversarial Networks (cGANs) for trading strategies calibration and aggregation. To this purpose, we provide a full methodology on: (i) the training and selection of a cGAN for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. To provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. We compared cGAN with an ensemble scheme and model validation methods, both suited for time series. Our results suggest that cGANs are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using Conditional Generative Adversarial Networks (cGANs) for trading strategies calibration and aggregation, as proposed in the paper \"Generative Adversarial Networks for Financial Trading Strategies: Fine-Tuning and Combination\"?\n\n**A)** cGANs can only be used for ensemble modelling of time series data.\n**B)** cGANs provide outperformance when traditional techniques fail to generate any alpha, making them a suitable alternative for strategies calibration and combination.\n**C)** cGANs are only suitable for calibration of individual trading strategies, but not for combination of multiple strategies.\n**D)** cGANs require a large amount of data to train and select the best model for time series data.\n\n**Correct Answer:** B) cGANs provide outperformance when traditional techniques fail to generate any alpha, making them a suitable alternative for strategies calibration and combination.\n\n**Explanation:** The correct answer is B) because the paper states that \"our results suggest that cGANs are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha.\" This indicates that cGANs have an advantage over traditional techniques in terms of generating alpha, making them a suitable choice for trading strategies calibration and combination."}, "46": {"documentation": {"title": "Detection of isotropic cosmic birefringence and its implications for\n  axion-like particles including dark energy", "source": "Tomohiro Fujita, Kai Murai, Hiromasa Nakatsuka, and Shinji Tsujikawa", "docs_id": "2011.11894", "section": ["astro-ph.CO", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detection of isotropic cosmic birefringence and its implications for\n  axion-like particles including dark energy. We investigate the possibility that axion-like particles (ALPs) with various potentials account for the isotropic birefringence recently reported by analyzing the Planck 2018 polarization data. For the quadratic and cosine potentials, we obtain lower bounds on the mass, coupling constant to photon $g$, abundance and equation of state of the ALP to produce the observed birefringence. Especially when the ALP is responsible for dark energy, it is possible to probe the tiny deviation of dark energy equation of state from $-1$ through the cosmic birefringence. We also explore ALPs working as early dark energy (EDE), which alleviates the Hubble tension problem. Since the other parameters are limited by the EDE requirements, we narrow down the ALP-photon coupling to $10^{-19}\\, {\\rm GeV}^{-1}\\lesssim g\\lesssim 10^{-16}\\, {\\rm GeV}^{-1}$ for the decay constant $f=M_\\mathrm{pl}$. Therefore, the Hubble tension and the isotropic birefringence imply that $g$ is typically the order of $f^{-1}$, which is a non-trivial coincidence."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Investigate the implications of the detection of isotropic cosmic birefringence on the properties of axion-like particles (ALPs) and their potential role in dark energy.\n\nA) The detection of isotropic cosmic birefringence implies that ALPs with quadratic and cosine potentials can account for the observed birefringence, but it does not provide any information about the decay constant $f$.\n\nB) The lower bounds on the mass, coupling constant to photon $g$, abundance, and equation of state of the ALP obtained from the analysis of Planck 2018 polarization data suggest that ALPs working as early dark energy (EDE) alleviate the Hubble tension problem, but they do not provide any information about the relationship between $g$ and the decay constant $f$.\n\nC) The fact that the Hubble tension and the isotropic birefringence imply that $g$ is typically the order of $f^{-1}$ is a non-trivial coincidence, and it does not provide any information about the potential role of ALPs in dark energy.\n\nD) The detection of isotropic cosmic birefringence and its implications for ALPs, including dark energy, suggest that the ALP-photon coupling $g$ is typically the order of $f^{-1}$, and this relationship can be used to probe the tiny deviation of dark energy equation of state from $-1$.\n\nCorrect Answer: D) The detection of isotropic cosmic birefringence and its implications for ALPs, including dark energy, suggest that the ALP-photon coupling $g$ is typically the order of $f^{-1}$, and this relationship can be used to probe the tiny deviation of dark energy equation of state from $-1$.\n\nExplanation: This question requires the candidate to understand the implications of the detection of isotropic cosmic birefringence on the properties of ALPs and their potential role in dark energy. The correct answer, D, highlights the relationship between the ALP-photon coupling $g$ and the decay constant $f$, and how this relationship can be used to probe the tiny deviation of dark energy equation of state from $-1$. The other options are incorrect because they either do not provide any information about the relationship between $g$ and $f$, or they focus on the implications of the detection of isotropic cosmic birefringence without considering the relationship between $g$ and $f$."}, "47": {"documentation": {"title": "Towards Lefschetz thimbles regularization of heavy-dense QCD", "source": "Kevin Zambello, Francesco Di Renzo", "docs_id": "1811.03605", "section": ["hep-lat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Lefschetz thimbles regularization of heavy-dense QCD. At finite density, lattice simulations are hindered by the well-known sign problem: for finite chemical potentials, the QCD action becomes complex and the Boltzmann weight $e^{-S}$ cannot be interpreted as a probability distribution to determine expectation values by Monte Carlo techniques. Different workarounds have been devised to study the QCD phase diagram, but their application is mostly limited to the region of small chemical potentials. The Lefschetz thimbles method takes a new approach in which one complexifies the theory and deforms the integration paths. By integrating over Lefschetz thimbles, the imaginary part of the action is kept constant and can be factored out, while $e^{-Re(S)}$ can be interpreted as a probability measure. The method has been applied in recent years to more or less difficult problems. Here we report preliminary results on Lefschetz thimbles regularization of heavy-dense QCD. While still simple, this is a very interesting problem. It is a first look at thimbles for QCD, although in a simplified, effective version. From an algorithmic point of view, it is a nice ground to test effectiveness of techniques we developed for multi thimbles simulations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of the Lefschetz thimbles method in studying the QCD phase diagram at finite density, and how does it address the sign problem?\n\nA) It allows for the direct integration of the complex action, eliminating the need for complexification of the theory.\nB) It enables the separation of the real and imaginary parts of the action, allowing for the interpretation of $e^{-Re(S)}$ as a probability measure.\nC) It introduces a new deformation of the integration path, which facilitates the extraction of physical observables.\nD) It relies on the use of a different gauge field, which simplifies the calculation of the QCD action.\n\nCorrect Answer: B) It enables the separation of the real and imaginary parts of the action, allowing for the interpretation of $e^{-Re(S)}$ as a probability measure.\n\nExplanation: The Lefschetz thimbles method addresses the sign problem in QCD at finite density by deforming the integration path and separating the real and imaginary parts of the action. This allows for the interpretation of $e^{-Re(S)}$ as a probability measure, which is essential for determining expectation values using Monte Carlo techniques. The correct answer highlights this key advantage of the method."}, "48": {"documentation": {"title": "Nonlinear propagation and control of acoustic waves in phononic\n  superlattices", "source": "No\\'e Jim\\'enez and Ahmed Mehrem and Rub\\'en Pic\\'o and Llu\\'is M.\n  Garc\\'ia-Raffi and V\\'ictor J. S\\'anchez-Morcillo", "docs_id": "1508.03656", "section": ["nlin.PS", "cond-mat.mes-hall", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonlinear propagation and control of acoustic waves in phononic\n  superlattices. The propagation of intense acoustic waves in a one-dimensional phononic crystal is studied. The medium consists in a structured fluid, formed by a periodic array of fluid layers with alternating linear acoustic properties and quadratic nonlinearity coefficient. The spacing between layers is of the order of the wavelength, therefore Bragg effects such as band-gaps appear. We show that the interplay between strong dispersion and nonlinearity leads to new scenarios of wave propagation. The classical waveform distortion process typical of intense acoustic waves in homogeneous media can be strongly altered when nonlinearly generated harmonics lie inside or close to band gaps. This allows the possibility of engineer a medium in order to get a particular waveform. Examples of this include the design of media with effective (e.g. cubic) nonlinearities, or extremely linear media (where distortion can be cancelled). The presented ideas open a way towards the control of acoustic wave propagation in nonlinear regime."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of phononic superlattices, what is the primary effect of the interplay between strong dispersion and nonlinearity on the propagation of acoustic waves?\n\n**A)** The appearance of new band-gaps due to the periodic array of fluid layers.\n**B)** The cancellation of nonlinear distortion effects, allowing for the design of extremely linear media.\n**C)** The generation of nonlinear harmonics that lie inside or close to band-gaps, leading to altered waveform distortion processes.\n**D)** The suppression of Bragg effects due to the alternating linear acoustic properties of the fluid layers.\n\n**Correct Answer:** C) The generation of nonlinear harmonics that lie inside or close to band-gaps, leading to altered waveform distortion processes.\n\n**Explanation:** The correct answer, C, is supported by the text, which states that the interplay between strong dispersion and nonlinearity leads to new scenarios of wave propagation. Specifically, it is mentioned that nonlinearly generated harmonics that lie inside or close to band-gaps can strongly alter the classical waveform distortion process typical of intense acoustic waves in homogeneous media. This allows for the possibility of engineering a medium to achieve a particular waveform.\n\nThe other options are incorrect because:\n\n* A) The appearance of new band-gaps is mentioned in the text, but it is not the primary effect of the interplay between dispersion and nonlinearity.\n* B) The cancellation of nonlinear distortion effects is mentioned as an example of the design of extremely linear media, but it is not the primary effect of the interplay between dispersion and nonlinearity.\n* D) The suppression of Bragg effects is not mentioned in the text as a consequence of the interplay between dispersion and nonlinearity."}, "49": {"documentation": {"title": "Differential two-particle number and momentum correlations with the\n  AMPT, UrQMD, and EPOS models in Pb-Pb collisions at $\\sqrt{\\textit{s}_{_{\\rm\n  NN}}}$= 2.76 TeV", "source": "Sumit Basu, Victor Gonzalez, Jinjin Pan, Anders Knospe, Ana Marin,\n  Christina Markert, Claude Pruneau", "docs_id": "2001.07167", "section": ["nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Differential two-particle number and momentum correlations with the\n  AMPT, UrQMD, and EPOS models in Pb-Pb collisions at $\\sqrt{\\textit{s}_{_{\\rm\n  NN}}}$= 2.76 TeV. We report studies of charge-independent (CI) and charge-dependent (CD) two-particle differential number correlation functions, $R_{2} \\left( \\Delta \\eta, \\Delta \\varphi \\right)$, and transverse momentum correlation functions, $P_2 \\left( \\Delta \\eta, \\Delta \\varphi \\right)$ of charged particles produced in \\PbPb\\ collisions at the LHC centre-of-mass energy $\\sqrt{s_{\\rm NN}} =$ 2.76 TeV with the UrQMD, AMPT and EPOS models. Model predictions for $R_2$ and $P_2$ correlation functions are presented for inclusive charged hadrons ($h^\\pm$) in selected transverse momentum ranges and with full azimuthal coverage in the pseudorapidity range $|\\eta|< 1.0$. We compare these predictions for the strength, shape, and particularly the width of the correlation functions with recent measurements of these observables by the ALICE collaboration. Our analysis indicate that comparative studies of $R_2$ and $P_2$ correlation functions provide valuable insight towards the understanding of particle production in Pb--Pb collisions. We find, in particular, that these models have quantitatively different predictions for these three observables but none reproduce the measured correlation functions reported by the ALICE collaboration. Accounting for quantum number conservation in models, particularly charge conservation, is mandatory to reproduce the detailed measurements of number and transverse momentum correlation functions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the study report that the UrQMD, AMPT, and EPOS models have quantitatively different predictions for the two-particle differential number correlation functions ($R_2$) and transverse momentum correlation functions ($P_2$) compared to the measurements by the ALICE collaboration. However, none of the models reproduce the measured correlation functions.\n\n**Which of the following statements best summarizes the authors' conclusion regarding the limitations of the UrQMD, AMPT, and EPOS models in reproducing the measured correlation functions?**\n\nA) The models are able to accurately reproduce the measured correlation functions, but the measurements are inconsistent with each other.\nB) The models have quantitatively different predictions for $R_2$ and $P_2$ correlation functions, but the differences are not significant enough to reproduce the measured correlation functions.\nC) The models are unable to reproduce the measured correlation functions due to fundamental differences in their underlying physics, particularly charge conservation.\nD) The models are able to reproduce the measured correlation functions, but the measurements are limited by experimental uncertainties.\n\n**Correct Answer:** C) The models are unable to reproduce the measured correlation functions due to fundamental differences in their underlying physics, particularly charge conservation.\n\n**Explanation:** The correct answer is C) because the authors explicitly state that \"Accounting for quantum number conservation in models, particularly charge conservation, is mandatory to reproduce the detailed measurements of number and transverse momentum correlation functions.\" This suggests that the models' inability to reproduce the measured correlation functions is due to a fundamental limitation in their ability to conserve charge, which is a key aspect of quantum number conservation."}, "50": {"documentation": {"title": "On the Non-Asymptotic Concentration of Heteroskedastic Wishart-type\n  Matrix", "source": "T. Tony Cai and Rungang Han and Anru R. Zhang", "docs_id": "2008.12434", "section": ["math.ST", "math.PR", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Non-Asymptotic Concentration of Heteroskedastic Wishart-type\n  Matrix. This paper focuses on the non-asymptotic concentration of the heteroskedastic Wishart-type matrices. Suppose $Z$ is a $p_1$-by-$p_2$ random matrix and $Z_{ij} \\sim N(0,\\sigma_{ij}^2)$ independently, we prove that \\begin{equation*} \\bbE \\left\\|ZZ^\\top - \\bbE ZZ^\\top\\right\\| \\leq (1+\\epsilon)\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(p_1 \\wedge p_2)} + C\\sigma_*^2\\log(p_1 \\wedge p_2)\\right\\}, \\end{equation*} where $\\sigma_C^2 := \\max_j \\sum_{i=1}^{p_1}\\sigma_{ij}^2$, $\\sigma_R^2 := \\max_i \\sum_{j=1}^{p_2}\\sigma_{ij}^2$ and $\\sigma_*^2 := \\max_{i,j}\\sigma_{ij}^2$. A minimax lower bound is developed that matches this upper bound. Then, we derive the concentration inequalities, moments, and tail bounds for the heteroskedastic Wishart-type matrix under more general distributions, such as sub-Gaussian and heavy-tailed distributions. Next, we consider the cases where $Z$ has homoskedastic columns or rows (i.e., $\\sigma_{ij} \\approx \\sigma_i$ or $\\sigma_{ij} \\approx \\sigma_j$) and derive the rate-optimal Wishart-type concentration bounds. Finally, we apply the developed tools to identify the sharp signal-to-noise ratio threshold for consistent clustering in the heteroskedastic clustering problem."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Prove that the upper bound for the non-asymptotic concentration of the heteroskedastic Wishart-type matrix is tight for the specific case where $Z$ is a $p_1$-by-$p_2$ random matrix with $\\sigma_{ij} \\sim N(0,1)$ independently, and $p_1 = p_2 = 10$.\n\nA) $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(10)} + C\\sigma_*^2\\log(10)\\right\\}$\n\nB) $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(100)} + C\\sigma_*^2\\log(100)\\right\\}$\n\nC) $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(1000)} + C\\sigma_*^2\\log(1000)\\right\\}$\n\nD) $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(10000)} + C\\sigma_*^2\\log(10000)\\right\\}$\n\n**Correct Answer:** B) $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(100)} + C\\sigma_*^2\\log(100)\\right\\}$\n\n**Explanation:**\n\nThe correct answer is B) because the upper bound is derived under the assumption that $\\sigma_{ij} \\sim N(0,1)$ independently, and the logarithmic term $\\log(p_1 \\wedge p_2)$ is evaluated at $p_1 = p_2 = 10$. The correct bound is therefore $\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 + C\\sigma_R\\sigma_*\\sqrt{\\log(100)} + C\\sigma_*^2\\log(100)\\right\\}$.\n\nThe other options are incorrect because they do not match the specific case described in the question. Option A is incorrect because it uses $\\sqrt{\\log(10)}$ instead of $\\sqrt{\\log(100)}$. Option C is incorrect because it uses $\\sqrt{\\log(1000)}$ instead of $\\sqrt{\\log(100)}$. Option D is incorrect because it uses $\\sqrt{\\log(10000)}$ instead of $\\sqrt{\\log(100)}$."}, "51": {"documentation": {"title": "Interacting SPT phases are not Morita invariant", "source": "Luuk Stehouwer", "docs_id": "2110.07408", "section": ["hep-th", "cond-mat.str-el", "math-ph", "math.AT", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interacting SPT phases are not Morita invariant. Class D topological superconductors have been described as invertible topological phases protected by charge $Q$ and particle-hole symmetry $C$. A competing description is that class D has no internal symmetries except for the fermion parity group $\\mathbb{Z}_2^F = \\{1, (-1)^F\\}$. In the weakly interacting setting, it can be argued that `particle-hole symmetry cancels charge' in a suitable sense. Namely, the classification results are independent of which of the two internal symmetry groups are taken because of a Morita equivalence. However, we argue that for strongly interacting particles, the group of symmetry-protected topological phases in the two cases are nonisomorphic in dimension $2+1$. This shows that in contrast to the free case, interacting phases are not Morita invariant. To accomplish this, we use the approach to interacting phases using invertible field theories and bordism. We give explicit expressions of invertible field theories which have the two different groups $\\mathbb{Z}_2^F$ and $U(1)_Q \\rtimes \\mathbb{Z}_2^C$ as internal symmetries and give spacetime manifolds on which their partition functions disagree. Techniques from algebraic topology are used to compute the relevant bordism groups, most importantly the James spectral sequence. The result is that there are both a new $\\mathbb{Z}_2$- and a new $\\mathbb{Z}$-invariant for $U(1)_Q \\rtimes \\mathbb{Z}_2^F$ that are not present for $\\mathbb{Z}_2^F$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider the invertible field theories for class D topological superconductors with internal symmetries $\\mathbb{Z}_2^F$ and $U(1)_Q \\rtimes \\mathbb{Z}_2^C$. Compute the bordism group $K(\\mathbb{Z}_2, 2)$ for the partition function of the theory with $\\mathbb{Z}_2^F$ symmetry, and compare it to the bordism group $K(\\mathbb{Z}, 2)$ for the partition function of the theory with $U(1)_Q \\rtimes \\mathbb{Z}_2^C$ symmetry.\n\nA) $K(\\mathbb{Z}_2, 2) = \\mathbb{Z}_2 \\oplus \\mathbb{Z}_2$\nB) $K(\\mathbb{Z}_2, 2) = \\mathbb{Z}_2$\nC) $K(\\mathbb{Z}_2, 2) = \\mathbb{Z}$\nD) $K(\\mathbb{Z}_2, 2) = \\mathbb{Z} \\oplus \\mathbb{Z}_2$\n\nCorrect Answer: A) $K(\\mathbb{Z}_2, 2) = \\mathbb{Z}_2 \\oplus \\mathbb{Z}_2$\n\nExplanation: The James spectral sequence is used to compute the bordism group $K(\\mathbb{Z}_2, 2)$. The sequence is given by:\n\n$$E_2^{p,q} = H^p(\\mathbb{Z}_2^F; H^q(\\mathbb{Z}_2)) \\Rightarrow K(\\mathbb{Z}_2, 2)$$\n\nUsing the fact that $H^q(\\mathbb{Z}_2) = \\mathbb{Z}_2$ for $q = 0, 2$, and $H^q(\\mathbb{Z}_2) = 0$ for $q \\neq 0, 2$, we can compute the terms of the spectral sequence:\n\n$$E_2^{0,0} = H^0(\\mathbb{Z}_2^F; \\mathbb{Z}_2) = \\mathbb{Z}_2$$\n$$E_2^{0,2} = H^0(\\mathbb{Z}_2^F; \\mathbb{Z}_2) = \\mathbb{Z}_2$$\n$$E_2^{1,1} = H^1(\\mathbb{Z}_2^F; \\mathbb{Z}_2) = 0$$\n$$E_2^{1,3} = H^1(\\mathbb{Z}_2^F; \\mathbb{Z}_2) = 0$$\n\nThe spectral sequence converges to $K(\\mathbb{Z}_2, 2) = \\mathbb{Z}_2 \\oplus \\mathbb{Z}_2$, which is the correct answer.\n\nNote: The other options are incorrect because the James spectral sequence does not converge to $K(\\mathbb{Z}, 2)$ or $\\mathbb{Z} \\oplus \\mathbb{Z}_2$. The correct answer requires a deep understanding of the James spectral sequence and its application to bordism groups."}, "52": {"documentation": {"title": "Two-Channel Totally Asymmetric Simple Exclusion Processes", "source": "Ekaterina Pronina and Anatoly B. Kolomeisky", "docs_id": "cond-mat/0407224", "section": ["cond-mat.stat-mech", "cond-mat.soft", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-Channel Totally Asymmetric Simple Exclusion Processes. Totally asymmetric simple exclusion processes, consisting of two coupled parallel lattice chains with particles interacting with hard-core exclusion and moving along the channels and between them, are considered. In the limit of strong coupling between the channels, the particle currents, density profiles and a phase diagram are calculated exactly by mapping the system into an effective one-channel totally asymmetric exclusion model. For intermediate couplings, a simple approximate theory, that describes the particle dynamics in vertical clusters of two corresponding parallel sites exactly and neglects the correlations between different vertical clusters, is developed. It is found that, similarly to the case of one-channel totally asymmetric simple exclusion processes, there are three stationary state phases, although the phase boundaries and stationary properties strongly depend on inter-channel coupling. An extensive computer Monte Carlo simulations fully support the theoretical predictions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of two-channel totally asymmetric simple exclusion processes, what is the primary effect of strong coupling between the channels on the particle dynamics, and how does this differ from the case of one-channel totally asymmetric simple exclusion processes?\n\nA) Strong coupling leads to a decrease in particle currents and density profiles, resulting in a single stationary state phase.\nB) Strong coupling leads to an increase in particle currents and density profiles, resulting in three stationary state phases with distinct phase boundaries.\nC) Strong coupling leads to a phase transition from a single stationary state phase to a two-phase system, with the two phases being vertically aligned clusters of particles.\nD) Strong coupling leads to a decrease in particle currents and density profiles, resulting in a single stationary state phase, but with a more complex phase diagram.\n\nCorrect Answer: B) Strong coupling leads to an increase in particle currents and density profiles, resulting in three stationary state phases with distinct phase boundaries.\n\nExplanation: The correct answer is based on the fact that the documentation states that in the limit of strong coupling between the channels, the particle currents, density profiles, and a phase diagram are calculated exactly by mapping the system into an effective one-channel totally asymmetric exclusion model. This implies that strong coupling leads to an increase in particle currents and density profiles, resulting in three stationary state phases with distinct phase boundaries. The other options are incorrect because they either contradict the documentation or oversimplify the effect of strong coupling on the particle dynamics."}, "53": {"documentation": {"title": "Numerical modeling of core-collapse supernovae and compact objects", "source": "K. Sumiyoshi", "docs_id": "1212.6131", "section": ["astro-ph.HE", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Numerical modeling of core-collapse supernovae and compact objects. Massive stars (M> 10Msun) end their lives with spectacular explosions due to gravitational collapse. The collapse turns the stars into compact objects such as neutron stars and black holes with the ejection of cosmic rays and heavy elements. Despite the importance of these astrophysical events, the mechanism of supernova explosions has been an unsolved issue in astrophysics. This is because clarification of the supernova dynamics requires the full knowledge of nuclear and neutrino physics at extreme conditions, and large-scale numerical simulations of neutrino radiation hydrodynamics in multi-dimensions. This article is a brief overview of the understanding (with difficulty) of the supernova mechanism through the recent advance of numerical modeling at supercomputing facilities. Numerical studies with the progress of nuclear physics are applied to follow the evolution of compact objects with neutrino emissions in order to reveal the birth of pulsars/black holes from the massive stars."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in clarifying the mechanism of supernova explosions, according to the article?\n\nA) The complexity of nuclear reactions in massive stars\nB) The difficulty in simulating neutrino radiation hydrodynamics in multi-dimensions\nC) The need for precise calculations of cosmic ray ejection and heavy element formation\nD) The limitation of current computational power in simulating supernova dynamics\n\nCorrect Answer: B) The difficulty in simulating neutrino radiation hydrodynamics in multi-dimensions\n\nExplanation: The article states that \"clarification of the supernova dynamics requires the full knowledge of nuclear and neutrino physics at extreme conditions, and large-scale numerical simulations of neutrino radiation hydrodynamics in multi-dimensions.\" This suggests that the primary challenge in clarifying the mechanism of supernova explosions is the difficulty in simulating neutrino radiation hydrodynamics in multi-dimensions, which is option B."}, "54": {"documentation": {"title": "Poisson to GOE transition in the distribution of the ratio of\n  consecutive level spacings", "source": "N. D. Chavda, H. N. Deota and V. K. B. Kota", "docs_id": "1405.6321", "section": ["cond-mat.stat-mech", "nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Poisson to GOE transition in the distribution of the ratio of\n  consecutive level spacings. Probability distribution for the ratio ($r$) of consecutive level spacings of the eigenvalues of a Poisson (generating regular spectra) spectrum and that of a GOE random matrix ensemble are given recently. Going beyond these, for the ensemble generated by the Hamiltonian $H_\\lambda = (H_0+\\lambda V)/\\sqrt{1+\\lambda^2}$ interpolating Poisson ($\\lambda=0$) and GOE ($\\lambda \\rightarrow \\infty$) we have analyzed the transition curves for $\\langle r\\rangle$ and $\\langle \\tilde{r}\\rangle$ as $\\lambda$ changes from $0$ to $\\infty$; $\\tilde{r} = min(r,1/r)$. Here, $V$ is a GOE ensemble of real symmetric $d \\times d$ matrices and $H_0$ is a diagonal matrix with a Gaussian distribution (with mean equal to zero) for the diagonal matrix elements; spectral variance generated by $H_0$ is assumed to be same as the one generated by $V$. Varying $d$ from 300 to 1000, it is shown that the transition parameter is $\\Lambda \\sim \\lambda^2\\,d$, i.e. the $\\langle r\\rangle$ vs $\\lambda$ (similarly for $\\langle \\tilde{r}\\rangle$ vs $\\lambda$) curves for different $d$'s merge to a single curve when this is considered as a function of $\\Lambda$. Numerically, it is also found that this transition curve generates a mapping to a $3 \\times 3$ Poisson to GOE random matrix ensemble. Example for Poisson to GOE transition from a one dimensional interacting spin-1/2 chain is presented."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a random matrix ensemble generated by the Hamiltonian $H_\\lambda = (H_0+\\lambda V)/\\sqrt{1+\\lambda^2}$, where $H_0$ is a diagonal matrix with a Gaussian distribution for its diagonal elements, and $V$ is a GOE ensemble of real symmetric $d \\times d$ matrices. The spectral variance generated by $H_0$ is assumed to be the same as the one generated by $V$. As $\\lambda$ changes from $0$ to $\\infty$, what is the behavior of the transition parameter $\\Lambda$ as a function of $d$, and how does it relate to the transition from a Poisson to a GOE random matrix ensemble?\n\n**A)** $\\Lambda \\sim \\lambda^2\\,d$, with the transition curve merging to a single curve when considered as a function of $\\Lambda$.\n\n**B)** $\\Lambda \\sim \\lambda\\,d$, with the transition curve not merging to a single curve when considered as a function of $\\Lambda$.\n\n**C)** $\\Lambda \\sim \\lambda^3\\,d$, with the transition curve not merging to a single curve when considered as a function of $\\Lambda$.\n\n**D)** $\\Lambda \\sim \\lambda\\,d^2$, with the transition curve merging to a single curve when considered as a function of $\\Lambda$.\n\n**Correct Answer:** A) $\\Lambda \\sim \\lambda^2\\,d$, with the transition curve merging to a single curve when considered as a function of $\\Lambda$.\n\n**Explanation:** The correct answer is based on the provided documentation, which states that the transition parameter is $\\Lambda \\sim \\lambda^2\\,d$. This means that as $\\lambda$ changes from $0$ to $\\infty$, the transition parameter $\\Lambda$ increases quadratically with $\\lambda$ and linearly with $d$. The documentation also states that varying $d$ from 300 to 1000 results in the transition curve merging to a single curve when considered as a function of $\\Lambda$."}, "55": {"documentation": {"title": "The Relation between EIT Waves and Coronal Mass Ejections", "source": "P. F. Chen", "docs_id": "0905.3272", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Relation between EIT Waves and Coronal Mass Ejections. More and more evidence indicates that \"EIT waves\" are strongly related to coronal mass ejections (CMEs). However, it is still not clear how the two phenomena are related to each other. We investigate a CME event on 1997 September 9, which was well observed by both EUV imaging telescope (EIT) and the high-cadence MK3 coronagraph at Mauna Loa Solar Observatory, and compare the spatial relation between the \"EIT wave\" fronts and the CME leading loops. It is found that \"EIT wave\" fronts are co-spatial with the CME leading loops, and the expanding EUV dimmings are co-spatial with the CME cavity. It is also found that the CME stopped near the boundary of a coronal hole, a feature common to observations of \"EIT waves\". It is suggested that \"EIT waves\"/dimmings are the EUV counterparts of the CME leading loop/cavity, based on which we propose that, as in the case of \"EIT waves\", CME leading loops are apparently-moving density enhancements that are generated by successive stretching (or opening-up) of magnetic loops."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the relationship between EIT waves and coronal mass ejections (CMEs) based on the study described in the provided documentation. What is the proposed mechanism for the generation of CME leading loops, and how do the findings of the study support this mechanism?\n\n**A)** The study suggests that CME leading loops are generated by the collapse of magnetic loops, which is supported by the observation that CMEs often stop near the boundary of a coronal hole.\n\n**B)** The study proposes that CME leading loops are generated by successive stretching (or opening-up) of magnetic loops, and that the expanding EUV dimmings are the EUV counterparts of the CME leading loop/cavity.\n\n**C)** The study finds that CME leading loops are co-spatial with the \"EIT wave\" fronts, but does not provide any information on the mechanism of their generation.\n\n**D)** The study suggests that CME leading loops are generated by the interaction between the solar wind and the coronal magnetic field, and that the \"EIT wave\" fronts are a result of this interaction.\n\n**Correct Answer:** B) The study proposes that CME leading loops are generated by successive stretching (or opening-up) of magnetic loops, and that the expanding EUV dimmings are the EUV counterparts of the CME leading loop/cavity.\n\n**Explanation:** The correct answer is supported by the text, which states that \"CME leading loops are apparently-moving density enhancements that are generated by successive stretching (or opening-up) of magnetic loops.\" Additionally, the text notes that the expanding EUV dimmings are co-spatial with the CME cavity, which is consistent with the proposed mechanism. The other options are incorrect because they either misrepresent the study's findings or propose alternative mechanisms that are not supported by the text."}, "56": {"documentation": {"title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data", "source": "Yang Yifan, Guo Ju'e, Sun Shaolong, and Li Yixin", "docs_id": "2002.09656", "section": ["q-fin.ST", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data. Faced with the growing research towards crude oil price fluctuations influential factors following the accelerated development of Internet technology, accessible data such as Google search volume index are increasingly quantified and incorporated into forecasting approaches. In this paper, we apply multi-scale data that including both GSVI data and traditional economic data related to crude oil price as independent variables and propose a new hybrid approach for monthly crude oil price forecasting. This hybrid approach, based on divide and conquer strategy, consists of K-means method, kernel principal component analysis and kernel extreme learning machine , where K-means method is adopted to divide input data into certain clusters, KPCA is applied to reduce dimension, and KELM is employed for final crude oil price forecasting. The empirical result can be analyzed from data and method levels. At the data level, GSVI data perform better than economic data in level forecasting accuracy but with opposite performance in directional forecasting accuracy because of Herd Behavior, while hybrid data combined their advantages and obtain best forecasting performance in both level and directional accuracy. At the method level, the approaches with K-means perform better than those without K-means, which demonstrates that divide and conquer strategy can effectively improve the forecasting performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using a hybrid approach for crude oil price forecasting, as proposed in the paper, and how does it compare to using GSVI data or traditional economic data alone?\n\n**A)** The hybrid approach improves forecasting accuracy by leveraging the strengths of multiple methods, including K-means, KPCA, and KELM, to capture complex patterns in the data.\n\n**B)** The hybrid approach is more computationally efficient than using individual methods, allowing for faster forecasting and analysis.\n\n**C)** The hybrid approach is more prone to overfitting than using individual methods, which can lead to poor forecasting performance.\n\n**D)** The hybrid approach relies solely on GSVI data, which is more accurate than traditional economic data in both level and directional forecasting accuracy.\n\n**Correct Answer:** A) The hybrid approach improves forecasting accuracy by leveraging the strengths of multiple methods, including K-means, KPCA, and KELM, to capture complex patterns in the data.\n\n**Explanation:** The paper proposes a hybrid approach that combines the strengths of multiple methods, including K-means, KPCA, and KELM, to improve forecasting accuracy. The empirical results show that the hybrid approach outperforms individual methods, including GSVI data and traditional economic data, in both level and directional forecasting accuracy. This suggests that the hybrid approach is more effective in capturing complex patterns in the data and improving forecasting performance."}, "57": {"documentation": {"title": "Structural and dynamic features of liquid Si under high pressure above\n  the melting line minimum", "source": "T. Demchuk, T. Bryk, A. P. Seitsonen", "docs_id": "2009.00834", "section": ["cond-mat.dis-nn", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Structural and dynamic features of liquid Si under high pressure above\n  the melting line minimum. We report an {\\it ab initio} simulation study of changes in structural and dynamic properties of liquid Si at 7 pressures ranging from 10.2 GPa to 24.3 GPa along the isothermal line 1150~K, which is above the minimum of the melting line. The increase of pressure from 10.2 GPa to 16 GPa causes strong reduction in the tetrahedral ordering of the most close neighbors. The diffusion coefficient shows a linear decay vs drop in atomic volume, that agrees with theoretical prediction for simple liquid metals, thus not showing any feature at the pressures corresponding to the different crystal phase boundaries. The Fourier-spectra of velocity autocorrelation function shows two-peak structure at pressures 20 GPa and higher. These characteristic frequencies correspond well to the peak frequencies of the transverse current spectral function in the second pseudo-Brillouin zone. Two almost flat branches of short-wavelength transverse modes were observed for all the studied pressures. We discuss the pressure evolution of characteristic frequencies in the longitudinal and transverse branches of collective modes."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the structural and dynamic features of liquid Si under high pressure above the melting line minimum. What is the expected relationship between the diffusion coefficient and atomic volume, and how does it relate to the theoretical prediction for simple liquid metals?\n\n**A)** The diffusion coefficient increases with decreasing atomic volume, which contradicts the theoretical prediction for simple liquid metals.\n\n**B)** The diffusion coefficient decreases with decreasing atomic volume, which agrees with the theoretical prediction for simple liquid metals.\n\n**C)** The diffusion coefficient remains constant with decreasing atomic volume, regardless of the theoretical prediction for simple liquid metals.\n\n**D)** The diffusion coefficient shows no relationship with atomic volume, and its value is independent of pressure.\n\n**Correct Answer:** B) The diffusion coefficient decreases with decreasing atomic volume, which agrees with the theoretical prediction for simple liquid metals.\n\n**Explanation:** According to the text, the diffusion coefficient shows a linear decay vs drop in atomic volume, which agrees with the theoretical prediction for simple liquid metals. This means that as the pressure increases, the atomic volume decreases, leading to a decrease in the diffusion coefficient. This relationship is consistent with the theoretical prediction for simple liquid metals, which is that the diffusion coefficient decreases with decreasing atomic volume."}, "58": {"documentation": {"title": "Optimal transport on large networks, a practitioner's guide", "source": "Arthur Charpentier, Alfred Galichon, Lucas Vernet", "docs_id": "1907.02320", "section": ["econ.GN", "cs.DS", "econ.EM", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal transport on large networks, a practitioner's guide. This article presents a set of tools for the modeling of a spatial allocation problem in a large geographic market and gives examples of applications. In our settings, the market is described by a network that maps the cost of travel between each pair of adjacent locations. Two types of agents are located at the nodes of this network. The buyers choose the most competitive sellers depending on their prices and the cost to reach them. Their utility is assumed additive in both these quantities. Each seller, taking as given other sellers prices, sets her own price to have a demand equal to the one we observed. We give a linear programming formulation for the equilibrium conditions. After formally introducing our model we apply it on two examples: prices offered by petrol stations and quality of services provided by maternity wards. These examples illustrate the applicability of our model to aggregate demand, rank prices and estimate cost structure over the network. We insist on the possibility of applications to large scale data sets using modern linear programming solvers such as Gurobi. In addition to this paper we released a R toolbox to implement our results and an online tutorial (http://optimalnetwork.github.io)"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the spatial allocation problem on large networks, what is the primary assumption made about the utility of buyers, and how does it relate to the equilibrium conditions?\n\n**A)** Buyers' utility is assumed to be non-additive, and the equilibrium conditions are derived from a nonlinear programming formulation.\n\n**B)** Buyers' utility is assumed to be additive in both price and travel cost, and the equilibrium conditions are given by a linear programming formulation.\n\n**C)** Buyers' utility is assumed to be non-additive in price, and the equilibrium conditions are derived from a nonlinear programming formulation.\n\n**D)** Buyers' utility is assumed to be additive in price, but not in travel cost, and the equilibrium conditions are given by a nonlinear programming formulation.\n\n**Correct Answer:** B) Buyers' utility is assumed to be additive in both price and travel cost, and the equilibrium conditions are given by a linear programming formulation.\n\n**Explanation:** The correct answer is B) because the documentation states that \"The buyers choose the most competitive sellers depending on their prices and the cost to reach them. Their utility is assumed additive in both these quantities.\" This implies that the utility of buyers is additive in both price and travel cost, and the equilibrium conditions are given by a linear programming formulation."}, "59": {"documentation": {"title": "Basel III capital surcharges for G-SIBs fail to control systemic risk\n  and can cause pro-cyclical side effects", "source": "Sebastian Poledna, Olaf Bochmann and Stefan Thurner", "docs_id": "1602.03505", "section": ["q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Basel III capital surcharges for G-SIBs fail to control systemic risk\n  and can cause pro-cyclical side effects. In addition to constraining bilateral exposures of financial institutions, there are essentially two options for future financial regulation of systemic risk (SR): First, financial regulation could attempt to reduce the financial fragility of global or domestic systemically important financial institutions (G-SIBs or D-SIBs), as for instance proposed in Basel III. Second, future financial regulation could attempt strengthening the financial system as a whole. This can be achieved by re-shaping the topology of financial networks. We use an agent-based model (ABM) of a financial system and the real economy to study and compare the consequences of these two options. By conducting three \"computer experiments\" with the ABM we find that re-shaping financial networks is more effective and efficient than reducing leverage. Capital surcharges for G-SIBs can reduce SR, but must be larger than those specified in Basel III in order to have a measurable impact. This can cause a loss of efficiency. Basel III capital surcharges for G-SIBs can have pro-cyclical side effects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the potential drawbacks of implementing Basel III capital surcharges for Systemically Important Financial Institutions (SIFIs), and how do these compare to alternative approaches to managing systemic risk?\n\n**A)** Basel III capital surcharges can effectively reduce systemic risk by increasing the capital requirements of SIFIs, but may lead to pro-cyclical side effects if the surcharges are not sufficient.\n\n**B)** Implementing Basel III capital surcharges for SIFIs can actually increase systemic risk, as it may lead to a loss of efficiency and a decrease in the overall resilience of the financial system.\n\n**C)** The two options for future financial regulation of systemic risk, namely reducing the financial fragility of SIFIs and strengthening the financial system as a whole, are equivalent in terms of their effectiveness and efficiency.\n\n**D)** Basel III capital surcharges for SIFIs are a necessary and sufficient condition for managing systemic risk, and can be implemented without any potential drawbacks.\n\n**Correct Answer:** A) Basel III capital surcharges can effectively reduce systemic risk by increasing the capital requirements of SIFIs, but may lead to pro-cyclical side effects if the surcharges are not sufficient.\n\n**Explanation:** The correct answer is A) because the documentation states that Basel III capital surcharges for SIFIs can reduce systemic risk, but only if the surcharges are larger than those specified in Basel III. If the surcharges are not sufficient, they can lead to pro-cyclical side effects, which can actually increase systemic risk. The other options are incorrect because they either misrepresent the potential drawbacks of Basel III capital surcharges (B) or imply that the two options for managing systemic risk are equivalent (C), which is not the case. Option D) is also incorrect because it implies that Basel III capital surcharges are a necessary and sufficient condition for managing systemic risk, which is not supported by the documentation."}}