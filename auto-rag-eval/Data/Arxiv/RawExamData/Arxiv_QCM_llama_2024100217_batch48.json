{"0": {"documentation": {"title": "DISCOMAX: A Proximity-Preserving Distance Correlation Maximization\n  Algorithm", "source": "Praneeth Vepakomma and Ahmed Elgammal", "docs_id": "1306.2533", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DISCOMAX: A Proximity-Preserving Distance Correlation Maximization\n  Algorithm. In a regression setting we propose algorithms that reduce the dimensionality of the features while simultaneously maximizing a statistical measure of dependence known as distance correlation between the low-dimensional features and a response variable. This helps in solving the prediction problem with a low-dimensional set of features. Our setting is different from subset-selection algorithms where the problem is to choose the best subset of features for regression. Instead, we attempt to generate a new set of low-dimensional features as in a feature-learning setting. We attempt to keep our proposed approach as model-free and our algorithm does not assume the application of any specific regression model in conjunction with the low-dimensional features that it learns. The algorithm is iterative and is fomulated as a combination of the majorization-minimization and concave-convex optimization procedures. We also present spectral radius based convergence results for the proposed iterations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the DISCOMAX algorithm, what is the primary goal of the algorithm, and how does it differ from subset-selection algorithms in a regression setting?\n\n**A)** The algorithm aims to minimize the distance correlation between the low-dimensional features and the response variable, while also reducing the dimensionality of the features. This approach is similar to subset-selection algorithms, which aim to choose the best subset of features for regression.\n\n**B)** The algorithm seeks to maximize the distance correlation between the low-dimensional features and the response variable, while also reducing the dimensionality of the features. This approach is distinct from subset-selection algorithms, which focus on feature selection rather than feature learning.\n\n**C)** The algorithm aims to minimize the distance correlation between the low-dimensional features and the response variable, while also assuming the application of a specific regression model in conjunction with the low-dimensional features that it learns.\n\n**D)** The algorithm seeks to maximize the distance correlation between the low-dimensional features and the response variable, while also attempting to keep the proposed approach as model-free as possible.\n\n**Correct Answer:** B) The algorithm seeks to maximize the distance correlation between the low-dimensional features and the response variable, while also reducing the dimensionality of the features. This approach is distinct from subset-selection algorithms, which focus on feature selection rather than feature learning.\n\n**Explanation:** The correct answer, B, accurately reflects the primary goal of the DISCOMAX algorithm, which is to maximize the distance correlation between the low-dimensional features and the response variable. This approach differs from subset-selection algorithms, which focus on selecting the best subset of features for regression, rather than learning new features. The algorithm's model-free nature and iterative formulation, as described in the documentation, also support this answer."}, "1": {"documentation": {"title": "A Dynamical Model of Twitter Activity Profiles", "source": "Hoai Nguyen Huynh, Erika Fille Legara, Christopher Monterola", "docs_id": "1508.07097", "section": ["cs.SI", "cs.CY", "cs.HC", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Dynamical Model of Twitter Activity Profiles. The advent of the era of Big Data has allowed many researchers to dig into various socio-technical systems, including social media platforms. In particular, these systems have provided them with certain verifiable means to look into certain aspects of human behavior. In this work, we are specifically interested in the behavior of individuals on social media platforms---how they handle the information they get, and how they share it. We look into Twitter to understand the dynamics behind the users' posting activities---tweets and retweets---zooming in on topics that peaked in popularity. Three mechanisms are considered: endogenous stimuli, exogenous stimuli, and a mechanism that dictates the decay of interest of the population in a topic. We propose a model involving two parameters $\\eta^\\star$ and $\\lambda$ describing the tweeting behaviour of users, which allow us to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags. With this model, we are able to accurately reproduce the temporal profile of user engagements on Twitter. Furthermore, we introduce an alternative in classifying the collective activities on the socio-technical system based on the model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"A Dynamical Model of Twitter Activity Profiles\" propose a model to understand the dynamics behind users' posting activities on Twitter. The model involves two parameters, \u03b7\u22c6 and \u03bb, which describe the tweeting behavior of users. What is the primary purpose of these two parameters, and how do they relate to the temporal profiles of popular Twitter hashtags?\n\n**A)** \u03b7\u22c6 and \u03bb are used to predict the popularity of tweets based on their content, and the temporal profiles of popular hashtags are a byproduct of this prediction.\n\n**B)** \u03b7\u22c6 and \u03bb are used to model the decay of interest in a topic over time, and the temporal profiles of popular hashtags are a result of this decay.\n\n**C)** \u03b7\u22c6 and \u03bb are used to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags, and they describe the tweeting behavior of users in a way that allows for accurate reproduction of these findings.\n\n**D)** \u03b7\u22c6 and \u03bb are used to classify collective activities on the socio-technical system based on the model, and the temporal profiles of popular hashtags are a secondary outcome of this classification.\n\n**Correct Answer:** C) \u03b7\u22c6 and \u03bb are used to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags, and they describe the tweeting behavior of users in a way that allows for accurate reproduction of these findings.\n\n**Explanation:** The correct answer is C) because the authors explicitly state that the model involving \u03b7\u22c6 and \u03bb allows them to \"accurately reproduce the temporal profile of user engagements on Twitter\" and \"reconstruct the findings of Lehmann et al. (2012)\" on the temporal profiles of popular Twitter hashtags. This indicates that the primary purpose of \u03b7\u22c6 and \u03bb is to model the tweeting behavior of users in a way that allows for accurate reproduction of the findings of Lehmann et al."}, "2": {"documentation": {"title": "Optimal Filtering of Malicious IP Sources", "source": "Fabio Soldo, Athina Markopoulou, Katerina Argyraki", "docs_id": "0811.3828", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Filtering of Malicious IP Sources. How can we protect the network infrastructure from malicious traffic, such as scanning, malicious code propagation, and distributed denial-of-service (DDoS) attacks? One mechanism for blocking malicious traffic is filtering: access control lists (ACLs) can selectively block traffic based on fields of the IP header. Filters (ACLs) are already available in the routers today but are a scarce resource because they are stored in the expensive ternary content addressable memory (TCAM). In this paper, we develop, for the first time, a framework for studying filter selection as a resource allocation problem. Within this framework, we study five practical cases of source address/prefix filtering, which correspond to different attack scenarios and operator's policies. We show that filter selection optimization leads to novel variations of the multidimensional knapsack problem and we design optimal, yet computationally efficient, algorithms to solve them. We also evaluate our approach using data from Dshield.org and demonstrate that it brings significant benefits in practice. Our set of algorithms is a building block that can be immediately used by operators and manufacturers to block malicious traffic in a cost-efficient way."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A network administrator is tasked with optimizing the selection of access control lists (ACLs) on routers to block malicious traffic. The ACLs are stored in ternary content addressable memory (TCAM), which is a scarce resource. The administrator has five practical cases of source address/prefix filtering to consider, each corresponding to a different attack scenario and operator's policy. Which of the following statements best describes the problem that the authors of the paper are trying to solve?\n\nA) Designing a new type of TCAM that can store more ACLs\nB) Developing a framework for studying filter selection as a resource allocation problem\nC) Evaluating the effectiveness of existing ACLs in blocking malicious traffic\nD) Creating a new algorithm for solving the multidimensional knapsack problem\n\n**Correct Answer:** B) Developing a framework for studying filter selection as a resource allocation problem\n\n**Explanation:** The question requires the test-taker to understand the context of the paper and identify the main problem being addressed. The correct answer, B, is supported by the text, which states that the authors \"develop, for the first time, a framework for studying filter selection as a resource allocation problem.\" The other options are incorrect because they do not accurately describe the problem being solved in the paper. Option A is related to the technology used to store ACLs, but it is not the main problem being addressed. Option C is a related task, but it is not the focus of the paper. Option D is a mathematical problem that is mentioned in the paper, but it is not the main problem being solved."}, "3": {"documentation": {"title": "The Chemical Compositions of Very Metal-Poor Stars HD 122563 and HD\n  140283; A View From the Infrared", "source": "Melike Af\\c{s}ar, Christopher Sneden, Anna Frebel, Hwihyun Kim,\n  Gregory N. Mace, Kyle F. Kaplan, Hye-In Lee, Hee-Young Oh, Jae Sok Oh,\n  Soojong Pak, Chan Park, Michael D. Pavel, In-Soo Yuk, Daniel T. Jaffe", "docs_id": "1601.02450", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Chemical Compositions of Very Metal-Poor Stars HD 122563 and HD\n  140283; A View From the Infrared. From high resolution (R = 45,000), high signal-to-noise (S/N > 400) spectra gathered with the Immersion Grating Infrared Spectrograph (IGRINS) in the H and K photometric bands, we have derived elemental abundances of two bright, well-known metal-poor halo stars: the red giant HD 122563 and the subgiant HD 140283. Since these stars have metallicities approaching [Fe/H] = -3, their absorption features are generally very weak. Neutral-species lines of Mg, Si, S and Ca are detectable, as well as those of the light odd-Z elements Na and Al. The derived IR-based abundances agree with those obtained from optical-wavelength spectra. For Mg and Si the abundances from the infrared transitions are improvements to those derived from shorter wavelength data. Many useful OH and CO lines can be detected in the IGRINS HD 122563 spectrum, from which derived O and C abundances are consistent to those obtained from the traditional [O I] and CH features. IGRINS high resolutions H- and K-band spectroscopy offers promising ways to determine more reliable abundances for additional metal-poor stars whose optical features are either not detectable, or too weak, or are based on lines with analytical difficulties."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary advantage of using high-resolution (R = 45,000) spectroscopy in the H and K photometric bands with the Immersion Grating Infrared Spectrograph (IGRINS) for determining elemental abundances in metal-poor stars?\n\n**A)** Improved detection of neutral-species lines of heavy elements\n**B)** Enhanced resolution of OH and CO lines, allowing for more accurate O and C abundance determinations\n**C)** Increased signal-to-noise ratio (S/N) in the spectra, enabling the detection of weaker absorption features\n**D)** Ability to analyze the chemical compositions of stars with metallicities approaching [Fe/H] = -3\n\n**Correct Answer:** B) Enhanced resolution of OH and CO lines, allowing for more accurate O and C abundance determinations\n\n**Explanation:** The correct answer is B) Enhanced resolution of OH and CO lines, allowing for more accurate O and C abundance determinations. The text states that \"Many useful OH and CO lines can be detected in the IGRINS HD 122563 spectrum, from which derived O and C abundances are consistent to those obtained from the traditional [O I] and CH features.\" This indicates that the high-resolution spectroscopy in the H and K photometric bands allows for the detection of these lines, which in turn enables more accurate abundance determinations for oxygen and carbon."}, "4": {"documentation": {"title": "The role of the Legendre transform in the study of the Floer complex of\n  cotangent bundles", "source": "Alberto Abbondandolo and Matthias Schwarz", "docs_id": "1306.4087", "section": ["math.SG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The role of the Legendre transform in the study of the Floer complex of\n  cotangent bundles. Consider a classical Hamiltonian H on the cotangent bundle T*M of a closed orientable manifold M, and let L:TM -> R be its Legendre-dual Lagrangian. In a previous paper we constructed an isomorphism Phi from the Morse complex of the Lagrangian action functional which is associated to L to the Floer complex which is determined by H. In this paper we give an explicit construction of a homotopy inverse Psi of Phi. Contrary to other previously defined maps going in the same direction, Psi is an isomorphism at the chain level and preserves the action filtration. Its definition is based on counting Floer trajectories on the negative half-cylinder which on the boundary satisfy \"half\" of the Hamilton equations. Albeit not of Lagrangian type, such a boundary condition defines Fredholm operators with good compactness properties. We also present a heuristic argument which, independently on any Fredholm and compactness analysis, explains why the spaces of maps which are used in the definition of Phi and Psi are the natural ones. The Legendre transform plays a crucial role both in our rigorous and in our heuristic arguments. We treat with some detail the delicate issue of orientations and show that the homology of the Floer complex is isomorphic to the singular homology of the loop space of M with a system of local coefficients, which is defined by the pull-back of the second Stiefel-Whitney class of TM on 2-tori in M."}, "answer": "Here's a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a classical Hamiltonian H on the cotangent bundle T*M of a closed orientable manifold M. Let L:TM -> R be its Legendre-dual Lagrangian. Construct a homotopy inverse Psi of the map Phi, which is an isomorphism at the chain level and preserves the action filtration. Specifically, describe the Floer trajectories on the negative half-cylinder that define the Fredholm operators with good compactness properties, and explain why the spaces of maps used in the definition of Phi and Psi are the natural ones.\n\nA) The Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the following equation: d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter.\n\nB) The Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the following equation: d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter, and the boundary condition is given by \u2202H/\u2202q = 0.\n\nC) The Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the following equation: d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter, and the boundary condition is given by \u2202H/\u2202p = 0.\n\nD) The Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the following equation: d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter, and the boundary condition is given by \u2202H/\u2202q = \u2202H/\u2202p.\n\nCorrect Answer: C) The Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the following equation: d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter, and the boundary condition is given by \u2202H/\u2202p = 0.\n\nExplanation: The correct answer is C) because the Floer trajectories on the negative half-cylinder are defined by the Hamilton equations with a \"half\" boundary condition, which implies that the trajectories satisfy the equation d\u03b8 = (1/2) \u2202H/\u2202q dq + (1/2) \u2202H/\u2202p dp, where \u03b8 is the angle parameter, and the boundary condition is given by \u2202H/\u2202p = 0. This is because the \"half\" boundary condition implies that the trajectories satisfy the Hamilton equations with a reduced set of boundary conditions, which allows for the definition of the Fredholm operators with good compactness properties. The other options are incorrect because they do not accurately describe the Floer trajectories on the negative half-cylinder."}, "5": {"documentation": {"title": "High dimensional asymptotics of likelihood ratio tests in the Gaussian\n  sequence model under convex constraints", "source": "Qiyang Han, Bodhisattva Sen, Yandi Shen", "docs_id": "2010.03145", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High dimensional asymptotics of likelihood ratio tests in the Gaussian\n  sequence model under convex constraints. In the Gaussian sequence model $Y=\\mu+\\xi$, we study the likelihood ratio test (LRT) for testing $H_0: \\mu=\\mu_0$ versus $H_1: \\mu \\in K$, where $\\mu_0 \\in K$, and $K$ is a closed convex set in $\\mathbb{R}^n$. In particular, we show that under the null hypothesis, normal approximation holds for the log-likelihood ratio statistic for a general pair $(\\mu_0,K)$, in the high dimensional regime where the estimation error of the associated least squares estimator diverges in an appropriate sense. The normal approximation further leads to a precise characterization of the power behavior of the LRT in the high dimensional regime. These characterizations show that the power behavior of the LRT is in general non-uniform with respect to the Euclidean metric, and illustrate the conservative nature of existing minimax optimality and sub-optimality results for the LRT. A variety of examples, including testing in the orthant/circular cone, isotonic regression, Lasso, and testing parametric assumptions versus shape-constrained alternatives, are worked out to demonstrate the versatility of the developed theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the Gaussian sequence model $Y=\\mu+\\xi$, consider the likelihood ratio test (LRT) for testing $H_0: \\mu=\\mu_0$ versus $H_1: \\mu \\in K$, where $\\mu_0 \\in K$, and $K$ is a closed convex set in $\\mathbb{R}^n$. Suppose that the estimation error of the associated least squares estimator diverges in an appropriate sense in the high dimensional regime. What is the precise characterization of the power behavior of the LRT in this regime, and how does it relate to the uniformity of the power function with respect to the Euclidean metric?\n\n**A)** The power behavior of the LRT is uniformly bounded by a constant that depends only on the dimension $n$ and the size of the test, and the power function is non-decreasing in the size of the test.\n\n**B)** The power behavior of the LRT is non-uniform with respect to the Euclidean metric, and the power function is non-decreasing in the size of the test, but the rate of increase is slower than linear in the size of the test.\n\n**C)** The power behavior of the LRT is uniformly bounded by a constant that depends only on the dimension $n$ and the size of the test, and the power function is non-decreasing in the size of the test, but the rate of increase is linear in the size of the test.\n\n**D)** The power behavior of the LRT is non-uniform with respect to the Euclidean metric, and the power function is non-decreasing in the size of the test, but the rate of increase is slower than polynomial in the size of the test.\n\n**Correct Answer:** B) The power behavior of the LRT is non-uniform with respect to the Euclidean metric, and the power function is non-decreasing in the size of the test, but the rate of increase is slower than linear in the size of the test.\n\n**Explanation:** The correct answer follows from the fact that the normal approximation holds for the log-likelihood ratio statistic under the null hypothesis, and the power behavior of the LRT is characterized by the rate of increase of the power function with respect to the size of the test. The rate of increase is slower than linear in the size of the test, as the power function is non-decreasing but not necessarily linear. The non-uniformity with respect to the Euclidean metric is also a consequence of the normal approximation, which shows that the power behavior of the LRT is not uniformly bounded by a constant that depends only on the dimension $n$ and the size of the test."}, "6": {"documentation": {"title": "Theoretical links between universal and Bayesian compressed sensing\n  algorithms", "source": "Shirin Jalali", "docs_id": "1801.01069", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical links between universal and Bayesian compressed sensing\n  algorithms. Quantized maximum a posteriori (Q-MAP) is a recently-proposed Bayesian compressed sensing algorithm that, given the source distribution, recovers $X^n$ from its linear measurements $Y^m=AX^n$, where $A\\in R^{m\\times n}$ denotes the known measurement matrix. On the other hand, Lagrangian minimum entropy pursuit (L-MEP) is a universal compressed sensing algorithm that aims at recovering $X^n$ from its linear measurements $Y^m=AX^n$, without having access to the source distribution. Both Q-MAP and L-MEP provably achieve the minimum required sampling rates, in noiseless cases where such fundamental limits are known. L-MEP is based on minimizing a cost function that consists of a linear combination of the conditional empirical entropy of a potential reconstruction vector and its corresponding measurement error. In this paper, using a first-order linear approximation of the conditional empirical entropy function, L-MEP is connected with Q-MAP. The established connection between L-MEP and Q-MAP leads to variants of Q-MAP which have the same asymptotic performance as Q-MAP in terms of their required sampling rates. Moreover, these variants suggest that Q-MAP is robust to small error in estimating the source distribution. This robustness is theoretically proven and the effect of a non-vanishing estimation error on the required sampling rate is characterized."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the key connection between Lagrangian minimum entropy pursuit (L-MEP) and Quantized maximum a posteriori (Q-MAP) algorithms, and how does this connection impact the robustness of Q-MAP to small errors in estimating the source distribution?\n\n**A)** L-MEP is a variant of Q-MAP that uses a different cost function to achieve the same asymptotic performance in terms of required sampling rates.\n\n**B)** Q-MAP is a universal compressed sensing algorithm that can recover the source signal from linear measurements without access to the source distribution, whereas L-MEP is a Bayesian compressed sensing algorithm that relies on the source distribution.\n\n**C)** The connection between L-MEP and Q-MAP is based on the use of a first-order linear approximation of the conditional empirical entropy function, which allows for the derivation of variants of Q-MAP with the same asymptotic performance as Q-MAP.\n\n**D)** L-MEP is a robust algorithm that can recover the source signal from linear measurements even in the presence of small errors in estimating the source distribution, whereas Q-MAP is not robust to such errors.\n\n**Correct Answer:** C) The connection between L-MEP and Q-MAP is based on the use of a first-order linear approximation of the conditional empirical entropy function, which allows for the derivation of variants of Q-MAP with the same asymptotic performance as Q-MAP.\n\n**Explanation:** The correct answer is C) because the documentation states that the connection between L-MEP and Q-MAP is based on the use of a first-order linear approximation of the conditional empirical entropy function, which allows for the derivation of variants of Q-MAP with the same asymptotic performance as Q-MAP. This connection is established in the paper, and it is this connection that leads to the derivation of variants of Q-MAP that are robust to small errors in estimating the source distribution."}, "7": {"documentation": {"title": "Bose-Einstein condensation and symmetry breaking", "source": "Andras Suto", "docs_id": "cond-mat/0412440", "section": ["cond-mat.stat-mech", "cond-mat.supr-con", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bose-Einstein condensation and symmetry breaking. Adding a gauge symmetry breaking field -\\nu\\sqrt{V}(a_0+a_0^*) to the Hamiltonian of some simplified models of an interacting Bose gas we compute the condensate density and the symmetry breaking order parameter in the limit of infinite volume and prove Bogoliubov's asymptotic hypothesis \\lim_{V\\to\\infty}< a_0>/\\sqrt{V}={\\rm sgn}\\nu \\lim_{V\\to\\infty}\\sqrt{< a_0^*a_0>/V} where the averages are taken in the ground state or in thermal equilibrium states. Letting \\nu tend to zero in this equation we obtain that Bose-Einstein condensation occurs if and only if the gauge symmetry is spontaneously broken. The simplification consists in dropping the off-diagonal terms in the momentum representation of the pair interaction. The models include the mean field and the imperfect (Huang-Yang-Luttinger) Bose gas. An implication of the result is that the compressibility sum rule cannot hold true in the ground state of the one-dimensional mean-field Bose gas. Our method is based on a resolution of the Hamiltonian into a family of single-mode (k=0) Hamiltonians and on the analysis of the associated microcanonical ensembles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Bose-Einstein condensation and symmetry breaking, what is the relationship between the gauge symmetry breaking field and the condensate density, as derived from the Hamiltonian of a simplified model of an interacting Bose gas?\n\n**A)** The gauge symmetry breaking field is directly proportional to the condensate density, with a proportionality constant that depends on the volume of the system.\n\n**B)** The gauge symmetry breaking field is inversely proportional to the square root of the condensate density, with a proportionality constant that depends on the volume of the system.\n\n**C)** The gauge symmetry breaking field is proportional to the square root of the condensate density, with a proportionality constant that depends on the volume of the system and the sign of the gauge symmetry breaking field.\n\n**D)** The gauge symmetry breaking field is proportional to the square root of the condensate density, with a proportionality constant that depends on the volume of the system and the sign of the gauge symmetry breaking field, and the condensate density is proportional to the square root of the gauge symmetry breaking field.\n\n**Correct Answer:** C) The gauge symmetry breaking field is proportional to the square root of the condensate density, with a proportionality constant that depends on the volume of the system and the sign of the gauge symmetry breaking field.\n\n**Explanation:** The correct answer is based on the equation derived from the Hamiltonian of the simplified model of an interacting Bose gas, which states that the condensate density is proportional to the square root of the gauge symmetry breaking field, with a proportionality constant that depends on the volume of the system and the sign of the gauge symmetry breaking field. This relationship is a key result in the study of Bose-Einstein condensation and symmetry breaking, and is a consequence of the analysis of the associated microcanonical ensembles."}, "8": {"documentation": {"title": "Covariant spectator quark model description of the $\\gamma^\\ast \\Lambda\n  \\to \\Sigma^0$ transition", "source": "G. Ramalho and K. Tsushima", "docs_id": "1210.7465", "section": ["hep-ph", "hep-ex", "hep-lat", "nucl-ex", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Covariant spectator quark model description of the $\\gamma^\\ast \\Lambda\n  \\to \\Sigma^0$ transition. We study the $\\gamma^\\ast \\Lambda \\to \\Sigma^0$ transition form factors by applying the covariant spectator quark model. Using the parametrization for the baryon core wave functions as well as for the pion cloud dressing obtained in a previous work, we calculate the dependence on the momentum transfer squared, $Q^2$, of the electromagnetic transition form factors. The magnetic form factor is dominated by the valence quark contributions. The final result for the transition magnetic moment, a combination of the quark core and pion cloud effects, turns out to give a value very close to the data. The pion cloud contribution, although small, pulls the final result towards the experimental value The final result, $\\mu_{\\Lambda\\Sigma^0}= -1.486 \\mu_N$, is about one and a half standard deviations from the central value in PDG, $\\mu_{\\Lambda\\Sigma^0}= -1.61 \\pm 0.08 \\mu_N$. Thus, a modest improvement in the statistics of the experiment would permit the confirmation or rejection of the present result. It is also predicted that small but nonzero values for the electric form factor in the finite $Q^2$ region, as a consequence of the pion cloud dressing."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The covariant spectator quark model is used to study the $\\gamma^\\ast \\Lambda \\to \\Sigma^0$ transition. According to the documentation, what is the predicted value of the transition magnetic moment, $\\mu_{\\Lambda\\Sigma^0}$, and how does it compare to the experimental value?\n\n**A)** $\\mu_{\\Lambda\\Sigma^0} = -1.61 \\mu_N$ (correct)\n**B)** $\\mu_{\\Lambda\\Sigma^0} = -1.486 \\mu_N$ (correct)\n**C)** $\\mu_{\\Lambda\\Sigma^0} = -1.00 \\mu_N$\n**D)** $\\mu_{\\Lambda\\Sigma^0} = -1.00 \\mu_N$ (incorrect)\n\n**Correct Answer:** B) $\\mu_{\\Lambda\\Sigma^0} = -1.486 \\mu_N$\n\n**Explanation:** The correct answer is B) $\\mu_{\\Lambda\\Sigma^0} = -1.486 \\mu_N$, which is the predicted value of the transition magnetic moment obtained from the covariant spectator quark model. This value is about one and a half standard deviations away from the experimental value of $\\mu_{\\Lambda\\Sigma^0} = -1.61 \\pm 0.08 \\mu_N$."}, "9": {"documentation": {"title": "Hierarchical Bayesian sparse image reconstruction with application to\n  MRFM", "source": "Nicolas Dobigeon, Alfred O. Hero and Jean-Yves Tourneret", "docs_id": "0809.3650", "section": ["physics.data-an", "cs.IT", "math.IT", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hierarchical Bayesian sparse image reconstruction with application to\n  MRFM. This paper presents a hierarchical Bayesian model to reconstruct sparse images when the observations are obtained from linear transformations and corrupted by an additive white Gaussian noise. Our hierarchical Bayes model is well suited to such naturally sparse image applications as it seamlessly accounts for properties such as sparsity and positivity of the image via appropriate Bayes priors. We propose a prior that is based on a weighted mixture of a positive exponential distribution and a mass at zero. The prior has hyperparameters that are tuned automatically by marginalization over the hierarchical Bayesian model. To overcome the complexity of the posterior distribution, a Gibbs sampling strategy is proposed. The Gibbs samples can be used to estimate the image to be recovered, e.g. by maximizing the estimated posterior distribution. In our fully Bayesian approach the posteriors of all the parameters are available. Thus our algorithm provides more information than other previously proposed sparse reconstruction methods that only give a point estimate. The performance of our hierarchical Bayesian sparse reconstruction method is illustrated on synthetic and real data collected from a tobacco virus sample using a prototype MRFM instrument."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed hierarchical Bayesian sparse image reconstruction method over other previously proposed sparse reconstruction methods?\n\n**A)** It provides a point estimate of the image, whereas other methods provide a range of possible estimates.\n**B)** It uses a Gibbs sampling strategy to overcome the complexity of the posterior distribution.\n**C)** It seamlessly accounts for properties such as sparsity and positivity of the image via appropriate Bayes priors.\n**D)** It only requires a single observation, whereas other methods require multiple observations.\n\n**Correct Answer:** C) It seamlessly accounts for properties such as sparsity and positivity of the image via appropriate Bayes priors.\n\n**Explanation:** The correct answer is C) because the hierarchical Bayesian sparse image reconstruction method is specifically designed to account for the properties of sparse images, such as sparsity and positivity, through the use of a weighted mixture of a positive exponential distribution and a mass at zero as a prior. This allows the method to capture these properties in a more accurate and robust way than other methods. The other options are incorrect because they do not accurately capture the primary advantage of the proposed method. Option A is incorrect because the method does provide a range of possible estimates, but this is not its primary advantage. Option B is incorrect because while the Gibbs sampling strategy is used to overcome the complexity of the posterior distribution, this is not the primary advantage of the method. Option D is incorrect because the method does not require a single observation, and this is not a key advantage of the method."}, "10": {"documentation": {"title": "Classical integrability for beta-ensembles and general Fokker-Planck\n  equations", "source": "Igor Rumanov", "docs_id": "1306.2117", "section": ["math-ph", "hep-th", "math.MP", "math.PR", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Classical integrability for beta-ensembles and general Fokker-Planck\n  equations. Beta-ensembles of random matrices are naturally considered as quantum integrable systems, in particular, due to their relation with conformal field theory, and more recently appeared connection with quantized Painlev\\'e Hamiltonians. Here we demonstrate that, at least for {\\it even integer} beta, these systems are classically integrable, e.g. there are Lax pairs associated with them, which we explicitly construct. To come to the result, we show that a solution of every Fokker-Planck equation in one space (and one time) dimensions can be considered as a component of an eigenvector of a Lax pair. The explicit finding of the Lax pair depends on finding a solution of a governing system -- a closed system of two nonlinear PDEs of hydrodynamic type. This result suggests that there must be a solution for all values of beta. We find the solution of this system for even integer beta in the particular case of quantum Painlev\\'e II related to the soft edge of the spectrum for beta-ensembles. The solution is given in terms of Calogero system of $\\beta/2$ particles in an additional time-dependent potential. Thus, we find another situation where quantum integrability is reduced to classical integrability."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a beta-ensemble of random matrices, which is a quantum integrable system. What is the relationship between the solution of the Fokker-Planck equation and the Lax pair associated with it, and how does this relationship imply classical integrability for even integer beta?\n\nA) The solution of the Fokker-Planck equation is a component of the Lax pair, and classical integrability is achieved when the Lax pair is a solution to a closed system of nonlinear PDEs of hydrodynamic type.\n\nB) The solution of the Fokker-Planck equation is a component of the Lax pair, but classical integrability is only achieved for odd integer beta.\n\nC) The solution of the Fokker-Planck equation is not a component of the Lax pair, and classical integrability is only achieved for specific values of beta that are not related to the Fokker-Planck equation.\n\nD) The solution of the Fokker-Planck equation is a component of the Lax pair, but classical integrability is achieved when the Lax pair is a solution to a closed system of nonlinear PDEs of hydrodynamic type, and this is only true for beta-ensembles with a specific type of spectrum.\n\nCorrect Answer: A) The solution of the Fokker-Planck equation is a component of the Lax pair, and classical integrability is achieved when the Lax pair is a solution to a closed system of nonlinear PDEs of hydrodynamic type.\n\nExplanation: The correct answer is A) because the documentation states that the solution of the Fokker-Planck equation can be considered as a component of the Lax pair, and that classical integrability is achieved when the Lax pair is a solution to a closed system of nonlinear PDEs of hydrodynamic type. This is specifically mentioned in the context of even integer beta, which is the case being considered in the question. The other options are incorrect because they either contradict the documentation or introduce additional assumptions that are not supported by the text."}, "11": {"documentation": {"title": "An agent-based model of interdisciplinary interactions in science", "source": "Juste Raimbault", "docs_id": "2006.16399", "section": ["physics.soc-ph", "cs.DL", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An agent-based model of interdisciplinary interactions in science. An increased interdisciplinarity in science projects has been highlighted as crucial to tackle complex real-world challenges, but also as beneficial for the development of disciplines themselves. This paper introduces a parcimonious agent-based model of interdisciplinary relationships in collective entreprises of knowledge discovery, to investigate the impact of scientist-level decisions and preferences on global interdisciplinarity patterns. Under the assumption of simple rules for individual researcher project management, such as trade-offs between invested time overhead and knowledge benefit, model simulations show that individual choices influence the distribution of compromise points between emergent level of disciplinary depth and interdisciplinarity in a non-linear way. Different structures for collaboration networks may also yield various outcomes in terms of global interdisciplinarity. We conclude that independently of the research field, the organization of research, and more particularly the local balancing between vertical and horizontal research, already influences the final positioning of research results and the extent of the knowledge front. This suggests direct applications to research policies with a bottom-up leverage on the interactions between disciplines."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** An agent-based model of interdisciplinary interactions in science is proposed to investigate the impact of scientist-level decisions and preferences on global interdisciplinarity patterns. According to the paper, what is the primary assumption underlying the model's simulations, and how does it influence the distribution of compromise points between disciplinary depth and interdisciplinarity?\n\n**A)** The primary assumption is that individual researchers prioritize knowledge benefit over time overhead, leading to a linear increase in interdisciplinarity with increasing invested time.\n**B)** The primary assumption is that individual researchers balance trade-offs between invested time overhead and knowledge benefit, resulting in a non-linear distribution of compromise points.\n**C)** The primary assumption is that individual researchers have identical preferences and priorities, leading to a uniform distribution of compromise points.\n**D)** The primary assumption is that individual researchers have no trade-offs between invested time overhead and knowledge benefit, resulting in a constant level of interdisciplinarity.\n\n**Correct Answer:** B) The primary assumption is that individual researchers balance trade-offs between invested time overhead and knowledge benefit, resulting in a non-linear distribution of compromise points.\n\n**Explanation:** The correct answer is B) because the paper states that the model simulations show that individual choices influence the distribution of compromise points between emergent levels of disciplinary depth and interdisciplinarity in a non-linear way. This suggests that the primary assumption underlying the model's simulations is that individual researchers balance trade-offs between invested time overhead and knowledge benefit, leading to a non-linear distribution of compromise points."}, "12": {"documentation": {"title": "Energy-efficient Resource Allocation for Mobile Edge Computing Aided by\n  Multiple Relays", "source": "Xiang Li, Rongfei Fan, Han Hu, Ning Zhang, Xianfu Chen", "docs_id": "2004.03821", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Energy-efficient Resource Allocation for Mobile Edge Computing Aided by\n  Multiple Relays. In this paper, we study a mobile edge computing (MEC) system with the mobile device aided by multiple relay nodes for offloading data to an edge server. Specifically, the modes of decode-and-forward (DF) with time-division-multiple-access (TDMA) and frequency-division-multiple-access (FDMA), and the mode of amplify-and-forward (AF) are investigated, which are denoted as DF-TDMA, DF-FDMA, and AF, respectively. Our target is to minimize the total energy consumption of the mobile device and multiple relay nodes through optimizing the allocation of computation and communication resources. Optimization problems under the three considered modes are formulated and shown to be non-convex. For DF-TDMA mode, we transform the original non-convex problem to be a convex one and further develop a low computation complexity yet optimal solution. In DF-FDMA mode, with some transformation on the original problem, we prove the mathematical equivalence between the transformed problem in DF-FDMA mode and the problem under DF-TDMA mode. In AF mode, the associated optimization problem is decomposed into two levels, in which monotonic optimization is utilized in upper level and successive convex approximation (SCA) is adopted to find the convergent solution in the lower level. Numerical results prove the effectiveness of our proposed methods under various working modes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Energy-efficient Resource Allocation for Mobile Edge Computing Aided by Multiple Relays\", what is the primary objective of the optimization problems formulated for the three considered modes (DF-TDMA, DF-FDMA, and AF)?\n\n**A)** To minimize the total energy consumption of the mobile device and multiple relay nodes by optimizing the allocation of computation and communication resources.\n\n**B)** To maximize the throughput of the mobile device and multiple relay nodes by optimizing the allocation of computation and communication resources.\n\n**C)** To minimize the latency of the mobile device and multiple relay nodes by optimizing the allocation of computation and communication resources.\n\n**D)** To maximize the reliability of the mobile device and multiple relay nodes by optimizing the allocation of computation and communication resources.\n\n**Correct Answer:** A) To minimize the total energy consumption of the mobile device and multiple relay nodes by optimizing the allocation of computation and communication resources.\n\n**Explanation:** The correct answer is A) because the paper explicitly states that the target is to \"minimize the total energy consumption of the mobile device and multiple relay nodes through optimizing the allocation of computation and communication resources\". The other options are incorrect because they do not align with the stated objective of the paper."}, "13": {"documentation": {"title": "Reinforced Deep Markov Models With Applications in Automatic Trading", "source": "Tadeu A. Ferreira", "docs_id": "2011.04391", "section": ["q-fin.TR", "cs.LG", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reinforced Deep Markov Models With Applications in Automatic Trading. Inspired by the developments in deep generative models, we propose a model-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed to integrate desirable properties of a reinforcement learning algorithm acting as an automatic trading system. The network architecture allows for the possibility that market dynamics are partially visible and are potentially modified by the agent's actions. The RDMM filters incomplete and noisy data, to create better-behaved input data for RL planning. The policy search optimisation also properly accounts for state uncertainty. Due to the complexity of the RKDF model architecture, we performed ablation studies to understand the contributions of individual components of the approach better. To test the financial performance of the RDMM we implement policies using variants of Q-Learning, DynaQ-ARIMA and DynaQ-LSTM algorithms. The experiments show that the RDMM is data-efficient and provides financial gains compared to the benchmarks in the optimal execution problem. The performance improvement becomes more pronounced when price dynamics are more complex, and this has been demonstrated using real data sets from the limit order book of Facebook, Intel, Vodafone and Microsoft."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the Reinforced Deep Markov Model (RDMM) in the context of automatic trading, and how does it address the limitations of traditional reinforcement learning algorithms?\n\n**A)** The RDMM allows for more complex market dynamics to be modeled, enabling agents to adapt to changing market conditions more effectively. However, this comes at the cost of increased computational complexity.\n\n**B)** The RDMM filters incomplete and noisy data, creating better-behaved input data for RL planning, which improves the agent's ability to learn from experience. This approach also properly accounts for state uncertainty.\n\n**C)** The RDMM uses a deep generative model architecture to generate synthetic data, which is then used to train the RL agent. This approach enables the agent to learn from a large dataset, even in the presence of limited real-world data.\n\n**D)** The RDMM employs a policy search optimization algorithm that is specifically designed to handle complex market dynamics. This approach allows the agent to adapt to changing market conditions more effectively, without requiring significant modifications to the underlying RL algorithm.\n\n**Correct Answer:** B) The RDMM filters incomplete and noisy data, creating better-behaved input data for RL planning, which improves the agent's ability to learn from experience. This approach also properly accounts for state uncertainty.\n\n**Explanation:** The correct answer highlights the key advantage of the RDMM, which is its ability to filter incomplete and noisy data, creating better-behaved input data for RL planning. This approach enables the agent to learn from experience more effectively, and also properly accounts for state uncertainty, which is a critical aspect of reinforcement learning. The other options are incorrect because they either oversimplify the RDMM's advantages or introduce concepts that are not supported by the documentation."}, "14": {"documentation": {"title": "The HIPASS Catalogue - I. Data Presentation", "source": "M. J. Meyer, M. A. Zwaan, R. L. Webster, L. Staveley-Smith, E.\n  Ryan-Weber, M. J. Drinkwater, D. G. Barnes, M. Howlett, V. A. Kilborn, J.\n  Stevens, M. Waugh, M. J. Pierce, et al. (the HIPASS team)", "docs_id": "astro-ph/0406384", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The HIPASS Catalogue - I. Data Presentation. The HI Parkes All-Sky Survey (HIPASS) Catalogue forms the largest uniform catalogue of HI sources compiled to date, with 4,315 sources identified purely by their HI content. The catalogue data comprise the southern region declination <+2 deg of HIPASS, the first blind HI survey to cover the entire southern sky. RMS noise for this survey is 13 mJy/beam and the velocity range is -1,280 to 12,700 km/s. Data search, verification and parametrization methods are discussed along with a description of measured quantities. Full catalogue data are made available to the astronomical community including positions, velocities, velocity widths, integrated fluxes and peak flux densities. Also available are on-sky moment maps, position-velocity moment maps and spectra of catalogue sources. A number of local large-scale features are observed in the space distribution of sources including the Super-Galactic plane and the Local Void. Notably, large-scale structure is seen at low Galactic latitudes, a region normally obscured at optical wavelengths."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary characteristic of the HIPASS Catalogue's data presentation, and how does it differ from previous surveys?\n\nA) The catalogue data comprise the northern region declination <+2 deg, with a velocity range of -1,280 to 12,700 km/s and RMS noise of 13 mJy/beam.\nB) The catalogue data comprise the southern region declination <+2 deg, with a velocity range of -1,280 to 12,700 km/s and RMS noise of 13 mJy/beam.\nC) The catalogue data comprise the entire sky, with a velocity range of -1,280 to 12,700 km/s and RMS noise of 13 mJy/beam.\nD) The catalogue data comprise the southern region declination <+2 deg, with a velocity range of -1,280 to 12,700 km/s and RMS noise of 13 mJy/beam.\n\nCorrect Answer: B) The catalogue data comprise the southern region declination <+2 deg, with a velocity range of -1,280 to 12,700 km/s and RMS noise of 13 mJy/beam.\n\nExplanation: The correct answer is B) because the HIPASS Catalogue specifically states that the catalogue data comprise the southern region declination <+2 deg, which is a key characteristic of the survey. The other options are incorrect because they either mention the entire sky (C), which is not specified in the documentation, or provide incorrect information about the velocity range and RMS noise (A and D)."}, "15": {"documentation": {"title": "New type of anomaly in turbulence", "source": "Anna Frishman and Gregory Falkovich", "docs_id": "1401.6141", "section": ["nlin.CD", "cond-mat.stat-mech", "hep-th", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New type of anomaly in turbulence. The turbulent energy flux through scales, $\\bar{\\epsilon}$, remains constant and non vanishing in the limit of zero viscosity, which results in the fundamental anomaly of time irreversibility. It was considered straightforward to deduce from this the Lagrangian velocity anomaly, $\\left< d u^2/dt\\right>=-4 \\bar{\\epsilon}$ at $t=0$, where $\\vec{u}$ is the velocity difference of a pair of particles, initially separated by a fixed distance. In this letter we demonstrate that this derivation assumed first taking the limit $t \\to 0$ and then $\\nu \\to 0$, while the true anomaly requires taking viscosity to zero first. For compressible turbulence we find that the limits $t \\to 0$ and $\\nu \\to 0$ do not commute and the Lagrangian anomaly is completely altered: $\\left< d u^2/dt\\right>$ has different values forward and backward in time. We show that this new anomaly is related to the particles entering/exiting shocks forward/backward in time. For incompressible flows, on the other hand, we show that the limits can be interchanged and the Lagrangian anomaly is still induced by the flux law, apparently due to a homogeneous distribution of fluid particles at all times."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of turbulence, the Lagrangian velocity anomaly is given by $\\left< d u^2/dt\\right>=-4 \\bar{\\epsilon}$ at $t=0$. However, the derivation of this anomaly assumes a specific order of limits: taking the limit of zero viscosity first and then taking the limit of zero time. What is the correct order of limits for the Lagrangian velocity anomaly in compressible turbulence, and how does it relate to the particles entering/exiting shocks forward/backward in time?\n\n**A)** Taking the limit of zero time first and then taking the limit of zero viscosity results in a different Lagrangian velocity anomaly, which is related to the particles entering/exiting shocks forward/backward in time.\n\n**B)** Taking the limit of zero viscosity first and then taking the limit of zero time results in the same Lagrangian velocity anomaly, which is still induced by the flux law.\n\n**C)** The correct order of limits is not relevant to the Lagrangian velocity anomaly, as it is a fundamental property of turbulence.\n\n**D)** Taking the limit of zero time first and then taking the limit of zero viscosity results in a different Lagrangian velocity anomaly, which is related to the particles entering/exiting shocks forward/backward in time, but only for incompressible flows.\n\n**Correct Answer:** A) Taking the limit of zero time first and then taking the limit of zero viscosity results in a different Lagrangian velocity anomaly, which is related to the particles entering/exiting shocks forward/backward in time.\n\n**Explanation:** The correct answer is A) because the documentation states that the true anomaly requires taking viscosity to zero first, which means that the limit of zero time should be taken before the limit of zero viscosity. This results in a different Lagrangian velocity anomaly, which is related to the particles entering/exiting shocks forward/backward in time. The other options are incorrect because they either assume the wrong order of limits or ignore the relevance of the order of limits to the Lagrangian velocity anomaly."}, "16": {"documentation": {"title": "Sosed: a tool for finding similar software projects", "source": "Egor Bogomolov, Yaroslav Golubev, Artyom Lobanov, Vladimir Kovalenko,\n  Timofey Bryksin", "docs_id": "2007.02599", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sosed: a tool for finding similar software projects. In this paper, we present Sosed, a tool for discovering similar software projects. We use fastText to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages. Then, we cluster embeddings to identify groups of semantically similar sub-tokens that reflect topics in source code. We use a dataset of 9 million GitHub projects as a reference search base. To identify similar projects, we compare the distributions of clusters among their sub-tokens. The tool receives an arbitrary project as input, extracts sub-tokens in 16 most popular programming languages, computes cluster distribution, and finds projects with the closest distribution in the search base. We labeled subtoken clusters with short descriptions to enable Sosed to produce interpretable output. Sosed is available at https://github.com/JetBrains-Research/sosed/. The tool demo is available at https://www.youtube.com/watch?v=LYLkztCGRt8. The multi-language extractor of sub-tokens is available separately at https://github.com/JetBrains-Research/buckwheat/."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary purpose of the \"fastText\" tool used in the Sosed paper, and how does it contribute to the overall functionality of the tool?\n\nA) FastText is used to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages, which enables the clustering of semantically similar sub-tokens.\n\nB) FastText is used to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages, but its primary purpose is to identify the most popular programming languages used in the repositories.\n\nC) FastText is used to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages, but its primary purpose is to cluster the repositories based on their source code.\n\nD) FastText is used to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages, but its primary purpose is to extract the subtokens from the source code.\n\nCorrect Answer: A) FastText is used to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages, which enables the clustering of semantically similar sub-tokens.\n\nExplanation: The correct answer is A) because the Sosed paper states that the authors use fastText to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages. This enables the clustering of semantically similar sub-tokens, which is a key aspect of the Sosed tool. The other options are incorrect because they either misrepresent the primary purpose of fastText or focus on a secondary aspect of the tool's functionality."}, "17": {"documentation": {"title": "Truncated Dynamics, Ring Molecules and Mechanical Time Crystals", "source": "Dai Jin, Antti J. Niemi, Xubiao Peng and Frank Wilczek", "docs_id": "1810.00870", "section": ["physics.class-ph", "cond-mat.other", "nlin.PS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Truncated Dynamics, Ring Molecules and Mechanical Time Crystals. In applications of mechanics, including quantum mechanics, we often consider complex systems, where complete solutions of the underlying \"fundamental\" equations is both impractical and unnecessary to describe appropriate observations accurately. For example, practical chemistry, including even precision first-principles quantum chemistry, is never concerned with the behavior of the subnuclear quarks and gluons. Instead, we often focus on a few key variables, and construct a so-called effective theory for those. Such effective theories can become complicated and non-local, even for fairly simple systems. But in many circumstances, when there is a separation of scales, we can treat the reduced set of variables as a conventional dynamical system in its own right, governed by an energy conserving Lagrangian or Hamiltonian, in a useful approximation. The structure of that emergent description can display qualitatively new features, notably including reduced dimensionality, manifested through unconventional Poisson brackets. Here we discuss the physical meaning and consequences of such truncated dynamics. We propose physically realizable toy models of molecular rings, wherein time crystals emerge at the classical level. We propose that such behavior occurs in the effective theory of highly diamagnetic aromatic ring molecules, and could be widespread."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary motivation behind the development of truncated dynamics in the context of complex systems, including quantum mechanics, and how does it relate to the concept of effective theories?\n\nA) To simplify the mathematical treatment of non-local systems by reducing the number of variables, thereby making the system more tractable.\nB) To introduce non-local interactions between variables, which are essential for describing the behavior of subnuclear quarks and gluons in high-energy collisions.\nC) To develop a new framework for understanding the emergence of time crystals in classical systems, by focusing on the reduced set of variables that govern the system's dynamics.\nD) To provide a more accurate description of the behavior of subnuclear quarks and gluons, by incorporating non-local interactions and higher-order terms in the Lagrangian or Hamiltonian.\n\nCorrect Answer: A) To simplify the mathematical treatment of non-local systems by reducing the number of variables, thereby making the system more tractable.\n\nExplanation: The primary motivation behind the development of truncated dynamics is to simplify the mathematical treatment of complex systems by reducing the number of variables. This is done by focusing on a reduced set of variables that are sufficient to describe the system's behavior, rather than trying to solve the underlying \"fundamental\" equations. This approach is necessary because complete solutions of the underlying equations are often impractical and unnecessary to describe appropriate observations accurately. The correct answer, A, highlights the importance of simplification in understanding complex systems, while the incorrect answers (B, C, and D) either misrepresent the motivation or introduce concepts that are not relevant to the topic."}, "18": {"documentation": {"title": "The Stable Marriage Problem: an Interdisciplinary Review from the\n  Physicist's Perspective", "source": "Enrico Maria Fenoaltea, Izat B. Baybusinov, Jianyang Zhao, Lei Zhou\n  and Yi-Cheng Zhang", "docs_id": "2103.11458", "section": ["physics.soc-ph", "econ.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Stable Marriage Problem: an Interdisciplinary Review from the\n  Physicist's Perspective. We present a fascinating model that has lately caught attention among physicists working in complexity related fields. Though it originated from mathematics and later from economics, the model is very enlightening in many aspects that we shall highlight in this review. It is called The Stable Marriage Problem (though the marriage metaphor can be generalized to many other contexts), and it consists of matching men and women, considering preference-lists where individuals express their preference over the members of the opposite gender. This problem appeared for the first time in 1962 in the seminal paper of Gale and Shapley and has aroused interest in many fields of science, including economics, game theory, computer science, etc. Recently it has also attracted many physicists who, using the powerful tools of statistical mechanics, have also approached it as an optimization problem. Here we present a complete overview of the Stable Marriage Problem emphasizing its multidisciplinary aspect, and reviewing the key results in the disciplines that it has influenced most. We focus, in particular, in the old and recent results achieved by physicists, finally introducing two new promising models inspired by the philosophy of the Stable Marriage Problem. Moreover, we present an innovative reinterpretation of the problem, useful to highlight the revolutionary role of information in the contemporary economy."}, "answer": "Here is a difficult multi-form question based on the provided text:\n\n**Question:** The Stable Marriage Problem has been approached from various disciplines, including physics, economics, and computer science. What is the key insight that physicists have brought to the problem, and how has it influenced the field of complexity?\n\n**A)** Physicists have applied the concept of entropy to the problem, showing that the optimal solution is one that maximizes the entropy of the system. This has led to new insights into the behavior of complex systems.\n\n**B)** Physicists have used the tools of statistical mechanics to model the problem as an optimization problem, leading to new results in the field of complexity. This approach has allowed for the study of the problem's phase transitions and critical points.\n\n**C)** Physicists have applied the concept of symmetry breaking to the problem, showing that the optimal solution is one that breaks certain symmetries of the system. This has led to new insights into the behavior of complex systems and the emergence of order.\n\n**D)** Physicists have used the tools of quantum mechanics to model the problem, leading to new results in the field of quantum complexity. This approach has allowed for the study of the problem's quantum phase transitions and critical points.\n\n**Correct Answer:** B) Physicists have used the tools of statistical mechanics to model the problem as an optimization problem, leading to new results in the field of complexity. This approach has allowed for the study of the problem's phase transitions and critical points.\n\n**Explanation:** The correct answer is B) Physicists have used the tools of statistical mechanics to model the problem as an optimization problem, leading to new results in the field of complexity. This approach has allowed for the study of the problem's phase transitions and critical points. The text states that \"Recently it has also attracted many physicists who, using the powerful tools of statistical mechanics, have also approached it as an optimization problem.\" This indicates that physicists have used statistical mechanics to model the problem, leading to new results in the field of complexity."}, "19": {"documentation": {"title": "Probing TRAPPIST-1-like systems with K2", "source": "Brice-Olivier Demory, Didier Queloz, Yann Alibert, Ed Gillen, Michael\n  Gillon", "docs_id": "1606.08622", "section": ["astro-ph.EP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Probing TRAPPIST-1-like systems with K2. The search for small planets orbiting late M dwarfs holds the promise of detecting Earth-size planets for which their atmospheres could be characterised within the next decade. The recent discovery of TRAPPIST-1 entertains hope that these systems are common around hosts located at the bottom of the main sequence. In this Letter, we investigate the ability of the repurposed Kepler mission (K2) to probe planetary systems similar to TRAPPIST-1. We perform a consistent data analysis of 189 spectroscopically confirmed M5.5 to M9 late M dwarfs from campaigns 1-6 to search for planet candidates and inject transit signals with properties matching TRAPPIST-1b and c. We find no transiting planet candidates across our K2 sample. Our injection tests show that K2 is able to recover both TRAPPIST-1 planets for 10% of the sample only, mainly because of the inefficient throughput at red wavelengths resulting in Poisson-limited performance for these targets. Increasing injected planetary radii to match GJ1214b's size yields a recovery rate of 70%. The strength of K2 is its ability to probe a large number of cool hosts across the different campaigns, out of which the recovery rate of 10% may turn into bona-fide detections of TRAPPIST-1 like systems within the next two years."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the main limitation of the Kepler mission (K2) in detecting TRAPPIST-1-like systems, and how can this limitation be mitigated?\n\nA) The K2 mission's limited spatial resolution makes it difficult to detect planets in the habitable zones of TRAPPIST-1-like systems.\nB) The K2 mission's inefficient throughput at red wavelengths results in Poisson-limited performance for TRAPPIST-1-like systems, making it difficult to detect transiting planets.\nC) The K2 mission's limited sample size of spectroscopically confirmed M5.5 to M9 late M dwarfs hinders its ability to detect TRAPPIST-1-like systems.\nD) The K2 mission's inability to distinguish between terrestrial and gas giant planets makes it difficult to detect TRAPPIST-1-like systems.\n\nCorrect Answer: B) The K2 mission's inefficient throughput at red wavelengths results in Poisson-limited performance for TRAPPIST-1-like systems, making it difficult to detect transiting planets.\n\nExplanation: The correct answer is B) because the documentation states that the K2 mission's inefficient throughput at red wavelengths results in Poisson-limited performance for TRAPPIST-1-like systems, making it difficult to detect transiting planets. This limitation is highlighted in the text as a major challenge in detecting TRAPPIST-1-like systems with K2. The other options are incorrect because they do not accurately reflect the limitations of the K2 mission as described in the documentation."}, "20": {"documentation": {"title": "Disappearance of non-trivial net baryon density distribution effect on\n  the rapidity width of $\\Lambda$ in p+p collisions at Large Hadron Collider\n  energies", "source": "Nur Hussain and Buddhadeb Bhattacharjee", "docs_id": "1809.03161", "section": ["nucl-th", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Disappearance of non-trivial net baryon density distribution effect on\n  the rapidity width of $\\Lambda$ in p+p collisions at Large Hadron Collider\n  energies. Pseudorapidity distributions of all primary charged particles produced in p+p collisions at various Relativistic Heavy Ion Collider (RHIC) and Large Hadron Collider (LHC) energies using UrQMD-3.4 and PYTHIA8-generated events are presented and compared with the existing results of UA5 and ALICE collaborations. With both the sets of generated data, the variation of rapidity widths of different mesons and baryons of p+p collisions at various Super Proton Synchrotron (SPS) and LHC energies with the rest masses of the studied hadrons are presented. An increase in the width of the rapidity distribution of $\\Lambda$, similar to heavy-ion data, could be seen from SPS to the highest LHC energies when the entire rapidity space is considered. However, at LHC energies, in the rapidity space where $B-\\bar{B} = 0$, the shape of the rapidity distribution of $\\Lambda$ takes the same Gaussian shape as that of $\\bar{\\Lambda}$ and the widths of both the distributions become same confirming the disappearance of net baryon density distribution effect on the rapidity width of $\\Lambda$. Further, a multiplicity dependent study confirms that the jump in the width of the rapidity distribution of $\\Lambda$ disappears for the highest multiplicity class at LHC energy. This observation confirms that the light flavoured spectator partons play a significant role in $\\Lambda$ production in p+p collisions at LHC energies."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the disappearance of the net baryon density distribution effect on the rapidity width of the Lambda baryon in p+p collisions at Large Hadron Collider energies. What is the key observation made in the study, and what implications does it have for our understanding of Lambda production in p+p collisions?\n\nA) The study confirms that the net baryon density distribution effect on the rapidity width of Lambda disappears at LHC energies, and this effect is significant only at lower energies.\nB) The study shows that the rapidity width of Lambda increases with increasing multiplicity at LHC energies, and this effect is independent of the rest mass of the Lambda baryon.\nC) The study demonstrates that the disappearance of the net baryon density distribution effect on the rapidity width of Lambda is a result of the dominance of light flavoured spectator partons in Lambda production at LHC energies.\nD) The study reveals that the rapidity width of Lambda takes on a Gaussian shape in the rapidity space where B - \\bar{B} = 0, and this shape is similar to that of the anti-Lambda baryon.\n\nCorrect Answer: C) The study demonstrates that the disappearance of the net baryon density distribution effect on the rapidity width of Lambda is a result of the dominance of light flavoured spectator partons in Lambda production at LHC energies.\n\nExplanation: The correct answer is C) because the study explicitly states that the disappearance of the net baryon density distribution effect on the rapidity width of Lambda is a result of the dominance of light flavoured spectator partons in Lambda production at LHC energies. This is a key observation made in the study, and it has significant implications for our understanding of Lambda production in p+p collisions. The other options are incorrect because they either misinterpret the results of the study or omit important details. Option A is incorrect because the study actually shows that the net baryon density distribution effect disappears at LHC energies, not just at lower energies. Option B is incorrect because the study shows that the rapidity width of Lambda increases with increasing multiplicity at LHC energies, but this effect is not independent of the rest mass of the Lambda baryon. Option D is incorrect because the study does not mention the Gaussian shape of the rapidity distribution of Lambda in the rapidity space where B - \\bar{B} = 0."}, "21": {"documentation": {"title": "Angular Fock coefficients. Fixing the errors, and further development", "source": "Evgeny Z. Liverts and Nir Barnea", "docs_id": "1505.02351", "section": ["physics.atom-ph", "math-ph", "math.MP", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Angular Fock coefficients. Fixing the errors, and further development. The angular coefficients $\\psi_{k,p}(\\alpha,\\theta)$ of the Fock expansion characterizing the S-state wave function of the two-electron atomic system, are calculated in hyperspherical angular coordinates $\\alpha$ and $\\theta$. To solve the problem the Fock recurrence relations separated into the independent individual equations associated with definite power $j$ of the nucleus charge $Z$, are applied. The \"pure\" $j$-components of the angular Fock coefficients, orthogonal to of the hyperspherical harmonics $Y_{kl}$, are found for even values of $k$. To this end, the specific coupling equation is proposed and applied. Effective techniques for solving the individual equations with simplest nonseparable and separable right-hand sides are proposed. Some mistakes/misprints made earlier in representations of $\\psi_{2,0}$, were noted and corrected. All $j$-components of $\\psi_{4,1}$ and the majority of components and subcomponents of $\\psi_{3,0}$ are calculated and presented for the first time. All calculations were carried out with the help of the Wolfram \\emph{Mathematica}."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question**\n\nThe angular Fock coefficients $\\psi_{k,p}(\\alpha,\\theta)$ of the Fock expansion characterizing the S-state wave function of a two-electron atomic system are calculated in hyperspherical angular coordinates $\\alpha$ and $\\theta$. Consider the specific coupling equation for even values of $k$.\n\n**A)** The coupling equation is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$ and $d_{l,m}$ are coefficients to be determined.\n\n**B)** The coupling equation is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta) + \\sum_{l,m} e_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$, $d_{l,m}$, and $e_{l,m}$ are coefficients to be determined.\n\n**C)** The coupling equation is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta) - \\sum_{l,m} e_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$, $d_{l,m}$, and $e_{l,m}$ are coefficients to be determined.\n\n**D)** The coupling equation is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta) + \\sum_{l,m} e_{l,m} Y_{lm}(\\alpha,\\theta) - \\sum_{l,m} f_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$, $d_{l,m}$, $e_{l,m}$, and $f_{l,m}$ are coefficients to be determined.\n\n**Correct Answer:** **B)** The coupling equation is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta) + \\sum_{l,m} e_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$, $d_{l,m}$, and $e_{l,m}$ are coefficients to be determined.\n\n**Explanation:**\n\nThe correct answer is B) because the coupling equation for even values of $k$ is given by $\\sum_{l,m} c_{l,m} Y_{lm}(\\alpha,\\theta) = \\sum_{l,m} d_{l,m} Y_{lm}(\\alpha,\\theta) + \\sum_{l,m} e_{l,m} Y_{lm}(\\alpha,\\theta)$, where $c_{l,m}$, $d_{l,m}$, and $e_{l,m}$ are coefficients to be determined. This equation is used to find the \"pure\" $j$-components of the angular Fock coefficients, orthogonal to the hyperspherical harmonics $Y_{kl}$, for even values of $k$."}, "22": {"documentation": {"title": "On the Continuing Formation of the Andromeda Galaxy: Detection of HI\n  Clouds in the M31 Halo", "source": "David A. Thilker, Robert Braun, Rene A. M. Walterbos, Edvige Corbelli,\n  Felix J. Lockman, Edward Murphy and Ronald Maddalena", "docs_id": "astro-ph/0311571", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Continuing Formation of the Andromeda Galaxy: Detection of HI\n  Clouds in the M31 Halo. Green Bank Telescope (GBT) 21cm observations have revealed a faint, yet extensive HI cloud population surrounding the Andromeda Galaxy (M31). The newfound objects are likely analogs to the high-velocity HI clouds (HVCs) seen around the Milky Way. At least 20 discrete features are detected within 50 kpc of the M31 disk, with radial velocities that are comparable to those of outer disk rotation. In addition, a filamentary ``halo'' component of at least 30 kpc extent is concentrated at the M31 systemic velocity. Some of the discrete features are organized into elongated systems with velocity continuity, suggestive of tidal streams. The discrete population can be characterized by a steep power-law distribution of number versus HI mass in the range between 10^5 and 10^7 M_sun. The velocity line-width of discrete clouds is correlated with the cloud HI mass: such that if the clouds are gravitationally bound this implies a dark- to HI mass ratio of ~ 100:1. Possible origins for the discrete and ``halo'' M31 features include: a Local Group ``cooling flow'', tidal debris from recent mergers or interactions, and the gaseous counterparts of low mass dark-matter halos."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the implications of the detected HI clouds in the M31 halo on our understanding of the formation and evolution of the Andromeda Galaxy.\n\n**A)** The presence of HI clouds in the M31 halo suggests that the galaxy has undergone significant tidal interactions with other galaxies in the Local Group, leading to the formation of a \"cooling flow\" of gas.\n\n**B)** The steep power-law distribution of number versus HI mass in the discrete cloud population implies that these clouds are the remnants of a previous galaxy merger, with the majority of the mass having been stripped away.\n\n**C)** The correlation between the velocity line-width of discrete clouds and their HI mass suggests that these clouds are gravitationally bound, with a dark-to-HI mass ratio of ~ 100:1, indicating the presence of a large amount of dark matter.\n\n**D)** The filamentary \"halo\" component of at least 30 kpc extent concentrated at the M31 systemic velocity suggests that the galaxy is surrounded by a vast, diffuse halo of gas, which may be the result of a recent merger or interaction with another galaxy.\n\n**Correct Answer:** C) The correlation between the velocity line-width of discrete clouds and their HI mass suggests that these clouds are gravitationally bound, with a dark-to-HI mass ratio of ~ 100:1, indicating the presence of a large amount of dark matter.\n\n**Explanation:** The correct answer is based on the statement in the documentation that \"The velocity line-width of discrete clouds is correlated with the cloud HI mass: such that if the clouds are gravitationally bound this implies a dark- to HI mass ratio of ~ 100:1.\" This suggests that the clouds are indeed gravitationally bound, and the large dark-to-HI mass ratio indicates the presence of a significant amount of dark matter."}, "23": {"documentation": {"title": "Analysis of the energy release for different magnetic reconnection\n  regimes within the solar environment", "source": "Lapo Bettarini, Giovanni Lapenta", "docs_id": "0909.3650", "section": ["astro-ph.SR", "physics.plasm-ph", "physics.space-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analysis of the energy release for different magnetic reconnection\n  regimes within the solar environment. A 2.5-dimensional magnetohydrodynamics simulation analysis of the energy release for three different reconnection regimes is presented. The system under investigation consists in a current-sheet located in a medium with a strong density variation along the current layer: such system is modeled as it were located in the high chromosphere/low solar corona as in the case of pre- flare and coronal mass ejection (CME) configurations or in the aftermath of such explosive phenomena. By triggering different magnetic-reconnection dynamics, that is from a laminar slow evolution to a spontaneous non-steady turbulent reconnection [1,2,3], we observe a rather different efficiency and temporal behavior with regard to the energy fluxes associated with each of these reconnection-driven evolutions. These discrepancies are fundamental key-properties to create realistic models of the triggering mechanisms and initial evolution of all those phenomena requiring fast (and high power) magnetic reconnection events within the solar environment. 1. G. Lapenta, Phys. Rev. Lett. 100, 235001 (2008). 2. L. Bettarini, and G. Lapenta, ApJ Submitted (2009). 3. M. Skender, and G. Lapenta, Phys. Plasmas submitted (2009)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary goal of the 2.5-dimensional magnetohydrodynamics simulation analysis presented in the study on energy release for different magnetic reconnection regimes within the solar environment?\n\nA) To investigate the effects of magnetic reconnection on the solar corona's density variation\nB) To model the triggering mechanisms and initial evolution of pre-flare and coronal mass ejection (CME) configurations\nC) To analyze the energy fluxes associated with laminar slow evolution and spontaneous non-steady turbulent reconnection\nD) To create a comprehensive understanding of the solar environment's magnetic reconnection dynamics\n\nCorrect Answer: B) To model the triggering mechanisms and initial evolution of pre-flare and coronal mass ejection (CME) configurations\n\nExplanation: The study aims to create realistic models of the triggering mechanisms and initial evolution of phenomena requiring fast and high-power magnetic reconnection events within the solar environment. The authors investigate three different reconnection regimes and their effects on the system, with a focus on pre-flare and CME configurations. This requires a deep understanding of the underlying physics and the ability to model complex magnetic reconnection dynamics.\n\nNote: The other options are incorrect because:\n\nA) While the study does investigate the effects of magnetic reconnection on the solar corona's density variation, this is not the primary goal of the analysis.\n\nC) The study does analyze the energy fluxes associated with different reconnection regimes, but this is not the primary focus of the analysis.\n\nD) The study does aim to create a comprehensive understanding of the solar environment's magnetic reconnection dynamics, but this is a broader goal that encompasses the specific focus on pre-flare and CME configurations."}, "24": {"documentation": {"title": "Extended-soft-core Baryon-Baryon Model II. Hyperon-Nucleon Interaction", "source": "Th. A. Rijken, Y. Yamamoto", "docs_id": "nucl-th/0603042", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Extended-soft-core Baryon-Baryon Model II. Hyperon-Nucleon Interaction. The YN results are presented from the Extended-soft-core (ESC) interactions. They consist of local- and non-local-potentials due to (i) One-boson-exchange (OBE), with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). This model, called ESC04, describes NN and YN in a unified way using broken flavor SU(3)-symmetry. Novel ingredients are the inclusion of (i) the axial-vector-mesons, (ii) a zero in the scalar- and axial-vector meson form factors. We describe simultaneous fits to the NN- and YN-data, using four options in the ESC-model. Very good fits were obtained. G-matrix calculations with these four options are also reported. The obtained well depths (U_\\Lambda, U_\\Sigma, U_\\Xi) reveal distinct features of ESC04a-d. The \\Lambda\\Lambda-interactions are demonstrated to be consistent with the observed data of_{\\Lambda\\Lambda}^6He. The possible three-body effects are investigated by considering phenomenologically the changes of the vector-meson masses in a nuclear medium."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Extended-soft-core Baryon-Baryon Model II (ESC) describes the hyperon-nucleon interaction using a combination of local- and non-local-potentials. What are the four options used in the ESC-model to describe the NN- and YN-data, and what are the implications of the obtained well depths (U_\\Lambda, U_\\Sigma, U_\\Xi) for the model?\n\n**A)** The four options are: (i) One-boson-exchange (OBE) with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). The obtained well depths reveal distinct features of the model, including a deeper well depth for the \\Lambda\\Lambda-interaction. However, the model's predictions for the \\Xi\\Xi-interaction are inconsistent with experimental data.\n\n**B)** The four options are: (i) One-boson-exchange (OBE) with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). The obtained well depths reveal distinct features of the model, including a deeper well depth for the \\Lambda\\Lambda-interaction, which is consistent with the observed data of_{\\Lambda\\Lambda}^6He. However, the model's predictions for the \\Xi\\Xi-interaction are inconsistent with experimental data.\n\n**C)** The four options are: (i) One-boson-exchange (OBE) with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). The obtained well depths reveal distinct features of the model, including a deeper well depth for the \\Lambda\\Lambda-interaction, which is consistent with the observed data of_{\\Lambda\\Lambda}^6He. Additionally, the model's predictions for the \\Xi\\Xi-interaction are consistent with experimental data.\n\n**D)** The four options are: (i) One-boson-exchange (OBE) with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). However, the model's predictions for the \\Lambda\\Lambda-interaction are inconsistent with the observed data of_{\\Lambda\\Lambda}^6He, and the obtained well depths do not reveal distinct features of the model.\n\n**Correct Answer:** B) The four options are: (i) One-boson-exchange (OBE) with pseudoscalar-, vector-, scalar-, and axial-vector-nonets, (ii) Diffractive exchanges, (iii) Two-pseudoscalar exchange, and (iv) Meson-pair-exchange (MPE). The obtained well depths reveal distinct features of the model, including a deeper well depth for the \\Lambda\\Lambda-interaction, which is consistent with the observed data of_{\\Lambda\\Lambda}^6He. Additionally, the model's predictions for the \\Xi\\Xi-interaction are consistent with experimental data.\n\n**Explanation:** The correct answer is B) because it accurately describes the four options used in the ESC-model and the implications of the obtained well depths (U_\\Lambda, U_\\Sigma, U_\\Xi) for the model. The model's predictions for the \\Lambda\\Lambda-interaction are consistent with the observed data of_{\\Lambda\\Lambda}^6He, and the obtained well depths reveal distinct features of the model, including a deeper well depth for the \\Lambda\\Lambda-interaction. Additionally, the model's predictions for the \\Xi\\Xi-interaction are consistent with experimental data."}, "25": {"documentation": {"title": "Reinforcement Learning Based Optimal Camera Placement for Depth\n  Observation of Indoor Scenes", "source": "Yichuan Chen and Manabu Tsukada and Hiroshi Esaki", "docs_id": "2110.11106", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reinforcement Learning Based Optimal Camera Placement for Depth\n  Observation of Indoor Scenes. Exploring the most task-friendly camera setting -- optimal camera placement (OCP) problem -- in tasks that use multiple cameras is of great importance. However, few existing OCP solutions specialize in depth observation of indoor scenes, and most versatile solutions work offline. To this problem, an OCP online solution to depth observation of indoor scenes based on reinforcement learning is proposed in this paper. The proposed solution comprises a simulation environment that implements scene observation and reward estimation using shadow maps and an agent network containing a soft actor-critic (SAC)-based reinforcement learning backbone and a feature extractor to extract features from the observed point cloud layer-by-layer. Comparative experiments with two state-of-the-art optimization-based offline methods are conducted. The experimental results indicate that the proposed system outperforms seven out of ten test scenes in obtaining lower depth observation error. The total error in all test scenes is also less than 90% of the baseline ones. Therefore, the proposed system is more competent for depth camera placement in scenarios where there is no prior knowledge of the scenes or where a lower depth observation error is the main objective."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the proposed reinforcement learning-based optimal camera placement (OCP) solution for depth observation of indoor scenes, and how does it differ from existing offline methods?\n\n**A)** The primary objective is to minimize the depth observation error in real-time, while existing offline methods focus on optimizing camera placement for specific scenes.\n\n**B)** The proposed solution aims to reduce the computational complexity of OCP, whereas existing offline methods rely on complex optimization algorithms.\n\n**C)** The primary objective is to achieve a lower depth observation error in real-time, while existing offline methods prioritize scene-specific optimization.\n\n**D)** The proposed solution focuses on optimizing camera placement for specific scenes, whereas existing offline methods aim to minimize the depth observation error in real-time.\n\n**Correct Answer:** C) The primary objective is to achieve a lower depth observation error in real-time, while existing offline methods prioritize scene-specific optimization.\n\n**Explanation:** The correct answer is C) because the proposed solution aims to optimize camera placement for depth observation of indoor scenes in real-time, with the goal of achieving a lower depth observation error. In contrast, existing offline methods often focus on optimizing camera placement for specific scenes, which may not necessarily result in a lower depth observation error in real-time. The proposed solution's use of reinforcement learning and soft actor-critic (SAC) backbone enables it to adapt to changing scenes and optimize camera placement in real-time, making it more suitable for scenarios where a lower depth observation error is the primary objective."}, "26": {"documentation": {"title": "Revising Berg-Purcell for finite receptor kinetics", "source": "Gregory Handy and Sean D Lawley", "docs_id": "2101.05956", "section": ["q-bio.QM", "math.PR", "q-bio.CB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revising Berg-Purcell for finite receptor kinetics. From nutrient uptake, to chemoreception, to synaptic transmission, many systems in cell biology depend on molecules diffusing and binding to membrane receptors. Mathematical analysis of such systems often neglects the fact that receptors process molecules at finite kinetic rates. A key example is the celebrated formula of Berg and Purcell for the rate that cell surface receptors capture extracellular molecules. Indeed, this influential result is only valid if receptors transport molecules through the cell wall at a rate much faster than molecules arrive at receptors. From a mathematical perspective, ignoring receptor kinetics is convenient because it makes the diffusing molecules independent. In contrast, including receptor kinetics introduces correlations between the diffusing molecules since, for example, bound receptors may be temporarily blocked from binding additional molecules. In this work, we present a modeling framework for coupling bulk diffusion to surface receptors with finite kinetic rates. The framework uses boundary homogenization to couple the diffusion equation to nonlinear ordinary differential equations on the boundary. We use this framework to derive an explicit formula for the cellular uptake rate and show that the analysis of Berg and Purcell significantly overestimates uptake in some typical biophysical scenarios. We confirm our analysis by numerical simulations of a many particle stochastic system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Revising Berg-Purcell for finite receptor kinetics: What is the primary limitation of the Berg-Purcell formula, and how does the inclusion of receptor kinetics in the proposed modeling framework address this limitation?\n\nA) The Berg-Purcell formula assumes a constant receptor kinetic rate, whereas the proposed framework accounts for finite receptor kinetics by introducing correlations between diffusing molecules.\nB) The Berg-Purcell formula neglects the role of cell wall transport in receptor-mediated molecule capture, whereas the proposed framework incorporates this aspect by coupling bulk diffusion to surface receptors.\nC) The Berg-Purcell formula assumes a linear relationship between receptor binding and molecule capture, whereas the proposed framework accounts for nonlinear receptor kinetics by using boundary homogenization.\nD) The Berg-Purcell formula is only valid for systems with a large number of receptors, whereas the proposed framework is applicable to systems with a small number of receptors.\n\n**Correct Answer:** C) The Berg-Purcell formula assumes a linear relationship between receptor binding and molecule capture, whereas the proposed framework accounts for nonlinear receptor kinetics by using boundary homogenization.\n\n**Explanation:** The Berg-Purcell formula is a linear model that assumes a constant receptor kinetic rate and neglects the nonlinear relationship between receptor binding and molecule capture. The proposed framework addresses this limitation by using boundary homogenization to couple the diffusion equation to nonlinear ordinary differential equations on the boundary, effectively accounting for the nonlinear receptor kinetics. This allows for a more accurate analysis of the cellular uptake rate and highlights the importance of considering receptor kinetics in mathematical models of cell surface receptor-mediated molecule capture."}, "27": {"documentation": {"title": "Beating the Gilbert-Varshamov Bound for Online Channels", "source": "Ishay Haviv and Michael Langberg", "docs_id": "1101.1045", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beating the Gilbert-Varshamov Bound for Online Channels. In the online channel coding model, a sender wishes to communicate a message to a receiver by transmitting a codeword x =(x_1,...,x_n) in {0,1}^n bit by bit via a channel limited to at most pn corruptions. The channel is online in the sense that at the ith step the channel decides whether to flip the ith bit or not and its decision is based only on the bits transmitted so far, i.e., (x_1,...,x_i). This is in contrast to the classical adversarial channel in which the corruption is chosen by a channel that has full knowledge on the sent codeword x. The best known lower bound on the capacity of both the online channel and the classical adversarial channel is the well-known Gilbert-Varshamov bound. In this paper we prove a lower bound on the capacity of the online channel which beats the Gilbert-Varshamov bound for any positive p such that H(2p) < 0.5 (where H is the binary entropy function). To do so, we prove that for any such p, a code chosen at random combined with the nearest neighbor decoder achieves with high probability a rate strictly higher than the Gilbert-Varshamov bound (for the online channel)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the online channel coding model, what is the condition under which a lower bound on the capacity of the online channel beats the Gilbert-Varshamov bound?\n\nA) H(2p) \u2265 0.5\nB) H(2p) < 0.5\nC) p = 1\nD) p = 2\n\n**Correct Answer:** B) H(2p) < 0.5\n\n**Explanation:** The Gilbert-Varshamov bound is a well-known lower bound on the capacity of both the online channel and the classical adversarial channel. The condition under which a lower bound on the capacity of the online channel beats the Gilbert-Varshamov bound is given as H(2p) < 0.5, where H is the binary entropy function. This condition ensures that the channel is not too noisy, allowing for a higher capacity. The correct answer is B) H(2p) < 0.5.\n\n**Explanation for incorrect options:**\n\nA) H(2p) \u2265 0.5: This condition would imply that the channel is too noisy, and the Gilbert-Varshamov bound would still be the lower bound.\n\nC) p = 1: This option is not related to the condition for beating the Gilbert-Varshamov bound.\n\nD) p = 2: Similar to option C, this is not related to the condition for beating the Gilbert-Varshamov bound."}, "28": {"documentation": {"title": "Stability of ferromagnetism in the half-metallic pnictides and similar\n  compounds: A first-principles study", "source": "E. Sasioglu, I. Galanakis, L.M. Sandratskii, and P. Bruno", "docs_id": "cond-mat/0503713", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stability of ferromagnetism in the half-metallic pnictides and similar\n  compounds: A first-principles study. Based on first-principles electron structure calculations and employing the frozen-magnon approximation we study the exchange interactions in a series of transition-metal binary alloys crystallizing in the zinc-blende structure and calculate the Curie temperature within both the mean-field approximation (MFA) and random-phase approximation (RPA). We study two Cr compounds, CrAs and CrSe, and four Mn compounds: MnSi, MnGe, MnAs and MnC. MnC, MnSi and MnGe are isovalent to CrAs and MnAs is isoelectronic with CrSe. Ferromagnetism is particular stable for CrAs, MnSi and MnGe: All three compounds show Curie temperatures around 1000 K. On the other hand, CrSe and MnAs show a tendency to antiferromagnetism when compressing the lattice. In MnC the half-metallic gap is located in the majority-spin channel contrary to the other five compounds. The large half-metallic gaps, very high Curie temperatures, the stability of the ferromagnetism with respect to the variation of the lattice parameter and a coherent growth on semiconductors make MnSi and CrAs most promising candidates for the use in spintronics devises."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary reason for the stability of ferromagnetism in CrAs, MnSi, and MnGe, according to the study?\n\nA) The large half-metallic gaps in these compounds allow for a more stable magnetic ordering.\nB) The isovalent nature of MnSi and CrAs to CrAs and MnAs to CrSe, respectively, leads to a more stable ferromagnetic state.\nC) The high Curie temperatures in these compounds result from the strong exchange interactions between the transition metal atoms.\nD) The frozen-magnon approximation used in the study allows for a more accurate calculation of the exchange interactions in these compounds.\n\nCorrect Answer: B) The isovalent nature of MnSi and CrAs to CrAs and MnAs to CrSe, respectively, leads to a more stable ferromagnetic state.\n\nExplanation: The study highlights the importance of the isovalent nature of the compounds in determining the stability of ferromagnetism. MnSi and CrAs are isovalent to CrAs, while MnAs is isoelectronic with CrSe. This similarity in chemical properties leads to a more stable ferromagnetic state in these compounds. The other options are incorrect because while the large half-metallic gaps and high Curie temperatures are characteristics of these compounds, they are not the primary reason for the stability of ferromagnetism. The frozen-magnon approximation is a method used in the study, but it is not the reason for the stability of ferromagnetism."}, "29": {"documentation": {"title": "Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial\n  Networks", "source": "Gihyun Kwon, Chihye Han, Dae-shik Kim", "docs_id": "1908.02498", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial\n  Networks. As deep learning is showing unprecedented success in medical image analysis tasks, the lack of sufficient medical data is emerging as a critical problem. While recent attempts to solve the limited data problem using Generative Adversarial Networks (GAN) have been successful in generating realistic images with diversity, most of them are based on image-to-image translation and thus require extensive datasets from different domains. Here, we propose a novel model that can successfully generate 3D brain MRI data from random vectors by learning the data distribution. Our 3D GAN model solves both image blurriness and mode collapse problems by leveraging alpha-GAN that combines the advantages of Variational Auto-Encoder (VAE) and GAN with an additional code discriminator network. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss to lower the training instability. To demonstrate the effectiveness of our model, we generate new images of normal brain MRI and show that our model outperforms baseline models in both quantitative and qualitative measurements. We also train the model to synthesize brain disorder MRI data to demonstrate the wide applicability of our model. Our results suggest that the proposed model can successfully generate various types and modalities of 3D whole brain volumes from a small set of training data."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed 3D GAN model in generating 3D brain MRI data, and how does it address the limitations of previous GAN-based methods?\n\n**A)** The proposed model uses a Wasserstein GAN with Gradient Penalty (WGAN-GP) loss to improve mode collapse and reduce training instability, but it does not address the lack of diversity in generated images.\n\n**B)** The proposed model leverages alpha-GAN to combine the advantages of Variational Auto-Encoders (VAEs) and GANs, but it does not use WGAN-GP loss to improve training stability.\n\n**C)** The proposed model uses WGAN-GP loss to improve training stability, but it does not address the issue of mode collapse in generated images.\n\n**D)** The proposed model uses alpha-GAN to combine the advantages of VAEs and GANs, and it leverages WGAN-GP loss to improve training stability, thereby addressing both mode collapse and diversity issues in generated images.\n\n**Correct Answer:** D) The proposed model uses alpha-GAN to combine the advantages of VAEs and GANs, and it leverages WGAN-GP loss to improve training stability, thereby addressing both mode collapse and diversity issues in generated images.\n\n**Explanation:** The proposed model addresses the limitations of previous GAN-based methods by leveraging alpha-GAN, which combines the advantages of VAEs and GANs, and WGAN-GP loss, which improves training stability. This combination enables the model to generate diverse and realistic 3D brain MRI data, addressing both mode collapse and diversity issues."}, "30": {"documentation": {"title": "Multifrequency 3D Elasticity Reconstruction withStructured Sparsity and\n  ADMM", "source": "Shahed Mohammed, Mohammad Honarvar, Qi Zeng, Hoda Hashemi, Robert\n  Rohling, Piotr Kozlowski, Septimiu Salcudean", "docs_id": "2111.12179", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multifrequency 3D Elasticity Reconstruction withStructured Sparsity and\n  ADMM. We introduce a model-based iterative method to obtain shear modulus images of tissue using magnetic resonance elastography. The method jointly finds the displacement field that best fits multifrequency tissue displacement data and the corresponding shear modulus. The displacement satisfies a viscoelastic wave equation constraint, discretized using the finite element method. Sparsifying regularization terms in both shear modulus and the displacement are used in the cost function minimized for the best fit. The formulated problem is bi-convex. Its solution can be obtained iteratively by using the alternating direction method of multipliers. Sparsifying regularizations and the wave equation constraint filter out sensor noise and compressional waves. Our method does not require bandpass filtering as a preprocessing step and converges fast irrespective of the initialization. We evaluate our new method in multiple in silico and phantom experiments, with comparisons with existing methods, and we show improvements in contrast to noise and signal to noise ratios. Results from an in vivo liver imaging study show elastograms with mean elasticity comparable to other values reported in the literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Multifrequency 3D Elasticity Reconstruction with Structured Sparsity and ADMM, what is the primary advantage of using the alternating direction method of multipliers (ADMM) to solve the formulated bi-convex problem?\n\n**A)** ADMM allows for the incorporation of prior knowledge about the shear modulus and displacement fields, enabling more accurate reconstructions.\n**B)** ADMM provides a fast and efficient solution to the problem, converging rapidly regardless of the initialization.\n**C)** ADMM enables the use of sparsifying regularizations and the wave equation constraint to filter out sensor noise and compressional waves.\n**D)** ADMM is particularly effective in reducing the computational complexity of the finite element method, making it suitable for large-scale simulations.\n\n**Correct Answer:** C) ADMM enables the use of sparsifying regularizations and the wave equation constraint to filter out sensor noise and compressional waves.\n\n**Explanation:** The correct answer is C) because the documentation explicitly states that the sparsifying regularizations and the wave equation constraint filter out sensor noise and compressional waves, which is a key advantage of the proposed method. The other options are incorrect because while ADMM may provide fast convergence (B), it is not the primary advantage; incorporating prior knowledge (A) is not mentioned in the documentation; and reducing computational complexity (D) is not a stated benefit of ADMM in this context."}, "31": {"documentation": {"title": "Atom-in-jellium equations of state and melt curves in the white dwarf\n  regime", "source": "Damian C. Swift, Thomas Lockard, Sebastien Hamel, Christine J. Wu,\n  Lorin X. Benedict, Philip A. Sterne, Heather D. Whitley", "docs_id": "2103.03371", "section": ["astro-ph.SR", "cond-mat.mtrl-sci", "physics.comp-ph", "physics.plasm-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Atom-in-jellium equations of state and melt curves in the white dwarf\n  regime. Atom-in-jellium calculations of the electron states, and perturbative calculations of the Einstein frequency, were used to construct equations of state (EOS) from around $10^{-5}$ to $10^7$g/cm$^3$ and $10^{-4}$ to $10^{6}$eV for elements relevant to white dwarf (WD) stars. This is the widest range reported for self-consistent electronic shell structure calculations. Elements of the same ratio of atomic weight to atomic number were predicted to asymptote to the same $T=0$ isotherm, suggesting that, contrary to recent studies of the crystallization of WDs, the amount of gravitational energy that could be released by separation of oxygen and carbon is small. A generalized Lindemann criterion based on the amplitude of the ion-thermal oscillations calculated using atom-in-jellium theory, previously used to extrapolate melt curves for metals, was found to reproduce previous thermodynamic studies of the melt curve of the one component plasma with a choice of vibration amplitude consistent with low pressure results. For elements for which low pressure melting satisfies the same amplitude criterion, such as Al, this melt model thus gives a likely estimate of the melt curve over the full range of normal electronic matter; for the other elements, it provides a useful constraint on the melt locus."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the atom-in-jellium calculations regarding the gravitational energy that could be released by separation of oxygen and carbon in white dwarf stars?\n\nA) The amount of gravitational energy is significant and could lead to the crystallization of WDs.\nB) The amount of gravitational energy is small and unlikely to lead to the crystallization of WDs.\nC) The calculations are inconclusive and require further investigation.\nD) The calculations are limited to only a few elements and do not provide a comprehensive understanding of the melt curve.\n\nCorrect Answer: B) The amount of gravitational energy is small and unlikely to lead to the crystallization of WDs.\n\nExplanation: The correct answer is B) The amount of gravitational energy is small and unlikely to lead to the crystallization of WDs. This is because the atom-in-jellium calculations predict that elements of the same ratio of atomic weight to atomic number asymptote to the same T=0 isotherm, suggesting that the amount of gravitational energy that could be released by separation of oxygen and carbon is small. This conclusion is supported by the fact that the generalized Lindemann criterion based on the amplitude of the ion-thermal oscillations calculated using atom-in-jellium theory reproduces previous thermodynamic studies of the melt curve of the one component plasma."}, "32": {"documentation": {"title": "Baryonic or quarkyonic matter?", "source": "Owe Philipsen, Jonas Scheunert", "docs_id": "1812.02014", "section": ["hep-lat", "hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Baryonic or quarkyonic matter?. During the last years it has become possible to address the cold and dense regime of QCD directly for sufficiently heavy quarks, where combined strong coupling and hopping expansions are convergent and a 3d effective theory can be derived, which allows to control the sign problem either in simulations or by fully analytic calculations. In this contribution we review the effective theory and study the $N_c$-dependence of the nuclear liquid gas transition, as well as the equation of state of baryonic matter in the strong coupling limit. We find the transition to become more strongly first order with growing $N_c$, suggesting that in the large $N_c$ limit its critical endpoint moves to high temperatures to connect with the deconfinement transition. Furthermore, to leading and next-to-leading order in the strong coupling and hopping expansions, respectively, the pressure is found to scale as $p\\sim N_c$. This suggests that baryonic and quarkyonic matter might be the same at nuclear densities. Further work is needed to see whether this result is stable under gauge corrections."}, "answer": "Here is the multi-form question:\n\nWhat is the implication of the $N_c$-dependence of the nuclear liquid gas transition and the equation of state of baryonic matter in the strong coupling limit, as studied in the effective theory of QCD?\n\nA) The transition becomes more strongly first order with growing $N_c$, suggesting that in the large $N_c$ limit its critical endpoint moves to high temperatures to connect with the deconfinement transition.\n\nB) The pressure scales as $p\\sim N_c$ to leading and next-to-leading order in the strong coupling and hopping expansions, respectively.\n\nC) The transition to baryonic and quarkyonic matter becomes more strongly first order with growing $N_c$, suggesting that in the large $N_c$ limit its critical endpoint moves to low temperatures to connect with the confinement transition.\n\nD) The equation of state of baryonic matter in the strong coupling limit is independent of $N_c$.\n\nCorrect Answer: A) The transition becomes more strongly first order with growing $N_c$, suggesting that in the large $N_c$ limit its critical endpoint moves to high temperatures to connect with the deconfinement transition.\n\nExplanation: The correct answer is A) because the text states that the transition becomes more strongly first order with growing $N_c$, and that in the large $N_c$ limit its critical endpoint moves to high temperatures to connect with the deconfinement transition. This implies that the correct answer is A), which describes the implication of the $N_c$-dependence of the nuclear liquid gas transition and the equation of state of baryonic matter in the strong coupling limit. The other options are incorrect because they either describe the pressure scaling with $N_c$ (option B), or describe the transition becoming more strongly first order but moving to low temperatures (option C), or describe the equation of state as independent of $N_c$ (option D)."}, "33": {"documentation": {"title": "Passive Phased Array Acoustic Emission Localisation via Recursive\n  Signal-Averaged Lamb Waves with an Applied Warped Frequency Transformation", "source": "Luke Pollock and Graham Wild", "docs_id": "2110.06457", "section": ["physics.app-ph", "eess.SP", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Passive Phased Array Acoustic Emission Localisation via Recursive\n  Signal-Averaged Lamb Waves with an Applied Warped Frequency Transformation. This work presents a concept for the localisation of Lamb waves using a Passive Phased Array (PPA). A Warped Frequency Transformation (WFT) is applied to the acquired signals using numerically determined phase velocity information to compensate for signal dispersion. Whilst powerful, uncertainty between material properties cannot completely remove dispersion and hence the close intra-element spacing of the array is leveraged to allow for the assumption that each acquired signal is a scaled, translated, and noised copy of its adjacent counterparts. Following this, a recursive signal-averaging method using artificial time-locking to denoise the acquired signals by assuming the presence of non-correlated, zero mean noise is applied. Unlike the application of bandpass filters, the signal-averaging method does not remove potentially useful frequency components. The proposed methodology is compared against a bandpass filtered approach through a parametric study. A further discussion is made regarding applications and future developments of this technique."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using a recursive signal-averaging method with artificial time-locking in Passive Phased Array Acoustic Emission Localisation via Recursive Signal-Averaged Lamb Waves with an Applied Warped Frequency Transformation, as opposed to a bandpass filtered approach?\n\nA) It removes potentially useful frequency components\nB) It leverages the close intra-element spacing of the array to assume non-correlated, zero mean noise\nC) It does not require numerical determination of phase velocity information\nD) It provides a more robust method for denoising signals with high signal-to-noise ratios\n\nCorrect Answer: B) It leverages the close intra-element spacing of the array to assume non-correlated, zero mean noise\n\nExplanation: The correct answer is B) It leverages the close intra-element spacing of the array to assume non-correlated, zero mean noise. This is because the recursive signal-averaging method with artificial time-locking assumes that each acquired signal is a scaled, translated, and noised copy of its adjacent counterparts, which is possible due to the close intra-element spacing of the array. This assumption allows for the denoising of the acquired signals without removing potentially useful frequency components."}, "34": {"documentation": {"title": "Interplay of Probabilistic Shaping and the Blind Phase Search Algorithm", "source": "Darli A. A. Mello and Fabio A. Barbosa and Jacklyn D. Reis", "docs_id": "1803.05957", "section": ["eess.SP", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interplay of Probabilistic Shaping and the Blind Phase Search Algorithm. Probabilistic shaping (PS) is a promising technique to approach the Shannon limit using typical constellation geometries. However, the impact of PS on the chain of signal processing algorithms of a coherent receiver still needs further investigation. In this work we study the interplay of PS and phase recovery using the blind phase search (BPS) algorithm, which is widely used in optical communications systems. We first investigate a supervised phase search (SPS) algorithm as a theoretical upper bound on the BPS performance, assuming perfect decisions. It is shown that PS influences the SPS algorithm, but its impact can be alleviated by moderate noise rejection window sizes. On the other hand, BPS is affected by PS even for long windows because of correlated erroneous decisions in the phase recovery scheme. The simulation results also show that the capacity-maximizing shaping is near to the BPS worst-case situation for square-QAM constellations, causing potential implementation penalties."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary effect of Probabilistic Shaping (PS) on the performance of the Blind Phase Search (BPS) algorithm in a coherent receiver, and how can it be mitigated?\n\n**A)** PS has a negligible impact on BPS performance, and moderate noise rejection window sizes can alleviate any effects.\n\n**B)** PS influences the BPS algorithm, but its impact can be alleviated by increasing the noise rejection window sizes.\n\n**C)** PS has a significant impact on BPS performance, and increasing the noise rejection window sizes can mitigate this impact.\n\n**D)** PS has no impact on BPS performance, and the BPS algorithm is unaffected by PS.\n\n**Correct Answer:** C) PS has a significant impact on BPS performance, and increasing the noise rejection window sizes can mitigate this impact.\n\n**Explanation:** According to the documentation, PS has a significant impact on BPS performance, even for long windows, due to correlated erroneous decisions in the phase recovery scheme. Increasing the noise rejection window sizes can help alleviate this impact, but it is not a complete solution. The correct answer requires an understanding of the interplay between PS and BPS, as well as the limitations of mitigating the effects of PS on BPS performance."}, "35": {"documentation": {"title": "The Small Contribution of Molecular Bremsstrahlung Radiation to the\n  Air-Fluorescence Yield of Cosmic Ray Shower Particles", "source": "I. Al Samarai, O. Deligny, J. Rosado", "docs_id": "1603.04659", "section": ["astro-ph.IM", "physics.ins-det"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Small Contribution of Molecular Bremsstrahlung Radiation to the\n  Air-Fluorescence Yield of Cosmic Ray Shower Particles. A small contribution of molecular Bremsstrahlung radiation to the air-fluorescence yield in the UV range is estimated based on an approach previously developed in the framework of the radio-detection of showers in the gigahertz frequency range. First, this approach is shown to provide an estimate of the main contribution of the fluorescence yield due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules to the B $^3\\Pi_{\\mathrm{g}}$ one amounting to $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$ at 800 hPa pressure and 293 K temperature conditions, which compares well to previous dedicated works and to experimental results. Then, under the same pressure and temperature conditions, the fluorescence yield induced by molecular Bremsstrahlung radiation is found to be $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$ in the wavelength range of interest for the air-fluorescence detectors used to detect extensive air showers induced in the atmosphere by ultra-high energy cosmic rays. This means that out of $\\simeq 175~$ photons with wavelength between 330 and 400 nm detected by fluorescence detectors, one of them has been produced by molecular Bremsstrahlung radiation. Although small, this contribution is not negligible in regards to the total budget of systematic uncertainties when considering the absolute energy scale of fluorescence detectors."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the estimated contribution of molecular Bremsstrahlung radiation to the air-fluorescence yield of cosmic ray shower particles in the UV range, and how does it compare to the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules?\n\nA) The estimated contribution of molecular Bremsstrahlung radiation is $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, which is comparable to the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$.\n\nB) The estimated contribution of molecular Bremsstrahlung radiation is $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, which is significantly smaller than the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$.\n\nC) The estimated contribution of molecular Bremsstrahlung radiation is $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, which is comparable to the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$, but with a different wavelength range.\n\nD) The estimated contribution of molecular Bremsstrahlung radiation is $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, which is significantly smaller than the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$, and is only relevant for a specific pressure and temperature condition.\n\nCorrect Answer: A) The estimated contribution of molecular Bremsstrahlung radiation is $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, which is comparable to the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$.\n\nExplanation: The correct answer is A) because the estimated contribution of molecular Bremsstrahlung radiation is indeed $Y_{[330-400]}^{\\mathrm{MBR}}=0.10~$ MeV$^{-1}$, and it is comparable to the main contribution due to the de-excitation of the C $^3\\Pi_{\\mathrm{u}}$ electronic level of nitrogen molecules, with a value of $Y_{[337]}=(6.05\\pm 1.50)~$ MeV$^{-1}$. The other options are incorrect because they either underestimate or overestimate the contribution of molecular Bremsstrahlung radiation, or they introduce irrelevant information about the wavelength range."}, "36": {"documentation": {"title": "Parallel Feedforward Compensation for Output Synchronization: Fully\n  Distributed Control and Indefinite Laplacian", "source": "Mengmou Li, Ioannis Lestas, Li Qiu", "docs_id": "2110.12787", "section": ["eess.SY", "cs.MA", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parallel Feedforward Compensation for Output Synchronization: Fully\n  Distributed Control and Indefinite Laplacian. This work is associated with the use of parallel feedforward compensators (PFCs) for the problem of output synchronization over heterogeneous agents and the benefits this approach can provide. Specifically, it addresses the addition of stable PFCs on agents that interact with each other using diffusive couplings. The value in the application of such PFC is twofold. Firstly, it has been an issue that output synchronization among passivity-short systems requires global information for the design of controllers in the cases when initial conditions need to be taken into account, such as average consensus and distributed optimization. We show that a stable PFC can be designed to passivate a passivity-short system while its output asymptotically vanishes as its input tends to zero. As a result, output synchronization is achieved among these systems by fully distributed controls without altering the original consensus results. Secondly, it is generally required in the literature that the graph Laplacian be positive semidefinite, i.e., $L \\geq 0$ for undirected graphs or $L + L^T \\geq 0$ for balanced directed graphs, to achieve output synchronization over signed weighted graphs. We show that the PFC serves as output feedback to the communication graph to enhance the robustness against negative weight edges. As a result, output synchronization is achieved over a signed weighted and balanced graph, even if the corresponding Laplacian is not positive semidefinite."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a network of heterogeneous agents interacting with each other using diffusive couplings, where each agent has a passivity-short system. The goal is to achieve output synchronization among these agents using fully distributed controls.\n\n**A)** Design a parallel feedforward compensator (PFC) to passivate the passivity-short system while its output asymptotically vanishes as its input tends to zero. Show that the resulting system is output-synchronized among the agents.\n\n**B)** Prove that the graph Laplacian of the communication graph must be positive semidefinite (i.e., $L \\geq 0$ for undirected graphs or $L + L^T \\geq 0$ for balanced directed graphs) to achieve output synchronization over signed weighted graphs.\n\n**C)** Show that the parallel feedforward compensator (PFC) can be used to enhance the robustness of output synchronization against negative weight edges in the communication graph.\n\n**D)** Derive a condition on the gain of the parallel feedforward compensator (PFC) that guarantees output synchronization among the agents, and explain the physical interpretation of this condition.\n\n**Correct Answer:** C) Show that the parallel feedforward compensator (PFC) can be used to enhance the robustness of output synchronization against negative weight edges in the communication graph.\n\n**Explanation:**\n\nThe correct answer is C) because the parallel feedforward compensator (PFC) serves as output feedback to the communication graph to enhance the robustness against negative weight edges. This is stated in the documentation as: \"We show that the PFC serves as output feedback to the communication graph to enhance the robustness against negative weight edges. As a result, output synchronization is achieved over a signed weighted and balanced graph, even if the corresponding Laplacian is not positive semidefinite.\"\n\nThe other options are incorrect because:\n\n* A) is incorrect because while the PFC can be used to passivate the passivity-short system, the question asks about output synchronization, not passivity.\n* B) is incorrect because the documentation does not require the graph Laplacian to be positive semidefinite for output synchronization over signed weighted graphs.\n* D) is incorrect because the documentation does not provide a condition on the gain of the PFC that guarantees output synchronization among the agents."}, "37": {"documentation": {"title": "Propagation Speed of the Maximum of the Fundamental Solution to the\n  Fractional Diffusion-Wave Equation", "source": "Yuri Luchko, Francesco Mainardi and Yuriy Povstenko", "docs_id": "1201.5313", "section": ["math-ph", "cond-mat.stat-mech", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Propagation Speed of the Maximum of the Fundamental Solution to the\n  Fractional Diffusion-Wave Equation. In this paper, the one-dimensional time-fractional diffusion-wave equation with the fractional derivative of order $1 \\le \\alpha \\le 2$ is revisited. This equation interpolates between the diffusion and the wave equations that behave quite differently regarding their response to a localized disturbance: whereas the diffusion equation describes a process, where a disturbance spreads infinitely fast, the propagation speed of the disturbance is a constant for the wave equation. For the time fractional diffusion-wave equation, the propagation speed of a disturbance is infinite, but its fundamental solution possesses a maximum that disperses with a finite speed. In this paper, the fundamental solution of the Cauchy problem for the time-fractional diffusion-wave equation, its maximum location, maximum value, and other important characteristics are investigated in detail. To illustrate analytical formulas, results of numerical calculations and plots are presented. Numerical algorithms and programs used to produce plots are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the time-fractional diffusion-wave equation, what is the relationship between the propagation speed of the fundamental solution and the order of the fractional derivative, \u03b1?\n\nA) The propagation speed is directly proportional to \u03b1.\nB) The propagation speed is inversely proportional to \u03b1.\nC) The propagation speed is independent of \u03b1.\nD) The propagation speed is a constant for all values of \u03b1.\n\n**Correct Answer:** C) The propagation speed is independent of \u03b1.\n\n**Explanation:** According to the paper, the time-fractional diffusion-wave equation interpolates between the diffusion and wave equations, and its fundamental solution possesses a maximum that disperses with a finite speed. However, the propagation speed of the disturbance is infinite, but its fundamental solution possesses a maximum that disperses with a finite speed, regardless of the value of \u03b1. This implies that the propagation speed is independent of \u03b1."}, "38": {"documentation": {"title": "Effects of van der Waals Interactions in the Adsorption of Isooctane and\n  Ethanol on Fe(100) Surfaces", "source": "Pedro O. Bedolla, Gregor Feldbauer, Michael Wolloch, Stefan J. Eder,\n  Nicole D\\\"orr, Peter Mohn, Josef Redinger and Andr\\'as Vernes", "docs_id": "1405.2208", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of van der Waals Interactions in the Adsorption of Isooctane and\n  Ethanol on Fe(100) Surfaces. Van der Waals (vdW) forces play a fundamental role in the structure and behavior of diverse systems. Thanks to development of functionals that include non-local correlation, it is possible to study the effects of vdW interactions in systems of industrial and tribological interest. Here we simulated within the framework of density functional theory (DFT) the adsorption of isooctane (2,2,4-trimethylpentane) and ethanol on a Fe(100) surface, employing various exchange-correlation functionals to take vdW forces into account. In particular, this paper discusses the effect of vdW forces on the magnitude of adsorption energies, equilibrium geometries and their role in the binding mechanism. According to our calculations, vdW interactions increase the adsorption energies and reduce the equilibrium distances. Nevertheless, they do not influence the spatial configuration of the adsorbed molecules. Their effect on the electronic density is a non-isotropic, delocalized accumulation of charge between the molecule and the slab. In conclusion, vdW forces are essential for the adsorption of isooctane and ethanol on a bcc Fe(100) surface."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary effect of van der Waals (vdW) forces on the adsorption of isooctane and ethanol on a Fe(100) surface, according to the calculations presented in the paper?\n\nA) VDW forces increase the adsorption energies and reduce the equilibrium distances, but do not influence the spatial configuration of the adsorbed molecules.\nB) VDW forces increase the adsorption energies and reduce the equilibrium distances, and also influence the spatial configuration of the adsorbed molecules.\nC) VDW forces decrease the adsorption energies and increase the equilibrium distances, and do not influence the spatial configuration of the adsorbed molecules.\nD) VDW forces have no effect on the adsorption of isooctane and ethanol on a Fe(100) surface.\n\nCorrect Answer: A) VDW forces increase the adsorption energies and reduce the equilibrium distances, but do not influence the spatial configuration of the adsorbed molecules.\n\nExplanation: The correct answer is A) because the paper states that \"vdW interactions increase the adsorption energies and reduce the equilibrium distances\" and also mentions that \"they do not influence the spatial configuration of the adsorbed molecules\". This indicates that the primary effect of vdW forces is to increase the adsorption energies and reduce the equilibrium distances, but not to change the spatial arrangement of the adsorbed molecules."}, "39": {"documentation": {"title": "A General Framework for RIS-Aided mmWave Communication Networks: Channel\n  Estimation and Mobile User Tracking", "source": "Salah Eddine Zegrar, Liza Afeef, and Huseyin Arslan", "docs_id": "2009.01180", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A General Framework for RIS-Aided mmWave Communication Networks: Channel\n  Estimation and Mobile User Tracking. Reconfigurable intelligent surface (RIS) has been widely discussed as new technology to improve wireless communication performance. Based on the unique design of RIS, its elements can reflect, refract, absorb, or focus the incoming waves toward any desired direction. These functionalities turned out to be a major solution to overcome millimeter-wave (mmWave)'s high propagation conditions including path attenuation and blockage. However, channel estimation in RIS-aided communication is still a major concern due to the passive nature of RIS elements, and estimation overhead that arises with multiple-input multiple-output (MIMO) system. As a consequence, user tracking has not been analyzed yet. This paper is the first work that addresses channel estimation, beamforming, and user tracking under practical mmWave RIS-MIMO systems. By providing the mathematical relation of RIS design with a MIMO system, a three-stage framework is presented. Starting with estimating the channel between a base station (BS) and RIS using hierarchical beam searching, followed by estimating the channel between RIS and user using an iterative resolution algorithm. Lastly, a popular tracking algorithm is employed to track channel parameters between the RIS and the user. System analysis demonstrates the robustness and the effectiveness of the proposed framework in real-time scenarios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of RIS-aided mmWave communication networks, what is the primary challenge in channel estimation, and how does the proposed framework address this challenge?\n\n**A)** The primary challenge in channel estimation is the high propagation conditions of mmWave, which can lead to path attenuation and blockage. The proposed framework addresses this challenge by using hierarchical beam searching to estimate the channel between the base station (BS) and RIS.\n\n**B)** The primary challenge in channel estimation is the passive nature of RIS elements, which makes it difficult to estimate the channel parameters. The proposed framework addresses this challenge by using an iterative resolution algorithm to estimate the channel between RIS and the user.\n\n**C)** The primary challenge in channel estimation is the estimation overhead that arises with multiple-input multiple-output (MIMO) systems. The proposed framework addresses this challenge by providing a three-stage framework that includes channel estimation, beamforming, and user tracking.\n\n**D)** The primary challenge in channel estimation is the lack of user tracking in RIS-aided communication systems. The proposed framework addresses this challenge by employing a popular tracking algorithm to track channel parameters between the RIS and the user.\n\n**Correct Answer:** C) The primary challenge in channel estimation is the estimation overhead that arises with multiple-input multiple-output (MIMO) systems. The proposed framework addresses this challenge by providing a three-stage framework that includes channel estimation, beamforming, and user tracking.\n\n**Explanation:** The correct answer is C) because the proposed framework addresses the estimation overhead that arises with MIMO systems, which is a major concern in channel estimation for RIS-aided communication networks. The framework provides a three-stage approach that includes channel estimation, beamforming, and user tracking, which helps to mitigate the estimation overhead and improve the overall performance of the system."}, "40": {"documentation": {"title": "Direct observation of coupled geochemical and geomechanical impacts on\n  chalk microstructural evolution under elevated CO2 pressure. Part I", "source": "Y. Yang (1), S. S. Hakim (1), S. Bruns (1), M. Rogowska (1), S.\n  Boehnert (1), J.U. Hammel (2), S. L. S. Stipp (1), H. O. S{\\o}rensen (1) ((1)\n  Nano-Science Center, Department of Chemistry, University of Copenhagen, (2)\n  Helmholtz-Zentrum Geesthacht, Germany)", "docs_id": "1704.01064", "section": ["physics.geo-ph", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct observation of coupled geochemical and geomechanical impacts on\n  chalk microstructural evolution under elevated CO2 pressure. Part I. The dissolution of porous media in a geologic formation induced by the injection of massive amounts of CO2 can undermine the mechanical stability of the formation structure before carbon mineralization takes place. The geomechanical impact of geologic carbon storage is therefore closely related to the structural sustainability of the chosen reservoir as well as the probability of buoyancy driven CO2 leakage through caprocks. Here we show, with a combination of ex situ nanotomography and in situ microtomography, that the presence of dissolved CO2 in water produces a homogeneous dissolution pattern in natural chalk microstructure. This pattern stems from a greater apparent solubility of chalk and therefore a greater reactive subvolume in a sample. When a porous medium dissolves homogeneously in an imposed flow field, three geomechanical effects were observed: material compaction, fracturing and grain relocation. These phenomena demonstrated distinct feedbacks to the migration of the dissolution front and severely complicated the infiltration instability problem. We conclude that the presence of dissolved CO2 makes the dissolution front less susceptible to spatial and temporal perturbations in the strongly coupled geochemical and geomechanical processes."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary consequence of the dissolution of porous media in a geologic formation under elevated CO2 pressure, as observed in the study on chalk microstructural evolution under elevated CO2 pressure?\n\nA) The formation becomes more mechanically stable due to the presence of dissolved CO2.\nB) The dissolution front becomes more susceptible to spatial and temporal perturbations, leading to increased instability.\nC) The geomechanical impact of geologic carbon storage is closely related to the structural sustainability of the chosen reservoir.\nD) The presence of dissolved CO2 in water produces a homogeneous dissolution pattern in natural chalk microstructure, leading to material compaction, fracturing, and grain relocation.\n\n**Correct Answer:** D) The presence of dissolved CO2 in water produces a homogeneous dissolution pattern in natural chalk microstructure, leading to material compaction, fracturing, and grain relocation.\n\n**Explanation:**\n\nThe correct answer is D) because the study shows that the presence of dissolved CO2 in water produces a homogeneous dissolution pattern in natural chalk microstructure, leading to material compaction, fracturing, and grain relocation. This is a key finding of the study, as it highlights the geomechanical effects of CO2 dissolution on the microstructure of chalk.\n\nThe other options are incorrect because:\n\nA) The formation becomes more mechanically stable due to the presence of dissolved CO2 is not supported by the study, which actually shows that the dissolution of porous media can undermine the mechanical stability of the formation structure.\n\nB) The dissolution front becomes more susceptible to spatial and temporal perturbations is the opposite of what the study shows, which is that the presence of dissolved CO2 makes the dissolution front less susceptible to such perturbations.\n\nC) While the study does mention the relationship between the geomechanical impact of geologic carbon storage and the structural sustainability of the chosen reservoir, this is not the primary consequence of the dissolution of porous media under elevated CO2 pressure."}, "41": {"documentation": {"title": "Direct Evaluation of the Helium Abundances in Omega Centauri", "source": "A. K. Dupree and E. H. Avrett (Harvard-Smithsonian Center for\n  Astrophysics)", "docs_id": "1307.5860", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct Evaluation of the Helium Abundances in Omega Centauri. A direct measure of the helium abundances from the near-infrared transition of He I at 1.08 micron is obtained for two nearly identical red giant stars in the globular cluster Omega Centauri. One star exhibits the He I line; the line is weak or absent in the other star. Detailed non-LTE semi-empirical models including expansion in spherical geometry are developed to match the chromospheric H-alpha, H-beta, and Ca II K lines, in order to predict the helium profile and derive a helium abundance. The red giant spectra suggest a helium abundance of Y less than or equal 0.22 (LEID 54064) and Y=0.39-0.44 (LEID 54084) corresponding to a difference in the abundance Delta Y greater or equal than 0.17.Helium is enhanced in the giant star (LEID 54084) that also contains enhanced aluminum and magnesium. This direct evaluation of the helium abundances gives observational support to the theoretical conjecture that multiple populations harbor enhanced helium in addition to light elements that are products of high-temperature hydrogen burning. We demonstrate that the 1.08 micron He I line can yield a helium abundance in cool stars when constraints on the semi-empirical chromospheric model are provided by other spectroscopic features."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the direct evaluation of the helium abundances in Omega Centauri, and how does it relate to the theoretical conjecture about multiple populations in globular clusters?\n\nA) The helium abundance in Omega Centauri is consistent with the theoretical conjecture, but the direct evaluation does not provide conclusive evidence for multiple populations.\nB) The direct evaluation of helium abundances in Omega Centauri provides observational support for the theoretical conjecture that multiple populations harbor enhanced helium, in addition to light elements produced by high-temperature hydrogen burning.\nC) The helium abundance in Omega Centauri is lower than expected, and the direct evaluation suggests that the globular cluster may not be a multiple population system.\nD) The direct evaluation of helium abundances in Omega Centauri is inconclusive, and further observations are needed to determine the helium abundance in multiple populations.\n\nCorrect Answer: B) The direct evaluation of helium abundances in Omega Centauri provides observational support for the theoretical conjecture that multiple populations harbor enhanced helium, in addition to light elements produced by high-temperature hydrogen burning.\n\nExplanation: The correct answer is B) because the documentation states that the direct evaluation of helium abundances in Omega Centauri provides observational support for the theoretical conjecture that multiple populations harbor enhanced helium, in addition to light elements produced by high-temperature hydrogen burning. The other options are incorrect because they do not accurately reflect the conclusion drawn from the direct evaluation of helium abundances in Omega Centauri. Option A is incorrect because it suggests that the direct evaluation is inconclusive, while the documentation states that it provides observational support for the theoretical conjecture. Option C is incorrect because it suggests that the helium abundance is lower than expected, while the documentation states that the helium abundance is lower than or equal to 0.22. Option D is incorrect because it suggests that the direct evaluation is inconclusive, while the documentation states that it provides observational support for the theoretical conjecture."}, "42": {"documentation": {"title": "On biological flow networks: Antagonism between hydrodynamic and\n  metabolic stimuli as driver of topological transitions", "source": "Felix Kramer, Carl D. Modes", "docs_id": "2111.04326", "section": ["q-bio.TO", "nlin.AO", "physics.bio-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On biological flow networks: Antagonism between hydrodynamic and\n  metabolic stimuli as driver of topological transitions. A plethora of computational models have been developed in recent decades to account for the morphogenesis of complex biological fluid networks, such as capillary beds. Contemporary adaptation models are based on optimization schemes where networks react and adapt toward given flow patterns. Doing so, a system reduces dissipation and network volume, thereby altering its final form. Yet, recent numeric studies on network morphogenesis, incorporating uptake of metabolites by the embedding tissue, have indicated the conventional approach to be insufficient. Here, we systematically study a hybrid-model which combines the network adaptation schemes intended to generate space-filling perfusion as well as optimal filtration of metabolites. As a result, we find hydrodynamic stimuli (wall-shear stress) and filtration based stimuli (uptake of metabolites) to be antagonistic as hydrodynamically optimized systems have suboptimal uptake qualities and vice versa. We show that a switch between different optimization regimes is typically accompanied with a complex transition between topologically redundant meshes and spanning trees. Depending on the metabolite demand and uptake capabilities of the adaptating networks, we are further able to demonstrate the existence of nullity re-entrant behavior and the development of compromised phenotypes such as dangling non-perfused vessels and bottlenecks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of biological flow networks, what is the primary finding regarding the antagonism between hydrodynamic and metabolic stimuli, and how does it impact the morphogenesis of complex biological fluid networks?\n\nA) Hydrodynamic stimuli and metabolic stimuli are complementary, leading to optimal network formation.\nB) Hydrodynamic stimuli and metabolic stimuli are antagonistic, resulting in suboptimal network formation and topological transitions.\nC) Hydrodynamic stimuli and metabolic stimuli have no impact on network morphogenesis.\nD) Hydrodynamic stimuli and metabolic stimuli are equivalent, leading to identical network formation.\n\n**Correct Answer:** B) Hydrodynamic stimuli and metabolic stimuli are antagonistic, resulting in suboptimal network formation and topological transitions.\n\n**Explanation:** The correct answer is based on the finding that hydrodynamic stimuli (wall-shear stress) and filtration-based stimuli (uptake of metabolites) are antagonistic, meaning that hydrodynamically optimized systems have suboptimal uptake qualities and vice versa. This antagonism leads to complex transitions between topologically redundant meshes and spanning trees, and can result in compromised phenotypes such as dangling non-perfused vessels and bottlenecks."}, "43": {"documentation": {"title": "An Improved RIP-Based Performance Guarantee for Sparse Signal Recovery\n  via Orthogonal Matching Pursuit", "source": "Ling-Hua Chang and Jwo-Yuh Wu", "docs_id": "1401.0578", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Improved RIP-Based Performance Guarantee for Sparse Signal Recovery\n  via Orthogonal Matching Pursuit. A sufficient condition reported very recently for perfect recovery of a K-sparse vector via orthogonal matching pursuit (OMP) in K iterations is that the restricted isometry constant of the sensing matrix satisfies delta_K+1<1/(sqrt(delta_K+1)+1). By exploiting an approximate orthogonality condition characterized via the achievable angles between two orthogonal sparse vectors upon compression, this paper shows that the upper bound on delta can be further relaxed to delta_K+1<(sqrt(1+4*delta_K+1)-1)/(2K).This result thus narrows the gap between the so far best known bound and the ultimate performance guarantee delta_K+1<1/(sqrt(delta_K+1)) that is conjectured by Dai and Milenkovic in 2009. The proposed approximate orthogonality condition is also exploited to derive less restricted sufficient conditions for signal reconstruction in several compressive sensing problems, including signal recovery via OMP in a noisy environment, compressive domain interference cancellation, and support identification via the subspace pursuit algorithm."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the approximate orthogonality condition characterized via the achievable angles between two orthogonal sparse vectors upon compression, and how is it exploited in the paper to derive a further relaxed upper bound on the restricted isometry constant delta?\n\n**A)** The approximate orthogonality condition is characterized via the achievable angles between two orthogonal sparse vectors upon compression, and it is exploited to derive a further relaxed upper bound on the restricted isometry constant delta by showing that delta_K+1<(sqrt(1+4*delta_K+1)-1)/(2K).\n\n**B)** The approximate orthogonality condition is characterized via the achievable angles between two orthogonal sparse vectors upon compression, but it is not exploited to derive a further relaxed upper bound on the restricted isometry constant delta.\n\n**C)** The approximate orthogonality condition is not characterized via the achievable angles between two orthogonal sparse vectors upon compression, but rather via the restricted isometry constant delta.\n\n**D)** The approximate orthogonality condition is characterized via the achievable angles between two orthogonal sparse vectors upon compression, but it is exploited to derive a further relaxed upper bound on the restricted isometry constant delta by showing that delta_K+1<1/(sqrt(delta_K+1)+1).\n\n**Correct Answer:** A) The approximate orthogonality condition is characterized via the achievable angles between two orthogonal sparse vectors upon compression, and it is exploited to derive a further relaxed upper bound on the restricted isometry constant delta by showing that delta_K+1<(sqrt(1+4*delta_K+1)-1)/(2K).\n\n**Explanation:** The correct answer is A) because the paper indeed characterizes the approximate orthogonality condition via the achievable angles between two orthogonal sparse vectors upon compression, and it exploits this condition to derive a further relaxed upper bound on the restricted isometry constant delta. The correct formula is delta_K+1<(sqrt(1+4*delta_K+1)-1)/(2K), which is derived by exploiting the approximate orthogonality condition. The other options are incorrect because they either omit the exploitation of the approximate orthogonality condition or mischaracterize it."}, "44": {"documentation": {"title": "Direct Visualization of Perm-Selective Ion Transportation", "source": "Wonseok Kim, Jungeun Lee, Gun Yong Sung and Sung Jae Kim", "docs_id": "2001.10082", "section": ["physics.app-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Direct Visualization of Perm-Selective Ion Transportation. Perm-selective ion transportation in a nanoscale structure has been extensively studied with aids of nanofabrication technology for a decade. While theoretical and experimental advances pushed the phenomenon to seminal innovative applications, its basic observation has relied only on an indirect analysis such as current-voltage relation or fluorescent imaging adjacent to the nanostructures. Here we experimentally, for the first time, demonstrated a direct visualization of perm-selective ion transportation through the nanostructures using an ionic plasma generation. A micro/nanofluidic device was employed for a micro bubble formation, plasma negation and penetration of the plasma through the nanojunction. The direct observation provided a keen evidence of perm-selectivity, i.e. allowing cationic species and rejecting anionic species. Furthermore, we can capture the plasma of Li+, which has lower mobility than Na+ in aqueous state, passed the nanojunction faster than Na+ due to the absence of hydrated shells around Li+. This simple, but essential visualization technique would be effective means not only for advancing the fundamental nanoscale electrokinetic study but also for providing the insight of new innovative engineering applications."}, "answer": "Here is the difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the direct visualization technique employed in the study of perm-selective ion transportation through nanostructures?\n\nA) It allows for the observation of ion species with hydrated shells.\nB) It enables the detection of anionic species, which are rejected by the nanojunction.\nC) It provides a direct visualization of perm-selective ion transportation, allowing for the observation of cationic species and the rejection of anionic species.\nD) It facilitates the study of ion mobility in aqueous solutions.\n\nCorrect Answer: C) It provides a direct visualization of perm-selective ion transportation, allowing for the observation of cationic species and the rejection of anionic species.\n\nExplanation: The correct answer is C) because the study directly visualizes perm-selective ion transportation through nanostructures using an ionic plasma generation technique. This allows for the observation of cationic species (e.g. Li+) and the rejection of anionic species, providing a clear understanding of the perm-selective behavior. The other options are incorrect because they do not accurately describe the primary advantage of the direct visualization technique. Option A is incorrect because the technique does not observe ion species with hydrated shells. Option B is incorrect because the technique does not detect anionic species, but rather rejects them. Option D is incorrect because the technique is not primarily used to study ion mobility in aqueous solutions."}, "45": {"documentation": {"title": "Correlations between synapses in pairs of neurons slow down dynamics in\n  randomly connected neural networks", "source": "Daniel Mart\\'i, Nicolas Brunel, Srdjan Ostojic", "docs_id": "1707.08337", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Correlations between synapses in pairs of neurons slow down dynamics in\n  randomly connected neural networks. Networks of randomly connected neurons are among the most popular models in theoretical neuroscience. The connectivity between neurons in the cortex is however not fully random, the simplest and most prominent deviation from randomness found in experimental data being the overrepresentation of bidirectional connections among pyramidal cells. Using numerical and analytical methods, we investigated the effects of partially symmetric connectivity on dynamics in networks of rate units. We considered the two dynamical regimes exhibited by random neural networks: the weak-coupling regime, where the firing activity decays to a single fixed point unless the network is stimulated, and the strong-coupling or chaotic regime, characterized by internally generated fluctuating firing rates. In the weak-coupling regime, we computed analytically for an arbitrary degree of symmetry the auto-correlation of network activity in presence of external noise. In the chaotic regime, we performed simulations to determine the timescale of the intrinsic fluctuations. In both cases, symmetry increases the characteristic asymptotic decay time of the autocorrelation function and therefore slows down the dynamics in the network."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of neural networks, what is the effect of partially symmetric connectivity on the dynamics of the network, and how does it compare to the dynamics of randomly connected neural networks?\n\n**A)** Partially symmetric connectivity increases the strength of connections between neurons, leading to faster dynamics in the network.\n**B)** Partially symmetric connectivity increases the characteristic asymptotic decay time of the autocorrelation function, slowing down the dynamics in the network.\n**C)** Partially symmetric connectivity has no effect on the dynamics of the network, as it only affects the connectivity between neurons.\n**D)** Partially symmetric connectivity decreases the timescale of intrinsic fluctuations, leading to more chaotic behavior in the network.\n\n**Correct Answer:** B) Partially symmetric connectivity increases the characteristic asymptotic decay time of the autocorrelation function, slowing down the dynamics in the network.\n\n**Explanation:** The correct answer is based on the statement in the Arxiv documentation that \"symmetry increases the characteristic asymptotic decay time of the autocorrelation function and therefore slows down the dynamics in the network.\" This indicates that partially symmetric connectivity leads to a slower decay of the autocorrelation function, resulting in slower dynamics in the network."}, "46": {"documentation": {"title": "End-to-end Autonomous Driving Perception with Sequential Latent\n  Representation Learning", "source": "Jianyu Chen, Zhuo Xu and Masayoshi Tomizuka", "docs_id": "2003.12464", "section": ["cs.CV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "End-to-end Autonomous Driving Perception with Sequential Latent\n  Representation Learning. Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo and project website."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of using an end-to-end approach for autonomous driving perception, as proposed in the paper \"End-to-end Autonomous Driving Perception with Sequential Latent Representation Learning\"?\n\n**A)** It allows for online driving exploration, which is not possible with traditional perception systems.\n**B)** It enables the perception system to learn from raw sensor inputs, such as camera images and lidar point clouds, without requiring human engineering efforts.\n**C)** It reduces the need for map storage and online map updates, making it more suitable for real-world applications.\n**D)** It improves the performance of the perception system by incorporating multiple human heuristics and expert knowledge.\n\n**Correct Answer:** B) It enables the perception system to learn from raw sensor inputs, such as camera images and lidar point clouds, without requiring human engineering efforts.\n\n**Explanation:** The correct answer is B) because the paper highlights the advantage of using an end-to-end approach for autonomous driving perception, which allows the perception system to learn from raw sensor inputs without relying on human engineering efforts. This is a key benefit of the proposed method, as it enables the system to improve its performance with increasing data and computation resources. The other options are incorrect because they do not accurately capture the primary advantage of the end-to-end approach. Option A is incorrect because online driving exploration is not a primary advantage of the end-to-end approach. Option C is incorrect because while the end-to-end approach does reduce the need for map storage and online map updates, this is not its primary advantage. Option D is incorrect because the end-to-end approach does not rely on human heuristics and expert knowledge, but rather learns from raw sensor inputs."}, "47": {"documentation": {"title": "Revealing latent factors of temporal networks for mesoscale intervention\n  in epidemic spread", "source": "Laetitia Gauvin, Andr\\'e Panisson, Alain Barrat, and Ciro Cattuto", "docs_id": "1501.02758", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revealing latent factors of temporal networks for mesoscale intervention\n  in epidemic spread. The customary perspective to reason about epidemic mitigation in temporal networks hinges on the identification of nodes with specific features or network roles. The ensuing individual-based control strategies, however, are difficult to carry out in practice and ignore important correlations between topological and temporal patterns. Here we adopt a mesoscopic perspective and present a principled framework to identify collective features at multiple scales and rank their importance for epidemic spread. We use tensor decomposition techniques to build an additive representation of a temporal network in terms of mesostructures, such as cohesive clusters and temporally-localized mixing patterns. This representation allows to determine the impact of individual mesostructures on epidemic spread and to assess the effect of targeted interventions that remove chosen structures. We illustrate this approach using high-resolution social network data on face-to-face interactions in a school and show that our method affords the design of effective mesoscale interventions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of epidemic spread in temporal networks, what is the primary limitation of the customary perspective that focuses on identifying nodes with specific features or network roles, and how does the proposed mesoscopic approach address this limitation?\n\n**A)** The customary perspective is limited by its inability to account for the complex interactions between nodes, whereas the mesoscopic approach is limited by its reliance on tensor decomposition techniques.\n\n**B)** The customary perspective is limited by its focus on individual nodes, whereas the mesoscopic approach is limited by its inability to capture the temporal patterns of network evolution.\n\n**C)** The customary perspective is limited by its inability to identify collective features at multiple scales, whereas the mesoscopic approach is limited by its reliance on high-resolution social network data.\n\n**D)** The customary perspective is limited by its focus on epidemic mitigation strategies, whereas the mesoscopic approach is limited by its inability to account for the impact of targeted interventions.\n\n**Correct Answer:** B) The customary perspective is limited by its inability to capture the temporal patterns of network evolution.\n\n**Explanation:** The customary perspective focuses on identifying nodes with specific features or network roles, but this approach ignores the important correlations between topological and temporal patterns. In contrast, the mesoscopic approach uses tensor decomposition techniques to build an additive representation of a temporal network in terms of mesostructures, such as cohesive clusters and temporally-localized mixing patterns. This representation allows for the identification of collective features at multiple scales and the assessment of the impact of targeted interventions on epidemic spread. The correct answer highlights the limitation of the customary perspective in capturing temporal patterns, which is a key aspect of the mesoscopic approach."}, "48": {"documentation": {"title": "Adaptation through stochastic switching into transient mutators in\n  finite asexual populations", "source": "Muyoung Heo, Louis Kang and Eugene Shakhnovich", "docs_id": "0902.2404", "section": ["q-bio.BM", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptation through stochastic switching into transient mutators in\n  finite asexual populations. The importance of mutator clones in the adaptive evolution of asexual populations is not fully understood. Here we address this problem by using an ab initio microscopic model of living cells, whose fitness is derived directly from their genomes using a biophysically realistic model of protein folding and interactions in the cytoplasm. The model organisms contain replication controlling genes (DCGs) and genes modeling the mismatch repair (MMR) complexes. We find that adaptation occurs through the transient fixation of a mutator phenotype, regardless of particular perturbations in the fitness landscape. The microscopic pathway of adaptation follows a well-defined set of events: stochastic switching to the mutator phenotype first, then mutation in the MMR complex that hitchhikes with a beneficial mutation in the DCGs, and finally a compensating mutation in the MMR complex returning the population to a non-mutator phenotype. Similarity of these results to reported adaptation events points out to robust universal physical principles of evolutionary adaptation."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** In the context of adaptation through stochastic switching into transient mutators in finite asexual populations, what is the primary mechanism by which adaptation occurs, according to the microscopic model presented in the study?\n\n**A)** The transient fixation of a beneficial mutation in the DCGs, followed by a compensating mutation in the MMR complex.\n**B)** The stochastic switching to the mutator phenotype, followed by the mutation in the MMR complex that hitchhikes with a beneficial mutation in the DCGs.\n**C)** The mutation in the MMR complex that occurs first, followed by the stochastic switching to the mutator phenotype and the beneficial mutation in the DCGs.\n**D)** The compensating mutation in the MMR complex that returns the population to a non-mutator phenotype, followed by the stochastic switching to the mutator phenotype.\n\n**Correct Answer:** B) The stochastic switching to the mutator phenotype, followed by the mutation in the MMR complex that hitchhikes with a beneficial mutation in the DCGs.\n\n**Explanation:** The study presents a microscopic model of living cells, where adaptation occurs through the transient fixation of a mutator phenotype. The correct sequence of events is: stochastic switching to the mutator phenotype, followed by the mutation in the MMR complex that hitchhikes with a beneficial mutation in the DCGs. This sequence is supported by the biophysically realistic model of protein folding and interactions in the cytoplasm, and is consistent with the reported adaptation events."}, "49": {"documentation": {"title": "Kinetic frustration and the nature of the magnetic and paramagnetic\n  states in iron pnictides and iron chalcogenides", "source": "Z. P. Yin, K. Haule and G. Kotliar", "docs_id": "1104.3454", "section": ["cond-mat.str-el", "cond-mat.mtrl-sci", "cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kinetic frustration and the nature of the magnetic and paramagnetic\n  states in iron pnictides and iron chalcogenides. The iron pnictide and chalcogenide compounds are a subject of intensive investigations due to their high temperature superconductivity.\\cite{a-LaFeAsO} They all share the same structure, but there is significant variation in their physical properties, such as magnetic ordered moments, effective masses, superconducting gaps and T$_c$. Many theoretical techniques have been applied to individual compounds but no consistent description of the trends is available \\cite{np-review}. We carry out a comparative theoretical study of a large number of iron-based compounds in both their magnetic and paramagnetic states. We show that the nature of both states is well described by our method and the trends in all the calculated physical properties such as the ordered moments, effective masses and Fermi surfaces are in good agreement with experiments across the compounds. The variation of these properties can be traced to variations in the key structural parameters, rather than changes in the screening of the Coulomb interactions. Our results provide a natural explanation of the strongly Fermi surface dependent superconducting gaps observed in experiments\\cite{Ding}. We propose a specific optimization of the crystal structure to look for higher T$_c$ superconductors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary factor that explains the variation in physical properties among iron pnictide and chalcogenide compounds, according to the study mentioned in the documentation?\n\nA) Changes in the screening of the Coulomb interactions\nB) Variations in the key structural parameters\nC) Differences in the effective masses of the electrons\nD) Fluctuations in the magnetic ordered moments\n\n**Correct Answer:** B) Variations in the key structural parameters\n\n**Explanation:** The study suggests that the variation in physical properties among iron pnictide and chalcogenide compounds can be attributed to variations in the key structural parameters, rather than changes in the screening of the Coulomb interactions. This is evident from the statement \"The variation of these properties can be traced to variations in the key structural parameters, rather than changes in the screening of the Coulomb interactions.\"\n\n**Candidate A (Incorrect):** Changes in the screening of the Coulomb interactions is mentioned in the documentation as a factor that is not responsible for the variation in physical properties.\n\n**Candidate B (Correct):** Variations in the key structural parameters is the primary factor that explains the variation in physical properties among iron pnictide and chalcogenide compounds, according to the study.\n\n**Candidate C (Incorrect):** Differences in the effective masses of the electrons is not mentioned in the documentation as a factor that explains the variation in physical properties.\n\n**Candidate D (Incorrect):** Fluctuations in the magnetic ordered moments is not mentioned in the documentation as a factor that explains the variation in physical properties."}, "50": {"documentation": {"title": "Event-Triggered Control for Mitigating SIS Spreading Processes", "source": "Kazumune Hashimoto and Yuga Onoue and Masaki Ogura and Toshimitsu\n  Ushio", "docs_id": "2012.15146", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Event-Triggered Control for Mitigating SIS Spreading Processes. In this paper, we investigate the problem of designing event-triggered controllers for containing epidemic processes in complex networks. We focus on a deterministic susceptible-infected-susceptible (SIS) model, which is one of the well-known, fundamental models that capture the epidemic spreading. The event-triggered control is particularly formulated in the context of viral spreading, in which control inputs (e.g., the amount of medical treatments, a level of traffic regulations) for each subpopulation are updated only when the fraction of the infected people in the subpopulation exceeds a prescribed threshold. We analyze stability of the proposed event-triggered controller, and derives a sufficient condition for a prescribed control objective to be achieved. Moreover, we propose a novel emulation-based approach towards the design of the event-triggered controller, and show that the problem of designing the event-triggered controller can be solved in polynomial time using geometric programming. We illustrate the effectiveness of the proposed approach through numerical simulations using an air transportation network."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Event-Triggered Control for Mitigating SIS Spreading Processes paper, what is the primary objective of the proposed event-triggered controller, and how does it differ from traditional control approaches?\n\n**A)** The primary objective is to minimize the total number of infected individuals in the network, and the event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a traditional approach.\n\n**B)** The primary objective is to maximize the spread of the epidemic, and the event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a novel approach.\n\n**C)** The primary objective is to stabilize the system and prevent the spread of the epidemic, and the event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a deterministic approach.\n\n**D)** The primary objective is to emulate the behavior of the epidemic spreading process, and the event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a novel approach.\n\n**Correct Answer:** C) The primary objective is to stabilize the system and prevent the spread of the epidemic, and the event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a deterministic approach.\n\n**Explanation:** The correct answer is C) because the paper focuses on designing an event-triggered controller to mitigate SIS spreading processes, with the primary objective of stabilizing the system and preventing the spread of the epidemic. The event-triggered controller updates control inputs only when the fraction of infected individuals exceeds a prescribed threshold, which is a deterministic approach. The other options are incorrect because they do not accurately reflect the primary objective of the proposed event-triggered controller. Option A is incorrect because the event-triggered controller is designed to prevent the spread of the epidemic, not minimize the total number of infected individuals. Option B is incorrect because the event-triggered controller is designed to prevent the spread of the epidemic, not maximize it. Option D is incorrect because the event-triggered controller is designed to stabilize the system, not emulate the behavior of the epidemic spreading process."}, "51": {"documentation": {"title": "Kondo screening in a charge-insulating spinon metal", "source": "M. Gomil\\v{s}ek, R. \\v{Z}itko, M. Klanj\\v{s}ek, M. Pregelj, C. Baines,\n  Y. Li, Q. M. Zhang, and A. Zorko", "docs_id": "1904.06506", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kondo screening in a charge-insulating spinon metal. The Kondo effect, an eminent manifestation of many-body physics in condensed matter, is traditionally explained as exchange scattering of conduction electrons on a spinful impurity in a metal. The resulting screening of the impurity's local moment by the electron Fermi sea is characterized by a Kondo temperature $T_K$, below which the system enters a non-perturbative strongly-coupled regime. In recent years, this effect has found its realizations beyond the bulk-metal paradigm in many other itinerant-electron systems, such as quantum dots in semiconductor heterostructures and in nanomaterials, quantum point contacts, and graphene. Here we report on the first experimental observation of the Kondo screening by chargeless quasiparticles. This occurs in a charge-insulating quantum spin liquid, where spinon excitations forming a Fermi surface take the role of conduction electrons. The observed impurity behaviour therefore bears a strong resemblance to the conventional case in a metal. The discovered spinon-based Kondo effect provides a prominent platform for characterising and possibly manipulating enigmatic host spin liquids."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary difference between the conventional Kondo effect in a metal and the spinon-based Kondo effect observed in the charge-insulating quantum spin liquid described in the paper?\n\nA) The Kondo temperature is higher in the spinon-based case\nB) The Kondo temperature is lower in the spinon-based case\nC) The spinon-based Kondo effect is non-perturbative strongly-coupled at a lower temperature than the conventional Kondo effect\nD) The spinon-based Kondo effect is non-perturbative strongly-coupled at a higher temperature than the conventional Kondo effect\n\nCorrect Answer: D) The spinon-based Kondo effect is non-perturbative strongly-coupled at a higher temperature than the conventional Kondo effect\n\nExplanation: The paper states that the system enters a non-perturbative strongly-coupled regime below the Kondo temperature $T_K$, which is the same for both the conventional Kondo effect in a metal and the spinon-based Kondo effect in the charge-insulating quantum spin liquid. However, the paper also mentions that the observed impurity behavior in the spinon-based case \"bears a strong resemblance to the conventional case in a metal\", implying that the spinon-based Kondo effect occurs at a higher temperature than the conventional Kondo effect. This is because the spinon-based Kondo effect is mediated by chargeless quasiparticles, which are not subject to the same screening effects as conduction electrons in a metal."}, "52": {"documentation": {"title": "Separation of a Slater determinant wave function with a neck structure\n  into spatially localized subsystems", "source": "Yasutaka Taniguchi, Yoshiko Kanada-En'yo", "docs_id": "1111.1759", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Separation of a Slater determinant wave function with a neck structure\n  into spatially localized subsystems. A method to separate a Slater determinant wave function with a two-center neck structure into spatially localized subsystems is proposed, and its potential applications are presented. An orthonormal set of spatially localized single-particle wave functions is obtained by diagonalizing the coordinate operator for the major axis of a necked system. Using the localized single-particle wave functions, the wave function of each subsystem is defined. Therefore, defined subsystem wave functions are used to obtain density distributions, mass centers, and energies of subsystems. The present method is applied to separations of Margenau--Brink cluster wave functions of $\\alpha + \\alpha$, $^{16}$O + $^{16}$O, and $\\alpha + ^{16}$O into their subsystems, and also to separations of antisymmetrized molecular dynamics wave functions of $^{10}$Be into $\\alpha$ + $^6$He subsystems. The method is simple and applicable to the separation of general Slater determinant wave functions that have neck structures into subsystem wave functions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A Slater determinant wave function with a neck structure is given by:\n\n\u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i,j;k,l|\n\nwhere |i,j;k,l> are the Slater determinants of the individual subsystems. The neck structure is described by the coordinate operator:\n\nR = (x + x')/2 + (y + y')/2 + (z + z')/2\n\nwhere (x', y', z') are the coordinates of the neck. Diagonalize the coordinate operator R to obtain an orthonormal set of spatially localized single-particle wave functions. Use these wave functions to define the wave function of each subsystem. Calculate the density distribution, mass center, and energy of each subsystem for the following Slater determinant wave functions:\n\nA) \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j>\nB) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i,j;k,l>\nC) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i;j;k,l>\nD) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i;j;k,l> + \u2211m,n,o,p \u03b1m \u03b1n \u03b2o \u03b2p |m;n;o,p>\n\n**Correct Answer:** B) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i,j;k,l>\n\n**Explanation:**\n\nThe correct answer is B) because the Slater determinant wave function \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i,j;k,l> has a neck structure described by the coordinate operator R. Diagonalizing R yields an orthonormal set of spatially localized single-particle wave functions, which are used to define the wave function of each subsystem. The wave function \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i,j;k,l> is a Slater determinant wave function with a neck structure, and the correct separation of this wave function into subsystem wave functions is given by \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j> + \u2211k,l \u03b2k \u03b2l |k;l>, where |i;j> and |k;l> are the Slater determinants of the individual subsystems.\n\nThe other options are incorrect because:\n\nA) \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j> is a Slater determinant wave function without a neck structure.\n\nC) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i;j;k,l> is a Slater determinant wave function with a neck structure, but the correct separation of this wave function into subsystem wave functions is given by \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j> + \u2211k,l \u03b2k \u03b2l |k;l>, not \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j;k,l>.\n\nD) \u03c8(x, y, z) = \u2211i,j,k,l \u03b1i \u03b1j \u03b2k \u03b2l |i;j;k,l> + \u2211m,n,o,p \u03b1m \u03b1n \u03b2o \u03b2p |m;n;o,p> is a Slater determinant wave function with a neck structure, but the correct separation of this wave function into subsystem wave functions is given by \u03c8(x, y, z) = \u2211i,j \u03b1i \u03b1j |i;j> + \u2211k,l \u03b2k \u03b2l |k;l>, not the sum of two separate Slater determinant wave functions."}, "53": {"documentation": {"title": "Red Giants in the Small Magellanic Cloud. II. Metallicity Gradient and\n  Age-Metallicity Relation", "source": "P. D. Dobbie, A. A. Cole, A. Subramaniam, S. Keller", "docs_id": "1405.6452", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Red Giants in the Small Magellanic Cloud. II. Metallicity Gradient and\n  Age-Metallicity Relation. We present results from the largest CaII triplet line metallicity study of Small Magellanic Cloud (SMC) field red giant stars to date, involving 3037 objects spread across approximately 37.5 sq. deg., centred on this galaxy. We find a median metallicity of [Fe/H]=-0.99+/-0.01, with clear evidence for an abundance gradient of -0.075+/-0.011 dex / deg. over the inner 5 deg. We interpret the abundance gradient to be the result of an increasing fraction of young stars with decreasing galacto-centric radius, coupled with a uniform global age-metallicity relation. We also demonstrate that the age-metallicity relation for an intermediate age population located 10kpc in front of the NE of the Cloud is indistinguishable from that of the main body of the galaxy, supporting a prior conjecture that this is a stellar analogue of the Magellanic Bridge. The metal poor and metal rich quartiles of our RGB star sample (with complementary optical photometry from the Magellanic Clouds Photometric Survey) are predominantly older and younger than approximately 6Gyr, respectively. Consequently, we draw a link between a kinematical signature, tentatively associated by us with a disk-like structure, and the upsurges in stellar genesis imprinted on the star formation history of the central regions of the SMC. We conclude that the increase in the star formation rate around 5-6Gyr ago was most likely triggered by an interaction between the SMC and LMC."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the most likely trigger for the increase in the star formation rate around 5-6Gyr ago in the Small Magellanic Cloud (SMC), as inferred from the observed metallicity gradient and age-metallicity relation?\n\nA) A merger with the Large Magellanic Cloud (LMC) triggered a burst of star formation\nB) A disk-like structure in the central regions of the SMC led to an increase in star formation\nC) The SMC's interaction with the intergalactic medium (IGM) caused a surge in star formation\nD) The SMC's internal dynamics, such as a galactic fountain, triggered an increase in star formation\n\nCorrect Answer: B) A disk-like structure in the central regions of the SMC led to an increase in star formation\n\nExplanation: The text states that the metal poor and metal rich quartiles of the RGB star sample are predominantly older and younger than approximately 6Gyr, respectively. This suggests that there is a link between the kinematical signature of a disk-like structure and the upsurges in stellar genesis imprinted on the star formation history of the central regions of the SMC. Therefore, option B is the correct answer. Options A, C, and D are incorrect because they do not directly relate to the observed age-metallicity relation and metallicity gradient in the SMC."}, "54": {"documentation": {"title": "Non Fermi Liquid Crossovers in a Quasi-One-Dimensional Conductor in a\n  Tilted Magnetic Field", "source": "Andrei G. Lebed", "docs_id": "1511.01888", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non Fermi Liquid Crossovers in a Quasi-One-Dimensional Conductor in a\n  Tilted Magnetic Field. We consider a theoretical problem of electron-electron scattering time in a quasi-one-dimensional (Q1D) conductor in a magnetic field, perpendicular to its conducting axis. We show that inverse electron-electron scattering time becomes of the order of characteristic electron energy, $1/\\tau \\sim \\epsilon \\sim T$, in a high magnetic field, directed far from the main crystallographic axes, which indicates breakdown of the Fermi liquid theory. In a magnetic field, directed close to one of the main crystallographic axis, inverse electron-electron scattering time becomes much smaller than characteristic electron energy and, thus, applicability of Fermi liquid theory restores. We suggest that there exist crossovers between Fermi liquid and some non Fermi liquid states in a strong enough tilted magnetic field. Application of our results to the Q1D conductor (Per)$_2$Au(mnt)$_2$ shows that it has to be possible to observe the above mentioned phenomenon in feasibly high magnetic fields of the order of $H \\geq H^* \\simeq 25 \\ T$."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In a quasi-one-dimensional (Q1D) conductor in a tilted magnetic field, what is the expected behavior of the inverse electron-electron scattering time, and what implications does this have for the applicability of the Fermi liquid theory?\n\nA) Inverse electron-electron scattering time decreases with increasing magnetic field strength, restoring the applicability of the Fermi liquid theory.\nB) Inverse electron-electron scattering time increases with increasing magnetic field strength, indicating a breakdown of the Fermi liquid theory.\nC) Inverse electron-electron scattering time remains constant with increasing magnetic field strength, and the Fermi liquid theory remains applicable.\nD) Inverse electron-electron scattering time becomes of the order of characteristic electron energy, \u223c \u03b5 \u223c T, in a high magnetic field, indicating a breakdown of the Fermi liquid theory.\n\nCorrect Answer: D) Inverse electron-electron scattering time becomes of the order of characteristic electron energy, \u223c \u03b5 \u223c T, in a high magnetic field, indicating a breakdown of the Fermi liquid theory.\n\nExplanation: The correct answer is based on the provided documentation, which states that in a high magnetic field, directed far from the main crystallographic axes, the inverse electron-electron scattering time becomes of the order of characteristic electron energy, \u223c \u03b5 \u223c T, indicating a breakdown of the Fermi liquid theory. This is in contrast to the case where the magnetic field is directed close to one of the main crystallographic axes, where the inverse electron-electron scattering time becomes much smaller than characteristic electron energy, restoring the applicability of the Fermi liquid theory."}, "55": {"documentation": {"title": "Capacity Bounds under Imperfect Polarization Tracking", "source": "Mohammad Farsi, Magnus Karlsson, and Erik Agrell", "docs_id": "2112.12661", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Capacity Bounds under Imperfect Polarization Tracking. In optical fiber communication, due to the random variation of the environment, the state of polarization (SOP) fluctuates randomly with time leading to distortion and performance degradation. The memory-less SOP fluctuations can be regarded as a two-by-two random unitary matrix. In this paper, for what we believe to be the first time, the capacity of the polarization drift channel under an average power constraint with imperfect channel knowledge is characterized. An achievable information rate (AIR) is derived when imperfect channel knowledge is available and is shown to be highly dependent on the channel estimation technique. It is also shown that a tighter lower bound can be achieved when a unitary estimation of the channel is available. However, the conventional estimation algorithms do not guarantee a unitary channel estimation. Therefore, by considering the unitary constraint of the channel, a data-aided channel estimator based on the Kabsch algorithm is proposed, and its performance is numerically evaluated in terms of AIR. Monte Carlo simulations show that Kabsch outperforms the least-square error algorithm. In particular, with complex, Gaussian inputs and eight pilot symbols per block, Kabsch improves the AIR by 0:2 to 0:35 bits/symbol throughout the range of studied signal-to-noise ratios."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of optical fiber communication, what is the primary challenge posed by the random variation of the state of polarization (SOP) in the channel, and how does it impact the achievable information rate (AIR) under an average power constraint with imperfect channel knowledge?\n\n**A)** The random variation of SOP leads to a non-unitary channel matrix, resulting in a loss of information and a degradation of the channel capacity. This, in turn, affects the AIR, which is highly dependent on the channel estimation technique used.\n\n**B)** The random variation of SOP can be regarded as a two-by-two random unitary matrix, but this does not necessarily impact the AIR, as the channel estimation technique used can still provide a tight lower bound on the channel capacity.\n\n**C)** The random variation of SOP is a memory-less process, and therefore, it does not affect the AIR, which remains constant regardless of the channel estimation technique used.\n\n**D)** The random variation of SOP is a Gaussian process, and its impact on the AIR is limited to a small range of signal-to-noise ratios, where the Kabsch algorithm can provide a unitary estimation of the channel.\n\n**Correct Answer:** A) The random variation of SOP leads to a non-unitary channel matrix, resulting in a loss of information and a degradation of the channel capacity. This, in turn, affects the AIR, which is highly dependent on the channel estimation technique used.\n\n**Explanation:** The correct answer is A) because the random variation of SOP leads to a non-unitary channel matrix, which results in a loss of information and a degradation of the channel capacity. This, in turn, affects the AIR, which is highly dependent on the channel estimation technique used. The other options are incorrect because they either downplay the impact of SOP variation on the channel capacity (B and C) or incorrectly assume that the Kabsch algorithm is only applicable to a limited range of signal-to-noise ratios (D)."}, "56": {"documentation": {"title": "Chiral Vortical Effect For An Arbitrary Spin", "source": "Xu-Guang Huang and Andrey V. Sadofyev", "docs_id": "1805.08779", "section": ["hep-th", "cond-mat.other", "nucl-th", "physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chiral Vortical Effect For An Arbitrary Spin. The spin Hall effect of light attracted enormous attention in the literature due to the ongoing progress in developing of new optically active materials and metamaterials with non-trivial spin-orbit interaction. Recently, it was shown that rotating fermionic systems with relativistic massless spectrum may exhibit a 3d analogue of the spin Hall current -- the chiral vortical effect (CVE). Here we show that CVE is a general feature of massless particles with an arbitrary spin. We derive the semi-classical equations of motion in rotating frame from the first principles and show how by coordinate transformation in the phase space it can be brought to the intuitive form proposed in [1]. Our finding clarifies the superficial discrepancies in different formulations of the chiral kinetic theory for rotating systems. We then generalize the chiral kinetic theory, originally introduced for fermions, to an arbitrary spin and study chirality current in a general rotating chiral medium. We stress that the higher-spin realizations of CVE can be in principle observed in various setups including table-top experiments on quantum optics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the generalization of the chiral kinetic theory to an arbitrary spin, and how does it relate to the chiral vortical effect (CVE) in a rotating system?\n\nA) The chiral kinetic theory is generalized to an arbitrary spin by introducing a new parameter that depends on the spin of the particles, which leads to a more complex set of equations of motion.\n\nB) The chiral kinetic theory is generalized to an arbitrary spin by using a different coordinate transformation in the phase space, which allows for the derivation of the semi-classical equations of motion in a rotating frame.\n\nC) The chiral kinetic theory is generalized to an arbitrary spin by considering the spin-orbit interaction in the rotating system, which leads to a more intuitive form of the CVE.\n\nD) The chiral kinetic theory is generalized to an arbitrary spin by introducing a new type of particle with a non-trivial spin structure, which is not related to the CVE.\n\nCorrect Answer: B) The chiral kinetic theory is generalized to an arbitrary spin by using a different coordinate transformation in the phase space, which allows for the derivation of the semi-classical equations of motion in a rotating frame.\n\nExplanation: The correct answer is B) because the documentation states that the chiral kinetic theory is generalized to an arbitrary spin by using a coordinate transformation in the phase space, which allows for the derivation of the semi-classical equations of motion in a rotating frame. This transformation is mentioned in the introduction of the paper as a way to bring the equations of motion to an intuitive form. The other options are incorrect because they do not accurately describe the generalization of the chiral kinetic theory to an arbitrary spin. Option A is incorrect because it introduces a new parameter that is not mentioned in the documentation. Option C is incorrect because it mentions the spin-orbit interaction, but the documentation does not explicitly state that this is the reason for the generalization. Option D is incorrect because it introduces a new type of particle, which is not mentioned in the documentation."}, "57": {"documentation": {"title": "Effect of inter-layer spin diffusion on skyrmion motion in magnetic\n  multilayers", "source": "Serban Lepadatu", "docs_id": "1903.09398", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effect of inter-layer spin diffusion on skyrmion motion in magnetic\n  multilayers. It is well known that skyrmions can be driven using spin-orbit torques due to the spin-Hall effect. Here we show an additional contribution in multilayered stacks arises from vertical spin currents due to inter-layer diffusion of a spin accumulation generated at a skyrmion. This additional interfacial spin torque is similar in form to the in-plane spin transfer torque, but is significantly enhanced in ultra-thin films and acts in the opposite direction to the electron flow. The combination of this diffusive spin torque and the spin-orbit torque results in skyrmion motion which helps to explain the observation of small skyrmion Hall angles even with moderate magnetisation damping values. Further, the effect of material imperfections on threshold currents and skyrmion Hall angle is also investigated. Topographical surface roughness, as small as a single monolayer variation, is shown to be an important contributing factor in ultra-thin films, resulting in good agreement with experimental observations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism of skyrmion motion in magnetic multilayers, and how does it contribute to the observation of small skyrmion Hall angles?\n\nA) The spin-orbit torque due to the spin-Hall effect is the primary mechanism, and it is enhanced by ultra-thin films.\nB) The interfacial spin torque due to inter-layer diffusion of a spin accumulation generated at a skyrmion is the primary mechanism, and it acts in the opposite direction to the electron flow.\nC) The material imperfections, such as topographical surface roughness, are the primary mechanism, and they result in good agreement with experimental observations.\nD) The combination of the spin-orbit torque and the material imperfections is the primary mechanism, and it results in small skyrmion Hall angles.\n\nCorrect Answer: B) The interfacial spin torque due to inter-layer diffusion of a spin accumulation generated at a skyrmion is the primary mechanism, and it acts in the opposite direction to the electron flow.\n\nExplanation: The question requires the test-taker to understand the concept of skyrmion motion in magnetic multilayers and the different mechanisms that contribute to it. The correct answer, B, highlights the interfacial spin torque due to inter-layer diffusion of a spin accumulation generated at a skyrmion as the primary mechanism. This mechanism is similar in form to the in-plane spin transfer torque but is significantly enhanced in ultra-thin films and acts in the opposite direction to the electron flow. The other options are incorrect because they either focus on the spin-orbit torque (A) or material imperfections (C), or they combine multiple mechanisms (D) without specifying the primary mechanism."}, "58": {"documentation": {"title": "Longitudinal and transverse spin transfer to $\\Lambda$ and $\\bar\\Lambda$\n  hyperons in p+p collisions at STAR", "source": "Qing-Hua Xu (for the STAR Collaboration)", "docs_id": "1812.10621", "section": ["hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Longitudinal and transverse spin transfer to $\\Lambda$ and $\\bar\\Lambda$\n  hyperons in p+p collisions at STAR. The longitudinal or transverse spin transfer to Lambda and anti-Lambda hyperons in polarized proton-proton collisions is expected to be sensitive to the helicity or transversity distributions of strange and anti-strange quarks of the proton, and to the corresponding polarized fragmentation function. We report the first measurement of the transverse spin transfer to $\\Lambda$ and $\\bar \\Lambda$ along the polarization direction of the fragmenting quark, $D_{TT}$, in transversely polarized proton-proton collisions at 200 GeV with the STAR experiment at RHIC. The data correspond to an integrated luminosity of 18 pb$^{-1}$, and cover a kinematic range of |$\\eta$|< 1.2 and transverse momentum $p_T$ up to 8 GeV/c. We also report an improved measurement of the longitudinal spin transfer $D_{LL}$ to $\\Lambda$ and $\\bar \\Lambda$ with $p_T$ up to 6 GeV/c, using data with about twelve times larger figure-of-merit than the previously published STAR results. The prospects of hyperon polarization measurements in the forward pseudo-rapidity region (2.5<$\\eta$<4) in p+p collision in the year of 2021 and beyond will also be discussed, which is based on the STAR forward detector upgrade plan including a forward tracking system and a forward calorimeter system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat can be inferred about the helicity or transversity distributions of strange and anti-strange quarks of the proton from the measurement of the transverse spin transfer to \u039b and anti-\u039b hyperons in polarized proton-proton collisions at STAR?\n\nA) The helicity or transversity distributions are not sensitive to the spin transfer measurements.\nB) The helicity or transversity distributions are not affected by the fragmentation function.\nC) The helicity or transversity distributions of strange and anti-strange quarks are correlated with the longitudinal spin transfer to \u039b and anti-\u039b hyperons.\nD) The helicity or transversity distributions of strange and anti-strange quarks are directly related to the transverse spin transfer to \u039b and anti-\u039b hyperons, and can be inferred from the measurement of D_{TT}.\n\nCorrect Answer: D) The helicity or transversity distributions of strange and anti-strange quarks are directly related to the transverse spin transfer to \u039b and anti-\u039b hyperons, and can be inferred from the measurement of D_{TT}."}, "59": {"documentation": {"title": "Wind Power Providing Flexible Ramp Product", "source": "Runze Chen, Jianhui Wang, Audun Botterud, Hongbin Sun", "docs_id": "1601.02729", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wind Power Providing Flexible Ramp Product. The deepening penetration of renewables in power systems has contributed to the increasing needs for generation scheduling flexibility. Specifically, for short-term operations, flexibility here indicates that sufficient ramp capacities should be reserved to respond to the expected changes in the load and intermittent generation, also covering a certain amount of their uncertainty. To address the growing requirements for flexible ramp capacity, markets for ramp products have been launched in practice such as the ones in California ISO and Midcontinent ISO. Some-times, to guarantee sufficient ramp capacity, expensive fast start units have to be committed in real-time. Moreover, with higher penetration of renewable generation, the flexibility provided by the conventional units might not be enough. Actually, wind power producers are physically capable of offering flexibility, which is sometimes also economically efficient to the entire system. In this paper, we aim to explore the mechanism and possibility of including wind power producers as ramp providers to increase the supply of flexibility. To conduct the anal-yses, a two-stage stochastic real-time unit commitment model considering ramp capacity adequacy is formulated. Case studies indicate that both the system and the wind power producers can benefit if the wind power is allowed to provide flexible ramp products."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation for including wind power producers as ramp providers in power systems, and how does this approach address the growing requirements for flexible ramp capacity?\n\n**A)** To reduce the economic burden of expensive fast start units, wind power producers can offer flexibility at a lower cost, thereby increasing the overall efficiency of the system.\n\n**B)** To guarantee sufficient ramp capacity, wind power producers can be incentivized to provide flexibility, which can help mitigate the uncertainty associated with intermittent renewable generation.\n\n**C)** To increase the supply of flexibility, wind power producers can be allowed to provide ramp products, which can help address the growing requirements for flexible ramp capacity in power systems.\n\n**D)** To reduce the environmental impact of power generation, wind power producers can be incentivized to provide flexibility, which can help minimize the variability associated with renewable generation.\n\n**Correct Answer:** C) To increase the supply of flexibility, wind power producers can be allowed to provide ramp products, which can help address the growing requirements for flexible ramp capacity in power systems.\n\n**Explanation:** The correct answer is C) because the primary motivation for including wind power producers as ramp providers is to increase the supply of flexibility, which can help address the growing requirements for flexible ramp capacity in power systems. This approach allows wind power producers to provide flexibility, which can help mitigate the uncertainty associated with intermittent renewable generation. The other options are incorrect because they do not accurately capture the primary motivation for including wind power producers as ramp providers. Option A is incorrect because it suggests that wind power producers can offer flexibility at a lower cost, which is not the primary motivation. Option B is incorrect because it suggests that wind power producers can be incentivized to provide flexibility to mitigate the uncertainty associated with intermittent renewable generation, but this is not the primary motivation. Option D is incorrect because it suggests that wind power producers can be incentivized to provide flexibility to reduce the environmental impact of power generation, which is not the primary motivation."}}