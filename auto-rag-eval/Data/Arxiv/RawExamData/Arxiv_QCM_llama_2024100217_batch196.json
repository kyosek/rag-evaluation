{"0": {"documentation": {"title": "Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available", "source": "Diego Didona, Kristina Spirovska, Willy Zwaenepoel", "docs_id": "1702.04263", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available. Okapi is a new causally consistent geo-replicated key- value store. Okapi leverages two key design choices to achieve high performance. First, it relies on hybrid logical/physical clocks to achieve low latency even in the presence of clock skew. Second, Okapi achieves higher resource efficiency and better availability, at the expense of a slight increase in update visibility latency. To this end, Okapi implements a new stabilization protocol that uses a combination of vector and scalar clocks and makes a remote update visible when its delivery has been acknowledged by every data center. We evaluate Okapi with different workloads on Amazon AWS, using three geographically distributed regions and 96 nodes. We compare Okapi with two recent approaches to causal consistency, Cure and GentleRain. We show that Okapi delivers up to two orders of magnitude better performance than GentleRain and that Okapi achieves up to 3.5x lower latency and a 60% reduction of the meta-data overhead with respect to Cure."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary design choice of Okapi that enables it to achieve high performance, despite the presence of clock skew, and how does it compare to other approaches to causal consistency?\n\nA) Okapi relies on a single logical clock to achieve low latency, which is more efficient than using a hybrid logical/physical clock.\nB) Okapi uses a centralized clock tower to synchronize clocks across all data centers, which reduces latency and improves availability.\nC) Okapi implements a new stabilization protocol that uses a combination of vector and scalar clocks to ensure that remote updates are visible to every data center, which is more efficient than using a single logical clock.\nD) Okapi uses a distributed clock system that is synchronized across all data centers, which reduces latency and improves availability.\n\nCorrect Answer: C) Okapi implements a new stabilization protocol that uses a combination of vector and scalar clocks to ensure that remote updates are visible to every data center, which is more efficient than using a single logical clock.\n\nExplanation: The correct answer is C) because the documentation states that Okapi relies on hybrid logical/physical clocks to achieve low latency even in the presence of clock skew, and that it implements a new stabilization protocol that uses a combination of vector and scalar clocks to ensure that remote updates are visible to every data center. This protocol is more efficient than using a single logical clock, which is the primary design choice of Okapi that enables it to achieve high performance."}, "1": {"documentation": {"title": "Sequence selection by dynamical symmetry breaking in an autocatalytic\n  binary polymer model", "source": "Harold Fellermann, Shinpei Tanaka, Steen Rasmussen", "docs_id": "1708.04779", "section": ["q-bio.MN", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sequence selection by dynamical symmetry breaking in an autocatalytic\n  binary polymer model. Template directed replication of nucleic acids is at the essence of all living beings and a major milestone for any origin of life scenario. We here present an idealized model of prebiotic sequence replication, where binary polymers act as templates for their autocatalytic replication, thereby serving as each others reactants and products in an intertwined molecular ecology. Our model demonstrates how autocatalysis alters the qualitative and quantitative system dynamics in counter-intuitive ways. Most notably, numerical simulations reveal a very strong intrinsic selection mechanism that favours the appearance of a few population structures with highly ordered and repetitive sequence patterns when starting from a pool of monomers. We demonstrate both analytically and through simulation how this \"selection of the dullest\" is caused by continued symmetry breaking through random fluctuations in the transient dynamics that are amplified by autocatalysis and eventually propagate to the population level. The impact of these observations on related prebiotic mathematical models is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism by which autocatalysis in the binary polymer model leads to the selection of highly ordered and repetitive sequence patterns in the population?\n\nA) The autocatalytic reaction favors the formation of longer polymers, which in turn increase the likelihood of template-directed replication.\nB) The autocatalytic reaction creates a feedback loop that amplifies the presence of highly ordered sequences, leading to their increased replication and selection.\nC) The autocatalytic reaction introduces random fluctuations in the transient dynamics, which are amplified by autocatalysis and eventually propagate to the population level, favoring the appearance of highly ordered sequences.\nD) The autocatalytic reaction leads to the formation of a stable equilibrium state, where the population is composed of highly ordered and repetitive sequences.\n\nCorrect Answer: C) The autocatalytic reaction introduces random fluctuations in the transient dynamics, which are amplified by autocatalysis and eventually propagate to the population level, favoring the appearance of highly ordered sequences.\n\nExplanation: The correct answer is based on the idea that the autocatalytic reaction introduces random fluctuations in the transient dynamics, which are amplified by autocatalysis and eventually propagate to the population level, favoring the appearance of highly ordered sequences. This is described in the documentation as \"continued symmetry breaking through random fluctuations in the transient dynamics that are amplified by autocatalysis and eventually propagate to the population level.\" The other options are incorrect because they do not accurately describe the mechanism by which autocatalysis leads to the selection of highly ordered and repetitive sequence patterns."}, "2": {"documentation": {"title": "Cardy condition for open-closed field algebras", "source": "Liang Kong", "docs_id": "math/0612255", "section": ["math.QA", "hep-th", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cardy condition for open-closed field algebras. Let $V$ be a vertex operator algebra satisfying certain reductivity and finiteness conditions such that $\\mathcal{C}_V$, the category of V-modules, is a modular tensor category. We study open-closed field algebras over V equipped with nondegenerate invariant bilinear forms for both open and closed sectors. We show that they give algebras over certain $\\C$-extension of the Swiss-cheese partial dioperad, and we obtain Ishibashi states easily in such algebras. We formulate Cardy condition algebraically in terms of the action of the modular transformation $S: \\tau \\mapsto -\\frac{1}{\\tau}$ on the space of intertwining operators. We then derive a graphical representation of S in the modular tensor category $\\mathcal{C}_V$. This result enables us to give a categorical formulation of Cardy condition and modular invariant conformal full field algebra over $V\\otimes V$. Then we incorporate the modular invariance condition for genus-one closed theory, Cardy condition and the axioms for open-closed field algebra over V equipped with nondegenerate invariant bilinear forms into a tensor-categorical notion called Cardy $\\mathcal{C}_V|\\mathcal{C}_{V\\otimes V}$-algebra. We also give a categorical construction of Cardy $\\mathcal{C}_V|\\mathcal{C}_{V\\otimes V}$-algebra in Cardy case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of open-closed field algebras over a vertex operator algebra $V$, what is the relationship between the Cardy condition and the modular tensor category $\\mathcal{C}_V$, and how does it relate to the construction of Cardy $\\mathcal{C}_V|\\mathcal{C}_{V\\otimes V}$-algebras?\n\n**A)** The Cardy condition is equivalent to the existence of a nondegenerate invariant bilinear form on the space of intertwining operators, which in turn is related to the modular tensor category structure of $\\mathcal{C}_V$.\n\n**B)** The Cardy condition is a categorical formulation of the modular invariant conformal full field algebra over $V\\otimes V$, which can be represented graphically in the modular tensor category $\\mathcal{C}_V$.\n\n**C)** The Cardy condition is a tensor-categorical notion that incorporates the modular invariance condition for genus-one closed theory, Cardy condition, and the axioms for open-closed field algebra over $V$ equipped with nondegenerate invariant bilinear forms, and can be constructed categorically in the Cardy case.\n\n**D)** The Cardy condition is a mathematical framework that provides a categorical formulation of open-closed field algebras over $V$, which can be used to construct Ishibashi states and modular invariant conformal full field algebras over $V\\otimes V$.\n\n**Correct Answer:** C) The Cardy condition is a tensor-categorical notion that incorporates the modular invariance condition for genus-one closed theory, Cardy condition, and the axioms for open-closed field algebra over $V$ equipped with nondegenerate invariant bilinear forms, and can be constructed categorically in the Cardy case.\n\n**Explanation:** The correct answer is C) because it accurately summarizes the relationship between the Cardy condition and the modular tensor category $\\mathcal{C}_V$, as well as the construction of Cardy $\\mathcal{C}_V|\\mathcal{C}_{V\\otimes V}$-algebras. The other options are incorrect because they either oversimplify or misrepresent the relationship between the Cardy condition and the modular tensor category, or they fail to mention the incorporation of the modular invariance condition for genus-one closed theory and the axioms for open-closed field algebra."}, "3": {"documentation": {"title": "Ordering phenomena of spin trimers accompanied by large geometrical Hall\n  effect", "source": "Shang Gao, Max Hirschberger, Oksana Zaharko, Taro Nakajima, Takashi\n  Kurumaji, Akiko Kikkawa, Junichi Shiogai, Atsushi Tsukazaki, Shojiro Kimura,\n  Satoshi Awaji, Yasujiro Taguchi, Taka-hisa Arima, Yoshinori Tokura", "docs_id": "1908.07728", "section": ["cond-mat.str-el", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ordering phenomena of spin trimers accompanied by large geometrical Hall\n  effect. The wavefuntion of conduction electrons moving in the background of a non-coplanar spin structure can gain a quantal phase - Berry phase - as if the electrons were moving in a strong fictitious magnetic field. Such an emergent magnetic field effect is approximately proportional to the solid angle subtended by the spin moments on three neighbouring spin sites, termed the scalar spin chirality. The entire spin chirality of the crystal, unless macroscopically canceled, causes the geometrical Hall effect of real-space Berry-phase origin, whereas the intrinsic anomalous Hall effect (AHE) in a conventional metallic ferromagnet is of the momentum-space Berry-phase origin induced by relativistic spin-orbit coupling (SOC). Here, we report the ordering phenomena of the spin-trimer scalar spin chirality and the consequent large geometrical Hall effect in the breathing kagom\\'e lattice compound Dy$_3$Ru$_4$Al$_{12}$, where the Dy$^{3+}$ moments form non-coplanar spin trimers with local spin chirality. Using neutron diffraction, we show that the local spin chirality of the spin trimers as well as its ferroic/antiferroic orders can be switched by an external magnetic field, accompanying large changes in the geometrical Hall effect. Our finding reveals that systems composed of tunable spin trimers can be a fertile field to explore large emergent electromagnetic responses arising from real-space topological magnetic orders."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the origin of the geometrical Hall effect in a conventional metallic ferromagnet, and how does it differ from the origin in a system with tunable spin trimers?\n\n**A)** The geometrical Hall effect in a conventional metallic ferromagnet is of the momentum-space Berry-phase origin, induced by relativistic spin-orbit coupling (SOC). In contrast, the geometrical Hall effect in a system with tunable spin trimers is of the real-space Berry-phase origin, caused by the scalar spin chirality.\n\n**B)** The geometrical Hall effect in a conventional metallic ferromagnet is of the real-space Berry-phase origin, caused by the scalar spin chirality. In contrast, the geometrical Hall effect in a system with tunable spin trimers is of the momentum-space Berry-phase origin, induced by relativistic spin-orbit coupling (SOC).\n\n**C)** The geometrical Hall effect in a conventional metallic ferromagnet is of the momentum-space Berry-phase origin, induced by relativistic spin-orbit coupling (SOC). In contrast, the geometrical Hall effect in a system with tunable spin trimers is of the real-space Berry-phase origin, caused by the scalar spin chirality, which can be switched by an external magnetic field.\n\n**D)** The geometrical Hall effect in a conventional metallic ferromagnet is of the real-space Berry-phase origin, caused by the scalar spin chirality. In contrast, the geometrical Hall effect in a system with tunable spin trimers is of the momentum-space Berry-phase origin, induced by relativistic spin-orbit coupling (SOC), and can be switched by an external magnetic field.\n\n**Correct Answer:** C) The geometrical Hall effect in a conventional metallic ferromagnet is of the momentum-space Berry-phase origin, induced by relativistic spin-orbit coupling (SOC). In contrast, the geometrical Hall effect in a system with tunable spin trimers is of the real-space Berry-phase origin, caused by the scalar spin chirality, which can be switched by an external magnetic field.\n\n**Explanation:**\n\n* The correct answer, C, accurately describes the difference between the origins of the geometrical Hall effect in conventional metallic ferromagnets and systems with tunable spin trimers.\n* Option A is incorrect because it states that the geometrical Hall effect in conventional metallic ferromagnets is of the real-space Berry-phase origin, which is not the case.\n* Option B is incorrect because it reverses the correct relationship between the origins of the geometrical Hall effect in conventional metallic ferromagnets and systems with tunable spin trimers.\n* Option D is incorrect because it incorrectly states that the geometrical Hall effect in conventional metallic ferromagnets is of the real-space Berry-phase origin, caused by the scalar spin chirality, which is not the case."}, "4": {"documentation": {"title": "Semiclassical spatial correlations in chaotic wave functions", "source": "Fabricio Toscano and Caio H. Lewenkopf", "docs_id": "nlin/0108032", "section": ["nlin.CD", "cond-mat"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiclassical spatial correlations in chaotic wave functions. We study the spatial autocorrelation of energy eigenfunctions $\\psi_n({\\bf q})$ corresponding to classically chaotic systems in the semiclassical regime. Our analysis is based on the Weyl-Wigner formalism for the spectral average $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ of $\\psi_n({\\bf q}^{+})\\psi_n^*({\\bf q}^{-})$, defined as the average over eigenstates within an energy window $\\epsilon$ centered at $E$. In this framework $C_{\\epsilon}$ is the Fourier transform in momentum space of the spectral Wigner function $W({\\bf x},E;\\epsilon)$. Our study reveals the chord structure that $C_{\\epsilon}$ inherits from the spectral Wigner function showing the interplay between the size of the spectral average window, and the spatial separation scale. We discuss under which conditions is it possible to define a local system independent regime for $C_{\\epsilon}$. In doing so, we derive an expression that bridges the existing formulae in the literature and find expressions for $C_{\\epsilon}({\\bf q^{+}}, {\\bf q^{-}},E)$ valid for any separation size $|{\\bf q^{+}}-{\\bf q^{-}}|$."}, "answer": "Here is the exam question:\n\nQuestion: In the context of the Weyl-Wigner formalism, what is the relationship between the spectral average $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ and the spectral Wigner function $W({\\bf x},E;\\epsilon)$, and how does this relationship depend on the size of the spectral average window $\\epsilon$ and the spatial separation scale $|{\\bf q^{+}}-{\\bf q^{-}}|$?\n\nA) $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is the Fourier transform in momentum space of $W({\\bf x},E;\\epsilon)$, and the relationship is independent of $\\epsilon$ and $|{\\bf q^{+}}-{\\bf q^{-}}|$.\n\nB) $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is the Fourier transform in position space of $W({\\bf x},E;\\epsilon)$, and the relationship is independent of $\\epsilon$ and $|{\\bf q^{+}}-{\\bf q^{-}}|$.\n\nC) $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is the Fourier transform in momentum space of $W({\\bf x},E;\\epsilon)$, and the relationship depends on the size of the spectral average window $\\epsilon$ and the spatial separation scale $|{\\bf q^{+}}-{\\bf q^{-}}|$.\n\nD) $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is the Fourier transform in position space of $W({\\bf x},E;\\epsilon)$, and the relationship depends on the size of the spectral average window $\\epsilon$ and the spatial separation scale $|{\\bf q^{+}}-{\\bf q^{-}}|$.\n\nCorrect Answer: C) $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is the Fourier transform in momentum space of $W({\\bf x},E;\\epsilon)$, and the relationship depends on the size of the spectral average window $\\epsilon$ and the spatial separation scale $|{\\bf q^{+}}-{\\bf q^{-}}|$.\n\nExplanation: The correct answer is C) because the spectral average $C_{\\epsilon}({\\bf q^{+}},{\\bf q^{-}},E)$ is indeed the Fourier transform in momentum space of the spectral Wigner function $W({\\bf x},E;\\epsilon)$. However, the relationship between $C_{\\epsilon}$ and $W$ also depends on the size of the spectral average window $\\epsilon$ and the spatial separation scale $|{\\bf q^{+}}-{\\bf q^{-}}|$, as mentioned in the original text."}, "5": {"documentation": {"title": "Effects of electrical and optogenetic deep brain stimulation on\n  synchronized oscillatory activity in Parkinsonian basal ganglia", "source": "Shivakeshavan Ratnadurai-Giridharan, Chung Cheung, Leonid Rubchinsky", "docs_id": "1706.00976", "section": ["q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effects of electrical and optogenetic deep brain stimulation on\n  synchronized oscillatory activity in Parkinsonian basal ganglia. Conventional deep brain stimulation (DBS) of basal ganglia uses high-frequency regular electrical pulses to treat Parkinsonian motor symptoms and has a series of limitations. Relatively new and not yet clinically tested optogenetic stimulation is an effective experimental stimulation technique to affect pathological network dynamics. We compared the effects of electrical and optogenetic stimulation of the basal ganglia on the pathological parkinsonian rhythmic neural activity. We studied the network response to electrical stimulation and excitatory and inhibitory optogenetic stimulations. Different stimulations exhibit different interactions with pathological activity in the network. We studied these interactions for different network and stimulation parameter values. Optogenetic stimulation was found to be more efficient than electrical stimulation in suppressing pathological rhythmicity. Our findings indicate that optogenetic control of neural synchrony may be more efficacious than electrical control because of the different ways of how stimulations interact with network dynamics."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Compare and contrast the effects of electrical and optogenetic deep brain stimulation on synchronized oscillatory activity in Parkinsonian basal ganglia, considering the limitations of conventional deep brain stimulation and the potential benefits of optogenetic stimulation.\n\nA) Optogenetic stimulation is more efficient than electrical stimulation in suppressing pathological rhythmicity, but it requires more complex equipment and is more expensive.\n\nB) Electrical stimulation is more effective than optogenetic stimulation in treating Parkinsonian motor symptoms, but it can lead to tissue damage and is less precise.\n\nC) Optogenetic stimulation is more efficacious than electrical stimulation in controlling neural synchrony, but it may not be suitable for patients with certain medical conditions.\n\nD) Both electrical and optogenetic stimulation have similar effects on synchronized oscillatory activity in Parkinsonian basal ganglia, but optogenetic stimulation is more efficient in suppressing pathological rhythmicity.\n\nCorrect Answer: C) Optogenetic stimulation is more efficacious than electrical stimulation in controlling neural synchrony, but it may not be suitable for patients with certain medical conditions.\n\nExplanation: The correct answer is based on the finding that optogenetic stimulation was found to be more efficient than electrical stimulation in suppressing pathological rhythmicity. However, the question also asks to consider the limitations of optogenetic stimulation, such as its potential incompatibility with certain medical conditions. This requires the test-taker to weigh the benefits and limitations of both stimulation techniques and make a nuanced comparison."}, "6": {"documentation": {"title": "Doubly Robust Difference-in-Differences Estimators", "source": "Pedro H. C. Sant'Anna, Jun B. Zhao", "docs_id": "1812.01723", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Doubly Robust Difference-in-Differences Estimators. This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying articular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a difference-in-differences (DID) research design where the treatment effect is estimated using doubly robust estimators. Suppose we have access to panel data, which allows us to estimate the ATT in DID designs. However, we also have the option to use repeated cross-section data as an alternative. Which of the following statements about the efficiency of the doubly robust estimators is true?\n\nA) The doubly robust estimators are semiparametrically efficient when using panel data, but not when using repeated cross-section data.\nB) The doubly robust estimators are semiparametrically efficient when using repeated cross-section data, but not when using panel data.\nC) The doubly robust estimators are semiparametrically efficient when using either panel data or repeated cross-section data, as long as the working models are correctly specified.\nD) The doubly robust estimators are semiparametrically efficient when using either panel data or repeated cross-section data, but only if the propensity score is correctly specified.\n\n**Correct Answer:** C) The doubly robust estimators are semiparametrically efficient when using either panel data or repeated cross-section data, as long as the working models are correctly specified.\n\n**Explanation:** According to the article, the doubly robust estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. Additionally, the article shows that the proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified, regardless of whether panel or repeated cross-section data are used. Therefore, option C is the correct answer."}, "7": {"documentation": {"title": "Interacting Q-balls", "source": "Yves Brihaye (Universite de Mons-Hainaut, Belgium) and Betti Hartmann\n  (Jacobs University Bremen, Germany)", "docs_id": "0711.1969", "section": ["hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interacting Q-balls. We study non-topological solitons, so called Q-balls, which carry a non-vanishing Noether charge and arise as lump solutions of self-interacting complex scalar field models. Explicit examples of new axially symmetric non-spinning Q-ball solutions that have not been studied so far are constructed numerically. These solutions can be interpreted as angular excitations of the fundamental $Q$-balls and are related to the spherical harmonics. Correspondingly, they have higher energy and their energy densities possess two local maxima on the positive z-axis. We also study two Q-balls interacting via a potential term in (3+1) dimensions and construct examples of stationary, solitonic-like objects in (3+1)-dimensional flat space-time that consist of two interacting global scalar fields. We concentrate on configurations composed of one spinning and one non-spinning Q-ball and study the parameter-dependence of the energy and charges of the configuration. In addition, we present numerical evidence that for fixed values of the coupling constants two different types of 2-Q-ball solutions exist: solutions with defined parity, but also solutions which are asymmetric with respect to reflexion through the x-y-plane."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of interacting Q-balls, what is the physical significance of the spherical harmonics in the construction of new axially symmetric non-spinning Q-ball solutions?\n\nA) They represent the angular momentum of the Q-ball.\nB) They describe the parity symmetry of the Q-ball configuration.\nC) They correspond to the angular excitations of the fundamental Q-balls.\nD) They are a consequence of the Noether charge carried by the Q-ball.\n\nCorrect Answer: C) They correspond to the angular excitations of the fundamental Q-balls.\n\nExplanation: The correct answer is C) They correspond to the angular excitations of the fundamental Q-balls. The documentation states that the new axially symmetric non-spinning Q-ball solutions can be interpreted as angular excitations of the fundamental Q-balls, and are related to the spherical harmonics. This implies that the spherical harmonics are a mathematical representation of the angular excitations of the fundamental Q-balls.\n\nThe other options are incorrect because:\n\nA) The angular momentum of the Q-ball is not directly related to the spherical harmonics.\n\nB) The parity symmetry of the Q-ball configuration is not the primary significance of the spherical harmonics.\n\nD) The Noether charge carried by the Q-ball is not directly related to the spherical harmonics.\n\nThis question requires the test-taker to understand the mathematical and physical context of the Q-ball solutions, and to identify the correct relationship between the spherical harmonics and the angular excitations of the fundamental Q-balls."}, "8": {"documentation": {"title": "Transition to complete synchronization and global intermittent\n  synchronization in an array of time-delay systems", "source": "R. Suresh, D. V. Senthilkumar, M. Lakshmanan and J. Kurths", "docs_id": "1207.4888", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transition to complete synchronization and global intermittent\n  synchronization in an array of time-delay systems. We report the nature of transitions from nonsynchronous to complete synchronization (CS) state in arrays of time-delay systems, where the systems are coupled with instantaneous diffusive coupling. We demonstrate that the transition to CS occurs distinctly for different coupling configurations. In particular, for unidirectional coupling, locally (microscopically) synchronization transition occurs in a very narrow range of coupling strength but for a global one (macroscopically) it occurs sequentially in a broad range of coupling strength preceded by an intermittent synchronization. On the other hand, in the case of mutual coupling a very large value of coupling strength is required for local synchronization and, consequently, all the local subsystems synchronize immediately for the same value of the coupling strength and hence globally synchronization also occurs in a narrow range of the coupling strength. In the transition regime, we observe a new type of synchronization transition where long intervals of high quality synchronization which are interrupted at irregular times by intermittent chaotic bursts simultaneously in all the systems, which we designate as global intermittent synchronization (GIS). We also relate our synchronization transition results to the above specific types using unstable periodic orbit theory. The above studies are carried out in a well known piecewise linear time-delay system."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the study on transition to complete synchronization and global intermittent synchronization in an array of time-delay systems, what is the primary difference in the synchronization transition behavior between unidirectional and mutual coupling configurations?\n\n**A)** Unidirectional coupling requires a larger range of coupling strength for local synchronization, while mutual coupling requires a smaller range of coupling strength for local synchronization.\n\n**B)** Unidirectional coupling exhibits global intermittent synchronization (GIS) in a broad range of coupling strength, while mutual coupling exhibits GIS in a narrow range of coupling strength.\n\n**C)** Unidirectional coupling requires a very large value of coupling strength for local synchronization, while mutual coupling requires a very small value of coupling strength for local synchronization.\n\n**D)** Unidirectional coupling exhibits sequential global synchronization, while mutual coupling exhibits simultaneous global synchronization.\n\n**Correct Answer:** B) Unidirectional coupling exhibits global intermittent synchronization (GIS) in a broad range of coupling strength, while mutual coupling exhibits GIS in a narrow range of coupling strength.\n\n**Explanation:** The study reports that in the transition regime, a new type of synchronization transition is observed, where long intervals of high quality synchronization are interrupted at irregular times by intermittent chaotic bursts simultaneously in all the systems, which is designated as global intermittent synchronization (GIS). This phenomenon is observed in both unidirectional and mutual coupling configurations, but the range of coupling strength required for GIS differs between the two. Specifically, unidirectional coupling exhibits GIS in a broad range of coupling strength, while mutual coupling exhibits GIS in a narrow range of coupling strength."}, "9": {"documentation": {"title": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections", "source": "Ping Li", "docs_id": "cs/0611114", "section": ["cs.DS", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections. This paper will focus on three different aspects in improving the current practice of stable random projections. Firstly, we propose {\\em very sparse stable random projections} to significantly reduce the processing and storage cost, by replacing the $\\alpha$-stable distribution with a mixture of a symmetric $\\alpha$-Pareto distribution (with probability $\\beta$, $0<\\beta\\leq1$) and a point mass at the origin (with a probability $1-\\beta$). This leads to a significant $\\frac{1}{\\beta}$-fold speedup for small $\\beta$. Secondly, we provide an improved estimator for recovering the original $l_\\alpha$ norms from the projected data. The standard estimator is based on the (absolute) sample median, while we suggest using the geometric mean. The geometric mean estimator we propose is strictly unbiased and is easier to study. Moreover, the geometric mean estimator is more accurate, especially non-asymptotically. Thirdly, we provide an adequate answer to the basic question of how many projections (samples) are needed for achieving some pre-specified level of accuracy. \\cite{Proc:Indyk_FOCS00,Article:Indyk_TKDE03} did not provide a criterion that can be used in practice. The geometric mean estimator we propose allows us to derive sharp tail bounds which can be expressed in exponential forms with constants explicitly given."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Very Sparse Stable Random Projections, Estimators and Tail Bounds for Stable Random Projections, what is the primary advantage of using a mixture of a symmetric \u03b1-Pareto distribution and a point mass at the origin, and how does it impact the processing and storage cost?\n\n**A)** The mixture distribution reduces the computational complexity of the \u03b1-stable distribution, leading to a linear speedup.\n**B)** The mixture distribution significantly reduces the processing and storage cost by replacing the \u03b1-stable distribution with a mixture of a symmetric \u03b1-Pareto distribution (with probability \u03b2, 0<\u03b2\u22641) and a point mass at the origin (with a probability 1-\u03b2), leading to a speedup of O(1/\u03b2).\n**C)** The mixture distribution increases the dimensionality of the projected data, leading to a quadratic speedup.\n**D)** The mixture distribution does not affect the processing and storage cost, as the \u03b1-stable distribution is already very sparse.\n\n**Correct Answer:** B) The mixture distribution significantly reduces the processing and storage cost by replacing the \u03b1-stable distribution with a mixture of a symmetric \u03b1-Pareto distribution (with probability \u03b2, 0<\u03b2\u22641) and a point mass at the origin (with a probability 1-\u03b2), leading to a speedup of O(1/\u03b2).\n\n**Explanation:** The correct answer is B) because the mixture distribution is proposed to reduce the processing and storage cost by replacing the \u03b1-stable distribution with a mixture of a symmetric \u03b1-Pareto distribution and a point mass at the origin. This leads to a significant speedup of O(1/\u03b2) for small \u03b2. The other options are incorrect because they do not accurately describe the advantage of the mixture distribution. Option A is incorrect because the mixture distribution does not reduce the computational complexity of the \u03b1-stable distribution. Option C is incorrect because the mixture distribution does not increase the dimensionality of the projected data. Option D is incorrect because the mixture distribution does affect the processing and storage cost."}, "10": {"documentation": {"title": "Search-based Test-Case Generation by Monitoring Responsibility Safety\n  Rules", "source": "Mohammad Hekmatnejad, Bardh Hoxha and Georgios Fainekos", "docs_id": "2005.00326", "section": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Search-based Test-Case Generation by Monitoring Responsibility Safety\n  Rules. The safety of Automated Vehicles (AV) as Cyber-Physical Systems (CPS) depends on the safety of their consisting modules (software and hardware) and their rigorous integration. Deep Learning is one of the dominant techniques used for perception, prediction, and decision making in AVs. The accuracy of predictions and decision-making is highly dependant on the tests used for training their underlying deep-learning. In this work, we propose a method for screening and classifying simulation-based driving test data to be used for training and testing controllers. Our method is based on monitoring and falsification techniques, which lead to a systematic automated procedure for generating and selecting qualified test data. We used Responsibility Sensitive Safety (RSS) rules as our qualifier specifications to filter out the random tests that do not satisfy the RSS assumptions. Therefore, the remaining tests cover driving scenarios that the controlled vehicle does not respond safely to its environment. Our framework is distributed with the publicly available S-TALIRO and Sim-ATAV tools."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Search-based Test-Case Generation by Monitoring Responsibility Safety for Automated Vehicles (AVs), what is the primary purpose of using Responsibility Sensitive Safety (RSS) rules as qualifier specifications in the proposed method?\n\n**A)** To improve the accuracy of predictions and decision-making in AVs by filtering out tests that do not satisfy RSS assumptions.\n**B)** To enhance the robustness of AVs by generating and selecting qualified test data that covers driving scenarios where the controlled vehicle does not respond safely to its environment.\n**C)** To reduce the computational complexity of simulation-based driving test data by eliminating redundant tests.\n**D)** To increase the efficiency of the testing process by automating the procedure for generating and selecting qualified test data.\n\n**Correct Answer:** B) To enhance the robustness of AVs by generating and selecting qualified test data that covers driving scenarios where the controlled vehicle does not respond safely to its environment.\n\n**Explanation:** The correct answer is B) because the proposed method uses RSS rules to filter out tests that do not satisfy the assumptions of the RSS framework, thereby ensuring that the remaining tests cover driving scenarios where the controlled vehicle does not respond safely to its environment. This is crucial for ensuring the safety of AVs, as it allows for the identification of potential safety issues and the development of more robust control systems. The other options are incorrect because they do not accurately reflect the primary purpose of using RSS rules in the proposed method."}, "11": {"documentation": {"title": "Dominant Resource Fairness with Meta-Types", "source": "Steven Yin, Shatian Wang, Lingyi Zhang, Christian Kroer", "docs_id": "2007.11961", "section": ["econ.TH", "cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dominant Resource Fairness with Meta-Types. Inspired by the recent COVID-19 pandemic, we study a generalization of the multi-resource allocation problem with heterogeneous demands and Leontief utilities. Unlike existing settings, we allow each agent to specify requirements to only accept allocations from a subset of the total supply for each resource. These requirements can take form in location constraints (e.g. A hospital can only accept volunteers who live nearby due to commute limitations). This can also model a type of substitution effect where some agents need 1 unit of resource A \\emph{or} B, both belonging to the same meta-type. But some agents specifically want A, and others specifically want B. We propose a new mechanism called Dominant Resource Fairness with Meta Types which determines the allocations by solving a small number of linear programs. The proposed method satisfies Pareto optimality, envy-freeness, strategy-proofness, and a notion of sharing incentive for our setting. To the best of our knowledge, we are the first to study this problem formulation, which improved upon existing work by capturing more constraints that often arise in real life situations. Finally, we show numerically that our method scales better to large problems than alternative approaches."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A hospital has a limited supply of medical staff, and there are three types of medical staff: doctors, nurses, and orderlies. The hospital has a total of 100 medical staff positions to fill, with the following constraints:\n\n* 20% of the staff must be doctors, 30% must be nurses, and 50% must be orderlies.\n* The hospital has a location constraint, where only staff members who live within a 50-mile radius of the hospital can be hired.\n* Some staff members require both doctors and nurses to be present in the same shift, while others require only one or the other.\n\nA mechanism is proposed to allocate the medical staff positions fairly, taking into account these constraints. The mechanism is based on a linear program that solves for the optimal allocation of staff members to shifts.\n\n**Which of the following mechanisms is guaranteed to satisfy the following properties:**\n\nA) Pareto optimality, envy-freeness, and strategy-proofness, but may not capture the substitution effect between doctors and nurses.\n\nB) Pareto optimality, envy-freeness, and strategy-proofness, and also captures the substitution effect between doctors and nurses.\n\nC) Pareto optimality, envy-freeness, and strategy-proofness, but may not scale well to large problems.\n\nD) Pareto optimality, envy-freeness, strategy-proofness, and a notion of sharing incentive, but may not capture the location constraint.\n\n**Correct Answer:** B) Pareto optimality, envy-freeness, and strategy-proofness, and also captures the substitution effect between doctors and nurses.\n\n**Explanation:**\n\nThe correct answer is B) Pareto optimality, envy-freeness, and strategy-proofness, and also captures the substitution effect between doctors and nurses. This is because the mechanism proposed in the paper, Dominant Resource Fairness with Meta-Types, is designed to capture both the location constraint and the substitution effect between doctors and nurses. The paper shows that this mechanism satisfies Pareto optimality, envy-freeness, and strategy-proofness, and also captures the substitution effect between doctors and nurses.\n\nThe other options are incorrect because:\n\nA) The mechanism may not capture the substitution effect between doctors and nurses, as it is not explicitly stated in the paper.\n\nC) The mechanism may not scale well to large problems, as the paper states that it scales better to large problems than alternative approaches.\n\nD) The mechanism satisfies Pareto optimality, envy-freeness, strategy-proofness, and a notion of sharing incentive, but the paper does not explicitly state that it captures the location constraint."}, "12": {"documentation": {"title": "A multitype sticky particle construction of Wasserstein stable\n  semigroups solving one-dimensional diagonal hyperbolic systems with large\n  monotonic data", "source": "Benjamin Jourdain (MATHRISK, CERMICS), Julien Reygner (CERMICS, LPMA,\n  Phys-ENS)", "docs_id": "1501.01498", "section": ["math.AP", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A multitype sticky particle construction of Wasserstein stable\n  semigroups solving one-dimensional diagonal hyperbolic systems with large\n  monotonic data. This article is dedicated to the study of diagonal hyperbolic systems in one space dimension, with cumulative distribution functions, or more generally nonconstant monotonic bounded functions, as initial data. Under a uniform strict hyperbolicity assumption on the characteristic fields, we construct a multitype version of the sticky particle dynamics and obtain existence of global weak solutions by compactness. We then derive a $L^p$ stability estimate on the particle system uniform in the number of particles. This allows to construct nonlinear semigroups solving the system in the sense of Bianchini and Bressan [Ann. of Math. (2), 2005]. We also obtain that these semigroup solutions satisfy a stability estimate in Wasserstein distances of all orders, which encompasses the classical $L^1$ estimate and generalises to diagonal systems the results by Bolley, Brenier and Loeper [J. Hyperbolic Differ. Equ., 2005] in the scalar case. Our results are obtained without any smallness assumption on the variation of the data, and only require the characteristic fields to be Lipschitz continuous and the system to be uniformly strictly hyperbolic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a diagonal hyperbolic system in one space dimension with initial data given by a nonconstant monotonic bounded function. Under the assumption of uniform strict hyperbolicity on the characteristic fields, what is the necessary condition for the existence of global weak solutions to the system, and what is the role of the Lipschitz continuity of the characteristic fields in this context?\n\nA) The existence of global weak solutions requires the initial data to be Lipschitz continuous, and the Lipschitz continuity of the characteristic fields is necessary for the stability of the solutions in Wasserstein distances of all orders.\nB) The existence of global weak solutions requires the initial data to be uniformly strictly hyperbolic, and the Lipschitz continuity of the characteristic fields is sufficient for the stability of the solutions in $L^p$ norms.\nC) The existence of global weak solutions requires the initial data to be uniformly strictly hyperbolic, and the Lipschitz continuity of the characteristic fields is necessary for the stability of the solutions in $L^p$ norms.\nD) The existence of global weak solutions requires the initial data to be Lipschitz continuous, and the Lipschitz continuity of the characteristic fields is sufficient for the stability of the solutions in Wasserstein distances of all orders.\n\nCorrect Answer: C) The existence of global weak solutions requires the initial data to be uniformly strictly hyperbolic, and the Lipschitz continuity of the characteristic fields is necessary for the stability of the solutions in $L^p$ norms.\n\nExplanation: The correct answer is C) because the documentation states that the existence of global weak solutions is obtained by compactness, and the uniform strict hyperbolicity assumption on the characteristic fields is necessary for this. Additionally, the Lipschitz continuity of the characteristic fields is required for the stability of the solutions in $L^p$ norms, as stated in the documentation. The other options are incorrect because they either misstate the necessary condition for the existence of global weak solutions or incorrectly attribute the role of the Lipschitz continuity of the characteristic fields."}, "13": {"documentation": {"title": "Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads", "source": "Tom\\'as Ram\\'irez (1), Carlos Mosquera (1), Nader Alagha (2) ((1)\n  atlanTTic Research Center, Universidade de Vigo, Galicia, Spain, (2) European\n  Space Agency Technical Research Center (ESTEC), Noordwijk, The Netherlands)", "docs_id": "2109.09385", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads. This work explores the flexible assignment of users to beams in order to match the non-uniform traffic demand in satellite systems, breaking the conventional cell boundaries and serving users potentially by non-dominant beams. The additional degree of freedom from the flexible beam-user mapping brings about a user-centric framework, with a two-step optimization process as a practical strategy. The smart beam-user mapping is jointly explored with adjustable bandwidth allocation per beam, and tested against different techniques for payloads with flexible radio resource allocation. Numerical results are obtained for various non-uniform traffic distributions to evaluate the performance of the solutions. The traffic profile across beams is shaped by the Dirichlet distribution, which can be conveniently parameterized, and makes simulations easily reproducible. Even with ideal conditions for the power allocation, both flexible beam-user mapping and adjustable power allocation similarly enhance the flexible assignment of the bandwidth on average. Results show that a smart pairing of users and beams provides significant advantages in highly asymmetric demand scenarios, with improvements up to 10\\% and 30\\% in terms of the offered and the minimum user rates, respectively, in hot-spot like cases with no impact on the user equipment."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of flexible user mapping for radio resource assignment in advanced satellite payloads, what is the primary advantage of using a smart beam-user mapping approach, as described in the paper?\n\n**A)** It allows for more efficient use of bandwidth resources, but may lead to increased complexity in beam management.\n**B)** It enables the optimization of power allocation and bandwidth allocation per beam, leading to improved performance in highly asymmetric demand scenarios.\n**C)** It provides a user-centric framework, but may not be suitable for satellite systems with limited beam resources.\n**D)** It offers significant advantages in terms of offered and minimum user rates, but may require significant computational resources for optimization.\n\n**Correct Answer:** B) It enables the optimization of power allocation and bandwidth allocation per beam, leading to improved performance in highly asymmetric demand scenarios.\n\n**Explanation:** The correct answer is B) because the paper describes the smart beam-user mapping approach as a two-step optimization process that jointly explores the flexible beam-user mapping and adjustable bandwidth allocation per beam. This approach is shown to provide significant advantages in highly asymmetric demand scenarios, with improvements up to 10% and 30% in terms of the offered and minimum user rates, respectively. The other options are incorrect because they either focus on secondary benefits (A and C) or do not accurately capture the primary advantage of the smart beam-user mapping approach (D)."}, "14": {"documentation": {"title": "Gravitational wave production: A strong constraint on primordial\n  magnetic fields", "source": "Chiara Caprini and Ruth Durrer", "docs_id": "astro-ph/0106244", "section": ["astro-ph", "gr-qc", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gravitational wave production: A strong constraint on primordial\n  magnetic fields. We compute the gravity waves induced by anisotropic stresses of stochastic primordial magnetic fields. The nucleosynthesis bound on gravity waves is then used to derive a limit on the magnetic field amplitude as function of the spectral index. The obtained limits are extraordinarily strong: If the primordial magnetic field is produced by a causal process, leading to a spectral index $n\\ge 2$ on super horizon scales, galactic magnetic fields produced at the electroweak phase transition or earlier have to be weaker than $B_\\la \\le 10^{-27}$Gauss! If they are induced during an inflationary phase (reheating temperature $T\\sim 10^{15}$GeV) with a spectral index $n\\sim 0$, the magnetic field has to be weaker than $B_\\la \\le 10^{-39}$Gauss! Only very red magnetic field spectra, $n\\sim -3$ are not strongly constrained. We also find that a considerable amount of the magnetic field energy is converted into gravity waves. The gravity wave limit derived in this work rules out most of the proposed processes for primordial seeds for the large scale coherent magnetic fields observed in galaxies and clusters."}, "answer": "Here is the difficult multi-form question:\n\nWhat is the most stringent constraint on the amplitude of primordial magnetic fields, and how does it relate to the spectral index of the magnetic field?\n\nA) The constraint is that the magnetic field amplitude must be less than or equal to 10^-20 Gauss, and this is only possible if the spectral index is greater than or equal to 2.\n\nB) The constraint is that the magnetic field amplitude must be less than or equal to 10^-27 Gauss, and this is only possible if the spectral index is greater than or equal to 2, or if the magnetic field is induced during an inflationary phase with a spectral index of approximately 0.\n\nC) The constraint is that the magnetic field amplitude must be less than or equal to 10^-30 Gauss, and this is only possible if the spectral index is less than or equal to 2.\n\nD) The constraint is that the magnetic field amplitude must be less than or equal to 10^-39 Gauss, and this is only possible if the spectral index is approximately -3.\n\nCorrect Answer: B) The constraint is that the magnetic field amplitude must be less than or equal to 10^-27 Gauss, and this is only possible if the spectral index is greater than or equal to 2, or if the magnetic field is induced during an inflationary phase with a spectral index of approximately 0.\n\nExplanation: The correct answer is B) because the text states that if the primordial magnetic field is produced by a causal process, leading to a spectral index n \u2265 2 on super horizon scales, galactic magnetic fields produced at the electroweak phase transition or earlier have to be weaker than B_\u2113 \u2264 10^-27 Gauss. Additionally, if the magnetic field is induced during an inflationary phase with a spectral index n \u2248 0, the magnetic field has to be weaker than B_\u2113 \u2264 10^-39 Gauss. Therefore, the correct constraint is that the magnetic field amplitude must be less than or equal to 10^-27 Gauss, and this is only possible if the spectral index is greater than or equal to 2, or if the magnetic field is induced during an inflationary phase with a spectral index of approximately 0."}, "15": {"documentation": {"title": "Is Information in the Brain Represented in Continuous or Discrete Form?", "source": "James Tee and Desmond P. Taylor", "docs_id": "1805.01631", "section": ["q-bio.NC", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Is Information in the Brain Represented in Continuous or Discrete Form?. The question of continuous-versus-discrete information representation in the brain is a fundamental yet unresolved question. Historically, most analyses assume a continuous representation without considering the discrete alternative. Our work explores the plausibility of both, answering the question from a communications systems engineering perspective. Using Shannon's communications theory, we posit that information in the brain is represented in discrete form. We address this hypothesis using 2 approaches. First, we identify the fundamental communication requirements of the brain. Second, we estimate the symbol error probability and channel capacity for a continuous information representation. Our work concludes that information cannot be communicated and represented reliably in the brain using a continuous representation - it has to be in a discrete form. This is a major demarcation from conventional and current wisdom. We apply this discrete result to the 4 major neural coding hypotheses, and illustrate the use of discrete ISI neural coding in analyzing electrophysiology experimental data. We further posit and illustrate a plausible direct link between Weber's Law and discrete neural coding. We end by outlining a number of key research questions on discrete neural coding."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** According to the study \"Is Information in the Brain Represented in Continuous or Discrete Form?\", what is the primary conclusion reached by the authors regarding the representation of information in the brain?\n\nA) Information in the brain is best represented in a continuous form, as it allows for more efficient transmission and processing.\nB) The authors found that information in the brain is represented in a discrete form, which is more reliable and efficient than a continuous representation.\nC) The study suggests that the brain's neural coding mechanisms are too complex to be accurately modeled using either continuous or discrete representations.\nD) The authors propose that information in the brain is represented in a hybrid form, combining elements of both continuous and discrete representations.\n\n**Correct Answer:** B) The authors found that information in the brain is represented in a discrete form, which is more reliable and efficient than a continuous representation.\n\n**Explanation:** The study uses Shannon's communications theory to explore the possibility of both continuous and discrete information representation in the brain. The authors conclude that information cannot be communicated and represented reliably in the brain using a continuous representation, and instead propose that it is represented in a discrete form. This conclusion is based on the identification of fundamental communication requirements of the brain and the estimation of symbol error probability and channel capacity for a continuous information representation."}, "16": {"documentation": {"title": "Diagonalizing operators over continuous fields of C*-algebras", "source": "V.M.Manuilov", "docs_id": "funct-an/9605001", "section": ["math.FA", "math.OA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Diagonalizing operators over continuous fields of C*-algebras. It is well known that in the commutative case, i.e. for $A=C(X)$ being a commutative C*-algebra, compact selfadjoint operators acting on the Hilbert C*-module $H_A$ (= continuous families of such operators $K(x)$, $x\\in X$) can be diagonalized if we pass to a bigger W*-algebra $L^\\infty(X)={\\bf A} \\supset A$ which can be obtained from $A$ by completing it with respect to the weak topology. Unlike the \"eigenvectors\", which have coordinates from $\\bf A$, the \"eigenvalues\" are continuous, i.e. lie in the C*-algebra $A$. We discuss here the non-commutative analog of this well-known fact. Here the \"eigenvalues\" are defined not uniquely but in some cases they can also be taken from the initial C*-algebra instead of the bigger W*-algebra. We prove here that such is the case for some continuous fields of real rank zero C*-algebras over a one-dimensional manifold and give an example of a C*-algebra $A$ for which the \"eigenvalues\" cannot be chosen from $A$, i.e. are discontinuous. The main point of the proof is connected with a problem on almost commuting operators. We prove that for some C*-algebras if $h\\in A$ is a selfadjoint, $u\\in A$ is a unitary and if the norm of their commutant $[u,h]$ is small enough then one can connect $u$ with the unity by a path $u(t)$ so that the norm of $[u(t),h]$ would be also small along this path."}, "answer": "Here's a possible exam question based on the provided documentation:\n\nQuestion: Let $A$ be a C*-algebra and $h \\in A$ be a selfadjoint element. Suppose that $u \\in A$ is a unitary element such that the norm of their commutant $[u, h]$ is small enough. What can be concluded about the existence of a path $u(t)$ in $A$ such that $u(t)$ connects $u$ to the unity and the norm of $[u(t), h]$ is small along this path?\n\nA) The path $u(t)$ must be a continuous function from $[0,1]$ to $A$.\nB) The path $u(t)$ must be a continuous function from $[0,1]$ to the W*-algebra $L^\\infty(X)$.\nC) The path $u(t)$ must be a continuous function from $[0,1]$ to the C*-algebra $A$.\nD) The path $u(t)$ must be a continuous function from $[0,1]$ to the C*-algebra $A$ with the weak topology.\n\nCorrect Answer: C) The path $u(t)$ must be a continuous function from $[0,1]$ to the C*-algebra $A$.\n\nExplanation: The problem statement mentions that the norm of $[u, h]$ is small enough, which implies that $u$ and $h$ are almost commuting. This allows us to conclude that there exists a path $u(t)$ in $A$ that connects $u$ to the unity and has small norm of $[u(t), h]$ along this path. The fact that $A$ is a C*-algebra and $u$ is a unitary element ensures that the path $u(t)$ is indeed a continuous function from $[0,1]$ to $A$."}, "17": {"documentation": {"title": "Learning low-frequency temporal patterns for quantitative trading", "source": "Joel da Costa, Tim Gebbie", "docs_id": "2008.09481", "section": ["q-fin.ST", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning low-frequency temporal patterns for quantitative trading. We consider the viability of a modularised mechanistic online machine learning framework to learn signals in low-frequency financial time series data. The framework is proved on daily sampled closing time-series data from JSE equity markets. The input patterns are vectors of pre-processed sequences of daily, weekly and monthly or quarterly sampled feature changes. The data processing is split into a batch processed step where features are learnt using a stacked autoencoder via unsupervised learning, and then both batch and online supervised learning are carried out using these learnt features, with the output being a point prediction of measured time-series feature fluctuations. Weight initializations are implemented with restricted Boltzmann machine pre-training, and variance based initializations. Historical simulations are then run using an online feedforward neural network initialised with the weights from the batch training and validation step. The validity of results are considered under a rigorous assessment of backtest overfitting using both combinatorially symmetrical cross validation and probabilistic and deflated Sharpe ratios. Results are used to develop a view on the phenomenology of financial markets and the value of complex historical data-analysis for trading under the unstable adaptive dynamics that characterise financial markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary objective of the proposed modularised mechanistic online machine learning framework in the paper \"Learning low-frequency temporal patterns for quantitative trading\"?\n\nA) To predict short-term price movements in financial markets using supervised learning techniques.\nB) To identify and learn low-frequency temporal patterns in financial time series data using a stacked autoencoder and online supervised learning.\nC) To develop a framework for backtesting and evaluating the performance of trading strategies using combinatorially symmetrical cross validation and probabilistic and deflated Sharpe ratios.\nD) To apply restricted Boltzmann machine pre-training and variance-based initializations to the weights of an online feedforward neural network.\n\n**Correct Answer:** B) To identify and learn low-frequency temporal patterns in financial time series data using a stacked autoencoder and online supervised learning.\n\n**Explanation:** The correct answer is B) because the paper specifically states that the framework is designed to learn signals in low-frequency financial time series data, and that it uses a stacked autoencoder via unsupervised learning to learn features from pre-processed sequences of daily, weekly, and monthly or quarterly sampled feature changes. The framework then uses online supervised learning to make point predictions of measured time-series feature fluctuations. The other options are incorrect because they either focus on short-term price movements (A), backtesting and evaluation (C), or weight initialization techniques (D), which are not the primary objective of the framework."}, "18": {"documentation": {"title": "Agreement dynamics on small-world networks", "source": "Luca Dall'Asta (LPT), Andrea Baronchelli, Alain Barrat (LPT), Vittorio\n  Loreto", "docs_id": "cond-mat/0603205", "section": ["cond-mat.stat-mech", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Agreement dynamics on small-world networks. In this paper we analyze the effect of a non-trivial topology on the dynamics of the so-called Naming Game, a recently introduced model which addresses the issue of how shared conventions emerge spontaneously in a population of agents. We consider in particular the small-world topology and study the convergence towards the global agreement as a function of the population size $N$ as well as of the parameter $p$ which sets the rate of rewiring leading to the small-world network. As long as $p \\gg 1/N$ there exists a crossover time scaling as $N/p^2$ which separates an early one-dimensional-like dynamics from a late stage mean-field-like behavior. At the beginning of the process, the local quasi one-dimensional topology induces a coarsening dynamics which allows for a minimization of the cognitive effort (memory) required to the agents. In the late stages, on the other hand, the mean-field like topology leads to a speed up of the convergence process with respect to the one-dimensional case."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the effect of the small-world topology on the dynamics of the Naming Game in the context of the paper \"Agreement dynamics on small-world networks\". How does the crossover time scaling as a function of the population size $N$ and the parameter $p$ relate to the emergence of shared conventions in the population of agents?\n\n**A)** The crossover time scaling is directly proportional to the population size $N$, indicating that the emergence of shared conventions is independent of the network topology.\n\n**B)** The crossover time scaling is inversely proportional to the parameter $p$, suggesting that the emergence of shared conventions is accelerated as the rewiring rate increases.\n\n**C)** The crossover time scaling is quadratic in the population size $N$ and inversely proportional to the parameter $p$, indicating that the emergence of shared conventions is influenced by both the network topology and the rewiring rate.\n\n**D)** The crossover time scaling is constant with respect to both the population size $N$ and the parameter $p$, implying that the emergence of shared conventions is unaffected by the network topology and rewiring rate.\n\n**Correct Answer:** C) The crossover time scaling is quadratic in the population size $N$ and inversely proportional to the parameter $p$, indicating that the emergence of shared conventions is influenced by both the network topology and the rewiring rate.\n\n**Explanation:** The correct answer is based on the analysis of the paper, which states that the crossover time scaling as a function of the population size $N$ and the parameter $p$ is given by $N/p^2$. This indicates that the emergence of shared conventions is influenced by both the network topology (small-world network) and the rewiring rate (parameter $p$). The correct answer, option C, reflects this relationship. The other options are incorrect because they do not accurately represent the relationship between the crossover time scaling and the network topology and rewiring rate."}, "19": {"documentation": {"title": "Uniqueness and weak-BV stability for $2\\times 2$ conservation laws", "source": "Geng Chen (University of Kansas), Sam G. Krupa (Max Planck Institute\n  for Mathematics in the Sciences in Leipzig), Alexis F. Vasseur (The\n  University of Texas at Austin)", "docs_id": "2010.04761", "section": ["math.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Uniqueness and weak-BV stability for $2\\times 2$ conservation laws. Let a 1-d system of hyperbolic conservation laws, with two unknowns, be endowed with a convex entropy. We consider the family of small $BV$ functions which are global solutions of this equation. For any small $BV$ initial data, such global solutions are known to exist. Moreover, they are known to be unique among $BV$ solutions verifying either the so-called Tame Oscillation Condition, or the Bounded Variation Condition on space-like curves. In this paper, we show that these solutions are stable in a larger class of weak (and possibly not even $BV$) solutions of the system. This result extends the classical weak-strong uniqueness results which allow comparison to a smooth solution. Indeed our result extends these results to a weak-$BV$ uniqueness result, where only one of the solutions is supposed to be small $BV$, and the other solution can come from a large class. As a consequence of our result, the Tame Oscillation Condition, and the Bounded Variation Condition on space-like curves are not necessary for the uniqueness of solutions in the $BV$ theory, in the case of systems with 2 unknowns. The method is $L^2$ based. It builds up from the theory of a-contraction with shifts, where suitable weight functions $a$ are generated via the front tracking method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a 1-d system of hyperbolic conservation laws with two unknowns, endowed with a convex entropy. Suppose we have a small $BV$ initial data and a global solution of the equation that is unique among $BV$ solutions verifying either the Tame Oscillation Condition or the Bounded Variation Condition on space-like curves. What can be concluded about the stability of this global solution in the following scenarios?\n\nA) The global solution is stable under perturbations of the initial data, but the perturbations are small $BV$ functions.\n\nB) The global solution is stable under perturbations of the initial data, but the perturbations are not necessarily small $BV$ functions.\n\nC) The global solution is stable under perturbations of the initial data, and the perturbations are small $BV$ functions, but the global solution is not unique among $BV$ solutions.\n\nD) The global solution is not stable under perturbations of the initial data, regardless of the nature of the perturbations.\n\nCorrect Answer: B) The global solution is stable under perturbations of the initial data, but the perturbations are not necessarily small $BV$ functions.\n\nExplanation: The paper shows that the global solutions of the system are stable in a larger class of weak (and possibly not even $BV$) solutions of the system. This means that the global solution is stable under perturbations of the initial data, but the perturbations do not need to be small $BV$ functions. The correct answer is B) because it accurately reflects the result of the paper, which extends the classical weak-strong uniqueness results to a weak-$BV$ uniqueness result."}, "20": {"documentation": {"title": "Lattice QCD Evidence that the Lambda(1405) Resonance is an\n  Antikaon-Nucleon Molecule", "source": "Jonathan M. M. Hall, Waseem Kamleh, Derek B. Leinweber, Benjamin J.\n  Menadue, Benjamin J. Owen, Anthony W. Thomas, Ross D. Young", "docs_id": "1411.3402", "section": ["hep-lat", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lattice QCD Evidence that the Lambda(1405) Resonance is an\n  Antikaon-Nucleon Molecule. For almost 50 years the structure of the Lambda(1405) resonance has been a mystery. Even though it contains a heavy strange quark and has odd parity, its mass is lower than any other excited spin-1/2 baryon. Dalitz and co-workers speculated that it might be a molecular state of an antikaon bound to a nucleon. However, a standard quark-model structure is also admissible. Although the intervening years have seen considerable effort, there has been no convincing resolution. Here we present a new lattice QCD simulation showing that the strange magnetic form factor of the Lambda(1405) vanishes, signaling the formation of an antikaon-nucleon molecule. Together with a Hamiltonian effective-field-theory model analysis of the lattice QCD energy levels, this strongly suggests that the structure is dominated by a bound antikaon-nucleon component. This result clarifies that not all states occurring in nature can be described within a simple quark model framework and points to the existence of exotic molecular meson-nucleon bound states."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion drawn from the lattice QCD simulation and Hamiltonian effective-field-theory model analysis regarding the structure of the Lambda(1405) resonance?\n\nA) The Lambda(1405) resonance is a standard quark-model structure with no exotic components.\nB) The Lambda(1405) resonance is a bound antikaon-nucleon molecule with a vanishing strange magnetic form factor.\nC) The Lambda(1405) resonance is a simple quark-model state with no molecular components.\nD) The Lambda(1405) resonance is a meson-nucleon bound state with a non-zero strange magnetic form factor.\n\nCorrect Answer: B) The Lambda(1405) resonance is a bound antikaon-nucleon molecule with a vanishing strange magnetic form factor.\n\nExplanation: The correct answer is B) because the lattice QCD simulation shows that the strange magnetic form factor of the Lambda(1405) vanishes, indicating the formation of an antikaon-nucleon molecule. This result is further supported by the Hamiltonian effective-field-theory model analysis, which suggests that the structure of the Lambda(1405) is dominated by a bound antikaon-nucleon component."}, "21": {"documentation": {"title": "Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data", "source": "David Hallac, Suvrat Bhooshan, Michael Chen, Kacem Abida, Rok Sosic,\n  Jure Leskovec", "docs_id": "1806.04795", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data. With automobiles becoming increasingly reliant on sensors to perform various driving tasks, it is important to encode the relevant CAN bus sensor data in a way that captures the general state of the vehicle in a compact form. In this paper, we develop a deep learning-based method, called Drive2Vec, for embedding such sensor data in a low-dimensional yet actionable form. Our method is based on stacked gated recurrent units (GRUs). It accepts a short interval of automobile sensor data as input and computes a low-dimensional representation of that data, which can then be used to accurately solve a range of tasks. With this representation, we (1) predict the exact values of the sensors in the short term (up to three seconds in the future), (2) forecast the long-term average values of these same sensors, (3) infer additional contextual information that is not encoded in the data, including the identity of the driver behind the wheel, and (4) build a knowledge base that can be used to auto-label data and identify risky states. We evaluate our approach on a dataset collected by Audi, which equipped a fleet of test vehicles with data loggers to store all sensor readings on 2,098 hours of driving on real roads. We show in several experiments that our method outperforms other baselines by up to 90%, and we further demonstrate how these embeddings of sensor data can be used to solve a variety of real-world automotive applications."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the Drive2Vec method, and how does it achieve this goal?\n\nA) To predict the exact values of sensors in the short term by learning a high-dimensional representation of the sensor data.\nB) To forecast the long-term average values of sensors by using a stacked gated recurrent unit (GRU) architecture.\nC) To infer additional contextual information about the vehicle and its occupants by encoding the sensor data in a low-dimensional yet actionable form.\nD) To build a knowledge base that can be used to auto-label data and identify risky states by predicting sensor values in the short term.\n\n**Correct Answer:** C) To infer additional contextual information about the vehicle and its occupants by encoding the sensor data in a low-dimensional yet actionable form.\n\n**Explanation:** The Drive2Vec method is designed to encode sensor data in a compact form that captures the general state of the vehicle. This allows for the inference of additional contextual information that is not encoded in the data, such as the identity of the driver behind the wheel. The method achieves this goal by using a stacked GRU architecture to learn a low-dimensional representation of the sensor data. This representation can then be used to solve a range of tasks, including predicting sensor values in the short term, forecasting long-term average values, and identifying risky states."}, "22": {"documentation": {"title": "Factorized Neural Transducer for Efficient Language Model Adaptation", "source": "Xie Chen, Zhong Meng, Sarangarajan Parthasarathy, Jinyu Li", "docs_id": "2110.01500", "section": ["cs.CL", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Factorized Neural Transducer for Efficient Language Model Adaptation. In recent years, end-to-end (E2E) based automatic speech recognition (ASR) systems have achieved great success due to their simplicity and promising performance. Neural Transducer based models are increasingly popular in streaming E2E based ASR systems and have been reported to outperform the traditional hybrid system in some scenarios. However, the joint optimization of acoustic model, lexicon and language model in neural Transducer also brings about challenges to utilize pure text for language model adaptation. This drawback might prevent their potential applications in practice. In order to address this issue, in this paper, we propose a novel model, factorized neural Transducer, by factorizing the blank and vocabulary prediction, and adopting a standalone language model for the vocabulary prediction. It is expected that this factorization can transfer the improvement of the standalone language model to the Transducer for speech recognition, which allows various language model adaptation techniques to be applied. We demonstrate that the proposed factorized neural Transducer yields 15% to 20% WER improvements when out-of-domain text data is used for language model adaptation, at the cost of a minor degradation in WER on a general test set."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of the factorized neural Transducer model, as described in the paper \"Factorized Neural Transducer for Efficient Language Model Adaptation\"?\n\n**A)** To improve the performance of acoustic models in streaming E2E based ASR systems.\n**B)** To address the challenge of joint optimization of acoustic model, lexicon, and language model in neural Transducer models.\n**C)** To enable the application of various language model adaptation techniques to neural Transducer models.\n**D)** To reduce the computational complexity of language model adaptation.\n\n**Correct Answer:** C) To enable the application of various language model adaptation techniques to neural Transducer models.\n\n**Explanation:** The correct answer is C) because the paper proposes a novel model, factorized neural Transducer, to address the issue of joint optimization of acoustic model, lexicon, and language model in neural Transducer models. The factorization of blank and vocabulary prediction, and the adoption of a standalone language model for vocabulary prediction, enables the application of various language model adaptation techniques to neural Transducer models. This is the primary motivation behind the development of the factorized neural Transducer model."}, "23": {"documentation": {"title": "Impact parameter dependence of pion ratio in probing the nuclear\n  symmetry energy using heavy-ion collisions", "source": "Gao-Feng Wei, Guo-Qiang He, Xin-Wei Cao, and Yi-Xin Lu", "docs_id": "1601.04246", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Impact parameter dependence of pion ratio in probing the nuclear\n  symmetry energy using heavy-ion collisions. The impact parameter dependence of \\rpi ratio is examined in heavy-ion collisions at 400MeV/nucleon within a transport model. It is shown that the sensitivity of \\rpi ratio on symmetry energy shows a transition from central to peripheral collisions, i.e., the stiffer symmetry energy leads to a larger \\rpi ratio in peripheral collisions while the softer symmetry energy always leads this ratio to be larger in central collisions. After checking the kinematic energy distribution of \\rpi ratio, we found this transition of sensitivity of \\rpi ratio to symmetry energy is mainly from less energetic pions, i.e., the softer symmetry energy gets the less energetic pions to form a smaller \\rpi ratio in peripheral collisions while these pions generate a larger \\rpi ratio in central collisions. Undoubtedly, the softer symmetry energy can also lead more energetic pions to form a larger \\rpi ratio in peripheral collisions. Nevertheless, considering that most of pions are insufficient energetic at this beam energy, we therefore suggest the \\rpi ratio as a probe of the high-density symmetry energy effective only in central at most to midcentral collisions, thereby avoiding the possible information of low-density symmetry energy carried in \\rpi ratio from peripheral collisions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary conclusion regarding the use of the pion ratio as a probe of the nuclear symmetry energy in heavy-ion collisions, and what are the implications of this conclusion for the interpretation of results in peripheral collisions?\n\nA) The pion ratio is a reliable probe of the nuclear symmetry energy only in peripheral collisions, where softer symmetry energies lead to larger pion ratios. This is because peripheral collisions produce more energetic pions, which are more sensitive to symmetry energy changes.\n\nB) The pion ratio is a reliable probe of the nuclear symmetry energy only in central collisions, where stiffer symmetry energies lead to larger pion ratios. This is because central collisions produce fewer energetic pions, which are less sensitive to symmetry energy changes.\n\nC) The pion ratio is a reliable probe of the nuclear symmetry energy in both central and peripheral collisions, but the sensitivity to symmetry energy changes depends on the energy of the pions produced. Softer symmetry energies lead to larger pion ratios in peripheral collisions, while stiffer symmetry energies lead to larger pion ratios in central collisions.\n\nD) The pion ratio is not a reliable probe of the nuclear symmetry energy in heavy-ion collisions, as the results are sensitive to the energy of the pions produced and the collision geometry. Therefore, the pion ratio should be used with caution in interpreting results.\n\n**Correct Answer:** C) The pion ratio is a reliable probe of the nuclear symmetry energy in both central and peripheral collisions, but the sensitivity to symmetry energy changes depends on the energy of the pions produced. Softer symmetry energies lead to larger pion ratios in peripheral collisions, while stiffer symmetry energies lead to larger pion ratios in central collisions.\n\n**Explanation:** The correct answer is C) because the documentation states that the transition of sensitivity of the pion ratio to symmetry energy is mainly from less energetic pions, and that softer symmetry energies lead to a larger pion ratio in peripheral collisions while stiffer symmetry energies always lead to a larger pion ratio in central collisions. This suggests that the pion ratio is a reliable probe of the nuclear symmetry energy in both central and peripheral collisions, but the sensitivity to symmetry energy changes depends on the energy of the pions produced."}, "24": {"documentation": {"title": "Dynamic Interference Mitigation for Generalized Partially Connected\n  Quasi-static MIMO Interference Channel", "source": "Liangzhong Ruan, Vincent K.N. Lau", "docs_id": "1105.0286", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Interference Mitigation for Generalized Partially Connected\n  Quasi-static MIMO Interference Channel. Recent works on MIMO interference channels have shown that interference alignment can significantly increase the achievable degrees of freedom (DoF) of the network. However, most of these works have assumed a fully connected interference graph. In this paper, we investigate how the partial connectivity can be exploited to enhance system performance in MIMO interference networks. We propose a novel interference mitigation scheme which introduces constraints for the signal subspaces of the precoders and decorrelators to mitigate \"many\" interference nulling constraints at a cost of \"little\" freedoms in precoder and decorrelator design so as to extend the feasibility region of the interference alignment scheme. Our analysis shows that the proposed algorithm can significantly increase system DoF in symmetric partially connected MIMO interference networks. We also compare the performance of the proposed scheme with various baselines and show via simulations that the proposed algorithms could achieve significant gain in the system performance of randomly connected interference networks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Dynamic Interference Mitigation for Generalized Partially Connected Quasi-static MIMO Interference Channel\", what is the primary goal of the proposed interference mitigation scheme, and how does it balance the trade-off between interference nulling constraints and precoder/decorrelator design freedoms?\n\n**A)** The primary goal is to minimize the number of interference nulling constraints, while maximizing the design freedoms of the precoders and decorrelators. This is achieved by introducing constraints on the signal subspaces of the precoders and decorrelators.\n\n**B)** The primary goal is to maximize the number of interference nulling constraints, while minimizing the design freedoms of the precoders and decorrelators. This is achieved by introducing constraints on the signal subspaces of the precoders and decorrelators.\n\n**C)** The primary goal is to maintain the same number of interference nulling constraints as in the fully connected interference graph, while adjusting the design freedoms of the precoders and decorrelators to accommodate the partial connectivity.\n\n**D)** The primary goal is to minimize the design freedoms of the precoders and decorrelators, while maximizing the number of interference nulling constraints. This is achieved by introducing constraints on the signal subspaces of the precoders and decorrelators.\n\n**Correct Answer:** A) The primary goal is to minimize the number of interference nulling constraints, while maximizing the design freedoms of the precoders and decorrelators. This is achieved by introducing constraints on the signal subspaces of the precoders and decorrelators.\n\n**Explanation:** The correct answer is A) because the paper states that the proposed algorithm introduces constraints for the signal subspaces of the precoders and decorrelators to mitigate \"many\" interference nulling constraints at a cost of \"little\" freedoms in precoder and decorrelator design. This indicates that the primary goal is to minimize the number of interference nulling constraints, while maximizing the design freedoms of the precoders and decorrelators."}, "25": {"documentation": {"title": "Group Testing: An Information Theory Perspective", "source": "Matthew Aldridge, Oliver Johnson, Jonathan Scarlett", "docs_id": "1902.06002", "section": ["cs.IT", "cs.DM", "math.IT", "math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Group Testing: An Information Theory Perspective. The group testing problem concerns discovering a small number of defective items within a large population by performing tests on pools of items. A test is positive if the pool contains at least one defective, and negative if it contains no defectives. This is a sparse inference problem with a combinatorial flavour, with applications in medical testing, biology, telecommunications, information technology, data science, and more. In this monograph, we survey recent developments in the group testing problem from an information-theoretic perspective. We cover several related developments: efficient algorithms with practical storage and computation requirements, achievability bounds for optimal decoding methods, and algorithm-independent converse bounds. We assess the theoretical guarantees not only in terms of scaling laws, but also in terms of the constant factors, leading to the notion of the {\\em rate} of group testing, indicating the amount of information learned per test. Considering both noiseless and noisy settings, we identify several regimes where existing algorithms are provably optimal or near-optimal, as well as regimes where there remains greater potential for improvement. In addition, we survey results concerning a number of variations on the standard group testing problem, including partial recovery criteria, adaptive algorithms with a limited number of stages, constrained test designs, and sublinear-time algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of group testing, what is the notion of \"rate\" of group testing, and how does it relate to the amount of information learned per test?\n\n**A)** The rate of group testing refers to the minimum number of tests required to detect a single defective item, and is a measure of the algorithm's efficiency.\n\n**B)** The rate of group testing is a measure of the amount of information learned per test, indicating the trade-off between the number of tests and the accuracy of the results, and is a key concept in understanding the theoretical guarantees of group testing algorithms.\n\n**C)** The rate of group testing is a measure of the algorithm's computational complexity, and is used to compare the performance of different algorithms in terms of their processing power requirements.\n\n**D)** The rate of group testing is a measure of the algorithm's storage requirements, and is used to compare the performance of different algorithms in terms of their memory usage.\n\n**Correct Answer:** B) The rate of group testing is a measure of the amount of information learned per test, indicating the trade-off between the number of tests and the accuracy of the results, and is a key concept in understanding the theoretical guarantees of group testing algorithms.\n\n**Explanation:** The rate of group testing is a concept introduced in the provided documentation to describe the amount of information learned per test. It is a measure of the trade-off between the number of tests and the accuracy of the results, and is a key concept in understanding the theoretical guarantees of group testing algorithms. The correct answer, B, accurately reflects this concept. The other options, A, C, and D, are incorrect because they do not accurately describe the rate of group testing."}, "26": {"documentation": {"title": "On a Fejer-Riesz factorization of generalized trigonometric polynomials", "source": "Tryphon T. Georgiou and Anders Lindquist", "docs_id": "2005.11920", "section": ["math.OC", "cs.SY", "eess.SY", "math.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On a Fejer-Riesz factorization of generalized trigonometric polynomials. Function theory on the unit disc proved key to a range of problems in statistics, probability theory, signal processing literature, and applications, and in this, a special place is occupied by trigonometric functions and the Fejer-Riesz theorem that non-negative trigonometric polynomials can be expressed as the modulus of a polynomial of the same degree evaluated on the unit circle. In the present note we consider a natural generalization of non-negative trigonometric polynomials that are matrix-valued with specified non-trivial poles (i.e., other than at the origin or at infinity). We are interested in the corresponding spectral factors and, specifically, we show that the factorization of trigonometric polynomials can be carried out in complete analogy with the Fejer-Riesz theorem. The affinity of the factorization with the Fejer-Riesz theorem and the contrast to classical spectral factorization lies in the fact that the spectral factors have degree smaller than what standard construction in factorization theory would suggest. We provide two juxtaposed proofs of this fundamental theorem, albeit for the case of strict positivity, one that relies on analytic interpolation theory and another that utilizes classical factorization theory based on the Yacubovich-Popov-Kalman (YPK) positive-real lemma."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a matrix-valued trigonometric polynomial with specified non-trivial poles, and suppose that its spectral factors have degree smaller than what standard construction in factorization theory would suggest. What is the key difference between the factorization of this matrix-valued trigonometric polynomial and the classical spectral factorization, and how do the two proofs provided in the note differ from each other?\n\nA) The factorization is based on the Yacubovich-Popov-Kalman (YPK) positive-real lemma, while the classical spectral factorization is based on the Fejer-Riesz theorem.\nB) The factorization is based on classical factorization theory, while the classical spectral factorization is based on the YPK positive-real lemma.\nC) The factorization is based on analytic interpolation theory, while the classical spectral factorization is based on the Fejer-Riesz theorem.\nD) The factorization is based on the Fejer-Riesz theorem, while the classical spectral factorization is based on the YPK positive-real lemma.\n\nCorrect Answer: C) The factorization is based on analytic interpolation theory, while the classical spectral factorization is based on the Fejer-Riesz theorem.\n\nExplanation: The note provides two proofs of the factorization of matrix-valued trigonometric polynomials, one based on analytic interpolation theory and the other based on classical factorization theory using the YPK positive-real lemma. The key difference between the two proofs is that the first one relies on analytic interpolation theory, while the second one relies on classical factorization theory. This difference is highlighted in the note as the contrast between the factorization and classical spectral factorization."}, "27": {"documentation": {"title": "Six New Planets from the Keck Precision Velocity Survey", "source": "Steven S. Vogt (UCO/Lick Observatory, UCSC) Geoffrey W. Marcy\n  (University of California, Berkeley) R. Paul Butler (Department of\n  Terrestrial Magnetism, CIW) Kevin Apps (University of Sussex)", "docs_id": "astro-ph/9911506", "section": ["astro-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Six New Planets from the Keck Precision Velocity Survey. We report results of a search for planets around 500 main sequence stars using the Keck high resolution spectrometer which has provided Doppler precision of 3 m/s during the past 3 yr. We report 6 new strong planet candidates having complete Keplerian orbits, with periods ranging from 24 d to 3 yr. We also provide updated orbital parameters for 4 previously announced planets. Four of the six newly discovered planets have minimum Msini masses less than 2 Mjup, while the remaining two have Msini $\\sim$ 5 Mjup. The distribution of planetary masses continues to exhibit a rise toward lower masses. The orbital eccentricities of the new planets range from 0.12 to 0.71 which also continues the ubiquity of high eccentricities. All 18 known extrasolar planets orbiting beyond 0.2 AU have eccentricities greater than $\\sim$0.1. The current limiting Doppler precision of the Keck Doppler survey is 3 m/s per observation as determined from observations of both stable stars and residuals to Keplerian fits."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the significance of the distribution of planetary masses in the newly discovered exoplanets, and how does it relate to the distribution of orbital eccentricities?\n\nA) The distribution of planetary masses is consistent with the distribution of orbital eccentricities, with both showing a rise toward lower masses.\n\nB) The distribution of planetary masses is consistent with the distribution of orbital eccentricities, with both showing a rise toward higher masses.\n\nC) The distribution of planetary masses is inconsistent with the distribution of orbital eccentricities, with the former showing a rise toward lower masses and the latter showing a rise toward higher masses.\n\nD) The distribution of planetary masses is inconsistent with the distribution of orbital eccentricities, with the former showing a rise toward higher masses and the latter showing a rise toward lower masses.\n\nCorrect Answer: C) The distribution of planetary masses is inconsistent with the distribution of orbital eccentricities, with the former showing a rise toward lower masses and the latter showing a rise toward higher masses.\n\nExplanation: The text states that \"The distribution of planetary masses continues to exhibit a rise toward lower masses\" and also mentions that \"All 18 known extrasolar planets orbiting beyond 0.2 AU have eccentricities greater than $\\sim$0.1\". This suggests that the distribution of planetary masses is rising toward lower masses, while the distribution of orbital eccentricities is rising toward higher masses. Therefore, the correct answer is C) The distribution of planetary masses is inconsistent with the distribution of orbital eccentricities, with the former showing a rise toward lower masses and the latter showing a rise toward higher masses."}, "28": {"documentation": {"title": "Erasure/list exponents for Slepian-Wolf decoding", "source": "Neri Merhav", "docs_id": "1305.5626", "section": ["cs.IT", "cond-mat.stat-mech", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Erasure/list exponents for Slepian-Wolf decoding. We analyze random coding error exponents associated with erasure/list Slepian-Wolf decoding using two different methods and then compare the resulting bounds. The first method follows the well known techniques of Gallager and Forney and the second method is based on a technique of distance enumeration, or more generally, type class enumeration, which is rooted in the statistical mechanics of a disordered system that is related to the random energy model (REM). The second method is guaranteed to yield exponent functions which are at least as tight as those of the first method, and it is demonstrated that for certain combinations of coding rates and thresholds, the bounds of the second method are strictly tighter than those of the first method, by an arbitrarily large factor. In fact, the second method may even yield an infinite exponent at regions where the first method gives finite values. We also discuss the option of variable-rate Slepian-Wolf encoding and demonstrate how it can improve on the resulting exponents."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the second method of analyzing random coding error exponents associated with erasure/list Slepian-Wolf decoding, as described in the Arxiv documentation?\n\nA) It is based on a well-known technique that has been widely used in the field of coding theory.\nB) It guarantees to yield exponent functions that are at least as tight as those of the first method.\nC) It is limited to only certain combinations of coding rates and thresholds.\nD) It is not applicable to variable-rate Slepian-Wolf encoding.\n\n**Correct Answer:** B) It guarantees to yield exponent functions that are at least as tight as those of the first method.\n\n**Explanation:** The second method of analyzing random coding error exponents is based on a technique of distance enumeration, or more generally, type class enumeration, which is rooted in the statistical mechanics of a disordered system. This method is guaranteed to yield exponent functions that are at least as tight as those of the first method, which follows the well-known techniques of Gallager and Forney. This is a key advantage of the second method, as it provides a more rigorous and reliable approach to analyzing the error exponents of erasure/list Slepian-Wolf decoding.\n\n**Additional Explanation:** The second method may even yield an infinite exponent at regions where the first method gives finite values, which highlights the potential for significant improvements in the resulting exponents. This demonstrates the importance of considering alternative approaches to analyzing coding error exponents, and the potential benefits of using more advanced techniques in the field of coding theory."}, "29": {"documentation": {"title": "End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car", "source": "Wenhui Zhang, Tejas Mahale", "docs_id": "1812.05914", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car. Safety and decline of road traffic accidents remain important issues of autonomous driving. Statistics show that unintended lane departure is a leading cause of worldwide motor vehicle collisions, making lane detection the most promising and challenge task for self-driving. Today, numerous groups are combining deep learning techniques with computer vision problems to solve self-driving problems. In this paper, a Global Convolution Networks (GCN) model is used to address both classification and localization issues for semantic segmentation of lane. We are using color-based segmentation is presented and the usability of the model is evaluated. A residual-based boundary refinement and Adam optimization is also used to achieve state-of-art performance. As normal cars could not afford GPUs on the car, and training session for a particular road could be shared by several cars. We propose a framework to get it work in real world. We build a real time video transfer system to get video from the car, get the model trained in edge server (which is equipped with GPUs), and send the trained model back to the car."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary motivation behind the development of a Global Convolution Networks (GCN) model for semantic segmentation of lanes in autonomous driving, and how does it address the challenges of lane detection?\n\n**A)** The GCN model is designed to improve the accuracy of lane detection in low-light conditions, and its primary motivation is to overcome the limitations of traditional computer vision techniques. However, this is not the primary motivation.\n\n**B)** The GCN model is proposed to address the challenges of lane detection in autonomous driving, specifically to improve the classification and localization of lanes. This is the correct answer.\n\n**C)** The GCN model is designed to reduce the computational requirements of lane detection, and its primary motivation is to enable real-time processing of video feeds from autonomous vehicles. However, this is not the primary motivation.\n\n**D)** The GCN model is proposed to improve the robustness of lane detection to various environmental conditions, such as weather and road surface types. However, this is not the primary motivation.\n\n**Correct Answer:** B) The GCN model is proposed to address the challenges of lane detection in autonomous driving, specifically to improve the classification and localization of lanes.\n\n**Explanation:** The correct answer is B) because the documentation states that the GCN model is used to address both classification and localization issues for semantic segmentation of lanes, making it a crucial component in the development of autonomous driving systems. The other options are incorrect because they either misrepresent the primary motivation behind the GCN model or introduce additional factors that are not mentioned in the documentation."}, "30": {"documentation": {"title": "Segmentation of Lung Tumor from CT Images using Deep Supervision", "source": "Farhanaz Farheen, Md. Salman Shamil, Nabil Ibtehaz, M. Sohel Rahman", "docs_id": "2111.09262", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Segmentation of Lung Tumor from CT Images using Deep Supervision. Lung cancer is a leading cause of death in most countries of the world. Since prompt diagnosis of tumors can allow oncologists to discern their nature, type and the mode of treatment, tumor detection and segmentation from CT Scan images is a crucial field of study worldwide. This paper approaches lung tumor segmentation by applying two-dimensional discrete wavelet transform (DWT) on the LOTUS dataset for more meticulous texture analysis whilst integrating information from neighboring CT slices before feeding them to a Deeply Supervised MultiResUNet model. Variations in learning rates, decay and optimization algorithms while training the network have led to different dice co-efficients, the detailed statistics of which have been included in this paper. We also discuss the challenges in this dataset and how we opted to overcome them. In essence, this study aims to maximize the success rate of predicting tumor regions from two dimensional CT Scan slices by experimenting with a number of adequate networks, resulting in a dice co-efficient of 0.8472."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary approach used in the paper \"Segmentation of Lung Tumor from CT Images using Deep Supervision\" to improve the accuracy of lung tumor segmentation from CT scan images?\n\nA) Applying 3D convolutional neural networks (CNNs) directly on the CT scan images\nB) Using a combination of 2D discrete wavelet transform (DWT) and neighboring CT slice information to enhance texture analysis\nC) Training a single-layer neural network on the LOTUS dataset\nD) Employing a traditional edge detection algorithm to segment the tumors\n\nCorrect Answer: B) Using a combination of 2D discrete wavelet transform (DWT) and neighboring CT slice information to enhance texture analysis\n\nExplanation: The paper uses a combination of 2D discrete wavelet transform (DWT) and neighboring CT slice information to improve the accuracy of lung tumor segmentation. This approach allows for more meticulous texture analysis and integration of information from neighboring CT slices, which is essential for accurate tumor segmentation. The other options are incorrect because they do not accurately describe the primary approach used in the paper."}, "31": {"documentation": {"title": "Multi-view Locality Low-rank Embedding for Dimension Reduction", "source": "Lin Feng, Xiangzhu Meng, Huibing Wang", "docs_id": "1905.08138", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-view Locality Low-rank Embedding for Dimension Reduction. During the last decades, we have witnessed a surge of interests of learning a low-dimensional space with discriminative information from one single view. Even though most of them can achieve satisfactory performance in some certain situations, they fail to fully consider the information from multiple views which are highly relevant but sometimes look different from each other. Besides, correlations between features from multiple views always vary greatly, which challenges multi-view subspace learning. Therefore, how to learn an appropriate subspace which can maintain valuable information from multi-view features is of vital importance but challenging. To tackle this problem, this paper proposes a novel multi-view dimension reduction method named Multi-view Locality Low-rank Embedding for Dimension Reduction (MvL2E). MvL2E makes full use of correlations between multi-view features by adopting low-rank representations. Meanwhile, it aims to maintain the correlations and construct a suitable manifold space to capture the low-dimensional embedding for multi-view features. A centroid based scheme is designed to force multiple views to learn from each other. And an iterative alternating strategy is developed to obtain the optimal solution of MvL2E. The proposed method is evaluated on 5 benchmark datasets. Comprehensive experiments show that our proposed MvL2E can achieve comparable performance with previous approaches proposed in recent literatures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary challenge in learning a low-dimensional space with discriminative information from multiple views, and how does the proposed MvL2E method address this challenge?\n\nA) The primary challenge is that most existing methods fail to consider the correlations between features from multiple views, which are highly relevant but sometimes look different from each other. MvL2E addresses this challenge by adopting low-rank representations that make full use of these correlations.\n\nB) The primary challenge is that most existing methods fail to maintain the correlations between features from multiple views, which are highly relevant but sometimes look different from each other. MvL2E addresses this challenge by using a centroid-based scheme to force multiple views to learn from each other.\n\nC) The primary challenge is that most existing methods fail to construct a suitable manifold space to capture the low-dimensional embedding for multi-view features. MvL2E addresses this challenge by using an iterative alternating strategy to obtain the optimal solution.\n\nD) The primary challenge is that most existing methods fail to consider the correlations between features from multiple views, which are highly relevant but sometimes look different from each other. MvL2E addresses this challenge by using a low-rank representation that captures the correlations between features from multiple views.\n\nCorrect Answer: A) The primary challenge is that most existing methods fail to consider the correlations between features from multiple views, which are highly relevant but sometimes look different from each other. MvL2E addresses this challenge by adopting low-rank representations that make full use of these correlations.\n\nExplanation: The question requires the test-taker to understand the primary challenge in learning a low-dimensional space with discriminative information from multiple views, as mentioned in the documentation. The correct answer, A, highlights the challenge of considering correlations between features from multiple views, which is a key aspect of the MvL2E method. The other options, while related to the topic, do not accurately capture the primary challenge and the way MvL2E addresses it."}, "32": {"documentation": {"title": "Evaluating betting odds and free coupons using desirability", "source": "Nawapon Nakharutai, Camila C. S. Caiado, Matthias C. M. Troffaes", "docs_id": "1901.03645", "section": ["q-fin.MF", "math.CO", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evaluating betting odds and free coupons using desirability. In the UK betting market, bookmakers often offer a free coupon to new customers. These free coupons allow the customer to place extra bets, at lower risk, in combination with the usual betting odds. We are interested in whether a customer can exploit these free coupons in order to make a sure gain, and if so, how the customer can achieve this. To answer this question, we evaluate the odds and free coupons as a set of desirable gambles for the bookmaker. We show that we can use the Choquet integral to check whether this set of desirable gambles incurs sure loss for the bookmaker, and hence, results in a sure gain for the customer. In the latter case, we also show how a customer can determine the combination of bets that make the best possible gain, based on complementary slackness. As an illustration, we look at some actual betting odds in the market and find that, without free coupons, the set of desirable gambles derived from those odds avoids sure loss. However, with free coupons, we identify some combinations of bets that customers could place in order to make a guaranteed gain."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of evaluating betting odds and free coupons, what is the primary goal of using the Choquet integral to assess the set of desirable gambles offered by bookmakers, and how does this relate to the potential for a customer to make a sure gain?\n\n**A)** To determine the expected value of each bet and identify the most profitable option for the customer.\n**B)** To evaluate the risk associated with each bet and identify the combination of bets that minimizes the risk for the customer.\n**C)** To check whether the set of desirable gambles incurs sure loss for the bookmaker, and hence, results in a sure gain for the customer.\n**D)** To identify the optimal combination of bets that maximizes the customer's expected return, while minimizing the bookmaker's risk.\n\n**Correct Answer:** C) To check whether the set of desirable gambles incurs sure loss for the bookmaker, and hence, results in a sure gain for the customer.\n\n**Explanation:** The Choquet integral is used to evaluate the set of desirable gambles offered by bookmakers, and the primary goal is to check whether this set of desirable gambles incurs sure loss for the bookmaker. If it does, then the customer can make a sure gain. This is because the bookmaker is essentially guaranteeing a loss, which means the customer can exploit this to their advantage. The correct answer requires an understanding of the Choquet integral and its application in this context."}, "33": {"documentation": {"title": "Hearing your touch: A new acoustic side channel on smartphones", "source": "Ilia Shumailov, Laurent Simon, Jeff Yan, Ross Anderson", "docs_id": "1903.11137", "section": ["cs.CR", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hearing your touch: A new acoustic side channel on smartphones. We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and \"hear\" the finger's touch, and the wave's distortions are characteristic of the tap's location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim's data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** What is the primary vulnerability exploited by the acoustic side-channel attack described in the paper \"Hearing your touch: A new acoustic side channel on smartphones\"?\n\nA) The device's microphone(s) are unable to distinguish between different touch inputs due to ambient noise.\nB) The device's screen surface is not designed to produce a unique sound wave for each touch input.\nC) The device's microphone(s) can recover the sound wave generated by a user's touch input, which is characteristic of the tap's location on the screen.\nD) The device's operating system is not designed to prevent the recording of audio through the built-in microphone(s).\n\n**Correct Answer:** C) The device's microphone(s) can recover the sound wave generated by a user's touch input, which is characteristic of the tap's location on the screen.\n\n**Explanation:** The correct answer is C) because the paper states that \"the device's microphone(s) can recover this wave and 'hear' the finger's touch, and the wave's distortions are characteristic of the tap's location on the screen.\" This means that the microphone(s) can detect the unique sound wave generated by each touch input, allowing the attacker to infer the user's input. The other options are incorrect because they do not accurately describe the vulnerability exploited by the attack."}, "34": {"documentation": {"title": "Some Limit Properties of Markov Chains Induced by Stochastic Recursive\n  Algorithms", "source": "Abhishek Gupta and Hao Chen and Jianzong Pi and Gaurav Tendolkar", "docs_id": "1904.10778", "section": ["cs.LG", "cs.SY", "math.OC", "math.PR", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Some Limit Properties of Markov Chains Induced by Stochastic Recursive\n  Algorithms. Recursive stochastic algorithms have gained significant attention in the recent past due to data driven applications. Examples include stochastic gradient descent for solving large-scale optimization problems and empirical dynamic programming algorithms for solving Markov decision problems. These recursive stochastic algorithms approximate certain contraction operators and can be viewed within the framework of iterated random operators. Accordingly, we consider iterated random operators over a Polish space that simulate iterated contraction operator over that Polish space. Assume that the iterated random operators are indexed by certain batch sizes such that as batch sizes grow to infinity, each realization of the random operator converges (in some sense) to the contraction operator it is simulating. We show that starting from the same initial condition, the distribution of the random sequence generated by the iterated random operators converges weakly to the trajectory generated by the contraction operator. We further show that under certain conditions, the time average of the random sequence converges to the spatial mean of the invariant distribution. We then apply these results to logistic regression, empirical value iteration, and empirical Q value iteration for finite state finite action MDPs to illustrate the general theory develop here."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a recursive stochastic algorithm that simulates an iterated contraction operator over a Polish space. Suppose the algorithm is indexed by batch sizes that grow to infinity, and each realization of the random operator converges to the contraction operator in some sense. What can be concluded about the distribution of the random sequence generated by the iterated random operators, starting from the same initial condition?\n\n**A)** The distribution of the random sequence converges strongly to the trajectory generated by the contraction operator.\n\n**B)** The distribution of the random sequence converges weakly to the trajectory generated by the contraction operator.\n\n**C)** The distribution of the random sequence remains unchanged under the iterated random operators.\n\n**D)** The distribution of the random sequence diverges to infinity as the batch sizes grow to infinity.\n\n**Correct Answer:** B) The distribution of the random sequence converges weakly to the trajectory generated by the contraction operator.\n\n**Explanation:** The correct answer is B) because the documentation states that the distribution of the random sequence generated by the iterated random operators converges weakly to the trajectory generated by the contraction operator, starting from the same initial condition. This is a consequence of the fact that the iterated random operators are indexed by batch sizes that grow to infinity, and each realization of the random operator converges to the contraction operator in some sense."}, "35": {"documentation": {"title": "Geometry and entanglement in the scattering matrix", "source": "Silas R. Beane and Roland C. Farrell", "docs_id": "2011.01278", "section": ["hep-th", "nucl-th", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Geometry and entanglement in the scattering matrix. A formulation of nucleon-nucleon scattering is developed in which the S-matrix, rather than an effective-field theory (EFT) action, is the fundamental object. Spacetime plays no role in this description: the S-matrix is a trajectory that moves between RG fixed points in a compact theory space defined by unitarity. This theory space has a natural operator definition, and a geometric embedding of the unitarity constraints in four-dimensional Euclidean space yields a flat torus, which serves as the stage on which the S-matrix propagates. Trajectories with vanishing entanglement are special geodesics between RG fixed points on the flat torus, while entanglement is driven by an external potential. The system of equations describing S-matrix trajectories is in general complicated, however the very-low-energy S-matrix -- that appears at leading-order in the EFT description -- possesses a UV/IR conformal invariance which renders the system of equations integrable, and completely determines the potential. In this geometric viewpoint, inelasticity is in correspondence with the radius of a three-dimensional hyperbolic space whose two-dimensional boundary is the flat torus. This space has a singularity at vanishing radius, corresponding to maximal violation of unitarity. The trajectory on the flat torus boundary can be explicitly constructed from a bulk trajectory with a quantifiable error, providing a simple example of a holographic quantum error correcting code."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the geometric formulation of nucleon-nucleon scattering, the S-matrix trajectory on the flat torus boundary is described by a bulk trajectory with a quantifiable error. What is the physical interpretation of this error in terms of entanglement, and how does it relate to the inelasticity of the system?\n\nA) The error represents the amount of entanglement between the nucleons, and inelasticity is a consequence of the S-matrix trajectory deviating from the geodesic path due to this entanglement.\n\nB) The error is a measure of the non-unitarity of the S-matrix, and inelasticity is a result of the trajectory's deviation from the flat torus boundary due to this non-unitarity.\n\nC) The error is a quantification of the S-matrix's non-conformal behavior, and inelasticity is a consequence of the trajectory's deviation from the UV/IR conformal fixed points due to this non-conformal behavior.\n\nD) The error represents the amount of \"quantum error\" in the S-matrix, and inelasticity is a result of the trajectory's deviation from the geodesic path due to this quantum error.\n\nCorrect Answer: A) The error represents the amount of entanglement between the nucleons, and inelasticity is a consequence of the S-matrix trajectory deviating from the geodesic path due to this entanglement.\n\nExplanation: The correct answer is A) because the error in the S-matrix trajectory is described as a \"quantifiable error\" in the context of the geometric formulation. This error is related to entanglement, which is a fundamental concept in quantum mechanics. The S-matrix trajectory deviating from the geodesic path due to entanglement is a consequence of the system's inelasticity. The other options are incorrect because they do not accurately describe the relationship between the error, entanglement, and inelasticity in the context of the geometric formulation."}, "36": {"documentation": {"title": "A new universal ratio in Random Matrix Theory and chaotic to integrable\n  transition in Type-I and Type-II hybrid Sachdev-Ye-Kitaev models", "source": "Fadi Sun, Yu Yi-Xiang, Jinwu Ye and W.-M. Liu", "docs_id": "1809.07577", "section": ["cond-mat.str-el", "hep-th", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new universal ratio in Random Matrix Theory and chaotic to integrable\n  transition in Type-I and Type-II hybrid Sachdev-Ye-Kitaev models. We investigate chaotic to integrable transition in two types of hybrid SYK models which contain both $ q=4 $ SYK with interaction $ J $ and $ q=2 $ SYK with an interaction $ K $ in type-I or $(q=2)^2$ SYK with an interaction $ \\sqrt{K} $ in type-II. These models include hybrid Majorana fermion, complex fermion and bosonic SYK. For the Majorana fermion case, we discuss both $ N $ even and $ N $ odd case. We make exact symmetry analysis on the possible symmetry class of both types of hybrid SYK in the 10 fold way by Random Matrix Theory (RMT) and also work out the degeneracy of each energy levels. We introduce a new universal ratio which is the ratio of the next nearest neighbour (NNN) energy level spacing to characterize the RMT. We perform exact diagonalization to evaluate both the known NN ratio and the new NNN ratio, then use both ratios to study Chaotic to Integrable transitions (CIT) in both types of hybrid SYK models. Some preliminary results on possible quantum analog of Kolmogorov-Arnold-Moser (KAM) theorem and its dual version in the quantum chaotic side are given. We explore some intrinsic connections between the two complementary approaches to quantum chaos: the RMT and the Lyapunov exponent by the $ 1/N $ expansion in the large $ N $ limit at a suitable temperature range. Comments on some previously related works are given. Some future perspectives, especially the failure of the Zamoloddchikov's c-theorem in 1d CFT RG flow are outlined."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary objective of the study on chaotic to integrable transition in Type-I and Type-II hybrid Sachdev-Ye-Kitaev models, and how does the introduction of a new universal ratio in Random Matrix Theory (RMT) facilitate this objective?\n\nA) To investigate the role of Majorana fermions in the hybrid SYK models and their impact on the energy level spacing.\nB) To explore the connections between Random Matrix Theory (RMT) and the Lyapunov exponent in the large N limit at a suitable temperature range.\nC) To introduce a new universal ratio in RMT that characterizes the chaotic to integrable transition in hybrid SYK models and evaluate its implications on the energy level spacing.\nD) To discuss the degeneracy of energy levels in hybrid SYK models and its relation to the 10-fold way symmetry class.\n\nCorrect Answer: C) To introduce a new universal ratio in RMT that characterizes the chaotic to integrable transition in hybrid SYK models and evaluate its implications on the energy level spacing.\n\nExplanation: The correct answer is C) because the introduction of a new universal ratio in RMT is the primary objective of the study, as stated in the abstract. This ratio is used to characterize the chaotic to integrable transition in hybrid SYK models and evaluate its implications on the energy level spacing. The other options are related to the study, but they are not the primary objective. Option A is related to the role of Majorana fermions, option B is related to the connection between RMT and the Lyapunov exponent, and option D is related to the degeneracy of energy levels."}, "37": {"documentation": {"title": "Electroweak Symmetry Breaking and Singlino Dark Matter with Deflected\n  Anomaly Mediation", "source": "Norimi Yokozaki", "docs_id": "0903.2632", "section": ["hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Electroweak Symmetry Breaking and Singlino Dark Matter with Deflected\n  Anomaly Mediation. We investigate the phenomenology of the Nearly Minimal Supersymmetric Standard Model (nMSSM) in the deflected anomaly mediation scenario. We also include the Fayet-Iliopoulos D-term of the standard model gauge group. In the nMSSM, the mu term is replaced by the vacuum expectation value of the gauge singlet; therefore, there is no difficulty in generating the B-term of the SUSY breaking scale. Although the messenger sector is introduced, direct couplings between nMSSM fields and messenger sector fields are forbidden by the discrete symmetry. Therefore, the phenomenology at the weak scale does not depend on the detail of the messenger sector. We show that there are regions of parameter space in which electroweak symmetry breaking occurs successfully and the lightest Higgs is heavier than the LEP bound. We show that the gluino is light in this scenario. The lightest neutralino, which is mainly composed of a singlino, is a candidate for dark matter. The relic density explains the observed abundance of dark matter. The dark matter-nucleon scattering cross section satisfies the current limit from CDMS and XENON10 with a small value for the strange quark content of a nucleon."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the deflected anomaly mediation scenario of the Nearly Minimal Supersymmetric Standard Model (nMSSM), what is the primary composition of the lightest neutralino, and how does it relate to the observed abundance of dark matter?\n\nA) The lightest neutralino is primarily composed of a wino, and its relic density explains the observed abundance of dark matter.\nB) The lightest neutralino is primarily composed of a singlino, and its relic density explains the observed abundance of dark matter.\nC) The lightest neutralino is primarily composed of a bino, and its relic density does not explain the observed abundance of dark matter.\nD) The lightest neutralino is primarily composed of a gluino, and its relic density does not explain the observed abundance of dark matter.\n\nCorrect Answer: B) The lightest neutralino is primarily composed of a singlino, and its relic density explains the observed abundance of dark matter.\n\nExplanation: According to the documentation, the lightest neutralino in the deflected anomaly mediation scenario is mainly composed of a singlino, which is a candidate for dark matter. The relic density of this neutralino explains the observed abundance of dark matter."}, "38": {"documentation": {"title": "Emergence of universality in the transmission dynamics of COVID-19", "source": "Ayan Paul, Jayanta Kumar Bhattacharjee, Akshay Pal and Sagar\n  Chakraborty", "docs_id": "2101.12556", "section": ["physics.soc-ph", "nlin.AO", "physics.bio-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Emergence of universality in the transmission dynamics of COVID-19. The complexities involved in modelling the transmission dynamics of COVID-19 has been a roadblock in achieving predictability in the spread and containment of the disease. In addition to understanding the modes of transmission, the effectiveness of the mitigation methods also needs to be built into any effective model for making such predictions. We show that such complexities can be circumvented by appealing to scaling principles which lead to the emergence of universality in the transmission dynamics of the disease. The ensuing data collapse renders the transmission dynamics largely independent of geopolitical variations, the effectiveness of various mitigation strategies, population demographics, etc. We propose a simple two-parameter model -- the Blue Sky model -- and show that one class of transmission dynamics can be explained by a solution that lives at the edge of a blue sky bifurcation. In addition, the data collapse leads to an enhanced degree of predictability in the disease spread for several geographical scales which can also be realized in a model-independent manner as we show using a deep neural network. The methodology adopted in this work can potentially be applied to the transmission of other infectious diseases and new universality classes may be found. The predictability in transmission dynamics and the simplicity of our methodology can help in building policies for exit strategies and mitigation methods during a pandemic."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Emergence of universality in the transmission dynamics of COVID-19\" propose a simple two-parameter model, the Blue Sky model, to explain the transmission dynamics of the disease. What is the key characteristic of this model that allows it to capture the emergence of universality in the transmission dynamics, and how does this relate to the predictability of disease spread?\n\n**A)** The Blue Sky model is based on a single parameter that captures the effectiveness of mitigation strategies, making it independent of geopolitical variations and population demographics.\n\n**B)** The Blue Sky model is a two-parameter model that incorporates a blue sky bifurcation, which leads to a data collapse that renders the transmission dynamics largely independent of these factors.\n\n**C)** The Blue Sky model is a complex model that requires multiple parameters to capture the nuances of transmission dynamics, making it less predictable than simpler models.\n\n**D)** The Blue Sky model is a model-independent approach that uses a deep neural network to predict disease spread, making it applicable to various geographical scales.\n\n**Correct Answer:** B) The Blue Sky model is a two-parameter model that incorporates a blue sky bifurcation, which leads to a data collapse that renders the transmission dynamics largely independent of geopolitical variations, the effectiveness of various mitigation strategies, population demographics, etc.\n\n**Explanation:** The correct answer is B) because the Blue Sky model is a two-parameter model that incorporates a blue sky bifurcation, which leads to a data collapse that renders the transmission dynamics largely independent of various factors. This is a key characteristic of the model that allows it to capture the emergence of universality in the transmission dynamics. The other options are incorrect because they do not accurately describe the Blue Sky model or its relationship to universality and predictability. Option A is incorrect because the Blue Sky model is not based on a single parameter that captures the effectiveness of mitigation strategies. Option C is incorrect because the Blue Sky model is not a complex model that requires multiple parameters. Option D is incorrect because the Blue Sky model is not a model-independent approach that uses a deep neural network to predict disease spread."}, "39": {"documentation": {"title": "The AiiDA-KKR plugin and its application to high-throughput impurity\n  embedding into a topological insulator", "source": "Philipp R\\\"u{\\ss}mann, Fabian Bertoldo, Stefan Bl\\\"ugel", "docs_id": "2003.08315", "section": ["cond-mat.mtrl-sci", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The AiiDA-KKR plugin and its application to high-throughput impurity\n  embedding into a topological insulator. The ever increasing availability of supercomputing resources led computer-based materials science into a new era of high-throughput calculations. Recently, Pizzi et al. [Comp. Mat. Sci. 111, 218 (2016)] introduced the AiiDA framework that provides a way to automate calculations while allowing to store the full provenance of complex workflows in a database. We present the development of the AiiDA-KKR plugin that allows to perform a large number of ab initio impurity embedding calculations based on the relativistic full-potential Korringa-Kohn-Rostoker Green function method. The capabilities of the AiiDA-KKR plugin are demonstrated with the calculation of several thousand impurities embedded into the prototypical topological insulator Sb2Te3. The results are collected in the JuDiT database which we use to investigate chemical trends as well as Fermi level and layer dependence of physical properties of impurities. This includes the study of spin moments, the impurity's tendency to form in-gap states or its effect on the charge doping of the host-crystal. These properties depend on the detailed electronic structure of the impurity embedded into the host crystal which highlights the need for ab initio calculations in order to get accurate predictions."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of using the AiiDA-KKR plugin for high-throughput impurity embedding calculations in topological insulators, and how does it enable the study of chemical trends and physical properties of impurities?\n\nA) The AiiDA-KKR plugin allows for the automation of complex workflows, enabling the storage of full provenance in a database, which facilitates the analysis of large datasets.\nB) The plugin enables the calculation of several thousand impurities embedded into Sb2Te3, allowing for the investigation of chemical trends and physical properties of impurities.\nC) The AiiDA-KKR plugin provides a way to perform ab initio impurity embedding calculations based on the relativistic full-potential Korringa-Kohn-Rostoker Green function method, which is essential for accurate predictions of physical properties.\nD) The plugin enables the study of spin moments, the impurity's tendency to form in-gap states, and its effect on charge doping of the host-crystal, which are critical aspects of understanding impurity behavior in topological insulators.\n\nCorrect Answer: A) The AiiDA-KKR plugin allows for the automation of complex workflows, enabling the storage of full provenance in a database, which facilitates the analysis of large datasets.\n\nExplanation: The correct answer highlights the primary advantage of using the AiiDA-KKR plugin, which is the automation of complex workflows and the storage of full provenance in a database. This enables the analysis of large datasets and facilitates the investigation of chemical trends and physical properties of impurities. The other options, while related to the topic, do not capture the primary advantage of the plugin. Option B is incorrect because while the plugin does enable the calculation of several thousand impurities, it is not the primary advantage. Option C is incorrect because while the plugin does provide a way to perform ab initio calculations, it is not the primary advantage. Option D is incorrect because while the plugin does enable the study of spin moments and other physical properties, it is not the primary advantage."}, "40": {"documentation": {"title": "Freestanding and permeable nanoporous gold membranes for\n  surface-enhanced Raman scattering", "source": "Roman M. Wyss, Markus Parzefall, Cynthia M. Gruber, Sebastian\n  Busschaert, Karl-Philipp Schlichting, Carin Rae Lightner, Emanuel Loertscher,\n  Lukas Novotny, and Sebastian Heeg", "docs_id": "2109.13309", "section": ["physics.optics", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Freestanding and permeable nanoporous gold membranes for\n  surface-enhanced Raman scattering. Surface-enhanced Raman spectroscopy (SERS) demands reliable, high enhancement substrates in order to be used in different fields of application. Here, we introduce freestanding porous gold membranes (PAuM) as easy to produce, scalable, mechanically stable, and effective SERS substrates. We fabricate large-scale sub-30 thick PAuM, that form freestanding membranes with varying morphologies depending on the nominal gold thickness. These PAuM are mechanically stable for pressures up to $>3$ bar, and exhibit surface-enhanced Raman scattering with local enhancement factors of $10^4$ to $10^5$, which we demonstrate by wavelength-dependent and spatially resolved Raman measurements using graphene as a local Raman probe. Numerical simulations reveal that the enhancement arises from individual, nanoscale pores in the membrane acting as optical slot antennas. Our PAuM are mechanically stable, provide robust SERS enhancement for excitation power densities up to $10^6$W$\\,$cm$^{-2}$, and may find use as a building block in flow-through sensor applications based on SERS."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary mechanism behind the surface-enhanced Raman scattering (SERS) enhancement observed in the freestanding porous gold membranes (PAuM) fabricated in this study?\n\nA) The gold nanoparticles in the membrane act as local Raman probes, enhancing the signal through quantum mechanical effects.\nB) The individual, nanoscale pores in the membrane act as optical slot antennas, concentrating the incident light and enhancing the Raman signal.\nC) The mechanical stress applied to the membrane causes the gold atoms to rearrange, creating a new surface that enhances the Raman signal.\nD) The graphene used as a local Raman probe interacts with the PAuM, transferring energy and enhancing the Raman signal.\n\nCorrect Answer: B) The individual, nanoscale pores in the membrane act as optical slot antennas, concentrating the incident light and enhancing the Raman signal.\n\nExplanation: The correct answer is based on the statement in the documentation that \"numerical simulations reveal that the enhancement arises from individual, nanoscale pores in the membrane acting as optical slot antennas.\" This indicates that the pores in the membrane are responsible for concentrating the incident light and enhancing the Raman signal, rather than the gold nanoparticles, mechanical stress, or graphene interaction."}, "41": {"documentation": {"title": "Light Transmission Through Metallic-Mean Quasiperiodic Stacks with\n  Oblique Incidence", "source": "Stefanie Thiem, Michael Schreiber, Uwe Grimm", "docs_id": "1010.1396", "section": ["physics.optics"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Light Transmission Through Metallic-Mean Quasiperiodic Stacks with\n  Oblique Incidence. The propagation of s- and p-polarized light through quasiperiodic multilayers, consisting of layers with different refractive indices, is studied by the transfer matrix method. In particular, we focus on the transmission coefficient of the systems in dependency on the incidence angle and on the ratio of the refractive indices. We obtain additional bands with almost complete transmission in the quasiperiodic systems at frequencies in the range of the photonic band gap of a system with a periodic alignment of the two materials for both types of light polarization. With increasing incidence angle these bands bend towards higher frequencies, where the curvature of the transmission bands in the quasiperiodic stack depends on the metallic mean of the construction rule. Additionally, in the quasiperiodic systems for p-polarized light the bands show almost complete transmission near the Brewster's angle in contrast to the results for s-polarized light. Further, we present results for the influence of the refractive indices at the midgap frequency of the periodic stack, where the quasiperiodicity was found to be most effective."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the transmission behavior of s- and p-polarized light through quasiperiodic multilayers with different refractive indices, focusing on the effects of incidence angle and refractive index ratio on the transmission coefficient.\n\n**A)** The transmission coefficient of s-polarized light through quasiperiodic multilayers is independent of the incidence angle and refractive index ratio.\n\n**B)** The quasiperiodic multilayers exhibit almost complete transmission of p-polarized light near the Brewster's angle, whereas s-polarized light shows almost complete transmission at the midgap frequency of the periodic stack.\n\n**C)** The curvature of the transmission bands in the quasiperiodic stack depends on the metallic mean of the construction rule, and increases with increasing incidence angle.\n\n**D)** The transmission coefficient of s-polarized light through quasiperiodic multilayers is enhanced by increasing the refractive index ratio between the two materials.\n\n**Correct Answer:** B) The quasiperiodic multilayers exhibit almost complete transmission of p-polarized light near the Brewster's angle, whereas s-polarized light shows almost complete transmission at the midgap frequency of the periodic stack.\n\n**Explanation:**\n\nThe correct answer, B, is supported by the text, which states that \"in the quasiperiodic systems for p-polarized light the bands show almost complete transmission near the Brewster's angle in contrast to the results for s-polarized light.\" This indicates that the quasiperiodic multilayers exhibit enhanced transmission of p-polarized light near the Brewster's angle, whereas s-polarized light shows almost complete transmission at the midgap frequency of the periodic stack.\n\nThe other options are incorrect because:\n\nA) The transmission coefficient of s-polarized light is not independent of the incidence angle and refractive index ratio, as the text states that the bands bend towards higher frequencies with increasing incidence angle.\n\nC) The curvature of the transmission bands does not depend on the metallic mean of the construction rule, but rather on the incidence angle.\n\nD) The transmission coefficient of s-polarized light is not enhanced by increasing the refractive index ratio between the two materials, as the text does not provide any evidence to support this claim."}, "42": {"documentation": {"title": "Omega and Eta (Eta-prime) mesons from NN and ND collisions at\n  intermediate energies", "source": "L. P. Kaptari, B. Kampfer", "docs_id": "0911.0160", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Omega and Eta (Eta-prime) mesons from NN and ND collisions at\n  intermediate energies. The production of pseudo scalar, Eeta, Eta-prime, and vector, Omega, Rho, Phi, mesons in NN collisions at threshold-near energies is analyzed within a covariant effective meson-nucleon theory. It is shown that a good description of cross sections and angular distributions, for vector meson production, can be accomplished by considering meson and nucleon currents only, while for pseudo scalar production an inclusion of nucleon resonances is needed. The di-electron production from subsequent Dalitz decay of the produced mesons, $\\eta'\\to \\gamma \\gamma^* \\to\\gamma e^+e^-$ and $\\omega\\to \\pi\\gamma^*\\to \\pi e^+e^-$ is also considered and numerical results are presented for intermediate energies and kinematics of possible experiments with HADES, CLAS and KEK-PS. We argue that the transition form factor $\\omega\\to \\gamma^*\\pi$ as well as $\\eta'\\to \\gamma^*\\gamma$ can be defined in a fairly model independent way and the feasibility of an experimental access to transition form factors is discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary difference in the description of cross sections and angular distributions for vector meson production versus pseudo scalar meson production in NN collisions at threshold-near energies, according to the covariant effective meson-nucleon theory?\n\nA) Vector mesons require inclusion of nucleon resonances, while pseudo scalars do not.\nB) Pseudo scalars require inclusion of nucleon resonances, while vector mesons do not.\nC) Both vector and pseudo scalar mesons require consideration of nucleon currents only.\nD) Both vector and pseudo scalar mesons require consideration of nucleon resonances and currents.\n\nCorrect Answer: B) Pseudo scalars require inclusion of nucleon resonances, while vector mesons do not.\n\nExplanation: According to the text, the production of pseudo scalar mesons (such as Eta and Eta-prime) requires an inclusion of nucleon resonances, while the production of vector mesons (such as Omega and Rho) can be described by considering meson and nucleon currents only. This is the primary difference between the two cases."}, "43": {"documentation": {"title": "Solid shell prism elements based on hierarchical, heterogeneous, and\n  anisotropic shape functions", "source": "Lukasz Kaczmarczyk, Hoang Nguyen, Zahur Ullah, Mebratu Wakeni, Chris\n  Pearce", "docs_id": "2010.08799", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solid shell prism elements based on hierarchical, heterogeneous, and\n  anisotropic shape functions. The formulation of a new prism finite element is presented for the nonlinear analysis of solid shells subject to large strains and large displacements. The element is based on hierarchical, heterogeneous, and anisotropic shape functions. As with other solid shell formulations, only displacement degrees of freedom are required to describe the shell kinematics and general three-dimensional material laws can be adopted. However, the novelty of this formulation is the ability to capture complex shell behaviour and avoid locking phenomena, without the need to use reduced integration or adopt additional natural strain or enhanced strain fields. Thus, this element is ideally suited for geometrically and physically nonlinear problems. This is achieved by constructing independent approximation shape functions on both the prism element's triangular faces and through the thickness, where the latter is associated with a local coordinate system that convects with deformation of the shell. The element is extremely efficient, with the hierarchical property lending itself to an efficient and highly scalable multigrid solver, and the heterogeneity property enables local p-adaptivity. The paper demonstrates performance of the element for a number of linear and geometrically nonlinear problems, benchmarked against well established problems in the literature. The formulation has been implemented in the MoFEM."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed solid shell prism element formulation, as described in the Arxiv paper, in terms of its ability to capture complex shell behavior and avoid locking phenomena?\n\nA) It uses reduced integration to improve numerical stability.\nB) It employs a reduced integration scheme to avoid locking phenomena and capture complex shell behavior.\nC) It utilizes a hierarchical, heterogeneous, and anisotropic shape function approach to capture complex shell behavior and avoid locking phenomena.\nD) It adopts a natural strain field to enhance the accuracy of the solution.\n\nCorrect Answer: C) It utilizes a hierarchical, heterogeneous, and anisotropic shape function approach to capture complex shell behavior and avoid locking phenomena.\n\nExplanation: The correct answer is C) because the paper states that the novelty of this formulation is the ability to capture complex shell behavior and avoid locking phenomena, without the need to use reduced integration or adopt additional natural strain or enhanced strain fields. This is achieved through the use of hierarchical, heterogeneous, and anisotropic shape functions. The other options are incorrect because they either mention reduced integration (A and B), which is not a primary advantage of this formulation, or adopt a natural strain field (D), which is not mentioned in the paper as a key feature."}, "44": {"documentation": {"title": "Discussing anthropogenic global warming from an econometric perspective:\n  a change scenario based on the Arima paleoclimate time series model", "source": "Gilmar V. F. Santos, Lucas G. Cordeiro, Claudio A. Rojo e Edison L.\n  Leismann", "docs_id": "2109.10419", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discussing anthropogenic global warming from an econometric perspective:\n  a change scenario based on the Arima paleoclimate time series model. Global warming has divided the scientific community worldwide with predominance for anthropogenic alarmism. This article aims to project a climate change scenario using a stochastic model of paleotemperature time series and compare it with the dominant thesis. The ARIMA model, an integrated autoregressive process of moving averages, popularly known as Box-Jenkins, was used for this purpose. The results showed that the estimates of the model parameters were below 1 degree Celsius for a scenario of 100 years which suggests a period of temperature reduction and a probable cooling, contrary to the prediction of the IPCC and the anthropogenic current of an increase in 1,50 degree to 2,0 degree Celsius by the end of this century. Thus, we hope with this study to contribute to the discussion by adding a statistical element of paleoclimate in counterpoint to the current consensus and to placing the debate in a long term historical dimension, in line with other research already present in the scientific literature."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary conclusion of the study regarding the projected climate change scenario, and how does it differ from the dominant thesis of anthropogenic global warming?\n\nA) The study confirms the IPCC's prediction of a 1.5 to 2.0 degree Celsius increase in global temperature by the end of the century, and suggests a period of temperature reduction is unlikely.\nB) The study uses the ARIMA model to project a climate change scenario that suggests a period of temperature reduction and probable cooling, contrary to the dominant thesis of anthropogenic alarmism.\nC) The study finds that the estimates of the model parameters are above 1 degree Celsius for a scenario of 100 years, indicating a period of temperature increase, and supports the IPCC's prediction.\nD) The study uses a different statistical model to project a climate change scenario that suggests a period of temperature stability, and does not differ from the dominant thesis of anthropogenic global warming.\n\nCorrect Answer: B) The study uses the ARIMA model to project a climate change scenario that suggests a period of temperature reduction and probable cooling, contrary to the dominant thesis of anthropogenic alarmism.\n\nExplanation: The correct answer is B) because the study uses the ARIMA model to project a climate change scenario that suggests a period of temperature reduction and probable cooling, which is contrary to the dominant thesis of anthropogenic alarmism. The study's results show that the estimates of the model parameters are below 1 degree Celsius for a scenario of 100 years, indicating a period of temperature reduction, rather than an increase. This conclusion challenges the dominant thesis of anthropogenic global warming and adds a statistical element of paleoclimate to the discussion."}, "45": {"documentation": {"title": "Estimating Tipping Points in Feedback-Driven Financial Networks", "source": "Zvonko Kostanjcar, Stjepan Begusic, H. E. Stanley, and Boris Podobnik", "docs_id": "1509.04952", "section": ["q-fin.CP", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating Tipping Points in Feedback-Driven Financial Networks. Much research has been conducted arguing that tipping points at which complex systems experience phase transitions are difficult to identify. To test the existence of tipping points in financial markets, based on the alternating offer strategic model we propose a network of bargaining agents who mutually either cooperate or where the feedback mechanism between trading and price dynamics is driven by an external \"hidden\" variable R that quantifies the degree of market overpricing. Due to the feedback mechanism, R fluctuates and oscillates over time, and thus periods when the market is underpriced and overpriced occur repeatedly. As the market becomes overpriced, bubbles are created that ultimately burst in a market crash. The probability that the index will drop in the next year exhibits a strong hysteresis behavior from which we calculate the tipping point. The probability distribution function of R has a bimodal shape characteristic of small systems near the tipping point. By examining the S&P500 index we illustrate the applicability of the model and demonstate that the financial data exhibits a hysteresis and a tipping point that agree with the model predictions. We report a cointegration between the returns of the S&P 500 index and its intrinsic value."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the alternating offer strategic model, what is the primary mechanism driving the fluctuations in the external \"hidden\" variable R, which quantifies the degree of market overpricing?\n\nA) The feedback mechanism between trading and price dynamics is driven by the collective behavior of the bargaining agents.\nB) The probability distribution function of R is influenced by the intrinsic value of the S&P 500 index.\nC) R fluctuates and oscillates over time due to the feedback mechanism between trading and price dynamics, which is driven by an external factor.\nD) The market overpricing is solely determined by the collective behavior of the bargaining agents.\n\n**Correct Answer:** C) R fluctuates and oscillates over time due to the feedback mechanism between trading and price dynamics, which is driven by an external factor.\n\n**Explanation:** The correct answer is C) because the documentation states that the feedback mechanism between trading and price dynamics is driven by an external \"hidden\" variable R, which quantifies the degree of market overpricing. This external factor causes R to fluctuate and oscillate over time, leading to periods of underpricing and overpricing in the market. The other options are incorrect because they either misrepresent the mechanism driving R's fluctuations (A and D) or incorrectly attribute the influence of R to the intrinsic value of the S&P 500 index (B)."}, "46": {"documentation": {"title": "Arterial Tube Laws and Wave Speeds", "source": "Kim H. Parker", "docs_id": "2106.10061", "section": ["physics.flu-dyn", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Arterial Tube Laws and Wave Speeds. The 1-D theory of flow in the arteries yields an equation for the wave speed in terms of the density of blood and the distensibility of the vessel. By means of this equation there is a duality between the equation for the wave speed and the tube law describing the area of the vessel as a function of pressure. We explore this duality for the equations for wave speed and tube laws that are most commonly used in theoretical arterial hemodynamics. We see that there are qualitative differences between these laws and the experimental data on wave speed in canine arteries measured by Anliker and his colleagues 50 years ago. We suggest an empirical equation for wave speed (and its dual tube law) that fits the experimental data as well as the common expectation that arteries become stiffer as the pressure increases. We conclude with a cautionary historical tale about the differences between the theoretical predictions and the experimental measurements of the speed of sound in air that persisted for more than 200 years."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The 1-D theory of flow in the arteries yields an equation for the wave speed in terms of the density of blood and the distensibility of the vessel. According to the text, what is the primary difference between the theoretical predictions of wave speed and the experimental data on wave speed in canine arteries?\n\nA) The theoretical predictions are more accurate for low pressures, while the experimental data are more accurate for high pressures.\nB) The theoretical predictions are more accurate for high pressures, while the experimental data are more accurate for low pressures.\nC) The theoretical predictions are more accurate for arteries with high distensibility, while the experimental data are more accurate for arteries with low distensibility.\nD) The theoretical predictions are more accurate for arteries with low density, while the experimental data are more accurate for arteries with high density.\n\n**Correct Answer:** B) The theoretical predictions are more accurate for high pressures, while the experimental data are more accurate for low pressures.\n\n**Explanation:** The text states that \"we see that there are qualitative differences between these laws and the experimental data on wave speed in canine arteries measured by Anliker and his colleagues 50 years ago.\" This suggests that the theoretical predictions of wave speed are not consistent with the experimental data, particularly at high pressures. The correct answer, B, reflects this discrepancy. The other options are incorrect because they do not accurately summarize the relationship between theoretical predictions and experimental data."}, "47": {"documentation": {"title": "Decoherence as Decay of the Loschmidt Echo in a Lorentz Gas", "source": "F. M. Cucchietti, H. M. Pastawski and D. A. Wisniacki", "docs_id": "cond-mat/0102135", "section": ["cond-mat.dis-nn", "cond-mat.mes-hall", "nlin.CD", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Decoherence as Decay of the Loschmidt Echo in a Lorentz Gas. Classical chaotic dynamics is characterized by the exponential sensitivity to initial conditions. Quantum mechanics, however, does not show this feature. We consider instead the sensitivity of quantum evolution to perturbations in the Hamiltonian. This is observed as an atenuation of the Loschmidt Echo, $M(t)$, i.e. the amount of the original state (wave packet of width $\\sigma$) which is recovered after a time reversed evolution, in presence of a classically weak perturbation. By considering a Lorentz gas of size $L$, which for large $L$ is a model for an {\\it unbounded} classically chaotic system, we find numerical evidence that, if the perturbation is within a certain range, $M(t)$ decays exponentially with a rate $1/\\tau_{\\phi}$ determined by the Lyapunov exponent $\\lambda$ of the corresponding classical dynamics. This exponential decay extends much beyond the Eherenfest time $t_{E}$ and saturates at a time $t_{s}\\simeq \\lambda^{-1}\\ln (\\widetilde{N})$, where $\\widetilde{N}\\simeq (L/\\sigma)^2$ is the effective dimensionality of the Hilbert space. Since $\\tau _{\\phi}$ quantifies the increasing uncontrollability of the quantum phase (decoherence) its characterization and control has fundamental interest."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Loschmidt Echo and decoherence in a Lorentz gas, what is the relationship between the Lyapunov exponent \u03bb of the corresponding classical dynamics and the rate of exponential decay of the Loschmidt Echo M(t)?\n\n**A)** \u03bb is directly proportional to the rate of exponential decay of M(t)\n**B)** \u03bb is inversely proportional to the rate of exponential decay of M(t)\n**C)** \u03bb is equal to the rate of exponential decay of M(t)\n**D)** \u03bb is proportional to the effective dimensionality of the Hilbert space, \u221d ln(\u03c4_\u03c6)\n\n**Correct Answer:** B) \u03bb is inversely proportional to the rate of exponential decay of M(t)\n\n**Explanation:** The text states that the rate of exponential decay of M(t) is given by 1/\u03c4_\u03c6, which is determined by the Lyapunov exponent \u03bb of the corresponding classical dynamics. This implies that \u03bb is inversely proportional to the rate of exponential decay of M(t), i.e. \u03bb \u221d 1/(1/\u03c4_\u03c6). This relationship is a fundamental aspect of the decoherence process in quantum mechanics, and understanding it is crucial for controlling and characterizing decoherence."}, "48": {"documentation": {"title": "Hi-C Observations of Sunspot Penumbral Bright Dots", "source": "Shane E. Alpert, Sanjiv K. Tiwari, Ronald L. Moore, Amy R. Winebarger,\n  and Sabrina L. Savage", "docs_id": "1603.04968", "section": ["astro-ph.SR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hi-C Observations of Sunspot Penumbral Bright Dots. We report observations of bright dots (BDs) in a sunspot penumbra using High Resolution Coronal Imager (Hi-C) data in 193 \\AA\\ and examine their sizes, lifetimes, speeds, and intensities. The sizes of the BDs are on the order of 1\\arcsec\\ and are therefore hard to identify in the Atmospheric Imaging Assembly (AIA) 193 \\AA\\ images, which have 1.2\\arcsec\\ spatial resolution, but become readily apparent with Hi-C's five times better spatial resolution. We supplement Hi-C data with data from AIA's 193 \\AA\\ passband to see the complete lifetime of the BDs that appeared before and/or lasted longer than Hi-C's 3-minute observation period. Most Hi-C BDs show clear lateral movement along penumbral striations, toward or away from the sunspot umbra. Single BDs often interact with other BDs, combining to fade away or brighten. The BDs that do not interact with other BDs tend to have smaller displacements. These BDs are about as numerous but move slower on average than Interface Region Imaging Spectrograph (IRIS) BDs, recently reported by \\cite{tian14}, and the sizes and lifetimes are on the higher end of the distribution of IRIS BDs. Using additional AIA passbands, we compare the lightcurves of the BDs to test whether the Hi-C BDs have transition region (TR) temperature like that of the IRIS BDs. The lightcurves of most Hi-C BDs peak together in different AIA channels indicating that their temperature is likely in the range of the cooler TR ($1-4\\times 10^5$ K)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the characteristics of bright dots (BDs) observed in sunspot penumbra using High Resolution Coronal Imager (Hi-C) data. What can be inferred about the temperature of these BDs based on their lightcurves?\n\nA) The BDs have a temperature in the range of the hotter TR ($5-10\\times 10^5$ K), as indicated by their lightcurves peaking together in AIA channels corresponding to higher temperatures.\n\nB) The BDs have a temperature in the range of the cooler TR ($1-4\\times 10^5$ K), as indicated by their lightcurves peaking together in AIA channels corresponding to lower temperatures.\n\nC) The BDs have a temperature that is difficult to determine from their lightcurves, as their lightcurves do not provide clear evidence of their temperature.\n\nD) The BDs have a temperature that is not consistent with the temperature of the surrounding corona, as indicated by their lightcurves showing a significant difference in intensity between different AIA channels.\n\n**Correct Answer:** B) The BDs have a temperature in the range of the cooler TR ($1-4\\times 10^5$ K), as indicated by their lightcurves peaking together in AIA channels corresponding to lower temperatures.\n\n**Explanation:** The correct answer is B) because the lightcurves of most Hi-C BDs peak together in different AIA channels, indicating that their temperature is likely in the range of the cooler TR ($1-4\\times 10^5$ K). This is inferred from the fact that the BDs do not interact with other BDs and tend to have smaller displacements, which suggests that they are cooler than the hotter TR ($5-10\\times 10^5$ K) temperature range."}, "49": {"documentation": {"title": "Robust linear least squares regression", "source": "Jean-Yves Audibert, Olivier Catoni", "docs_id": "1010.0074", "section": ["math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Robust linear least squares regression. We consider the problem of robustly predicting as well as the best linear combination of $d$ given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. For the ridge estimator and the ordinary least squares estimator, and their variants, we provide new risk bounds of order $d/n$ without logarithmic factor unlike some standard results, where $n$ is the size of the training data. We also provide a new estimator with better deviations in the presence of heavy-tailed noise. It is based on truncating differences of losses in a min--max framework and satisfies a $d/n$ risk bound both in expectation and in deviations. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Experimental results strongly back up our truncated min--max estimator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a linear least squares regression problem with $d$ given functions and a training dataset of size $n$. Suppose we want to estimate the best linear combination of these functions using a robust estimator. What is the key common surprising factor of the new risk bounds obtained for the ridge estimator, ordinary least squares estimator, and their variants, which allows them to achieve exponential deviations without an exponential moment condition on the output distribution?\n\n**A)** The use of a truncated min-max framework to truncate differences of losses.\n**B)** The absence of logarithmic factors in the risk bounds.\n**C)** The use of a PAC-Bayesian analysis on truncated differences of losses.\n**D)** The truncation of differences of losses to obtain a new estimator with better deviations.\n\n**Correct Answer:** C) The use of a PAC-Bayesian analysis on truncated differences of losses.\n\n**Explanation:** The correct answer is C) The use of a PAC-Bayesian analysis on truncated differences of losses. The key common surprising factor of the new risk bounds is that they are obtained through a PAC-Bayesian analysis on truncated differences of losses, which allows them to achieve exponential deviations without an exponential moment condition on the output distribution. This is mentioned in the introduction of the paper as a \"surprising factor\". The other options are not correct because they do not capture the key common surprising factor of the new risk bounds. Option A is related to the method used to obtain the risk bounds, but it is not the key common surprising factor. Option B is a characteristic of the risk bounds, but it is not the key common surprising factor. Option D is a characteristic of the new estimator, but it is not the key common surprising factor."}, "50": {"documentation": {"title": "Transverse momentum spectra of hadrons in high energy pp and heavy ion\n  collisions", "source": "Kapil Saraswat, Prashant Shukla and Venktesh Singh", "docs_id": "1706.04860", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transverse momentum spectra of hadrons in high energy pp and heavy ion\n  collisions. We present a study of transverse momentum ($p_{T}$) spectra of unidentified charged particles in pp collisions at RHIC and LHC energies from $\\sqrt{s}$ = 62.4 GeV to 13 TeV using Tsallis/Hagedorn function. The power law of Tsallis/Hagedorn form gives excellent description of the hadron spectra in $p_{T}$ range from 0.2 to 300 GeV/$c$. The power index $n$ of the $p_T$ distributions is found to follow a function of the type $a+b/\\sqrt {s}$ with asymptotic value $a = 5.72$. The parameter $T$ governing the soft bulk contribution to the spectra remains almost same over wide range of collision energies. We also provide a Tsallis/Hagedorn fit to the $p_{T}$ spectra of hadrons in pPb and different centralities of PbPb collisions at $\\sqrt{s_{NN}}$ = 5.02 TeV. The data/fit shows deviations from the Tsallis distribution which become more pronounced as the system size increases. We suggest simple modifications in the Tsallis/Hagedorn power law function and show that the above deviations can be attributed to the transverse flow in low $p_T$ region and to the in-medium energy loss in high $p_T$ region."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary conclusion regarding the power index $n$ of the $p_T$ distributions in high-energy pp and heavy ion collisions, as described by the Tsallis/Hagedorn function?\n\nA) The power index $n$ is independent of the collision energy and remains constant.\nB) The power index $n$ follows a function of the type $a+b/\\sqrt {s}$, with an asymptotic value of $a = 5.72$.\nC) The power index $n$ is directly proportional to the collision energy.\nD) The power index $n$ is inversely proportional to the collision energy.\n\nCorrect Answer: B) The power index $n$ follows a function of the type $a+b/\\sqrt {s}$, with an asymptotic value of $a = 5.72$.\n\nExplanation: The correct answer is B) because the documentation states that the power index $n$ of the $p_T$ distributions is found to follow a function of the type $a+b/\\sqrt {s}$, with an asymptotic value of $a = 5.72$. This indicates that the power index $n$ is not independent of the collision energy, but rather depends on it in a specific way. The other options are incorrect because they do not accurately reflect the relationship between the power index $n$ and the collision energy described in the documentation."}, "51": {"documentation": {"title": "Unbiased deep solvers for parametric PDEs", "source": "Marc Sabate Vidales and David Siska and Lukasz Szpruch", "docs_id": "1810.05094", "section": ["q-fin.CP", "cs.LG", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Unbiased deep solvers for parametric PDEs. We develop several deep learning algorithms for approximating families of parametric PDE solutions. The proposed algorithms approximate solutions together with their gradients, which in the context of mathematical finance means that the derivative prices and hedging strategies are computed simulatenously. Having approximated the gradient of the solution one can combine it with a Monte-Carlo simulation to remove the bias in the deep network approximation of the PDE solution (derivative price). This is achieved by leveraging the Martingale Representation Theorem and combining the Monte Carlo simulation with the neural network. The resulting algorithm is robust with respect to quality of the neural network approximation and consequently can be used as a black-box in case only limited a priori information about the underlying problem is available. We believe this is important as neural network based algorithms often require fair amount of tuning to produce satisfactory results. The methods are empirically shown to work for high-dimensional problems (e.g. 100 dimensions). We provide diagnostics that shed light on appropriate network architectures."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed algorithms in approximating families of parametric PDE solutions, and how does it relate to the use of neural networks in mathematical finance?\n\n**A)** The algorithms can efficiently compute derivative prices and hedging strategies simultaneously, reducing the need for separate simulations.\n**B)** The algorithms can leverage the Martingale Representation Theorem to remove bias in the deep network approximation of the PDE solution, making them robust to the quality of the neural network approximation.\n**C)** The algorithms can be used to approximate high-dimensional PDE solutions with a large number of dimensions, such as 100 dimensions.\n**D)** The algorithms can be used as a black-box solution in cases where limited a priori information about the underlying problem is available.\n\n**Correct Answer:** B) The algorithms can leverage the Martingale Representation Theorem to remove bias in the deep network approximation of the PDE solution, making them robust to the quality of the neural network approximation.\n\n**Explanation:** The correct answer is B) because the documentation states that the proposed algorithms approximate solutions together with their gradients, which allows for the removal of bias in the deep network approximation of the PDE solution using the Martingale Representation Theorem. This makes the algorithm robust with respect to the quality of the neural network approximation, making it suitable for use as a black-box solution in cases where limited a priori information is available."}, "52": {"documentation": {"title": "Inverse Problems of Combined Photoacoustic and Optical Coherence\n  Tomography", "source": "Peter Elbau, Leonidas Mindrinos and Otmar Scherzer", "docs_id": "1601.03320", "section": ["math.AP", "math-ph", "math.MP", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inverse Problems of Combined Photoacoustic and Optical Coherence\n  Tomography. Optical coherence tomography (OCT) and photoacoustic tomography (PAT) are emerging non-invasive biological and medical imaging techniques. It is a recent trend in experimental science to design experiments that perform PAT and OCT imaging at once. In this paper we present a mathematical model describing the dual experiment. Since OCT is mathematically modelled by Maxwell's equations or some simplifications of it, whereas the light propagation in quantitative photoacoustics is modelled by (simplifications of) the radiative transfer equation, the first step in the derivation of a mathematical model of the dual experiment is to obtain a unified mathematical description, which in our case are Maxwell's equations. As a by-product we therefore derive a new mathematical model of photoacoustic tomography based on Maxwell's equations. It is well known by now, that without additional assumptions on the medium, it is not possible to uniquely reconstruct all optical parameters from either one of these modalities alone. We show that in the combined approach one has additional information, compared to a single modality, and the inverse problem of reconstruction of the optical parameters becomes feasible."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of combined photoacoustic and optical coherence tomography, what is the primary mathematical model used to describe the dual experiment, and what is the significance of deriving a unified mathematical description for both modalities?\n\n**A)** The radiative transfer equation is used to model the light propagation in photoacoustic tomography, and Maxwell's equations are used to model the light propagation in optical coherence tomography. The unified mathematical description is significant because it allows for the reconstruction of optical parameters in the combined approach.\n\n**B)** The radiative transfer equation is used to model the light propagation in photoacoustic tomography, and Maxwell's equations are used to model the light propagation in optical coherence tomography. The unified mathematical description is significant because it enables the simultaneous reconstruction of both optical and photoacoustic parameters.\n\n**C)** Maxwell's equations are used to model the light propagation in photoacoustic tomography, and the radiative transfer equation is used to model the light propagation in optical coherence tomography. The unified mathematical description is significant because it allows for the reconstruction of optical parameters in the combined approach.\n\n**D)** The radiative transfer equation is used to model the light propagation in photoacoustic tomography, and Maxwell's equations are used to model the light propagation in optical coherence tomography. The unified mathematical description is significant because it enables the simultaneous reconstruction of both optical and photoacoustic parameters, and it is a by-product of deriving a mathematical model of photoacoustic tomography based on Maxwell's equations.\n\n**Correct Answer:** D) The radiative transfer equation is used to model the light propagation in photoacoustic tomography, and Maxwell's equations are used to model the light propagation in optical coherence tomography. The unified mathematical description is significant because it enables the simultaneous reconstruction of both optical and photoacoustic parameters, and it is a by-product of deriving a mathematical model of photoacoustic tomography based on Maxwell's equations.\n\n**Explanation:** The correct answer is D) because it accurately describes the primary mathematical model used to describe the dual experiment, which is the combination of Maxwell's equations for optical coherence tomography and the radiative transfer equation for photoacoustic tomography. The unified mathematical description is significant because it enables the simultaneous reconstruction of both optical and photoacoustic parameters, and it is a by-product of deriving a mathematical model of photoacoustic tomography based on Maxwell's equations."}, "53": {"documentation": {"title": "Momentum-Dependent Mass and AC Hall Conductivity of Quantum Anomalous\n  Hall Insulators and Their Relation to the Parity Anomaly", "source": "Christian Tutschku, Jan B\\\"ottcher, Ren\\'e Meyer, and E. M. Hankiewicz", "docs_id": "2003.03146", "section": ["cond-mat.mes-hall", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Momentum-Dependent Mass and AC Hall Conductivity of Quantum Anomalous\n  Hall Insulators and Their Relation to the Parity Anomaly. The Dirac mass of a two-dimensional QAH insulator is directly related to the parity anomaly of planar quantum electrodynamics, as shown initially in Phys. Rev. Lett. 52, 18 (1984). In this work, we connect the additional momentum-dependent Newtonian mass term of a QAH insulator to the parity anomaly. By calculating the effective action, we reveal that the Newtonian mass term acts like a parity-breaking element of a high-energy regularization scheme. As such, it is directly related to the parity anomaly. In addition, the calculation of the effective action allows us to determine the finite frequency correction to the DC Hall conductivity of a QAH insulator. We derive that the leading order AC correction contains a term proportional to the torsional Hall viscosity. This paves the way to measure this non-dissipative transport coefficient via electrical or magneto-optical experiments. Moreover, we prove that the Newtonian mass significantly changes the resonance structure of the AC Hall conductivity in comparison to pure Dirac systems like graphene."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: In the context of the Parity Anomaly in Quantum Anomalous Hall Insulators (QAHIs), what is the relationship between the Dirac mass and the parity anomaly, and how does the introduction of a momentum-dependent Newtonian mass term affect the effective action and the DC Hall conductivity?\n\nA) The Dirac mass is directly proportional to the parity anomaly, and the Newtonian mass term is a parity-breaking element that affects the effective action and the DC Hall conductivity.\n\nB) The Dirac mass is inversely proportional to the parity anomaly, and the Newtonian mass term is a parity-preserving element that does not affect the effective action and the DC Hall conductivity.\n\nC) The Dirac mass is directly related to the parity anomaly, but the Newtonian mass term only affects the effective action at high energies, and does not impact the DC Hall conductivity.\n\nD) The Dirac mass is a function of the parity anomaly, but the Newtonian mass term is a constant that does not affect the effective action or the DC Hall conductivity.\n\nCorrect Answer: A) The Dirac mass is directly proportional to the parity anomaly, and the Newtonian mass term is a parity-breaking element that affects the effective action and the DC Hall conductivity.\n\nExplanation: The correct answer is A) because the documentation states that the Dirac mass of a two-dimensional QAH insulator is directly related to the parity anomaly of planar quantum electrodynamics. Additionally, the Newtonian mass term is described as a parity-breaking element that acts like a parity-breaking element of a high-energy regularization scheme, which is consistent with option A. Options B, C, and D are incorrect because they either misrepresent the relationship between the Dirac mass and the parity anomaly, or incorrectly state the effect of the Newtonian mass term on the effective action and the DC Hall conductivity."}, "54": {"documentation": {"title": "Observability of dynamical networks from graphic and symbolic approaches", "source": "Irene Sendi\\~na-Nadal and Christophe Letellier", "docs_id": "1907.10316", "section": ["nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Observability of dynamical networks from graphic and symbolic approaches. A dynamical network, a graph whose nodes are dynamical systems, is usually characterized by a large dimensional space which is not always accesible due to the impossibility of measuring all the variables spanning the state space. Therefore, it is of the utmost importance to determine a reduced set of variables providing all the required information for non-ambiguously distinguish its different states. Inherited from control theory, one possible approach is based on the use of the observability matrix defined as the Jacobian matrix of the change of coordinates between the original state space and the space reconstructed from the measured variables. The observability of a given system can be accurately assessed by symbolically computing the complexity of the determinant of the observability matrix and quantified by symbolic observability coefficients. In this work, we extend the symbolic observability, previously developed for dynamical systems, to networks made of coupled $d$-dimensional node dynamics ($d>1$). From the observability of the node dynamics, the coupling function between the nodes, and the adjacency matrix, it is indeed possible to construct the observability of a large network with an arbitrary topology."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a dynamical network consisting of $n$ nodes, each described by a $d$-dimensional state space, where $d > 1$. The nodes are coupled through a coupling function $f$ and are connected by an adjacency matrix $A$. The observability of the network can be assessed using the observability matrix $\\mathcal{O}$, which is the Jacobian matrix of the change of coordinates between the original state space and the space reconstructed from the measured variables.\n\n**A)** Show that the observability of the network is equivalent to the observability of each individual node, and explain why this is the case.\n\n**B)** Derive an expression for the observability matrix $\\mathcal{O}$ in terms of the coupling function $f$, the adjacency matrix $A$, and the dimension $d$ of the state space.\n\n**C)** Explain why the observability coefficients, which quantify the complexity of the determinant of the observability matrix, are useful for assessing the observability of the network.\n\n**D)** Consider a network with $n$ nodes, each described by a 2-dimensional state space, and the nodes are connected by an undirected graph with $m$ edges. Show that the observability of the network is limited by the number of edges $m$, and explain why this is the case.\n\n**Correct Answer:** D\n\n**Explanation:**\n\nThe correct answer is D because the observability of the network is limited by the number of edges $m$ in the undirected graph connecting the nodes. This is because the observability matrix $\\mathcal{O}$ is constructed from the coupling function $f$ and the adjacency matrix $A$, which encodes the connectivity of the network. The number of edges $m$ in the graph determines the number of independent variables that can be measured, which in turn limits the observability of the network.\n\nThe other options are incorrect because:\n\n* A) The observability of the network is not equivalent to the observability of each individual node, because the coupling function $f$ and the adjacency matrix $A$ introduce non-linear interactions between the nodes that affect the observability of the network.\n* B) The expression for the observability matrix $\\mathcal{O}$ is not straightforward to derive, and it requires a deep understanding of the underlying mathematical structure of the network.\n* C) The observability coefficients are useful for assessing the observability of the network, but they do not provide a complete understanding of the network's behavior."}, "55": {"documentation": {"title": "Counterfactual Mean Embeddings", "source": "Krikamol Muandet, Motonobu Kanagawa, Sorawit Saengkyongam, Sanparith\n  Marukatat", "docs_id": "1805.08845", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Counterfactual Mean Embeddings. Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modeling of outcome distributions associated with different interventions -- known as counterfactual distributions -- is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) that can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the proposed Counterfactual Mean Embedding (CME) representation in modeling counterfactual distributions, and how does it enable causal inference over the entire landscape of the counterfactual distribution?\n\n**A)** The CME representation allows for non-parametric estimation of counterfactual distributions, enabling the estimation of causal effects over entire outcome distributions without requiring any parametric assumptions about the underlying distributions.\n\n**B)** The CME representation enables the estimation of counterfactual distributions using a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, allowing for the quantification of causal effects over complex outcomes such as images, sequences, and graphs.\n\n**C)** The CME representation provides a novel Hilbert space representation that can be used to model counterfactual distributions associated with different interventions, enabling the estimation of causal effects over entire outcome distributions.\n\n**D)** The CME representation allows for the estimation of counterfactual distributions using observational data without requiring any parametric assumptions about the underlying distributions, enabling the estimation of causal effects over entire outcome distributions.\n\n**Correct Answer:** A) The CME representation allows for non-parametric estimation of counterfactual distributions, enabling the estimation of causal effects over entire outcome distributions without requiring any parametric assumptions about the underlying distributions.\n\n**Explanation:** The correct answer is A) because the CME representation is a key innovation of the proposed work, enabling non-parametric estimation of counterfactual distributions and causal effects over entire outcome distributions without requiring any parametric assumptions about the underlying distributions. This is a critical advantage of the CME representation, as it allows for more flexible and accurate modeling of complex causal relationships."}, "56": {"documentation": {"title": "Heat capacity and thermal expansion of metal crystalline materials based\n  on dynamic thermal vibration", "source": "Jieqiong Zhang, Junzhi Cui, Zihao Yang, Yifan Yu and Liang Ma", "docs_id": "1805.05803", "section": ["cond-mat.stat-mech", "cond-mat.mtrl-sci", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heat capacity and thermal expansion of metal crystalline materials based\n  on dynamic thermal vibration. A novel approach based on dynamic thermal vibration is proposed to calculate the heat capacity and thermal expansion coefficient (TEC) for metal crystalline materials from 0K to the melting point. The motion of metal atomic clusters is decomposed into structural deformation and thermal vibration. Then thermal vibration equations are established by the fourth-order Taylor expansion of Hamiltonian at the transient structural deformation position $\\bar{\\mathbf{x}}$. As a result, the thermal vibration frequencies dynamically change with the structural deformation positions and temperatures. A parameter $\\bar\\delta (\\bar{\\mathbf{x}}, T)$ is newly introduced to illustrate how the thermal vibration frequencies vary with the temperature $T$. Besides, the modified temperature-dependent Gr\\\"uneisen parameter $\\bar \\gamma (\\bar{\\mathbf{x}}, T)$ is given. Finally, the formulae of heat capacity and TEC for metal crystalline materials are derived from the dynamic thermal vibration frequencies and $\\bar \\delta (\\bar{\\mathbf{x}}, T)$ as well as $\\bar \\gamma (\\bar{\\mathbf{x}}, T)$. The numerical results of heat capacity and TEC for metals Cu, Al, Au, Ag, Ni, Pd, Pt and Pb show a temperature dependence and agree well with the experimental data from 0K to the melting point. This work suggests an efficient approach to calculate thermodynamic properties of metal materials for a wide range of temperatures, up to the melting point."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the main contribution of the proposed approach in calculating the heat capacity and thermal expansion coefficient (TEC) for metal crystalline materials?\n\nA) The introduction of the parameter $\\bar\\delta (\\bar{\\mathbf{x}}, T)$ to illustrate the temperature dependence of thermal vibration frequencies is the main contribution.\n\nB) The use of the fourth-order Taylor expansion of the Hamiltonian at the transient structural deformation position $\\bar{\\mathbf{x}}$ is the key innovation.\n\nC) The derivation of the formulae for heat capacity and TEC from the dynamic thermal vibration frequencies and $\\bar \\gamma (\\bar{\\mathbf{x}}, T)$ is the main contribution.\n\nD) The decomposition of metal atomic clusters into structural deformation and thermal vibration is the fundamental step in the proposed approach.\n\n**Correct Answer:** D) The decomposition of metal atomic clusters into structural deformation and thermal vibration is the fundamental step in the proposed approach.\n\n**Explanation:** The correct answer is D) because the documentation states that \"The motion of metal atomic clusters is decomposed into structural deformation and thermal vibration.\" This is the fundamental step in the proposed approach, as it allows for the separation of thermal vibration from structural deformation, enabling the calculation of heat capacity and TEC.\n\nThe other options are incorrect because:\n\nA) While the introduction of $\\bar\\delta (\\bar{\\mathbf{x}}, T)$ is an important aspect of the approach, it is not the main contribution.\n\nB) The use of the fourth-order Taylor expansion of the Hamiltonian is a mathematical tool used in the approach, but it is not the main contribution.\n\nC) The derivation of the formulae for heat capacity and TEC is a consequence of the approach, but it is not the main contribution."}, "57": {"documentation": {"title": "Superconductivity in two-band non-adiabatic systems", "source": "M.E. Palistrant, V.A. Ursu", "docs_id": "cond-mat/0602551", "section": ["cond-mat.supr-con"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Superconductivity in two-band non-adiabatic systems. Theory of superconductivity in two-band non-adiabatic systems with strong electron correlations in the linear approximation over non-adiabaticity is built in the article. Having assumed weak electron-phonon interaction analytical expressions for the vertex functions and \"intersecting \" functions for each energy band are obtained. Mass operators of Green functions with account of diagrams with intersection of two lines of electron- phonon interaction (exceeding the frameworks of Migdal theorem) are derived as well as main equations of theory of superconductivity in two-band systems. Analytical formulas for the temperature of superconducting transition Tc are obtained as well as renormalization of the quantities which enter these formulas because of effects of non-adiabaticity and strong electron correlations. Dependence of Tc and coefficient of isotope effect on Migdal parameter m is studied. Overlapping of energy bands on Fermi surface with effects of non-adiabaticity at low values of transferred momentum is shown to obtain high values of Tc even assuming weak electron-phonon interaction."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the theory of superconductivity in two-band non-adiabatic systems with strong electron correlations, what is the main equation that describes the mass operators of Green functions, taking into account diagrams with intersection of two lines of electron-phonon interaction?\n\n**A)** \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, q, \u03c9) \u03c6(k, q, \u03c9)\n\n**B)** \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, q, \u03c9) \u03c6(k, \u03c9) \u03c6(q, \u03c9)\n\n**C)** \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, \u03c9) \u03c6(q, \u03c9) \u03c6(k, q, \u03c9)\n\n**D)** \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, \u03c9) \u03c6(k, q, \u03c9) \u03c6(q, \u03c9)\n\n**Correct Answer:** C) \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, \u03c9) \u03c6(q, \u03c9) \u03c6(k, q, \u03c9)\n\n**Explanation:** The correct answer is option C. The mass operators of Green functions in the theory of superconductivity in two-band non-adiabatic systems with strong electron correlations are described by the equation \u222b[d^3k] \u222b[d^3q] G(k, q, \u03c9) \u03c6(k, \u03c9) \u03c6(q, \u03c9) \u03c6(k, q, \u03c9), where G(k, q, \u03c9) is the Green function and \u03c6(k, \u03c9) is the vertex function. This equation takes into account diagrams with intersection of two lines of electron-phonon interaction, which is beyond the framework of the Migdal theorem."}, "58": {"documentation": {"title": "Compact Securities Markets for Pareto Optimal Reallocation of Risk", "source": "David M. Pennock, Michael P. Wellman", "docs_id": "1301.3886", "section": ["cs.GT", "q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compact Securities Markets for Pareto Optimal Reallocation of Risk. The emph{securities market} is the fundamental theoretical framework in economics and finance for resource allocation under uncertainty. Securities serve both to reallocate risk and to disseminate probabilistic information. emph{Complete} securities markets - which contain one security for every possible state of nature - support Pareto optimal allocations of risk. Complete markets suffer from the same exponential dependence on the number of underlying events as do joint probability distributions. We examine whether markets can be structured and \"compacted\" in the same manner as Bayesian network representations of joint distributions. We show that, if all agents' risk-neutral independencies agree with the independencies encoded in the market structure, then the market is emph{operationally complete}: risk is still Pareto optimally allocated, yet the number of securities can be exponentially smaller. For collections of agents of a certain type, agreement on Markov independencies is sufficient to admit compact and operationally complete markets."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a securities market with two underlying assets, A and B, and three possible states of nature: {H, L}, {H, H}, and {L, L}. The market is initially incomplete, with only one security available for each state of nature. However, the agents in the market agree on the Markov independencies between the assets, which are encoded in the market structure as follows:\n\n* P(A|H) = P(A|L) = 0.5\n* P(B|H) = 0.8\n* P(B|L) = 0.2\n\nUsing the results from the paper \"Compact Securities Markets for Pareto Optimal Reallocation of Risk\", determine whether the market can be compacted to achieve operational completeness. If so, describe the compacted market structure and explain how it achieves operational completeness.\n\n**A)** The market can be compacted to achieve operational completeness, and the compacted market structure consists of two securities: one for the state {H, H} and one for the state {L, L}. The compacted market structure is operationally complete because the agents' risk-neutral independencies agree with the independencies encoded in the market structure.\n\n**B)** The market cannot be compacted to achieve operational completeness, as the number of underlying events is too large.\n\n**C)** The market can be compacted to achieve operational completeness, and the compacted market structure consists of three securities: one for each state of nature. The compacted market structure is operationally complete because the agents' risk-neutral independencies agree with the independencies encoded in the market structure.\n\n**D)** The market can be compacted to achieve operational completeness, but the compacted market structure is not unique. There are multiple possible compacted market structures that achieve operational completeness.\n\n**Correct Answer:** A) The market can be compacted to achieve operational completeness, and the compacted market structure consists of two securities: one for the state {H, H} and one for the state {L, L}. The compacted market structure is operationally complete because the agents' risk-neutral independencies agree with the independencies encoded in the market structure.\n\n**Explanation:** The correct answer is A) because the agents' risk-neutral independencies agree with the independencies encoded in the market structure. Specifically, the Markov independencies between the assets are encoded in the market structure as follows:\n\n* P(A|H) = P(A|L) = 0.5 (independence between A and H/L)\n* P(B|H) = 0.8 (dependence between B and H)\n* P(B|L) = 0.2 (dependence between B and L)\n\nThese independencies are consistent with the agents' risk-neutral independencies, which are encoded in the market structure. Therefore, the market can be compacted to achieve operational completeness, and the compacted market structure consists of two securities: one for the state {H, H} and one for the state {L, L}."}, "59": {"documentation": {"title": "Wilkinson's bus: Weak condition numbers, with an application to singular\n  polynomial eigenproblems", "source": "Martin Lotz and Vanni Noferini", "docs_id": "1905.05466", "section": ["math.NA", "cs.NA", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wilkinson's bus: Weak condition numbers, with an application to singular\n  polynomial eigenproblems. We propose a new approach to the theory of conditioning for numerical analysis problems for which both classical and stochastic perturbation theory fail to predict the observed accuracy of computed solutions. To motivate our ideas, we present examples of problems that are discontinuous at a given input and have infinite classical and stochastic condition number, but where the solution is still computed to machine precision without relying on structured algorithms. Stimulated by the failure of classical and stochastic perturbation theory in capturing such phenomena, we define and analyse a weak worst-case and a weak stochastic condition number. This new theory is a more powerful predictor of the accuracy of computations than existing tools, especially when the worst-case and the expected sensitivity of a problem to perturbations of the input is not finite. We apply our analysis to the computation of simple eigenvalues of matrix polynomials, including the more difficult case of singular matrix polynomials. In addition, we show how the weak condition numbers can be estimated in practice."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Consider a matrix polynomial of the form $A(x) = x^2 + Bx + C$, where $A(x)$ is a $3 \\times 3$ matrix polynomial with entries $a_{ij}(x)$. Suppose that the matrix $A(x)$ is singular at $x = 0$, and the classical condition number of $A(x)$ is infinite. What can be said about the weak stochastic condition number of $A(x)$, and how does it relate to the accuracy of the computed eigenvalues of $A(x)$?\n\nA) The weak stochastic condition number is infinite, and the accuracy of the computed eigenvalues is limited by the stochastic nature of the perturbations.\n\nB) The weak stochastic condition number is finite, and the accuracy of the computed eigenvalues is limited by the classical condition number of $A(x)$.\n\nC) The weak stochastic condition number is finite, and the accuracy of the computed eigenvalues is limited by the weak stochastic condition number itself, which is a measure of the sensitivity of the eigenvalues to perturbations in the input.\n\nD) The weak stochastic condition number is infinite, and the accuracy of the computed eigenvalues is limited by the weak stochastic condition number, which is a measure of the expected sensitivity of the eigenvalues to perturbations in the input.\n\nCorrect Answer: C) The weak stochastic condition number is finite, and the accuracy of the computed eigenvalues is limited by the weak stochastic condition number itself, which is a measure of the sensitivity of the eigenvalues to perturbations in the input.\n\nExplanation: The correct answer is C) because the weak stochastic condition number is a measure of the expected sensitivity of the eigenvalues to perturbations in the input, and it is finite even when the classical condition number is infinite. This is because the weak stochastic condition number takes into account the stochastic nature of the perturbations, which can lead to a more accurate prediction of the accuracy of the computed eigenvalues. The other options are incorrect because they do not accurately reflect the relationship between the weak stochastic condition number and the accuracy of the computed eigenvalues. Option A is incorrect because the weak stochastic condition number is not necessarily infinite, even when the classical condition number is infinite. Option B is incorrect because the weak stochastic condition number is not limited by the classical condition number of $A(x)$. Option D is incorrect because the weak stochastic condition number is not necessarily infinite, even when the classical condition number is infinite."}}