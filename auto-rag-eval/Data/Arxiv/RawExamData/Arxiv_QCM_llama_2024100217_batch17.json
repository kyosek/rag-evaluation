{"0": {"documentation": {"title": "Vetoing Cosmogenic Muons in A Large Liquid Scintillator", "source": "Marco Grassi, Jarah Evslin, Emilio Ciuffoli, Xinmin Zhang", "docs_id": "1505.05609", "section": ["physics.ins-det", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Vetoing Cosmogenic Muons in A Large Liquid Scintillator. At upcoming medium baseline reactor neutrino experiments the spallation 9Li background will be somewhat larger than the inverse beta decay reactor neutrino signal. We use new FLUKA simulations of spallation backgrounds to optimize a class of veto strategies and find that surprisingly the optimal veto for the mass hierarchy determination has a rejection efficiency below 90%. The unrejected background has only a modest effect on the physics goals. For example $\\Delta\\chi^2$ for the hierarchy determination falls by 1.4 to 3 points depending on the muon tracking ability. The optimal veto strategy is essentially insensitive to the tracking ability, consisting of 2 meter radius, 1.1 second cylindrical vetoes of well tracked muons with showering energies above 3 to 4 GeV and 0.7 second full detector vetoes for poorly tracked muons above 15 to 18 GeV. On the other hand, as the uncertainty in theta12 will be dominated by the uncertainty in the reactor neutrino spectrum and not statistical fluctuations, the optimal rejection efficiency for the measurement of theta12 is 93% in the case of perfect tracking."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the optimal veto strategy for the mass hierarchy determination in a large liquid scintillator experiment, and how does it impact the measurement of theta12?\n\nA) The optimal veto strategy consists of 2 meter radius, 1.1 second cylindrical vetoes for well-tracked muons with showering energies above 3 to 4 GeV, and 0.7 second full detector vetoes for poorly tracked muons above 15 to 18 GeV. This rejection efficiency is 90% for the mass hierarchy determination.\n\nB) The optimal veto strategy consists of 2 meter radius, 1.1 second cylindrical vetoes for well-tracked muons with showering energies above 3 to 4 GeV, and 0.7 second full detector vetoes for poorly tracked muons above 15 to 18 GeV. This rejection efficiency is 93% for the measurement of theta12.\n\nC) The optimal veto strategy consists of 2 meter radius, 1.1 second cylindrical vetoes for poorly tracked muons with showering energies above 15 to 18 GeV, and 0.7 second full detector vetoes for well-tracked muons with showering energies above 3 to 4 GeV. This rejection efficiency is 90% for the measurement of theta12.\n\nD) The optimal veto strategy consists of 2 meter radius, 1.1 second cylindrical vetoes for poorly tracked muons with showering energies above 15 to 18 GeV, and 0.7 second full detector vetoes for well-tracked muons with showering energies above 3 to 4 GeV. This rejection efficiency is 93% for the mass hierarchy determination.\n\nCorrect Answer: B) The optimal veto strategy consists of 2 meter radius, 1.1 second cylindrical vetoes for well-tracked muons with showering energies above 3 to 4 GeV, and 0.7 second full detector vetoes for poorly tracked muons above 15 to 18 GeV. This rejection efficiency is 93% for the measurement of theta12.\n\nExplanation: The correct answer is B) because the optimal veto strategy is described as consisting of 2 meter radius, 1.1 second cylindrical vetoes for well-tracked muons with showering energies above 3 to 4 GeV, and 0.7 second full detector vetoes for poorly tracked muons above 15 to 18 GeV. This rejection efficiency is specifically stated to be 93% for the measurement of theta12, which is the dominant source of uncertainty in this case. The other options are incorrect because they either describe the veto strategy incorrectly or provide a rejection efficiency that is not relevant to the measurement of theta12."}, "1": {"documentation": {"title": "Multiwavelength characterisation of an ACT-selected, lensed dusty\n  star-forming galaxy at z=2.64", "source": "G. W. Roberts-Borsani, M. J. Jim\\'enez-Donaire, M. Dapr\\`a, K.\n  Alatalo, I. Aretxaga, J. \\'Alvarez-M\\'arquez, A. J. Baker, S. Fujimoto, P. A.\n  Gallardo, M. Gralla, M. Hilton, J. P. Hughes, C. Jim\\'enez, N. Laporte, T. A.\n  Marriage, F. Nati, J. Rivera, A. Sievers, A. Wei{\\ss}, G. W. Wilson, E. J.\n  Wollack, M. S. Yun", "docs_id": "1706.06099", "section": ["astro-ph.GA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiwavelength characterisation of an ACT-selected, lensed dusty\n  star-forming galaxy at z=2.64. We present \\ci\\,(2--1) and multi-transition $^{12}$CO observations of a dusty star-forming galaxy, ACT\\,J2029+0120, which we spectroscopically confirm to lie at $z$\\,=\\,2.64. We detect CO(3--2), CO(5--4), CO(7--6), CO(8--7), and \\ci\\,(2--1) at high significance, tentatively detect HCO$^{+}$(4--3), and place strong upper limits on the integrated strength of dense gas tracers (HCN(4--3) and CS(7--6)). Multi-transition CO observations and dense gas tracers can provide valuable constraints on the molecular gas content and excitation conditions in high-redshift galaxies. We therefore use this unique data set to construct a CO spectral line energy distribution (SLED) of the source, which is most consistent with that of a ULIRG/Seyfert or QSO host object in the taxonomy of the \\textit{Herschel} Comprehensive ULIRG Emission Survey. We employ RADEX models to fit the peak of the CO SLED, inferring a temperature of T$\\sim$117 K and $n_{\\text{H}_2}\\sim10^5$ cm$^{-3}$, most consistent with a ULIRG/QSO object and the presence of high density tracers. We also find that the velocity width of the \\ci\\ line is potentially larger than seen in all CO transitions for this object, and that the $L'_{\\rm C\\,I(2-1)}/L'_{\\rm CO(3-2)}$ ratio is also larger than seen in other lensed and unlensed submillimeter galaxies and QSO hosts; if confirmed, this anomaly could be an effect of differential lensing of a shocked molecular outflow."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\nWhat can be inferred about the molecular gas content and excitation conditions in the high-redshift galaxy ACT\\,J2029+0120, based on the multi-transition CO observations and dense gas tracers?\n\nA) The galaxy has a low molecular gas content and is in a state of low excitation.\nB) The galaxy has a high molecular gas content and is in a state of high excitation, consistent with a ULIRG/QSO host object.\nC) The galaxy has a moderate molecular gas content and is in a state of moderate excitation, with no clear indication of ULIRG/QSO characteristics.\nD) The galaxy has a low molecular gas content and is in a state of high excitation, with a possible shock-induced molecular outflow.\n\nCorrect Answer: B) The galaxy has a high molecular gas content and is in a state of high excitation, consistent with a ULIRG/QSO host object.\n\nExplanation: The RADEX models used to fit the CO spectral line energy distribution (SLED) of the source infer a temperature of T$\\sim$117 K and $n_{\\text{H}_2}\\sim10^5$ cm$^{-3}$, which is most consistent with a ULIRG/QSO object. This suggests that the galaxy has a high molecular gas content and is in a state of high excitation, consistent with the characteristics of a ULIRG/QSO host object."}, "2": {"documentation": {"title": "Disparate Vulnerability to Membership Inference Attacks", "source": "Bogdan Kulynych, Mohammad Yaghini, Giovanni Cherubin, Michael Veale,\n  Carmela Troncoso", "docs_id": "1906.00389", "section": ["cs.LG", "cs.CR", "cs.CY", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Disparate Vulnerability to Membership Inference Attacks. A membership inference attack (MIA) against a machine-learning model enables an attacker to determine whether a given data record was part of the model's training data or not. In this paper, we provide an in-depth study of the phenomenon of disparate vulnerability against MIAs: unequal success rate of MIAs against different population subgroups. We first establish necessary and sufficient conditions for MIAs to be prevented, both on average and for population subgroups, using a notion of distributional generalization. Second, we derive connections of disparate vulnerability to algorithmic fairness and to differential privacy. We show that fairness can only prevent disparate vulnerability against limited classes of adversaries. Differential privacy bounds disparate vulnerability but can significantly reduce the accuracy of the model. We show that estimating disparate vulnerability to MIAs by na\\\"ively applying existing attacks can lead to overestimation. We then establish which attacks are suitable for estimating disparate vulnerability, and provide a statistical framework for doing so reliably. We conduct experiments on synthetic and real-world data finding statistically significant evidence of disparate vulnerability in realistic settings. The code is available at https://github.com/spring-epfl/disparate-vulnerability"}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of differential privacy in preventing disparate vulnerability to membership inference attacks, as discussed in the paper \"Disparate Vulnerability to Membership Inference Attacks\"?\n\nA) Differential privacy can only prevent disparate vulnerability against a specific class of adversaries.\nB) Differential privacy can significantly reduce the accuracy of the model, but does not prevent disparate vulnerability.\nC) Differential privacy can prevent disparate vulnerability, but may require additional fairness measures.\nD) Differential privacy is equivalent to preventing disparate vulnerability, and can be used as a substitute.\n\n**Correct Answer:** B) Differential privacy can significantly reduce the accuracy of the model, but does not prevent disparate vulnerability.\n\n**Explanation:** The paper discusses the limitations of differential privacy in preventing disparate vulnerability to membership inference attacks. While differential privacy provides bounds on disparate vulnerability, it can significantly reduce the accuracy of the model, making it a trade-off between fairness and model performance. This is in contrast to fairness measures, which can prevent disparate vulnerability against limited classes of adversaries."}, "3": {"documentation": {"title": "Investigating the influence of relativistic effects on absorption\n  spectra for platinum complexes with light-activated activity against cancer\n  cells", "source": "Joel Creutzberg and Erik Donovan Hedeg{\\aa}rd", "docs_id": "2006.16097", "section": ["physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigating the influence of relativistic effects on absorption\n  spectra for platinum complexes with light-activated activity against cancer\n  cells. We report the first investigation of relativistic effects on the UV-vis spectra of two prototype complexes for so-called photo-activated chemotherapy (PACT), trans-trans-trans-[Pt(N3)2(OH)2(NH3)2] and cis-trans-cis-[Pt(N3)2(OH)2(NH3)2].In PACT, design of new drugs requires in-depth understanding of the photo-activation mechanisms. A first step is usually to rationalize their UV-vis spectra for which time-dependent density functional theory (TD-DFT) is an indispensable tool. We carried out TD-DFT calculations with a systematic series of non-relativistic(NR), scalar-relativistic (SR), and four-component (4c) Hamiltonians. Large differences are found between spectra calculated within 4c and NR frameworks, while the most intense features (found at higher energies below 300 nm) can be reasonably well reproduced within a SR framework. Yet the underlying transitions can be strongly influenced by spin-orbit coupling introduced in the 4c framework: while this can affect both intense and less intense transitions in the spectra, the effect is most pronounced for weaker transitions at lower energies, above 300 nm. Since the investigated complexes are activated with light of wavelengths above 300 nm, employing a method with explicit inclusion of spin-orbit coupling may be crucial to rationalize the activation mechanism. All calculations were carried out with both the CAM-B3LYP and B3LYP functionals; we generally find the former to perform best in comparison with experimental spectra."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary reason why the inclusion of spin-orbit coupling in the 4c Hamiltonian is crucial for rationalizing the activation mechanism of the investigated platinum complexes?\n\nA) Because it improves the accuracy of the calculations for intense transitions at higher energies below 300 nm.\nB) Because it allows for a more detailed understanding of the underlying transitions that are influenced by spin-orbit coupling.\nC) Because it enables the calculation of spectra that are more consistent with experimental data.\nD) Because it is necessary to accurately model the relativistic effects on the absorption spectra of the complexes.\n\nCorrect Answer: B) Because it allows for a more detailed understanding of the underlying transitions that are influenced by spin-orbit coupling.\n\nExplanation: The correct answer is B) Because it allows for a more detailed understanding of the underlying transitions that are influenced by spin-orbit coupling. The text states that the inclusion of spin-orbit coupling in the 4c framework can affect both intense and less intense transitions, and that its effect is most pronounced for weaker transitions at lower energies. This suggests that the primary reason for including spin-orbit coupling is to gain a more detailed understanding of the underlying transitions that are influenced by this effect."}, "4": {"documentation": {"title": "Art Pricing with Computer Graphic Techniques", "source": "Lan Ju, Zhiyong Tu, Changyong Xue", "docs_id": "1910.03800", "section": ["q-fin.GN"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Art Pricing with Computer Graphic Techniques. This paper makes the first attempt to introduce the tools from computer graphics into the art pricing research. We argue that the creation of a painting calls for a combination of conceptual effort and painting effort from the artist. However, as the important price determinants, both efforts are long missing in the traditional hedonic model because they are hard to measure. This paper draws on the digital pictures of auctioned paintings from various renowned artists, and applies the image recognition techniques to measure the variances of lines and colors of these paintings. We then use them as the proxies for the artist's painting effort, and include them in the hedonic regression to test their significance. Our results show that the variances of lines and colors of a painting can significantly positively explain the sales price in a general context. Our suggested measurements can better capture the content heterogeneity of paintings hence improving on the traditional art pricing methodology. Our approach also provides a quantitative perspective for both valuation and authentication of paintings."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of the traditional hedonic model in art pricing research, according to the paper \"Art Pricing with Computer Graphic Techniques\"?\n\nA) The model fails to account for the role of the artist's conceptual effort in pricing a painting.\nB) The model is unable to capture the variances of lines and colors in a painting, which are crucial in determining its price.\nC) The model neglects the importance of the artist's experience and reputation in pricing a painting.\nD) The model is unable to distinguish between original and fake paintings.\n\n**Correct Answer:** B) The model is unable to capture the variances of lines and colors in a painting, which are crucial in determining its price.\n\n**Explanation:** The paper argues that the traditional hedonic model is limited because it fails to account for the artist's conceptual effort and painting effort, which are both difficult to measure. The authors propose using image recognition techniques to measure the variances of lines and colors in a painting as proxies for the artist's painting effort, which can significantly positively explain the sales price. Therefore, option B is the correct answer."}, "5": {"documentation": {"title": "Phenomenology of CP-even ALP", "source": "Kodai Sakurai and Wen Yin", "docs_id": "2111.03653", "section": ["hep-ph", "astro-ph.CO", "hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Phenomenology of CP-even ALP. Axion or axion-like particle (ALP) has been usually considered as a CP-odd Nambu-Goldstone boson (NGB) from the spontaneous breakdown of a global U(1) symmetry. In this paper, we point out that the NGB behaves as a CP-even particle coupled to the SM particles in a large class of simple (or perhaps the simplest) renormalizable models. We provide a first study of the collider phenomenology and cosmology of the CP-even ALP. In a natural parameter region, the CP-even ALP can be produced from the Higgs boson decay in colliders. When the mass is not very light, the signals will be Higgs exotic decays, Higgs decay to displaced vertex $\\times 2$, Higgs decay to displaced vertex + missing energy. The signal can be discriminated from other models, e.g. hidden photon, by measuring the decay length and the decay products of the light new particle. In addition, when $ m_a\\lesssim \\,$MeV, in which case the Higgs boson invisible decay may be probed in the colliders, the CP-even ALP is a nice Dark matter (DM) candidate. The DM can be probed by 21cm line measurement, the future measurement of the Milky way mass halo function in the Vera Rubin Observatory, as well as X- or $\\gamma$-ray observations. The DM production mechanisms are discussed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Phenomenology of CP-even ALP, what are the possible ways to distinguish the signals of the CP-even ALP from other models, such as the hidden photon, in high-energy colliders?\n\n**A)** By measuring the decay products of the light new particle and the decay length of the CP-even ALP.\n\n**B)** By analyzing the Higgs boson decay channels, specifically the Higgs exotic decays, Higgs decay to displaced vertex \u00d7 2, and Higgs decay to displaced vertex + missing energy.\n\n**C)** By probing the Higgs boson invisible decay in colliders for masses less than MeV.\n\n**D)** By using the 21cm line measurement, future measurement of the Milky way mass halo function in the Vera Rubin Observatory, and X- or \u03b3-ray observations to detect the Dark matter (DM) production mechanisms of the CP-even ALP.\n\n**Correct Answer:** B) By analyzing the Higgs boson decay channels, specifically the Higgs exotic decays, Higgs decay to displaced vertex \u00d7 2, and Higgs decay to displaced vertex + missing energy.\n\n**Explanation:** The correct answer is B) because the question specifically asks about distinguishing the signals of the CP-even ALP from other models in high-energy colliders. The Higgs boson decay channels mentioned in option B are specific and relevant to the context of the Phenomenology of CP-even ALP. The other options, while related to the topic, are not directly relevant to distinguishing the signals of the CP-even ALP from other models in colliders. Option A is related to measuring the decay products and length of the CP-even ALP, which is not directly relevant to distinguishing it from other models. Option C is related to probing the Higgs boson invisible decay, which is a different aspect of the CP-even ALP phenomenology. Option D is related to detecting the Dark matter production mechanisms, which is a broader topic that is not directly relevant to distinguishing the signals of the CP-even ALP from other models in colliders."}, "6": {"documentation": {"title": "Economic complexity of prefectures in Japan", "source": "Abhijit Chakraborty, Hiroyasu Inoue, Yoshi Fujiwara", "docs_id": "2002.05785", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Economic complexity of prefectures in Japan. Every nation prioritizes the inclusive economic growth and development of all regions. However, we observe that economic activities are clustered in space, which results in a disparity in per-capita income among different regions. A complexity-based method was proposed by Hidalgo and Hausmann [PNAS 106, 10570-10575 (2009)] to explain the large gaps in per-capita income across countries. Although there have been extensive studies on countries' economic complexity using international export data, studies on economic complexity at the regional level are relatively less studied. Here, we study the industrial sector complexity of prefectures in Japan based on the basic information of more than one million firms. We aggregate the data as a bipartite network of prefectures and industrial sectors. We decompose the bipartite network as a prefecture-prefecture network and sector-sector network, which reveals the relationships among them. Similarities among the prefectures and among the sectors are measured using a metric. From these similarity matrices, we cluster the prefectures and sectors using the minimal spanning tree technique.The computed economic complexity index from the structure of the bipartite network shows a high correlation with macroeconomic indicators, such as per-capita gross prefectural product and prefectural income per person. We argue that this index reflects the present economic performance and hidden potential of the prefectures for future growth."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary goal of the study on economic complexity of prefectures in Japan, and how does it relate to the concept of inclusive economic growth and development?\n\nA) To identify the most economically complex prefectures in Japan and their potential for future growth, with the goal of promoting inclusive economic growth and development.\nB) To analyze the relationships among prefectures and industrial sectors in Japan, with the goal of understanding the underlying drivers of economic complexity.\nC) To develop a new metric for measuring economic complexity at the regional level, with the goal of identifying areas for targeted economic development interventions.\nD) To examine the impact of economic complexity on macroeconomic indicators such as per-capita gross prefectural product and prefectural income per person, with the goal of promoting inclusive economic growth and development.\n\n**Correct Answer:** A) To identify the most economically complex prefectures in Japan and their potential for future growth, with the goal of promoting inclusive economic growth and development.\n\n**Explanation:** The study aims to understand the economic complexity of prefectures in Japan and its relationship with macroeconomic indicators. By analyzing the industrial sector complexity of prefectures, the researchers aim to identify areas of high economic potential and hidden growth opportunities, which can inform policies to promote inclusive economic growth and development. The study's findings suggest that the economic complexity index is highly correlated with macroeconomic indicators, indicating that it is a useful tool for understanding the economic performance and potential of prefectures in Japan."}, "7": {"documentation": {"title": "Chiral and U(1) axial symmetry restoration in linear sigma models with\n  two quark flavors", "source": "Stefan Michalski", "docs_id": "hep-ph/0601255", "section": ["hep-ph", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chiral and U(1) axial symmetry restoration in linear sigma models with\n  two quark flavors. We study the restoration of chiral symmetry in linear sigma models with two quark flavors. The models taken into consideration have a U(2)_L x U(2)_R and an O(4) internal symmetry. The physical mesons of these models are sigma, pion, eta and a_0 where the latter two are not present in the O(4) model. Including two-loop contributions through sunset graphs we calculate the temperature behavior of the order parameter and the masses for explicit chiral symmetry breaking and in the chiral limit. Decay threshold effects introduced by the sunset graphs alter the temperature dependence of the condensate and consequently that of the masses as well. This correctly reproduces a second-order phase transition for the O(4) model and for the U(2)_L x U(2)_R model with an axial U(1) anomaly as expected from universality class arguments. Chiral symmetry tends to be restored at higher temperatures in the two-loop approximation than in the Hartree-Fock approximation. To model a restoration of the axial U(1) symmetry we imply a temperature-dependent anomaly parameter that sharply drops at about 175 MeV. This triggers the restoration of chiral symmetry before the full symmetry is restored and lowers the transition temperatures significantly below 200 MeV."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of linear sigma models with two quark flavors, what is the effect of including two-loop contributions through sunset graphs on the temperature behavior of the order parameter and masses for explicit chiral symmetry breaking?\n\n**A)** The two-loop contributions lead to a first-order phase transition, which alters the temperature dependence of the condensate and masses.\n\n**B)** The two-loop contributions have no effect on the temperature behavior of the order parameter and masses, as they only introduce decay threshold effects that are negligible at high temperatures.\n\n**C)** The two-loop contributions trigger the restoration of chiral symmetry before the full symmetry is restored, and lower the transition temperatures significantly below 200 MeV.\n\n**D)** The two-loop contributions lead to a second-order phase transition, which correctly reproduces the expected universality class behavior for the O(4) model and the U(2)_L x U(2)_R model with an axial U(1) anomaly.\n\n**Correct Answer:** D) The two-loop contributions lead to a second-order phase transition, which correctly reproduces the expected universality class behavior for the O(4) model and the U(2)_L x U(2)_R model with an axial U(1) anomaly.\n\n**Explanation:** The correct answer is D) because the two-loop contributions through sunset graphs are shown to alter the temperature dependence of the condensate and masses, leading to a second-order phase transition that correctly reproduces the expected universality class behavior for the O(4) model and the U(2)_L x U(2)_R model with an axial U(1) anomaly. This is in line with the findings of the study, which demonstrates the importance of including two-loop contributions in the calculation of the temperature behavior of the order parameter and masses."}, "8": {"documentation": {"title": "Zeroth-order optimisation on subsets of symmetric matrices with\n  application to MPC tuning", "source": "Alejandro I. Maass, Chris Manzie, Iman Shames, Hayato Nakada", "docs_id": "2106.14359", "section": ["math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Zeroth-order optimisation on subsets of symmetric matrices with\n  application to MPC tuning. This paper provides a zeroth-order optimisation framework for non-smooth and possibly non-convex cost functions with matrix parameters that are real and symmetric. We provide complexity bounds on the number of iterations required to ensure a given accuracy level for both the convex and non-convex case. The derived complexity bounds for the convex case are less conservative than available bounds in the literature since we exploit the symmetric structure of the underlying matrix space. Moreover, the non-convex complexity bounds are novel for the class of optimisation problems we consider. The utility of the framework is evident in the suite of applications that use symmetric matrices as tuning parameters. Of primary interest here is the challenge of tuning the gain matrices in model predictive controllers, as this is a challenge known to be inhibiting industrial implementation of these architectures. To demonstrate the framework we consider the problem of MIMO diesel air-path control, and consider implementing the framework iteratively ``in-the-loop'' to reduce tracking error on the output channels. Both simulations and experimental results are included to illustrate the effectiveness of the proposed framework over different engine drive cycles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary advantage of the zeroth-order optimisation framework proposed in the paper, and how does it relate to the complexity bounds for the convex and non-convex cases?\n\n**A)** The framework exploits the symmetric structure of the underlying matrix space to derive less conservative complexity bounds for the convex case, but does not provide any advantages for the non-convex case.\n\n**B)** The framework provides a novel approach to handling non-convex optimisation problems, but the complexity bounds for the convex case are not less conservative than available bounds in the literature.\n\n**C)** The framework's ability to exploit the symmetric structure of the underlying matrix space leads to more efficient complexity bounds for both the convex and non-convex cases, making it a more effective approach for tuning gain matrices in model predictive controllers.\n\n**D)** The framework's complexity bounds for the convex case are less conservative than available bounds in the literature, but the non-convex complexity bounds are not novel for the class of optimisation problems considered.\n\n**Correct Answer:** C) The framework's ability to exploit the symmetric structure of the underlying matrix space leads to more efficient complexity bounds for both the convex and non-convex cases, making it a more effective approach for tuning gain matrices in model predictive controllers.\n\n**Explanation:** The correct answer, C, highlights the key advantage of the framework, which is the exploitation of the symmetric structure of the underlying matrix space to derive less conservative complexity bounds for both the convex and non-convex cases. This allows for more efficient optimisation and better performance in tuning gain matrices for model predictive controllers. The other options are incorrect because they either misrepresent the framework's advantages or fail to acknowledge the novel complexity bounds for the non-convex case."}, "9": {"documentation": {"title": "Dynamic Prize Linked Savings: Maximizing Savings and Managing Risk", "source": "Oisin Connolly", "docs_id": "1701.05176", "section": ["q-fin.ST", "q-fin.CP", "q-fin.RM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dynamic Prize Linked Savings: Maximizing Savings and Managing Risk. Prize linked savings accounts provide a return in the form of randomly chosen accounts receiving large cash prizes, in lieu of a guaranteed and uniform interest rate. This model became legal for American national banks upon bipartisan passage of the American Savings Promotion Act in December 2014, and many states have deregulated this option for state chartered banks and credit unions in recent years. Prize linked savings programs have unique appeal and proven societal benefits, but the product is still not available to the vast majority of Americans. There is demonstrated interest in these products, but the supply side may be the bottleneck, because the prevailing consensus is that prize linked savings primarily appeal to low income consumers. This paper examines a less common, dynamic prize, model of prize linked savings and shows why it might result in a larger average account size. The paper proposes three methods of managing risk under this model, and tests two of them using a Monte Carlo simulation. We conclude that both tested methods are effective at mitigating the most severe risks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary limitation of prize linked savings programs in the United States, according to the paper, and how does the proposed dynamic prize model address this limitation?\n\nA) The primary limitation is that prize linked savings programs are not available to all Americans due to regulatory barriers, and the dynamic prize model addresses this by increasing the supply of these products. However, this may still appeal primarily to low-income consumers.\n\nB) The primary limitation is that prize linked savings programs are not effective in managing risk, and the dynamic prize model addresses this by introducing new risk management methods. However, these methods may not be sufficient to mitigate the most severe risks.\n\nC) The primary limitation is that prize linked savings programs primarily appeal to low-income consumers, and the dynamic prize model addresses this by introducing a more diverse range of prizes that can appeal to a broader range of consumers. This can lead to a larger average account size.\n\nD) The primary limitation is that prize linked savings programs are not available to all Americans due to regulatory barriers, and the dynamic prize model addresses this by increasing the supply of these products. However, this may still lead to a concentration of prizes among a small number of accounts.\n\n**Correct Answer:** C) The primary limitation is that prize linked savings programs primarily appeal to low-income consumers, and the dynamic prize model addresses this by introducing a more diverse range of prizes that can appeal to a broader range of consumers. This can lead to a larger average account size.\n\n**Explanation:** The paper suggests that the dynamic prize model may result in a larger average account size because it introduces a more diverse range of prizes that can appeal to a broader range of consumers. This is in contrast to traditional prize linked savings programs, which may primarily appeal to low-income consumers. By introducing a more diverse range of prizes, the dynamic prize model can attract a wider range of customers and lead to a larger average account size."}, "10": {"documentation": {"title": "Relative Contagiousness of Emerging Virus Variants: An Analysis of\n  SARS-CoV-2 Alpha and Delta Variants", "source": "Peter Reinhard Hansen", "docs_id": "2110.00533", "section": ["econ.EM", "stat.AP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relative Contagiousness of Emerging Virus Variants: An Analysis of\n  SARS-CoV-2 Alpha and Delta Variants. We propose a simple dynamic model for estimating the relative contagiousness of two virus variants. Maximum likelihood estimation and inference is conveniently invariant to variation in the total number of cases over the sample period and can be expressed as a logistic regression. Using weekly Danish data we estimate the Alpha variant of SARS-CoV-2 to increase the reproduction number by a factor of 1.51 [CI 95%: 1.50, 1.53] relative to the ancestral variant. The Delta variant increases the reproduction number by a factor of 2.17 [CI 95%: 1.99, 2.36] relative to the Alpha variant and a factor of 3.28 [CI 95%: 3.01, 3.58] relative to the ancestral variant. Forecasting the proportion of an emerging virus variant is straight forward and we proceed to show how the effective reproduction number for the new variant can be estimated without contemporary sequencing results. This is useful for assessing the state of the pandemic in real time as we illustrate empirically with the inferred effective reproduction number for the Alpha variant."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: A dynamic model is proposed to estimate the relative contagiousness of two virus variants, SARS-CoV-2 Alpha and Delta. The model is based on logistic regression and is invariant to variation in the total number of cases over the sample period. Using weekly Danish data, the estimated increase in the reproduction number for the Alpha variant relative to the ancestral variant is:\n\nA) 1.50\nB) 1.51\nC) 1.53\nD) 1.99\n\nCorrect Answer: B) 1.51\n\nExplanation: The correct answer is B) 1.51, as the documentation states that the Alpha variant increases the reproduction number by a factor of 1.51 [CI 95%: 1.50, 1.53] relative to the ancestral variant. The other options are incorrect because they either provide a range of values (A and C) or a value that is not supported by the documentation (D)."}, "11": {"documentation": {"title": "Exclusive electroproduction of J/psi mesons at HERA", "source": "ZEUS Collaboration", "docs_id": "hep-ex/0404008", "section": ["hep-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exclusive electroproduction of J/psi mesons at HERA. The exclusive electroproduction of J/psi mesons, ep->epJ/psi, has been studied with the ZEUS detector at HERA for virtualities of the exchanged photon in the ranges 0.15<Q^2<0.8 GeV^2 and 2<Q^2<100 GeV^2 using integrated luminosities of 69 pb^-1 and 83 pb^-1, respectively.The photon-proton centre-of-mass energy was in the range 30<W<220 GeV and the squared four-momentum transfer at the proton vertex |t|<1.The cross sections and decay angular distributions are presented as functions of Q^2, W and t. The effective parameters of the Pomeron trajectory are in agreement with those found in J/psi photoproduction. The spin-density matrix elements, calculated from the decay angular distributions, are consistent with the hypothesis of s-channel helicity conservation. The ratio of the longitudinal to transverse cross sections, sigma_L/sigma_T, grows with Q^2, whilst no dependence on W or t is observed. The results are in agreement with perturbative QCD calculations and exhibit a strong sensitivity to the gluon distribution in the proton."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nExclusive electroproduction of J/\u03c8 mesons at HERA: A comparison of theoretical predictions and experimental results.\n\nA) The ratio of the longitudinal to transverse cross sections, \u03c3_L/\u03c3_T, is found to be independent of the photon-proton centre-of-mass energy W, in agreement with perturbative QCD calculations. However, the experimental results exhibit a strong sensitivity to the gluon distribution in the proton, which is not accounted for in the theoretical predictions.\n\nB) The spin-density matrix elements, calculated from the decay angular distributions, are consistent with the hypothesis of s-channel helicity conservation, but the effective parameters of the Pomeron trajectory are found to be in disagreement with those obtained in J/\u03c8 photoproduction.\n\nC) The experimental results are in agreement with perturbative QCD calculations, but the ratio of the longitudinal to transverse cross sections, \u03c3_L/\u03c3_T, is found to grow with Q^2, which is not predicted by the theoretical models.\n\nD) The gluon distribution in the proton is found to be a crucial factor in the experimental results, which exhibit a strong sensitivity to the gluon distribution, but the theoretical predictions are not sensitive to this parameter.\n\nCorrect Answer: C) The experimental results are in agreement with perturbative QCD calculations, but the ratio of the longitudinal to transverse cross sections, \u03c3_L/\u03c3_T, is found to grow with Q^2, which is not predicted by the theoretical models.\n\nExplanation: The correct answer is C) because the experimental results are indeed in agreement with perturbative QCD calculations, but the ratio of the longitudinal to transverse cross sections, \u03c3_L/\u03c3_T, is found to grow with Q^2, which is not predicted by the theoretical models. This discrepancy highlights the limitations of the current theoretical understanding of exclusive electroproduction of J/\u03c8 mesons at HERA."}, "12": {"documentation": {"title": "Using Machine Learning to Create an Early Warning System for Welfare\n  Recipients", "source": "Dario Sansone and Anna Zhu", "docs_id": "2011.12057", "section": ["econ.GN", "q-fin.EC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using Machine Learning to Create an Early Warning System for Welfare\n  Recipients. Using high-quality nation-wide social security data combined with machine learning tools, we develop predictive models of income support receipt intensities for any payment enrolee in the Australian social security system between 2014 and 2018. We show that off-the-shelf machine learning algorithms can significantly improve predictive accuracy compared to simpler heuristic models or early warning systems currently in use. Specifically, the former predicts the proportion of time individuals are on income support in the subsequent four years with greater accuracy, by a magnitude of at least 22% (14 percentage points increase in the R2), compared to the latter. This gain can be achieved at no extra cost to practitioners since the algorithms use administrative data currently available to caseworkers. Consequently, our machine learning algorithms can improve the detection of long-term income support recipients, which can potentially provide governments with large savings in accrued welfare costs."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The authors of the paper \"Using Machine Learning to Create an Early Warning System for Welfare Recipients\" claim that their machine learning algorithms can improve predictive accuracy by at least 22% compared to simpler heuristic models or early warning systems. However, they also state that this gain can be achieved at no extra cost to practitioners since the algorithms use administrative data currently available to caseworkers.\n\n**Which of the following statements accurately summarizes the authors' claim about the cost implications of their machine learning algorithms?**\n\nA) The algorithms require significant additional resources and infrastructure to implement.\nB) The algorithms can be used to identify long-term income support recipients at no extra cost to practitioners.\nC) The algorithms can be used to improve predictive accuracy, but this comes at the cost of increased administrative burden.\nD) The algorithms are not cost-effective and should not be implemented.\n\n**Correct Answer:** B) The algorithms can be used to identify long-term income support recipients at no extra cost to practitioners.\n\n**Explanation:** The correct answer is B) because the authors explicitly state that the machine learning algorithms can improve predictive accuracy at no extra cost to practitioners, as they use administrative data currently available to caseworkers. This means that the algorithms can provide valuable insights without requiring additional resources or infrastructure."}, "13": {"documentation": {"title": "Surface Manifestation of Stochastically Excited Internal Gravity Waves", "source": "Daniel Lecoanet, Matteo Cantiello, Evan H. Anders, Eliot Quataert,\n  Louis-Alexandre Couston, Mathieu Bouffard, Benjamin Favier, Michael Le Bars", "docs_id": "2105.04558", "section": ["astro-ph.SR", "physics.flu-dyn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Surface Manifestation of Stochastically Excited Internal Gravity Waves. Recent photometric observations of massive stars show ubiquitous low-frequency \"red-noise\" variability, which has been interpreted as internal gravity waves (IGWs). Simulations of IGWs generated by convection show smooth surface wave spectra, qualitatively matching the observed red-noise. On the other hand, theoretical calculations by Shiode et al (2013) and Lecoanet et al (2019) predict IGWs should manifest at the surface as regularly-spaced peaks associated with standing g-modes. In this work, we compare these theoretical approaches to simplified 2D numerical simulations. The simulations show g-mode peaks at their surface, and are in good agreement with Lecoanet et al (2019). The amplitude estimates of Shiode et al (2013) did not take into account the finite width of the g-mode peaks; after correcting for this finite width, we find good agreement with simulations. However, simulations need to be run for hundreds of convection turnover times for the peaks to become visible; this is a long time to run a simulation, but a short time in the life of a star. The final spectrum can be predicted by calculating the wave energy flux spectrum in much shorter simulations, and then either applying the theory of Shiode et al (2013) or Lecoanet et al (2019)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Compare and contrast the theoretical approaches of Shiode et al (2013) and Lecoanet et al (2019) to the manifestation of internal gravity waves (IGWs) at the surface of massive stars, and discuss the implications of the findings from the simplified 2D numerical simulations.\n\n**A)** Shiode et al (2013) and Lecoanet et al (2019) both predict that IGWs should manifest at the surface as regularly-spaced peaks associated with standing g-modes, but the former neglects the finite width of the g-mode peaks, leading to a discrepancy with the simulations.\n\n**B)** The simulations show that g-mode peaks are visible at the surface after running for hundreds of convection turnover times, which is a long time in the life of a star, but can be predicted by calculating the wave energy flux spectrum in much shorter simulations.\n\n**C)** The findings from the simulations are in good agreement with Lecoanet et al (2019), but Shiode et al (2013) require corrections for the finite width of the g-mode peaks to match the simulations.\n\n**D)** The theoretical approaches of Shiode et al (2013) and Lecoanet et al (2019) are mutually exclusive, with Shiode et al predicting a smooth surface wave spectrum and Lecoanet et al predicting regularly-spaced peaks associated with standing g-modes.\n\n**Correct Answer:** C) The findings from the simulations are in good agreement with Lecoanet et al (2019), but Shiode et al (2013) require corrections for the finite width of the g-mode peaks to match the simulations.\n\n**Explanation:** The correct answer is C) because the simulations show that g-mode peaks are visible at the surface after running for hundreds of convection turnover times, which is a long time in the life of a star, but can be predicted by calculating the wave energy flux spectrum in much shorter simulations. The simulations are in good agreement with Lecoanet et al (2019), but Shiode et al (2013) require corrections for the finite width of the g-mode peaks to match the simulations."}, "14": {"documentation": {"title": "Zero-Temperature Configurations of Short Odd-Numbered Classical Spin\n  Chains with Bilinear and Biquadratic Exchange Interactions", "source": "N. P. Konstantinidis", "docs_id": "1405.5931", "section": ["cond-mat.str-el"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Zero-Temperature Configurations of Short Odd-Numbered Classical Spin\n  Chains with Bilinear and Biquadratic Exchange Interactions. The lowest energy configurations of short odd open chains with classical spins are determined for antiferromagnetic bilinear and biquadratic nearest-neighbor exchange interactions. The zero field residual magnetization generates differences with the magnetic behavior of even chains, as the odd chain is like a small magnet for weak magnetic fields. The lowest energy configuration is calculated as a function of the total magnetization M, even for M less than the zero field residual magnetization. Analytic expressions and their proofs are provided for the threshold magnetic field needed to drive the system away from the antiferromagnetic configuration and the spin polar angles in its vicinity, when the biquadratic interaction is relatively weak. They are also given for the saturation magnetic field and the spin polar angles close to it. Finally, an analytic expression along with its proof is given for the maximum magnetization in zero magnetic field for stronger biquadratic interaction, where the lowest energy configuration is highly degenerate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the zero-field residual magnetization and the magnetic behavior of short odd open chains with classical spins, and how does it differ from that of even chains?\n\nA) The zero-field residual magnetization is directly proportional to the number of spins in the chain.\nB) The zero-field residual magnetization is inversely proportional to the number of spins in the chain.\nC) The zero-field residual magnetization is a threshold value that determines the transition from antiferromagnetic to ferromagnetic behavior in odd chains.\nD) The zero-field residual magnetization is a measure of the system's tendency to align its spins in the direction of the magnetic field.\n\nCorrect Answer: C) The zero-field residual magnetization is a threshold value that determines the transition from antiferromagnetic to ferromagnetic behavior in odd chains.\n\nExplanation: According to the documentation, the zero-field residual magnetization generates differences with the magnetic behavior of even chains, and the lowest energy configuration of short odd open chains is like a small magnet for weak magnetic fields. This suggests that the zero-field residual magnetization acts as a threshold value that determines the transition from antiferromagnetic to ferromagnetic behavior in odd chains."}, "15": {"documentation": {"title": "Measurement of $\\gamma$ rays from $^6$LiF tile as an inner wall of a\n  neutron-decay detector", "source": "J. Koga, S. Ieki, A. Kimura, M. Kitaguchi, R. Kitahara, K. Mishima, N.\n  Nagakura, T. Okudaira, H. Otono, H. M. Shimizu, N. Sumi, S. Takada, T.\n  Tomita, T. Yamada, T. Yoshioka", "docs_id": "2007.15302", "section": ["physics.ins-det", "hep-ex", "nucl-ex"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Measurement of $\\gamma$ rays from $^6$LiF tile as an inner wall of a\n  neutron-decay detector. A neutron lifetime measurement conducted at the Japan Proton Accelerator Research Complex (J-PARC) is counting the number of electrons from neutron decays with a time projection chamber (TPC). The $\\gamma$ rays produced in the TPC cause irreducible background events. To achieve the precise measurement, the inner walls of the TPC consist of $^6$Li-enriched lithium-fluoride ($^6$LiF) tiles to suppress the amount of $\\gamma$ rays. In order to estimate the amount of $\\gamma$ rays from the $^{6}{\\rm LiF}$ tile, prompt gamma ray analysis (PGA) measurements were performed using germanium detectors. We reconstructed the measured $\\gamma$-ray energy spectrum using a Monte Carlo simulation with the stripping method. Comparing the measured spectrum with a simulated one, the number of $\\gamma$ rays emitted from the$^{6}{\\rm LiF}$ tile was $(2.3^{+0.7}_{-0.3}) \\times 10^{-4}$ per incident neutron. This is $1.4^{+0.5}_{-0.2}$ times the value assumed for a mole fraction of the $^{6}{\\rm LiF}$ tile. We concluded that the amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is not more twice the originally assumed value."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** A neutron-decay detector at the Japan Proton Accelerator Research Complex (J-PARC) uses $^6$LiF tiles as an inner wall to suppress $\\gamma$ rays produced in the time projection chamber (TPC). The prompt gamma ray analysis (PGA) measurements revealed that the number of $\\gamma$ rays emitted from the $^{6}{\\rm LiF}$ tile is $(2.3^{+0.7}_{-0.3}) \\times 10^{-4}$ per incident neutron. However, the original assumption for the mole fraction of the $^{6}{\\rm LiF}$ tile was $1.4^{+0.5}_{-0.2}$ times the actual value. What can be concluded about the amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile?\n\nA){The amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is more than twice the originally assumed value.}\nB){The amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is not more than twice the originally assumed value.}\nC){The amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is equal to the originally assumed value.}\nD){The amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is less than the originally assumed value.}\n\n**Correct Answer:** B){The amount of $\\gamma$ rays produced from the $^{6}{\\rm LiF}$ tile is not more than twice the originally assumed value.}\n\n**Explanation:** The prompt gamma ray analysis revealed that the number of $\\gamma$ rays emitted from the $^{6}{\\rm LiF}$ tile is $(2.3^{+0.7}_{-0.3}) \\times 10^{-4}$ per incident neutron, which is $1.4^{+0.5}_{-0.2}$ times the value assumed for a mole fraction of the $^{6}{\\rm LiF}$ tile. This means that the actual value is less than or equal to the assumed value, not more than twice the assumed value. Therefore, the correct answer is B."}, "16": {"documentation": {"title": "Studies of Thermally Unstable Accretion Disks around Black Holes with\n  Adaptive Pseudospectral Domain Decomposition Method. II. Limit-Cycle Behavior\n  in accretion disks around Kerr black holes", "source": "Li Xue, Aleksander S\\k{a}dowski, Marek A. Abramowicz and Ju-Fu Lu", "docs_id": "1105.4534", "section": ["astro-ph.HE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Studies of Thermally Unstable Accretion Disks around Black Holes with\n  Adaptive Pseudospectral Domain Decomposition Method. II. Limit-Cycle Behavior\n  in accretion disks around Kerr black holes. For the first time ever, we derive equations governing the time-evolution of fully relativistic slim accretion disks in the Kerr metric, and numerically construct their detailed non-stationary models. We discuss applications of these general results to a possible limit-cycle behavior of thermally unstable disks. Our equations and numerical method are applicable in a wide class of possible viscosity prescriptions, but in this paper we use a diffusive form of the \"standard alpha prescription\" that assumes the viscous torque is proportional to the total pressure. In this particular case, we find that the parameters which dominate the limit-cycle properties are the mass-supply rate and the value of the alpha-viscosity parameter. Although the duration of the cycle (or the outburst) does not exhibit any clear dependence on the black hole spin, the maximal outburst luminosity (in the Eddington units) is positively correlated with the spin value. We suggest a simple method for a rough estimate of the black hole spin based on the maximal luminosity and the ratio of outburst to cycle durations. We also discuss a temperature-luminosity relation for the Kerr black hole accretion discs limit-cycle. Based on these results we discuss the limit-cycle behavior observed in microquasar GRS 1915+105. We also extend this study to several non-standard viscosity prescriptions, including a \"delayed heating\" prescription recently stimulated by the recent MHD simulations of accretion disks."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the relationship between the maximal outburst luminosity and the black hole spin in the context of thermally unstable accretion disks around Kerr black holes?\n\nA) The maximal outburst luminosity is negatively correlated with the black hole spin.\nB) The maximal outburst luminosity is positively correlated with the black hole spin.\nC) The maximal outburst luminosity is independent of the black hole spin.\nD) The maximal outburst luminosity is inversely proportional to the black hole spin.\n\nCorrect Answer: B) The maximal outburst luminosity is positively correlated with the black hole spin.\n\nExplanation: According to the documentation, the maximal outburst luminosity (in Eddington units) is positively correlated with the spin value of the Kerr black hole. This means that as the spin value increases, the maximal outburst luminosity also increases. This relationship is discussed in the paper as a key finding in the study of thermally unstable accretion disks around Kerr black holes."}, "17": {"documentation": {"title": "UPDATE February 2012 - The Food Crises: Predictive validation of a\n  quantitative model of food prices including speculators and ethanol\n  conversion", "source": "Marco Lagi, Yavni Bar-Yam, Karla Z. Bertrand and Yaneer Bar-Yam", "docs_id": "1203.1313", "section": ["physics.soc-ph", "q-fin.GN", "q-fin.ST"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UPDATE February 2012 - The Food Crises: Predictive validation of a\n  quantitative model of food prices including speculators and ethanol\n  conversion. Increases in global food prices have led to widespread hunger and social unrest---and an imperative to understand their causes. In a previous paper published in September 2011, we constructed for the first time a dynamic model that quantitatively agreed with food prices. Specifically, the model fit the FAO Food Price Index time series from January 2004 to March 2011, inclusive. The results showed that the dominant causes of price increases during this period were investor speculation and ethanol conversion. The model included investor trend following as well as shifting between commodities, equities and bonds to take advantage of increased expected returns. Here, we extend the food prices model to January 2012, without modifying the model but simply continuing its dynamics. The agreement is still precise, validating both the descriptive and predictive abilities of the analysis. Policy actions are needed to avoid a third speculative bubble that would cause prices to rise above recent peaks by the end of 2012."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** The authors of the study on food prices model extend their previous work to January 2012, without modifying the model. What is the primary reason for this extension, according to the study?\n\nA) To test the model's predictive power on a new dataset\nB) To validate the model's ability to capture the effects of investor speculation and ethanol conversion on food prices\nC) To account for the impact of policy actions on food prices\nD) To update the model's parameters to reflect changes in global economic conditions\n\n**Correct Answer:** B) To validate the model's ability to capture the effects of investor speculation and ethanol conversion on food prices\n\n**Explanation:** The study extends the previous work to January 2012 without modifying the model, which suggests that the primary reason is to validate the model's ability to capture the effects of investor speculation and ethanol conversion on food prices. This is because the model was previously validated to fit the FAO Food Price Index time series from January 2004 to March 2011, and the extension aims to demonstrate that the model can continue to accurately predict food prices without any changes to the underlying dynamics.\n\n**Candidate A:** This option is incorrect because while the extension does test the model's predictive power, it is not the primary reason for the extension.\n\n**Candidate B:** This is the correct answer, as explained above.\n\n**Candidate C:** This option is incorrect because the study does not mention the impact of policy actions on food prices as a reason for the extension.\n\n**Candidate D:** This option is incorrect because the study does not mention any changes to the model's parameters or updates to reflect changes in global economic conditions as a reason for the extension."}, "18": {"documentation": {"title": "On the role of data in PAC-Bayes bounds", "source": "Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino,\n  Daniel M. Roy", "docs_id": "2006.10929", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the role of data in PAC-Bayes bounds. The dominant term in PAC-Bayes bounds is often the Kullback--Leibler divergence between the posterior and prior. For so-called linear PAC-Bayes risk bounds based on the empirical risk of a fixed posterior kernel, it is possible to minimize the expected value of the bound by choosing the prior to be the expected posterior, which we call the oracle prior on the account that it is distribution dependent. In this work, we show that the bound based on the oracle prior can be suboptimal: In some cases, a stronger bound is obtained by using a data-dependent oracle prior, i.e., a conditional expectation of the posterior, given a subset of the training data that is then excluded from the empirical risk term. While using data to learn a prior is a known heuristic, its essential role in optimal bounds is new. In fact, we show that using data can mean the difference between vacuous and nonvacuous bounds. We apply this new principle in the setting of nonconvex learning, simulating data-dependent oracle priors on MNIST and Fashion MNIST with and without held-out data, and demonstrating new nonvacuous bounds in both cases."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of PAC-Bayes bounds, what is the significance of using a data-dependent oracle prior, and how does it differ from the traditional oracle prior approach?\n\n**A)** A data-dependent oracle prior is equivalent to the traditional oracle prior approach, and both yield the same bounds.\n\n**B)** Using a data-dependent oracle prior can lead to vacuous bounds, whereas the traditional oracle prior approach yields nonvacuous bounds.\n\n**C)** A data-dependent oracle prior is a heuristic that can be used to learn a prior distribution from the training data, but it is not essential for obtaining optimal bounds.\n\n**D)** In the setting of nonconvex learning, using a data-dependent oracle prior can mean the difference between vacuous and nonvacuous bounds, and it is a new principle that has been demonstrated in simulations on MNIST and Fashion MNIST.\n\n**Correct Answer:** D) In the setting of nonconvex learning, using a data-dependent oracle prior can mean the difference between vacuous and nonvacuous bounds, and it is a new principle that has been demonstrated in simulations on MNIST and Fashion MNIST.\n\n**Explanation:** The correct answer is D) because the question asks about the significance of using a data-dependent oracle prior in the context of nonconvex learning, and the documentation states that using data can mean the difference between vacuous and nonvacuous bounds. Additionally, the documentation mentions that this principle has been demonstrated in simulations on MNIST and Fashion MNIST, which supports the correctness of answer D."}, "19": {"documentation": {"title": "Identification and Estimation of Spillover Effects in Randomized\n  Experiments", "source": "Gonzalo Vazquez-Bare", "docs_id": "1711.02745", "section": ["econ.EM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identification and Estimation of Spillover Effects in Randomized\n  Experiments. I study identification, estimation and inference for spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used reduced-form linear-in-means (RF-LIM) regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the RF-LIM coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the paper \"Identification and Estimation of Spillover Effects in Randomized Experiments\", what is the primary limitation of the commonly-used reduced-form linear-in-means (RF-LIM) regression approach in estimating spillover effects in experiments where units' outcomes may depend on the treatment assignments of other units within a group?\n\n**A)** RF-LIM regression fails to account for the correlation between treatment assignments and spillover effects.\n**B)** RF-LIM regression assumes a linear relationship between treatment assignments and spillover effects, which may not hold in reality.\n**C)** RF-LIM regression only estimates the weighted sum of spillover effects, without accounting for the combination of direct and spillover effects.\n**D)** RF-LIM regression is inconsistent and asymptotically biased due to the precise relationship between the number of parameters of interest, the total sample size, and the treatment assignment mechanism.\n\n**Correct Answer:** C) RF-LIM regression only estimates the weighted sum of spillover effects, without accounting for the combination of direct and spillover effects.\n\n**Explanation:** The paper highlights that the RF-LIM regression approach identifies a weighted sum of spillover effects with some negative weights, but fails to account for the combination of direct and spillover effects entering with different signs. This limitation is addressed by the proposed nonparametric estimators, which can flexibly account for both direct and spillover effects."}, "20": {"documentation": {"title": "Active particles in reactive disordered media: how does adsorption\n  affects diffusion?", "source": "R. Salgado-Garc\\'ia", "docs_id": "2105.10840", "section": ["cond-mat.stat-mech", "cond-mat.dis-nn"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Active particles in reactive disordered media: how does adsorption\n  affects diffusion?. In this work we study analytically and numerically the transport properties of non-interacting active particles moving on a $d$-dimensional disordered media. The disorder in the space is modeled by means of a set of non-overlapping spherical obstacles. We assume that obstacles are reactive in the sense that they react in the presence of the particles in an attractive manner: when the particle collides with an obstacle, it is attached during a random time (adsorption time), i.e., it gets adsorbed by an obstacle; thereafter the particle is detached from the obstacle and continues its motion in a random direction. We give an analytical formula for the effective diffusion coefficient when the mean adsorption time is finite. When the mean adsorption time is infinite, we show that the system undergoes a transition from a normal to anomalous diffusion regime. We also show that another transition takes place in the mean number of adsorbed particles: in the anomalous diffusion phase all the particles become adsorbed in the average. We show that the fraction of adsorbed particles, seen as an order parameter of the system, undergoes a second-order-like phase transition, because the fraction of adsorbed particles is not differentiable but changes continuously as a function of a parameter of the model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Analyze the transport properties of non-interacting active particles moving on a $d$-dimensional disordered media with reactive obstacles. How does the mean adsorption time affect the diffusion regime, and what is the nature of the phase transition in the fraction of adsorbed particles?\n\n**A)** The mean adsorption time affects the diffusion regime by causing a transition from normal to anomalous diffusion, but the fraction of adsorbed particles remains constant as a function of the parameter.\n\n**B)** The mean adsorption time causes a transition from normal to anomalous diffusion, and the fraction of adsorbed particles undergoes a second-order-like phase transition, where the fraction of adsorbed particles changes continuously as a function of the parameter.\n\n**C)** The mean adsorption time has no effect on the diffusion regime, and the fraction of adsorbed particles remains constant and differentiable as a function of the parameter.\n\n**D)** The mean adsorption time causes a transition from anomalous to normal diffusion, and the fraction of adsorbed particles remains constant and differentiable as a function of the parameter.\n\n**Correct Answer:** B) The mean adsorption time causes a transition from normal to anomalous diffusion, and the fraction of adsorbed particles undergoes a second-order-like phase transition, where the fraction of adsorbed particles changes continuously as a function of the parameter.\n\n**Explanation:** The correct answer is based on the analysis of the documentation, which states that the mean adsorption time causes a transition from normal to anomalous diffusion, and that the fraction of adsorbed particles undergoes a second-order-like phase transition. This is because the fraction of adsorbed particles is not differentiable but changes continuously as a function of the parameter. The other options are incorrect because they either misrepresent the effect of the mean adsorption time on the diffusion regime or the nature of the phase transition in the fraction of adsorbed particles."}, "21": {"documentation": {"title": "Understanding the Energy and Precision Requirements for Online Learning", "source": "Charbel Sakr, Ameya Patil, Sai Zhang, Yongjune Kim, Naresh Shanbhag", "docs_id": "1607.00669", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Understanding the Energy and Precision Requirements for Online Learning. It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency. The precision requirements for the training algorithm are also important for systems that learn on-the-fly. Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations. These works suffer from two key limitations. First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision. Second, prior works are empirical studies. In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM). Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier. Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained. These bounds are validated using both synthetic and the UCI breast cancer dataset. Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the two key limitations of prior work on quantizing data and hyperparameters in learning systems, and how do the proposed analytical lower bounds in this article address these limitations?\n\n**A)** Prior work assumes uniform precision for the classifier and training algorithm, and neglects to consider the impact of precision on energy consumption. The proposed bounds address these limitations by deriving lower bounds on precision requirements in terms of desired classification accuracy and hyperparameter precision.\n\n**B)** Prior work relies on empirical studies, whereas the proposed bounds are based on theoretical derivations. The proposed bounds address the limitations of prior work by providing a more comprehensive understanding of the precision requirements for data and hyperparameters.\n\n**C)** Prior work fails to consider the impact of precision on latency, whereas the proposed bounds address this limitation by deriving lower bounds on precision requirements in terms of desired classification accuracy and hyperparameter precision.\n\n**D)** Prior work assumes that quantizing data and hyperparameters incurs a significant penalty in classification accuracy, whereas the proposed bounds address this limitation by showing that lower bounds on precision requirements can be achieved without sacrificing accuracy.\n\n**Correct Answer:** A) Prior work assumes uniform precision for the classifier and training algorithm, and neglects to consider the impact of precision on energy consumption. The proposed bounds address these limitations by deriving lower bounds on precision requirements in terms of desired classification accuracy and hyperparameter precision.\n\n**Explanation:** The correct answer is A) because the article highlights that prior work assumes uniform precision for the classifier and training algorithm, which is a limitation. Additionally, the article states that prior work neglects to consider the impact of precision on energy consumption, which is another limitation. The proposed bounds in the article address these limitations by deriving lower bounds on precision requirements in terms of desired classification accuracy and hyperparameter precision."}, "22": {"documentation": {"title": "Tailoring the nucleation of domain walls along multi-segmented\n  cylindrical nanoelements", "source": "R. F. Neumann, M. Bahiana, S. Allende, D. Altbir, D. G\\\"orlitz, K.\n  Nielsch", "docs_id": "1410.5742", "section": ["cond-mat.mes-hall", "physics.comp-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tailoring the nucleation of domain walls along multi-segmented\n  cylindrical nanoelements. The magnetization reversal of three-segment cylindrical nanoelements comprising of alternating nanowire and nanotube sections is investigated by means of Monte Carlo simulations. Such nanoelements may feature a three-state behaviour with an intermediate plateau in the hysteresis curve due to a metastable pinning of the domain walls at the wire-tube interfaces. It turns out that vortex as well as transverse domain walls contribute to the magnetization reversal. By varying the geometric parameters, the sequence, or the material of the elements the nucleation location of domain walls, as well as their nucleation field, can be tailored. Especially interesting is the novel possibility to drive domain walls coherently in the same or in opposite directions by changing the geometry of the hybrid nanoelement. This important feature adds additional flexibility to the construction of logical devices based on domain wall movement. Another prominent outcome is that domain walls can be nucleated near the centre of the element and then traverse to the outer tips of the cylindrical structure when the applied field is increased, which also opens the possibility to use these three-segment nanoelements for the field induced delivery of domain walls as substitutes for large nucleation pads."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of tailoring the nucleation of domain walls along multi-segmented cylindrical nanoelements, as investigated in the study, and how does it relate to the construction of logical devices?\n\nA) The ability to control the direction of domain wall movement, allowing for the creation of complex logical devices.\nB) The potential to reduce the energy required for domain wall nucleation, leading to more efficient magnetic devices.\nC) The possibility to drive domain walls coherently in the same or opposite directions, enabling the creation of novel logical devices.\nD) The ability to increase the stability of domain walls, reducing the risk of magnetic instability.\n\nCorrect Answer: C) The possibility to drive domain walls coherently in the same or opposite directions, enabling the creation of novel logical devices.\n\nExplanation: The study highlights the novel possibility of driving domain walls coherently in the same or opposite directions by changing the geometry of the hybrid nanoelement. This feature adds additional flexibility to the construction of logical devices based on domain wall movement, making option C the correct answer. Options A and D are related to the topic but are not the primary advantage, while option B is not mentioned in the study."}, "23": {"documentation": {"title": "(Unintended) Consequences of export restrictions on medical goods during\n  the Covid-19 pandemic", "source": "Marco Grassia, Giuseppe Mangioni, Stefano Schiavo, Silvio Traverso", "docs_id": "2007.11941", "section": ["physics.soc-ph", "econ.GN", "q-fin.EC", "q-fin.TR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "(Unintended) Consequences of export restrictions on medical goods during\n  the Covid-19 pandemic. In the first half of 2020, several countries have responded to the challenges posed by the Covid-19 pandemic by restricting their export of medical supplies. Such measures are meant to increase the domestic availability of critical goods, and are commonly used in times of crisis. Yet, not much is known about their impact, especially on countries imposing them. Here we show that export bans are, by and large, counterproductive. Using a model of shock diffusion through the network of international trade, we simulate the impact of restrictions under different scenarios. We observe that while they would be beneficial to a country implementing them in isolation, their generalized use makes most countries worse off relative to a no-ban scenario. As a corollary, we estimate that prices increase in many countries imposing the restrictions. We also find that the cost of restraining from export bans is small, even when others continue to implement them. Finally, we document a change in countries' position within the international trade network, suggesting that export bans have geopolitical implications."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the unintended consequences of export restrictions on medical goods during the Covid-19 pandemic, as demonstrated in the study. What is the primary finding regarding the impact of export bans on countries imposing them, and how does this relate to the overall effect on the global economy?\n\n**A)** Export bans are beneficial to countries imposing them, as they increase domestic availability of critical goods and reduce reliance on international trade.\n**B)** Export bans are counterproductive, leading to increased prices and reduced economic growth for countries imposing them, while also having geopolitical implications.\n**C)** The study suggests that export bans have no significant impact on countries imposing them, and that the benefits of increased domestic availability outweigh the costs.\n**D)** The study finds that export bans are only effective in reducing prices and increasing economic growth for countries imposing them, but have no impact on other countries.\n\n**Correct Answer:** B) Export bans are counterproductive, leading to increased prices and reduced economic growth for countries imposing them, while also having geopolitical implications.\n\n**Explanation:** The study finds that export bans are counterproductive, as they lead to increased prices and reduced economic growth for countries imposing them. This is because the restrictions reduce the availability of critical goods, leading to shortages and price increases. Additionally, the study suggests that the generalized use of export bans makes most countries worse off relative to a no-ban scenario, indicating that the benefits of increased domestic availability are outweighed by the costs. The geopolitical implications of export bans are also highlighted, as the study documents a change in countries' position within the international trade network."}, "24": {"documentation": {"title": "A New Frame Synchronization Algorithm for Linear Periodic Channels with\n  Memory -- Full Version", "source": "Oren Kolaman and Ron Dabora", "docs_id": "2007.05571", "section": ["eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Frame Synchronization Algorithm for Linear Periodic Channels with\n  Memory -- Full Version. Identifying the start time of a sequence of symbols received at the receiver, commonly referred to as \\emph{frame synchronization}, is a critical task for achieving good performance in digital communications systems employing time-multiplexed transmission. In this work we focus on \\emph{frame synchronization} for linear channels with memory in which the channel impulse response is periodic and the additive Gaussian noise is correlated and cyclostationary. Such channels appear in many communications scenarios, including narrowband power line communications and interference-limited wireless communications. We derive frame synchronization algorithms based on simplifications of the optimal likelihood-ratio test, assuming the channel impulse response is unknown at the receiver, which is applicable to many practical scenarios. The computational complexity of each of the derived algorithms is characterized, and a procedure for selecting nearly optimal synchronization sequences is proposed. The algorithms derived in this work achieve better performance than the noncoherent correlation detector, and, in fact, facilitate a controlled tradeoff between complexity and performance."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary assumption made in the derivation of the frame synchronization algorithms presented in the paper, and how does it relate to the applicability of the algorithms in practical scenarios?\n\nA) The channel impulse response is perfectly known at the receiver, and the algorithms are designed to optimize performance under this assumption.\nB) The channel impulse response is unknown at the receiver, and the algorithms are designed to simplify the optimal likelihood-ratio test under this assumption.\nC) The additive Gaussian noise is perfectly correlated and cyclostationary, and the algorithms are designed to optimize performance under this assumption.\nD) The channel impulse response is periodic and the additive Gaussian noise is correlated and cyclostationary, but the algorithms are designed to assume a non-periodic channel impulse response.\n\n**Correct Answer:** B) The channel impulse response is unknown at the receiver, and the algorithms are designed to simplify the optimal likelihood-ratio test under this assumption.\n\n**Explanation:** The paper states that the algorithms are derived based on simplifications of the optimal likelihood-ratio test, assuming the channel impulse response is unknown at the receiver. This assumption is crucial in many practical scenarios, as it allows the algorithms to be designed to optimize performance without requiring prior knowledge of the channel impulse response. The correct answer highlights this key assumption and its relevance to the applicability of the algorithms."}, "25": {"documentation": {"title": "Complex trajectories in chaotic dynamical tunneling", "source": "D.G. Levkov, A.G. Panin, S.M. Sibiryakov", "docs_id": "nlin/0701063", "section": ["nlin.CD", "hep-th", "math-ph", "math.MP", "physics.chem-ph", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complex trajectories in chaotic dynamical tunneling. We develop the semiclassical method of complex trajectories in application to chaotic dynamical tunneling. First, we suggest a systematic numerical technique for obtaining complex tunneling trajectories by the gradual deformation of the classical ones. This provides a natural classification of the tunneling solutions. Second, we present a heuristic procedure for sorting out the least suppressed trajectory. As an illustration, we apply our technique to the process of chaotic tunneling in a quantum mechanical model with two degrees of freedom. Our analysis reveals rich dynamics of the system. At the classical level, there exists an infinite set of unstable solutions forming a fractal structure. This structure is inherited by the complex tunneling paths and plays the central role in the semiclassical study. The process we consider exhibits the phenomenon of optimal tunneling: the suppression exponent of the tunneling probability has a local minimum at a certain energy which is thus (locally) the optimal energy for tunneling. We test the proposed method by comparison of the semiclassical results with the results of the exact quantum computations and find a good agreement."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary contribution of the proposed method for complex trajectories in chaotic dynamical tunneling, and how does it relate to the fractal structure of unstable solutions at the classical level?\n\nA) The method provides a way to classify tunneling solutions based on their complexity, but it does not address the fractal structure of unstable solutions.\nB) The method suggests a heuristic procedure for sorting out the least suppressed trajectory, but it does not provide a systematic approach to understanding the fractal structure of unstable solutions.\nC) The method provides a systematic numerical technique for obtaining complex tunneling trajectories, which is inherited by the fractal structure of unstable solutions at the classical level.\nD) The method presents a heuristic procedure for sorting out the least suppressed trajectory, but it does not address the fractal structure of unstable solutions at the classical level.\n\nCorrect Answer: C) The method provides a systematic numerical technique for obtaining complex tunneling trajectories, which is inherited by the fractal structure of unstable solutions at the classical level.\n\nExplanation: The correct answer is C) because the proposed method provides a systematic numerical technique for obtaining complex tunneling trajectories, which is a key contribution of the paper. Additionally, the paper states that the fractal structure of unstable solutions at the classical level is inherited by the complex tunneling paths, which supports the idea that the method provides a way to understand the fractal structure of unstable solutions."}, "26": {"documentation": {"title": "Gassmann Theory Applies to Nanoporous Media", "source": "Gennady Y. Gor and Boris Gurevich", "docs_id": "1710.05216", "section": ["physics.geo-ph", "cond-mat.soft", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gassmann Theory Applies to Nanoporous Media. Recent progress in extraction of unconventional hydrocarbon resources has ignited the interest in the studies of nanoporous media. Since many thermodynamic and mechanical properties of nanoscale solids and fluids differ from the analogous bulk materials, it is not obvious whether wave propagation in nanoporous media can be described using the same framework as in macroporous media. Here we test the validity of Gassmann equation using two published sets of ultrasonic measurements for a model nanoporous medium, Vycor glass, saturated with two different fluids, argon and n-hexane. Predictions of the Gassmann theory depend on the bulk and shear moduli of the dry samples, which are known from ultrasonic measurements, and the bulk moduli of the solid and fluid constituents. The solid bulk modulus can be estimated from adsorption-induced deformation or from elastic effective medium theory. The fluid modulus can be calculated according to the Tait-Murnaghan equation at the solvation pressure in the pore. Substitution of these parameters into the Gassmann equation provides predictions consistent with measured data. Our findings set up a theoretical framework for investigation of fluid-saturated nanoporous media using ultrasonic elastic wave propagation."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary assumption underlying the application of the Gassmann equation to nanoporous media, and how does it differ from the analogous assumption for macroporous media?\n\nA) The Gassmann equation assumes that the bulk and shear moduli of the dry sample are equal to those of the fluid, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the solid.\n\nB) The Gassmann equation assumes that the bulk and shear moduli of the dry sample are equal to those of the solid, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the fluid.\n\nC) The Gassmann equation assumes that the bulk and shear moduli of the dry sample are equal to those of the fluid, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the solid.\n\nD) The Gassmann equation assumes that the bulk and shear moduli of the dry sample are equal to those of the solid, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the fluid, but only for the solid constituent.\n\nCorrect Answer: B) The Gassmann equation assumes that the bulk and shear moduli of the dry sample are equal to those of the solid, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the fluid.\n\nExplanation: The Gassmann equation is a theoretical framework for describing wave propagation in nanoporous media. The primary assumption underlying its application is that the bulk and shear moduli of the dry sample are equal to those of the solid constituent, whereas for macroporous media, the bulk and shear moduli are assumed to be equal to those of the fluid. This assumption allows for the calculation of the fluid modulus using the Tait-Murnaghan equation at the solvation pressure in the pore. The correct answer, B, highlights this key difference between the Gassmann equation for nanoporous media and macroporous media."}, "27": {"documentation": {"title": "How to Test the Existence of the Early Parton Cascade Using Photon HBT\n  Correlations?", "source": "Daniel Ferenc", "docs_id": "nucl-ex/9810002", "section": ["nucl-ex", "hep-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "How to Test the Existence of the Early Parton Cascade Using Photon HBT\n  Correlations?. We report on a possible application of the HBT phenomenon in testing the existence of two hypothetical phenomena. First, it is argued that the existence of a rapidly developing parton cascade in the earliest stages of a high energy nuclear collision process can be tested by studying two-photon HBT correlations over a wide longitudinal momentum scale - corresponding to the early photon emission time from the hypothetical parton system. This method provides the needed selectivity for the early emitted photons, since the photons emitted at later times correlate over progressively narrower momentum scales. Second, in a similar way we argue that the existence of a hypothetic dark matter candidate, the Weakly Interacting Massive Particle (WIMP), may be tested by studying HBT correlations of cosmic gamma rays at a relatively long detection time scale - corresponding to the very narrow spectral line of the photons emerging from WIMP annihilations. Background photons leave no signature since they essentially do not correlate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary advantage of studying two-photon HBT correlations in high energy nuclear collisions to test the existence of a rapidly developing parton cascade?\n\nA) It allows for the detection of photons emitted at very late times, which would be difficult to distinguish from background photons.\nB) It enables the measurement of HBT correlations over a wide longitudinal momentum scale, corresponding to the early photon emission time from the hypothetical parton system.\nC) It provides a way to distinguish between photons emitted by the parton cascade and those emitted by the strong interaction.\nD) It allows for the study of HBT correlations in a relatively short detection time scale, which would be more suitable for detecting cosmic gamma rays.\n\nCorrect Answer: B) It enables the measurement of HBT correlations over a wide longitudinal momentum scale, corresponding to the early photon emission time from the hypothetical parton system.\n\nExplanation: The correct answer is B) because the question specifically states that the method provides the needed selectivity for the early emitted photons, which is achieved by studying HBT correlations over a wide longitudinal momentum scale. This allows for the detection of photons emitted at early times, which would be difficult to distinguish from background photons. The other options are incorrect because they do not accurately describe the primary advantage of studying two-photon HBT correlations in this context."}, "28": {"documentation": {"title": "Spin-orbit torque switching without external field with a ferromagnetic\n  exchange-biased coupling layer", "source": "Yong-Chang Lau, Davide Betto, Karsten Rode, JMD Coey, Plamen Stamenov", "docs_id": "1511.05773", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spin-orbit torque switching without external field with a ferromagnetic\n  exchange-biased coupling layer. Magnetization reversal of a perpendicular ferromagnetic free layer by spin-orbit torque (SOT) is an attractive alternative to spin-transfer torque (STT) switching in magnetic random-access memory (MRAM) where the write process involves passing a high current across an ultrathin tunnel barrier. A small symmetry-breaking bias field is usually needed for deterministic SOT switching but it is impractical to generate the field externally for spintronic applications. Here, we demonstrate robust zero-field SOT switching of a perpendicular Co90Fe10 (CoFe) free layer where the symmetry is broken by magnetic coupling to a second in-plane exchange-biased CoFe layer via a nonmagnetic Ru spacer. The preferred magnetic state of the free layer is determined by the current polarity and the nature of the interlayer exchange coupling (IEC). Our strategy offers a scalable solution to realize bias-field-free SOT switching that can lead to a generation of SOT-based devices, that combine high storage density and endurance with potentially low power consumption."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the proposed spin-orbit torque switching method without external field, as demonstrated in the study, and how does it compare to traditional spin-transfer torque (STT) switching in MRAM?\n\nA) The method eliminates the need for a high current to switch the magnetization, reducing power consumption.\nB) The method uses a nonmagnetic Ru spacer to break the symmetry, allowing for deterministic switching without an external bias field.\nC) The method enables the use of a ferromagnetic exchange-biased coupling layer, improving the storage density and endurance of MRAM devices.\nD) The method relies on a small symmetry-breaking bias field, which is impractical to generate externally.\n\nCorrect Answer: B) The method uses a nonmagnetic Ru spacer to break the symmetry, allowing for deterministic switching without an external bias field.\n\nExplanation: The correct answer is B) because the study demonstrates that the proposed method uses a nonmagnetic Ru spacer to break the symmetry, allowing for deterministic switching without the need for an external bias field. This is a key advantage of the method, as it eliminates the need for a small symmetry-breaking bias field, which is impractical to generate externally. The other options are incorrect because they do not accurately describe the primary advantage of the proposed method. Option A is incorrect because the method does not eliminate the need for a high current to switch the magnetization. Option C is incorrect because the method does not rely on a ferromagnetic exchange-biased coupling layer. Option D is incorrect because the method does not rely on a small symmetry-breaking bias field."}, "29": {"documentation": {"title": "Features in Dark Energy Equation of State and Modulations in the Hubble\n  Diagram", "source": "Jun-Qing Xia, Gong-Bo Zhao, Hong Li, Bo Feng and Xinmin Zhang", "docs_id": "astro-ph/0605366", "section": ["astro-ph", "gr-qc", "hep-ph", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Features in Dark Energy Equation of State and Modulations in the Hubble\n  Diagram. We probe the time dependence of the dark energy equation of state (EOS) in light of three-year WMAP (WMAP3) and the combination with other tentative cosmological observations from galaxy clustering (SDSS) and Type Ia Supernova (SNIa). We mainly focus on cases where the EOS is oscillating or with local bumps. By performing a global analysis with the Markov Chain Monte Carlo (MCMC) method, we find the current observations, in particular the WMAP3 + SDSS data combination, allow large oscillations of the EOS which can leave oscillating features on the (residual) Hubble diagram, and such oscillations are potentially detectable by future observations like SNAP, or even by the CURRENTLY ONGOING SNIa observations. Local bumps of dark energy EOS can also leave imprints on CMB, LSS and SNIa. In cases where the bumps take place at low redshifts and the effective EOS are close to -1, CMB and LSS observations cannot give constraints on such possibilities. However, geometrical observations like (future) SNIa can possibly detect such features. On the other hand when the local bumps take place at higher redshifts beyond the detectability of SNIa, future precise observations like Gamma-ray bursts, CMB and LSS may possibly detect such features. In particular, we find that bump-like dark energy EOS on high redshifts might be responsible for the features of WMAP on ranges l \\sim 30-50, which is interesting and deserves addressing further."}, "answer": "Here is the difficult multi-form question:\n\nWhat are the implications of oscillating or local bump-like features in the dark energy equation of state (EOS) on the detectability of such features by future observations, and how might these features be constrained or detected by different types of observations?\n\nA) The oscillating features in the EOS would be detectable by future observations of Type Ia Supernovae (SNIa) at low redshifts, but not by future observations of Gamma-ray bursts, CMB, and LSS.\n\nB) The local bump-like features in the EOS would be constrained by future observations of CMB and LSS, but not by future observations of SNIa, which would be unable to detect such features at high redshifts.\n\nC) The oscillating features in the EOS would be detectable by future observations of SNIa at high redshifts, but not by future observations of CMB and LSS, which would be unable to constrain such features.\n\nD) The local bump-like features in the EOS would be detectable by future observations of SNIa at low redshifts, and would also be constrained by future observations of CMB, LSS, and Gamma-ray bursts.\n\nCorrect Answer: D) The local bump-like features in the EOS would be detectable by future observations of SNIa at low redshifts, and would also be constrained by future observations of CMB, LSS, and Gamma-ray bursts.\n\nExplanation: The correct answer is D) because the text states that local bumps of dark energy EOS can leave imprints on CMB, LSS, and SNIa, and that geometrical observations like (future) SNIa can possibly detect such features. Additionally, the text states that bump-like dark energy EOS on high redshifts might be responsible for the features of WMAP on ranges l \u223c 30-50, which is interesting and deserves addressing further. This suggests that future observations of CMB, LSS, and Gamma-ray bursts may be able to detect or constrain such features."}, "30": {"documentation": {"title": "Abnormal Ionic Current Rectification Caused by Reversed Electroosmotic\n  Flow under Viscosity Gradients across Thin Nanopores", "source": "Yinghua Qiu, Zuzanna S. Siwy, and Meni Wanunu", "docs_id": "1811.12878", "section": ["physics.chem-ph", "cond-mat.soft"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Abnormal Ionic Current Rectification Caused by Reversed Electroosmotic\n  Flow under Viscosity Gradients across Thin Nanopores. Single nanopores have attracted much scientific interest due to their versatile applications. The majority of experiments have been performed with nanopores being in contact with the same electrolyte on both sides of the membrane, while solution gradients across semi-permeable membranes are omnipresent in natural systems. In this manuscript, we studied ionic and fluidic movement through thin nanopores under viscosity gradients both experimentally and using simulations. Ionic current rectification was observed under these conditions, due to solutions with different conductivities filled across the pore under different biases caused by electroosmotic flow. We found that a pore filled with high viscosity solutions exhibited current increase with applied voltage in a steeper slope beyond a threshold voltage, which abnormally reduced the current rectification ratio. Through simulations, we found reversed electroosmotic flow that filled the pore with aqueous solutions of lower viscosities was responsible for this behavior. The reversed electroosmotic flow could be explained by slower depletion of coions than counterions along the pore. By increasing the surface charge density of pore surfaces, current rectification ratio could reach the value of the viscosity gradient across thin nanopores. Our findings shed light on fundamental aspects to be considered when performing experiments with viscosity gradients across nanopores and nanofluidic channels."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the underlying mechanism responsible for the abnormal ionic current rectification observed in thin nanopores under viscosity gradients, as suggested by the experimental and simulation results?\n\nA) Reversed electroosmotic flow caused by unequal depletion of coions and counterions along the pore surface\nB) Increased surface charge density of the pore surface, leading to enhanced electrostatic interactions between ions\nC) Viscosity gradient-induced flow of aqueous solutions of lower viscosities through the pore\nD) Electroosmotic flow caused by unequal distribution of ions across the pore, resulting in a net flow of ions\n\nCorrect Answer: A) Reversed electroosmotic flow caused by unequal depletion of coions and counterions along the pore surface\n\nExplanation: The correct answer is A) Reversed electroosmotic flow caused by unequal depletion of coions and counterions along the pore surface. The experimental results showed that the current rectification ratio decreased abnormally when the pore was filled with high-viscosity solutions, and simulations revealed that reversed electroosmotic flow was responsible for this behavior. This flow was caused by slower depletion of coions than counterions along the pore, leading to a net flow of ions in the opposite direction of the applied voltage. The other options are incorrect because they do not accurately describe the underlying mechanism responsible for the observed phenomenon."}, "31": {"documentation": {"title": "Alpha Discovery Neural Network based on Prior Knowledge", "source": "Jie Fang, Shutao Xia, Jianwu Lin, Zhikang Xia, Xiang Liu, and Yong\n  Jiang", "docs_id": "1912.11761", "section": ["q-fin.ST", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Alpha Discovery Neural Network based on Prior Knowledge. Genetic programming (GP) is the state-of-the-art in financial automated feature construction task. It employs reverse polish expression to represent features and then conducts the evolution process. However, with the development of deep learning, more powerful feature extraction tools are available. This paper proposes Alpha Discovery Neural Network (ADNN), a tailored neural network structure which can automatically construct diversified financial technical indicators based on prior knowledge. We mainly made three contributions. First, we use domain knowledge in quantitative trading to design the sampling rules and object function. Second, pre-training and model pruning has been used to replace genetic programming, because it can conduct more efficient evolution process. Third, the feature extractors in ADNN can be replaced by different feature extractors and produce different functions. The experiment results show that ADNN can construct more informative and diversified features than GP, which can effectively enriches the current factor pool. The fully-connected network and recurrent network are better at extracting information from the financial time series than the convolution neural network. In real practice, features constructed by ADNN can always improve multi-factor strategies' revenue, sharpe ratio, and max draw-down, compared with the investment strategies without these factors."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What are the main contributions of the Alpha Discovery Neural Network (ADNN) proposed in the paper, and how do they differ from the state-of-the-art genetic programming (GP) approach in financial automated feature construction?\n\n**A)** ADNN's main contributions are the use of domain knowledge in quantitative trading to design sampling rules and object functions, and the replacement of genetic programming with pre-training and model pruning. However, this approach is not more efficient than GP. ADNN's feature extractors can be replaced by different feature extractors, but this is not a significant advantage over GP.\n\n**B)** ADNN's main contributions are the use of domain knowledge in quantitative trading to design sampling rules and object functions, and the replacement of genetic programming with pre-training and model pruning. Additionally, ADNN's feature extractors can be replaced by different feature extractors, which allows for more diversified financial technical indicators to be constructed. This approach is more efficient and effective than GP.\n\n**C)** ADNN's main contributions are the use of domain knowledge in quantitative trading to design sampling rules and object functions, but not the replacement of genetic programming with pre-training and model pruning. ADNN's feature extractors can be replaced by different feature extractors, but this is not a significant advantage over GP.\n\n**D)** ADNN's main contributions are the use of domain knowledge in quantitative trading to design sampling rules and object functions, but not the replacement of genetic programming with pre-training and model pruning. Additionally, ADNN's feature extractors are not replaceable by different feature extractors, which limits its ability to construct diversified financial technical indicators.\n\n**Correct Answer:** B) ADNN's main contributions are the use of domain knowledge in quantitative trading to design sampling rules and object functions, and the replacement of genetic programming with pre-training and model pruning. Additionally, ADNN's feature extractors can be replaced by different feature extractors, which allows for more diversified financial technical indicators to be constructed. This approach is more efficient and effective than GP.\n\n**Explanation:** The correct answer is B) because it accurately summarizes the three main contributions of ADNN: (1) using domain knowledge in quantitative trading to design sampling rules and object functions, (2) replacing genetic programming with pre-training and model pruning, and (3) allowing feature extractors to be replaced by different feature extractors, which enables the construction of more diversified financial technical indicators. These contributions are highlighted in the paper as the key advantages of ADNN over GP."}, "32": {"documentation": {"title": "Evidence of Random Matrix Corrections for the Large Deviations of\n  Selberg's Central Limit Theorem", "source": "Eli Amzallag, Louis-Pierre Arguin, Emma Bailey, Kelvin Hui, Rajesh Rao", "docs_id": "2104.07403", "section": ["math.PR", "math.NT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evidence of Random Matrix Corrections for the Large Deviations of\n  Selberg's Central Limit Theorem. Selberg's central limit theorem states that the values of $\\log|\\zeta(1/2+i \\tau)|$, where $\\tau$ is a uniform random variable on $[T,2T]$, is distributed like a Gaussian random variable of mean $0$ and standard deviation $\\sqrt{\\frac{1}{2}\\log \\log T}$. It was conjectured by Radziwi{\\l}{\\l} that this breaks down for values of order $\\log\\log T$, where a multiplicative correction $C_k$ would be present at level $k\\log\\log T$, $k>0$. This constant should be equal to the leading asymptotic for the $2k^{th}$ moment of $\\zeta$, as first conjectured by Keating and Snaith using random matrix theory. In this paper, we provide numerical and theoretical evidence for this conjecture. We propose that this correction has a significant effect on the distribution of the maximum of $\\log|\\zeta|$ in intervals of size $(\\log T)^\\theta$, $\\theta>0$. The precision of the prediction enables the numerical detection of $C_k$ even for low $T$'s of order $T=10^8$. A similar correction appears in the large deviations of the Keating-Snaith central limit theorem for the logarithm of the characteristic polynomial of a random unitary matrix, as first proved by F\\'eray, M\\'eliot and Nikeghbali."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the leading asymptotic for the $2k^{th}$ moment of $\\zeta$, and how does it relate to the proposed correction $C_k$ in Selberg's Central Limit Theorem?\n\nA) The leading asymptotic for the $2k^{th}$ moment of $\\zeta$ is $\\frac{1}{2}\\log \\log T$, and $C_k$ is equal to this value.\n\nB) The leading asymptotic for the $2k^{th}$ moment of $\\zeta$ is $\\frac{1}{2}\\log \\log T + C_k$, where $C_k$ is a constant that depends on $k$.\n\nC) The leading asymptotic for the $2k^{th}$ moment of $\\zeta$ is $\\frac{1}{2}\\log \\log T + C_k$, where $C_k$ is a constant that is equal to the leading asymptotic for the $2k^{th}$ moment of $\\zeta$.\n\nD) The leading asymptotic for the $2k^{th}$ moment of $\\zeta$ is $\\frac{1}{2}\\log \\log T - C_k$, where $C_k$ is a constant that depends on $k$.\n\nCorrect Answer: C) The leading asymptotic for the $2k^{th}$ moment of $\\zeta$ is $\\frac{1}{2}\\log \\log T + C_k$, where $C_k$ is a constant that is equal to the leading asymptotic for the $2k^{th}$ moment of $\\zeta$.\n\nExplanation: The correct answer is C) because the problem statement mentions that the constant $C_k$ should be equal to the leading asymptotic for the $2k^{th}$ moment of $\\zeta$, which is $\\frac{1}{2}\\log \\log T$. This is a key point in the conjecture made by Keating and Snaith using random matrix theory."}, "33": {"documentation": {"title": "On Privacy Protection of Latent Dirichlet Allocation Model Training", "source": "Fangyuan Zhao, Xuebin Ren, Shusen Yang and Xinyu Yang", "docs_id": "1906.01178", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Privacy Protection of Latent Dirichlet Allocation Model Training. Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for discovery of hidden semantic architecture of text datasets, and plays a fundamental role in many machine learning applications. However, like many other machine learning algorithms, the process of training a LDA model may leak the sensitive information of the training datasets and bring significant privacy risks. To mitigate the privacy issues in LDA, we focus on studying privacy-preserving algorithms of LDA model training in this paper. In particular, we first develop a privacy monitoring algorithm to investigate the privacy guarantee obtained from the inherent randomness of the Collapsed Gibbs Sampling (CGS) process in a typical LDA training algorithm on centralized curated datasets. Then, we further propose a locally private LDA training algorithm on crowdsourced data to provide local differential privacy for individual data contributors. The experimental results on real-world datasets demonstrate the effectiveness of our proposed algorithms."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary concern with the process of training a Latent Dirichlet Allocation (LDA) model, and how do the proposed algorithms in the paper address this issue?\n\nA) The primary concern is that LDA models are too computationally intensive, and the proposed algorithms focus on improving computational efficiency. However, this is not the primary concern.\n\nB) The primary concern is that LDA models are not robust to noisy data, and the proposed algorithms focus on improving data quality. However, this is not the primary concern.\n\nC) The primary concern is that the process of training a LDA model may leak sensitive information from the training datasets, and the proposed algorithms focus on mitigating this privacy risk. This is the correct answer.\n\nD) The primary concern is that LDA models are not scalable to large datasets, and the proposed algorithms focus on improving scalability. However, this is not the primary concern.\n\n**Correct Answer:** C) The primary concern is that the process of training a LDA model may leak sensitive information from the training datasets, and the proposed algorithms focus on mitigating this privacy risk.\n\n**Explanation:** The question requires the test-taker to understand the context of the paper and identify the primary concern with LDA model training. The correct answer, C, highlights the privacy risk associated with LDA model training, which is a key theme of the paper. The incorrect answers, A, B, and D, are plausible but incorrect, requiring the test-taker to carefully analyze the question and choose the correct answer based on their understanding of the topic."}, "34": {"documentation": {"title": "Distributed Scalar Quantization for Computing: High-Resolution Analysis\n  and Extensions", "source": "Vinith Misra, Vivek K Goyal, Lav R. Varshney", "docs_id": "0811.3617", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distributed Scalar Quantization for Computing: High-Resolution Analysis\n  and Extensions. Communication of quantized information is frequently followed by a computation. We consider situations of \\emph{distributed functional scalar quantization}: distributed scalar quantization of (possibly correlated) sources followed by centralized computation of a function. Under smoothness conditions on the sources and function, companding scalar quantizer designs are developed to minimize mean-squared error (MSE) of the computed function as the quantizer resolution is allowed to grow. Striking improvements over quantizers designed without consideration of the function are possible and are larger in the entropy-constrained setting than in the fixed-rate setting. As extensions to the basic analysis, we characterize a large class of functions for which regular quantization suffices, consider certain functions for which asymptotic optimality is achieved without arbitrarily fine quantization, and allow limited collaboration between source encoders. In the entropy-constrained setting, a single bit per sample communicated between encoders can have an arbitrarily-large effect on functional distortion. In contrast, such communication has very little effect in the fixed-rate setting."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a distributed functional scalar quantization system where the sources are correlated and the function is smooth. Suppose we have two encoders, each with a limited number of bits available for quantization. In the entropy-constrained setting, what is the effect of allowing a single bit per sample to be communicated between the encoders on the mean-squared error (MSE) of the computed function?\n\n**A)** The MSE will decrease by a factor of 2.\n**B)** The MSE will remain unchanged.\n**C)** The MSE will increase by a factor of 2.\n**D)** The MSE will decrease by a factor of 4.\n\n**Correct Answer:** D) The MSE will decrease by a factor of 4.\n\n**Explanation:** In the entropy-constrained setting, a single bit per sample communicated between encoders can have an arbitrarily-large effect on functional distortion. This is because the entropy-constrained setting allows for more efficient use of the available bits, enabling the encoders to transmit more information about the sources. As a result, the MSE of the computed function will decrease by a factor of 4, assuming the sources are correlated and the function is smooth.\n\nNote: The other options are incorrect because:\n\n* A) The MSE will not decrease by a factor of 2, as the entropy-constrained setting allows for more efficient use of the available bits.\n* B) The MSE will not remain unchanged, as the communication between encoders will have an effect on the functional distortion.\n* C) The MSE will not increase by a factor of 2, as the entropy-constrained setting is designed to minimize the MSE."}, "35": {"documentation": {"title": "Weak equilibriums for time-inconsistent stopping control problems, with\n  applications to investment-withdrawal decision model", "source": "Zongxia Liang and Fengyi Yuan", "docs_id": "2105.06607", "section": ["math.OC", "q-fin.MF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Weak equilibriums for time-inconsistent stopping control problems, with\n  applications to investment-withdrawal decision model. This paper considers time-inconsistent problems when control and stopping strategies are required to be made simultaneously (called stopping control problems by us). We first formulate the time-inconsistent stopping control problems under general multi-dimensional controlled diffusion model and propose a formal definition of their equilibriums. We show that an admissible pair $(\\hat{u},C)$ of control-stopping policy is equilibrium if and only if the axillary function associated to it solves the extended HJB system. We provide almost equivalent conditions to the boundary term of this extended HJB system, which is related to the celebrated smooth fitting principles. As applications of our theoretical results, we develop an investment-withdrawal decision model for time-inconsistent decision makers in infinite time horizon. We provide two concrete examples, one of which includes constant proportion investment with one side threshold withdrawal strategy as equilibrium; in another example, all strategies with constant proportion investment are proved to be irrational, no matter what the withdrawal strategy is."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Consider a time-inconsistent stopping control problem under a general multi-dimensional controlled diffusion model. Suppose we have two admissible pairs of control-stopping policies: $(\\hat{u},C)$ and $(\\hat{u}',C')$. We are given that $(\\hat{u},C)$ is an equilibrium pair if and only if the auxiliary function associated with it solves the extended HJB system. Additionally, we know that $(\\hat{u}',C')$ is an equilibrium pair if and only if the auxiliary function associated with it solves the extended HJB system with a boundary term that is equivalent to the smooth fitting principle.\n\nWhich of the following statements is true?\n\nA) Both $(\\hat{u},C)$ and $(\\hat{u}',C')$ are equilibrium pairs if and only if they solve the extended HJB system with the same boundary term.\nB) $(\\hat{u},C)$ is an equilibrium pair if and only if $(\\hat{u}',C')$ is an equilibrium pair.\nC) $(\\hat{u},C)$ is an equilibrium pair if and only if the auxiliary function associated with it solves the extended HJB system with the same boundary term as $(\\hat{u}',C')$.\nD) $(\\hat{u},C)$ and $(\\hat{u}',C')$ are both equilibrium pairs if and only if they solve the extended HJB system with equivalent boundary terms.\n\n**Correct Answer:** C) $(\\hat{u},C)$ is an equilibrium pair if and only if the auxiliary function associated with it solves the extended HJB system with the same boundary term as $(\\hat{u}',C')$.\n\n**Explanation:** The correct answer is C) because the problem statement explicitly states that $(\\hat{u},C)$ is an equilibrium pair if and only if the auxiliary function associated with it solves the extended HJB system, and similarly for $(\\hat{u}',C')$. The boundary term equivalence is mentioned as a separate condition, but it is not a necessary condition for both pairs to be equilibrium pairs. Therefore, the correct answer is the one that states that the auxiliary function solves the extended HJB system with the same boundary term as $(\\hat{u}',C')$."}, "36": {"documentation": {"title": "High-Dimensional $L_2$Boosting: Rate of Convergence", "source": "Ye Luo and Martin Spindler", "docs_id": "1602.08927", "section": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "High-Dimensional $L_2$Boosting: Rate of Convergence. Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal Boosting\\textquotedblright\\ where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. We show that the rate of convergence of the classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant. To show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Our results also allow a direct comparison between LASSO and boosting which has been missing from the literature. Finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms LASSO."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of High-Dimensional $L_2$Boosting, what is the key finding regarding the rate of convergence of post-$L_2$Boosting and orthogonal boosting compared to LASSO in a sparse, high-dimensional setting?\n\n**A)** Both post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO, but with a different optimal number of iterations.\n\n**B)** Post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO, but with a different optimal design matrix.\n\n**C)** Post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO, but with a different sparse eigenvalue constant.\n\n**D)** Post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO, but with a different revisiting behavior.\n\n**Correct Answer:** C) Post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO, but with a different sparse eigenvalue constant.\n\n**Explanation:** The correct answer is C) because the paper states that the rate of convergence of classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant, and that post-$L_2$Boosting and orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. This implies that the sparse eigenvalue constant is a key factor in determining the rate of convergence of these methods."}, "37": {"documentation": {"title": "Blind Identification of State-Space Models in Physical Coordinates", "source": "Runzhe Han, Christian Bohn, Georg Bauer", "docs_id": "2108.08498", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Blind Identification of State-Space Models in Physical Coordinates. Blind identification is popular for modeling a system without the input information, such as in the research areas of structural health monitoring and audio signal processing. Existing blind identification methods have both advantages and disadvantages, in this paper, we briefly outline current methods and propose a novel blind identification method for identifying state-space models in physical coordinates. The idea behind this proposed method is first to regard the collected input data of a state-space model as a part of a periodic signal sequence, and then transform the state-space model with input and output into a model without input by augmenting the state-space model with the input model (which is a periodic signal model), and afterwards use merely the output information to identify a state-space model up to a similarity transformation, and finally derive the state-space model in physical coordinates by using a unique similarity transformation. With the above idea, physical parameters and modal parameters of a state-space system can be obtained. Both numerical and practical examples were used to validate the proposed method. The result showed the effectiveness of the novel blind identification method."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of blind identification of state-space models in physical coordinates, what is the primary goal of the proposed method, and how does it differ from existing blind identification methods?\n\n**A)** The primary goal is to identify the input signal model, and the method differs from existing methods in that it uses a unique similarity transformation to derive the state-space model in physical coordinates.\n\n**B)** The primary goal is to identify the state-space model up to a similarity transformation, and the method differs from existing methods in that it augments the state-space model with the input model, which is a periodic signal model.\n\n**C)** The primary goal is to identify the physical parameters and modal parameters of a state-space system, and the method differs from existing methods in that it uses numerical examples to validate the proposed method.\n\n**D)** The primary goal is to identify the state-space model without input information, and the method differs from existing methods in that it regards the collected input data as a part of a periodic signal sequence.\n\n**Correct Answer:** B) The primary goal is to identify the state-space model up to a similarity transformation, and the method differs from existing methods in that it augments the state-space model with the input model, which is a periodic signal model.\n\n**Explanation:** The correct answer is B) because the proposed method aims to identify the state-space model up to a similarity transformation, and it achieves this by augmenting the state-space model with the input model, which is a periodic signal model. This is in contrast to existing blind identification methods, which may not use this approach. The other options are incorrect because they either misstate the primary goal of the proposed method (A and D) or only partially describe the method's approach (C)."}, "38": {"documentation": {"title": "Deterministic characterization of stochastic genetic circuits", "source": "Matthew Scott, Terence Hwa and Brian Ingalls", "docs_id": "0704.2793", "section": ["q-bio.MN", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deterministic characterization of stochastic genetic circuits. For cellular biochemical reaction systems where the numbers of molecules is small, significant noise is associated with chemical reaction events. This molecular noise can give rise to behavior that is very different from the predictions of deterministic rate equation models. Unfortunately, there are few analytic methods for examining the qualitative behavior of stochastic systems. Here we describe such a method that extends deterministic analysis to include leading-order corrections due to the molecular noise. The method allows the steady-state behavior of the stochastic model to be easily computed, facilitates the mapping of stability phase diagrams that include stochastic effects and reveals how model parameters affect noise susceptibility, in a manner not accessible to numerical simulation. By way of illustration we consider two genetic circuits: a bistable positive-feedback loop and a negative-feedback oscillator. We find in the positive feedback circuit that translational activation leads to a far more stable system than transcriptional control. Conversely, in a negative-feedback loop triggered by a positive-feedback switch, the stochasticity of transcriptional control is harnessed to generate reproducible oscillations."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of stochastic genetic circuits, what is the primary limitation of deterministic rate equation models in capturing the behavior of systems with significant molecular noise?\n\nA) They are unable to account for the effects of transcriptional control on protein expression.\nB) They are limited to analyzing systems with a large number of molecules.\nC) They fail to incorporate leading-order corrections due to molecular noise.\nD) They are not suitable for studying the steady-state behavior of stochastic systems.\n\n**Correct Answer:** C) They fail to incorporate leading-order corrections due to molecular noise.\n\n**Explanation:** The question requires the test-taker to understand the limitations of deterministic rate equation models in capturing the behavior of stochastic systems. The correct answer, C, highlights the key limitation of these models, which is their inability to incorporate leading-order corrections due to molecular noise. This is a critical aspect of the method described in the Arxiv documentation, which extends deterministic analysis to include these corrections.\n\n**Candidate A:** This option is incorrect because transcriptional control is actually a key aspect of the method described in the documentation, which is used to harness stochasticity to generate reproducible oscillations.\n\n**Candidate B:** This option is incorrect because the limitation of deterministic rate equation models is not related to the number of molecules in the system, but rather to their inability to capture the effects of molecular noise.\n\n**Candidate D:** This option is incorrect because the question is specifically asking about the limitations of deterministic rate equation models, and the correct answer is related to their inability to incorporate leading-order corrections due to molecular noise. The steady-state behavior of stochastic systems is actually a key aspect of the method described in the documentation, which is facilitated by this correction."}, "39": {"documentation": {"title": "Excitonic Zeeman Splittings in Colloidal CdSe Quantum Dots Doped with\n  Single Magnetic Impurities", "source": "Charles J. Barrows, Rachel Fainblat, Daniel R. Gamelin", "docs_id": "1705.05429", "section": ["cond-mat.mes-hall"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Excitonic Zeeman Splittings in Colloidal CdSe Quantum Dots Doped with\n  Single Magnetic Impurities. Doping a semiconductor quantum dot with just a single impurity atom can completely transform its physical properties. Here, we report and analyze the magnetic circular dichroism (MCD) spectra of colloidal CdSe quantum dot samples containing on average fewer than one Mn2+ per quantum dot. Even at this sub-single-dopant level, the low-temperature low-field data are dominated by impurity-induced Zeeman splittings caused by dopant-carrier sp-d exchange. Unlike in more heavily doped quantum dots, however, the MCD intensity at the first CdSe exciton shows a field-induced sign flip as the field strength is increased, reflecting competition between sp-d exchange and the intrinsic Zeeman splittings of comparable magnitude. Most unusually, the competition between these two effects leads to a large apparent shift in the first MCD peak maximum, which we show is attributable to a difference in sign of the intrinsic excitonic g factor between the first and second excitons. Finally, the sp-d and intrinsic contributions to the excitonic Zeeman splittings each exhibit unique magnetic-field and temperature dependencies, allowing the MCD spectra of undoped, singly doped, and bi-doped quantum dot sub-ensembles to be analyzed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the unusual outcome observed in the magnetic circular dichroism (MCD) spectra of colloidal CdSe quantum dot samples containing on average fewer than one Mn2+ per quantum dot, and what is the underlying reason for this phenomenon?\n\nA) The MCD intensity at the first CdSe exciton shows a field-induced sign flip as the field strength is increased, reflecting competition between sp-d exchange and the intrinsic Zeeman splittings of comparable magnitude.\nB) The MCD spectra of undoped, singly doped, and bi-doped quantum dot sub-ensembles exhibit unique magnetic-field and temperature dependencies, but the first MCD peak maximum does not show a sign flip.\nC) The competition between sp-d exchange and the intrinsic Zeeman splittings leads to a large apparent shift in the first MCD peak maximum, which is attributable to a difference in sign of the intrinsic excitonic g factor between the first and second excitons.\nD) The MCD intensity at the first CdSe exciton does not show a field-induced sign flip as the field strength is increased, and the competition between sp-d exchange and the intrinsic Zeeman splittings does not lead to a large apparent shift in the first MCD peak maximum.\n\nCorrect Answer: C) The competition between sp-d exchange and the intrinsic Zeeman splittings leads to a large apparent shift in the first MCD peak maximum, which is attributable to a difference in sign of the intrinsic excitonic g factor between the first and second excitons.\n\nExplanation: The correct answer is C) because the documentation states that the competition between sp-d exchange and the intrinsic Zeeman splittings leads to a large apparent shift in the first MCD peak maximum, which is attributable to a difference in sign of the intrinsic excitonic g factor between the first and second excitons. This is a key finding in the study, and the correct answer requires an understanding of the complex interactions between the dopant-carrier sp-d exchange and the intrinsic Zeeman splittings."}, "40": {"documentation": {"title": "Very fast formation of superconducting MgB2/Fe wires with high Jc", "source": "X.L. Wang, S. Soltanian, J. Horvat, M.J. Qin, H.K. Liu and S.X. Dou", "docs_id": "cond-mat/0106148", "section": ["cond-mat.supr-con", "cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Very fast formation of superconducting MgB2/Fe wires with high Jc. In this paper we have investigated the effects of sintering time and temperature on the formation and critical current densities of Fe-clad MgB2 wires. MgB2 wires were fabricated using the powder-in-tube process and sintered for different periods of time at predetermined temperatures. All the samples were examined using XRD, SEM and magnetisation measurements. In contrast to the common practice of sintering for several hours, the present results show that there is no need for prolonged heat treatment in the fabrication of Fe-clad MgB2 wires. A total time in the furnace of several minutes is more than enough to form nearly pure MgB2 with high performance characteristics. The results from Tc, Jc and Hirr show convincingly that the samples which were sintered for 3 minutes above 800 oC are as good as those sintered for longer times. In fact, the Jc field performance for the most rapidly sintered sample is slightly better than for all other samples. Jc of 4.5 times 10 ^5 A/cm2 in zero field and above 10 ^5 A/cm2 in 2T at 15 K has been achieved for the best Fe-clad MgB2 wires. As a result of such a short sintering there is no need for using high purity argon protection and it is possible to carry out the heat treatment in a much less protective atmosphere or in air. These findings substantially simplify the fabrication process, making it possible to have a continuous process for fabrication and reducing the costs for large-scale production of MgB2 wires."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the primary advantage of the sintering time and temperature conditions used in the fabrication of Fe-clad MgB2 wires, as reported in the paper?\n\nA) The use of high-purity argon protection is necessary to achieve high critical current densities.\nB) The sintering time and temperature conditions result in the formation of impurities in the MgB2 wires.\nC) The sintering time of several minutes at temperatures above 800\u00b0C is sufficient to achieve high performance characteristics, including high critical current densities.\nD) The use of a continuous process for fabrication simplifies the production of MgB2 wires, but does not affect the critical current densities.\n\nCorrect Answer: C) The sintering time of several minutes at temperatures above 800\u00b0C is sufficient to achieve high performance characteristics, including high critical current densities.\n\nExplanation: The correct answer is C) because the paper states that the samples sintered for 3 minutes above 800\u00b0C have high performance characteristics, including high critical current densities (Jc of 4.5 x 10^5 A/cm^2 in zero field and above 10^5 A/cm^2 in 2T at 15 K). This contradicts option A, which states that high-purity argon protection is necessary. Option B is incorrect because the paper does not mention the formation of impurities as a result of the sintering conditions. Option D is also incorrect because the paper states that the continuous process simplifies the fabrication process, but does not affect the critical current densities."}, "41": {"documentation": {"title": "Monochromaticity in Neutral Evolutionary Network Models", "source": "Arda Halu and Ginestra Bianconi", "docs_id": "1207.3811", "section": ["q-bio.MN", "cond-mat.dis-nn", "cond-mat.stat-mech"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Monochromaticity in Neutral Evolutionary Network Models. Recent studies on epistatic networks of model organisms have unveiled a certain type of modular property called monochromaticity in which the networks are clusterable into functional modules that interact with each other through the same type of epistasis. Here we propose and study three epistatic network models that are inspired by the Duplication-Divergence mechanism to gain insight into the evolutionary basis of monochromaticity and to test if it can be explained as the outcome of a neutral evolutionary hypothesis. We show that the epistatic networks formed by these stochastic evolutionary models have monochromaticity conflict distributions that are centered close to zero and are statistically significantly different from their randomized counterparts. In particular, the last model we propose yields a strictly monochromatic solution. Our results agree with the monochromaticity findings in real organisms and point toward the possible role of a neutral mechanism in the evolution of this phenomenon."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary hypothesis being tested by the authors of the study \"Monochromaticity in Neutral Evolutionary Network Models\" regarding the evolutionary basis of monochromaticity in epistatic networks?\n\nA) That monochromaticity is the result of a positive selection mechanism.\nB) That monochromaticity is the outcome of a neutral evolutionary hypothesis.\nC) That monochromaticity is a result of genetic drift alone.\nD) That monochromaticity is a characteristic of only certain types of epistatic networks.\n\n**Correct Answer:** B) That monochromaticity is the outcome of a neutral evolutionary hypothesis.\n\n**Explanation:** The authors of the study propose and test three epistatic network models inspired by the Duplication-Divergence mechanism to investigate the evolutionary basis of monochromaticity. They aim to determine if monochromaticity can be explained by a neutral evolutionary hypothesis, which suggests that the evolution of monochromaticity is not driven by selection or other non-neutral mechanisms. The study's findings support the idea that a neutral mechanism may be responsible for the evolution of monochromaticity in epistatic networks.\n\n**Additional explanations:**\n\n* Candidate A is incorrect because the study does not suggest that monochromaticity is the result of a positive selection mechanism.\n* Candidate C is incorrect because the study does not focus solely on genetic drift as the mechanism driving monochromaticity.\n* Candidate D is incorrect because the study does not limit its findings to only certain types of epistatic networks, but rather explores the general phenomenon of monochromaticity in these networks."}, "42": {"documentation": {"title": "Conformal Titanium Nitride in a Porous Silicon Matrix: a Nanomaterial\n  for In-Chip Supercapacitors", "source": "Kestutis Grigoras, Jari Keskinen, Leif Gr\\\"onberg, Elina Yli-Rantala,\n  Sampo Laakso, Hannu V\\\"alim\\\"aki, Pertti Kauranen, Jouni Ahopelto, Mika\n  Prunnila", "docs_id": "1603.00798", "section": ["cond-mat.mtrl-sci", "physics.chem-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conformal Titanium Nitride in a Porous Silicon Matrix: a Nanomaterial\n  for In-Chip Supercapacitors. Today's supercapacitor energy storages are typically discrete devices aimed for printed boards and power applications. The development of autonomous sensor networks and wearable electronics and the miniaturisation of mobile devices would benefit substantially from solutions in which the energy storage is integrated with the active device. Nanostructures based on porous silicon (PS) provide a route towards integration due to the very high inherent surface area to volume ratio and compatibility with microelectronics fabrication processes. Unfortunately, pristine PS has limited wettability and poor chemical stability in electrolytes and the high resistance of the PS matrix severely limits the power efficiency. In this work, we demonstrate that excellent wettability and electro-chemical properties in aqueous and organic electrolytes can be obtained by coating the PS matrix with an ultra-thin layer of titanium nitride by atomic layer deposition. Our approach leads to very high specific capacitance (15 F/cm$^3$), energy density (1.3 mWh/cm$^3$), power density (up to 214 W/cm$^3$) and excellent stability (more than 13,000 cycles). Furthermore, we show that the PS-TiN nanomaterial can be integrated inside a silicon chip monolithically by combining MEMS and nanofabrication techniques. This leads to realisation of in-chip supercapacitor, i.e., it opens a new way to exploit the otherwise inactive volume of a silicon chip to store energy."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat are the key advantages of using a titanium nitride (TiN) coating on a porous silicon (PS) matrix in the development of in-chip supercapacitors, and how do these advantages impact the overall performance of the device?\n\nA) The TiN coating improves the wettability of the PS matrix, leading to increased power efficiency and energy density, but at the cost of reduced stability.\n\nB) The TiN coating enhances the electrochemical properties of the PS matrix, resulting in improved specific capacitance, energy density, and power density, while maintaining excellent stability.\n\nC) The TiN coating reduces the surface area to volume ratio of the PS matrix, leading to decreased specific capacitance and energy density, but improving stability.\n\nD) The TiN coating has no significant impact on the performance of the PS matrix, and the device's performance is solely determined by the properties of the silicon chip.\n\nCorrect Answer: B) The TiN coating enhances the electrochemical properties of the PS matrix, resulting in improved specific capacitance, energy density, and power density, while maintaining excellent stability."}, "43": {"documentation": {"title": "Derivation of Matrix Product Ansatz for the Heisenberg Chain from\n  Algebraic Bethe Ansatz", "source": "Hosho Katsura and Isao Maruyama", "docs_id": "0911.4215", "section": ["cond-mat.stat-mech", "cond-mat.str-el", "hep-th", "math-ph", "math.MP", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Derivation of Matrix Product Ansatz for the Heisenberg Chain from\n  Algebraic Bethe Ansatz. We derive a matrix product representation of the Bethe ansatz state for the XXX and XXZ spin-1/2 Heisenberg chains using the algebraic Bethe ansatz. In this representation, the components of the Bethe eigenstates are expressed as traces of products of matrices which act on ${\\bar {\\mathscr H}}$, the tensor product of auxiliary spaces. By changing the basis in ${\\bar {\\mathscr H}}$, we derive explicit finite-dimensional representations for the matrices. These matrices are the same as those appearing in the recently proposed matrix product ansatz by Alcaraz and Lazo [Alcaraz F C and Lazo M J 2006 {\\it J. Phys. A: Math. Gen.} \\textbf{39} 11335.] apart from normalization factors. We also discuss the close relation between the matrix product representation of the Bethe eigenstates and the six-vertex model with domain wall boundary conditions [Korepin V E 1982 {\\it Commun. Math. Phys.}, \\textbf{86} 391.] and show that the change of basis corresponds to a mapping from the six-vertex model to the five-vertex model."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Derive the matrix product representation of the Bethe eigenstates for the Heisenberg chain using the algebraic Bethe ansatz, and explain the relationship between this representation and the six-vertex model with domain wall boundary conditions.\n\nA) The matrix product representation of the Bethe eigenstates for the Heisenberg chain can be derived by expressing the components of the Bethe eigenstates as traces of products of matrices acting on the tensor product of auxiliary spaces. The resulting matrices are the same as those appearing in the matrix product ansatz by Alcaraz and Lazo, apart from normalization factors.\n\nB) The matrix product representation of the Bethe eigenstates for the Heisenberg chain can be derived by expressing the components of the Bethe eigenstates as linear combinations of the eigenstates of the six-vertex model with domain wall boundary conditions. The resulting matrices are the same as those appearing in the matrix product ansatz by Alcaraz and Lazo, apart from normalization factors.\n\nC) The matrix product representation of the Bethe eigenstates for the Heisenberg chain can be derived by expressing the components of the Bethe eigenstates as traces of products of matrices acting on the tensor product of auxiliary spaces. The resulting matrices are the same as those appearing in the matrix product ansatz by Alcaraz and Lazo, apart from normalization factors, and the change of basis corresponds to a mapping from the six-vertex model to the five-vertex model.\n\nD) The matrix product representation of the Bethe eigenstates for the Heisenberg chain can be derived by expressing the components of the Bethe eigenstates as linear combinations of the eigenstates of the five-vertex model. The resulting matrices are the same as those appearing in the matrix product ansatz by Alcaraz and Lazo, apart from normalization factors.\n\nCorrect Answer: C) The matrix product representation of the Bethe eigenstates for the Heisenberg chain can be derived by expressing the components of the Bethe eigenstates as traces of products of matrices acting on the tensor product of auxiliary spaces. The resulting matrices are the same as those appearing in the matrix product ansatz by Alcaraz and Lazo, apart from normalization factors, and the change of basis corresponds to a mapping from the six-vertex model to the five-vertex model.\n\nExplanation: The correct answer is C) because it accurately describes the derivation of the matrix product representation of the Bethe eigenstates for the Heisenberg chain using the algebraic Bethe ansatz. The correct answer also correctly states the relationship between this representation and the six-vertex model with domain wall boundary conditions, and explains that the change of basis corresponds to a mapping from the six-vertex model to the five-vertex model. The other options are incorrect because they either omit important details or make incorrect statements about the derivation and relationship between the matrix product representation and the six-vertex model."}, "44": {"documentation": {"title": "Lattice solitons with quadrupolar intersite interactions", "source": "Yongyao Li, Jingfeng Liu, Wei Pang, and Boris A. Malomed", "docs_id": "1312.2969", "section": ["nlin.PS", "cond-mat.quant-gas"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lattice solitons with quadrupolar intersite interactions. We study two-dimensional (2D) solitons in the mean-field models of ultracold gases with long-range quadrupole-quadrupole interaction (QQI) between particles. The condensate is loaded into a deep optical-lattice (OL) potential, therefore the model is based on the 2D discrete nonlinear Schr\\\"{o}dinger equation with contact onsite and long-range intersite interactions, which represent the QQI. The quadrupoles are built as pairs of electric dipoles and anti-dipoles orientated perpendicular to the 2D plane to which the gas is confined. Because the quadrupoles interact with the local gradient of the external field, they are polarized by inhomogeneous dc electric field that may be supplied by a tapered capacitor. Shapes, stability, mobility, and collisions of fundamental discrete solitons are studied by means of systematic simulations. In particular, threshold values of the norm, necessary for the existence of the solitons, are found, and anisotropy of their static and dynamical properties is explored. As concerns the mobility and collisions, it is the first analysis of such properties for discrete solitons on 2D lattices with long-range intersite interactions of any type. Estimates demonstrate that the setting can be realized under experimentally available conditions, predicting solitons built of $\\sim$ 10,000 particles."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the lattice solitons with quadrupolar intersite interactions, what is the primary mechanism by which the quadrupoles are polarized in the system?\n\nA) The local gradient of the external field is responsible for the polarization of the quadrupoles.\nB) The long-range intersite interactions between the quadrupoles are the primary mechanism for their polarization.\nC) The external electric field is polarized by the quadrupoles, leading to their polarization.\nD) The onsite interactions between the particles are responsible for the polarization of the quadrupoles.\n\n**Correct Answer:** A) The local gradient of the external field is responsible for the polarization of the quadrupoles.\n\n**Explanation:** According to the documentation, the quadrupoles are polarized by the inhomogeneous dc electric field that may be supplied by a tapered capacitor, which is a result of the local gradient of the external field. This is stated in the sentence: \"quadrupoles are polarized by inhomogeneous dc electric field that may be supplied by a tapered capacitor.\"\n\n**Note:** The other options are incorrect because:\n\n* B) The long-range intersite interactions between the quadrupoles are not mentioned as the primary mechanism for their polarization.\n* C) The external electric field is not polarized by the quadrupoles, but rather the quadrupoles are polarized by the external field.\n* D) The onsite interactions between the particles are not mentioned as the primary mechanism for the polarization of the quadrupoles."}, "45": {"documentation": {"title": "Fault Tolerance for Remote Memory Access Programming Models", "source": "Maciej Besta, Torsten Hoefler", "docs_id": "2010.09025", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fault Tolerance for Remote Memory Access Programming Models. Remote Memory Access (RMA) is an emerging mechanism for programming high-performance computers and datacenters. However, little work exists on resilience schemes for RMA-based applications and systems. In this paper we analyze fault tolerance for RMA and show that it is fundamentally different from resilience mechanisms targeting the message passing (MP) model. We design a model for reasoning about fault tolerance for RMA, addressing both flat and hierarchical hardware. We use this model to construct several highly-scalable mechanisms that provide efficient low-overhead in-memory checkpointing, transparent logging of remote memory accesses, and a scheme for transparent recovery of failed processes. Our protocols take into account diminishing amounts of memory per core, one of major features of future exascale machines. The implementation of our fault-tolerance scheme entails negligible additional overheads. Our reliability model shows that in-memory checkpointing and logging provide high resilience. This study enables highly-scalable resilience mechanisms for RMA and fills a research gap between fault tolerance and emerging RMA programming models."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary difference between fault tolerance for Remote Memory Access (RMA) and resilience mechanisms targeting the Message Passing (MP) model, according to the paper \"Fault Tolerance for Remote Memory Access Programming Models\"?\n\n**A)** RMA fault tolerance is more complex due to the hierarchical nature of its hardware, while MP resilience is simpler due to its flat architecture.\n**B)** RMA fault tolerance is fundamentally different from MP resilience because RMA relies on in-memory checkpointing and logging, whereas MP relies on message retransmission and acknowledgement.\n**C)** RMA fault tolerance is more scalable than MP resilience due to the diminishing amounts of memory per core in future exascale machines.\n**D)** RMA fault tolerance is less reliable than MP resilience because RMA relies on more complex protocols and mechanisms.\n\n**Correct Answer:** B) RMA fault tolerance is fundamentally different from MP resilience because RMA relies on in-memory checkpointing and logging, whereas MP relies on message retransmission and acknowledgement.\n\n**Explanation:** The paper states that \"Fault tolerance for RMA is fundamentally different from resilience mechanisms targeting the message passing (MP) model.\" This implies that the primary difference lies in the underlying mechanisms used to achieve fault tolerance in RMA and MP. The correct answer, B, highlights this key distinction, noting that RMA relies on in-memory checkpointing and logging, whereas MP relies on message retransmission and acknowledgement. The other options are incorrect because they either misrepresent the relationship between RMA and MP fault tolerance or introduce irrelevant factors, such as hardware complexity or scalability."}, "46": {"documentation": {"title": "Consequences on variable Lambda-models from distant Type Ia supernovae\n  and compact radio sources", "source": "R. G. Vishwakarma (IUCAA)", "docs_id": "astro-ph/0012492", "section": ["astro-ph", "gr-qc"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Consequences on variable Lambda-models from distant Type Ia supernovae\n  and compact radio sources. We study the m-z relation for Type Ia supernovae data and the \\theta-z relation for the updated compact radio sources data in 4 variable \\Lambda- models: \\Lambda \\sim S^{-2}, \\Lambda \\sim H^2, \\Lambda \\sim \\rho and \\Lambda \\sim t^{-2}. It is found that all the models fit the data sets equally well and require non-zero, positive values of \\Lambda. The supernovae data favour an accelerating expansion of the universe whereas the radio sources data imply either sign of the deceleration parameter. The estimates of the density parameter for the variable \\Lambda-models are found higher than those for the constant \\Lambda-Friedmann model. It is also found that the Gurvits et al' model (FRW model with \\Lambda=0) is not the best-fitting model for the constant \\Lambda case. The best-fitting Friedmann model (with constant \\Lambda) is found to be a low density, vacuum-dominated accelerating universe. However, for realistic values of the matter density parameter, the only interesting solutions are (a) estimated from the supernovae data: the best-fit solutions for the flat models (including the constant \\Lambda case); (b) estimated from the radio sources data: the global best-fit solutions for the models \\Lambda \\sim H^2 and \\Lambda \\sim \\rho, the best-fit solution for the flat model with \\Lambda=constant and the Gurvits et al' model. It is noted that, as in the case of recent CMB analyses, the data sets seem to favour a spherical universe (k>0)."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: Analyze the implications of the variable Lambda-models on the cosmological parameters of the universe, specifically the density parameter and the deceleration parameter, in the context of Type Ia supernovae and compact radio sources data.\n\nA) The variable Lambda-models require non-zero, positive values of Lambda, which is consistent with the observed accelerating expansion of the universe, but the estimates of the density parameter are higher than those for the constant Lambda-Friedmann model.\n\nB) The variable Lambda-models imply either sign of the deceleration parameter, which is consistent with the observed data from compact radio sources, but the estimates of the density parameter are lower than those for the constant Lambda-Friedmann model.\n\nC) The variable Lambda-models favor a spherical universe (k>0) as indicated by the data sets, but the estimates of the density parameter are higher than those for the constant Lambda-Friedmann model, and the deceleration parameter is consistent with the observed data from Type Ia supernovae.\n\nD) The variable Lambda-models are inconsistent with the observed accelerating expansion of the universe, and the estimates of the density parameter are lower than those for the constant Lambda-Friedmann model, but the deceleration parameter is consistent with the observed data from compact radio sources.\n\nCorrect Answer: A) The variable Lambda-models require non-zero, positive values of Lambda, which is consistent with the observed accelerating expansion of the universe, but the estimates of the density parameter are higher than those for the constant Lambda-Friedmann model."}, "47": {"documentation": {"title": "Magnetic Field Effects on Transport Properties of PtSn4", "source": "Eundeok Mun, Hyunjin Ko, Gordon J. Miller, German D. Samolyuk, Sergey\n  L. Bud'ko, and Paul. C. Canfield", "docs_id": "1201.4091", "section": ["cond-mat.mtrl-sci"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Magnetic Field Effects on Transport Properties of PtSn4. The anisotropic physical properties of single crystals of orthorhombic PtSn4 are reported for magnetic fields up to 140 kOe, applied parallel and perpendicular to the crystallographic b-axis. The magnetic susceptibility has an approximately temperature independent behavior and reveals an anisotropy between ac-plane and b-axis. Clear de Haas-van Alphen oscillations in fields as low as 5 kOe and at temperatures as high as 30 K were detected in magnetization isotherms. The thermoelectric power and resistivity of PtSn4 show the strong temperature and magnetic field dependencies. A change of the thermoelectric power at H = 140 kOe is observed as high as ~ 50 mu-V/K. Single crystals of PtSn4 exhibit very large transverse magnetoresistance of ~ 5x10^5% for the ac-plane and of ~ 1.4x10^5% for the b-axis resistivity at 1.8 K and 140 kOe, as well as pronounced Shubnikov-de Haas oscillations. The magnetoresistance of PtSn4 appears to obey Kohler's rule in the temperature and field range measured. The Hall resistivity shows a linear temperature dependence at high temperatures followed by a sign reversal around 25 K which is consistent with thermoelectric power measurements. The observed quantum oscillations and band structure calculations indicate that PtSn4 has three dimensional Fermi surfaces."}, "answer": "Here is a difficult multi-form question based on the provided Arxiv documentation:\n\n**Question:** Analyze the magnetic field effects on the transport properties of PtSn4, specifically focusing on the anisotropic physical properties and quantum oscillations. How do the observed changes in thermoelectric power, resistivity, and magnetoresistance relate to the crystallographic structure and Fermi surfaces of PtSn4?\n\n**A)** The anisotropic physical properties of PtSn4 are primarily influenced by the magnetic field, resulting in a significant increase in thermoelectric power and resistivity in the ac-plane compared to the b-axis. The observed quantum oscillations are consistent with a three-dimensional Fermi surface.\n\n**B)** The magnetic field effects on PtSn4 are characterized by a strong temperature dependence of the thermoelectric power and resistivity, with a sign reversal around 25 K. The observed magnetoresistance follows Kohler's rule, indicating a linear relationship between the magnetic field and resistivity.\n\n**C)** The anisotropic physical properties of PtSn4 are primarily influenced by the crystallographic structure, resulting in a significant increase in thermoelectric power and resistivity in the b-axis compared to the ac-plane. The observed quantum oscillations are consistent with a two-dimensional Fermi surface.\n\n**D)** The magnetic field effects on PtSn4 are characterized by a strong anisotropy between the ac-plane and b-axis, resulting in a significant increase in thermoelectric power and resistivity in the ac-plane compared to the b-axis. The observed magnetoresistance follows Kohler's rule, indicating a linear relationship between the magnetic field and resistivity.\n\n**Correct Answer:** A\n\n**Explanation:**\n\nThe correct answer, A, is supported by the text, which states that the anisotropic physical properties of PtSn4 are reported for magnetic fields up to 140 kOe, applied parallel and perpendicular to the crystallographic b-axis. The text also mentions that the observed quantum oscillations are consistent with a three-dimensional Fermi surface, which is a key point in understanding the magnetic field effects on PtSn4.\n\nCandidate A is the correct answer because it accurately summarizes the main points of the text, including the anisotropic physical properties, quantum oscillations, and Fermi surface. The other candidates, B, C, and D, contain errors or incomplete information, making them incorrect answers.\n\n* Candidate B is incorrect because it mentions a sign reversal around 25 K, which is not mentioned in the text.\n* Candidate C is incorrect because it states that the observed quantum oscillations are consistent with a two-dimensional Fermi surface, which is not supported by the text.\n* Candidate D is incorrect because it states that the observed magnetoresistance follows Kohler's rule in the ac-plane, but the text only mentions that the magnetoresistance follows Kohler's rule in the temperature and field range measured, without specifying the plane."}, "48": {"documentation": {"title": "Self-organized system-size oscillation of a stochastic lattice-gas model", "source": "Mareike Bojer, Isabella R. Graf, Erwin Frey", "docs_id": "1803.03942", "section": ["physics.bio-ph", "cond-mat.stat-mech", "q-bio.SC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Self-organized system-size oscillation of a stochastic lattice-gas model. The totally asymmetric simple exclusion process (TASEP) is a paradigmatic stochastic model for non-equilibrium physics, and has been successfully applied to describe active transport of molecular motors along cytoskeletal filaments. Building on this simple model, we consider a two-lane lattice-gas model that couples directed transport (TASEP) to diffusive motion in a semi-closed geometry, and simultaneously accounts for spontaneous growth and particle-induced shrinkage of the system's size. This particular extension of the TASEP is motivated by the question of how active transport and diffusion might influence length regulation in confined systems. Surprisingly, we find that the size of our intrinsically stochastic system exhibits robust temporal patterns over a broad range of growth rates. More specifically, when particle diffusion is slow relative to the shrinkage dynamics, we observe quasi-periodic changes in length. We provide an intuitive explanation for the occurrence of these self-organized temporal patterns, which is based on the imbalance between the diffusion and shrinkage speed in the confined geometry. Finally, we formulate an effective theory for the oscillatory regime, which explains the origin of the oscillations and correctly predicts the dependence of key quantities, as for instance the oscillation frequency, on the growth rate."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary mechanism underlying the self-organized system-size oscillation observed in the two-lane lattice-gas model, and how does it relate to the imbalance between diffusion and shrinkage speed in the confined geometry?\n\n**A)** The oscillations are caused by the interplay between active transport and diffusion, leading to a feedback loop that regulates the system's size.\n\n**B)** The oscillations arise from the spontaneous growth and particle-induced shrinkage of the system's size, which creates a bistable equilibrium that oscillates over time.\n\n**C)** The oscillations are a result of the imbalance between the diffusion and shrinkage speed in the confined geometry, where the slow diffusion rate relative to the shrinkage dynamics leads to a periodic change in system size.\n\n**D)** The oscillations are an emergent property of the system's non-equilibrium dynamics, arising from the interplay between directed transport and diffusive motion in the semi-closed geometry.\n\n**Correct Answer:** C) The oscillations are a result of the imbalance between the diffusion and shrinkage speed in the confined geometry, where the slow diffusion rate relative to the shrinkage dynamics leads to a periodic change in system size.\n\n**Explanation:** The correct answer is based on the explanation provided in the documentation, which states that when particle diffusion is slow relative to the shrinkage dynamics, the system exhibits quasi-periodic changes in length. This is attributed to the imbalance between the diffusion and shrinkage speed in the confined geometry, which creates a periodic change in system size. The other options are incorrect because they either oversimplify the mechanism underlying the oscillations (A and D) or misattribute the cause of the oscillations (B)."}, "49": {"documentation": {"title": "Two-pulse solutions in the fifth-order KdV equation : rigorous theory\n  and numerical approximations", "source": "Marina Chugunova and Dmitry Pelinovsky", "docs_id": "nlin/0605043", "section": ["nlin.PS", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-pulse solutions in the fifth-order KdV equation : rigorous theory\n  and numerical approximations. We revisit existence and stability of two-pulse solutions in the fifth-order Korteweg--de Vries (KdV) equation with two new results. First, we modify the Petviashvili method of successive iterations for numerical (spectral) approximations of pulses and prove convergence of iterations in a neighborhood of two-pulse solutions. Second, we prove structural stability of embedded eigenvalues of negative Krein signature in a linearized KdV equation. Combined with stability analysis in Pontryagin spaces, this result completes the proof of spectral stability of the corresponding two-pulse solutions. Eigenvalues of the linearized problem are approximated numerically in exponentially weighted spaces where embedded eigenvalues are isolated from the continuous spectrum. Approximations of eigenvalues and full numerical simulations of the fifth-order KdV equation confirm stability of two-pulse solutions related to the minima of the effective interaction potential and instability of two-pulse solutions related to the maxima points."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nWhat is the significance of the Petviashvili method of successive iterations in the context of numerical approximations of pulses in the fifth-order KdV equation, and how does it relate to the convergence of iterations in a neighborhood of two-pulse solutions?\n\nA) The Petviashvili method is a numerical technique used to approximate eigenvalues of the linearized KdV equation, and its convergence is crucial for stability analysis in Pontryagin spaces.\n\nB) The Petviashvili method is a spectral approximation method that proves convergence of iterations in a neighborhood of two-pulse solutions, allowing for the numerical approximation of eigenvalues in exponentially weighted spaces.\n\nC) The Petviashvili method is a numerical technique used to approximate the effective interaction potential, and its convergence is essential for stability analysis of two-pulse solutions.\n\nD) The Petviashvili method is a linearized KdV equation solver, and its convergence is necessary for the numerical approximation of eigenvalues in the context of spectral stability.\n\nCorrect Answer: B) The Petviashvili method is a spectral approximation method that proves convergence of iterations in a neighborhood of two-pulse solutions, allowing for the numerical approximation of eigenvalues in exponentially weighted spaces.\n\nExplanation: The correct answer is B) because the Petviashvili method is indeed a spectral approximation method used to approximate pulses in the fifth-order KdV equation. The documentation states that the method \"proves convergence of iterations in a neighborhood of two-pulse solutions\", which is crucial for the numerical approximation of eigenvalues in exponentially weighted spaces. This is a key result in the context of spectral stability of two-pulse solutions."}, "50": {"documentation": {"title": "SO(d,1)-invariant Yang-Baxter operators and the dS/CFT correspondence", "source": "Stefan Hollands and Gandalf Lechner", "docs_id": "1603.05987", "section": ["gr-qc", "hep-th", "math-ph", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SO(d,1)-invariant Yang-Baxter operators and the dS/CFT correspondence. We propose a model for the dS/CFT correspondence. The model is constructed in terms of a \"Yang-Baxter operator\" $R$ for unitary representations of the deSitter group $SO(d,1)$. This $R$-operator is shown to satisfy the Yang-Baxter equation, unitarity, as well as certain analyticity relations, including in particular a crossing symmetry. With the aid of this operator we construct: a) A chiral (light-ray) conformal quantum field theory whose internal degrees of freedom transform under the given unitary representation of $SO(d,1)$. By analogy with the $O(N)$ non-linear sigma model, this chiral CFT can be viewed as propagating in a deSitter spacetime. b) A (non-unitary) Euclidean conformal quantum field theory on ${\\mathbb R}^{d-1}$, where $SO(d,1)$ now acts by conformal transformations in (Euclidean) spacetime. These two theories can be viewed as dual to each other if we interpret ${\\mathbb R}^{d-1}$ as conformal infinity of deSitter spacetime. Our constructions use semi-local generator fields defined in terms of $R$ and abstract methods from operator algebras."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary role of the Yang-Baxter operator $R$ in the proposed model for the dS/CFT correspondence, and how does it relate to the construction of chiral and Euclidean conformal quantum field theories?\n\nA) The Yang-Baxter operator $R$ is used to define the semi-local generator fields, which in turn are used to construct the chiral CFT. The $R$-operator is also used to impose analyticity relations, including crossing symmetry, on the chiral CFT.\n\nB) The Yang-Baxter operator $R$ is used to define the conformal transformations in the Euclidean spacetime, which in turn are used to construct the non-unitary Euclidean CFT. The $R$-operator is also used to impose analyticity relations, including crossing symmetry, on the Euclidean CFT.\n\nC) The Yang-Baxter operator $R$ is used to define the internal degrees of freedom of the chiral CFT, which in turn are used to construct the chiral CFT. The $R$-operator is also used to impose analyticity relations, including crossing symmetry, on the chiral CFT.\n\nD) The Yang-Baxter operator $R$ is used to define the conformal transformations in the deSitter spacetime, which in turn are used to construct the chiral CFT. The $R$-operator is also used to impose analyticity relations, including crossing symmetry, on the chiral CFT.\n\nCorrect Answer: A) The Yang-Baxter operator $R$ is used to define the semi-local generator fields, which in turn are used to construct the chiral CFT. The $R$-operator is also used to impose analyticity relations, including crossing symmetry, on the chiral CFT.\n\nExplanation: The correct answer is A) because the Yang-Baxter operator $R$ plays a crucial role in defining the semi-local generator fields, which are used to construct the chiral CFT. Additionally, the $R$-operator is used to impose analyticity relations, including crossing symmetry, on the chiral CFT, which is a key feature of the proposed model for the dS/CFT correspondence. The other options are incorrect because they do not accurately describe the role of the Yang-Baxter operator $R$ in the proposed model."}, "51": {"documentation": {"title": "Mapping systemic risk: critical degree and failures distribution in\n  financial networks", "source": "Matteo Smerlak, Brady Stoll, Agam Gupta, James S. Magdanz", "docs_id": "1402.4783", "section": ["q-fin.GN", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mapping systemic risk: critical degree and failures distribution in\n  financial networks. The 2008 financial crisis illustrated the need for a thorough, functional understanding of systemic risk in strongly interconnected financial structures. Dynamic processes on complex networks being intrinsically difficult, most recent studies of this problem have relied on numerical simulations. Here we report analytical results in a network model of interbank lending based on directly relevant financial parameters, such as interest rates and leverage ratios. Using a mean-field approach, we obtain a closed-form formula for the \"critical degree\", viz. the number of creditors per bank below which an individual shock can propagate throughout the network. We relate the failures distribution (probability that a single shock induces $F$ failures) to the degree distribution (probability that a bank has $k$ creditors), showing in particular that the former is fat-tailed whenever the latter is. Our criterion for the onset of contagion turns out to be isomorphic to the condition for cooperation to evolve on graphs and social networks, as recently formulated in evolutionary game theory. This remarkable connection supports recent calls for a methodological rapprochement between finance and ecology."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the network model of interbank lending described in the paper \"Mapping systemic risk: critical degree and failures distribution in financial networks\", what is the relationship between the \"critical degree\" and the \"failures distribution\", and how does this relationship relate to the degree distribution?\n\n**A)** The critical degree is directly proportional to the failures distribution, and the relationship is linear.\n\n**B)** The critical degree is inversely proportional to the failures distribution, and the relationship is logarithmic.\n\n**C)** The critical degree is equal to the failures distribution, and the relationship is exponential.\n\n**D)** The critical degree is related to the failures distribution through a fat-tailed distribution, and the relationship is isomorphic to the condition for cooperation to evolve on graphs and social networks.\n\n**Correct Answer:** D) The critical degree is related to the failures distribution through a fat-tailed distribution, and the relationship is isomorphic to the condition for cooperation to evolve on graphs and social networks.\n\n**Explanation:** The paper states that \"We relate the failures distribution (probability that a single shock induces F failures) to the degree distribution (probability that a bank has k creditors), showing in particular that the former is fat-tailed whenever the latter is.\" This indicates that the relationship between the critical degree and the failures distribution is fat-tailed, and that this relationship is isomorphic to the condition for cooperation to evolve on graphs and social networks, as recently formulated in evolutionary game theory."}, "52": {"documentation": {"title": "Differentially Private Inference for Binomial Data", "source": "Jordan Awan and Aleksandra Slavkovic", "docs_id": "1904.00459", "section": ["math.ST", "cs.CR", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Differentially Private Inference for Binomial Data. We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a 'Neyman-Pearson lemma' for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin ''Truncated-Uniform-Laplace'' (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact $p$-values, which are easily computed in terms of the Tulap random variable. Using the above techniques, we show that our tests can be applied to give uniformly most accurate one-sided confidence intervals and optimal confidence distributions. We also derive uniformly most powerful unbiased (UMPU) two-sided tests, which lead to uniformly most accurate unbiased (UMAU) two-sided confidence intervals. We show that our results can be applied to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that all our tests have exact type I error, and are more powerful than current techniques."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of Differentially Private Inference for Binomial Data, what is the name of the distribution used to coin the random variable in the post-processing of a test statistic, and what is its significance in computing exact p-values?\n\n**A)** Truncated-Uniform-Laplace (Tulap) distribution, which is a generalization of the Staircase and discrete Laplace distributions, and is used to compute exact p-values by transforming the test statistic into a function of the Tulap random variable.\n\n**B)** Uniform distribution, which is used to coin the random variable in the post-processing of a test statistic, and is significant in computing exact p-values by transforming the test statistic into a function of the Uniform random variable.\n\n**C)** Laplace distribution, which is used to coin the random variable in the post-processing of a test statistic, and is significant in computing exact p-values by transforming the test statistic into a function of the Laplace random variable.\n\n**D)** Exponential distribution, which is used to coin the random variable in the post-processing of a test statistic, and is significant in computing exact p-values by transforming the test statistic into a function of the Exponential random variable.\n\n**Correct Answer:** A) Truncated-Uniform-Laplace (Tulap) distribution, which is a generalization of the Staircase and discrete Laplace distributions, and is used to compute exact p-values by transforming the test statistic into a function of the Tulap random variable.\n\n**Explanation:** The correct answer is A) Truncated-Uniform-Laplace (Tulap) distribution. The Tulap distribution is a generalization of the Staircase and discrete Laplace distributions, and is used to coin the random variable in the post-processing of a test statistic. This distribution is significant in computing exact p-values by transforming the test statistic into a function of the Tulap random variable. The other options are incorrect because the Uniform distribution is not used to coin the random variable, the Laplace distribution is not a generalization of the Staircase and discrete Laplace distributions, and the Exponential distribution is not used to coin the random variable."}, "53": {"documentation": {"title": "Selecting Regions of Interest in Large Multi-Scale Images for Cancer\n  Pathology", "source": "Rui Aguiar, Jon Braatz", "docs_id": "2007.01866", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Selecting Regions of Interest in Large Multi-Scale Images for Cancer\n  Pathology. Recent breakthroughs in object detection and image classification using Convolutional Neural Networks (CNNs) are revolutionizing the state of the art in medical imaging, and microscopy in particular presents abundant opportunities for computer vision algorithms to assist medical professionals in diagnosis of diseases ranging from malaria to cancer. High resolution scans of microscopy slides called Whole Slide Images (WSIs) offer enough information for a cancer pathologist to come to a conclusion regarding cancer presence, subtype, and severity based on measurements of features within the slide image at multiple scales and resolutions. WSIs' extremely high resolutions and feature scales ranging from gross anatomical structures down to cell nuclei preclude the use of standard CNN models for object detection and classification, which have typically been designed for images with dimensions in the hundreds of pixels and with objects on the order of the size of the image itself. We explore parallel approaches based on Reinforcement Learning and Beam Search to learn to progressively zoom into the WSI to detect Regions of Interest (ROIs) in liver pathology slides containing one of two types of liver cancer, namely Hepatocellular Carcinoma (HCC) and Cholangiocarcinoma (CC). These ROIs can then be presented directly to the pathologist to aid in measurement and diagnosis or be used for automated classification of tumor subtype."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** What is the primary challenge in using Convolutional Neural Networks (CNNs) for object detection and classification in Whole Slide Images (WSIs) of microscopy slides, and how do the authors propose to address this challenge?\n\n**A)** WSIs have too few features, making it difficult for CNNs to learn from them. The authors propose using a combination of CNNs and traditional image processing techniques to extract features.\n\n**B)** WSIs have too many features, making it difficult for CNNs to focus on the most relevant ones. The authors propose using Reinforcement Learning and Beam Search to learn to progressively zoom into the WSI to detect Regions of Interest (ROIs).\n\n**C)** WSIs are too small, making it difficult for CNNs to detect objects. The authors propose using a larger input size for the CNNs to capture more features.\n\n**D)** WSIs are too complex, making it difficult for CNNs to classify them. The authors propose using a hierarchical approach to classify WSIs, starting from coarse features and moving to finer ones.\n\n**Correct Answer:** B) WSIs have too many features, making it difficult for CNNs to focus on the most relevant ones. The authors propose using Reinforcement Learning and Beam Search to learn to progressively zoom into the WSI to detect Regions of Interest (ROIs).\n\n**Explanation:** The correct answer is B) because the documentation states that WSIs have \"feature scales ranging from gross anatomical structures down to cell nuclei\", which is too complex for standard CNN models. The authors propose using Reinforcement Learning and Beam Search to learn to progressively zoom into the WSI to detect ROIs, which is a more suitable approach for handling the complexity of WSIs. The other options are incorrect because they do not accurately reflect the challenge posed by WSIs and the authors' proposed solution."}, "54": {"documentation": {"title": "Coexistence under hierarchical resource exploitation: the role of\n  R*-preemption tradeoff", "source": "Man Qi, Niv DeMalach, Tao Sun, Hailin Zhang", "docs_id": "1908.08464", "section": ["q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coexistence under hierarchical resource exploitation: the role of\n  R*-preemption tradeoff. Resource competition theory predicts coexistence and exclusion patterns based on species R*s, the minimum resource values required for a species to persist. A central assumption of the theory is that all species have equal access to resources. However, many systems are characterized by preemption exploitation, where some species deplete resources before their competitors can access them (e.g., asymmetric light competition, contest competition among animals). We hypothesize that coexistence under preemption requires an R*-preemption tradeoff, i.e., the species with the priority access should have a higher R* (lower efficiency). Thus, we developed an extension of resource competition theory to investigate partial and total preemption (in the latter, the preemptor is unaffected by species with lower preemption rank). We found that an R*-preemption tradeoff is a necessary condition for coexistence in all models. Moreover, under total preemption, the tradeoff alone is sufficient for coexistence. In contrast, under partial preemption, more conditions are needed, which restricts the parameter space of coexistence. Finally, we discussed the implications of our finding for seemingly distinct tradeoffs, which we view as special cases of R*-preemption tradeoff. These tradeoffs include the digger-grazer, the competition-colonization, and tradeoffs related to light competition between trees and understories."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of resource competition theory, what is the necessary condition for coexistence under preemption exploitation, and how does it differ between total and partial preemption?\n\nA) An R*-preemption tradeoff is sufficient for coexistence under both total and partial preemption, and it is a necessary condition for coexistence in all models.\nB) An R*-preemption tradeoff is a necessary condition for coexistence under total preemption, but it is not sufficient for coexistence under partial preemption.\nC) An R*-preemption tradeoff is a necessary condition for coexistence under partial preemption, but it is not sufficient for coexistence under total preemption.\nD) An R*-preemption tradeoff is a necessary condition for coexistence under both total and partial preemption, but it is not sufficient for coexistence in all models.\n\n**Correct Answer:** B) An R*-preemption tradeoff is a necessary condition for coexistence under total preemption, but it is not sufficient for coexistence under partial preemption.\n\n**Explanation:**\n\nThe correct answer is B) An R*-preemption tradeoff is a necessary condition for coexistence under total preemption, but it is not sufficient for coexistence under partial preemption.\n\nThe documentation states that an R*-preemption tradeoff is a necessary condition for coexistence in all models, and that under total preemption, the tradeoff alone is sufficient for coexistence. However, under partial preemption, more conditions are needed for coexistence, which restricts the parameter space of coexistence.\n\nTherefore, option B is the correct answer, as it accurately reflects the relationship between R*-preemption tradeoff and coexistence under total and partial preemption."}, "55": {"documentation": {"title": "Semiclassical description of a sixth order quadrupole boson Hamiltonian", "source": "F. D. Aaron and A. A. Raduta", "docs_id": "nucl-th/0507069", "section": ["nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Semiclassical description of a sixth order quadrupole boson Hamiltonian. A sixth order quadrupole boson Hamiltonian is treated through a time dependent variational principle approach choosing as trial function a coherent state with respect to zeroth $b^{\\dagger}_0$ and second $b^{\\dagger}_2+b^{\\dagger}_{-2}$ components of the quadrupole bosons. The coefficients involved in the model Hamiltonian are chosen so that the classical effective potential energy term has two distinct minima. The equation of motion for the radial coordinate is analytically solved and the resulting trajectories are extensively studied. One distinguishes three energy regions exhibiting different types of trajectories. When one passes from the region characterized by two wells to the region of energies higher than the maximum value of the effective potential the trajectories period exhibits a singularity which reflects a phase transition. The classical trajectories are quantized by a constraint similar to the Bohr-Sommerfeld quantization condition. The semiclassical spectra corresponding to the two potential wells have specific properties. The tunneling process through the potential barrier is also studied. The transmission coefficients exhibit jumps in magnitude when the angular momentum acquires certain values."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the time-dependent variational principle approach to the sixth order quadrupole boson Hamiltonian, what is the physical significance of the phase transition reflected by the singularity in the period of the classical trajectories when transitioning from the region of two wells to the region of energies higher than the maximum value of the effective potential energy term?\n\n**A)** The phase transition marks the onset of quantum fluctuations in the system, leading to a breakdown in the classical description.\n\n**B)** The phase transition is a result of the quantization of the classical trajectories by the Bohr-Sommerfeld quantization condition, which introduces a new constraint on the system.\n\n**C)** The phase transition is a manifestation of the tunneling process through the potential barrier, where the classical trajectories exhibit a sudden change in behavior due to the presence of a classically forbidden region.\n\n**D)** The phase transition is a consequence of the choice of the trial function, specifically the coherent state with respect to zeroth $b^{\\dagger}_0$ and second $b^{\\dagger}_2+b^{\\dagger}_{-2}$ components of the quadrupole bosons, which leads to a change in the effective potential energy term.\n\n**Correct Answer:** D) The phase transition is a consequence of the choice of the trial function, specifically the coherent state with respect to zeroth $b^{\\dagger}_0$ and second $b^{\\dagger}_2+b^{\\dagger}_{-2}$ components of the quadrupole bosons, which leads to a change in the effective potential energy term.\n\n**Explanation:** The correct answer is D) because the phase transition is indeed a consequence of the choice of the trial function, which affects the effective potential energy term. The trial function is chosen to have two distinct minima, leading to two wells in the effective potential energy term. When the energy exceeds the maximum value of the effective potential, the classical trajectories exhibit a singularity, reflecting a phase transition. This phase transition is a direct result of the change in the effective potential energy term, which is a consequence of the choice of the trial function."}, "56": {"documentation": {"title": "Inferring neutron star properties from GW170817 with universal relations", "source": "Bharat Kumar, Philippe Landry", "docs_id": "1902.04557", "section": ["gr-qc", "astro-ph.HE", "nucl-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inferring neutron star properties from GW170817 with universal relations. Because all neutron stars share a common equation of state, tidal deformability constraints from the compact binary coalescence GW170817 have implications for the properties of neutron stars in other systems. Using equation-of-state insensitive relations between macroscopic observables like moment of inertia ($I$), tidal deformability ($\\Lambda$) and stellar compactness, we derive constraints on these properties as a function of neutron-star mass based on the LIGO-Virgo collaboration's canonical deformability measurement, $\\Lambda_{1.4} = 190^{+390}_{-120}$. Specific estimates of $\\Lambda$, $I$, dimensionless spin $\\chi$, and stellar radius $R$ for a few systems targeted by radio or X-ray studies are extracted from the general constraints. We also infer the canonical neutron-star radius as $R_{1.4} = 10.9^{+1.9}_{-1.5}$ km at 90$\\%$ confidence. We further demonstrate how a gravitational-wave measurement of $\\Lambda_{1.4}$ can be combined with independent measurements of neutron-star radii to tighten constraints on the tidal deformability as a proxy for the equation of state. We find that GW170817 and existing observations of six thermonuclear bursters in low-mass X-ray binaries jointly imply $\\Lambda_{1.4} = 196^{+92}_{-63}$ at the 90$\\%$ confidence level."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** Inferring neutron star properties from GW170817 with universal relations involves using equation-of-state insensitive relations between macroscopic observables. What is the implication of combining a gravitational-wave measurement of tidal deformability with independent measurements of neutron-star radii, and how does this affect the constraints on the tidal deformability as a proxy for the equation of state?\n\n**A)** The combined measurement will lead to a tighter constraint on the dimensionless spin \u03c7, but will not affect the constraints on the stellar radius R.\n\n**B)** The combined measurement will lead to a tighter constraint on the stellar radius R, but will not affect the constraints on the dimensionless spin \u03c7.\n\n**C)** The combined measurement will lead to a tighter constraint on the tidal deformability \u039b, which will in turn affect the constraints on the equation of state.\n\n**D)** The combined measurement will lead to a tighter constraint on the equation of state, but will not affect the constraints on the tidal deformability \u039b.\n\n**Correct Answer:** C) The combined measurement will lead to a tighter constraint on the tidal deformability \u039b, which will in turn affect the constraints on the equation of state.\n\n**Explanation:** The correct answer is C) because the combined measurement of tidal deformability with independent measurements of neutron-star radii will lead to a tighter constraint on the tidal deformability \u039b, which is a proxy for the equation of state. This is because the tidal deformability is related to the equation of state, and a more precise measurement of \u039b will provide more information about the equation of state. The other options are incorrect because the combined measurement does not lead to a tighter constraint on the dimensionless spin \u03c7 (A), the stellar radius R (B), or the equation of state (D)."}, "57": {"documentation": {"title": "Some connections between the Classical Calogero-Moser model and the Log\n  Gas", "source": "Sanaa Agarwal, Manas Kulkarni, Abhishek Dhar", "docs_id": "1903.09380", "section": ["cond-mat.stat-mech", "hep-th", "math-ph", "math.MP", "nlin.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Some connections between the Classical Calogero-Moser model and the Log\n  Gas. In this work we discuss connections between a one-dimensional system of $N$ particles interacting with a repulsive inverse square potential and confined in a harmonic potential (Calogero-Moser model) and the log-gas model which appears in random matrix theory. Both models have the same minimum energy configuration, with the particle positions given by the zeros of the Hermite polynomial. Moreover, the Hessian describing small oscillations around equilibrium are also related for the two models. The Hessian matrix of the Calogero-Moser model is the square of that of the log-gas. We explore this connection further by studying finite temperature equilibrium properties of the two models through Monte-Carlo simulations. In particular, we study the single particle distribution and the marginal distribution of the boundary particle which, for the log-gas, are respectively given by the Wigner semi-circle and the Tracy-Widom distribution. For particles in the bulk, where typical fluctuations are Gaussian, we find that numerical results obtained from small oscillation theory are in very good agreement with the Monte-Carlo simulation results for both the models. For the log-gas, our findings agree with rigorous results from random matrix theory."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** The Calogero-Moser model and the log-gas model share a common minimum energy configuration, with particle positions given by the zeros of the Hermite polynomial. However, the Hessian matrix of the Calogero-Moser model is the square of that of the log-gas model. What is the relationship between the single-particle distribution of the Calogero-Moser model and the marginal distribution of the boundary particle of the log-gas model at finite temperature?\n\nA) The single-particle distribution of the Calogero-Moser model is given by the Tracy-Widom distribution, while the marginal distribution of the boundary particle of the log-gas model is given by the Wigner semi-circle.\n\nB) The single-particle distribution of the Calogero-Moser model is given by the Wigner semi-circle, while the marginal distribution of the boundary particle of the log-gas model is given by the Tracy-Widom distribution.\n\nC) The single-particle distribution of the Calogero-Moser model is given by the Tracy-Widom distribution, while the marginal distribution of the boundary particle of the log-gas model is given by the Wigner semi-circle.\n\nD) The single-particle distribution of the Calogero-Moser model is given by the Wigner semi-circle, while the marginal distribution of the boundary particle of the log-gas model is given by the Tracy-Widom distribution.\n\n**Correct Answer:** A) The single-particle distribution of the Calogero-Moser model is given by the Tracy-Widom distribution, while the marginal distribution of the boundary particle of the log-gas model is given by the Wigner semi-circle.\n\n**Explanation:**\n\nThe correct answer is A) because the single-particle distribution of the Calogero-Moser model is indeed given by the Tracy-Widom distribution, as mentioned in the documentation. Similarly, the marginal distribution of the boundary particle of the log-gas model is given by the Wigner semi-circle. The relationship between the two models is further supported by the fact that the Hessian matrix of the Calogero-Moser model is the square of that of the log-gas model, indicating a deep connection between the two models.\n\nThe incorrect options (B, C, and D) are plausible but incorrect, and require careful reading of the documentation to eliminate. Option B is incorrect because the single-particle distribution of the Calogero-Moser model is not given by the Wigner semi-circle, but rather by the Tracy-Widom distribution. Option C is incorrect because the marginal distribution of the boundary particle of the log-gas model is not given by the Tracy-Widom distribution, but rather by the Wigner semi-circle. Option D is incorrect because the single-particle distribution of the Calogero-Moser model is not given by the Wigner semi-circle, but rather by the Tracy-Widom distribution."}, "58": {"documentation": {"title": "Fractional Statistics in One-Dimension: View From An Exactly Solvable\n  Model", "source": "Z. N. C. Ha (School of Natural Sciences, IAS Princeton)", "docs_id": "cond-mat/9410101", "section": ["cond-mat", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fractional Statistics in One-Dimension: View From An Exactly Solvable\n  Model. One-dimensional fractional statistics is studied using the Calogero-Sutherland model (CSM) which describes a system of non-relativistic quantum particles interacting with inverse-square two-body potential on a ring. The inverse-square exchange can be regarded as a pure statistical interaction and this system can be mapped to an ideal gas obeying the fractional exclusion and exchange statistics. The details of the exact calculations of the dynamical correlation functions for this ideal system is presented in this paper. An effective low-energy one-dimensional ``anyon'' model is constructed; and its correlation functions are found to be in agreement with those in the CSM; and this agreement provides an evidence for the equivalence of the first- and the second-quantized construction of the 1D anyon model at least in the long wave-length limit. Furthermore, the finite-size scaling applicable to the conformally invariant systems is used to obtain the complete set of correlation exponents for the CSM."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\n**Question:** In the context of the Calogero-Sutherland model (CSM), what is the physical significance of the equivalence between the first- and second-quantized constructions of the 1D anyon model, as demonstrated in the provided Arxiv documentation?\n\n**A)** The equivalence implies that the CSM can be reduced to a single-particle problem, allowing for a simplified analysis of the system's behavior.\n\n**B)** The equivalence demonstrates that the CSM can be mapped to an ideal gas obeying fractional exclusion and exchange statistics, providing a new perspective on the system's behavior.\n\n**C)** The equivalence suggests that the CSM is a conformally invariant system, and the finite-size scaling analysis provides a complete set of correlation exponents for the system.\n\n**D)** The equivalence implies that the CSM can be used to study the behavior of anyons in a 1D system, and the results can be applied to the study of topological phases of matter.\n\n**Correct Answer:** B) The equivalence demonstrates that the CSM can be mapped to an ideal gas obeying fractional exclusion and exchange statistics, providing a new perspective on the system's behavior.\n\n**Explanation:** The correct answer is B) because the provided documentation states that the CSM can be mapped to an ideal gas obeying fractional exclusion and exchange statistics, which is a key result of the equivalence between the first- and second-quantized constructions of the 1D anyon model. This mapping provides a new perspective on the system's behavior, allowing for a deeper understanding of the underlying physics. The other options are incorrect because they do not accurately reflect the physical significance of the equivalence demonstrated in the documentation."}, "59": {"documentation": {"title": "Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion\n  Deblurring", "source": "Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan", "docs_id": "2004.05343", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion\n  Deblurring. This paper tackles the problem of motion deblurring of dynamic scenes. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in non-uniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Existing approaches achieve a large receptive field by increasing the number of generic convolution layers and kernel-size, but this comes at the expense of of the increase in model size and inference speed. In this work, we propose an efficient pixel adaptive and feature attentive design for handling large blur variations across different spatial locations and process each test image adaptively. We also propose an effective content-aware global-local filtering module that significantly improves performance by considering not only global dependencies but also by dynamically exploiting neighbouring pixel information. We use a patch-hierarchical attentive architecture composed of the above module that implicitly discovers the spatial variations in the blur present in the input image and in turn, performs local and global modulation of intermediate features. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate that our design offers significant improvements over the state-of-the-art in accuracy as well as speed."}, "answer": "Here is a difficult multi-form question based on the provided documentation:\n\nQuestion: What is the primary limitation of existing approaches to non-uniform motion deblurring, according to the paper \"Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring\"?\n\nA) They do not consider global dependencies between pixels.\nB) They do not exploit neighbouring pixel information.\nC) They increase the model size and inference speed at the expense of performance.\nD) They do not handle large blur variations across different spatial locations.\n\nCorrect Answer: C) They increase the model size and inference speed at the expense of performance.\n\nExplanation: The paper states that existing approaches achieve a large receptive field by increasing the number of generic convolution layers and kernel-size, but this comes at the expense of the increase in model size and inference speed. This is the primary limitation mentioned in the paper."}}